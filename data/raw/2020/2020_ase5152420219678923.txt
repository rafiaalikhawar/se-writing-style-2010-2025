Automating Developer Chat Mining
Shengyi Pan∗†, Lingfeng Bao∗¶, Xiaoxue Ren∗, Xin Xia‡, David Lo§, Shanping Li∗
∗College of Computer Science and Technology, Zhejiang University, China
‡Faculty of Information Technology, Monash University, Australia
§School of Information Systems, Singapore Management University, Singapore
shengyipan@outlook.com, {lingfengbao,xxren,shan}@zju.edu.cn, Xin.Xia@monash.edu, davidlo@smu.edu.sg
Abstract —Online chatrooms are gaining popularity as a com-
munication channel between widely distributed developers of
Open Source Software (OSS) projects. Most discussion threads in
chatrooms follow a Q&A format, with some developers (askers)raising an initial question and others ( respondents) joining in
to provide answers. These discussion threads are embedded withrich information that can satisfy the diverse needs of various OSSstakeholders. However, retrieving information from threads ischallenging as it requires a thread-level analysis to understand thecontext. Moreover, the chat data is transient and unstructured,consisting of entangled informal conversations. In this paper, weaddress this challenge by identifying the information types avail-able in developer chats and further introducing an automatedmining technique. Through manual examination of chat datafrom three chatrooms on Gitter, using card sorting, we builda thread-level taxonomy with nine information categories andcreate a labeled dataset with 2,959 threads. We propose a classi-ﬁcation approach (named F2C
HA T ) to structure the vast amount
of threads based on the information type automatically, helpingstakeholders quickly acquire their desired information. F2C
HA T
effectively combines handcrafted non-textual features with deeptextual features extracted by neural models. Speciﬁcally, it hastwo stages with the ﬁrst one leveraging the siamese architectureto pretrain the textual feature encoder, and the second onefacilitating an in-depth fusion of two types of features. Evaluationresults suggest that our approach achieves an average F1-scoreof 0.628, which improves the baseline by 57%. Experimentsalso verify the effectiveness of our identiﬁed non-textual featuresunder both intra-project and cross-project validations.
Index T erms—Developer Chatrooms, Information Mining,
Deep Learning, Gitter
I. I NTRODUCTION
Recent studies showed that online chatrooms are gaining
popularity as a channel for global collaboration among devel-
opers of Open Source Software (OSS) projects and replacingthe traditional communication platforms, including emails andmailing lists [1]–[3]. Chatrooms like Gitter [4], Slack [5], andDiscord [6], are modern instant messaging systems integratedwith diverse external services (e.g., bots, issue linking), mak-ing developers easier to communicate and collaborate withothers [7]. Developers use chatrooms to report problems, shareopinions, and discuss implementation details [8], [9].
Most discussion threads in developer chatrooms generally
follow a Q&A format [8], [9], with some developers (askers)raising an initial question and others (respondents) later joiningin to provide answers. Previous studies [7]–[11] have revealed
†Also with PengCheng Laboratory.¶Corresponding author.
A:I'm getting an error...
A: "o.d.o.s.VectorizedNonZeroStoppingConjugateGradient - main C
onjugateGradient: At iteration 1464, cost = NaN -100 “
A: Surely NaN isn't the kind of result I should be looking for? Wha t 
does revert back to GA mean?
R:@Asorry just saw this, Didn't see the notification
R:Do me a favor and type: Nd4j.ENFORCE_NUMERICAL_STABILITY
= true; Put this above your training.
R:Your weights are diverging
Report an encountered problem
A:hello, If I saved network with ModelSerializer.writeModel() can I t
hen restore it and continue training? Is it done like this? MultiLayer
Network net =(MultiLayerNetwork) ModelSerializer.restoreMultiLayerNetwork(file) net.fit(trainIter)R:yes, don't forget restore updater
R:with boolean argument set to true
A:thank you
Ask for help about the use of an API
A: @Rhave you seen https://github.com/deeplearning4j/dl4j-exam
ples/issues/590 ? Weird behavior with SequenceRecordReaderData
setIterator . Probably shouldn’t have posted this issue under exampl
es though.R: @A no, I missed that. mind moving it to DL4J issues?
A: @R how can I move issues? Or do I have to close and reopen?
R: close and reopen
Collaborate on solving an issue1
2
3
Fig. 1: Example discussion threads with three different intents
from Deeplearning4j chatroom on Gitter. Aand Rrepresent
the asker and the respondent, respectively.
that these discussion threads are embedded with rich infor-
mation, which can be utilized to support various developmentactivities. Conversations in developer chatrooms are typicallyinformal, with rapid exchanges of messages between two ormore participants and fewer constraints on discussion topics.Compared with other communication channels, e.g., mailinglists and issue tracking systems (ITS), both types and amountof valuable information in chatrooms are much richer [9], [12].
The needs to acquire information from discussions in chat-
rooms vary among different OSS stakeholders based on theirtasks and interests. For development teams, they may want tobe aware of the problems reported by end-users to deliver anearly ﬁx to a bug. They may also want to monitor the progressof tasks towards the resolutions of issues to facilitate better col-laboration. For end-users, they may care more about solutionsand opinions in previous discussions related to their problems.Similar questions are usually repeated many times by thecommunity as the chat conversations are generally short-lived[9], quickly ﬂooded away by the incoming messages. Figure 1illustrates that information embedded in discussion threads is
8542021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000802021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678923
978-1-6654-0337-5/21/$31.00  ©2021  IEEE
capable of fulﬁlling the needs of various OSS stakeholders.
Hence, a comprehensive analysis of information categoriesavailable in developer chats is of vital importance.
Retrieving information from massive chat data requires a
huge effort, as some discussions could be very lengthy andhard to follow. Automated mining techniques are urgentlyneeded to collect various information embedded in massivechat data and categorize it properly, helping OSS stakeholdersdirectly retrieve their desired information from the well struc-tured data. However, there are several hurdles that may preventan effective mining. 1) Thread-level analysis. Different from
prior mining tasks [11], [13]–[15] focusing on sentence-level classiﬁcation, thread is a natural granularity for mininginformation from developer chatrooms as the smallest unitcontaining comprehensive context information. A sentence-level analysis abandoning the context prevents an accurateinterpretation during classiﬁcation, and faces lots of short andincomplete sentences (see examples in Figure 1). 2) Noisy
data. Chatrooms have multiple participants who take part
in different discussions at the same time, thus disentanglingthreads from the stream of messages is critical for enablinga thread-level analysis. Additionally, informal conversationscontain many short and meaningless instant messages, as wellas typos and special tokens (e.g., code snippet, URL), whichgreatly affect the performance of text classiﬁcation techniques.To the best of our knowledge, the approach proposed in [12](named FRMiner) is the only one targeted towards mininginformation from developer chats at thread level. However,FRMiner is only focused on detecting one speciﬁc type ofdiscussion threads, i.e., threads with hidden feature requests,to support the release team, while ignoring other valuableinformation and different interests of other OSS stakeholders.
The machine learning (ML) based mining techniques in
previous studies typically rely on shallow textual features (e.g.,TF-IDF, bag-of-words) [16], [17], non-textual features (e.g.,sentence length, time gap) [18], [19] or the combination ofboth features [13], [20]. Specially, Arya et al. [13] and Wood et
al.[20] reported that non-textual features are more useful com-
pared with the textual counterpart under certain circumstances.The deep learning (DL) based approaches in recent studies[12], [21] leverage advanced neural models to extract deeptextual features, which are powerful representations with high-level semantic information. However, we argue that the textualfeatures ignore other information of the discussion thread (e.g.,structure, participant). Thus, the DL-based approaches maystill beneﬁt from the handcrafted non-textual features in thisspeciﬁc task.
In this work, we take the ﬁrst step to analyze the infor-
mation types available in discussion threads from developerchatrooms. We aim to reveal the characteristics of informationtypes, their primary intents, and possible applications, whichare essential for designing automated mining techniques tofulﬁll the needs of various OSS stakeholders. Through manualexamination of historical chat data (2,959 threads) from threechatrooms on Gitter, using card sorting, we build a thread-level taxonomy with nine information categories. Further, wepropose a classiﬁcation approach, namely F2C
HA T , which
effectively combines handcrafted non-textual Features with
deep textual Features extracted by neural models. Speciﬁcally,
it has two stages with the ﬁrst one leveraging the siamesearchitecture to pretrain the textual feature encoder, and thesecond one facilitating an in-depth fusion of two types offeatures. We evaluate our approach on 2,959 threads labeledusing the deﬁned taxonomy. The experimental results indicatethat our approach improves the performance of FRMiner by57%, with an overall F1-score of 0.628. The experiments alsoverify the effectiveness of our identiﬁed non-textual featuresunder both intra-project and cross-project validations.
The contributions of our work are summarized as follows:
•We are the ﬁrst to build a thread-level taxonomy ofinformation types for threads in developer chatrooms.
•We propose an automated mining approach (F2C HA T ),
which combines handcrafted non-textual features withdeep textual features extracted by neural models.
•We conduct extensive experiments to evaluate F2C HA T
on three chatrooms from Gitter. The experimental resultsindicate that our approach substantially outperforms FR-Miner and the identiﬁed non-textual features are effectivein both intra-project and cross-project validations.
•We open source our replication package and a datasetof 2,959 discussion threads [22], annotated with theidentiﬁed taxonomy of information categories.
II. RELA TED WORK
A. Developer Online Chatrooms
Previous studies reported that online chatroom plays an
increasingly important role in various development activities,and tried to understand the way of developers using chat-rooms by analyzing their behaviors and interactions. Shihabet al. [23], [24] reported that there is a shift from mailinglists to developer Internet Relay Chat (IRC) meetings, andfurther investigated the role of IRC meetings using two opensource projects. Lin et al. [1] conducted an exploratory study
to investigate the way of developers using Slack. Their ﬁndingssuggested that Slack is used for personal, team-wide, andcommunity-wide purposes, and is gradually replacing theemails. Sahar et al. [7] assessed the impact of Gitter on project
and team dynamics. They focused on issue report discussionsand found that they are closely related to activities in theGitHub issue tracker. Ehasan et al. [8] conducted a study to
understand the general Q&A behaviors of discussions in Gitter.They are the ﬁrst to propose an automatic approach for threadidentiﬁcation in developer chatrooms and further explore thenature of discussions (e.g., topics) by performing a thread-levelanalysis. These two works suggested that Gitter chatrooms arerich sources for information related to the software.
Other works focused on mining information from devel-
oper chats to support software development and maintenance.Alkadhi et al. [10], [11] conducted exploratory studies to
examine the frequency and completeness of rationale hidden inchat messages. They found that chat messages are a valuable
855source for rationale and the machine-learning based algorithms
are capable of automatically extracting rationale from IRCmessages. Chatterjee et al. [9] pointed out that conversations
in developer chatrooms generally follow a Q&A format andinvestigated the types, amount and possible mining hurdlesof information in Slack Q&A chats compared with StackOverﬂow Q&A posts. They also emphasized the importanceof a thread-level analysis, and later released a dataset ofsoftware related conversations from Slack with a customizeddisentanglement algorithm in [25]. Recently, Shi et al. [12]
designed a deep-learning model to detect threads with hiddenfeature requests. Their experimental results on three OSSchatrooms from Slack suggested that their method outperformsthe existing sentence-level methods by a large margin.
Most prior works are exploratory studies, aiming to under-
stand the role of chatrooms in supporting developers or inves-tigate the validity of chatrooms as a mining source. Our workdiffers from these studies. We take a step forward to performan in-depth thread-level analysis for identifying informationtypes available in developer chats on Gitter. Additionally, wedesign a classiﬁcation approach and evaluate its effectivenessin detecting information types of discussion threads.
B. Classiﬁcation of Software-related Artifacts
In recent years, an increasing number of studies have
focused on mining information from software-related artifacts
by classifying them into categories relevant to various soft-ware activities. The automated mining techniques proposedby prior works can be generally categorized into three groupsaccording to the leveraged features: 1) Textual features.
Bacchelli et al. [16] represented lines of development emails
as vectors of term frequencies (TF) and applied machine-learning (ML) techniques to classify them into ﬁve categories(nature language, source code, patch, stack trace, and junk)based on the speciﬁc content. However, Di Sorbo et al. [14]
and Panichella et al. [15] argued that techniques based on
lexicon analysis, such as V ector Space Models [26] (e.g., TF-IDF, bag-of-words) and topic models (e.g., LDA), would notbe sufﬁcient as they failed to reveal the developers’ intents.To bridge the gap, they applied heuristics to capture linguisticpatterns for the classiﬁcation of sentences in emails [14] andapp reviews [15] based on developers’ purposes. Recently,deep learning (DL) approaches have been introduced in thistask, which automatically learn linguistic patterns and are morepowerful in extracting high-level semantic information. Huanget al. [21] applied textCNN [27] to classify sentences in both
emails and ITSs. Their method substantially outperformedDi Sorbo et al.’s heuristics [14] and ﬁve ML based text
classiﬁcation techniques using V ector Space Model. Shi et
al.[12] proposed a novel approach to enable a thread-level
analysis for detecting hidden feature requests from discussionson Slack. Besides, They are the ﬁrst to incorporate few-shot learning techniques [28] to make the maximum use oflimited labeled data. 2) Non-textual features. Rastkar et
al.[18] identiﬁed 24 non-textual features from four aspects
(structure, participant, length and lexicon), and trained logisticregression (LR) classiﬁers for automatic summarization of bugreports. Similar non-textual features were also utilized in [19]to generate summaries for developer-client conversations. 3)Both textual and non-textual features. Wood et al. [20]
utilized bag-of-words as the textual feature together with threenon-textual features to train LR classiﬁers for detection of26 speech act types in conversations during bug repair. Theyreported that non-texture features are very useful for certainspeech act types. Arya et al. [13] leveraged several ML al-
gorithms for classifying sentences in issues. They applied TF-IDF weights as the textual feature and identiﬁed 14 non-textualfeatures. They found that using non-textual features alone iseven better than both features under intra-project validation.The shallow textual features used in these two works onlycontain lexical information. Moreover, the fusion of featuresis simple as inputting to the ML classiﬁers simultaneously.
Our work differs from the existing studies. To the best of
our knowledge, we are the ﬁrst to combine handcrafted non-textual features with deep textual features extracted by neuralmodels for classiﬁcation of software-related artifacts. We alsopropose an architecture to facilitate an in-depth fusion of twotypes of features. Furthermore, while most of the prior worksperformed classiﬁcation at the granularity of sentences, wefocus on a thread-level analysis of developer discussions.
III. A
NALYZING INFORMA TION CA TEGORIES OF
DISCUSSION THREADS
In this section, we try to identify potential information types
available in developer chats that may satisfy the diverse needsof various OSS stakeholders. We ﬁrst introduce the four stepsused to collect chat data from Gitter chatrooms. Then, wedescribe the details of identifying information types.
A. Data Preparation
Step 1: Chatroom Selection. we select three chatrooms
on Gitter: Angular [29], Spring-boot [30] and Deeplearn-
ing4j [31]. We choose these chatrooms for the following rea-sons: 1) These chatrooms belong to three different categories.Gitter divides all its chatrooms into 24 categories [32], e.g.,Frontend, Android, and Data science. Choosing chatroomsof different categories strengthens the generalizability of ourwork by allowing us to investigate chat data of diversedevelopment topics. 2) Large numbers of developers activelycommunicate with each other in these chatrooms. We sort allthe chatrooms on Gitter based on the number of participantssince it is one of the attributes supported by Gitter API.We further exclude chatrooms with limited chat data (lessthan 50,000 messages) by crawling and counting all of itshistorical messages. Finally, we select the top three chatroomsunder the premise of belonging to different categories. Thecharacteristics of these chatrooms are presented in Table I.
Step 2: Data Crawling. We crawl the historical chat data of
the selected chatrooms using the ofﬁcial API [33] provided by
Gitter. We crawled data on August 20, 2020.
Step 3: Thread Disentanglement. In chatrooms, multiple
developers participate in different threads at the same time.
856TABLE I: The characteristics of the selected chatrooms
Project Categories #Participants #Messages Time Duration
Angular Javascript 22,771 1,120,438 2015.03-2020.08
Spring-boot Java 9,665 70,590 2014.10-2020.08
Deeplearning4j Data Science 8,356 415,422 2015.03-2020.08
Two latest disentanglement algorithms targeted for technical
discussions in developer chatrooms are [8], [25]. Chatterjeeet al. [25] modiﬁed Elsner and Charniak’s algorithm [34] for
the customization of several Slack speciﬁc features. Ehsan et
al.[8] identiﬁed three categories of features (i.e., users, content
and back-and-forth communication) through manual analysisof Gitter chats and futher designed a heuristic-based algorithm.We apply Ehasan et al.’s algorithm to disentangle the distinct
threads from the stream of messages, since it was demonstratedto achieve satisfactory results on Gitter chats with an F1-scoreof 0.81 [8]. While Chatterjee et al.’s algorithm still requires to
be adapted to work well on chat platforms other than Slack,including Gitter used in our study, as suggested in [25].
Step 4: Thread Sampling. We randomly sample 1,000 threads
from each chatroom and manually exclude low-quality threads
of the following characteristics: 1) Threads that contain toomuch unformatted source code or stack traces. 2) Threads withtoo many spelling and grammatical errors or written in non-English languages. The number of available threads for eachchatroom is shown in column THD of Table IV.
B. Building Taxonomy of Information Types
The sentence-level information types have been widely stud-
ied in prior works regarding the classiﬁcation of developmentcommunication artifacts [13], [20], [21]. But they are notapplicable to threads with multiple messages, as they focuson the content of an individual sentence while abandoningthe context. However, we manage to utilize the knowledge intwo latest sentence-level works: 1) Arya et al. [13] identiﬁed
16 information types (e.g., Bug Reproduction and SolutionDiscussion) for discussions in ITSs based on the underlyingpurposes of users. We build our thread-level taxonomy bysummarizing their sentence-level information types. 2) Woodet al. [20] uncovered 26 speech act types (e.g., Documentation
Answer and API Question) in conversations during bug repairbased on the speciﬁc discussed development contents. Theirwork inspires us to build a taxonomy from the aspect of dis-cussion contents. Di Sorbo et al. [14] and Panichella et al. [15]
reported that taxonomies in related works are generally deﬁnedfrom two aspects: text contents or developers’ purposes.
Besides, we follow the card sorting process [35] to identify
the information types of discussion threads, which has beenproven effective in previous works, e.g., identifying intentionsof sentences in ITS [21]. We created one card for each sampledthread. The ﬁrst two authors worked together to determine thelabel of each card. The whole process has two iterations:
Iteration 1.
We ﬁrst used 20% of the cards. The two
authors coded each thread individually to identify possiblecategories at thread level. They ﬁrst read all messages in thethread and identiﬁed information type for each one using thetaxonomy described in [13]. Then, they summarized potentialcommonalities of the sentence-level information types andinferred the underlying purpose of the discussion. After that,they focused on the speciﬁc development contents discussedin the thread. Finally, they worked together to discuss theﬁndings and disagreements. In this iteration, we built a two-level taxonomy (Table II) along with a handbook to guideclassiﬁcation based on the following ﬁndings:
Level 1: We summarize three intents of developer discussions
by leveraging the 16 sentence-level information types in [13].
•Problem Report. Askers (mostly the project end-users)
report bugs or describe unexpected behaviors by providing
source code, full stack traces, or specifying what they havetried to do. Respondents try to help askers ﬁnd the cause oftheir problems, guide them in the right direction, or sharepossible solutions. See 1/circlecopyrtin Figure 1 for an example.
•Information Retrieval. Askers (mostly the project end-
users) attempt to obtain information or help from the com-munity about API usage, library installation, documentationresources, etc. Usually, askers describe what they want toaccomplish and try to get some help before they dive intothe speciﬁc development. Respondents share opinions aboutthe best practice or provide suggestions of implementationdetails. See 2/circlecopyrtin Figure 1 for an example.
•Project Management. Although most discussion threads
in developer chatrooms are initiated with end-users askingquestions about the use of the software, team membersand certain users (potential contributors) also collaboratethrough online chatting to solve issues ﬁled on GitHub. Forexample, working together to identify causes and solutions,discussing the testing procedure and results, and requestingor reporting the progress of tasks. Besides, the team alsoannounce the release of a new version and answer questionsregarding the future roadmap of the software evolution inonline chatrooms. See 3/circlecopyrtin Figure 1 for an example.
Level 2: The speciﬁc development contents discussed in online
chatrooms can be categorized into nine classes as shown incolumn level 2 of Table II. We ﬁnd the development contents
discussed in chatrooms are generally consistent with thoseidentiﬁed by [20] in chat conversations during bug repair.However, we add a sub-class General Information under
information Retrieval, since there are some social discussionsthat are not closely related to the project itself, e.g., the bestchoice of IDE, job hunting experience, and the design ofdeep learning models. Besides, we divide Project Management
into two sub-classes: 1) Task Progress. The development team
and end-users request or report task progresses, inquire orinform release plans, and discuss future roadmap. 2) Technical
Discussion. Discussions of this class focus more on technical
parts, where participants collaborate to ﬁnd the cause of issues,analyze the testing results and seek possible solutions.
Iteration 2.
Two authors independently labeled the remain-
ing cards into nine categories listed in Table II. We useCohen’s Kappa coefﬁcient [36] to measure the agreementbetween two authors. Their Kappa value for nine categories
857TABLE II: The built taxonomy of information categories for developer discussion threads
Level 1 Description Level 2 Description
Problem
ReportAsker: report bugs or describe unexpected
behaviors.
Respondent: help the asker ﬁnd the cause of the
problem and provide possible solutions.Programming
Problem (PP)problems related to programming, e.g., syntax,parameter, API and implementation
Library
Problem (LP)problems related to library installation, deploymentand conﬁguration
Documentation
Problem (DP)problems related to documentation resources, e.g.,examples, guidance and docs
Information
RetrievalAsker: try to obtain information or help from the
community regarding some development issues.Respondent: share opinions about the best practice
or suggestions of implementation details.Programming
Information (PI)information related to programming, e.g., syntax,parameter, API and implementation
Library
Information (LI)information related to library installation,deployment and conﬁguration
Documentation
Information (DI)information related to documentation resources,e.g., examples, guidance and docs
General
Information (GI)information related to general knowledge, e.g., jobhunting experience and choices of IDEs
Project
ManagementEnd-user: request the progress of certain issues or
the release schedule of the project.Team: collaborate on solving issues and inform
community about future plans.Technical
Discussion (TD)technical discussions during collaboration onsolving issues, e.g., ﬁnding causes and solutions
Task
Progress (TP)communications on task progresses, releaseschedules and future plans
Fig. 2: Distribution of the percentage of each information type
in the sampled discussion threads
of level 2 is 0.76, which is lower than the one (0.83) for three
categories of level 1. Both Kappa values indicate a substantialagreement between the two authors. For cards with disagree-ments, two authors discussed to reach a common decision andfurther reﬁned the guidance handbook for classiﬁcation.
After two iterations of card sorting, we labeled 2,959 discus-
sion threads from three chatrooms on Gitter. The annotation isextremely expensive, with a cost of 480 person-hours. The dis-tribution of each information type is shown in Figure 2. Over80% of the discussion threads in all three chatrooms belongto the category Problem Report orInformation Retrieval, and
majority of them are related to programming (e.g., API, syntaxand parameter). It indicates that developers mainly use onlinechatrooms to seek solutions or answers for their problems.
IV . A
UTOMA TED CLASSIFICA TION OF INFORMA TION
CA TEGORIES
In this section, we ﬁrst describe the detailed process for
preprocessing the chat messages. Then, we build a two-stagemodel (Figure 3), namely F2C
HA T , for automated classiﬁca-
tion of information categories.A. Preprocessing of Chat Messages
Messages in developer chatrooms are noisy for text classi-
ﬁcation algorithms (as described in Section I). A throughoutdata preprocessing is critical for automated techniques toachieve satisfactory performances. In this work, we take thefollowing steps to preprocess the raw chat messages:
1)Fine-grained special tokens replacement. The develop-
ment chat messages contain lots of special tokens such as
source code, URL, and issue ID. To clean the sentences,we replace these tokens with speciﬁc tags (e.g., CODETAG,
URLTAG and ISSUETAG) using regular expressions. This
step is widely adopted in the related works [12], [13].However, we argue that the replacements in previousstudies are not precise enough. Speciﬁcally, special tokensshould be replaced based on the speciﬁc artifacts containedrather than their forms. For example, a URL can linkto an issue on Github, a page of the ofﬁcial docs, etc.These differences should not be ignored, i.e., we shouldnot replace all URLs with a single tag. Moreover, the sameartifact is referred to using different forms (e.g., an issuecan be referred to using its ID or a direct URL link).This diversity (e.g., replacing issues with different tags)can confuse the model. We perform a ﬁne-grained specialtokens replacement leveraging the knowledge from manualexamination of chat data.
2)Merge consecutive messages from the same devel-oper. Unlike the well-structured discussion threads in issue
tracking systems, developer chats are generally informalconversations with lots of quick and incomplete messages.While some developers prefer to comprehensively describehis problems and thoughts within a single long message(2/circlecopyrt,3/circlecopyrtin Figure 1), others may prefer a series of short mes-
sages instead ( 1/circlecopyrtin Figure 1). This inconsistency caused
by the personal habits of different developers will bring
858troubles when we calculate certain non-textual features
(e.g., number of messages within a thread) or model themessage sequence. We merge consecutive messages fromthe same developer to eliminate this inconsistency.
B. Pretraining of Textual Feature Encoder
We separate the training of encoders for two types of
features (Figure 3) due to the following concerns: 1) Textualfeatures and non-textual features have different data types, i.e.,high-dimensional sparse vectors vs. numerical values (TableIII), training two encoders (one for each type of features)simultaneously (let alone a uniﬁed encoder) is not the bestpractice. 2) Textual features are encoded by deep neural mod-els with large amounts of parameters, thus few-shot learningtechniques are needed to overcome the overﬁtting problem.It is not the case for non-textual features since they areencoded using a two-layer feed-forward module. 3) Recastinga classiﬁcation task into similarity measurement will causeinformation loss [37].
݀ܽ݁ݎ݄ܶ ௔ ݀ܽ݁ݎ݄ܶ ௕share
weightsDistance Measure
ݎ݁݀݋ܿ݊ܧ ௧௘௫௧ ݎ݁݀݋ܿ݊ܧ ௧௘௫௧Feed Forwardp 1-p SoftMax
(a) Siamese architecture for pretraining the 
textual encoderMessage
SequenceNon-textual 
Featuresݎ݁݀݋ܿ݊ܧ ௧௘௫௧
(frozen)ݎ݁݀݋ܿ݊ܧ ௡௢௡௧௘௫௧
(trainable)Textual
EmbeddingNon-textual
EmbeddingFeed Forward (trainable)SoftMax ݌଴ ݌௖ ݌ଵ瀖
݀ܽ݁ݎ݄ܶ
(b) Architecture for combining two types of features
Fig. 3: Two-stage model architecture of F2C HA T
The ﬁrst stage of our proposed two-stage classiﬁcation
approach focuses on pretraining the textual feature encoder.
We refer to the model at this stage as F2C HA T -t, since it only
leverages textual features. Given the fact that the annotationis extremely expensive (Section III-B), we follow the methodsproposed by Shi et al. [12] to alleviate the overﬁtting problem
caused by the insufﬁcient data. By incorporating SiameseNetwork [38], a metric-based few-shot learning technique [28],they recast the traditional text classiﬁcation task of classifyinga single thread to the correct class into the task of determiningwhether a pair of threads belong to the same class or not.The siamese architecture for pretraining the textual encoder isshown in Figure 3(a). A pair of threads (either from the sameclass or different classes) are sent into two identical encoders(same structure and parameters) separately to get the textualfeature embeddings e=Encoder
text(t). The embeddings
eaand ebare then used to measure the distance between
two threads in the latent space. Speciﬁcally, we follow themetric in [12] to use a two-layer fully connected feed-forwardnetwork for distance measurement with the concatenation oftwo embeddings e
ab=ea/circleplustextebas the input. Finally, the
distance embeddings are sent to a two-unit softmax layer to݉ଵ ݉ଶ ݉௡ …LSTM
Cell…
……… … …Max Pooling
۩۩۩ …
12
LSTM
CellLSTM
CellLSTM
CellLSTM
CellLSTM
Cell
BERT BERT BERT
Fig. 4: The detailed structure of textual feature encoder
get the normalized score [score same , score diff], indicating
whether the two threads belong to the same class or not.
The detailed architecture of Encoder text is presented in
Figure 4, which contains following two modules:
Message encoding module. We ﬁrst encode each message in
the thread ( 1/circlecopyrtin Figure 4). We utilize BERT [39], one of
the state-of-the-art pretrained models (PTMs), to extract the
textual representation for each message. Compared with thetraditional context-free word embeddings (e.g., GloV e [40]),BERT aims to learn contextual word embeddings (i.e., theembedding of a word changes dynamically based on thecontext where it appears) from large unlabeled corpora, thussubstantially boosting the performance of multiple natural lan-guage processing (NLP) tasks [41]. Besides, applying BERT indownstream tasks follows a simple ﬁne-tuning process, whichprevents us from diving into the design of sophisticated modelarchitectures and introduces minimal task-speciﬁc parameters,which is important considering only a small dataset is availablein our task. According to [39], we use the embedding of[CLS] (a special token in BERT which is added in front of
every input sample) as the representation of the whole messagefor our classiﬁcation task. BERT uses a subword tokenizer thatavoids the out-of-vocabulary (OOV) problem by decomposingOOV words into known subwords. However, it does notapply to the technical terms in developer communications.Hence, we add several domain-speciﬁc terms (e.g., maven,
npm, and stackblitz) to the original BERT vocabulary based on
frequency. The embeddings for these terms will be optimizedduring ﬁne-tuning.
Thread encoding module. We then model the contextual
information of the entire thread from a sequence of message
embeddings ( 2/circlecopyrtin Figure 4), as the semantic logic is a crucial
pattern for thread-level classiﬁcation. Bidirectional Long ShortTerm Memory Network (Bi-LSTM) [42] is utilized to capturethe bidirectional contextual information for this sequencelearning task, which stacks two standard LSTM layers witheach one responsible for learning one-direction representation.
We concatenate the outputs of Bi-LSTM h=[− →h/circleplustext← −h]in
each time step and further utilize a max-pooling layer to get
the ﬁnal embedding of the thread.
859TABLE III: List of non-textual features
Type Feature1Description Aspect2
Lengthtoken a length of thread / ﬁrst message / messages from the asker (number of tokens) T,F,A
token r length of ﬁrst message / messages from the asker divided by thread length (number of tokens) F,A
character a length of thread / ﬁrst message / messages from the asker (number of characters) T,F,A
character r length of ﬁrst message / messages from the asker divided by thread length (number of characters) F,A
Structuraltimegap a thread duration / gap between the ﬁrst and second message (time) T,F
timegapMean a mean of all gaps in the thread (time) T
timegapStd a standard deviation of all gaps in the thread (time) T
timegap r gap between the ﬁrst and second message divided by thread duration (time) F
messgap a thread duration / gap between the ﬁrst and second message (number of messages) T,F
messgapMean a mean of all gaps in the thread (number of messages) T
messgapStd a standard deviation of all gaps in the thread (number of messages) T
messgap r gap between the ﬁrst and second message divided by thread duration (number of messages) F
mess a number of messages within the thread / from the asker T,A
mess r number of messages from the asker divided by total number of messages in the thread A
Participant participant a number of participants involved in the thread T
Special-tokenquestionmark a number of question marks in the ﬁrst message F
greeting a number of greeting words (e.g., hello, hey and hi) in the ﬁrst message F
code a number of code snippts in thread / ﬁrst message / messages from the asker T,F,A
error a number of stack traces in thread / ﬁrst message / messages from the asker T,F,A
doc a number of documentations mentioned in thread / ﬁrst message / messages from the asker T,F,A
issue a number of issues mentioned in thread / ﬁrst message / messages from the asker T,F,A
1Footnote aandrdenote the features with absolute value ranged in (0,+∞) and the features with relative value ranged in (0,1], respectively.
2T,Fand Adenote the aspects of whole thread, ﬁrst message and the asker, respectively.
C. Incorporating Non-textual Features
The second stage of F2C HA T focuses on the training of non-
textual feature encoder and the combination of both textual
and non-textual features. We ﬁrst introduce the identiﬁednon-textual features and then describe the details of modelarchitecture for stage two.
Handcrafted non-textual features. The non-textual features
used in [13], [19], [20], [43] are generally based on the features
ﬁrst proposed by Murray and Carenini [44]. However, thesefeatures are targeted for classiﬁcation at the granularity ofsentence, thus they need to be adapted and further enrichedfor our thread-level algorithm. Inspired by the sentence-levelfeatures, we identify 21 features (Table III) which can becategorized into four groups:
1)Length Features refer to the features measured by absolute
or relative (w.r.t. the entire thread) length in either token-
level or character-level. Rationale:
Threads of Problem
Report are generally longer as the askers need to elaborate
the details of the encountered problems, while expressingthe needs of certain information in threads of Information
Retrieval is relatively straightforward.
2)Structural Features refer to the features that are related
to the structure of a thread, e.g., the time interval betweenﬁrst and last message of the thread (i.e., the duration ofthe discussion). We also measure through the number ofintermediate messages since the time interval can be easilyaffected by external factors, e.g., the activeness of thechatroom which is different with respect to the speciﬁctime period [8]. Rationale:
Threads of Problem Report
usually have more messages and longer lasting time, sincesolving a problem usually requires an in-depth discussionto ﬁgure out the causes and takes time to verify the validityof potential solutions.
3)Participant Feature is the number of participants involved
in the thread. Rationale:
Although most of the discussionsare between two participants, i.e., the asker and one respon-dent, more respondents will participate when the reportedbugs are tricky ( Problem Report ) or a team collaboration
is required (Project Management ).
4)Special-token Features refer to the features measured by
the number of special tokens (e.g., code snippets, stacktraces and URLs). Rationale:
Some special tokens are
strong indicators of certain information categories, e.g.,stack traces for Programming Problem and URLs (linking
to ofﬁcial docs) for Documentation Information.
Furthermore, these 21 features are measured from three
different aspects (column Aspect in Table III): 1) Features
related to the whole thread. These features characterize the
thread as a whole. 2) Features related to the ﬁrst message.
The ﬁrst message is important as it usually indicates thepurposes and requirements of the thread initiator (asker). Forexample, a large relative length of the ﬁrst message w.r.t. theentire thread suggests that the thread is most likely to belongtoProblem Report, since the asker needs to fully elaborate
the encountered problems. 3) Features related to the asker.
These features characterize the behaviour of the asker in thediscussion thread. By extracting features from this aspect,we want to emphasize the differences between the two roles(asker and respondent) of the participants. For example, if alarge portion of code snippets in the discussion thread areprovided by the asker, then it is a strong indicator of Problem
Report. However, this may not be the case if provided by therespondents, since respondents in the threads of Information
Retrieval sometimes need to share the implementation details
while the askers just simply express their needs.
Combining both textual and non-textual features. Figure
3(b) presents the model architecture for combining the two
types of features in stage two. Unlike the textual counterpart,non-textual features are numerical values (Table III) that canbe directly computed. There is no need for a complex en-
860coder with a large number of parameters or few-shot learning
techniques. We use a fully connected feed-forward networkwith one hidden layer to encode the non-textual featurese
nontext =Encoder nontext (t), while the embedding of the
textual features is extracted using the pretrained encoder fromstage one (Figure 3) e
text =Encoder text(t). We set an
extremely small learning rate (1e-5) for parameters in thepretrained textual feature encoder, since we want to avoidcollapsing the well-trained encoder while slightly ﬁne-tuneit to accommodate the newly added non-textual features.Considering that some patterns require an in-depth integrationof both textual and non-textual features, we perform an earlyfusion at the feature level instead of a late fusion at thedecision level. We concatenate the embeddings of textual andnon-textual features as the ﬁnal representation of the threade=[e
text/circleplustextenontext ]and then use it for decision making,
instead of making two decisions using textual and non-textualfeatures respectively and then fuse them to get the ﬁnaldecision. We use a feed-forward network for decision making.Finally, a softmax layer is utilized to get the normalized scoreof each information type for this multi-class classiﬁcation task.
V. E
XPERIMENT DESIGN &R ESULTS
A. Experiment Settings
We use the dataset built in Section III for evaluation. The
detailed statistics presented in Table IV include the numberof sampled threads (THD), the median of the thread lastingtime (DUR), the average number of participants per thread(PCP), the average number of messages per thread (MSG),and the average number of tokens per message (MSGLEN).The experimental environment is a server equipped with anNVIDIA V100 GPU, Intel Xeon Platinum 8163 CPU, 16GBRAM, running Ubuntu OS.
TABLE IV: The detailed statistics of the dataset
PROJ THD DUR(min) PCP MSG(∗) MSGLEN(∗)
Angular 989 34 2.8 19.7(11.4) 12.2(20.9)
Spring-boot 985 145 2.3 9.3(6.0) 19.5(30.2)Deeplearning4j 985 28 2.4 16.4(8.8) 12.0(22.4)
(∗)denotes the statistics after merging consecutive messages from the same
developer.
Testing Scenarios. The evaluations are performed under the
following two scenarios:
1)Five-fold-cross-validation for intra-project scenario. In this
scenario, annotated threads from chatrooms of the test-
ing projects are partially available in the training set,meaning the model can gain knowledge of the testingproject during the training. We conduct a stratiﬁed ﬁve-fold-cross-validation for each chatroom in this testing sce-nario. Speciﬁcally, threads are divided into ﬁve folds usingstratiﬁed random sampling, with each fold preserving theoriginal distribution of information types. Every time, weuse four folds to train the model and the remaining one fortesting. The process is repeated ﬁve times to alleviate therandomness, and we report the average evaluation results.2)Leave-one-project-out-cross-validation for cross-projectscenario. This scenario simulates the situation when users
want to apply the trained algorithm to a new chatroom,which requires the learned knowledge of the model to begeneralizable as the testing project is previously unseenduring the training. We use a leave-one-project-out strategy,i.e., two projects are used as the source projects to train
the model and the remaining one as the target project for
testing. We iterate the process three times and report theaverage evaluation results.
Implementation Details. We use the pretrained BERT-Small
model [45] from HuggingFace Transformer library [46] dueto the limited computation resources. It produces a 512-dimensional embedding to represent each input message. Theoutput dimension of Bi-LSTM is 512 (256 for each). Fur-ther, through a non-linear projection header, we get a 256-dimensional embedding for the textual features of the thread.Non-textual features are normalized before encoding. Thedimension of the non-textual feature embedding is the same asthe textual counterpart. The ﬁnal representation of the threadin stage two, i.e., the concatenation of embeddings of twotypes of features, is a 512-dimensional embedding. We usecross-entropy as the loss function in the two stages.
To avoid the over-ﬁtting problem, we apply dropout [47] to
the outputs of every fully connected layer with the drop rateset to 0.1. We use AdamW [48] as the optimizer for modeltraining in both two stages. In stage one, we set the learningrate (lr) to 1e-3 except for the pretrained BERT module, whoselr is set to 2e-5 as suggested in [39]. Specially, the pairsof threads used for training are sampled from the datasetin a 3:1 ratio of negative pairs (two threads with differentclasses) to the positive ones (two threads with the same class),which is considered as an optimal ratio for training siamesenetworks [49]. In stage two, we set lr to 1e-5 for ﬁne-tuning thepretrained textual feature encoder and 1e-3 for other modules.Besides, we adjust class weights in the loss function to tacklethe unbalanced dataset problem (Figure 2).
Evaluation Metrics. We use the following metrics to evaluate
the performance of F2C
HA T :1 )Precision. Precision for class
Ciis the ratio of the number of threads that are correctly
classiﬁed as Cito the total number of predictions made for Ci.
2)Recall. Recall for class Ciis the ratio of number of threads
that are correctly classiﬁed as Cito the total number of threads
that belong to Ciin the ground truth. 3) F1-score. F1-score for
class Ciis the harmonic mean of its precision and recall. The
above three metrics evaluate the performance for a speciﬁc
category. For the evaluation of the overall performance, wecalculate the average F1-score of all classes weighted by
Support, i.e., the number of threads of each class in the testset. These metrics are widely adopted in the previous studiesthat involve classiﬁcation of software artifacts [13], [20], [21].
B. Research Questions
Our evaluation explores the following research questions:
RQ1: Does our approach work well in the intra-project
scenario? We follow the ﬁve-fold-cross-validation described
861in Section V-A to evaluate the performance of our approach
in the intra-project scenario. We use FRMiner, proposed byShi et al. [12], as our baseline. To the best of our knowledge,
FRMiner is the only one targeted for thread-level classiﬁcationof developer communication artifacts. The experimental resultssuggest that FRMiner substantially outperforms two advancedsentence-level approaches [21], [50] and four general textclassiﬁcation approaches [51]–[54]. Furthermore, F2C
HA T -t
(stage one of our approach) uses the same siamese architectureas in their work to pretrain the textual feature encoder. Hence,we only consider FRMiner for performance comparisons withour approach in the experiments. However, FRMiner is de-signed to identify one certain type of discussion threads indeveloper chatrooms, i.e., threads with hidden feature requests.To enable a comparison, we adapt it to our task by modifyingthe number of output units in the last classiﬁcation layers andretraining on our dataset using the same settings in [12].
RQ2: Does our approach work well in the cross-project
scenario? To evaluate the performance of our approach in
the cross-project scenario, we follow the leave-one-project-out-cross-validation described in Section V-A. We also useFRMiner as our baseline for this RQ.
RQ3: How does our approach beneﬁt from the hand-
crafted non-textual features? We investigate whether our
identiﬁed non-textual features help. To do so, we compare theperformance of F2C
HA T -t and F2C HA T (leverages two types
of features) in both intra-project and cross-project scenarios.
TABLE V: The performance comparisons between our ap-
proach and FRMiner for each chatroom in intra-project setting
Metric ApproachChatroom
Avg.Angular Spring-boot Deeplearning4j
PrecisionFRMiner 0.384 0.425 0.404 0.404
F2C HA T -t 0.659 0.576 0.543 0.593
F2C HA T 0.686 0.647 0.588 0.640
RecallFRMiner 0.444 0.460 0.407 0.437
F2C HA T -t 0.663 0.610 0.555 0.609
F2C HA T 0.689 0.650 0.593 0.644
F1-scoreFRMiner 0.398 0.403 0.399 0.400
F2C HA T -t 0.656 0.581 0.519 0.585
F2C HA T 0.681 0.632 0.572 0.628
C. Experiment Results
RQ1: Performance in Intra-project Validation. The per-
formance comparisons for each chatroom under the intra-
project setting are shown in Table V. Here, the precision,recall, and F1-score are averages of all information categoriesweighted by support. The best results are highlighted in bold.
F2C
HA T -t improves the performance of FRMiner (w.r.t. F1-
score) by 64.8%, 44.2%, and 30.1% for each of the threechatrooms, respectively. Considering both approaches usingonly textual features and sharing the same siamese architec-ture, the performance improvement veriﬁes the effectiveness of1) Better preprocessing of chat messages (Section IV-A) and2) Introducing BERT for sentence modeling (Section IV-B).Although F2C
HA T -t has already achieved satisfactory results
by leveraging deep textual features, F2C HA T further boostsTABLE VI: The average performance of F2C HA T across three
chatrooms for each information type in intra-project setting
Information Category Precision Recall F1-score Support
Programming Problem (PP) 0.638 0.690 0.663 718
Library Problem (LP) 0.620 0.577 0.581 303
Documentation Problem (DP) 0.333 0.183 0.236 75Programming Information (PI) 0.749 0.834 0.787 1,145Library Information (LI) 0.453 0.541 0.474 181
Documentation Information (DI) 0.472 0.446 0.455 176General Information (GI) 0.250 0.107 0.150 161
Technical Discussion (TD) 0.815 0.222 0.318 117Task Progress (TP) 0.467 0.241 0.318 83
Weighted Avg. 0.640 0.644 0.628 —
the performances in all three chatrooms, with an average of
0.640, 0.644, and 0.628 in precision, recall, and F1-score.
The average performance achieved by F2C HA T across all
three chatrooms for each information category is shown inTable VI. Generally, F2C
HA T achieves better performance on
information categories with larger support. The best perfor-mance is on Programming Information with an average F1-
score of 0.787. For categories where F2C
HA T performs poorly,
it is mainly due to the limited data samples, thus failingto capture effective patterns. The performance comparisonsfor each information category are shown in Table VII. Here,the results are averaged across all three chatrooms. The bestresults are highlighted in bold. Through manual checking ofspeciﬁc testing samples, we ﬁnd that compared with FR-Miner, F2C
HA T -t can better distinguish between Programming
Problem and Programming Information, as well as Library
Problem and Library Information. It beneﬁts from ﬁne-grained
special tokens replacement (Section IV-A). Since source codeand stack trace embedded in the message text share thesame format (both of them are considered as code snippets),replacing them with the same tag will confuse the learning-based model. Besides, by incorporating non-textual features,
F2C
HA T further improves the performance on almost every
information category.
TABLE VII: The performance comparisons for each informa-
tion type in intra-project setting
Metric ApproachInformation Category
PP LP DP PI LI DI GI TD TP
Pre-
cisionFRMiner 0.339 0.429 0.051 0.574 0.236 0.240 0.215 0.111 0.167
F2C HA T -t 0.622 0.561 0.111 0.720 0.377 0.229 0.210 0.622 0.500
F2C HA T 0.638 0.620 0.333 0.749 0.453 0.472 0.250 0.815 0.467
Re-callFRMiner 0.235 0.376 0.067 0.718 0.359 0.304 0.126 0.139 0.056
F2C
HA T -t 0.592 0.453 0.100 0.870 0.489 0.325 0.137 0.278 0.130
F2C HA T 0.690 0.577 0.183 0.834 0.541 0.446 0.107 0.222 0.241
F1-scoreFRMiner 0.254 0.377 0.058 0.634 0.282 0.256 0.149 0.123 0.083
F2C
HA T -t 0.603 0.501 0.105 0.784 0.426 0.258 0.145 0.346 0.198
F2C HA T 0.663 0.581 0.236 0.787 0.474 0.455 0.150 0.318 0.318
RQ2: Performance in Cross-project Validation. The perfor-
mance comparisons for each chatroom under the cross-project
setting are listed in Table VIII. Here, the chatroom refers to theone selected as the target project (Section V-A). In general,
the ﬁndings are similar to those in the intra-project setting.
F2C
HA T -t surpasses the FRMiner, and F2C HA T further boosts
the performance. Compared with the results in the intra-project
862TABLE VIII: The performance comparisons for each chatroom
in cross-project setting
Metric ApproachChatroom
Avg.Angular Spring-boot Deeplearning4j
PrecisionFRMiner 0.435 0.485 0.393 0.438
F2C HA T -t 0.601 0.539 0.482 0.541
F2C HA T 0.646 0.604 0.530 0.593
RecallFRMiner 0.438 0.468 0.395 0.434
F2C HA T -t 0.573 0.530 0.515 0.539
F2C HA T 0.631 0.577 0.542 0.583
F1-scoreFRMiner 0.419 0.464 0.328 0.404
F2C HA T -t 0.572 0.522 0.465 0.520
F2C HA T 0.616 0.584 0.517 0.572
validation (Table V), the performance of F2C HA T declines
on all three chatrooms by 8.9% over the average F1-score.However, the performance changes of FRMiner vary acrossdifferent chatrooms. We observe an improvement on Angular
and Spring-Boot, while a decline for Deeplearning4j. The
improvement is mainly due to a larger training set. In the cross-project setting, all threads from two chatrooms (leave-one-project-out-cross-validation) are available for training, whilein the intra-project setting, only 80% of threads from asingle chatroom (ﬁve-fold-cross-validation) are available. Thehuge decline on Deeplearning4j is because it is a library for
supporting deep learning algorithms, but the other two projects(Angular &Spring-Boot ) are related to web development, thus
making the linguistic patterns learned during the training hardto generalize on the testing set.
TABLE IX: The performance comparisons for each informa-
tion type in cross-project setting
Metric ApproachInformation Category
PP LP DP PI LI DI GI TD TP
Pre-
cisionFRMiner 0.487 0.352 0.000 0.596 0.238 0.224 0.143 0.056 0.367
F2C HA T -t 0.656 0.480 0.333 0.685 0.321 0.194 0.195 0.172 0.194
F2C HA T 0.613 0.593 0.204 0.729 0.464 0.451 0.204 0.238 0.688
Re-callFRMiner 0.449 0.420 0.000 0.610 0.251 0.259 0.178 0.095 0.058
F2C
HA T -t 0.575 0.517 0.033 0.772 0.466 0.120 0.132 0.228 0.057
F2C HA T 0.695 0.459 0.070 0.751 0.330 0.409 0.154 0.302 0.315
F1-scoreFRMiner 0.393 0.383 0.000 0.582 0.221 0.239 0.116 0.070 0.099
F2C
HA T -t 0.610 0.495 0.061 0.723 0.331 0.148 0.054 0.153 0.088
F2C HA T 0.651 0.508 0.093 0.738 0.372 0.337 0.146 0.261 0.419
The average performance achieved by FRMiner and our
approach across the three chatrooms for each information
category is shown in Table IX. The results further verify theeffectiveness of F2C
HA T under the cross-project setting as
it outperforms the FRMiner on every information category.Compared with the results under the intra-project setting(Table VI), larger declines are observed on information typeswith smaller support. Besides, we ﬁnd the performance ofboth FRMiner and F2C
HA T -t on Programming Problem and
Library Problem are close to or even better than the results
in intra-project validation. This suggests that threads of thesetwo categories share the most similar linguistic patterns thatare irrelevant to project-speciﬁc concepts.
RQ3: Effectiveness of Handcrafted Non-textual Features.
To evaluate the effectiveness of our identiﬁed non-textual fea-tures, we focus on the comparison of F2C
HA T with F2C HA T -t
in this RQ. First, we discuss the effectiveness of handcraftednon-textual features under the intra-project setting. As shownin Table V, by incorporating non-textual features, F2C
HA T
outperforms F2C HA T -t in all three chatrooms, indicating our
identiﬁed features (Table III) can supplement the textualcounterpart with effective information (e.g., thread structure,discussion participants). Besides, we observe that non-textualfeatures are more powerful when the performances are poorerusing only textual features. F2C
HA T improves the F1-score
of F2C HA T -t by 10.2% for Deeplearning4j, while only 3.8%
for Angular. Regarding speciﬁc information categories, non-
textual features are more effective on certain types (Table VII).This indicates that non-textual features, such as the durationof the thread, number of participants, and number of specialtokens (e.g., URL, issue), are strong indicators of informationcategories, including Library Problem, Documentation Infor-
mation and Task Progress. While for General Information and
Technical Discussion, adding non-textual features does nothelp or even decreases the performance. This suggests thatthreads of these categories vary a lot in lengths, structures,and discussion contents.
Second, we discuss the effectiveness of handcrafted non-
textual features under the cross-project setting. The resultspresented in Table VIII suggest that our identiﬁed non-textualfeatures are generalizable across different projects, improv-ing the average F1-score by 10.0% in cross-project settingcompared with 7.4% under intra-project setting (Table V).When investigating the generalizability of non-textual featureson speciﬁc information categories, we observe that they aremore generalizable on Documentation Information and Task
Progress (Table IX). Although the performance on these
categories using only textual features is relatively poorer, aswe discussed in the last paragraph that this could be the causefor larger improvements, it reveals that discussions of thesecategories share the most similar structures and formats evenin different projects and communities.
VI. D
ISCUSSION
Effectiveness of Two-stage Model Design. We discuss the
motivations for separating the training process of textualand non-textual feature encoders in Section IV-B. Here, weconduct an ablation study to further verify the effectivenessof our two-stage model design. We replace the Encoder
text
in Figure 3(a) into the architecture in Figure 3(b) withoutthe last classiﬁcation layers (the feed-forward module and thesoftmax layer), and refer this model as F2C
HA T -s. F2C HA T -s
simultaneously train both encoders for textual and non-textualfeatures using the siamese architecture. We compare its per-formance against F2C
HA T -t and F2C HA T under intra-project
scenario. F2C HA T -t shares the same siamese architecture, but
it only leverages textual features. F2C HA T utilizes two types
of features, but it separates the training processes. The averageperformances of each model across three chatrooms are shownin Table X. The performances of F2C
HA T -s and F2C HA T -t are
close, while F2C HA T signiﬁcantly surpasses both two models.
863TABLE X: The performance comparisons in ablation study
Precision Recall F1-score
F2C HA T -s 0.592 0.604 0.582
F2C HA T -t 0.593 0.609 0.585
F2C HA T 0.640 0.644 0.628
Effectiveness of Incorporating Hand-crafted Non-textual
Features. Although the deep textual features are powerful
in presenting the semantic information, we argue that theyonly focus on the linguistic aspect, while neglecting otherinformation of the discussion thread (e.g., structure, partic-ipant). Moreover, the textual features do not consider thedifferences brought by the speaker’s identity (i.e., the asker andrespondents). Based on the observations from manual analysisof the data, we discuss that these kinds of information cancontribute to the prediction of information categories (SectionIV-C). Hence, we supply F2C
HA T with handcrafted non-
textual features to bridge the information gap. Figure 5 showsexamples to illustrate the effectiveness of non-textual features.
(a) Number of tokens (Length)
(c) Number of participants (Participant) (d) Number of code snip pets (Special-token)(b) Time duration (Structural)
Fig. 5: Distributions of four non-textual features (one fromeach type) of different information types in Angular chatroom
VII. T
HREA TS TO VALIDITY
There are two major threats to the validity of this work. 1)
The generalizability of the identiﬁed information categoriesand the proposed mining technique. We only sample 2,959threads from three active chatrooms on Gitter due to the hugecost of manual analysis of developer discussion threads. Toensure the diversity of the sampled data, we select chatroomswith several principals (Section III-A), e.g., most actively usedand belonging to different categories. However, the project-speciﬁc characteristics of the selected chatrooms might affectthe generalizability of our identiﬁed information categoriesand the performance of the proposed automated mining tech-nique. Speciﬁcally, in the preprocessing of chat messages(Section IV-A), we argue that the special tokens should be re-placed based on the speciﬁc artifacts contained instead of theirforms. We notice that developer communications, includinginstant chats, discussions in ITS, etc., typically involve varioussoftware artifacts (e.g., code snippets, issue reports and stacktraces). These artifacts are closely related to the discussion top-ics, and are embedded in the text using diverse forms. Hence,we argue that the core idea of ﬁne-grained special tokensreplacement is applicable to other mining sources of developercommunications. However, the detailed implementations andeffectiveness may vary across different sources. 2) The qualityof the dataset used for evaluation. We build a larger dataset(2,959 threads) than the prior work [12], which has only1,035 threads and focuses on one speciﬁc information type.We leverage the state-of-the-art method proposed by Ehsanet al. [8] to disentangle the stream of messages to threads.However, the disentangled threads may contain more or fewermessages, confusing the thread-level classiﬁcation techniques.Moreover, the ground-truth labels of the threads are manuallyannotated, which may subject to the personal experience ofthe annotators. Two annotators work collaboratively followinga card-sorting process to eliminate this inconsistency.
VIII. C
ONCLUSION AND FUTURE WORK
In this paper, we conduct an in-depth analysis to identify
the information categories available in discussion threads thatmay satisfy the diverse needs of various OSS stakeholders.First, we build a thread-level taxonomy with nine informationcategories through manual examination of 2,959 threads fromthree chatrooms on Gitter. Second, we propose an automatedclassiﬁcation technique, namely F2C
HA T , which combines
handcrafted non-textual features with deep textual featuresextracted by neural models. Evaluation results suggest that
F2C
HA T outperforms FRMiner by 57% with an average F1-
score of 0.628. Our approach also achieves considerable per-formance in cross-project validation, which indicates F2C
HA T
can extract patterns that are generalizable across variousprojects. The experiment results also verify the effectivenessof our indentiﬁed non-textual features under both intra-projectand cross-project validation. In future work, we plan to reﬁneour taxonomy by exploring discussions from more chatrooms.We also plan to develop a tool for Gitter chatrooms to supportthe daily development tasks of OSS stakeholders by providingwell structured and categorized historical chat data.
IX. A
CKNOWLEDGMENTS
This research/project is supported by the National Science
Foundation of China (No. U20A20173 and No. 6190234),Key Research and Development Program of Zhejiang Province(No.2021C01014), and National Research Foundation, Sin-gapore under its Industry Alignment Fund – Pre-positioning(IAF-PP) Funding Initiative. Any opinions, ﬁndings and con-clusions or recommendations expressed in this material arethose of the author(s) and do not reﬂect the views of NationalResearch Foundation, Singapore.
864REFERENCES
[1] B. Lin, A. Zagalsky, M.-A. Storey, and A. Serebrenik, “Why developers
are slacking off: Understanding how software teams use slack,” in
Proceedings of the 19th ACM Conference on Computer SupportedCooperative Work and Social Computing Companion, 2016, pp. 333–336.
[2] M.-A. Storey, L. Singer, B. Cleary, F. Figueira Filho, and A. Zagalsky,
“The (r) evolution of social media in software engineering,” in Future
of Software Engineering Proceedings, 2014, pp. 100–116.
[3] V . K ¨afer, D. Graziotin, I. Bogicevic, S. Wagner, and J. Ramadani,
“Communication in open-source projects-end of the e-mail era?” in Pro-
ceedings of the 40th International Conference on Software Engineering:Companion Proceeedings, 2018, pp. 242–243.
[4] “Gitter.” [Online]. Available: https://gitter.im/[5] “Slack.” [Online]. Available: https://slack.com/[6] “Discord.” [Online]. Available: https://discord.com/[7] H. Sahar, A. Hindle, and C.-P . Bezemer, “How are issue reports
discussed in gitter chat rooms?” Journal of Systems and Software, vol.
172, p. 110852, 2019.
[8] O. Ehsan, S. Hassan, M. E. Mezouar, and Y . Zou, “An empirical study
of developer discussions in the gitter platform,” ACM Transactions on
Software Engineering and Methodology (TOSEM) , vol. 30, no. 1, pp.
1–39, 2020.
[9] P . Chatterjee, K. Damevski, L. Pollock, V . Augustine, and N. A. Kraft,
“Exploratory study of slack q&a chats as a mining source for softwareengineering tools,” in 2019 IEEE/ACM 16th International Conference
on Mining Software Repositories (MSR). IEEE, 2019, pp. 490–501.
[10] R. Alkadhi, T. Lata, E. Guzmany, and B. Bruegge, “Rationale in
development chat messages: an exploratory study,” in 2017 IEEE/ACM
14th International Conference on Mining Software Repositories (MSR).IEEE, 2017, pp. 436–446.
[11] R. Alkadhi, M. Nonnenmacher, E. Guzman, and B. Bruegge, “How do
developers discuss rationale?” in 2018 IEEE 25th International Con-
ference on Software Analysis, Evolution and Reengineering (SANER).IEEE, 2018, pp. 357–369.
[12] L. Shi, M. Xing, M. Li, Y . Wang, S. Li, and Q. Wang, “Detection of
hidden feature requests from massive chat messages via deep siamesenetwork,” in 2020 IEEE/ACM 42nd International Conference on Soft-
ware Engineering (ICSE). IEEE, 2020, pp. 641–653.
[13] D. Arya, W. Wang, J. L. Guo, and J. Cheng, “Analysis and detection
of information types of open source software issue discussions,” in2019 IEEE/ACM 41st International Conference on Software Engineering(ICSE). IEEE, 2019, pp. 454–464.
[14] A. Di Sorbo, S. Panichella, C. A. Visaggio, M. Di Penta, G. Canfora,
and H. C. Gall, “Development emails content analyzer: Intention miningin developer discussions (t),” in 2015 30th IEEE/ACM International
Conference on Automated Software Engineering (ASE). IEEE, 2015,pp. 12–23.
[15] S. Panichella, A. Di Sorbo, E. Guzman, C. A. Visaggio, G. Canfora,
and H. C. Gall, “How can i improve my app? classifying user reviewsfor software maintenance and evolution,” in 2015 IEEE international
conference on software maintenance and evolution (ICSME). IEEE,2015, pp. 281–290.
[16] A. Bacchelli, T. Dal Sasso, M. D’Ambros, and M. Lanza, “Content clas-
siﬁcation of development emails,” in 2012 34th International Conference
on Software Engineering (ICSE). IEEE, 2012, pp. 375–385.
[17] N. Chen, J. Lin, S. C. Hoi, X. Xiao, and B. Zhang, “Ar-miner: mining
informative reviews for developers from mobile app marketplace,” inProceedings of the 36th international conference on software engineer-ing, 2014, pp. 767–778.
[18] S. Rastkar, G. C. Murphy, and G. Murray, “Summarizing software
artifacts: a case study of bug reports,” in 2010 ACM/IEEE 32nd
International conference on Software Engineering, vol. 1. IEEE, 2010,pp. 505–514.
[19] P . Rodeghero, S. Jiang, A. Armaly, and C. McMillan, “Detecting user
story information in developer-client conversations to generate extractivesummaries,” in 2017 IEEE/ACM 39th International Conference on
Software Engineering (ICSE). IEEE, 2017, pp. 49–59.
[20] A. Wood, P . Rodeghero, A. Armaly, and C. McMillan, “Detecting speech
act types in developer question/answer conversations during bug repair,”inProceedings of the 2018 26th ACM Joint Meeting on European
Software Engineering Conference and Symposium on the F oundations
of Software Engineering, 2018, pp. 491–502.[21] Q. Huang, X. Xia, D. Lo, and G. C. Murphy, “Automating intention
mining,” IEEE Transactions on Software Engineering, 2018.
[22] “Our replication package.” [Online]. Available: https://github.com/
panshengyi/F2Chat
[23] E. Shihab, Z. M. Jiang, and A. E. Hassan, “Studying the use of developer
irc meetings in open source projects,” in 2009 IEEE International
Conference on Software Maintenance. IEEE, 2009, pp. 147–156.
[24] ——, “On the use of internet relay chat (irc) meetings by developers
of the gnome gtk+ project,” in 2009 6th IEEE International Working
Conference on Mining Software Repositories. IEEE, 2009, pp. 107–
110.
[25] P . Chatterjee, K. Damevski, N. A. Kraft, and L. Pollock, “Software-
related slack chats with disentangled conversations,” in Proceedings
of the 17th International Conference on Mining Software Repositories,2020, pp. 588–592.
[26] R. Baeza-Yates, B. Ribeiro-Neto et al., Modern information retrieval .
ACM press New Y ork, 1999, vol. 463.
[27] Y . Kim, “Convolutional neural networks for sentence classiﬁcation,”
inProceedings of the 2014 Conference on Empirical Methods in
Natural Language Processing (EMNLP). Doha, Qatar: Associationfor Computational Linguistics, Oct. 2014, pp. 1746–1751. [Online].Available: https://www.aclweb.org/anthology/D14-1181
[28] Y . Wang, Q. Yao, J. T. Kwok, and L. M. Ni, “Generalizing from a few
examples: A survey on few-shot learning,” ACM Computing Surveys
(CSUR), vol. 53, no. 3, pp. 1–34, 2020.
[29] “Angular chatroom on gitter.” [Online]. Available: https://gitter.im/
angular/angular
[30] “Spring-boot chatroom on gitter.” [Online]. Available: https://gitter.im/
spring-projects/spring-boot
[31] “Deeplearning4j chatroom on gitter.” [Online]. Available: https:
//gitter.im/eclipse/deeplearning4j
[32] “Gitter explore page.” [Online]. Available: https://gitter.im/home/explore[33] “Gitter developer page.” [Online]. Available: https://developer.gitter.im/[34] M. Elsner and E. Charniak, “Y ou talking to me? a corpus and algorithm
for conversation disentanglement,” in Proceedings of ACL-08: HLT,
2008, pp. 834–842.
[35] D. Spencer, Card sorting: Designing usable categories. Rosenfeld
Media, 2009.
[36] J. Cohen, “A coefﬁcient of agreement for nominal scales,” Educational
and psychological measurement, vol. 20, no. 1, pp. 37–46, 1960.
[37] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework
for contrastive learning of visual representations,” in International
conference on machine learning. PMLR, 2020, pp. 1597–1607.
[38] J. Bromley, I. Guyon, Y . LeCun, E. S ¨ackinger, and R. Shah, “Signature
veriﬁcation using a” siamese” time delay neural network,” Advances in
neural information processing systems, pp. 737–737, 1994.
[39] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
of deep bidirectional transformers for language understanding,” arXiv
preprint arXiv:1810.04805, 2018.
[40] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vectors
for word representation,” in Proceedings of the 2014 conference on
empirical methods in natural language processing (EMNLP), 2014, pp.1532–1543.
[41] X. Qiu, T. Sun, Y . Xu, Y . Shao, N. Dai, and X. Huang, “Pre-trained
models for natural language processing: A survey,” Science China
Technological Sciences, pp. 1–26, 2020.
[42] A. Graves, A.-r. Mohamed, and G. Hinton, “Speech recognition with
deep recurrent neural networks,” in 2013 IEEE international conference
on acoustics, speech and signal processing. Ieee, 2013, pp. 6645–6649.
[43] S. Rastkar, G. C. Murphy, and G. Murray, “Automatic summarization
of bug reports,” IEEE Transactions on Software Engineering, vol. 40,
no. 4, pp. 366–380, 2014.
[44] G. Murray and G. Carenini, “Summarizing spoken and written conver-
sations,” in Proceedings of the 2008 Conference on Empirical Methods
in Natural Language Processing, 2008, pp. 773–782.
[45] I. Turc, M.-W. Chang, K. Lee, and K. Toutanova, “Well-read students
learn better: On the importance of pre-training compact models,” arXiv
preprint arXiv:1908.08962v2, 2019.
[46] “Bert-small on huggingface.” [Online]. Available: https://huggingface.
co/google/bert
uncased L-4 H-256 A-4
[47] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut-
dinov, “Dropout: a simple way to prevent neural networks from over-ﬁtting,” The journal of machine learning research , vol. 15, no. 1, pp.
1929–1958, 2014.
865[48] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”
arXiv preprint arXiv:1711.05101, 2017.
[49] P . Neculoiu, M. V ersteegh, and M. Rotaru, “Learning text similarity
with siamese recurrent networks,” in Proceedings of the 1st Workshop
on Representation Learning for NLP, 2016, pp. 148–157.
[50] L. Shi, C. Chen, Q. Wang, S. Li, and B. Boehm, “Understanding
feature requests by leveraging fuzzy method and linguistic analysis,” in
2017 32nd IEEE/ACM International Conference on Automated SoftwareEngineering (ASE). IEEE, 2017, pp. 440–450.
[51] A. McCallum, K. Nigam et al., “A comparison of event models for
naive bayes text classiﬁcation,” in AAAI-98 workshop on learning fortext categorization, vol. 752, no. 1. Citeseer, 1998, pp. 41–48.
[52] A. Liaw, M. Wiener et al., “Classiﬁcation and regression by randomfor-
est,” Rn e w s , vol. 2, no. 3, pp. 18–22, 2002.
[53] G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye, and T.-
Y . Liu, “Lightgbm: A highly efﬁcient gradient boosting decision tree,”Advances in neural information processing systems, vol. 30, pp. 3146–3154, 2017.
[54] A. Joulin, E. Grave, P . Bojanowski, M. Douze, H. J ´egou, and T. Mikolov,
“Fasttext. zip: Compressing text classiﬁcation models,” arXiv preprint
arXiv:1612.03651, 2016.
866