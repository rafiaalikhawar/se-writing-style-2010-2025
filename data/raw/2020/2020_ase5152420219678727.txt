Evolutionary-Guided Synthesis
of Veriﬁed Pareto-Optimal MDP Policies
Simos Gerasimou
Department of Computer Science
University of York, UK
simos.gerasimou@york.ac.ukJavier Cámara
Department of Computer Science
University of York, UK
javier.camaramoreno@york.ac.ukRadu Calinescu
Department of Computer Science
University of York, UK
radu.calinescu@york.ac.uk
Naif Alasmari
Department of Computer Science
University of York, UK
nnma500@york.ac.ukFaisal Alhwikem
Department of Computer Science
University of York, UK
faisal.alhwikem@york.ac.ukXinwei Fang
Department of Computer Science
University of York, UK
xinwei.fang@york.ac.uk
Abstract —We present a new approach for synthesising Pareto-
optimal Markov decision process (MDP) policies that satisfy
complex combinations of quality-of-service (QoS) software re-quirements. These policies correspond to optimal designs orconﬁgurations of software systems, and are obtained by trans-lating MDP models of these systems into parametric Markovchains, and using multi-objective genetic algorithms to synthesisePareto-optimal parameter values that deﬁne the required MDPpolicies. We use case studies from the service-based systems androbotic control software domains to show that our MDP policysynthesis approach can handle a wide range of QoS requirementcombinations unsupported by current probabilistic model check-ers. Moreover, for requirement combinations supported by thesemodel checkers, our approach generates better Pareto-optimalpolicy sets according to established quality metrics.
I. I NTRODUCTION
Markov decision processes (MDPs) provide a powerful
mathematical framework for modelling and analysing sequen-
tial decision-making problems under uncertainty [1], [2]. Theirability to capture the complexity and uncertainty of modernsoftware-intensive systems has led to numerous MDP appli-cations for stochastic control and dynamic optimisation, indomains ranging from software product lines [3] and service-based systems [4] to self-adaptive systems [5] and robotics [6].
Software engineers can employ MDPs both during system
design to analyse different system architectures [4], [7] and atruntime to support system reconﬁguration [5], [8]. Consider aservice-based system whose operations can be performed byalternative combinations of functionally equivalent third-partyservices that operate with different reliability, response timeand cost. Modelling this service orchestration problem as anMDP allows engineers to analyse how the use of different ser-vice combinations affects the quality attributes of the system.The solution to the MDP is a policy that determines whichconcrete services should be selected so that a given objective,such as the overall system reliability or operational cost, isoptimised. Given the MDP representation of such a systemand a temporal logic speciﬁcation [9] that formally deﬁnesthe objective to be optimised, probabilistic model checkerslike PRISM [10] and Storm [11] can automatically synthesisean optimal policy for the speciﬁcation.
Software systems often require the simultaneous optimisa-
tion of multiple objectives whilst also satisfying a set of strictconstraints. In a service-based system, software engineers maybe interested in policies corresponding to services orchestra-tion that minimise the system operation cost and responsetime, subject to keeping the system reliability above a criticalthreshold. This is an instance of a multi-objective optimisationproblem [12]. In software-intensive systems, these objectivesare typically conﬂicting, e.g., a more reliable or responsiveservice tends to be more expensive. As such, the MDP policysynthesis needs to generate Pareto-optimal policy sets, i.e., sets
of policies that (i) satisfy all constraints, and (ii) for whichno policy exists that also satisﬁes the system constraints andachieves better values for all the optimisation objectives [13].
Executing multi-objective model checking on MDPs for
the synthesis of Pareto-optimal policies is an important andnon-trivial problem [14]. Despite recent advances [13], [15],[16], [17], [18], existing approaches either use simple iter-ative methods, or rely on reductions and simpliﬁcations tosolve the problem using linear programming. This limits theirapplicability to (i) single-objective problems with multiplestrict constraints (for which a single best policy exists); or(ii) unconstrained problems with up to three optimisationobjectives. Accordingly, these approaches support only asmall fragment of the multi-objective MDP model checkingspectrum, and cannot synthesise Pareto-optimal policies formany practical problems encountered, for instance, in softwareproduct lines [3], [19].
Our paper introduces EvoPoli, an approach that supports
the synthesis of Pareto-optimal policies for MDPs with arbi-trary combinations of constraints and optimisation objectives.EvoPoli uses evolutionary algorithms [12] to synthesise poli-cies that cover sufﬁciently the policy space enabling decision-makers to obtain a holistic view of the tradeoffs between thepolicies in the objective space and make an informed decision.The crux of our approach is to cast the synthesis of Pareto-
8422021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000792021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678727
978-1-6654-0337-5/21/$31.00  ©2021  IEEE
0'3PRGHO
0'34R6
FRQVWUDLQWV	RSWLPLVDWLRQREMHFWLYHV0XOWLREMHFWLYH
VHDUFKEDVHG
SROLF\
V\QWKHVLVS'70&
S'70&FRPSOLDQW
4R6FRQVWUDLQWV	RSWLPLVDWLRQREMHFWLYHV$FWLRQVSDFH0'3
WUDQVIRUPDWLRQ3DUHWRIURQWDSSUR[LPDWLRQ3)REMHFWLYHV
3DUHWRVHWDSSUR[LPDWLRQ36SROLFLHV
Fig. 1: EvoPoli high-level workﬂow.
optimal policies for MDPs as a multi-objective search-basedproblem [20] and leverage the power of evolutionary algo-rithms [12] to compute the required Pareto-optimal policies.
As shown in Figure 1, EvoPoli takes as inputs an MDP
model and a set of quality-of-service (QoS) constraints andoptimisation objectives formally deﬁned in probabilistic com-putational tree logic (PCTL) [9]. Through an MDP analy-sis and transformation step, EvoPoli produces a parametricdiscrete-time Markov chain (pDTMC) in which the model pa-rameters encode the actions of the original MDP, and extractsthe action space, i.e., the set of possible actions modelledin the MDP. During this step, EvoPoli also converts theconstraints and optimisation objectives into equivalent PCTLspeciﬁcations that comply with the pDTMC representation.Next, EvoPoli executes a multi-objective search-based policysynthesis procedure that successively evolves a population ofcandidate policies until a termination criterion is met (eitherthe search budget is exhausted or no improvement occursover a speciﬁed number of evolution rounds). The result isan approximate Pareto optimal set of policies, along with theassociated approximate Pareto front of QoS attribute values.
The main contributions of our paper are:
•The EvoPoli approach for the synthesis of Pareto-optimalpolicies that extends the multi-objective model checkingon MDPs to a much broader spectrum of QoS softwarerequirement combinations than currently possible;
•An extensive EvoPoli evaluation on several variants of twoMDPs modelling real-world problems, for a wide varietyof constraints and optimisation objectives. Our experimentsshow that EvoPoli can handle multiple QoS requirementcombinations unsupported by current probabilistic modelcheckers. Moreover, for requirement combinations sup-ported by these model checkers, EvoPoli produces muchbetter Pareto-optimal policy sets according to establishedquality indicators [21] and statistical analyses [22].
•A prototype open-source EvoPoli tool and case studyrepository, both available from our project web page athttps://github.com/gerasimou/MDPSynthesis.
II. P
RELIMINARIES
A. Discrete-time Markov Chains
Deﬁnition 1 (Discrete-time Markov chain). A discrete-time
Markov chain (DTMC) over a set of atomic propositions AP
is a tuple D=(S,sI,P,L,R), whereS/negationslash=∅is a ﬁnite set
of states; sI∈Sis the initial state; P:S×S→[0,1]is a transition probability matrix such that, for any states
s,s/prime∈S,P(s,s/prime)gives the probability of transitioning from
stos/prime, and/summationtext
s/prime∈SP(s,s/prime)=1 for anys∈S;L:S→2AP
is a labelling function that maps every state s∈Sto the
atomic propositions from AP that hold in that state; and Ris
a (possibly empty) set of functions ρ:S→R≥0that associate
non-negative values with the DTMC states.
A parametric DTMC (pDTMC) is a discrete-time Markov
chain whose transition probabilities P(s,s/prime)are speciﬁed as
rational functions over a set of parameters [23], [24], [25].
B. Markov Decision Processes
Markov decision processes generalise DTMCs with the
ability to model nondeterminism.
Deﬁnition 2 (Markov decision process). A Markov decision
process (MDP) over a set of atomic propositions AP is a
tupleM=(S,sI,A,Δ,L,R ), whereS,sI,LandRare
deﬁned as for a DTMC; A/negationslash=∅is a ﬁnite set of actions; and
Δ:S×A→Dist(S)is a partial probabilistic transition
function that maps state-action pairs to discrete probability
distributions over S.
In each state s∈S, the set of actions a∈Afor which
Δ(s,a) is deﬁned contains the actions enabled in states, and
is denoted by A(s). The choice of which action from A(s)
to take in every state sis assumed to be nondeterministic.
We reason about the behaviour of MDPs using policies.A policy resolves the nondeterministic choices of an MDP,selecting the action taken in every state. MDP policies can beclassiﬁed into inﬁnite-memory, ﬁnite-memory and memorylesspolicies (depending on whether the action selected in a statedepends on all, a ﬁnite number, or none of the previouslyvisited states and on the actions selected in those states).Our work, and probabilistic model checkers such as PRISMand Storm, consider memoryless policies. Memoryless policiescan be further classiﬁed into deterministic (when the sameaction is selected each time when a state is reached) andrandomised (when the action selected in a state is given by adiscrete probability distribution over the feasible actions). Inthis work, we use deterministic memoryless policies (calledsimply ‘policies’ in the rest of the paper).
Deﬁnition 3 (MDP policy). A (deterministic memoryless)
policy of an MDP is a function σ:S→Athat maps each
states∈Sto an action from A(s).
C. Probabilistic Computation Tree Logic
Probabilistic computation tree logic (PCTL) [9], [26] is used
to quantify properties related to probabilities and rewards in
system speciﬁcations modelled by DTMCs and MDPs.
Deﬁnition 4 (PCTL formulae). State PCTL formulae Φand
path PCTL formulae Ψover an atomic proposition set AP
are deﬁned by the grammar:
Φ::=true|α|Φ∧Φ|¬Φ|P
∼p[Ψ]|R∼r[C≤k]|R∼r[FΦ]
Ψ::=XΦ|ΦUΦ|ΦU≤kΦ
(1)
843TABLE I: Quality requirements for TAS
ID Type Description PCTL
R1 Constraint Workﬂow executions must succeed
with probability at least 95%P≥0.95[Fw O K ]
R2 Objective Minimise the average response time Rtime
min=? [C]
R3 Objective Minimise the average operation cost Rcostmin=?[C]
whereα∈AP is an atomic proposition, ∼∈ {≥,>,<,≤} is
a relational operator , p∈[0,1]is a probability bound, r∈R+
0
is a reward bound, and k∈N>0is a timestep bound.
The PCTL semantics is deﬁned using a satisfaction
relation|=over the states S. Given a state sof an MDP
M,s|=Φ means “Φ holds in state s”, and we have:
alwayss|=true;s|=αiffα∈L(s);s|=¬Φiff
¬(s|=Φ ) ; ands|=Φ 1∧Φ2iffs|=Φ 1ands|=Φ 2.
The time-bounded until formula Φ1U≤kΦ2holds for a path
iffΦ1holds in the ﬁrst i<k path states and Φ2holds in
the(i+1 ) -th path state; and the unbounded until formula
Φ1UΦ 2removes the bound kfrom the time-bounded until
formula. The next formula XΦholds ifΦis satisﬁed in the
next state. The semantics of the probability Pand reward R
operators are deﬁned over all policies σofMas follows:
P∼p[Ψ]speciﬁes that the probability that paths starting at
a chosen state ssatisfy a path property Ψis∼pfor all
policies; R∼r[C≤k]holds if the expected cumulated reward
up to time-step kis∼rfor all policies; and R∼r[FΦ]
holds if the expected reward cumulated before reaching a
state satisfying Φis∼rfor all policies. Replacing ∼p(or
∼r) from (1) with min=?ormax=?speciﬁes that the
calculation of the minimum/maximum probability (or reward)over all MDP policies is required. For a full description of thePCTL semantics, see [9], [26].
III. R
UNNING EXAMPLE
We illustrate EvoPoli on a service-based Tele Assistance
System (TAS) introduced in [27]. The TAS continually tracksa patient’s vital parameters, adapts the drug type or dosewhenever needed, and takes action in case of emergency.
TAS combines three service types in a workﬂow (Figure 2).
When the system receives a request that includes the patient’svital parameters, a Medical Service analyses the data and
replies with instructions to (i) change the patient’s drug type,(ii) change the drug dose, or (iii) trigger an alarm for ﬁrstresponders.When changing the drug type or dose, TAS notiﬁesa local pharmacy using a Drug Service, and the alarm to notify
the ﬁrst responders is executed via an Alarm Service.
The functionality of each service type can be fulﬁlled by
multiple service providers that offer functionally equivalentservice implementations with different levels of reliability,performance, and cost. Reliability is given by the percentageof service failures over a predeﬁned time period, performanceis given by the service’s mean response time, and cost is theprice per service invocation.
At run time, the quality attributes of the services can vary,
so TAS periodically reconﬁgures its workﬂow service bindings:Tele
Assistance
Service:Drug
Service:Medical
Analysis
Service:Alarm
Service
pick=pickTask()
sendAlarm()
sendAlarm()altopt
[analysisResult!=patientOK]
[analysisResult==sendAlarm]alt
[pick==vitalParamsMsg]loop
[pick==buttonMsg]data=getVitalParams()
analysisResult=analyzeData(data)
changeDrug(patientId)
changeDose(patientId)[pick!=stopMsg]
[analysisResult==changeDrug]
[analysisResult==changeDose]
Fig. 2: TAS service workﬂow (adapted from [27]).
to select the combination of service implementations that
optimises its operation, based on the requirements in Table I.
The reconﬁguration decision can be cast as an MDP policy
synthesis problem and modeled using high-level speciﬁcationlanguages employed by commonly used probabilistic modelcheckers. Figure 3 illustrates the encoding of a TAS probleminstance in Prism, which contains a reconﬁguration module(reconf, lines 2-16) in charge of selecting the alternativeservice implementations (one per service type) at the startof the execution. Each of the implementation selections isunderspeciﬁed in the model and encoded as a nondetermin-istic choice that will be resolved by the policy synthesisprocess (lines 7-15). Once reconﬁguration is complete, theTASWorkflow module executes the workﬂow, communicat-
ing with the different service implementations selected viasynchronous actions with shared labels (between “[]” in eachcommand). If a service invocation fails, the workﬂow canhandle timeouts by retrying calls (line 34). The number ofretries is conﬁgurable via parameter MAX_TIMEOUTS (line
22). Due to space constraints, we only represent a subset ofcommands that bind workﬂow calls with alternative serviceimplementations. Below the workﬂow module, the ﬁgureshows an excerpt of one of the modules that encode serviceimplementations (medical analysis service MS1), which ac-
crues cost and time rewards (lines 55-59, 60-65, respectively),whenever a synchronization with TASWorkflow actions oc-
curs, e.g., MS1_call (lines 48, 30).
The problem instance presented here is deliberately small
for illustration purposes. However, the solution space can growexponentially as alternative service implementations are added,resulting in situations in which ﬁnding optimal policies forservice selection cannot be achieved using exhaustive search.
8441mdp
2module reconf
3 MSsel: [0..MAX MS] init0;
4 DSsel: [0..MAX DS] init0;
5 ASsel: [0..MAX AS] init0;
6
7 [selMS1] (MS sel=0)−> (MS sel’=1);
8 ...
9 [selMS5] (MS sel=0)−> (MS sel’=5);
10 ...
11 [selDS1] (MS sel>0) & (DS sel=0)−> (DS sel’=1);
12 ...
13 [selAS1] (DS sel>0) & (AS sel=0)−> (AS sel’=1);
14 ...
15 [selAS3] (DS sel>0) & (AS sel=0)−> (AS sel’=3);
16 endmodule
1718
module TASWorkﬂow
19 task:[notSelected..buttonMsg] initnotSelected; ...
20 wOK : bool init false ;
21 wDone : bool init false ;
22 tos:[0..MAX TIMEOUTS] initMAX TIMEOUTS;
2324
[] (reconf done) & (task=notSelected) −> 0.5: (task’=getVitalParams)
25 + 0.5: (task’=buttonMsg);
2627
[] (task=buttonMsg) & (!MSInvoked) −> (MSInvoked’= true)
28 & (res’=sendAlarm);
2930
[MS1 call] (task=getVitalParams) & (!MSInvoked) −> (MSInvoked’= true);
31 [MS1 patientOK] (MSInvoked) −> (res’=patientOK) & (wOK’= true)
32 & (wDone’= true);
33 ...
34 [MS1 to] (tos >0) & (MSInvoked) −> (MSInvoked’= false) & (tos’=tos −1);
35 [MS1 to] (tos=0) & (MSInvoked) −> (wDone’= true);
36 ...
37 [MS5 call] (task=getVitalParams) & (!MSInvoked) −> (MSInvoked’= true);
3839
[DS1 call] (MSInvoked & !DInvoked) & (res=changeDrug) −> (DInvoked’= true);
4041
[AS1 call] (MSInvoked & !AInvoked) & (res=sendAlarm) −> (AInvoked’= true);
42 endmodule
4344
module MS1
45 MS1 OK: bool init false ;
46 MS1 ready : bool init true ;
4748
[MS1 call] (MS sel=1) & (MS1 ready)−>
49 MS1 failure rate: (MS1 OK’=false ) & (MS1 ready’= false)
50 +1−MS1 failure rate: (MS1 OK’=true ) & (MS1 ready’= false);
51 ...
52 [MS1 to] (MS sel=1) & (!MS1 ready) & (!MS1 OK)−> (MS1 ready’= true);
53 endmodule
5455
rewards ”cost”
56 [MS1 call] true : MS1 cost;
57 ...
58 [AS3 call] true : AS3 cost;
59 endrewards
60 rewards ”time”
61 [MS1 patientOK] true : MS1 response time;
62 ...
63 [AS3 sendAlarmOK] true : AS3 response time;
64 [AS3 to]true : AS3 response time∗TIMEOUT MULT FACTOR;
65 endrewardsOrdered selection of
implementation for: medical
analysis, drug, and alarmservices, encoded asnondeterministic choices.reconf module selects alternative
implementations for diﬀerent service types.
TASWorkﬂow module models the workﬂow
depicted in Figure 2.
Once reconﬁguration
is done, picks task with
equal probability.
If buttonMsg picked, skips analysis, goes directly to alarm.
Calls/handles MS response to check patient’s vital parameters.
Call to service fails with probability encodingfailure rate for this service implementation.MS determined to change drug/dose.
MS determined to raise alarm.
MSX, DSX, ASX modules model alternative
service implementations.
cost reward accrues economic cost per service call.
time reward accrues time spent on service operations.Commands enabled only if service implementation selected.MS timeout handling.
Fig. 3: MDP model of the Tele Assistance System [27]
encoded in the high-level modelling language of PRISM [10].
IV . E VOPOLI
A. Problem Deﬁnition
EvoPoli is applicable to systems whose behaviour can be
modelled by MDPs, with the action set A(s)from Deﬁnition 2
encoding the choices (e.g., of functionally equivalent services
that can be invoked to perform an operation) available whenthe system state is modelled by state s∈Sof the MDP.
Deﬁnition 5 (Policy Decision Space). The policy decision
space of an MDP M=(S,s
I,A,Δ,L,R )is the set of allvalid MDP policies, DS={σ:S→A|σ(s)∈A(s)}. The
number of such policies is #DS=/producttext
s∈S#A(s).
In line with the standard practice in the engineering of
software-intensive systems [20], EvoPoli considers systemswithn
1≥0constraints andn2≥1optimisation objectives.
A constraint speciﬁes a bound for the acceptable values ofa quality attribute, while an optimisation objective speciﬁeswhether a quality attribute should be maximised or minimisedsubject to satisfying all n
1constraints.
Given an MDP M=(S,sI,A,Δ,L,R ),n1≥0PCTL-
encoded constraints of the form
Ci::=P∼pi[·]|R∼ri[·],1≤i≤n1, (2)
andn2≥1PCTL-encoded optimisation objectives of the form
Oi::=Pmax[·]|Pmin[·]|Rmax[·]|Rmin[·],1≤i≤n2,(3)
where ‘· ’ is a placeholder for the set of PCTL probability
and reward properties supported by (1), the constrained multi-
objective policy synthesis problem solved by EvoPoli is to ﬁnd
the Pareto-optimal set PSof MDP policies that satisfy the
n1constraints and are Pareto-optimal with respect to the n2
optimisation objectives. Formally,
PS={σ∈DS|/logicalandtextn1
i=1B(M,σ,C i)∧(∄σ/prime∈DS•σ/prime≺σ)}
(4)
whereB(M,σ,C i)∈BisTrue if the constraint Ciis satisﬁed
for the MDP model Mand policy σ, and False otherwise. The
dominance relation ≺:DS×DS→B, assuming minimisation
of the optimisation objectives O1,O2,···,On2,i sg i v e nb y
∀σ,σ/prime∈DS•σ≺σ/prime≡∀1≤i≤n2•Q(M,σ,O i)≤
Q(M,σ/prime,Oi)∧∃1≤i≤n2•Q(M,σ,O i)<Q(M,σ/prime,Oi)
(5)
whereQ(M,σ,O i)∈Rdenotes the value of the optimisation
objective Oifor policy σon model M.
Finally, given the Pareto-optimal policies set PS, the Pareto-
optimal front PFis deﬁned by
PF={(Q(M,σ,O 1),...,Q(M,σ,O n2))|σ∈PS}.(6)
Example 1. Requirements R1–R3 from Table I deﬁne a
constrained multi-objective optimisation problem for the MDPmodelling the TAS system from our running example (Fig-ure 3), where n
1=1,C1=R1 ,n2=2,O1=R2 andO2=R3 .
Solving the constrained multi-objective policy synthesis
problem to establish the set PSof Pareto-optimal policies (4)
and the Pareto front PF(6) is complex and non-trivial [13].
Existing research [28], [18], [16], [17] can only solve simplerforms of this problem, i.e., those for which n
2=1(i.e., nu-
merical queries)o r n1=0(i.e., unconstrained Pareto queries).
We explain next how our EvoPoli approach supports the
synthesis of Pareto-optimal policies for an arbitrary number ofconstraints and optimisation objectives. Furthermore, throughexperiments detailed in Section VI, we illustrate how EvoPolisubsumes the policies produced by the current state-of-the-arttechniques for the simpler problem variants they can solve.
845B. MDP to pDTMC Transformation
To solve the constrained multi-objective policy synthesis
problem for an MDP M=(S,sI,A,Δ,L,R ), we construct
a parametric DTMC D(M)=(S,sI,P,L,R)with the same
state space, initial state, labelling function and reward function
set asM. For any pDTMC states s,s/prime∈Swith actions
A(s)={a1,a2,...,a n}enabled in state s, the transition proba-
bilityP(s,s/prime)is deﬁned over a parameter x(s)∈{1,2,...,n}:
P(s,s/prime)=Δ/parenleftbig
s,ax(s)/parenrightbig
(s/prime). (7)
We use the shorthand notation x:S→Nto refer to all
the parameters of this pDTMC. Next, we deﬁne n2pDTMC
optimisation objectives O/prime
1,O/prime
2, ...,O/prime
n2analogous to the
MDP optimisation objectives from (3) such that, if the i-th
MDP optimisation objective is Pmax=?[·], thenO/prime
iis ‘max-
imiseP=?[·]’, etc. The next result shows that solving the
MDP policy synthesis problem from the previous section isequivalent to solving a similar problem for this pDTMC.
Theorem 1. IfPS
/primeis the set of combinations of parameter
values for which D(M)satisﬁes the constraints (2) and is
Pareto-optimal with respect to the objectives O/prime
1,O/prime
2, ...,
O/prime
n2, the solution PS of the constrained multi-objective policy
synthesis problem for the MDP Mis given by
Policies(PS/prime)={σ:S→A|∃x∈PS/prime.(∀s∈S.σ(s)=ax(s))}.
(8)
Proof. We prove the theorem by contradiction. First, suppose
thatPolicies(PS/prime)contains a policy σ/∈PS, and letx∈PS/prime
be the combination of pDTMC parameter values associated
with this policy. As x∈PS/prime, theD(M)instance associated
withxsatisﬁes the constraints (2). Also, according to (7),
theD(M)instance associated with xand the MDP Munder
policyσhave identical transition probabilities, so Mmust also
satisfy these constraints under policy σ. As such, σ/∈PS=⇒
∃σ/prime∈PS.σ/prime≺σ. Additionally, for all 1≤i≤n2,D(M)
instance associated with xand the MDP Munder policy σ
must yield the same values for the properties evaluated for theoptimisation objectives O
iandO/prime
i, respectively.
Consider now the D(M)parameter combination x/primethat
satisﬁes∀s∈S.σ/prime(s)=ai=⇒x/prime(s)=i. As before, since
σ/prime∈PS, both the MDP Munder policy σ/primeand theD(M)
instance associated with x/primemust satisfy the constraints (7)
and must yield identical values for the properties evaluated
for the optimisation objectives OiandO/prime
i, respectively, for all
1≤i≤n2. It follows that x/primedominates x, and therefore
x/∈PS/prime, which contradicts the assumption we started from.
Accordingly, Policies(PS/prime)\PS=∅. The same reasoning
can be used to show that PS\Policies(PS/prime)=∅, and
therefore we must have PS=Policies(PS/prime).
Example 2. Figure 4 shows the result of applying the MDP
to pDTMC transformation described above to the reconf
module from the TAS system MDP in Figure 3. This pDTMCfragment shows how the nondeterministic choices from theMDP are replaced by choices parameterised by the threepDTMC parameters deﬁned in lines 2–4.pDTMC parameters
(possible values
shown in comments)1dtmc
2const int xMS sel;// 1, 2, ..., MAX MS
3const int xDS sel;// 1, 2, ..., MAX DS
4const int xAS sel;// 1, 2, ..., MAX AS
5module reconf
6 MSsel: [0..MAX MS] init0;
7 DSsel: [0..MAX DS] init0;
8 ASsel: [0..MAX AS] init0;
9
10 [selMS1] (MS sel=0) & (xMS sel=1)−> (MS sel’=1);
11 ...
12 [selMS5] (MS sel=0) & (xMS sel=5)−> (MS sel’=5);
13 ...
14 [selDS1] (MS sel>0) & (DS sel=0) & (xDS sel=1)−> (DS sel’=1);
15 ...
16 [selAS1] (DS sel>0) & (AS sel=0) & (xAS sel=1)−> (AS sel’=1);
17 ...
18 [selAS3] (DS sel>0) & (AS sel=0) & (xAS sel=3)−> (AS sel’=3);
19 endmoduleOrdered selection of service
implementations, encoded
as parameterised choices.
Fig. 4: pDTMC encoding of the reconf module from the TAS
MDP in Figure 3.
C. Evolutionary-based Policy Synthesis
Using exhaustive analysis to solve the constraint multi-
objective synthesis problem is unfeasible since the policy
decision space DS (cf. Def 5) is typically extremely large.
For instance, for the MDP model of our TAS running example
from Section III |DS|≈1065, while for the systems consid-
ered in our experimental evaluation |DS|>101000. Clearly,
enumerating and evaluating all possible policies is both time-consuming and computationally-prohibitive.
EvoPoli reformulates the policy synthesis problem as
a search-based optimisation problem [20] and uses multi-objective genetic algorithms (MOGA) [12], like the widely-used NSGA-II [29] and SPEA2 [30] algorithms, to intelli-gently navigate the decision space. EvoPoli iteratively evolvesa population of candidate policies to identify promising re-gions in the decision space and synthesise a close approx-imation of the Pareto-optimal policies set PS. EvoPoli
encodes each candidate policy (i.e., solution) as a tuple ofgenes. Each state s∈Sfor which the cardinality of its set
of enabled actions |A(s)|≥2is mapped to a gene. For any
states, the corresponding gene can take values from the set
{1,2,···,|A(s)|}. We refer interested readers to [7], [31] for
a detailed description of this encoding.
Algorithm 1 shows the high-level process underpinning
EvoPoli for the synthesis of the Pareto-optimal policies setPS and the corresponding Pareto front set PF. Given as
inputs the DTMC D(M)induced by the MDP M, the decision
spaceDS, and the lists of constraints (C
1,C2,···,Cn1)
and optimisation objectives (O1,O2,···,On2), EvoPoli starts
with empty PSandPF sets (line 2) and iteratively evolves
them through the loop (lines 3-25) until a termination crite-rion is met. The function T
ERMINATE (PS,DS)holds when
the maximum number of candidate policy evaluations hasbeen carried out (i.e., budget exhausted), or when no newupdates have been made in PS over a ﬁxed number of
successive iterations (i.e., the decision space has been ex-plored sufﬁciently yielding diverse and Pareto-optimal poli-
846Algorithm 1 Evolutionary-based Pareto Optimal Policy Syn-
thesis
1:function SYNTHESIS (D(M),DS,(Ci)1≤i≤n1,(Oi)1≤i≤n2)
2:PS←∅,PF←∅
3: while¬TERMINATE (PS,DS )do
4: G← GENERATE CANDIDATE POLICIES (DS,PS )
5: for allσ∈G do
6: {(Ci,σ)1≤i≤n1,(Oi,σ)1≤i≤n2}←
EVA L UAT E POLICY (D(M),σ,(Ci)1≤i≤n1,(Oi)1≤i≤n2)
7: if∧1≤i≤n1{Ci,σ}then
8: dominated ←false
9: for allσ/prime∈PS do
10: ifσ≺σ/primethen
11: PS=PS\{σ/prime}
12: PF=PF\{(Q(D(M),σ/prime,Oi))1≤i≤n2}
13: else ifσ/prime≺σthen
14: dominated ←true
15: break
16: end if
17: end for
18: if¬dominated then
19: PS=PS∪{σ}
20: PF =PF∪{(Oi,σ)1≤i≤n2}
21: end if
22: end if
23: end for
24: PS,PF ←DIVERSIFY POLICIES (PS,PF )
25: end while
26: returnPS,PF
27: end function
cies). Within each iteration, EvoPoli initially employs the
GENERATE CANDIDATE POLICIES function (line 4) to create
a population Gof plausible policies using MOGA-speciﬁc
crossover and mutation operators. Crossover randomly chooses
two ﬁt policies from the current Pareto-optimal set PS and
exchanges their genes to produce new policies. Mutation, onthe other hand, creates a new policy by randomly changinga subset of the genes of a policy based on its value rangeencoded in the decision space DS. Next, the for loop (lines 5-
23) evaluates each policy σ∈G and establishes its dominance
relation (cf. Eq. 5) with respect to the policies in PS.
To this end, the E
VA L UAT E POLICY function (line 6) uses
a probabilistic model checker to determine the satisfactioncondition of the n
1constraints and obtain the values for the n2
optimisation objectives. The policy σand the objectives tuple
are added to PS andPF, respectively, only if σsatisﬁes
all constraints and is not dominated by any other policy inPS. Similarly, policies dominated by σare removed from
PSalong with their associated objectives tuple (lines 7-22).
The execution of D
IVERSIFY POLICIES (line 24) uses MOGA-
speciﬁc mechanisms for diversity preservation to select poli-cies from PSthat will participate in the next iteration. These
mechanisms maintain diversity in the population and generatea PF that covers sufﬁciently the objective space. For instance,the diversity mechanism used by NSGA-II [29] combines thenon-domination level of each evaluated policy and a crowdingdistance metric, i.e., the population density in its area ofthe search space. Once the evolution terminates, the Pareto-optimal set approximation PS is returned along with the
Pareto-optimal front approximation PF (line 26).Fig. 5: Pareto front of policies for the TAS quality require-ments from Table I synthesised using EvoPoli instrumentedwith NSGA-II [29] and SPEA2 [30].
Example 3. Figure 5 shows two Pareto front PF sets obtained
for our TAS running example using the quality requirements
from Table I. As shown, the NSGA-II-instrumented EvoPoliproduces more policies than its SPEA2-instrumented counter-part. Both MOGAs had the same experimental setup, i.e., 1000evaluations and a population of 20. We should also highlightthat neither PRISM [10] nor Storm [11] can produce a Paretofront for this combination of objectives and constraints.
V. I
MPLEMENTATION
To ease the evaluation and adoption of EvoPoli, we have im-
plemented a prototype tool in Java that realises the high-levelEvoPoli workﬂow from Figure 1. The MDP transformationcomponent consumes an MDP model speciﬁed in the high-level modelling language of the PRISM model checker [10]and the PCTL-encoded constraints (2) and optimisation objec-tives (3), and applies the process described in Section IV-B toproduce the pDTMC and the pDTMC-compliant constraintsand optimisation objectives. We have developed the synthesismethod from Algorithm 1 on top of the search-based soft-ware engineering tool EvoChecker [7], [31]. The open-sourcecode of EvoPoli, the full experimental results summarised inthe following section, additional information about EvoPoliand the case studies used for its evaluation are available athttps://github.com/gerasimou/MDPSynthesis.
VI. E
V ALUATION
A. Research Questions
RQ1 (Validation): How does our approach perform com-
pare to existing probabilistic model checkers? We analyse if
our approach can synthesise policies of similar quality to thoseproduced by the probabilistic model checkers PRISM [10]and Storm [11] for the simpler class of problems (i.e., uncon-strained Pareto queries) that these model checkers can solve.
RQ2 (Effectiveness): How do EvoPoli instances instru-
mented with different MOGAs compare to each other? We
used this research question to analyse the impact of differentMOGAs in the performance of EvoPoli. To this end, we study
847TABLE II: Quality requirements for Ocean Worlds
ID Type Description PCTL
R1 Constraint The robotic lander must complete
its mission within 30 minsRT
≤30[C]
R2 Objective Maximise science value RSV
=max?[C]
R3 Objective Maximise probability of success Pmax=? [Fd o n e ]
R4 Objective Minimise energy consumption RECmin=? [C]
the quality of EvoPoli-synthesised policies when our approach
uses the established MOGAs NSGA-II [29] and SPEA2 [32].
RQ3 (Decision support): Can EvoPoli provide useful in-
sights into the trade-offs between the quality attributesvalues produced by different policies? To support decision
making and help software engineers to make informed de-cisions, EvoPoli must synthesise policies with different trade-offs. Hence, we assessed the trade-offs in policies produced byEvoPoli for the software systems analysed in our evaluation.
B. Evaluation Methodology
Software Systems. We performed a wide range of exper-
iments to evaluate EvoPoli using multiple variants of two
software systems derived from different application domains:(1) the service-based Tele Assistance System (TAS) adaptedfrom [27] and described in Section III; and (2) a prototyperobotic planner software component for ocean world (OW)exploration [33] which we describe next.
Ocean Worlds (OW). The Ocean Worlds Autonomy Testbed
for Exploration Research and Simulation project led by NASA
Ames Research Center is developing an autonomy softwaretestbed to spur the development of autonomy technologies forsurface missions [33]. This testbed is conceived for missionsin which a robotic lander collects and analyses samples,and then sends relevant data back to Earth. To completethe mission, the robot must choose among xloc alternative
excavation locations each of which has an associated science
value (a measurement of the potential interest of samples in
that location) and an excavatability risk (signifying the safety
and difﬁculty of excavating in that part of the terrain). Foreach successful excavation, the robot must choose where todump the resulting rubble by selecting among dloc available
dumping locations. Excavating and moving around the robot’sarm consumes a corresponding amount of energy. Data is sentback to Earth during a speciﬁc time window for processing andfurther analysis. Table II shows the OW mission requirements.
The autonomy software on the robotic lander includes a fa-
cility to replace existing plans as the mission progresses, withupdated plans coming from Earth or generated by automatedplanners on-board and/or on Earth. One of such plannersemploys MDP policy synthesis to make high-level decisionsabout excavation and dumping location selections, which areencoded as nondeterministic choices in a MDP model. Forthe excavation location selection, each alternative is encodedas a command in which a failure to excavate is associatedwith a probability that encodes the excavatability risk. Three
reward structures capture the science value, time, and energyTABLE III: System variants analysed using EvoPoli
Variant Details #DS Trun:mean(±SD )
TAS2 MAX_Timeout=2 10509647.21(±601.56)
TAS3 MAX_Timeout=3 106716827.73(±598.35)
TAS4 MAX_Timeout=4 108426519.87(±1016.59)
OW4 xloc=4, dloc=4 1072668.57(±31.15)
OW5 xloc=5, dloc=5 10983756.21(±230.38)
OW6 xloc=6, dloc=6 1013816362.04(±688.61)
consumption associated with each selection. Due to space
constraints we omit the full details of the MDP model of thissystem; we refer interested readers to our project webpage.
Experimental Setup. We performed a wide range of ex-
periments using the TAS and OW system variants from
Table III. The ‘Details’ column lists the values speciﬁed for thevariables of each system variant, i.e., the maximum timeout(MAX_Timeout) for the TAS, and the number of excavation(xloc) and dumping (dloc) locations for the OW system.The column ‘#DS ’ reports the search space of the policies
according to Def. 5. Finally, the column T
runreports the
average running time (and standard deviation in parenthesis)for completing a policy synthesis run per system variant.
We instrumented the evolutionary-based policy synthesis
algorithm of EvoPoli using the established MOGAs NSGA-II [29] and SPEA2 [32]. We also used the following conﬁgura-tion to evaluate our approach: 5,000 evaluations with an initialpopulation of 100 individuals (i.e., 50 generations in total), anddefault values for single-point crossover probability p
c=0.9
and uniform polynomial mutation probability pm=0.8.
We selected these values based on our experience in theﬁeld [7], [15], [31] and after performing a set of preliminaryexperiments.
To alleviate the potential impact of randomness in the per-
formance and effectiveness of MOGAs (e.g., when choosingthe crossover point, when sampling randomly to execute themutation operation), we followed the established procedurein search-based software engineering [20]. We executed 30independent runs per system variant from Table III and eachmultiobjective optimisation algorithm [22]. All the experi-ments were run on a CentOS Linux 6.5 64bit server with two2.6GHz Intel Xeon E5-2670 processors and 32GB of memory.
Statistical analysis. For real-world systems such as those
used in our experimental evaluation the policy decision space
DS (Def. 5) is extremely large. Thus, producing the actual
Pareto front is typically unfeasible. Aligned with the standardpractice [21], for each system variant we produce the reference
front comprising the nondominated policies from all the runs
executed across all MOGA-based EvoPoli instances and thepolicies produced by the probabilistic model checkers Stormand PRISM (for the simple class of multi-objective policysynthesis problems that these models checkers can handle).We used this reference front and the widely-used Pareto-frontquality indicators below to quantify the ‘goodness of ﬁt’ ofPareto front approximations synthesised by EvoPoli instances,Storm and PRISM. For each quality indicator, we use a boxplot to present its central tendency and distribution.
848Fig. 6: Boxplots comparing EvoPoli (NSGA-II), EvoPoli (SPEA2), PRISM and Storm for the TAS (left) and OW (right) system
variants and for unconstrained Pareto queries (i.e., n1=0,n2=2), evaluated using quality indicators IHV,I/epsilon1andIIGD.
•TheIHV (Hypervolume) indicator uses a reference front
and measures the volume in the objective space consumedby a Pareto front approximation. I
HVmeasures both diver-
sity and convergence, and is strictly Pareto compliant1[21].
Better Pareto front approximations have larger IHVvalues.
•TheI/epsilon1(Unary additive epsilon) denotes the minimum
additive term needed to alter the objective vector from aPareto front approximation to dominate the correspondingobjective vector of the reference front. This indicator showsconvergence to the reference front and is Pareto compliant.SmallerI
/epsilon1values mean better Pareto front approximations.
•TheIIGD (Inverted Generational Distance) indicator mea-
sures the Euclidean distance in the objective space betweenthe reference front and the Pareto front approximation.I
IGD signiﬁes an “error measure”, and indicates both
diversity and convergence to the reference front. SmallerI
IGD values signify better Pareto front approximations.
Following the recommended practice [22], we used infer-
ential statistical tests to compare the quality indicator valuesobtained by EvoPoli instances and the values obtained byPRISM and Storm. We employed the Shapiro-Wilk test andconﬁrmed that the quality indicator values do not follow anormal distribution. Thus, we used the Mann-Whitney andKruskal-Wallis non-parametric tests with 95% conﬁdence level(α=0.05) to analyse the results without making assumptions
about the data distribution or the homogeneity of its vari-ance. We also performed a post-hoc analysis with pairwisecomparisons between the algorithms, using the conservativeBonferroni correction p
crit=α/k (kis the number of
comparisons) to control the family-wise error rate.
When statistical signiﬁcance exists, we use Cohen’s d to
quantify the importance of the observed effect [22]. Cohen’sd score summarises the difference between two groups as the
1Pareto compliant indicators conform to the order speciﬁed by the Pareto
dominance relation on Pareto front approximations [21]number of standard deviations with d=0.2,d=0.5andd=0.8
denoting a small, medium and large effect size, respectively.
C. Results & Discussion
RQ1 (Validation). Since neither PRISM nor Storm can solve
the constrained multi-objective policy synthesis problem from
Section IV-A, we can ensure a fair comparison only bytransforming the problem into an unconstrained Pareto query
(i.e.,n
1=0,n2=2) that both model checkers can handle2.
To achieve this, we removed constraint R1from both systems
and retained requirements R2,R3(minimise response time,
minimise cost) and R2,R4(maximise science value, minimise
energy) for TAS and OW, respectively.
Figure 6 shows the boxplots for the IHV,I/epsilon1andIIGD
quality indicators for all six system variants from Table III.Undoubtedly, for all quality indicators and across all systemvariants there is a clear distance between the quality indicatorvalues obtained by EvoPoli instrumented with NSGA-II orSPEA2 and those produced by PRISM and Storm. We con-ﬁrmed our ﬁndings from the visual inspection of the boxplotsby using the Kruskal-Wallis test which showed statisticalsigniﬁcance (p-value <0.05) for all system variants and
for all quality indicators. We also ran a post-hoc analysisof pairwise comparisons between the EvoPoli instances, andPRISM and Storm using the Mann-Whitney test. For allcomparisons, we observed statistically signiﬁcant differencesin favour of EvoPoli, with the p-value being in the range[6.2473E−15,1.0016E−13]and with a high effect size
(d>0.8). This is a key result of our validation experiments
that indicates EvoPoli’s capacity to produce Pareto fronts ofhigher quality than those produced by PRISM and Storm.
We support further our ﬁndings through the Pareto front
approximations produced by NSGA-II-based EvoPoli, PRISMand Storm for the OW system variants (Figure 7). Evidently,
2We selected the maximum number of objectives that both model checkers
support; Storm can handle up to three objectives.
849Fig. 7: Pareto front approximation for the OW system variants (OW4 left, O5 middle, OW6 right) and objectives R2 (maximise
science value) and R4 (minimise energy) from Table II using PRISM, Storm and NSGA-II-based EvoPoli.
Fig. 8: Boxplots comparing EvoPoli (NSGA-II) and EvoPoli (SPEA2) for the TAS (left) and OW (right) system variants andrequirements from Tables I and II, respectively, evaluated using quality indicators I
HV,I/epsilon1andIIGD.
the policies synthesised by EvoPoli for this typical run closelyapproximate those produced by the model checkers while alsocovering a larger spectrum of the objective space. In general,both model checkers found the same policies as shown bythe identical Pareto fronts (Figure 7) and the almost identicalquality indicator values (Figure 6) – in few problem instancesStorm produced more solutions than PRISM. Irrespectiveof the system variant, however, the produced policies areconstrained to the boundaries of the objective space. Sincethe model checkers employ linear programming, they, unsur-prisingly, have difﬁculties ﬁnding useful policies when theobjective space is non-convex [17]. In contrast, EvoPoli withits MOGA-based specialisation is not sensitive to the shape orcontinuity of the Pareto front, and, thus, can synthesise policieswhen the objective space is also discontinuous or concave [12].We note that due to the iterative nature of MOGAs usedin EvoPoli, our approach takes more time than PRISM orStorm. We have demonstrated, however, that EvoPoli producesa richer and more diversiﬁed set of solutions than the othermodel checkers. Investigating mechanisms to improve thescalability of EvoPoli is part of our future work.These ﬁndings clearly demonstrate that EvoPoli can syn-
thesise policies of equivalent quality to those produced byPRISM and Storm for the simpler class of problems (i.e.,unconstrained Pareto queries) that these model checkers cansolve. Also, the EvoPoli-produced Pareto front is greatly morediverse and covers a wider spectrum of the objective space.
RQ2 (Effectiveness). We answer this research question by
comparing the quality of the Pareto fronts synthesised by
two EvoPoli instances using NSGA-II [29] and SPEA2 [32]for the TAS and OW system variants and the full set ofrequirements from Tables I and II, respectively. Figure 5 showstwo derived Pareto fronts for a typical run using these twoEvoPoli instances. As shown in Figure 8, the distributionsof theI
HV,I/epsilon1andIIGD quality indicators for the SPEA2-
instrumented EvoPoli have a larger overall variability. Incontrast, the NSGA-II-based boxplots are more concentratedas indicated by the smaller whiskers and the very few pointsabove or below them. Since both MOGAs generally followthe same evolutionary algorithm principles and apply elitism,i.e., they propagate the best policies across generations, thisbehaviour could occur due to the different diversity preserva-
850tion mechanisms used; NSGA-II employs a crowding distance
while SPEA2 invokes an archive truncating procedure [12].
The statistical comparison using the Mann-Whitney test
showed statistical signiﬁcance across all system variants,with p-value ranging [1.716E−12,2.599E−10]and[6.255E−
10,2.655E−06]for TAS and OW, respectively. The effect size
was large in all system variant-quality indicator combinationsexcept from the TAS4-I
HVpair where the effect was medium.
These results provide strong empirical evidence that EvoPoli
with NSGA-II can synthesise policies that achieve betterquality indicator values than policies synthesised by EvoPoliusing SPEA2. More importantly, we have shown that EvoPolican form effective Pareto optimal policies sets using alternativeMOGAs, thus demonstrating the ability of EvoPolito solve theconstraint multi-objective policy synthesis problem.
RQ3 (Decision Support). We answer this research question
by qualitatively analysing the Pareto front approximations to
identify actionable insights concerning the trade-offs betweenthe quality attributes encoded by the synthesised policies. First,through the use of MOGAs, EvoPoli can examine efﬁcientlythe discontinuous, and likely non-convex, policy decisionspace to produce Pareto front policy approximations thatcover sufﬁciently the space. Given this information, softwareengineers can have a more informed view of the differentquality attributes trade-off for their system.
Second, EvoPoli enables the identiﬁcation of the “points
of diminishing returns” where every increase in the valueof a quality attribute incurs a disproportional deterioration tothe other quality attributes. For the OW6 system variant, forinstance, one such point is approximately located at (5,0.75)signifying that policies which contribute higher science val-ues consume signiﬁcantly more energy. Depending on thesystem-speciﬁc preferences, software engineers can use thisinformation to eliminate such policies (if a balance in qualityattributes is preferred) or analyse further these policies (e.g.,equip the robot with a larger battery to accommodate theincreased energy consumption and enable to use this policy).
Finally, a closer inspection of the Pareto policies set revealed
multiple policies that yielded the same quality attribute values.From a planning perspective, these alternative policies (cannotbe shown on the Pareto fronts as their values overlap) arevery useful as they can support fast system reconﬁgurationwithout the need to perform another policy synthesis operation.Having, for instance, a repository of policies with the samequality attributes enables the quick selection of the functioningpolicies when a malfunction renders the currently active policyunusable. This is a unique feature of EvoPoli that does notexist in either Storm or PRISM.
D. Threats to V alidity
We limit construct validity threats that could occur due
to simpliﬁcations in the adopted experimental methodology
using the widely-studied TAS case study [27]. We obtainedthe information for the OW system from the literature [33].
We mitigate internal validity threats that could introduce
bias when establishing the causality between our ﬁndings andthe experimental study by assessing EvoPoli using independentresearch questions. We reported results over 30 independentruns per system variant, thus reducing threats due to thestochasticity of the employed multi-objective evolutionaryalgorithms. Also, we used the inferential statistical tests Mann-Whitney and Kruskal-Wallis to check for statistical signif-icance (α =0.05), supported by post-hoc analysis using
Mann-Whitney’s test and Bonferroni’s correction to controlthe family-wise error rate. Finally, we employed Cohen’s d toassess the effect size and calculate the amount of improvement.
We mitigate external validity threats that could affect the
generalisation of our approach by developing EvoPoli on topof the search-based software engineering tool EvoChecker [31]that uses MDP models encoded in the high-level modellinglanguage of PRISM [10]. The experimental evaluation usingmultiple variants of two software systems reduces furtherthe risk that EvoPoli may be difﬁcult to use in practice.However, further experiments are needed to establish theapplicability, feasibility and scalability of EvoPoli in domainsand applications with characteristics different from those usedin our evaluation.
VII. R
ELATED WORK
Markov decision processes (MDP) have a wide range of ap-
plications in software systems across many domains [8], [34],[35], [36]. MDP models can leave nondeterministic choicesunderspeciﬁed, which can be resolved in disparate ways bydifferent control policies that can balance multiple, potentiallyconﬂicting, objectives [3], [37], [38]. In a high optimisationspace, there is typically no single policy that optimises allobjectives, but rather, a set of Pareto optimal policies withdifferent tradeoffs that form a Pareto front. For existing modelcheckers, Pareto fronts are often obtained by either using linearprogramming [16], [13] or iterating over weighted sums ofobjectives [17], [39], [40]. Employing these methods lead tolimited applicability and scalability due to the computationalcost involved, constrained search space and the limited numberof optimisation objectives supported [7], [31], [18], [28].PRISM and Storm, for instance, are two of the most advancedprobabilistic model checkers currently available, and they arelimited to synthesis of MDP multi-objective policies that canconsider up to two and three optimisation objectives withoutconstraints (in Storm and PRISM, respectively), or only oneobjective if the problem contains constraints. In contrast,EvoPoli can handle an arbitrary combination of any numberof constraints and objectives. Also, Pareto fronts generatedby our approach contain much more diversity because, unlikeother approaches, the applicability of evolutionary algorithmsis not constrained to convex optimisation problems, where theset of achievable values for a Pareto query is also convex [17].
Multi-objective Reinforcement learning (RL) is a technique,
orthogonal to model checking, for obtaining Pareto optimalpolicies. A major research direction of multi-objective RLis currently on improving the efﬁciency of training [41],[42], [43]. The approximation of Pareto fronts using RL isdetermined by minimising the difference between sampling
851actions and feedback signals. In contrast to conventional RL,
multi-objective RL uses one scalar feedback signal per ob-jective, which ampliﬁes training complexity and makes it lessefﬁcient [44]. Another issue of using RL for obtaining Paretooptimal policies is that it does not always guarantee safetyproperties, although recent works started introducing extramechanisms to mitigate this issue (e.g., by integrating humanor domain knowledge in the training) [45]. However, theseapproaches have several limitations, i.e., do not support multi-objective optimisation [46], make strong assumptions [47], orneed complex preprocessing [43].
Search-based software engineering (SBSE), has been exten-
sively studied in various applications and domains, includingproject management [48], [49], [50], software testing [51],[52], [53], model checking [20], [54], [55], [56] and featureselection in software product lines [57], [58]. SBSE has alsobeen extended to synthesising Pareto-optimal sets of proba-bilistic models [7], [31], [59], [60]. EvoChecker [7], [31] usesmultiobjective optimisation (i.e., genetic algorithms) to au-tomatically produce approximate Pareto-optimal probabilisticmodel sets with respect to given requirements or constraints. Inour work, we leverage EvoChecker as a means of supportingthe synthesis method from Algorithm 1.
EvoPoli is, to the best of our knowledge, the ﬁrst that can
solve the multi-objective constrained policy synthesis problem.Concretely, EvoPoli can approximate a set of Pareto optimalpolicies and the Pareto front for an arbitrary combination ofany number of optimisation objectives and constraints.
VIII. C
ONCLUSION
We presented EvoPoli, a tool-supported approach for the
automated synthesis of Pareto-optimal policies for MDPswith complex combinations of constraints and optimisationobjectives. We evaluated EvoPoli on two case studies fromdifferent domains and demonstrated its ability to synthesisepolicies for problems that can be handled by the probabilisticmodel checkers PRISM [10] and Storm [11] as well as formore complex problems that neither of them can support. Ourfuture work includes (1) extending EvoPoli to support policysynthesis on timed MDPs; (2) explore parallelisation methodsto improve EvoPoli’s scalability; and (3) applying EvoPoli toother applications and scenarios.
Acknowledgements: This research was supported by the
European Union’s Horizon 2020 project SESAME (grantagreement No 101017258), the UKRI project EP/V026747/1‘Trustworthy Autonomous Systems Node in Resilience’, theUK EPSRC project EP/R026173/1 ‘Offshore Robotics forCertiﬁcation of Assets’ (through its PRF project COVE), andthe Assuring Autonomy International Programme.
R
EFERENCES
[1] M. L. Puterman, “Markov decision processes,” Handbooks in operations
research and management science, vol. 2, pp. 331–434, 1990.
[2] C. Baier and J.-P. Katoen, Principles of model checking. MIT press,
2008.[3] P. Chrszon, C. Dubslaff, S. Klüppelholz, and C. Baier, “Profeat: feature-
oriented engineering for family-based probabilistic model checking,”
Formal Aspects of Computing, vol. 30, no. 1, pp. 45–75, 2018.
[4] J. Cámara, D. Garlan, and B. Schmerl, “Synthesizing tradeoff spaces
with quantitative guarantees for families of software systems,” Journal
of Systems and Software, vol. 152, pp. 33–49, 2019.
[5] G. Su, T. Chen, Y . Feng, D. S. Rosenblum, and P. Thiagarajan, “An
iterative decision-making scheme for markov decision processes and itsapplication to self-adaptive systems,” in International Conference on
Fundamental Approaches to Software Engineering. Springer, 2016,pp. 269–286.
[6] B. Lacerda, D. Parker, and N. Hawes, “Optimal policy generation for
partially satisﬁable co-safe ltl speciﬁcations.” in IJCAI, 2015, pp. 1587–
1593.
[7] S. Gerasimou, G. Tamburrelli, and R. Calinescu, “Search-based synthesis
of probabilistic models for quality-of-service software engineering,” in2015 30th IEEE/ACM International Conference on Automated SoftwareEngineering (ASE). IEEE, 2015, pp. 319–330.
[8] G. A. Moreno, J. Cámara, D. Garlan, and B. Schmerl, “Proactive self-
adaptation under uncertainty: a probabilistic model checking approach,”inProceedings of the 2015 10th Joint Meeting on Foundations of
Software Engineering, 2015, pp. 1–12.
[9] H. Hansson and B. Jonsson, “A logic for reasoning about time and
reliability,” Formal Aspects of Computing, vol. 6, no. 5, pp. 512–535,
1994.
[10] M. Kwiatkowska, G. Norman, and D. Parker, “Prism 4.0: Veriﬁcation
of probabilistic real-time systems,” in International Conference on
Computer Aided V eriﬁcation. Springer, 2011, pp. 585–591.
[11] C. Dehnert, S. Junges, J.-P. Katoen, and M. V olk, “A storm is coming:
A modern probabilistic model checker,” in International Conference on
Computer Aided V eriﬁcation. Springer, 2017, pp. 592–600.
[12] C. A. C. Coello, G. B. Lamont, D. A. Van Veldhuizen et al., Evolutionary
algorithms for solving multi-objective problems. Springer, 2007, vol. 5.
[13] K. Etessami, M. Kwiatkowska, M. Y . Vardi, and M. Yannakakis,
“Multi-objective model checking of Markov decision processes,” inInternational Conference on Tools and Algorithms for the Constructionand Analysis of Systems. Springer, 2007, pp. 50–65.
[14] C. Baier, H. Hermanns, and J.-P. Katoen, “The 10,000 facets of MDP
model checking,” in Computing and Software Science. Springer, 2019,
pp. 420–451.
[15] R. Calinescu, M. Autili, J. Cámara, A. Di Marco, S. Gerasimou,
P. Inverardi, A. Perucci, N. Jansen, J.-P. Katoen, M. Kwiatkowska et al.,
“Synthesis and veriﬁcation of self-aware computing systems,” in Self-
Aware Computing Systems. Springer, 2017, pp. 337–373.
[16] V . Forejt, M. Kwiatkowska, G. Norman, D. Parker, and H. Qu,
“Quantitative multi-objective veriﬁcation for probabilistic systems,” inInternational Conference on Tools and Algorithms for the Constructionand Analysis of Systems. Springer, 2011, pp. 112–127.
[17] V . Forejt, M. Kwiatkowska, and D. Parker, “Pareto curves for prob-
abilistic model checking,” in International Symposium on Automated
Technology for V eriﬁcation and Analysis. Springer, 2012, pp. 317–332.
[18] F. Delgrange, J.-P. Katoen, T. Quatmann, and M. Randour, “Simple
strategies in multi-objective MDPs,” in International Conference on
Tools and Algorithms for the Construction and Analysis of Systems.Springer, 2020, pp. 346–364.
[19] J. R. Harbin, S. Gerasimou, N. Matragkas, A. Zolotas, and R. Calinescu,
“Model-driven simulation-based analysis for multi-robot systems,” inACM/IEEE 24th International Conference on Model Driven EngineeringLanguages and Systems (MODELS), 2021.
[20] M. Harman, S. A. Mansouri, and Y . Zhang, “Search-based software
engineering: Trends, techniques and applications,” ACM Computing
Surveys (CSUR), vol. 45, no. 1, pp. 1–61, 2012.
[21] E. Zitzler, J. Knowles, and L. Thiele, “Quality assessment of Pareto set
approximations,” Multiobjective optimization, pp. 373–404, 2008.
[22] A. Arcuri and L. Briand, “A practical guide for using statistical tests
to assess randomized algorithms in software engineering,” in 2011 33rd
International Conference on Software Engineering (ICSE). IEEE, 2011,pp. 1–10.
[23] C. Daws, “Symbolic and parametric model checking of discrete-time
Markov chains,” in First International Conference on Theoretical As-
pects of Computing (ICTAC), 2005, pp. 280–294.
[24] R. Calinescu, C. Paterson, and K. Johnson, “Efﬁcient parametric model
checking using domain knowledge,” IEEE Transactions on Software
Engineering, vol. 47, no. 6, pp. 1114–1133, 2021.
852[25] X. Fang, R. Calinescu, S. Gerasimou, and F. Alhwikem, “Fast parametric
model checking through model fragmentation,” in 2021 IEEE/ACM 43rd
International Conference on Software Engineering (ICSE). IEEE, 2021,
pp. 835–846.
[26] A. Bianco and L. De Alfaro, “Model checking of probabilistic and
nondeterministic systems,” in International Conference on Foundations
of Software Technology and Theoretical Computer Science . Springer,
1995, pp. 499–513.
[27] D. Weyns and R. Calinescu, “Tele assistance: A self-adaptive service-
based system exemplar,” in 10th IEEE/ACM International Symposium on
Software Engineering for Adaptive and Self-Managing Systems, SEAMS2015. IEEE Computer Society, 2015.
[28] T. Quatmann, S. Junges, and J.-P. Katoen, “Markov automata with
multiple objectives,” Formal Methods in System Design, pp. 1–54, 2021.
[29] K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan, “A fast and elitist
multiobjective genetic algorithm: NSGA-II,” IEEE Transactions on
Evolutionary Computation, vol. 6, no. 2, pp. 182–197, 2002.
[30] E. Zitzler, M. Laumanns, and L. Thiele, “SPEA2: Improving the strength
Pareto evolutionary algorithm,” in Evolutionary Methods for Design
Optimization and Control with Applications to Industrial Problems(EUROGEN’01), 2001, pp. 95–100.
[31] S. Gerasimou, R. Calinescu, and G. Tamburrelli, “Synthesis of proba-
bilistic models for quality-of-service software engineering,” Automated
Software Engineering, vol. 25, no. 4, pp. 785–831, 2018.
[32] E. Zitzler and L. Thiele, “Multiobjective evolutionary algorithms: a com-
parative case study and the strength pareto approach,” IEEE transactions
on Evolutionary Computation, vol. 3, no. 4, pp. 257–271, 1999.
[33] L. Edwards, U. Wong, K. Dalal, C. Kulkarni, A. Rogg, A. Tardy,
T. Stucky, O. Umurhan, D. Catanoso, and T. Welsh, “An autonomysoftware testbed simulation for ocean worlds missions,” in Earth and
Space 2021, 2021, pp. 369–380.
[34] T. L. Cheung, K. Okamoto, F. Maker III, X. Liu, and V . Akella, “Markov
decision process (MDP) framework for optimizing software on mobilephones,” in Proceedings of the seventh ACM International Conference
on Embedded software, 2009, pp. 11–20.
[35] A. Ksentini, T. Taleb, and M. Chen, “A Markov decision process-
based service migration procedure for follow me cloud,” in 2014 IEEE
International Conference on Communications (ICC). IEEE, 2014, pp.1350–1354.
[36] J. Noppen, M. Aksit, B. Tekinerdogan, and V . Nicola, “Market-driven
approach based on Markov decision theory for optimal use of resourcesin software development,” IEE proceedings-Software, vol. 151, no. 2,
pp. 85–94, 2004.
[37] K. Chatterjee, “Markov decision processes with multiple long-run aver-
age objectives,” in International Conference on Foundations of Software
Technology and Theoretical Computer Science. Springer, 2007, pp.473–484.
[38] E. M. Hahn, V . Hashemi, H. Hermanns, M. Lahijanian, and A. Turrini,
“Multi-objective robust strategy synthesis for interval Markov decisionprocesses,” in International Conference on Quantitative Evaluation of
Systems. Springer, 2017, pp. 207–223.
[39] C. Hensel, S. Junges, J.-P. Katoen, T. Quatmann, and M. V olk, “The
probabilistic model checker storm,” arXiv preprint arXiv:2002.07080,
2020.
[40] A. Hartmanns, S. Junges, J.-P. Katoen, and T. Quatmann, “Multi-cost
bounded reachability in MDP,” in International Conference on Tools
and Algorithms for the Construction and Analysis of Systems. Springer,2018, pp. 320–339.
[41] G. Tesauro, R. Das, H. Chan, J. O. Kephart, D. Levine, F. L. Rawson III,
and C. Lefurgy, “Managing power consumption and performance ofcomputing systems using reinforcement learning.” in NIPS, vol. 7.
Citeseer, 2007, pp. 1–8.
[44] K. Van Moffaert and A. Nowé, “Multi-objective reinforcement learning
using sets of pareto dominating policies,” The Journal of Machine
Learning Research, vol. 15, no. 1, pp. 3483–3512, 2014.[42] L. Barrett and S. Narayanan, “Learning all optimal policies with
multiple criteria,” in Proceedings of the 25th International Conference
on Machine Learning, 2008, pp. 41–47.
[43] K. Van Moffaert, M. M. Drugan, and A. Nowé, “Scalarized multi-
objective reinforcement learning: Novel design techniques,” in 2013
IEEE Symposium on Adaptive Dynamic Programming and Reinforce-ment Learning (ADPRL). IEEE, 2013, pp. 191–199.
[45] J. Garcıa and F. Fernández, “A comprehensive survey on safe reinforce-
ment learning,” Journal of Machine Learning Research, vol. 16, no. 1,
pp. 1437–1480, 2015.
[46] S. Junges, N. Jansen, C. Dehnert, U. Topcu, and J.-P. Katoen, “Safety-
constrained reinforcement learning for MDPs,” in International Con-
ference on Tools and Algorithms for the Construction and Analysis ofSystems. Springer, 2016, pp. 130–146.
[47] M. Alshiekh, R. Bloem, R. Ehlers, B. Könighofer, S. Niekum, and
U. Topcu, “Safe reinforcement learning via shielding,” in Proceedings
of the AAAI Conference on Artiﬁcial Intelligence, vol. 32, no. 1, 2018.
[48] F. Ferrucci, M. Harman, J. Ren, and F. Sarro, “Not going to take this
anymore: Multi-objective overtime planning for software engineeringprojects,” in 35th International Conference on Software Engineering
(ICSE). IEEE, 2013, pp. 462–471.
[49] J. Ren, M. Harman, and M. Di Penta, “Cooperative co-evolutionary
optimization of software project staff assignments and job scheduling,”inInternational Symposium on Search Based Software Engineering.
Springer, 2011, pp. 127–141.
[50] C. Stylianou, S. Gerasimou, and A. S. Andreou, “A novel prototype tool
for intelligent software project scheduling and stafﬁng enhanced withpersonality factors,” in 24th IEEE International Conference on Tools
with Artiﬁcial Intelligence, vol. 1. IEEE, 2012, pp. 277–284.
[51] J. H. Andrews, T. Menzies, and F. C. Li, “Genetic algorithms for
randomized unit testing,” IEEE Transactions on Software Engineering,
vol. 37, no. 1, pp. 80–94, 2011.
[52] G. Fraser and A. Arcuri, “Whole test suite generation,” IEEE Transac-
tions on Software Engineering, vol. 39, no. 2, pp. 276–291, 2012.
[53] M. Harman, Y . Jia, and W. B. Langdon, “Strong higher order mutation-
based test data generation,” in Proceedings of the 19th ACM SIGSOFT
Symposium and the 13th European Conference on Foundations ofSoftware Engineering, 2011, pp. 212–222.
[54] G. Katz and D. Peled, “Synthesis of parametric programs using genetic
programming and model checking,” arXiv preprint arXiv:1402.6785,
2014.
[55] E. Alba and F. Chicano, “Finding safety errors with aco,” in Proceedings
of the 9th Annual Conference on Genetic and Evolutionary Computation,2007, pp. 1066–1073.
[56] ——, “Searching for liveness property violations in concurrent systems
with ACO,” in Proceedings of the 10th annual Conference on Genetic
and Evolutionary Computation, 2008, pp. 1727–1734.
[57] L. Ochoa, O. Gonzalez-Rojas, A. P. Juliana, H. Castro, and G. Saake,
“A systematic literature review on the semi-automatic conﬁguration ofextended product lines,” Journal of Systems and Software, vol. 144, pp.
511–532, 2018.
[58] C. Henard, M. Papadakis, M. Harman, and Y . Le Traon, “Combining
multi-objective search and constraint solving for conﬁguring largesoftware product lines,” in 37th IEEE/ACM International Conference
on Software Engineering, vol. 1. IEEE, 2015, pp. 517–528.
[59] R. Calinescu, M. ˇCeška, S. Gerasimou, M. Kwiatkowska, and N. Pao-
letti, “Efﬁcient synthesis of robust models for stochastic systems,”Journal of Systems and Software, vol. 143, pp. 140–158, 2018.
[60] ——, “Designing robust software systems through parametric markov
chain synthesis,” in 2017 IEEE International Conference on Software
Architecture (ICSA). IEEE, 2017, pp. 131–140.
853