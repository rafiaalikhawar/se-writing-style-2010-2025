Online Testing ofRESTfulAPIs:Promisesand Challenges
AlbertoMartin-Lopez
alberto.martin@us.es
SCORELab, I3USInstitute,
Universidadde Sevilla
Seville, SpainSergioSegura
sergiosegura@us.es
SCORELab, I3USInstitute,
Universidadde Sevilla
Seville, SpainAntonio Ruiz-Cort√©s
aruiz@us.es
SCORELab, I3USInstitute,
Universidadde Sevilla
Seville, Spain
ABSTRACT
Online testing of web APIs√êtesting APIs in production√êis gaining
traction in industry. Platforms such as RapidAPI and Sauce Labs
provideonlinetestingandmonitoringservicesofwebAPIs24/7,
typicallybyre-executingmanuallydesignedtestcasesonthetarget
APIs on a regular basis. In parallel, research on the automated gen-
erationoftestcasesforRESTfulAPIshasseensignificantadvances
in recent years. However, despite their promising results in the
lab,itisunclearwhetherresearchtoolswouldscaletoindustrial-
size settings and, more importantly, how they would perform in
an online testing setup, increasingly common in practice. In this
paper, we report the results of an empirical study on the use of au-
tomatedtestcasegenerationmethodsforonlinetestingofRESTful
APIs.Specifically,weusedtheRESTestframeworktoautomatically
generate and execute test cases in 13 industrial APIs for 15 days
non-stop,resultinginoveronemilliontestcases.Toscaleatthis
level, we had to transition from a monolithic tool approach to a
multi-bot architecture with over 200 bots working cooperatively in
tasks like test generation and reporting. As a result, we uncovered
about390Kfailures,whichweconservativelytriagedinto254bugs,
65 of which have been acknowledged or fixed by developers to
date.Amongothers,weidentifiedconfirmedfaultsintheAPIsof
Amadeus,Foursquare,Yelp,andYouTube,accessedbymillionsof
applicationsworldwide.Moreimportantly,ourreportshaveguided
developersonimprovingtheirAPIs,includingbugfixesanddoc-
umentation updates in the APIs of Amadeus and YouTube. Our
results show the potential of online testing of RESTful APIs as
the next must-have feature in industry, but also some of the key
challenges to overcome for its full adoption inpractice.
CCS CONCEPTS
¬∑Informationsystems ‚ÜíRESTfulwebservices ;¬∑Software
andits engineering ‚ÜíSoftware testinganddebugging .
KEYWORDS
REST,webAPI, bot, black-box testing,onlinetesting
Permissionto make digitalor hard copies of allorpart ofthis work for personalor
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACM
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,
topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ESEC/FSE ‚Äô22, November 14≈õ18,2022, Singapore, Singapore
¬©2022 Associationfor Computing Machinery.
ACM ISBN 978-1-4503-9413-0/22/11...$15.00
https://doi.org/10.1145/3540250.3549144ACMReference Format:
AlbertoMartin-Lopez,SergioSegura,andAntonioRuiz-Cort√©s.2022.Online
Testing of RESTful APIs: Promises and Challenges. In Proceedings of the
30th ACM Joint European Software Engineering Conference and Symposium
on the Foundations of Software Engineering (ESEC/FSE ‚Äô22), November 14≈õ
18,2022,Singapore,Singapore. ACM,NewYork,NY,USA, 13pages.https:
//doi.org/10.1145/3540250.3549144
1 INTRODUCTION
WebAPIsallowsystemstointeractwitheachotheroverthenet-
work, typically using web services [ 84]. Most modern web APIs
comply with the REST architectural style [ 74], being referred to as
RESTfulwebAPIs.RESTfulwebAPIs[ 95]provideaccesstodata
andservicesbymeansofcreate,read,update,anddelete(CRUD)
operations over resources (e.g., a video in the YouTube API [ 50] or
aplaylistintheSpotifyAPI[ 44]).RESTfulAPIsareubiquitousin
themodern-daysociety:publicinstitutionssuchastheAmerican
government [ 3] expose their existing assets as a set of RESTful
APIs;softwarecompaniessuchasMicrosoft[ 29]andNetflix[ 30]
basemanyoftheirsystemscommunicationsontheirRESTfulAPIs;
evennon-softwarecompaniessuchasMarvel[ 28]provideAPIsfor
developerstobuildapplicationsontopofthem.Theimportanceand
pervasivenessofwebAPIsisalsoreflectedonthesizeofpopular
APIrepositoriessuchasProgrammableWeb[ 37]andRapidAPI[ 38],
whichcurrently index over 24Kand30KAPIs, respectively.
CheckingthecorrectfunctioningofwebAPIsiscritical.Asin-
gle bug in an API may affect tens or hundreds of other services
leveragingit.Inthisscenario,testthoroughnessandautomation
are of utmost importance. Industry standard tools and libraries for
testing RESTful APIs such as Postman [ 36] and REST Assured [ 41]
automatetestexecution,buttestcasesstillneedtobemanuallyim-
plemented.Industrialeffortsare alsoshifting towardthemodelof
onlinetesting,whereAPIsarecontinuouslytestedwhileinproduc-
tion.ThisisthecaseofplatformssuchasDatadog[ 22],RapidAPI
Testing [39], and Sauce Labs [ 43], which offer web API testing and
monitoringasaservice.Underthismodel,APIsaretestedandmon-
itored 24/7 with pre-defined API requests and output assertions
following a black-box approach. Customers may choose among
differentpricingplans[ 40]determiningthenumberofAPIcallsper
day/month, theintegration with CI/CDplatforms,and thetype of
notifications, among others. These platforms completely automate
testcaseexecution,buttheirgenerationstillrequiresmanualwork,
eitherfor the creation oramplification [ 71]of test cases.
Research on the automated generation of test cases for RESTful
APIshasseensignificantadvancesinrecentyears.Mostapproaches
follow a black-box strategy, where test cases are automatically
derivedfromthespecificationoftheAPIundertest,typicallyinthe
OpenAPI Specification (OAS) format [ 34]. These approaches are
408ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore Alberto Martin-Lopez,Sergio Segura, andAntonio Ruiz-Cort√©s
capable of automatically generating (sequences of) HTTP requests,
sendingthemtotheAPIandassertingthevalidityoftheresponses.
Test data generation strategies are varied [ 54,56,57,70,72,78,
85,86,90], while test oracles are mostly limited to checking the
conformance with the API specification and the absence of API
crashes. Despite their promising results, research approaches have
typically been evaluated in controlled environments with a few
open source APIs [ 55,68,87], a limited number of test cases [ 70,
72,90],orwithintheboundariesofasingleorganization[ 58,78].
Thus,thereisnoevidenceoftheirapplicabilityandgeneralizability
to industrial settings and, more importantly, of how they would
fit into the online testing model increasingly found in practice.
Hence,asamatteroffact,thereisacleargapbetweenindustrial
solutions, mostly concerned with test case execution, and research
approaches,focusedonautomatedtestcasegeneration,butwith
evaluation results limitedto lab settings.
In this paper, we report the results of an empirical study on the
useofautomatedtestcasegenerationmethodsforonlinetesting
of RESTful APIs. Specifically, we automatically generated and exe-
cuted test cases on 13 industrial APIs during the course of 15 days
non-stop. We assessed different black-box testing strategies includ-
ingfuzzing[ 77],adaptiverandomtesting[ 82],andconstraint-based
testing [79], among others, resulting in over one million test cases.
The evaluation was conducted using RESTest [ 91], an open source
testing framework for RESTful web APIs. However, to scale at this
level, we had to transition from a monolithic tool approach to a
multi-bot architecture, resulting in over 200 bots√êhighly cohesive
andautonomousprograms√êworkingcooperativelyintaskssuch
as test case generation, test case execution, and test case reporting.
We uncovered about 390K test failures, which we automatically
clusteredinto4,818potentialfaults.Afterasystematicselectionand
manualinspectionof586faultclusters,weconservativelyidentified
254 reproducible issues in all the APIs under test, 65 of which have
been acknowledged or fixed by the API developers to date [ 92].
Thebugsdetectedarevaried,includingAPIcrashescausedbothby
valid[10]andinvalidAPIinputs[ 9],unexpectedclienterrors[ 8,19]
andsecurityvulnerabilities [ 13], amongmanyothers.Ourbugre-
ports led to fixes in the documentation and the implementation of
theAPIsofYouTubeandAmadeus,usedbymillionsofusersworld-
wide. The results provide helpful insights on the fault detection
capabilityofonlinetestingofRESTfulAPIsaswellasitslimitations,
includingits high computational costand thedebugging effortre-
quired. We also report the differences observed among different
testing techniques, and how these techniques complement each
other for finding more and more varied bugs. Overall, our work
contributestonarrowingthegapbetweenresearchandpracticeon
online testing of RESTful APIs, showing the lights and shadows of
currenttest casegenerationtechniques at scale.
Insummary,afterdiscussingrelatedwork(Section 2)andpre-
sentingtheempiricalstudyperformed(Section 3),thispapermakes
the following originalresearchandengineeringcontributions:
(1)Anassessmentonthefailureandfaultdetectioncapability
ofonlinetestingofRESTfulAPIs (Sections 4.1and4.2).
(2)Acomparisonoftestdataandtestcasegenerationtechniques
for onlinetestingofRESTfulAPIs (Section 4.3).(3)Asetofproblemsuncoveredbyonlinetestingin13industrial
APIs, including254reproduciblebugs(Section 5).
(4)Alistofchallengesfortheadoptionoftestcasegeneration
techniques for onlinetestingof RESTfulAPIs (Section 6).
(5)A publicly available implementation of a multi-bot architec-
tureforonlinetestingofRESTfulAPIs,andadatasetofover
1Mtest casesand390K test failures [ 92].
Finally, we address threats to validity in Section 7and conclude
the paper inSection 8.
2 RELATED WORK
2.1 Automated TestingofRESTful APIs
Research on the automated generation of test cases for RESTful
APIshasthrivedinrecentyears.Mostapproachesfollowablack-
box approach, where test cases are automatically derived from the
API specification, typically in the OpenAPI Specification (OAS) for-
mat[34].AnOASdocumentrepresentsacontractonhowaRESTful
APImaybeused,i.e.,itdescribestheAPIintermsoftheallowedin-
puts(HTTPrequests)andoutputs(HTTPresponses),inamachine-
readableformat.GivenanOASdocument,currenttechniquesauto-
matically generate pseudo-random test cases√êsequences of HTTP
requests√êand test oracles√êassertions on the HTTP responses. Ap-
proaches mainly differ in the way in which they generate input
data and test cases. Regarding test data generation strategies, these
include using default and example values [ 72], fuzzing dictionar-
ies [57], purely random inputs [ 56], perturbed data [ 86], custom
datageneratorsand datadictionaries[ 85,91], dataextracted from
knowledge bases (e.g., DBpedia [ 64]) [54], and contextual data (i.e.,
extracted from previous API responses) [ 70,78]. Test case gener-
ationstrategiesincluderandomtesting(parametersareassigned
valuesrandomly)[ 57,70,72,85],adaptiverandomtesting(aimingto
diversify test cases as much as possible) [ 91], and constraint-based
testing(parameters areassignedvaluessuch thatinter-parameter
dependencies [ 88] are satisfied) [ 90]. In terms of test oracles, these
are mainly limited to checking the absence of API crashes (500 sta-
tuscodes)andtheconformancewiththespecification.Otheroracles
include checking thestatus code [ 90], metamorphicrelations[ 97]
andsecurity properties[ 58]. White-boxtechniques require access
tothecodeoftheAPIundertestandarefarlesscommonthanblack-
boxtechniques.Existingapproaches employsearch algorithmsto
maximizecode coverageandfaultdetection [ 55,96,99].
Related approaches on the automated generation of test cases
for RESTful APIs have beenmostly evaluatedin labsettings using
a few open source APIs [ 55,68,87], a limited number of testcases
(up to a few thousands) [ 70,72,90], during a limited amount of
time (up to a few hours) [ 57,70,78] or within the boundaries of
a singleorganization[ 58,78]. Incontrast,our workcomplements
previousworkbyassessingthestrengthsandlimitationsofdifferent
state-of-the-arttestcasegenerationtechniquesinanonlinetesting
scenario.Specifically,weautomaticallygeneratedandrantestcases
on13industrialAPIswithmillionsofusersworldwideduring15
daysnon-stop.Thisprovidesanovelperspectiveonthepotential
ofautomatedtestcasegenerationtechniquesforRESTfulAPIsin
practice: we uncovered 254 uniquebugs in 13 industrial APIs, 65
ofwhichhavebeenacknowledgedorfixedbydeveloperstodate,
and some of which have led to changes in the APIs of Amadeus
409Online Testingof RESTful APIs: Promises andChallenges ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore
andYouTube.Moreimportantly,ourworkopensnewpromising
researchdirections toaddresssome of the challenges identified in
areas such as debuggingandtestingas aservice.
2.2 Online Testing
Onlinetestingreferstothetestingperformedonproductionsoft-
ware [60,63], as opposed to offline testing, which is done on a
development environment, before the software is released. Online
testingcanberegardedbothasa passivemonitoringactivity[ 60,61]
or as anactivetesting process [ 65,80]. In the former, the system
is observed under real-world usage aiming to detect inconsisten-
cies or anomalies[ 59,83].In thelatter,thesystem iscontinuously
stimulated with new test cases, possibly derived from the feedback
obtainedfromprevioustestresults[ 65,100].Inthispaper,weadopt
theseconddefinitionandrefertoonlinetestingasanactivetesting
process consisting in generating and executing test cases (i.e., API
requests)inthe APIs inproduction.
Online testing platforms for web APIs are gaining popularity in
industry. These platforms provide continuous testing and monitor-
ingcapabilitiesasaservice,withdifferentpricingplansdetermining
featuressuchasthetestexecutionfrequency,theautomationdegree
and the number of users, among others. Datadog [ 22] is a monitor-
ingplatformfortrackinganAPI‚Äôsavailabilityandresponsetime
by periodically executing a set of manually configured API calls.
RapidAPI Testing [ 39] and Sauce Labs (formerly API Fortress) [ 43]
allow to create functional tests composed of one or more HTTP
requests with custom assertions intheHTTP responses. The data
used in the requests can be static, contextual (i.e., obtained from
a previous response) or random, based on a data category (e.g.,
≈Çyellow≈æforthecategory≈Çcolor≈æ).However,testcasesmustbeindi-
viduallycreated(RapidAPI)oramplifiedbasedonatemplate(Sauce
Labs). The same test cases are executed continuously at regular
intervals.Intheresearcharena,somepapersaddressonlinetesting
ofRESTfulAPIs(i.e.,testingwebAPIsinproduction)[ 70,72,78,86],
but with a limited number of test cases or during a few hours, and
typicallyusingasingletestingtechnique.Onlinetestinghasalso
been applied to service-oriented architectures (SOA) [ 65,67,80],
including non-functional testing [ 62,94], and other less related
domains such as mobile applications [ 66], embedded systems [ 53]
andsoftware product lines[ 81], among others.
Compared to prior work, we report the first empirical study on
theuseofautomatedtestcasegenerationtechniquesforonlinetest-
ing ofRESTful APIs inindustrial-likesettings.Thisresembles the
modeloftestingasaserviceofferedbypopularindustrialplatforms.
Thisallowedustostudythefailureandfaultdetectioncapability
ofdifferentblack-boxtestcasegenerationtechniquesovertime(15
days),aswellassomeofthekeychallengestoovercomefortheir
adoption inpractice.
3 EMPIRICALSTUDY
Inthissection,wereporttheresultsofourstudyononlinetestingof
RESTful APIs, including both automated generation and execution
oftest cases.
3.1 Research Questions
We aim to answer the following researchquestions:Table 1: RESTful APIs under test. O = Operations, P = Param-
eters,R =Read, W=Write.
API Category Serviceundertest O P R W
Amadeus[ 2] Travel Hotelsearch 1 27 X
DHL [23] Shipping Service point search 1 9 X
FDIC [24] Banking all 6 73 X
Foursquare[ 26] Social, mapping Venuessearch 1 17 X
LanguageTool [ 27] Languages Textproofread 1 11 X
Marvel[28] Entertainment Characters 6 72 X
Ohsome[ 31] Mapping all 122 1,146 X
OMDb [32] Media all 1 9 X
RESTcountries [ 42] Reference all 21 42 X
Spotify [44] Media Playlists 12 48 X X
Stripe[45] Financial Products 5 47 X X
Yelp [48] Recommendations Businesses search 1 14 X
YouTube[ 49] Media Comments 10 148 X X
YouTube[ 51] Media Search 1 31 X
Total 189 1,694 13 4
RQ1:What is the failure detection capability of online testing
of RESTful APIs? We aim to investigate the number and types of
failures detectedover time.
RQ2:What is the fault detection capability of online testing of
RESTful APIs? Out of all the failures uncovered, we aim to find out
the number andtypes of uniquefaultsdetectedover time.
RQ3:Whatarethedifferencesobservedbetweendifferenttesting
approaches? We wish to compare different testing strategies in
terms of API coverage and fault detection capability, aiming to
understandhowdifferentapproachescomplementeachother.
3.2 APIs UnderTest
Table1depicts the RESTful APIs under test. For each API, we spec-
ifyitsname,theservicetested( ≈Çall≈æifthewholeAPIistested),the
number of operations and parameters tested (columns ≈ÇO≈æ and ≈ÇP≈æ,
respectively), and whether it provides read and write operations
(columns≈ÇR≈æand≈ÇW≈æ,respectively).Wetestthe SearchandCom-
mentsservicesofYouTubeasdifferentAPIs,asdonebyprevious
authors [ 90,93], since they are subject to different testing strate-
gies (e.g., the former is a read-only API while the latter provides
write functionality). Our benchmark comprises large and complex
real-worldAPIstestedbypreviousauthors[ 54,87,90,93],which
includes both commercial APIs used by millions of users world-
wide (e.g., Spotify and Yelp), but also industrial-size open source
APIs (e.g., Ohsome1and LanguageTool). We add another API to
our benchmark, FDIC (Federal Deposit Insurance Corporation), as
a good representative of complex APIs developed by public institu-
tions(i.e.,theAmericangovernment).TheOASspecificationsofthe
APIs (required to test them) were obtained from the APIs‚Äô websites
or from public repositories such as APIs.guru [ 4]. Exceptionally,
Foursquare,RESTCountriesandYelpdidnothavepublicOASdocu-
ments,sowereusedthemfromourpreviouswork[ 87,90],ensuring
they accurately reflectedthe API documentations.
Overall, we selected 13 APIs from varied application domains,
whichcomprehend189operationsand1,694inputparameters.Most
of the APIs tested provide only read operations. This is a common
1Ohsome developers showed interest in our preliminary results and deployed an API
instance identical to the production one where we could run our experiments without
restrictions.
410ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore Alberto Martin-Lopez,Sergio Segura, andAntonio Ruiz-Cort√©s
Table 2:Taxonomyoftestbotsandtotal bots used pertype,according to theinclusion criteriaforour experiments.
Botdimension BotID Botname Description Inclusioncriterion Total
Write-safetyR Read-only Testsonlyread operations (i.e.,HTTPGETrequests) The APIcontainsread operations 55
RW Read/write Testsbothread and writeoperations (e.g.,POST and DELETE requests) The APIcontainswriteoperations 20
Testdata
generation
techniqueFD Fuzzing dictionaries Uses type-awarefuzzing dictionariesand malformed strings for all request parameters AllAPIs 14
CDG Customized datageneratorsParametersareconfigured with customized datagenerators(e.g.,ageneratorofdates
in‚Äòyyyy/mm/dd‚Äô format)AllAPIs 32
DP Dataperturbation Makesaminor change tothe dataused inan APIrequest tomake it invalid AllAPIs 14
SD Semantically-related dataUses conceptsrelated tothe semantics ofthe parameters,extracted from knowledge
bases like DBpedia[ 64]The APIcontainsparametersfrom which to
extractsemantically-related concepts5
CD Contextualdata Uses values observed inprevious APIresponsesThe APIresponsesinclude reusable datafor
subsequentrequests10
Testcase
generation
techniqueRT Randomtesting Parametersareassigned values randomly AllAPIs 39
CBT Constraint-based testing Constraintsolvers areused tosatisfyall inter-parameter dependencies ofthe API[ 88] The APIcontainsinter-parameter dependencies 27
ART‚òÖAdaptive randomtesting Similar toRT or CBT,butaimingtogenerate asdiversetest cases aspossible[ 82]The APIcontainsat leastone operation with
morethan10parameters9
‚òÖART bots employRT or CBT(depending onwhether the APIhas inter-parameter dependencies [ 88]or not)asabasis togenerate test casecandidates, from which the most diversetestcases areselected [ 82].
pattern in commercial APIs, where API users can only query ex-
isting data (e.g., characters from Marvel comics), and this data can
onlybemodifiedbytheAPIproviders.However,wealsotestser-
viceswhosemainfunctionalityisinvokedviawriteoperations(e.g.,
proofreadingatextinLanguageTool),andevenserviceswhichre-
quirestatefulinteractions(e.g.,inSpotify,creatingaplaylist,adding
songsto it,anddeleting it).
3.3 ExperimentalSetup
Inwhatfollows,wedescribethetoolingusedandhowitwasconfig-
ured, and we explain several considerations to answer the research
questions.Alltheexperimentswereperformedinasingle virtual
machine equipped with 512GB RAM, 32 CPU cores and 2TB HDD
running RedHatEnterprise Linux 8, Java 8andNodeJS 16.
3.3.1 Testing Ecosystem. For the generation and execution of test
cases we used RESTest [ 91], anopen source framework integrating
varied test case generation strategies, a key requirement for our
study. After somepreliminaryexperiments,we soon realizedthat
themonolithicapproachofthetool√êcombiningtestcasegenera-
tion,executionandreportinginthesameprocess√êwouldhinder
the development and deployment of the application at the desired
scale.InspiredbytheWESplatformofFacebook[ 52],weconceived
a novel architecture decoupling the key parts of the testing pro-
cess into highly cohesive and autonomous programs called bots,
illustrated in Figure 1. Bots can be independently developed and
deployed using different technologies. We distinguish two types
of bots: input and output bots. Input bots support the generation
and execution of test cases. Output bots are responsible for analyz-
ingandleveragingthetestoutputs.Botsarestarted,stoppedand
monitored automaticallybya controller component,and they can
optionallyinteractwitheachother,e.g.,bytriggeringtheupdate
oftest reports.
We devise two types of input bots: test bots, which generate and
executetestcases,and garbagecollectors ,whichdeleteresourcescre-
ated by test bots. Garbage collectors are implemented in JavaScript
and testbotsareimplemented usingRESTest. Asillustrated inTa-
ble2, test bots are classified along three dimensions, namely: (1)
write-safety , which concerns the types of operations invoked by
the bot; (2) test data generation technique , which refers to the input
Figure 1:Testing ecosystem architecture.
data usedin the APIrequests; and (3) test case generation technique ,
whichrelatestohowAPIrequests(i.e.,testcases)arebuilt.Testbots
arecreatedbycombiningthesethreedimensionsinanyway.As
an example, R-FD-RT represents a read-only (R) bot that performs
fuzzing [57], i.e.,random testing (RT) with fuzzing dictionaries (FD).
In total, we have 2 (write-safety) √ó5 (test data generation tech-
nique)√ó3(testcasegenerationtechnique) +1(garbagecollector)
=31types ofinputbots. Regardingoutputbots,weusedboth test
reporters(TR),whichgenerategraphicaltestreportswiththeAllure
test reporting framework [ 1], andtest coverage computers (TCC),
whichcomputethetestcoverageachievedbytestbotsaccording
to standardcoveragecriteriafor RESTfulwebAPIs [ 89].
3.3.2 Bots Selection Criteria. We selected a subset of bots for test-
ing each API based on their potential applicability, for example,
read/writebotsareonly applicabletoAPIswithwriteoperations.
Table2illustrates the inclusion criteria for selecting test bots in
theAPIsundertest,andhowmanybotsofeachtypewereselected.
According to these criteria, we selected 75 test bots, most of which
areread-only (55 bots), use customized data generators (32), and ap-
plyrandom testing (39),whereas read/write bots(20),botsthatuse
semantically-relateddata (5),andbotsthatapply adaptiverandom
testing(9) are the least common. We may remark that we do not
aim at doing a rigorous comparison of testing techniques. Instead,
weaimtoassesshowdifferenttestgenerationstrategiesperform
inanonlinescenarioandhowtheycomplementeachother.Table 4
shows the full list of test bots used in our experiment. Besides test
bots, we configured three garbage collectors for the APIs of Spo-
tify, Stripe and YouTube, and one test reporter and test coverage
411Online Testingof RESTful APIs: Promises andChallenges ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore
computerpertestbot.AsillustratedinFigure 1,weconfigureda
total of 75 (test bots) +3 (garbage collectors) +75 (test reporters) +
75 (testcoveragecomputers) =228bots.
3.3.3 BotsConfiguration. Alltestbotswereconfiguredtocontinu-
ouslygenerateandexecutetestcasesduring15days(378hours).
Bots were configured to comply with the quotas imposed by the
APIs. Quotas describe the limitations of use for a fixed period of
time[76](e.g.,5Krequests/dayinYelp[ 48]).Torespectquotas,test
botsworkbyiterations.Ineveryiteration,theygenerateasetof
test cases, execute them, and sleep for a given time. We configured
alltestbotsofthesameAPItogeneratethesamenumberofAPI
calls periteration, however, the sleep time between iterations was
adjustedaccordingtothetestdatagenerationtechniqueusedby
the bot. FD (fuzzing dictionaries) and DP(data perturbation) test
bots sleep four times longer than the rest, therefore they generate
fourtimeslessAPIcalls.Thisisbecausethesebotsarenotexpected
togeneratevalidtestcasesthatachieveahighAPIcoverage,and
so theyare assigned a lowertest budget[ 54,68]. The specific con-
figurationusedforeachbot(i.e.,testcasesperiterationandsleep
time) isavailable inthe supplementary materialofthe paper [ 92].
Test reporters and test coverage computers are invoked after
every test bot iteration, continuously updating test reports and
coverage.Garbagecollectorsareexecutedaftertestbotsarestopped,
thus deleting allresourcescreatedduringthe experiments.
3.3.4 Test Oracles. To detect failures, we consider six test oracles,
namely:(1)5XXstatuscodes(servererrors)returnedwhensend-
ingavalidrequest( F_5XXV);(2)5XXstatuscodesreturned when
sending an invalid request ( F_5XXI); (3) 2XX status codes (success-
fulresponses)withrequestsviolatingsomeparameterconstraint
(F_2XXP); (4) 2XX statuscodeswith requests violating some inter-
parameterdependency[ 88](F_2XXD);(5)400statuscodes(client
errors)withvalidrequests( F_400);and(6)disconformitieswiththe
OAS specification of the API ( F_OAS).F_5XXVandF_5XXIfailures
are due to server errors, and so they can reveal API crashes (500
statuscodes)ortemporaryoutages(503statuscodes),amongoth-
ers.F_2XXPandF_2XXDfailures reveal inconsistencies in the form
of invalid API inputs (which expect a 4XX client error response)
wrongly handled as valid (since they obtain a 2XX successful re-
sponse). For instance, if a required parameter is not included in an
API call, it should notobtain a 2XX status code ( F_2XXP). Likewise,
iftwomutuallyexclusiveparametersarebothincludedinanAPI
call,itshould notobtaina2XXstatuscode( F_2XXD).F_400failures
revealunexpectedclienterrors(400statuscode)withAPIcallsthat
arevalid,thereforetheyshouldobtaina2XXsuccessfulresponse.
Lastly,F_OASfailuresrevealconformanceerrors,forexample,an
API response not including a property defined as ≈Çrequired≈æ in the
API specification.
Itis worthclarifying that not all testoraclesareavailabletoall
test bots, and therefore different bots uncover different types of
failures. In particular, disconformities with the API specification
(F_OAS) and 5XX status codes ( F_5XXVandF_5XXI) can be uncov-
ered by any bot. On the other hand, to detect an unexpected client
error(F_400),theAPIrequestmustbevalid,i.e.,itmustusevalid
input data.OnlyCDGbots canuncover this typeoffailures, since
theyalwaysusevaliddataandthereforeexpectsuccessfulAPIre-
sponses. While SD and CD bots (i.e., semantic and contextual databots) also have potential to uncover these faults, they are reported
as ≈Çwarnings≈æ, since they may use invalid data unintentionally, for
instance, when extracting country names instead of country codes
from a knowledge base [ 54]. Regarding F_2XXPandF_2XXDfaults,
they can only be detected by DP bots, since they purposely manip-
ulatetheinputdatausedtomakeitinvalid(byviolatingparameter
constraints or inter-parameter dependencies), so they expect client
errorresponses.
3.3.5 Evaluation Metrics. We used the following metrics: (1) total
numberoffailuresandfaultsdetected,overallandclassifiedbytype
(i.e.,accordingtotheoracleviolated);(2)failureandfaultdetection
ratio (FDR and FtDR, respectively); (3) average percentage of faults
detected(APFD);and(4)APItestcoverage.TheAPFDmeasuresthe
weighted average ofthepercentageof faults detected over thelife
ofthetestsuite,anditisagoodrepresentativeofhowfastfaultsare
found[73].TheAPItestcoverageismeasuredaccordingtostandard
black-boxcoveragecriteriaforRESTfulwebAPIs[ 89],implemented
intoolslikeRESTest[ 91],Restats[ 69]andOpenAPIspecification
coverage [ 35], and previously used to compare black-box testing
techniques[ 54,68].Thesecriteriameasurethedegreetowhichaset
ofAPIrequestsandresponsescovertheelementsdefinedintheAPI
specification.APIrequestscoverinputelementssuchasoperations
andparametervalues, whileAPIresponsescoveroutputelements
such as status codes and response body properties. As a difference
compared to the original test coverage approach [ 89], we consider
input criteria to be covered onlyif the API request covering the
elementsobtainedasuccessfulresponse,i.e.,theAPIrequestwas
valid. For instance,if an API request obtains a 400 status code, we
considersuch codeas ≈Çcovered≈æ,buttheparameter values used in
the request are ≈Çnot covered≈æ, since theywere rejected by theAPI
(clienterror).WefollowthisapproachbecauseinvalidAPIrequests
do not generallyexercise the core functionalityof the API.
3.3.6 ClassifyingFailuresintoFaults. Wefollowedatwo-stepsemi-
automated approach to classify failures into unique faults: first,
failures were automatically grouped into fault clusters; then, fault
clusterswereeitherconfirmed(iftheyrepresenteduniquefaults)or
discarded, manually. A fault cluster represents a unique potentially
incorrect behavior in the API (e.g., a server error when setting a
parameterwithcertainvalue),anditcomprehendsallthefailures
revealing that issue. We consider that two failures belong to the
samefaultcluster,andthereforetheyarecausedbythesamepoten-
tialfault,if:(1)theyviolatethesametestoracle,(2)theyoccurin
thesameAPIoperation(e.g., GET /search ),(3)theyhavethesame
statuscode(e.g.,404)andcontent-type(e.g., application/json ),
and (4) they are similar enough. Similarity between failures was
measured using the normalized Levenshtein distance (NLD) [ 102]
to compare HTTP requests, HTTP responses and error logs. If the
distance is lower than certain threshold, failures are considered
similar.Theonlyexceptionwas F_OASfailures,whoseerrorlogs
arecomposedofstructurederrormessagescontainingdynamicval-
ues (see example in Figure 2). These failures are considered similar
if, after replacing dynamic values with wild cards and removing
duplicate error messages, the resulting sanitized error logs are the
same.Table 3summarizes the heuristics andthresholds used.
412ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore Alberto Martin-Lopez,Sergio Segura, andAntonio Ruiz-Cort√©s
Figure2: F_OASerrorlog.Redboxesdepicterrormessages.
Yellow highlighting depicts dynamic values.
Table 3:Heuristics to determinesimilar failures.
Type Failures ùëì1andùëì2aresimilarif:
F_5XXVùëÅùêøùê∑(ùëüùëíùë†ùëùùëúùëõùë†ùëí ùëì1,ùëüùëíùë†ùëùùëúùëõùë†ùëí ùëì2)<0.5
F_5XXIùëÅùêøùê∑(ùëüùëíùë†ùëùùëúùëõùë†ùëí ùëì1,ùëüùëíùë†ùëùùëúùëõùë†ùëí ùëì2)<0.5 &ùëÅùêøùê∑(ùëíùëüùëüùëúùëü_ùëôùëúùëîùëì1,ùëíùëüùëüùëúùëü_ùëôùëúùëîùëì2)<0.5
F_2XXPùëÅùêøùê∑(ùëíùëüùëüùëúùëü_ùëôùëúùëîùëì1,ùëíùëüùëüùëúùëü_ùëôùëúùëîùëì2)<0.5
F_2XXDùëÅùêøùê∑(ùëüùëíùëûùë¢ùëíùë†ùë° ùëì1,ùëüùëíùëûùë¢ùëíùë†ùë° ùëì2)<0.1
F_400 ùëÅùêøùê∑(ùëüùëíùë†ùëùùëúùëõùë†ùëí ùëì1,ùëüùëíùë†ùëùùëúùëõùë†ùëí ùëì2)<0.5 &ùëÅùêøùê∑(ùëüùëíùëûùë¢ùëíùë†ùë° ùëì1,ùëüùëíùëûùë¢ùëíùë†ùë° ùëì2)<0.25
F_OAS ùë¢ùëõùëñùëûùë¢ùëí(ùë§ùëñùëôùëêùëéùëüùëë(ùëíùëüùëüùëúùëü_ùëôùëúùëîùëì1))==ùë¢ùëõùëñùëûùë¢ùëí(ùë§ùëñùëôùëêùëéùëüùëë(ùëíùëüùëüùëúùëü_ùëôùëúùëîùëì2))
4 RESULTS
Next,wedescribetheresultsofourstudyandhowtheyanswerthe
target researchquestions.
4.1 RQ 1:FailureDetectionCapability
Table4shows the number of test cases, API coverage, failures and
faults obtained in every API under test and by each test bot. As
illustrated, a total of 389,216 failures were uncovered, i.e., 1 every 3
test cases. The majority of failures (243,394) consist in disconfor-
mities of the responses with the API specification ( F_OAS). In fact,
this is one of the main conclusions derived from our study: API
specificationspoorlyreflecttheactualAPIimplementation .Wefound
inconsistencies in all APIs under test except in RESTCountries, but
this is because the documentation of this API did not explicitly
statetheformatoftheresponses,hencenoinconsistenciescould
befound.Disconformitiesincludestatuscodesandcontent-types
not listed in the specification, as well as violations in the format
ofthe response bodies, e.g., anobjectmissing a required property.
SystematicviolationsoftheAPIcontractsmeanastronglimitation
to client applications, which could crash if they run into a scenario
not described (or even forbidden) in the API specification (e.g., a
non-nullablepropertybeing null).BesidesOASdisconformities,
weuncoveredthousandsofotherfailuresrelatedtoservererrors
(5XX status codes), client errors (400 status codes) and wrongly
returnedsuccessfulresponses (2XX statuscodes).
We studied the FDR for all APIs under test. Overall, the FDR
ranged from 0.04 (9,141 failures out of 258,297 test cases) in the
APIofRESTCountriesto0.79(31,811outof40,480)intheAPIof
LanguageTool. We also studied the evolution of the FDR over time.
This allowed us to detect different tendencies and anomalies in
severalAPIs.Figure 3depictstheFDRoverthetestsuitefractionin
fiveAPIs:Amadeus,DHL,Ohsome,Spotify,andYelp.Theremaining
APIsshowasimilartrendtotheoneobservedinSpotify,andare
not includedforthe sake ofclarity (chartsfor allAPIsare available
in the supplementary material [ 92]). Spotify represents a common
scenario, where the FDR remains constant, i.e., the same failures
are uncovered all thetime.In Amadeus, theFDRslightly declines,
meaning that test cases fail less often over time. We suspect this
Figure 3:FDR inSpotify,Amadeus, DHL,Ohsomeand Yelp.
happensbecauseAPIdevelopersarecontinuouslyfixingissues.The
APIs of DHL, Ohsome and Yelp are clear anomalies. In DHL, all
testcasesstartedtofailat60%oftheoveralltestbudget,because
we were banned from the API,even thoughwe always complied
with the imposed quota. In Ohsome, a similar phenomenon was
observed, but the failures were due to 500 status codes, all with
thesameerrormessage: ‚ÄúThe timerange metadata could not
be retrieved from the db‚Äù . In Yelp, the FDR spiked four times
because the API only returned 500 status codes, likely due to some
non-functionalbugcausing temporary outages.
Summary ofanswers to RQ1
(1)Automaticallygeneratedtestcasesuncoveredabout390Kfailures
(1 out ofevery 3test cases)in the APIs undertest.
(2)Failures were revealed in all APIs, with the FDR ranging from 0.04
to 0.79.
(3)63%offailuresareduetoinconsistenciesbetweentheAPIresponses
andtheirspecifications.
(4)The FDR evolution revealed anomalies√êtemporary outages and
APIbans√êin DHL, Ohsome andYelp.
4.2 RQ 2:FaultDetectionCapability
Our fault clustering approach yielded 4,818 potential faults (i.e.,
faultclusters)fromthe389,216testfailuresrevealed.FDIC[ 24]and
Ohsome[ 31]weretheAPIsforwhichmostpotentialfaultswere
detected, 2,871 and 1,445, respectively. We discarded these APIs
when assessing the faultdetection capability of our onlinetesting
setup, as we could not manually analyze all these fault clusters.
We did analyze a subset of them, nonetheless, and we identified
numerous bugs (more details in Section 5). Without considering
FDICandOhsome,atotalof218,419testfailureswereautomatically
clustered into 502 potential faults which, after manual revision,
resulted in 139 reproducible (or already fixed) issues in all the APIs
under test.Besides the issues extracted from the faultclusters, we
spotted8additionalissuesfromthereportsgeneratedbythetest
reporter bots (available at [ 33]).
Overall, we uncovered a total of 147 faults in 12 APIs. Half of
thesefaults(73)aredisconformitiesbetweentheAPIresponsesand
theirspecifications( F_OAS),whichiscoherentwiththefindings
413Online Testingof RESTful APIs: Promises andChallenges ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore
Table 4: Per-bot breakdown of test cases, coverage, failures and faults. Last column depicts unique faults, i.e., only uncovered
by that bot.Rowsinboldface depictbots uncoveringthemostfaults.Rowshighlighted ingray depictthe total stats perAPI.
API Bot Test cases Coverage (%)Failures/Faults Unique
F_5XXVF_5XXIF_400 F_2XX PF_2XXDF_OAS Total faults
Amadeus R-CDG-ART 384 78.77 0/0 0/0 137/7 0/0 0/0 112/5 249/12 3
Amadeus R-CDG-CBT 384 76.42 0/0 0/0 203/7 0/0 0/0 53/2 256/9 0
Amadeus R-DP-CBT 384 76.42 0/0 0/0 0/0 37/3 32/0 0/0 69/3 3
Amadeus R-SD-CBT 384 74.53 0/0 0/0 0/0 0/0 0/0 13/2 13/2 0
Amadeus R-CD-CBT 384 72.17 0/0 0/0 0/0 0/0 0/0 5/2 5/2 0
Amadeus R-FD-RT 408 3.3 0/0 0/0 0/0 0/0 0/0 48/1 48/1 1
Amadeus R-CDG-RT 384 76.42 0/0 0/0 0/0 0/0 0/0 12/2 12/2 0
Amadeus All bots 2,712 - 0/0 0/0 340/7 37/3 32/0 243/6 652/16 -
DHL R-FD-RT 2,280 13.7 0/0 0/0 0/0 0/0 0/0 913/2 913/2 0
DHL R-CDG-RT 8,976 91.78 1/0 0/0 90/3 0/0 0/0 3,678/1 3,769/4 3
DHL R-DP-RT 2,280 97.26 0/0 15/1 0/0 516/3 0/0 912/2 1,443/6 4
DHL R-SD-RT 8,952 91.78 2/0 0/0 0/0 0/0 0/0 3,587/1 3,589/1 0
DHL R-CD-RT 8,951 87.67 0/0 0/0 0/0 0/0 0/0 6,280/2 6,280/2 1
DHL All bots 31,439 - 3/0 15/1 90/3 516/3 0/015,370/3 15,994/10 -
FDIC R-CDG-ART 12,744 85.54 190/- 0/- 2,141/- 0/- 0/- 10,113/- 12,444/- -
FDIC R-FD-RT 3,456 85.54 0/- 139/- 0/- 0/- 0/- 140/- 279/- -
FDIC R-CDG-RT 12,240 85.54 188/- 0/- 1,956/- 0/- 0/- 9,125/- 11,269/- -
FDIC R-DP-RT 3,456 85.54 0/- 252/- 0/- 409/- 0/- 0/- 661/- -
FDIC All bots 31,896 -378/- 391/- 4,097/- 409/- 0/-19,378/- 24,653/- -
Foursquare R-CDG-ART 12,240 65.79 7/0 0/0 2/0 0/0 0/0 12,024/3 12,033/3 0
Foursquare R-CDG-CBT 22,200 65.79 4/0 0/0 23/0 0/0 0/0 16,229/3 16,256/3 0
Foursquare R-DP-CBT 5,700 68.42 0/0 3/0 0/0 617/2 1,905/3 2,528/2 5,053/7 6
Foursquare R-FD-RT 5,700 63.16 0/0 1/0 0/0 0/0 0/0 1,516/2 1,517/2 0
Foursquare R-CDG-RT 22,200 65.79 0/0 4/0 0/0 0/0 0/0 15,108/3 15,112/3 0
Foursquare All bots 68,040 -11/0 8/0 25/0 617/2 1,905/3 47,405/4 49,971/9 -
LanguageTool RW-CDG-ART 6,680 47.95 0/0 0/0 1,177/1 0/0 0/0 5,503/5 6,680/6 0
LanguageTool RW-CDG-CBT 7,500 47.95 0/0 0/0 1,018/1 0/0 0/0 6,482/5 7,500/6 0
LanguageTool RW-DP-CBT 1,900 45.21 0/0 0/0 0/0 82/1 0/0 1,818/1 1,900/2 1
LanguageTool RW-SD-CBT 7,500 45.21 0/0 0/0 0/0 0/0 0/0 3,126/4 3,126/4 0
LanguageTool RW-CD-CBT 7,500 53.42 2/0 0/0 0/0 0/0 0/0 3,203/4 3,205/4 0
LanguageTool RW-FD-RT 1,900 0 0/0 0/0 0/0 0/0 0/0 1,900/1 1,900/1 0
LanguageTool RW-CDG-RT 7,500 47.95 0/0 2/0 0/0 0/0 0/0 7,498/5 7,500/5 0
LanguageTool All bots 40,480 - 2/0 2/02,195/1 82/1 0/029,530/6 31,811/8 -
Marvel R-CDG-ART 14,273 95.45 3/0 0/0 0/0 0/0 0/0 7,233/14 7,236/14 0
Marvel R-FD-RT 3,744 23.86 0/0 3/1 0/0 0/0 0/0 590/12 593/13 11
Marvel R-CDG-RT 14,507 95.45 4/0 0/0 0/0 0/0 0/0 5,726/14 5,730/14 0
Marvel R-DP-RT 3,744 88.64 0/0 41/1 0/0 1,332/10 0/0 0/0 1,373/11 11
Marvel R-CD-RT 14,508 96.02 2/0 0/0 0/0 0/0 0/0 7,408/16 7,410/16 1
Marvel All bots 50,776 - 9/0 44/2 0/01,332/10 0/020,957/26 22,342/38 -
Ohsome R-CDG-CBT 73,198 80.55 8,477/- 0/- 19,389/- 0/- 0/- 34,855/- 62,721/- -
Ohsome R-DP-CBT 56,120 21.45 0/- 13,100/- 0/- 283/- 6/- 0/- 13,389/- -
Ohsome R-FD-RT 56,730 9.98 0/- 12,815/- 0/- 0/- 0/- 0/- 12,815/- -
Ohsome R-CDG-RT 178,120 80.23 0/- 46,209/- 0/- 0/- 0/- 11,010/- 57,219/- -
Ohsome All bots 364,168 -8,477/- 72,124/- 19,389/- 283/- 6/-45,865/- 146,144/- -
OMDb R-CDG-CBT 13,388 78.26 0/0 0/0 0/0 0/0 0/0 725/1 725/1 0
OMDb R-DP-CBT 3,420 78.26 0/0 36/1 0/0 1,674/2 1,710/1 0/0 3,420/4 3
OMDb R-SD-CBT 11,518 78.26 0/0 0/0 0/0 0/0 0/0 149/1 149/1 0
OMDb R-CD-CBT 13,388 78.26 0/0 0/0 0/0 0/0 0/0 700/2 700/2 0
OMDb R-FD-RT 3,420 86.96 0/0 552/1 0/0 0/0 0/0 261/3 813/4 1
OMDb R-CDG-RT 13,391 78.26 1/0 0/0 0/0 0/0 0/0 668/1 669/1 0
OMDb All bots 58,525 - 1/0 588/1 0/01,674/2 1,710/1 2,503/3 6,476/7 -
RESTCountries R-FD-RT 19,739 48.34 0/0 0/0 0/0 0/0 0/0 0/0 0/0 0
RESTCountries R-CDG-RT 72,869 82.12 224/0 0/0 0/0 0/0 0/0 0/0 224/0 0
RESTCountries R-DP-RT 19,740 49.01 1/0 2/0 0/0 8,478/11 0/0 0/0 8,481/11 11
RESTCountries R-SD-RT 73,290 77.48 218/0 0/0 0/0 0/0 0/0 0/0 218/0 0
RESTCountries R-CD-RT 72,659 70.2 218/0 0/0 0/0 0/0 0/0 0/0 218/0 0
RESTCountries All bots 258,297 -661/0 2/0 0/08,478/11 0/0 0/0 9,141/11 -
Spotify R-CDG-RT 22,139 27.43 1/0 0/0 0/0 0/0 0/0 16,832/8 16,833/8 0
Spotify RW-FD-RT 5,700 17.4 0/0 0/0 0/0 0/0 0/0 662/1 662/1 0
Spotify RW-DP-RT 5,700 20.65 0/0 0/0 0/0 58/2 0/0 475/0 533/2 2
Spotify RW-CD-RT 22,140 72.27 26/1 0/0 0/0 0/0 0/0 11,863/13 11,889/14 6
Spotify All bots 55,679 -27/1 0/0 0/0 58/2 0/029,832/13 29,917/16 -
Stripe R-CDG-CBT 11,160 17.24 0/0 0/0 0/0 0/0 0/0 773/1 773/1 0
Stripe RW-CDG-ART 10,737 25.52 0/0 0/0 9/0 0/0 0/0 10,728/1 10,737/1 0
Stripe RW-CDG-CBT 11,158 25.52 0/0 0/0 16/0 0/0 0/0 11,142/1 11,158/1 0
Stripe RW-DP-CBT 2,850 17.24 0/0 0/0 0/0 0/0 4/1 0/0 4/1 1
Stripe RW-CD-CBT 11,098 91.03 0/0 0/0 0/0 0/0 0/0 2,616/4 2,616/4 2
Stripe RW-FD-RT 2,850 3.45 0/0 0/0 0/0 0/0 0/0 0/0 0/0 0
Stripe RW-CDG-RT 11,158 25.52 0/0 0/0 0/0 0/0 0/0 1,432/1 1,432/1 0
Stripe All bots 61,011 - 0/0 0/0 25/0 0/0 4/126,691/4 26,720/5 -
Yelp R-CDG-ART 11,340 100 2,088/0 0/0 1,226/1 0/0 0/0 1,875/6 5,189/7 1
Yelp R-CDG-CBT 22,140 100 8,110/0 0/0 2,344/1 0/0 0/0 1,554/5 12,008/6 0
Yelp R-DP-CBT 5,700 100 0/0 456/0 0/0 76/1 221/1 154/1 907/3 3
Yelp R-FD-RT 5,700 54.55 0/0 1/1 0/0 0/0 0/0 0/0 1/1 0
Yelp R-CDG-RT 22,200 100 0/0 3,988/0 0/0 0/0 0/0 1,299/3 5,287/3 0
Yelp All bots 67,080 -10,198/0 4,445/1 3,570/1 76/1 221/1 4,882/6 23,392/10 -
YouTubeComments R-CDG-CBT 1,152 11.84 0/0 0/0 128/1 0/0 0/0 0/0 128/1 1
YouTubeComments R-CDG-RT 1,152 11.84 0/0 0/0 0/0 0/0 0/0 0/0 0/0 0
YouTubeComments RW-CD-ART 1,152 49.05 0/0 0/0 0/0 0/0 0/0 46/1 46/1 0
YouTubeComments RW-DP-CBT 288 0 0/0 0/0 0/0 27/7 2/1 0/0 29/8 8
YouTubeComments RW-CD-CBT 1,152 47.57 0/0 0/0 0/0 0/0 0/0 18/1 18/1 0
YouTubeComments RW-FD-RT 288 0 0/0 0/0 0/0 0/0 0/0 0/0 0/0 0
YouTubeComments All bots 5,184 - 0/0 0/0 128/1 27/7 2/1 64/1 221/10 -
YouTubeSearch R-CDG-ART 1,840 87.5 0/0 0/0 456/3 0/0 0/0 417/1 873/4 0
YouTubeSearch R-CDG-CBT 1,885 87.5 0/0 0/0 486/3 0/0 0/0 230/1 716/4 0
YouTubeSearch R-DP-CBT 475 85.58 0/0 0/0 0/0 165/2 1/1 0/0 166/3 3
YouTubeSearch R-FD-RT 474 2.88 0/0 0/0 0/0 0/0 0/0 0/0 0/0 0
YouTubeSearch R-CDG-RT 1,885 86.54 0/0 0/0 0/0 0/0 0/0 27/1 27/1 0
YouTubeSearch All bots 6,559 - 0/0 0/0 942/3 165/2 1/1 674/1 1,782/7 -
Total All APIs 1,101,846 - 19,767/1 77,619/5 30,801/16 13,754/44 3,881/8 243,394/73 389,216/147 -
414ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore Alberto Martin-Lopez,Sergio Segura, andAntonio Ruiz-Cort√©s
obtainedintermsoffailuredetectioncapability(RQ 1).Aboutone
third of the faults (52) are caused by invalid API inputs handled as
valid (F_2XXPandF_2XXD). This type of faults are pervasive: APIs
are sometimes designed to return successful responses even when
receiving invalid inputs (e.g., by ignoring them). For instance, in
theStripeAPI[ 45],whenusingtwomutuallyexclusiveparameters,
the API ignores one of them instead of properly returning a 400
(‚ÄúBad Request‚Äù )statuscode.ThiswasconfirmedbytheStripeAPI
developers. Invalid inputs do, however, cause API crashes in some
cases, due to poor input validation. We found 5 bugs of this type
(F_5XXI)in4APIs(DHL,Marvel,OMDbandYelp).Servererrorson
validinputs( F_5XXV)areamuchmoreseverefault,sincetheymay
be causedby misconfigurationsor datacorruption, among others.
WefoundoneoftheseintheSpotifyAPI,unveiledwhenreordering
or replacing the tracks of a playlist. Lastly, we uncovered 16 client
errorsobtainedwithvalidAPIinputs( F_400).Thesefaultsgenerally
revealinconsistenciesbetweenthedocumentationandtheimple-
mentationofanAPI.Forinstance,intheDHLAPI,thecountrycode
‚ÄòKV‚Äô(Kosovo)is rejectedbytheAPI(400statuscode),althoughit
should be supportedaccording to the API documentation [ 23].
TheFtDRrangedfrom0.04 √ó10‚àí3(11faultsoutof258,297test
cases) in the API of RESTCountries to 5.8 √ó10‚àí3(16 out of 2,712) in
the API of Amadeus. An overview of when and how often faults
arefoundisshowninFigure 4,whichshowstheAPFDforevery
API under test. For all APIs, two thirds of the test budget (i.e.,
the test cases generated in 10 days) was enough to uncover all
faults.LanguageTool,RESTCountriesandStripeexposedalltheir
faults within the first day of testing. These faults were related to
disconformities with the API specifications ( F_OAS) and wrong
handling of invalid inputs ( F_2XXPandF_2XXD). On the other
hand, Amadeus, DHL, Marvel and YouTubeComments required
over one week to uncover all faults. More varied issues were found
inthesecases,as explainedinSection 5.
Summary ofanswers to RQ2
(1)Automaticallygeneratedtest casesfound147faultsin 11APIs(65
ofwhichconfirmed orfixed by developers).
(2)Halfofthe faults (73) arecaused by OAS disconformities.
(3)Aboutone third of thefaults (52)are caused by invalidAPI inputs
handled as valid.
(4)Only6faults aredue to internalservererrors (5XXstatuscodes).
(5)87% of faults (128) were found in the first 3 days of testing, with 10
days being enoughto uncoverallfaults.
4.3 RQ 3:ComparisonofTestingApproaches
Alltestingapproachesuncoveredfaults,evenafter3daysoftesting,
but we foundseveral differences among the testdata and test case
generation techniques under evaluation, discussed below. As previ-
ously mentioned, the results for the APIs of FDIC and Ohsome are
discussed later (Section 5.1) due to the number of failures revealed.
Thus, the results reportednextrefer to the 11 remaining APIs.
All test data generation techniques except semantically-related
data(SD)founduniquebugsorcontributedtoimprovethecoverage
inallAPIsundertest.Regardingcoverage,customizeddatagenera-
tors (CDG) and contextual data (CD) obtained the best results in
mostAPIs(8outof11).Thiswasexpected,sincevaliddatatends
Figure 4:APFDperAPI.Markersdenote faults.
Figure5:Coverageandfaultsovertestcasesclassifiedperbot
fortheYouTubeSearch API.Markersdenote uniquefaults.
to cover more input elements (e.g., parameter values) and to obtain
successfulAPIresponses,thuscoveringmoreoutputelements(e.g.,
responsebodyproperties).Invaliddata√êdataperturbation(DP)and
fuzzing dictionaries (FD)√êachieved the highest coverage in 3 APIs,
because they covered client and server errors (4XX and 5XX status
codes)thatvaliddatacouldnotcover.Intermsoffaultsfound,valid
andinvaliddatagenerationuncovered76and81faults,respectively
(10ofthesefaultswerefoundbybothapproaches).Infact,thebugs
uncovered by DP are often unique (i.e., not uncovered by other
techniques). Figure 5(right-hand side) illustrates this phenomenon
in the API of YouTubeSearch (charts of all APIs available in the
supplementary material [ 92]), where allbugs foundby the DP bot
were unique. Overall, DP found the largest amount of unique bugs,
55 in total, followed by CDG with 27. Different techniques uncover
different types of bugs. For instance, F_2XXPandF_2XXDfaults
can only be detected by DP bots, since they purposely manipulate
theinputdatausedtomakeitinvalid.Similarly, F_400faultscan
only be detected by CDG bots, since they use manually set valid
data whichshould never obtain 400status codes.
Whenusingthesametestdatagenerationtechnique,different
test case generation strategies achieved similar coverage. However,
constraint-based testing (CBT) was more cost-effective than ran-
domtesting(RT).Forinstance,inYouTubeSearch(left-handside
of Figure 5), CBT required just about 50 test cases to achieve the
samecoverageasRTwithabout700.Regardingfaultsfound,RT,
CBT and adaptive random testing (ART) found 63, 31 and 4 unique
bugs, respectively. However, in the APIs where it was evaluated,
415Online Testingof RESTful APIs: Promises andChallenges ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore
CBTfoundmorebugsthanRT.Atthesametime,ARTdidnotyield
any improvement over RT or CBT, except in Amadeus, where 3
morebugswerefound,comparedto CBT. Inparticular,thesefaults
belong to the F_OAScategory, therefore the diversification of API
requests may have led to more diverse API responses, where more
variedOASdisconformitieswere found.
Summary ofanswers to RQ3
(1)Validandinvalidtestdatagenerationstrategies(i.e.,positiveand
negativetesting)areclearlycomplementary,astheyuncovereda
similar numberofunique bugs.
(2)All test data generation strategies contributed to detect unique
faultsorimprovethecoverageinsomeAPI,beingdataperturbation
the most effectiveone. Theonlyexception was semantic data.
(3)All test case generation strategies contributed to detect unique
faultsor improvethe coverageinsomeAPI,being constraint-based
testingthe most effectiveone.
(4)TestcasediversityintermsofAPIparametersandinputvaluesdoes
not seem tohelp tofind new bugs orachieve higherAPI coverage.
5 DISCUSSION
In this section, we discuss theresults ofFDIC andOhsome, where
thousandsofpotentialfaultswereidentified,aswellassomeofthe
issues detectedin allAPIs andthe overall costof our experiments.
5.1 FDIC andOhsomeAPIs
In the APIs of FDIC and Ohsome, a total of 24,653 and 146,144 test
failures were classified into 2,871 and 1,445 potential faults (i.e.,
fault clusters), respectively. In FDIC, 2,686 of these clusters (94% of
the total) are related to disconformities with the OAS specification.
After a quick analysis, we conjecture that most of these clusters
represent,indeed, uniquefaults.Thisisbecausethespecification
of this API is so complex and its responses are so varied that every
testcasecanpotentiallyuncovernewinconsistenciesintheOAS
specification. In Ohsome, the same types of faults seem to occur in
allits122operations.Thisexplainsthehighnumberoffaultclusters
found.Althoughitispossiblethatasinglefaultmanifestinmultiple
operations, this cannot be known in a black-box testing setup,
therefore we adopt the same criteria as previous authors, where
faultsindifferentoperations are consideredunique [ 55,70,86].
Weanalyzedasubset offault clustersinFDIC and Ohsometo
graspabetterunderstandingofhoweffectivelyourfaultclustering
approachfoundreal-worldbugsintheseAPIs,andwhatshapethey
have.Wecreatedthesubsetasfollows:wesortedfaultclustersover
timeandweselected42clusters(meannumberofclustersfoundin
theotherAPIsundertest)equidistanttoeachother.Forexample,
in Ohsome, out of the 1,445 faults clusters found, we analyzed
thoseatpositions [1,36,71,...,1375,1410,1445].InFDIC,allfault
clusters were unique faults. We also spotted 33 additional bugs
inthegraphicaltestreports[ 33],conformingatotalof75unique
bugsinthisAPI.InOhsome,28outof42faultclusterswereunique
faults. Additionally, we spotted 3 faults related to 500 status codes
in the test reports, making a total of 31 unique bugs. In both cases,
more faults could have been manuallyfound in the test reports, for
instance, faults affecting other API operations in Ohsome, or more
disconformitieswiththe OASspecification inFDIC.5.2 Issues Detected
Testing APIs in production can be helpful to uncover problems
that escaped offline testing. In fact, online testing allowed us to
detectnotonlyfunctionalbugs,butalsoproblemsrelatedtonon-
functionalrequirementsoftheAPIsundertest.Wedetailthesefirst,
andthen we delve intosomeof the functional bugsfound.
5.2.1 Non-Functional Requirements. We found problems related to
security,reliability,availability andSLA compliance.
Security.Whenusingmalformedinputdata,wefound500status
codesshowingthestacktraceoftheexceptionthrownintheOMDb
API [13], and 400 status codes with the error message ‚ÄúThreat
Detected‚Äù in the DHL API [ 7]. Disclosing this information to a
potential attacker is dangerous, since they could exploit vulnerabil-
itiesrevealed in thestack trace (e.g., outdated libraries) or findan
inputcompromisingthe integrityof the system.
Reliability and availability . We detected temporary outages in
the form of 5XX status codes in 9 APIs (DHL, Foursquare, Lan-
guageTool,Marvel,Ohsome,OMDb,RESTCountries,Spotifyand
Yelp). Outages are generally caused by limitations of the servers,
and they can affect the overall user experience, especially when
they are prolongedintime (e.g.,as inYelp andOhsome).
SLA compliance . In the ninth day of testing, we were banned
from the DHL API, even though we always complied with the
specifiedquotalimitations.SLA-awareAPIspecificationssuchas
SLA4OAI [ 75] could help detect more SLA violations like this in a
more systematic way.
5.2.2 FunctionalBugs. We identified 254reproduciblebugsin all
APIs under test√ê209 extracted from 586 fault clusters, 44 found in
the test reports [ 33], and 1 found by the garbage collector bot of
Spotify (when unfollowing playlists, the bot would start obtaining
server errors if requests were sent quickly, e.g., every 5 millisec-
onds[16]).Thisisaconservativeapproximation,sincemorebugs
could have been found with a more sophisticated and precise fault
clustering approach (Section 3.3.6), or with a more thorough man-
ualanalysisofthepotentialfaults foundinFDICand Ohsome.To
date, 65 bugs from 7 APIs (Amadeus, DHL, Foursquare, OMDb,
RESTCountries, Yelp and YouTube) have been acknowledged by
developers, reproducedby other usersoralreadyfixed.Next, we
describesometypesofbugsuncoveredinourexperiments(allbugs
are documentedinthe supplementary material[ 92]).
ErrorsduetoinvaliddataintroducedbyAPIclients .IntheAmadeus
API,hotelobjectsmustcontaintwoproperties, phoneandfax,both
ofwhichmustmatchtheregularexpression ‚Äò[0-9]{2}‚Äô .Wefound
response bodies whose hotel objects contained invalid phone num-
bers (e.g., ‚Äò+1 2 9‚Äô ) [5] and missing fax numbers [ 6]. Client appli-
cations may not be able to parse such API responses, oreven crash.
Amadeus developers confirmed that: (1) these errors are caused by
hotel providers themselves, as they may introduce invalid hotel
data; and (2) they would update the API specification to reflect this
newscenario.Issueslikethesecanhardlybediscoveredwithoffline
testing,since they dealwithdata from the real-world systems.
Inconsistenciesbetween API implementation and API documenta-
tion.IndustrialAPIsarecomplexandevolverapidly,andsodoes
their documentation. In some cases, the implementation and docu-
mentationofanAPImaynotbeinsync.Forinstance,theYouTube
416ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore Alberto Martin-Lopez,Sergio Segura, andAntonio Ruiz-Cort√©s
search API documentation [ 51] does not state that parameters
location andchannelType aremutuallyexclusive,althoughwhen
using both in an API request, a 400 status code is obtained [ 19].
Similarly,thedocumentationoftheYouTubecommentsAPI[ 49]
statesthatparameters idandmaxResults aremutuallyexclusive,
although when using both in an API request, a successful response
is obtained [ 18]. YouTube developers acknowledged both issues,
the former as incomplete documentation (they updated the API doc-
umentationaccordingly)[ 21], andthelatterasan implementation
defect(they introduced a bug fix) [ 20]. A continuous online testing
setupcan rapidly detecttheseinconsistencies.
Internalservererrorswithvalidinputdata .APIcrashescausedby
invalidrequests√êforinstance,usinganout-of-rangevalueforan
enumparameter in Marvel [ 11], or using a negative number for the
limitparameterinDHL[ 9]√êarerelativelycommonandeasytofix,
namely,bydoingproperinputvalidation.However,servererrors
on valid inputs represent a more serious fault.For instance, in the
OhsomeAPI,avalidrequestobtaineda500statuscodewiththeer-
rormessage ‚ÄúNo message available‚Äù [12].Similarly,intheFDIC
API,multiplevalidrequestsobtainedservererrorswiththemessage
‚ÄúCannot read property ‚Äòmap‚Äô of undefined‚Äù [10].Debugging
thesefaultsishard,especiallyduetothelittleinformationprovided
inthe errormessages.
Beyond all these bugs, we uncovered dozens of other types of
issuesrelatedtoOASdisconformities[ 17],inconsistenciesbetween
the status codes and response bodies [ 15], unparseable JSON re-
sponses [14] and unexpected client errors [ 8], among many others.
5.3 CostofourOnline TestingSetup
Our testing ecosystem is heavily based on RESTest. This greatly
influencedthemanualworkrequiredtodeployalltestbots,aswell
as the overallconsumption ofcomputationalresources.
In terms of manual work, we had to configure 75 test bots
(RESTest instances), which involved writing both test configura-
tion files and data dictionaries (mainly used by CDG bots). For the
testconfigurationfiles,wewroteabout26Klinesofcode(LOCs),
althoughabout95%ofthemwereduplicated,i.e.,copiedandpasted.
Thishighpercentagerevealsthatthedataformatusedforconfig-
urationfilesinRESTestshouldbeimprovednecessarily,tomake
it less verbose. Despite this limitation, half of the bots required
just 2 or less manually written LOCs, since the configuration of
different bots of the same API can be reused. Regarding data dic-
tionaries,weused50dictionariescontaining52,084values,about
1Kvalues perdictionary.Notethat datadictionariesaregenerally
not manually written, but rather extracted from the API documen-
tation (e.g., Foursquare categories [ 25] and Stripe tax codes [ 46])
orothergeneral-purposeknowledgebases(e.g.,countrycodesin
Wikipedia [ 47]).
Intermsofcomputationalresources,theRAManddiskconsump-
tion kept increasing over the 15 days of online testing. Preliminary
experiments show that RAM usage could be reduced by restarting
bots regularly (e.g., every day), to avoid potential memory leaks.
Wealsomeasuredthecomputationalcostperbot.Althoughwedid
notfindnoticeabledifferencesacrossdifferentbotsofthesameAPI,
wedidfindevidentdifferencesbetweendifferentAPIs.Forinstance,
thebotstestingFDICandRESTCountriesusedmuchmoreRAM(upto20GB)andtookupmuchmorediskspace(upto250GB)than
therest.Thisisduetotheshapeandsizeoftheresponsesobtained
intheseAPIs,someofwhichtookhundredsofMBs.Handlingsuch
responses(e.g.,parsingandanalyzingtheminthesearchforfaults)
implies anon-negligible overhead.
6 CHALLENGES
Weidentifythefollowingchallengesfortheadoptionoftestcase
generationtechniques for onlinetestingof RESTfulAPIs.
Challenge #1 :Automated fault identification . Automatically deter-
miningtherootcausesofthemanyfailuresfound(about390Kin
ourwork)ischallenging.Ourfaultclusteringapproachhelpedus
identify 139 unique bugs out of 502 potential faults, but this also
meansthat363potentialfaultsweremisclassified(i.e.,duplicated,
non-reproducible or not actual faults), and 8 bugs that we spot-
ted in the test reports were missed in the fault clustering. Faults
are misclassified due to the heterogeneity of error logs, requests,
andespecially responses.Thethresholdsused maynotbe equally
effectivefor allfailures.Moresophisticated faultidentificationap-
proaches are desirable, e.g., using dynamic thresholds according
tothe type offailureand API,applyingnon-supervisedmachine
learning techniques to improve the precision of the automatic clas-
sification, or employing delta debugging to isolate failure-inducing
inputs[103].
Challenge#2 :Effectivehumaninteraction .Whilebotsworkmostly
autonomously, they can benefit greatly from some human interac-
tion. For instance, fault clusters could be formed by output bots,
andthencheckedbyahumantoimprovetheaccuracyoftheclassi-
ficationandtoavoidtheformationofduplicatedclusters.Similarly,
botscouldreportsomefailuresas≈Çpotentialfaults≈æandrequesthu-
maninputtoconfirmordiscardthem.Thiscouldbeaddressedwith
active learning algorithms [ 98], for example, integrating human
inputas apart ofthe testinganddebuggingprocess.
Challenge#3 :Optimalselectionoftestingstrategies .Testersmay
choose different testing strategies according to several factors such
asthetypesofbugsthattheywishtouncover(e.g.,crashes,regres-
sionsorinconsistenciesbetweentheAPIanditsdocumentation),
or the characteristics of the API under test (e.g., size, output for-
mat and quota limitations). Automatically determining the most
appropriatetestingtechniques basedontheseandotherfactorsis
an open problem.
Challenge #4 :Optimal test execution scheduling . The test execu-
tion schedule determines when, how many, and in what order
tests shouldbe executed.This wasmanually configuredinourex-
periments.Ideally,however,thescheduleshouldbeautomatically
computedtooptimizetheavailablequota(basedontheAPIpricing
plans),thetimeandeconomicbudget(e.g.,theinfrastructurecosts),
and the testing strategies used (e.g., prioritizing those strategies
that are more cost-effective).
Challenge #5 :Optimization of computational resources . This is
bothanengineering andaresearchchallenge.Testbotsshouldbe
optimizedtousetheleastresourcespossible,especiallyavoiding
memoryleaks.Fromaresearchpointofview,sophisticatedtech-
niquesfortestregression,selectionandprioritizationcouldhelp
417Online Testingof RESTful APIs: Promises andChallenges ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore
in devisingmore effective test suites or reusingexisting ones (e.g.,
with metamorphic testing [ 97]), thus saving resources by avoid-
ing the creation and execution of test cases that are not likely to
uncover newfaults.
7 THREATS TO VALIDITY
Ourworkissubjecttoanumberofvaliditythreats,discussedbelow.
Internalvalidity .Threatstotheinternalvalidityrelatetothose
factorsthatmayaffecttheresultsofourevaluation.Faultsintheim-
plementationofourtestingecosystemmightcompromisethevalid-
ityofourconclusions.Eventhoughthetoolsused(e.g.,RESTest[ 91]
and Allure [ 1]) are thoroughly tested, it cannot be guaranteed that
theyarefreeofbugs.Tomitigatethisthreat,andtoenablereplicabil-
ity of our results, we provide a supplementary package containing
thesourcecodeandtheimplementationofalltoolsusedinourtest-
ingecosystem(e.g.,thebotsandthecontrollercomponent),aswell
as scripts to fully replicate the results reportedinthis paper [ 92].
The use of RESTest as the selected test case generation tool may
have influenced the results obtained in our experiments, especially
dependingonhowtheevaluatedtechniques(e.g.,dataperturbation
and adaptive random testing) were implemented in the framework.
However,thisdoesnotinvalidateourresults;onthecontrary,itsup-
ports the potential of our multi-bot architecture to integrate other
testingtechniques andtools(e.g.,RESTler[ 57]andRestCT[ 101]).
To mitigate possible errors of our fault clustering approach (Sec-
tion3.3.6), wemanuallyreviewedallfaultclustersgenerated,and
reportedallidentifiedfaultstotheAPIdevelopers,soastocounton
their confirmation when possible. We adopt a conservative approx-
imation and report only faults that have been manually confirmed.
Despite possiblelimitations of our fault clustering approach, it au-
tomatically classified 218,419 failures into 502 potential faults in 11
APIs.Fromthese,weidentified139actualuniquebugs,65ofwhich
have been acknowledgedorfixedbyAPI developers to date.
Externalvalidity .Threatstotheexternalvaliditymightaffectthe
generalizabilityofourfindings.Themainthreatinthisregardisthe
selectionofcasestudies.Wetested13industrialAPIscomprising
189 operations in total, although this might not be a sufficiently
representative sample. Tominimize this threat, we selectedhighly
popular APIs with millions of users worlwide, from different appli-
cation domains (e.g., media, financial and social), and with diverse
characteristics andsizes.
8 CONCLUSIONS
In this paper, we assessed the potential of automated black-box
test case generation approaches for online testing of RESTful APIs,
resembling the modeloftesting as aservice increasingly foundin
industry. To this end, we devised a multi-bot architecture allowing
ustogenerateandexecutetestcaseswithvariedstrategiesduring15
daysnon-stopin13highlypopularAPIs.Theresultsarepromising,
withover200bugsfound,bothfunctionalandnon-functional,some
of which have led developers to take actions including fixes and
documentation updates inthe APIsof Amadeus and YouTube. This
provides an encouraging vision on the future of testing of web
APIsasaservice,whereplatformscouldofferarichcatalogofbots
providingdiverseautomatedtestcasegenerationcapabilitiesunderdifferentpricingplans.Severalchallengesstayinthewaythough.
In this regard, our work paves the way for new promising research
directions toovercome someofthe problems identified,including
theneedforautomateddebuggingmechanisms,human-in-the-loop
models,andoptimal test executionschedulingstrategies.
DATA-AVAILABILITYSTATEMENT
In order to enable reproducibility of the results obtained in this
paper, we provide a supplementary material containing the source
codeofthescriptsandprogramsdeveloped,theexecutablebinaries,
the data generated in the experiments and instructions on how
to replicate the experiments, reproduce the results and reuse the
material for further research. The artifact can be downloaded from
Zenodo at [ 92]:
https://doi.org/10.5281/zenodo.6941292
ACKNOWLEDGMENTS
ThisworkhasbeensupportedbytheEuropeanCommission(FEDER)
andJuntadeAndaluc√≠aunderprojectsMEMENTO(US-1381595),
APOLO (US-1264651) and EKIPMENT-PLUS (P18-FR-2895), by the
Spanish Government (FEDER/Ministerio de Ciencia e Innovaci√≥n ≈õ
AgenciaEstataldeInvestigaci√≥n)underprojectHORATIO(RTI2018-
101204-B-C21),byMCIN/AEI/10.13039/501100011033/FEDER,UE
underprojectBUBO(PID2021-126227NB-C22),bytheExcellence
Network SEBASENet 2.0 (RED2018-102472-T), by the FPU scholar-
shipprogram,grantedbytheSpanishMinistryofEducationand
VocationalTraining(FPU17/04077),andbyaFulbrightSpainPre-
doctoralResearchgrant attheUniversityofCalifornia,Berkeley.
We are grateful to Prof. Armando Fox for his feedback on early
stages of this work. We would also like to thank Juan Domingo
Mart√≠n G√≥mez for his technical support for the analysis of the
data and for insightful discussions regarding the proposed fault
clustering approach. [Albertospeaking:] Thanks, dad.
REFERENCES
[1][n.d.]. Allure test reporting framework. http://allure.qatools.ru . Accessed:
March2022.
[2][n.d.]. AmadeusHotelSearchAPI. https://developers.amadeus.com/self-service/
category/hotel/api-doc/hotel-search/api-reference/v/2.1 . Accessed:March2022.
[3] [n.d.]. api.data.gov. https://api.data.gov . Accessed:March2022.
[4] [n.d.]. APIs.guru. https://apis.guru . Accessed:March2022.
[5][n.d.]. Bug in Amadeus API ≈õ Response contains invalid phone num-
ber.http://restest.us.es/fse2022/tf/target/allure-reports/amadeus__r_art_
custom/#behaviors/b1a8273437954620fa374b796ffaacdd/26e68cd72fea6346/ . Ac-
cessed:March2022.
[6][n.d.]. Bug in Amadeus API ≈õ Response lacks required fax prop-
erty.http://restest.us.es/fse2022/tf/target/allure-reports/amadeus__r_art_
custom/#behaviors/b1a8273437954620fa374b796ffaacdd/8fe8e0421e364228/ . Ac-
cessed:March2022.
[7][n.d.]. BuginDHLAPI≈õ400statuscodewithmessage≈ÇThreatDetected≈æ. http:
//restest.us.es/fse2022/tf/target/allure-reports/dhl_locationFindByAddress_
_r_ft_/#behaviors/b1a8273437954620fa374b796ffaacdd/cfb50a0a6efc15e9/ .
Accessed:March2022.
[8][n.d.]. Bug in DHL API ≈õ 400 status code with supported coun-
try code ‚ÄòKV‚Äô (Kosovo). http://restest.us.es/fse2022/tf/target/allure-
reports/dhl_locationFindByAddress__r_rt_custom/#behaviors/
b1a8273437954620fa374b796ffaacdd/2cf5fccaf1d0ac1a/ . Accessed:March2022.
[9][n.d.]. Bug in DHL API ≈õ 500 status code when using a negative num-
ber for the limit parameter. http://restest.us.es/fse2022/tf/target/allure-
reports/dhl_locationFindByAddress__r_rt_perturbation/#behaviors/
b1a8273437954620fa374b796ffaacdd/a2c251ace36b16b0/ . Accessed: March 2022.
[10][n.d.]. Bug in FDIC API ≈õ 500 status code with valid request.
http://restest.us.es/fse2022/tf/target/allure-reports/fdic__r_art_custom/
418ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore Alberto Martin-Lopez,Sergio Segura, andAntonio Ruiz-Cort√©s
#behaviors/b1a8273437954620fa374b796ffaacdd/f8ef9837ff94e078/ . Accessed:
March2022.
[11][n.d.]. BuginMarvelAPI≈õ500statuscodewhenusinganout-of-rangevalue
for an enum parameter. http://restest.us.es/fse2022/tf/target/allure-reports/
marvel__r_rt_perturbation/#behaviors/b1a8273437954620fa374b796ffaacdd/
d18f37597689cbd9/ . Accessed:March2022.
[12][n.d.]. Bug in Ohsome API ≈õ 500 status code with valid request.
http://restest.us.es/fse2022/tf/target/allure-reports/ohsome__r_cbt_custom/
#categories/882a17760bce9dccc99edbd9f7e59f35/ecb286a3c5503006/ . Accessed:
March2022.
[13][n.d.]. Bug in OMDb API ≈õ 500 status code showing stack trace of thrown
exception. http://restest.us.es/fse2022/tf/target/allure-reports/omdb__r_ft_/#
behaviors/b1a8273437954620fa374b796ffaacdd/68a2df1b2b5866af/ . Accessed:
March2022.
[14][n.d.]. Bug in OMDb API ≈õ JSON response contains non-escaped ‚Äò"‚Äô charac-
ter.http://restest.us.es/fse2022/tf/target/allure-reports/omdb__r_cbt_stateful/
#behaviors/b1a8273437954620fa374b796ffaacdd/cb2ee5c85b6bed2c/ . Accessed:
March2022.
[15][n.d.]. BuginRESTCountriesAPI≈õInconsistencybetweenstatuscodeanderror
message. http://restest.us.es/fse2022/tf/target/allure-reports/restcountries_
_r_rt_perturbation/#behaviors/b1a8273437954620fa374b796ffaacdd/
5b8c51678fa58536/ . Accessed:March2022.
[16][n.d.]. Bug in Spotify API ≈õ 502 status code when unfollowing playlists
quickly.https://community.spotify.com/t5/Spotify-for-Developers/Web-API-
issue-502-Server-Error-when-unfollowing-playlists/td-p/5346160 . Accessed:
March2022.
[17][n.d.]. Bug in Yelp API ≈õ Value of price property (‚Äò ‚Ç¨‚Äô) not found in
enum possible values (‚Äò$‚Äô, ‚Äò$$‚Äô, ‚Äò$$$‚Äô, ‚Äò$$$$‚Äô). http://restest.us.es/fse2022/
tf/target/allure-reports/yelp_Businesses__r_cbt_custom/#behaviors/
b1a8273437954620fa374b796ffaacdd/d50ee751a281f9a3/ . Accessed: March 2022.
[18][n.d.]. Bug in YouTube comments API ≈õ 200 status code with in-
valid request. http://restest.us.es/fse2022/tf/target/allure-reports/
youTube_CommentsAndThreads__rw_cbt_perturbation/#behaviors/
b1a8273437954620fa374b796ffaacdd/bbb974de70976367/ . Accessed: March 2022.
[19][n.d.]. Bug in YouTube search API ≈õ 400 status code with valid request.
http://restest.us.es/fse2022/tf/target/allure-reports/youTube_Search__r_cbt_
custom/#behaviors/b1a8273437954620fa374b796ffaacdd/25f179afd3f2f309/ .
Accessed:March2022.
[20][n.d.]. Bug report in YouTube comments API ≈õ [GET /comments] Request with
parametersidandmaxResultsreturnssuccessfulresponse,althoughtheyare
mutuallyexclusive. https://issuetracker.google.com/issues/220795144 . Accessed:
March2022.
[21][n.d.]. Bug report in YouTube search API ≈õ [GET /search] Valid request obtains
400statuscode.Undocumentedrestriction:Iflocationisused,thenchannelType
cannotbeused. https://issuetracker.google.com/issues/220859560 . Accessed:
March2022.
[22] [n.d.]. Datadog. https://www.datadoghq.com . Accessed:March2022.
[23][n.d.]. DHL Location Finder API. https://developer.dhl.com/api-reference/
location-finder#reference-docs-section . Accessed:March2022.
[24][n.d.]. FDICAPI. https://banks.data.fdic.gov/docs/#api_endpoints . Accessed:
March2022.
[25][n.d.]. Foursquarecategories. https://developer.foursquare.com/docs/categories .
Accessed:March2022.
[26][n.d.]. Foursquare Venus Search API. https://developer.foursquare.com/
reference/v2-venues-search . Accessed:March2022.
[27][n.d.]. LanguageToolAPI. https://languagetool.org/http-api/#/default . Accessed:
March2022.
[28] [n.d.]. MarvelAPI. https://developer.marvel.com/docs . Accessed:March2022.
[29][n.d.]. Microsoft Graph REST API. https://docs.microsoft.com/en-us/graph/api/
overview . Accessed:March2022.
[30][n.d.]. NetflixConductor. https://netflix.github.io/conductor/apispec . Accessed:
March2022.
[31][n.d.]. Ohsome API. https://api.ohsome.org/v1/swagger-ui.html . Accessed:
March2022.
[32] [n.d.]. OMDb API. http://www.omdbapi.com . Accessed:March2022.
[33][n.d.]. Online test reports generated. http://restest.us.es/fse2022/showcase .
Accessed:March2022.
[34][n.d.]. OpenAPISpecification. https://www.openapis.org . Accessed:January
2022.
[35][n.d.]. OpenAPI specification coverage. https://github.com/meetmatt/open-api-
coverage. Accessed:March2022.
[36] [n.d.]. Postman. https://www.postman.com . Accessed:March2022.
[37][n.d.]. ProgrammableWebAPIDirectory. http://www.programmableweb.com .
Accessed:March2022.
[38][n.d.]. RapidAPI Hub. https://rapidapi.com/products/hub . Accessed: March
2022.
[39][n.d.]. RapidAPI Testing. https://rapidapi.com/products/api-testing . Accessed:
March2022.[40][n.d.]. RapidAPITesting-Pricing. https://rapidapi.com/products/api-testing/
#pricing. Accessed:March2022.
[41] [n.d.]. REST Assured. http://rest-assured.io . Accessed:March2022.
[42] [n.d.]. RESTCountriesAPI. https://restcountries.com . Accessed:March2022.
[43] [n.d.]. Sauce Labs. https://saucelabs.com . Accessed:March2022.
[44][n.d.]. Spotify Web API. https://developer.spotify.com/documentation/web-
api/reference . Accessed:March2022.
[45][n.d.]. Stripe Products API. https://stripe.com/docs/api/products . Accessed:
March2022.
[46][n.d.]. Stripetaxcodes. https://stripe.com/docs/tax/tax-codes . Accessed:March
2022.
[47][n.d.]. Wikipedia,thefreeencyclopedia. https://wikipedia.org . Accessed:March
2022.
[48][n.d.]. Yelp Businesses Search API. https://www.yelp.com/developers/
documentation/v3/business_search . Accessed:March2022.
[49][n.d.]. YouTube Comments API. https://developers.google.com/youtube/v3/
docs/comments . Accessed:March2022.
[50][n.d.]. YouTube Data API. https://developers.google.com/youtube/v3/docs .
Accessed:March2022.
[51][n.d.]. YouTubeSearchAPI. https://developers.google.com/youtube/v3/docs/
search/list . Accessed:March2022.
[52]JohnAhlgren,MariaEugeniaBerezin,KingaBojarczuk,ElenaDulskyte,Inna
Dvortsova, Johann George, Natalija Gucevska, Mark Harman, Ralf Laemmel,
Erik Meijer, et al .2020. WES: Agent-Based User Interaction Simulation on Real
Infrastructure. In Proceedings of the IEEE/ACM 42nd International Conference on
SoftwareEngineering Workshops . 276≈õ284.
[53]Khaled Alnawasreh, Patrizio Pelliccione, Zhenxiao Hao, M√•rten R√•nge, and
AntoniaBertolino.2017. OnlineRobustnessTestingofDistributedEmbedded
Systems: AnIndustrial Approach. In 2017 IEEE/ACM 39th International Confer-
ence on Software Engineering: Software Engineering in Practice Track (ICSE-SEIP) .
133≈õ142.
[54]JuanC.Alonso,AlbertoMartin-Lopez,SergioSegura,JoseMariaGarcia,and
Antonio Ruiz-Cortes. 2022. ARTE: Automated Generation of Realistic Test
Inputs for Web APIs. IEEE Transactions onSoftwareEngineering (2022).
[55]Andrea Arcuri. 2019. RESTful API Automated Test Case Generation with
EvoMaster. ACMTOSEM 28,1 (2019), 1≈õ37.
[56]AndreaArcuri.2021. AutomatedBlackboxandWhiteboxTestingofRESTful
APIsWith EvoMaster. IEEE Software (2021).
[57]VaggelisAtlidakis,PatriceGodefroid,andMarinaPolishchuk.2019. RESTler:
Stateful REST API Fuzzing. In International Conference on Software Engineering .
748≈õ758.
[58]VaggelisAtlidakis,Patrice Godefroid, andMarinaPolishchuk.2020. Checking
Security Properties of Cloud Services REST APIs. In International Conference on
SoftwareTesting,Verification and Validation .
[59]Xiaoying Bai, Muyang Li, Xiaofei Huang, Wei-Tek Tsai, and Jerry Gao. 2013.
Vee@Cloud: The Virtual Test Lab on the Cloud. In 2013 8th International Work-
shoponAutomationofSoftwareTest(AST) . 15≈õ18.
[60]Antonia Bertolino. 2007. Software Testing Research: Achievements, Challenges,
Dreams.In FutureofSoftwareEngineering (FOSE‚Äô07) . 85≈õ103.
[61]AntoniaBertolino,GuglielmoDeAngelis,LarsFrantzen,andAndreaPolini.2007.
The Plastic Framework and Tools for Testing Service-Oriented Applications. In
SoftwareEngineering:InternationalSummerSchools,ISSSE2006-2008 .106≈õ139.
[62]AntoniaBertolino,GuglielmoDeAngelis,AntonioGuerriero,BrenoMiranda,
Roberto Pietrantuono, and Stefano Russo. 2020. DevOpRET: Continuous Relia-
bilityTestinginDevOps. JournalofSoftware:EvolutionandProcess (2020),In
press.
[63]Antonia Bertolino, Pietro Braione, Guglielmo De Angelis, Luca Gazzola, Fitsum
Kifetew,LeonardoMariani,MatteoOrr√π,MauroPezze,RobertoPietrantuono,
StefanoRusso,etal .2021. ASurveyofField-basedTestingTechniques. ACM
ComputingSurveys(CSUR) 54,5 (2021), 1≈õ39.
[64]Christian Bizer, Jens Lehmann, Georgi Kobilarov, S√∂ren Auer, Christian Becker,
Richard Cyganiak, and Sebastian Hellmann. 2009. DBpedia - A Crystallization
Pointfor the Web of Data. Journal ofWeb Semantics 7,3 (2009), 154≈õ165.
[65]Tien-DungCao,PatrickF√©lix,RichardCastanet,andIsmailBerrada.2010.Online
TestingFramework for Web Services. In 2010 Third International Conferenceon
SoftwareTesting,Verification and Validation . 363≈õ372.
[66]MarianoCeccato,DavideCorradini,LucaGazzola,FitsumMesheshaKifetew,
Leonardo Mariani, Matteo Orru, and Paolo Tonella. 2020. A Framework for In-
Vivo Testing of Mobile Applications. In 2020 IEEE 13th International Conference
onSoftwareTesting,Validation and Verification (ICST) . 286≈õ296.
[67]Wing Kwong Chan, Shing Chi Cheung, and Karl RPH Leung. 2007. A Meta-
morphic Testing Approach for Online Testing of Service-Oriented Software
Applications. International Journal of Web Services Research (IJWSR) 4, 2 (2007),
61≈õ81.
[68]Davide Corradini, Amedeo Zampieri, Michele Pasqua, and Mariano Ceccato.
2021. Empirical Comparison of Black-Box Test Case Generation Tools for
RESTfulAPIs.In 2021IEEE21stInternationalWorkingConferenceonSourceCode
Analysisand Manipulation (SCAM) . 226≈õ236.
419Online Testingof RESTful APIs: Promises andChallenges ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore
[69]Davide Corradini, Amedeo Zampieri, Michele Pasqua, and Mariano Ceccato.
2021. Restats:ATestCoverageToolforRESTfulAPIs.In 2021IEEEInternational
Conference onSoftwareMaintenance and Evolution(ICSME) . 594≈õ598.
[70]Davide Corradini, Amedeo Zampieri, Michele Pasqua, Emanuele Viglianisi,
Michael Dallago, and Mariano Ceccato. 2022. Automated Black-Box Testing of
NominalandErrorScenariosinRESTfulAPIs. SoftwareTesting,Verificationand
Reliability (2022).
[71]Benjamin Danglot, Oscar Vera-Perez, Zhongxing Yu, Andy Zaidman, Martin
Monperrus, andBenoitBaudry.2019. ASnowballingLiteratureStudyon Test
Amplification. Journal ofSystemsand Software 157(2019), 110398.
[72]HamzaEd-douibi,JavierLuis C√°novas Izquierdo,and Jordi Cabot.2018. Auto-
maticGenerationofTestCasesforRESTAPIs:ASpecification-BasedApproach.
InInternationalEnterpriseDistributedObject ComputingConference . 181≈õ190.
[73]Sebastian Elbaum, Alexey G Malishevsky, and Gregg Rothermel. 2002. Test
CasePrioritization:AFamilyofEmpiricalStudies. IEEEtransactionsonsoftware
engineering 28,2 (2002), 159≈õ182.
[74]RoyThomasFielding.2000. ArchitecturalStylesandtheDesignofNetwork-based
SoftwareArchitectures . Ph.D. Dissertation.
[75]Antonio Gamez-Diaz, Pablo Fernandez, and Antonio Ruiz-Cortes. 2019. Au-
tomating SLA-Driven API Development with SLA4OAI. In International Confer-
ence onService-OrientedComputing . 20≈õ35.
[76]AntonioGamez-Diaz,PabloFernandez,AntonioRuiz-Cort√©s,PedroJMolina,
Nikhil Kolekar, Prithpal Bhogill, Madhurranjan Mohaan, and Francisco M√©ndez.
2019. TheRoleofLimitationsandSLAsintheAPIIndustry.In Proceedingsof
the201927thACMJointMeetingonEuropeanSoftwareEngineeringConference
and Symposiumonthe FoundationsofSoftwareEngineering . 1006≈õ1014.
[77]Patrice Godefroid. 2020. Fuzzing: Hack, Art, and Science. Commun. ACM 63, 2
(2020), 70≈õ76.
[78]PatriceGodefroid,Bo-YuanHuang,andMarinaPolishchuk.2020. Intelligent
RESTAPIDataFuzzing.In ACMJointMeetingonEuropeanSoftwareEngineering
ConferenceandSymposiumontheFoundationsofSoftwareEngineering .725≈õ736.
[79]ArnaudGotlieb.2015.Constraint-BasedTesting:AnEmergingTrendinSoftware
Testing. In AdvancesinComputers . Vol. 99.Elsevier, 67≈õ101.
[80]Michaela Greiler, Hans-Gerhard Gross, and Arie van Deursen. 2010. Evalu-
ation of Online Testing for Services: A Case Study. In Proceedings of the 2nd
International Workshop on Principles of Engineering Service-Oriented Systems .
36≈õ42.
[81]Joachim H√§nsel and Holger Giese. 2017. Towards Collective Online and Offline
Testing for Dynamic Software Product Line Systems. In 2017 IEEE/ACM 2nd
InternationalWorkshoponVariabilityandComplexityinSoftwareDesign(VACE) .
9≈õ12.
[82]RubingHuang,WeifengSun,YinyinXu,HaiboChen,DaveTowey,andXinXia.
2019. A Survey on Adaptive Random Testing. IEEE Transactions on Software
Engineering 47,10(2019), 2052≈õ2083.
[83]Mohammad S Islam, William Pourmajidi, Lei Zhang, John Steinbacher, Tony
Erwin, and Andriy Miranskyy. 2021. Anomaly Detection in a Large-scale
Cloud Platform. In 2021 IEEE/ACM 43rd International Conference on Software
Engineering: SoftwareEngineering inPractice (ICSE-SEIP) . 150≈õ159.
[84]Daniel Jacobson, Greg Brail, and Dan Woods. 2011. APIs: A Strategy Guide .
O‚ÄôReillyMedia,Inc.
[85]Stefan Karlsson, Adnan Causevic, and Daniel Sundmark. 2020. QuickREST:
Property-based Test Generation of OpenAPI Described RESTful APIs. In Inter-
nationalConference onSoftwareTesting,Verification and Validation .[86]NunoLaranjeiro,Jo√£oAgnelo,andJorgeBernardino.2021. ABlackBoxTool
for RobustnessTesting of REST Services. IEEE Access 9 (2021), 24738≈õ24754.
[87]Alberto Martin-Lopez, Andrea Arcuri, Sergio Segura, and Antonio Ruiz-Cort√©s.
2021.Black-BoxandWhite-BoxTestCaseGenerationforRESTfulAPIs:Enemies
or Allies?. In 2021 IEEE 32nd International Symposium on Software Reliability
Engineering (ISSRE) . 231≈õ241.
[88] Alberto Martin-Lopez, Sergio Segura, Carlos M√ºller, and Antonio Ruiz-Cort√©s.
2021. Specification and Automated Analysis of Inter-Parameter Dependencies
in Web APIs. IEEE Transactions onServicesComputing (2021). Articlein press.
[89]Alberto Martin-Lopez, Sergio Segura, and Antonio Ruiz-Cort√©s. 2019. Test
CoverageCriteria for RESTfulWeb APIs.In A-TEST. 15≈õ21.
[90]Alberto Martin-Lopez,Sergio Segura,and AntonioRuiz-Cort√©s. 2020. RESTest:
Black-Box Constraint-Based Testing of RESTful Web APIs. In International
Conference onService-OrientedComputing . 459≈õ475.
[91]Alberto Martin-Lopez,Sergio Segura,and AntonioRuiz-Cort√©s. 2021. RESTest:
AutomatedBlack-Box Testing of RESTfulWeb APIs.In Proceedings of the 30th
ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA
‚Äô21).
[92]Alberto Martin-Lopez, Sergio Segura, and Antonio Ruiz-Cort√©s. 2022. [Supple-
mentarymaterial]OnlineTestingofRESTfulAPIs:PromisesandChallenges.
https://doi.org/10.5281/zenodo.6941292 .
[93]A. Giuliano Mirabella, Alberto Martin-Lopez, Sergio Segura, Luis Valencia-
Cabrera, and Antonio Ruiz-Cort√©s. 2021. Deep Learning-Based Prediction of
TestInputValidityforRESTfulAPIs.In InternationalWorkshoponTestingfor
DeepLearning and DeepLearning for Testing .
[94]MarcOriol,XavierFranch,andJordiMarco.2015. MonitoringtheService-based
System Lifecycle with SALMon. Expert Systems with Applications 42, 19 (2015),
6507≈õ6521.
[95]LeonardRichardson,Mike Amundsen,and SamRuby. 2013. RESTful WebAPIs .
O‚ÄôReillyMedia,Inc.
[96]OmurSahinandBahriyeAkay.2021. ADiscreteDynamicArtificialBeeColony
withHyper-ScoutforRESTfulwebserviceAPItestsuitegeneration. Applied
SoftComputing 104(2021), 107246.
[97]Sergio Segura, Jos√© A Parejo, Javier Troya, and Antonio Ruiz-Cort√©s. 2018.
Metamorphic Testing of RESTful Web APIs. IEEE Transactions on Software
Engineering 44,11(2018), 1083≈õ1099.
[98]BurrSettles. 2012. Active Learning. Synthesis LecturesonArtificialIntelligence
and MachineLearning 18(2012), 1 ≈õ 111.
[99]DimitriStallenberg,MitchellOlsthoorn,andAnnibalePanichella.2021. Improv-
ingTestCaseGenerationforRESTAPIsThroughHierarchicalClustering.In
202136thIEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering
(ASE). 117≈õ128.
[100]Margus Veanes, Pritam Roy, and Colin Campbell. 2006. Online Testing with
Reinforcement Learning. In Formal Approaches to Software Testing and Runtime
Verification . 240≈õ253.
[101]Huayao Wu, Lixin Xu, Xintao Niu, and Changhai Nie. 2022. Combinatorial
TestingofRESTfulAPIs.In ACM/IEEE44thInternationalConferenceonSoftware
Engineering .
[102]LiYujianandLiuBo.2007. ANormalizedLevenshteinDistanceMetric. IEEE
transactionsonpatternanalysisandmachineintelligence 29,6(2007),1091≈õ1095.
[103]AndreasZellerandRalfHildebrandt.2002. SimplifyingandIsolatingFailure-
InducingInput. IEEETransactionsonSoftwareEngineering 28,2(2002),183≈õ200.
420