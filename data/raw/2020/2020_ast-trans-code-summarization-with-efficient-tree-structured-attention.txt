AST-Trans: Code Summarization with Efficient Tree-Structured
Attention
Ze Tang
State Key Laboratory for Novel
Software Technology
Nanjing University
Nanjing, China
2228291607@qq.comXiaoyu Shenâˆ—
Alexa AI
Amazon
Berlin, Germany
gyouu@amazon.comChuanyi Li, Jidong Ge
State Key Laboratory for Novel
Software Technology
Nanjing University
Nanjing, China
lcy,gjd@nju.edu.cn
Liguo Huang
Department of Computer Science
Southern Methodist University
Dallas, Texas, USA
lghuang@lyle.smu.eduZhelin Zhu, Bin Luo
State Key Laboratory for Novel
Software Technology
Nanjing University
Nanjing, China
zzl,luobin@nju.edu.cn
ABSTRACT
Code summarization aims to generate brief natural language de-
scriptions for source codes. The state-of-the-art approaches follow
atransformer-basedencoder-decoderarchitecture.Asthesource
code is highly structured and follows strict grammars, its Abstract
Syntax Tree (AST) is widely used for encoding structural infor-
mation.However,ASTsaremuchlongerthanthecorresponding
source code. Existing approaches ignore the size constraint and
simply feed the whole linearized AST into the encoders. We argue
thatsuchasimpleprocessmakesitdifficulttoextractthetrulyuse-
ful dependency relations from the overlong input sequence. It also
incurssignificantcomputationaloverheadsinceeachnodeneeds
to apply self-attention to all other nodes in the AST. To encode
the AST more effectively and efficiently, we propose AST-Transin this paper which exploits two types of node relationships in
theAST:ancestor-descendantandsiblingrelationships.Itapplies
the tree-structured attention to dynamically allocate weights forrelevant nodes and exclude irrelevant nodes based on these tworelationships.Wefurtherproposeanefficientimplementationto
supportfastparallelcomputationfortree-structureattention.On
thetwocodesummarizationdatasets,experimentalresultsshow
thatAST-Transsignificantlyoutperformsthestate-of-the-artswhile
being times more efficient than standard transformers1.
âˆ—Work done before joining.
1All thecodes and dataare availableat https://github.com/zetang94/ICSE2022_AST_
Trans.git
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.3510224CCS CONCEPTS
â€¢Softwareanditsengineering â†’Documentation ;â€¢Comput-
ing methodologies â†’Natural language generation.
KEYWORDS
tree-based neural network, source code summarization
ACM Reference Format:
Ze Tang, Xiaoyu Shen, Chuanyi Li, Jidong Ge, Liguo Huang, and Zhelin
Zhu,BinLuo.2022.AST-Trans:CodeSummarizationwithEfficientTree-
Structured Attention. In 44th International Conference on Software Engineer-
ing(ICSEâ€™22),May21â€“29,2022,Pittsburgh,PA,USA. ACM,NewYork,NY,
USA, 13 pages. https://doi.org/10.1145/3510003.3510224
1 INTRODUCTION
Thesummaryofsourcecodeisabriefnaturallanguagedescription
explainingthepurposeofthecode[ 29].Thecodetobesummarized
can be with different units. In this work, we focus on summarizing
the subroutines or defined methods in a program.
Previousstudieshaveshownthatsuchashortdescriptioncan
assistprogramdeveloperstoquicklydigestthecodewithouttravers-
ing over it themselves [ 43]. Nonetheless, maintaining high-quality
codesummariesrequiresexpensivemanuallaborinreality.Inmany
projects, these summaries are often mismatched, missing or out-
datedwhichslowdownthedevelopingprogress[ 18].Automatic
code summarization can greatly save developersâ€™ time by avoiding
writing such summaries manually for every single code snippet.
The traditional methods utilized handcrafted rules like Software
Word-Usage Model (SWUM) [ 43] or stereotypes [ 30] to synthe-
size the code summaries. However, when identifiers or methods
arepoorlynamed,theycannotextractaccuratekeywordstopro-
ducegoodsummaries.SomeusedInformationRetrieval(IR)tech-
niques[13,14]tominesummariesfromsimilarexistingcodebanks
which, unfortunately, cannot generalize to unseen code snippets
with different functions.
Recently,withthedevelopmentofopensourceplatformssuchas
Github, more and more data for code summarization can be easily
extractedfromonlineresources.Data-drivenstrategiesbasedon
1502022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:54 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Ze Tang, Xiaoyu Shen, Chuanyi Li, Jidong Ge, Liguo Huang, and Zhelin Zhu, Bin Luo

	

	 

ASTcodereturn 0 if x<0, else return x 
itself.

	
	
orelse
NameLoad(x)body
constant(0)Compare
constant(0) Lt NameLoad(x)IfExpReturnsummary
Figure 1: Example of code-AST-summary triples. We mainly need
tounderstandtheancestor-descendentandsiblingrelationshipsin
the AST to generate a summary.
neural networks start to raise more and more attention [ 20,37â€“
39,56]. Current state-of-the-arts all follow the Transformer-based
encoder-decoder architecture [ 5,8,45,48,49] and can be trained
end-to-end with code-summary pairs. Since the source code is
highlystructuredandfollowsstrictprogramminglanguagegram-
mars, a common practice is to also leverage the Abstract Syntax
Tree(AST)tohelptheencoderdigestthestructuredinformation.
TheASTisusuallylinearizedbydifferentalgorithmslikepre-order
traversal[ 21],structure-basedtraversal(SBT)[ 18]andpathdecom-
position[ 4],thenfedintotheencoder.Severalworksalsoproposed
architectures specific for tree encoding like tree-LSTM [11, 51].
However, the linearized ASTs, as containing additional struc-
turedinformation,aremuchlongerthantheircorrespondingsource
code sequence. Some linearization algorithms can further increase
the length. For example, linearizing with SBT usually makes the
size times longer. This makes the model extremely difficult to accu-
rately detect useful dependency relations from the overlong input
sequence2.Moreover,itbringssignificantcomputationaloverhead,
especially for state-of-the-art Transformer-based models where
thenumberofself-attentionoperationsgrowsquadraticallywith
thesequencelength.EncodingASTswithtree-basedmodelslike
tree-LSTM will incur extra complexity because it needs to traverse
the whole tree to obtain the state of each node.
In this work, we assume that the state of a node in the AST is
affectedmostbyits(1)ancestor-descendentnodes,whichrepresent
the hierarchical relationship across different blocks, and (2) sibling
nodes, which represent the temporal relationship within one block.
We show an example of code summarization in Figure 1. As can be
seen, we need the ancestor-descendent relationship to understand
thehigh-levelprocedure,andthesiblingrelationshiptounderstand
thelow-leveldetailswithinablock.Capturingthesetworelation-
ships are enough for producing the summary and modelling the
full attention among all nodes is unnecessary.
Based on this intuition, we propose AST-Trans, a simple variant
ofthe Transformer modeltoefficientlyhandle thetree-structured
AST.AST-Transexploitsancestor-descendantandsiblingrelation-
ship matrices to represent the tree-structure, and uses these ma-
trices to dynamically exclude irrelevant nodes in different self-
attentionlayers.Theabsolutepositionembeddingfromtheoriginal
Transformer is replaced with relative position embeddings defined
2Indeed, encoding the overlong AST with SBT even underperforms directly encoding
the source code when using Transformer with relative position embeddings [1].bythetworelationshipmatricestobettermodelthedependency.
We further describe several implementations of the proposed AST-
Trans andhave acomprehensiveanalysisof theircomputational
complexity. In short, the contributions of this paper are as below:
â€¢We propose AST-Trans that can efficiently encode long AST
sequences withlinear complexity, incontrast with thequa-
dratic complexity of the standard Transformer.
â€¢Weperformacomprehensiveanalysis,withboththeoretical
andempiricalevidences,onthecomputationalcomplexity
of different implementations.
â€¢WevalidateourproposedmodelontwodatasetsofJavaand
Python.ExperimentalresultsshowthatAST-Transoutper-
forms the state-of-the-arts by a substantial margin.
â€¢We compare representative methods for AST encoding and
discuss their pros and cons.
PaperOrganization Theremainder ofthispaperisorganized
as follows. Section 2 presents background knowledge on the Trans-
formerandAST.Section3elaboratesonthedetailsofAST-Trans,
section 4 presents its different implementation and the complexity
is analyzed in section 5. Section 6 explains the experimental setup
andanalyzestheresults.Section7discussesthreatstovalidity.Sec-
tion 8 surveys the related work. Finally, section 9 concludes the
paper and points out future research directions.
2 BACKGROUND
Transformer. TheTransformerarchitecturewasinitiallyproposed
forneuralmachinetranslation[ 49].Itconsistsofmulti-headstacked
encoder and decoder layers. In each encoder stack, the inputs first
flow through a self-attention sublayer, and then are fed into a
position-wise feed-forward network followed by a layer normaliza-
tion. The decoder has a set of the cross-attention layers to help the
decoderfocusonrelevantpartsoftheinputsequence.TheTrans-
former architecture removes the recurrence mechanism in favor of
theself-attention.Aseachwordinasentencesimultaneouslyflows
throughtheencoderanddecoderstack,themodelitselfdoesnot
have any sense of the word order. Therefore, a position embedding
is added to each word embedding to inform the order information.
Abstract Syntax Tree (AST). An Abstract Syntax Tree (AST)
uniquely represents a source code snippet in a given language
andgrammar[ 4].Theleavesofthetreeareterminals,usuallyre-
ferring to variables, types and method names. The non-leaf nodes
arenon-terminalsandrepresentarestrictedsetofstructuresinthe
programming language, e.g., loops, expressions, and variable decla-
rations.Forexample,inFigure1, variables (suchas NameLoad(x) )
arerepresentedasterminalsofAST.Syntacticstructures(suchas
Compare) are represented as non-terminals. Since the variable and
method names can be rather freely defined, directly processing the
sourcecodecanbechallenging.ItscorrespondingAST,duetoits
strictstructure,oftenservesassubstitutewhenencodingthesource
code.
3 AST-TRANS
This section details our proposed AST-Trans. For an AST, it will
be firstly linearized into a sequence. Then the ancestor-descendent
and sibling relationships among its nodes will be denoted through
151
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:54 UTC from IEEE Xplore.  Restrictions apply. AST-Trans: Code Summarization with Efficient Tree-Structured Attention ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table 1: Linearized AST of the tree in Fig 1 with POT,SBT and PD.
Methods Linearized AST sequence
POTReturn IfExp Compare NameLoad(x) Lt constant(0) body constant(0) orelse
NameLoad(x)
SBT( Return( IfExp( Compare ( constant(0) ) constant(0) ( Lt ) Lt ( NameLoad(x)
) NameLoad(x) ) Compare (bod y( constant(0) ) constant(0) )bod y( orelse
( NameLoad(x) ) NameLoad(x) ) orelse) IfExp) Return
PDPath1: Path1: Lt Compare constant(0)Path2: NameLoad(x) Compare constant(0)Path3: Path3: constant(0) Compare IfExp body constant(0)...
twospecificmatrices.Basedonthematrices,wereplacethestan-
dardself-attentionwithtree-structuredattentiontobettermodel
these two relationships. Irrelevant nodes are dynamically ruled
outto reducecomputationalcost.Wewill firstintroducedifferent
linearizationmethods(section3.1),thenexplaintheconstruction
of two relationship matrices (section 3.2), and finally present the
tree-structure attention to utilize the matrices(section 3.3).
3.1 AST Linearization
Inordertoencodethetree-shapedAST,itfirstneedstobeconverted
into a sequence with a linearization method. There are the three
most representative linearization methods used in current works:
(1)Pre-order Traversal (POT): It visits the tree nodes with pre-
ordertraversal.Sequencesobtainedbypre-ordertraversal
are lossy since the original ASTs cannot be unambiguously
reconstructed back from them.
(2)Structure-based Traversal (SBT): It adds additional brack-
ets[18]toindicatetheparental-descendentrelationshipsuch
thateachsequencecanbeunambiguouslymappedbackto
the AST, but it also doubles the size of the linearized se-
quence.
(3)Path Decomposition (PD): It represents the AST by concate-
natingthepathbetweentworandomleafnodes.Thetotal
numberofpathscanbetoolargeforcomputingandthere-
fore random sampling is needed [4].
Table 1 shows the AST in Figure 1 linearized with the above
three different methods. For POT and SBT, the linearized trees
can be directly fed into the encoder. For PD, the average total
numberofpathscanbeover200,concatenatingthemalltotrain
is infeasible [ 4]. In practice, mean pooling is run over the states
of each path such that each path has one unique representation.
The decoder only attends to these unique representations of paths
instead of specific nodes within paths. This can affect the model
when copying user-defined names (in leaf nodes) is needed.
We adopt the simplest POT linearization for our model. We
show that it has already achieved SOTA results and more complex
linearizationmethodslikeSBTdonothelp.PDdoesnotapplytoourmodelsinceittreatsonepathasawhole.Wewillshowinsection6.3
that this leads to poor performance in code summarization.
3.2 Relationship Matrices
Wedefinetwokindsofrelationshipsbetweennodesinthetreethat
wecareabout:ancestor-descendant( ğ´)andsibling( ğ‘†)relationships.
Theformerrepresentsthehierarchicalinformationacrossblocks,
andthelatterrepresentsthetemporalinformationwithinoneblock.

 	 
 
	 

	
 	
 
	 

	 
 	 
   
	 Ä

 Ä 	 
ÄÄ
	Ä

Ä


 
Figure 2: Example of generating position matrices for ancestor-
descendent (A) and sibling relationship (S). Position matrix gener-
atedfromthelinearrelationshipisusedinstandardTransformers.
Specifically,twonodeshavetheancestor-descendantrelationshipif
thereexistsadirectedpathfromrootnodethatcantraversethrough
them. Two nodes have the sibling relationship if they share the
same parent node.
We use two position matrices ğ´ğ‘Ã—ğ‘andğ‘†ğ‘Ã—ğ‘to represent
theancestor-descendentandsiblingrelationshipsrespectively. ğ‘
is the total number of nodes in AST. We denote the ğ‘–th node in
the linearized AST as ğ‘›ğ‘–.ğ´ğ‘–ğ‘—is the distance of the shortest path
between ğ‘›ğ‘–andğ‘›ğ‘—in the AST. ğ‘†ğ‘–ğ‘—is horizontal sibling distance
betweenğ‘›ğ‘–andğ‘›ğ‘—intheASTiftheysatisfythesiblingrelationship.
If one relationship is not satisfied, its value in the matrix will be
infinity.Notethatweconsidertherelativerelationshipbetweentwo
nodes, which means ğ´ğ‘–ğ‘—=âˆ’ğ´ğ‘—ğ‘–andğ‘†ğ‘–ğ‘—=âˆ’ğ‘†ğ‘—ğ‘–if a relationship
exists between ğ‘›ğ‘–andğ‘›ğ‘—.
Formally, we use SPD(ğ‘–,ğ‘—) andSID(ğ‘–,ğ‘—) to denote the Shorted
PathDistanceandhorizontal SIblingDistancebetween ğ‘›ğ‘–andğ‘›ğ‘—
in the AST. The values in the relationship matrices are defined as:
ğ´ğ‘–ğ‘—=/braceleftbiggSPD(ğ‘–,ğ‘—)if|SPD(ğ‘–,ğ‘—)| â‰¤ğ‘ƒ
âˆotherwise
ğ‘†ğ‘–ğ‘—=/braceleftbiggSID(ğ‘–,ğ‘—)if|SID(ğ‘–,ğ‘—)| â‰¤ğ‘ƒ
âˆotherwise(1)
ğ‘ƒis a pre-defined threshold and nodes with relative distance
beyondğ‘ƒwill be ignored. We hypothesize that precise relative dis-
tanceisnotusefulbeyondacertainrange.Itcanbothconstrainthecomputationcomplexitywithinaconstantrangeandsavememory
space for storing the relative position embeddings. Figure 2 shows
anexampleofgeneratingmatrix ğ´andğ‘†,incomparisonwiththe
position matrix generated from a linear relationship, which is used
in standard Transformers. In the next section, we will introducehow to use these two matrices to dynamically incorporate such
relationship information through a tree-structured attention.
3.3 Tree-Structured Attention
Tree-structured attention is built on the standard self-attention
with relative position embeddings and disentangled attention. It
replacestherelativepositionembeddingsderivedfromthelinear
relationship into the two matrices derived from the tree structure.
Self-Attention. Standard self-attention transforms the input
sequence x=(ğ‘¥1,...,ğ‘¥ğ‘›)(ğ‘¥ğ‘–âˆˆRğ‘‘whichstandsfortheembedding
ofğ‘›ğ‘–)intoasequenceofoutputvectors o=(ğ‘œ1,...,ğ‘œğ‘›)(ğ‘œğ‘–âˆˆRğ‘‘).
152
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:54 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Ze Tang, Xiaoyu Shen, Chuanyi Li, Jidong Ge, Liguo Huang, and Zhelin Zhu, Bin Luo
The single-head self-attention [49] can be formulated as:
ğœ¶ğ’Šğ’‹=ğ‘¸(ğ‘¥ğ‘–)ğ‘²(ğ‘¥ğ‘—)/intercal
âˆš
ğ‘‘
ğ‘œğ‘–=ğ‘›/summationdisplay.1
ğ‘—=1ğœ(ğœ¶ğ’Šğ’‹)ğ‘½(ğ‘¥ğ‘—)(2)
where ğ‘¸,ğ‘²:Rğ‘‘â†’Rğ‘šarequeryandkeyfunctionsrespectively,
ğ‘½:Rğ‘‘â†’Rğ‘‘is a value function, ğœis a scoring function (e.g.
softmax or hardmax).
Relativepositionembedding. Eq2isacontent-onlyattention
withoutanypositioninformation.TheinitialTransformermodel
uses absolute position embeddings to inform about the position.
Shaw et al . [36]proposed replacing them with relative position
embeddings,whichhasshownmoreeffectiveincodesummariza-
tion tasks [ 1]. The relative position ğ›¿(ğ‘–,ğ‘—)reflects the pairwise
distance between ğ‘›ğ‘–andğ‘›ğ‘—. Denoteğ‘ƒas the max relative distance,
ğ›¿(ğ‘–,ğ‘—)âˆˆ[0,2ğ‘ƒ]can be defined as:
ğ›¿(ğ‘–,ğ‘—)=â§âªâª â¨
âªâªâ©0 for ğ‘–âˆ’ğ‘—â‰¤âˆ’ğ‘ƒ
2ğ‘ƒforğ‘–âˆ’ğ‘—â‰¥ğ‘ƒ
ğ‘–âˆ’ğ‘—+ğ‘ƒothers.(3)
In this way, wecan map each relative distance intoan embedding
representation. The relative position embeddings can be added on
top of Eq 2 to inform the pairwise distance.
DisentangledAttention. DisentangledAttention[ 16]usesrel-
ative position embedding as bias in self-attention process. Each
word is represented using two vectors that encode its content and
relativepositioninandisentangledway.Theattentioncomputa-
tionisthendividedintothreeparts:content-to-content,content-
to-position and position-to-content, defined as:
Ëœğ›¼ğ‘–,ğ‘—=ğ‘¸(ğ‘¥ğ‘–)ğ‘²(ğ‘¥ğ‘—)/intercal
/bracehtipupleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipdownright/bracehtipdownleft /bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipupright
content-to-content+ğ‘¸(ğ‘¥ğ‘–)ğ‘²ğ‘·
ğ›¿(ğ‘–,ğ‘—)/intercal
/bracehtipupleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipdownright/bracehtipdownleft /bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipupright
content-to-position+ğ‘¸ğ‘·
ğœ¹(ğ’‹,ğ’Š)ğ‘²(ğ‘¥ğ‘—)/intercal
/bracehtipupleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipdownright/bracehtipdownleft /bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipupright
position-to-content(4)
where ğ‘¸ğ‘·,ğ‘²ğ‘·âˆˆR(2ğ‘ƒ+1)Ã—ğ‘šrepresentthequeryandkeyprojec-
tion matrices of relative positions. ğ‘²ğ‘·
ğ›¿(ğ‘–,ğ‘—)is theğ›¿(ğ‘–,ğ‘—)-th row of
ğ‘²ğ‘·andğ‘¸ğ‘·
ğ›¿(ğ‘–,ğ‘—)istheğ›¿(ğ‘–,ğ‘—)-throwof ğ‘¸ğ‘·respectively.Thelasttwo
items, i.e., content-to-position and position-to-content, are used to
measure the relative positions between a word pair.
Besides, for content-to-position computation, as all possible rel-
ative positions are always in [0,2ğ‘ƒ], the scores of query content
ğ‘¸(ğ‘¥)to all key positions ğ‘²ğ‘·can be first computed as ğ‘¸(ğ‘¥)ğ‘²ğ‘·/intercal,
and then gathered into Ëœğ›¼withğ›¿(ğ‘–,ğ‘—)as index. In this way, The
relative position embedding can be reused for all query contents
and thus reduce the space complexity to ğ‘‚(2ğ‘ƒğ‘š).
AttentionwithTree-StructuredRelationships. Ourmethod
essentially replaces ğ›¿(ğ‘–,ğ‘—), the relative distance defined under the
linear relationship, with ğ›¿ğ‘…(ğ‘–,ğ‘—)whereğ‘…stands for either the
ancestor-descendent relationship ğ´or the sibling relationship ğ‘†in
the tree structure. ğ›¿ğ‘…(ğ‘–,ğ‘—)is defined as:
ğ›¿ğ‘…(ğ‘–,ğ‘—)=/braceleftbiggğ‘…ğ‘–ğ‘—+ğ‘ƒ+1i fğ‘…ğ‘–ğ‘—âˆˆ[ âˆ’ğ‘ƒ,ğ‘ƒ]
0i fğ‘…ğ‘–ğ‘—=âˆ(5)
ğ‘…ğ‘–ğ‘—referstoeither ğ´ğ‘–ğ‘—orğ‘†ğ‘–ğ‘—definedinEq1.Astherearetwokinds
of relationships, we consider only one relationship in each head sothatitwillnotaddanyadditionalparameterontopofthestandard
Transformer. â„ğ´headswilluse ğ›¿ğ´(ğ‘–,ğ‘—)andtherest â„ğ‘†headswill
useğ›¿ğ‘†(ğ‘–,ğ‘—). Information from the two relationships will be merged
togetherthroughmulti-headattention.Wethenreplace ğ›¿(ğ‘–,ğ‘—)in
Eq4with ğ›¿ğ‘…(ğ‘–,ğ‘—)inFormula5,andapplyascalingfactorof1âˆš
3ğ‘‘on
Ëœğ›¼ğ‘–,ğ‘—(becauseithas3items).Thefinaloutputvectoriscomputedas
in Eq(6), where ğ‘½ğ‘·represents the value project matrix of relative
distances and ğ‘½ğ‘·
ğ‘¹ğ’Šğ’‹is theğ‘…ğ‘–ğ‘—-th row of ğ‘½ğ‘·.
Ëœğ‘œğ‘–=ğ‘—âˆˆ{ğ‘—|ğ›¿ğ‘…(ğ‘–,ğ‘—)>0} /summationdisplay.1
ğ‘—ğœ(Ëœğ›¼ğ‘–,ğ‘—âˆš
3ğ‘‘)(ğ‘½(ğ‘¥ğ‘—)+ğ‘½ğ‘·
ğ‘¹ğ’Šğ’‹) (6)
Note that we only compute the attention weights for node pairs
whereğ›¿ğ‘…(ğ‘–,ğ‘—)>0), which is similar to the idea of sliding win-
dow [7] andcan reduce the timeand space complexityof the self-
attention process. We will discuss its implementation and analyze
its complexity in sections 4 and 5 respectively.
4 EFFICIENT IMPLEMENTATION
AlimitationofthefullattentionmechanisminstandardTransform-
ers is the computational and memory cost that grows quadratically
with the sequence length. AST-Trans we proposed can alleviate
thisproblemsincetheattentionscoresonlyneedtobecomputed
for node pairs where ğ›¿ğ‘…(ğ‘–,ğ‘—)>0. Nevertheless, a memory and
computationalefficientimplementationofAST-Transthatsupports
parallelprocessingisnon-trivial.TheessenceofAST-Transissimilar
topreviousworksthatapplyslidingwindowstoconstraintheat-
tentionwithinafixedrange[ 7,54].Withslidingwindows,thenode
pairs in the sequence data can be planned into a linear distribution
(by ignoring node pairs with ğ›¿(ğ‘–,ğ‘—)=0or2ğ‘ƒâˆ’1) and computed
inparallel withmatrixpartitioning.However, thistechniquedoes
not apply to us since the position distribution of relevant nodes
changeswith everytree structure, whichmakesmatrix blockinginfeasible. In this section, we present the following 5 alternative
implementations of AST-Trans and discuss the pros and cons:
Mask.Mask out the attention scores where ğ›¿ğ‘…(ğ‘–,ğ‘—)=0 after
computing thefull attention amongall nodes. It hasthe same qua-
dratic time and space complexity as in the standard Transformer.
Loop.Loopovernodepairswhere ğ›¿ğ‘…(ğ‘–,ğ‘—)>0andcomputethe
attention scores. It is memory and computational efficient but does
not support parallel processing.
Sparse.Wecanstore ğ›¿ğ‘…asasparsetensor ğ‘†ğ‘‡(ğ›¿ğ‘…)anddeeplearn-
ingframeworks,suchasPytorch,canautomaticallyskipoperations
withzeroelementswhenmultiplyingasparsetensorwithanormal
tensor.Themaskoperationcanbeoptimized(forexample,content-
to-position attention scores in Eq 4 can be computed by gathering
ğ‘„(ğ‘¥)ğ¾ğ‘ƒ/intercalwithğ‘†ğ‘‡(ğ›¿ğ‘…)). However, it can only apply to content-to-
positionandposition-to-content.Forcontent-to-content,westill
havetousethe MaskorLoopstrategysincetheproductionoftwo
sparse tensors is not directly supported.
Gather with COO (GC). On the basis of Sparse, the content-
to-content computation can be optimized by additional gather op-
erations.ThecoreideaofGCistoputquery-keypairsthatneedto
be computed into one-to-one correspondence, and store them as
densematrices.Coordinateformat(COO)isacommonwaytostoresparsetensors,whereonlynon-zeroelementsarestoredastuplesof
153
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:54 UTC from IEEE Xplore.  Restrictions apply. AST-Trans: Code Summarization with Efficient Tree-Structured Attention ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
 
























    	
  
Figure 3: Decompose the relative distance matrix ğ›¿ğ‘…of the tree
â€œabcd" with max relative distance ğ‘ƒ=1.
elementindicesandthecorrespondingvalues.Let ğ¶ğ‘‚ğ‘‚ğ‘Ÿğ‘œğ‘¤/ğ¶ğ‘‚ğ‘‚ğ‘ğ‘œğ‘™
denotes the list of row/column indexes, and ğ¶ğ‘‚ğ‘‚ğ‘£ğ‘ğ‘™denotes the
listofvaluesintheCOOformatof ğ›¿ğ‘….Wethenusethemasindexes
to gather the query and key of content as:
ğ‘„ğ‘Ÿğ‘œğ‘¤=ğ‘„(ğ‘¥)[ğ¶ğ‘‚ğ‘‚ğ‘Ÿğ‘œğ‘¤;:];ğ¾ğ‘ğ‘œğ‘™=ğ¾(ğ‘¥)[ğ¶ğ‘‚ğ‘‚ğ‘ğ‘œğ‘™;:]
ğ‘„ğ‘ƒ
ğ‘£ğ‘ğ‘™=ğ‘„ğ‘ƒ[ğ¶ğ‘‚ğ‘‚ğ‘£ğ‘ğ‘™;:];ğ¾ğ‘ƒ
ğ‘£ğ‘ğ‘™=ğ¾ğ‘ƒ[ğ¶ğ‘‚ğ‘‚ğ‘£ğ‘ğ‘™;:]
Bythisway,eachcolumninthequerycontent ğ‘„ğ‘Ÿğ‘œğ‘¤correspondsto
the same column in the key content ğ¾ğ‘ğ‘œğ‘™. Then we can use matrix
dot production to compute attention scores:
ğ›¼ğ‘ğ‘œğ‘œ=ğ‘„ğ‘Ÿğ‘œğ‘¤âŠ™ğ¾ğ‘ğ‘œğ‘™+ğ‘„ğ‘Ÿğ‘œğ‘¤âŠ™ğ¾ğ‘ƒ
ğ‘£ğ‘ğ‘™+ğ‘„ğ‘ƒ
ğ‘£ğ‘ğ‘™âŠ™ğ¾ğ‘ğ‘œğ‘™
whereâŠ™indicatesdotproduction. ğ›¼ğ‘ğ‘œğ‘œisavectorandcorresponds
tothenon-zerovaluesin Ëœğ›¼(Eq.4),and Ëœğ›¼[ğ¶ğ‘‚ğ‘‚ğ‘Ÿğ‘œğ‘¤[ğ‘–];ğ¶ğ‘‚ğ‘‚ğ‘ğ‘œğ‘™[ğ‘–]]=
ğ›¼ğ‘ğ‘œğ‘œ[ğ‘–].Thecontent-to-positionorposition-to-contentcanbecom-
putedthe sameasin Sparse,and thetotalnumberof gatheropera-
tionsinattentioncomputationis4timesofnon-zeroelementsin
ğ›¿ğ‘…: 2 for gathering the content and 2 for gathering the position.
GatherwithdecomposedCOO(GDC). Toreducethenumber
of gather operations in GC, we can add a matrix decomposition
operationontopofit.First,wedecompose ğ›¿ğ‘…byğ¶ğ‘‚ğ‘‚ğ‘£ğ‘ğ‘™suchthat
eachsub-matrix ğ›¿ğ‘ 
ğ‘…containsonlynode-pairswiththesamerelative
distanceğ‘ . An example is shown in Figure 3, where the original ğ›¿ğ‘…
contains 3 distinct values and we decompose it into 3 sub-matrices
accordingly.Wetransfereachsub-matrix ğ›¿ğ‘ 
ğ‘…intoitsCOOformat
and useğ¶ğ‘‚ğ‘‚ğ‘ to indicates the sub-matrix with ğ‘£ğ‘ğ‘™=ğ‘ . For each
sub-matrix ğ¶ğ‘‚ğ‘‚ğ‘ , we gather content embeddings of nodes by:
ğ‘„ğ‘Ÿğ‘œğ‘¤ğ‘ =ğ‘„(ğ‘¥)[ğ¶ğ‘‚ğ‘‚ğ‘ 
ğ‘Ÿğ‘œğ‘¤;:],ğ¾ğ‘ğ‘œğ‘™ğ‘ =ğ¾(ğ‘¥)[ğ¶ğ‘‚ğ‘‚ğ‘ 
ğ‘ğ‘œğ‘™;:]
whereğ‘„ğ‘Ÿğ‘œğ‘¤ğ‘ indicates the query content ordered by ğ¶ğ‘‚ğ‘‚ğ‘ ğ‘Ÿğ‘œğ‘¤, and
ğ¾ğ‘ğ‘œğ‘™ğ‘ represents the key content ordered by ğ¶ğ‘‚ğ‘‚ğ‘ 
ğ‘ğ‘œğ‘™. The attention
scores can then be computed as:
ğ›¼ğ‘ğ‘œğ‘œğ‘ =(ğ‘„ğ‘Ÿğ‘œğ‘¤ğ‘ +ğ‘„ğ‘ƒ
ğ‘ )âŠ™(ğ¾ğ‘Ÿğ‘œğ‘¤ğ‘ +ğ¾ğ‘ƒ
ğ‘ )âˆ’(ğ‘„ğ‘ƒ
ğ‘ âŠ™ğ¾ğ‘ƒ
ğ‘ )
whereğ›¼ğ‘ğ‘œğ‘œğ‘ corresponds to the attention scores of node pairs in
ğ›¿ğ‘ 
ğ‘…. Note that ğ›¼ğ‘ğ‘œğ‘œğ‘ is a vector of the same shape as ğ¶ğ‘‚ğ‘‚ğ‘ ğ‘Ÿğ‘œğ‘¤.B y
paddingall ğ¶ğ‘‚ğ‘‚ğ‘ tothesamelength,theattentionscorescanbe
computedinparallelandthefinalattentionscoresequaltothesum
of allğ›¼ğ‘ğ‘œğ‘œğ‘ :
ğ›¼ğ‘ğ‘œğ‘œ=2ğ‘ƒ+1/summationdisplay.1
ğ‘ =1ğ›¼ğ‘ğ‘œğ‘œğ‘ There are 3 benefits of this approach compared with GC:
â€¢ğ¾ğ‘ƒandğ‘„ğ‘ƒcanbereused,aseach ğ‘„ğ‘Ÿğ‘œğ‘¤ğ‘ andğ¾ğ‘Ÿğ‘œğ‘¤ğ‘ havethe
samerelativedistance ğ‘ .Thepositionembeddingsof ğ‘ canbe
directly added into the content without gather operations.
â€¢Only a quarter of number of gather operation is needed
(discussed in 5.3).
â€¢Onlyonedotproductionisrequired,asthesecond ğ‘„ğ‘ƒğ‘ âŠ™ğ¾ğ‘ƒğ‘ 
canbereusedandonly (ğ‘„ğ‘Ÿğ‘œğ‘¤ğ‘ +ğ‘„ğ‘ƒğ‘ )âŠ™(ğ¾ğ‘Ÿğ‘œğ‘¤ğ‘ +ğ¾ğ‘ƒğ‘ )needs
to be calculated.
See Appendix A for the complete algorithm.
5 COMPLEXITY ANALYSIS
Inthissection,wewilldiscussthebest,worstandaveragecomplex-
ity of 5 implementations mentioned above. We use FLOPs (floating
point operations) to measure the computational complexity. The
consideredoperationsincludes:matrixmultiplication,matrixdot
production,addandgatheroperationwhicharethemainoperations
involved for the attention computation. FLOPs of these operations
are listed below:
ğ¹ğ¿ğ‘‚ğ‘ƒğ‘ (ğ´+ğµ)=ğ‘(ğ‘šâˆ’1);ğ¹ğ¿ğ‘‚ğ‘ƒğ‘†(ğ´[ğ¶;:])=|ğ¶|âˆ—ğ‘š
ğ¹ğ¿ğ‘‚ğ‘ƒğ‘ (ğ´âŠ™ğµ)=ğ‘ğ‘š2+ğ‘(ğ‘šâˆ’1)
ğ¹ğ¿ğ‘‚ğ‘ƒğ‘ (ğ´Ã—ğµ/intercal)=ğ‘âˆ—ğ¹ğ¿ğ‘‚ğ‘ƒğ‘ (ğ´âŠ™ğµ)(7)
whereğ´andğµaretwomatriceswithshape [ğ‘,ğ‘š],ğ´[ğ¶;:]indicates
gatherğ´withğ¶as the index, |ğ¶|is the number of elements in ğ¶.
Wewillfocusouranalysisonattentionheadsusingtheancestor-
descendentrelationship( ğ´),butsimilarideascanbeusedtoanalyze
the sibling relationship ( ğ‘†) straightforwardly. As the complexity is
related to the number of non-zero elements in ğ›¿ğ´(denoted with
|ğ›¿ğ´>0|).Wefirstanalyzetherangeof |ğ›¿ğ´>0|,thenpresentthe
complexity of each implementation.
5.1 Range of |ğ›¿ğ´>0|
Theorem5.1. Foranydirectedtree ğ‘‡,letE(i)representthenumber
of paths in ğ‘‡with length ğ‘–,ğ¿represent the length of the longest path
inğº, we have:
ğ¸(1)>ğ¸(2)>Â·Â·Â·>ğ¸(ğ¿)
Proof.Assuming there are ğ‘nodes in the tree, and the root
node is at level 1. Define ğ‘ğ‘—as the number of nodes at level ğ‘—. For
each node at level ğ‘—,i fğ‘—âˆ’ğ‘–>0, there exists one path of length
ğ‘–ending with this node, otherwise no such path exists. Hence,
ğ¸(ğ‘–)=ğ‘âˆ’/summationtext.1ğ‘–
ğ‘—=1ğ‘ğ‘—andğ‘ğ‘—>0. Therefore we must have ğ¸(ğ‘–)>
ğ¸(ğ‘–+1). /square
Theorem 5.2. Every tree with ğ‘nodes has exactly ğ‘âˆ’1edges.
Proof.Imaginestartingwith ğ‘isolatednodesandaddingedges
oneatatime.Byaddingoneedge,wewilleither(1)connecttwo
components together, or (2) close a circuit. Since a tree is fully
connectedandhasnocircuit,wemustaddexactly ğ‘âˆ’1edges. /square
Least upper & Greatest lower bound. Letğ¸(0)=ğ‘denote
the number of nodes in a tree. We have |ğ›¿ğ´>0|=ğ¸(0)+2(ğ¸(1)+
ğ¸(2)+...ğ¸(ğ‘ƒ))sinceweconsiderbothpositiveandnegativedis-
tance inğ›¿ğ´. Based on the above two theorems, we can have:
ğ¸(ğ‘–)â‰¤ğ¸(ğ‘–âˆ’1)âˆ’1â‰¤...ğ¸(0)âˆ’ğ‘–=ğ‘âˆ’ğ‘–
154
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:54 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Ze Tang, Xiaoyu Shen, Chuanyi Li, Jidong Ge, Liguo Huang, and Zhelin Zhu, Bin Luo
Figure 4: |ğ›¿ğ´>0|in case of random trees, the abscissa is the max
relative distance ğ‘ƒand the ordinate is the non-zero elements in ğ›¿ğ´
with the unit of ğ‘‚(ğ‘). The coefficient decreases with growing ğ‘ƒ.
|ğ›¿ğ´>0|â‰¤ğ‘+2(ğ‘âˆ’1+ğ‘âˆ’2+...ğ‘âˆ’ğ‘ƒ)=(ğ‘âˆ’ğ‘ƒ)(2ğ‘ƒ+1)
Itistheleastupperboundfortheancestor-descendentrelationship
andisachievedonlywheneachnodehasstrictlyonechildnode.
The greatest lower bound can be achieved when the treeâ€™s depth is
2. In this situation, ğ¸(ğ‘–)=0 forğ‘–â‰¥2 and|ğ›¿ğ´>0|=3ğ‘âˆ’2.
Average. WecanusethePrÃ¼fersequence[ 35]tosimulateran-
domtreessowecanestimatetheaverageof |ğ›¿ğ´>0|withdifferent
treestructures.Thetreesize ğ‘issetintherangeof [50,500]and
theout-degreeofeachnodeisrandomlyselectedfrom1to ğ‘âˆ’1
(controlled by the max value in PrÃ¼fer sequence). We did 1,000
simulation experiments and Figure 4 shows the result.
Theaverage |ğ›¿ğ´>0|whenğ‘ƒissampledfromauniformdistri-
butionin [1,50]is1.16ğ‘ƒğ‘.Wecanseethat thecoefficientinFigure4
graduallydecreases.Forlarger ğ‘ƒ,theaverage |ğ›¿ğ´>0|willbemuch
smaller than the upper bound of (2ğ‘ƒ+1)(ğ‘âˆ’ğ‘ƒ).
5.2 Mask & Loop & Sparse & GC
Maskcontains 1 matrix multiplication with [ğ‘,ğ‘š]Ã—[ğ‘š,ğ‘]in
content-to-content,2matrixmultiplicationwith [ğ‘,ğ‘š]Ã—[ğ‘š,2ğ‘ƒ+1]
and 2 gather operations with index shape [ğ‘,ğ‘]for content-to-
position and position-to-content, and 2 add operations are used
for final score computation. The complexity is (ğ‘2+(2ğ‘ƒ+1)ğ‘)âˆ—
(ğ‘š2+ğ‘šâˆ’1)+2ğ‘2+ğ‘âˆ’1.
LoopAs loop only computes non-zero elements in ğ›¿ğ´, the com-
plexityincludes1dotproductionof |ğ›¿ğ´>0|(ğ‘š2+ğ‘šâˆ’1)and2add
operations |ğ›¿ğ´>0|âˆ—2(ğ‘šâˆ’1),andequalsto |ğ›¿ğ´>0|(ğ‘š2+3ğ‘šâˆ’3).
Sparseâ€™s complexity is same as Maskapart from the gather opera-
tionwithindexshape |ğ›¿ğ´>0|(thetimecomplexityforgathering
sparsetensorasindexequalstothenumberofnon-zeroelementsin
it),whichequalsto (ğ‘2+(2ğ‘ƒ+1)ğ‘)âˆ—(ğ‘š2+ğ‘šâˆ’1)+2|ğ›¿ğ´>0|+ğ‘âˆ’1.
GCThe complexity in GC is all related to |ğ›¿ğ´>0|. It contains 4
gather operations, 3 dot production and 2 add operations, which
leads to the complexity of |ğ›¿ğ´>0|(ğ‘š2+3ğ‘š+4)+2(2ğ‘ƒ+1)ğ‘ğ‘š.
5.3 GDC
There are two implementation details in GDCto optimize the time
andspacecomplexity.Firstly,inatree,if ğ‘ â‰¥ğ‘ƒ+1,thedecomposed
sub-matrix ğ¶ğ‘‚ğ‘‚ğ‘ has at most one non-zero value in each row.
(forexample,eachnon-rootnodehasexactlyoneparentnodein
Figure 3.) We can fix ğ¶ğ‘‚ğ‘‚ğ‘ ğ‘Ÿğ‘œğ‘¤to[0,1,...,ğ‘âˆ’1]and only store
thecorresponding ğ¶ğ‘‚ğ‘‚ğ‘ 
ğ‘ğ‘œğ‘™.Whenğ‘ <ğ‘ƒ+1,astherelationshipis
symmetric, ğ¶ğ‘‚ğ‘‚ğ‘ can be represented with ğ¶ğ‘‚ğ‘‚2ğ‘ƒ+2âˆ’ğ‘ . Based on
this,when ğ‘ â‰¥ğ‘ƒ+1,thequerycontentdoesnotneedtobegathered
   
		

Figure 5: Theoretical complexity with ğ‘ƒ=5,ğ‘š=32. loop has the
lowest complexity but cannot be parallelized in practice.
Table 2: Statistics of Java and Python Datasets
Perspectives Java Python
#of Train instances 69,708 55,538
#of Validation instances 8,714 18,505
#of Test instances 8,714 18,502
Avg. # of tokens in code 120 48
Avg. # of nodes in AST 158 100
Avg. # of tokens in SBT 632 402
Avg. # of tokens in summary 18 9
(asğ¶ğ‘‚ğ‘‚ğ‘ ğ‘Ÿğ‘œğ‘¤isthesameindexofquery),andwhen ğ‘ <ğ‘ƒ+1,thekey
contentdoesnotneedtobegathered.Hence,weonlyneed (2ğ‘ƒ+1)ğ‘
gatheroperationsfromcontent.Secondly,paddingpositionsdonot
need to be computed in dot production as the padding positionsof both
ğ‘„ğ‘Ÿğ‘œğ‘¤ğ‘ andğ¾ğ‘Ÿğ‘œğ‘¤ğ‘ are the same. After adding the position
bias,allğ‘„ğ‘Ÿğ‘œğ‘¤ğ‘ andğ¾ğ‘Ÿğ‘œğ‘¤ğ‘ canbepackedbeforedotproduction,then
unpacked to their original length afterwards. By this way, we only
need to compute related node pairs with onedot production.
In consequence, the complexity of GDCincludes (2ğ‘ƒ+1)ğ‘ğ‘š
gatheroperations,1dotproductionwithshape [|ğ›¿ğ´>0|,ğ‘š]and
3 add operations with shape [|ğ›¿ğ´>0|], which equals to |ğ›¿ğ´>
0|(ğ‘š2+ğ‘šâˆ’1)+(6ğ‘ƒ+3)ğ‘ğ‘š+(2ğ‘ƒ+1)ğ‘.
For better comparison, we also show the theoretical complexity
inFigure5 underthehyper-parametersin ourexperiments.Ascan
be seen,loophas the lowest complexity but cannot be parallelized.
maskandsparsegrow quadratically with the AST size. GDC
slightly outperforms GCand has a complexity close to loop.
6 EXPERIMENTS
In this section, we first explain the experimental setup, evaluation
metrics and baseline approaches, then report the main results and
performablationstudies.Theruntimespeedandmemor ycostof
different implementations are provided for comparison. Finally, we
present a qualitative analysis and discuss the future directions.
6.1 Experimental Setup
Datasets. Experiments are conducted on the two public code sum-
marizationbenchmarks,oneinJava[ 19]andtheotherinPython[ 51].
To ensure the quality of comments, we filter the comments with
lessthan4words,constructors,setters,getters,andtestermethods,
sameasinShidoetal . [41].Whenthecommenthastwoormore
sentences,onlythefirstsentenceiskeptasthedescriptionofthe
155
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:54 UTC from IEEE Xplore.  Restrictions apply. AST-Trans: Code Summarization with Efficient Tree-Structured Attention ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table 3: Comparison of AST-Trans with the baseline methods, categorized based on the input type. * means implemented by ourselves.
Methods InputJava Python
BLEU (%) METEOR (%) ROUGE-L (%) BLEU (%) METEOR (%) ROUGE-L (%)
CODE-NN[20]
Code27.6 12.61 41.10 17.36 09.29 37.81
API+CODE[19] 41.31 23.73 52.25 15.36 08.57 33.65
Dual Model[53] 42.39 25.77 53.61 21.80 11.14 39.45
BaseTrans*[1] 44.58 29.12 53.63 25.77 16.33 38.95
Code-Transformer*[57] 45.74 29.65 54.96 30.93 18.42 43.67
Tree2Seq[11]
AST(Tree)37.88 22.55 51.50 20.07 08.96 35.64
RL+Hybrid2Seq[51] 38.22 22.75 51.91 19.28 09.75 39.34
GCN*[22] 43.94 28.92 55.45 32.31 19.54 39.67
GAT*[50] 44.63 29.19 55.84 32.16 19.30 39.12
Graph-Transformer*[40] 44.68 29.29 54.98 32.55 19.58 39.66
Code2Seq*[4]AST(PD)24.42 15.35 33.95 17.54 08.49 20.93
Code2Seq(Transformer)* 35.08 21.69 42.77 29.79 16.73 40.59
DeepCom[18]
AST(SBT)39.75 23.06 52.67 20.78 09.98 37.35
Transformer(SBT)* 43.37 28.36 52.37 31.33 19.02 44.09
AST-Trans(SBT)* 44.15 29.58 54.73 32.86 19.89 45.92
Transformer(POT)*AST(POT)39.62 26.30 50.63 31.86 19.63 44.73
AST-Trans 48.29 30.94 55.85 34.72 20.71 47.77
Figure 6: Distribution of relative distance ğ‘in training sets
method. Table 2 shows the statistics of the datasets. We also count
the distribution of relative distances in Fig 6. As can be seen, most
ancestor-descendent and sibling relationships are within the range
of 5 and 10 respectively.
Pre-processing. First,wepre-processthesummariesbyremoving
the punctuations. Next, we split multi-words, such as â€œgettable-
types",insummarieswithwordninja3sincetheircorresponding
tokensinthesourcecodearesplittoo[ 53].Wealsosplittheleaf
nodesinASTsintosub-tokensiftheyareinformoftheCamelCase
orsnake_case.Thesplitnodesaretreatedasnewchildrenoftheoriginal parent node. Finally, we reverse the children of the root
nodetopreventtheimportantinformation,suchasfunctionnames
or parameters, from being cut when the size of input AST exceeds
the maximum size allowed.Hyper-parameters.
If not specified, the maximum size of AST
issetto200forallexperiments,andthevocabularysizesofboth
ASTsandcommentsaresetto30,000.Weuse4layersofstacked
encoder-decoder and set the hidden size ğ‘‘=256,ğ‘š=32. For
each attention layer, we set â„ğ´=1 andâ„ğ‘†=7. The max relative
distanceforancestor-descendant/siblingrelationship ğ‘ƒğ´issetto
10/5 respectively.Feed-forward inner-layer dimensionis 2048 and
theactivationfunctionisgelu[ 17].Whiletraining,thebatchsizeis
128 and the maximum epochs is 500. Models are trained using the
3https://github.com/keredson/wordninjaAdamWoptimizer[ 28]withğ‘™ğ‘Ÿ=1ğ‘’âˆ’3,ğ›½1=0.9,ğ›½2=0.999,ğœƒ=1ğ‘’âˆ’
6, label smoothing with ğœƒğ‘™ğ‘ =0.1[46] and dropout probability [ 44]
of 0.2. The patience in the early stopping mechanism [ 32] is set to
20 and we select the model based on the BLEU in the validation set
4.
Evaluation Metrics. We evaluate the performance with corpus
BLEU [33], METEOR [6], and ROUGE-L [27].
The experiments used the GPUs provided by Aliyun, which use
EFLOPS [ 9] architecture and ACCL [ 10]. EFlops architecture im-
proves the scalability and efficiency of commodilty clusters (CoW),
andACCLbringtheperformantefficiencyofEFlopsarchitecture
to general cluster systems and Cloud scenarios.
6.2 Baselines
WecomparetheproposedAST-Transformerwith16baselinemeth-
ods. They can be divided into 5 groups based on the input type :
1: Code. Modelswiththecodeasinput.Ittreatscodeasplain
textanddoesnotleverageASTs. Code-NN [20]usedRNNwhile
BaseTrans [1] used the Transformer. On the basis of Code-NN,
Dual Model [53]usedduallearningtotraincodesummarization
and generation together. API+CODE [19] used multi encoders
to encode code along with the API call sequence. To make upfor the lack of structural information,
Code-Transformer [57]
additionallyaddsfourstructuredistances,includingtwokindsof
distancementionedinSec3.2,tothecodetokensanddoesattention
computation separately for each kind of distance. Differently, it
doesnotdistinguishembeddingsofdifferentrelationsandusessine
and cosine functions to represent distance embeddings.
2:AST(Tree). ModelswiththeASTasinputandencodeitwith
tree-specificencoders.Therearetwomaintypesofsuchencoders.
OneusesTree-LSTM,suchas Tree2Seq [11]andRL+Hybrid2Seq [51].
RL+Hybrid2Seq adds the code information and deep reinforce-
mentfortraining.TheothertreatstheASTasgraphandencodes
4We also report the results with best METEOR and ROUGE-L in the validation set in
Appendix B
156
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:54 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Ze Tang, Xiaoyu Shen, Chuanyi Li, Jidong Ge, Liguo Huang, and Zhelin Zhu, Bin Luo
it with graph neural network (GNN) models. We consider three
kinds of GNN models including GCN[22],GAT[50] andGraph-
Transformer [40]. The edges fed to GNN includes the ancestor-
descendant and sibling edges, distinguished by the edge attributes.
3:AST(PD). ModelswiththeASTlinearizedwithpathdecom-
position as input. Path representation needs to be encoded fromthe nodes, then the whole AST representation is encoded fromthe path representations.
Code2Seq [4] is the first approach us-
ingPD,anditusedtwoLSTMmodelstoencodehierarchicalnet-
works.Forfairnessofcomparison,wealsodesignanewbaseline
Code2Seq(Transformer) by replacing these two LSTM models
with the Transformer.
4: AST(SBT). Models with the AST linearized with Structure-
basedTraversalasinput. DeepCom [18]isthefirstworkthatuses
AST (SBT) as input, which encodes it with LSTM. We design a new
baselineTransformer (SBT) that encodes AST (SBT) with the
Transformer. AST-Trans(SBT) is ourproposed modelthat inputs
SBT with relationship matrices.
5:AST(POT). ModelswiththeASTlinearizedwithpre-order-
traversal as input. Transformer (POT) is the standard Trans-
former architecture with AST (POT) as input and AST-Trans is
our proposed model with tree-structured attention.
All Transformer-based models are based on the relative position
embeddingswith disentangledattentionmentioned inSection3.3
withthe same number of parameters. The same hype-parameters are
used through the way for a fully fair comparison.
6.3 Main Results
The main result of AST-Trans and the baselines are presented inTable 3
5.AST-Trans outperforms all the baselines on all the three
metrics.Specifically,itoutperformsthebestbaselineby3.61,2.17
inBLEU,1.65, 1.08inMETEORand0.87,3.04in ROUGE-Lonthe
Java and Python datasets respectively.
Code vs AST (Tree) vs AST (linearized). Apart from AST-
Trans,onbothtwodatasets, usingGNNstoencodeAST(Tree)achieved
the best results. The reason is that the AST has both structural and
semantic information, and the other two input types both lose part
ofthestructuralinformation.AllthreevariantsofGNNsachieve
similarresultsandoutperformtheTree-LSTMinencodingtheAST
(Tree). Comparedwithtaking thelinearized ASTasinput, models
onlyusing thecodeperformbetter ontheJavadataset butworseon
thePythondataset.Thiscouldberelatedtothecodelength.Ascode
and corresponding ASTs in Python are relatively shorter, encoding
ASTs is more effective than in the Java dataset. Therefore, mod-
els using linearized ASTs, with the help of additional structural
information, are able to outperform models using only the code.
AST(PD) vs AST(SBT) vs AST(POT). Among three lineariza-
tion methods, when using the Transformer encoder/decoders, AST
(SBT) performs the best on the Java dataset and AST (POT) performs
the best on the Python dataset. AST(SBT) and AST(POT) both have
theirownadvantages.AST(SBT)maintainsmorestructuralinfor-
mation than AST(POT) while AST(POT) has the shortest length
5TheresultsofBaseTrans[ 1]inthePythondatasetarelowerthanreportedinthepaper
(-6.75 BLEU, -3.44 METEOR and -7.78 ROUGE), then we set max relative distance ğ‘ƒto
16(kept thesame asoriginalpaper) andget 27.27(-5.25)BLEU,15.90(-3.87) METEOR,
38.58(-8.15)ROUGE-L.Thisreductionmaybebecausethatweadditionallysegment
multi-words in comments.Table 4: Ablation study on AST-Trans with/without ğ´andğ‘†.
Model Dataset BLEU(%) METEOR (%) ROUGE (%)
AST-Trans w/o A
Java47.74 30.21 54.56
AST-Trans w/o S 48.07 30.62 55.29
AST-Trans 48.29 30.94 55.85
AST-Trans w/o A
Python34.35 20.15 46.62
AST-Trans w/o S 34.32 20.28 46.87
AST-Trans 34.72 20.71 47.77
Table 5: Ablation study on â„ğ´andâ„ğ‘†on Java Dataset.
â„ğ´â„ğ‘†BLEU(%) METEOR (%) ROUGE-L (%)
0847.74 30.21 54.56
17 48.29 30.94 55.85
2648.28 30.94 55.64
35 48.25 30.92 55.66
44 48.23 30.96 55.68
5348.11 30.93 55.46
62 48.1 30.74 55.22
71 48.24 30.91 55.57
80 48.07 30.62 55.29
among these three linearization methods. Using the AST (PD) as
inputleadstopoorperformanceonbothdatasets.Therearetwomain
reasons. On the one hand, AST(PD) method was first proposed for
methodnamecompletion.Methodnamesaremuchshorterthanthe
code summaries, and do not include many details. PD linearization
extractsfeaturesfrompaths,whichaggregateshigh-levelcharac-
tersbutignoresthedetailedinformationinthenode.However,codesummarizationrequiresmoredetailedinformationinthecodesuchasthetypeofthereturnvalue,whichisstoredintheleafnodes.Ontheotherhand,Code2Seq(Transformer)usesahierarchicalnetwork
and the amount of trained parameters is much larger. It is thereby
harder to converge than Transformer(SBT) and Transformer(POT).
Impact of relationship matrix ğ‘….We compared the perfor-
manceofthreekindsofinputswithorwithouttherelationmatrix ğ‘…:
Code-Transformer vs BaseTrans, AST-Trans (SBT) vs Transformer
(SBT) and AST-Trans (POT) vs Transformer(POT). Results show
thataddingğ‘…improvesthe performance forall these inputsand AST-
Trans (POT) performs the best. This is because Code-Transformer
ignores non-leaf node information, and AST-Trans (SBT) stores
duplicate information, resulting in too long sequence length. AST-
Trans (POT) maintains a short sequence length without losing
necessary structural or semantic information.
AST-Trans vs GNN. AST-Trans outperforms GNNs, the best-
performed baseline model in both datasets. With the help of rela-tionship matrix, AST-Trans includes additional relative distanceinformation.Nodescanperceiveinformationfromits
ğ‘-distance
neighbors at each layer. For GNN, however, each node needs ğ‘
hopstopropagateinformationfromtheseneighbors.Inaddition,
AST-Transusesmulti-headmechanismtocomputedifferentrela-
tionshipsindifferentheads,whileallrelationships,distinguishedbyedgeattribute,arecalculatedtogetherinGNNs.AST-Transalsouses
extra feed-forward layers and residual connections in the encoder,
which could help improve the model generalization.
157
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:54 UTC from IEEE Xplore.  Restrictions apply. AST-Trans: Code Summarization with Efficient Tree-Structured Attention ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table 6: Ablation study on ğ‘ƒğ´andğ‘ƒğ‘†on Java Dataset.
ğ‘ƒğ´ğ‘ƒğ‘†BLEU(%) METEOR (%) ROUGE-L (%)
0036.34 23.83 45.58
11 46.95 30.33 54.24
51 47.45 30.11 54.28
53 47.82 30.29 54.62
55 48.14 30.77 55.45
10 5 48.29 30.94 55.85
Table 7: Ablation study on the number of layers on Java Dataset.
ğ‘›ğ‘¢ğ‘šBLEU(%) METEOR (%) ROUGE-L (%)
1 46.11 29.36 53.07
2 47.68 30.53 54.97
3 47.41 30.04 54.07
4 48.29 30.94 55.85
5 47.8 30.39 54.61
648.31 30.58 55.09
6.4 Ablation studies
We conducted ablation studies on four hyper-parameters: use of
eachrelationship,numberofheadsusedforancestor-descendant
(â„ğ´) and sibling relationships ( â„ğ‘†), max relative distance ğ‘ƒand the
numberoflayers.Ineverystudy,apartfromthehype-parameter
that needs to be analyzed, we keep the rest settings unchanged.
Use of two relationships . We verified the impact of using
ancestor-descendantorsiblingrelationshipseparatelyinTable4.
Resultsshowthattheperformanceisachievedwhenusingthem
all.However, usingoneoftherelationshipsalonecanalreadyachieve
close results and outperform all previous baselines.
Number of attention heads . We change the number of heads
usedfortheancestor-descendantrelationship â„ğ´from0to8andfix
thetotalnumberofheadsto8.AscanbeseemfromTable5,thebest
performance is obtained with â„ğ´=1 andâ„ğ‘†=7, but there is no
significantdifferenceamongallcombinationsof â„ğ´andâ„ğ‘†.Ev en
when one relationship is missing ( â„ğ´=0o râ„ğ‘†=0), the effects
are still marginal. However, when both relationships are removed
â„ğ´=â„ğ‘†=0, the performance drops a lot. We conjecture that this
phenomenonisrelatedtothecharacteristicsofAST.Knowingabout
one relationship can help the model â€œguess" the other relationship
properly.Forexample,thenodeâ€œCompare"canbethechildnodeof
â€œWhileExpâ€, â€œIFExpâ€or â€œSwitchExpâ€, etc,but when it isthe sibling
ofnodeâ€œCaseâ€,itcanonlybethechildofnodeâ€œSwitchExpâ€.The
information about its parent can be â€œguessed" in attention compu-
tationwithitssiblingâ€œCaseâ€.Similarly,nodeâ€œNameStoreâ€canonly
appear on the left side of a statement, and nodes with the same
parentasitmustbeitsrightsiblings.Messagesofthesesiblingscan
be passed to â€œNameStoreâ€ through their common parent. However,
there are many cases that the â€œguess" will not be successful. For
example, statements ğ‘>ğ‘andğ‘>ğ‘have the same child nodes
and canonly bedistinguished bysibling relationship, whilestate-
mentsğ‘=ğ‘+ğ‘;ğ‘=ğ‘âˆ’ğ‘andğ‘=ğ‘âˆ’ğ‘;ğ‘=ğ‘+ğ‘only differ in
ancestor-descendant relationship. It could be that the testset does
nothaveenoughhardexamplesthatneedthisfine-graineddistinction
or the current metrics are not enough to reflect the difference.
 


	

Figure7: Runtimeandmemorycostoffiveimplementationswith
batch size=16. The cost of the mask implementation is equal to the
standardTransformer,whichgrowsquadraticallywiththeASTsize.
Maxrelative distance Weanalyze theimpactof themaxrela-
tivedistance ğ‘ƒinTable6.AccordingtoTable6,theout-degreeand
depthofmostnodesinASTisin[0,5]and[0,10].Therefore,the
max relative distance of ancestor-descendant ( ğ‘ƒğ´) and sibling rela-
tion(ğ‘ƒğ‘†)areselectedfrom[1,5,10]and[1,3,5]respectively.Results
showthatastherelativedistancegrows,theperformanceimproves
too, suggesting a wider view of nodes in AST relationships is help-
ful. However, the improvement is marginal and even with ğ‘ƒ=1,
the model performance can already outperform all other baselines.
Thismightbeascribedtothemulti-layerstackedencoders.Even
forğ‘ƒ=1, longer-distance nodescan still be attended to indirectly
on upper layers. In practice, ğ‘ƒcan be set as a hyperparameter to
balance the performance-efficiency trade-off.
Number of Layers Finally, we perform ablation study by vary-
ingthenumberoflayers,andtheresultsarepresentedinTable7.
In ourexperiments, weobserve thata deepermodel (morelayers)
performs better, but the improvement saturates after 4 layers.
6.5 Complexity analysis
In Fig 7, We analyzed the rum time and memory usage of different
implementationsmentionedinsection4.Differentfromthetheoret-
ical complexity which analyze the attention computation in isolate,
operationsinGPUcanbecomputedinparallel,andthereareother
factors, e.g. decoder parameters, dependent libraries, vocabulary
embeddingsthatallneedmemoryusage.Therefore,theneedfor
computingattentionscoresisonlyonepartofitandleadstothegap
between Fig 7 and 5, where the difference across implementations
inFig7ismuchlarger.Nevertheless, thetrendstaysthesame.Time
and memory usage of GDCandGCboth scale linearlywith the
AST size, while the cost of MaskandSparsegrowsquadratically.
Even with the batched parallelism in GPUs, the implementation
ofmaskandsparseare still slower than GDCandGCwhile re-
quiring significantly more memory cost. GDCis faster and with
less memory usage than GC. The main reason is that GDCuses
one quarter of gather operations compared with GC.Loopshows
alineargrowthinmemoryusagewithASTsize,butitstimecost
is much higher as it does not support parallel operations. Whenthe AST size grows further, we can expect the difference across
implementations will become larger and larger.
158
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:54 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Ze Tang, Xiaoyu Shen, Chuanyi Li, Jidong Ge, Liguo Huang, and Zhelin Zhu, Bin Luo





	
	
	




Figure 8: Heatmaps of relative position representations. x-axis is
the relative position representation and the y-axis is the relative
positions. The variance for the sibling relation ( ğ‘†) is much larger
than that for the ancestor-descendent relation ( ğ´).
6.6 Visualization and Qualitative Analysis
Visualization. We further visualize the relative position represen-
tationsofancestor-descendant( ğ´)andsibling( ğ‘†)relationshipsin
Fig 8. As can be seen, the variance of relative position embeddings
inğ‘†is much larger than in ğ´. It implies that our model is not sensi-
tivetotherelativedistancebetweenancestoranddescendantnodes,
as the embeddings are almost the same regardless of the positions.
In contrast, the variance for sibling nodes is relatively large, and
the modelcan distinguishthe siblingnodes withdifferent relative
distances.Inaddition,therelativeembeddingsin ğ´aredemarcated
between theupper and lower part, suggestinga clear distinction
betweenancestoranddescendantnodes.Itshowsthatourmodel
pays more attention to direction rather than distance in ğ´. It is likely
thattheexactdistancebetweensiblingnodesaremoreimportant
than that between ancestor-descendant nodes in ASTs.
Qualitativeanalysis. Weprovideacoupleofexamplesforqualita-
tiveanalysisinTable8.ItcanbeobservedthatAST-Transgenerates
theclosestsummarytothereference,andlackof ğ´orğ‘†hurtsthe
qualityofsummarization.Inthefirstcase,thekeyinformationis
the connection between the sibling nodes method call (â€œaddAllâ€)
andparameter(â€œactionsâ€).BothAST-TransandAST-Transw/o ğ´
generates the summary as a batch add operation, while AST-Trans
w/oğ‘†misunderstandsitasâ€œaddsanactionâ€.Onthecontrary,the
meaningofthethirdcaseistogetjobbythetagfirstthendelete
it. The order of execution is controlled by the ancestor-descent
relationship(themethodcallâ€œgetâ€isthechildnodeofâ€œdeleteâ€),and
AST-Transw/o ğ´justignorestheâ€œgetâ€operation.Thesummariesof
AST-Transw/o ğ´andw/oğ‘†arebothcorrectinthesecondcase.The
statements of the second case are relatively simple and ignoring
the order of statements will not affect the function comprehension.
7 THREATS TO VALIDITY
Therearethreemainthreatstothevalidityofourevaluation.Firstly,
many public datasets are proposed to explore code summarization.Table 8: Qualitative examples.
public QuickActionView addActions(Collection <Action> actions){
checkShown();
mActions.addAll(actions);return this;
}
AST-Trans w/o S: adds a sub - action to the menu
AST-Trans w/o A: adds the given actions to the list of actions
AST-Trans: adds a collection of actions to the quick action view
Human Written: adds a collection of actions to the quick action view
public java.lang.Object newInstance() {
Object o =newInstanceImpl();
if(o==null){
throw new InstantiationException();
}
return o;
}
AST-Trans w/o S: creates a new object initialized to the string object
AST-Trans w/o A: returns a new instance of the object class
AST-Trans: returns a new instance of the object
Human Written: creates a new instance of a class
def job_delete_by_tag(tag):
Job.objects.get(tag=tag).delete()
return (job_get_by_tag(tag) isNone)
AST-Trans w/o S: delete a job and return tag
AST-Trans w/o A: delete a job objects
AST-Trans: delete a job based on its tag
Human Written: deletes a job entry based on its tag
We select two widely used ones to evaluate the proposed AST-
Transformer, but they may not be representative of other program-
ming languages. Secondly, to ensure a fair comparison as much as
possible, we build baselines on the top of the same Transformer
architecture.Thearchitectureandhyperparameterchoicemightbe
sub-optimalforcertainapproaches6.Finally,therewillbeacertain
gapbetweentheautomaticevaluationandthemanualevaluation
ofthesummarizationresults.Weselectthreedifferentautomatic
evaluation methods to avoid bias as much as possible.
8 RELATED WORKS
Code Summarization. Most approaches on code summarization
frametheproblemasasequencegenerationtaskanduseanencoder-decoderarchitecture.Theonlydifferencebetweenitandtraditional
machine translation is that programming languages are unam-biguous and follow rigid grammar rules. Most approaches eithertreat the source code as natural language (i.e., a sequence of to-
kens without specified structures), or utilize its structural informa-
tion with the help from ASTs or other parsed forms. To encodethe code sequence, there exist many encoder architectures likeCNN [
3], RNN [20,55] and the Transformer [ 1]. To leverage the
tree-structured AST, tree-based models such as Recursive NN [ 26],
Tree-LSTM [41,51] andTree-Transformer [15,52], areused toen-
codeASTdirectly.Astreeisaspecialkindofgraph,graph-based
approaches[ 2,12,23]canalsobeusedtoencodeASTs.Someworks
alsocombinethecodetokensequencewiththeASTandobserve
improvement[ 23â€“25].OurapproachonlyneedsthelinearizedAST
6Nevertheless, AST-Trans performs best among all reported results on both datasets.
159
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:54 UTC from IEEE Xplore.  Restrictions apply. AST-Trans: Code Summarization with Efficient Tree-Structured Attention ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
andcanbebuiltupontheTransformerarchitecture.Moreimpor-
tantly,itrestrictstheattentionrangeandmakesitpossibletoencode
very long AST sequences.
Tree-basedNeuralNetworks. Theexistingtree-basedneuralnet-
workscanbegroupedintotwocategoriesdependingontheirinputs:
(1) The models that directly take the tree as input [ 15,31,34,47].
These models are strongly coupled with the tree structure, and the
calculation process needs to be performed simultaneously with
the tree traversal. Since trees generally have different shapes by
nature, parallization of training these models is non-trivial. (2) The
modelsthattakethesequence(s)extractedfromthetreeasinput,
suchasthesampledpathsinthetree[ 4,21],thetraversalsequence
with tree positionalembedding [ 42] or the structurebased traver-
sal (SBT) sequence [ 18]. Taking sampled paths as input is with a
certain degree of randomness and instability, and the method of
tree positional embeddingignores the concept ofpaths in the tree
(all nodes, even if not related, will participate in the calculation
together).Ourmethodimprovesfromthesetwomethods,which
can be guaranteed that each node exchanges message on and only
on all paths containing it.
9 CONCLUSION
In this paper, we present AST-Trans which can encode ASTs effec-
tively for code summarization. In AST-Trans, each node only pays
attention to nodes which share the ancestor-descendent or sibling
relationshipswithit.Itbringstwobenefits:(1)themodelisgiven
aninductivebiasandwillnotgetlostintheoverlongASTsequence,
and (2) it can reduce the computational complexity from quadratic
tolinear.Thelattermakesitpossibletoencodelongcodesequence,
e.g., a whole file, which is prohibitively expensive for standard
Transformers.Weconductcomprehensiveexperiments,showing
that AST-Trans achieve SOTA results on two popular benchmarks
while significantly reducing the computational cost.
We believe the basic idea of AST-Trans can also be applied in
otherstructureddatalikedatadependenceandcontrolflowgraphs.
Thecodeismadepubliclyavailabletobenefittherelevantresearch.
In future work, we plan to improve AST-Trans by incorporating
more features of the code snippet, such as API sequence and node
type, into the self-attention mechanism.
10 ACKNOWLEDGMENTS
This work is supported by National Natural Science Foundation of
China (61802167,61802095) ,Natural Science Foundation of Jiangsu
Province (No.BK20201250),CooperationFund of Huawei-NJUCre-
ativeLaboratoryfortheNextProgramming,andNSFaward2034508.
WethankAlibabaCloudforitshigh-efficientAIcomputingservice
from EFlops Cluster. We also thank the reviewers for their help-ful comments. Chuanyi Li and Jidong Ge are the corresponding
authors.
REFERENCES
[1]WasiUddinAhmad,SaikatChakraborty,BaishakhiRay,andKai-WeiChang.2020.
A Transformer-based Approach for Source Code Summarization. In Proceedings
of the 58th Annual Meeting of the Association for Computational Linguistics, ACL
2020, Online, July 5-10, 2020, Dan Jurafsky, Joyce Chai, Natalie Schluter, and
Joel R. Tetreault (Eds.). Association for Computational Linguistics, 4998â€“5007.
https://doi.org/10.18653/v1/2020.acl-main.449[2]MiltiadisAllamanis,MarcBrockschmidt,andMahmoudKhademi.2018. Learning
to Represent Programs with Graphs. In 6th International Conference on Learning
Representations,ICLR2018,Vancouver,BC,Canada,April30-May3,2018,Con-
ferenceTrackProceedings.OpenReview.net. https://openreview.net/forum?id=
BJOFETxR-
[3]Miltiadis Allamanis, Hao Peng, and Charles Sutton. 2016. A Convolutional
AttentionNetworkforExtremeSummarizationofSourceCode.In Proceedingsof
the33ndInternationalConferenceonMachineLearning,ICML2016,NewYorkCity,
NY,USA,June19-24,2016 (JMLRWorkshopandConferenceProceedings,Vol.48),
Maria-Florina Balcan and Kilian Q. Weinberger (Eds.). JMLR.org, 2091â€“2100.
http://proceedings.mlr.press/v48/allamanis16.html
[4]UriAlon,ShakedBrody,OmerLevy,andEranYahav.2019. code2seq:Generating
Sequences from Structured Representations of Code. In 7th International Con-
ferenceonLearningRepresentations,ICLR2019,NewOrleans,LA,USA,May6-9,
2019. OpenReview.net. https://openreview.net/forum?id=H1gKYo09tX
[5]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine
Translation by Jointly Learning to Align and Translate. In 3rd International
Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May
7-9,2015,ConferenceTrackProceedings,YoshuaBengioandYannLeCun(Eds.).
http://arxiv.org/abs/1409.0473
[6]SatanjeevBanerjeeandAlonLavie.2005. METEOR:AnAutomaticMetricforMT
Evaluation with Improved Correlation with Human Judgments. In Proceedings of
the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Transla-
tion and/or Summarization@ACL 2005, Ann Arbor, Michigan, USA, June 29, 2005,
JadeGoldstein, AlonLavie,Chin-YewLin,and ClareR.Voss (Eds.).Association
for Computational Linguistics, 65â€“72. https://www.aclweb.org/anthology/W05-
0909/
[7]Iz Beltagy, Matthew E. Peters, andArman Cohan. 2020. Longformer: The Long-
Document Transformer. CoRRabs/2004.05150 (2020). arXiv:2004.05150 https:
//arxiv.org/abs/2004.05150
[8]ErnieChang,XiaoyuShen,Hui-SyuanYeh,andVeraDemberg.2021. OnTraining
Instance Selection for Few-Shot Neural Text Generation. In Proceedings of the
59th Annual Meeting of the Association for Computational Linguistics and the 11th
InternationalJointConferenceonNaturalLanguageProcessing(Volume2:Short
Papers). 8â€“13.
[9]JianboDong,ZhengCao,TaoZhang,JianxiYe,ShaochuangWang,FeiFeng,Li
Zhao,XiaoyongLiu,LiuyihanSong,LiweiPeng,etal .2020. Eflops:Algorithm
andsystemco-designforahighperformancedistributedtrainingplatform.In
2020IEEEInternationalSymposiumonHighPerformanceComputerArchitecture
(HPCA). IEEE, 610â€“622.
[10] Jianbo Dong,Shaochuang Wang,Fei Feng,Zheng Cao,Heng Pan,Lingbo Tang,
PengchengLi,HaoLi,QianyuanRan,YiqunGuo,etal .2021. ACCL:Architecting
HighlyScalableDistributedTrainingSystemswithHighly-EfficientCollective
Communication Library. IEEE Micro (2021).
[11]Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa Tsuruoka. 2016. Tree-to-
Sequence Attentional Neural Machine Translation. In Proceedings of the 54th
AnnualMeetingoftheAssociationforComputationalLinguistics,ACL2016,August7-12,2016,Berlin,Germany,Volume1:LongPapers.TheAssociationforComputer
Linguistics. https://doi.org/10.18653/v1/p16-1078
[12]Patrick Fernandes, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Struc-tured Neural Summarization. In 7th International Conference on Learning Rep-
resentations,ICLR2019,NewOrleans,LA,USA,May6-9,2019.OpenReview.net.
https://openreview.net/forum?id=H1ersoRqtm
[13]Sonia Haiduc, Jairo Aponte, and Andrian Marcus. 2010. Supporting programcomprehension with source code summarization. In Proceedings of the 32nd
ACM/IEEEInternationalConferenceonSoftwareEngineering-Volume2,ICSE2010,
Cape Town, South Africa, 1-8 May 2010, Jeff Kramer, Judith Bishop, Premkumar T.
Devanbu,andSebastiÃ¡nUchitel(Eds.).ACM,223â€“226. https://doi.org/10.1145/
1810295.1810335
[14]SoniaHaiduc,JairoAponte,LauraMoreno,andAndrianMarcus.2010. Onthe
UseofAutomatedTextSummarizationTechniquesforSummarizingSourceCode.
In17th Working Conference on Reverse Engineering, WCRE 2010, 13-16 October
2010,Beverly,MA,USA,GiulianoAntoniol,MartinPinzger,andElliotJ.Chikofsky
(Eds.). IEEE Computer Society, 35â€“44. https://doi.org/10.1109/WCRE.2010.13
[15]Jacob Harer, Christopher P. Reale, and Peter Chin. 2019. Tree-Transformer:
A Transformer-Based Method for Correction of Tree-Structured Data. CoRR
abs/1908.00449 (2019). arXiv:1908.00449 http://arxiv.org/abs/1908.00449
[16]PengchengHe,XiaodongLiu,JianfengGao,andWeizhuChen.2021. Deberta:
decoding-Enhanced Bert with Disentangled Attention. In 9th International Con-
ferenceonLearningRepresentations,ICLR2021,VirtualEvent,Austria,May3-7,
2021. OpenReview.net. https://openreview.net/forum?id=XPZIaotutsD
[17]Dan Hendrycks and Kevin Gimpel. 2016. Bridging Nonlinearities and Stochastic
Regularizers with Gaussian Error Linear Units. CoRRabs/1606.08415 (2016).
arXiv:1606.08415 http://arxiv.org/abs/1606.08415
[18]Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018. Deep code comment
generation.In Proceedingsofthe26thConferenceonProgramComprehension,ICPC
2018, Gothenburg, Sweden, May 27-28, 2018, Foutse Khomh, Chanchal K. Roy, and
Janet Siegmund (Eds.). ACM, 200â€“210. https://doi.org/10.1145/3196321.3196334
160
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:54 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Ze Tang, Xiaoyu Shen, Chuanyi Li, Jidong Ge, Liguo Huang, and Zhelin Zhu, Bin Luo
[19]Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi Jin. 2018. Summarizing
Source Code with Transferred API Knowledge. In Proceedings of the Twenty-
SeventhInternationalJointConferenceonArtificialIntelligence,IJCAI2018,July
13-19,2018,Stockholm,Sweden,JÃ©rÃ´meLang(Ed.).ijcai.org,2269â€“2275. https:
//doi.org/10.24963/ijcai.2018/314
[20]Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016.
Summarizing Source Code using a Neural Attention Model. In Proceedings of the
54thAnnualMeetingoftheAssociationforComputationalLinguistics,ACL2016,
August7-12,2016,Berlin,Germany,Volume1:LongPapers.TheAssociationfor
Computer Linguistics. https://doi.org/10.18653/v1/p16-1195
[21]Seohyun Kim, Jinman Zhao, Yuchi Tian, and Satish Chandra. 2021. Code Predic-
tionbyFeedingTreestoTransformers.In 43rdIEEE/ACMInternationalConference
onSoftwareEngineering,ICSE2021,Madrid,Spain,22-30May2021.IEEE,150â€“162.
https://doi.org/10.1109/ICSE43902.2021.00026
[22]Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification withGraph Convolutional Networks. In 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track
Proceedings. OpenReview.net. https://openreview.net/forum?id=SJU4ayYgl
[23]AlexanderLeClair,SakibHaque,LingfeiWu,andCollinMcMillan.2020.ImprovedCodeSummarizationviaaGraphNeuralNetwork.In ICPCâ€™20:28thInternational
Conference on Program Comprehension, Seoul, Republic of Korea, July 13-15, 2020.
ACM, 184â€“195. https://doi.org/10.1145/3387904.3389268
[24]Alexander LeClair, Siyuan Jiang, and Collin McMillan. 2019. A neural model for
generatingnaturallanguagesummariesofprogramsubroutines.In Proceedings
of the 41st International Conference on Software Engineering, ICSE 2019, Montreal,
QC,Canada,May25-31,2019,JoanneM.Atlee,TevfikBultan,andJonWhittle
(Eds.). IEEE / ACM, 795â€“806. https://doi.org/10.1109/ICSE.2019.00087
[25]Boao Li, Meng Yan, Xin Xia, Xing Hu, Ge Li, and David Lo. 2020. DeepCom-
menter:adeepcodecommentgenerationtoolwithhybridlexicalandsyntactical
information. In ESEC/FSE â€™20: 28th ACM Joint European Software Engineering
ConferenceandSymposiumontheFoundationsofSoftwareEngineering,Virtual
Event,USA,November8-13,2020,PremDevanbu,MyraB.Cohen,andThomas
Zimmermann (Eds.). ACM, 1571â€“1575. https://doi.org/10.1145/3368089.3417926
[26]YudingLiangandKennyQiliZhu.2018.AutomaticGenerationofTextDescriptive
Comments for Code Blocks. In Proceedings of the Thirty-Second AAAI Conference
onArtificialIntelligence,(AAAI-18),the30thinnovativeApplicationsofArtificial
Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advancesin Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7,
2018,SheilaA.McIlraithandKilianQ.Weinberger(Eds.).AAAIPress,5229â€“5236.
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16492
[27]Chin-YewLin.2004. ROUGE:APackageforAutomaticEvaluationofSummaries.
InText Summarization Branches Out. Association for Computational Linguistics,
Barcelona, Spain, 74â€“81. https://www.aclweb.org/anthology/W04-1013
[28]IlyaLoshchilovandFrankHutter.2019. DecoupledWeightDecayRegularization.
In7thInternationalConferenceonLearningRepresentations,ICLR2019,NewOr-
leans, LA,USA, May6-9, 2019. OpenReview.net. https://openreview.net/forum?
id=Bkg6RiCqY7
[29]Paul W. McBurney and Collin McMillan. 2016. Automatic Source Code Sum-
marizationofContextforJavaMethods. IEEETrans.SoftwareEng. 42,2(2016),
103â€“119. https://doi.org/10.1109/TSE.2015.2465386
[30]LauraMoreno,JairoAponte,GiriprasadSridhara,AndrianMarcus,LoriL.Pollock,andK.Vijay-Shanker.2013. Automaticgenerationofnaturallanguagesummaries
for Java classes. In IEEE 21st International Conference on Program Comprehension,
ICPC 2013, San Francisco, CA, USA, 20-21 May, 2013. IEEE Computer Society,
23â€“32. https://doi.org/10.1109/ICPC.2013.6613830
[31]LiliMou,GeLi,LuZhang,TaoWang,andZhiJin.2016. ConvolutionalNeural
NetworksoverTreeStructuresforProgrammingLanguageProcessing.In Pro-
ceedingsoftheThirtiethAAAIConferenceonArtificialIntelligence,February12-17,
2016, Phoenix, Arizona, USA, Dale Schuurmans and Michael P. Wellman (Eds.).
AAAI Press, 1287â€“1293. http://www.aaai.org/ocs/index.php/AAAI/AAAI16/
paper/view/11775
[32]Genevieve B. Orr and Klaus-Robert MÃ¼ller (Eds.). 1998. Neural Networks: Tricks
of the Trade. Lecture Notes in Computer Science, Vol. 1524. Springer. https:
//doi.org/10.1007/3-540-49430-8
[33]Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a
Method forAutomatic Evaluation ofMachine Translation.In Proceedings ofthe
40thAnnualMeetingoftheAssociationforComputationalLinguistics,July6-12,
2002, Philadelphia, PA, USA. ACL, 311â€“318. https://www.aclweb.org/anthology/
P02-1040/
[34]JordanB.Pollack.1990. RecursiveDistributedRepresentations. Artif.Intell. 46,
1-2 (1990), 77â€“105. https://doi.org/10.1016/0004-3702(90)90005-K
[35]Heinz PrÃ¼fer.1918. Neuerbeweis eines satzesÃ¼ber permutationen. Arch. Math.
Phys27, 1918 (1918), 742â€“744.
[36]Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-Attention withRelative Position Representations. In Proceedings of the 2018 Conference of the
North American Chapter of the Association for Computational Linguistics: Human
LanguageTechnologies,NAACL-HLT,NewOrleans,Louisiana,USA,June1-6,2018,
Volume 2(ShortPapers),MarilynA.Walker,HengJi,andAmandaStent (Eds.).AssociationforComputationalLinguistics,464â€“468. https://doi.org/10.18653/
v1/n18-2074
[37]Xiaoyu Shen, Youssef Oualil, Clayton Greenberg, Mittul Singh, and Dietrich
Klakow.2017. EstimationofGapBetweenCurrentLanguageModelsandHuman
Performance. Proc.Inters peech2017 (2017), 553â€“557.
[38]Xiaoyu Shen, Jun Suzuki, Kentaro Inui, Hui Su, Dietrich Klakow, and SatoshiSekine. 2019. Select and Attend: Towards Controllable Content Selection in
TextGeneration.In Proceedingsofthe2019ConferenceonEmpiricalMethodsin
NaturalLanguageProcessingandthe9thInternationalJointConferenceonNatural
Language Processing (EMNLP-IJCNLP). 579â€“590.
[39]Xiaoyu Shen, Yang Zhao, Hui Su, and Dietrich Klakow. 2019. Improving la-
tentalignmentintextsummarization bygeneralizingthepointergenerator.In
Proceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguagePro-
cessing and the 9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP). 3753â€“3764.
[40]YunshengShi,ZhengjieHuang,ShikunFeng,HuiZhong,WenjingWang,and
Yu Sun. 2021. Masked Label Prediction: Unified Message Passing Model for
Semi-Supervised Classification. In Proceedings of the Thirtieth International Joint
Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada,
19-27August2021,Zhi-HuaZhou(Ed.).ijcai.org,1548â€“1554. https://doi.org/10.
24963/ijcai.2021/214
[41]YusukeShido,YasuakiKobayashi,AkihiroYamamoto,AtsushiMiyamoto,and
Tadayuki Matsumura. 2019. Automatic Source Code Summarization with Ex-
tended Tree-LSTM. In International Joint Conference on Neural Networks, IJCNN
2019 Budapest, Hungary, July 14-19, 2019. IEEE, 1â€“8. https://doi.org/10.1109/
IJCNN.2019.8851751
[42]VighneshLeonardoShivandChrisQuirk.2019. Novelpositionalencodingsto
enable tree-based transformers. In Advances in Neural Information Processing
Systems 32: Annual Conference on Neural Information Processing Systems 2019,NeurIPS2019,December8-14,2019,Vancouver,BC,Canada ,HannaM.Wallach,
Hugo Larochelle, Alina Beygelzimer, Florence dâ€™AlchÃ©-Buc, Emily B. Fox, and
Roman Garnett (Eds.). 12058â€“12068.
[43]Giriprasad Sridhara, Emily Hill, Divya Muppaneni, Lori L. Pollock, and K. Vijay-
Shanker.2010. TowardsautomaticallygeneratingsummarycommentsforJava
methods. In ASE 2010, 25th IEEE/ACM International Conference on Automated
Software Engineering, Antwerp, Belgium, September 20-24, 2010, Charles Pecheur,
JamieAndrews,andElisabettaDiNitto (Eds.).ACM,43â€“52. https://doi.org/10.
1145/1858996.1859006
[44]NitishSrivastava,GeoffreyE.Hinton,AlexKrizhevsky,IlyaSutskever,andRuslan
Salakhutdinov.2014. Dropout:asimplewaytopreventneuralnetworksfrom
overfitting. J.Mach.Learn.Res. 15,1(2014),1929â€“1958. http://dl.acm.org/citation.
cfm?id=2670313
[45]Hui Su, Xiaoyu Shen, Zhou Xiao, Zheng Zhang, Ernie Chang, Cheng Zhang,
ChengNiu,andJieZhou.2020. Moviechats:Chatlikehumansinacloseddomain.
InProceedings of the 2020 Conference on Empirical Methods in Natural Language
Processing (EMNLP). 6605â€“6619.
[46]Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbig-
niew Wojna. 2016. Rethinking the Inception Architecture for Computer Vi-
sion. In2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR
2016,LasVegas,NV,USA,June27-30,2016.IEEEComputerSociety,2818â€“2826.
https://doi.org/10.1109/CVPR.2016.308
[47]Kai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved
Semantic Representations From Tree-Structured Long Short-Term Memory Net-
works. In Proceedings of the 53rd Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint Conference on Natural Language
Processing of the Asian Federation of Natural Language Processing, ACL 2015, July
26-31, 2015, Beijing, China, Volume 1: Long Papers. The Association for Computer
Linguistics, 1556â€“1566. https://doi.org/10.3115/v1/p15-1150
[48]ZeTang,ChuanyiLi,JidongGe,XiaoyuShen,ZhelingZhu,andBinLuo.2021.
AST-Transformer: Encoding Abstract Syntax Trees Efficiently for Code Summa-
rization.arXiv preprint arXiv:2112.01184 (2021).
[49]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All
y o uN eed .I nAdvances in Neural Information Processing Systems 30: Annual Con-
ferenceonNeural InformationProcessingSystems2017,4-9 December2017,Long
Beach, CA, USA, Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M.
Wallach,RobFergus,S.V.N.Vishwanathan,andRomanGarnett(Eds.).5998â€“6008.
http://papers.nips.cc/paper/7181-attention-is-all-you-need
[50]Petar Velickovic,Guillem Cucurull,Arantxa Casanova,Adriana Romero,Pietro
LiÃ²,andYoshuaBengio.2018. GraphAttentionNetworks.In 6thInternational
ConferenceonLearningRepresentations,ICLR2018,Vancouver,BC,Canada,April30
-May3,2018,ConferenceTrackProceedings.OpenReview.net. https://openreview.
net/forum?id=rJXMpikCZ
[51]Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, andPhilip S. Yu. 2018. Improving automatic source code summarization via deep
reinforcementlearning.In Proceedingsofthe33rdACM/IEEEInternationalConfer-
enceonAutomatedSoftwareEngineering,ASE2018,Montpellier,France,September
3-7, 2018, Marianne Huchard, Christian KÃ¤stner, and Gordon Fraser (Eds.). ACM,
161
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:54 UTC from IEEE Xplore.  Restrictions apply. AST-Trans: Code Summarization with Efficient Tree-Structured Attention ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
397â€“407. https://doi.org/10.1145/3238147.3238206
[52]WenhuaWang,YuqunZhang,ZhengranZeng,andGuandongXu.2020. TranSË†3:
ATransformer-basedFrameworkforUnifyingCodeSummarizationandCode
Search.CoRRabs/2003.03238(2020). arXiv:2003.03238 https://arxiv.org/abs/2003.
03238
[53]Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin. 2019. Code Generation as a
Dual Task of Code Summarization. In Advances in Neural Information Processing
Systems 32: Annual Conference on Neural Information Processing Systems 2019,
NeurIPS2019,December8-14,2019,Vancouver,BC,Canada,HannaM.Wallach,Hugo Larochelle, Alina Beygelzimer, Florence dâ€™AlchÃ©-Buc, Emily B. Fox, and
Roman Garnett (Eds.). 6559â€“6569.
[54]Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris
Alberti, Santiago OntaÃ±Ã³n, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang,
and Amr Ahmed. 2020. Big Bird: Transformers for Longer Sequences. In Ad-
vances in Neural Information Processing Systems 33: Annual Conference on Neural
InformationProcessingSystems2020,NeurIPS2020,December6-12,2020,virtual ,
Hugo Larochelle, Marcâ€™Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan,and Hsuan-Tien Lin (Eds.). https://proceedings.neurips.cc/paper/2020/hash/
c8512d142a2d849725f31a9a7a361ab9-Abstract.html
[55]YangZhao,XiaoyuShen,WeiBi,andAkikoAizawa.2019. Unsupervisedrewriter
for multi-sentence compression. In Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics. 2235â€“2240.
[56]Yuxiang Zhu and Minxue Pan. 2019. Automatic Code Summarization: A Sys-tematic Literature Review. CoRRabs/1909.04352 (2019). arXiv:1909.04352
http://arxiv.org/abs/1909.04352
[57]Daniel ZÃ¼gner, Tobias Kirschstein, Michele Catasta, Jure Leskovec, and Stephan
GÃ¼nnemann. 2021. Language-Agnostic Representation Learning of Source Code
from Structure and Context. In 9th International Conference on Learning Rep-
resentations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.
https://openreview.net/forum?id=Xh5eMZVONGF
A ALGORITHM OF GDC
Algorithm 1 Self-Attention with Relationship matrix
Input:Hiddenstate ğ‘¯,COOformatofrelationshipmartix ğ¶ğ‘‚ğ‘‚,
contentfunctions ğ‘¸,ğ‘²,ğ‘½,relativedistanceprojectionmatrix
ğ‘¸ğ‘·,ğ‘²ğ‘·,ğ‘½ğ‘·.
1:ğ‘²ğ’„=ğ‘²(ğ»),ğ‘¸ğ’„=ğ‘¸(ğ»),ğ‘½ğ’„=ğ‘½(ğ»)
2:forğ‘–=0,...,2ğ‘ƒ+1do
3:forğ‘—=0,...,ğ‘âˆ’1do
4: Ëœğ‘¸ğ’„[ğ‘–;ğ‘—;:]=ğ‘¸ğ’„[ğ‘ªğ‘¶ğ‘¶ğ’„ğ’ğ’[ğ‘–âˆ—ğ‘+ğ‘—];:]
5: Ëœğ‘²ğ’„[ğ‘–;ğ‘—;:]=ğ‘²ğ’„[ğ‘ªğ‘¶ğ‘¶ğ’“ğ’ğ’˜[ğ‘–âˆ—ğ‘+ğ‘—];:]
6: Ëœğ‘½ğ’„[ğ‘–;ğ‘—;:]=ğ‘½ğ’„[ğ‘ªğ‘¶ğ‘¶ğ’“ğ’ğ’˜[ğ‘–âˆ—ğ‘+ğ‘—];:]
7:end for
8:end for
9:Ëœğœ¶=(ğ‘¸ğ’„+ğ‘¸ğ‘·)âŠ™(ğ‘²ğ’„+ğ‘²ğ‘·)âˆ’ğ‘¸ğ‘·âŠ™ğ‘²ğ‘·
10:Ëœğœ¶=exp(Ëœğœ¶âˆš
3ğ‘‘)
11:forğ‘–=0,...,2ğ‘ƒ+1do
12:forğ‘—=0,...,ğ‘âˆ’1do
13: Ëœğœ¶ğ’”ğ’–ğ’[:;ğ‘ªğ‘¶ğ‘¶ğ’“ğ’ğ’˜[ğ‘–âˆ—ğ‘+ğ‘—]]+=Ëœğœ¶[ğ‘–,ğ‘—]
14:end for
15:end for
16:Ëœğœ¶=Ëœğœ¶
Ëœğœ¶ğ’”ğ’–ğ’
17:forğ‘–=0,...,2ğ‘ƒ+1do
18:forğ‘—=0,...,ğ‘âˆ’1do
19: Ëœğ’[ğ‘ªğ‘¶ğ‘¶ğ’“ğ’ğ’˜[ğ‘–âˆ—ğ‘+ğ‘—];:]=(Ëœğ‘½ğ’„[ğ‘–;ğ‘—;:]+ğ‘½ğ‘·[ğ‘–;:])Â·Ëœğœ¶[ğ‘–,ğ‘—]
20:end for
21:end for
Output: Ëœğ’
For better re-implementation, we also show the algorithm of
GDC. line 1-10 describes the attention score computation process.
Ëœğ‘„ğ‘,Ëœğ¾ğ‘andËœğ‘‰ğ‘arereshapedto [2ğ‘ƒ+1,ğ‘,ğ‘‘].NotethattheattentionTable 9: Comparison of AST-Trans with different model selection
strategy on Java Dataset.
Model BLEU METEOR ROUGE-L
AST-Trans(best_eval_BLEU) 48.29 30.94 55.85
AST-Trans(best_eval_METEOR) 47.02 31.90 55.72
AST-Trans(best_eval_ROUGE-L) 46.92 29.99 57.01
scoresËœğ›¼have a different shape with traditional attention scores,
so we redesigned the softmax function in line 11-16. The atten-
tion scores belonging to the same query vector, distinguished by
ğ¶ğ‘‚ğ‘‚ğ‘Ÿğ‘œğ‘¤[ğ‘–âˆ—ğ‘+ğ‘—], are added together as Ëœğ›¼ğ‘ ğ‘¢ğ‘š. Then the softmax
functioncanbeformedas Ëœğ›¼divideby Ëœğ›¼ğ‘ ğ‘¢ğ‘š.Finallyinline17-21,
relative distance bias ğ‘‰ğ‘ƒis added to the value context, and then is
multiplied with the attention scores Ëœğ›¼.
B THE INFLUENCE OF MODEL SELECTION
STRATEGY
Theresultsreportedinthepapercomefromthemodelwithbest
BLEU score in the validation dataset. We then separately selecttwo other models with the best METEOR, and ROUGE-L score
inthevaliddataset,andthenevaluatetheirperformancesontest
dataset.ResultsinTable9showthatthemodelselectionstrategy
indeed influences the performance. This may explain why that the
improvement of AST-Trans is inconsistent in different metrics.
162
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:54 UTC from IEEE Xplore.  Restrictions apply. 