UI Test Migration Across Mobile Platforms
Saghar Talebipour
University of Southern California
Los Angeles, USA
talebipo@usc.eduYixue Zhao
University of Massachusetts Amherst
Amherst, USA
yixuezhao@cs.umass.eduLuka Dojcilovi ¬¥c
University of Southern California
Los Angeles, USA
dojcilov@usc.edu
Chenggang Li
University of Southern California
Los Angeles, USA
chenggan@usc.eduNenad Medvidovi ¬¥c
University of Southern California
Los Angeles, USA
neno@usc.edu
Abstract ‚ÄîWriting UI tests manually requires signiÔ¨Åcant effort.
Several approaches have tried to address this problem in mobile
apps: by exploiting the similarities of different apps within thesame domain on a single platform, they have shown that itis possible to transfer tests that exercise similar functionalitybetween the apps. A related recent technique enables transfer ofUI tests uni-directionally, from an open-source iOS app to thesame app implemented for Android. This paper presents MAPIT,a technique that expands existing work in three important ways:
(1) it enables bi-directional UI test transfer between pairs of
‚Äúsibling‚Äù Android and iOS apps; (2) it does not assume that the
apps‚Äô source code is available; (3) it is capable of transferringtests containing oracles in addition to UI events. MAPIT runsexisting tests on a ‚Äúsource‚Äù app and builds a partial modelof the app corresponding to each test. The model comprisesthe app‚Äôs screenshots, obtainable properties of each screenshot‚Äôsconstituent elements, and labeled transitions between the screen-shots. MAPIT uses this model to determine the correspondinginformation on the ‚Äútarget‚Äù app and generates an equivalent test,
via a novel approach that leverages computer vision and NLP .
Our evaluation on a diverse set of widely used, closed-sourcesibling Android and iOS apps shows that MAPIT is feasible,accurate, and useful in transferring UI tests across platforms.
I. I NTRODUCTION
Writing UI tests manually requires signiÔ¨Åcant effort. This is
an especially acute problem on mobile platforms given their
rapid app-development lifecycle. A popular alternative is to
automatically generate the UI tests, e.g., by relying on model-
based or random testing [ 1], [2]. While these approaches have
been shown effective for generating UI tests with high code
coverage, they cannot generate usage-based tests that target an
app‚Äôs speciÔ¨Åc functionality [ 3], such as login, sign-up, make
reservation, etc. It has been shown that such usage-based tests
are highly valuable to developers and testers [3], [4].
Recent work has demonstrated the possibility of generating
usage-based tests through test reuse across apps within a
single domain (e.g., news, shopping, etc.) [ 5], [3], [6], [7],
[8], [9], [10]. Guided by this insight, prior work has primarily
focused on transferring existing usage-based tests to a new
app within the same domain on a single platform [5], [6], [8],
[7], [3], [10]. SpeciÔ¨Åcally, these approaches leverage existing
usage-based tests from a source Android app to automatically
generate equivalent tests for a target Android app.A largely unexplored variant of this problem is transferring
tests written for an app implemented on one platform (e.g.,
Android), to the same app implemented for another platform
(e.g., iOS). We refer to such pairs of apps as sibling apps.
Cross-platform transfer has unique challenges as compared to
test transfer within Android alone. First, different platforms
employ different technologies, such as various app-development
languages and frameworks, which add signiÔ¨Åcant complexity
to the problem. Second, iOS is a closed-source platform, whichhas led to fewer and more limited tools for analyzing iOS apps
compared to Android. Finally, most iOS apps themselves are
closed-source, making any code-based analysis impossible.
The closest attempt at this problem is TestMig [ 11], which
has addressed uni-directional test transfer for sibling iOS and
Android apps. TestMig has three important limitations. First,
it assumes the availability of both the Android and iOS apps‚Äô
source code, which, as mentioned, is especially unlikely foriOS apps. Second, TestMig only covers transferring tests in
one direction (iOS to Android). Supporting the other direction
(Android to iOS) is inherently challenging: unlike Android, for
which many open-source reverse engineering tools are readily
available (e.g., bytecode decompilers, Soot [ 12], Gator [ 13]),
iOS is a closed platform with a smaller developer base and
static analyses that existing test transfer techniques rely on arenot an option. Third, TestMig only targets UI events, but cannot
transfer test oracles or system events. Oracles are responsible
for evaluating the outcomes of tests and are therefore an
essential part of usage-based testing. Inability to migrate system
events additionally limits the set of test cases that can be
transferred. For instance, navigating to the previous screen is
an Andriod system event that frequently occurs in UI tests.
To address these limitations, we have developed MAPIT,
a novel approach for bi-directional transfer of usage-based
tests across different mobile platforms, with no source code
required on either platform. MAPIT is also the Ô¨Årst approach
capable of transferring oracle events and system events across
mobile platforms. SpeciÔ¨Åcally, MAPIT takes as input (1) the
binaries of the sibling apps-under-test implemented for both
iOS and Android as well as the (2) pre-existing tests for one
of these platforms, and automatically generates equivalent
7562021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000722021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ¬©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678643
978-1-6654-0337-5/21/$31.00  ¬©2021  IEEE
tests for the other platform. The transfer process comprises
two major phases. First, MAPIT dynamically extracts a GUI
model of the app on the source platform while executing the
source test. The GUI model contains the app screen bitmaps,
information regarding the relevant widgets contained on each
screen (e.g., widget images and descriptive attributes), and the
events that cause transitions from one app screen to the next.
Second, based on this GUI model, the source test is migrated
to the target platform by mapping the GUI widgets from the
source app to the most similar widgets in the sibling app. Thisis done using a novel approach that combines computer vision
and NLP techniques, and therefore leverages both visual and
textual features of apps for mapping GUI widgets. This process
additionally recognizes and transfers oracle and system events.
We empirically evaluated MAPIT on 25 pairs of sibling
Android and iOS apps spanning 5 app categories. For each
pair of sibling apps, we transferred 4 test cases corresponding
to representative usage scenarios in both directions to evaluateMAPIT‚Äôs ability to correctly transfer both (1) individual events
and test oracles, as well as (2) complete tests. In total, ourevaluation yielded bidirectional transfers of 200 test cases,including 828 UI events, 176 oracle events, and 50 system
events. Overall, MAPIT achieved over 75% event mapping ac-curacy and showed to be useful in reducing the required manualeffort by over 55%. Furthermore, 58 (29%) complete test cases
were transferred correctly, eliminating the manual effort. Note
that, even if a test is not completely transferred correctly, the
reduction in manual effort MAPIT affords is proportional to thefraction of individual events that were successfully transferred.
In those cases, the developer can complete the partially
transferred test by modifying the events tagged as incorrectly
transferred. Furthermore, in our evaluation of MAPIT‚Äôs accu-
racy when transferring individual events, we compared ourcomposite mapping technique against only textual or only
visual information (as used in previous cross-platform transfer
techniques [ 11], [14]) and showed that our composite approach
outperforms the previous techniques in nearly all cases.
This paper makes the following contributions:
‚Ä¢A novel technique for bi-directional, cross-platform trans-
fer of usage-based tests for closed-source apps.
‚Ä¢A novel UI widget mapping solution that combines
pluggable computer vision and NLP techniques.
‚Ä¢An extensible approach for transferring test oracles and
system events across mobile platforms.
‚Ä¢An empirical evaluation on 25 closed source real-world
apps, and a public repository with MAPIT‚Äôs implementa-
tion and artifacts to foster future research [15].
Section II presents our work‚Äôs background via an exampleand introduces the key terminology. Section III presents ourapproach and Section IV its empirical evaluation. Section V
overviews the related work. Section VII concludes the paper.
II. B ACKGROUND AND TERMINOLOGY
Figure 1 shows the screenshots of the login pages of Etsy,
a popular shopping app, on Android (left) and iOS (right).Although the two login pages are not identical, they share sig-
niÔ¨Åcant similarities in (1) the appearances of their UI widgets,
(2) the textual data describing these widgets, and (3) the wid-
gets‚Äô position on the respective screens. Such, and even greater,
pairwise similarities between ‚Äúsibling‚Äù apps are common.
Let us assume that a test of Etsy‚Äôs login functionality exists
on Android, and that we want to transfer it to iOS. The widgets
involved in the login test are framed and labeled for both
platforms in Figure 1. We will use this scenario to introduce
the terms and describe the concepts relevant to our approach.
The source app is the app with existing tests that are to
be transferred to the target app. Source platform and target
platform are the platforms on which the source and target
apps run, respectively. The source test is the existing test to
be transferred, while the target test is the transferred test. A
ground-truth test is an existing test for the target app that tests
the same functionality as the transferred source test .Atest
scenario is an informal description of a test case in natural
language. For instance, the login test scenario consists of
entering username and password and clicking the ‚Äúlogin‚Äù button.
Aground-truth test is thus used for evaluating the success of
a test transfer corresponding to the same test scenario in the
two sibling apps. Note that the source test that is transferred,
e.g., from iOS to Android, serves the ground-truth test when
transferring the same test scenario in the opposite direction.
The contents of a given screen of an app form an app
state. Equivalent states on the source and target apps are the
states intended to provide the same or equivalent functionalities,usually with similar-looking UIs. For instance, Figure 1 shows
two equivalent states of the Etsy app on Android and iOS
since they both target login functionality.
Anevent is deÔ¨Åned as a 4-tuple ( a,w,t,o). Each event has
one required element: a, which is the type of action associated
with the event. The remaining three elements are optional: wis
the UI widget; tis the input text associated with the event, such
as text entered by the keyboard; and ois the oracle type. MAPIT
supports three types of events: (1) UI events, for which the sup-
ported action types are click andkeyboard input ; (2) oracle
events, which are the assertions in the UI tests that determine
D
D
D
E
E
E
E
D
Fig. 1: Login pages of Etsy on Android (left) and iOS (right).
757$SS([SORUHU
6RXUFH
7HVW,QWHUQDO7HVW
*HQHUDWRU
$SS([SORUHU
7HVW
*HQHUDWRU7DUJHW
7HVW6RXUFH'DWD([WUDFWLRQ3KDVH 7HVW0LJUDWLRQ3KDVH
'HYLFHRQ
6RXUFH3ODWIRUP$&RPPDQG(YHQWV8,7UDQVLWLRQ0RGHO
UHSUHVHQWLQJ6RXUFH7HVW
8,6FUHHQ8,6WDWH'HYLFHRQ7DUJHW
3ODWIRUP8,6FUHHQ$&RPPDQG
0DSSHG(YHQW
0DSSHG
(YHQW8,7UDQVLWLRQ0RGHO
UHSUHVHQWLQJ7DUJHW7HVW
6RXUFH
$SS
7DUJHW
$SS
,QWHUQDO
7HVW
8,(YHQW
0DSSHU2UDFOH
0DSSHU6\VWHP
(YHQW
0DSSHU(YHQW0DSSHU
Fig. 2: High-level workÔ¨Çow of MAPIT.
whether a test should pass or fail, for which the supported action
type is oracle ; and (3) system events, for which the supported
action types are back andenter . Note that wis optional since
not all events have an associated widget (e.g., system events).
If the event is an oracle event, ocontains the speciÔ¨Åc oracle
type. To demonstrate MAPIT‚Äôs ability to transfer oracles,in this paper we focus on a representative cross-section ofassertion types identiÔ¨Åed by prior work [
5], [8], which will
be detailed in Section III.
As an example, the UI event of entering the username ‚ÄúUsr 1‚Äù
for logging into Etsy on Android is represented as a 4-tuple
(a:‚Äò‚Äòkeyboard input‚Äô‚Äô, w:a1, t:‚Äò‚ÄòUsr_1‚Äô‚Äô, o:‚àí) .
Finally, each UI test consists of a sequence of 4-tuple events.
III. A PPROACH
Figure 2 shows an overview of MAPIT‚Äôs workÔ¨Çow. The
input to MAPIT is three-fold: (1) Source Test written for
the source platform, (2) Source App that runs on the source
platform, and (3) its sibling, Target App that runs on the target
platform. MAPIT automatically transfers the Source Test
through two major phases: (1) Source Data Extraction, during
which the data needed for the test transfer is dynamically
extracted from the Source App, and (2) Test Migration, during
which the extracted data is used to generate the Target Test,
the Source Test ‚Äôs equivalent on the target platform. The
remainder of this section details the two phases.
In developing MAPIT, we made several implementation deci-
sions driven by the third-party technologies on which we relied.
We highlight those whenever they are instrumental in enabling
a particular facet of MAPIT. Overall, MAPIT is implemented
in 4.5 KSLOC of Python, additionally integrating off-the-shelf
tools for mobile app monitoring, analysis, and testing.
A. Source Data Extraction
During the source data extraction phase, the Internal Test
Generator component Ô¨Årst transforms the pre-existing Source
Test to the Internal Test, which is captured in MAPIT‚Äôs internal
representation for test cases. This internal representation is both
programming language- and testing framework-independent,
a critical requirement of cross-platform test transfer. Basedon the Internal Test, the App Explorer component gradually
generates a UI Transition Model, which consists of the
observed UI states of the app and the transitions between
them. Each transition represents one event (e.g., button click)
within the corresponding test case. The UI Transition Modelis generated by executing each event of the Internal Test on
the source platform, and dynamically extracting the requisite
information from the source app. We describe the Internal Test
Generator and App Explorer components in more detail next.
Internal Test Generator
As mentioned above, this component translates the Source
Test into the language- and platform-independent Internal
Test . Figure 3 illustrates this with an example of translating a
partial test of Etsy‚Äôs login functionality, written in Python for
the Appium testing framework [ 16] (Figure 3-a), to MAPIT‚Äôs
internal format (Figure 3-b). A test is represented internallyby MAPIT as a sequence of 4-tuple event s, as deÔ¨Åned in
Section II, with the event elements w,t, and obeing optional.
Note that each widget wcontains the information used to locate
this widget in the source test, such as accessibility id ,
resource id ,XPath ,o r coordinates . Thus wis represented as
alocator-type and its corresponding locator-value .
As a proof of concept, MAPIT currently includes support
for translating tests written in Python for Appium [ 16] and the
Robot [ 17] framework. These two frameworks are widely used
in mobile-app testing. The translation in each case is doneby mapping framework-speciÔ¨Åc tests to MAPIT‚Äôs internaltest representation through regular expression matching.For instance, the Ô¨Årst two lines of the Appium Python testshown in Figure 3-a are matched by the regular expressions
‚Äò‚Äò.*driver.find_element_by_(. *)\(\"(. *)\"\)‚Äô‚Äô , and
‚Äò‚Äòel. *\.(. *)\(( . *)\)‚Äô‚Äô respectively, and the action ,
locator-type ,locator-value , and input elements are
extracted from them to form the Ô¨Årst event in MAPIT‚Äôs
HO $SSLXPZHEGULYHUILQGBHOHPHQWBE\BLG FRPHWV\DQGURLGLGHGLWBSDVVZRUG 
HOVHQGBNH\V SDVVZRUG  D
HO $SSLXPZHEGULYHUILQGBHOHPHQWBE\BLG FRPHWV\DQGURLGLGEXWWRQBVLJQLQ 
HOFOLFN
HO $SSLXPZHEGULYHUILQGBHOHPHQWBE\BDFFHVVLELOLW\BLG <RXWDERI 
HOFOLFN:HE'ULYHUZDLW$SSLXPZHEGULYHU XQWLO (&YLVLELOLW\BRIBHOHPHQWBORFDWHG%\LG 
FRPHWV\DQGURLGLGXVHUQDPH 
‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
^DFWLRQNH\ERDUGLQSXW 
:LGJHW^ORFDWRUW\SH UHVRXUFH,G ORFDWRUYDOXH FRPHWV\DQGURLGLGHGLWBSDVVZRUG `
WH[WLQSXWSDVVZRUG¬¥ `
^DFWLRQFOLFN
:LGJHW^ORFDWRUW\SH UHVRXUFH,G ORFDWRUYDOXH FRPHWV\DQGURLGLGEXWWRQBVLJQLQ ``
^DFWLRQFOLFN
:LGJHW^ORFDWRUW\SH DFFHVVLELOLW\,G ORFDWRUYDOXH <RXWDERI ``
^DFWLRQRUDFOH
:LGJHW^ORFDWRUW\SH LGORFDWRUYDOXH FRPHWV\DQGURLGLGXVHUQDPH `
RUDFOHZLGJHWGLVSOD\HG `ZRUG`E
Fig. 3: Translating (a) Etsy‚Äôs login test written in Python for
the Appium framework to (b) MAPIT‚Äôs internal representation.
758	

	
	
	
	
	
		







	

Fig. 4: App Explorer ‚Äôs internal architecture.
internal test format shown in Figure 3-b. This mechanism
can be easily extended to translate UI tests written in other
programming languages and/or for other testing frameworks.
App Explorer
As shown in Figure 2, the App Explorer component interacts
with a mobile device and gradually generates the UI Transition
Model of the Source App while executing the sequence of
Events in the Internal Test. SpeciÔ¨Åcally, App Explorer consists
of three sub-components, as shown in Figure 4: Event Executor,
State Extractor, and Model Generator.
1) Event Executor: This sub-component is tasked with
initiating and maintaining an active connection with the mobile
device. It takes the Events from MAPIT‚Äôs Internal Test as input,
and transforms each event to the corresponding commands thatare transmitted to the device and executed. Event Executor uses
Appium [
16] for device communication. In turn, Appium relies
on Android Debug Bridge [ 18], a tool for communicating with
Android devices, and Web Driver Agent [ 19], an interface for
remotely interacting with iOS devices.
2) State Extractor: For each event triggered by Event
Executor, State Extractor captures and processes the data
associated with the current device screen, in order to generate
the current UI State. A UI state Sconsists of (1) the app‚Äôs
current screenshot, (2) the graph representing the screen‚Äôs UI
layout hierarchy, and (3) all UI widgets that exist on the current
screen. Figure 5 shows an example of the UI state extracted
from the Etsy app at the beginning of a login test scenario.
SpeciÔ¨Åcally, State Extractor Ô¨Årst captures the bitmap of the
current screen (shown in the center of Figure 5) and extracts
itspage source, which contains the screen‚Äôs UI information
as an XML hierarchy [ 20]. It then iterates through the UI
layout contained in the page source and builds a graph based
on the UI element hierarchy (shown on the left of Figure 5).
While iterating through the UI layout, State Extractor extracts
the boundaries of each UI widget and crops the capturedscreenshot to get the image representing the widget on thecurrent screen. Also extracted and stored are each widget‚Äôsdescriptive attributes in the UI layout, such as
resource id ,
name , coordinates, element type, and whether it is interactable
(shown on the right of Figure 5). Finally, if the widget has any
visible text on its image, such text is captured using OCR. To
this end, we leveraged the Tesseract OCR engine [21].
3) Model Generator: App Explorer ‚Äôs third sub-component
incrementally generates a UI transition model (UITM ), based ontheUI States extracted from State Extractor (e.g., recall Figure
5)) and their corresponding Events obtained from Internal Test
Generator (recall Figure 2). UITM is a linear FSM representing
the transitions in the app taken while executing each event.
UITM has no back transitions: app state associated with a given
screen is captured separately each time the screen is visited.
SpeciÔ¨Åcally, UITM(A, T )is the transition model of app A
associated with UI test case T, which is a sequence of events
e1,e2, ..., e n. The initial UI state of the app when executing T
is annotated as S0. Every state Siis reached by successfully
executing event eiin state Si‚àí1. If the event sequence contains
nevents, the UITM will contain n+1 states: S0,S1, ..., Sn.
Figure 6 shows the UITM extracted from the Android version
of Etsy, representing a login test case consisting of Ô¨Åve events.
This UI transition model is a platform independent represen-
tation of the app under execution, and can be reused in other
cross-platform mappings involving closed-source apps. MAPIT
extracts and populates the model with more comprehensive
data from each state of an app compared to the corresponding
models offered by the existing approach that focuses on
extracting models from both iOS and Android apps [14].
B. Test Migration
In MAPIT‚Äôs second phase, the source test is migrated to the
target platform. This is done by transforming UITM(A, T ),e x -
tracted during the previous phase, into UITM (A/prime,T/prime), where
A/primeis the sibling app of AandT/primeis the test generated by MAPIT
to target the same functionality on A/primethatTtargeted on A.
Test migration is accomplished iteratively via three principal
components: App Explorer, Event Mapper, and Test Generator
(recall Figure 2). A high-level summary of this phase is
provided in Algorithm 1. UITM(A/prime,T/prime)is initialized with
S/prime
0, which is the initial state of the target app (Lines 1-3).
As depicted in Figure 2, this state is extracted using a second
instance of the App Explorer component discussed in the
previous phase; this instance of App Explorer is responsible
for interacting with the target device. For each state Siand
transition-triggering event ei+1 inUITM(A, T ), the Event
Mapper component Ô¨Ånds the equivalent e/prime
i+1 event in the
current state S/prime
ion the target platform (Line 5). e/primei+1 is
executed by App Explorer, resulting in the transition from
S/prime
itoS/prime
i+1 (Line 6). Additionally, e/primei+1 is added to the target
testT/primeby the Test Generator component (Line 7). At this
point, S/prime
i+1 becomes the current state and its corresponding
information will be extracted by App Explorer (Line 8), and
added to UITM (A/prime,T/prime)as a new state connected to S/prime
ivia
the transition corresponding to e/primei+1 (Lines 9-10). When the
Ô¨Ånal state of the source app‚Äôs model is reached, all events in
the source test have been migrated to the target platform.
Event Mapper is the core component of MAPIT. It dynami-
cally maps each event from the source platform to its equivalent
event that is executable in the current state of the target app. Ittakes each state and event in the extracted
UITM (A, T)from
the previous phase, as well as the corresponding state in the
target app, and outputs the mapped event. Recall from Section
II that MAPIT supports the transfer of three types of events:
759Algorithm 1: High-Level Test Migration Process
Input: App A/prime, UI Transition Model UITM(A, T )
Output: Test T/prime, UI Transition Model UITM(A/prime,T/prime)
1UITM(A/prime,T/prime)=‚àÖ;
2S/prime
0=extract current state(A/prime);
3UITM(A/prime,T/prime).add state(S/prime
0);
4foreach (Si,ei+1)in UITM(A, T) do
5 e/prime
i+1=map event(S i,ei+1,S/prime
i);
6 execute event(A/prime,e/primei+1);
7 T/prime.add event(e/primei+1);
8 S/prime
i+1=extract current state(A/prime);
9 UITM (A/prime,T/prime).add state(S/prime
i+1);
10 UITM (A/prime,T/prime).add transition(S/prime
i,S/prime
i+1,e/primei+1);
11end
(1) UI, (2) oracle, and (3) system events. Correspondingly,
as shown in Figure 2, Event Mapper consists of UI Event
Mapper, Oracle Event Mapper, and System Event Mapper sub-
components. Mapping a source event is handled differently by
its corresponding mapper based on the event type. We now
detail each of these mappers. We will then elaborate on Test
Generator, the Ô¨Ånal component in MAPIT‚Äôs architecture.
UI Event Mapper
Recall that an event is a 4-tuple ( a,w,t,o), some of whose
elements may be optional. All elements present in a given
event must be mapped from the source to the target platform.
For UI events, action type acan be click orkeyboard
input . In both cases, awill be migrated to the target platform
as-is. For keyboard input actions, the event will also contain
text input t, which remains unchanged in the mapped event.
To map a source UI widget wto its most similar widget in the
current UI state of the target app, MAPIT leverages both visual
and textual information. As shown in Figure 7, it does so via
Visual Comparator, Textual Comparator, and Widget Selector
sub-components. By contrast, previous work has explored using
only textual [ 11], [5], [6], [8] or only visual [ 14] features of the
screen for widget mapping. In the ensuing discussion, we referto the combination of these three sub-components as UI Widget
Mapper, to distinguish them from the entire UI Event Mapper :although a key function of UI Event Mapper is indeed the
mapping of UI widgets w, it is also responsible for mapping
the other elements of an event (a, t, o) to the target platform.
1) Visual Comparator: This sub-component extracts the
cropped image and coordinates of widget wfrom source app‚Äôs
state S, and all widgets that exist in target app‚Äôs current UI state
S/prime. It then calculates a visual similarity score with respect to w
for each widget of S/primeand ranks the target widgets accordingly.
The intuition behind this component is that equivalent
widgets on different platforms tend to have very similar looks
by design. Visual similarity of two widgets is computed as the
weighted average of their (1) image, (2) screen location, and
(3) size similarities. Computing the latter two scores is relatively
straightforward. The proximity score of two widgets is com-
puted based on the Euclidean distance [ 22] of their locations
on the screen. The size similarity score is determined based on
the difference in widget sizes (normalized by the device size).
To compute the image similarity score, Visual Comparator
leverages the key points and feature descriptors extracted
from both the source and destination widget images. Animage‚Äôs key points are its pixels that have a prominent
difference of intensity with their adjacent pixels [ 23]. Feature
descriptors are numerical representations that encode data
about each key point‚Äôs neighborhood [ 23] and are used widely
for image comparison [ 24]. For detecting an image‚Äôs key
points and subsequently its feature descriptors we use ORB[
25] algorithm, which is an state-of-the-art image matching
technique that has shown to be highly efÔ¨Åcient.
Once the sets of feature descriptors corresponding to source
and destination images have been obtained, Visual Comparator
computes the Hamming distance [ 26] for each pair of source
and destination descriptors. It then ranks the destination descrip-tors for each source descriptor based on the computed distance.Good matches between these two sets of descriptors are next de-termined via Lowe‚Äôs ratio test [
27], which is widely used in im-
age matching tasks. In this test, for each descriptor of the source
Fig. 5: UI state captured from Etsy‚Äôs login page. The two UI layout hierarchy elements highlighted on the left correspond to
the widget data shown on the right.
! 

		  	

 	

		 
	
	
	
	     
Fig. 6: UITM extracted from Etsy‚Äôs login test case. Each state S0-S5is in the format shown in Figure 5.
7606HYHQWDZWR
0DSSHGHYHQW:LGJHW6HOHFWRU7H[WXDO
&RPSDUDWRU9LVXDO
&RPSDUDWRU8,:LGJHW 0DSSHU
0DSSHG8,ZLGJHW8,:LG8,(YHQW0DSSHUDWR
Z8,70$7
6¬∂
DWR
Z
Fig. 7: UI Event Mapper ‚Äôs internal architecture.
image, the two closest matches among the destination descrip-
tors per the computed Hamming distance are selected. If the
value ofdistance (closest match )
distance (second closest match )is less than a customiz-
able ratio, then the closest match is considered to be a good
match. We empirically explored different ratios, and set the
value to 0.8in our evaluation reported in Section IV. At the end
of this process, each descriptor of the source image is labeled
asmatched if it has a good matching destination descriptor.
Finally, Visual Comparator computes a normalized image
similarity score for each destination image with respect to
the source widget w. This score is calculated as the ratio
ofmatched source descriptors to the maximum number of
descriptors extracted from the source and destination images.
2) Textual Comparator: This sub-component determines
atextual similarity score between two widgets. It leverages
the widgets‚Äô textual data from their respective UITMs ex-
tracted by MAPIT‚Äôs App Explorer (recall Section III-A ). A
widget‚Äôs textual data consists of the values of its textual
attributes. These attributes are a subset of the information
extracted by State Extractor (recall Figure 4) for each widget
and include: (1) content descriptor , (2) resource id , and
(3)text , for the widgets on Android; and their iOS counterparts
(1)accessibility id , (2) name , and (3) label . Any text ex-
tracted from a widget‚Äôs image is also included in its textual data.
For illustration, in the Etsy example from
Figure 1, the textual data describing the ‚Äò‚ÄòSign In‚Äô‚Äô
button on Android ( a3)i s {content descriptor:-,
resource id:‚Äò‚Äòcom.etsy.android:id/button_signin‚Äô‚Äô,
text:-, widget text:‚Äò‚ÄòSign In‚Äô‚Äô} while the textual
data describing the corresponding button on iOS ( b3)i s
{accessibility id:-, name:‚Äò‚ÄòSign in‚Äô‚Äô, label:‚Äò‚ÄòSignin‚Äô‚Äô, widget text:‚Äò‚ÄòSign in‚Äô‚Äô}
. Note that a UI widget
need not have all of the mentioned textual attributes. Also, the
values of multiple attributes may be identical. In the above
case, the Android widget ( a3) does not have the content
descriptor attribute, while the values of name ,label , and
widget text attributes for the iOS widget (b3) are the same.
Textual Comparator Ô¨Årst pre-processes the extracted textual
data using common NLP practices, such as tokenization and
stop-word elimination. In addition to general-purpose stop-
words [ 28], we constructed a new list [ 15] of common stop-
words in widgets‚Äô textual attributes that typically do not
convey meaningful information, such as ‚Äò‚Äòview‚Äô‚Äô ,‚Äò‚Äòbar‚Äô‚Äô ,and‚Äò‚Äòcontainer‚Äô‚Äô .
Textual Comparator computes a pairwise similarity score for
a given pair of source ‚àítarget widgets‚Äô textual attributes. It uses
Word2V ec [ 29] and the standard tf-idf formula [ 30] to transform
each textual attribute into its embeddings. The similarity score is
then calculated based on the embeddings‚Äô cosine similarity [ 31].
Textual Comparator computes the similarity score of all pairs
of textual attributes regardless of their types (e.g., it will
compare text andaccessibility id ), to maximize the chance
of discovering similar widgets. The reason is that meaningful
textual values may be arbitrarily assigned to any attribute in
practice. The textual similarity score between two widgets is
then calculated as the highest cosine similarity score among thetextual attribute pairs. This process naturally Ô¨Ålters out the simi-larity scores calculated based on meaningless textual attributes.
As an example, the home button in Etsy on Android
has ‚Äò‚Äòcom.etsy.android:id/menu_bottom_nav_home‚Äô‚Äô
as its resource id and ‚Äò‚ÄòHome, tab 1 of 4‚Äô‚Äô as its
accessibility id . The same button on iOS has ‚Äò‚ÄòHome‚Äô‚Äô as
itsname and‚Äò‚Äò1‚Äô‚Äô as its value attribute. After the preprocess-
ing step, the textual attributes for the Android widget become
‚Äò‚ÄòHome‚Äô‚Äô and ‚Äò‚Äòmenu bottom nav home‚Äô‚Äô , respectively.
While the cosine similarity between, e.g., ‚Äò‚Äòmenu bottom nav
home" and‚Äò‚Äò1‚Äô‚Äô is very low (0.007), Android‚Äôs resource id
and iOS‚Äôs name are identical ( ‚Äò‚ÄòHome‚Äô‚Äô ). This means that the
textual similarity score between the two widgets is 1.0.
3) Widget Selector: UI Event Mapper ‚Äôs third sub-component
selects the mapped UI widget based on the visual and textual
similarity scores. It does so by Ô¨Årst checking the top-ranked
widgets based on the visual and textual similarity scores. If the
respective top-ranked widgets are the same, Widget Selector will
select this as the mapped widget. Otherwise, since the textual
data is more informative, Widget Selector Ô¨Årst checks whether
the top-ranked widget in the textual similarity ranking has a
score higher than a given, adjustable threshold. If such a widget
w/primeexists, then Widget Selector will Ô¨Årst select all target widgets
whose textual similarity score is within an adjustable proximity
range of the textual similarity score of w/prime. These selected
widgets are considered as close textual matches. Widget Selector
will choose the widget with the highest visual similarity score
among the close textual matches. If no widget‚Äôs score is above
the speciÔ¨Åed textual similarity threshold, then the mapped
widget is the one with the highest visual similarity score.
At this point, the Ô¨Ånal mapped widget is checked for compati-
bility, based on whether its action type is supported on the target
device. If the mapped widget cannot support the transferred
action, Widget Selector will remove it from both rankings, and
then choose another UI widget based on the above process.
Oracle Event Mapper
This component is responsible for mapping all four elements
(a, w, o, t ) of an oracle event. For these events, the action type a
that is oracle and oracle type oare always required, and are both
migrated to the target platform as-is. Mapping the other two
elements, wcorresponding to the widget and tcorresponding to
text, is more challenging. MAPIT handles the transfer of these
761two elements depending on the oracle‚Äôs type as detailed below.
As explained in Section II, we currently support several com-
mon types of oracles identiÔ¨Åed by prior work [ 5], [8]; MAPIT
can be easily extended to include additional oracle types. Table I
shows the oracle types currently supported by MAPIT, divided
into widget-independent and widget-dependent oracles. Note
that transferring test oracles is more challenging across plat-
forms than on a single platform since the supported oracle typesand widget attributes will differ across platforms. This requires
more challenging, heuristic-based mappings as detailed next.
The widget-independent oracle types currently supported
by MAPIT are text existence(txt) and text invisible(txt). These
oracles, respectively, check the presence and visibility of text
in the app‚Äôs current state. In representing these oracles viaMAPIT‚Äôs internal events, the value of the parameter txtis
captured by the oracle event‚Äôs element
t. In these cases, the
text input txt of the source event will be transferred to the
target event as-is, and the presence or visibility of the same
text checked on the target platform.
The widget-dependent oracle types MAPIT currently
supports are widget exists(w), widget invisible(w), and
assert equal(w, attr , val) . For this group of oracles, the
corresponding MAPIT events also contain the widget element
w, which is mapped by UI Widget Mapper as discussed above.
Transferring the widget invisible(w) oracle is more
challenging than widget exists(w). The reason is that the
widget wdoes not exist in the current state of the source
app and, therefore, it is not possible to extract the needed
data for the widget mapping process from the source UI state.
Instead, we hypothesized that in most test cases in which the
invisibility of a widget wis asserted, wis visible in some other
app state that is visited during the execution of the scenario
under test. Thus, for each widget invisible oracle in the source
test, the existence of its associated widget wis checked in all
states of the source app that are visited during the migration
process. If wis found in any state Son the source app, UI
Widget Mapper will search for its equivalent widget w/primein the
equivalent state S/primeon the destination platform. After mapping
w, all remaining elements of the oracle event are mapped to
the target platform. If wis not found, its corresponding oracle
event will be marked as ‚Äúnot mappable‚Äù on the target test.
The assert equal(w, attr , val) oracle checks whether the value
of the attribute attr of widget wis equal to the asserted value
val. In this case, MAPIT captures the combination of attr
andval parameters as oracle event‚Äôs element t. Transferring
this oracle type is challenged by the differences in the widget
attributes maintained by iOS and Android. The only attribute
that exists on both platforms is enabled . Some widget attributes
Widget-independent Test Oracles
textexistence(txt)
textinvisible(txt)
Widget-dependent Test Oracles
widg etexists(w)
widg etinvisible(w)
assert equal(w , attr , val)
TABLE I: Oracle types supported by MAPIT.have different names but equivalent meanings. This includes
the attributes forming the previously discussed textual data and
their corresponding mappings, as well as Android‚Äôs visible
and iOS‚Äôs displayed attributes. Another group of attributes
can be mapped using heuristics. For instance, the selected and
checked attributes assess whether a widget (e.g., radio button,
tab) is selected/checked on Android. Although these two at-
tributes do not exist on iOS, developers usually denote a widgetbeing selected/checked by assigning 1 to its
value attribute or
including the word ‚Äúselected‚Äù in its accessibility id .
For other attributes (e.g., Android‚Äôs clickable ), a mapping
is not possible. MAPIT transfers such assert equal(w, attr , val)
oracles to a not-mappable event on the destination platform.
System Event Mapper
For system events, MAPIT supports the action types enter
and back . For these events, only the action type ain the
4-tuple ( a,w,t,o) has a value. If aisenter , it will remain
unchanged between the source and target events. The challenge
in transferring system events is that they may be handleddifferently across platforms. An example is the transfer of a
back system event. A back is a system event on Android, but
no corresponding system event exists on iOS. Instead, iOS‚Äôs
equivalent would be a UI event with a widget that appears as
a ‚Äúback‚Äù button on the app screen. We discuss how MAPIT
handles this event‚Äôs mapping in both directions.
When mapping from iOS to Android, MAPIT only has to
check whether the source event is indeed a back event. This
is done by checking whether the source event‚Äôs action type
aisclick , and whether the textual attributes of its widget
wcontain the keywords ‚Äò‚Äòback‚Äô‚Äô or‚Äò‚Äòprevious‚Äô‚Äô .
The mapping from Android to iOS is more challenging since
it requires relating a system event to a UI event, where the lattercontains information not present in the former. To address this,MAPIT internally introduces a virtual
click event and an asso-
ciated back-button widget. This widget contains the bitmap of atypical back button, with coordinates normalized by device size,and relevant textual data to describe the button (e.g., the string
‚Äò‚Äòback button‚Äô‚Äô as the value of the content descriptor
attribute). The normalized coordinates are calculated based
on the observation that, in most iOS apps, the back button is
located in the bottom-left corner of the screen. Finally, the
corresponding back button on iOS is identiÔ¨Åed by mapping this
virtual widget using UI Widget Mapper as discussed previously.
Test Generator
Finally, the Test Generator component generates a UI test for
the target platform based on the mapped events. The generated
test is in the Internal Test format discussed in Section III-A.
Generating tests is particularly challenging when the UI
widget w/primein a mapped event (a/prime,w/prime,t/prime,o/prime)does not con-
tain an attribute that can be used as a widget locator
(e.g., accessibility id orresource id ). This information
is needed to identify a speciÔ¨Åc UI widget to trigger an event.
In such cases, MAPIT needs to generate a locator that is
understandable to the target device.
762There are two options to locate the target widget: (1) using
the widget‚Äôs coordinates or (2) using the widget‚Äôs XPath ,
which is the ancestral path from the root of the UI layout
hierarchy [ 32]. We choose XPath because coordinates are
tied to a speciÔ¨Åc device, which would result in brittle tests that
are not executable on other devices. To automatically generatethe
XPath locator for a target widget, Test Generator leverages
the UI layout hierarchy graph (recall Figure 5), which is
stored as part of the target app‚Äôs UITM (Figure 6). SpeciÔ¨Åcally,
Test Generator traverses this graph until the mapped event
(a/prime,w/prime,t/prime,o/prime)is reached, and retrieves the XPath associated
with the target widget w/prime. The XPath is then stored as the
mapped event‚Äôs locator using Internal Test ‚Äôs representation
discussed in Section III-A and depicted in Figure 3-b.
IV . E V ALUA TION
Our evaluation focuses on two key aspects of MAPIT: (1) its
accuracy in mapping events from a source to a target platform
and (2) usefulness of the tests it transfers. We Ô¨Årst describe
our empirical setup and then present the evaluation results.
A. Evaluation Setup
MAPIT is not tied to a speciÔ¨Åc Android or iOS version
or device, and is only practically constrained by the tools on
which its implementation currently relies. Our evaluation was
performed on an iPhone 7 running iOS 14.4, and a Pixel 4
emulator running Android 11.0 installed on a macOS laptop
with 16GB RAM and 3.5GHz dual-core core i7 processor.
As discussed in the context of UI Event Mapper in
Section III-B , MAPIT has two adjustable parameters:
(1) textual similarity threshold and (2) proximity range. We
empirically determined the best performing values for these
parameters to be 0.5 and 0.1, respectively. Our results reported
in this section were obtained using these values.
Recall that, unlike the lone existing approach for cross-
platform test transfer [ 11], MAPIT does not require the apps‚Äô
source code. Since MAPIT targets the bi-directional transfer
of the same app across different platforms, we selectedpopular apps that are available on both Android and iOS.
We Ô¨Årst chose Ô¨Åve different app categories: News, Shopping,
ToDo List, Web Browser, and Mail Client. We chose these
categories for two reasons: (1) they have a number of sibling
apps on iOS‚Äôs App Store and Android‚Äôs Google Play, and
(2) apps from these categories have been used to evaluate test
migration techniques previously [ 8], [3]. In each category, we
selected Ô¨Åve frequently downloaded apps that are available
on both platforms, totaling 25 app pairs, shown in Table II.
Table III shows the test scenarios we used to evaluate
MAPIT. We selected the most common scenarios for each app
category as identiÔ¨Åed by prior work [ 8], [3] and subsequently
expanded the scenarios by further examining the subject
apps. For each app, we evaluated four scenarios. More thanfour scenarios are shown in three of the categories becausea given scenario may not be applicable to all subject apps.
Within each scenario, we also identiÔ¨Åed a set of oracles, i.e.,
conditions that must hold true in the app at a given point.
To generate the test scripts for both Android and iOS
platforms, we manually trigger the events, including both UI
and system events, in each test scenario, and used Appium [ 16]
to record the process. Appium automatically converts the
recorded test scenarios to test scripts in the Robot framework
format [ 17]. These test scripts serve as both source tests to be
transferred from, and ground-truth tests to evaluate MAPIT‚Äôs
accuracy and usefulness. We manually added oracle events to
the tests after they were translated to MAPIT‚Äôs internal formatduring the Source Data Extraction phase (recall Section
III-A ).
This was done because the employed Appium interface did not
allow us to automatically add the oracle events while recording
the tests. In the required manual process, we decided to add
News Shopping ToDo List Web Browser Mail Client
BBC Wish Google Tasks Chrome Gmail
CCN Etsy Microsoft To-Do Firefox Blue Mail
ABC News ebay Todoist DuckDuckGo Edison Mail
The Guardian Poshmark Any.do Brave Spark Mail
USA Today AliExpress My Tasks Edge Newton
TABLE II: Subject apps used for MAPIT‚Äôs evaluation.
 
 	

 	 
  


Fig. 8: The accuracy of MAPIT‚Äôs event mapping across different app categories. Within each category, there are two clusters ofresults: the left (unhighlighted) cluster represent the mapping from Android to iOS and the right (highlighted) from iOS to Android.
Within each cluster, the results are divided by mapping strategy: vision-only (left), text-only (middle), and composite (right).
763these events to the internal test rather than the original tests.
Note that while this decision may be a limitation of MAPIT‚Äôs
current implementation, it does not present a threat to MAPIT‚Äôs
validity or applicability: the same regular expression matching
algorithm used in Internal Test Generator can be leveraged
to translate oracle events from any format or language that
supports them to MAPIT‚Äôs internal representation.
We used MAPIT to transfer each test script from Android
to iOS and vice-versa. This yielded 200 transfer cases in total
(25 apps √ó4 tests √ó2 directions). Overall, our tests contain
828 UI events, 176 oracle events and 50 system events. This
averages to slightly over 5 events per test. Overall, the choices
we made in evaluating MAPIT (number of apps, tests, oracles,
and test sizes) are at least comparable to, and in several
instances signiÔ¨Åcantly surpass, those reported in the emerging
literature on mobile-app test transfer [3].
Our approach does not require all tests to have oracle events
since in some cases the goal of usage-based tests is only
to conÔ¨Årm that a speciÔ¨Åc scenario can be executed on a
device without causing the app to crash. Therefore, we include
scenarios both with and without oracle events. This makes
our evaluation reÔ¨Çective actual usage-based testing in practice.
Furthermore, there was no notable difference between the
mapping accuracies for different types of events. This stronglysuggests that omitting oracle events from certain tests does not
impact the validity of our evaluation results.
B. Accuracy of Event Mappings
This part of our evaluation focuses on MAPIT‚Äôs ability
to correctly transfer a given app, oracle, or system event e
from a source test containing eto the corresponding event
Cate gory Test Scenario
News1)Save or bookmark speciÔ¨Åc news article
2)Navigate to speciÔ¨Åc category of news
3)Search for speciÔ¨Åc news topic
4)Personalize newsfeed based on news topics
5)Change edition
6)Follow author
Shop1)Login to user account
2)Remove item from shopping cart
3)Navigate between product categories
4)Add item to shopping cart
5)Make wishlist
6)Filter products
ToDo1)Add ToDo task
2)Remove ToDo task
3)Edit ToDo task
4)Change due date of ToDo task
Web1)Access website by URL
2)Navigate to previous page
3)Navigate to new browser tab
4)Bookmark URL
Mail Client1)Compose email
2)Search email by keyword
3)Move emails across folders
4)Archive existing email
5)Reply to email
TABLE III: The evaluated scenarios for each app category.e/primein the target test. SpeciÔ¨Åcally, this reÔ¨Çects the accuracy of
Event Mapper, MAPIT‚Äôs core component (recall Section III-B ).
Measuring Event Mapper ‚Äôs accuracy requires that we isolate
its impact from MAPIT‚Äôs remaining components. To explain
how we accomplish that, consider the following scenario. Siis
a state in the source app and S/prime
jits equivalent state in the target
app. Event ei+1 is an event in the extracted UITM representing
a source test that takes the source app from SitoSi+1 (recall
Figure 6) . We evaluate whether Event Mapper is successful
in Ô¨Ånding the correct mapping for ei+1 that will advance the
target app from state S/prime
jtoS/prime
j+1.
To this end, we manually inspect each pair of sibling
apps and detect their equivalent states for each test scenario
based on the functionality they provide. We feed those
states alongside the source event to Event Mapper. Manually
detecting equivalent states in sibling apps was straightforward
in practice: in more than 95% of the cases in our subject apps,
there existed one-to-one mappings between the source and
target states, and they occurred in the same order (i.e., i=j).
This is consistent with our guiding hypothesis that sibling
apps will have highly similar functionalities by design.
The correctness of each source event‚Äôs mapping is determined
by manually comparing the transferred test and the ground-
truth test. Note that there can be multiple correct mappings fora given source event. For example, the correct mapping of the
click event on widget a3in Figure 1 can be a click on either
b3orb4since they both result in the same action.
MAPIT‚Äôs test transfer approach assumes that there exists
one-to-one mappings between UI states of the sibling apps.However, in certain, rare cases the numbers of events thatrepresent the same test scenario will differ between the two
platforms. For example, Etsy‚Äôs login on iOS requires the user tochoose the account type Ô¨Årst and then navigate to the main login
page, whereas on Android the user chooses the account type
on the login page itself. In the 100 test scenarios used in our
evaluation, we encountered only 8 such cases; this prevalence
(8%) is consistent with previously reported results [ 11]. Such
differences do not impact MAPIT‚Äôs event-mapping accuracy. In-stead, mismatched UI states affect the usefulness of transferred
tests and are taken into account in Section IV-C below.
Recall from Section III that MAPIT introduces a combination
of visual and textual techniques for mapping events between
platforms. We thus also evaluate the beneÔ¨Åts of this compositemapping. Note that the sole previously existing cross-platform
test migration technique, TestMig, only employs textualmapping [
11]. Adding visual information was important in
our case since we target closed-source apps whose textualinformation is limited due to the unavailability of app code.
We were unable to directly compare MAPIT‚Äôs accuracy with
TestMig for two reasons. First, TestMig requires access to
an app‚Äôs source code while the code of nearly all of oursubject apps is unavailable. Second, TestMig only supports
uni-directional transfer from iOS to Android.
Figure 8 shows the accuracy of our bi-directional event map-
ping in each app category, based on vision-only, text-only, andcomposite mappings. With one exception, MAPIT‚Äôs composite
764mapper outperforms the other two strategies. The composite
mapper was able to accurately map events in over3
4of all cases
(see ‚ÄúAll Categories‚Äù in Figure 8). No notable differences in
accuracy emerged when mapping UI, oracle, or system events.
Furthermore, these results are independent of the platform:
the overall results are separated by a single percentage point
between the Android-to-iOS and iOS-to-Android mappings.
The lone exception to the above trends is the iOS-to-Android
mapping of Web Browser apps. Our subsequent analysis
uncovered a likely reason and a possible remedy. Namely,
in MAPIT‚Äôs analysis of some of the browser apps, a textually
mapped widget would have been the correct widget to select,
but its textual similarity with the source widget was below thethreshold discussed in Section
III-B . In those cases, the visually
mapped widget was chosen per the strategy adopted by MAPIT.However, this was a Ô¨Çawed strategy because of the sibling apps‚Äôlayout differences. This suggests that the thresholds, which we
set across all apps, may need to be further tuned for different
app categories and possibly based on other criteria.
C. Usefulness of Transferred Tests
To assess how useful the tests transferred by MAPIT are, we
leverage a metric introduced by recent work [ 3], which measurs
the reduction in the manual effort required to accomplish a test-
transfer task. SpeciÔ¨Åcally, the manual effort required after using
MAPIT is quantiÔ¨Åed as the number of steps needed to rectify
the incorrectly-mapped events in the transferred test, while the
effort without using MAPIT amounts to the number of steps
needed to write the entire ground-truth test from scratch. A
step can be an event‚Äôs insertion, deletion, or substitution [3].
To perform this evaluation, we provide MAPIT with the
binaries of sibling apps as well as a test script on the source
platform, and compare the transferred test to the ground truth
on the target platform. Recall from Section IV-B that there
may exist multiple correct mappings for a source event. For
this reason, we manually inspect each transferred test to verify
the correctly mapped events.
Figure 9, shows the average effort reduction across the
different subject-app categories. Overall, MAPIT reduces morethan half the manual effort required to write UI tests for a new
platform. Furthermore, the average reductions are similar in
the two transfer directions, indicating that MAPIT‚Äôs usefulness
is independent of the platform. In our evaluation, MAPIT
  	  
  	

Fig. 9: Effort reduction afforded by MAPIT. Result pairs within
each app category correspond to the mappings from Android
to iOS (left) and from iOS to Android (right, highlighted).was able to achieve 100% reduction‚Äîeliminating all manual
effort‚Äîin 58 of 200 test cases (29%).
If we consider these results in tandem with those from
Figure 8, it is interesting to note that, in a number of instances,
MAPIT achieved high accuracy but relatively low reduction
(i.e., usefulness). Initially, this seemed counter-intuitive since,
in principle, accurate event mappings should result in high-quality transferred tests. However, a more detailed analysis
uncovered that the incorrectly-mapped events in these cases are
rare, but they start appearing relatively early in a transferred
test. In turn, this leads a target app into an incorrect state early
during the test migration phase and causes it to ‚Äúget lost‚Äù so
that all subsequent source events are also mapped incorrectly.
In a great majority of cases, these subsequent events would
have been mapped correctly if the app were in the correctstate (as can be conÔ¨Årmed by MAPIT‚Äôs complete accuracy
data [ 15]). In fact, 74 of the 200 test cases (37%) would only
have one incorrectly-mapped event if a correct app state were
reached. This strongly suggests that minor human effort hasthe potential to improve MAPIT‚Äôs usefulness signiÔ¨Åcantly.
For example, slightly ‚Äúnudging‚Äù MAPIT in certain cases‚Äîby
manually providing a correct mapping of a single event or by
guiding the target app once to a correct state‚Äîwould combine
these 74 cases with the 58 fully transferred test cases to raise
MAPIT‚Äôs reduction in effort to nearly zero in2
3of cases.
Another aspect of MAPIT‚Äôs usefulness we measured is
its performance. Although we have not optimized MAPIT
for speed, this aspect of our prototype is an indication of its
real-world applicability. On average, MAPIT‚Äôs Source Data
Extraction phase took 101 seconds and its Test Migration
phase took 217 seconds; in other words, the entire transfer
process averaged slightly over 5 minutes per test. In general,
MAPIT‚Äôs execution time depends on the number of events in
a test, as well as the complexity of an app‚Äôs screen layout ateach step of the execution. An average test in our evaluation
had 5 events, while each app screen averaged 25 widgets that
needed to be extracted and compared.
V. R ELA TED WORK
TestMig [ 11] is the lone existing approach for migrating tests
across platforms. However, TestMig only transfers tests from
iOS to Android and requires the source code of both source and
target apps. Other approaches in the mobile-app domain have
focused on migrating UI tests between different Android apps
within the same category. Behrang et al. [ 5], [6] and Lin et
al. [8] rely on static code analysis for extracting the GUI models
of the app, which are not available for closed-source iOS apps.
Mariani et al. [ 10] formulated the test reuse problem as a
search problem and used evolutionary testing to transfer tests
across different Android apps. Hu et al. [ 7] proposed a machine
learning-based approach for generating UI tests for an app usinga library of existing tests. This work generates regression tests
for a speciÔ¨Åc app rather than enabling test migration. Zhao
et al. [ 3] proposed a framework for automatically evaluating
the previous approaches, but did not speciÔ¨Åcally address test
migration. Mariani et al. [ 33] presented an empirical study
765on techniques for semantic matching of GUI events used by
existing test reuse approaches.
Beyond the mobile-app arena, Rau et al. [ 34] proposed
an approach for efÔ¨Åciently generating UI tests by learning
from the existing tests of other apps, but their work targets
web applications. Similarly, Yeh et al. [ 35] proposed an early
image-based platform-independent testing tool for testingdesktop and web applications. Finally, Mariani et al. [
36]
proposed an approach that automatically exploits the common
functionalities of Java applications to generate UI tests.
Another related body of work focuses on remote app
execution. Y u et al.[ 14] proposed LIRA T, a record-and-replay
technique for executing scenarios across mobile platforms.
Their approach is based on image feature matching and UI
layout characterization. Compared to MAPIT‚Äôs compositeapproach, using only visual features lowers the mapping
accuracy, especially in cases where the source and target apps
are not visually identical. Furthermore, leveraging textual data
using a technique such as Word2V ec, which focuses on the
semantics behind the data instead of the exact word matching,
makes MAPIT‚Äôs approach more suitable as a foundation for
cross-platform test transfer than LIRA T [ 14]. Another recent
approach [ 37], [38] enabled remote interaction with iOS devices
and dynamic extraction of partial app UI models. Theseapproaches do not generate a test case from an existing UI
test, but replay on a target device a speciÔ¨Åc scenario that was
recorded on a source device. Cross-platform test migration is
different from them in at least three important ways. (1) Actualtest cases must be human readable and modiÔ¨Åable. (2) Migrated
test cases include oracle events, which may in fact beneÔ¨Åtrecord-and-replay but are not considered by it. (3) Migratedtest cases are not device coordinate-dependent and can be
directly reused across devices on the same platform.
There is an emerging body of work that extracts UI models
from Android apps and uses them to guide testing. This may be
done statically [ 39], dynamically [ 2], [40], or by a combined
strategy [ 41], [42]. Our approach is platform-independent and
is, in principle, closer to dynamic approaches that do not
assume the existence of app code.
VI. L IMITA TIONS AND DISCUSSION
MAPIT‚Äôs approach and current implementation have several
potential limitations. We discuss them in this section.
One limitation is presented by sibling apps whose events
are not related 1-to-1. Supporting 1-to-N event mappings isespecially challenging in bi-directional test transfer across
mobile platforms. This is because of an important difference
between Android and iOS. In Android, many reverse engi-
neering tools are available that can help to extract a complete
model of a closed-source app (e.g., Gator [ 13]). On the other
hand, no analogous mechanism exists for iOS, which makes it
challenging to predict possible future events in each state of a
closed-source iOS apps. Even though both our evaluation and
prior work indicate that sibling apps whose events have the
1-to-N relationships occur relatively infrequently, Ô¨Ånding ways
to address this issue would further improve MAPIT‚Äôs utility.MAPIT currently only supports clicks and keyboard inputs
as UI events. These two UI events are the most common types
of events. They are also more challenging to map than other
UI events since they are associated with UI widgets. MAPIT‚Äôsunderlying modular design makes it easily extensible to support
additional types of UI events, such as swiping and scrolling.One strategy for doing so would be by introducing event-speciÔ¨Åc heuristics. We will have to implement and evaluate
the effectiveness of such an approach.
Another limitation of MAPIT is its inability to extract
data (e.g., the UITM discussed in Section III-A ) from certain
commercial apps. Our analysis of this problem identiÔ¨Åed four
potential reasons: (1) some apps may use obfuscation and makecertain UI elements inaccessible; (2) hybrid apps that combine
web and native code may not be analyzable by tools such as
Appium; (3) certain UI states may not allow data extraction
for security reasons (e.g., no screenshots may be taken on the
login screen); and (4) the execution of some tests requires
currently unrecognized types of action (e.g., scrolling). Some
of these (e.g., adding support for scrolling) are straightforwardextensions to MAPIT, but others (e.g., overcoming obfuscation)
present compelling research challenges.
Finally, as discussed in Section III-A , MAPIT currently
relies on regular expression matching for translating tests toMAPIT‚Äôs internal representation. We have considered using
methods based on program analysis (e.g., AST parsing) for the
translation. However, we decided to use regular expressions
because they are fast and accurate, and most testing frameworks
have APIs that generate tests in a speciÔ¨Åc format. The
requirement that original tests be in a speciÔ¨Åc format can limit
the number of input tests MAPIT can handle. As a potential
remedy, MAPIT‚Äôs modular architecture makes it possible to
substitute the current Internal Test Generator component with
a more complex translator.
VII. C ONCLUSION
Our work has demonstrated that it is viable to Ô¨Çexibly
transfer both individual app events and entire UI tests across
mobile platforms. This will serve as a foundation for a range offollow-on activities in this area. Several of those will, naturally,focus on improving MAPIT‚Äôs accuracy and usefulness, and onaddressing its current shortcomings. This may involve relaxing
some of our assumptions, such as taking advantage of code
when it is available. It may also involve leveraging MAPIT‚Äôs
modular architecture to introduce platform- or technology-speciÔ¨Åc components when appropriate, as discussed above.
Future work will also require overcoming speciÔ¨Åc challenges
enumerated in Section VI that have not been our focus to date
for practical reasons.
VIII. A CKNOWLEDGMENTS
This work is supported in part by the U.S. National Science
Foundation under grants 1717963, 1823354, and 2030859 (the
Computing Research Association for the CIFellows Project),
and the U.S. OfÔ¨Åce of Naval Research under grant N00014-
17-1-2896.
766REFERENCES
[1] W. Wang, D. Li, W. Yang, Y . Cao, Z. Zhang, Y . Deng, and T. Xie, ‚ÄúAn
empirical study of android test generation tools in industrial cases,‚Äù in
2018 33rd IEEE/ACM International Conference on Automated Software
Engineering (ASE). IEEE, 2018, pp. 738‚Äì748.
[2] D. AmalÔ¨Åtano, A. R. Fasolino, P . Tramontana, B. D. Ta, and A. M.
Memon, ‚ÄúMobiguitar: Automated model-based testing of mobile apps,‚Äù
IEEE software, vol. 32, no. 5, pp. 53‚Äì59, 2014.
[3] Y . Zhao, J. Chen, A. SejÔ¨Åa, M. Schmitt Laser, J. Zhang, F. Sarro,
M. Harman, and N. Medvidovic, ‚ÄúFruiter: a framework for evaluating ui
test reuse,‚Äù in Proceedings of the 28th ACM Joint Meeting on European
Software Engineering Conference and Symposium on the Foundations of
Software Engineering, 2020, pp. 1190‚Äì1201.
[4] M. Linares-V ¬¥asquez, C. Bernal-C ¬¥ardenas, K. Moran, and D. Poshyvanyk,
‚ÄúHow do developers test android applications?‚Äù in 2017 IEEE Interna-
tional Conference on Software Maintenance and Evolution (ICSME) .
IEEE, 2017, pp. 613‚Äì622.
[5] F. Behrang and A. Orso, ‚ÄúTest migration for efÔ¨Åcient large-scaleassessment of mobile app coding assignments,‚Äù in Proceedings of the
27th ACM SIGSOFT International Symposium on Software Testing and
Analysis, 2018, pp. 164‚Äì175.
[6] ‚Äî‚Äî, ‚ÄúTest migration between mobile apps with similar functionality,‚Äù in
2019 34th IEEE/ACM International Conference on Automated Software
Engineering (ASE). IEEE, 2019, pp. 54‚Äì65.
[7] G. Hu, L. Zhu, and J. Yang, ‚ÄúAppÔ¨Çow: using machine learning tosynthesize robust, reusable ui tests,‚Äù in Proceedings of the 2018 26th
ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering. ACM, 2018,
pp. 269‚Äì282.
[8] J.-W. Lin, R. Jabbarvand, and S. Malek, ‚ÄúTest transfer across mobile
apps through semantic mapping,‚Äù in 2019 34th IEEE/ACM International
Conference on Automated Software Engineering (ASE). IEEE, 2019,
pp. 42‚Äì53.
[9] A. Rau, J. Hotzkow, and A. Zeller, ‚ÄúTransferring tests across web
applications,‚Äù in International Conference on Web Engineering. Springer,
2018, pp. 50‚Äì64.
[10] L. Mariani, M. Pezz `e, V . Terragni, and D. Zuddas, ‚ÄúAn evolution-
ary approach to adapt tests across mobile apps,‚Äù arXiv preprint
arXiv:2104.05233, 2021.
[11] X. Qin, H. Zhong, and X. Wang, ‚ÄúTestmig: Migrating gui test cases fromios to android,‚Äù in Proceedings of the 28th ACM SIGSOFT International
Symposium on Software Testing and Analysis, 2019, pp. 284‚Äì295.
[12] ‚ÄúSoot,‚Äù 2021. [Online]. Available: http://soot-oss.github.io/soot/[13]
S. Yang, H. Zhang, H. Wu, Y . Wang, D. Yan, and A. Rountev, ‚ÄúStatic
window transition graphs for Android,‚Äù in IEEE/ACM International
Conference on Automated Software Engineering, 2015, pp. 658‚Äì668.
[14] S. Y u, C. Fang, Y . Y un, and Y . Feng, ‚ÄúLayout and image recognitiondriving cross-platform automated mobile testing,‚Äù in 2021 IEEE/ACM
43rd International Conference on Software Engineering (ICSE). IEEE,
2021, pp. 1561‚Äì1571.
[15] [Online]. Available: https://github.com/ase-test-migration/test-migration
[16] ‚ÄúAppium,‚Äù 2021. [Online]. Available: https://appium.io/[17]
‚ÄúRobot framework,‚Äù 2021. [Online]. Available: //https://robotframework.
org/
[18] ‚ÄúAndroid debug bridge,‚Äù 2021. [Online]. Available: https://developer.
android.com/studio/command-line/adb/
[19] ‚ÄúWeb driver agent,‚Äù 2021. [Online]. Available: https://github.com/
facebookarchive/WebDriverAgent
[20] ‚ÄúPage source,‚Äù 2021. [Online]. Available: https://www.google.com/search?q=page+source+appium&oq=page+source+appium&aqs=
chrome.0.69i59j0i22i30.1929j0j7&sourceid=chrome&ie=UTF-8
[21] ‚ÄúTesseract,‚Äù 2021. [Online]. Available: https://github.com/tesseract-ocr/
tesseract[22] ‚ÄúEuclidean distance,‚Äù 2021. [Online]. Available: https://www.
oxfordreference.com/view/10.1093/oi/authority.20110803095800175
[23] E. Rublee, V . Rabaud, K. Konolige, and G. Bradski, ‚ÄúOrb: An efÔ¨Åcient
alternative to sift or surf,‚Äù in 2011 International Conference on Computer
Vision, 2011, pp. 2564‚Äì2571.
[24] O. Chum, J. Philbin, A. Zisserman et al., ‚ÄúNear duplicate image detection:
Min-hash and tf-idf weighting.‚Äù in Bmvc, vol. 810, 2008, pp. 812‚Äì815.
[25] E. Rublee, V . Rabaud, K. Konolige, and G. Bradski, ‚ÄúOrb: An efÔ¨Åcient
alternative to sift or surf,‚Äù in 2011 International conference on computer
vision. Ieee, 2011, pp. 2564‚Äì2571.
[26] ‚ÄúHamming distance,‚Äù 2021. [Online]. Available: https://www.
oxfordreference.com/view/10.1093/oi/authority.20110803095918607
[27] D. G. Lowe, ‚ÄúDistinctive image features from scale-invariant keypoints,‚Äù
International journal of computer vision, vol. 60, no. 2, pp. 91‚Äì110,
2004.
[28] ‚ÄúNltk stop words,‚Äù 2021. [Online]. Available: https://www.nltk.org/book/
ch02.html
[29] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean, ‚ÄúDistributed
representations of words and phrases and their compositionality,‚Äù arXiv
preprint arXiv:1310.4546, 2013.
[30] A. Aizawa, ‚ÄúAn information-theoretic perspective of tf‚Äìidf measures,‚Äù
Information Processing & Management, vol. 39, no. 1, pp. 45‚Äì65, 2003.
[31] ‚ÄúCosine similarity,‚Äù 2021. [Online]. Available: https://www.itl.nist.gov/
div898/software/dataplot/refman2/auxillar/cosdist.htm
[32] ‚ÄúXpath,‚Äù 2021. [Online]. Available: https://developer.mozilla.org/en-US/
docs/Web/XPath
[33] L. Mariani, A. Mohebbi, M. Pezz `e, and V . Terragni, ‚ÄúSemantic matching
of gui events for test reuse: are we there yet?‚Äù in Proceedings of the
30th ACM SIGSOFT International Symposium on Software Testing and
Analysis, 2021, pp. 177‚Äì190.
[34] A. Rau, J. Hotzkow, and A. Zeller, ‚ÄúPoster: EfÔ¨Åcient gui test generation bylearning from tests of other apps,‚Äù in 2018 IEEE/ACM 40th International
Conference on Software Engineering: Companion (ICSE-Companion).
IEEE, 2018, pp. 370‚Äì371.
[35] T. Yeh, T.-H. Chang, and R. C. Miller, ‚ÄúSikuli: using gui screenshotsfor search and automation,‚Äù in Proceedings of the 22nd annual ACM
symposium on User interface software and technology, 2009, pp. 183‚Äì
192.
[36] L. Mariani, M. Pezz `e, and D. Zuddas, ‚ÄúAugusto: Exploiting popular
functionalities for the generation of semantic gui tests with oracles,‚ÄùinProceedings of the 40th International Conference on Software
Engineering, 2018, pp. 280‚Äì290.
[37] N. Luki ¬¥c, S. Talebipour, and N. Medvidovi ¬¥c, ‚ÄúAirmochi‚Äìa tool for
remotely controlling ios devices,‚Äù in 2020 35th IEEE/ACM International
Conference on Automated Software Engineering (ASE). IEEE, 2020,
pp. 1273‚Äì1277.
[38] ‚Äî‚Äî, ‚ÄúRemote control of ios devices via accessibility features,‚Äù in
Proceedings of the 2020 ACM Workshop on Forming an Ecosystem
Around Software Transformation, 2020, pp. 35‚Äì40.
[39] A. Rountev and D. Yan, ‚ÄúStatic reference analysis for gui objects in
android software,‚Äù in Proceedings of Annual IEEE/ACM International
Symposium on Code Generation and Optimization, 2014, pp. 143‚Äì153.
[40] D. AmalÔ¨Åtano, A. R. Fasolino, P . Tramontana, S. De Carmine, and A. M.Memon, ‚ÄúUsing gui ripping for automated testing of android applications,‚Äù
in2012 Proceedings of the 27th IEEE/ACM International Conference
on Automated Software Engineering. IEEE, 2012, pp. 258‚Äì261.
[41] M. Linares-V ¬¥asquez, M. White, C. Bernal-C ¬¥ardenas, K. Moran, and
D. Poshyvanyk, ‚ÄúMining android app usages for generating actionable gui-based execution scenarios,‚Äù in 2015 IEEE/ACM 12th Working Conference
on Mining Software Repositories. IEEE, 2015, pp. 111‚Äì122.
[42] T. Su, G. Meng, Y . Chen, K. Wu, W. Yang, Y . Yao, G. Pu, Y . Liu, and
Z. Su, ‚ÄúGuided, stochastic model-based gui testing of android apps,‚Äù in
Proceedings of the 2017 11th Joint Meeting on Foundations of Software
Engineering, 2017, pp. 245‚Äì256.
767