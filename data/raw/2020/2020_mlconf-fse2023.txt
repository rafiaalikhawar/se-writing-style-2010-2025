Can Machine Learning Pipelines Be Better Configured?
Yibo Wangâˆ—
Northeastern University
Shenyang, China
yibowangcz@outlook .comYing Wangâ€ 
Northeastern University, and HKUST
Shenyang, Hong Kong, China
wangying@swc .neu.edu.cnTingwei Zhang
Northeastern University
Shenyang, China
592131686@qq .com
Yue Yu
National University of Defense
Technology
Changsha, China
yuyue@nudt .edu.cnShing-Chi Cheung
The Hong Kong University of Science
and Technology
Hong Kong, China
scc@cse.ust.hkHai Yu
Northeastern University
Shenyang, China
yuhai@mail .neu.edu.cn
Zhiliang Zhu
National Frontiers Science Center for Industrial
Intelligence and Systems Optimization, and Key
Laboratory of Data Analytics and Optimization
for Smart Industry, Northeastern University
Shenyang, China
ZHUZhiLiang_NEU@163 .com
ABSTRACT
AMachine Learning (ML) pipeline configures the workflow of a
learning task using the APIs provided by ML libraries. However, a
pipelineâ€™s performance can vary significantly across different con-
figurations of ML library versions. Misconfigured pipelines can re-
sult in inferior performance, such as inefficient executions ,numeric
errors and even crashes . A pipeline is subject to misconfiguration if
it exhibits significantly inconsistent performance upon changes in
the versions of its configured libraries or the combination of these
libraries. We refer to such performance inconsistency as a pipeline
configuration (PLC) issue .
A systematic understanding of PLC issues helps configure ef-
fective ML pipelines and identify misconfigured ones. To this end,
we conduct the first empirical study of PLC issuesâ€™ pervasiveness,
impact and root causes. To facilitate scalable in-depth analysis, we
develop Piecer , an infrastructure that automatically generates a
set of pipeline variants by varying different version combinations
of ML libraries and detects their performance inconsistencies. We
apply Piecer to the 3,380 pipelines that can be deployed out of
the 11,363 ML pipelines collected from multiple ML competitions
atKaggle platform. The empirical study results show that 1,092
(32.3%) of the 3,380 pipelines manifest significant performance in-
consistencies on at least one variant. We find that 399, 243 and 440
pipelines can achieve better competition scores, execution time and
memory usage, respectively, by adopting a different configuration.
âˆ—Yibo Wang and Ying Wang made equal contributions to this work.
â€ Ying Wang is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
Â©2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0327-0/23/12. . . $15.00
https://doi .org/10 .1145/3611643 .3616352Based on our findings, we construct a repository containing 164
defective APIs and 106 API combinations from 418 library versions.
The defective API repository facilitates future studies of automated
detection techniques for PLC issues. Leveraging the repository, we
captured PLC issues in 309 real-world ML pipelines.
CCS CONCEPTS
â€¢Software and its engineering â†’Software libraries and repos-
itories .
KEYWORDS
Machine Learning Libraries, Empirical Study
ACM Reference Format:
Yibo Wang, Ying Wang, Tingwei Zhang, Yue Yu, Shing-Chi Cheung, Hai
Yu, and Zhiliang Zhu. 2023. Can Machine Learning Pipelines Be Better Con-
figured?. In Proceedings of the 31st ACM Joint European Software Engineer-
ing Conference and Symposium on the Foundations of Software Engineering
(ESEC/FSE â€™23), December 3â€“9, 2023, San Francisco, CA, USA. ACM, New York,
NY, USA, 13 pages. https://doi .org/10 .1145/3611643 .3616352
1 INTRODUCTION
Machine Learning (ML) libraries (e.g., TensorFlow and PyTorch )
have gained much attention in both academia and industry, which
are used in a wide range of domains, including natural language
processing, image processing, autonomous driving, etc [ 38,55,56,
58,79]. These libraries provide off-the-shelf ML solutions, optimized
numerical computations, efficient data structures, visualization, and
other handy utilities, to facilitate application development.
In practice, the calls to ML library APIs are often organized by
means of an ML pipeline, which automates the workflow of an ML
task in nine stages as shown in Figure 1. To timely incorporate
support for new hardware, algorithms and bug fixes, ML libraries
keep evolving rapidly. However, different combinations of ML li-
brary versions can cause inconsistencies in pipelinesâ€™ performances,
crashes and NaN bugs, as demonstrated by the real-life example
463ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Yibo Wang, Ying Wang, Tingwei Zhang, Yue Yu, Shing-Chi Cheung, Hai Yu, and Zhiliang Zhu
in Section 2.2. In this paper, we refer to the performance inconsis-
tencies of ML pipelines induced by varying ML library versions as
PipeLine C onfiguration (PLC) issues .
Despite the widespread use of ML pipelines, there have been no
prior systematic studies on PLC issues. We have little knowledge of
PLC issues, such as their pervasiveness in real-world ML pipelines,
their impact on ML tasksâ€™ performances, and their root causes.
The deficient knowledge can hinder effective deployment of ML
pipelines in various ways: (1) ML library vendors lack awareness of
the need to test the interactions between specific library versions
that could be concurrently deployed by a pipeline. (2) ML pipeline
developers lack advice on good and bad practices in ML pipeline
configuration. (3) Tool builders lack scientific criteria to recommend
appropriate versions of ML libraries to pipeline developers when
maintaining and evolving ML libraries.
To fill the knowledge gap, we conduct a large-scale empirical
study to understand the status quo of PLC issues. To facilitate
scalable analysis, we develop Piecer , which automatically generates
a set of pipeline variants by varying version combinations of ML
libraries and compares three aspects of performance inconsistencies:
execution time ,memory usage , and prediction performance (precision
andrecall ). To understand the status quo of PLC issues, we leverage
Piecer to analyze 11,363 ML pipelines from various competition
categories on Kaggle platform. The pipelines utilize 112 popular
ML libraries. We investigate the pervasiveness and severity of PLC
issues in ML pipelines and analyze their root causes of inducing
performance inconsistencies. The main empirical findings are:
(1)We identify 601 library combinations commonly adopted by the
11,363 ML pipelines in our dataset.
(2)Among the 3,380 ML pipelines that we are able to deploy, 1,092
(32.3%) manifest significant performance inconsistencies on at
least one variant. We find that 399, 243 and 440 pipelines among
the deployable pipelines could achieve higher competition scores,
shorter execution time and lower memory usage, respectively,
using a different configuration of the library versions available
at the time of competition.
(3)By analyzing the historical evolution data of 577 defective APIs
that induce PLC issues, we identify four types of issue root causes.
Specifically, PLC issues exposed in 164 defective APIs are affected
by one library version. PLC issues in 3,720 pipeline variants are
caused by combinations of APIs from different library versions.
In summary, this paper makes four major contributions:
â€¢An Infrastructure for analyzing ML library version impacts.
We developed an infrastructure, Piecer , to automatically gener-
ate pipeline variants with different ML library version combina-
tions and analyze their performance inconsistencies.
â€¢An empirical study on the impacts of various ML library
version combinations. Leveraging Piecer , we conduct the first
empirical study to systematically explore the impacts on different
ML library version combinations and categorize the root causes
of inducing performance inconsistencies.
â€¢A repository of defective APIs for detecting PLC issues. Based
on our empirical findings, we constructed a repository containing
164 defective APIs and 106 API combinations from 418 ML library
versions, which shed light on designing automated approaches
to detecting PLC issues. Based on the repository, we captured
PLC issues in 309 real-world ML pipelines.
Stages of ML Pipelines
Model 
Requirements (MR)istheprocess ofidentifying theappropriate MLmodels
andsuitable features
Data Collection
(DCO )istheprocess ofgathering andmeasuring information
from countless different sources
Data Cleaning
(DCL )refers toidentifying andcorrecting errors inthedataset
thatmaynegatively impact amodel
Data Labeling
(DL)istheprocess ofassigning ground truth labels toeach
datarecord
Feature 
Engineering (FE)helps inpreparing, transforming, andextracting features
from rawdatatoprovide thebestinputs toaMLmodel
Model Training
(MT)isaprocess inwhich amachine learning algorithm isfed
withtraining datafrom which itcanlearn
Model Evaluation 
(ME)istheprocess ofusing different evaluation metrics to
understand amachine learning model's performance
Model Deployment 
(MD)istheprocess ofdeploying amachine learning model ina
liveenvironment
Model Monitoring 
(MM)refers tothe process ofclosely tracking the
performance ofmachine learning models inproduction
ï‚©Machine learning workflows are highly non -linear and contain several feedback loops. Figure 1: Description of the stages in ML pipeline
2 PRELIMINARIES
2.1 Background
ML Libraries : Machine learning comprises several kinds of learn-
ing algorithms such as supervised learning ,unsupervised learning ,
deep learning , and reinforcement learning . To provide fast access
to these algorithms and facilitate the complicated data analysis
process, the open-source community developed two types of ML
libraries: ML frameworks anddata science libraries .ML Frame-
works (e.g., Scikit-Learn [9] and PyTorch [7]) provide ready-made
algorithms and allow developers to easily apply ML-based solutions
to applications. Data science libraries provide data visualization (e.g.,
Matplotlib [4]), efficient data structures (e.g., Pandas [6]), scientific
computing functions (e.g., Numpy [5]) or other utilities.
ML Pipelines : The term pipeline corresponds to the pipes-and-
filter design pattern that decomposes the software architecture into
several stages with processing units (filters) and ordered connec-
tions (pipes). By the ML pipeline , we are referring to a nine-stage
pipeline that goes through data-oriented (collection ,cleaning and
labeling ) and model-oriented (model requirements ,feature engi-
neering ,training ,evaluation ,deployment , and monitoring ) stages.
Figure 1 explains each ML stage. According to the investigation
results in studies [ 49,85], most performance issues or crashes oc-
curred in the DCO ,DCL,FE,MT, and MEstages of ML pipelines.
The five stages are commonly implemented based on various ready-
made algorithms encapsulated in ML libraries.
2.2 Motivation
Since ML libraries rapidly evolve to add/remove/change features
and fix bugs, various version combinations may dramatically affect
pipelinesâ€™ performance. Figure 2 shows an illustrative example of a
PLC issue, which participates in the competition MNIST [26] hosted
onKaggle platform. This pipeline aims to identify digits from a
dataset of tens of thousands of handwritten images, leveraging
twoTensorFlow APIs and eight Keras APIs to implement DCL
and MTstages, respectively. Interestingly, when using different
version combinations of TensorFlow andKeras , the categorization
accuracy of the pipeline varies from 0.559 to 0.997, leading to a
rise in competition ranking from 523 ğ‘¡â„to 1ğ‘ ğ‘¡. Even worse, some
version combinations cause crashes due to API-breaking changes.
464Can Machine Learning Pipelines Be Better Configured? ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
{Keras , Tensorflow }
VersionsScore
(AUC)RankingTime 
(ms)Memory
(MB)
{2.7.1 , 2.7.0 } Crash Crash Crash Crash
{2.4.3 , 2.4.1 } 0.768 522 1895.656 1244.446
{2.4.3 , 2.3.1 } 0.737 521 1882.099 1241.001
{2.4.3 , 2.2.0 } 0.559 523 1926.980 1248.518
{2.3.1 , 2.4.1 } Crash Crash Crash Crash
{2.3.1 , 2.3.1 } Crash Crash Crash Crash
{2.3.1 , 2.2.0 } 0.997 1 1877.330 1199.130
{2.3.1 , 2.1.0 } 0.997 1 1888.612 1202.602
{2.3.1 , 2.0.0 } Crash Crash Crash Crash
{2.3.1 , 1.15.2 } 0.997 1 1989.861 1194.425
{2.3.1 , 1.14.0 } 0.997 1 1853.423 1196.269
{2.3.1 , 1.13.1 } 0.997 1 1901.693 1183.1230.559 523
Figure 2: An illustrative example of a PLC issue
Similar situations can be found in many issue reports. In the
ğ‘ƒğ¿ğ¶ issue#20642 [ 31] of project Sklearn , a developer complained
that the execution time of API Sklearn .cluster.KMeans() in their
ML pipeline varies significantly when using different versions of
Numpy . This issue attracted 34 comments from experienced devel-
opers to diagnose the root causes and was linked to four similar
performance bugs induced by ML library versions (i.e., scipy #15050,
Sklearn #21729, scipy #15129, and Sklearn #21808). The above ex-
amples motivate us to conduct a large-scale empirical study to
investigate the impacts of different combinations of ML library
versions on ML pipelinesâ€™ performances, which can shed light on
approaches for detecting or repairing PLC issues.
3 EMPIRICAL STUDY
Our study aims to answer the following three research questions:
â€¢RQ1 (Common ML Library Combinations): What combina-
tions of ML libraries do developers commonly use? To answer
RQ1, we collect 11,363 published ML pipelines and analyze their
library usage to identify common library combinations.
â€¢RQ2 (Impacts of Different ML Library Version Combina-
tions): Do different version combinations of ML libraries
affect pipelinesâ€™ performances? To answer RQ2, for each ML
pipeline, we generate a series of variants with different ML library
version combinations to inspect their performance inconsisten-
cies. In particular, we define the generation rules of ML library
version combinations, to systematically explore their impacts on
pipelinesâ€™ performances.
â€¢RQ3 (Root Causes of Performance Inconsistencies): What
are the root causes of pipelinesâ€™ performance inconsisten-
cies when adopting different version combinations of ML
libraries? To answer RQ3, we consider the pipeline variants that
induce (1) significant performance inconsistencies, (2) crashes
and (3) NaN bugs as subjects, to analyze the corresponding root
causes and triggering conditions.
3.1 Piecer
To scale up the impact analysis of possible ML library version com-
binations on pipelinesâ€™ performances, we develop an infrastructure,
Piecer (PIpElineConfiguration Explo Re), to automate such a pro-
cess. Figure 2 shows an overall architecture of Piecer . It mainly
consists of three components:
Component 1: Constructing ML Library Version Pool. For a
given ML pipeline, Piecer constructs a dependency pool ğ·ğ‘={ğ‘‰ğ¿ğ‘–ğ‘ 1,ğ‘‰ğ¿ğ‘–ğ‘ 2,Â·Â·Â·,ğ‘‰ğ¿ğ‘–ğ‘ğ‘›}to collect all the installable version candi-
datesğ‘‰ğ¿ğ‘–ğ‘ğ‘–={ğ‘£1,ğ‘£2,Â·Â·Â·,ğ‘£ğ‘š}of referenced ML libraries ğ¿ğ‘–ğ‘ğ‘–. To
achieve this, it first parses the pipelineâ€™s dependency management
script ( Requirement.txt ,Setup.py orMETADATA ) to identify all the
versions of direct dependencies that satisfy the specified version
constraints. For each version of direct dependency, it iteratively col-
lects the version constraints of the required transitive dependencies.
If a libraryğ¿ğ‘–ğ‘ğ‘–corresponds to multiple version constraints, Piecer
considers their intersection as installable version candidates ğ‘‰ğ¿ğ‘–ğ‘ğ‘–.
In the cases where a library ğ¿ğ‘–ğ‘ğ‘–is specified with incompatible
version constraints, we remove the versions of direct dependencies
that induce such dependency conflicts from ğ·ğ‘.
Component 2: Generating Pipeline Variants with Different
ML Library Version Combinations. To explore the impacts of
library versions on a pipelineâ€™s performance, Piecer generates
a set of pipeline variants with different version combinations of
ML libraries selected from our dependency pool ğ·ğ‘. For a given
ML pipeline, it considers the newest versions of referenced ML
libraries as a version combination baseline. By defining a series
of comparison rules (see definitions in Section 3.5), Piecer generates
a set of ML library version combinations by varying the versions
of concerned libraries and keeping the remaining library versions
be consistent with the baseline.
According to the dependency resolution rules of Python build
tool pip, it installs the newest library versions from PyPI central
repository that satisfies the corresponding version constraint. To
enable the installation of an expected version combination of ML
libraries, Piecer customizes the dependency resolution rules of pip
to select our specified library version candidates (satisfying the
corresponding version constraints) from our dependency pool ğ·ğ‘.
Component 3: Analyzing Performance Inconsistencies. For each
ML pipeline, Piecer runs the baseline and all the variants to check
their performance inconsistencies across various library version
combinations. It focuses on three aspects of performance inconsis-
tencies: execution time ,memory usage , and evaluation metrics (e.g.,
Precision andRecall ). Specifically, Piecer performs two tasks:
â€¢Measuring performance inconsistencies: For each library API ref-
erenced by the pipeline, it leverages measurement functions to
capture the performance inconsistencies between the variant and
baseline. For example, Piecer adopts functions timeit .default_
timer() ,tracemalloc .get_traced_memory() andpynvml .nvml
DeviceGet-MemoryInfo() to measure the execution time, and CPU
and GPU memory usage of the changed APIs, respectively. The
evaluation metrics can be implemented with the aid of functions
sklearn .metrics.accuracy_score() andsklearn .metrics.recall_score
(), etc. For the pipelines involving deep learning models, Piecer re-
trains the model to capture the impacts of library version changes
precisely. To reduce the interference of random factors, it runs
each pipeline five times and averages the outcomes.
â€¢Recording the problematic library version combinations: Compared
with the baseline, Piecer records the library version combina-
tions (with the concerned APIs) that (1) induce significant perfor-
mance inconsistencies of pipelines (see definitions in Section 3.5);
(2) cause program crashes or NaN bugs, for further analysis.
3.2 Data Collection
We collected pipelines from Kaggle [2], the most popular crowd-
sourced platform for ML competitions. We select Kaggle pipelines
465ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Yibo Wang, Ying Wang, Tingwei Zhang, Yue Yu, Shing-Chi Cheung, Hai Yu, and Zhiliang Zhu
Component 1
Constructing ML Library Version PoolInputComponent 2
Generating Pipeline VariantsComponent 3
Analyzing Performance Changes
ML
pipelinespipeline
sklearn pandas
numpy scipy
numpy(â‰¤1.2.4, â‰¥0.24.2) (â‰¤ 1.0.1, â‰¥0.22)
ï¼ˆâ‰¥1.11.0)
(<1.23.0, ï¼1.16.5)(â‰¤ 1.7.32, â‰¥1.3.0)
Library ML library version pool
pandas [â€˜1.2.4â€™, â€˜1.1.5â€™, â€˜1.0.5â€™, â€˜0.25.3â€™, â€˜0.24.2â€™]
sklearn [â€˜1.0.1â€™, â€˜0.24.2â€™, â€˜0.23.2â€™, â€˜0.22.1â€™, â€˜0.22â€™]
scipy ['1.7.3', '1.5.4', '1.5.2', '1.4.1', '1.3.1 ', '1.3.0'']
numpy ['1.19.5â€™,â€™1.18.5â€™,â€™1.17.4â€™]Identifying the installable 
version candidates Baseline
Variant 1 Variant 2 Variant n ...Generating  pipeline variants with 
different library version combinations
Defining comparison rules
[â€˜pandas ', â€˜sklearn ', â€˜scipy â€™, â€˜numpy ']
Customizing pipâ€™s dependency 
resolving rules
Combination 2: [â€˜0.24.2â€™, â€˜0.22â€™, â€˜1.7.3', â€˜1.19.5']
â€¦pipCombination 1: [â€˜1.2.4', â€˜1.0.1â€™, â€˜1.7.3', â€˜1.19.5']Output
Baseline
Defective 
library version combinationswith concerned  
APIsVariant 1
Variant 2
Variant n...Performance changes
Memory
Execution 
time
Evaluation metrics
Crashes
NaN bugs
The newest versions of ML librariesApplication of 
Empirical Findings
Repository of 
defective APIs
<{API 1from lib1, 
API 2from lib2}, 
{lib 1> version 2.0 , 
lib2> version 3.4}, 
Lower Accuracy
 >
Repository of defective 
APIs sheds light on 
designing automated 
approaches to detecting 
PLC issues
Figure 3: The overall architecture of Piecer (Repository of defective APIs is described in Section 4)
as subjects to conduct our empirical study for four reasons: (1)
all the ML pipelines in a competition correspond to the unified
evaluation metrics for ranking, which enable us to compare the
outcomes across different combinations of library versions; (2) each
pipeline provides its corresponding source code and dataset, which
facilitate our reproduction; (3) competitions in Kaggle platform
cover a wide range of domains and techniques, which guarantee
the diversity of ML tasks; (4) the small size pipelines on Kaggle
have few interference factors (e.g., environmental dependencies),
which enables our analysis for the root causes of PLC issues. We did
not select open-source large-scale ML pipelines as subjects, because
they rarely provide (1) detailed training steps in documentation, (2)
complete datasets, and (3) all the required dependencies. Therefore,
we face challenges on deploying the complex pipelines. However,
the common ML library version combinations identified in Kaggle
pipelines are also adopted by large-scale ML pipelines (see statistics
in Section 4). As such, the empirical findings identified in Kaggle
pipelines can scale to more complex ML-based projects.
We collected the ML pipelines from Kaggle competitions that
satisfy three criteria. First, the competitions were hosted in the
last three years (from Jan 1, 2019 to Jul 31, 2021). The search re-
turned 168 competitions that involve 14,772 pipelines. Second, we
only kept the competitions that contain more than 100 competing
pipelines (i.e., popularity). Third, the pipelines should depend on
more than three ML libraries, including both ML frameworks and
data science libraries (i.e., complexity). With the above process, we
finally obtained 11,363 pipelines from 42 competitions.
In total, our collected pipelines depend on 112 ML libraries, in-
cluding 23 ML frameworks and 89 data science libraries. Among
them, the top five most frequently used libraries are Pandas (13,054),
Sklearn (7,821), Numpy (6,967), Tensorflow (3,279) and PyTorch (2,836).
Figure 4 shows the demographics of the 112 ML libraries. On
average, each ML library has 29,156 Â±338 downstream projects,
4,094,010Â±3,285 monthly downloads, 7,460 Â±121 stars, and 603Â±45
issues and 4,358Â±198 commits in their GitHub repositories. Accord-
ing to a ranked list of awesome ML libraries provided by a popular
GitHub project (updated weekly) [ 8], the 112 libraries cover 25
categories (including image & video processing, etc.). The above
statistics demonstrate that the ML libraries used in Kaggle pipelines
are also widely used in large amounts of open-source ML pipelines,
indicating their representativeness.
3.3 RQ1: Common ML Library Combinations
Study Methodology. A combination of ML libraries are typically
used in certain stages of ML pipeline workflows [ 38]. To granularly
identify common library combinations, we analyze the library usage
in pipeline workflow stages. Specifically, we perform two tasks in
(a) the 112 ML libraries used by collected pipelines
(b) demographics of the 112 ML libraries#Libraries in pipelines #Downstream projects 
#Downloads #Stars
#Issues #Commits
Figure 4: Description of the stages in ML pipeline
Table 1: Occurrences of ML library combinations in the pipelines
Occurrences DCO DCL FEMT MEData Oriented
StagesModel Oriented
Stages
[1,10) 125 80 386 425 23 2,124 641
[10,50) 34 13 66 60 5 161 73
[50,100) 10 6 9 8 0 18 16
[100,500) 2 11 16 14 2 11 21
[500,âˆ) 4 1 2 5 1 2 4
Avg 59 90 21 20 275 5 14
#Common 16 13 55 56 2 372 87
â€ Data Oriented Stages = DCO + DCL + FE; Model Oriented Stages = MT + ME.
Avg denotes the average occurrences of ML library combinations in each stage;
#Common denotes the number of common library combinations;
We consider a library combination is commonly adopted by ML pipelines
if its occurrences in each stage is above Avg.
the investigation: (1) Leveraging Jedi [3], a static analysis tool for
Python, we located 8,853 unique ML library APIs invoked by our
collected 11,363 pipelines. (2) Two authors of this paper assigned
label of workflow stages for each ML library API. The labels were
assigned based on the code comments in ML pipelines for stating
the API usage, or the description of the package (or sub-package),
in which the API was declared. For example, we considered API
xgboost .score() worked in the MEstage, since its declaration was
accompanied by a explanatory code comment â€œ #Evaluate model
performance â€ in the ML pipeline. We did not assign labels for the
APIs that ( i) did not have a description in its package or ( ii) the
two authors could not reach a consensus on the descriptions. We
employed Cohenâ€™s Kappa [73] to measure the consistencies between
the two authors. If the Kappa value was less than 0.9, the authors
needed to discuss to resolve disagreements. This iterative process
would stop once the Kappa value was equal to or greater than 0.9,
indicating substantial agreement. Eventually, we obtained 7,983
466Can Machine Learning Pipelines Be Better Configured? ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
Table 2: Statistics of top 5 common ML library combinations in single-stage
Data CO llection Data CL eaning Feature E ngineering Model T raining Model E valuationRankCombinations Occurrences Combinations Occurrences Combinations Occurrences Combinations Occurrences Combinations Occurrences
1Pandas 3,858 Numpy 1,745 Sklearn 1,295 Sklearn 1,503 Sklearn 5,940
2Numpy, Pandas,
PyTorch807 PyTorch 428 Numpy 701 Tensorflow 1,422 Tensorflow 284
3Pandas, PyTorch 553 Nltk 358 Keras 385 PyTorch 993 Sklearn, Tensorflow 258
4Nltk, Pandas 521 Tensorflow 332 Tensorflow 359 Keras 851 Keras, Sklearn 12
5OpenCV, Numpy,
Pandas, PyTorch487 Pandas 277 Pandas 357 Lightgbm 519Pytorch-lightning,
Sklearn11
â€ The top common ML library combinations in multi-stages are provided on our website http://piecer-plc.github.io
labeled ML library APIs. The labelling process took two months
for the two authors who have over two years ML development
experience to analyze and perform cross validation. We released
the dataset on the website for public scrutiny.
Results. By inspecting the 7,983 labeled ML library APIs, we ob-
served that the pipelines from Kaggle platform only involved five
stages of workflows: DCO ,DCL,FE,MT,ME. As aforementioned,
recent empirical studies [ 49,85] revealed that the majority of per-
formance issues or crashes occurred in the five stages. In our study,
we focus on these core stages of pipeline workflows, to identify
their common combinations of library usage.
Table 1 summarizes the occurrences of ML library combinations
in the single-stage/multi-stages of pipeline workflows. We identified
176, 112 and 32 library combinations in the DCO ,DCL and ME
stages, respectively. Since the open-source community provides
various solutions to help developers extract critical features from
raw data and automatically train their ML models, 480 and 513
library combinations were identified in the FEandMTstages of our
collected pipelines. Our results reveal that 16/176, 13/112, 55/480,
56/513 and 2/32 of library combinations were commonly adopted
by the majority of ML pipelines to implement DCO ,DCL,FE,MT
andMEstages, respectively (occurrences â‰¥Avg). The combinations
may include both ML frameworks and data science libraries. Table 2
illustrates the top 5 common library combinations in single-stage.
For example, 3,858 pipelines use Pandas inDCO stage, while 807
pipelines combines Numpy ,Pandas with PyTorch for data collection.
As shown in Table 1, during the synthesis of the data oriented
stages ( DCO ,DCL and FE), we identified the use of 2,316 library
combinations. 372 out of the 2,316 combinations occurred in more
than five pipelines. We also identified 755 ways of implementing
model oriented stages ( MTandME) using combinations of libraries.
87 of them were commonly used in our collected pipelines. The
top common ML library combinations used in multi-stages are
provided on our website. We observed that 1,134 pipelines adopts
a combination of Pandas ,Numpy and Sklearn for data oriented
stages. 725 pipelines combine TensorFlow with Sklearn for modeling
oriented stages.
Answer to RQ1: We identified 16, 13, 55, 56 and 2 common ML library
combinations in the DCO, DCL, FE, MT and ME stages, respectively. In
the data oriented stages (DCO, DCL and FE) and model oriented stages
(MT and ME), we identified 372 and 87 common ML library combinations,
respectively. Such common ML library combinations are adopted by the
majority of ML pipelines.
3.4 RQ2: Impacts of Different ML Library
Version Combinations
Study Methodology. We present the study methodology of RQ2 in
three aspects: Subject selection ,Generation rules of library version
combinations andExperiment setup .Subject selection. Since each pipeline corresponds to a series of
variants, the comparison experiment is time-consuming and would
take lots of machine resources. To guarantee the feasibility of our
RQ2 study, we set three filter conditions to select subjects from our
collected 11,363 pipelines:
â€¢1,490 pipelines that do not use common ML library combinations
in the single- or multi-stages are filtered out.
â€¢4,299 pipelines that involve the private datasets (not published
onKaggle ) are filtered out.
â€¢157 pipelines that use the private libraries (could not be found
from PyPI or GitHub repositories) are filtered out.
â€¢We restrict the dataset size of pipelines to 100GB. 2,037 pipelines
are filtered out according to this condition.
Eventually, we obtained 3,380 pipelines, involving 13 ML frame-
works and 77 data science libraries. The 90 ML libraries cover 80.4%
of popular ones we presented in Figure 4.
Generation rules of library version combinations. We deployed
Component 1 ofPiecer to take each pipeline as an input to construct
a dependency pool ğ·ğ‘={ğ‘‰ğ¿ğ‘–ğ‘ 1,ğ‘‰ğ¿ğ‘–ğ‘ 2,Â·Â·Â·,ğ‘‰ğ¿ğ‘–ğ‘ğ‘›}, by collecting all
the installable version candidates of the referenced ML libraries. To
reduce the computation complexity, in our study, we only consid-
ered popular versions of each ML library in ğ·ğ‘that are widely used
by downstream users (the popularity of library versions are pro-
vided on Libraries.io website). Leveraging Component 2 ofPiecer ,
we generate a set of pipeline variants with different ML library
version combinations. To this end, we define two generation rules
of ML library version combinations, to systematically explore their
impacts on pipelinesâ€™ performances:
â€¢Rule 1 : Varying version combinations of ML libraries in the single-
stage: For a given ML pipeline, Piecer iteratively varies the
version combinations of ML libraries used in each workflow
stage ( DCO ,DCL,FE,MTandME), while keeps the remaining
libraries to be the newest versions in our dependency pool ğ·ğ‘.
Letğ¿={ğ¿ğ‘–ğ‘1,ğ¿ğ‘–ğ‘ 2,Â·Â·Â·,ğ¿ğ‘–ğ‘ ğ‘š}be a ML library combination
(direct dependencies) used in a certain workflow stage. Each
libraryğ¿ğ‘–ğ‘ğ‘–âˆˆğ¿corresponds to a set of version candidates
ğ‘‰ğ¿ğ‘–ğ‘ğ‘–={ğ‘£1,ğ‘£2,Â·Â·Â·,ğ‘£ğ‘¡}in our dependency pool ğ·ğ‘.Piecer de-
termines a set of version combinations of ML libraries used in a
certain workflow stage as ğ¿ğ‘‰=ğ‘‰ğ¿ğ‘–ğ‘ 1Ã—ğ‘‰ğ¿ğ‘–ğ‘ 2Â·Â·Â·Ã—ğ‘‰ğ¿ğ‘–ğ‘ğ‘š, where
ğ¿ğ‘–ğ‘ğ‘–âˆˆğ¿and the symbol â€œÃ—â€ denotes Cartesian product .
â€¢Rule 2 : Varying version combinations of ML libraries in the multi-
stages: Piecer iteratively varies the version combinations of ML
libraries used in data-oriented stages ( DCO ,DCL and FE) and
model-oriented stages ( MTandME), while keeps the remaining
libraries to be the newest versions in our dependency pool ğ·ğ‘.
Letğ¿ğ‘‰(ğ·ğ¶ğ‘‚),ğ¿ğ‘‰(ğ·ğ¶ğ¿),ğ¿ğ‘‰(ğ¹ğ¸),ğ¿ğ‘‰(ğ‘€ğ‘‡)andğ¿ğ‘‰(ğ‘€ğ¸)be the
sets of installable version combinations of ML libraries used
inDCO ,DCL,FE,MTand MEworkflow stages, respectively
467ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Yibo Wang, Ying Wang, Tingwei Zhang, Yue Yu, Shing-Chi Cheung, Hai Yu, and Zhiliang Zhu
Table 3: Statistics of pipeline variants generated by Piecer
#Variants generated by Rule 1 #Variants generated by Rule 2
#Pipeline #VariantDCO DCL FE MT MEData
orientedModel
orientedOverall
architecture
3,380 74,373 357 447 867 4,333 530 12,741 21,359 33,639
â€ We filtered out the overlapping pipeline variants generated by Rules 1 and 2
(generated based on Rule 1 ).Piecer determines the ML library
version combinations in the multi-stages as follows: (1) Data-
oriented stages :ğ¿ğ‘‰(ğ·ğ‘ğ‘¡ğ‘)=ğ¿ğ‘‰(ğ·ğ¶ğ‘‚)Ã—ğ¿ğ‘‰(ğ·ğ¶ğ¿)Ã—ğ¿ğ‘‰(ğ¹ğ¸).
(2)Model-oriented stages :ğ¿ğ‘‰(ğ‘€ğ‘œğ‘‘ğ‘’ğ‘™)=ğ¿ğ‘‰(ğ‘€ğ‘‡)Ã—ğ¿ğ‘‰(ğ‘€ğ¸). (3)
Overall architecture :ğ¿ğ‘‰(ğ‘‚ğ‘£ğ‘’ğ‘Ÿğ‘ğ‘™ğ‘™)=ğ¿ğ‘‰(ğ·ğ‘ğ‘¡ğ‘)Ã—ğ¿ğ‘‰(ğ‘€ğ‘œğ‘‘ğ‘’ğ‘™).
Table 3 shows the statistics of pipeline variants. In total, Piecer
generated 74,373 variants for the 3,380 pipelines, based on the
generation rules of library version combinations. On average, each
pipeline corresponds to 22 Â±8 variants.
Experiment setup. For a ML pipeline, Piecer considers the newest
versions of referenced ML libraries as a version combination base-
line. Using Component 3 , we run the baseline and all its correspond-
ing variants, to capture the performance inconsistencies. Based on
confidence interval analysis [ 57], we consider the library version
combinations induce performance inconsistencies, if:
âˆƒğ‘€ğ‘’ğ‘¡ğ‘Ÿğ‘–ğ‘ ğ‘|ğ‘€ğ‘’ğ‘¡ğ‘Ÿğ‘–ğ‘ ğ‘£âˆ‰[Â¯ğ‘¥âˆ’ğ‘§ğ‘ âˆšğ‘›,Â¯ğ‘¥+ğ‘§ğ‘ âˆšğ‘›] (1)
whereğ‘€ğ‘’ğ‘¡ğ‘Ÿğ‘–ğ‘ ğ‘andğ‘€ğ‘’ğ‘¡ğ‘Ÿğ‘–ğ‘ ğ‘£denote the execution time ,memory
usage orcompetition scores of baseline and a pipeline variant, re-
spectively; Â¯ğ‘¥andğ‘ are the average value and standard deviation
ofğ‘€ğ‘’ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘  ğ‘£, respectively; ğ‘›denotes the total number of pipeline
variants. According to the parameter setting and ğ‘§table provided
in approach [ 80], we calculate the 99.9% confidence interval of the
mean and the ğ‘§value is taken as 3.29053. Note that, competition
scores (e.g., Precision andRecall ) are the metrics provided by Kaggle
for evaluating the pipelinesâ€™ outcomes.
We use Cohen â€™sğ‘‘[35] to measure how far the performance
inconsistent outcomes exceed the confidence interval. According to
formula (2), we divide the outcomes into two groups: performance
inconsistency group and normal group. Cohen â€™sğ‘‘is used to describe
the standardized mean difference between two group means.
ğ‘‘=Â¯ğ‘¥1âˆ’Â¯ğ‘¥2âˆšï¸‚
(ğ‘›1âˆ’1)ğ‘ 2
1+(ğ‘›2âˆ’1)ğ‘ 2
2)
ğ‘›1+ğ‘›2âˆ’2(2)
In this formula, Â¯ğ‘¥1and Â¯ğ‘¥2are the mean values of performance in-
consistency group and normal group, respectively; ğ‘›ğ‘–is the sample
size (i.e., number of outcomes in each group), and ğ‘ ğ‘–is the sample
variance of each group. According to approach [ 66],Cohen â€™sğ‘‘in-
terprets effect sizes in the meta-analysis: small effect = 0.2 - 0.5;
medium effect = 0.5 - 0.8; large effect = 0.8 and higher. In our study,
we consider the outcomes with large effect size as the significant
performance inconsistencies induced by pipeline variants.
Our study is conducted on three machines with the same config-
uration: Intel Xeon Silver 4110 machine with 128GB RAM, Ubuntu
16.04.7 LTS, and two NVIDIA Tesla V100 GPUs. The comparison
experiments took about eight months for three authors of this paper
to deploy and run the 74,373 variants of 3,380 pipelines.
Results. Among the 3,380 pipelines, 1,092 (32.3%) manifest signif-
icant performance inconsistencies on at least one variant. 42,488
and 536 variants (involving 2,323 pipelines) induce program crashes
andNaN bugs, respectively.
Figure 5 and Table 4 summarize the performance inconsistencies
of the 1,092 pipelines. We can observe that:
0000213174088873
071525265692128210292500
1523352181113571
900 -500 -100
Competition Score Execution Time Memory Usage
Pdiff:[10%, 20%)
Pdiff:[20%, 30%)
Pdiff:[30%, 40%)
Pdiff:[40%, 50%)
Pdiff:[50%, 60%)
Pdiff:[60%, 70%)
Pdiff:[70%, 80%)
Pdiff:[80%, 90%)
Pdiff:[90%, 100%)
Pdiff:[100%, +âˆ)
1010379242950116767
3308393111104126154190240298475
5121833611141525514
0 500500
#pipelines #pipelinesBaseline achieves 
better performanceVariants achieve 
better performance
900Pdiff:(0%, 10%)
500 900ğ‘ƒğ‘ƒğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘=|ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ ğ‘ğ‘âˆ’ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ ğ‘£ğ‘£|
ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ ğ‘ğ‘Figure 5: Performance inconsistencies: Baseline v.s. Variants
Table 4: Performance inconsistencies: Baseline v.s. Variants
Competition Score Execution Time Memory Usage #Pipeline #Variants
â‡‘ â‡‘â‡‘ 141 596
â‡‘ â‡‘â‡“ 97 318
â‡‘ â‡“â‡‘ 158 742
â‡‘ â‡“â‡“ 125 471
â‡‘ â‡“ - 143 404
â‡‘ â‡‘ - 128 367
â‡“ â‡‘â‡‘ 144 570
â‡“ â‡‘â‡“ 115 329
â‡“ â‡“â‡‘ 184 960
â‡“ â‡“â‡“ 151 526
â‡“ â‡“ - 154 406
â‡“ â‡‘ - 125 346
â‡‘ - - 249 587
â‡“ - - 275 614
- â‡‘ - 465 1,535
- â‡“ - 597 1,566
- -â‡‘ 644 2,488
- -â‡“ 652 2,422
â€œâ‡‘/â‡“â€ denotes that at least one pipeline variant achieves better/worse
performance than the baseline; â€œ-â€ denotes unchanged performances.
Table 5: Impacts of library versions on competition ranking
#pipeline variants that affect competition rankings
CompetitionsRise
/Fall
R: 0%â€“10%
R: 10%â€“20%
R: 20%â€“30%
R: 30%â€“40%
R: 40%â€“50%
R: 50%â€“60%
R: 60%â€“70%
R: 70%â€“80%
R: 80%â€“90%
R: 90%â€“100%
â†‘ 219 5 2 3 17 2 0 0 0 0Competition 1 [14]â†“ 262 0 7124 0 32 22 0 0 0
â†‘ 184 67 29 15 3 1 0 1 0 0Competition 2 [16]â†“ 120 38 20 30 8 22 16 5 0 0
â†‘ 142 38 12 0 0 0 0 0 0 0Competition 3 [17]â†“ 77 21 18 1 2 0 0 0 0 0
â†‘ 705 17 0 0 0 8 0 2 6 0Competition 4 [18]â†“ 677 35 9 2 0 7 12 0 0 0
â†‘ 1,144 62 4 13 10 12 35 18 1 0Competition 5 [19]â†“ 1,230 218 81 59 87 110 145 238 45 0
â†‘ 126 0 0 0 0 0 0 0 0 0Competition 6 [20]â†“ 52 0 71 0 0 0 0 0 0 0
â†‘ 188 1 0 1 0 0 0 0 11 0Competition 7 [21]â†“ 348 14 10 16 0 1 0 0 0 0
â†‘ 54 23 18 11 12 0 0 0 0 0Competition 8 [22]â†“ 59 17 5 1 0 4 3 1 0 0
â†‘ 24 2 2 0 0 0 0 0 0 0Competition 9 [23]â†“ 14 2 2 0 24 0 0 3 0 0
â†‘ 4 2 3 0 0 0 0 55 0 0Competition 10 [15]â†“ 0 9 0 0 0 0 0 0 0 0
R = (Rank ğ‘- Rank ğ‘£) / Rank ğ‘, where Rank ğ‘and Rank ğ‘£are competition rankings of the
baseline and variant, respectively.
â€¢For 399, 243 and 440 pipelines, changing the version combina-
tions of ML libraries from the baselines (i.e., the newest versions)
could achieve better performances in competition scores ,execu-
tion time and memory usage , respectively. The variants of 109
468Can Machine Learning Pipelines Be Better Configured? ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
Table 6: Statistics of the ML libraries whose varied versions significantly affect pipelinesâ€™ performances
#Problematic APIsMetrics Type ML LibrariesÃ—#pipelinesDCO DCL FE MT ME
Baseline betterPandasÃ—1,Opencv-pythonÃ—1,NltkÃ—1,NumpyÃ—1,TorchvisionÃ—1,AutokerasÃ—1
CatboostÃ—2,SklearnÃ—39,KerasÃ—52,TensorflowÃ—726 3 20 109 6
Competition Score
Baseline worsePandasÃ—1,TransformersÃ—1,Opencv-pythonÃ—2,NumpyÃ—2,SklearnÃ—9,KerasÃ—12,
TensorflowÃ—245 4 11 61 4
Baseline betterScipyÃ—3,SklearnÃ—135,Opencv-pythonÃ—18,NltkÃ—31,Category_encoders Ã—13,NumpyÃ—12
PandasÃ—10,GensimÃ—3,TransformersÃ—11,CatboostÃ—40,XgboostÃ—63,TensorFlowÃ—191
LightgbmÃ—58,KerasÃ—146,SpacyÃ—2215 22 80 348 28
Execution Time
Baseline worseAutokerasÃ—1,TorchvisionÃ—3,NumpyÃ—7,Opencv-pythonÃ—7,CatboostÃ—9,NltkÃ—12
PandasÃ—18,KerasÃ—682,TensorflowÃ—789,SklearnÃ—7955 14 42 159 14
Baseline betterOptunaÃ—1,SpacyÃ—1,CatboostÃ—1,TransformersÃ—2,NumpyÃ—2,NltkÃ—2,PytorchÃ—3
LightgbmÃ—3,PandasÃ—4,KerasÃ—4,XgboostÃ—5,SklearnÃ—11,TensorflowÃ—144 9 16 92 6 Memory Usage
Baseline worse KerasÃ—1,NumpyÃ—1,XgboostÃ—1,PandasÃ—2,SklearnÃ—3 3 4 3 7 5
#pipeline denotes the number of pipelines that adopt the ML libraries.
pipeline manifested over 50% of performance differences com-
pared with the baselines. For example, CFEC#1254 [11] adopts the
APIs of library Sklearn sklearn .neighbors.KNeighborsClassifier()
andsklearn .naive_bayes.GaussianNB() inMTworkflow stage.
Combining older version Sklearn 0.19.2 with other ML librariesâ€™
newest versions, the pipeline achieves significant improvement
inexecution time andmemory usage , compared with the baseline
(22.6s & 409.8MB v.s.451.2s & 2,337.9MB).
â€¢For 252, 340 and 209 pipelines, the baselines outperform all the
other ML library version combinations in competition scores ,ex-
ecution time and memory usage , respectively. In 496 pipelines,
the newest versions of ML libraries manifested over 50% of per-
formance differences, compared with the average outcomes of
variants. For example, TPS-#839 [32] uses the API of library Cat-
boost Catboost .CatBoostRegressor() inMTworkflow stage. The
baseline significantly outperforms the other pipeline variants in
execution time (14.0s v.s.141.6s).
â€¢As shown in Table 4, compared with baselines, the variants
of 1,057 pipelines achieved gain on certain performance met-
rics, with a drop of other ones. The variants of 141 and 151
pipelines manifest an overall rise and fall in performance metrics,
respectively. .....The.........results ...........indicate ......that..a..............significant ...............proportion ...of
................performance ...........changes .....are.....the........issues ............deserved ..........further ..............investiga-
.......tions, .........rather ......than.............trade-offs ............between ...........different ....................measurements.
In general, ML pipelines with different library combinations
more easily induce significant inconsistencies of execution time and
memory usage . 60.0% of 653 pipelines still manifest the inconsis-
tencies of competition scores on the variants. Table 5 illustrates ten
competitions whose rankings (determined by competition scores)
can be affected by changing the ML library versions in pipelines.
Interestingly, we observed that the rankings in Competitions 5 [19],
9[23] and 10[15] even changed dramatically on the pipeline vari-
ants. For example, among the 4,668 variants generated by Piecer
for the 249 pipelines in Competitions 5 [19], 1,299 and 2,213 variants
can cause the rise and fall in ranking, respectively. In particular,
MNIST#85 [ 27] moved up from 201 thto 5thplace in the ranking,
after changing the versions of libraries keras 2.7.0 and tensorflow
2.7.0 to keras 2.3.1 and tensorflow 2.1.1, respectively.
Piecer exposed PLC issues in 37 ML libraries, involving 890
APIs. Table 6 shows statistics of the ML libraries whose versions
significantly affect pipelinesâ€™ performances. We can observe that:
â€¢The defective ML libraries include both ML frameworks and
data science libraries. For each ML library, a proportion of APIs
perform best in the newest versions, while the other APIs achieve
the optimal performances in their old versions. For example,
TensorFlow provides 41, 125 and 65 APIs whose newest versions
negatively affect the pipelinesâ€™ competition scores ,execution timeandmemory usage , respectively. Besides, the three metrics reward
the 200 APIs of TensorFlow in the newest versions.
â€¢Among the five workflow stages, MTinvolves the most library
APIs (479 APIs from 22 libraries) that can induce PLC issues into
892 ML pipelines.
Answer to RQ2: Among the 3,380 deployable ML pipelines, 1,092 (32.3%)
manifest significant performance inconsistencies on at least one variant.
For 399, 243 and 440 pipelines, changing the version combinations of ML
libraries from the baselines (i.e., the newest versions) could achieve better
performances in competition scores, execution time and memory usage,
respectively. For 252, 340 and 209 pipelines, the baselines outperform all
the other ML library version combinations in competition scores, execution
time and memory usage, respectively.
Implication: Due to the version constraints between dependencies, or
the limitations of the installation environment, various library version
combinations are likely to be adopted in ML pipelines. Understanding their
impacts on pipeline performances helps developers avoid PLC issues.
3.5 RQ3: Root Causes of Performance
Inconsistencies
Study Methodology. To further analyze the root causes of PLC
issues, we randomly sampled: (a) 294 out of the 1,092 pipelines
that induced significant performance inconsistencies after varying
library versions, (b) 329 out of the 2,276 pipelines causing crashes,
and (c) 42 out of the 47 pipelines inducing NaN bugs. This sampling
approach [ 53] guarantees that the root causes distilled from the
sampled pipelines can be generalized to the whole dataset with a
95% confidence level and a 5% confidence interval. Specifically, we
performed four tasks as follows.
â€¢For each pipeline variant, we identified a collection of ML library
APIsğ´={ğ‘1,ğ‘2,Â·Â·Â·,ğ‘ğ‘›}that induced PLC issues, using Com-
ponent 3 ofPiecer . Letğ‘1.ğ¿ğ‘–ğ‘ğ‘£ be the ML library version that
contains API ğ‘ğ‘–âˆˆğ´. In total, we obtained 577 APIs from 452
library versions in the 19,048 pipeline variants.
â€¢We performed an in-depth analysis on defective APIs by inspect-
ing their historical evolution data across the ML library versions,
including release notes, issues, code commits and code comments
(on GitHub repositories). To understand whether the PLC issue of
an APIğ‘ğ‘–âˆˆğ´was affected by a combination of library versions,
we divided the APIs into two groups:
â€“Group 1 : We consider a PLC issue is induced by the indi-
vidual library , if it satisfies: (1) the issue is exposed in API
ğ‘ğ‘–âˆˆğ´; (2) among the library versions used by the pipeline
variant, only ğ‘ğ‘–.ğ¿ğ‘–ğ‘ğ‘£ is inconsistent with that in baseline. 295
APIsğ‘ğ‘–âˆˆğ´inducing PLC issues in 9,055 pipeline variants fall
into Group 1 . We considered the PLC issues were induced by
the evolution of library ğ‘ğ‘–.ğ¿ğ‘–ğ‘ğ‘£ , and focused on its historical
data for inspection.
469ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Yibo Wang, Ying Wang, Tingwei Zhang, Yue Yu, Shing-Chi Cheung, Hai Yu, and Zhiliang Zhu
pipeline variant pipeline variant
â€¦
API with PLC issues Library version is inconsistent with baseline
â€¦Orderly connected by a pipeline
T ype A
Indirect dependency interferenceT ype B
Data flow interferenceâ€¦
Figure 6: Usage patterns of defective library version combinations
â€“Group 2 : As shown in Figure 6, we divided the PLC issues
induced by combinations of library versions into two types:
Type A. Suppose a pipeline variant directly uses APIs ğ‘ğ‘–and
ğ‘ğ‘—from different ML libraries, and ğ‘ğ‘–transitively invokes the
APIs from library ğ‘ğ‘—.ğ¿ğ‘–ğ‘ğ‘£ . We consider a PLC issue is induced
byindirect dependency interference , if it satisfies: (1) the
issue is exposed in API ğ‘ğ‘–âˆˆğ´; (2) among the library versions
used by the pipeline variant, only ğ‘ğ‘—.ğ¿ğ‘–ğ‘ğ‘£ is inconsistent with
that in baseline. We then consider { ğ‘ğ‘–.ğ¿ğ‘–ğ‘ğ‘£ ,ğ‘ğ‘—.ğ¿ğ‘–ğ‘ğ‘£ } is a defec-
tive library version combination. For the 98 defective APIs in
3,799 pipeline variants falling into Type A , we inspected their
historical evolution data.
Type B. Type B. Suppose APIs ğ‘ğ‘—,ğ‘ğ‘˜,Â·Â·Â·,ğ‘ğ‘¢andğ‘ğ‘–from
different libraries are orderly connected by a pipeline, and the
outcomes of APIs ğ‘ğ‘—,ğ‘ğ‘˜,Â·Â·Â·,ğ‘ğ‘¢can affect the inputs of ğ‘ğ‘–. We
consider a PLC issue is induced by data flow interference , if
it satisfies: (1) the issue is exposed in API ğ‘ğ‘–âˆˆğ´; (2) among
the library versions used by the pipeline variant, only ğ‘ğ‘—.ğ¿ğ‘–ğ‘ğ‘£
is inconsistent with that in baseline; (3) ğ‘ğ‘–does not invoke
APIs fromğ‘ğ‘—.ğ¿ğ‘–ğ‘ğ‘£ . We then consider { ğ‘ğ‘—.ğ¿ğ‘–ğ‘ğ‘£ ,ğ‘ğ‘˜.ğ¿ğ‘–ğ‘ğ‘£ ,Â·Â·Â·,
ğ‘ğ‘¢.ğ¿ğ‘–ğ‘ğ‘£ ,ğ‘ğ‘–.ğ¿ğ‘–ğ‘ğ‘£ } is a defective library version combination. We
inspected the historical evolution data of defective API ğ‘ğ‘—and
analyzed the impacts of defective library version combination.
â€¢We followed an open coding procedure [36], a widely-used ap-
proach for qualitative research, to distill and categorize the root
causes of performance inconsistencies. Initially, two authors of
this paper, who had over two years ML development experience,
independently analyzed the release notes, issues, code commits
and code comments of 577 defective APIs across the library ver-
sions in variant and baseline. After the first round of analysis
and labeling, the two authors gathered to compare and discuss
their results, in order to adjust the taxonomy, with the help of a
third author to resolve conflicts. In this manner, we constructed
the pilot taxonomy. This led to a more clear-cut labeling strategy.
Next, the first two authors continued to label the root causes of
the remaining 306 APIsâ€™ performance inconsistencies and itera-
tively refined the results. The conflicts of labeling were further
discussed during meetings and resolved by the third author. We
adjusted the pilot taxonomy and obtained the final results.
â€¢For each defective API, we identified all the pipeline variants that
use it from the same library version, to further inspect whether
they have similar performance inconsistencies. Such inspections
helped us understand the triggering conditions of PLC issues.
Results. For the 577 defective APIs, we identified the root causes
of PLC issues by analyzing related library release notes, issues andcode commits. We identified four common root causes that can
arise in two scenarios: performance impacts induced by individual
library andinduced by library version combinations .
Performance impacts induced by individual ML library. PLC
issues of 164 defective APIs were affected by one library version.
The issues share four common root causes below.
â€¢API Optimization (96/164 = 58.5%). For 96 defective APIs, we
found descriptions in the historical evolution data for declaring
the optimization for hardware support ,data processing , and cal-
culations , etc. For example, in NLPDT#plyger [ 29], the memory
usage of API Transformers .trainer() is changed from 2MB to
91MB, when downgrading library Transformers to 4.11.3 or
lower versions. In commit log 7a0adbb [13], developers stated
that they added support for gradient checkpointing in BERT since
Transformers 4.12.0 for optimizing memory performance. The
API optimization results in the improvement of execution time
(36/96), memory usage (32/96) and competition scores (28/96).
On average, the library versions containing non-optimized APIs
still have 45,647 monthly downloads in PyPI.
â€¢Default Hyperparameter Changes (16/164 = 9.8%). For 16 de-
fective APIs, developers added, deleted, or changed the values of
default hyperparameters for training ML models, leading to per-
formance inconsistencies across different library versions. The
above clues can be located in the historical evolution data of
libraries. For example, the release note [ 30] of library Sklearn
0.22.0 states that developers change a default hyperparameter
â€™gammaâ€™ in API Sklearn .svm.SVC() . We observed that the accu-
racy of MNIST#85 [ 27] rose from 0.143 to 0.986 after upgrading
Sklearn to 0.22.0 or higher versions.
The symptoms of changing default hyperparameters are mani-
fested as the improvement of execution time (5/16), memory us-
age (7/16), competition scores (2/16) or cause NaN bugs (2/16). For
each API with changed default hyperparameters, all the pipeline
variants using it could reproduce the symptoms of performance
changes, in the cases that developers did not customize the argu-
ments. However, such library versions with poor performances
still have 905,851 monthly downloads on average.
â€¢Technical Debts (5/164 = 3%). For 5 defective APIs, we found
declarations in their code comments stating that there are techni-
cal debts in the old library versions. Developers either considered
them as experimental APIs or incomplete implementations. For
example, in the code comments of API Sklearn .ensemble.HistGra-
dientBoostingClassifier() , developers explained that they improved
this experimental API since Sklearn 0.24.2. We observed that
the memory usage of CFECII#385 [ 12] reduced from 105MB to
55MB, after upgrading Sklearn to 0.24.2 or higher versions.
The symptoms of technical debts are manifested in the bad perfor-
mance of execution time (2/5), memory usage (2/5) or competition
scores (1/5) . By inspection, for each API with technical debts,
all the pipeline variants adopting it could manifest the similar
performance changes. This means that such PLC issues can be
easily triggered if any ML pipeline uses our identified defective
APIs. On average, the above library versions with technical debts
have 589,3827 monthly downloads.
â€¢API Breaking Changes (47/164 = 28.7%). For 47 defective APIs,
developers introduced breaking changes (i.e., API addition ,re-
moval and signature changes ) during library evolution. They
470Can Machine Learning Pipelines Be Better Configured? ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
induced 41,204 crashes and 492 NaN bugs in the pipeline vari-
ants. Digging out the above API breaking changes in the popular
library versions can help developers avoid compatibility issues.
Performance impacts induced by ML library version combina-
tions. According to the definition of Types A andBissues in Group
2,....for.....the...........defective ..........library ..........version ..................combinations, .....the......root.........causes ...of
...........problems .......may .......arise .......from ......one.....ML...........library, .....but.....the.......PLC ........issues .....are
..........actually ..........exposed ...in.....the......APIs ......from .......other ............libraries, ....due...to.....the...............interactions
..........between ...........libraries ...in.....the............pipelines. By inspecting historical evolution
data of the concerned defective APIs, we observed that the root
causes can be API Optimization ,Default Hyperparameter Changes ,
Technical Debts orAPI Breaking Changes . In total, we identified PLC
issues in 3,720 pipeline variants that are affected by 106 defective
API combinations from different ML library versions.
â€¢Type A: Indirect Dependency Interference (41/106 = 38.7%). We
located 41 Type A library version combinations, involving 57
APIs from 10 ML libraries. 2,511 pipeline variants adopting the
defective library version combinations manifested the poor per-
formance of execution time (15/41), memory usage (11/41), com-
petition scores (5/41) or induced crashes (7/41) and NaN bugs
(3/41). For example, pipeline CFEC#1211 [ 10] directly depends on
ML libraries { Sklearn 1.0.1, Pandas 0.25.0} in the MTstage. The
pipeline invokes API Sklearn .logisticRegression() , which transi-
tively depends on API Pandas .rolling() from library Pandas .
We observed the memory usage of Sklearn .logisticRegression()
changed from 593MB to 1,329MB after downgrading Pandas
to version 0.24.2. After investigation, we found an issue Pan-
das#25893 [ 25] that API Pandas .rolling() suffered from memory
bugs in Pandas 0.24.2. The combination of Sklearn andPan-
dasis used by 19,774 pipelines on Kaggle and 1,616 of them
invoke Sklearn .linear_model.logisticRegression() . Such defective
versions are liable to induce PLC issues.
â€¢Type B: Data Flow Interference (65/106 = 61.3%). We located 96 Type
Blibrary version combinations from 4,972 pipeline variants, in-
volving 121 APIs from 15 ML libraries. The above defective library
version combinations caused inferior performance of execution
time (24/96), memory usage (16/96), competition scores (19/96) or
induced crashes (3/96) and NaN bugs (3/96). For example, pipeline
NLPDT#785 [ 28] invokes APIs Nltk .stem.porter.PorterStemmer() ,
Sklearn .extraction. text.TfidfVectorizer() andCatboost .CatBoost
Classifier() from libraries { Nltk 3.6,Sklearn 1.0.1, Catboost
1.0.3}. The outcomes of former two APIs can affect the inputs of
Catboost .CatBoostClassifier() via data flow interactions in the
pipeline, while the former two APIs are not invoked by the latter
one. After downgrading Nltk to version 3.5, we observed the ex-
ecution time of Catboost .CatBoostClassifier() varied from 16s to
36s. The combination of Nltk ,Sklearn andCatboost is adopted
by 310 pipelines on Kaggle , 42% of them invoked the concerned
three APIs. Such PLC issues can be easily triggered since the
buggy version Nltk 3.5 has 831,749 monthly downloads.
Answer to RQ3: By analyzing the historical evolution data of 577 defective
APIs, we identified four types of essential root causes of inducing PLC issues.
PLC issues exposed in 164 defective APIs are affected by one library version.
PLC issues in 3,720 pipeline variants are caused by combinations of APIs
from different library versions.
Implication: Future research may focus on designing approaches to de-
tecting and repairing PLC issues by automatically mining and monitoring
the evolution of defective ML library APIs.4 APPLICATION OF EMPIRICAL FINDINGS
Repository of Defective APIs. Let a 3-tuple ğ·=<ğ´,ğ¿ğ‘‰,ğ‘ƒ >
be a defective API (combination) whose invocation(s) potentially
trigger PLC issues, where ğ´denotes the signature(s) of API (com-
bination);ğ¿ğ‘‰denotes the defective library version range; ğ‘ƒde-
notes the aspects of performance changes (e.g., execution time,
memory usage and Precision ). For example, <{Nltk.ğ‘ ğ‘¡ğ‘’ğ‘š.ğ‘ğ‘œğ‘Ÿ -
ğ‘¡ğ‘’ğ‘Ÿ.ğ‘ƒğ‘œğ‘Ÿğ‘¡ğ‘’ğ‘Ÿğ‘†ğ‘¡ğ‘’ğ‘šğ‘šğ‘’ğ‘Ÿ (),Catboost.ğ¶ğ‘ğ‘¡ğµğ‘œğ‘œğ‘ ğ‘¡ğ¶ğ‘™ğ‘ğ‘ ğ‘ ğ‘–ğ‘“ğ‘–ğ‘’ğ‘Ÿ ()},{Catboo -
st=1.0.3,Nltk=3.5},ğ¸ğ‘¥ğ‘’ğ‘ğ‘¢ğ‘¡ğ‘–ğ‘œğ‘›ğ‘¡ğ‘–ğ‘šğ‘’ >denotes an API combina-
tion of Nltk.ğ‘ ğ‘¡ğ‘’ğ‘š.ğ‘ğ‘œğ‘Ÿğ‘¡ğ‘’ğ‘Ÿ.ğ‘ƒğ‘œğ‘Ÿğ‘¡ğ‘’ğ‘Ÿğ‘†ğ‘¡ğ‘’ğ‘šğ‘šğ‘’ğ‘Ÿ ()andCatboost.ğ¶ğ‘ğ‘¡ğµğ‘œğ‘œ
ğ‘ ğ‘¡ğ¶ğ‘™ğ‘ğ‘ ğ‘ ğ‘–ğ‘“ğ‘–ğ‘’ğ‘Ÿ()in libraries Catboost 1.0.3 and Nltk 3.5, respectively,
which achieves worse performance in execution time. Based on our
empirical findings in RQs1-3, we construct a repository containing
164 defective APIs and 109 API combinations, involving 19 ML
libraries. Leveraging the defective API repository, we can capture
the PLC issues in real-world ML pipelines and ML libraries.
Our study aims to answer the following two research questions:
â€¢RQ4: Do real-world ML complex pipelines use the identified
defective library APIs?
â€¢RQ5: Can the repository of defective APIs help capture real
PLC issues?
Experiment Setup. To answer RQ4, we collected large-scale repre-
sentative ML pipelines from GitHub and Hugging Face [1] platforms
released by leading IT companies (including Microsoft ,Google ,Ten-
cent andAlibaba ) as subjects. The large-scale industrial pipelines
typically handle real-world challenging tasks, which are typically
more complex than the specific competition tasks implemented by
Kaggle pipelines. We selected Hugging Face platform because it
is the most popular AI model repository, where users can share
pre-trained models, datasets, and demos of ML projects [ 72,76]. To
achieve this, we formulated search keywords as { topic : machine-
learning, topic : deep-learning}Ã—{org:Microsoft ,org:Google ,org:
Tencent ,org:Aalibaba }. The search returns 124 and 34 ML pipelines
from GitHub and Hugging Face , repectively. Table 7 shows the sta-
tistics of collected large-scale pipelines. On average, they achieve
1,660 stars on GitHub and 262,248 downloads on Hugging Face (pop-
ular), have 53,583 KLOC (large in scale), depend on 5 ML libraries
and 136 ML library APIs (complex). As we discussed in Section 3.2,
we face challenges on deploying large-scale pipelines, due to lack-
ing detailed training steps in documentation, complete datasets, or
required dependencies. For the collected ML pipelines, we inves-
tigate whether they use defective APIs in the problematic library
version combinations identified in our repository.
To answer RQ5, we collected 1,064 pipelines as subjects from
the 17 competitions hosted between 1 August 2021 and 1 June 2022
onKaggle platform. Eventually, 497 out of 1,064 pipelines have
been successfully deployed. Among them, 303 and 194 pipelines
invoke 80 (80/164=48.8%) defective APIs and 69 (69/106=65.1%)
API combinations in our repository, respectively. For each pipeline
using defective APIs, we checked if the performance inconsisten-
cies, crashes and NaN bugs can be exposed on the corresponding
problematic library versions. Since Kaggle platform has no issue
trackers, we posted the validated results (i.e., performance changes
across various library versions) as comments on the pipelinesâ€™ web-
sites to warn pipeline developers/users against PLC issues. For the
43 defective APIs invoked by 17 pipelines that achieve worse perfor-
mances on ML librariesâ€™ newest versions, we also reported the PLC
issues to their issue trackers to ask library developers for validation.
471ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Yibo Wang, Ying Wang, Tingwei Zhang, Yue Yu, Shing-Chi Cheung, Hai Yu, and Zhiliang Zhu
Table 7: Statistics of collected real-world large-scale pipelines
Stars/
DownloadsKLOC# ML
Libraries# ML
Library APIs
Min Max Avg Min Max Avg Min Max Avg Min Max Avg
GitHub
Pipelines9 20,249 1,660 312,543,862 59,513 1 18 4 21,510 110
Hugging Face
Pipelines107 6,057,167 262,248 691 447,470 47,653 1 29 6 11,499 161
Microsoft : 86 pipelines, Google : 48 pipelines, Tencent : 5 pipelines, Alibaba : 19 pipelines
Table 8: Experiment results of RQ4
ğ‘€1 ğ‘€2 ğ‘€3 ğ‘€4
GitHub Pipelines 101 55 18 123
Hugging Face Pipelines 31 14 14 73
Sum 142 69 18 184
ğ‘€1: #Pipelines using defective ML library version combinations in our repository;
ğ‘€2: #Pipelines using the defective APIs (combinations) in our repository;
ğ‘€3: #Defective library version combinations used by the pipelines;
ğ‘€4: #Defective APIs (combinations) used by the pipelines
{Sklearn , xgboost }
VersionsScoreTime 
(ms)Memory
(MB){Sklearn , xgboost }
VersionsScoreTime
(ms)Memory
(MB)
{1.0.1 , 1.5.1 } 0.622 3755.792 2330.385 {0.22.1 , 1.4.2 } 0.650 4207.324 2354.084
{1.0.1 , 1.2.1 } 0.653 3807.698 2649.305 {0.22.1 , 1.1.1 } 0.652 7974.568 2648.538
{1.0.1 , 1.1.1 } 0.652 8151.661 2649.224 {0.22.1 , 1.0.2 } 0.656 7451.785 2647.906
{1.0.1 , 1.0.2 } 0.656 7783.346 2649.181 {0.22.1 , 0.90} 0.654 4743.717 2329.738
{1.0.1 , 0.90} 0.654 4634.113 2649.073 {0.22.0 , 1.5.1 } 0.622 3752.547 2357.178
{0.24.2 , 1.5.1 } 0.621 3763.312 2330.292 {0.22.0 , 1.4.2 } 0.650 4168.140 2354.079
{0.24.2 , 1.1.1 } 0.652 7926.313 2651.979 {0.22.0 , 1.0.2 } 0.657 7664.165 2647.902
{0.24.2 , 1.0.2 } 0.655 7645.457 2651.934 {0.22.0 , 0.90} 0.654 4683.703 2330.310
{0.22.1 , 1.5.1 } 0.622 3772.575 2357.182 {0.20.3 , 1.0.2 } 0.652 406.435 2649.0100.621
2651.9790.657
406.4352329.738
Figure 7: A PLC issue GoogleBrain-VPP #1686 [24] reported by us
Results of RQ4. Table 8 reports the experiment results of RQ4.
Among the collected 158 large-scale ML pipelines, 142 (89.9%) of
them depend on the identified defective ML library version combi-
nations in our repository, and 69 of them invoke the corresponding
defective APIs (combinations). In total, the collected large-scale ML
pipelines involve 18 defective library version combinations (75%)
and 184 defective APIs (combinations) (67.4%) in our repository.
For example, API cv2.imread () from library OpenCV is used by 19
ML pipelines in DCO stage, which may induce significant perfor-
mance inconsistencies in execution time. The results indicate that
our empirical findings identified in Kaggle pipelines can scale to
more complex industrial pipelines.
Results of RQ5. Table 9 reports the statistics of our experiment
results. In total, we exposed PLC issues in 309 out of 497 real-world
pipelines. 208 (67.3%) of them were induced by one ML library
version, while the remaining 101 (32.7%) of pipelines were caused
by the combinations of ML library versions. The PLC issues involve
58 defective APIs and 42 API combinations. Specifically, 251, 50 and
9 pipelines manifested in performance inconsistencies, crashes and
NaN bugs, respectively. We can observe that 188 (37.8%) pipelines
invoking the defective API (combination) did not reveal PLC issues.
The conditions of triggering such issues may be affected by many
factors, such as hyperparameters and dataset size, which deserve
further investigations in the future research.
Encouragingly, twenty pipeline developers shown their great
interests in our detailed diagnosis info of PLC issues generated
byPiecer . For example, as shown in Figure 7, Piecer â€™s testing re-
sults pointed out that the execution time of pipeline GoogleBrain-
VPP#1686 [ 24] changed dramatically across different version com-
binations of Sklearn andXgboost . Developers confirmed the issue
and left a comment: â€œ Thanks for this valuable information. I will
surely try and compare different versions. â€
For the 43 defective APIs that achieve worse performance on ML
librariesâ€™ newest versions, we filed them into 17 issue reports onGitHub. 7 (41.2%) of them had been quickly confirmed by developers
and were being fixed based on our testing results. 6 issues (35.3%)
were acknowledged by the developers to be worthy of further
investigation. Developers considered Xgboost#8033 were not bugs.
They had to improve some aspects of performance at the cost of
other metrics. The remaining reports are pending probably because
of inactive project maintenance.
For example, we reported a PLC issue to library Catboost whose
APICatboost .CatBoostClassifier() in the latest version nearly con-
sumes five times memory, compared with that in versions â‰¤0.10.3.
Developers quickly confirmed this issue and commented that â€œ I
could reproduce your results and saw a jump in RAM consumption
from 34 MB to 150 MB. â€
The experiment results demonstrate that our defective API repos-
itory shed light on designing automated approaches to detecting
PLC issues on real-world pipelines and ML libraries.
5 IMPLICATIONS
For ML pipeline developers. Our empirical study reveals the perva-
siveness and seriousness of PLC issues in ML pipelines. Developers
can utilize Piecer or our provided defective API repository to se-
lect version combinations of ML libraries when optimizing the
performance of their pipelines.
For ML library vendors. Our empirical findings show that an ML
library may achieve worse performance on its latest version when
working with other ML libraries in the pipelines. ML library devel-
opers should consider common library combinations and perform
integration testing before releasing new versions.
For SE researchers. For SE researchers, future research can focus
on designing promising techniques to detect and repair PLC issues.
For tool builders. Tool builders can leverage our defective API
repository and Piecer to recommend appropriate version com-
binations of ML libraries to pipeline developers when maintain-
ing/evolving ML libraries.
We hope that this paper can inspire a symbiotic ecosystem where
researchers, tool builders, and library vendors work together to
assist developers combat PLC issues.
6 THREATS TO VALIDITY
Internal Validity. Our empirical study involved manual effort,
which might introduce subjectivity and bias. To mitigate this threat,
we followed an open coding scheme where two authors indepen-
dently checked and cross-validated all the results. We employed
Cohenâ€™s Kappa [73] to measure the consistencies between the two
authors. If the Kappa value was less than 0.9, the authors needed
to discuss to resolve disagreements. To enhance transparency and
accountability, we made our dataset publicly accessible for scrutiny.
External Validity. The external validity concerns about the gener-
ality of our results. To mitigate such a threat, we collected 11,363
high-quality ML pipelines from Kaggle platform as subjects to con-
duct our empirical study. The pipelines depend on 112 popular ML
libraries, including 23 ML frameworks and 89 data science libraries.
Besides, we also collected large-scale representative ML pipelines
from GitHub and Hugging Face [1] platforms released by leading
IT companies as subjects, to investigate whether they use defective
APIs in the problematic library version combinations identified in
our repository. Our empirical findings obtained based on such a
representative dataset can be generalized to more ML pipelines.
472Can Machine Learning Pipelines Be Better Configured? ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
Table 9: Results of RQ5: PLC issues identified in real-world pipelines and ML libraries
PLC Issues Identified in 497 Real Pipelines (# pipeline ID )
Performance Impacts
Induced by One ML
Library Version#36502â†“â†“â†“, #36523â†‘â†‘â†‘, #36544â†“â†“, #36567â†“â†‘, #36575â†“â†‘â†‘, #36577â†‘â†“â†‘, #36586â†“â†“, #36590â†“, #36594â†‘, #36600â†‘â†“â†‘, #36616â†‘â†“â†“, #36618â†“â†“, #36625â†‘â†‘â†“, #36630â†“â†‘â†“,
#11094â†“â†“â†“, #11119â†‘â†“, #11167â†‘â†‘â†“, #11169â†‘, #31772â†“â†“â†“, #31773â†“â†“, #31776â†‘â†“â†‘, #31784â†‘â†‘, #31785â†“â†“â†‘, #31824â†“â†“â†“, #31856â†‘â†‘, #31870â†“â†“â†‘, #31883â†“, #31902â†‘â†“,
#37112â†“â†‘â†“, #37117â†“â†‘, #37124â†“â†‘â†‘, #37258â†“â†“, #37259â†“, #37261â†“â†“â†‘, #37297â†“â†“â†“, #37298â†‘â†“, #37317â†“â†“â†“, #37321â†“â†“â†‘, #37335â†‘, #37403â†“, #37410â†“â†“â†‘, #37426â†“â†“â†“,
#36151â†“â†‘â†‘, #36153â†“â†“â†‘, #36169â†“â†“â†“, #36173â†“â†‘â†“, #36199â†“â†“, #36209â†“â†“, #36237â†“â†‘â†‘, #36326â†“â†“, #36333â†‘, #36335â†“, #36471â†‘â†‘â†‘, #36476â†“â†“, #36488â†“â†‘, #36501â†‘â†‘,
#36634â†“â†‘â†“, #36635â†‘â†‘, #36637â†‘, #36639â†‘â†‘, #36641â†“â†‘, #36642â†‘â†“â†‘, #36647â†“â†‘â†‘, #36649â†“, #36690â†‘â†‘â†‘, #36734â†“â†‘â†‘, #36916â†“â†“, #36938â†‘â†“â†‘, #37107â†‘â†‘, #37111â†‘â†‘,
#33431â†‘â†“, #33445â†“â†‘, #33468â†“â†‘â†‘, #33472â†‘â†“, #33487â†“â†“, #33514â†“â†‘â†“, #33600â†“â†“â†‘, #33602â†“, #33603â†“â†‘â†‘, #33620â†“, #33659â†‘â†“â†“, #33668â†‘â†“â†‘, #33704â†‘â†“, #33705â†“â†“,
#33125â†‘â†“, #33129â†“â†‘, #33147â†“, #33171â†“â†‘â†“, #33176â†“â†“â†‘, #33185â†“â†‘, #33194â†“â†“, #33253â†“â†“, #33275â†“, #33288â†“â†‘, #33325â†“â†‘â†“, #33372â†“â†“, #33377â†‘â†“, #33409â†‘â†“â†‘,
#34197â†“â†“, #34198â†“â†“â†“, #34207â†“â†“, #34215â†“â†‘â†“, #34220â†‘â†“â†“, #34229â†‘â†“, #34241â†“â†“, #34243â†‘â†“, #34243â†“, #34248â†“, #34327â†“â†“â†“, #34328â†“, #34331â†‘, #34333â†“â†‘â†‘,
#31920â†“â†“â†‘, #31950â†“â†‘, #31975â†“â†“â†‘, #32053â†‘â†‘, #32292â†‘, #32347â†“, #32427â†‘â†‘â†“, #32449â†‘â†‘â†“, #32532â†‘, #32564â†“, #32569â†‘â†‘â†‘, #32574â†“, #32835â†“â†‘, #33123â†“â†“,
#33797â†“â†“â†‘, #33801â†“â†‘, #33807â†“â†“, #33823â†‘â†“â†“, #33941â†‘â†“, #33948â†‘â†“â†‘, #33981â†‘â†‘â†“, #34150â†“â†‘, #34154â†“â†‘, #34159â†‘â†“â†“, #34185â†‘â†‘â†“, #34186â†“â†“â†“, #34187â†“â†“â†“,
#34334â†‘, #34367â†“â†‘â†‘, #34388â†“, #34442â†“, #34463â†“â†“â†‘, #34474â†‘â†“â†“, #34601â†‘â†‘, #34658â†“â†‘â†‘, #34789â†“â†“, #35760â†‘â†“, #35776â†“, #36124â†“â†‘, #36128â†“â†“, #36146â†“â†“,
#37445â†“â†“â†“, #37652â†“â†‘â†“, #37731â†“â†‘, #38020â†‘â†‘â†‘, #38107â†‘â†“â†‘, #38150â†“, #31915â†“â†‘, #33423â†“, #33789â†“â†‘â†“, #34767, #34923, #36172, #36252, #36187, #36214, #33335,
#11180, #31900, #31947, #31997, #32009, #32253, #32518, #32560, #32566, #32607, #32923, #32928, #33138, #33193, #33335, #33346, #33721, #33724, #33346,
#33742, #33754, #33770, #33776, #33790, #33796, #33811, #33812, #33877, #33878, #33913, #33886, #34068, #34116, #34332, #34336, #34352, #37763
Type A#11087â†“â†“â†“, #11166â†“â†“, #11193â†“â†“â†‘, #11195â†‘â†“â†‘, #11224â†“â†“, #31751â†‘â†‘, #31753â†“â†‘â†‘, #31770â†‘â†‘, #32490â†‘â†‘, #33247â†‘â†“â†‘, #33274â†‘â†‘â†‘, #33302â†“â†‘, #33303â†“â†“â†‘, #33329â†‘â†‘,
#33341â†“â†“, #33342â†“â†“â†‘, #33394â†“â†‘â†‘, #33395â†“â†“â†‘, #33398â†‘â†“â†“, #33437â†‘â†“â†‘, #33439â†“â†“â†‘, #33539â†“â†“â†‘, #33744â†‘â†‘â†“, #33805â†“â†“â†“, #33837â†‘â†“â†‘, #33839â†“â†“, #33853â†‘â†“â†“,
#33959â†‘â†“â†“, #33963â†“â†“â†“, #33994â†“â†‘, #34177â†“â†‘â†‘, #34226â†“â†‘â†‘, #34712â†“â†“â†“, #35699â†“â†‘â†‘, #36157â†‘â†“â†‘, #36174â†‘â†“â†‘, #36188â†“â†‘â†“, #36198â†“â†‘â†“, #36211â†‘â†“â†“, #36265â†‘â†“,
#36306â†“â†‘â†‘, #36316â†“â†“â†“, #36563â†‘â†“â†‘, #36572â†‘â†‘, #36582â†“, #36728â†‘â†‘â†“, #36729â†“â†“â†“, #37061â†“â†“, #37359â†“â†‘â†‘, #37448â†‘â†“â†“, #37758â†“â†‘, #37832â†‘â†‘, #37913â†“â†‘,
#36267â†“â†‘â†‘, #33939â†“â†“â†‘, #36267â†“â†‘â†‘, #36487, #36557, #36624, #36650, #36662, #34249, #34300, #36552 Performance Impacts
Induced by Combinations
of ML Library VersionsType B#32522â†“, #33734â†“â†‘, #33735â†“, #33775â†“â†“â†‘, #33794â†“â†“â†“, #33850â†“â†“â†‘, #33857â†“â†“, #33896â†“, #33910â†“â†‘â†“, #33935â†“, #34344â†‘â†“â†‘, #34374â†“â†“, #34637â†‘â†“, #34646â†‘â†“â†“,
#34650â†‘â†“, #34693â†‘â†“, #34699â†“, #34702â†‘â†“, #36239â†‘, #36264â†‘, #36284â†“â†“, #36574â†“â†“, #36587â†‘â†“, #36664â†‘#36848â†“#37343â†‘â†‘#37360â†‘â†“#37543â†“â†“â†“
#37552â†“â†‘#37768â†‘â†“#37776â†‘â†‘#38067â†“â†“#38126â†“â†“#38162â†‘#38230â†‘â†“#36695, #36879, #36907, #34299, #34322, #36621
18 PLC Issues Repored to ML libraries ( Report ID ,#Problematic APIs )
[Catboost #2142, 2 APIs]â™ ,[ImbalancedLearn #923, 2 APIs]â™ , [Keras #17217, 8 APIs]â™ , [OpenCV #22472, 2 APIs]â™ , [Optuna #3976, 2 APIs]â™ ,[Sklearn #23427, 3 APIs]â™ ,[Xgboost #8033, 2 APIs]â™ ,
[CategoryEncoders #362, 2 APIs]â™£,[CategoryEncoders #364, 2 APIs]â™£,[LightGBM #5336, 3 APIs]â™£,[LightGBM #5475, 3 APIs]â™£,[Sklearn #24138, 1 APIs]â™£, [Transformers #18950, 3 APIs]â™£,
[Tensorflow #56600, 4 APIs]â™£,[Tensorflow #56958, 7 APIs]â™£, [Shap #2644, 1 APIs]â™ , [Shap #2644, 1 APIs]â™ 
PLC issues were manifested in performance changes (rise â†‘/ fallâ†“) of three aspects: Competition scores, Execution time and Memory usage, and Crashes/ NaN bugs;
â™ Confirmed issues under fixing; â™£Acknowledged by developers (Developers considered the issues were worthy of further investigation); â™ Not bugs;â™£Not bugs in the latest versions; â™ Pending Issues.
7 RELATED WORK
Machine Learning Libraries. There have been a lot of work focus-
ing on the performance enhancement and quality assurance of ML
frameworks. Zhang et al. [ 85] conducted an empirical study on the
common bugs of ML applications using TensorFlow . Nargiz et al. [ 47]
and Islam et al. [ 49] further conducted studies on Keras ,TensorFlow ,
PyTorch ,Caffe andTheano with a taxonomy of bugs. Han et al. [ 43]
performed an exploratory study on the dependency networks of
deep learning libraries. They studied the application domains, de-
pendency degrees, version update behaviors of ML applications that
depend on Tensorflow ,PyTorch , and Theano . Dilhata et al. [ 38] con-
ducted a quantitative and qualitative empirical study to investigate
the ML library usage and evolution. They only focused on the ML
frameworks and explored how developers used these libraries (e.g.,
common library combinations) and how the usage evolved over the
projectsâ€™ lifetime. Wang et al. [ 74], Guo et al. [ 42] and Phamet et
al. [63] proposed novel mutation testing/differential techniques to
detect behavioral inconsistencies across multiple ML frameworks.
Georgiou et al. [ 40] presented an in-depth empirical analysis to
investigate and compare the energy consumption and run-time
performance of DL frameworks ( PyTorch andTensorFlow ).
Our study distinguishes from the existing work in three aspects:
(1) we focus on various types of common ML libraries including
both ML frameworks and data science libraries; (2) we conduct
the first empirical study to explore the impacts of different library
version combinations on ML pipelinesâ€™ performances; (3) with the
aid of our identified defective API repository, we detect many ğ‘ƒğ¿ğ¶
issues in real-world ML applications.
Machine Learning Applications. Several SE researchers presented
comprehensive empirical studies on real bugs and technical chal-
lenges of ML applications. Islam, et al. [ 50], Guo et al. [ 41], Han et
al. [44], Zhang et al. [ 84] and Chen et al. [ 34] collected a set of high-
quality DL questions&answers on Stack Overflow and summarized
the main challenges in developing and deploying ML applications,
respectively. Approaches [ 54,83] presented a root cause taxonomy
of deep learning specific failures to facilitate future software devel-
opment. Shen et al. [ 67] studied the bugs rising in the popular deep
learning compilers, and provided a series of valuable guidelines for
deep learning compiler bug detection and debugging. Phamet etal. [64] studied the variance of DL systems and the awareness of this
variance among researchers and practitioners. Existing and new
testing techniques [ 33,37,39,45,45,48,51,52,59,60,62,65,69â€“
71,75,77,78,81,82,86] were also proposed for and adapted to
machine learning applications to expose real bug. The works by Ma
et al. [ 46,61] and Shen et al. [ 68] proposed mutation operators spe-
cific to deep learning applications. Humbatova et al. [ 48] followed a
systematic process to extract mutation operators from existing bug
taxonomies to simulate the effects of real DL bugs. In this paper,
we capture the performance inconsistencies of ML pipelines using
differential testing across various library version combinations.
8 CONCLUSION AND FUTURE WORK
In this paper, we empirically studied 11,363 ML pipelines from
diverse competitions on Kaggle to explore the impacts of differ-
ent ML library version combinations on their performances. Our
study reveals the pervasiveness and severity of ğ‘ƒğ¿ğ¶ issues in ML
pipelines. Our findings can motivate the establishment of a symbi-
otic ecosystem where researchers, tool builders, and library vendors
work together to assist developers in combating ğ‘ƒğ¿ğ¶ issues.
9 DATA AVAILABILITY
We provide a reproduction package at http://piecer-plc.github.io
to facilitate future research. The package includes (1) a dataset con-
taining 11,363 ML pipelines and 7,983 library APIs, (2) an available
toolPiecer , (2) a defective API repository for detecting PLC issues,
and (4) a list of PLC issues captured by us from real-world pipelines
and ML libraries, along with our issue reproducing results.
ACKNOWLEDGMENTS
The authors express thanks to the anonymous reviewers for their
constructive comments. The work is supported by the National Nat-
ural Science Foundation of China (Grant Nos. 62141210, 61932021,
61902056), the Hong Kong RGC/GRF grant 16205722, MSRA grant,
ITF grant (MHP/055/19, PiH/255/21), Research Grants Council (RGC)
Research Impact Fund under Grant R5034-18, the Fundamental Re-
search Funds for the Central Universities (Grant No. N2217005),
Open Fund of State Key Lab. for Novel Software Technology, Nan-
jing University (KFKT2021B01), and 111 Project (B16009).
473ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Yibo Wang, Ying Wang, Tingwei Zhang, Yue Yu, Shing-Chi Cheung, Hai Yu, and Zhiliang Zhu
REFERENCES
[1] 2021. Hugging Face. https://huggingface .co/models. Accessed: 2021-08-01.
[2] 2021. Kaggle. https://www .kaggle .com/. Accessed: 2021-08-01.
[3] 2021. Kaggle. https://jedi .readthedocs .io//. Accessed: 2021-08-01.
[4] 2021. Matplotlib. https://matplotlib .org/. Accessed: 2021-08-01.
[5] 2021. Numpy. https://numpy .org/. Accessed: 2021-08-01.
[6] 2021. Pandas. https://pandas .pydata .org/. Accessed: 2021-08-01.
[7] 2021. Pytorch. https://pytorch .org/. Accessed: 2021-08-01.
[8]2021. A ranked list of awesome ML librarie. https://github .com/ml-tooling/best-
of-ml-python. Accessed: 2021-08-01.
[9] 2021. Scikit-Learn. https://scikit-learn .org/stable/. Accessed: 2021-08-01.
[10] 2022. CFEC#1211. https://www .kaggle .com/code/nandhuelan/catembed/
notebook. Accessed: 2022-07-01.
[11] 2022. CFEC#1254. https://www .kaggle .com/dskagglemt/categorical-feature-
encoding-challenge. Accessed: 2022-07-01.
[12] 2022. CFECII#385. https://www .kaggle .com/nandhuelan/let-s-tickle-the-cat-
meow. Accessed: 2022-07-01.
[13] 2022. Commit log 7a0adbb. https://github .com/huggingface/transformers/pull/
4659/commits/7a0adbb56ae719a784f781bb2d80edf856e71916. Accessed: 2022-07-
01.
[14] 2022. Competition 1. https://www .kaggle .com/c/lish-moa. Accessed: 2022-07-01.
[15] 2022. Competition 10. https://www .kaggle .com/c/champs-scalar-coupling. Ac-
cessed: 2022-07-01.
[16] 2022. Competition 2. https://www .kaggle .com/c/aerial-cactus-identification.
Accessed: 2022-07-01.
[17] 2022. Competition 3. https://www .kaggle .com/c/tabular-playground-series-
may-2021. Accessed: 2022-07-01.
[18] 2022. Competition 4. https://www .kaggle .com/competitions/nlp-getting-started.
Accessed: 2022-07-01.
[19] 2022. Competition 5. https://www .kaggle .com/c/Kannada-MNIST. Accessed:
2022-07-01.
[20] 2022. Competition 6. https://www .kaggle .com/competitions/cat-in-the-dat.
Accessed: 2022-07-01.
[21] 2022. Competition 7. https://www .kaggle .com/competitions/tabular-
playground-series-apr-2021. Accessed: 2022-07-01.
[22] 2022. Competition 8. https://www .kaggle .com/c/plant-pathology-2020-fgvc7.
Accessed: 2022-07-01.
[23] 2022. Competition 9. https://www .kaggle .com/c/optiver-realized-volatility-
prediction. Accessed: 2022-07-01.
[24] 2022. GoogleBrain-VPP#1686. https://www .kaggle .com/code/ranjeetshrivastav/
ventilator-pressure-prediction-xgboost/comments. Accessed: 2022-07-01.
[25] 2022. Issue Pandas#25893. https://www .kaggle .com/dskagglemt/categorical-
feature-encoding-challenge. Accessed: 2022-07-01.
[26] 2022. MNIST. https://www .kaggle .com/c/Kannada-MNIST. Accessed: 2022-07-
01.
[27] 2022. MNIST#85. https://www .kaggle .com/joesmithkaggle/the-simple-kernel-
for-kannada-mnist. Accessed: 2022-07-01.
[28] 2022. NLPDT#785. https://www .kaggle .com/code/onuraydere/
decisiontreeandbernoullinb. Accessed: 2022-07-01.
[29] 2022. NLPDT#plyger. https://www .kaggle .com/code/plyger/
notebook268f8a3109/notebook. Accessed: 2022-07-01.
[30] 2022. Release note of Sklearn 0.22.0. https://scikit-learn .org/stable/modules/
generated/sklearn .svm.SVC.html. Accessed: 2022-07-01.
[31] 2022. Sklearn#20642. https://github .com/scikit-learn/scikit-learn/issues/20642.
Accessed: 2022-07-01.
[32] 2022. TPS-Jul#839. https://www .kaggle .com/code/tps-july-2021-simple-fast-
code. Accessed: 2022-07-01.
[33] Houssem Ben Braiek and Foutse Khomh. 2019. Deepevolution: A search-based
testing approach for deep neural networks. In 2019 IEEE International Conference
on Software Maintenance and Evolution (ICSME) . IEEE, 454â€“458. https://doi .org/
10.1109/ICSME .2019.00078
[34] Zhenpeng Chen, Yanbin Cao, Yuanqiang Liu, Haoyu Wang, Tao Xie, and Xuanzhe
Liu. 2020. A comprehensive study on challenges in deploying deep learning based
software. In Proceedings of the 28th ACM Joint Meeting on European Software
Engineering Conference and Symposium on the Foundations of Software Engineering .
750â€“762. https://doi .org/10 .1145/3368089 .3409759
[35] Jacob Cohen. 2013. Statistical power analysis for the behavioral sciences . Routledge.
https://doi .org/10 .4324/9780203771587
[36] John W. Creswell. 2013. Qualitative Inquiry and Research Design: Choosing Among
Five Approaches (3rd Edition) . https://doi .org/10 .1177/1524839915580941
[37] Yinlin Deng, Chenyuan Yang, Anjiang Wei, and Lingming Zhang. 2022. Fuzzing
deep-learning libraries via automated relational API inference. In Proceedings of
the 30th ACM Joint European Software Engineering Conference and Symposium
on the Foundations of Software Engineering . 44â€“56. https://doi .org/10 .1145/
3540250 .3549085
[38] Malinda Dilhara, Ameya Ketkar, and Danny Dig. 2021. Understanding Software-
2.0: A Study of Machine Learning library usage and evolution. ACM Transactions
on Software Engineering and Methodology (TOSEM) 30, 4 (2021), 1â€“42. https:
//doi.org/10 .1145/3453478[39] Xiaoning Du, Xiaofei Xie, Yi Li, Lei Ma, Yang Liu, and Jianjun Zhao. 2019. Deep-
stellar: Model-based quantitative analysis of stateful deep learning systems. In
Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering . 477â€“487.
https://doi .org/10 .1145/3338906 .3338954
[40] Stefanos Georgiou, Maria Kechagia, Tushar Sharma, Federica Sarro, and Ying
Zou. 2022. Green ai: Do deep learning frameworks have different costs?. In
Proceedings of the 44th International Conference on Software Engineering . 1082â€“
1094. https://doi .org/10 .1145/3510003 .3510221
[41] Qianyu Guo, Sen Chen, Xiaofei Xie, Lei Ma, Qiang Hu, Hongtao Liu, Yang Liu,
Jianjun Zhao, and Xiaohong Li. 2019. An empirical study towards characterizing
deep learning development and deployment across different frameworks and
platforms. In 2019 34th IEEE/ACM International Conference on Automated Software
Engineering (ASE) . IEEE, 810â€“822. https://doi .org/10 .1109/ASE .2019.00080
[42] Qianyu Guo, Xiaofei Xie, Yi Li, Xiaoyu Zhang, Yang Liu, Xiaohong Li, and Chao
Shen. 2020. Audee: Automated testing for deep learning frameworks. In 2020 35th
IEEE/ACM International Conference on Automated Software Engineering (ASE) .
IEEE, 486â€“498. https://doi .org/10 .1145/3324884 .3416571
[43] Junxiao Han, Shuiguang Deng, David Lo, Chen Zhi, Jianwei Yin, and Xin Xia.
2020. An empirical study of the dependency networks of deep learning libraries.
In2020 IEEE International Conference on Software Maintenance and Evolution
(ICSME) . IEEE, 868â€“878. https://doi .org/10 .1109/ICSME46990 .2020.00116
[44] Junxiao Han, Emad Shihab, Zhiyuan Wan, Shuiguang Deng, and Xin Xia. 2020.
What do programmers discuss about deep learning frameworks. Empirical Soft-
ware Engineering 25 (2020), 2694â€“2747. https://doi .org/10 .1007/s10664-020-
09819-6
[45] Qiang Hu, Yuejun Guo, Xiaofei Xie, Maxime Cordy, Lei Ma, Mike Papadakis, and
Yves Le Traon. 2022. Efficient Testing of Deep Neural Networks via Decision
Boundary Analysis. arXiv preprint arXiv:2207.10942 (2022). https://doi .org/
10.48550/arXiv .2207.10942
[46] Qiang Hu, Lei Ma, Xiaofei Xie, Bing Yu, Yang Liu, and Jianjun Zhao. 2019. Deep-
mutation++: A mutation testing framework for deep learning systems. In 2019
34th IEEE/ACM International Conference on Automated Software Engineering (ASE) .
IEEE, 1158â€“1161. https://doi .org/10 .1109/ASE .2019.00126
[47] Nargiz Humbatova, Gunel Jahangirova, Gabriele Bavota, Vincenzo Riccio, Andrea
Stocco, and Paolo Tonella. 2020. Taxonomy of real faults in deep learning sys-
tems. In Proceedings of the ACM/IEEE 42nd International Conference on Software
Engineering . 1110â€“1121. https://doi .org/10 .1145/3377811 .3380395
[48] Nargiz Humbatova, Gunel Jahangirova, and Paolo Tonella. 2021. DeepCrime:
mutation testing of deep learning systems based on real faults. In Proceedings of
the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis .
67â€“78. https://doi .org/10 .1145/3460319 .3464825
[49] Md Johirul Islam, Giang Nguyen, Rangeet Pan, and Hridesh Rajan. 2019. A
comprehensive study on deep learning bug characteristics. In Proceedings of
the 2019 27th ACM Joint Meeting on European Software Engineering Conference
and Symposium on the Foundations of Software Engineering . 510â€“520. https:
//doi.org/10 .1145/3338906 .3338955
[50] Md Johirul Islam, Rangeet Pan, Giang Nguyen, and Hridesh Rajan. 2020. Repairing
deep neural networks: Fix patterns and challenges. In Proceedings of the ACM/IEEE
42nd International Conference on Software Engineering . 1135â€“1146. https://doi .org/
10.1145/3377811 .3380378
[51] Hong Jin Kang, Pattarakrit Rattanukul, Stefanus Agus Haryono, Truong Giang
Nguyen, Chaiyong Ragkhitwetsagul, Corina Pasareanu, and David Lo. 2022.
SkipFuzz: Active Learning-based Input Selection for Fuzzing Deep Learning
Libraries. arXiv preprint arXiv:2212.04038 (2022). https://doi .org/10 .48550/
arXiv .2212.04038
[52] Jinhan Kim, Robert Feldt, and Shin Yoo. 2019. Guiding deep learning system
testing using surprise adequacy. In 2019 IEEE/ACM 41st International Confer-
ence on Software Engineering (ICSE) . IEEE, 1039â€“1049. https://doi .org/10 .1109/
ICSE.2019.00108
[53] Robert V Krejcie and Daryle W Morgan. 1970. Determining sample size for
research activities. Educational and psychological measurement 30, 3 (1970),
607â€“610. https://doi .org/10 .1177/00131644700300030
[54] Yunkai Liang, Yun Lin, Xuezhi Song, Jun Sun, Zhiyong Feng, and Jin Song Dong.
2022. gDefects4DL: a dataset of general real-world deep learning program defects.
InProceedings of the ACM/IEEE 44th International Conference on Software Engi-
neering: Companion Proceedings . 90â€“94. https://doi .org/10 .1145/3510454 .3516826
[55] Ben Liblit, Linghui Luo, Alejandro Molina Ramirez, Rajdeep Mukherjee, Zachary
Patterson, Goran Piskachev, Martin SchÃ¤f, Omer Tripp, and Willem Visser. 2023.
Shifting left for early detection of machine-learning bugs. (2023). https://doi .org/
10.1007/978-3-031-27481-7_33
[56] Chao Liu, Cuiyun Gao, Xin Xia, David Lo, John Grundy, and Xiaohu Yang. 2021.
On the reproducibility and replicability of deep learning in software engineering.
ACM Transactions on Software Engineering and Methodology (TOSEM) 31, 1 (2021),
1â€“46. https://doi .org/10 .1145/3477535
[57] Geoffrey R Loftus and Michael EJ Masson. 1994. Using confidence intervals
in within-subject designs. Psychonomic bulletin & review 1, 4, 476â€“490. https:
//doi.org/10 .3758/BF03210951
[58] Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle, and Zhenchang Xing. 2022.
Towards a roadmap on software engineering for responsible AI. In Proceedings of
the 1st International Conference on AI Engineering: Software Engineering for AI .
474Can Machine Learning Pipelines Be Better Configured? ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
101â€“112. https://doi .org/10 .1145/3522664 .3528607
[59] Lei Ma, Felix Juefei-Xu, Minhui Xue, Bo Li, Li Li, Yang Liu, and Jianjun Zhao. 2019.
Deepct: Tomographic combinatorial testing for deep learning systems. In 2019
IEEE 26th International Conference on Software Analysis, Evolution and Reengi-
neering (SANER) . IEEE, 614â€“618. https://doi .org/10 .1109/SANER .2019.8668044
[60] Lei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Chun-
yang Chen, Ting Su, Li Li, Yang Liu, et al .2018. Deepgauge: Multi-granularity
testing criteria for deep learning systems. In Proceedings of the 33rd ACM/IEEE
International Conference on Automated Software Engineering . 120â€“131. https:
//doi.org/10 .1145/3238147 .3238202
[61] Lei Ma, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Felix Juefei-Xu, Chao Xie,
Li Li, Yang Liu, Jianjun Zhao, et al .2018. Deepmutation: Mutation testing of deep
learning systems. In 2018 IEEE 29th International Symposium on Software Reliabil-
ity Engineering (ISSRE) . IEEE, 100â€“111. https://doi .org/10 .1109/ISSRE .2018.00021
[62] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. 2017. Deepxplore: Auto-
mated whitebox testing of deep learning systems. In proceedings of the 26th
Symposium on Operating Systems Principles . 1â€“18. https://doi .org/10 .1145/
3132747 .3132785
[63] Hung Viet Pham, Thibaud Lutellier, Weizhen Qi, and Lin Tan. 2019. CRADLE:
cross-backend validation to detect and localize bugs in deep learning libraries.
In2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE) .
IEEE, 1027â€“1038. https://doi .org/10 .1109/ICSE .2019.00107
[64] Hung Viet Pham, Shangshu Qian, Jiannan Wang, Thibaud Lutellier, Jonathan
Rosenthal, Lin Tan, Yaoliang Yu, and Nachiappan Nagappan. 2020. Problems and
opportunities in training deep learning software systems: An analysis of vari-
ance. In Proceedings of the 35th IEEE/ACM international conference on automated
software engineering . 771â€“783. https://doi .org/10 .1145/3324884 .3416545
[65] Vincenzo Riccio, Gunel Jahangirova, Andrea Stocco, Nargiz Humbatova, Michael
Weiss, and Paolo Tonella. 2020. Testing machine learning based systems: a
systematic mapping. Empirical Software Engineering 25 (2020), 5193â€“5254. https:
//doi.org/10 .1007/s10664-020-09881-0
[66] Shlomo S Sawilowsky. 2009. New effect size rules of thumb. Journal of mod-
ern applied statistical methods 8, 2 (2009), 26. https://doi .org/10 .22237/jmasm/
1257035100
[67] Qingchao Shen, Haoyang Ma, Junjie Chen, Yongqiang Tian, Shing-Chi Cheung,
and Xiang Chen. 2021. A comprehensive study of deep learning compiler bugs.
InProceedings of the 29th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering . 968â€“980.
https://doi .org/10 .1145/3468264 .3468591
[68] Weijun Shen, Jun Wan, and Zhenyu Chen. 2018. Munn: Mutation analysis
of neural networks. In 2018 IEEE International Conference on Software Quality,
Reliability and Security Companion (QRS-C) . IEEE, 108â€“115. https://doi .org/
10.1109/QRS-C .2018.00032
[69] Youcheng Sun, Min Wu, Wenjie Ruan, Xiaowei Huang, Marta Kwiatkowska, and
Daniel Kroening. 2018. Concolic testing for deep neural networks. In Proceedings
of the 33rd ACM/IEEE International Conference on Automated Software Engineering .
109â€“119. https://doi .org/10 .1145/3238147 .3238172
[70] Florian Tambon, Foutse Khomh, and Giuliano Antoniol. 2023. A probabilistic
framework for mutation testing in deep neural networks. Information and Soft-
ware Technology 155 (2023), 107129. https://doi .org/10 .1016/j .infsof .2022.107129
[71] Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. 2018. Deeptest: Automated
testing of deep-neural-network-driven autonomous cars. In Proceedings of the
40th international conference on software engineering . 303â€“314. https://doi .org/
10.1145/3180155 .3180220
[72] Lewis Tunstall, Leandro Von Werra, and Thomas Wolf. 2022. Natural language
processing with transformers . " Oâ€™Reilly Media, Inc.".[73] Anthony J Viera, Joanne M Garrett, et al .2005. Understanding interobserver
agreement: the kappa statistic. Fam med 37, 5 (2005), 360â€“363.
[74] Zan Wang, Ming Yan, Junjie Chen, Shuang Liu, and Dongdi Zhang. 2020. Deep
learning library testing via effective model generation. In Proceedings of the 28th
ACM Joint Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering . 788â€“799. https://doi .org/10 .1145/
3368089 .3409761
[75] Moshi Wei, Yuchao Huang, Jinqiu Yang, Junjie Wang, and Song Wang. 2022.
Cocofuzzing: Testing neural code models with coverage-guided fuzzing. IEEE
Transactions on Reliability (2022). https://doi .org/10 .1109/TR .2022.3208239
[76] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
Anthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz, et al .
2020. Transformers: State-of-the-art natural language processing. In Proceedings
of the 2020 conference on empirical methods in natural language processing: system
demonstrations . 38â€“45. https://doi .org/10 .18653/v1/2020 .emnlp-demos .6
[77] Xiaofei Xie, Lei Ma, Felix Juefei-Xu, Minhui Xue, Hongxu Chen, Yang Liu, Jianjun
Zhao, Bo Li, Jianxiong Yin, and Simon See. 2019. Deephunter: a coverage-guided
fuzz testing framework for deep neural networks. In Proceedings of the 28th ACM
SIGSOFT International Symposium on Software Testing and Analysis . 146â€“157.
https://doi .org/10 .1145/3293882 .3330579
[78] Xiaofei Xie, Lei Ma, Haijun Wang, Yuekang Li, Yang Liu, and Xiaohong Li. 2019.
Diffchaser: Detecting disagreements for deep neural networks. International Joint
Conferences on Artificial Intelligence Organization. https://doi .org/10 .24963/
ijcai.2019/800
[79] Minke Xiu, Zhen Ming Jack Jiang, and Bram Adams. 2020. An exploratory
study of machine learning model stores. IEEE Software 38, 1 (2020), 114â€“122.
https://doi .org/10 .1109/MS .2020.2975159
[80] Chunyong Yin, Bo Li, and Zhichao Yin. 2020. A distributed sensing data anomaly
detection scheme. Computers & Security 97 (2020), 101960. https://doi .org/
10.1016/j .cose.2020.101960
[81] Jie M Zhang, Mark Harman, Lei Ma, and Yang Liu. 2020. Machine learning testing:
Survey, landscapes and horizons. IEEE Transactions on Software Engineering (2020).
https://doi .org/10 .1109/TSE .2019.2962027
[82] Mengshi Zhang, Yuqun Zhang, Lingming Zhang, Cong Liu, and Sarfraz Khur-
shid. 2018. DeepRoad: GAN-based metamorphic testing and input validation
framework for autonomous driving systems. In 2018 33rd IEEE/ACM Interna-
tional Conference on Automated Software Engineering (ASE) . IEEE, 132â€“142.
https://doi .org/10 .1145/3238147 .3238187
[83] Ru Zhang, Wencong Xiao, Hongyu Zhang, Yu Liu, Haoxiang Lin, and Mao Yang.
2020. An empirical study on program failures of deep learning jobs. In 2020
IEEE/ACM 42nd International Conference on Software Engineering (ICSE) . IEEE,
1159â€“1170. https://doi .org/10 .1145/3377811 .3380362
[84] Tianyi Zhang, Cuiyun Gao, Lei Ma, Michael Lyu, and Miryung Kim. 2019. An Em-
pirical Study of Common Challenges in Developing Deep Learning Applications.
In2019 IEEE 30th International Symposium on Software Reliability Engineering
(ISSRE) . 104â€“115. https://doi .org/10 .1109/ISSRE .2019.00020
[85] Yuhao Zhang, Yifan Chen, Shing-Chi Cheung, Yingfei Xiong, and Lu Zhang.
2018. An empirical study on TensorFlow program bugs. In Proceedings of the
27th ACM SIGSOFT International Symposium on Software Testing and Analysis .
129â€“140. https://doi .org/10 .1145/3213846 .3213866
[86] Jianyi Zhou, Feng Li, Jinhao Dong, Hongyu Zhang, and Dan Hao. 2020. Cost-
effective testing of a deep learning model through input reduction. In 2020 IEEE
31st International Symposium on Software Reliability Engineering (ISSRE) . IEEE,
289â€“300. https://doi .org/10 .1109/ISSRE5003 .2020.00035
Received 2023-03-02; accepted 2023-07-27
475