PyExplainer: Explaining the Predictions of
Just-In-Time Defect Models
Chanathip Pornprasit∗, Chakkrit Tantithamthavorn∗§, Jirayus Jiarpakdee∗, Michael Fu∗, Patanamon Thongtanunam†
∗Monash University, Australia.†The University of Melbourne, Australia.
Abstract —Just-In-Time (JIT) defect prediction (i.e., an AI/ML
model to predict defect-introducing commits) is proposed to help
developers prioritize their limited Software Quality Assurance(SQA) resources on the most risky commits. However, theexplainability of JIT defect models remains largely unexplored(i.e., practitioners still do not know why a commit is predicted asdefect-introducing). Recently, LIME has been used to generateexplanations for any AI/ML models. However, the randomperturbation approach used by LIME to generate syntheticneighbors is still suboptimal, i.e., generating synthetic neighborsthat may not be similar to an instance to be explained, producinglow accuracy of the local models, leading to inaccurate explana-tions for just-in-time defect models. In this paper, we proposePyExplainer—i.e., a local rule-based model-agnostic techniquefor generating explanations (i.e., why a commit is predicted asdefective) of JIT defect models. Through a case study of twoopen-source software projects, we ﬁnd that our PyExplainerproduces (1) synthetic neighbors that are 41%-45% more similarto an instance to be explained; (2) 18%-38% more accurate localmodels; and (3) explanations that are 69%-98% more uniqueand 17%-54% more consistent with the actual characteristics ofdefect-introducing commits in the future than LIME (a state-of-the-art model-agnostic technique). This could help practitionersfocus on the most important aspects of the commits to mitigatethe risk of being defect-introducing. Thus, the contributionsof this paper build an important step towards ExplainableAI for Software Engineering, making software analytics moreexplainable and actionable. Finally, we publish our PyExplaineras a Python package to support practitioners and researchers(https://github.com/awsm-research/PyExplainer).
Index Terms—Explainable AI, Just-In-Time Defect Prediction,
Explainable Software Analytics
I. I NTRODUCTION
Modern software development projects tend to release soft-
ware products in rapid cycles. To ensure the quality of all
newly arrived commits, developers need to conduct codereview and provide feedback prior to merging them into therelease branch. However, such code review activities are stilltime-consuming and expensive. Thus, performing exhaustivecode review activities for all commits is infeasible due to thelimited Software Quality Assurance (SQA) resources.
Just-In-Time (JIT) defect prediction [17, 19, 26, 30]—
an AI/ML model to predict defect-introducing commits—hasbeen proposed to help developers efﬁciently prioritize theirlimited SQA resources on the most risky commits. In addition,JIT defect prediction is also used to provide insights about theimportant characteristics of defect-introducing commits. Such
§The corresponding author. Email: chakkrit@monash.eduinsights can help QA teams and managers to develop proactivesoftware quality improvement plans to prevent pitfalls in thepast that lead to software defects in the future [27].
However, the predictions of existing JIT defect prediction
approaches are still not explainable, hindering the adoption ofJIT defect models in practice [5, 13, 14, 40]. Recent researchshows that practitioners still asked many why-questions (e.g.,
why a commit is predicted as defective) [5, 13, 14, 41], sincecurrent JIT defect prediction approaches are treated as black-box which only provide the predictions, not the explanations.Such a lack of explainability of JIT defect prediction ap-proaches could lead to suboptimal software quality assurancepractices and suboptimal operational decision-makings.
Recently, LIME—a state-of-the-art model-agnostic tech-
nique [32]—has been adopted in software engineering research(e.g., line-level just-in-time defect prediction [30], and explain-able ﬁle-level defect prediction [13]). LIME is a techniquethat explains a prediction of AI/ML models (i.e., what are thefeatures that inﬂuence a given prediction). Generally speaking,given an instance to be explained (e.g., a commit), LIMEproduces an explanation from a local model (F
/prime) that is trained
using the randomly generated synthetic instances around theinstance to be explained (X
/prime) (i.e., synthetic neighbors) and
the predictions (Y/prime) obtained from the global black-box model.
This allows the local model to mimic the behavior of theunderlying global black-box models.
The quality of explanation produced by LIME heavily relies
on the neighborhood generation process [12]. Ideally, theneighborhood generation process should generate syntheticneighbors that are closely similar to the instance to be ex-plained so that the local model can accurately approximatethe prediction of the global models. In LIME, the randomperturbation approach is used to generate synthetic neighbors.However, such a simple random perturbation approach maynot be suitable for sparse and high dimensional data likeJIT datasets [52]. It is possible that the random perturbationapproach will generate synthetic neighbors that may not besimilar to an instance to be explained, which will lead thelocal model to inaccurately approximate the predictions of theglobal model. Thus, these local models may not be effectivein generating explanations (e.g., generic).
In this paper, we propose PyExplainer, a local rule-based
model-agnostic technique for explaining the predictions of JITdefect prediction models. To produce a more accurate explana-tion for the prediction of JIT defect models, our PyExplainer
4072021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000442021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678763
978-1-6654-0337-5/21/$31.00  ©2021  IEEE
generates synthetic neighbors based on the actual character-
istics of defect-introducing commits in the JIT dataset usingthe crossover and mutation operations. Instead of generatingan explanation with a single rule feature with an importancescore like LIME (e.g., the importance score of Churn> 100 is
0.9), our PyExplainer generates an explanation that accountsfor interactions between rule features (e.g., Churn> 100 &
#Reviewers< 2⇒Defect).
To evaluate our PyExplainer, we compare with LIME [32]
in three dimensions: (1) the similarity between synthetic
neighbors and an instance to be explained ; (2) the accuracy
of the local models; and (3) the effectiveness in generating
explanations. Through a case study of 40,798 commits thatspan across two large-scale software systems (i.e., OpenStackand Qt), we address the following three research questions:
(RQ1) Does our PyExplainer produce better synthetic
neighbors than LIME for JIT defect models?
The synthetic neighours produced by our PyExplainerare 41%-55% more similar to an instance to be ex-plained than LIME, indicating that our PyExplainerproduces synthetic neighbors that are more closelysimilar to an instance to be explained than LIME.
(RQ2) Does our PyExplainer produce higher accuracy of
local models than LIME for JIT defect models?When explaining the RF and LR JIT defect models,PyExplainer produces local models that are 18%-38%more accurate (AUC) than the local models producedby LIME, indicating that the PyExplainer produceslocal models that have a higher ability to discriminatethe characteristics between defect and clean classes.
(RQ3) Is our PyExplainer more effective in generating
explanations than LIME for JIT defect models?The explanations generated by our PyExplainer are69%-98% more unique (i.e., more speciﬁc to an in-stance to be explained) than LIME. On the otherhand, the explanations generated by our PyExplainerare 17%-54% more consistent with the actual defect-introducing commits in the testing data than LIME.
Thus, the explanations generated by PyExplainer could help
practitioners to focus on the most important aspects that areassociated with the risk of being defect-introducing for a givenprediction, instead of focusing on the less important aspects.
Contributions. The contributions of this paper are as follows:
•We propose PyExplainer, a local rule-based model-
agnostic technique for explaining the predictions of JITdefect models.
•Our results show that PyExplainer produces (1) syntheticneighbours that are more similar to an instance to beexplained; (2) more accurate local models; and (3) ex-planations that are more unique and more consistent withthe actual characteristics of defect-introducing commits inthe future than LIME.
•Finally, we developed a proof-of-concept of visual ex-planations and what-if visualizations, and published our
PyExplainer as a python package.II. R
ELATED WORK &R ESEARCH QUESTIONS
Prior studies pointed out that practitioners often do not
understand the reasons behind the predictions of softwareanalytics [5, 14, 25]. Recent work also raises concerns thata lack of explainability of software analytics often hinder theadoption of software analytics in practice [5, 13, 14, 18, 40,41]. Importantly, Jiarpakdee et al. [14] found that 91% of
recent defect prediction studies often focus on improving theaccuracy, while as few as 4% of recent defect prediction stud-ies focus on making ﬁle-level defect prediction models moreexplainable. However, Jiarpakdee et al. [14] found that prac-
titioners perceived that providing the explanations of defectprediction models are as important and useful as improving theaccuracy of defect prediction models. Yet, the explainabilityof JIT defect models remains largely unexplored.
The explanability of software analytics can be achieved
at the global and the local levels.
The global explanation can be produced using model-
speciﬁc interpretation techniques that are built in the AI/MLmodels (e.g., ANOV A for regression analysis, variable impor-tance analysis for random forest). This explanation helps re-searchers and software practitioners understand what importantfeatures that inﬂuence the predictions of the models [35, 45–48]. However, this global explanation is not speciﬁc to theprediction of each instance (e.g., a commit) in the testing orunseen data, since the global explanation is derived from thetraining dataset [49]. Hence, the global explanation may notbe accurate for a particular prediction.
On the other hand, the local explanation is produced for
a particular prediction of an instance in the testing or unseendataset, which allow practitioners better understand the reasonsbehind the predictions of the AI/ML models [13]. LIME [32]
is a state-of-the-art model-agnostic technique which has beenwidely adopted to address various software engineering prob-lems and other domains (5,000+ citations). For example, recentwork [30, 49] employed LIME for line-level defect predictions(i.e., identifying defective lines that contain the risky codetokens explained by LIME). Jiarpakdee et al. [13] found that
LIME [32] is effective in explaining the predictions of ﬁle-level defect prediction models (i.e., why a ﬁle is predicted asdefective). However, LIME has the following limitations.
First, the LIME’s neighborhood generation process is
still suboptimal. The quality of an explanation for a predic-
tion heavily relies on the quality of the generated syntheticneighbors around the instance to be explained [23]. Thus,if the neighbor generation process is suboptimal, the localmodel may fail to provide accurate insights about the logicalreasoning of the global model. Jia et al. [12] found that
the size of the neighbourhood has a large impact on thequality of the explanation. Krishnan et al. [22] found that
when a model is learned from sparse and high dimensionaldata (e.g., just-in-time defect dataset [52]), the model is oftenunderﬁtting, failing to capture the phenomenon of the databeing trained. Thus, the neighbor generation process shouldideally generate synthetic neighbors that are similar to the
408instance to be explained. Therefore, we investigate whether
our PyExplainer produce better synthetic neighbors (i.e., thesynthetic neighbors that are more similar to an instance to beexplained) than LIME for JIT defect models. We formulatethe following RQ:
(RQ1) Does our PyExplainer produce better syntheticneighbors than LIME for JIT defect models?
Second, the approximation of the LIME local models to
the predictions of the global model is still suboptimal. One
of the key principles of model-agnostic techniques is to buildthe best local model to mimic the behavior of the predictionsof the global models. Accuracy is often used to measure theextent to which how well the local model can approximate thepredictions of the global model [32]. Thus, a high accuracy oflocal models is desirable in order to derive the highest qualityexplanation, i.e., the local models can accurately approximatethe global model predictions for a subset of the data (e.g., localsurrogate models). Since LIME uses the random perturbationmethod to generate synthetic neighbors, the approximation ofthe LIME local models to the predictions of the global modelmay be suboptimal, leading to the inaccurate local modelsproduced by LIME. Thus, we formulate the following RQ:
(RQ2) Does our PyExplainer produce higher accuracy oflocal models than LIME for JIT defect models?
Third, the explanations generated by LIME are still
not speciﬁc to an instance to be explained. Another key
principle of the model-agnostic techniques is to build a uniqueexplanation that is speciﬁc to the prediction of an instanceto be explained. Thus, explanations should be unique andhighlight the key characteristics (i.e., features) of a committhat leads a global model to predict as defect-introducing.However, LIME uses a Quantile discretization (i.e., 1
st,2nd,
3rdQuantiles) for generating rule features. The three bins used
by LIME may not be enough to capture the highly-complexand highly-skewed JIT defect datasets. Thus, the rule featuresused by LIME may produce generic explanations that maynot be speciﬁc to the instance to be explained, which may notreﬂect the actual characteristics of defect-introducing commits.Therefore, we formulate the following RQ:
(RQ3) Is our PyExplainer more effective in generatingexplanations than LIME for JIT defect models?
III. O URPYEXPLAINER :AL OCAL RULE-BASED
MODEL -AGNOSTIC TECHNIQUE
In this section, we present our PyExplainer, a local rule-
based model-agnostic approach for explaining the predictionsof JIT defect models.
Overview. Figure 1 illustrates an overview of the PyEx-
plainer approach, which consists of four main steps. First,given an instance to be explained (i.e., a commit) and aglobal model, our PyExplainer will generate synthetic neigh-bors around the instance to be explained using the crossoverand mutation techniques [37]. Second, our PyExplainer willobtains the predictions of the synthetic neighbors from theglobal model. Third, our PyExplainer builds a local rule-basedregression model in order to learn the associations between thecharacteristics of the synthetic instances and the predictionsfrom the global model. In the fourth step, our PyExplainergenerates an explanation from the local model for the instanceto be explained. We now describe each of the four steps below.
(Step 1) Generate Synthetic Neighbors Around the
Instance to be Explained. Our PyExplainer will ﬁrst generate
synthetic neighbors (X
/prime) around the instance to be explained
using the crossover and mutation techniques [37]. To do so,PyExplainer will ﬁnd an initial set of actual neighbors, i.e.,the actual instances around the instance to be explained in thetraining dataset. To identify the actual neighbors, PyExplainerapplies the exponential kernel function (see Eq. 1) to calculatethe similarity score between each instance in the trainingdataset (i
k) and the instance to be explained (i e).
K(ik,ie)=e x p (−dist(i k,ie)2
2w2) (1)
wheredist(i k,ie)is the euclidean distance between instances
ikandie, andwis the kernel width as the multiplication of
0.75 and the number of features of an instance (w =0.75×
#features) as suggested by Ribeiro et al. [32].
Based on the initial set of actual neighbor, PyExplainer
generates synthetic neighbors using crossover and mutationtechniques to expand the initial set. The calculation of thecrossover (I
crossover ) and mutation (I mutation ) techniques can
be derived as follows:
Icrossover=i1+(i2−i1)∗α (2)
Imutation=i1+(i2−i3)∗μ (3)
wherei1,i2, andi3are the randomly selected neighbourhood
instances, αis a randomly generated number between 0and
1; andμis a randomly generated number between 0.5and1.
(Step 2) Obtain the Predictions of the Synthetic In-
stances using the Global Model. In Step 1, only the features
of a synthetic neighbor (X/prime) are generated. Hence, PyEx-
plainer uses the global model to obtain the predictions (i.e.,whether it is defective or clean given features of a syntheticinstance). This allows PyExplainer to learn the behaviour ofthe underlying global model.
(Step 3) Build a Local Rule-based Regression Model
using the RuleFit technique. To build a local model (F
/prime),
PyExplainer uses a rule-based logistic regression technique,called RuleFit [7]. RuleFit is a classiﬁer that combines treeensembles and linear models, which allows us to interpret themodel like a traditional regression model, while understandingthe logical reasons learnt from the rule features.
Broadly speaking, RuleFit will ﬁrst generate rule features
(X
/prime
R), e.g.,{Churn>100 & #Reviewers <2}based on
ensemble decision trees (e.g., Gradient Boosting Trees). Then,RuleFit uses a regression model (i.e., logistic regression forbinary outcomes, or linear regression for continuous outcomes)to model the association between the predicted outcomes (Y
/prime)
409A Local Model
A global black- 
box model
An Explanation
+-
An Instance  
to be explained(Step 1) Generate  
synthetic instances
(Step 2) Obtain 
predictions 
from the global 
model (Y’)Synthetic neighbors around  
the instance to be explainedY’
F
T
T
F
FX’
NeighborhoodSynthetic NeighborsAn instance to  
be explained
Actual Neighbors
(Step 4) Extract 
an explanation(Step 3) Build  
a rule-based 
regression 
model
LA > 100 & DEV>10  
Score=15, Coef. = +/-Synthetic Neighbors
Fig. 1: An overview of the PyExplainer approach. Given an
instance to be explained, PyExplainer produces four maincomponent i.e., (1) synthetic neighbors, (2) a local model, and(3) an explanation. Each PyExplainer’s explanation producesthree pieces of information i.e., (1) a rule-based explanation,(2) an importance score, and (3) the direction of relationshipof either supporting (+) or contradicting (-) the prediction.
and the rule features (X
/prime
R) together with the original features
(X). Then, the degree of importance and the coefﬁcients of
rule features and the original features can be analyzed from
this regression model.
The use of RuleFit in our PyExplainer will address the
limitation of LIME which does not account for interactionsbetween features (i.e., the combination of rule features). Al-though existing rule-based model-agnostic techniques (e.g.,SQAPlanner [31], Anchors [33], and LORE [9]) have beenproposed, these techniques employed association rule miningtechniques (e.g., Apriori, FP-Growth) which do not provide thedegree of importance of the rules and the coefﬁcients. Withoutthe degree of importance of the rules and the coefﬁcientsprovided by such techniques, we cannot quantify how strongthe association between the rules and the predicted outcomeand what the direction of the relationship is.
(Step 4) Extract an Explanation from the Local Rule-
based Model. To generate an explanation, our PyExplainer
analyzes the local model which is built using the RuleFit tech-nique in Step 3. In the local model, there are three key piecesinformation: (1) rule features, (2) importance scores, and (3)coefﬁcients. The importance score indicates the strength ofthe association between the rule feature and the predictedoutcome. The coefﬁcient can be used to indicate the directionof the relationship. For example, a positive coefﬁcient indicatesthat a rule feature has a contribution towards the prediction ofthe TRUE class (i.e., DEFECT).
Based on the three key pieces of information in the localmodel, our PyExplainer generates an explanation by identify-ing the rule feature that has the highest importance score, hasa positive coefﬁcient, and satisﬁes the actual feature values ofthe instance to be explained. For example, suppose that a rulefeature (Churn> 100 & #Reviewers< 2) has the highest impor-
tance score and has a positive coefﬁcient, our PyExplainer willgenerate the following rule-based explanation: Churn> 100 &
#Reviewers< 2⇒DEFECT, which means that a commit is
predicted as defective since Churn is greater than 100 and thenumber of reviewers is less than 2.
IV . E
XPERIMENTAL DESIGN
In this section, we present the studied datasets and explain
the details of our experimental design.
Studied JIT Datasets. We select just-in-time defect datasets
from two large-scale open-source software projects (i.e., Open-
stack and Qt) as provided by McIntosh and Kamei [27].Openstack is an open-source software for cloud infrastruc-ture service. Qt is a cross-platform application developmentframework. We choose Openstack and Qt datasets for ourstudy, since both datasets (1) are often used as a benchmark indefect prediction studies [10, 11, 27, 30]; and (2) are manuallyveriﬁed for the validity of the SZZ algorithm [36] to reducethe number of false positives and false negatives [4, 51]. TableI provides an overview of the studied datasets.
Commit Features. For each dataset, there are 17 commit-level
features that span across 5 dimensions, i.e., Size (e.g., lines
added, lines deleted), Diffusion (e.g., #modiﬁed ﬁles), History(#developers), Experience, and Code Review Activities.
Experiment Design. Figure 2 presents an overview of our
experimental design, which is composed of four main steps.
(Step 1) Split Data into Training and Testing Datasets. To
ensure that the evaluation of our just-in-time defect prediction
reﬂects a real-world scenario, we ﬁrst sort the date of the com-mits to preserve the order of the commits in a chronologicalorder [30, 43]. Then, we use a time-wise hold-out validationtechnique (as used by McIntosh and Kamei [27]) to split thedataset into training (70%) and testing (30%) datasets. The useof the time-wise hold-out validation technique ensures that thecommits that appear later will not be used in model training.Similarly, the commits that appear earlier will not be used inmodel evaluation.
(Step 2) Build JIT Defect Models. For each training
dataset, we ﬁrst mitigate collinearity using AutoSpearman andhandle class imbalance using SMOTE prior to build JIT defectmodels. Below, we describe each step in detail.
(Step 2-1) Mitigate Collinearity using AutoSpearman. To
ensure that the interpretation of our JIT defect models is highlyaccurate, we mitigate collinearity and multi-collinearity, assuggested by prior studies [15, 16, 38]. We use AutoSpearman,
an automated feature selection approach to automatically se-lect one feature from each group of highly correlated featuresthat shares the least correlation with the other features that arenot in the group [16]. As suggested by Kraemer et al. [20], we
use a threshold of 0.7 to indicate strong correlation betweenfeatures. As suggested by Fox [6], we use a VIF threshold
410TABLE I: An overview of the studied JIT defect datasets provided by McIntosh and Kamei [27].
Training Data Testing Data
Project Start Date End Date # Commits # Defective Commits Start Date End Date # Commits # Defective Commits
Openstack 11/30/2011 08/13/2013 9,246 980 (11%) 08/13/2013 02/28/2014 3,963 646 (16%)
Qt 06/18/2011 05/08/2013 19,312 1,577 (8%) 05/08/2013 03/18/2014 8,277 476 (6%)
RQ2
Local Models
JIT Defect 
Dataset
Training Data
PyExplainer
LIME
Synthetic 
Neighbors
Global JIT  
Defect ModelsExplanations
Synthetic 
NeighborRQ1
(Step 2) 
Build JIT 
defect modelsBuild
Local ModelsGenerate
ExplanationsRQ3(Step 4) Evaluate PyExplainer and 
LIME model-agnostic techniques to  
answer research questions BuildGenerate
Generate Generate(Step 1) Split 
train/test data
+-
An Instance  
to be explained 
in testing data
(Step 3) Apply PyExplainer and LIME  
model-agnostic techniques to  
generate explanations
Fig. 2: An overview of the experimental design.
value of 5 to indicate multicollinearity. We use the implemen-
tation of AutoSpearman as provided by the PyExplainer
Python package. After using AutoSpearman, we ﬁnally select7 features that are not highly-correlated with each other.
(Step 2-2) Handle Class Imbalance using SMOTE. To
ensure that the predictions of our JIT defect models arehighly accurate, we apply a class rebalancing technique, assuggested by prior work [1, 39]. Since the defective ratio ofour studied JIT defect datasets are highly imbalanced (i.e., 8%-16%), we apply SMOTE [2] to handle class imbalance only
on the training dataset. We choose the SMOTE technique,as suggested by prior work [1, 39] who found that theSMOTE technique outperforms other class rebalancing tech-niques. SMOTE performs the following steps. First, SMOTEcalculates the k-nearest neighbors of a set of minority class.
Then, SMOTE randomly chooses the neighbors and generatessynthetic instances around such neighbors. Finally, SMOTEcombines the synthetic instances with the undersampling ofthe majority class to produce the ﬁnal set of balanced in-stances. We use the implementation of SMOTE as providedbyImbalanced-Learn Python library [24]. We use the
default setting (k =5) of SMOTE, since our experiment with
variousksettings has shown that varying the ksettings has
little impact on the performance of our JIT defect models.
(Step 2-3) Evaluate Global JIT Defect Models. We build
global JIT defect models using the training data of eachproject. We select the two classiﬁcation techniques that arecommonly-used in prior studies [8, 17], since they found thatRandom Forests (RF) and Logistic Regressions (LR) oftenoutperform other classiﬁcation techniques. Then, we evaluatethe global JIT defect models on the testing dataset using2 effort-aware measures (i.e., Recall@20%effort, and P
opt)
and 2 traditional performance measures (i.e., Area Underthe ROC Curve (AUC) and F1 (with a cutoff threshold of0.5). We select classiﬁers using Recall@20%effort to ensurethat they are practical when they are deployed in practices[28]. Recall@20%effort measures the percentage of correctlypredicted defect-introducing commits that can be found wheninspecting the top-20% LOC of the most risky commits.
Our global JIT defect models trained using both RF
and LR techniques achieve similar performance for bothOpenStack and Qt projects. Table II presents the accuracy
of the JIT defectt models. For Openstack, our RF classiﬁerachieves a Recall@20%Effort of 0.56 for RF and 0.54 for LR,indicating that our JIT defect models can correctly predicted54%-56% of defect-introducing commits when spending only20% code inspection effort (i.e., LOC). Similarly, for Qt, ourRF classiﬁer achieves a Recall@20%Effort of 0.83 for RFand 0.82 for LR, indicating that our JIT defect models cancorrectly predicted 82%-83% of defect-introducing commitswhen inspecting only 20% code inspection effort (i.e., LOC).
(Step 3) Apply our PyExplainer and the LIME model-
agnostic techniques to generate explanations. For each
prediction of JIT defect models, we apply our PyExplainerand LIME to generate an explanation of each prediction.We choose LIME as a baseline comparison, since LIME hasbeen widely used in SE research [13, 29, 30, 49] Similar toPyExplainer, LIME produces three main components: (1) syn-thetic neighbors; (2) local models; and (3) explanations. LIMEperforms the following four steps to produce an explanation.
First, LIME randomly generates synthetic neighbors sur-
rounding an instance to be explained using a random perturba-tion method with an exponential kernel function of euclideandistance. Second, LIME obtains the predictions of the syn-thetic neighbors from the global JIT defect models. Third,LIME builds a local sparse linear regression model (K-Lasso)using the randomly generated instances and their predictionsfrom the global JIT defect models. Forth, LIME generatesan explanation using the coefﬁcients of the local K-Lassomodel with three key pieces of information: (1) a decision
411TABLE II: The accuracy of JIT defect models that are trained
using Random Forest (RF) and Logistic Regression (LR).
OpenStack
Classiﬁcation Recall@20%Effort Popt AUC F1
Random Forest 0.56 0.82 0.75 0.36
Logistic Regression 0.54 0.82 0.66 0.36
Qt
Classiﬁcation Recall@20%Effort Popt AUC F1
Random Forest 0.83 0.94 0.74 0.21
Logistic Regression 0.82 0.95 0.64 0.16
rule feature; (2) the importance score; and (3) the directionof relationship of either supporting (+) or contradicting (-) theprediction towards a TRUE class (i.e., Defect).
(Step 4) Evaluate the PyExplainer and LIME model-
agnostic techniques. Both PyExplainer and LIME use differ-
ent techniques to generate synthetic neighbors (i.e., crossoverand mutation vs. random perturbation) and the local models(i.e., RuleFit vs. K-Lasso). Thus, they may produce differentexplanations. Therefore, we aim to investigate which model-agnostic technique is the best to generate an explanation of theprediction obtained from JIT defect models. To evaluate Py-Explainer and LIME, we focus on the three main componentsalong two dimensions. In the ﬁrst dimension, we focus on thecommon internal mechanism of the model-agnostic techniquesi.e., the synthetic neighbour and the accuracy of their localmodels, since these two components are used to generatean explanation for a prediction. In the second dimension,we focus on the explanations generated by PyExplainer andLIME. We describe the analysis approach for each researchquestion in Section V.
V. E
XPERIMENTAL RESULTS
In this section, we present the approach, and results with
respect to our three research questions.
(RQ1) Does our PyExplainer produce better synthetic neigh-
bors than LIME for JIT defect models?
Approach. To address RQ1, we analyze the distance between
an instance to be explained and the synthetic instances around
the neighbourhood using the Euclidean Distance measure. TheEuclidean Distance measure is the calculation of distancebetween two feature vectors in an n-dimensional feature
space (i.e., d(i
1,i2)=/radicalBig/summationtextn
j=1(i1j−i2j)2, whered(i1,i2)
is a Euclidean Distance of two instances i1andi2). The
smaller the distance, the higher similarity of the both vectors
(instances). Thus, the lower distances between an instance tobe explained and the synthetic instances indicate that suchgenerated synthetic instances yield high similarity with theinstance to be explained.
For each instance to be explained in the testing dataset, we
calculate the Euclidean Distance between the instance to beexplained and their synthetic instances. Since the data is notnormally distributed, we compute the median value (insteadof the average) of the Euclidean Distance to approximate the●
●●
●
●●●●●
●●●
●●
●●●●
●●
●●●
●
●●
●●●
●●
●●●●●
●●
●●●●●●
●
●●●
●●●●●
●●●●●●
●● ●●
●●●●
●●●●
●●
●●
●●●
●
●●●
●●●●●●●●●●●
●OpenStack Qt
RF LR RF LR010002000300040005000PyExplainer LIME
Fig. 3: (RQ1) The Euclidean Distance of neighborhood in-
stances and instances to be explained, obtained from model-agnostic techniques (i.e., PyExplainer and LIME).
average similarity of the instances around the neighbourhood.
Then, we compare the distributions of the median EuclideanDistance of the synthetic neighbors produced by both PyEx-plainer and LIME.
Finally, we apply two statistical test (i.e., Wilcoxon singed-
rank test and Cliff’s |δ|effect size) to identify whether dif-
ferences of the Euclidean Distance produced by PyExplainerand LIME are statistically signiﬁcant. The Wilcoxon signed-rank test is a non-parametric test that measures the differenceof distribution between two population (i.e., the EuclideanDistance of PyExplainer and LIME). Cliff’s |δ|is a non-
parametric effect size test that measures the magnitude ofthe differences of the given two distributions. We use theCliff’s|δ|interpretation of Romano et al. [34] as follows, i.e.,
negligible for |δ|≤0.147, small for |δ|≤0.33, medium for
|δ|≤0.474, and large for |δ|>0.474 Finally, we compute
the relative percentage difference using the following equation:%diﬀ =
PyExplainer− LIME
LIME×100.
Results. The synthetic neighours produced by our Py-Explainer is 41%-55% more similar to an instance tobe explained than LIME for both RF and LR JITdefect models. Figure 3 shows that our PyExplainer achieves
41%-49% and 47%-55% lower Euclidean Distance for bothRF and LR JIT defect models for both Openstack and Qt.For Openstack, we ﬁnd that our PyExplainer achieves amedian Euclidean Distance of 492, while LIME achieves amedian Euclidean Distance 839. For Qt, we ﬁnd that ourPyExplainer achieves a median Euclidean Distance of 492,while LIME achieves a median Euclidean Distance 1,825. TheWilcoxon signed-ranked test conﬁrms that the distributionsof the Euclidean Distance of our PyExplainer is statisticallysigniﬁcantly smaller than LIME (p-value <0.05) with a large
Cliff’s|δ|effect size for both classiﬁers and both projects.
This ﬁnding indicates that our PyExplainer produces syn-
thetic neighbors that are more closely similar to an instanceto be explained than LIME. The less similarity of syntheticneighbors generated by LIME has to do with the use of randomperturbation approach. The random perturbation approach per-
412●●
●●●
●●●
●●●●●●●●●●●●●●●● ●●●●●●●●●●●● ●●●●●●●●●●●●●●●●●● ●●●●●●●●● ●● ●●●●●●●●● ●●●●●●●
●●
●●●●●●●
●
●●●●●●
●●●●●●
●●●●●
●●●●●●
●●●●●
●●●● ●●●●●●●
●●●●●●
●●●
●
●●●
●● ●
●●●●
●●●●●●●
●●●
●●●●
●●●●●●
●●●
●●●●●●
●●
●●●
●●●●●●●
●
●●●●●●●●●●●●●●●●●● ●●●●●●●●●●●●●●●●●●●●●● ●●● ●●●●●●●●●●●●●●●
●●●
●●
●●
●●●
●●●●●●●●●●
●●
●●●
●●
●●●●●
●●
●●
●●●
●●●●●●
●
●●●●
●
●●●●
●●
●●
●●OpenStack QtAUC F1
RF LR RF LR0.60.70.80.91.0
0.000.250.500.751.00PyExplainer LIME
Fig. 4: (RQ2) The accuracy of the local models produced by
our PyExplainer and LIME in terms of AUC and F1.
turbs an instance to be explained by adding a value randomly
drawn from a normal distribution. However, such a simplerandom perturbation approach is not suitable for sparse andhigh dimensional data like JIT datasets [52]. In particular,the random perturbation approach does not account for thecharacteristics of the actual JIT datasets. Thus, we found thatthe random perturbation approach often generates syntheticneighbors that are less similar to an instance to be explainedthan our PyExplainer. On the other hand, our PyExplainergenerates synthetic neighbors based on the characteristicsof JIT dataset using the crossover and mutation operations,producing a more accurate explanation for the predictions ofJIT defect models.
(RQ2) Does our PyExplainer produce higher accuracy of
local models than LIME for JIT defect models?
Approach. To address RQ2, we analyze the accuracy of
the local models generated by PyExplainer and LIME. The
accuracy of local models indicates how well local modelscan approximate (or mimic) the logic of the global models.To do so, we ﬁrst obtain the predicted class (i.e., CLEANand DEFECT) of the synthetic instances from the global JITdefect models. Then, we obtain the probability of CLEAN andDEFECT class of synthetic instances from the local models.Then, we evaluate the accuracy of the predictions between thelocal models and the global JIT model using two traditionalperformance measures, i.e., Area Under the ROC Curve (AUC)and F1. Similar to RQ1, we apply the Wilcoxon signed-ranktest and the Cliff’s |δ|effect size test to evaluate whether
the accuracy of local models of PyExplainer are statisticallysigniﬁcantly higher than LIME.
Results. PyExplainer produces local models that are 18%-
Fig. 5: (RQ2) The probability (y -axis) of synthetic instances
predicted by the local models of PyExplainer and LIME, when
comparing to the actual class of that instances (i.e., the legendof defect and clean) from the RF and LR global JIT defectmodels.
38% more accurate than the LIME’s local models. Figure 4
shows that the local models produced by our PyExplainer
achieve a median AUC of 0.99 when explaining the RF andLR JIT defect models for both Openstack and Qt. On the otherhand, the local models produced by LIME achieves a medianAUC of 0.75 for Openstack and 0.72 for Qt when explainingthe RF JIT defect models, while achieving a median AUCof 0.85 for Openstack and 0.82 for Qt when explaining theLR JIT defect models. This indicates that our PyExplainerproduces local models that are 32%-38% and 18%-24% moreaccurate (AUC) than LIME for both RF and LR JIT defectmodels. Finally, we observe a similar conclusion when usingF-measure, i.e., PyExplainer produces local models that are242.86%-413.5% and 23.46%-29.87% more accurate (F1) thanLIME for both RF and LR JIT defect models. The Wilcoxonsigned-ranked test conﬁrms that the accuracy of local mod-els produced by our PyExplainer is statistically signiﬁcantlyhigher than the accuracy of local models produced by LIME(p-value<0.05) with a large Cliff’s |δ|effect size.
The more accurate local models (i.e., high AUC) produced
by our PyExplainer have to do with the quality of syntheticneighbors generated by our PyExplainer. First, our PyEx-plainer uses crossover and mutation techniques to generatesynthetic neighbors. Thus, the synthetic neighbors are moreclosely similar to an instance to be explained and more similarto the actual characteristics of defect-introducing commits andclean commits from the training data. Therefore, we performa deeper investigation to better understand the distributionof the probability of synthetic neighbours generated by the
413Instance to be explained
Generated instances
Global modelTraining data
Neighborhood
LIME produces 
(RQ1) less similar synthetic neighbors; and 
 (RQ2) a less accurate local model.PyExplainer produces 
(RQ1) more similar synthetic neighbors; and 
 (RQ2) a more accurate local model.
Fig. 6: The characteristics of synthetic neighbors generated by
PyExplainer and LIME.
PyExplainer and LIME local models. Figure 5 shows that the
median probability of defect class (0.98-1.00) and clean class(0.00-0.04) generated by the PyExplainer local models differsby 0.96-0.98. On the other hand, the median probability ofdefect (0.39-0.63) and clean (0.28-0.47) classes generated bythe LIME local models differs by 0.11-0.16. This ﬁndingsindicates that the PyExplainer local models have a higherability to discriminate the characteristics between DEFECTand CLEAN classes, producing higher AUC than the LIMElocal models.
Finally, we illustrate the characteristics of synthetic neigh-
bors generated by PyExplainer and LIME in Figure 6. InRQ1, the smaller Euclidean Distance by PyExplainer indicatesthat PyExplainer produces synthetic neighbors that are moresimilar to (1) an instance to be explained; and (2) the actualcharacteristics of the JIT defect datasets due to the use ofcrossover and mutation operations on training data. In RQ2,the higher AUC and F1 by PyExplainer indicates that ourPyExplainer produces better synthetic neighbors, leading tomore accurate local models than LIME. Therefore, the expla-nation generated by PyExplainer is more closely similar to theexplanation of an instance to be explained than the explanationgenerated by LIME.
(RQ3) Is our PyExplainer more effective in generating
explanations than LIME for JIT defect models?
Approach. To address RQ3, we analyze the explanations
produced by PyExplainer and LIME using the two measures.
•%Unique measures the percentage of unique explanations
generated by each technique. The higher percentage of
unique explanations indicates that a model-agnostic tech-nique can effectively generate a more speciﬁc (i.e., lessduplicate) explanation to the instance to be explained.
•%Consistency measures the percentage of the defect-
introducing commits in the testing data that have char-acteristics satisfying the rule feature in the generatedexplanation. The higher percentage of the consistencyindicates that a model-agnostic technique can effectivelygenerate an explanation that is consistent with the actualcharacteristics of defect-introducing commits.●●
●● ●
●●●
● ● ● ●●
●●
●
●● ● ● ●●
●●
●●●
●●
●● ● ● ● ● ●
●● ● ●
●●●
●●
●● ●
●● ● ● ● ●●
●
●● ● ●
●●
●● ● ● ● ● ●
●● ●
●●●
● ● ● ● ● ●
●
● ●● ● ● ●●
● ●
●● ● ● ● ●
●●
●● ●
●● ● ● ● ● ● ● ● ● ● ●
●●●
● ● ● ●●
●
●●●
● ● ●
●●
●●●
●
●●●●●●
●●
● ●●
●●●
●●
●
●●
●●
● ● ● ● ●
●● ●
●●●● ● ● ●
●●
●●
●● ●●
●● ● ● ●●
●●
● ●
●● ●
●●
● ●●● ●●● ● ●OpenStack Qt
RF LR RF LR0255075100PyExplainer LIME
Fig. 7: (RQ3) The percentage of the defect-introducing com-
mits in the testing data that are consistent with the generatedexplanation.
Results. The explanations generated by our PyExplainer
are 69%-98% more unique (i.e., more speciﬁc to aninstance to be explained) than LIME. We ﬁnd that Py-
Explainer can produce 100% unique explanations for all ofthe instances to be explained for both studied datasets. Onthe other hand, LIME can produce as few as 2%-4% uniqueexplanations for OpenStack and 3%-31% unique explanationsfor Qt. In other words, for OpenStack, we ﬁnd that as muchas 72%-86% of defect-introducing commits have the sameexplanation, despite having different characteristics of thefeature values. Similarly, for Qt, we ﬁnd that as much as 53%-74% of commits have the same explanation, despite havingdifferent characteristics of the testing instances. Thus, the lessduplicate explanations generated by PyExplainer indicates thatPyExplainer can generate explanations that are more speciﬁcto an instance to be explained rather than LIME.
The explanations generated by our PyExplainer are
17%-54% more consistent with the actual defect-introducing commits in the testing data than LIME.Figure 7 shows that PyExplainer achieves a median con-sistency of 73%-75% for OpenStack and 72%-73% for Qt.On the other hand, we ﬁnd that LIME achieves a medianconsistency of 54% for OpenStack and 18%-56% for Qt. Theexperiment result indicates that the explanations generated byour PyExplainer are 19%-21% and 17%-54% more consistentwith the actual defect-introducing commits in the testing datathan LIME for OpenStack and Qt, respectively. The Wilcoxonsigned-ranked test conﬁrms that the percentage consistencyvalue of PyExplainer is statistically signiﬁcantly higher thanLIME (p-value <0.05) with a large Cliff’s |δ|effect size for
both JIT defect models and both studied datasets.
VI. D
ISCUSSION
In this section, we ﬁrst discuss the usage scenario of how
PyExplainer can be used in practice. Then, we present ananalysis of the What-If simulation when the explanations were
414considered (i.e., what if we change this, would it reverse the
predictions of the JIT defect models?). Finally, we describe the
implementation details of the PyExplainer Python package.
A. A Usage Scenario
Let’s consider Bob as a developer in a large-scale software
project that adopts modern code review practices. Bob has
his main responsibility to inspect commits that are submittedby other developers to ensure the quality of commits prior tointegration into the release branch. Suppose that on averageBob spends one hour to review one commit. Hence, withhis average 8 working hours per day, he can review only8 commits per day. Given a huge number of newly arrivedcommits everyday (e.g., 100 commits per day), Bob does notknow which commits should be reviewed ﬁrst. With the useof JIT model, the list of commits can be prioritized basedon the likelihood of being defect-introducing provided by theJIT defect model so that Bob can efﬁciently spend his limitedtime on the most risky commits. However, Bob still may notbe convinced by the predictions of JIT defect models, sincehedoes not understand why a commit is predicted as defect-
introducing. Thus, Bob may not trust the JIT defect models andmay decide to ignore the predictions, resulting in suboptimalSQA resource allocation and prioritization.
With PyExplainer, Bob now better understands why a
commit is predicted as defect-introducing since PyExplainerprovides an explanation (i.e., which feature is the most impor-tant for a given prediction). For example, PyExplainer providesan explanation (e.g., Churn >100⇒Defect) that a commit
is predicted as defect-introducing because the churn size isgreater than 100. This kind of explanation could help Bob tofocus on the most important aspects that are associated withthe risk of being defect-introducing (i.e., considering reducingthe churn size of the commit), instead of focusing on the lessimportant aspects (e.g., inviting more reviewers). However,it remains challenging for Bob to consider which value ofa feature that should be changed to mitigate the risk. Inparticular, given an explanation (e.g., Churn >100⇒Defect),
Bob still does not know how small a Churn value should bethat could reverse the prediction of JIT models from DEFECTto CLEAN. Thus, an interactive what-if visualization tool is
needed to help Bob making better decisions of how much theChurn value that should be changed.
B. What-If Analysis
We conducted a what-if simulation based on a hypothetical
scenario if the explanations of our PyExplainer were consid-
ered. In particular, we investigated what if we change the value
of a feature guided by the explanation, would it reverse the pre-diction of the JIT defect model?. For example, an explanation(Churn> 100⇒DEFECT) generated by PyExplainer means
that a commit is predicted as defect-introducing since Churnis greater than 100. Thus, what if Churn was less than 100,would the prediction be reversed from DEFECT to CLEAN.
To conduct this what-if simulation, we ﬁrst generate a simu-
lated instance. The simulated instance is an instance where the84%
69%84%
67%OpenStack Qt
RF LR RF LR0255075100
(a) The percentage of the sim-
ulated instances that can reversethe prediction from DEFECT toCLEAN.●●
●●
●●OpenStack Qt
RF LR RF LR0255075100
(b) The difference between the
probability of the original in-stance and the probability of thesimulated instance.
Fig. 8: The results of the what-if analysis.
actual value of a feature guided by the rule-based explanation
was changed in the opposite direction of the comparison op-erator of the explanation (i.e., decrease for >, increase for <)
by one SD (a standard deviation of that feature in the trainingdata) from the rule threshold. According to the above example,the simulated instance is the modiﬁed original instance wherethe actual value of a feature (e.g., Churn=120) guided bythe rule-based explanation (Churn> 100⇒DEFECT) was
changed in the opposite direction of the comparison operatorof the explanation (i.e., decrease for >) by one SD (e.g., 20)
from the rule threshold (100). Thus, the Churn value of thesimulated instance is 80 (i.e., 100-20). Then, we input thissimulated instance to the global JIT defect model and analyzewhether the simulated instance could reverse the predictionsof the global JIT defect models.
We perform this what-if simulation for all the explanations
generated by our PyExplainer for all of the commits in the test-ing dataset that are correctly predicted as defect-introducing bythe JIT defect models. Then, we measure (1) %reversed, i.e.,
the percentage of the simulated instances that can reverse theprediction from DEFECT to CLEAN; and (2) %prob
diff, i.e.,
the difference between the probability of the original instanceand the probability of the simulated instance.
Figure 8a shows that, when considering the explanations
guided by our PyExplainer, 84% (RF) and 67%-69% (LR)of the simulated instances that can reverse the predictionfrom DEFECT to CLEAN of the global JIT defect models.Furthermore, Figure 8b also shows that, after considering theexplanations guided by our PyExplainer, the probability of thesimulated instance is decreased by 30% for the RF modelsand 22% - 28% for the LR models when comparing to theprobability of the original instance. This simulation highlightsthe importance of our PyExplainer for helping practitioners tofocus on the most important aspects that are associated withthe risk of being defect-introducing for a given commit, insteadof focusing on the less important aspects. Nevertheless, the oneSD used in this what-if simulations is just an example, theactual changed value should be subject to the domain experts.
415Why this commit is predicted as defect-introducing?
#1 The value of LinesAdded  is more than 155
Actual = 155
04 50 400 350 300 250 200 150 100 50
02 0 18 14 12 10 8 6 4 21 6
01 0 9 7 6 5 4 3 2 18#2 The value of ReviewRevisions  is less than 12
Actual = 12
#3 The value of Reviewers is less than 3
Actual = 3
#1 The value of LinesAdded  is more than 155
#2 The value of ReviewRevisions  is less than 12
#3 The value of Reviewers is less than 3155
12
3Risk Score: 94%
(a) The visual explanation of the original instance (predicted as
DEFECT with a risk score of 98%).
Why this commit is predicted as defect-introducing?
#1 The value of LinesAdded  is more than 155
Actual = 155
450 400 350 300 250 200 150 100 50
20 18 14 12 10 8 6 4 21 6
10 9 7 6 5 4 3 2 18#2 The value of ReviewRevisions  is less than 12
Actual = 12
#3 The value of Reviewers is less than 3
Actual = 3
#1 The value of LinesAdded  is more than 155
#2 The value of ReviewRevisions  is less than 12
#3 The value of Reviewers is less than 350
14
2Risk Score: 28%
0
0
0
(b) The visual explanation of the simulated instance when changing
the feature values (predicted as CLEAN with a risk score of 28%).
Fig. 9: The proof-of-concept visualization of our PyExplainer consists of (1) the risk score (i.e., the probability of an instance
to be explained by the global JIT model); (2) the visual explanation (in the black border); and (3) the interactive what-ifvisualization for our PyExplainer.
C. The PyExplainer Python package
To ease the adoption of our PyExplainer by practitioners and
to facilitate the replication of future research, we developedthe PyExplainer Python package. In our PyExplainer Pythonpackage, we also developed a proof-of-concept of the visualexplanation and the interactive what-if visualization.
The visual explanation is developed to present the rule-
based explanation in a form of a bullet plot visualizationwith textual explanations. Figure 9a shows an example of thevisual explanation of an OpenStack commit (a9a59cc). Ourvisual explanation is designed to provide the following keyinformation: (1) textual descriptions that explain why a commitis predicted as defect-introducing; (2) the actual feature valuesof the commit (i.e., the vertical black bars); and (3) therange of feature values associated with the risk score (i.e.,the predicted probability). The green shades indicate the non-risky range values of a feature, while the red shades indicatethe risky range values of a feature.
An interactive what-if visualization is developed to help
practitioners interactively change the value of a feature, whileimmediately generating the new estimated risk score (i.e.,the probability obtained from the JIT defect model). Thisvisualization will allow practitioners to explore different valuesof a feature prior to making a decision.
Figure 9b shows an example of an interactive what-if visu-
alization for an OpenStack commit (a9a59cc). Through thevisualization, the user can change the value of the feature (e.g.,changing the value of Lines Added from 155 to 50, the value
ofReview Revisions from 12 to 14, and the value of Reviewers
from 3 to 2). Then, the visualization will responsively updatethe predicted probability generated by the JIT defect model(e.g., from 94% to 28%).
D. Implications to Practitioners and Researchers
The contributions of this paper build an important step
towards a new research area of Explainable AI for SE, by
making the predictions of just-in-time defect models moreexplainable and actionable. A lack of explainability and ac-tionability of software analytics has been raised by bothpractitioners [5, 14, 25] and researchers [3, 13, 21, 29, 31, 50].For example, Rajapaksha et al. [31] proposed an approach to
generate actionable suggestions (i.e., counterfactual explana-tions) for ﬁle-level defect prediction. Peng and Menzies [29]also proposed a TimeLIME approach (an extension of LIMEmodel-agnostic technique) to generate actionable suggestions(i.e., defect reduction plans). However, the approaches ofboth Rajapaksha et al. [31] and Peng and Menzies [29] are
designed for release-based defect prediction, which requiremultiple releases for training and evaluation. Thus, they arenot applicable to JIT defect prediction models. On the otherhand, our results show that our PyExplainer is more effectivein generating explanations than LIME for the predictions ofJIT defect models, while providing an interactive what-ifvisualization so practitioners can make better data-informeddecisions. Similar effort to other state-of-the-art model agnos-tic techniques (e.g., LIME [32]), we make our PyExplainerPython package publicly-available to ease the adoption bypractitioners and researchers.
E. Threats to V alidity
Threats to construct validity relates to the hyperparameter
settings of RandomForest, SMOTE, and LIME techniques
when conducting our experiment [39, 42, 44]. To ensure thereprehensibility, the used parameter setting of such techniquesare reported in the replication package in our GitHub reposi-tory (the replication-package branch).
Threats to internal validity relates to the randomization of
our PyExplainer (i.e., the neighbour generation process). Tomitigate any conclusion instability threat, we chose to generate2,000 neighbours. After we repeated the experiment ﬁve times,the conclusion of our paper remains the same. Nevertheless,future work can explore what would be the minimum syntheticneighbours that can produce stable local explanations (i.e., thesame local explanations when they are regenerated).
Threats to the external validity relates to the generalizability
of our PyExplainer approach. However, our experiment onlyfocused on the just-in-time defect prediction problem, with thelimited number of the studied classiﬁcation techniques, and thelimited number of studied projects. Thus, other classiﬁcationtechniques and other projects should be explore in future work.
416VII. C ONCLUSION
Prior studies proposed Just-In-Time (JIT) defect prediction,
yet its explanability remains largely unexplored (i.e., practi-
tioners still do not know why a commit is predicted as defect-introducing). In this paper, we propose PyExplainer, a novellocal rule-based model-agnostic technique for explaining thepredictions of JIT defect models. Through a case study of twoopen-source software projects, we ﬁnd that our PyExplainerproduces: (1) synthetic neighbours that are 41%-45% moresimilar to an instance to be explained; (2) 18%-38% moreaccurate local models; and (3) explanations that are 69%-98%more unique and 17%-54% more consistent with the actualcharacteristics of defect-introducing commits in the future thanLIME (a state-of-the-art model-agnostic technique).
PyExplainer is designed for explaining the predictions of
any classiﬁcation problems. Future work should explore if ourPyExplainer can be used to effectively explain the predictionsof other classiﬁcation problems (e.g., vulnerability prediction,code smell detection) in software engineering.
Publishing the PyExplainer Python Package. To ease the
adoption of our PyExplainer by practitioners and to facilitatethe replication of future research, the PyExplainer packageis available in both conda andpip (Package Installer for
Python). Our PyExplainer Python package also has a codecoverage of 93% measured by CodeCov with an A+ qualitygraded by LGTM.
g yy
VIII. A T UTORIAL OF THE PYEXPLAINER PACKAGE .
Below, we present a tutorial of how to use the PyExplainer
Python package step-by-step using Code Block 1.
(Step 1) The PyExplainer package is installed using pip
(Python Package Management system).
(Step 2) The PyExplainer package is imported.
(Step 3) The data for demonstration is obtained from
PyExplainer package. The data is composed of X_train,
y_train, indep, dep, blackbox_model, X_explain,
y_explain. The X_train variables are used to gen-
erate neighborhood instances. The indep anddep vari-
ables specify the feature names and label, respectively. Theblackbox_model is the global JIT defect models from
Scikit-learn module. The X_explain represents an instance
to be explained while the y_explain is the label of the
instance to be explained.
(Step 4) A PyExplainer object is created and an explanation
is obtained from the explain function.
(Step 5) The visual explanation and the what-if visualiza-
tion are generated by the visualise function.
IX. A
CKNOWLEDGEMENT
Chakkrit Tantithamthavorn was supported by the Aus-
tralian Research Council’s Discovery Early Career ResearcherAward (DECRA) funding scheme (DE200100941). PatanamonThongtanunam was supported by the Australian ResearchCouncil’s Discovery Early Career Researcher Award (DE-CRA) funding scheme (DE210101091).1# step 1 - install the pyexplainer package
2!pip install pyexplainer
3# step 2 - import necessary libraries
4from pyexplainer.pyexplainer_pyexplainer import
PyExplainer as pyexp
5from pyexplainer import pyexplainer_pyexplainer
6# step 3 - get the preprocessed data and global
model to be tuned in to the PyExplainer object
7dflt = pyexplainer_pyexplainer.get_dflt()
8# step 4 - create a PyExplainer object using the
preprocessed data and model, and generate rules
by utilising the built-in local model in
PyExplainer
9exp = pyexp(X_train=dflt[’X_train’],
10 y_train=dflt[’y_train’],
11 indep=dflt[’indep’],
12 dep=dflt[’dep’],
13 blackbox_model=dflt[’blackbox_model’])
14rules = exp.explain(X_explain=dflt[’X_explain’],
15 y_explain=dflt[’y_explain’])
16# step 5 - visualise the rules generated by the
local model and the prediction generated by the
global model
17exp.visualise(rules)
Code Block 1: An Example Tutorial of the PyExplainer Pythonpackage.
REFERENCES
[1] A. Agrawal and T. Menzies, “Is Better Data Better Than Better Data
Miners?: On the Beneﬁts of Tuning SMOTE for Defect Prediction,” in
Proceedings of the International Conference on Software Engineering(ICSE), 2018, pp. 1050–1061.
[2] N. V . Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer,
“SMOTE: Synthetic Minority Over-sampling Technique,” Journal of
Artiﬁcial Intelligence Research, pp. 321–357, 2002.
[3] D. Chen, W. Fu, R. Krishna, and T. Menzies, “Applications of Psy-
chological Science for Actionable Analytics,” in Proceedings of the
European Software Engineering Conference and Symposium on theF oundations of Software Engineering (ESEC/FSE), 2018, pp. 456–467.
[4] D. A. da Costa, S. McIntosh, W. Shang, U. Kulesza, R. Coelho, and
A. E. Hassan, “A Framework for Evaluating the Results of the SZZApproach for Identifying Bug-introducing Changes,” IEEE Transactions
on Software Engineering (TSE), vol. 43, no. 7, pp. 641–657, 2017.
[5] H. K. Dam, T. Tran, and A. Ghose, “Explainable Software Analytics,” in
Proceedings of the International Conference on Software Engineering:New Ideas and Emerging Results (ICSE-NIER), 2018, pp. 53–56.
[6] J. Fox, Applied regression analysis and generalized linear models , 2015.
[7] J. H. Friedman, B. E. Popescu et al., “Predictive learning via rule
ensembles,” Annals of Applied Statistics, pp. 916–954, 2008.
[8] T. Fukushima, Y . Kamei, S. McIntosh, K. Yamashita, and N. Ubayashi,
“An empirical study of just-in-time defect prediction using cross-projectmodels,” in Proceedings of the Working Conference on Mining Software
Repositories (MSR), 2014, pp. 172–181.
[9] R. Guidotti, A. Monreale, S. Ruggieri, D. Pedreschi, F. Turini, and
F. Giannotti, “Local rule-based explanations of black box decisionsystems,” arXiv preprint arXiv:1805.10820, 2018.
[10] T. Hoang, H. K. Dam, Y . Kamei, D. Lo, and N. Ubayashi, “DeepJIT: an
end-to-end deep learning framework for just-in-time defect prediction,”inProceedings of the International Conference on Mining Software
Repositories (MSR), 2019, pp. 34–45.
[11] T. Hoang, H. J. Kang, D. Lo, and J. Lawall, “CC2Vec: Distributed
representations of code changes,” in Proceedings of the International
Conference on Software Engineering (ICSE), 2020, pp. 518–529.
[12] Y . Jia, J. Bailey, K. Ramamohanarao, C. Leckie, and M. E. Houle, “Im-
proving the quality of explanations with local embedding perturbations,”inProceedings of International Conference on Special Interest Group on
Knowledge Discovery & Data Mining (SIGKDD), 2019, pp. 875–884.
[13] J. Jiarpakdee, C. Tantithamthavorn, H. K. Dam, and J. Grundy, “An
Empirical Study of Model-Agnostic Techniques for Defect Prediction
417Models,” IEEE Transactions on Software Engineering (TSE),p .T o
Appear, 2020.
[14] J. Jiarpakdee, C. Tantithamthavorn, and J. Grundy, “Practitioners’ Per-
ceptions of the Goals and Visual Explanations of Defect Prediction
Models,” in Proceedings of the International Conference on Mining
Software Repositories (MSR), 2021, p. To Appear.
[15] J. Jiarpakdee, C. Tantithamthavorn, and A. E. Hassan, “The Impact of
Correlated Metrics on Defect Models,” IEEE Transactions on Software
Engineering (TSE), p. To Appear, 2019.
[16] J. Jiarpakdee, C. Tantithamthavorn, and C. Treude, “AutoSpearman:
Automatically Mitigating Correlated Software Metrics for InterpretingDefect Models,” in Proceedings of the International Conference on
Software Maintenance and Evolution (ICSME), 2018, pp. 92–103.
[17] Y . Kamei, E. Shihab, B. Adams, A. E. Hassan, A. Mockus, A. Sinha, and
N. Ubayashi, “A Large-Scale Empirical Study of Just-In-Time QualityAssurance,” IEEE Transactions on Software Engineering (TSE) , vol. 39,
no. 6, pp. 757–773, 2013.
[18] C. Khanan, W. Luewichana, K. Pruktharathikoon, J. Jiarpakdee, C. Tan-
tithamthavorn, M. Choetkiertikul, C. Ragkhitwetsagul, and T. Sunet-nanta, “JITBot: An Explainable Just-In-Time Defect Prediction Bot,” in2020 35th IEEE/ACM International Conference on Automated SoftwareEngineering (ASE). IEEE, 2020, pp. 1336–1339.
[19] S. Kim, T. Zimmermann, E. J. Whitehead Jr, and A. Zeller, “Predict-
ing Faults from Cached History,” in Proceedings of the International
Conference on Software Engineering (ICSE), 2007, pp. 489–498.
[20] H. C. Kraemer, G. A. Morgan, N. L. Leech, J. A. Gliner, J. J. Vaske,
and R. J. Harmon, “Measures of Clinical Signiﬁcance,” Journal of the
American Academy of Child & Adolescent Psychiatry (JAACAP), pp.1524–1529, 2003.
[21] R. Krishna and T. Menzies, “Learning Actionable Analytics from
Multiple Software Projects,” Empirical Software Engineering (EMSE),
pp. 3468–3500, 2020.
[22] R. Krishnan, D. Liang, and M. Hoffman, “On the challenges of learning
with inference networks on sparse, high-dimensional data,” in Interna-
tional Conference on Artiﬁcial Intelligence and Statistics. PMLR, 2018,pp. 143–151.
[23] T. Laugel, X. Renard, M.-J. Lesot, C. Marsala, and M. Detyniecki,
“Deﬁning locality for surrogates in post-hoc interpretablity,” arXiv
preprint arXiv:1806.07498, 2018.
[24] G. Lema ˆıtre, F. Nogueira, and C. K. Aridas, “Imbalanced-learn: A
python toolbox to tackle the curse of imbalanced datasets in machinelearning,” Journal of Machine Learning Research, pp. 1–5, 2017.
[25] C. Lewis, Z. Lin, C. Sadowski, X. Zhu, R. Ou, and E. J. Whitehead Jr,
“Does Bug Prediction Support Human Developers? Findings from aGoogle Case Study,” in Proceedings of the International Conference on
Software Engineering (ICSE), 2013, pp. 372–381.
[26] D. Lin, C. Tantithamthavorn, and A. E. Hassan, “The impact of
data merging on the interpretation of cross-project just-in-time defectmodels,” IEEE Transactions on Software Engineering, 2021.
[27] S. McIntosh and Y . Kamei, “Are ﬁx-inducing changes a moving tar-
get? a longitudinal case study of just-in-time defect prediction,” IEEE
Transactions on Software Engineering (TSE), pp. 412–428, 2017.
[28] C. Ni, X. Xia, D. Lo, X. Chen, and Q. Gu, “Revisiting supervised and
unsupervised methods for effort-aware cross-project defect prediction,”IEEE Transactions on Software Engineering, 2020.
[29] K. Peng and T. Menzies, “Defect Reduction Planning (using Time-
LIME),” IEEE Transactions on Software Engineering (TSE), 2021.
[30] C. Pornprasit and C. Tantithamthavorn, “JITLine: A Simpler, Better,
Faster, Finer-grained Just-In-Time Defect Prediction,” in Proceedings of
the International Conference on Mining Software Repositories (MSR),2021, p. To Appear.
[31] D. Rajapaksha, C. Tantithamthavorn, J. Jiarpakdee, C. Bergmeir,
J. Grundy, and W. Buntine, “SQAPlanner: Generating Data-InformedSoftware Quality Improvement Plans,” IEEE Transactions on Software
[32] M. T. Ribeiro, S. Singh, and C. Guestrin, “Why should I trust you?:
Explaining the Predictions of Any Classiﬁer,” in Proceedings of the
International Conference on Knowledge Discovery & Data Mining(KDD) , 2016, pp. 1135–1144.Engineering (TSE), 2021.
[33] ——, “Anchors: High-precision model-agnostic explanations,” in Thirty-
Second AAAI Conference on Artiﬁcial Intelligence, 2018.
[34] J. Romano, J. D. Kromrey, J. Coraggio, and J. Skowronek, “Appropriate
Statistics for Ordinal Level Data: Should we really be using T-test andCohen’s d for Evaluating group differences on the NSSE and othersurveys,” in Annual meeting of the Florida Association of Institutional
Research (F AIR), 2006, pp. 1–33.
[35] S. Ruangwan, P. Thongtanunam, A. Ihara, and K. Matsumoto, “The
Impact of Human Factors on the Participation Decision of Reviewers inModern Code Review,” Empirical Software Engineering (EMSE) ,p .I n
press, 2018.
[36] J. ´Sliwerski, T. Zimmermann, and A. Zeller, “When do changes induce
ﬁxes?” in Proceedings of the International Workshop on Mining Soft-
ware Repositories (MSR), 2005, p. 1–5.
[37] M. Srinivas and L. M. Patnaik, “Adaptive probabilities of crossover and
mutation in genetic algorithms,” IEEE Transactions on Systems, Man,
and Cybernetics (TSMC), pp. 656–667, 1994.
[38] C. Tantithamthavorn and A. E. Hassan, “An Experience Report on
Defect Modelling in Practice: Pitfalls and Challenges,” in In Proceedings
of the International Conference on Software Engineering: SoftwareEngineering in Practice Track (ICSE-SEIP), 2018, pp. 286–295.
[39] C. Tantithamthavorn, A. E. Hassan, and K. Matsumoto, “The Impact of
Class Rebalancing Techniques on The Performance and Interpretation ofDefect Prediction Models,” IEEE Transactions on Software Engineering
(TSE), p. To Appear, 2019.
[40] C. Tantithamthavorn, J. Jiarpakdee, and J. Grundy, “Explainable AI for
Software Engineering,” arXiv preprint arXiv:2012.01614, 2020.
[41] ——, “Actionable analytics: Stop telling me what it is; please tell me
what to do,” IEEE Software, vol. 38, no. 4, pp. 115–120, 2021.
[42] C. Tantithamthavorn, S. McIntosh, A. E. Hassan, and K. Matsumoto,
“Automated Parameter Optimization of Classiﬁcation Techniques forDefect Prediction Models,” in Proceedings of the International Con-
ference on Software Engineering (ICSE), 2016, pp. 321–332.
[43] ——, “An Empirical Comparison of Model Validation Techniques for
Defect Prediction Models,” IEEE Transactions on Software Engineering
(TSE), vol. 43, no. 1, pp. 1–18, 2017.
[44] ——, “The Impact of Automated Parameter Optimization on Defect
Prediction Models,” IEEE Transactions on Software Engineering (TSE),
pp. 683–711, 2018.
[45] P. Thongtanunam, S. McIntosh, A. E. Hassan, and H. Iida, “Revisiting
Code Ownership and its Relationship with Software Quality in theScope of Modern Code Review,” in Proceedings of the International
Conference on Software Engineering (ICSE), 2016, pp. 1039–1050.
[46] ——, “Review Participation in Modern Code Review,” Empirical Soft-
ware Engineering (EMSE), vol. 22, no. 2, pp. 768–817, 2017.
[47] P. Thongtanunam, W. Shang, and A. E. Hassan, “Will this clone be short-
lived? towards a better understanding of the characteristics of short-livedclones,” Empirical Software Engineering, vol. 24, no. 2, pp. 937–972,
2019.
[48] D. Wang, T. Xiao, P. Thongtanunam, R. G. Kula, and K. Matsumoto,
“Understanding shared links and their intentions to meet informationneeds in modern code review,” in The Journal of Empirical Software
Engineering (EMSE), vol. 26, no. 96, 2021, p. to appear.
[49] S. Wattanakriengkrai, P. Thongtanunam, C. Tantithamthavorn, H. Hata,
and K. Matsumoto, “Predicting defective lines using a model-agnostictechnique,” IEEE Transactions on Software Engineering (TSE), 2020.
[50] Y . Yang, D. Falessi, T. Menzies, and J. Hihn, “Actionable analytics for
software engineering,” IEEE Software, pp. 51–53, 2017.
[51] S. Yathish, J. Jiarpakdee, P. Thongtanunam, and C. Tantithamthavorn,
“Mining Software Defects: Should We Consider Affected Releases?” inIn Proceedings of the International Conference on Software Engineering(ICSE), 2019, p. To Appear.
[52] H. Zhang, A. Nelson, and T. Menzies, “On the value of learning from
defect dense components for software defect prediction,” in Proceedings
of the International Conference on Predictive Models in SoftwareEngineering (PROMISE), 2010, pp. 1–9.
418