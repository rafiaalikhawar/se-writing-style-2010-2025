Deep GUI: Black-box GUI Input Generation with
Deep Learning
Faraz Y azdaniBanafsheDaragh
School of Information and Computer Sciences
University of California, Irvine, USA
faraz.yazdani@uci.eduSam Malek
School of Information and Computer Sciences
University of California, Irvine, USA
malek@uci.edu
Abstract —Despite the proliferation of Android testing tools,
Google Monkey has remained the de facto standard for prac-
titioners. The popularity of Google Monkey is largely due tothe fact that it is a black-box testing tool, making it widely
applicable to all types of Android apps, regardless of theirunderlying implementation details. An important drawback ofGoogle Monkey, however, is the fact that it uses the most naiveform of test input generation technique, i.e., random testing.In this work, we present Deep GUI, an approach that aims
to complement the beneﬁts of black-box testing with a moreintelligent form of GUI input generation. Given only screenshotsof apps, Deep GUI ﬁrst employs deep learning to construct amodel of valid GUI interactions. It then uses this model togenerate effective inputs for an app under test without theneed to probe its implementation details. Moreover, since thedata collection, training, and inference processes are performedindependent of the platform, the model inferred by Deep GUIhas application for testing apps in other platforms as well.We implemented a prototype of Deep GUI in a tool calledMonkey++ by extending Google Monkey and evaluated it for its
ability to crawl Android apps. We found that Monkey++ achievessigniﬁcant improvements over Google Monkey in cases where anapp’s UI is complex, requiring sophisticated inputs. Furthermore,our experimental results demonstrate the model inferred usingDeep GUI can be reused for effective GUI input generation acrossplatforms without the need for retraining.
I. I NTRODUCTION
Automatic input generation for Android applications (apps)
has been a hot topic for the past decade in the software
engineering community [1]–[14]. Input generators have avariety of applications. Among others, they are used forverifying functional correctness (e.g., [13], [15], [16]), security(e.g., [17], [18]), energy consumption (e.g., [19], [20]), andaccessibility (e.g., [21]) of apps. Depending on the objectiveat hand, input generators can be very generic, and simply crawlapps to maximize coverage [22]–[24], or can be very speciﬁc,looking for certain criteria to be fulﬁlled, such as reachingactivities with speciﬁc attributes [2].
Common across the majority of existing input generators
is the fact that they are white-box, i.e., require access to
implementation details of the app under test (AUT).F o r
instance, many tools use static analysis to ﬁnd the rightcombination of interactions with the AUT [1]–[4], while othertools depend on the XML-based GUI layout of the AUT toﬁnd the GUI widgets and interact with them [5]–[14]. Theunderlying implementation details of an AUT provide thesetools with insights to produce effective inputs, but also posesevere limitations that compromise the applicability of thesetools. First, there is a substantial degree of heterogeneityin the implementation details of apps. Consider for instancethe fact that many Android apps are non-native, e.g., builtout of activities that are just wrappers for web content. Inthese situations, the majority of existing tools either fail tooperate or achieve very poor results. Second, the source codeanalyses underlying these tools are tightly coupled to theAndroid platform, and often to speciﬁc versions of it, makingthem extremely fragile when used for testing apps in a newenvironment.
Black-box input generation tools do not suffer from the
same shortcomings. Google Monkey is the most widely usedblack-box testing tool for Android. Despite being a randominput generator, prior studies suggest Google Monkey outper-forms many of the existing white- and gray-box tools [25].This can be attributed to the fact that Google Monkey issigniﬁcantly more robust than almost all other existing tools,i.e., it works on all types of apps regardless of how theyare implemented. However, Google Monkey employs the mostbasic form of input generation strategy. It blindly interacts withthe screen without knowing if its actions are valid. This mightwork well in apps with a simple GUI, where the probabilityof randomly choosing a valid action is high, but not in appswith a complex GUI. For instance, take Figure 1. In Figure1a, since most of the screen contains buttons, almost all of thetimes that Google Monkey decides to generate a touch action,it touches something valid and therefore tests a functionality.However, in Figure 1b, it is much less probable for GoogleMonkey to successfully touch the one button that exists on thescreen, and therefore it takes much longer than needed for itto test the app’s functionality.
This article presents Deep GUI, a black-box GUI input
generation technique with deep learning that aims to addressthe above-mentioned shortcoming. Deep GUI is able to ﬁlterout the parts of the screen that are irrelevant with respect toa speciﬁc action, such as touch, and therefore increases theprobability of correctly interacting with the AUT. For example,given the screenshot shown in Figure 1b, Deep GUI ﬁrstproduces the heatmap in Figure 1c, which shows for eachpixel the probability of that pixel belonging to a touchablewidget. It then uses this heatmap to touch the pixels with a
9052021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000842021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678778
978-1-6654-0337-5/21/$31.00  ©2021  IEEE
(a)
 (b)
(c)
 (d)
 (e)
Fig. 1: Two examples where it is respectively easy (a) and difﬁcult (b) for Google Monkey to ﬁnd a valid action, as well as
the heatmaps generated by Deep GUI associated with (b) for touch (c), scroll (d), and swipe (e) actions respectively. Note thatin (c) the model correctly identiﬁes both the button and the hyperlink –and not the plain text– as touchable.
probability that is proportionate to their heatmap value, henceincreasing the chance of touching the button in this example.
In order to produce such heatmaps, Deep GUI undertakes
a deep-learning approach. We further show that this approachis a special case of a more general method known as deepreinforcement learning, and we discuss how this method canbe used to develop even more intelligent input generationtools. Moreover, what makes Deep GUI unique is that it usesa completely black-box and cross-platform method to collectdata, learn from it, and produce the mentioned heatmaps, andhence supports all situations, applications, and platforms. Italso uses the power of transfer learning to make its training
more data-efﬁcient and faster. Our experimental evaluationshows that Deep GUI is able to improve Google Monkey’sperformance on apps with complex GUIs, where GoogleMonkey struggles to ﬁnd valid actions. It also shows that wecan take a Deep GUI model that is trained on Android, anduse it on other platforms, speciﬁcally web in our experiments,for efﬁcient input generation.
In summary, this article makes the following contributions:
1) We propose Deep GUI, a black-box approach for gen-
eration of GUI inputs using deep learning. To the bestof our knowledge, this is the ﬁrst approach that usesa completely black-box and cross-platform approach fordata collection, training, and inference in the generationof test inputs.
2) We provide an implementation of Deep GUI for Android,
called Monkey++, by extending Google Monkey. We
make this tool available publicly.
1
3) We present detailed evaluation of Deep GUI using An-
drotest benchmark [25], consisting of 31 real-world mo-bile apps, as well as the top 15 websites in the US [26].Our results corroborate Deep GUI’s ability to improveboth the code coverage and the speed with which thiscoverage can be attained.
The remainder of this paper is organized as follows. Sec-
tion II describes the details of our approach. Section IIIprovides our evaluation results. Section IV reviews the mostrelevant prior work. Finally, in Section V, the paper concludeswith a discussion of our contributions, limitations of our work,and directions for future research.
1https://github.com/Feri73/deep-gui
906II. A PPROACH
We formally provide our deﬁnition of the problem for
automatically generating inputs in a test environment. Suppose
that at each timestep t, the environment provides us with its
statest. This can be as simple as the screenshot, or can be a
more complicated content such as the UI tree. Also, supposewe deﬁne A={α
1,...αN}as the set of all possible actions
that can be performed in the environment at all timesteps.For instance, in Figure 1b, all of the touch events associatedwith all pixels on the screen can be included in A. Note
that these actions are not necessarily valid. We deﬁne a validaction as an action that results in triggering a functionality(like touching the send button) or changing the UI state (likescrolling down a list). Let us deﬁne r
t=r(st,at)to be 1
ifatis valid when performed on st, and 0 otherwise. Our
goal is to come up with a function Qthat, given st, produces
the probability of validity for each possible action. That is,Q(s
t,at)identiﬁes how probable it is for atto be a valid action
when performed on st. Therefore, Qis essentially a binary
classiﬁer (valid vs. non-valid) conditioned on stindependently
for each action in the set A. For simplicity, we also deﬁne
Q(st)as a function that, given an action α, returnsQ(st,α).
That is,Q(st)(α)=Q(st,α).
In Deep GUI, we consider stto be the screenshot of AUT
at each timestep. Set Aconsists of touch, up and down scroll,
and right and left swipe events, on all of the pixels of thescreen. We also deﬁne r
tas follows:
r(st,at)=/braceleftBigg
0 if equals( st,st+1)
1 otherwise
That is, if the screenshot undergoes a legitimate change afteran action, we consider that action to be a valid one in thatscreen. We deﬁne what a legitimate change means later in thissection. Note that we deﬁned s
t,A, andrtindependent of the
platform on which AUT operates. Therefore, this approach canbe used in almost all existing test environments.
This work consists of four components:
A. Data collection: This component helps in collecting nec-essary data to learn from.
B. Model: At the core of this component is a deep neuralnetwork that processes s
tand produces a heatmap Q(st)
for all possible actions at, such as the ones shown in
Figure 1. The neural network is initialized with weightslearned from large image classiﬁcation tasks to providefaster training.
C. Inference: After training, and at the inference time, thereare multiple readout mechanisms available for using theproduced heatmaps and generating a single action. Thesemechanisms are used in a hybrid fashion to provide uswith the advantages of all of them.
D. Monkey++: This is the only component that is specializedfor Android, and its application is to fairly compare DeepGUI with Google Monkey. It also provides a convenientmedium to use Deep GUI for testing of Android apps, asit can replace Google Monkey and be used in practicallythe same way.
Figure 2 shows an overview of these four components andhow they interact.
A. Data Collection
Since we reduced the problem to a classiﬁcation problem,
each datapoint in our dataset needs to be in the form of a
three-way tuple (s
t,at,rt), where our model tries to classify
the pair (st,at)into one of the two values that rtrepresents,
i.e. whether performing the action aton the state stis valid or
not. Training a deep neural network requires a large amount ofdata for training. To that end, we have developed an automaticmethod to generate this dataset.
As deﬁned above, r
trepresents whether the screen has a
legitimate change after an action. We here deﬁne legitimatechange as a change that does not involve an animated partof the screen. In other words, if speciﬁc parts of the screenchange even in case of no interaction with the app, we ﬁlterthose parts out when computing r
t. For instance, in Android,
when focused on a textbox, a cursor keeps appearing and dis-appearing every second. We ﬁlter out the pixels correspondingto the cursor.
For data collection, we ﬁrst dedicate a set of apps to be
crawled. Then, for each app, we randomly interact with theapp with the actions in the set Aand record the screenshot, the
action, and whether the action resulted in a legitimate change.In order to ﬁlter out animated parts of the screen, before eachaction, we ﬁrst record the screen for 5 seconds and considerall pixels that change during this period to be animated pixels.While this method does not fully ﬁlter all of the illegitimatechanges
2, as our experimental results suggest, it is adequate.
A keen observer would realize that this method of data
collection is a very natural choice in the realm of Android. Foryears, Google Monkey has been used to crawl Android appsfor different purposes, but the valuable data that it produceshas never been leveraged to improve its effectiveness. That is,even if a particular app has already been crawled by GoogleMonkey thousands of times before, when Google Monkey isused to crawl that app, it still crawls randomly and makes allof the mistakes that it has already made thousands of timesbefore. The collection method described here is an attempt to
share these experiences by training a model and exploitingsuch model to improve the effectiveness of testing, as wediscuss next.
B. Model
While, as discussed above, the problem is to classify the
validity of a single action a
twhen performed on st, it does
not mean that each datapoint (st,at,rt)cannot be informative
about actions other than at. For instance, if touching a point
results in a valid action, touching its adjacent points may
also result in a valid action with a high probability. This can
2For instance, if an accumulative progress bar is being shown, this method
may not work.
907Fig. 2: Overview of the components comprising Deep GUI & Monkey++.
make our training process much faster and more data-efﬁcient.
Therefore, we need a model that can capture such logic.
1) Input and Output: As the ﬁrst step toward this goal, in
our model, we deﬁne input and output as follows. Input is a3-channel image that represents s
t, the screenshot of the AUT
at timet. For output, we require our model to perform the
classiﬁcation task for all the actions of all types (i.e., touch,scroll, swipe, etc.), and not just a
t. While we do not directly
use the prediction for other actions to generate gradients whentraining, this enables us to (1) use a more intuitive model, and(2) use the model at inference time by choosing the action thatis most conﬁdently classiﬁed to be valid. We use a T-channel
heatmap to represent our output; Tbeing the number of action
types, i.e. touch, scroll, swipe. Note that we do not differentiatebetween up/down scroll or left/right swipe at this stage. Eachchannel is a heatmap for the action type it represents. Foreach action type, the value at (i,j)of the heatmap associated
with that action type represents the probability that the modelassigns to the validity of performing that action type at location(i,j). For instance, in Figure 1, the three heatmaps 1c, 1d, and
1e show the model’s conﬁdence in performing touch, scroll,and swipe, respectively, at different locations on the screen.
2) UNet: We also would need a model that can intuitively
relate the input and output, as deﬁned above. We use a UNet
architecture, since it has shown to be effective in applicationssuch as image segmentation, where the output is an alteredversion of the input image [27]. In this architecture, the inputimage is ﬁrst processed in a sequence of convolutional layers
known as the contracting path. Each of these layers reduces
the dimensionality of the data while potentially encodingdifferent parts of the information relevant to the task at hand.The contracting path is followed by the expansive path, where
various pieces of information at different layers are combinedusing transposed convolutional layers
3to expand the dimen-
sionality to the suitable format required by the problem. In ourcase, the output would be a 3-channel heatmap. In order forthis heatmap to produce values between 0 and 1 (as explainedabove), it is processed by a sigmoid function in the last step
of the model. As one can notice, because of the nature ofconvolutional and transposed convolutional layers, adjacentcoordinate pairs are processed more similarly than other pairs.
3In some references these are referred to as deconvolutional layers.This makes it easier for the network to make deductionsabout all actions, and not just a
t. Moreover, the entire model
seems to have an intuitive design: First, the relevant parts ofinformation are extracted and grouped in different layers, andthen combined to form the output. This is similar to how theUI elements are usually represented in software applicationsas a GUI tree.
3) Transfer Learning: While Google Monkey might strug-
gle in ﬁnding valid actions when crawling an app, and othertools might need to use other information such as GUI treeor source code to detect such actions, humans ﬁnd the logicbehind a valid action to be pretty intuitive, and can learn itwithin minutes of encountering a new environment. The reasonbehind this “intuition” lies in the much more elaborate visualexperience that humans have that goes beyond the Androidenvironments. Since birth, we see a myriad of objects ina myriad of contexts, and we learn to distinguish objectsfrom their backgrounds. This information helps us a lot todistinguish a button in the background of an app, even if thebackground itself is a complicated image. Because of this, wehumans do not need thousands of examples to learn to interactwith an environment.
How can we use this fact to get the same training perfor-
mance with fewer data in our tool? Research in machine learn-ing has shown the possibility of achieving this through transfer
learning [29]. In transfer learning, instead of a randomly
initialized network, an existing model previously trained on adataset for a potentially different but related problem is usedas the starting point of all or some part of the network. Thisway, we “transfer” all the experience related to that dataset (assummarized in the trained weights), without having investedtime to actually process it. Therefore, training is more data-efﬁcient. This is in particular important for us because, asdiscussed, the data collection process is very time-consuminggiven that the tool needs to monitor the screen for animationsbefore collecting each datapoint.
The contracting path of the UNet seems like a perfect
candidate for transfer learning because, unlike the expansivepath, it is more related to how the network processes the input,rather than how it produces the output. This means that anytrained model that exists for processing an image can be acandidate for us to use its weights.
908Fig. 3: The deep neural network architecture used in Deep GUI. The layers’ names shown in MobileNetV2 are from Tensorﬂow
[28] implementation of the architecture. ConvT is a transpose convolutional layer.
In this work, as the contracting path, we used part of
the network architecture MobileNetV2 [30] trained on theImageNet dataset [31].
4We chose MobileNetV2 because it
is powerful and yet lightweight enough to be used insidemobile phones if necessary. Figure 3 shows how MobileNetV2interacts with our expansive path to build the model usedin Deep GUI. Note that in order for the screenshot to becompatible with the already trained MobileNetV2 model, weﬁrst resize it to 224×224. Also, because of computational
reasons, the produced output is 56×56, and is later upsampled
linearly to the true screen size.
4) Training: At the training time, for each datapoint
(s
t,at,rt), the network ﬁrst produces Q(st)as the described
heatmaps. Then, using the information about the performedactiona
t, it indexes the network’s prediction for the action to
getQ(st)(at)=Q(st,at). Finally, since this is a classiﬁcation
task, we use a binary crossentropy loss between rtand
Q(st,at)to generate gradients and train the network.
C. Inference
Once we have the trained model, we would like to be able
to use it to pick an action given a screenshot of an app at aspeciﬁc state. Therefore, we require a readout function thatcan sample an action from the produced heatmaps. Here, wepropose two readouts, and we explain how we use both inDeep GUI.
The simplest possible readout is one that samples actions
based on their relative prediction. That is, the more probablethe network thinks it is for the action to be a valid one, themore probable it is for the action to be sampled. For this tohappen, we need to normalize the heatmaps to a probabilitydistribution over all actions of all types. Formally:
p(a
t=α|st)=f(Q(st,α))/summationtext
α/prime∈Af(Q(st,α/prime))
wherefidentiﬁes the kernel function. For instance if f(x)=
exp(x) , we have a softmax normalization. In our work, we
4Please note that we used this existing trained model as the initialization
of the contracting path. In the training step, we do train the weights on the
contracting path.
(a)
 (b)
Fig. 4: (a) An example of a screen with equally important
widgets of different sizes. (b) The touch channel of theproduced heatmap. The pixels belonging to different clustersthat the cluster_sampling readout detects are colored
with maroon, red, and white, depending on the cluster theybelong to.
chose to use the linear kernel f(x)=x. Using the probability
distribution that the linear kernel produces, we then sample
an action. We call this method the weighted_sampling
readout.
However, humans usually interact with apps differently. We
see widgets rather than pixels, and interact with those widgetsas a whole. The weighted_sampling readout does not
take this into account as it treats each pixel independently.Take Figure 4a as an example. The “Enable delivery reports”checkbox is potentially as important as the send button,because if it is checked a new functionality can be tested.However, because the button is larger than the checkbox, ittakes the weighted_sampling readout longer to ﬁnally
toggle the checkbox and test the new functionality.
To address this issue, we use the cluster_sampling
readout. In this approach, we ﬁrst ﬁlter out all the actions αfor
909which the predicted Q(st,α)is less than a certain threshold.
This way, we ensure only the actions that are highly probable
to be valid are considered. In Deep GUI this threshold is 0.99.Then, for each channel in Q(s
t), we use agglomerative cluster-
ing as implemented in python library scikit-learn [32] to
cluster the pixels into widgets. Figure 4b shows the clusteringresult for the touch channel of the heatmap correspondingto Figure 4a. After detecting the clusters, we ﬁrst randomlychoose one of the action types, and then randomly choose oneof the clusters (i.e. widgets) in the channel associated with thataction type. Finally, we choose a random pixel that belongsto that cluster and generate a
t.
While conﬁgurable, in our experiments we used a hy-
brid readout that uses weighted_sampling in 30% of
the times, and cluster_sampling in 70% of the times.
This way, we exploit the beneﬁts that cluster_sampling
offers, while we make sure we do not completely abandoncertain valid actions because of the imperfections of the tool.
The discussed readouts identify the action type and the
location of it on the screen. However, scroll and swipe alsorequire other parameters such as direction or length. Deep GUIchooses these parameters randomly. Also, because swipe andscroll are mostly used to discover other buttons, while touch isactually the action that triggers the functionality of the buttons,we conﬁgure the described readouts so that they are morebiased towards choosing the touch action.
5
D. Monkey++
While touch, swipe, and scroll are the most used action
types when interacting with an environment, there are otheractions that may affect the ability of a tool to crawl Androidapps. In order to cover those actions as well, and also in orderto be able to compare Google Monkey with our solution fairlyin the Android environment, we introduce Monkey++, whichis an extension to Google Monkey. Monkey++ consists of aserver side, which responds to queries with Deep GUI, and aclient side, which is implemented inside Google Monkey.
Google Monkey works as follows. First, it randomly
chooses an action type (based on the probabilities providedto it when starting it), and then randomly chooses the param-eters (such as the location to touch). Monkey++ works thesame as Google Monkey with one exception. If the chosenaction type is touch or gesture (which represents all types ofmovement, including scroll and swipe), instead of proceedingwith the standard random procedure in Google Monkey, itsends a query to the server side. Using the inference proceduredescribed above, Deep GUI samples an action and returns tothe client, which is then performed on the device. Algorithm1 shows how Monkey++ works.
III. E
V ALUA TION
We evaluated Deep GUI with respect to the following
research questions:
5Inweighted_sampling, we multiply each heatmap belong-
ing to touch, scroll, and swipe with 1, 0.3, and 0.1 respectively. In
cluster_sampling, when randomly choosing an action type from theavailable ones, we use the same three numbers to bias the probability.Algorithm 1: Monkey++ algorithm
while Google Monkey is running do
get action type tfrom Google Monkey;
iftis touch or gesture then
get action afrom Deep GUI server
else
continue with Google Monkey and get action a
end
performa
end
RQ1. How does Monkey++ compare to Google Monkey?RQ2. Can Deep GUI be used to generate effective test inputs
across platforms?
RQ3. How much is transfer learning helping Deep GUI in
learning better and faster?
We used the apps in the Androtest benchmark [25] as our
pool of apps. Out of 66 apps available
6, we randomly chose 28
for training, 6 for validation, and 31 for testing purposes. Wealso eliminated one of the apps because of its incompatibilitywith our data collection procedure.
7
To support a variety of screen sizes, we collected data from
virtual devices of size 240×320 and also 480×854, and trained
a single model that is used in the experiments explained inSections RQ1 and RQ2. We collected an overall amount of210, 000 data points. Virtual devices, both for data collection
and the Android experiments, were equipped with a 200MB
virtual SD card, as well as 4GB of RAM. For data collection,
training, and the experiments, we used an Ubuntu 18.04 L TSworkstation with 24 Intel Xenon CPUs and 150GB RAM.
We did not use GPUs at any stage of this work. The entiresource code for this work, the experiments, and the analysisis available at https://github.com/Feri73/deep-gui.
RQ1. Line Coverage
In order to test the ability of Monkey++ in exploring
Android apps, we ran both Monkey++ and Google Monkey
on each app in the test set for one hour, and monitored linecoverage of the AUT every 60 seconds using Emma [33]. Weran 9 instances of this experiment in parallel, and calculatedthe average across different executions of each tool. TableI shows the ﬁnal line coverage for the apps in the test set.While in some apps Monkey++ and Google Monkey performsimilarly, in other apps, such as com.kvance.Nectroid,
Monkey++ signiﬁcantly outperforms Google Monkey. Webelieve this is directly related to an attribute of apps, referredto as Crawling Complexity (CC) in this paper.
CC is a measure of the complexity of exploring an app.
Different factors can affect this value. For instance, if themajority of the app’s code is executed at the startup, there
6Three apps caused crashes in the emulators and hence were not used.
7Application org.jtb.alogcat keeps updating the screen with new
logs from the logcat regardless of the interactions with it, which highly
deviates from the behavior of a normal Android app.
910T ABLE I: The results of running Monkey++ and Google Monkey on the test set, sorted by Crawling Complexity. The shading
indicates the tool that achieved the best result.
Application Crawling Complexity Monkey++ Line Coverage G Monkey Line Coverage
es.senselesssolutions.gpl.weightchart 2.8 67% 65%
com.hectorone.multismssender 2.6 64% 67%
com.templaro.opsiz.aka 2.4 72% 66%
com.kvance.Nectroid 2.3 65% 50%
com.tum.yahtzee 2.3 67% 61%
in.shick.lockpatterngenerator 2.2 86% 84%
net.jaqpot.netcounter 2.2 71% 69%
org.waxworlds.edam.importcontacts 2.0 41% 34%
cri.sanity 1.8 25% 23%
com.chmod0.manpages 1.7 72% 63%
com.google.android.divideandconquer 1.5 85% 88%
com.example.android.musicplayer 1.3 71% 71%
ch.blinkenlights.battery 1.3 91% 93%
org.smerty.zooborns 1.2 34% 33%
com.android.spritemethodtest 1.2 71% 87%
com.android.keepass 1.1 7% 8%
org.dnaq.dialer2 1.0 39% 39%
hu.vsza.adsdroid 1.0 24% 24%
com.example.anycut 0.9 71% 71%
org.scoutant.blokish 0.9 45% 46%
org.beide.bomber 0.8 89% 88%
com.beust.android.translate 0.7 48% 48%
com.addi 0.6 18% 18%
org.wordpress.android 0.5 5% 5%
com.example.amazed 0.3 82% 81%
net.everythingandroid.timer 0.2 65% 65%
com.google.android.opengles.spritetext 0.1 59% 59%
aarddict.android 0.0 14% 14%
com.angrydoughnuts.android.alarmclock 0.0 6% 6%
com.everysoft.autoanswer 0.0 9% 9%
hiof.enigma.android.soundboard 0.0 100% 100%
com.tum.yahtzee: This is a dice game with fairly complicated logic and several buttons, each activating different scenarios over time.
org.waxworlds.edam.importcontacts: This app imports contacts from the SD card. There are multiple steps to reach to the ﬁnal
activity, and each contains multiple options that change the course of actions that the app ﬁnally takes.
hu.vsza.adsdroid: The only functionality of this app is to search for and list the data-sheets of electronic items. The search activity
contains one drop-down list for search criteria, and a search button.
org.wordpress.android: This app is for management of WordPress websites. At the startup, it either requires a login or opens a web
container, which does not affect the line coverage.
is not much code left to be explored. As another example,consider apps that require signing in to an account to accesstheir functionality. Unless it is explicitly supported by the tools(which is not in this study), not much can be explored withinthe app.
We hypothesize that Monkey++ outperforms Google Mon-
key in apps with high CC. In order to test this, we deﬁne CC asthe uncertainty in coverage when randomly interacting with anapp. That is, if random interactions with an app always resultin a similar trace of coverage, it means that the available partsof the app are trivial to reach and will always be executed,and therefore, not much is offered by the app to be explored.To compute uncertainty (and hence CC) for an app, we usethe concept of entropy.
The entropy of a random variable is a measure of the uncer-
tainty of the values that this variable can get. For instance, if arandom variable only gets one value (i.e., it is not random), theentropy would be zero. On the other hand, a random variablethat samples its values from a uniform distribution has a largeentropy, because it is more difﬁcult to predict its exact value.The formula for calculating entropy Hof a discrete random
variableXis as follows:
H(X)=−
n/summationdisplay
i=1p(xi)l o g2(p(xi))
wherexirepresents the values that Xcan get, and p(xi)is the
probability distribution for X. To calculate CC of an app using
entropy, we take all line coverage information for that app inall timesteps of all experiments involving Google Monkey (asa random interaction tool), and calculate the entropy of thedistribution of these coverage values using the above formula.The coverage values for two apps with low and high CC areshown in Figure 5.
Table I shows the CC value for each app, and discusses
some examples of apps with high and low CC, including theexamples in Figure 5. As one can notice, most of the apps inwhich Monkey++ achieves better coverage have higher CC.
911(a)
(b)
(c)
(d)
Fig. 5: The results of exploring two apps randomly in 9 independent runs: (a) An example of an app with low CC
(hu.vsza.adsdroid). (b) We obtain only 3 distinct coverage values for the entire 60 minutes of randomly testing the
adsdroid app across all 9 agents. This means the portion of the app that is accessible to be explored is very limited. (c) An
example of an app with high CC (com.tum.yahtzee). (d) Here, the coverage values that we obtain by randomly exploringtheyahtzee app span a much more uncertain space than the adsdroid app, which means more is offered by the app to
be explored and therefore it is more meaningful to compare the testing tools on this app.
To further evaluate the ability of Monkey++ in crawling
complex apps with high CC, we analyzed the progressivecoverage of the top 10 apps with the highest CC. Figure6 shows that Monkey++ achieves better results comparedto Google Monkey, and does so faster. This superiority isstatistically signiﬁcant in all timesteps, as calculated by a one-tail Kolmogorov–Smirnov (KS) test (p-value <0.05).
8
The improvement over Google Monkey is valuable, since
it is currently the most widely used testing tool that does notrequire the AUT to implement any speciﬁc API. For instance,most of the mainstream white-box testing tools fail on non-native applications, because these applications are essentially
8To calculate the error bars in Figure 6 and the p-value for KS-test, ﬁrst for
each app, the mean performance of Google Monkey on that app is subtracted
from the performance of both Google Monkey and Monkey++, and then theerror bars and the signiﬁcance are computed with regards to this value acrossall apps.web content wrapped in an android web viewer, and lack
standard UI elements that white-box tools depend on. Inthese scenarios, practitioners are bound to use random testingtools such as Google Monkey. Monkey++ provides a moreintelligent alternative in these situations that, as the resultssuggest, provide better coverage faster.
RQ2. Cross-Platform Ability
Since Deep GUI is completely blind with regards to the
app’s implementation or the platform it runs on, we hypothe-
size it is applicable not only in Android but in other platformssuch as web or iOS. Moreover, we claim that since UI designacross different platforms is very similar (e.g. buttons are verysimilar in Android and web), we can take a model trained onone platform and use it in other platforms. This is particularlyuseful when developers want to test different implementationsof the same app in different platforms.
912Fig. 6: The progressive line coverage of Monkey++ and
Google Monkey on the top 10 Android apps with the highestCC. Error bars indicate standard error of the mean.
Fig. 7: The progressive performance of Deep GUI and randomagent in web crawling. The difference between the three toolsis statistically signiﬁcant in all timesteps, as calculated byone tail KS-tests between all pairs (similar to the proceduredescribed in footnote 8).
To test whether our approach is truly cross-platform, we
implemented an interface to use Deep GUI for interacting withMozilla Firefox browser
9using Selenium web driver [34], and
compared it against a random agent10. Note that we did not
re-train our model, and used the exact same hyper-parametersand weights we used for the experiments in RQ1, which arelearned from Android apps.
For the web experiments, we used the top 15 websites in
the United States [26] as our test set, and ran each tool oneach website 20 times, each time for 600 steps. To measurethe performance, we counted the number of distinct URLsvisited in each website, and averaged this value for each tool.
9We used Responsive Design Mode in Mozilla Firefox with the resolution
of480×640.
10The random agent uses the same bias for action types that is explained
in footnote 5 of Section II.T ABLE II: The performance of Deep GUI and random agenton each web site
Website Deep GUI Random
google.com 17.4 12.9
youtube.com 94.3 12.1
amazon.com 13.2 15.2
yahoo.com 15.4 21.8
facebook.com 3.2 7.1
reddit.com 5.3 5.1
zoom.us 4.6 6.9
wikipedia.org 41.1 40.6
myshopify.com 3.6 6.0
ebay.com 13.4 11.4
netflix.com 5.1 4.8
bing.com 32.5 25.5
office.com 16.9 15
live.com 2.7 2.5
twitch.tv 65.6 30.1
average 22.2 14.4
(a)
 (b)
Fig. 8: A screenshot and its corresponding heatmap generatedby the model before training.
Figure 7 and table II show that our model outperforms random
agent, and conﬁrms that our model has learned the rules ofUI design, which is indeed independent of the platform.
The results of the web experiment demonstrate the power of
a black-box technique capable of understanding the dynamicsof GUI-based applications without relying on any sort ofplatform-dependent information. Such techniques infer gener-alized rules about GUI-based environments instead of relyingon speciﬁc APIs or implementation-choices in the constructionof an application, and hence enable users to apply the tools ondifferent applications and on different platforms without beingconstrained by the compatibility issues.
RQ3. Transfer Learning Effect
As described, we used transfer learning to make the training
process more data-efﬁcient, i.e. we crawl fewer data and train
faster. To study if using transfer learning was actually helpful,we repeated the web experiments, with the only difference thatinstead of using the model trained with transfer learning, wetrained another model with random initial weights. Figure 7
913shows that without transfer learning, the model’s performance
signiﬁcantly decreases.
To gain an intuitive understanding of the reason behind
this, consider Figure 8b. This ﬁgure shows the initial outputof the neural network for the screen of Figure 8a beforetraining, when initialized with the ImageNet weights. As onecan see, even without training, the buttons stand out fromthe background in the heatmap, which gives the model asigniﬁcant head-start compared to the randomly initializedmodel, and makes it possible for us to train it with a smallamount of data.
IV . R
ELA TED WORK
Many different input generation techniques with different
paradigms have been proposed in the past decade. Severaltechniques [35], [36] rely on a model of the GUI, usuallyconstructed dynamically and non-systematically, leading tounexplored program states. Sapienz [15], EvoDroid [16], andtime-travel testing [37] employ an evolutionary algorithm.ACTEve [38], and Collider [39] utilize symbolic execution.AppFlow [40] leverages machine learning to automaticallyrecognize common screens and widgets and generate testsaccordingly. Dynodroid [23] and Monkey [22] generate testinputs using random input values. Another group of techniquesfocus on testing for speciﬁc defects [20], [41], [42].
These approaches can be classiﬁed into two broad cate-
gories: context blind and context aware. The tools in the for-
mer category process information in each action independentof other actions. That is, when choosing a new action, theydo not consider the previous actions performed, and do notplan for future actions. Tools such as Google Monkey [22]and DynoDroid [23] are in this category. These tools are fastand require very simple pre-processing, but may miss entireactivities or functionalities, as this requires maintaining amodel of the app and visited states. Tools in the latter categoryincorporate various sources of information to construct amodel of an app, which is then used to plan for context-aware input generation. Most of the existing input generationtools are in this category. For instance, Sapienz [15] uses agenetic algorithm to learn a generic model of app, representinghow certain sequences of actions can be more effective thanothers. Tools that use different types of static analysis of thesource code or GUI to model the information ﬂow globallyalso belong to this category.
Not many tools have explored black-box and/or cross-
platform options for gathering information to be used forinput generation, either with a context-aware or a context-blind approach. Google Monkey is the only widely usedtool in Android that does not depend on any app-speciﬁcinformation. However, it follows the simplest form of testing,i.e., random testing. Humanoid [43] is an effort towardsbecoming less platform-dependent, while also generating moreintelligent inputs. However, it is still largely dependent onthe UI transition graph of AUT and the GUI tree extractedfrom the operating system. Moreover, since it depends on anexisting dataset for Android, it would not be easy to train itfor a new platform. The study of White et al. [44] is the mostsimilar to our work. They study the effect of machine-learning-powered processing of screenshots in generating inputs withrandom strategy. However, because they generate artiﬁcialapps for training their model, their data collection method islimited in expressing the variety of screens that the tool mightencounter. Furthermore, their approach is platform dependent.
Deep GUI uses deep learning to improve context-blind input
generation, while also limiting the processed information to beblack-box and platform independent. This allows it to be asversatile as Google Monkey in the Android platform, whilebeing more effective by intelligently generating the inputs forcrawling of apps.
V. D
ISCUSSION AND FUTURE WORK
Deep GUI is the ﬁrst attempt towards making a fully black-
box and cross-platform test input generation tool. However,there are multiple areas in which this tool can be improved.The ﬁrst limitation of the approach described here is the time-consuming nature of its data collection process, which limitsthe number of collected data points and may compromise thedataset’s expressiveness. By using transfer learning, we man-aged to mitigate this limitation to some degree. In addition,the complex set of hyperparameters (such as hybrid readoutprobabilities) and the time-consuming nature of validating themodel on apps make it difﬁcult to ﬁne-tune all the hyperpa-rameters systematically, which is required for optimizing theperformance to its maximum potential.
Deep GUI limits itself to context-blind information pro-
cessing, in that it does not consider the previous interactionswith AUT when generating new actions. However, it usesa paradigm that can easily be extended to take context intoaccount as well. We believe this paradigm should be exploredmore in the future of the ﬁeld of automated input generation.
Take our deﬁnition of the problem. If we call s
tthe state of
the environment, atthe action performed on the environment
in that state, rtthe reward that the environment provides
in response to that action, and Q(st,at)the predictions of
the model about the long-term reward that the environmentprovides when performing a
tinst(also known as the quality
matrix), then this work can essentially be viewed to proposea single-step deep Q-Learning [45] solution to the problemof test input generation. Looking at the problem this wayenables researchers in the area of automatic input generation tobeneﬁt from the rich and active research in the Q-Learning andreinforcement learning (RL) community, and explore differentdirections in the future such as the following:
•Multi-Step Cross-Platform Input Generation. Deep GUI
uses Q-Learning in a context-blind and single-step manner.However, by redeﬁning s
tto include more context (such
as previous screenshots, as tried in Humanoid [43]) andexpanding the deﬁnition of r
tto express a multi-step sense
of reward, one can use the same idea to utilize the fullpower of Q-Learning to train models that not only limit theiractions to only the valid ones (as this tool does), but also
914plan ahead and perform complex and meaningful sequence
of actions.
•Smarter Processing of Information. Even if a tool does
not want to limit itself to only platform-independent in-formation, it can still beneﬁt from using a Q-Learningsolution. For instance, one can deﬁne s
tto include the
GUI tree or the memory content to provide the model withmore information, but also use Q-Learning to process thisinformation more intelligently.
•Regression Testing and Test Transfer . While this work
presents a trained model that targets all apps, it is not limitedto this. Developers can take a Q-Learning model such as theone described in this work, collect data from the app (or afamily of related apps) they are developing, and train themodel extensively so that it learns what actions are valid,what sequences of actions are more probable to test animportant functionality, etc. This way, when new updatesof the app are available, or when the app becomes availablein new platforms, developers can quickly test for any faultin that update without having to rewrite the tests.
A
CKNOWLEDGMENT
This work was supported in part by award numbers 2106306
and 1823262 from the National Science Foundation and aGoogle Cloud Platform gift. We would like to thank theanonymous reviewers of this paper for their detailed feedback,which helped us improve the work.
R
EFERENCES
[1] T. Azim and I. Neamtiu, “Targeted and depth-ﬁrst exploration for
systematic testing of android apps,” in Proceedings of the 2013 ACM
SIGPLAN International Conference on Object Oriented Programming
Systems Languages & Applications, OOPSLA ’13, (New Y ork, NY ,USA), p. 641–660, Association for Computing Machinery, 2013.
[2] R. Bhoraskar, S. Han, J. Jeon, T. Azim, S. Chen, J. Jung, S. Nath,
R. Wang, and D. Wetherall, “Brahmastra: Driving apps to test thesecurity of third-party components,” in Proceedings of the 23rd USENIX
Conference on Security Symposium, SEC’14, (USA), p. 1021–1036,USENIX Association, 2014.
[3] M. Linares-V ´asquez, M. White, C. Bernal-C ´ardenas, K. Moran, and
D. Poshyvanyk, “Mining android app usages for generating actionablegui-based execution scenarios,” in Proceedings of the 12th Working
Conference on Mining Software Repositories , MSR ’15, p. 111–122,
IEEE Press, 2015.
[4] S. Y ang, H. Wu, H. Zhang, Y . Wang, C. Swaminathan, D. Y an, and
A. Rountev, “Static window transition graphs for android,” Automated
Software Engg., vol. 25, p. 833–873, Dec. 2018.
[5] D. Amalﬁtano, A. R. Fasolino, and P . Tramontana, “A gui crawling-
based technique for android mobile application testing,” in 2011 IEEE
F ourth International Conference on Software Testing, V eriﬁcation andV alidation Workshops, pp. 252–261, 2011.
[6] D. Amalﬁtano, A. R. Fasolino, P . Tramontana, S. De Carmine, and
A. M. Memon, “Using gui ripping for automated testing of androidapplications,” in 2012 Proceedings of the 27th IEEE/ACM International
Conference on Automated Software Engineering, pp. 258–261, 2012.
[7] Y . Baek and D. Bae, “Automated model-based android gui testing
using multi-level gui comparison criteria,” in 2016 31st IEEE/ACM
International Conference on Automated Software Engineering (ASE),pp. 238–249, 2016.
[8] N. P . Borges, M. G ´omez, and A. Zeller, “Guiding app testing with mined
interaction models,” in Proceedings of the 5th International Conference
on Mobile Software Engineering and Systems, MOBILESoft ’18, (NewY ork, NY , USA), p. 133–143, Association for Computing Machinery,2018.[9] W . Choi, G. Necula, and K. Sen, “Guided gui testing of android apps
with minimal restart and approximate learning,” SIGPLAN Not., vol. 48,
p. 623–640, Oct. 2013.
[10] S. Hao, B. Liu, S. Nath, W . G. Halfond, and R. Govindan, “Puma:
Programmable ui-automation for large-scale dynamic analysis of mobileapps,” in Proceedings of the 12th Annual International Conference on
Mobile Systems, Applications, and Services, MobiSys ’14, (New Y ork,NY , USA), p. 204–217, Association for Computing Machinery, 2014.
[11] K. Jamrozik and A. Zeller, “Droidmate: A robust and extensible test
generator for android,” in 2016 IEEE/ACM International Conference on
Mobile Software Engineering and Systems (MOBILESoft), pp. 293–294,2016.
[12] L. Mariani, M. Pezze, O. Riganelli, and M. Santoro, “Autoblacktest:
Automatic black-box testing of interactive applications,” in 2012 IEEE
Fifth International Conference on Software Testing, V eriﬁcation andV alidation, pp. 81–90, 2012.
[13] T. Su, G. Meng, Y . Chen, K. Wu, W . Y ang, Y . Y ao, G. Pu, Y . Liu, and
Z. Su, “Guided, stochastic model-based gui testing of android apps,” inProceedings of the 2017 11th Joint Meeting on F oundations of SoftwareEngineering, ESEC/FSE 2017, (New Y ork, NY , USA), p. 245–256,Association for Computing Machinery, 2017.
[14] Y uanchun Li, Ziyue Y ang, Y ao Guo, and Xiangqun Chen, “Droidbot:
a lightweight ui-guided test input generator for android,” in 2017
IEEE/ACM 39th International Conference on Software EngineeringCompanion (ICSE-C), pp. 23–26, 2017.
[15] K. Mao, M. Harman, and Y . Jia, “Sapienz: Multi-objective automated
testing for android applications,” in Proceedings of the 25th International
Symposium on Software Testing and Analysis, ISST A 2016, (New Y ork,NY , USA), p. 94–105, Association for Computing Machinery, 2016.
[16] R. Mahmood, N. Mirzaei, and S. Malek, “Evodroid: Segmented evo-
lutionary testing of android apps,” in Proceedings of the 22nd ACM
SIGSOFT International Symposium on F oundations of Software Engi-
neering, pp. 599–609, 2014.
[17] J. Garcia, M. Hammad, N. Ghorbani, and S. Malek, “Automatic gen-
eration of inter-component communication exploits for android applica-
tions,” in Proceedings of the 2017 11th Joint Meeting on F oundations
of Software Engineering, ESEC/FSE 2017, (New Y ork, NY , USA),p. 661–671, Association for Computing Machinery, 2017.
[18] C. Cao, N. Gao, P . Liu, and J. Xiang, “Towards analyzing the input
validation vulnerabilities associated with android system services,” inProceedings of the 31st Annual Computer Security Applications Con-ference, ACSAC 2015, (New Y ork, NY , USA), p. 361–370, Associationfor Computing Machinery, 2015.
[19] Y . Liu, C. Xu, S. Cheung, and J. L ¨u, “Greendroid: Automated diagnosis
of energy inefﬁciency for smartphone applications,” IEEE Transactions
on Software Engineering, vol. 40, no. 9, pp. 911–940, 2014.
[20] R. Jabbarvand, J.-W . Lin, and S. Malek, “Search-based energy testing of
android,” in 2019 IEEE/ACM 41st International Conference on Software
Engineering (ICSE), pp. 1119–1130, 2019.
[21] A. Alshayban, I. Ahmed, and S. Malek, “Accessibility issues in android
apps: State of affairs, sentiments, and ways forward,” in Proceedings of
the ACM/IEEE 42nd International Conference on Software Engineering,ICSE ’20, (New Y ork, NY , USA), p. 1323–1334, Association forComputing Machinery, 2020.
[22] “Ui/application exerciser monkey.” https://developer.android.com/studio/
test/monkey, 2020.
[23] A. Machiry, R. Tahiliani, and M. Naik, “Dynodroid: An input generation
system for android apps,” in Proceedings of the 2013 9th Joint Meeting
on F oundations of Software Engineering, ESEC/FSE 2013, (New Y ork,NY , USA), p. 224–234, Association for Computing Machinery, 2013.
[24] K. Mao, M. Harman, and Y . Jia, “Crowd intelligence enhances auto-
mated mobile testing,” in Proceedings of the 32nd IEEE/ACM Inter-
national Conference on Automated Software Engineering, ASE 2017,p. 16–26, IEEE Press, 2017.
[25] S. R. Choudhary, A. Gorla, and A. Orso, “Automated test input gen-
eration for android: Are we there yet? (e),” in 2015 30th IEEE/ACM
International Conference on Automated Software Engineering (ASE),pp. 429–440, 2015.
[26] A. Internet, “Top sites in united states,” 2020.
[27] O. Ronneberger, P . Fischer, and T. Brox, “U-net: Convolutional networks
for biomedical image segmentation,” in Medical Image Computing and
Computer-Assisted Intervention – MICCAI 2015 (N. Navab, J. Horneg-
ger, W . M. Wells, and A. F. Frangi, eds.), (Cham), pp. 234–241, Springer
International Publishing, 2015.
915[28] M. Abadi, P . Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,
S. Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga,
S. Moore, D. G. Murray, B. Steiner, P . Tucker, V . V asudevan, P . Warden,M. Wicke, Y . Y u, and X. Zheng, “Tensorﬂow: A system for large-scale machine learning,” in Proceedings of the 12th USENIX Conference
on Operating Systems Design and Implementation, OSDI’16, (USA),p. 265–283, USENIX Association, 2016.
[29] S. J. Pan and Q. Y ang, “A survey on transfer learning,” IEEE Transac-
tions on Knowledge and Data Engineering, vol. 22, no. 10, pp. 1345–1359, 2010.
[30] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L. Chen, “Mo-
bilenetv2: Inverted residuals and linear bottlenecks,” in 2018 IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 4510–4520, 2018.
[31] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, andL. Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,”International Journal of Computer Vision (IJCV), vol. 115, no. 3,pp. 211–252, 2015.
[32] F. Pedregosa, G. V aroquaux, A. Gramfort, V . Michel, B. Thirion,
O. Grisel, M. Blondel, P . Prettenhofer, R. Weiss, V . Dubourg, J. V ander-plas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duch-esnay, “Scikit-learn: Machine learning in Python,” Journal of Machine
Learning Research, vol. 12, pp. 2825–2830, 2011.
[33] V . Roubtsov, “Emma: a free java code coverage tool,” 2006.[34] Selenium, “The selenium browser automation project.” https://www.
selenium.dev/.
[35] T. Su, G. Meng, Y . Chen, K. Wu, W . Y ang, Y . Y ao, G. Pu, Y . Liu, and
Z. Su, “Guided, stochastic model-based gui testing of android apps,” inProceedings of the 2017 11th Joint Meeting on F oundations of SoftwareEngineering, pp. 245–256, 2017.
[36] T. Gu, C. Sun, X. Ma, C. Cao, C. Xu, Y . Y ao, Q. Zhang, J. Lu, and
Z. Su, “Practical gui testing of android applications via model abstractionand reﬁnement,” in 2019 IEEE/ACM 41st International Conference on
Software Engineering (ICSE), pp. 269–280, IEEE, 2019.
[37] Z. Dong, M. B ¨ohme, L. Cojocaru, and A. Roychoudhury, “Time-travel
testing of android apps,” in Proceedings of the 42nd International
Conference on Software Engineering (ICSE’20), pp. 1–12, 2020.
[38] S. Anand, M. Naik, M. J. Harrold, and H. Y ang, “Automated concolic
testing of smartphone apps,” in Proceedings of the ACM SIGSOFT 20th
International Symposium on the F oundations of Software Engineering,pp. 1–11, 2012.
[39] C. S. Jensen, M. R. Prasad, and A. Møller, “Automated testing with
targeted event sequence generation,” in Proceedings of the 2013 Inter-
national Symposium on Software Testing and Analysis, pp. 67–77, 2013.
[40] G. Hu, L. Zhu, and J. Y ang, “Appﬂow: using machine learning to
synthesize robust, reusable ui tests,” in Proceedings of the 2018 26th
ACM Joint Meeting on European Software Engineering Conference andSymposium on the F oundations of Software Engineering , pp. 269–282,
2018.
[41] R. Hay, O. Tripp, and M. Pistoia, “Dynamic detection of inter-
application communication vulnerabilities in android,” in Proceedings
of the 2015 International Symposium on Software Testing and Analysis,pp. 118–128, 2015.
[42] L. L. Zhang, C.-J. M. Liang, Y . Liu, and E. Chen, “Systematically
testing background services of mobile apps,” in 2017 32nd IEEE/ACM
International Conference on Automated Software Engineering (ASE),pp. 4–15, IEEE, 2017.
[43] Y . Li, Z. Y ang, Y . Guo, and X. Chen, “Humanoid: A deep learning-
based approach to automated black-box android app testing,” in 2019
34th IEEE/ACM International Conference on Automated Software En-gineering (ASE), pp. 1070–1073, 2019.
[44] T. D. White, G. Fraser, and G. J. Brown, “Improving random gui testing
with image-based widget detection,” in Proceedings of the 28th ACM
SIGSOFT International Symposium on Software Testing and Analysis,ISST A 2019, (New Y ork, NY , USA), p. 307–317, Association forComputing Machinery, 2019.
[45] V . Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wier-
stra, and M. A. Riedmiller, “Playing atari with deep reinforcementlearning,” ArXiv, vol. abs/1312.5602, 2013.
916