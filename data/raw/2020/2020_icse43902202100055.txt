The Mind Is a Powerful Place:
How Showing Code Comprehensibility Metrics
Inﬂuences Code Understanding
Marvin Wyrich∗, Andreas Preikschat†, Daniel Graziotin∗and Stefan Wagner∗
Institute of Software Engineering, University of Stuttgart
Stuttgart, Germany
∗{ﬁrstname.lastname}@iste.uni-stuttgart.de,†andreaspreikschat@posteo.de
Abstract —Static code analysis tools and integrated devel-
opment environments present developers with quality-related
software metrics, some of which describe the understandability
of source code. Software metrics inﬂuence overarching strategic
decisions that impact the future of companies and the prioritiza-
tion of everyday software development tasks. Several software
metrics, however, lack in validation: we just choose to trust
that they reﬂect what they are supposed to measure. Some of
them were even shown to not measure the quality aspects they
intend to measure. Yet, they inﬂuence us through biases in our
cognitive-driven actions. In particular, they might anchor us in
our decisions. Whether the anchoring effect exists with software
metrics has not been studied yet. We conducted a randomized
and double-blind experiment to investigate the extent to which a
displayed metric value for source code comprehensibility anchors
developers in their subjective rating of source code comprehen-
sibility, whether performance is affected by the anchoring effect
when working on comprehension tasks, and which individual
characteristics might play a role in the anchoring effect. We
found that the displayed value of a comprehensibility metric has
a signiﬁcant and large anchoring effect on a developer’s code
comprehensibility rating. The effect does not seem to affect the
time or correctness when working on comprehension questions
related to the code snippets under study. Since the anchoring
effect is one of the most robust cognitive biases, and we have
limited understanding of the consequences of the demonstrated
manipulation of developers by non-validated metrics, we call
for an increased awareness of the responsibility in code quality
reporting and for corresponding tools to be based on scientiﬁc
evidence.
Index Terms—behavioral software engineering, code compre-
hension, placebo effect, cognitive bias, anchoring effect, metrics
I. I NTRODUCTION
Software developers spend more than 50% of their time
on activities related to program understanding [1], [2]. De-
velopment teams strive to make their code as understandable
as possible—refactoring source code to make it more under-
standable is a central part of agile software methodologies [3],
[4]—and base their activities on the results of static code
analysis tools to identify areas of code that are still difﬁcult
to understand.
Most of the metrics reported by such tools are either not
validated [5] to the point that some are empirically demon-
strated to not measure what they are assumed to measure [6].
The latter issue seems to be especially prevalent in the ﬁeld ofcode comprehensibility [6], [7]. In other words: several metrics
do not reﬂect what they are supposed to measure. Yet, they are
considered when making decisions and change course of what
developers think of their source code. What is being shown,
or told, to people inﬂuences what they think.
The mind is a powerful place / And what you feed it can
affect you in a powerful way
—NF, The Search (song). 2019.
Feeding the mind with a belief inﬂuences it and causes
changes that might go beyond the mind itself. Crum and
Langer [8] divided a sample of room attendants at different
hotels into two groups. To the ﬁrst one only, the researchers
presented the supposed positive effects of work-related phys-
ical activities on their health. After four weeks, the informed
group felt that they received signiﬁcantly more exercise than
the second group. Not just that: Weight, blood pressure,
and body fat of the informed group signiﬁcantly decreased
compared to the non-informed one—without any detected
change in workload, outside work physical activity, or eating
habits. If the consequence of a treatment is not attributed
to the treatment itself, but to pure beliefs and expectations
of its effectiveness, we call it the placebo effect [9]. This
effect occurs in many ways. Placebos are administered in
clinical trials in the form of sham drugs to distinguish the
pharmaceutical effect of a drug from the placebo effect. The
placebo effect can go beyond the subjective perception of an
affected person and provide measurable effects. The room
attendants were speciﬁcally manipulated by the researchers
and a desirable effect was achieved.
Various contextual factors can also inﬂuence our reasoning
and decision making, some of which make us deviate from
beneﬁcial results. Many of these factors are cognitive in
nature [10]. Software engineering is no exception. In a recent
systematic mapping study on cognitive biases in software
engineering, 65 articles were identiﬁed that provide evidence
for the presence of cognitive biases of at least eight different
categories [11]. Negative consequences of such biases are,
for example, overly optimistic effort estimates or insufﬁcient
software modiﬁcations.
A cognitive bias which is related to estimates and adjust-
5122021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)
1558-1225/21/$31.00 ©2021 IEEE
DOI 10.1109/ICSE43902.2021.00055
ments is the anchoring effect, introduced by Tversky and
Kahneman [12]. The anchoring effect means that an initial
value is insufﬁciently adjusted so that “different starting points
yield different estimates, which are biased toward the initial
values” [12]. The anchoring effect is one of the most robust
cognitive biases [13].
In our study, which we align with the ﬁeld of quantitative
behavioral software engineering [14], [15], we investigate the
anchoring effect in the context of source code comprehensi-
bility1and software metrics.
We conducted a randomized, double-blind experiment. Par-
ticipants were divided into two groups and were asked to
work on source code comprehension tasks over code snippets.
We showed the two groups a metric value that represents the
understandability of the code snippets. One group saw a value
that indicates an easy understandability of the source code. The
other group saw one that indicates a hard understandability of
the source code. Unbeknownst to the participants, tasks and
snippets were the very same for both groups. Also, the metric
is not real and placed there to anchor them.
We investigated the extent to which the shown metric
anchors our participants in their understandability. As common
in code comprehension experiments, we assessed the effect
through the subjective evaluation of the participants [7], [17].
We also aimed to ﬁnd out whether the manipulation leads
to a placebo effect of the form that we measure differences
in actual code comprehension performance, i.e., the time
spent and correctness in answering comprehension questions.
Finally, we explored an initial pool of individual characteristics
(such as experience, personality, and mood) that are envisioned
to play a role in the anchoring effect despite a very limited
availability of studies [13].
To these ends, we formulated three research questions,
which are further framed later in section II and in section III-E:
•RQ1: Does the value of a shown code comprehensibility
metric inﬂuence subjective ratings of code comprehensi-
bility?
•RQ2: Does the value of a shown code comprehensibility
metric inﬂuence the actual code understanding?
•RQ3: To which extent do selected individual character-
istics correlate with the deviation of the subjective rating
from the shown metric value?
Understanding the consequences of displaying a metric
value brings valuable practical and theoretical implications.
First, showing metric values without proper validation can lead
to the revision of already well understandable source code,
which would unnecessarily take time and may also introduce
new defects. Second, if the presence of a single metric value
signiﬁcantly inﬂuences a developer’s rating, this would be a
strong call to meticulously control program comprehension
experiments for potential confounding variables that could bias
a developer’s judgement or to advise against this measurement
method altogether.
1We use the terms comprehensibility and understandability interchangeably,
in line with some of our peers, e.g., [6], [16].II. B ACKGROUND
In this section we deﬁne the central constructs of this work,
namely the placebo effect, the anchoring effect and source
code comprehension, and place the work in the context of
related literature. Given the limited availability of related work
in the ﬁeld of software engineering, we include related work in
the background section, under each of the three topic-centered
subsections that follow.
A. Placebo Effect
Probably one of the most quoted deﬁnitions of placebo
comes from Shapiro [9], who studied the etymology and
semantics of the word to provide a basis for an appropriate
deﬁnition and to address the diversity of opinion about the
meaning of the term. The proposed deﬁnition is as follows:
Aplacebo is deﬁned as any therapy (or that compo-
nent of any therapy) that is deliberately used for its
nonspeciﬁc psychologic or psychophysiologic effect,
or that is used for its presumed speciﬁc effect on a
patient, symptom, or illness, but which unknown to
therapist and patient is without speciﬁc activity for
the condition being treated. [9, p. 682]
The placebo effect is deﬁned as “the nonspeciﬁc psycho-
logic or psychophysiologic effect produced by a placebo” [9].
The introduction of the term in medical literature was ac-
companied by “the widespread introduction of controlled
methodology in the evaluation of treatment” [9] and it became
standard to control for the placebo effect in clinical trials. In
this paper we follow Shapiro’s deﬁnition. We would like to
point out that the placebo effect, however, is not limited to
the medical context and can be applied to everyday aspects,
as numerous studies have shown.
In a study on placebo sleep [18], participants had to report
their previous night’s sleep quality. One group was then told
after a supposedly reliable measurement that their sleep quality
was above average and the other group was informed that their
sleep quality was below average. The assigned sleep quality,
but not the self-reported sleep quality, signiﬁcantly predicted,
among others, the auditory information processing speed of the
participants. The authors conclude that mindset can inﬂuence
cognitive performance both positively and negatively [18].
Other studies show, for example, that smelling a supposedly
creativity-enhancing odorant actually results in a creativity-
enhancing effect [19], that non-invasive sham brain stimulation
improves learning performance [20], and that different forms
of placebos have an effect on the performance of athletes [21].
Investigations of the placebo effect in software engineering
research have rarely been conducted so far. One recent study
deals with the inﬂuence of a three-minute breathing exercise
on the perceived effectiveness of stand-up meetings in agile
project teams [22]. A placebo group was added to compare the
effect with a non-meditative form of relaxation, i.e. listening
to classical music. They conclude that the breathing exercise
has an immediate positive impact on meetings in agile teams.
Another study [23] investigates how the subjective evaluation
513of an automatically generated solution is positively inﬂuenced
by involving the decision maker in the process but not con-
sidering their decisions at all. They conducted a placebo-
controlled study with 12 software engineering practitioners
and found an increase of 68% in the subjective evaluation of
an automatically generated but supposedly decision inﬂuenced
solution is due to a placebo effect. We are not aware of any
study investigating a potential placebo effect on performance
in code understanding activities.
B. Anchoring Effect
In our behaviors, we act within a speciﬁc context. Such
context provides us with cues, verbal suggestions, and social
information that inﬂuence our expectations, appraisals and
memories, which in turn inﬂuence our behavior and reported
experiences [24]. Consequently, parts of the placebo effect
on subjective assessments are attributed to various forms of
decision bias [24].
A systematic mapping study on cognitive biases in software
engineering highlights that the everyday life of a software
engineer is also full of situations in which their decisions are
subconsciously manipulated [11]. Mohanani et al. identiﬁed 65
articles in the context of software engineering that investigated
37 cognitive biases of at least eight different categories. In
the worst case, such bias leads to systematic deviations from
optimal reasoning, such as overly optimistic effort estimates
or insufﬁcient software modiﬁcations [11].
The speciﬁc cognitive bias that we investigate is called
anchoring effect, which we deﬁned in section I. According
to the aforementioned mapping study it is the most frequently
investigated cognitive bias in the context of software engi-
neering [11]. For example, one study used SQL queries as
an anchor for query formulation tasks. They found that while
subjects complete the tasks more quickly when modifying a
query instead of writing it from scratch, accuracy decreases
and overconﬁdence in the results increases [25]. Another
example where anchoring plays a role is planning poker. In
planning poker, it is considered as positive that all effort
estimates remain initially hidden from view, so that no one
is anchored in their initial estimate by the estimates of their
colleagues [26].
The anchor would not even have to be relevant for the
estimate [13] and could, for example, result from the previous
turning of a wheel of fortune with numbers between 0 and
100 [12]. Since we are interested in the transfer of the
anchoring effect to a realistic software engineering scenario,
we have decided to display a code comprehensibility metric
value.
Limited literature is available that has shown that individual
characteristics of the participant may inﬂuence the strength
of the anchoring effect. Furnham and Boo argue in their
literature review on the anchoring effect [13] that previous
research “neglected individual differences variables because
people tend to look for a universal rule that would predict
reactions or behaviour”. Nevertheless, they could identify a
total of 17 studies that considered the inﬂuence of experience,personality, mood, motivation and cognitive abilities. We aim
to contribute to this investigation in the context of RQ3 and ex-
plored the inﬂuence of experience, personality, happiness and
dispositional optimism and pessimism on the anchoring effect.
All these constructs are also associated in literature with the
placebo effect [24], [27]–[29]. In III-D and III-E we describe
the used questionnaires and how we have operationalized and
measured the constructs.
C. Source Code Comprehension/Understanding
Source code understandability is deﬁned as the extent to
which “code possesses the characteristic of understandability
to the extent that its purpose is clear to the inspector” [30] and
in this study, we particularly consider bottom-up comprehen-
sion, in which the programmer analyzes the source code line
by line and from several lines, deduces ‘chunks’ of higher
abstraction and ﬁnally aggregates these chunks into high-
level plans [31]. Many factors impact the comprehensibility of
source code. For example, one study has shown that shorter
identiﬁers take longer to comprehend [16], and another that a
number of certain code patterns lead to an increased rate of
misunderstanding [32].
Static code analysis tools attempt to measure code under-
standability automatically to efﬁciently point out sections of
the code that are difﬁcult to comprehend and should therefore
be refactored. Not only are most of the metrics in static code
analysis tools not validated [5], but in addition there seems to
be only one metric that is validated and positively correlates
with measures of source code comprehensibility [6], [7].
Scalabrino et al. [6] investigated 121 metrics that would
measure source code understandability. Code snippets were
evaluated with 63 developers and various proxy variables that
are related to the time and correctness required to complete
comprehension tasks. According to their results, none of the
investigated metrics showed a signiﬁcant correlation with the
measured source code comprehensibility.
A recent study [7] empirically evaluated the Cognitive
Complexity, a newly introduced metric that claims to measure
source code understandability [33]. The metric evaluates code
syntactically and assigns each method a calculated value on a
ratio scale. For Java methods, the authors suggest a threshold
value of 15 above which a snippet should be refactored. The
authors of the evaluation study conclude that the metric is
a reliable predictor of the required understanding time and
the subjective comprehensibility rating of developers [7]. Both
aspects encouraged us to consider the metric when selecting
code snippets for our study (see III-D; they all had a value of
19).
Displaying non-validated metrics can lead to confusion
and unnecessary effort due to improper prioritization of de-
velopment efforts. It becomes especially problematic if the
displayed metric value actually has an inﬂuence on developers,
even if only in their subjective perception of the code because
this would mean that they are subconsciously manipulated and
are very unlikely to resist this circumstance since the anchor-
ing effect is one of the most robust cognitive processes [13].
514Finally, in experiments like ours, we do not intend to
measure how understandable source code is, but the degree
to which a participant has understood the given source code.
It is in the nature of our study to investigate selected inﬂuences
on code understanding and not to compare variants of source
code for their comprehensibility. The most common measures
for this purpose are time and correctness in processing compre-
hension questions, subjective ratings and physiological mea-
surements such as eye tracking [7], [17]. Through a literature
review, Siegmund and Schumann [34] have compiled a list
of confounding variables that can affect code comprehension.
Their catalog of confounding variables and control techniques
is interesting not only because we have taken it into account
for the design of our experiment but also because it reveals
whether studies in the past have mentioned cognitive biases as
confounding variables. While we cannot identify in their list of
individual confounding parameters any that correspond to this,
in the list of experimental confounding parameters we found a
handful that are related to cognitive biases. One example is the
Hawthorne effect [35], [36], which describes that participants
in experiments would behave differently because they were
observed. With the present study we close a research gap and
investigate whether the anchoring effect could add an entry to
the catalog of confounding variables on code comprehension
in the future.
III. M ETHODOLOGY
We follow the guidelines of Jedlitschka et al. on reporting
experiments in software engineering [37].
A. Goals
The goal of our study is to analyze the effect of showing
a speciﬁc code comprehensibility metric on measures of a
software engineer’s code understanding to identify a potential
cognitive bias and placebo effect. To this end we formulated
the three research questions given in the introduction.
B. Participants
We invited a convenience sample of students of a software
engineering MSc study program in Germany to participate
in the study. According to recently proposed guidelines on
sampling in software engineering [38], convenience sampling
is “ideal for pilot studies and studying universal phenomena
such as cognitive biases”. Additionally, we limited participants
to those with good knowledge of Java and German, as the
study was conducted in German; in both aspects we relied
on the self-assessment of the participants. We see the sample
properties of interest, namely enough experience to compre-
hend medium to hard to understand Java code, ensured by our
sampling strategy so that the ﬁndings can be transferred to a
population of experienced Java software engineers. Limitations
of our sampling strategy and their implications are discussed
in V-A.
As part of their study duties, students had to participate in
any study offered by the faculty. Students had the right to
register for a study and then withdraw their participation atany time (including before the start) without consequences,
with course organizers being unaware of their withdrawal.
We reminded them about this during the informed consent
phase, which included a partial design disclosure, health risks,
privacy and ethical issues, and our contact details. Consent was
obtained in written form.
Participants knew that we aimed to investigate factors that
inﬂuence the understanding of source code, and that they
would have to work on short methods written in Java and
calculate the results for given input values. They were not
aware of the metric manipulation.
C. Tasks
Participants were shown three independent Java methods,
one after the other. For each code snippet, ﬁve input values
were given, for which the participants were asked to specify
the return values according to the Javadoc and to determine
the actual return value. Since we told our participants that
there might be bugs in the code, they could not rely on
the Javadoc comment and had to understand what the code
actually does. We consider the deviation of the documentation
from the code and the inspection based on concrete values for
the input parameters to be a realistic scenario. Furthermore,
the task is in line with the conceptual model that a developer
in a maintenance scenario iteratively constructs and tests
hypotheses about the functioning of the code during program
understanding [39].
Right after determining the return values, the participants
were asked to rate the comprehensibility of the method on
a scale of 0 (very easy) to 10 (very hard) and ﬁll out
questionnaires on their individual characteristics (details in
section III-G).
D. Experimental Materials
1) Environment: The tasks were all solved on a laptop
provided by us. Code snippets were presented in a web
environment specially developed for this study. The look
and feel of the web environment is based on the Eclipse
IDE default look. Tooltips for variables and functions were
displayed as typically expected in IDEs when hovering them.
Syntax highlighting and line numbers were available. Selecting
a variable highlights all occurrences of that variable. Next to
the source code, the comprehensibility value of the method
was displayed. A screenshot of the environment is shown in
Fig. 1.
2) Code Snippets: We used a total of ﬁve Java code snippets
to conduct the study, two of which were used to introduce the
study and explain the task, and the remaining three had to be
understood by the participants. All participants received the
same Java code snippets, regardless of treatment. Each code
snippet consisted of exactly one class with exactly one method.
The method was documented via Javadoc.
The three task-related snippets were taken from either
the Apache Commons Lang or Apache Commons Collection
project. We selected the snippets in a way that no uncommon
prior knowledge on, e.g., frameworks, would be required
515Fig. 1. Look and feel of the development environment that all participants used for the code comprehension tasks.
to understand them. As a result, the code contained mostly
primitive data types and the features of newer Java versions
were avoided. The snippets were slightly modiﬁed, either to
introduce a bug or to make sure that all task snippets have the
same cognitive complexity, an indicator for the comprehensi-
bility of the method, which is particularly reliable regarding
the subjective rating of developers [7], [33]. This allowed us
to weigh the answers to the three tasks equally and limited
potential confusion or loss of trust in the displayed metric
if the same metric value was displayed (by design) but very
different difﬁculties were perceived. The three task snippets
had a cognitive complexity score of 19, which is considered
moderate to difﬁcult to understand for Java methods.
3) Comprehension Questions: For each of the three tasks
a participant was provided with a paper-based form which
included ﬁve rows of a three-column table that had to be ﬁlled
in. The cells of the ﬁrst column each contained a method call,
for example toBooleanObject("ofo"). The other two
columns had to be ﬁlled with the actual return value of the
method and the expected return value according to the Javadoc.
4) Questionnaires: Participants had to ﬁll out several ques-
tionnaires. Related constructs are detailed in the next section.
To assess happiness, we use the Scale of Positive and
Negative Experiences (SPANE) [40] which quantiﬁes the
frequency of positive (SPANE-P) and negative (SPANE-N)
affective experiences, and the happiness overall of our partic-
ipants (SPANE-B). The questionnaire was successfully used
(and fully described) in other studies of behavioral software
engineering, e.g., [41], [42]. To appraise personality traits, we
use the Big Five Inventory [43], [44]. To measure dispositional
optimism and pessimism, we use the Life Orientation Test
(LOT-R) [45].
All measurement instruments have been psychometrically
validated in several large-scale studies and show good psycho-
metric properties [45]–[51] including consistency across full-
time workers and students [46]. For all questionnaires we used
a further psychometrically validated German version, i.e. [52]
for SPANE, [53] for the Big Five Inventory and [54] for LOT-
R.
E. Hypotheses, Parameters, and Variables
The independent variable relevant to RQ1 and RQ2 is the
displayed metric value (DMV). We developed the DMV toexpress the understandability of the source code. The DMV
ranges from 0 (very easy to understand) to 10 (very hard to
understand). The choice was dictated to how natural it is for
human beings to rate a concept from 0 to 10. The individual
participant only saw three values for the metric. For the two
introductory examples to explain the study tasks, we have cho-
sen the values 1 (a very easy task) and 9 (a very hard task) to
show possible extremes for coding snippet understandability.
For all three experiment tasks, a participant either saw a value
of 4 (moderately easy) or 8 (moderately hard), to cause the
anchoring effect into two opposed directions (easy and hard).
Regarding RQ1, the relevant dependent variable
is perceived understandability (PU). Perceived
understandability is deﬁned as the sum of a participant’s
ratings for the three code snippets. The rating of each code
snippet ranges, identical to the DMV , from 0 (very easy to
understand) to 10 (very hard to understand). Accordingly, the
value for PU is in the range of 0 to 30.
H10: There is no signiﬁcant difference in perceived
understandability (PU ) between the two anchoring directions
of a displayed metric value (DMV ).
H1A: There is a signiﬁcant difference in perceived
understandability (PU ) between the two anchoring directions
of a displayed metric value (DMV ).
Regarding RQ2, we consider two common measures for
code comprehension, which are time needed to complete all
three tasks and correctness of the answers to the compre-
hension questions. The time was recorded for each task and
summed up at the end. Correctness is the sum of correct
answers to the comprehension questions of all three tasks,
including both correct answers to actual return values and
correct answers to return values according to the documen-
tation. Therefore, the value for correctness is in the range of
0 to 30. To answer the research question, we combine time
and correctness, as Scalabrino et al. [6] did, for example, to
score participants higher that are both fast and correct. This
results in timed actual understanding (TAU), a participant’s
performance score obtained by combining correctness and
time. Equation (1) provides the calculation for TAU, which
ranges from 0 (the worst possible) to 1 (the best possible)
516and in which tmax is the time of the participant who took the
longest.
correctness
30∗/parenleftbigg
1−time
tmax/parenrightbigg
(1)
H20: There is no signiﬁcant difference in timed actual
understanding (TAU ) between the two anchoring directions
of a displayed metric value (DMV ).
H2A: There is a signiﬁcant difference in timed actual
understanding (TAU ) between the two anchoring directions
of a displayed metric value (DMV ).
The investigation of RQ3, the extent to which individual
characteristics inﬂuence the deviation from the displayed met-
ric value, is exploratory research. Therefore, no hypotheses
were formulated for this research question. The metric devi-
ation is deﬁned as mean absolute deviation of a participant’s
rating from the displayed metric value and calculated as shown
in (2), where PUiis the perceived understandability for task
i.
|PU1−DMV|+|PU2−DMV|+|PU3−DMV|
3(2)
The individual characteristics of interest in this study are
experience with Java, personality, happiness and dispositional
optimism and pessimism. Java experience was measured in
years. Personality was operationalized by the ﬁve dimensions
of the Five Factor model, i.e., extraversion range= [0,32],
agreeableness r= [0,36], conscientiousness r= [0,36],
neuroticism r= [0,32]and openness to experience r= [0,40].
The higher the value, the more pronounced is the respective
personality facet of a participant. The range of SPANE-P
(positive affect) and SPANE-N (negative affect) is r= [6,30],
from low frequency to high frequency of positive and negative
experiences, respectively. SPANE-B, or happiness, has a range
r= [−24,24], the negative pole refers to unhappiness and
the positive one to happiness. Dispositional optimism and
pessimism are independent constructs rather than a bipolar
trait. Both are in the range r= [0,12], from low to high
degree of optimism and pessimism, respectively.
F . Experiment Design
The experiment was a between-subject design with two
treatment groups. Assignment to a treatment was double-blind
and random.2None of the authors knew which participant,
even as anonymous data point, was in which treatment group
until the data was evaluated.
One group saw a DMV of 4 next to all code snippets. We
call this group the easy group from this point on. The other
group saw a DMV of 8. We refer to this group as the hard
2We agree, to some extent, with Baltes and Ralph [38] that random should
be used sparingly, so we will add here that participants were assigned to a
condition based on the time slot they signed up for. When assigning treatment
conditions to a time slot, it was ensured that the conditions were distributed
equally over different times of day.group from this point on. There were no further differences in
the treatment of the two groups.
The reader might notice an absence of a control group,
which does not see any DMV . This is by design and sug-
gested in literature on the anchoring effect, which we discuss
in more detail in V-A.
Following Wohlin et al. [55] guidelines for conducting
experiments in software engineering, we had the study design
reviewed by two peers in two iterations and conducted a
pilot test. Furthermore, we have identiﬁed and implemented
a number of measures that we believe contribute to mitigating
threats to validity. Limitations of our ﬁnal study design, that
is, what affects the interpretation of our results, are discussed
in V-A.
1) Measures to Address Construct Validity: We used psy-
chometrically validated questionnaires for assessing individual
characteristics. Regarding the operationalization of code un-
derstanding, we have oriented ourselves on how the construct
was measured by our peers in peer reviewed research [6], [7].
With time and correctness we measured two argumentatively
important and objective aspects of code understanding and
combined them with equal weight similar to what Scalabrino
et al. did [6].
We designed a plausible scenario to justify the display of
the metric value and developed an experiment description for
participants which did not reveal the essence of the experiment
to prevent hypothesis guessing [55]. The description did not
present false information, but hid only the objective of the
anchoring effect. Closely related, we prevented the threat of
experimenter expectations [55] by double-blind assignment of
participants to treatment groups.
To prevent evaluation apprehension [55] and unnecessary
stress we always tried to have exactly two participants simul-
taneously in the room, the experimental supervisor could not
look over the shoulder of the participants and it was empha-
sized several times that the answers were anonymous. Due to
their intimate nature, the Big Five and LOT-R questionnaires
were completed only after the code comprehension tasks had
been completed. Both participants in the same time slot were
also in the same treatment group to avoid diffusion or imitation
of treatments [55] for example by one of the two participants
making a comment on the displayed metric value.
2) Measures to Address Internal Validity: We discussed
every variable listed in the mapping study on confounding
variables in code comprehension experiments [34]. Of the
37 variables listed, only two remained even after thorough
planning of the experiment, which we see as potential threats
to validity: the Hawthorne effect and selection (the generaliz-
ability of student participants). We discuss both in V-A.
Of the potential confounders that we have explicitly con-
trolled, we highlight the following two. First, we selected
several code snippets that a participant had to understand to
reduce the inﬂuence of individual data structures and program
semantics. Snippets were also neither too difﬁcult nor too easy
according to a validated code comprehensibility metric [7],
and moreover of comparable comprehensibility. Second, we
517implemented a tool to view and interact with the code in
the tasks. This gave us full control over the environment and
allowed us to reduce the displayed elements to a minimum
to increase internal validity. In addition, no participant had
an advantage, since no one was more familiar with the
environment than everyone else.
3) Measures to Address External Validity: We used code
snippets taken from real-world, actively developed famous
open-source projects and avoided removing comments or ob-
scuring method or variable names to force code understanding.
Instead, we have created a realistic software maintenance
scenario in which the developer needs to understand code
that does not necessarily ﬁt the documentation and that may
contain a bug, requiring the developer to check what the actual
output values are for certain input values.
4) Measures to Address Conclusion Validity: Regarding
reliability of treatment implementation [55], we used a strict
protocol and double-blind condition assignment to ensure that
each participant received the same information and was treated
equally. The same investigator conducted the experiment with
all 45 participants. The DMV of either 4 or 8 was displayed
the same for all participants in a treatment group, as it was an
automatic display in an environment controlled by us.
To prevent random irrelevancies in experimental setting [55]
we reserved the room two weeks in advance and for the entire
day on each day the study was conducted. We were prepared
to document irregularities, but did not have to do so.
G. Procedure
From the moment one or two participants arrived in the
room reserved for the experiment at the agreed time, the
following steps took place. Participants were provided with
a consent form and were informed verbally about the aim
of the study. They were shown the laptop and the ﬁles on
the laptop. Then they had to ﬁll out two questionnaires, one
on demographic data and on their happiness (SPANE). The
instructor made the participants save and close the question-
naires on their own after completing them, so that they did
not feel observed by the instructor.
The instructor explained the task, the scenario and the
development environment to the participants. Thereby the
participants were shown a ﬁlled out task sheet for one method
as an example and the solution was exempliﬁed for the ﬁrst
two input values. The visualization of the metric and its
display for supposedly informative reasons was explained and
a second introductory example, which was signiﬁcantly harder
to understand than the ﬁrst one, demonstrated that the metric
works well. When asked, the instructor told that the metric
was developed by experts, similar to what [18] did in their
placebo sleep experiment.
The instructor summarized the task and mentioned that the
employer in the task scenario wants them to work efﬁciently
but also correctly. Participants then had the opportunity to ask
questions before starting with the tasks. The time recording
was done per task and the recorded value was stored in a
spreadsheet without the participants noticing it. After all three1015202530
easy hard
Experimental groupPerceived Understandability (PU, all 3 tasks)group
easy
hard
Fig. 2. Perceived understandability (PU) of the tasks for the easy group and
the hard group (three tasks, range 0 (easiest) to 10 (hardest), combined range
0 (easiest) to 30 (hardest)).
tasks had been completed, the participants ﬁnally had to ﬁll
out the questionnaires on individual characteristics.
IV. R ESULTS
A. Descriptive statistics and dataset preparation
45 students participated in the study3. Two of them did
not submit complete data for RQ3 (e.g., missing an item
for the SPANE questionnaire). We decided to exclude them
from the dataset to enhance our conﬁdence in how serious
all participants were in completing all tasks. We thus had
an overall sample size of n= 43 participants (41 male,2
female). Mean age was 24.47(SD= 2.84), average declared
experience with the Java programming language was 5.83
years (SD = 2.37).
Participants were randomly allocated to the easy group,n=
20) or the hard group,n= 23. Declared experience with the
Java programming language was comparable for both groups
after random assignment (M = 5.25,SD= 2.60 for the easy
group,M= 6.33,SD= 2.06 for the hard group).
The easy group was shown a DMV of4for the three tasks,
or12combined, and provided a PU of15.40(SD= 4.17,
median= 14.50). The hard group was shown a DMV of8
for the three tasks, or 24combined, and provided PUof20.83
(SD= 4.23,median = 20.00). A graphical comparison is
offered in the boxplot of ﬁg. 2.
The easy group performed with an average TAU of
M=.37(SD=.17,median =.41). The hard group
3We cannot offer a precise acceptance rate because course attendance is
not mandatory and cannot be recorded.
518TABLE I
SPEARMAN ’SρCORRELATION COEFFICIENT OF INDIVIDUAL CHARACTERISTICS WITH THE DEVIATION OF THE SUBJECTIVE RATING .
Group Java Experience (years) SPANE-P SPANE-N SPANE-B Optimism Pessimism Extraversion Agreeableness Conscientiousness Neuroticism Openness
easy -0.28 0.11 -0.23 0.18 0.32 0.18 -0.1 0.08 0.46 -0.12 -0.01
hard 0.24 -0.15 0.13 -0.23 0 -0.04 -0.09 0 -0.09 -0.06 0.05
combined -0.03 -0.12 0.06 -0.16 0.14 0.07 -0.13 0.08 0.14 -0.05 0.01
performed with an average TAU ofM=.37(SD=.11,
median=.36). A further boxplot comparison (included in the
supplemental material) did not suggest signiﬁcant difference.
B. Hypothesis testing
There was a signiﬁcant difference between PU of the
two groups45,t(40.318) =−4.227,p=.000132, 95% CI
[−8.02,−2.83], with a large effect size, d=−1.29, 95% CI
[−1.97,−0.61].
We thus reject H10in favor to H1A:There is evidence
for a difference in perceived understandability between
the two anchoring directions of a displayed metric value.
There was no signiﬁcant difference between the timed
actual understanding (TAU ) of the two groups6,W= 256,
p=.5385, with a negligible effect size, d=−.006, 95% CI
[−0.62,0.61].
We do not reject H20.There is no evidence for a
difference in timed actual understanding between the two
anchoring directions of a displayed metric value.
C. Exploratory analysis
We provide in Table I the computed correlation coefﬁcients
of the metric deviation (calculated as in formula 2) with
affect-related metrics and personality-related metrics. The ﬁrst
two rows provide the correlation coefﬁcients for the two
experimental groups (between), while the third row combines
all participants (within).
For brevity’s sake, we will call the “metric deviation” simply
“deviation” the rest of this section. As a reminder, the wider
the deviation, the bigger the gap between the subjective rating
and the shown metric value, or the manipulation, on the easy
direction or on the hard direction.
Given the exploratory nature of RQ3, no estimation of
signiﬁcance was conducted for the correlation coefﬁcients. As
a cutoff for potentially interesting individual characteristics,
we will only consider correlation coefﬁcients |ρ|>0.1.
When exploring the data between the two experimental
groups, we notice that an increase in programming language
experience is associated with a decreased deviation when the
manipulation suggests an easy task and an increased deviation
when the manipulation suggests a hard task. The opposite
4Welch Two Sample t-test given evidence for non-normality, (Shapiro-Wilk
test,p > .26 for both groups) and no further assumption on the population
variance.
5We are aware of the open debate on whether Likert items are ordinal data
or continuous data [56]. We believe, in line with psychometric theory, to have
Likert items capture discrete points over a continuous scale. All scales that we
use for individual characteristics are psychometrically validated Likert items.
6Wilcoxon rank sum exact test given evidence for non-normality (Shapiro-
Wilk test, p=.03for the easy group).happens with happiness (SPANE-B). An increase in happiness
is associated with an increased deviation for the easy group
and a decreased deviation for the hard group. The two major
components of happiness, SPANE-P and SPANE-N, show
coherence with the aggregated happiness score.
An increase for both optimism and pessimism is associated
with a wider deviation for the easy group, while no correlation
is observed for the hard group. Of all personality traits, an
increase in conscientiousness seems to be strongly correlated
with an increased deviation for the easy group, followed by
neuroticism but with an inverse relationship. No personality
trait shows a |ρ|>0.1for the hard group.
When combining the two groups, in a within subject analy-
sis, with the assumption that the two groups are from the same
population, an overall negative relationship between happiness
and the deviation is observed (happier participants deviated
less). An increase in optimism is associated with a wider
deviation. Of the personality traits, an increase in optimism
and conscientiousness were correlated with a bigger deviation,
but an increase with extraversion was correlated with smaller
deviation. Programming experience seems to not play a role
in the deviation.
V. D ISCUSSION
Metrics provide developers with quantitative insights into
the quality of their source code, but most of the metrics used
in practice are not sufﬁciently validated. We have investigated
the extent to which developers are actually subconsciously
inﬂuenced by the value of a displayed made-up metric to create
an awareness of responsibility in code quality reporting.
We found a signiﬁcant and strong anchoring effect, which
means that developers are strongly inﬂuenced by a displayed
metric value in their rating of source code comprehensibility.
This ﬁnding is consistent with over 40 years of research on the
anchoring effect [13], yet investigation in a speciﬁc software
engineering context is nevertheless a valuable contribution
to build the foundation for future studies, for example to
investigate consequences of the demonstrated effect.
One such potential consequence could be an improved
or worsened understanding of the code. However, in our
study we could not provide evidence that the suggestion of
simple or difﬁcult code provokes a placebo effect, in the
sense that the beliefs concerning the comprehensibility of the
source code inﬂuences the speed and correctness with which
a software engineer answers comprehension questions. Since
cognitive performance and creativity, arguably both character-
istics necessary for code understanding, can be inﬂuenced by a
placebo [19], [20], we would have expected to observe such an
effect. Assuming that a placebo effect for code comprehension
519can be observed theoretically, we see two possible reasons why
we could not in our experiment.
First, it could be that a displayed metric value is simply
not a strong enough inﬂuence to cause performance changes.
Compared to other placebo studies [8], [18], [19], we did not
manipulate the participants to claim that our treatment had
speciﬁc beneﬁcial or performance-enhancing effects. Instead,
we displayed a metric value that indicated how easy or difﬁcult
code is to understand. We left it up to the participants to
interpret what it means and what consequences it might have
if source code is easy or hard to understand. Accordingly, code
comprehension may have actually improved or worsened as a
result of the manipulation, just not in the way we measured
it.
This brings us to the second possibility that time and cor-
rectness are not all-inclusive proxies for code comprehension.
We speculate that physiological measurements may have led
to a different result. For example, cognitive load and stress
levels at the end of task processing might have been lower
in the easy group, as they may have been more comfortable
with the task. While the question of ideal measures of code
comprehension is beyond the scope of this work, and we have
not measured these variables, we argue that they nevertheless
reﬂect relevant dimensions of code comprehension. With the
increase in physiological measurements in the ﬁeld of code
comprehension [57]–[59], we see much potential for future
studies to replicate our experiment with alternative measure-
ment methods to shed light on the matter.
Regarding individual characteristics that inﬂuence the
strength of the observed anchoring effect, our results, while
exploratory and based on correlations, are only partially con-
sistent with the few relevant studies conducted to date [13].
We observed that participants with low extraversion are less
subjected to the anchoring effect, which is consistent with
existing literature [60]. However, our results indicate that
participants with high conscientiousness are less susceptible
to the anchoring effect, which is contrary to the ﬁndings of
Eroglu and Croxto [60]. We could not ﬁnd a positive corre-
lation between anchoring and the personality trait openness
as McElroy and Dowd [61] did. Further, our results suggest
that happier people might be more subjected to the anchoring
effect, which also does not coincide with previous studies [62],
[63].
We found that optimism positively correlates with the metric
deviation, which corresponds to a weaker anchoring effect.
Programming experience on the other hand might not play a
role in anchoring, but this should be taken with caution, since
we had a homogeneous group of experienced developers and
results could be different for inexperienced developers.
If the code comprehensibility metric shows a low value for
a rather difﬁcult task, personality factors could play a bigger
role. In particular, conscientiousness might be the strongest
predictor of a deviation when metrics are too conservative
in how difﬁcult a task might be (easy group), followed by
optimism. In conclusion, our results suggest that the anchoring
effect might not be a universal rule that applies equally toall participants. The partial deviation of our results from
previous ﬁndings is an indication that more studies are needed
in this regard. The exploratory settings of RQ3 set basic
building blocks upon which we call our peers to conduct future
research.
A. Limitations
A detailed description of design decisions made in advance
to mitigate validity risks can be found in III-F. What follows
are limitations that affect the ﬁnal study design and should be
considered when interpreting our results.
As for potential confounding variables, we could reduce a
lengthy list of known variables [34] to two that might have
affected the results: the Hawthorne effect and the selection of
students as participants.
The Hawthorne effect [35], [36] describes that participants
in experiments would behave differently because they were
observed. While we addressed hypothesis guessing [55] with
a plausible scenario that does not reveal the essence of the
experiment, and we addressed the threat of evaluation appre-
hension [55] through privacy and data anonymization, it cannot
be ruled out that participants followed the proposed metric
value more closely than they would have done outside an
experimental setting. According to [13] the current dominant
view of the anchoring paradigm focuses on conﬁrmatory
hypothesis testing in the sense that information is activated
that is consistent with the anchor presented. We assume that
this also applies to our experiment and that the observed
anchoring effect, if at all, is only to a small extent due to
an experimentally provoked good will of the participants for
the metric. A future ﬁeld study could provide clarity in this
respect.
As argued earlier in the description of participants (III-B),
we invited a convenience sample of students of a software
engineering MSc study program. In our sampling strategy, we
prioritized internal validity, which was enhanced by a homo-
geneous level of experience with the programming language,
paradigm, and task type. We also believe that our results can be
generalized to a population of professional software engineers.
In the discussion about representativeness of software engi-
neering students, opposing opinions have existed for the last
20 years [64]–[66]. Recent studies have shown that comparable
results can be achieved with both groups of students and
professionals—as long as the scope of the investigation is
carefully considered (see, e.g., [67]). We have conﬁdence in
the robustness and soundness of our research design. Recent
commentaries [68] have, once again, highlighted how diverse
the views are on the topic. We side with the view summarized
by Runeson [68] as well as Baltes and Ralph [38] that a
convenience sample of students is justiﬁed in the investigation
of central behavioral and cognitive processes, as was the case
in our study. There is evidence, for example, that the anchoring
effect is not restricted to laymen and that more experienced
people are inﬂuenced by it as well [12], [13].
Then, since we did not have a control group in our ex-
periment that was notshown a metric value, we cannot say
520how a control group would have rated the snippets. For the
demonstration of the anchoring effect this is not a limitation
and it is consistent with the body of research on the anchoring
effect to not have a no-anchor control group [69], [70].
However, regarding the placebo effect, we would like to stress
that our design would not be able to decide strictly speaking
whether any observed effect is due to the placebo, but with
the design it is still possible to demonstrate the extent to
which a displayed metric value inﬂuences code understanding
in a positive or negative direction. Our study is based on a
comprehensive body of research that has provided evidence for
the placebo effect, so we assumed that the effect would also
exist in our scenario and opted for the study design described
above.
Finally, we are aware that the way in which the metric value
was displayed is very prominent. We argue that developers
are used to static code analysis tools reporting metric values
in similar ways and IDEs increasingly offer the possibility to
display code quality metrics directly in the source code. Even
if this situation is not as common, we refer to Critcher and
Gilovich [69] and related works showing that for the anchoring
effect to work, much more inconspicuous anchors, which do
not even have to be highlighted, are usually sufﬁcient. Again,
a ﬁeld study with realistic IDE plugins and existing metrics
could be an option to repeat the investigation of the effects in
an industrial environment.
B. Implications
Since developers are inﬂuenced by a shown metric value in
their subjective evaluation of a code snippet’s comprehensibil-
ity, we highlight the following implications.
First, those responsible for reporting code quality metrics
should be aware of their responsibility that non-validated
metrics lead to unwarranted manipulation of developers, the
consequences of which we do not know yet. Future studies can
build on our results and investigate possible consequences.
Second, since it is a common practice in code understanding
studies to ask developers for a subjective rating, such studies
should ensure that individual participants are not anchored
by context factors such as displayed metric values. It may
already be sufﬁcient that the instructor or the task description
hint at something about the complexity of a code snippet to
be examined. Also, for example, different amounts of time
available for processing different code snippets could lead to
an anchoring of the participants in their subjective assessment.
If the study cannot be controlled with certainty in this regard,
the measure of subjective ratings should not be used.
We echo the call of Mohanani et al. [11] and propose a
debiasing technique for the anchoring effect. The debiasing
here is about developing validated metrics (or validate existing
ones) before showing their values to software developers.
We do not consider it an issue when developers are anchored
in their subjective judgement by a validated metric that may
be able to consider more factors and evaluate more objectively
than a developer could. We are aware that, for example, static
code analysis tools do not intentionally lie, and that many ofthe metrics seem to make intuitive sense. It becomes problem-
atic, however, when developers interpret quality aspects into
metrics that were not intended to be measured by the metric,
or when the metric is no more than an implemented, albeit
well-thought-out, idea. Therefore, tools should describe very
precisely what the metric intends to measure and support this
measurement with systematic research.
A number of previous studies already called for more
effort to be put into disseminating research ﬁndings among
practitioners so that they can rely on evidence rather than
forming biased and error-prone conclusions based on personal
impressions [71], [72]. Especially with respect to the clearly
shown anchoring effect, validated metrics should have an easy
time anchoring developers where they should be anchored
evidence-wise and overcome the circumstance that developers
sometimes tend to prefer their own opinions over empirical
evidence [73].
We consider the negative results on RQ2 to be good results
under the circumstances described above. Since the situation is
unlikely to change in the near future, it is at least good to know
that a few random numbers may not have a negative impact
on a developer’s understanding time and correctness during
maintenance. Whether other aspects of code understanding can
be inﬂuenced by a placebo will need to be investigated in
future studies.
VI. C ONCLUSION
We investigated whether the value of a shown code compre-
hensibility metric inﬂuences subjective ratings of code com-
prehensibility and actual code comprehension performance,
i.e., the time spent and correctness in answering comprehen-
sion questions. In a randomized double-blind experiment two
groups of participants had to understand three code snippets,
answer comprehension questions and rate the understandabil-
ity of the code snippets. We found the shown metric value to
have a signiﬁcant and large effect on the developers’ ratings
but not on their performance. The strength of the anchoring
effect appears smaller for optimistic and conscientious people.
Since we have limited understanding of the consequences
of the demonstrated manipulation of developers by non-
validated metrics, we call for an increased awareness of the
responsibility in code quality reporting and for corresponding
tools to be based more strongly on scientiﬁc evidence. Studies
in which code comprehensibility is measured via subjective
ratings should control contextual factors such as shown metric
values that may potentially inﬂuence developers’ ratings, or
refrain from this measure altogether.
Future works should focus on investigating the conse-
quences of the belief that code is easy or hard to understand.
A similar study should be conducted to investigate the in-
ﬂuence of displayed metric values on other dimensions of
code understanding, such as cognitive load, and we consider
a slightly modiﬁed version of our study to be useful, e.g., to
investigate the inﬂuence of code complexity on the strength
of the anchoring effect.
521VII. D ATA AVAILABILITY
Following open science principles in software engineer-
ing [74], we disclose code snippets, task sheets with com-
prehension questions, anonymized raw data, and the R script
for the analysis openly [75].
ACKNOWLEDGMENTS
We are deeply thankful to our participants for taking part
in our study. We thank three anonymous reviewers for their
insightful comments and Katharina Plett for proofreading.
REFERENCES
[1] R. Minelli, A. Mocci, and M. Lanza, “I know what you did last summer-
an investigation of how developers spend their time,” in 2015 IEEE 23rd
International Conference on Program Comprehension, IEEE. IEEE,
2015, pp. 25–35.
[2] X. Xia, L. Bao, D. Lo, Z. Xing, A. E. Hassan, and S. Li, “Measuring
program comprehension: A large-scale ﬁeld study with professionals,”
IEEE Transactions on Software Engineering, vol. 44, no. 10, pp. 951–
976, 2018.
[3] M. Alshayeb, “Empirical investigation of refactoring effect on software
quality,” Information and software technology, vol. 51, no. 9, pp. 1319–
1326, 2009.
[4] K. Beck, Extreme programming explained: embrace change. addison-
wesley professional, 2000.
[5] M. Nilson, V . Antinyan, and L. Gren, “Do internal software quality tools
measure validated metrics?” in International Conference on Product-
Focused Software Process Improvement, ser. Lecture Notes in Computer
Science. Cham, Switzerland: Springer, 2019, vol. 11915, pp. 637–648.
[6] S. Scalabrino, G. Bavota, C. Vendome, M. Linares-Vasquez, D. Poshy-
vanyk, and R. Oliveto, “Automatically assessing code understandability,”
IEEE Transactions on Software Engineering, 2019.
[7] M. Mu ˜noz Bar ´on, M. Wyrich, and S. Wagner, “An empirical validation
of cognitive complexity as a measure of source code understandability,”
inProceedings of the 14th ACM / IEEE International Symposium on
Empirical Software Engineering and Measurement (ESEM). New York,
NY , USA: Association for Computing Machinery, 2020.
[8] A. J. Crum and E. J. Langer, “Mind-set matters: Exercise and the placebo
effect,” Psychological Science, vol. 18, no. 2, pp. 165–171, 2007.
[9] A. K. Shapiro, “Semantics of the placebo,” Psychiatric Quarterly,
vol. 42, no. 4, pp. 653–695, 1968.
[10] M. Hilbert, “Toward a synthesis of cognitive biases: how noisy in-
formation processing can bias human decision making.” Psychological
bulletin, vol. 138, no. 2, p. 211, 2012.
[11] R. Mohanani, I. Salman, B. Turhan, P. Rodr ´ıguez, and P. Ralph,
“Cognitive biases in software engineering: a systematic mapping study,”
IEEE Transactions on Software Engineering, 2018.
[12] A. Tversky and D. Kahneman, “Judgment under uncertainty: Heuristics
and biases,” science, vol. 185, no. 4157, pp. 1124–1131, 1974.
[13] A. Furnham and H. C. Boo, “A literature review of the anchoring effect,”
The journal of socio-economics, vol. 40, no. 1, pp. 35–42, 2011.
[14] P. Lenberg, R. Feldt, and L. G. Wallgren, “Behavioral software engineer-
ing: A deﬁnition and systematic literature review,” Journal of Systems
and software, vol. 107, pp. 15–37, 2015.
[15] D. Graziotin, P. Lenberg, R. Feldt, and S. Wagner, “Psychometrics in
behavioral software engineering: A methodological introduction with
guidelines,” 2020. [Online]. Available: https://arxiv.org/abs/2005.09959
[16] J. Hofmeister, J. Siegmund, and D. V . Holt, “Shorter identiﬁer names
take longer to comprehend,” in 2017 IEEE 24th International Conference
on Software Analysis, Evolution and Reengineering (SANER). IEEE,
2017, pp. 217–227.
[17] D. Oliveira, R. Bruno, F. Madeiral, and F. Castor, “Evaluating code
readability and legibility: An examination of human-centric studies,”
in2020 IEEE International Conference on Software Maintenance and
Evolution (ICSME). IEEE, 2020, pp. 348–359.
[18] C. Draganich and K. Erdal, “Placebo sleep affects cognitive functioning,”
Journal of Experimental Psychology: Learning, Memory, and Cognition,
vol. 40, no. 3, pp. 857–864, 2014.[19] L. Rozenkrantz, A. E. Mayo, T. Ilan, Y . Hart, L. Noy, and U. Alon,
“Placebo can enhance creativity,” PLOS ONE, vol. 12, no. 9, pp. 1–15,
09 2017.
[20] Z. Turi, E. Bjørkedal, L. Gunkel, A. Antal, W. Paulus, and M. Mittner,
“Evidence for cognitive placebo and nocebo effects in healthy individ-
uals,” Scientiﬁc reports, vol. 8, no. 1, pp. 1–14, 2018.
[21] M. B ´erdi, F. K ¨oteles, A. Szab ´o, and G. B ´ardos, “Placebo effects in
sport and exercise: a meta-analysis,” European Journal of Mental Health,
vol. 6, no. 2, pp. 196–212, 2011.
[22] P. den Heijer, W. Koole, and C. J. Stettina, “Don’t forget to breathe: A
controlled trial of mindfulness practices in agile project teams,” in Agile
Processes in Software Engineering and Extreme Programming. Cham:
Springer International Publishing, 2017, pp. 103–118.
[23] J. Souza, A. A. Ara ´ujo, I. Yeltsin, R. Saraiva, and P. Soares, “On the
placebo effect in interactive sbse: A preliminary study,” in Search-Based
Software Engineering. Cham: Springer International Publishing, 2018,
pp. 370–376.
[24] T. D. Wager and L. Y . Atlas, “The neuroscience of placebo effects:
connecting context, learning and health,” Nature Reviews Neuroscience,
vol. 16, no. 7, pp. 403–418, 2015.
[25] G. Allen and B. J. Parsons, “A little help can be a bad thing: Anchoring
and adjustment in adaptive query reuse,” ICIS 2006 Proceedings, p. 45,
2006.
[26] N. C. Haugen, “An empirical study of using planning poker for user
story estimation,” in AGILE 2006 (AGILE’06). IEEE, 2006, pp. 9–pp.
[27] A. L. Geers, J. A. Wellman, S. L. Fowler, S. G. Helfer, and C. R. France,
“Dispositional optimism predicts placebo analgesia,” The Journal of
Pain, vol. 11, no. 11, pp. 1165–1171, 2010.
[28] D. L. Morton, A. Watson, W. El-Deredy, and A. K. Jones, “Repro-
ducibility of placebo analgesia: Effect of dispositional optimism,” Pain,
vol. 146, no. 1-2, pp. 194–198, 2009.
[29] M. Peci ˜na, H. Azhar, T. M. Love, T. Lu, B. L. Fredrickson, C. S. Stohler,
and J.-K. Zubieta, “Personality trait predictors of placebo analgesia and
neurobiological correlates,” Neuropsychopharmacology, vol. 38, no. 4,
pp. 639–646, 2013.
[30] B. W. Boehm, J. R. Brown, and M. Lipow, “Quantitative evaluation of
software quality,” in Proceedings of the 2Nd International Conference
on Software Engineering, ser. ICSE ’76. Los Alamitos, CA, USA:
IEEE Computer Society Press, 1976, pp. 592–605.
[31] M. P. O’Brien, J. Buckley, and T. M. Shaft, “Expectation-based,
inference-based, and bottom-up software comprehension,” Journal of
Software Maintenance and Evolution: Research and Practice, vol. 16,
no. 6, pp. 427–447, 2004.
[32] D. Gopstein, J. Iannacone, Y . Yan, L. DeLong, Y . Zhuang, M. K.-C. Yeh,
and J. Cappos, “Understanding misunderstandings in source code,” in
Proceedings of the 2017 11th Joint Meeting on Foundations of Software
Engineering - ESEC/FSE 2017. New York, New York, USA: ACM
Press, 2017, pp. 129–139.
[33] G. A. Campbell, “Cognitive complexity: An overview and evaluation,”
inProceedings of the 2018 International Conference on Technical Debt,
ser. TechDebt ’18. New York, NY , USA: Association for Computing
Machinery, 2018, p. 57–58.
[34] J. Siegmund and J. Schumann, “Confounding parameters on program
comprehension: a literature survey,” Empirical Software Engineering,
vol. 20, no. 4, pp. 1159–1192, 2015.
[35] F. J. Roethlisberger and W. J. Dickson, Management and the Worker.
Psychology press, 2003, vol. 5.
[36] R. McCarney, J. Warner, S. Iliffe, R. Van Haselen, M. Grifﬁn, and
P. Fisher, “The hawthorne effect: a randomised, controlled trial,” BMC
medical research methodology, vol. 7, no. 1, p. 30, 2007.
[37] A. Jedlitschka, M. Ciolkowski, and D. Pfahl, “Reporting experiments
in software engineering,” in Guide to advanced empirical software
engineering. Springer, 2008, pp. 201–228.
[38] S. Baltes and P. Ralph, “Sampling in software engineering research:
A critical review and guidelines,” 2020. [Online]. Available:
https://arxiv.org/abs/2002.07764
[39] A. von Mayrhauser and A. M. Vans, “Program comprehension during
software maintenance and evolution,” Computer, vol. 28, no. 8, pp. 44–
55, 1995.
[40] E. Diener, D. Wirtz, W. Tov, C. Kim-Prieto, D.-w. Choi, S. Oishi,
and R. Biswas-Diener, “New well-being measures: Short scales to
assess ﬂourishing and positive and negative feelings,” Social Indicators
Research, vol. 97, no. 2, pp. 143–156, 2010.
522[41] D. Graziotin, F. Fagerholm, X. Wang, and P. Abrahamsson, “On the
unhappiness of software developers,” in Proceedings of the 21st interna-
tional conference on evaluation and assessment in software engineering,
2017, pp. 324–333.
[42] D. Graziotin, X. Wang, and P. Abrahamsson, “Happy software devel-
opers solve problems better: psychological measurements in empirical
software engineering,” PeerJ, vol. 2, p. e289, 2014.
[43] J. M. Digman, “Personality structure: Emergence of the ﬁve-factor
model,” Annual review of psychology, vol. 41, no. 1, pp. 417–440, 1990.
[44] R. R. McCrae and O. P. John, “An introduction to the ﬁve-factor model
and its applications,” Journal of personality, vol. 60, no. 2, pp. 175–215,
1992.
[45] M. F. Scheier, C. S. Carver, and M. W. Bridges, “Distinguishing
optimism from neuroticism (and trait anxiety, self-mastery, and self-
esteem): a reevaluation of the life orientation test.” Journal of personality
and social psychology, vol. 67, no. 6, p. 1063, 1994.
[46] A. J. Silva and A. Caetano, “Validation of the Flourishing Scale and
Scale of Positive and Negative Experience in Portugal,” Social Indicators
Research, vol. 110, no. 2, pp. 469–478, 2013.
[47] F. Li, X. Bai, and Y . Wang, “The Scale of Positive and Negative
Experience (SPANE): psychometric properties and normative data in
a large Chinese sample.” PloS one, vol. 8, no. 4, pp. 1–9, 4 2013.
[48] K. Sumi, “Reliability and Validity of Japanese Versions of the Flour-
ishing Scale and the Scale of Positive and Negative Experience,” Social
Indicators Research, vol. 118, no. 2, pp. 601–615, 2014.
[49] G. Corno, G. Molinari, and R. M. Ba ˜nos, “Assessing positive and
negative experiences: validation of a new measure of well-being in an
Italian population,” Rivista di psichiatria, vol. 51, no. 3, pp. 110–115,
2016.
[50] V . Jovanovi ´c, “Beyond the PANAS: Incremental validity of the Scale of
Positive and Negative Experience (SPANE) in relation to well-being,”
Personality and Individual Differences, vol. 86, pp. 487–491, 2015.
[51] V . Benet-Mart ´ınez and O. P. John, “Los cinco grandes across cultures
and ethnic groups: Multitrait-multimethod analyses of the big ﬁve in
spanish and english.” Journal of personality and social psychology,
vol. 75, no. 3, p. 729, 1998.
[52] T. Rahm, E. Heise, and M. Schuldt, “Measuring the frequency of
emotions—validation of the scale of positive and negative experience
(spane) in germany,” PloS one, vol. 12, no. 2, p. e0171288, 2017.
[53] F. R. Lang, O. L ¨udtke, and J. B. Asendorpf, “Testg ¨ute und psychome-
trische ¨aquivalenz der deutschen version des big ﬁve inventory (bﬁ) bei
jungen, mittelalten und alten erwachsenen,” Diagnostica, vol. 47, no. 3,
pp. 111–121, 2001.
[54] H. Glaesmer, J. Hoyer, J. Klotsche, and P. Y . Herzberg, “Die deutsche
version des life-orientation-tests (lot-r) zum dispositionellen optimismus
und pessimismus,” Zeitschrift f ¨ur Gesundheitspsychologie, vol. 16, no. 1,
pp. 26–31, 2008.
[55] C. Wohlin, P. Runeson, M. H ¨ost, M. C. Ohlsson, B. Regnell, and
A. Wessl ´en,Experimentation in software engineering. Springer Science
& Business Media, 2012.
[56] J. Murray, “Likert data: what to use, parametric or non-parametric?”
International Journal of Business and Social Science, vol. 4, no. 11,
2013.
[57] J. Siegmund, N. Peitek, A. Brechmann, C. Parnin, and S. Apel, “Study-
ing programming in the neuroage: just a crazy idea?” Communications
of the ACM, vol. 63, no. 6, pp. 30–34, 2020.
[58] S. Fakhoury, “Moving towards objective measures of program com-
prehension,” in Proceedings of the 2018 26th ACM Joint Meeting on
European Software Engineering Conference and Symposium on the
Foundations of Software Engineering, 2018, pp. 936–939.
[59] N. Peitek, J. Siegmund, C. Parnin, S. Apel, J. C. Hofmeister, and
A. Brechmann, “Simultaneous measurement of program comprehension
with fmri and eye tracking: A case study,” in Proceedings of the 12th
ACM/IEEE International Symposium on Empirical Software Engineering
and Measurement, 2018, pp. 1–10.
[60] C. Eroglu and K. L. Croxton, “Biases in judgmental adjustments of
statistical forecasts: The role of individual differences,” International
Journal of Forecasting, vol. 26, no. 1, pp. 116–133, 2010.
[61] T. McElroy and K. Dowd, “Susceptibility to anchoring effects: How
openness-to-experience inﬂuences responses to anchoring cues,” Judg-
ment and Decision making, vol. 2, no. 1, pp. 48–53, 2007.
[62] B. Englich and K. Soder, “Moody experts—how mood and expertise
inﬂuence judgmental anchoring,” Judgment and Decision making, vol. 4,
no. 1, p. 41, 2009.[63] G. V . Bodenhausen, S. Gabriel, and M. Lineberger, “Sadness and
susceptibility to judgmental bias: The case of anchoring,” Psychological
Science, vol. 11, no. 4, pp. 320–323, 2000.
[64] B. A. Kitchenham, S. L. Pﬂeeger, L. M. Pickard, P. W. Jones, D. C.
Hoaglin, K. El Emam, and J. Rosenberg, “Preliminary guidelines for
empirical research in software engineering,” IEEE Transactions on
software engineering, vol. 28, no. 8, pp. 721–734, 2002.
[65] D. I. Sjoberg, B. Anda, E. Arisholm, T. Dyba, M. Jorgensen, A. Kara-
hasanovic, E. F. Koren, and M. V ok ´ac, “Conducting realistic experiments
in software engineering,” in Proceedings international symposium on
empirical software engineering. IEEE, 2002, pp. 17–26.
[66] W. F. Tichy, “Hints for reviewing empirical work in software engineer-
ing,” Empirical Software Engineering, vol. 5, no. 4, pp. 309–312, 2000.
[67] I. Salman, A. T. Misirli, and N. Juristo, “Are students representatives of
professionals in software engineering experiments?” in 2015 IEEE/ACM
37th IEEE International Conference on Software Engineering, vol. 1.
IEEE, 2015, pp. 666–676.
[68] R. Feldt, T. Zimmermann, G. R. Bergersen, D. Falessi, A. Jedlitschka,
N. Juristo, J. M ¨unch, M. Oivo, P. Runeson, M. Shepperd et al., “Four
commentaries on the use of students and professionals in empirical
software engineering experiments,” Empirical Software Engineering,
vol. 23, no. 6, pp. 3801–3820, 2018.
[69] C. R. Critcher and T. Gilovich, “Incidental environmental anchors,”
Journal of Behavioral Decision Making, vol. 21, no. 3, pp. 241–251,
2008.
[70] T. Mussweiler and B. Englich, “Subliminal anchoring: Judgmental
consequences and underlying mechanisms,” Organizational Behavior
and Human Decision Processes, vol. 98, no. 2, pp. 133–143, 2005.
[71] P. Devanbu, T. Zimmermann, and C. Bird, “Belief & evidence in
empirical software engineering,” in 2016 IEEE/ACM 38th International
Conference on Software Engineering (ICSE). IEEE, 2016, pp. 108–119.
[72] B. A. Kitchenham, T. Dyba, and M. Jorgensen, “Evidence-based soft-
ware engineering,” in Proceedings. 26th International Conference on
Software Engineering. IEEE, 2004, pp. 273–281.
[73] A. Rainer, T. Hall, and N. Baddoo, “Persuading developers to” buy into”
software process improvement: a local opinion and empirical evidence,”
in2003 International Symposium on Empirical Software Engineering,
2003. ISESE 2003. Proceedings. IEEE, 2003, pp. 326–335.
[74] D. Mendez, D. Graziotin, S. Wagner, and H. Seibold, “Open science in
software engineering,” in Contemporary Empirical Methods in Software
Engineering. Cham: Springer International Publishing, 2020, pp. 477–
501.
[75] M. Wyrich, A. Preikschat, D. Graziotin, and S. Wagner, “Replication
package - The Mind Is a Powerful Place: How Showing Code
Comprehensibility Metrics Inﬂuences Code Understanding,” Dec. 2020.
[Online]. Available: https://doi.org/10.5281/zenodo.4001763
523