Measuring Discrimination to Boost Comparative
Testing for Multiple Deep Learning Models
Linghan Meng1,2, Yanhui Li1,2,⇤, Lin Chen1,2, Zhi Wang1,2, Di Wu3, Yuming Zhou1,2, Baowen Xu1,2
1. State Key Laboratory for Novel Software Technology, Nanjing University, China
2. Department of Computer Science and Technology, Nanjing University, China
3. Momenta, Suzhou, China
{menglinghan,wangz }@smail.nju.edu.cn, {yanhuili, lchen, zhouyuming, bwxu }@nju.edu.cn, wudi@momenta.ai
Abstract —The boom of DL technology leads to massive DL
models built and shared, which facilitates the acquisition and
reuse of DL models. For a given task, we encounter multiple
DL models available with the same functionality, which are
considered as candidates to achieve this task. Testers are expected
tocompare multiple DL models and select the more suitable ones
w.r.t. the whole testing context. Due to the limitation of labeling
effort, testers aim to select an efﬁcient subset of samples to make
an as precise rank estimation as possible for these models.
To tackle this problem, we propose Sample Discrimination
based Selection (SDS) to select efﬁcient samples that could
discriminate multiple models, i.e., the prediction behaviors
(right/wrong) of these samples would be helpful to indicate the
trend of model performance. To evaluate SDS, we conduct an
extensive empirical study with three widely-used image datasets
and 80 real world DL models. The experimental results show
that, compared with state-of-the-art baseline methods, SDS is an
effective and efﬁcient sample selection method to rank multiple
DL models.
Index Terms —Testing, Deep Learning, Comparative Testing,
Discrimination
I. I NTRODUCTION
Deep learning (DL) supports a general-purpose learning
procedure that discovers high-level representations of input
samples with multiple layers of abstraction based on artiﬁcial
neural networks (ANNs), which has shown signiﬁcant advan-
tages in establishing intricate structures of high dimensional
data when tackling complex classiﬁcation tasks [13]. Along
with increases in computation power [36] and data size [7],
DL technology achieves great success in constructing deeper
layers of more effective abstraction to enhance classiﬁcation
performance, and has beaten human experts and traditional
machine-learning technology in many areas [24], including
image recognition [12], speech recognition [9], autonomous
driving [4], playing Go [29], and so on. Meanwhile, concern
about the reliability of DL models has been raised, which
calls for novel testing techniques to deal with new DL testing
scenarios and challenges.
Most current DL testing techniques try to validate the qual-
ity of DL models in two testing scenarios: debug testing and
operational testing [6], [16]. On the one hand, debug testing
considers DL testing as a technology to improve reliability by
* Yanhui Li is the corresponding and co-ﬁrst author.ﬁnding faults1[37], where various testing criteria (e.g., Neuron
Activation Coverage [23] and Neuron Boundary Coverage
[19]) have been proposed to generate or select error-inducing
inputs which trigger faults. On the other hand, operational
testing aims to make reliability assessment for DL models in
the objective testing contexts. Li et al. proposed an effective
operational testing technique to estimate the accuracy of a
single DL model by constructing probabilistic models for the
distribution of testing contexts [16].
The boom of DL technology leads to DL models with ever-
increasing functionality scale and complexity, i.e., complex DL
models combine multi-function from multiple primitive DL
models. Exposing code and data to build models and sharing
model ﬁles (e.g., h5 ﬁles) boost the acquisition of DL models,
which drive developers to build complex models by reusing
available DL models achieving speciﬁc primitive functionality.
One statistic in the previous study [10] indicates that more
than 13.7% of complex DL models on Github reuse at least
one primitive DL model. On the positive side, this “plug-
and-play” pattern [27] has greatly facilitated the construction
and application of complex DL models. On the negative side,
for a given DL task, it is tough to select suitable models
because of the advent of numerous DL models constructed
by mass developers. These multiple models are produced by
third-part developers and are trained on samples with different
distributions. Therefore, their actual performance on the target
application domain is not guaranteed, and they are needed to
be tested.
These above points expedite the emergence of a new testing
scenario “comparative testing” , where testers may encounter
multiple DL models with the same functionality built by
different developers, all of which are considered as candidates
to accomplish a speciﬁc task, and testers are expected to
rank them to choose the more suitable models in the testing
contexts. Generally speaking, comparative testing is different
from current DL testing, i.e., debug and operational testing, in
the following two points:
•The testing object is multiple DL models instead of a
single DL model;
1The faults of DL models are usually considered as the mismatching
between the real labels and predicted labels of the input samples.
3852021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)
1558-1225/21/$31.00 ©2021 IEEE
DOI 10.1109/ICSE43902.2021.00045
Developers
Model 1github.com/utkumukan/CNN-Model
Developers
Model 2github.com/coreywho/mnist-CNN
Developers
Model 3github.com/kj7kunal/MNIST-Keras
Developers
...github.com/.../...
Developers
Modelngithub.com/kiranu1024/Keras-API
TestingSamples
SuitableModelsComparative Testing for Multiple DL ModelsRank    ModelsSelectsamples
TestersTestingContextFig. 1. An example of comparative testing scenarios with multiple real world
DL models designed for written digit identiﬁcation, all of which are trained
on MNIST [14] dataset and available on GitHub. The testers need to evaluate
and rank these DL models on the testing context.
•The testing aim is comparing performances among mul-
tiple DL models instead of improving/assessing perfor-
mances for a single DL model.
Figure 1 shows an example of comparative testing scenar-
ios considering multiple real world DL models available on
GitHub. Hypothetically, in this scenario, the target application
requires an implementation of written digit identiﬁcation,
and multiple candidate DL models are found to achieve this
functionality. Testers are expected to compare the accuracy
of written digit identiﬁcation among these models and choose
the more suitable ones to meet the requirements. As stated in
many previous studies [5], [16], [19], [28], sample labeling
is the bottle neck of testing resources for DL models, which
spends much manpower and is time-consuming. Due to the
limitation of labeling effort, testers can label only a very
small part from the whole testing contexts. Therefore, as
shown in Figure 1, testers are asked to execute comparative
testing by selecting and labeling a small but efﬁcient subset of
testing samples extracted from the testing context, and ranking
multiple models based on their performance of the selected
samples.
As mentioned above, comparative testing brings out a new
problem of DL testing: given limited labeling effort, how to
select an efﬁcient subset of samples (label and test them) to
rank multiple DL models as precise as possible ? To tackle
this problem, we propose a novel algorithm named Sample
Discrimination based Selection ( SDS) to measure the sample
discrimination and select samples with higher discrimination.
The main idea of our algorithm is to focus on efﬁcient samples
that could discriminate multiple models, i.e., the prediction
behaviors (right/wrong) of these samples would be helpful to
indicate the trend of model performance. Speciﬁcally, SDS
combines two aspects of technical thoughts: majority voting
[26] in ensemble learning and item discrimination [3] in
test analysis, which are introduced to estimate the sample
discrimination with the lack of actual labels (details are inSection III).
We evaluate our approach on three widely-used image
datasets MNIST [14], Fashion-MNIST [34], and CIFAR-
10 [22], each of which contains 10000 testing samples. To
simulate the comparative testing scenarios where multiple
DL modes are developed/submitted for the same task (e.g.,
digital identiﬁcation with MNIST and clothing classiﬁcation
with Fashion-MNIST), we introduce totally 80 models from
GitHub, including 28 models for MNIST, 25 for Fashion-
MNIST, and 27 for CIFAR-10. To assess the performance
of SDS, we introduce three sample selection methods as
the baselines: one state-of-the-art method from debug testing
(DeepGini at ISSTA’2020 [5]), one state-of-the-art method
from operational testing (CES at FSE’2019 [16]) and the sim-
ple random selection (SRS). The experimental results indicate
that our algorithm SDS is an effective and efﬁcient sample
selection method for comparative testing to solve the problem
“ranking multiple DL models under limited labeling efforts ”.
Our study makes the following contributions:
•Dimension. This study opens a new dimension of DL
testing “comparative testing” for DL models, which
focuses on comparing multiple DL models instead of
improving/assessing a single DL model.
•Strategy. This paper proposes a novel selection method
SDS to measure the discrimination of samples and select
samples with higher discrimination to rank multiple DL
models.
•Study. This paper contains an extensive empirical study
of 80 models with three datasets containing 10000 testing
inputs. The experimental results indicate that compared
with the baseline methods, SDS is an effective and
efﬁcient sample selection method for comparative testing.
The rest of this paper is organized as follows. In Section II,
we introduce a motivation example to show the difference
between comparative testing and debug/operational testing. In
Section III, we present a detailed description of our algorithm
SDS. In Section IV, we present our experimental settings,
including studied datasets and models, baseline methods, re-
search questions, and so on. Section V explains experimental
results and discoveries. Section VI further discusses some
important experimental details. Sections VII and VIII are
threats to validity and related works, respectively. Section IX
presents the conclusion of our paper.
II. T HEMOTIVATION EXAMPLE
As we mentioned in Section I, the aim of comparative
testing is comparing the performances of multiple models.
Here we introduce an example to show the differences between
comparative testing and debug/operational testing.
Figure 2 presents an example of comparative testing sce-
narios containing six testing samples s1,...,s 6and three
DL models M1,M2,M3, with the prediction results of
samples predicted by models. X/⇥indicates that the pre-
diction results of these models running against samples are
right/wrong (i.e., the predicted labels are identical/different
with the actual ones). By calculating the numbers of X/⇥,
386No.Prediction resultsAccs1s2s3s4s5s6
M1XXXX⇥⇥ 4/6
M2X⇥⇥X⇥X 3/6
M3⇥⇥X⇥⇥X 2/6
Fig. 2. An example of six testing samples with prediction results under three
DL models. Xand⇥show the prediction result: right and wrong.
we can obtain the accuracies of three models, i.e.,4
6for
M1,3
6forM2, and2
6forM3, respectively. As a result,
the actual rank of accuracies ( Acc()) for these models is
Acc(M1)> Acc (M2)> Acc (M3). As shown in Figure 2,
we have the following observations:
•As only six samples are considered, we can easily ﬁnd
that the most efﬁcient subset to indicate the actual rank
of these models is S⇤={s1,s2}:M1has two Xunder
S⇤,M2has one, and M3has none. We can obtain the
same rank of models for the accuracies ( Acc0()) w.r.t. S⇤:
Acc0(M1)=2
2> Acc0(M2)=1
2> Acc0(M3)=0
2.
•S⇤is not the target sampling subset in operational testing,
as it assesses model performance imprecisely: the accu-
racyAcc0(M1)/Acc0M2)/Acc0(M3)under S⇤is2
2/1
2/0
2,
which is much different from the actual4
6/3
6/2
6.
•S⇤is also not the target sampling subset in debug testing.
Debug testing would consider s5with the highest priority,
since it triggers the mismatching behaviors of all models.
These observations indicate that the differences of aims
between comparative testing and debug/operational testing
lead to the different sampling priority. In comparative testing,
we focus on the samples that could discriminate multiple
models, e.g., s1ands2in Figure 2. In the next section, we will
introduce a novel algorithm to measure sample discrimination
and select samples with higher discrimination.
III. M ETHODOLOGY
In this section, we present the detailed description of
our approach. First, we present the studied problem. Next,
we show an algorithm named Sample Discrimination based
Selection (SDS) to measure the sample discrimination and
select samples with higher discrimination.
A. The Studied Problem
We ﬁrst introduce some symbols and deﬁnitions, which are
helpful for readers to understand the rest of our paper.
Deﬁnition 1 (DL models). A DL model Mis usually
regarded as an implementation of complex classiﬁcation task
based on the layer structure of artiﬁcial neural networks, which
achieves a function mapping the high dimensional samples s
(e.g., a gray value matrix for ﬁgures) to labels Lin a given
label set SL={L1,L2,. . . ,Lc}:M(s)2SL.
Deﬁnition 2 (Accuracy). A DL model Mis tested under
the testing context Ctcontaining samples s. Let M(s)and
L(s)be the predicted label generated by Mand the actuallabel of s, respectively. The accuracy Acc(M,Ct)ofMw.r.t.
Ctis deﬁned as follows:
Acc(M,Ct)=|{s|s2Ct,M(s)=L(s)}|
|Ct|
We introduce accuracy as the main indicator to measure the
performance for comparing multiple DL models, as it has been
widely used in evaluating the performance of DL models [16],
[20]. Based on above deﬁnitions and symbols, we present the
studied problem “given limited labeling effort, for multiple DL
models, tester aim to select an efﬁcient subset of samples (label
and test them) to rank these models as precise as possible”
speciﬁcally:
Problem. M1,M2,···,Mnare tested under the testing
context Ct, and all samples sinCtare unlabeled. Given
limited labeling effort E(E⌧| Ct|), the task is to select
and label an efﬁcient subset Cr(|Cr|=E) from Ct, and
employ the results (i.e., Acc(Mi,Cr))onCrto estimate
therank of model performance (i.e., Acc(Mi,Ct)) on
the whole testing context Ct, with an as small rank error
as possible.
B. Sample Discrimination based Selection
As shown in the motivation example, comparative testing
need samples that could discriminate the multiple models. In
this subsection, we propose a novel algorithm named Sample
Discrimination based Selection (SDS) to measure the sample
discrimination and select samples with higher discrimination.
Generally, SDS combines two aspects of technical thoughts:
•Majority voting [26]. Majority voting is a simple weight-
ing method in ensemble learning, which selects the class
with the most votes as the ﬁnal decision. As our algorithm
has the precondition that all samples are unlabeled, we
employ majority voting as a procedure to deal with the
lack of actual labels, i.e., for a given sample, we choose
the predicted label with the most models as the estimation
of the actual label.
•Item discrimination [3]. Item discrimination is an indica-
tor to describe to what extent test items can discriminate
between good and poor students, which is widely used
in test analysis2. We introduce the idea of item discrim-
ination to measure sample discrimination, i.e., estimate
discrimination by calculating the difference performance
between good and bad models under each sample.
Speciﬁcally, given multiple DL models M1,M2,···,Mn,
the testing context Ct={s1,s2,···,sm}with unlabeled
samples si, the label set SL={L1,L2,. . . ,Lc}, and the
labeling effort E, SDS is composed of the following ﬁve steps,
as shown in Algorithm 1.
Step 1: Extract prediction results. We run multiple DL
models against the testing context (line 9). For model Mi
2https://www.medsci.ox.ac.uk/divisional-services/support-services-
1/learning-technologies/faqs/what-do-difﬁculty-correlation-discrimination-
etc-in-the-question-analysis-mean
387Algorithm 1: S ample Discrimination based Selection
SDS( SM,Ct,SL)
Input: the set of DL models SM={M 1,M2,···,Mn}, the
testing context Ct={s1,s2,···,sm}with unlabeled
sample si, and the label set SL={L1,L2,. . . ,Lc}.
Output: the subset CrwithCr⇢Ctand|Cr|=E.
1initialize Cr=;;
2initialize an array Ad[1...m ]:Ad[i]=0 ,1im;
3initialize a two dimensional ( n⇥m) array Apthat stores the
prediction matrix of nmodels on msamples, i.e., Ap[i][j]
(1in,1jm) indictors that Mipredicts sjas the
labelAp[i][j], with Ap[i][j]=null;
4initialize an array Af[1...c]that stores the frequency of labels in
the prediction results with Af[k]=0 ,1kc;
5initialize an array Av[1...m ]that stores the voting labels of m
samples with Av[j]=null, 1jm;
6initialize an array As[1...n ]that stores the scores of nmodels
withAs[m]=0 ,1in;
7fori=1 tondo //1: Extract prediction results
8 forj=1 tomdo
9 runMionsjand get the prediction label Lp2SL;
10 assign the prediction label to Ap[i][j]:Ap[i][j]=Lp;
11forj=1 tomdo //2: Vote for sample labels
12 fork=1 tocdo
13 Count the frequency of Lkin the nprediction results
Ap[:,j]of sample sj:Af[k]= freq(Lk,Ap[:,j]);
14 Use the majority voting results as the actual labels:
k⇤=a r gm a x1kc{Af[k]},Av[j]=Lk⇤;
15fori=1 tondo //3: Classify top/bottom models
16 initialize score =0;
17 forj=1 tomdo
18 ifAp[i][j]=Av[j]then
19 score =score +1;
20 As[i]=score ;
21Sort nDL models in descending order by As[i];
22Select the top and the bottom 27% models into StandSb,
respectively;
23forj=1 tomdo //4: Compute sample
discrimination
24 initialize discrimination =0;
25 fori=1 tondo
26 ifMi2Stthen
27 ifAp[i][j]=Av[j]then
28 discrimination =discrimination +1;
29 else if Mi2Sbthen
30 ifAp[i][j]=Av[j]then
31 discrimination =discrimination +( 1);
32 Ad[j]=discrimination/ |St|
33Sort msamples by their discrimination Ad[j]in descending order;
//5: select with higher discrimination
34Select the top 25% samples into the candidate set Sc;
35Randomly select Esamples from ScintoCr;
36return Cr;
and sample sj, we record the predicted label Lpin the element
Ap[i][j]of the prediction matrix Ap(line 10).
Step 2: Vote for estimated labels. For any sample sj, we
compute the frequency of predicted labels created by multiple
models (line 13). We choose the predicted label with the max
frequency, i.e., majority voting, as the estimated label (line
14), which is the basic of the following steps.
Step 3: Classify top/bottom models. We employ the votedlabels to score the predicted results of DL models on samples
one by one, if the predicted label equals to the voted label,
we add one score for the current model (line 19). After we
go through all the samples, we obtain an estimated score for
this model. We sort nmodels in descending order by their
estimated score (line 21). According to the classiﬁcation in
[3], we classify nmodels into three classes (line 22): topclass
containing the top 27% models, bottom class containing the
bottom 27% models, and other class containing other models.
Step 4: Compute sample discrimination. We employ differ-
ence performance of models in top/bottom class to calculate
discrimination. Speciﬁcally, for each sample sj, the value of
discrimination is the number of models with right prediction
in top class minus the number in bottom class (line 23-31).
Intuitively, if the number in top class is much larger than the
number in bottom class, the result of this sample is more
identical with the rank, i.e., it would be helpful to estimate
the rank of model performance. Finally, we normalize and
store the sample discrimination (line 32).
Step 5: We consider the samples with higher discrimination
as the ones which are more helpful to rank multiple DL
models. To eliminate the effects of outlier samples with higher
discrimination, we introduce random selecting instead of direct
selecting from higher discrimination to lower discrimination.
Speciﬁcally, we choose 25% as the cutoff point to construct the
subset of samples with higher discrimination since quartering
is common for dataset partition in software engineering [11],
i.e., we consider the top 25% samples as the candidates (line
34) and randomly select samples from them according to the
given labeling effort (line 35).
Figure 3 shows an example of SDS running on four DL
models M1,. . . , M4with the testing context containing four
samples s1,s2,···,s4, which are classiﬁed into three classes
F,N, and⌥. Four subﬁgures show the running results of the
ﬁrst four steps3of SDS, respectively, where the entries with a
gray background indicates the target information obtained in
each step. Next, we describe the subﬁgures one by one.
•Figure 3(a) shows that SDS constructs the 4⇥4prediction
matrix, where F,N, and⌥are the predicted labels.
•Figure 3(b) presents that SDS employs majority voting to
obtain the estimation of actual labels. For example, for
s1, three models predict it as F, and one as ⌥. Therefore,
SDS adds Fas its estimated label.
•Figure 3(c) presents that SDS estimates the scores of
models based on estimated labels, e.g., since M3have
three right and one wrong prediction, M3is scored 3;
and SDS classiﬁes M1into the top class and M2into the
bottom class.
•Figure 3(d) shows SDS counts the number of models with
right prediction in top class minus the number in bottom
class, e.g., for s2, both M1andM2predict right, the
discrimination of s2is1+( 1) = 0 .
3As step 5 is easy to understand, we omit its running here.
388PredictionSNo. s1s2s3s4
M1FN⌥F ?
M2⌥NFF ?
M3FN⌥N ?
M4FF⌥F ?
L ? ? ? ?
D ? ? ? ?
(a) Extract prediction resultsPredictionSNo. s1s2s3s4
M1FN⌥F ?
M2⌥NFF ?
M3FN⌥N ?
M4FF⌥F ?
LFN⌥F
D ? ? ? ?
(b) Vote for sample labels
PredictionSNo. s1s2s3s4
M1XXXX 4:T
M2⇥X⇥X 2:B
M3XXX⇥ 3
M4X⇥XX 3
LFN⌥F
D ? ? ? ?
(c) Classify top/bottom modelsPredictionSNo. s1s2s3s4
M11 1 1 1 T
M2 --1 --1 B
M3 - - - --
M4 - - - --
LFN⌥F
D 1 0 1 0
(d) Compute sample discrimination
Fig. 3. An example of SDS running on four DL models M1,. . . , M4
with the testing context containing four samples ( s1,s2,···,s4), which are
classiﬁed into three classes ( F,N, and⌥). Due to the limited space, some
abbreviations are used in the subﬁgures (L: estimated label, S: score, D:
discrimination, T: top 27% class, B: bottom 27% class).
IV. EXPERIMENTAL SETUPS
In this section, we present the experimental setup to evaluate
the performance of SDS.
A. Studied Dataset and Models
We introduce three widely used datasets MNIST [14],
Fashion-MNIST [34], and CIFAR-10 [22] to conduct our
experiments. MNIST is a dataset of handwritten digit images
with 60000 training samples and 10000 testing samples. Sam-
ples in MNIST are 28⇥28pixel grayscale images to denote
handwritten digits from 0 to 9. Fashion-MNIST is similar with
MNIST, containing 60000 training samples and 10000 testing
samples which are 28⇥28pixel grayscale images to describe
ten types of clothing. CIFAR-10 contains 60000 32⇥32pixel
color images (50000 for training and 10000 for testing), which
are equally distributed into 10 classes, e.g., cat, dog, ship,
and truck. In summary, each dataset supports 10000 testing
samples, which are considered as the testing context in the
following experiment.
For these three datasets, we extract a large amount of (80)
models on Github, 28 models for MNIST, 25 for Fashion-
MNIST, and 27 for CIFAR-10, respectively. To simulate the
different implements of the same tasks, we choose these DL
models with different stars (from a few to tens of thousands)
on Github, different model structures, and different accuracies.
For each model, if the model ﬁles (e.g., saved as h5 ﬁle) are
provided in the repository on GitHub, we reuse them directly;
otherwise, we employ the code and data provided to train the
studied models. Table I presents the detailed description of
these studied models with their GitHub repositories, param-
eters of model structure, and actual accuracies in the testing
context. As shown in Table I, some of studied models come
from the same repository, we put them together and providethe minimum and maximum values of layers, parameters, and
accuracies among them.
B. Experimental Settings
This section will describe some details in the experimental
settings in the following aspects.
Sampling Size. As mentioned above, the labeling effort is
the bottle neck, i.e., tester are limited to label only the very
small percent of testing samples. Following the experiment
design in [16], we focus on results on each sample size from
35 to 180 with intervals of 5 (i.e., 35, 40, . . . , 180), which are
0.35%-1.8% samples selected from the whole testing context.
Baseline Method. Given multiple DL models, our goal is
to rank the performance of these models by selecting and
labeling a discriminative subset. It’s worth pointing out that,
as comparative testing is a new testing scenario proposed in
this paper, there are not existing baseline methods. To clarify
the performance of our method, we conduct comparative
experiments with three baselines: two state-of-the-art sample
selection methods (CES at FSE’2019 [16] and DeepGini at
ISSTA’2020 [5]) in current DL testing and random selection.
•CES: Li et al. proposed an effective method named CES
to select samples for DL testing to assess the accuracy of
the single DL model. We choose CES as a baseline as it
also aims to select representative subsets of sample and
reduce the labeling costs. Since CES runs based on the
single model, given nDL models, CES may construct n
selections of samples for nmodels, respectively. Here,
we introduce the best of nselections (i.e., choose the
subset that gets the highest performance of ranking) as
the result generated by CES, which is a stronger baseline
to show the advance of our method.
•DeepGini: Feng et al. proposed a technique called Deep-
Gini to help prioritize testing DL models, which mea-
sures the likelihood ⇠of misclassiﬁcation by calculating
the set impurity of prediction probabilities for multiple
classiﬁcation. DeepGini supports a deterministic baseline
method, i.e., it sorts the test samples according to the
calculated likelihood ⇠and selects the samples according
to sampling size. As SDS and CES are with randomness,
we combine random selection and DeepGini to construct
a new baseline, in which we perform random sampling in
the ﬁrst 25% (the same cutoffs in SDS) samples according
to the rank of ⇠. To differentiate these two baseline
methods, we call the the former deterministic DeepGini
(DDG), and the latter random DeepGini (RDG).
•SRS: Simple Random Sampling (SRS) is a basic method
for subset selection, which is used as baseline for many
studies [16]. We randomly select a subset from the testing
set and test the ranking performance of this subset.
We implement SDS and baseline methods in python 3.6.3
with the frameworks including Tensorﬂow 2.3.0 and Keras
2.4.3. Our experiments are performed on a Ubuntu 18.04
server with 8 GPU cores “Tesla V100 SXM2 32GB”. We pro-
vide the replication package including the detailed description
389TABLE I
THE DETAILED DESCRIPTION OF THE STUDIED 80 DL MODELS ,INCLUDING 28FOR MNIST, 25FOR FASHION -MNIST, AND 27FOR CIFAR-10.
DatasetModelGitHub WebsiteModel StructureActual AccuracyNo. Layers Params
MNIST1,2 https://github.com/Rowing0914/simple CNN mnist 8 1199882 0.9889-0.9905
3-9 https://github.com/utkumukan/CNN-Model 14 206826 0.9858-0.9916
10 https://github.com/11510880/Keras model MNIST 99.66- 31 696402 0.9912
11 https://github.com/nanguoyu/MNIST Keras CNN 12 600810 0.992
12 https://github.com/coreywho/mnist-CNN 9 79280 0.9919
13 https://github.com/kj7kunal/MNIST-Keras 20 330730 0.9925
14 https://github.com/gee842/MNIST-Models 19 327242 0.9959
15 https://github.com/Aishuvenkat09/Predictions-using-Mnist-Model 8 1199882 0.9915
16-22 https://github.com/keras-team/keras 8 151306-1199882 0.9883-0.9922
23-28 https://github.com/avicorp/AmountRecognition/ 8-9 7218-444986 0.8818-0.9853
Fashion-MNIST1,2 https://github.com/avicorp/AmountRecognition/ 31 258826 0.9096-0.9254
3 https://github.com/fwsdonald/classiﬁcation-of-Fashion-Mnist 12 329962 0.9315
4,5 https://github.com/Sukhman75/Tensorﬂow Keras fashion mnist 11-13 356234-1199882 0.9195-0.9335
6-20 https://github.com/zsoltzombori/keras fashion mnist tutorial 7-31 693962-258826 0.9046-0.9338
21-25 https://github.com/zk31601102/FGSM-fashion-mnist 7 931080-1256080 0.9009-0.9273
CIFAR-101-3 https://github.com/kiranu1024/Keras-API 18-72 274442-1250858 0.7238-0.8000
4,5 https://github.com/uchidama/CIFAR10-Prediction-In-Keras 18-72 274442-1250858 0.7747-0.8527
6-9 https://github.com/Ken-Leo/BIGBALLONcifar-10-cnn 8-65 62006-39002738 0.7149-0.7529
10,11 https://github.com/hemrajchauhan/CIFAR10 Keras 12 1250858 0.7701-0.7926
12-15 https://github.com/night18/cifar-10-AlexNet 11-13 1248554-2883178 0.7009-0.7336
16 https://github.com/sahilunagar/CIFAR-10-image-classiﬁcation-using-
CNN-model-in-keras15 2122186 0.7362
17 https://github.com/saranshmanu/CIFAR-Image-Classiﬁcation 19 781992 0.7536
18,19 https://github.com/GodfatherPacino/CNN CIFAR 8-19 2915114-4210090 0.7091-0.7944
20 https://github.com/sonamtripathi/simple cnnmodel keras cifar10 dataset 12 1250858 0.7859
21 https://github.com/percent4/resnet 4cifar10 72 274442 0.7622
22-27 https://github.com/BIGBALLON/cifar-10-cnn 113 470218 0.7748-0.8081
of our proposed methods SDS and source code online (see
Section X).
Repetition. As SDS and several baseline methods are with
randomness, we conduct the experiment 50 times and report
the average of calculated results.
C. Evaluation Indicators
To evaluate to what extend the estimated rank w.r.t. selected
samples are identical with the actual rank w.r.t. the whole
testing context, we introduce two indicators Spearman’s rank
correlation coefﬁcient and Jaccard similarity coefﬁcient.
Spearman’s rank correlation coefﬁcient ⇢is a measure of
the correlation between two variables XandY[1], [31]. It
can be calculated by the following formula:
⇢=P
i(xi ¯x)(yi ¯y)pP
i(xi ¯x)2P
i(yi ¯y)2
The value of ⇢ranges from -1 to 1: the closer it is to 1 (-1),
the two sets of variables are positively (negatively) correlated.
Besides, we introduce Jaccard similarity coefﬁcient [32],
[39] (denoted as Jk) to evaluate the similarity between the top-
kmodel sets generated by the estimated rank and actual rank.
For example, the estimated rank is M1,M3,M5,···, and
the actual rank is M1,M3,M2,···. The Jaccard similarity
coefﬁcient between two top-3 model sets {M 1,M3,M5}and
{M 1,M2,M3}is calculated as:
J3=|{M 1,M3,M5}\{M 1,M2,M3}|
|{M 1,M3,M5}[{M 1,M2,M3}|=2
4=0.5
As we encounter dozens of models in the testing context (e.g.,
28 models for MNIST), we focus on k=1,3,5,10to evaluatethe performance of our method on different cutoff points. We
takek= 10 as the representative to report the evaluation under
Jaccard similarity coefﬁcient in the experimental results. The
other Jaccard coefﬁcient (when k=1,3,5) will be discussed
in the discussion part.
D. Analysis Method
First, we employ Wilcoxon rank sum test [2] to verify the
difference of the rank performance between our method and
the baselines. If the p-value are less than 0.05, the two sets of
data are considered signiﬁcantly different.
Next, we introduce Cliff’s delta  [25], which measures the
effect size for comparing two ordinal data lists. We judge the
difference between the two sets of data based on the range of
 : negligible, if | |<0.147; small, if 0.147| |<0.330;
medium, if 0.330| |<0.474, and large, if | | 0.474.
Finally, we use “W/T/L” [17], [21] to compare the results
of our approach and the baseline, where “W” means our
approach wins, “T” means the results are tie, and “L” means
our approach loses. Reaching the two standards shows that our
approach wins: (a) the p-value of Wilcoxon rank sum test is
less than 0.05 ( p< 0.05), which means the results between
our approach and baseline are signiﬁcantly different; (b) the
Cliff’s delta  is larger than 0.147 (  >0.147), which means
the difference between the two results are positive and not
negligible. If p< 0.05and  < 0.147, we consider our
approach loses. Otherwise, the result of comparision is tie.
E. Research Questions
We are committed to promoting the ranking performance
of the multiple models under limited labeling effort in the
390(a) Spearman coefﬁcient of MNIST
 (b) Spearman coefﬁcient of FASHION-MNIST
 (c) Spearman coefﬁcient of CIFAR-10
(d) Jaccard coefﬁcient of MNIST
 (e) Jaccard coefﬁcient of FASHION-MNIST
 (f) Jaccard coefﬁcient of CIFAR-10
Fig. 4. The results of our approach and the four baselines for ranking model performance under Spearman coefﬁcient and Jaccard coefﬁcient (k =1 0 ). In
each subﬁgure, the x-axis indicates the number of samples, from 35 to 180, and the y-axis represents the values of Spearman/Jaccard coefﬁcient. These ﬁve
studied methods are denoted by lines with different colors, i.e., ⌅for SDS, ⌅for CES, ⌅for DDG, ⌅for RDG, and ⌅for SRS.
comparative testing scenario. We propose the following two
research questions (RQs) to organize our experiments:
•RQ1 (Effectiveness): Whether our method SDS can sur-pass the state-of-the-art methods in ranking multiplemodels?
•RQ2 (Efﬁciency): Compared with the state-of-the-artmethods, is our method SDS efﬁcient?
V. EXPERIMENT RESULTS
In this section, we present the results of the experiments
and answer the above two RQs.
A. RQ1: Effectiveness
Motivation and Approach. Our problem is to obtain
effective ranking results of model performance with a very
low labeling percent of testing samples for multiple models inthe testing scenario. We hope to verify whether our proposedapproach SDS is more effective than the baseline methodswith limited labelling effort. To achieve this aim, we comparethe ranking performance of SDS with the baseline methods inthree testing contexts (containing the 10000 testing samplesfrom MNIST, Fashion-MNIST, and CIFAR-10, respectively)under the sampling sizes from 35 to 180 with intervals of5 (i.e., 35, 40, . . . , 180). Speciﬁcally, we employ these ﬁvestudied methods to sample the subset under different samplingsizes, and use the ranking result on the subset to estimate therank on the whole testing context. We repeat running thesemethods 50 times, and report the average of calculated results.Results. Figure 4 shows the comparison results of our ap-
proach and the four baselines for ranking model performance.The three subgraphs in the ﬁrst row show the comparisonresults of Spearman coefﬁcient ⇢, and the subgraphs in the
second row present the results of Jaccard coefﬁcient
4(J10).
In each subﬁgure, the x-axis indicates the number of samples,
from 35 to 180, and the y-axis represents the values of
Spearman/Jaccard coefﬁcient. These ﬁve studied methods aredenoted by lines with different colors, i.e., ⌅for SDS, ⌅for
CES,⌅for DDG, ⌅for RDG, and ⌅for SRS. It can be
seen in Figure 4 that in all sub-graphs, our method SDS isobviously better than the other baselines under all samplingsizes from 35 to 180. Besides, our approach is very stable;on the contrary, some baselines have a strong volatility, e.g.,DDG has wild gyrations when measuring Jaccard coefﬁcientfor MINIST, Fashion-MINIST, and CIFAR-10.
In order to show more details of the experiment results, we
choose six sampling points (35, 60, 90, 120, 150, and 180) asthe representatives. Table II presents the detailed results underthese six points, with the mean values of Spearman coefﬁcientand Jaccard coefﬁcient of ranking multiple models obtained by50 repetitions of running the ﬁve studied methods. The bestnumbers are highlighted in bold. In Table II, if our approachwins the baseline method (that is to say, the pvalue is less
than 0.05 and the  is greater than 0.147), then we add the
4We take k=1 0 as the representative to report the evaluation under Jaccard
similarity coefﬁcient in the experimental results. The other Jaccard coefﬁcient
(when k=1,3,5) will be discussed in the discussion part.
391TABLE II
THE RESULTS OF SPEARMAN CORRELATION AND JACCARD CORRELATION (J10)WITH OUR METHOD AND FOUR BASELINE METHODS .
Indicator CutoffMNIST FASHION-MNIST CIFAR-10
SRS CES RDG DDG SDS SRS CES RDG DDG SDS SRS CES RDG DDG SDS
Spearman35 0.563 0.593 0.541 0.603 0.635 0.298 0.336 0.233 0.051 0.578 0.533 0.562 0.179 0.393 0.790
60 0.587 0.636 0.595 0.447 0.673 0.359 0.367 0.315 0.382 0.670 0.657 0.685 0.211 0.192 0.845
90 0.628 0.671 0.630 0.405 0.708 0.386 0.452 0.368 0.335 0.743 0.711 0.738 0.226 0.061 0.872
120 0.645 0.684 0.634 0.405 0.747 0.461 0.552 0.407 0.249 0.777 0.753 0.785 0.235 -0.029 0.892
150 0.650 0.699 0.637 0.328 0.771 0.492 0.589 0.460 0.436 0.798 0.801 0.820 0.243 0.141 0.898
180 0.659 0.682 0.635 0.308 0.788 0.532 0.629 0.503 0.618 0.821 0.819 0.842 0.230 0.306 0.904
Average 0.622 0.661 0.612 0.416 0.720 0.421 0.487 0.381 0.345 0.731 0.713 0.739 0.221 0.177 0.867
W/T/L 6/0/0 3/3/0 6/0/0 6/0/0 / 6/0/0 6/0/0 6/0/0 6/0/0 / 6/0/0 6/0/0 6/0/0 6/0/0 /
Jaccard35 0.254 0.288 0.207 0.177 0.353 0.354 0.362 0.338 0.333 0.493 0.392 0.430 0.210 0.177 0.536
60 0.317 0.347 0.221 0.177 0.413 0.401 0.420 0.346 0.333 0.536 0.467 0.490 0.264 0.333 0.568
90 0.342 0.375 0.232 0.333 0.422 0.394 0.434 0.354 0.333 0.592 0.548 0.568 0.289 0.250 0.619
120 0.372 0.406 0.248 0.333 0.462 0.435 0.468 0.397 0.177 0.617 0.554 0.608 0.328 0.177 0.631
150 0.381 0.437 0.264 0.111 0.488 0.451 0.502 0.401 0.250 0.621 0.603 0.627 0.330 0.177 0.635
180 0.394 0.459 0.262 0.111 0.481 0.473 0.534 0.424 0.429 0.631 0.628 0.654 0.324 0.333 0.656
Average 0.344 0.385 0.239 0.207 0.436 0.418 0.453 0.377 0.309 0.582 0.532 0.563 0.291 0.241 0.607
W/T/L 6/0/0 4/2/0 6/0/0 6/0/0 / 6/0/0 6/0/0 6/0/0 6/0/0 / 4/2/0 2/4/0 6/0/0 6/0/0 /
gray background to the value of the baseline method. Based
on the values of Spearman and Jaccard coefﬁcients, we have
added two rows: “Average” to calculate the average value of
each column and “W/T/L” to record the number of times our
approach win/tie/lose other baselines.
From Table II, we have the following observations. (a) From
the average value of each point, our approach is higher than
all baselines, under both Spearman coefﬁcient and Jaccard co-
efﬁcient. (b) The gray background indicates that our approach
wins other baselines at the most of points. (c) The results of
W/T/L shows that our approach is not only higher than other
baselines in mean, but also signiﬁcantly better.
Answer to RQ1: In ranking multiple DL models, our
approach is signiﬁcantly better than all other baselines
in effectiveness.
B. RQ2: Efﬁciency
Motivation and Approach. In RQ1, we have observed that
our approach SDS is signiﬁcantly better than other baselines
under both Spearman coefﬁcient and Jaccard coefﬁcient in
raking multiple DL models. The process of sample selection
may be time consuming. In this RQ, we want to check the
efﬁciency of our approach compared with other baselines.
Results. Table III shows the total time consumed when
running studied methods with sampling from 35 to 180. From
Table 3, we ﬁnd that our approach SDS takes longer than
SRS because it contains sample sorting and operations on the
prediction matrix. The time SDS consumes is similar to the
other three baselines CES, RDG, and DDG, which is around
10,000 seconds.
Answer to RQ2: Except for SRS, our approach is similar
to other baselines in time consumption.TABLE III
THE TIME (SECOND )CONSUMED WHEN SAMPLES ARE SELECTED BY
DIFFERENT APPROACHES .
dataset SRS CES RDG DDG SDS
MNIST 1,117 3,513 11,334 9,493 10,256
Fashion-MNIST 1,403 31,703 10,082 8,538 9,179
CIFAR-10 4,799 15,347 11,679 9,669 10,642
VI. D ISCUSSION
In this section, we further discuss some parameter settings
and results in the experiments. First, we analyze the parameter
and indicator involved in the experiments. After that, we
discuss why our algorithm can effectively help multi-model
performance ranking and whether our method is effective
when the number of models is reduced.
A. The performance under other selection rates
In our experiment, the random sampling interval is set to
the top 25% as shown in Step 5 of Algorithm 1. We want
to further discuss the performance under other selection rates
by conducting experiments on ﬁve different selection rates
(i.e., random sampling of the top 15%, 20%, 25%, 30%, and
35% intervals). The results are shown in Figure 5, where
the performances of different rates are denoted by line with
different colors, i.e., ⌅for 15%, ⌅for 20%, ⌅for 25%, ⌅
for 30%, and ⌅for 35%, respectively.
It can be seen that the performances of different selection
rates on different datasets vary a lot. Generally speaking, there
is no obvious trend in all subﬁgures. In addition, the 25% sam-
pling interval (the green line) we set in the experiment obtains
the best ranking performance under the most of sampling sizes
in the CIFAR-10 dataset. As quartering is common for dataset
partition in software engineering and easy to implement [11],
we still suggest applying the 25% interval in our algorithm.
B. The performance of Jaccard coefﬁcient with k=1,3,5
We employ Jaccard coefﬁcient to measure the similarity
between the two top- kmodel sets generated by the selected
392(a) Spearman of MNIST
 (b) Jaccard of MNIST
(c) Spearman of FASHION-MNIST
 (d) Jaccard of FASHION-MNIST
(e) Spearman of CIFAR-10
 (f) Jaccard of CIFAR-10
Fig. 5. The graph of ranking performance when the random selection rates
changes from top 15% - top 35%. The performances of different rates are
denoted by line with different colors, i.e., ⌅for 15%, ⌅for 20%, ⌅for 25%,
⌅for 30%, and ⌅for 35%.
subset and the whole testing context, respectively. In the
previous experiments, when we use the Jaccard coefﬁcient,
we calculate it with k= 10 . In this section, we will discuss
whether our method has advantages when the values of kare
different, i.e., k=1,3,5. Due to space limitation, we cannot
display all the 3 ⇥3 (the former 3 for the three datasets and the
latter 3 for k=1,3,5) subgraphs, we calculate the average of
the three datasets in three subgraphs for k=1,3,5in Figure 6.
As shown in Figure 6, we compare our approach SDS (the
green line) with other baselines when k=1,3,5. Figure 6
shows the average values of the Jaccard coefﬁcient of the three
datasets when the sampling changes. It can be seen that our
approach still has advantages under the most of points, which
shows that our approach is still superior in ranking models
when considering k=1,3,5.
C. Analysis and insight of our algorithm
In this section we will discuss why our algorithm works. In
order to illustrate this point, we conduct a two-step analysis.
The ﬁrst is to measure the precision of the majority voting.
We compare estimated labels obtained by the majority voting
with true labels. Figure 7 shows the matched rate of estimated
labels with true labels when the majority voting gets different
numbers of votes. It can be seen that as the number of votes
(a) Jaccard coefﬁcient J1(k=1)
(b) Jaccard coefﬁcient J3(k=3)
(c) Jaccard coefﬁcient J5(k=5)
Fig. 6. The graphs for k=1,3,5in measuring Jaccard coefﬁcient with
the top- kmodel sets generated by the selected subset and the whole testing
context.
(a) MNIST
 (b) Fashion-MNIST
 (c) CIFAR-10
Fig. 7. The histogram of matched rate when the votes changes. The red line
represents the matched rate on the entire data set.
obtained increases, the matched rate also rises. In general,
the average matched rate of majority voting results with the
true labels reaches 0.9924 for MNIST, 0.9433 for Fahion-
MNIST, and 0.8613 for CIFAR-10, respectively, as shown by
the red line in each subﬁgure. In other words, majority voting
is close to the true label, which is the key to explain why
our method is effective. This ﬁnding leads to an insight for
following studies in comparative testing: the distribution of
predicted labels would be helpful to deal with the lack of actual
labels, which is a main difﬁculty in actual testing scenarios due
to the limitation of labelling effort . We encourage following
researchers to employ more effective methods to measure the
distribution in comparative testing.
In the second step, we analyze whether the sample discrim-
ination is positively correlated to the ranking performance,
i.e., whether higher discrimination is more helpful for ranking
multiple DL models. We conduct an additional experiment.
After sorting the samples according to the discrimination, we
randomly select samples in the top 25%, the 25%-50%, the
50%-75%, and the 75%-100% intervals to observe the results
of ranking performance. We take the averages of the three
datasets and show them in the Figure 8, the blue line represents
393(a) Spearman
 (b) Jaccard
Fig. 8. The graph of four intervals for random sampling (the ﬁrst 25%, 25%-
50%, 50%-75%, and 75%-100%) to show that the sample discrimination is
positively correlated to the ranking performance.
Fig. 9. Comparison result of ranking performance of SDS, SRS, and CES
when the number of models is 4.
the random sampling in the ﬁrst 25% interval, which is the
interval used in our experiment. It can be seen that the model
ranking effectiveness of random sampling in the ﬁrst 25% is
signiﬁcantly better than other intervals. That indicates higher
discrimination is more helpful for ranking multiple DL models.
D. The performance when there are fewer models
The previous experiment content is to calculate the ranking
performance of the SDS method when the number of models
is large (i.e., more than 20 models for a given task). In this
section, we report the performance of SDS on the model
ranking when there are few models. We have selected four
models in each data set to compare the ranking effect of SDS
and other baselines. We measure the Spearman coefﬁcient5
value when the sample size is from 35 to 180. The experiment
was repeated 50 times, and the average results were reported.
Figure 9 shows the comparison results of SDS, SRS, and CES,
which are the best three methods when the number of models
is large. Figure 9 presents the average ranking performance
on the three data sets, where the green curve denotes the
Spearman coefﬁcient of SDS.
We observe that SDS can still show superior performance
when there are fewer models, which obviously exceeds SRS
and CES. To some extent, the above result shows the gener-
alization of the SDS method.
5As there are only four models, Jaccard coefﬁcient ( k=1 0 ) is not
applicable here. We focus on Spearman coefﬁcient.
Fig. 10. Comparison result of ranking performance of SDS with ranking
performance when majority voting is used as the real label.
E. The ranking performance when choosing majority voting
as true labels
An intuitive idea is to use the labels obtained by the majority
voting as the true labels to measure the accuracy of the models,
and then get the ranking performance of the models (see
line 21 of Algorithm 1). In this section, we compare this
intuitive method with the results of SDS to verify whether
the calculation after line 21 in Algorithm 1 really plays a role
in model ranking.
We show the results of the comparison in Figure 10. The
blue curve in the ﬁgure is the average result of the spearman
coefﬁcient on the three data sets that vary with the sample
size, and the red line is the ranking result obtained by using
the majority voting results as the true labels, which is also the
average result of the three data sets.
It can be seen from Figure 10 that SDS overcomes majority
voting when the sample size is larger than 105, which is
roughly about one percent (i.e., 10000*1%=100) of the total
test set. Besides, the curve after this point still shows an
upward trend along with the increasing of sampling size. That
is to say, the calculation content after line 21 in Algorithm 1
is useful for the model ranking.
VII. THREAD TO V ALIDITY
The threat to validity is discussed in the following three
aspects for our study.
First, the datasets we select may be a threat. We use three
well-known graph classiﬁcation datasets, which are widely
used in many studies, but their complexity is not high. In the
future, we will explore on larger and more diverse datasets to
validate the effectiveness of our algorithm.
Second, the selection of models in the experiments could
become a threat. We try to choose a wide range of models on
GitHub, i.e., 28 models for MNIST, 25 for Fashion-MNIST,
and 27 for CIFAR-10, respectively, which include multiple DL
models with different stars (from a few to tens of thousands)
on Github, different model structures, and different accuracies.
However, these studied 80 models may not fully cover the real
situation. More models are expected in the following studies
to validate our results.
Finally, it may also be a threat to the implementation of
the models. As discussed earlier, if the trained model ﬁle is
394provided in the GitHub repository, we will use it directly,
otherwise we will use the provided python code and datasets
for training. Due to the difference in the training environment,
it may cause the reproduced model to be different from
the original one. For new trained models, we compare the
accuracies announced in the GitHub repository and actual
accuracies, and ﬁnd that the difference between them is slight.
VIII. RELATEDWORK
In this section, we introduce the related work. In the angel
of traditional software testing [6], on the one side, testing aims
to ﬁnd more bugs, which is called debug testing; on the other
side, testing aims to make reliability assessment of software
through conditioning, which is called operational testing.
The main body of current DL testing is to focus on debug
testing, i.e., the main aim is to ﬁnd bugs. Pei et al. proposed a
whitebox framework named DeepXplore, which uses neuron
coverage as the standard for DL model testing [23]. Tian et
al. implemented a tool named DeepTest to simulate the real
world to help ﬁnd behaviors that may cause accidents for
DNN-driven vehicles [32]. Zhang et al. proposed unsupervised
framework for DNN named DeepRoad, and utilized GANs
and metamorphic testing to test the inconsistent behaviors
in self-driving car [37]. Xie et al. proposed a coverage-
based framework named DeepHunter which used metamorphic
mutation to help ﬁnd defects for DNNs [35]. Sun et al.
presented an approach named TransRepair to help machine
translation systems test and repair inconsistency bugs [30].
Ma et al. proposed a set of testing criteria named DeepGauge
for measuring the testing adequacy of DNNs [19]. Ma et al.
proposed DeepCT, which applied the idea of combinatorial
testing to DL testing, and produced a series of combinatorial
testing criteria for DL systems [18]. Tian et al. developed a
technique called DeepInspect, which can detect the confusion
and bias errors based class for image classiﬁcation [33]. Lee
et al. presented a white-box testing approach named ADAPT,
which used an adaptive neuron selection strategy to ﬁnd
adversarial inputs [15].
Meanwhile, researchers have focused on the other aspects
of DL testing. Li et al. proposed an effective operational
testing technique to estimate the accuracy of the DL model
by constructing probabilistic models for the distribution of
testing contexts [16]. To evaluate the quality of test data, Ma
et al. applied the mutation framework to DL systems, and
proposed a technique named DeepMutation [20]. Zhou et al.
proposed a testing approach for the systematic physical world
called DeepBillboard, which is aimed to generate adversarial
test more robust [38]. Gerasimou et al. proposed a systematic
testing approach named DeepImportance, which is mixed with
an Importance-Driven (IDC) test adequacy criterion to support
more robust DL systems [8].
IX. CONCLUSION
The boom of DL technology leads to the reuse of DL
models, which expedites the emergence of a new testing
scenario comparative testing , where testers may encountermultiple DL models with the same functionality as candidates
to accomplish a speciﬁc task, and testers are expected to
rank them to choose the more suitable models in the testing
contexts. Due to the limitation of labeling effort, this testing
scenario brings out a new problem of DL testing: ranking
multiple DL models under limited labeling efforts .
To tackle this problem, we propose a novel algorithm named
Sample Discrimination based Selection ( SDS) to measure
the sample discrimination and select samples with higher
discrimination. We evaluate our approach on three widely-
used image datasets and 80 DL models. Our results lead us to
conclude that SDS is an effective and efﬁcient sample selection
method for comparative testing to rank multiple DL models.
Finally, we would like to emphasize that we do not seek to
claim the advantage of our method SDS. Instead, the key mes-
sages are that (a) a new testing scenario comparative testing
is introduced by our paper, where the testing aims are much
different with the current DL testing, i.e., debug/operational
testing; (b) the new testing scenario brings out the new testing
challenge ranking multiple DL models under limited labeling
efforts ; (c) our proposed method SDS leads to the insight
which would be helpful for the following researchers.
X. REPEATABILITY
We provide all datasets and code used to conduct this study
at https://github.com/Testing-Multiple-DL-Models/SDS.
ACKNOWLEDGEMENTS
The work is supported by National Key R&D Program
of China (Grant No. 2018YFB1003901) and the National
Natural Science Foundation of China (Grant No. 61872177,
61832009, 61772259, 61772263, and 61932012). We thank
the anonymous referees for their helpful comments on this
paper.
REFERENCES
[1]M. Beller, G. Gousios, A. Panichella, and A. Zaidman. When, how,
and why developers (do not) test in their ides. In Proceedings of the
2015 10th Joint Meeting on Foundations of Software Engineering , pages
179–190, 2015.
[2]L. D. Capitani and D. D. Martini. On stochastic orderings of the
wilcoxon rank sum test statistic with applications to reproducibility prob-
ability estimation testing. Statistics and Probability Letters , 81(8):937–
946, 2011.
[3]R. L. Ebel. Procedures for the analysis of classroom tests. Educational
and Psychological Measurement , 14(2):352–364, 1954.
[4]C. Farabet, C. Couprie, L. Najman, and Y. LeCun. Scene parsing with
multiscale feature learning, purity trees, and optimal covers. arXiv
preprint arXiv:1202.2160 , 2012.
[5]Y. Feng, Q. Shi, X. Gao, J. Wan, C. Fang, and Z. Chen. Deepgini: Prior-
itizing massive tests to enhance the robustness of deep neural networks.
InProceedings of the 29th ACM SIGSOFT International Symposium on
Software Testing and Analysis , ISSTA 2020, page 177–188, New York,
NY, USA, 2020. Association for Computing Machinery.
[6]Frankl, Phyllis, G., Hamlet, Richard, G., Littlewood, Bev, Strigini, and
Lorenzo. Evaluating testing methods by delivered reliability. IEEE
Transactions on Software Engineering , 1998.
[7]A. Geiger, P. Lenz, and R. Urtasun. Are we ready for autonomous
driving? the kitti vision benchmark suite. In 2012 IEEE Conference
on Computer Vision and Pattern Recognition , pages 3354–3361. IEEE,
2012.
[8]S. Gerasimou, H. F. Eniser, A. Sen, and A. Cakan. Importance-driven
deep learning system testing. CoRR , abs/2002.03433, 2020.
395[9]G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly,
A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, et al. Deep neural
networks for acoustic modeling in speech recognition: The shared views
of four research groups. IEEE Signal processing magazine , 29(6):82–97,
2012.
[10] Y. Ji, X. Zhang, S. Ji, X. Luo, and T. Wang. Model-reuse attacks on deep
learning systems. In Proceedings of the 2018 ACM SIGSAC Conference
on Computer and Communications Security , CCS ’18, page 349–363,
New York, NY, USA, 2018. Association for Computing Machinery.
[11] J. Kim, R. Feldt, and S. Yoo. Guiding deep learning system testing using
surprise adequacy. In Proceedings of the 41st International Conference
on Software Engineering , ICSE ’19, pages 1039–1049, Piscataway, NJ,
USA, 2019. IEEE Press.
[12] Krizhevsky, Alex, Sutskever, Ilya, Hinton, and E. Geoffrey. Imagenet
classiﬁcation with deep convolutional neural networks. Communications
of the ACM , 2017.
[13] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. nature ,
521(7553):436, 2015.
[14] Y. LeCun and C. Cortes. The mnist database of handwritten digits.
http://yann.lecun.com/exdb/mnist/, 2019. Accessed May 4, 2019.
[15] S. Lee, S. Cha, D. Lee, and H. Oh. Effective white-box testing of deep
neural networks with adaptive neuron-selection strategy. In S. Khurshid
and C. S. Pasareanu, editors, ISSTA ’20: 29th ACM SIGSOFT Interna-
tional Symposium on Software Testing and Analysis, Virtual Event, USA,
July 18-22, 2020 , pages 165–176. ACM, 2020.
[16] Z. Li, X. Ma, C. Xu, C. Cao, J. Xu, and J. L ¨u. Boosting operational
dnn testing efﬁciency through conditioning. In Proceedings of the 2019
27th ACM Joint Meeting on European Software Engineering Conference
and Symposium on the Foundations of Software Engineering , ESEC/FSE
2019, page 499–509, New York, NY, USA, 2019. Association for
Computing Machinery.
[17] Y. Liu, Y. Li, J. Guo, Y. Zhou, and B. Xu. Connecting software metrics
across versions to predict defects. In 2018 IEEE 25th International Con-
ference on Software Analysis, Evolution and Reengineering (SANER) ,
pages 232–243. IEEE, 2018.
[18] L. Ma, F. Juefei-Xu, M. Xue, B. Li, L. Li, Y. Liu, and J. Zhao.
Deepct: Tomographic combinatorial testing for deep learning systems.
In X. Wang, D. Lo, and E. Shihab, editors, 26th IEEE International
Conference on Software Analysis, Evolution and Reengineering, SANER
2019, Hangzhou, China, February 24-27, 2019 , pages 614–618. IEEE,
2019.
[19] L. Ma, F. Juefei-Xu, F. Zhang, J. Sun, M. Xue, B. Li, C. Chen, T. Su,
L. Li, Y. Liu, J. Zhao, and Y. Wang. Deepgauge: multi-granularity testing
criteria for deep learning systems. In Proceedings of the 33rd ACM/IEEE
International Conference on Automated Software Engineering, ASE
2018, Montpellier, France, September 3-7, 2018 , pages 120–131, 2018.
[20] L. Ma, F. Zhang, J. Sun, M. Xue, B. Li, F. Juefei-Xu, C. Xie, L. Li,
Y. Liu, J. Zhao, and Y. Wang. Deepmutation: Mutation testing of
deep learning systems. In 2018 IEEE 29th International Symposium
on Software Reliability Engineering (ISSRE) , pages 100–111, Oct 2018.
[21] J. Nam, W. Fu, S. Kim, T. Menzies, and L. Tan. Heterogeneous defect
prediction. IEEE Transactions on Software Engineering , 44(9):874–896,
Sep. 2018.
[22] N.Krizhevsky, H.Vinod, C.Geoffrey, M.Papadakis, and A.Ventresque.
The cifar-10 dataset. http://www.cs.toronto.edu/ ⇠kriz/cifar.html. Ac-
cessed May 4, 2019.
[23] K. Pei, Y. Cao, J. Yang, and S. Jana. Deepxplore: Automated whitebox
testing of deep learning systems. In Proceedings of the 26th Symposium
on Operating Systems Principles, Shanghai, China, October 28-31,
2017 , pages 1–18, 2017.[24] S. Pouyanfar, S. Sadiq, Y. Yan, H. Tian, Y. Tao, M. P. Reyes, M.-L. Shyu,
S.-C. Chen, and S. Iyengar. A survey on deep learning: Algorithms,
techniques, and applications. ACM Computing Surveys (CSUR) , 51(5):1–
36, 2018.
[25] J. Romano, J. D. Kromrey, J. Coraggio, J. Skowronek, and L. Devine.
Exploring methods for evaluating group differences on the nsse and
other surveys: Are the t-test and cohen’s d indices the most appropriate
choices. In In annual meeting of the Southern Association for Institu-
tional Research , 2006.
[26] O. Sagi and L. Rokach. Ensemble learning: A survey. Wiley Interdis-
ciplinary Reviews: Data Mining and Knowledge Discovery , 8(4):e1249,
2018.
[27] D. Sculley, G. Holt, D. Golovin, E. Davydov, T. Phillips, D. Ebner,
V. Chaudhary, M. Young, J.-F. Crespo, and D. Dennison. Hidden
technical debt in machine learning systems. In Advances in neural
information processing systems , pages 2503–2511, 2015.
[28] W. Shen, Y. Li, L. Chen, Y. Han, Y. Zhou, and B. Xu. Multiple-boundary
clustering and prioritization to promote neural network retraining. In
2020 35th IEEE/ACM International Conference on Automated Software
Engineering (ASE) , pages 410–422, 2020.
[29] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van
Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam,
M. Lanctot, et al. Mastering the game of go with deep neural networks
and tree search. nature , 529(7587):484–489, 2016.
[30] Z. Sun, J. M. Zhang, M. Harman, M. Papadakis, and L. Zhang.
Automatic testing and improvement of machine translation. CoRR ,
abs/1910.02688, 2019.
[31] P. Thongtanunam and A. E. Hassan. Review dynamics and their impact
on software quality. IEEE Transactions on Software Engineering , 2020.
[32] Y. Tian, K. Pei, S. Jana, and B. Ray. Deeptest: Automated testing of
deep-neural-network-driven autonomous cars. In Proceedings of the 40th
international conference on software engineering , pages 303–314, 2018.
[33] Y. Tian, Z. Zhong, V. Ordonez, and B. Ray. Testing deep neural network
based image classiﬁers. CoRR , abs/1905.07831, 2019.
[34] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image
dataset for benchmarking machine learning algorithms. arXiv preprint
arXiv:1708.07747 , 2017.
[35] X. Xie, L. Ma, F. Juefei-Xu, M. Xue, H. Chen, Y. Liu, J. Zhao,
B. Li, J. Yin, and S. See. Deephunter: a coverage-guided fuzz testing
framework for deep neural networks. In D. Zhang and A. Møller, editors,
Proceedings of the 28th ACM SIGSOFT International Symposium on
Software Testing and Analysis, ISSTA 2019, Beijing, China, July 15-19,
2019 , pages 146–157. ACM, 2019.
[36] Y. You, A. Buluc ¸, and J. Demmel. Scaling deep learning on gpu and
knights landing clusters. In Proceedings of the International Conference
for High Performance Computing, Networking, Storage and Analysis , SC
’17, pages 9:1–9:12, New York, NY, USA, 2017. ACM.
[37] M. Zhang, Y. Zhang, L. Zhang, C. Liu, and S. Khurshid. Deeproad:
Gan-based metamorphic testing and input validation framework for
autonomous driving systems. In Proceedings of the 33rd ACM/IEEE
International Conference on Automated Software Engineering, ASE
2018, Montpellier, France, September 3-7, 2018 , pages 132–142, 2018.
[38] H. Zhou, W. Li, Y. Zhu, Y. Zhang, B. Yu, L. Zhang, and C. Liu.
Deepbillboard: Systematic physical-world testing of autonomous driving
systems. CoRR , abs/1812.10812, 2018.
[39] Z. Q. Zhou, S. Xiang, and T. Y. Chen. Metamorphic testing for software
quality assessment: A study of search engines. IEEE Transactions on
Software Engineering , 42(3):264–284, 2015.
396