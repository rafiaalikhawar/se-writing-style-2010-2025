Faster Mutation Analysis with Fewer Processes and
Smaller Overheads
Bo Wang1,2,3*, Sirui Lu4,5*, Yingfei Xiong4,5Â§, Feng Liu1
1School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China
2Beijing Key Laboratory of Trafï¬c Data Analysis and Mining, Beijing, China
3CAAC Key Laboratory of Intelligent Passenger Service of Civil Aviation, Beijing, China
4Key Laboratory of High Conï¬dence Software Technologies (Peking University), MoE, Beijing, China
5Department of Computer Science and Technology, EECS, Peking University, Beijing, China
wangbo_cs@bjtu.edu.cn, {lsrcz, xiongyf}@pku.edu.cn, ï¬‚iu@bjtu.edu.cn
Abstract â€”Mutation analysis is a powerful dynamic approach
that has many applications, such as measuring the quality of
test suites or automatically locating faults. However, the inherentlow scalability hampers its practical use. To accelerate mutationanalysis, researchers propose approaches to reduce redundantexecutions. A family of fork-based approaches tries to share
identical executions among mutants. Fork-based approachescarry all mutants in one process and decide whether to fork
new child processes when reaching a mutated statement. Themutants carried by the parent process are split into groupsand distributed to different processes to ï¬nish the remainingexecutions. However, existing fork-based approaches have twolimitations: (1) the limited analysis scope on a single statementto compare and cluster mutants prevents their systems fromdetecting more equivalent mutants, and (2) the interpretationof the mutants and the runtime equivalence analysis introducesigniï¬cant overhead.
In this paper, we present a novel fork-based mutation analysis
approach WinMut, which (1) groups mutants in a scope ofmutated statements and, (2) removes redundant computationsinside interpreters. WinMut not only reduces the number ofinvoked processes but also has a lower cost for executing a singleprocess. Our experiments show that our approach can furtheraccelerate mutation analysis with an average speedup of 5.57xon top of the state-of-the-art fork-based approach, AccMut.
Index T ermsâ€”software testing, dynamic analysis, mutation
analysis, mutation testing, fork-based mutation analysis
I. I NTRODUCTION
Mutation Analysis [1], [2] is a dynamic program analysis
approach based on fault seeding. To perform mutation analysis,
we ï¬rst make simple syntactic changes to create a set of faultyprograms called mutants. Then we execute these mutantsagainst the test suite and compare the results with the resultof the original program.
Mutation analysis is originally designed for mutation test-
ing, i.e., evaluating the capability of a test suite for revealingfaults [3], [4], [5], [6], [7]. In mutation testing, the mutantsare treated as seeded faults. The more mutants a test suitedetects, the more effective it is. Besides evaluating test suites,
*These two authors contribute equally to the work.Â§Corresponding author.
This work is supported in part by the National Key Research and Devel-
opment Program of China No. 2019YFE0198100, National Natural ScienceFoundation of China under Grant Nos. 61922003.mutation analysis has been applied to many software engi-
neering problems. For example, mutation analysis is used toautomatically locate faults for relieving debugging burdens [8],[9], [10], [11], [12]. Mutants can be treated as not onlypatches in automated program repair [13], [14], [15], butalso substitutions of real-world bugs when they are hard tocollect [16]. Recently some research ï¬elds, such as smartcontract [17] and deep learning [18], adopt mutation analysisto enhance system quality.
Despite the promising prospect shown both in software
engineering research ï¬elds and industrial applications [19],[20], mutation analysis is still limited by its scalability issues.Given a test suite with ntest cases and a program with m
mutants, for each test case, the standard mutation analysis mustinvokemprocesses to execute the mutants. This procedure
results in mâˆ—nexecutions. Although some trivially equivalent
mutants can be ï¬ltered during compile time [21], mcould
still be very large with the program size scaling up, leadingto unaffordable costs in practice.
As a result, many approaches have been proposed to
enhance scalability. A basic method is to reduce run-timecosts by removing redundant computations. Among them, afamily of fork-based mutation analysis approaches tries to
reduce redundant executions among mutants. A fork-basedapproach invokes a single process to carry the executionof all mutants. Once it encounters a mutated statement, itdecides whether to fork new child process(es) to carry subsetsof the active mutants carried by the current process. Split-stream execution [22], [23], [24], which is an early fork-basedapproach, always forks child processes to carry the mutantswhen it executes a mutated statement for the ï¬rst time. Itshares the common executions of a mutant with the originalprocess before the ï¬rst mutated statement is executed. Acc-Mut [25], the state-of-the-art approach of the family, reducesredundancies by clustering mutants from the same mutatedstatement which are equivalent modulo the current state.
Concretely, AccMut starts a process representing all mutants,shares the same executions before mutated statements as split-stream execution. When the execution reaches a statement withmutants, AccMut interprets each active mutant of the statementand collects their output states. AccMut clusters the mutants
3812021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000422021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 Â©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678827
978-1-6654-0337-5/21/$31.00  Â©2021  IEEE
that have the identical output state, i.e., equivalent modulo
state, and forks a set of child processes. Each process carries
the mutants in a cluster. In this way, AccMut can share theremaining executions among the mutants in a cluster.
The overall running time of fork-based mutation analysis
approach can be roughly modeled as the product of thenumber of processes and per-process running time. However,the existing approaches are not optimal in the two aspects:
1)(number of processes) The existing approaches missed
a lot of opportunities to share the executions of themutants.
2)(per-process running time) AccMut relies on an inter-
preter to handle the mutations, introducing large over-head.
Let us consider the number of processes ï¬rst. For example,
in the following code snippet with 3 mutants (M
1,M2, and
M3), there are extra redundancies that are not recognized by
AccMut.
a=b+c ;//M1: (b+1)+c, M2: (b++)+c
d=a+e ;//M3: (a+1)+ereturn d;
In AccMut, these mutants will be separated into 3 different
child processes, because (1) M1andM2result in two states
where the values of bare distinct, and (2) M3is located
in a different statement. This is not the optimal solution.
As only the variable daffects the execution henceforth, the
difference in the values of bwill not affect the remaining
executions, so it should not be included in state comparison.Furthermore, after excluding bfrom the state comparison, we
can ï¬nd that the mutants from two different statements (i.e.,M
1andM3) can be equivalent. More generally, mutants in a
larger scope with different program states could ultimately beequivalent. A better solution for this case is to use the currentprocess to carry the original program and M
2, and fork a
child process to carry M1andM3. Extending the equivalent
mutation recognition scope is non-trivial as (1) we need torecognize the effective set of program variables to do the state-comparison, and (2) we need to carefully design the algorithmto avoid introducing too much extra run-time overhead.
For the per-process running time, the overhead introduced
by the current approaches is dramatically signiï¬cant. To man-age many mutants in a single process, the existing approachesinstrument code and use an interpreter to handle the mutatedstatements, and the overhead is getting more signiï¬cant as thealgorithm for identifying the equivalent mutants at run-timeis getting more complex. AccMut, for example, reports thatit executes 78x more statements for a process approximately.This constant overhead introduced by the interpreter cannotbe ignored. We notice the sparsity of mutated statements ofa single mutant in the ï¬rst-order mutation analysis scenario,that is, a mutant only contains one mutated statement. Basedon this, we ï¬nd that for a child process with only a subsetof all the mutants, the statements could be executed withdifferent policies: interpret the statements that are affectedby some mutant (i.e. the slow way) and execute other state-ments directly with the compiled original version (i.e. the1uint foo(int a,int b) {
2 int sum=a+b; //M1:a-b, M2:a *b, M3:a/b
3 int avg=sum/2; //M4:(--sum)/2, M5:(sum-1)/2
4 int c=bar(avg); //bar() is side effect free
5 return c;//M6:c-1
6}
7void test() {
8 assert(foo(5, 1)==RESULT);
9}
Figure 1: A motivating example. There are 5 mutants generated on
the two expressions in the function foo.
fast way). The implementation is non-trivial when combinedwith extended analysis scopes. We designed a fast algorithmto analyze the set of statements that are safe to be executedwith the compiled original version, and we designed the datastructures and instrumenting methods to support the runtimeselection of execution policies.
We implemented our approach in WinMut as an LL VM-
IR based mutation analysis framework. We have evaluatedWinMut on 10 large-scale, real-world C programs with morethan 20 million mutants and 1149 tests. The evaluation showsWinMut further accelerates mutation analysis with a geometricaverage speedup of 5.57x on top of the state-of-the-art ap-proach, AccMut. WinMut is open source and is available at therepo https://github.com/winmutase21/WinMutASE21Artifact.
II. M
OTIV A TING EXAMPLE
In this section, we describe the general idea of our two
optimizations. We describe how a fork-based mutation analysisalgorithm operates on the program in Fig. 1.
The code in Fig. 1 ï¬rst calculates the average avg ofaand
b(Line 2-3), then invokes the function bar to get the result c,
and ï¬nally returns c. To make the example simpler, we assume
that there are not mutants inside of bar, and it is side-effect-
free so that it will not change the value for sum andavg.
There are 6 mutants generated on the code snippet. M
1,M2,
andM3change the expression at Line 2, from a+b toa-b,
a*b, and a/b, respectively. M4andM5change the expression
at Line 3, from sum/2 to(--sum)/2 and(sum-1)/2,
respectively. M6changes the value to be returned at Line 5,
from ctoc-1.
Driven by the input, we perform standard mutation analysis,
split-stream execution, AccMut, and our approach WinMuton the program respectively. We ï¬rst demonstrate how theseapproaches execute on Line 2-4 and how WinMut reduces thenumber of processes. Then we show how these approachesexecute on Line 5 and how WinMut avoids the overhead insidethe interpreter.
A. Fewer Processes
The execution of a program can be viewed as a sequence
of state transitions. Fig. 2 shows the state transitions of these
approaches from the very beginning to Line 4. We use thefunctions mapping variables to their values to represent thestates.
382





 interpret
interpretsplit
splitinterpret
interpretpartition
{sum,a,b}
partition
{sum,avg,a,b}split
split







interpretinterpret
partition
{avg}
split
	
	
	


Line 2:
Line 3:Line 2:
Line 3:Line 2:
Line 3:
Line 4: Line 4: Line 4:execute
executeLine 2:
Line 3:
Line 4:


















 
  	
Ïƒ0:{Â·Â·Â·}
Ïƒ1:{sum/mapstoâ†’6,Â·Â·Â·}
Ïƒ3:{sum/mapstoâ†’4,Â·Â·Â·}
Ïƒ5:{sum/mapstoâ†’5,Â·Â·Â·}
Ïƒ2:{sum/mapstoâ†’6,avg/mapstoâ†’3,Â·Â·Â·}
Ïƒ4:{sum/mapstoâ†’4,avg/mapstoâ†’2,Â·Â·Â·}
Ïƒ6:{sum/mapstoâ†’5,avg/mapstoâ†’2,Â·Â·Â·}
Ïƒ7:{sum/mapstoâ†’6,avg/mapstoâ†’2,Â·Â·Â·}
Ïƒ9:{avg/mapstoâ†’3,Â·Â·Â·}
Ïƒ10:{avg/mapstoâ†’2,Â·Â·Â·}Ïƒ8,Ïƒ/prime
0:{sum/mapstoâ†’{ {Ori,M 4,M 5,M 6} /mapstoâ†’6,M 1/mapstoâ†’4,M 2/mapstoâ†’5,M 3/mapstoâ†’5},Â·Â·Â·}
Ïƒ/prime/prime
0:{(sum)/mapstoâ†’{(6)/mapstoâ†’{Ori,M 4,M 5,M 6},(4)/mapstoâ†’{M1},(5)/mapstoâ†’{M2,M 3}},Â·Â·Â·}
Ïƒ/prime
1:{sum/mapstoâ†’{ {Ori,M 6} /mapstoâ†’6,M 4/mapstoâ†’5,M 5/mapstoâ†’6},avg/mapstoâ†’{ {Ori,M 6} /mapstoâ†’3,M 4/mapstoâ†’2,M 5/mapstoâ†’2},Â·Â·Â·}
Ïƒ/prime/prime
1:{(sum,avg)/mapstoâ†’{(6,3)/mapstoâ†’{Ori,M 6},(5,2)/mapstoâ†’{M4},(6,2)/mapstoâ†’{M5}},Â·Â·Â·}
Ïƒ/prime
3:{sum/mapstoâ†’{ {M1} /mapstoâ†’4},avg/mapstoâ†’{ {M1} /mapstoâ†’2},Â·Â·Â·}
Ïƒ/prime/prime
3:{(sum,avg)/mapstoâ†’{(4,2)/mapstoâ†’{M1}},Â·Â·Â·}
Ïƒ/prime
5:{sum/mapstoâ†’{ {M3} /mapstoâ†’5},avg/mapstoâ†’{ {M3} /mapstoâ†’2},Â·Â·Â·}
Ïƒ/prime/prime
5:{(sum,avg)/mapstoâ†’{(5,2)/mapstoâ†’{M1}},Â·Â·Â·}
Ïƒ/prime
8:{avg/mapstoâ†’{ {Ori,M 6} /mapstoâ†’3,M 1/mapstoâ†’2,M 2/mapstoâ†’2,M 3/mapstoâ†’2,M 4/mapstoâ†’2,M 5/mapstoâ†’2},Â·Â·Â·}
Ïƒ/prime/prime
8:{(avg)/mapstoâ†’{(3)/mapstoâ†’{Ori,M 6},(2)/mapstoâ†’{M1,M 2,M 3,M 4,M 5}},Â·Â·Â·}
(e) Program States (The variables after interpret but before partition are illustrated as a mapping from mutants to values. Some mutants are grouped
because (1) in split-stream execution or AccMut, they are not generated on the current statement (2) in WinMut, they are handled with specialized data
structures which we will elaborate later. After partition, the partitioned states are illustrated as tuples of variables, and the values are illustrated as
mappings from tuples of values to sets of mutants). Such as aandb, some unnecessary variables omitted in the states.
Figure 2: The procedure of standard mutation analysis, split-stream execution, AccMut, and WinMut on the code in Fig. 1. The procedure
of handling a statement is decomposed into primitives. The circles represent the states after handling each statement. The squares represent
the states after the primitives. States with the same labels are identical.
In the ï¬gure, the arrows stand for the transitions of states.
We abstract the execution of fork-based mutation analysis with
an execution engine. The execution engine can be viewedas a virtual machine and is transparent to the program.The execution engine could execute the statements by justdelegating to the physical machine (the fast path), or interpretthe statements with the internal states stored in the engine(the slow path). For readability, we use circles to representstates at the statement level (i.e., the states after the executionengine fully executed a statement) and use squares to representinternal states between primitives inside an execution engine(which is transparent to the program). In the ï¬gure, the stateshaving the same name are identical. In other words, havingthem in multiple processes is redundant and is a waste ofcomputing resources.
Fig. 2(a) represents the procedure of standard mutation
analysis. To collect the results of the 6 mutants (M
1-M6)
and the original program (Ori), standard mutation analysisseparately compiles 7 programs and runs them against thetest suite in a brute force fashion. As we can see, thereare considerable redundant state transitions. For example, 6transitions to Ïƒ
0before Line 2 and Line 3 transitions from Ïƒ6to the end in the processes of M2-M4are redundant.
To remove redundant executions, fork-based approaches are
proposed to execute multiple mutants in a single process andsplit the execution stream into child processes when necessary.To execute a batch of mutated statements in one process, theexecution engines of fork-based approaches support a morecomplex design of state. In standard mutation analysis, a statemaps a variable to a value, for example, Ïƒ
1(sum)â†’6. While
in fork-based approaches, a state maps a variable to a functionwhich maps a set of mutants to a value, which we call a multi-
value variable. For example, in the state Ïƒ
/prime
0in Fig. 2(b), sum
is mapped to the function which consists of 4 mappings (i.e.{{Ori,M
4,M 5} /mapstoâ†’6,{M 1} /mapstoâ†’4,{M 2} /mapstoâ†’5,{M 3} /mapstoâ†’5}),
and the value of sum for the mutant M3isÏƒ/prime
0(sum)(M 3)â†’5.
Equipped with multi-value variables, we abstract conceptu-
ally atomic operations used by fork-based mutation analysisinto the following 4 primitives.
1)execute for delegating to the physical machine to
execute the original statement,
2)interpret for executing a set of active (carried by
the current process) mutated statements of the same lo-cation, and updates the states with multi-value variables,
3833)partition for partitioning the mutants into equiva-
lence classes by the states of multi-value variables,
4)split for splitting the execution stream into child
processes to ï¬nish remainder executions.
Fig. 2(b) represents the procedure of split-stream execution.
In split-stream execution, the execution engine starts a main
process carrying all 6 mutants (M 1-M6) and the original
program (Ori). When the main process reaches the mutatedstatement Line 2, the execution engine enters the interpreterand invokes interpret to evaluate 4 versions of Line 2,
which transforms the state to an internal state Ïƒ
/prime
0. The engine
performs split to fork 3 child processes for each mutant and
continues the execution of the main process. The main processsimilarly proceeds Line 3, where 2 more child processes areforked for M
4andM5respectively. Although split-stream
execution signiï¬cantly removes the redundant states beforethe ï¬rst mutated statement (e.g., 6 redundant transitions toÏƒ
0), it cannot reduce redundant states after the ï¬rst mutated
statement.
Fig. 2(c) represents the procedure of AccMut. It extends the
split-stream executionâ€™s algorithm with the ability to mergemutants. Like split-stream execution, AccMut also carriesall mutants with the main process. When the main processreaches Line 2, the execution engine interprets the mutatedstatements. The difference arises after interpreting, whereAccMut further invokes the primitive partition with all
accessible variables (sum, a, and b). In Fig. 2, the variables
used to perform partition are marked in green color. The
primitive partition clusters the states of these variables
into equivalent classes, and then each class is split into a newchild process. In this way, M
2andM3can share the executions
of equivalent mutants modulo the current state in a process.Similarly at Line 3, AccMut performs partition against
the states of {sum, avg, a,b} and forks 2 child processes
forM
4andM5respectively.
Although AccMut saves execution effort between M2and
M3, it is still not good enough. (1) AccMut is unable to
merge equivalent mutants generated on different locations. Forexample, M
4and(M2,M 3)are located at different statements,
and they would be separated into two child processes becausethey have different program states after executing Line 2.However, if we further execute the mutants, we ï¬nd that the3 mutants ï¬nally step into the same state Ïƒ
6under the given
input, which implies that these mutants could be carried byone process and share the remaining execution. (2) AccMutdoes not analyze what program variables affect the ï¬nal result,and can only merge mutants that result in an identical state.For instance, M
1and(M2,M 3)would step into Ïƒ4andÏƒ6
respectively. So they cannot be executed in one process inAccMut. However, referring to Fig. 1, we ï¬nd that the differentpart inÏƒ
4andÏƒ6, the program variable sum, does not affect
the ï¬nal result. So it is safe to ignore this difference andexecute the 3 mutants in a single process.
Fig. 2(d) shows the execution of our approach WinMut.
Referring to the executions of AccMut, though some mutantsare from different lines or in different states, they produce theinterpretLine 5:
(a) AccMut (b) WinMutÄ±aÄ±bÄ±cÄ±a
M4M5M1M2,M3Ä±b'Ä±c'Ä±a' Ä±a'
Ä±b"Ä±c"Ä±a" Ä±a"
Ä±dÄ±eÄ±fÄ±gÄ±h
Ä±iLine 5:
execute
partition
M1-M5split
Figure 3: Redundant primitive invocations. The contents of the states
(Ïƒa,Ïƒb,Â·Â·Â·) are not needed, so we leave them abstract.
same output after executing several statements further. Thus tomerge more mutants, WinMut tries to (1) postpone the timingto perform partition andsplit, and (2) conservatively
ï¬gure out the variables that may affect the test result to per-form partition. WinMut continuously interprets a scope
of statements (e.g., Line 2-3), until it reaches a statement thatchanges control-ï¬‚ow (e.g., the function call at Line 4), whereit should perform partition andsplit. We decide to split
at control-ï¬‚ow statements to avoid maintaining call stacks andpath conditions, which may lead to more complex states andmore signiï¬cant overhead. Moreover, at Line 4, the remaindercode only depends on the variable avg, which can be ï¬gured
out by static analysis. So WinMut only clusters the activemutants against the state of the variable avg, which enables
it to merge more mutants with possibly different programstates. For example, M
4andM5are clustered into a group
though the values of sum are different (Ïƒ 6(sum)(M 4)â†’5,
Ïƒ7(sum)(M 5)â†’6). Based on our 2 innovations, M1-M5are
all merged into one process, which signiï¬cantly reduces thenumber of processes.
B. Faster Processes
We compare the state-of-the-art approach AccMut with
ours, to illustrate how WinMut avoids redundant invocations
of high-cost primitives. Fig. 3(a) shows the executions of childprocesses after Line 4 in AccMut and WinMut. In the previoussub-section, AccMut forks 4 child processes carrying a setof mutants to continue the execution. When these processesreach a new mutated statement at Line 5, the execution enginestill enters the interpreter, performs the sequence of high-costprimitives, that is, interpret, partition andsplit.
However, by the deï¬nition of the ï¬rst-order mutation analysisthat each mutated program can have one mutated statement,these child processes deï¬nitely would not contain mutatedstatements at Line 5. In other words, these invocations of theinterpreter primitives are unnecessary.
Fig. 3(b) shows how WinMut executes after Line 4 in the
child process. WinMutâ€™s execution engine ï¬gures out thatthe child process has already carried mutants, and directlyperforms execute against Line 5. To support this, WinMut
384maintains a global set containing all the statements that need to
be interpreted, and the set is updated when it performs split.
More concretely, when WinMut performs split during in-
terpreting Line 3, it dynamically analyzes the statements thatwould be affected by the active mutants ( M
1-M5), and sets
the global set to these statements (Line 2-3). When the childprocess reaches Line 5, it ï¬nds the line does not belong to theset and executes it as is. Thus in this example, WinMut avoidsthe high overhead introduced by the interpreter.
This optimization is non-trivial when combined with ex-
tended analysis scopes for two reasons: (1) we need to analyzethe set of statements that is safe to be delegated to the physicalmachine, and (2) we need to make sure that the delegationpreserves the semantics. Unlike the AccMut, in which theexecution engine does not hold any internal states beyond theboundary of a statement and is automatically transparent to theprogram, our approach needs to use specialized data structuresto make the execution engine transparent and compatible withboth interpreter and the physical machine.
We will elaborate on our efï¬cient implementation later.
III. M
ETHODOLOGY
A. Deï¬nition and Notation
In this subsection, we deï¬ne a set of necessary concepts
and notions which enable us to describe from an abstract view.For conciseness, we adopt some necessary deï¬nitions from theAccMut paper [25].
A program Pcan be viewed as a set of locations, and a
mutation function pmaps each location to a set of variants.
Each variant vconsists of a code block (denoted as v.code),
which is either the original code block of the location or amutated one. Let the function ori map a location to the variant
containing the original code block. By the deï¬nition of ï¬rst-order mutation analysis, a mutant is a program with only onemutated location. Each mutant has a unique mutation ID i.
Let the function Î¼map a location land a mutation ID ito a
variantv, denoted as Î¼(l,i)â†’v, meaning that the mutant i
should use the variant vat the location l.
Given a program P, letÎ£be all the possible states and L
be its location set. Let the function Ï†:Î£â†’Lmap a state to
a location of Pto be executed, similar as the program counter.
The execution of a program can be viewed as a sequence ofstate transitions, from the initial state to the terminal state âŠ¥,
which means the process is ï¬nished.
In standard mutation analysis, a state is a function mapping
variables (i.e. storage units) to values (i.e. numbers), denotedasÏƒ:Sâ†’Z. HereSis the universal set of the storage units
of a physical machine, which includes not only the programvariables in the RAM, but also the ï¬les stored in the harddrive or other resources provided by the OS. They could behandled by lazily mapping to the RAM. However, in fork-based mutation analysis, as a process may execute a set ofvariants at a location, a variable may come from the results ofthe executions of different variants. Let a multi-value variable(demoted as MVV) be a map from mutants to values, denoted
asMVV:Mâ†’Z, whereMis the universal set of the mutantsofP. Thus the state in fork-based mutation analysis of a
program Pcan be deï¬ned as a function maps variables to
MVVs, denoted as:
Ïƒ:Sâ†’2
/uniontext
LâˆˆP/uniontext
vâˆˆp(L)/uniontext
zâˆˆZ{v/mapstoâ†’z}.
The state of standard mutation analysis is a special case ofthe fork-based state, whose MVV only contains one mapping
meaning that the variable only has one value, and this mappingmaps a variant set of size one.
To manipulate the multi-value variable equipped states, we
deï¬ne three operations: project, restrict and update. Given a
multi-value variable equipped state Ïƒand a mutant i, let the
operation project return the state where all multi-value vari-
ables are reduced to single-value variables corresponding to i,
denoted as Ïƒ@i. For example, given the state Ïƒ:{a/mapstoâ†’1,b/mapstoâ†’
{Ori/mapstoâ†’1,M
1/mapstoâ†’2}},w eh a v eÏƒ @M1:{a/mapstoâ†’1,b/mapstoâ†’2}.
As a convention, we will use Ïto denote the projected states,
where all variables map to single values. Given a state Ïƒand a
variable set V, the operation restrict denoted by Ïƒ|Vgets the
partial state that only contains the mappings of the elementsinV. The operation update denoted by Ïƒ[o]â†vreplaces the
value of the variable oinÏƒto the value v, wherevcould be
either a mapping from mutant to values or a single value.
Given a variant v and a state Ïƒ, the primitive
execute(v.code,Ïƒ )executes the code block v.code under
the state Ïƒand updates the state in-place. The function
evaluate( v.code,Ïƒ )evaluates vbased on Ïƒand returns
the output state without updating the state in-place. execute
andevaluate requires that all the input variables for vare
single-value.
B. Models of Existing F ork-based Approaches
Based on the notations and deï¬nitions, we model the exist-
ing fork-based mutation analysis approaches in this section.
Different from the standard mutation analysis, in fork-based
mutation analysis, we may execute more than one variant for
the locations or execute a variant based on different programstates. These locations should be interpreted by the execution
engine. To perform interpretation, fork-based approaches in-voke a procedure called proceed which implements their
core algorithms. The main loop of fork-based mutation analy-sis can be modeled as Algorithm 1. First, the execution engineinitializes the set Gwhich is used to control whether a location
is executed by interpretation or delegation to the physicalmachine, and activates all the mutants (Line 1-2). The processloops as long as there is a location to be executed (Line 3). Ateach step, the execution engine ï¬rst picks the location to beexecuted (Line 4), then analyzes whether the location shouldbe interpreted or executed by the physical machine (Line 5-9).
Split-stream execution and AccMut initialize Gby calling
the procedure initialize which adds all the mutated loca-
tions ofP, and will not change Ganymore. So the execution
engine interprets the location by the procedure proceed if
the current location is mutated (Line 6), otherwise delegates itto the physical machine by the primitive execute (Line 8).
385Input:P: a program
Data:Ïƒ: the program state initialized by test input
Data:G: a set of locations should be interpreted by the
current process
Data:I: a set of mutation IDs of the current process
1Gâ†initialize(P )
2Iâ† all mutant IDs of the program
3whileÏ†(Ïƒ)/negationslash=âŠ¥do
4lâ†Ï†(Ïƒ)
5 iflâˆˆGthen
6 proceed(l )
7 else
8 execute(ori(l ).code,Ïƒ )
9 end
10end
Algorithm 1: Main loop of fork-based mutation analysis.
In split-stream execution, the procedure proceed invokes
the primitive interpret andsplit in turn. That is, it
ï¬lters active mutants of the location L, then executes the code
block for each mutant and records the affected values for each
mutant, and ï¬nally forks new child processes for each mutantto ï¬nish the remainder executions.
AccMut optimizes the procedure proceed of split-
stream execution by inserting an invocation of the primitivepartition between interpret andsplit. The primi-
tivepartition builds equivalent classes based on the states
of active mutants and performs split for each equivalent
class, rather than a single mutant. In this way, AccMut mergesthe mutants in the equivalent classes in a process and sharestheir remainder executions.
As aforementioned, AccMut suffers from two limitations:
(1) unable to share the executions of mutants which are fromdifferent locations or step into different states, (2) introducingconsiderable overhead by unnecessary entering proceed too
many times. To overcome the limitations, we present WinMut,which reduces the number of processes and cuts down theexecution overhead.
C. Fewer processes
Input:l: the current location
Data:Ïƒ: the current state
Data:I: a set of mutation IDs of the current process
Data:G: a set of locations should be interpreted by the
current process
Data:CFG : the control-ï¬‚ow graph
1interpret(l )
2ifneed_split(l )then
3Oâ† output_variable(l , CFG)
4Xâ†partition(Ïƒ |O)
5 split(X )
6pidâ† getpid()
7 ifis_child_process(pid) then
8 Gâ† the forward slice of the locations of the
mutants in I
9 end
10end
Algorithm 2: Algorithm of proceed in WinMut.The general idea of using fewer processes is to (1) enlarge
the range of analysis rather than perform partition and
split at each location, (2) only cluster the mutants based on
a set of necessary variables (i.e. a partial state).
The procedure proceed of WinMut is shown in Algo-
rithm 2. First, the execution engine performs interpret
against the active mutants based on the current state (Line 1).
Theinterpret primitive evaluates each active mutant
and maintains them as multi-value variables in the program
state. If the current location is a point to perform split (Line
2), the execution engine ï¬lters the variables which may affectthe test result by the global control-ï¬‚ow graph CFG (Line
3). Note that these variables can be selected by compile-timeanalysis, which is a sound analysis by picking out all variablesthat may affect the result. Based on the live variable set O,
it performs the primitive partition on the partial state
Ïƒ|
Owhich only contains the mappings of the variables in O
(Line 4). Then the execution engine groups the mutants intoequivalent classes by comparing their partial states. At last,it performs the primitive split, for each equivalent class it
forks a new child process to carry the mutants of the class andï¬nish the remainder executions.
The scope of continuously interpreting is controlled by the
procedure need_split(). In general, more mutants could
be merged into the same equivalent class when the execu-tion engine postpones performing partition andsplit.
However, we can not neglect the overhead introduced byevaluating and maintaining the multi-value variables, becausethe primitives interpret, partition, and split are
operated on complex data structures. This requires us to ï¬nda reasonable timing to perform partition andsplit.F o r
example, if we maintain the multi-value variables in differentexecution paths, the multi-value should be further mapped bypath conditions, which leads to unaffordable overhead. Thuswe decide to partition and split when the location is a control-
ï¬‚ow statement, such as branch statements and function calls.The results of need_split(l )can be statically decided during
compile time.
D. Faster processes
The second improvement intends to speed up per-process
execution by removing redundant interpretations. The follow-
ing facts inspire us (1) a massive number of child processesare forked, (2) once a child process is split, the mutantscarried by it only affect a limited range of locations thathave to be interpreted, and (3) execution is much faster thaninterpretation.
Our basic idea is to interpret the locations that must be
interpreted and execute other locations in child processes.Refer to the previous section, each new child process is splitbased on a partial state, i.e. a smaller mapping from variablesto values, and we only need to interpret the locations whichhave active mutants and a slice of these locations.
Shown as Algorithm 1, the entrance of the interpreter
is controlled by the global set G, which is initialized by
the procedure initialize(P ). In split-stream execution and
386AccMut, Gcontains all the mutated locations of Pin all
processes. To selectively interpret, in WinMut, initialize(P )
adds the forward slice locations of all mutants of P, which
can be decided during compile time. The forward slice is the
set of locations that depend on multi-value variables. Note thatthe primitive split converts multi-value variables to single-
value ones, so the slice will not cross a split point.
Furthermore, split-stream execution and AccMut do not
updateGduring execution. In contrast, once the primitive
split is invoked, WinMut ï¬lters G in child processes,
leaving only the locations (1) which have the active mutantsof the current child process, and (2) the dynamic forwardslice of these locations, shown in Line 7-9 of Algorithm 2.Because WinMut performs split splits at every control-
ï¬‚ow or pointer access statement, which occurs frequently,the dynamic slice of the locations will not be so large.Consequently, Gis sharply reduced to a few locations in a
child process.
E. Basic Primitives
We abstract 4 necessary primitives from the operations
required by fork-based mutation analysis approaches. These
primitives, including execute, interpret, partition
andsplit, are atomic operations. The execute primitive
directly delegates a code block to the physical machine andupdates the program state in place, which does not need moreexplanation.
Input:l: the current location
Data:I: a set of mutants IDs of the current process
Data:Ïƒ: the global program state
1Ïƒpâ† an empty partial program state
2foreachiâˆˆIdo
3Ïâ†evaluate(Î¼( l,i).code,Ïƒ @i)
4 foreachoâˆˆoutvar(Î¼(l,i ))do
5 ifoâˆˆÏƒp.variables then
6 Ïƒp[o]â†Ïƒp[o]âˆª{i/mapstoâ†’Ï[o]}
7 else
8 Ïƒp[o]â†{i/mapstoâ†’Ï[o]}
9 end
10 end
11end
12foreachoâˆˆÏƒp.variables do
13Ïƒ[o]â†Ïƒp[o]
14end
Algorithm 3: The implementation of interpret.
Theinterpret primitive evaluates a set of mutants and
updates the program state with multi-value variables, shownas Algorithm 3. For each active mutant, it ï¬rst executes thevariant at the current location lwith mutant ID ion the
projected program state Ïƒ@ito get the result state Ï(Line
3). Then it updates the empty partial program state to ensurethatÏƒ
p@i|outvar(Î¼(l,i ))=Ï|outvar(Î¼(l,i ))(Line 4-10), where
outvar means the output variables of a variant. At last, it
writes the variables in the partial program state back to theglobal program state (Line 12-14).
Algorithm 4 shows the partition primitive, which clus-
ters the active mutants into equivalent classes based on theInput:Ïƒp: the input partial program state
Data:I: a set of mutants IDs of the current process
1Xâ† empty map from projected partial program states to
mutant sets
2foreachiâˆˆIdo
3Ïâ†Ïƒp@i
4 ifÏâˆˆX.keyset then
5 X[Ï]â†X[Ï]âˆª{i}
6 else
7 X[Ï]â†{i}
8 end
9end
10returnX
Algorithm 4: The implementation of partition.
projection of the input partial state. For each mutant i, it ï¬rst
get the projection of the input partial state (Line 3). Then theprimitive tries to ï¬nd the equivalent class that ibelongs to, and
adds it to that class (Line 4-8). Then it returns the partitionresultX(Line 10).
Input:X: a map from projected partial program states to
mutant sets
Data:I: the set of active mutation IDs of the current process
Data:Ïƒ: the global program state
1foreachÏâˆˆX.keyset do
2Mâ†X[Ï]
3pidâ† fork()
4 ifis_child_process(pid) then
5 foreachoâˆˆÏ.variables do
6 Ïƒ[o]â†Ï[o]
7 end
8 Iâ†M
9 return
10 end
11Iâ†(Iâˆ’M)
12end
Algorithm 5: The implementation of split.
Algorithm 5 shows the primitive split, which splits
executions into child process(es) for each equivalent class. Foreach key (i.e. a projected partial program state) in X, it gets
the corresponding set of mutants M (Line 2) and forks a new
process (Line 3). For the child processes, the primitive updatesthe variables (Line 5-7), then sets the mutants represented bythe child process to M (Line 8), and returns (Line 9). For the
parent process, it just removes M from the active mutants of
the current process (Line 11).
Note that although the algorithms conceptually iterate
through a huge set I, we can do some optimizations on this
to iterate only through a subset of it and get the same result.We will elaborate on this later.
IV . I
MPLEMENT A TION
In this section we present WinMut implementation details.
Same as AccMut, WinMut is a ï¬rst-order mutation executionengine on LL VM-IR [26], that is each location contains an IRinstruction. LL VM-IR is a high-level intermediate representa-tion (IR), which is the core concept of the LL VM compilerinfrastructure. IR-based mutation analysis approaches support
387Table I: Mutation operators in WinMut
Name Description Example
AOR Replace arithmetic operator a+bâ†’aâˆ’b
LOR Replace logic operator a&bâ†’a|b
ROR Replace relational operator a== bâ†’a>=b
LV R Replace literal value Tâ†’T+1
COR Replace logical connector a&& bâ†’a||b
SOR Replace shift operator a> >bâ†’a< <b
STDC Delete a call f()â†’nop
STDS Delete a store a=5â†’nop
UOI Insert a unary operation b=aâ†’a++ ; b=a
ROV Replace the operation value f(a, b)â†’f(b, a)
ABV Take absolute value f(a, b)â†’f(abs(a),b )
multiple front-end source languages without losing expres-
siveness. Particularly, LL VM-IR supports several mainstreamlanguages, such as C/C++, Python, Objective-C, and CUDA.Recently researchers have proposed several IR-based mutationapproaches, including LL VM-IR based [25], [27], [28], [29],[30], [31] and Java bytecode based [32], [15]. Note thatour algorithm is general which can be applied on differentcode granularity, e.g., on instruction level, expression level, orstatement level.
A. Mutation Operators
As each location holds an IR instruction in WinMut, we
should employ IR-based mutation operators. We adopt the
same set of mutation operators as AccMut, shown in Table I.These 11 mutation operators cover the mutation operators usedby the state-of-the-art mutation analysis tools, such as Ma-jor [16], [33], Javalanche [32], and SRCIROR [28]. Major is aJava source code level mutation analysis tool, while Javalancheis a Java bytecode level one. All their mutation operators areemployed except the Java language speciï¬ed ones. SRCIRORis the state-of-art LL VM-IR based tool employing a set of4 mutation operators, which is a subset of ours. In addition,these mutation operators are considered to be effective and arewidely used in existing approaches [34], [35], [36].
B. Data Structures and Instrumentation
Although we have ensured that execute will not be used
for an IR instruction affected by any mutation (either is
mutated itself or depends on any multi-value variables), weneed to ensure that (1) if an IR instruction is executed by theprimitive execute, the delegated physical instruction could
manage the multi-value variable data structure correctly, and(2) the interpretation effort should be as little as possible,which can be realized by reducing the set of mutants to beinterpreted (i.e. Iin Algorithm 3).
For a mutated location, we instrument the code as the
following pseudo-C code:
if(l in G) {
{output vars of all mutants} =
proceed(l, {input vars of all mutants})
}else {
{output vars} = execute(l, {input vars})
}
To make sure that execute works, we cannot change the
type declarations for the variables in the original code fromprimitive types to mappings to support multi-value variables.Instead, we maintain the multi-value variables as two parts:original program variable and additional mapping. In the
instrumented code, all of the variables are declared as theoriginal program and always hold a single value, we call thisvariable the original program variable. We maintain the valuesin the original program variable as if they are computed witha set of execute calls after the last split primitive call.
We associate each variable with an additional mapping
inside of the execution engine. We store those mutant/valuepairs in it for those mutants with different values from theoriginal program variable. A good property of this two-partmulti-value variable data structure is that we can treat single-value variables and multi-value variables in a uniï¬ed way. Asingle-value variable would have an empty mapping, whilemulti-value variables would have non-empty ones.
If the location lholds no mutants and the input variables are
all single-value, the output variables of proceed procedure
will all be single-value. The proceed procedure does nothing
but maintaining the original program variables. So we cansafely replace that proceed toexecute and still keeps the
multi-value data structures valid.
This can also reduce the redundant interpretation in Algo-
rithm 3 as what interpret does now is just computing
the additional mappings for the output variables. We do notneed to compute all the values for the mutants in the set I.
Those mutants that neither mutate the current location nor arepresented in any additional mapping for the input variablescan be skipped.
C. Transparent I/O System for F ork-based Mutation Analysis
Another contribution of WinMut is that we implemented a
new I/O system that is transparent to users. Some fork-based
mutation analysis tools [23], [25] rely on the POSIX system
callfork to perform split executions. Although the copy-on-
write mechanism of fork safely separates the virtual memory
spaces between the parent process and the child process whichavoids copying the physical memory whose pages are notwritten, it is unable to separate the I/O handlers between theprocesses. For example, if the child process writes a ï¬le thatis inherited from the parent process, not only the ï¬le contentis changed, but also the ï¬le pointer of the parent processis moved. To solve this problem, AccMut builds a memorymirror of all opened ï¬les, that is, it loads the whole ï¬le tomemory once it opens a ï¬le. However, AccMut requires usersto manually modify source code to replace all I/O operationswith theirs, which needs considerable efforts. It also restrictedthe available APIs mostly to C standard I/Os. To deal with therestrictions, we implemented a memory-based I/O library thatcan be linked transparently to replace the I/O system.
Our library supports not only C standard I/Os to read
or write a ï¬le, but also many ï¬le system operations likeremoving/creating the ï¬les. However, it would be infeasible forus to make the user-space library conform to POSIX standardand work all the same as the OS kernel. There are also featuresimpossible for us to implement, e.g. we cannot support a
388program invoking execve orfork system calls. We tried
our best to make the memory-based I/O system robust enough
to make sure it will not crash too many tests and affect ourmain result.
We have a lot of assertions in our library code trying
to detect inconsistency and unsupported features. When ourtool executes a program, it would ï¬rst execute the wholeprogram under the memory-based I/O library to detect if thereare any unsupported features. If any unsupported feature isdetected or any assertion is violated, the tool would skip themutation analysis on that program. This makes sure that wecan successfully execute a test script if only a small portionof the programs would crash under our I/O system.
In the future, we may provide kernel support for the I/O
operations to make them identical to the existing APIs, butthis is beyond the scope of this paper.
V. E
V ALUA TION
We have evaluated WinMut on a set of real-world subjects,
many of which are large-scale projects. We aim to empiricallyanswer the following research questions:
RQ1 How does WinMut perform compared to the state-of-the-art approach AccMut?
RQ2 How is the contribution of each optimization in WinMut?
A. Experimental Setup
We implemented WinMut as a fork of AccMut, which is
based on LL VM [26]. It is hard for us to manually modify thereal-world projects to replace the I/O operations, and thereare many I/O operations unhandled by AccMut in the real-world projects, so we modiï¬ed AccMut and replaced the I/Ohandling module with our memory-based I/O library. Thisalso makes sure that the two tools share the same setting,so we can compare the performance of the main algorithms.Just as AccMut, we have not implemented the support forsome instructions required by C++ yet, so we only considerC subjects to answer the research questions.
We select the subjects by the following criteria:
(1) we only consider real-world, open-source subjects that
have developer-written test suites;
(2) the target subjects can be compiled by LL VM;(3) the application of the subjects should be diverse.
We selected 10 projects and their properties are shown
in Table II. The column Loc shows the lines of code withoutcomments and empty-lines, collected by the tool cloc. The col-
umn # Mut/# BB/# Split shows the number of mutants/basic-blocks/split point of the subject. A split point is a location toperform the primitive split. The column # Mut per Inst/Split
is the average number of mutants for each instruction/scopeof the locations corresponding to a split point.
These subjects contain in total more than 1.5 million lines of
code, 20,203,516 mutants, 435,949 basic blocks, and 964,967split points. On average, each instruction holds 16.3 mutants,and each split point handles 20.9 mutants.
Moreover, the subjects are from different ï¬elds. Binutils-
gas is a portable assembler supplied by GNU. Coreutils is theGNU core utilities for manipulating ï¬les, shell and text. Gmpis an arithmetic library supplied by GNU. Libsodium is anencryption library. Lz4 is a lossless ï¬le compression program.Pcre2 is a regular expression parser that is compatible withPerl. Libpng is the ofï¬cial PNG library. Lua is an interpreterfor the Lua language. Grep is a utility for searching plain-textdata. Ffmpeg is a tool for video and audio.
As some of the subjects are very large, to complete the
evaluation in a practical time budget, we do not evaluate thetools on the whole test suite. For each subject, we executethe original test suite for 2 seconds and record the coveredones as our activated tests. We also skipped the tests requiringunhandled operations by our transparent I/O systems. Thecolumn # Exec Tests of Table II shows the number of executedtests. Note that due to the diversity of the projects, theyhave very different organizations of test suites, one test casereported by the build system may correspond to many smallertest cases written in the test framework. We choose not to usethe test case number reported by the building system but countthe invoked program number for mutation analysis.
In total, we collect 1,149 tests in our evaluation. Note that
although we only choose a subset of the tests, they can bestill extremely time-consuming due to the intrinsic high costof mutation analysis. In our experiment, the tests for libpngcovered within the execution of 2 seconds would cost morethan 4 days by AccMut. Moreover, in total, a 14-days run ofAccMut is large enough for evaluation.
Following AccMut, to avoid the execution time inï¬‚uenced
by process scheduling across multi-core, we serially executedtests without parallelization. In addition, we also limited thenumber of parallel processes to one for child processes. That
is, each mutant in our experiment was executed serially. Weran WinMut 3 times on each subject, and record the averagetime. All experiments were evaluated on an Intel Core i7-7700K CPU and 64GB memory with Ubuntu 18.04 L TS.
B. Results and Discussion
1) RQ1: Comparison with the State-of-the-art: To answer
the RQ1, we compared WinMut with the state-of-the-art fork-
based approach AccMut in the following two aspects: (1)the overall execution time and (2) the number of invokedprocesses.
The results are shown in Table III. The columns T
wand
Tarespectively show the overall execution time of WinMut
and AccMut. The column T a/Twshows the speedup of Win-
Mut over AccMut. While the columns P wand P ashow the
invoked process number of WinMut and AccMut. The column(P
w/Pa)% shows the percentage of process number of WinMut
over AccMut. The averages are computed as geometric mean.
First, we analyze the results of execution time and we have
the following ï¬ndings:
(1) WinMut is faster than AccMut on all the subjects with
an average speedup of 5.57x;
(2) WinMut achieves a speedup higher than 10x on 3 sub-
jects, namely Gmp, Libsodium and Lz4. Especially, it hasthe maximum speedup of 28.88x on Gmp;
389Table II: Subject programs
Name Loc # Exec Tests # Mut #B B # Split # Mut per Inst # Mut per Split
Binutils-gas 299K 290 166,488 6,477 11,261 13.5 14.8
Coreutils 144K 287 400,150 11,532 19,628 20.4 7.2
Gmp 115K 30 613,595 10,774 23,225 22.3 26.4
Libsodium 45K 43 426,025 5,657 13,813 18.4 30.8
Lz4 13K 185 472,591 11,286 22,656 16.9 22.7
Pcre2 80K 33 266,399 6,900 11,722 16.7 22.7
Libpng 56K 9 282,831 8,527 15,394 15.0 18.4
Lua 16K 19 172,493 6,981 11,840 13.6 14.6
Grep 83K 207 217,399 8,406 16,144 12.9 13.5
Ffmpeg 1,032K 46 17,185,545 359,409 819,284 16.2 21.0
Total 1,584K 1,149 20,203,516 435,949 964,967 16.3 20.9
Table III: The total run time and the number of invoked processes of WinMut and AccMut
Subject Tw To1 To2 Ta Ta/To1 Ta/To2 Ta/Tw Pw Pa (Pw/Pa)%
Binutils-gas 1.62h 2.74h 1.75h 2.80h 1.02 1.60 1.72 1,580,925 1,695,842 93.2%
Coreutils 2.92m 2.96m 2.94m 2.97m 1.01 1.01 1.02 68,137 71,022 95.9%
Gmp 1.19h 37.10h 1.30h 34.26h 0.92 26.34 28.88 148,461 158,069 93.9%
Libsodium 3.94h 90.28h 4.54h 86.17h 0.95 18.98 21.86 313,007 336,904 92.9%
Lz4 1.94h 25.15h 2.16h 25.94h 1.03 12.01 13.40 118,287 130,351 90.7%
Pcre2 0.62h 4.91h 0.64h 4.99h 1.02 7.75 8.08 208,107 221,859 93.8%
Libpng 15.14h 108.71h 16.61h 111.60h 1.03 6.72 7.37 71,187 78,919 90.2%
Lua 10.08h 84.38h 10.28h 84.57h 1.00 8.22 8.39 358,892 377,177 95.2%
Grep 0.39h 1.19h 0.41h 1.28h 1.08 3.13 3.30 888,151 957,265 92.8%
Ffmpeg 2.25h 2.34h 2.54h 2.64h 1.13 1.04 1.17 390,729 441,240 88.6%
Total 37.21h 356.85h 40.29h 354.29h 1.02 5.17 5.57 4,145,883 4,468,648 92.7%
In the timing representation, h/m means hour/minute.
(3) WinMut has a more signiï¬cant speedup on compute-
intensive programs, such as arithmetic and encryption
libraries.
Second, we evaluate the ability of WinMut to cluster more
mutants. We can observe that:
(1) WinMut consistently employs fewer processes than Acc-
Mut on all the subjects;
(2) WinMut further reduces the number of invoked processes
by 7.3% on average compared with AccMut.
2) RQ2: Contribution of Each Optimization: WinMut con-
sists of 2 individual optimizations, i.e., the one for mergingmore mutants and the one for operating more efï¬ciently.These optimizations may have different effects on the overallspeedup, and this question intends to detailed evaluate theircontribution. To answer this question, we conducted a con-trolled trial. That is, we only activate one optimization andcompare the overall execution time.
Table III shows the results. The columns T
w,To1,To2and
Tashow the execution time of WinMut, WinMut with the
ï¬rst optimization (for merging more mutants), WinMut withthe second optimization (for more efï¬cient execution), andAccMut, respectively. The column T
x/Tymeans the speedup
of the technique yoverx.
We can make the following ï¬ndings:
(1) the second optimization boosts the execution;(2) the ï¬rst optimization introduces speed reduction on the
subjects Gmp and Libsodum and improve the perfor-mance slightly on the remaining subjects;
(3) except on the subject Ffmpeg, the second optimization
contributes a higher speedup than the ï¬rst one on allsubjects;(4) the speedup of the second optimization is closer to the
ï¬nal speedup of WinMut;
(5) the combination of the 2 optimizations results in a better
speedup than employing just one of them.
As discussed in the previous section, merging more mutants
involves heavier costs that would cover the beneï¬ts. So itis reasonable that the ï¬rst optimization slightly slows downthe execution on Gmp and Libsodium. Moreover, the ï¬rstoptimization boosts more than the second optimization onthe subject Ffmpeg for it merges more mutants accordingto Table III. Finally, the ï¬nal speedup can not be predictedby simply multiply the speedup results of the optimizations,which implies the combination of the two techniques hascomplex mutual inï¬‚uence. However, the time required byWinMut is only 92% of the time required by AccMut +second optimization, which is very close to the process numberratio. This indicates that the second optimization minimizes theimpact of mutant merging algorithms and makes it possible touse a more powerful algorithm in the future research to mergemore mutants without introducing too much overhead.
C. The robustness and impact of the memory-based I/O system
To verify that our memory-based I/O system will not affect
the main results, we executed all test cases from the subjects
and recorded how many test cases it passes.
In our experiment, we are not counting the test case number
reported by the script, because each reported test case couldcorrespond to many smaller test cases. We count how manytest programs are executed with our I/O system. For the time,we only count the time for executing the subject program. Wedo not count the time for executing the external test framework(e.g. DejaGnu or manually written test script).
390First, we execute the whole test suite without our I/O
system, record the total test cases number. Then we execute
the whole test suite with our I/O system. If the test script
does not ï¬nish normally, we remove the failing test cases andthose test cases depending on them and restart until the testsremaining in the test script all pass. Then we record the process
number. We subtract this process number with the number ofprocesses with detected unsupported features, and we get thetotal passing test cases.
Then we rerun the current test script with and without our
I/O system and record the time. The results are shown in
Table IV. The running time is comparable, indicating that ourI/O library does not introduce too much overhead.
Table IV: Passed cases and running time with/without our I/O lib
Without I/O lib With I/O lib
Subject #Cases Time/s #Cases Time/s
Binutils-gas 2368 16.09 2368 20.91
Coreutils 31698 8.23 16597 29.67
Gmp 170 23.00 170 22.62
Libsodium 77 4.26 77 4.04
Lz4 185 8.52 185 5.79
Pcre2 33 0.17 33 0.19
Libpng 38 65.13 38 64.86
Lua 28 3.74 21 3.83
Grep 3604 0.92 2743 3.70
Ffmpeg 4398 146.31 4388 163.37
We noticed that the library breaks some test cases in
Coreutils, Lua, Grep and Ffmpeg. Coreutils is a library whosetest cases are testing all kinds of edge cases and we expectthat it would crash our library the most, and we found thatourunlink implementation fails to handle some cases. Lua
failed to execute some of the tests because they used fork.
Grep and Ffmpeg are querying unsupported ï¬les. e.g., Ffmpegis trying to read /dev/urandom, which is not supported.
VI. R
ELA TED WORK
In this section, we ï¬rst present related work on accelerating
mutation analysis, then we introduce related ï¬elds. Based onthe survey papers [34], [35], [36], we can roughly divide exist-ing approaches into static approaches and dynamic approaches.
Statically Accelerating Mutation Analysis. Static approaches
intend to reduce the cost of mutation analysis without execut-
ing mutants against test suites. Basically, static approaches aimto reduce cost during mutation generation and compile time.
Several approaches use static analysis of compilers to
remove the useless equivalent mutants [37], [21], [38] orimprove the effectiveness [27]. As the costs of mutationanalysis are positively associated with the number of mutantsand the size of test suites, existing approaches mainly focuson reducing them.
A popular class of methods is to select a subset of the
mutants, such as mutation sampling [39], mutation cluster-ing [40], and mutation operator selection [41], [42]. Somecomprehensive approaches combining several techniques [43],[44], [45], [46]. Some approaches utilize machine learningmodels trained by real-world bugs to prioritize the high-quality mutants [29], [47], or focus on the newly committedcode [48]. Some other methods analyze test suites, such astest selection [49] and ï¬gure out the reusable test results inregression testing [50]. Some ML-based methods try to predictthe results of mutants and avoid execution [51], [52]. Theseapproaches can be pre-process ï¬lters and combined with ours.
Dynamically Accelerating Mutation Analysis. As mutation
analysis is a kind of dynamic approach essentially, some
existing studies aim to reduce runtime costs of mutationanalysis. Besides mutant reduction techniques [53], [54], themajority of dynamic approaches focus on reducing redundantcertain parts of mutation analysis.
Some approaches intend to reduce compile time redundan-
cies. Mutant schemata [55] compiles all mutants once intoa single executable ï¬le. Some incipient approaches avoidcompile-time costs in an interpreting fashion [56], but theyare usually lumbered by the low-efï¬ciency of interpreters.
The prevalent dynamic method is to reduce redundancies
during executing mutants. Split-stream execution [23], [22]reduces the redundant executions before the ï¬rst mutatedstatement. Just et al. cluster mutants are test equivalent [57].AccMut [25] as mentioned before, tries to further merge mu-tants of the same states. As discussed before, our approachescould outperform these approaches.
Higher-order mutation analysis [58], [59] replaces more
than one statement once in a program, which is very differentfrom standard mutation analysis, and some approaches aimto share executions in higher-order mutation analysis [24],[60]. Finally, some works resort the test cases to kill mutantsfaster [61], [49]. These approaches are orthogonal to ours.
Sharing Executions in Software Product-line Testing,
Model Checking and Symbolic Execution. Similar ideas
of sharing executions also exist in the ï¬elds of softwareproduct-line testing, model checking and symbolic executionapproaches, which face the same challenge of redundancy.
A product in software product-line can be treated as a
higher-order mutant [60]. V ariational execution maintains aset of multi-value variable across the entire test execution toshare common executions [62], [63], [64]. These approachesaim to merge products (i.e. higher-order mutants) via purelyinterpreting, which leads to signiï¬cant overhead.
Multi-valued model checking supports variables in the ï¬-
nite state machine to be multi-valued [65]. Delta symbolicexecution [66], shadow symbolic execution [67], and multi-path symbolic execution [68] try to share common parts ofmultiple paths.
VII. C
ONCLUSION
In this paper, we propose a novel approach to accelerate
mutation analysis. We take the existing fork-based mutationanalysis approaches a step further by (1) reducing the numberof invoked processes, and (2) removing redundancies insidethe execution engine. We implemented our approach into thetool WinMut. The evaluation results show that our approachachieves an average speedup of 5.57x on top of the state-of-the-art approach, AccMut.
391REFERENCES
[1] R. A. DeMillo, R. J. Lipton, and F. G. Sayward, â€œHints on test data
selection: Help for the practicing programmer,â€ Computer, vol. 11, no. 4,
pp. 34â€“41, 1978.
[2] R. G. Hamlet, â€œTesting programs with the aid of a compiler,â€ Software
Engineering, IEEE Transactions on, vol. SE-3, no. 4, pp. 279â€“290, 1977.
[3] J. H. Andrews, L. C. Briand, Y . Labiche, and A. S. Namin, â€œUsing
mutation analysis for assessing and comparing testing coverage criteria,â€
IEEE Transactions on Software Engineering, vol. 32, no. 8, pp. 608â€“624,2006.
[4] L. Inozemtseva and R. Holmes, â€œCoverage is not strongly correlated
with test suite effectiveness,â€ in Proceedings of the 36th International
Conference on Software Engineering. ACM, 2014, pp. 435â€“445.
[5] T. T. Chekam, M. Papadakis, Y . Le Traon, and M. Harman, â€œAn
empirical study on mutation, statement and branch coverage faultrevelation that avoids the unreliable clean program assumption,â€ in 2017
IEEE/ACM 39th International Conference on Software Engineering(ICSE). IEEE, 2017, pp. 597â€“608.
[6] M. Kintis, M. Papadakis, A. Papadopoulos, E. V alvis, N. Malevris, and
Y . Le Traon, â€œHow effective are mutation testing tools? an empiricalanalysis of java mutation testing tools with manual analysis and realfaults,â€ Empirical Software Engineering, vol. 23, no. 4, pp. 2426â€“2463,
2018.
[7] M. Papadakis, D. Shin, S. Y oo, and D.-H. Bae, â€œAre mutation scores
correlated with real fault detection? a large scale empirical study onthe relationship between mutants and real faults,â€ in 2018 IEEE/ACM
40th International Conference on Software Engineering (ICSE). IEEE,2018, pp. 537â€“548.
[8] M. Papadakis and Y . Le Traon, â€œUsing mutants to locate" unknown"
faults,â€ in ICST, 2012, pp. 691â€“700.
[9] S. Moon, Y . Kim, M. Kim, and S. Y oo, â€œAsk the mutants: Mutating
faulty programs for fault localization,â€ in ICST, 2014, pp. 153â€“162.
[10] M. Papadakis and Y . Le Traon, â€œMetallaxis-ï¬‚: mutation-based fault
localization,â€ Software Testing, V eriï¬cation and Reliability, vol. 25, no.
5-7, pp. 605â€“628, 2015.
[11] S. Pearson, J. Campos, R. Just, G. Fraser, R. Abreu, M. D. Ernst,
D. Pang, and B. Keller, â€œEvaluating and improving fault localization,â€inProceedings of the 39th International Conference on Software Engi-
neering. IEEE Press, 2017, pp. 609â€“620.
[12] D. Zou, J. Liang, Y . Xiong, M. D. Ernst, and L. Zhang, â€œAn empirical
study of fault localization families and their combinations,â€ IEEE
Transactions on Software Engineering, 2019.
[13] C. Le Goues, M. Dewey-V ogt, S. Forrest, and W . Weimer, â€œA systematic
study of automated program repair: Fixing 55 out of 105 bugs for $8each,â€ in ICSE, 2012, pp. 3â€“13.
[14] W . Weimer, Z. Fry, and S. Forrest, â€œLeveraging program equivalence
for adaptive program repair: Models and ï¬rst results,â€ in ASE, 2013, pp.
356â€“366.
[15] A. Ghanbari, S. Benton, and L. Zhang, â€œPractical program repair
via bytecode mutation,â€ in Proceedings of the 28th ACM SIGSOFT
International Symposium on Software Testing and Analysis , 2019, pp.
19â€“30.
[16] R. Just, D. Jalali, L. Inozemtseva, M. D. Ernst, R. Holmes, and G. Fraser,
â€œAre mutants a valid substitute for real faults in software testing?â€ inFSE, 2014, pp. 654â€“665.
[17] Z. Li, H. Wu, J. Xu, X. Wang, L. Zhang, and Z. Chen, â€œMusc: A tool for
mutation testing of ethereum smart contract,â€ in 2019 34th IEEE/ACM
International Conference on Automated Software Engineering (ASE).IEEE, 2019, pp. 1198â€“1201.
[18] Q. Hu, L. Ma, X. Xie, B. Y u, Y . Liu, and J. Zhao, â€œDeepmutation++:
A mutation testing framework for deep learning systems,â€ in 2019 34th
IEEE/ACM International Conference on Automated Software Engineer-ing (ASE). IEEE, 2019, pp. 1158â€“1161.
[19] G. Petrovi Â´c and M. Ivankovi Â´c, â€œState of mutation testing at google,â€ in
Proceedings of the 40th international conference on software engineer-ing: Software engineering in practice, 2018, pp. 163â€“171.
[20] G. Petrovic, M. Ivankovic, B. Kurtz, P . Ammann, and R. Just, â€œAn
industrial application of mutation testing: Lessons, challenges, andresearch directions,â€ in 2018 IEEE International Conference on Software
Testing, V eriï¬cation and V alidation Workshops (ICSTW). IEEE, 2018,pp. 47â€“53.
[21] M. Papadakis, Y . Jia, M. Harman, and Y . Le Traon, â€œTrivial compiler
equivalence: A large scale empirical study of a simple, fast and effectiveequivalent mutant detection technique,â€ in ICSE, 2015, pp. 936â€“946.[22] K. N. King and A. J. Offutt, â€œA fortran language system for mutation-
based software testing,â€ Software: Practice and Experience, vol. 21,
no. 7, pp. 685â€“718, 1991.
[23] R. Gopinath, C. Jensen, and A. Groce, â€œTopsy-Turvy: a smarter and
faster parallelization of mutation analysis,â€ in ICSE, 2016, pp. 740â€“743.
[24] S. Tokumoto, H. Y oshida, K. Sakamoto, and S. Honiden, â€œMuvm:
Higher order mutation analysis virtual machine for c,â€ in ICST, 2016,
pp. 320â€“329.
[25] B. Wang, Y . Xiong, Y . Shi, L. Zhang, and D. Hao, â€œFaster mutation
analysis via equivalence modulo states,â€ in Proceedings of the 26th ACM
SIGSOFT International Symposium on Software Testing and Analysis .
ACM, 2017, pp. 295â€“306.
[26] C. Lattner and V . Adve, â€œLL VM: A compilation framework for lifelong
program analysis & transformation,â€ in CGO, 2004, pp. 75â€“86.
[27] F. Hariri, A. Shi, H. Converse, S. Khurshid, and D. Marinov, â€œEvaluating
the effects of compiler optimizations on mutation testing at the compilerir level,â€ in 2016 IEEE 27th International Symposium on Software
Reliability Engineering (ISSRE). IEEE, 2016, pp. 105â€“115.
[28] F. Hariri and A. Shi, â€œSrciror: A toolset for mutation testing of c
source code and llvm intermediate representation,â€ in Proceedings of
the 33rd ACM/IEEE International Conference on Automated SoftwareEngineering, 2018, pp. 860â€“863.
[29] M. Papadakis, T. T. Chekam, and Y . Le Traon, â€œMutant quality indi-
cators,â€ in 2018 IEEE International Conference on Software Testing,
V eriï¬cation and V alidation Workshops (ICSTW), 2018, pp. 32â€“39.
[30] F. Hariri, A. Shi, V . Fernando, S. Mahmood, and D. Marinov, â€œCom-
paring mutation testing at the levels of source code and compilerintermediate representation,â€ in 2019 12th IEEE Conference on Software
Testing, V alidation and V eriï¬cation (ICST), 2019, pp. 114â€“124.
[31] T. T. Chekam, M. Papadakis, and Y . Le Traon, â€œMart: a mutant
generation tool for llvm,â€ in Proceedings of the 2019 27th ACM Joint
Meeting on European Software Engineering Conference and Symposiumon the F oundations of Software Engineering, 2019, pp. 1080â€“1084.
[32] D. Schuler and A. Zeller, â€œJavalanche: efï¬cient mutation testing for
Java,â€ in ESEC/FSE, 2009, pp. 297â€“298.
[33] R. Just, F. Schweiggert, and G. M. Kapfhammer, â€œMajor: An efï¬cient
and extensible tool for mutation analysis in a Java compiler,â€ in ASE,
2011, pp. 612â€“615.
[34] Y . Jia and M. Harman, â€œAn analysis and survey of the development of
mutation testing,â€ Software Engineering, IEEE Transactions on, vol. 37,
no. 5, pp. 649â€“678, 2011.
[35] M. Papadakis, M. Kintis, J. Zhang, Y . Jia, Y . Le Traon, and M. Harman,
â€œMutation testing advances: an analysis and survey,â€ in Advances in
Computers. Elsevier, 2019, vol. 112, pp. 275â€“378.
[36] A. V . Pizzoleto, F. C. Ferrari, J. Offutt, L. Fernandes, and M. Ribeiro,
â€œA systematic literature review of techniques and metrics to reduce thecost of mutation testing,â€ Journal of Systems and Software , vol. 157, p.
110388, 2019.
[37] A. J. Offutt and W . M. Craft, â€œUsing compiler optimization techniques to
detect equivalent mutants,â€ Software Testing, V eriï¬cation and Reliability,
vol. 4, no. 3, pp. 131â€“154, 1994.
[38] M. Kintis, M. Papadakis, Y . Jia, N. Malevris, Y . Le Traon, and M. Har-
man, â€œDetecting trivial mutant equivalences via compiler optimisations,â€IEEE Transactions on Software Engineering , vol. 44, no. 4, pp. 308â€“333,
2017.
[39] W . E. Wong and A. P . Mathur, â€œReducing the cost of mutation testing:An empirical study,â€ Journal of Systems and Software, vol. 31, no. 3,
pp. 185â€“196, 1995.
[40] C. Ji, Z. Chen, B. Xu, and Z. Zhao, â€œA novel method of mutation
clustering based on domain analysis.â€ in SEKE, 2009, pp. 422â€“425.
[41] A. J. Offutt, G. Rothermel, and C. Zapf, â€œAn experimental evaluation
of selective mutation,â€ in Proc. ICSE, 1993, pp. 100â€“107.
[42] L. Zhang, S.-S. Hou, J.-J. Hu, T. Xie, and H. Mei, â€œIs operator-based
mutant selection superior to random mutant selection?â€ in Proc. ICSE,
2010, pp. 435â€“444.
[43] M. Gligoric, L. Zhang, C. Pereira, and G. Pokam, â€œSelective mutation
testing for concurrent code,â€ in Proc. ISSTA, 2013, pp. 224â€“234.
[44] M. Jimenez, T. T. Checkam, M. Cordy, M. Papadakis, M. Kintis,
Y . L. Traon, and M. Harman, â€œAre mutants really natural? a study onhow" naturalness" helps mutant selection,â€ in Proceedings of the 12th
ACM/IEEE International Symposium on Empirical Software Engineeringand Measurement, 2018, pp. 1â€“10.
392[45] J. M. Zhang, L. Zhang, D. Hao, L. Zhang, and M. Harman, â€œAn
empirical comparison of mutant selection assessment metrics,â€ in 2019
IEEE International Conference on Software Testing, V eriï¬cation and
V alidation Workshops (ICSTW). IEEE, 2019, pp. 90â€“101.
[46] L. Zhang, M. Gligoric, D. Marinov, and S. Khurshid, â€œOperator-based
and random mutant selection: Better together,â€ in Proc. ASE , 2013, pp.
92â€“102.
[47] T. T. Chekam, M. Papadakis, T. F. BissyandÃ©, Y . Le Traon, and K. Sen,
â€œSelecting fault revealing mutants,â€ Empirical Software Engineering,
vol. 25, no. 1, pp. 434â€“487, 2020.
[48] W . Ma, T. Laurent, M. Ojdani Â´c, T. T. Chekam, A. V entresque, and M. Pa-
padakis, â€œCommit-aware mutation testing,â€ in 2020 IEEE International
Conference on Software Maintenance and Evolution (ICSME). IEEE,2020, pp. 394â€“405.
[49] L. Zhang, D. Marinov, and S. Khurshid, â€œFaster mutation testing inspired
by test prioritization and reduction,â€ in Proc. ISSTA, 2013, pp. 235â€“245.
[50] L. Zhang, D. Marinov, L. Zhang, and S. Khurshid, â€œRegression mutation
testing,â€ in Proc. ISSTA, 2012, pp. 331â€“341.
[51] J. Zhang, L. Zhang, M. Harman, D. Hao, Y . Jia, and L. Zhang, â€œPre-
dictive mutation testing,â€ IEEE Transactions on Software Engineering,
vol. 45, no. 9, pp. 898â€“918, 2018.
[52] J. Zhang, Z. Wang, L. Zhang, D. Hao, L. Zang, S. Cheng, and L. Zhang,
â€œPredictive mutation testing,â€ in ISSTA, 2016, pp. 342â€“353.
[53] C.-a. Sun, F. Xue, H. Liu, and X. Zhang, â€œA path-aware approach
to mutant reduction in mutation testing,â€ Information and Software
Technology, vol. 81, pp. 65â€“81, 2017.
[54] C.-a. Sun, A. Fu, X. Guo, and T. Y . Chen, â€œRemusse: A redundant
mutant identiï¬cation technique based on selective symbolic execution,â€IEEE Transactions on Reliability, 2020.
[55] R. H. Untch, A. J. Offutt, and M. J. Harrold, â€œMutation analysis using
mutant schemata,â€ in Proc. ISSTA, 1993, pp. 139â€“148.
[56] A. Offutt VI and K. N. King, â€œA fortran 77 interpreter for mutation
analysis,â€ in ACM SIGPLAN Notices, vol. 22, no. 7. ACM, 1987, pp.
177â€“188.
[57] R. Just, M. D. Ernst, and G. Fraser, â€œEfï¬cient mutation analysis by
propagating and partitioning infected execution states,â€ in ISSTA, 2014,
pp. 315â€“326.
[58] Y . Jia and M. Harman, â€œHigher order mutation testing,â€ Information and
Software Technology, vol. 51, no. 10, pp. 1379â€“1393, 2009.[59] M. Harman, Y . Jia, P . Reales Mateo, and M. Polo, â€œAngels and monsters:
An empirical investigation of potential test effectiveness and efï¬ciencyimprovement from strongly subsuming higher order mutation,â€ in ASE,
2014, pp. 397â€“408.
[60] C.-P . Wong, J. Meinicke, and C. KÃ¤stner, â€œBeyond testing conï¬gurable
systems: applying variational execution to automatic program repairand higher order mutation testing,â€ in Proceedings of the 2018 26th
ACM Joint Meeting on European Software Engineering Conference andSymposium on the F oundations of Software Engineering. ACM, 2018,pp. 749â€“753.
[61] R. Just, G. M. Kapfhammer, and F. Schweiggert, â€œUsing non-redundant
mutation operators and test suite prioritization to achieve efï¬cient andscalable mutation analysis,â€ in ISSRE, 2012, pp. 11â€“20.
[62] C.-P . Wong, J. Meinicke, L. Lazarek, and C. KÃ¤stner, â€œFaster variational
execution with transparent bytecode transformation,â€ Proceedings of the
ACM on Programming Languages, vol. 2, no. OOPSLA, pp. 1â€“30, 2018.
[63] C. H. P . Kim, S. Khurshid, and D. Batory, â€œShared execution for
efï¬ciently testing product lines,â€ in 2012 IEEE 23rd International
Symposium on Software Reliability Engineering. IEEE, 2012, pp. 221â€“230.
[64] J. Meinicke, C.-P . Wong, C. KÃ¤stner, T. ThÃ¼m, and G. Saake, â€œOn
essential conï¬guration complexity: Measuring interactions in highly-conï¬gurable systems,â€ in Proceedings of the 31st IEEE/ACM Interna-
tional Conference on Automated Software Engineering, 2016, pp. 483â€“
494.
[65] M. Chechik, B. Devereux, S. Easterbrook, and A. Gurï¬nkel, â€œMulti-
valued symbolic model-checking,â€ ACM Transactions on Software En-
gineering and Methodology (TOSEM), vol. 12, no. 4, pp. 371â€“408, 2003.
[66] M. dâ€™Amorim, S. Lauterburg, and D. Marinov, â€œDelta execution for
efï¬cient state-space exploration of object-oriented programs,â€ IEEE
Transactions on Software Engineering, vol. 34, no. 5, pp. 597â€“613, 2008.
[67] T. Kuchta, H. Palikareva, and C. Cadar, â€œShadow symbolic execution for
testing software patches,â€ ACM Transactions on Software Engineering
and Methodology (TOSEM), vol. 27, no. 3, pp. 1â€“32, 2018.
[68] K. Sen, G. Necula, L. Gong, and W . Choi, â€œMultise: Multi-path symbolic
execution using value summaries,â€ in Proceedings of the 2015 10th Joint
Meeting on F oundations of Software Engineering, 2015, pp. 842â€“853.
393