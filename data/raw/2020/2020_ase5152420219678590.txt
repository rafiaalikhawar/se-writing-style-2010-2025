Distribution Models for Falsiﬁcation and
Veriﬁcation of DNNs
Felipe Toledo
Dept. of Computer Science
University of Virginia
ft8bn@virginia.eduDavid Shriver
Dept. of Computer Science
University of Virginia
dls2fc@virginia.eduSebastian Elbaum
Dept. of Computer Science
University of Virginia
selbaum@virginia.eduMatthew B. Dwyer
Dept. of Computer Science
University of Virginia
matthewbdwyer@virginia.edu
Abstract —DNN validation and veriﬁcation approaches that are
input distribution agnostic waste effort on irrelevant inputs and
report false property violations. Drawing on the large body ofwork on model-based validation and veriﬁcation of traditionalsystems, we introduce the ﬁrst approach that leverages environ-mental models to focus DNN falsiﬁcation and veriﬁcation on therelevant input space. Our approach, DFV, automatically buildsan input distribution model using unsupervised learning, preﬁxesthat model to the DNN to force all inputs to come from thelearned distribution, and reformulates the property to the inputspace of the distribution model. This transformed veriﬁcationproblem allows existing DNN falsiﬁcation and veriﬁcation tools totarget the input distribution – avoiding consideration of infeasibleinputs. Our study of DFV with 7 falsiﬁcation and veriﬁcationtools, two DNNs deﬁned over different data sets, and 93 distinctdistribution models, provides clear evidence that the counter-examples found by the tools are much more representative ofthe data distribution, and it shows how the performance of DFVvaries across domains, models, and tools.
Index T erms—neural networks, input distribution model, ver-
iﬁcation, falsiﬁcation
I. I NTRODUCTION
A deep neural network (DNN) is trained to accurately
approximate a partial target function, f:Rn→Rm.
The domain of deﬁnition of f– referred to as the data
distribution D– is typically an inﬁnitesimal portion the full
domain,|D|
|Rn|≈0. However, much of the recent literature
on validation and veriﬁcation of DNNs ignores the partiality
of a DNN’s deﬁnition with signiﬁcant negative consequences.First, existing test generation techniques [1], [2], [3], [4], [5]have been shown to produce a majority of inputs that lieoff of the data distribution [6], [7]. Second, white-box DNNcoverage criteria [8], [2] do not take the distribution intoaccount and this can drive coverage-directed test generators offthe distribution [6] and give misleading reports of the coverageachieved [7]. Third, faults that are detected for off distributioninputs constitute false reports [6], [7] which can lead to wastedeffort in fault triage, localization, and ﬁxing.
Whereas recent research has begun to explore how to
leverage models of the data distribution for testing [9], [7],in this paper we present the ﬁrst approach to use such modelsto support techniques for DNN veriﬁcation and falsiﬁcation – aform of property-driven validation [10]. Our distribution-based
falsiﬁcation and veriﬁcation (DFV) approach for DNNs draws
inspiration from the large body of research exploiting environ-mental models of the feasible input domain for software sys-tems to focus Veriﬁcation and Validation (V&V). These mod-els are typically built from the system requirements and can beexpressed in a variety of forms, e.g., simulations [11], state-machines [12], or logical speciﬁcations [13]. Such environment
models [14] have become an essential component of validation
and veriﬁcation approaches for software systems [15], [16],[17], [18], [19] and this has led them to be adopted in severaldomains [20].
To be amenable for V&V , environment models must satisfy
three requirements. First, they must be accurate in deﬁning the
set of feasible inputs. For example, for an underapproximatinganalysis, e.g., [18], an underapproximating model is requiredto guarantee feasible counter-examples; dually an overapprox-imating analysis requires an overapproximating environmentmodel. Second, they must be generative, providing the ability
to be executed, interpreted, or solved, so they can be leveragedto generate feasible inputs. For example, generating feasiblecounter-examples when veriﬁers or falsiﬁers detect propertyviolations [21]. Third, for veriﬁcation they must be amenable
to constraint-based encoding in a form that can be leveraged
by the veriﬁcation algorithm. For example, for a SMT-basedveriﬁcation method, e.g., [18], an environment model mustbe convertible to logical formulae in a supported theory. Forabstract interpretation, e.g., [22], an environment model mustbe convertible to supported abstract domains.
In this paper, we adapt the concept of an environment
model to support existing DNN veriﬁcation and falsiﬁcationtechniques [23], [24], [25], [26], [27], [28], [29], [30], [31],[32], [33], [34], [35], [36], [37], [38], [10]. To do this, wehave to address the challenge that it is intractable, in general, tospecify an accurate model of the feasible inputs for a complexDNN – like those that process images captured by a forwardfacing camera (see Figure 3b [39]). A key insight of this workis that we can leverage the rich body of research that themachine learning (ML) community has developed for learninggenerative models of the data distribution, which we use asenvironment models.
DFV transforms a DNN and a correctness property into
a falsiﬁcation or veriﬁcation problem focused on the datadistribution in three steps. First, a generative model of thedata distribution for a DNN is trained [40], [41]. Unlike man-ually developed environment models for traditional software
3172021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000372021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678590
978-1-6654-0337-5/21/$31.00  ©2021  IEEE
V&V , these environment models are constructed automatically
through an unsupervised training process. The design andtraining of the model of the data distribution can leverageML best-practices to produce a suitably accurate model [42],[43]. Second, the approach modiﬁes the original DNN to usethe appropriate component of the trained generative model,e.g., the decoder of a variational autoencoder (V AE), as a setof preﬁx layers to the DNN under analysis. This forces allinputs to the DNN to come from the learned data distribution.Third, the approach reformulates the correctness property overthe input space of the generative model. DFV supports thereporting of feasible on-distribution counter-examples, whenproperty violations are detected, and reporting that speciﬁedsubsets of the data distribution are free of violations, whenveriﬁers are able to discharge such proofs.
We evaluate DFV on DNNs trained to recognize images
of clothing [44] and trained to control a drone from imagedata [39], for a range of challenging correctness properties.We ﬁnd clear evidence that DFV enables existing falsiﬁcationand veriﬁcation techniques to produce counter-examples thatare much more representative of the data distribution than arecomputed otherwise – both visually and in terms of standardmeasures of similarity. While scaling of veriﬁcation tech-niques is challenging, we also ﬁnd evidence that distributionmodels can enable them to prove properties over the datadistribution. Building on these promising ﬁndings, we studyhow varying the architecture of the model and how shiftingbetween different families of generative models impacts theeffectiveness of the technique. Our results can be used to guidethe development of models to support DFV.
The primary contributions of this work are the: (1) formu-
lation of the ﬁrst model-based veriﬁcation and falsiﬁcationmethod for DNNs; (2) demonstration that distribution modelsyield substantially better counter-examples from veriﬁcationand falsiﬁcation; and (3) exploration of different models ofthe data distribution and their trade-offs.
II. B
ACKGROUND
In this section we provide background on deep neural
networks, DNN veriﬁcation and falsiﬁcation, and DNN testingapproaches that exploit the data distribution.
A. Deep Neural Networks
A deep neural network, N, is a type of machine learning
model that is trained to approximate a partial target function
f:R
n→Rm. For example, fmay classify some n-
dimensional input (e.g., an image) as one of mpossible classes
(e.g., a digit in the range 0 to 9). fis partial in the sense that
it is trained to generalize to a target data distribution, D⊆Rn.
For inputs off of the distribution, x/negationslash∼D, its behavior, N(x),
should be considered as undeﬁned.
DNNs are comprised of layers,l0,...,l k, each of which
performs some computation on their input (e.g., matrix multi-plication or convolution). A typical linear architecture deﬁnesa DNN as the composition of layers, N=l
k◦···◦l 1◦l0.
Layers are comprised of neurons. The input of a neuron isED x ˆx
Z≈N(0,1)dq(z|x)p(x|z)
(a)VA E with encoder E
trained to learn the pa-
rameters of the latent dis-tribution and decoder D
trained to learn the like-lihood of an input givenvalues in the latent space.T
G N(0,1)d/circleplustext
D
p(x?∼D )ˆxx
x?
(b) GAN with trainable input genera-
torGand discriminator Dthat predicts
the probability that an input is from thetrue data distribution.
Fig. 1: Generative latent distribution models that produce
unseen samples from the data distribution.
deﬁned as the weighted sum of the outputs of a set of neurons
in a preceding layer where the connections between neuronshave trainable parameters. The output of a neuron applies a
non-linear activation function to the input.
Training involves initializing the parameters and then ap-
plyingNto samples, (x,y), from the training set, T, and re-
peatedly updating parameters based on /bardblN(x)−y/bardbl. While the
goal of training is to learn the partial function fdeﬁned over
D,Dis generally unmanageably large (e.g., the set of road
images visible on a forward facing camera). Consequently,the training set is deﬁned as a representative sample of thedata distribution, T∼D . A well-trained network is said to
generalize to the data distribution [45].
B. Models of the Data Distribution
The ﬁeld of machine learning has long understood the im-
portance of modeling the data distribution. Broadly speaking,the ﬁeld has developed two types of approaches. Out-of-distribution detectors [46] are designed to determine whethera data point lies on the data distribution, but are generallyunable to generate new data from the distribution. In contrast,generative models are designed to generate unseen samples
from the data distribution. There are three broad classes ofgenerative models: variational autoencoders (V AE) [40], gen-erative adversarial networks (GAN) [41], and autoregressivemodels such as PixelCNN++ [47]. Among these, V AEs andGANs can be classiﬁed as latent variable models since they
make explicit the mathematical structure of the learned latentspace which models D. We leverage generative latent variable
models of the data distribution in this work.
Fig. 1a depicts a V AE as comprised of a pair of trainable
models: an encoder, E, and a decoder, D[40]. The encoder,
or inference network, is trained to learn the parameters ofthe latent distribution, q(z|x), that, through a regularization
term, seeks to match a given prior distribution - usually amultivariate Gaussian, N(0,1)
d. The decoder, or generative
network, is trained to learn the likelihood of an input givenvalues in the latent space, p(x|z). These networks are trained
together on inputs drawn from the data distribution, D,b y
minimizing the difference between posterior and latent prior
318and maximizing the likelihood estimation of the input. A V AE
is generative in the sense that one can sample from the latentspace, z∼N(0,1)
d, and then run the decoder, D(z)=ˆx,
to produce a sample that lies on the data distribution. V AEscan be leveraged for out-of-distribution detection by exploitingthe fact that for x∼D,E(x)produces a distribution that can
be sampled to generate inputs, ˆx. Computing /bardblx−ˆx/bardblfor a
number of samples yields the encoder-stochastic reconstruc-
tion error (ESRE) [48]. We adapt ESRE to use the structural
similarity index measure (SSIM) [49] to assess the quality ofgenerated image data in §IV.
Fig. 1b depicts a GAN as comprised of a pair of trainable
generator, G:R
d→Rn, and discriminator, D:Rn→
R, [41]. The generator produces an input, ˆx, from a set
of latent variables. The discriminator predicts the probabilitythat an input is from the true data distribution, p(x∼D).
The GAN is trained by presenting generated, ˆx, and training
inputs, x, to the discriminator without disclosing their source,
x
?. The generator loss function is high when the generated
data is classiﬁed as generated data by the discriminator, i.e.,p(ˆx∼D)is low. The loss function of the discriminator is
high when it incorrectly classiﬁes data, i.e., p(ˆx∼D)is high
orp(x∼D)is low, and a low value when it is correct.
The weights of the generator and discriminator are updated todecrease their respective loss values. Through this process, thegenerator learns to produce data close to the data distribution.
There is a rich literature on the design of V AEs and GANs
exploring the impact of latent dimension, complexity of modelarchitectures, and variation in loss functions on the accuracyof the learned model. Generally GANs are thought to possessbetter precision, i.e., produce sharper images, but suffer frompoor recall, whereas V AEs are thought to be the opposite, i.e.,good recall, but produce blurry images. Leveraging distributionmodels for V&V requires a measure of both, but the MLliterature continues to improve in this regard. For example,V AEs can now achieve precision that outperforms well-tunedGANs while retaining good recall [50].
C. DNN V eriﬁcation and Falsiﬁcation
A correctness problem is a pair, ψ=/angbracketleftN,φ/angbracketright, of a DNN,
N:R
n→Rm, and a property speciﬁcation φ, formed to
determine whether N|=φis valid or invalid. The property
speciﬁcation deﬁnes a set of constraints over the inputs, φX
– the pre-condition, and a set of constraints over the outputs,
φY– the post-condition. Veriﬁcation of φ(N)seeks to prove
or falsify: ∀x∈Rn:φX(x)→φY(N(x)). Falsiﬁcation seeks
only to falsify that formula.
Two common types of DNN properties are robustness and
reachability. Robustness originated with the study of adver-sarial examples [51], [52], and speciﬁes that inputs from agiven input region are all classiﬁed the same. This type ofproperty is common for evaluating veriﬁers [53], [26], [36],[31]. Reachability properties deﬁne the post-condition usingconstraints over output values, specifying that inputs from agiven input region reach outputs within a given safe outputregion. This type of property has been used to evaluate severalDNN veriﬁers [53], [35], [54].
A recent survey on DNN veriﬁcation [55] classiﬁes ap-
proaches for verifying DNN correctness problems based ontheir type: reachability, optimization, or search, or a combina-tion of these. Tools implementing a range of these approachesand their combination have been developed over the past threeyears [23], [24], [25], [26], [27], [28], [29], [30], [31], [32],[33], [34], [35], [36], [37]. Despite the signiﬁcant researchinto this topic scalability remains a challenge, but usabilityhas been improved by the availability of frameworks likeDNNV [38] which we use in our work.
Complementary to veriﬁcation, falsiﬁcation checks proper-
ties of DNNs by attempting to ﬁnd examples that violate thespeciﬁcation for a given model. Two categories of techniquesthat have been developed for falsifying DNN correctness prob-lems are adversarial attacks and fuzzing. Adversarial attacks
are methods optimized for detecting violations of robustnessproperties [52]. Fuzzing methods randomly generate inputs
within a given input region, and checking whether the outputsthey produce violate the post-condition. Fuzzing techniquesinclude TensorFuzz [3] and DeepHunter [4]. More recently,the range of applicability of adversarial attacks and fuzzinghas been increased to correctness properties by DNNF, whichreduces general DNN correctness properties to robustnessproperties [10], [56].
D. Distribution-aware DNN Testing
Recent work in testing has begun to explore models of the
input domain to support DNN testing. Riccio and Tonella
construct explicit models of the input space using domainknowledge to generate test cases in order to characterize thespace of DNN misbehaviors [57]. Dola et al. show that notconsidering the input distribution can bias the assessment ofDNN testing techniques, and then use V AEs to model theinput distribution and to augment the objective functions oftest generation techniques to remedy that bias [7]. Byun andRayadurgam also describe how to model the input distributionwith a V AE and use it to generate test inputs [58]. Ourapproach differs in that it composes the distribution model,e.g., a V AE decoder, with the DNN under analysis and,because we focus on veriﬁcation and falsiﬁcation, developsconstraint-based encodings that over-approximate designatedportions of the input space of that model.
III. A
PPROACH
We present DFV, our approach for focusing DNN veriﬁca-
tion and falsiﬁcation techniques on the data distribution.
A.DFV Overview
Fig. 2a depicts the generic structure of DFV. Our insight
is that properties should be veriﬁed not over the entire input
domain, Rn, of a DNN, but rather over the data distribution,
D, used to train the DNN, i.e., its domain of deﬁnition. Since
Dis a small subset of the domain there is the potential to
enable property veriﬁcation when violating inputs lie off of
319NM Z φY(.)φX(.)
(a) General DFV Framework. Zis
the latent space distribution, Mis
the generative model, Nis the net-
work, with pre-condition φXand
post-condition φY.M=DNφY(.)
Zc=[−c,c]d
(b) Instance of DFV Framework with a VAE
decoder DasM, precondition φX(x)=
true, andcbounds on the M(required by
falsiﬁers and veriﬁers).cZc=l∞
l2
(c) Forddimensions, a l2d-ball of radius
ccontains all points within cstandard
deviations of the mean. Since veriﬁers
do not support non-linear constraints torepresent l
2, we approximate it with l∞.
Fig. 2: Distribution-based Falsiﬁcation and Veriﬁcation Framework with Bounding Constraints
the distribution. Moreover, by restricting veriﬁcation to inputs
on the distribution, x∼D, counter-examples will be feasible
and worth ﬁxing.
To deﬁne D, we advocate the use of a latent variable
generative model, M:Rd→Rnwhered/lessmuchn. With
Manalyses can be formulated over the low-dimensional
latent space, Z≈N(0,1)d, to reason about the behavior of
systems on D. Two classes of such models that we explore,
introduced earlier in §II, are variational autoencoders (V AE)and generative adversarial networks (GAN).
Our goal is to deﬁne the set of inputs that lay on the
data distribution and that satisfy the property pre-condition,i.e.,x∼D∧φ
X(x). Fig. 2a depicts the enforcement of
these constraints using two mechanisms: the use of Mas
a preﬁx network, and the forwarding of generated inputs toenable the enforcement of the pre-condition prior to checkingpost-conditions – as described in [10]. We note that it is alsopossible to combine Mandφ
Xby deﬁning a generative model
capable only of producing inputs satisfying the precondition,in which case the forwarding generated inputs is not needed.With these elements the veriﬁcation problem can be reformu-lated as∀z∈Z:φ
X(M(z))→φY(N(M(z))).
In this paper, we explore in detail an instantiation of DFV,
depicted in Fig. 2b, that uses a V AE as the generative modeland targets output properties of the DNN, i.e., where theprecondition is true. As explained in §II, the V AE’s encoderand decoder are trained together, but we only exploit thedecoder,M=D, for DFV.
Since the dimension of the latent space of the V AE is gener-
ally much smaller than the input dimension, the generated setof inputs, {M(z):z∈Z}, takes up an inﬁnitesimal portion
of the ambient input space. For falsiﬁcation, this allows theproblem to be reformulated from the input space, ∃x∼D:
¬φ
Y(N(x)), to the latent space, ∃z∈Z:¬φY(N(M(z))).
More importantly, since existing falsiﬁcation algorithms can-not test that x∼D, this approach is the ﬁrst to yield counter-
examples that lie on the data distribution – subject to theprecision of Min modeling D. Similarly veriﬁcation can be
reformulated to the latent space, ∀z∈Z:φ
Y(N(M(z))).
Existing veriﬁcation and falsiﬁcation tools require that their
input space be bounded. When using Mthat input space isthe latent space of the distribution model and its Gaussianstructure allows us to formulate a meaningful bound. Forad-dimensional latent space, the l
2d-ball of radius c,a s
depicted in Fig. 2c, contains all points within cstandard
deviations of the mean. The value of ccan be speciﬁed to
contain an arbitrarily large portion of the distribution, e.g.,c=5 speciﬁes veriﬁcation of 99.99994% of the distribution.
However, existing DNN falsiﬁers and veriﬁers do not supportthe non-linear constraints necessary for deﬁning the l
2d-ball,
so we formulate hypercube approximations.
As depicted in Fig. 2c, the l∞d-ball is the smallest
hypercube that overapproximates the l2d-ball. We denote
withZcthe hypercube with radius c– side-length 2c. For-
mulating constraints that restrict each of the ddimensions to
the interval [−c,c] yields a veriﬁcation problem, ∀z∈Zc:
φY(N(M(z))), that will soundly verify cstandard deviations
inZ.
B.DFV Algorithm
Algorithm 1 deﬁnes DFV through a series of transforma-
tions followed by the invocation of the veriﬁer or falsiﬁer.
The DFV algorithm accepts a correctness problem, com-
prised of a DNN, N, and correctness property, φ. In addition,
it takes a model, Mof the data distribution, D, a veriﬁer or
falsiﬁer,V, and a radius, c, which deﬁnes how much of the
data distribution should be subjected to analysis. Its outputindicates that either a property violation has been detected,
the property is valid within radius c(for veriﬁers), or the result
isunknown – due to limitations in the veriﬁer or falsiﬁer used.
When a violation is reported a counter-example, ce, is returned
as well. We now describe the algorithm in more detail.
Decoupling the Training of the Distribution Model from
DFV .Algorithm 1 consumes M, separating the training of M
from DFV. This is important because there are many degreesof freedom when training M, often dependent on the type
of model and training used. For example, for a V AE, suchas those used in §IV, the effectiveness of the learned modeldepends on many factors, including the architecture of themodel (e.g., number, type, and conﬁguration of layers) and thetraining parameters (e.g., optimizer, batch size, and learningrate). Independent of the model and training process, the goal
320Algorithm 1: DFV
Input: Correctness problem ψ=/angbracketleftN,φ/angbracketright, Distribution
modelM, Veriﬁer/Falsiﬁer V, Radiusc
Output: {violation :ce,valid ,unknown}
1begin
2N/prime←N◦M
3φ/prime←∀z∈RM.d:(z∈
[−c,c]M.d)∧φX(M(z))→
4 φY(N/prime(z))
5ψ/prime←/angbracketleft N/prime,φ/prime/angbracketright
6result←check(V,ψ/prime)
7 ifresult=violation then
8 returnviolation :
M(getCounterExample (result))
9 returnresult
of this pre-stage to DFV is for Mto approximate D.W e
note that the development of distribution models that have high
precision and recall is an active area of ML research [42], [43],and that recent research has deﬁned high-precision V AEs [50].We note, however, that models with lower levels of precisioncan still be quite valuable. As we show in §IV, rather sim-ple V AE models can yield much more meaningful counter-examples than those produced without using a distributionmodel. In this work we explore V AE variations, but we leaveto future work a broader study of how model accuracy impactsthe cost and beneﬁt of DFV.
Problem Transformation. DFV transforms the correctness
problem as described in lines 2-5 of Algorithm 1. Line 2modiﬁes the original DNN by preﬁxing it with a generativemodel, as shown in Fig. 2a. This transformation preﬁxesthe original DNN with a latent variable generative model,such as the decoder of a V AE. Because the generative modelmaps inputs from a known distribution to the learned datadistribution, this step ensures that veriﬁcation and falsiﬁcationwill only check inputs from the data distribution learned bythe preﬁxed model. That is, the tools can focus on the inputsthat are within the distribution. Line 3 replaces the input pre-condition of the original property with a new pre-conditionspecifying that inputs come from the latent space of thegenerative model – M.ddenotes its dimension – and that
these inputs satisfy the pre-condition. Because the veriﬁersand falsiﬁers require inputs to be bounded, we assert boundson the latent space. When the latent space distribution of thegenerative model is Gaussian, we require zto be within c
standard deviations of the mean. We approximate this with ahypercube of radius ccentered at the origin. Line 5 joins the
outcomes of the transformation from line 2 and 3 to redeﬁnethe correctness problem on the data distribution.
V eriﬁcation and Falsiﬁcation. After transforming the prob-
lem, on lines 6-9, falsiﬁers and veriﬁers can be run on the mod-iﬁed correctness problem, ψ
/prime=/angbracketleftN/prime,φ/prime/angbracketright. If a counter-example
toψ/primeis found, then it can be mapped to a valid counter-
example of the original property by performing inference with
(a) FashionMNIST training images.
(b) DroNet training images.
Fig. 3: Samples from FashionMNIST and DroNet training sets.
the generative model, M. DFV will report, violation , along
with the counter-example, otherwise it will report the valid or
unknown result returned by V.
IV . S TUDY
In this section we assess the cost-effectiveness and scala-
bility of DFV by applying it in conjunction with multiple fal-
siﬁers and veriﬁers. Our evaluation will answer the followingresearch questions:
1) What is the cost-effectiveness of applying falsiﬁcation
and veriﬁcation with DFV?
2) How does the conﬁguration of the model used by DFV
affect the quality and quantity of counter-examples?
3) How well does DFV scale to more complex input do-
mains that required more sophisticated models?
A. Design
We now describe the problem benchmarks, generative mod-
els, falsiﬁers, veriﬁers, and metrics that constitute the threeexperiments in our study. We describe the experimental pro-cedures using these items under each research question.
1) Problem Benchmarks: Our criteria for the selection
of problem benchmarks required for them to have modelsdeveloped by others that offer a range of challenges in terms ofarchitectural complexity, domains and tasks, and architecturesand training data. They also had to have global reachabilityproperties that are supported by DFV, and be amenable to theapplication of existing tool sets. In the end, we identiﬁed twobenchmarks of correctness problems that ﬁt our criteria.
The GHPR-FMNIST benchmark is a new DNN correctness
problem benchmark, based on the GHPR-MNIST benchmarkfrom the evaluation of DNNF [10]. The benchmark con-sists of 20 global reachability properties applied to a smallFashionMNIST [44] network. A sample of images from theFashionMNIST training set is shown in Fig. 3a. The networkused is based on the architecture of the small MNIST networkfrom the evaluation of the Neurify veriﬁer [53]. There are2 formulations of properties in this benchmark. The ﬁrst 10properties, which we will refer to as type A, are of the form:for all inputs, if class ahas the maximal value, then the output
values for classes aandbare closer to one another than the
output values for classes aandc. For example, one of the
properties states that for all inputs, if that input is classiﬁed
321TABLE I: Generative Models
RQ Name Latent Layers/ Output
Dim. Neurons Activation
1,2 VAE MRS 100 2/768 Sigmoid
1 VAE RQ1 2 1/24 ∼Sigmoid
2 VAE d,l,h 1-32 1-4/16-1024 Sigmoid
3 FC-VAE DroNet 512 5/4608 Sigmoid
3 Conv-VAE DroNet 512 7/202729 Sigmoid
3GAN DroNet 512 2/3840000 Sigmoid
as asneaker , then the class sandal will be ranked higher
than class shirt . The other 10 properties (type B) are weaker
variations that drop the maximal value constraint.
The GHPR-DroNet benchmark, introduced in DNNF [10],
consists of 10 global reachability properties applied to the
DroNet DNN [39], which predicts a steering angle and prob-ability of collision for a quadrotor from 200 by 200 blackand white images. DroNet is a large DNN model consistingof 3 residual blocks and over 475,000 neurons. A sample ofimages from the DroNet training set is shown in Fig. 3b. Theproperties are of the form: for all inputs, if the probability ofcollision is between p
minandpmax, then the steering angle is
withinddegrees of 0. As pminincreases, so does d, capturing
the intuition that if the probability of collision is low, then thequadrotor vehicle should not make sharp turns.
2) Generative Models: We consider two powerful types of
latent variable generative models to learn the data distributionof the training set – V AEs and GANS. We selected these mod-els because: 1) they meet the requirements of the approach,2) they are among the most popular unsupervised learningapproaches to encode a data distribution, and 3) they workin different ways and provide different tradeoffs. Given thenumber of variables involved in our experiments, we choseV AEs for RQ1 and RQ2, and incorporated GANs for RQ3.Through the study we explored a total of 93 models, 91to characterize the distribution of GHPR-FMNIST a n d2t o
characterize the distribution of GHPR-DroNet. All models
used in the study are shown in Table I. Details for theconﬁguration of those models is provided in the experimentalprocedures for each of the research questions.
3) Falsiﬁers and V eriﬁers: For the falsiﬁers, we will use
four common adversarial techniques included in the DNNFtool [10]. DNNF reduces correctness problems to adversarialrobustness problems to allow them to be falsiﬁed by off-the-shelf adversarial attacks. We chose to use FGSM [59], BasicIterative Method (BIM) [60], DeepFool [61], and ProjectedGradient Descent (PGD) [62] as they were the top performingfalsiﬁers in the DNNF study. We use the same parameters foreach adversarial attack method as used in that study.
We will also use three top performing DNN veriﬁers. We
chose to use Neurify [53], VeriNet [63], and nnenum [64],which are all supported by DNNV [38], and have performedwell in recent benchmarks [65], [66]. In addition, each of theseveriﬁers have the ability to return counter-examples.
4) Metrics: For each run of the falsiﬁers and veriﬁers we
report the number of counter-examples found and the timeto ﬁnd each counter-example. To judge the quality of eachcounter-example we compute the mean reconstruction simi-larity (MRS) which, as discussed in §II, adapts ESRE to usethe SSIM metric. Given a reference V AE, V, MRS computes
for a given input, x, the expected similarity for a set of recon-
structed inputs, MRS(x,V)=
1
N/summationtextN
i=1SSIM(V(x),x).In
this work, we estimate the mean using a sample size, N,o f
100 reconstructions.
For each problem domain we also require a V AE model to
use as a ground truth for measuring the MRS. For Fashion-
MNIST we trained a fully-connected V AE model, VAE MRS ,
with a 100-dimensional latent space, and symmetric encoderand decoder, each with two hidden layers, one of 256 neuronsand one of 512 neurons, and ReLU activations. The decoderuses a Sigmoid activation so that output values are in the range0 to 1. We chose to use a model signiﬁcantly larger than thoseused for DFV for evaluating MRS under the assumption thata larger model would be able to better model the distributionand thus provide accurate MRS measures for all modelstested. For DroNet we trained a convolutional V AE model,Conv-VAE
DroNet , with symmetric encoder and decoder, and
a 512 dimensional latent space. The decoder consists of 8blocks, each composed of a convolutional transpose operationfollowed by batch normalization and an ELU activation, exceptfor the ﬁnal block, which uses a Sigmoid activation so thatoutput values are in the range 0 to 1. We chose to usethis model as the baseline for MRS, since we expected aconvolutional model to perform well on the image data ofthe DroNet benchmark.
5) Computing Resources: The experiments in this work
were run on nodes with Intel Xeon Silver 4214 processors at2.20 GHz and 512GB of memory. For RQ1 and RQ2 each jobwas allowed to use 1 processor core and unrestricted memory,and had a time limit of 1 hour, while falsiﬁcation jobs in RQ2– exploring the factors of the V AE – had a time limit of 5minutes. For RQ3, each job was allowed to use 2 processorcores and had a time limit of 1 hour.
V. R
ESULTS
A. RQ-1: on DFV efﬁcacy
In this ﬁrst experiment, we quantitatively and qualitatively
assess the effectiveness of DFV and its costs when applied inconjunction with 4 falsiﬁers and 3 veriﬁers.
Experimental Procedure. To answer RQ1, we use the
GHPR-FMNIST benchmark. We run both the veriﬁers andfalsiﬁers on this benchmark, with and without our DFVwith VAE
RQ1. We designed VAE RQ1so that all existing
tools could successfully run on it. This meant that we hadto constrain its size and type of activation functions sothat existing veriﬁers could process it. More speciﬁcally, wedesign VAE
RQ1with a single hidden layer of 24 neurons
in the decoder, and instead of a Sigmoid activation on theoutput, it uses an approximation of the Sigmoid functionwith ReLU activations, since, of the veriﬁers explored inthis work, only VeriNet supported non-ReLU activation func-tions. The approximation used is as follows Sigmoid (x)≈
ReLU(−ReLU(−0.25∗x+0.5)+1). We run each tool 5 times
322Fig. 4: MRS for generated counter-examples across falsiﬁers and veriﬁers, with and without DFV. Solid horizontal line indicates
the median MRS of the test set images reconstructed with VAE MRS . The shaded columns, measured in the y2-axis, represent
the number of counter-examples found. Falsiﬁers and veriﬁers applied with DFV generate fewer counter-examples and, onaverage, those counter-examples have four times higher MRS.
on every problem to account for random noise and we recordthe number of problems that return a sat result, indicating
that a counter-example was found, as well as the MRS of eachcounter-example. Each falsiﬁcation and veriﬁcation job had atimeout of 1 hour and used a radius of 3 in the latent space.
Analysis and Findings. We start by examining the mean
reconstruction similarity (MRS) measures for the counter-examples generated by DFV. The MRS values are computedbased on their reconstruction with VAE
MRS . Fig. 4 shows
box plots representing the distribution of the MRS of thecounter-examples found by each of the 7 tools (x-axis) whenapplied to the original DNN (red) and the DNN with theVAE
RQ1decoder (blue) generated by DFV. We ﬁnd that,
across all tools, the use of a model with DFV renders counter-examples that are reconstructed better by VAE
MRS than those
found in the original DNN. Indeed, the median MRS for thecounter-examples found in the original DNN is under 0.1,while the median MRS for the tools applied with DFV isabove 0.6. This implies that they are closer to the distributionlearned by VAE
MRS and thus may be closer to the true input
distribution. A statistical analysis of variance with the Kruskal-Wallis method
1conﬁrmed that the differences between using
and not using DFV on any given tool are signiﬁcant at p=0.05.
Fig. 4 also includes a horizontal line representing the
median MRS of the FashionMNIST test set. This line providesanother guideline to judge the quality of the counter-examples.Counter-examples found by the tools without DFV seem to bewell below the median MRS of the test data, indicating thatthey are constructed poorly, likely due to being far from thedistribution. Counter-examples found with DFV tend to have
1We had to perform the non-parametric Kruskal-Wallis test given the
different standard deviations observed across the distributions.MRS higher than the median of the test set, indicating thatthey come from the distribution.
The shaded columns in Fig. 4, measured on the y2-axis,
show the number of counter-examples found. These datashow that, as expected, the number of counter-examples foundwhen a tool is applied with a generative model decreases asirrelevant parts of the input space are pruned. For example,DeepFool found 74 counter-examples on the original DNNand 56 when applying DFV
2.
This portion of the study also revealed an interesting oppor-
tunity for veriﬁers. Based on the property design, we expectedthat the veriﬁers would not be able to prove a property oftype B, and were unlikely to prove one of type A on theoriginal DNN, which was indeed the case. However, whenwe used DFV, the nnenum veriﬁer was able to prove 25problems that held under the reduced input space encodedby the VAE
RQ1. This observation points to an opportunity
for enabling veriﬁcation to prove properties that may not holdover the whole input space but may hold over the relevantinput space as per the training distribution. In order for suchan approach to be effective, further studies are needed toguarantee that the generative model encodes a faithful modelof the input distribution. We discuss this further in future work.
We now qualitatively examine the counter-examples gener-
ated with and without DFV. The tabulated images in Fig. 5aare the counter-examples with the highest SSIM generated byeach tool on the DNN, and the ones in Fig. 5b with DFV.Without using DFV, we see in Fig. 5a the images generated
2One exception to this trend was nnenum, which reported a ﬂoating point
error when attempting to verify many properties on the original DNN but
it did not do the same with DFV. We conjecture that this is because DFVmay be steering the the tool away from inputs that cause the failure. We willcontact the developers to address this issue.
323(a) Without DFV
(b) With DFV
Fig. 5: Counter-examples with highest MRS found for GHPR-
FMNIST. Rows correspond to properties while columns corre-spond to tools. When applied with DFV , the counter-examplesappear to be much better aligned with the training distribution.
Fig. 6: Times to ﬁnd counter-examples by each tool. Blue boxplots represent the times of our approach, while red box plotsrepresent the times for a DNN alone. The shaded columns,measured in the y2-axis, represent the number of counter-examples found.
by all the falsiﬁers look like random noise, while the images
generated by the veriﬁers have a bit more structure, with largerblocks of similarly valued pixels, but still have little discerniblepattern. On the other hand, most of the counter-examplesgenerated with DFV in Fig. 5b bear some resemblance tothe training images (e.g., boots, pants, sandals), and some ofthem are clearly identiﬁable. We also notice that the counter-examples found with DFV for some properties correspondto distinct classes. We argue that when no counter-examplesare found for a property with a model, but are found for theoriginal DNN, like for Property A-1 and A-3, those counter-examples are likely to be invalid as they reside outside thedata distribution. By the same token, when counter-examplesare found with a model but not found without a model, likefor Property A-4, we argue that the model reduction enablestools to explore the pruned space more extensively, enablingtheir generation.
Last, we brieﬂy examine the time distribution for each tool
to generate the counter-examples. Fig. 6 presents box plots foreach of the tools, and we again plot the number of counter-examples on the y2-axis. As expected, falsiﬁers are faster thanveriﬁers. Looking at the 0.75 quartiles of the times spent bythe different tools, we can see that all falsiﬁers took under 1.5seconds, while the veriﬁers took up to 1444.5 seconds. PGDdetected the most counter-examples, 85 on the original DNNand 71 with the approach, while its median execution timewas just over a second. When comparing the boxes within atool, we ﬁnd that incorporating DFV did not have a majorimpact on the time taken by any of the tools
3.
Major Findings: Tools applied in conjunction with DFV
generate fewer counter-examples, with four times higher MRS,in negligible time, and that visually appear to be much betteraligned with the training distribution.
3The larger variation for Neurify can be attributed to the smaller number
of counter-examples it generated.
324(a) MRS of the counter-examples found
by PGD using DFV for each latent spacesize. The blue columns show the number ofcounter-examples found.
 (b) MRS of counter-examples found by PGDusing different decoder architectures with alatent space of size 8.
(c) MRS of counter-examples found by PGDusing DFV with VAE
8,2,256across different
radii.
Fig. 7: Study of impact of varying dimension, architecture, and radius of V AE in DFV.
B. RQ-2: on VAE structure effects on DFV
We now explore the effects of the V AE’s latent space size,
number and size of layers, and radius from the center of the
latent space distribution on the efﬁcacy of DFV.
Experimental Procedure. To answer RQ2, we again use
the GHPR-FMNIST benchmark while exploring several fac-tors that may affect the efﬁcacy of DFV. Given that there isa large number of conﬁgurations to explore, that the largerV AE conﬁgurations are not runnable by all veriﬁers, and thatthe performances of all falsiﬁers were similar in the ﬁrstexperiment, we selected to run all conﬁgurations and DFVwith only the PGD falsiﬁer. We ﬁrst explore factors relatedto the V AE architecture, varying the size of the latent space,the number of hidden layers, and the size of each layer. Weexplored latent space sizes of 1, 2, 4, 8, 16, and 32; hiddenlayer counts of 1, 2, and 4; and layer sizes of 16, 32, 64, 128,and 256. For each combination of factors, we trained a V AEon the Fashion-MNIST data and transformed the correctnessproblems using DFV, and ran PGD on the resulting problems.We will refer to each as VAE
d,l,h, wheredis the latent space
size,lis the number of hidden layers, and his the size of each
hidden layer. The model VAE 8,2,256 has an 8-dimensional
latent space with 2 hidden layers, each with 256 neurons.Second, we explore how the size of latent space region ofthe model affects the quality of the found counter-examples.We will specify the size of the input region by restricting theradius of the l
∞d-ball in the latent space of the V AE. We will
explore this factor with radii of 0.25 to 4, in 0.25 increments.To reduce the number of experiments, we use only the V AEthat performed best in the ﬁrst part of the experiment – witha high number of counter-examples found and high MRS. Forthis question, each falsiﬁcation job was run 5 times to reducethe effects of random noise and each job was given a timeoutof 5 minutes. For each combination of factors we report thenumber of counter-examples found, as well as the MRS ofeach counter-example.
Analysis and Findings. We explored a total of 90 V AE
conﬁgurations that work in conjunction with DFV. To controlfor randomness, we run each conﬁguration ﬁve times.
We start by examining the effect of the latent space size,
across all 15 of the V AE architectures, on the quality ofcounter-examples found, as measured by the MRS and thenumber of counter-examples found. Fig. 7a shows, across thelatent spaces, that the median MRS varies between 0.65 and0.75, and the number of counter-examples between 866 and1306. The maximum possible number of counter-examples inthis plot is 1500, since there are 15 V AE architectures for eachlatent space size, and 20 properties that were checked 5 timeseach for each architecture.
We observe that smaller latent spaces (LS={1,2}) appear to
generate counter-examples with slightly higher MRS, mainlybecause the model renders a less diverse set of images butof really good quality. The differences in MRS are conﬁrmedwith an ANOV A test of signiﬁcance and a multiple comparisonof latent space means with a Bonferroni correction acrossthe latent spaces. More speciﬁcally, the MRS for LS=1 issigniﬁcantly different from the rest of the latent spaces, anda LS=2 is signiﬁcantly different from the rest, at p=0.05.We conjecture that larger spaces are able to encode richerdata distributions enabling the generation of more and morediverse counter-examples that are sometimes farther from thedistribution (i.e., a sandal that appears as printed on a shirt).Still, for this particular benchmark the gains in the numberof counter-examples found and the losses in MRS seem tosaturate after latent spaces of size 8. Across all of the latentspaces sizes, PGD required a median of 0.5 seconds to ﬁndcounter-examples. The timing data for these experiments isavailable in the appendix.
We then selected the V AE architectures with LS=8, which
contained the architecture with the tying highest MRS withthe most counter-examples, to examine their variance. The x-axis of Fig. 7b contains the 15 V AE architectures we exploredspeciﬁed in the x-axis by the latent space size, the numberof layers, and the number of neurons. We note that thearchitectures with more layers appear to be able to producecounter-examples with higher MRS. For example, the medianfor the architectures with 1 layer was 0.58, with 2 layers was0.68, and with 4 layers was 0.79. An ANOV A conﬁrms thatthe differences across architectures are signiﬁcant at p=0.05,and a pair-wise comparison with a Bonferroni correctionreveals that all the architectures with 1 layer are signiﬁcantlydifferent from the ones with 4 layers. The ﬁgure also seems to
325indicate that, given the same number of layers, having more
neurons would render slightly higher MRS. For example, themedian for the architecture with 16 neurons was 0.59 andfor the three architectures with 256 neurons it was 0.71. Wenotice, however, that the number of counter-examples foundwas higher when fewer layers are used. We conjecture thathaving more layers further restricts the size of the input spacelearned by the V AE, perhaps due to the extra expressivepower of the additional layers. As with latent spaces, thetime to ﬁnd counter-examples did not vary signiﬁcantly acrossarchitectures, with most of them requiring a median of lessthan 1 second to ﬁnd a counter-example. The timing data forthese experiments is available in the appendix.
The last piece of this experiment explores changing the
radius of the constraints in the latent space. We examinethe effect of such changes on VAE
8,2,256 , the architecture
with the most counter-examples and greatest MRS in Fig. 7b.Fig. 7c shows that the MRS slightly decreases after the ﬁrstbound of 0.25 and then starts to increase with higher bounds,from a median of 0.80 for a radius of 0.25 to a medianof 0.68 for a radius of 1.25 and back to a median of over0.75 MRS with a radius of 4. An ANOV A test across radiiwas signiﬁcant at p=0.05, and a multple comparison with aBonferroni correction showed that radius 0.25 was deemedsigniﬁcantly different from radii 0.5, 0.75, 1 and 1.25 butnon-signiﬁcantly different from the higher radii (timing detailsare provided in the appendix). We also note that the numberof counter-examples found increases as the space to explorearound the captured data distribution increases from a radiusof 0.25 (25 counter-examples found) to 1.25 (95 counter-examples found).
Major Findings: V AE conﬁgurations with very limited
capacity (in layers, neurons, or latent space size) can have anoticeable effect on the DFV effectiveness, specially in thenumber of counter-examples being found. If more counter-examples are desirable, then one should increase the dimen-sionality of the latent space, reduce the number of layers,and increase the radius. If higher-quality counter-examples aremore desirable, then favoring a lower-dimensional latent spaceor smaller radius, or reducing the number of layers is indicated.
C. RQ-3: on DFV Scalability
In this experiment, we assess the scalability of DFV by
applying it to a large DNN model for autonomous UA V control
using 3 different input distribution models.
Experimental Procedure. To answer RQ3, we use the
larger and more complex GHPR-DroNet benchmark. We applythe PGD falsiﬁer to the benchmark, both as is, and using
DFV both with a V AE model, as well as with a GAN as
the generative model. We train a fully-connected V AE, FC-VAE
DroNet with a symmetric encoder and decoder. The
decoder of FC-VAE DroNet has 6 hidden layers in the decoder
with sizes 512, 512, 512, 512, 1024, and 2048, all withELU activations, except the ﬁnal layer which uses a Sigmoidactivation. For GAN
DroNet we train a DCGAN [67] model
on the DroNet dataset [39] with a Sigmoid on the ﬁnal layer.
Fig. 8: The mean reconstruction similarity (solid box), time(dashed box), and numbers of counter-examples (color bar) tothe DroNet properties.
Both models use a 512 dimensional latent space. As before, we
run each falsiﬁer 5 times to account for random noise and werecord the number of counter-examples found and the time toﬁnd each counter-example. Each job had a timeout of 1 hour.
Analysis and Findings. Fig. 8 shows box plots with solid
outlines for the distributions of the reconstruction similaritiesof counter-examples found using PGD on the DroNet DNNwithout DFV, as well as using DFV with the decoder ofFC-VAE
DroNet and the generator of GAN DroNet . Fig. 8
also shows the number of counter-examples found using eachmodel using bars with the count labeled above each bar. Weﬁnd that, for DFV with both models, while fewer counter-examples are found, they clearly have higher reconstructionsimilarities than those found using the DroNet model alone. In-deed, the MRS differences between DroNet, FC-VAE
DroNet ,
GAN DroNet are shown to be statistically signiﬁcant overall
by a Kruskal-Wallis test with p=0.05, and so do their pairwisedifferences. Corroborating the previous ﬁndings, this impliesthat the counter-examples found using DFV are closer to thedistribution learned by Conv-VAE
DroNet , the model used to
compute the MRS values, and thus may be closer to the actualinput distribution. Without DFV, violations were found for all10 properties across all 5 seeds. Using FC-VAE , 28 violations
were found for 6 properties. Using GAN , 9 violations were
found across 2 properties. While the lowest MRS for a counter-example found using DFV was 0.42, the MRS without DFVnever exceeded 0.11.
We now proceed to visually examine the counter-examples
generated with and without DFV for 5 properties. Fig. 9 showscounter-examples generated by PGD. The images generatedwithout DFV look like random noise, while the imagesgenerated with DFV, independent of the chosen model, havestructure and contain features seen in the training imagessuch as roads, trees, or horizon lines. The model used for
DFV has an impact on the images produced. While the
V AE model tended to produce blurrier counter-examples, theGAN model produced counter-examples with sharper lines,but fewer recognizable road features.
Finally, Fig. 8 shows box plots, with dashed outlines, of
the time to generate each counter-example using each model.
326(a) No model
(b) FC-VAE DroNet
(c)GAN DroNet
Fig. 9: Counter-examples to DroNet properties with 3 distinct input models. The counter-examples shown are from the ﬁnal
run of the falsiﬁer on each of the 10 properties. When applied in conjunction with DFV , whether using a V AE or a GAN, thegenerated counter-examples visually appear to be much better aligned with the training distribution.
The median time to falsify DroNet alone was 321 seconds,while DFV with FC-VAE
DroNet took 146 seconds and DFV
withGAN DroNet took 259 seconds, but there is enough
performance variance that those differences are not deemedstatistically signiﬁcant.
Major Findings: DFV can be applied with various models
without signiﬁcant time penalty, while also producing counter-examples with up to a nine-fold improvement in reconstructionsimilarity.
D. Threats to V alidity
External Validity. Three threats to the generalization of our
ﬁndings are our choice of tools, benchmarks, and generative
models to evaluate DFV. We mitigate the concern abouttools generality by selecting multiple falsiﬁers and veriﬁersas part of the ﬁrst research question. For the next questionswe traded generality across tools for more insights about theperformance of DFV under different models, which impliedthat we had to drop DNN veriﬁers from the rest of theassessment because they did not scale to the networks andmodels we were targeting. Regarding benchmarks, we selectedones from different domains, one a classiﬁcation task, and theother being a regression task, with very different architecturesand training data. Still, more benchmarks are needed to morebroadly explore the cost-effectiveness of DFV. To mitigatethe threat about model selection, we explored an extensive setof models in a systematic way. Still, the examination of moregenerative models is part of the future work.
Construct Validity. Our choice of MRS as a quality
measure and our personal qualitative judgment of generatedcounter-examples pose a threat in that the relevance of acounter-example could be judged by many means. We miti-gated this threat by basing MRS on a popular measure, ESRE,and specializing it to images with SSIM. We also provideresults using ESRE in the appendix, and we will exploreadditional measures including those for outlier detection inthe future and perform studies with users to help us judge thecounter-examples quality.
Internal Validity. Our training processes for the networks
and the models constitute a threat to the internal validity ofthe study as their correctness could have affect the ﬁndings.We have documented and programmed those processes whenpossible through scripts to facilitate their reproduction. Wealso mitigate this threat by making our data and scripts forrunning our experiments and analyzing our results publiclyavailable (see below). Another threat to validity is the ran-domness involved in training of networks and models, and inthe tools’ performance. We mitigated that threat by runningthose tools multiple times and showing their variability.
VI. C
ONCLUSION
This work introduces a novel approach, DFV, which en-
ables existing DNN veriﬁcation and falsiﬁcation techniquesto target the data distribution. DFV composes learned latentvariable generative distribution models with the DNN underanalysis, reformulating the problem so that generated counter-examples are on the data distribution. We explore different datadistribution models and ﬁnd that using even simple modelsyield substantially better counter-examples across a rangeof veriﬁcation and falsiﬁcation techniques for two differentbenchmarks.
These ﬁndings along with recent work on distribution-aware
testing [9], [7], suggest that models of the data distribution canplay an important role in V&V of DNNs. We plan to pursuefurther work along these lines. For example, how performancemetrics for latent variable generative models that assess theirprecision and recall [43], can guide the development of distri-bution models that are customized to best suite different V&Vactivities for DNNs.
A
RTIFACT AV AILABILITY
We provide an artifact containing the tool, as well as the data
and scripts required to replicate our study at https://zenodo.org/record/5104745
A
CKNOWLEDGMENT
This material is based in part upon work supported by
National Science Foundation awards 1900676 and 2019239.
327REFERENCES
[1] Y . Tian, K. Pei, S. Jana, and B. Ray, “Deeptest: automated testing
of deep-neural-network-driven autonomous cars,” in Proceedings of the
40th International Conference on Software Engineering, ICSE 2018,
Gothenburg, Sweden, May 27 - June 03, 2018, 2018, pp. 303–314.[Online]. Available: http://doi.acm.org/10.1145/3180155.3180220
[2] K. Pei, Y . Cao, J. Yang, and S. Jana, “Deepxplore: Automated
whitebox testing of deep learning systems,” in Proceedings of
the 26th Symposium on Operating Systems Principles, Shanghai,China, October 28-31, 2017, 2017, pp. 1–18. [Online]. Available:http://doi.acm.org/10.1145/3132747.3132785
[3] A. Odena, C. Olsson, D. Andersen, and I. Goodfellow, “TensorFuzz:
Debugging neural networks with coverage-guided fuzzing,” inProceedings of the 36th International Conference on Machine Learning,ser. Proceedings of Machine Learning Research, K. Chaudhuri andR. Salakhutdinov, Eds., vol. 97. Long Beach, California, USA:PMLR, 09–15 Jun 2019, pp. 4901–4911. [Online]. Available:http://proceedings.mlr.press/v97/odena19a.html
[4] X. Xie, L. Ma, F. Juefei-Xu, M. Xue, H. Chen, Y . Liu, J. Zhao, B. Li,
J. Yin, and S. See, “Deephunter: A coverage-guided fuzz testing frame-work for deep neural networks,” in 28th ACM SIGSOFT International
Symposium on Software Testing and Analysis, 2019.
[5] Y . Sun, M. Wu, W. Ruan, X. Huang, M. Kwiatkowska, and D. Kroening,
“Concolic testing for deep neural networks,” in Proceedings of the 33rd
ACM/IEEE International Conference on Automated Software Engineer-ing, 2018, pp. 109–119.
[6] D. Berend, X. Xie, L. Ma, L. Zhou, Y . Liu, C. Xu, and J. Zhao, “Cats are
not ﬁsh: Deep learning testing calls for out-of-distribution awareness,” in2020 35th IEEE/ACM International Conference on Automated SoftwareEngineering (ASE). IEEE, 2020, pp. 1041–1052.
[7] S. Dola, M. B. Dwyer, and M. L. Soffa, “Distribution-Aware Testing
of Neural Networks Using Generative Model,” in Proceedings of the
International Conference on Software Engineering, 2021.
[8] L. Ma, F. Juefei-Xu, F. Zhang, J. Sun, M. Xue, B. Li, C. Chen,
T. Su, L. Li, Y . Liu, J. Zhao, and Y . Wang, “Deepgauge:multi-granularity testing criteria for deep learning systems,” inProceedings of the 33rd ACM/IEEE International Conference onAutomated Software Engineering, ASE 2018, Montpellier , France,September 3-7, 2018, 2018, pp. 120–131. [Online]. Available:http://doi.acm.org/10.1145/3238147.3238202
[9] T. Byun and S. Rayadurgam, “Manifold for machine learning
assurance,” in Proceedings of the ACM/IEEE 42nd International
Conference on Software Engineering: New Ideas and EmergingResults, ser. ICSE-NIER ’20. New York, NY , USA: Associationfor Computing Machinery, 2020, p. 97–100. [Online]. Available:https://doi.org/10.1145/3377816.3381734
[10] D. Shriver, S. G. Elbaum, and M. B. Dwyer, “Reducing DNN Properties
to Enable Falsiﬁcation with Adversarial Attacks,” in Proceedings of the
International Conference on Software Engineering, 2021.
[11] C. E. Tuncali, G. Fainekos, H. Ito, and J. Kapinski, “Simulation-
based adversarial test generation for autonomous vehicles with machinelearning components,” in 2018 IEEE Intelligent V ehicles Symposium
(IV). IEEE, 2018, pp. 1555–1562.
[12] S. R. Choudhary, A. Gorla, and A. Orso, “Automated test input gen-
eration for android: Are we there yet?(e),” in 2015 30th IEEE/ACM
International Conference on Automated Software Engineering (ASE).IEEE, 2015, pp. 429–440.
[13] S. A. Khalek, G. Yang, L. Zhang, D. Marinov, and S. Khurshid,
“Testera: A tool for testing java programs using alloy speciﬁcations,” in2011 26th IEEE/ACM International Conference on Automated SoftwareEngineering (ASE 2011). IEEE, 2011, pp. 608–611.
[14] O. Tkachuk, M. B. Dwyer, and C. S. Pasareanu, “Automated en-
vironment generation for software model checking,” in 18th IEEE
International Conference on Automated Software Engineering, 2003.Proceedings. IEEE, 2003, pp. 116–127.
[15] D. Giannakopoulou, C. S. Pasareanu, and J. M. Cobleigh, “Assume-
guarantee veriﬁcation of source code with design-level assumptions,” inProceedings. 26th International Conference on Software Engineering.IEEE, 2004, pp. 211–220.
[16] C. Cadar, D. Dunbar, D. R. Engler et al., “Klee: unassisted and automatic
generation of high-coverage tests for complex systems programs.” inOSDI, vol. 8, 2008, pp. 209–224.[17] P. Dhaussy, J.-C. Roger, and F. Boniol, “Reducing state explosion with
context modeling for model-checking,” in 2011 IEEE 13th International
Symposium on High-Assurance Systems Engineering. IEEE, 2011, pp.130–137.
[18] D. Kroening and M. Tautschnig, “Cbmc–c bounded model checker,” in
International Conference on Tools and Algorithms for the Constructionand Analysis of Systems. Springer, 2014, pp. 389–391.
[19] M. R. Gadelha, F. R. Monteiro, J. Morse, L. C. Cordeiro, B. Fischer,
and D. A. Nicole, “Esbmc 5.0: an industrial-strength c model checker,”inProceedings of the 33rd ACM/IEEE International Conference on
Automated Software Engineering, 2018, pp. 888–891.
[20] M. Markthaler, S. Kriebel, K. S. Salman, T. Greifenberg, S. Hillemacher,
B. Rumpe, C. Schulze, A. Wortmann, P. Orth, and J. Richenhagen,“Improving model-based testing in automotive software engineering,”in2018 IEEE/ACM 40th International Conference on Software Engi-
neering: Software Engineering in Practice Track (ICSE-SEIP) . IEEE,
2018, pp. 172–180.
[21] C. S. P ˘as˘areanu, M. B. Dwyer, and W. Visser, “Finding feasible
counter-examples when model checking abstracted java programs,” inInternational Conference on Tools and Algorithms for the Constructionand Analysis of Systems. Springer, 2001, pp. 284–298.
[22] A. Gurﬁnkel, T. Kahsai, A. Komuravelli, and J. A. Navas, “The seahorn
veriﬁcation framework,” in International Conference on Computer Aided
V eriﬁcation. Springer, 2015, pp. 343–361.
[23] T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri, and
M. Vechev, “Ai2: Safety and robustness certiﬁcation of neural networkswith abstract interpretation,” in 2018 IEEE Symposium on Security and
Privacy (SP), May 2018, pp. 3–18.
[24] W. Ruan, X. Huang, and M. Kwiatkowska, “Reachability analysis of
deep neural networks with provable guarantees,” in IJCAI. ijcai.org,
2018, pp. 2651–2659.
[25] G. Singh, R. Ganvir, M. P ¨uschel, and M. T. Vechev, “Beyond the single
neuron convex barrier for neural network certiﬁcation,” in Advances
in Neural Information Processing Systems 32: Annual Conference onNeural Information Processing Systems 2019, NeurIPS 2019, 8-14December 2019, V ancouver , BC, Canada, H. M. Wallach, H. Larochelle,A. Beygelzimer, F. d’Alch ´e-Buc, E. B. Fox, and R. Garnett, Eds., 2019,
pp. 15 072–15 083. [Online]. Available: http://papers.nips.cc/paper/
9646-beyond-the-single-neuron-convex-barrier-for-neural-network-certiﬁcation
[26] G. Singh, T. Gehr, M. P ¨uschel, and M. T. Vechev, “An abstract domain
for certifying neural networks,” PACMPL, vol. 3, no. POPL, pp. 41:1–
41:30, 2019.
[27] W. Xiang, H. Tran, and T. T. Johnson, “Output reachable set estimation
and veriﬁcation for multilayer neural networks,” IEEE Transactions on
Neural Networks and Learning Systems, vol. 29, no. 11, pp. 5777–5783,Nov 2018.
[28] O. Bastani, Y . Ioannou, L. Lampropoulos, D. Vytiniotis, A. V .
Nori, and A. Criminisi, “Measuring neural net robustness withconstraints,” in Proceedings of the 30th International Conference
on Neural Information Processing Systems, ser. NIPS’16. USA:Curran Associates Inc., 2016, pp. 2621–2629. [Online]. Available:http://dl.acm.org/citation.cfm?id=3157382.3157391
[29] K. Dvijotham, R. Stanforth, S. Gowal, T. Mann, and P. Kohli, “A dual
approach to scalable veriﬁcation of deep networks,” in Proceedings
of the Thirty-F ourth Conference Annual Conference on Uncertainty inArtiﬁcial Intelligence (UAI-18). Corvallis, Oregon: AUAI Press, 2018,pp. 162–171.
[30] A. Raghunathan, J. Steinhardt, and P. Liang, “Certiﬁed defenses against
adversarial examples,” in ICLR. OpenReview.net, 2018.
[31] V . Tjeng, K. Y . Xiao, and R. Tedrake, “Evaluating robustness of
neural networks with mixed integer programming,” in International
Conference on Learning Representations, 2019. [Online]. Available:https://openreview.net/forum?id=HyGIdiRqtm
[32] E. Wong and J. Z. Kolter, “Provable defenses against adversarial
examples via the convex outer adversarial polytope,” in ICML, ser.
Proceedings of Machine Learning Research, vol. 80. PMLR, 2018,pp. 5283–5292.
[33] T. Weng, H. Zhang, H. Chen, Z. Song, C. Hsieh, L. Daniel, D. S. Boning,
and I. S. Dhillon, “Towards fast computation of certiﬁed robustnessfor relu networks,” in ICML, ser. Proceedings of Machine Learning
Research, vol. 80. PMLR, 2018, pp. 5273–5282.
[34] X. Huang, M. Kwiatkowska, S. Wang, and M. Wu, “Safety veriﬁcation
of deep neural networks,” in CA V (1), ser. Lecture Notes in Computer
Science, vol. 10426. Springer, 2017, pp. 3–29.
328[35] G. Katz, C. W. Barrett, D. L. Dill, K. Julian, and M. J.
Kochenderfer, “Reluplex: An efﬁcient SMT solver for verifying
deep neural networks,” in Computer Aided V eriﬁcation - 29th
International Conference, CA V 2017, Heidelberg, Germany, July 24-28,2017, Proceedings, Part I, 2017, pp. 97–117. [Online]. Available:https://doi.org/10.1007/978-3-319-63387-9
5
[36] R. Ehlers, “Formal veriﬁcation of piece-wise linear feed-forward neural
networks,” in Automated Technology for V eriﬁcation and Analysis
- 15th International Symposium, ATVA 2017, Pune, India, October3-6, 2017, Proceedings, 2017, pp. 269–286. [Online]. Available:https://doi.org/10.1007/978-3-319-68167-2\
19
[37] R. R. Bunel, I. Turkaslan, P. H. S. Torr, P. Kohli, and P. K. Mudigonda,
“A uniﬁed view of piecewise linear neural network veriﬁcation,” inNeurIPS, 2018, pp. 4795–4804.
[38] D. Shriver, S. G. Elbaum, and M. B. Dwyer, “DNNV: A framework for
deep neural network veriﬁcation,” in Computer Aided V eriﬁcation - 33rd
International Conference, CA V 2021, Virtual Event, July 20-23, 2021,Proceedings, Part I, ser. Lecture Notes in Computer Science, A. Silvaand K. R. M. Leino, Eds., vol. 12759. Springer, 2021, pp. 137–150.[Online]. Available: https://doi.org/10.1007/978-3-030-81685-8\
6
[39] A. Loquercio, A. I. Maqueda, C. R. D. Blanco, and D. Scaramuzza,
“Dronet: Learning to ﬂy by driving,” IEEE Robotics and Automation
Letters, 2018.
[40] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” in
2nd International Conference on Learning Representations, ICLR 2014,Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings,2014.
[41] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y . Bengio, “Generative adversarial nets,” inAdvances in Neural Information Processing Systems 27, Z. Ghahramani,M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, Eds.Curran Associates, Inc., 2014, pp. 2672–2680. [Online]. Available:http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf
[42] M. F. Naeem, S. J. Oh, Y . Uh, Y . Choi, and J. Yoo, “Reliable ﬁdelity and
diversity metrics for generative models,” in International Conference on
Machine Learning. PMLR, 2020, pp. 7176–7185.
[43] A. M. Alaa, B. van Breugel, E. Saveliev, and M. van der Schaar, “How
faithful is your synthetic data? sample-level metrics for evaluating andauditing generative models,” arXiv preprint arXiv:2102.08921, 2021.
[44] H. Xiao, K. Rasul, and R. V ollgraf. (2017) Fashion-mnist: a novel image
dataset for benchmarking machine learning algorithms.
[45] I. Goodfellow, Y . Bengio, and A. Courville, Deep Learning. MIT Press,
2016, http://www.deeplearningbook.org.
[46] L. Ruff, J. R. Kauffmann, R. A. Vandermeulen, G. Montavon, W. Samek,
M. Kloft, T. G. Dietterich, and K.-R. M ¨uller, “A unifying review of deep
and shallow anomaly detection,” Proceedings of the IEEE, 2021.
[47] T. Salimans, A. Karpathy, X. Chen, and D. P. Kingma, “Pixelcnn++:
Improving the pixelcnn with discretized logistic mixture likelihood andother modiﬁcations,” arXiv preprint arXiv:1701.05517, 2017.
[48] A. Vasilev, V . Golkov, M. Meissner, I. Lipp, E. Sgarlata, V . Tomassini,
D. K. Jones, and D. Cremers, “q-space novelty detection with varia-tional autoencoders,” in Computational Diffusion MRI, E. Bonet-Carne,
J. Hutter, M. Palombo, M. Pizzolato, F. Sepehrband, and F. Zhang, Eds.Cham: Springer International Publishing, 2020, pp. 113–124.
[49] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image
quality assessment: from error visibility to structural similarity,” IEEE
transactions on image processing, vol. 13, no. 4, pp. 600–612, 2004.
[50] B. Dai and D. P. Wipf, “Diagnosing and enhancing V AE models,” in
7th International Conference on Learning Representations, ICLR 2019,New Orleans, LA, USA, May 6-9, 2019, 2019. [Online]. Available:https://arxiv.org/abs/1903.05789
[51] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J.
Goodfellow, and R. Fergus, “Intriguing properties of neural networks,” in2nd International Conference on Learning Representations, ICLR 2014,Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings,Y . Bengio and Y . LeCun, Eds., 2014.
[52] X. Yuan, P. He, Q. Zhu, and X. Li, “Adversarial examples: Attacks and
defenses for deep learning,” IEEE Transactions on Neural Networks and
Learning Systems, vol. 30, no. 9, pp. 2805–2824, 2019.
[53] S. Wang, K. Pei, J. Whitehouse, J. Yang, and S. Jana, “Efﬁcient formal
safety analysis of neural networks,” in NeurIPS, 2018, pp. 6369–6379.
[54] G. Katz, D. A. Huang, D. Ibeling, K. Julian, C. Lazarus, R. Lim,
P. Shah, S. Thakoor, H. Wu, A. Zelji ´cet al., “The Marabou frameworkfor veriﬁcation and analysis of deep neural networks,” in International
Conference on Computer Aided V eriﬁcation, 2019, pp. 443–452.
[55] C. Liu, T. Arnon, C. Lazarus, C. Barrett, and M. J. Kochenderfer, “Algo-
rithms for verifying deep neural networks,” CoRR, vol. abs/1903.06758,
2019.
[56] D. Shriver, S. G. Elbaum, and M. B. Dwyer, “DNNF,” 2021,
https://github.com/dlshriver/DNNF. [Online]. Available: https://github.com/dlshriver/DNNF
[57] V . Riccio and P. Tonella, “Model-based exploration of the frontier of
behaviours for deep learning system testing,” in Proceedings of the 28th
ACM Joint Meeting on European Software Engineering Conferenceand Symposium on the F oundations of Software Engineering,ser. ESEC/FSE 2020. New York, NY , USA: Association forComputing Machinery, 2020, p. 876–888. [Online]. Available: https://doi.org/10.1145/3368089.3409730
[58] T. Byun, A. Vijayakumar, S. Rayadurgam, and D. Cofer, “Manifold-
based test generation for image classiﬁers,” in 2020 IEEE International
Conference On Artiﬁcial Intelligence Testing (AITest). IEEE, 2020, pp.15–22.
[59] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing
adversarial examples,” in 3rd International Conference on Learning
Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,Conference Track Proceedings, Y . Bengio and Y . LeCun, Eds., 2015.
[60] A. Kurakin, I. J. Goodfellow, and S. Bengio, “Adversarial examples in
the physical world,” in 5th International Conference on Learning Rep-
resentations, ICLR 2017, Toulon, France, April 24-26, 2017, WorkshopTrack Proceedings. OpenReview.net, 2017.
[61] S. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, “Deepfool: A simple
and accurate method to fool deep neural networks,” in 2016 IEEE
Conference on Computer Vision and Pattern Recognition, CVPR 2016,Las V egas, NV , USA, June 27-30, 2016 . IEEE Computer Society, 2016,
pp. 2574–2582.
[62] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards
deep learning models resistant to adversarial attacks,” in 6th Interna-
tional Conference on Learning Representations, ICLR 2018, V ancouver ,BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.OpenReview.net, 2018.
[63] P. Henriksen and A. Lomuscio, “Efﬁcient neural network veriﬁcation
via adaptive reﬁnement and adversarial search,” in ECAI 2020 - 24th
European Conference on Artiﬁcial Intelligence, 29 August-8 September2020, Santiago de Compostela, Spain, August 29 - September 8, 2020- Including 10th Conference on Prestigious Applications of ArtiﬁcialIntelligence (PAIS 2020), ser. Frontiers in Artiﬁcial Intelligence andApplications, G. D. Giacomo, A. Catal ´a, B. Dilkina, M. Milano,
S. Barro, A. Bugar ´ın, and J. Lang, Eds., vol. 325. IOS Press, 2020, pp.
2513–2520. [Online]. Available: https://doi.org/10.3233/FAIA200385
[64] S. Bak, H.-D. Tran, K. Hobbs, and T. T. Johnson, “Improved geometric
path enumeration for verifying relu neural networks,” in Computer
Aided V eriﬁcation, S. K. Lahiri and C. Wang, Eds. Cham: SpringerInternational Publishing, 2020, pp. 66–96.
[65] D. Xu, D. Shriver, M. B. Dwyer, and S. Elbaum, “Systematic gener-
ation of diverse benchmarks for dnn veriﬁcation,” in Computer Aided
V eriﬁcation CA V, 2020.
[66] C. Liu and T. T. Johnson, “Vnn-comp,”
https://sites.google.com/view/vnn20/vnncomp. [Online]. Available:https://sites.google.com/view/vnn20/vnncomp
[67] A. Radford, L. Metz, and S. Chintala, “Unsupervised representation
learning with deep convolutional generative adversarial networks,”in4th International Conference on Learning Representations, ICLR
2016, San Juan, Puerto Rico, May 2-4, 2016, Conference TrackProceedings, Y . Bengio and Y . LeCun, Eds., 2016. [Online]. Available:http://arxiv.org/abs/1511.06434
329