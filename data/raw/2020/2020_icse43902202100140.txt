FlakeFlagger: Predicting Flakiness Without
Rerunning Tests
Abdulrahman Alshammari1, Christopher Morris2, Michael Hilton2and Jonathan Bell3
1George Mason University, Fairfax, V A, USA
2Carnegie Mellon University, Pittsburgh, PA, USA
3Northeastern University, Boston, MA, USA
aalsha2@gmu.edu, christom@andrew.cmu.edu, mhilton@cmu.edu, j.bell@northeastern.edu
Abstract ‚ÄîWhen developers make changes to their code, they
typically run regression tests to detect if their recent changes
(re)introduce any bugs. However, many tests are Ô¨Çaky, and
their outcomes can change non-deterministically, failing without
apparent cause. Flaky tests are a signiÔ¨Åcant nuisance in the
development process, since they make it more difÔ¨Åcult for
developers to trust the outcome of their tests, and hence, it is
important to know which tests are Ô¨Çaky. The traditional approach
to identify Ô¨Çaky tests is to rerun them multiple times: if a
test is observed both passing and failing on the same code, it
is deÔ¨Ånitely Ô¨Çaky. We conducted a very large empirical study
looking for Ô¨Çaky tests by rerunning the test suites of 24 projects
10,000 times each, and found that even with this many reruns,
some previously identiÔ¨Åed Ô¨Çaky tests were still not detected. We
propose FlakeFlagger, a novel approach that collects a set of
features describing the behavior of each test, and then predicts
tests that are likely to be Ô¨Çaky based on similar behavioral
features. We found that FlakeFlagger correctly labeled as Ô¨Çaky
at least as many tests as a state-of-the-art Ô¨Çaky test classiÔ¨Åer, but
that FlakeFlagger reported far fewer false positives. This lower
false positive rate translates directly to saved time for researchers
and developers who use the classiÔ¨Åcation result to guide more
expensive Ô¨Çaky test detection processes. Evaluated on our dataset
of 23 projects with Ô¨Çaky tests, FlakeFlagger outperformed the
prior approach (by F1 score) on 16 projects and tied on 4
projects. Our results indicate that this approach can be effective
for identifying likely Ô¨Çaky tests prior to running time-consuming
Ô¨Çaky test detectors.
I. I NTRODUCTION
Regression testing is widely used in quality assurance to
determine if recent changes to a codebase (re)introduce errors.
When a test fails, developers typically expect that this failure
represents a bug recently introduced. However, a growing and
concerning trend is regression tests failures which are not in
fact due to recent changes, but are instead Ô¨Çaky tests.
Flaky tests are non-deterministic tests which pass and fail
when run on the exact same version of a codebase [1].
When tests alternate between passing and failing without
any code changes, developers can be frustrated: Ô¨Çaky tests
are challenging to debug, and a single failing test can halt
release cycles. Recent academic and industrial studies have
worked towards deÔ¨Åning and reporting Ô¨Çaky tests [1]‚Äì[5],
automatically detecting Ô¨Çaky tests [6], [7], and investigating
the occurrence of Ô¨Çakiness [8]‚Äì[10]. In fact, Ô¨Çaky tests appear
in most large-scale projects. Google has reported that 4.56%of test failures are caused by Ô¨Çaky tests [1], and Microsoft‚Äôs
Windows and Dynamics products report a similar 5% [5].
Flaky test detection is crucial because Ô¨Çakiness reduces
tests‚Äô reliability and frustrates developers. When Ô¨Çaky tests
only fail occasionally, these tests can be very hard to repro-
duce [1], and developers then spend a signiÔ¨Åcant amount of
time and resources trying to Ô¨Åx these Ô¨Çaky failures. Because
non-deterministic failures are not necessarily a result of a
regression in the code, the very nature of Ô¨Çakiness means
developers have a very difÔ¨Åcult time locating the source of the
Ô¨Çakiness. Hence, automatically determining if a test is Ô¨Çaky or
not has become a signiÔ¨Åcant topic in recent software testing
research [1], [5], [6], [9], [11]‚Äì[15].
Existing techniques for detecting Ô¨Çaky tests rely on rerun-
ning tests: if we can witness two different outcomes (passing
and failing) from the same test on the same version of the
codebase, then surely that test is Ô¨Çaky. Even state-of-the-
art techniques like NonDex [16] and iDFlakies [17] that
inject additional non-determinism still rely on re-running each
test case many times. However, rerunning tests can be quite
expensive. Most prior academic studies that detected which
tests were Ô¨Çaky have re-executed test suites a handful of times:
10 times [6], 16 times [7] or 100 times [17]. Developers have
reported re-running suspected Ô¨Çaky tests up to 1,000 times to
increase the likelihood of determining if a test is Ô¨Çaky [18].
In this paper, we describe an empirical study of Ô¨Çaky tests
that we conducted by rerunning tests 10,000 times each, which
can provide guidance to developers and other researchers on
how many times to re-run tests to detect Ô¨Çakiness. While rerun-
ning tests 10,000 times may be impractical for developers to
perform regularly (or even for researchers to perform regularly
to gather datasets), we used the result of this experiment to
construct a rich dataset of Ô¨Çaky tests. This dataset has many
uses, for instance, to study how developers can better identify,
repair and prevent Ô¨Çaky tests.
A promising alternative approach to detect Ô¨Çaky tests is
to create a machine learning classiÔ¨Åer that can distinguish
between Ô¨Çaky and non-Ô¨Çaky tests [12], [19]. In such an
approach, developers train a classiÔ¨Åer using a known corpus
of Ô¨Çaky and non-Ô¨Çaky tests, and then apply that classiÔ¨Åer to a
new codebase in order to detect new Ô¨Çaky tests. Alternatively,
rather than re-run alltests in a corpus thousands of times in
order to Ô¨Ånd Ô¨Çaky tests, developers or researchers could use
15722021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)
1558-1225/21/$31.00 ¬©2021 IEEE
DOI 10.1109/ICSE43902.2021.00140
this classiÔ¨Åer to determine which tests are more likely to be
Ô¨Çaky, and focus computational resources on re-running those
tests Ô¨Årst. Unfortunately, we found that existing approaches
are not suitable to this sort of use case: in our evaluation, a
state-of-the-art Ô¨Çaky test classiÔ¨Åer had promising recall (72%),
but extremely low precision (11%).
Ideally, the features used by the classiÔ¨Åer should be applica-
ble to a broad range of projects, so that a model built on one set
of projects (with known Ô¨Çaky tests) could be applied to a new
project to Ô¨Ånd new Ô¨Çaky tests. A recent survey has found 23
factors that increase, decrease and otherwise affect the ability
to identify Ô¨Çakiness in tests [8]. These factors include features
such as the presence of test smells, hard coded values, the
age of test cases, and more. Inspired by this work, we collect
various features like these for each of the tests in our Ô¨Çaky
test dataset and created FlakeFlagger, a tool to automatically
predict if a test is Ô¨Çaky or not using these features.
We evaluated FlakeFlagger on our large dataset of Ô¨Çaky
tests, and found it signiÔ¨Åcantly more effective than the state-
of-the-art Ô¨Çaky test classiÔ¨Åer: On the 23 projects evaluated,
FlakeFlagger outperformed the prior approach in 16 and
underperformed in only 3 projects ‚Äî where two of them have
only three Ô¨Çaky tests in total. The four remaining projects
have 0% F1 score in the FlakeFlagger and the state-of-the-art
Ô¨Çaky test classiÔ¨Åer evaluation. Section IV presents a complete
per-project evaluation.
This paper makes the following main contributions:
Dataset: A new dataset of Ô¨Çaky tests created by exhaus-
tively executing 24 project‚Äôs test suites 10,000 times each
‚Äî orders of magnitude more than prior work [6], [7],
[12]. We show that for many Ô¨Çaky tests, it is necessary
to rerun them very many times to detect them.
New Approach and Tool: FlakeFlagger, a new hybrid
static/dynamic approach to collect behavioral features
of tests and then predict whether tests are Ô¨Çaky or
not. FlakeFlagger can be used to guide other Ô¨Çaky test
detection tools towards those most likely to be Ô¨Çaky.
Evaluation: We show that FlakeFlagger can predict
whether a test is Ô¨Çaky or not with far fewer false positives
than prior work (just 380 false positives, compared to
4,674 from prior work). We show that test execution time,
test coverage of source code, test coverage of changed
lines, and usage of third party libraries are effective
predictors of Ô¨Çakiness. We do not Ô¨Ånd any tokens that
commonly exist in Ô¨Çaky tests, and we do not Ô¨Ånd test
smells to be a strong indicator of Ô¨Çakiness.
Our dataset, test running infrastructure, and FlakeFlagger itself
are publicly released under a BSD license and can be found
on GitHub [20] and in our permanently archived artifact [21].
II. E MPIRICAL STUDY
The traditional approach to Ô¨Ånd Ô¨Çaky tests is to rerun each
test many times: if a test outcome changes (without any change
to the code), then the test is Ô¨Çaky. This deÔ¨Ånition captures
both tests that typically pass (but infrequently fail), and those
that typically fail (but infrequently pass). However, there islittle guidance (from industry or academia) in terms of how
many times to rerun each test: most prior work has considered
rerunning 10 [6], 16 [7], 100 times [17] or in limited cases,
4,000 times [22]. But, if each Ô¨Çaky test is Ô¨Çaky for some
non-deterministic reason, what conÔ¨Ådence do we have that we
will observe its outcome change within those reruns? If it is
feasible to Ô¨Ånd most Ô¨Çaky tests after only rerunning them a
handful of times, then perhaps this Ô¨Çaky test rerun problem
is not such a big problem: rerunning tests only 3-5 times
does take time, but it certainly takes less time than rerunning
them 10,000 times. Since we don‚Äôt know the underlying
source of non-determinism that causes the Ô¨Çaky test to fail,
aseparate problem is: how much harder is it for a developer
to reproduce that Ô¨Çaky failure in a different environment , where
various uncontrolled factors might vary (e.g., OS, Java version,
CPU, available memory, network speed, etc.). This problem
is particularly relevant for developers who are tasked with
debugging a Ô¨Çaky test on their local machines which failed on
a build server. To motivate our work in detecting Ô¨Çaky tests, we
conducted an empirical study, investigating these questions:
MQ1: How many Ô¨Çaky tests can be found by rerunning tests
given different rerun budgets?
MQ2: How hard is it to reproduce a Ô¨Çaky test failure?
These questions are important not only for this work, but for
any researcher interested in developing new techniques to help
developers detect and debug Ô¨Çaky tests.
A. Study Design
To answer these questions, we selected 24 projects that have
been previously studied in the context of Ô¨Çaky tests [6], [7]. In
comparison, the dataset of Ô¨Çaky tests produced by Bell et al.‚Äôs
DeFlaker [6] was constructed by running tests once on a revi-
sion of a project, then only rerunning tests that were observed
to fail (up to 15 times). Lam et al.‚Äôs iDFlakies dataset limited
the number of reruns per-project to take no more than 56 hours
per-project, resulting in typically under 100 reruns per-project,
and often did not run all tests in every project [7]. While Lam
et al. have also conducted evaluations with more test re-runs
(4,000), these evaluations were targeted to only portions of
test suites with known Ô¨Çaky tests [22]. While this approach
can reduce the overall amount of computation time needed for
an experiment (Lam et al. report this experiment consumed 90
days of computation time), focusing on rerunning only known
Ô¨Çaky tests can bias a dataset to only include those Ô¨Çaky tests
that had previously been found through fewer reruns. Instead,
we ran each project‚Äôs entire test suite 10,000 times, using over
5 years of computation time in total.
We executed this experiment by creating a queue of jobs,
where each job represented a single execution of a project‚Äôs
test suite (running ‚Äúmvn install‚Äù). After running each job, we
archived all log Ô¨Åles and then removed all temporary Ô¨Åles and
rebooted the machine, and then proceeded to run the next test
suite in the queue. This approach was designed to provide
reasonable isolation between test runs, and simulates how a
real build server might compile and test a project.
1573TABLE I: Flaky tests detected by re-running test suites 10,000 times. We estimate the percentage of all Ô¨Çaky tests that would
be detected if only 10, 100 or 1,000 reruns had been performed. Color bars are stacked bar charts showing the percentage of
tests that failed with a given frequency. Columns DeFlaker andIDFlakies show the number of non-order dependent Ô¨Çaky tests
found in total by those prior works, and the number of Ô¨Çaky tests shared by both datasets. Blank cells indicate that a different
revision of the project was used due to historical compilation issues. Complete data and results available in our artifact [21].
Flaky by DeFlaker [6] iDFlakies [7] % of all Flaky Tests Detectable at: Distribution of Failure Frequencies, as % of Tests Failing
Project Tests Reruns Shared Total Shared Total 10 Reruns 100 Reruns 1,000 Reruns (0,10] (10, 100] (100, 1,000] (1,000, 10,000] runs of 10,000
spring-boot 2,128 163 0 5 71% 71% 77%
hbase 431 145 0 1 52% 59% 75%
alluxio 187 116 2 2 0% 91% 100%
okhttp 810 100 8% 12% 15%
ambari 324 52 1 1 0% 2% 94%
hector 142 33 1 1 3% 3% 100%
activiti 2,044 32 0% 3% 44%
java-websocket 145 23 22 52 0% 26% 87%
wildÔ¨Çy 1,238 23 0% 0% 4%
httpcore 712 22 1 1 0% 9% 9%
logback 842 22 5% 9% 41%
incubator-dubbo 2,177 19 5 12 5% 11% 26%
http-request 163 18 0% 83% 83%
wro4j 1,145 16 1 1 44% 50% 81%
orbit 86 7 0 1 14% 43% 86%
undertow 183 7 0 3 0% 0% 29%
achilles 1,317 4 0% 25% 75%
elastic-job-lite 558 3 1 6 0% 0% 0%
zxing 345 2 2 2 0% 100% 100%
assertj-core 6,267 1 1 1 0% 100% 100%
commons-exec 55 1 0% 0% 100%
handlebars.java 428 1 0% 100% 100%
ninja 306 1 1 1 0% 100% 100%
jimfs 212 0 No Ô¨Çaky tests observed
Total 22,245 811 10 20 28 70 26% 45% 67%
We considered approaches to increase the likelihood of
Ô¨Çaky test detection. For instance, some tests might be Ô¨Çaky
because they assume that a standard library method provides
deterministic behavior, even when it doesn‚Äôt (e.g., an iterator
over a set, which might return objects in the same order
each time, or might not) [16]. Other Ô¨Çaky tests may be order
dependent [23], which means that when they run in a different
order than is expected, they can fail (be Ô¨Çaky). Rather than
simply re-run such tests 10,000 times, one could rerun them
in a different order each time, thereby increasing the likelihood
of observing a Ô¨Çaky test [7]. However, there are far more root
causes of Ô¨Çaky tests than only these two [1], and we did not
want to bias our dataset to include more Ô¨Çaky tests of any
single category than another. For instance: in their survey of
developer-reported Ô¨Çaky tests, Luo et al. found that test order
dependencies accounted for only 13% of the Ô¨Çaky tests that
developers reported in issue trackers [1]. Instead, our dataset
represents the Ô¨Çaky tests that would be observed in the course
of normal development, when tests are run on a build server
‚Äî with no artiÔ¨Åcial non-determinism.
We ran these experiments on Ubuntu 18.04 VMs running
OpenJDK 1.8.0 242 with 4 CPUs and 8GB of RAM. Note that
perhaps, we could also Ô¨Ånd more Ô¨Çaky tests by using a variety
of platforms and computing devices: using different versions
of Java and different operating systems might introduce more
non-determinism overall, Ô¨Ånding more Ô¨Çaky tests. However,
again, our goal in this study is to Ô¨Ånd the Ô¨Çaky tests that a
developer might observe in the normal course of developmentwhen running their test suite on a standard continuous inte-
gration build server, and notto inject artiÔ¨Åcial noise. Hence,
each execution of each test occurs using a standard platform.
B. Study Results
By running each test 10,000 times, we observed 811 Ô¨Çaky
tests: about 3.6% of the total number of tests were Ô¨Çaky. The
number of observed Ô¨Çaky tests varies from one project to
another. We found 10 projects with less than 10 Ô¨Çaky tests, 4 of
which have only one, and one with none. On the other hand,
there are 4 projects which have more than 100 Ô¨Çaky tests.
Spring-boot has 163 Ô¨Çaky tests, 20% of the total observed
Ô¨Çaky tests. Table I summarizes these results.
MQ1: Flaky failures by number of reruns . While it is
not possible to truly state what the probability is of a Ô¨Çaky
test failing (since we don‚Äôt know the hidden, uncontrolled
condition that causes the failure), we estimate the probability
that each Ô¨Çaky test would have been detected with fewer
reruns by assuming that each test failure occurs independently.
Overall, only roughly a quarter of all of the Ô¨Çaky tests that we
found in 10,000 runs would have been found with 10 reruns,
roughly half with 100 reruns and roughly two thirds with
1,000 reruns. We also visualize this distribution in Table I,
categorizing each Ô¨Çaky test as failing either between 0 and
10 times, between 10 and 100 times, between 100 and 1,000
times, and Ô¨Ånally over 1,000 times (out of the 10,000 runs).
These sets are represented by colors as shown in Table I. The
redbar, which refers to tests that Ô¨Çake less than or equal 10
1574>_Project TestsRun TestsFeatures Collection PhaseList of FeaturesAnalyze CodeSource of Flaky TestsLabeledTestsFeature outputs for each testPrediction PhaseSupervised LearningDataSplit &ProcessingDataInspectionPrediction StrategyCheckTPFPFNTNPrediction ResultConfusion Matrixof TestsFig. 1: Overview of FlakeFlagger‚Äôs approach to predict likely Ô¨Çaky tests given a set of known Ô¨Çaky tests.
times, takes the majority in 8 projects. In general, we found
more than 33% of total Ô¨Çaky tests fail in less than or equal
10 times, 22% of Ô¨Çaky tests fail more than 10 and less than
or equal 100 out of 10,000. This suggests that researchers
building Ô¨Çaky test datasets should consider as many reruns as
possible in order to detect Ô¨Çaky tests: the chance of observing
that a test is Ô¨Çaky might be quite slim indeed. Furthermore, we
acknowledge that even after 10,000 re-runs, it is still possible
that we have not detected every Ô¨Çaky test in this dataset.
However, this threat is present in anyÔ¨Çaky test dataset, and we
believe that our empirical design is sufÔ¨Åcient for our needs.
MQ2: Reproducing Flaky Tests Failures. While MQ1
provides insight into the difÔ¨Åculty of identifying Ô¨Çaky tests
by re-running them on the same platform, this does not
quite capture the challenge that a developer would face when
reproducing those Ô¨Çaky tests in a different environment (e.g.
their local machine). To simulate this activity, we compared
the set of Ô¨Çaky tests identiÔ¨Åed from our 10,000 reruns with
those detected by prior researchers on the same versions of the
same projects, but in different environments. We speciÔ¨Åcally
chose the projects and revisions of the projects that we studied
in order to align with revisions of projects studied by either
DeFlaker [6] or iDFlakies [7]. The columns DeFlaker and
iDFlakies in Table I show the total number of Ô¨Çaky tests that
that paper reported on that version of that project, along with
the number of those tests that were also found to be Ô¨Çaky based
on our reruns. A blank entry indicates that the revision of that
project that we executed did not have any Ô¨Çaky tests reported
by the prior work. The DeFlaker dataset contains Ô¨Çaky tests
from many revisions of each project, but we only studied a
single revision of each project, and hence, it is possible that
DeFlaker had not identiÔ¨Åed any Ô¨Çaky tests in that revision.
The iDFlakies dataset consists of both order dependent tests
(which are detected by shufÔ¨Çing execution orders), and non-
order dependent tests (which are Ô¨Çaky regardless of execution
order). Since we purposefully did not shufÔ¨Çe the execution
order of the tests (as described above), we include only the
non-order dependent tests from iDFlakies for comparison.
Comparing to DeFlaker, we found 10 Ô¨Çaky tests out of the
20 tests identiÔ¨Åed as Ô¨Çaky by DeFlaker. Unfortunately, the
DeFlaker authors did not retain the build logs from their test
runs, so we are unable to diagnose why those tests appeared
as Ô¨Çaky to DeFlaker but not to our reruns. Comparing to
iDFlakies, we found 28 Ô¨Çaky tests out of the 70 non-order
dependent tests that we reran. In the case of iDFlakies, the
authors didretain the build logs that show how these testsfailed, and we conÔ¨Årmed by hand that the tests that we missed
in our rerun experiment truly were Ô¨Çaky, and could have been
detected as Ô¨Çaky if we had rerun them more. These results are
indicative of the true non-determinism of Ô¨Çaky tests and the
difÔ¨Åculties that developers face reproducing them: even with
10,000 reruns, we could not detect all Ô¨Çaky tests.
Study Summary: The results from this study demonstrate
that Ô¨Çaky tests are extremely difÔ¨Åcult to detect. If developers
would like to try to Ô¨Ånd all tests that are Ô¨Çaky in their test suite,
they likely should consider re-running those tests thousands of
times: only half of the Ô¨Çaky tests that we found failed more
than 100 times out of the 10,000 runs. Moreover, we know that
even running tests 10,000 times will still not guarantee that
all Ô¨Çaky tests have been found, since we did not succeed in
reproducing many Ô¨Çaky test failures observed in prior work.
This study underscores the need for approaches that detect
Ô¨Çaky tests without rerunning them.
III. A PPROACH
Our primary goal with FlakeFlagger is to create a new
approach to proactively identify which tests in a test suite are
Ô¨Çaky, before they become a nuisance and without rerunning
them many times. For instance, when a new test is introduced,
developers might use FlakeFlagger to identify if that new
test is likely to become Ô¨Çaky ‚Äî since research has shown
that most Ô¨Çaky tests are Ô¨Çaky when they are introduced [27].
Alternatively, developers might use FlakeFlagger to aid in a
search for Ô¨Çaky tests in a large test suite, where developers
identify that a portion of the test suite is or is not Ô¨Çaky, and
use FlakeFlagger to help label the rest of the tests as Ô¨Çaky
or not. Researchers constructing new approaches to detect
and repair Ô¨Çaky tests need large datasets of Ô¨Çaky tests, and
FlakeFlagger is perfectly suited to help Ô¨Ånd these Ô¨Çaky tests
faster. Instead of re-running every test in our dataset 10,000
times, if we already had trained FlakeFlagger on a subset
of the tests or projects, we could have greatly reduced the
execution time needed to rerun these tests by only re-running
those reported by FlakeFlagger as likely to be Ô¨Çaky. Note
that FlakeFlagger is orthogonal to techniques that mutate the
execution environment in order to increase the likelihood of
a test appearing Ô¨Çaky: these tools are still expensive to run,
and FlakeFlagger can help focus computing time on the tests
most likely to be Ô¨Çaky. Even if FlakeFlagger presents a false
positive (i.e., a test FlakeFlagger declares to be Ô¨Çaky, even
though it is not), our approach can help developers reduce
1575TABLE II: Complete list of features captured for test Ô¨Çakiness prediction. The Covered Lines Churn feature is rep-
resented in multiple forms based on the hvalues (number of the past commits). In our evaluation, we considered
h= 5;10;25;50;75;100;500and 10;000
Feature DescriptionTest SmellsIndirect Testing True if the test interacts with the object under test via an intermediary [24]
Eager Testing True if the test exercises more than one method of the tested object [24]
Test Run War True if the test allocates a Ô¨Åle or resource which might be used by other tests [24]
Conditional Logic True if the test has a conditional if-statement within the test method body [25]
Fire and Forget True if the test launches background threads or tasks. [26]
Mystery Guest True if the test accesses external resources [24]
Assertion Roulette True if the test has multiple assertions [24]
Resources Optimism True if the test accesses external resources without checking their availability [24]Numeric FeaturesTest Lines of Code Number of lines of code in the test method body
Number of Assertions Number of assertions checked by the test
Execution Time Running time for the test execution
Source Covered Lines Number of lines covered by each test, counting only production code
Covered Lines Total number of lines of code covered by the test
Source Covered Classes Total number of production classes covered by each test
External Libraries Number of external libraries used by the test
Covered Lines Churn h-index capturing churn of covered lines in past 5, 10, 25, 50, 75, 100, 500, and 10,000 commits. Each value hindicates that at least hlines
were modiÔ¨Åed at least htimes in that period.
the candidate number of tests they need to investigate via re-
running or manual inspection.
By proactively identifying Ô¨Çaky tests, we may also help
developers understand why these tests are Ô¨Çaky. Prior work
has suggested different properties of tests that might make
them more likely to be Ô¨Çaky, and FlakeFlagger can report
which of these features are present in each test [4], [8]. In
practice, if a feature has a strong correlation with Ô¨Çakiness,
developers might choose to focus on this feature in their future
test maintenance and development activities.
Figure 1 shows a high-level overview of our approach
to detect Ô¨Çaky tests. First, we collect a series of features
describing each test in a project. In a developer‚Äôs scenario,
we would assume that some of these tests would be known to
be Ô¨Çaky, and the developers would be interested in detecting
other Ô¨Çaky tests. For instance, the developer might know the
Ô¨Çaky tests that exist in their test suite currently, and would like
to identify if a newly written test is Ô¨Çaky. We collect behavioral
features (such as API usage) of each test, and then construct
a classiÔ¨Åer to predict which tests are Ô¨Çaky. In a controlled
experiment (with known Ô¨Çaky tests), we can perform cross-
validation, and generate a confusion matrix that describes the
performance of the classiÔ¨Åer. In practice, without an oracle,
we would present a report to developers including a list of
likely Ô¨Çaky tests.
A. Prediction Features
To develop a list of features that may be predictive of
Ô¨Çakiness, we look to prior Ô¨Çaky test research [1], [4], [6],
[8]. Ahmed et al. [8] categorized 23 developer-reported factors
which affect test Ô¨Çakiness. These features are described by
practitioners at a high level, and include test case complexity,
hard-coded values and test smells. Eck et al. [4] interviewed
21 developers about Ô¨Çaky tests and tabulated the frequency
of different kinds of Ô¨Çaky tests as well as developers‚Äô Ô¨Åxes
for those Ô¨Çaky tests. Whereas prior Ô¨Çakiness classiÔ¨Åcation
approaches used static, code-level features (e.g. presence of
textual tokens in the body of each test method), these surveys
describe features that are more nuanced. For instance, Eck etal. note that many Ô¨Çaky tests are Ô¨Çaky due to causes not in
the test method itself, but instead, in the production code that
is executed by that test [4].
Inspired by previous studies on test Ô¨Çakiness, we developed
a list of sixteen features, some of are based on general studies
on the causes of Ô¨Çaky tests [1], [8], while others are deÔ¨Åned as
bad practices in writing unit tests [28]. Unfortunately, some of
these Ô¨Çaky test root cases are too complicated to detect without
human intervention, for instance, Eck et al‚Äôs ‚ÄúToo restrictive
range‚Äù (which effectively describes the case where an assertion
is wrong). Hence, we considered all of the features described
in the prior works, and then selected only those for which we
could write automated detectors. We implemented detectors
for each of the features shown in Table II.
While some of the features can be detected by inspecting
the test method statically (speciÔ¨Åcally, the conditional logic
smell and test line of code), the rest of the features require
more than static analysis. For instance: existing automated test
smell detectors have analyzed only the code that is present in a
test method [29]‚Äì[32], and hence can fail to label tests that are
smelly because they invoke a helper method, which in turn,
performs smelly behavior. For instance, the ‚ÄúFire and Forget‚Äù
smell exists in tests that spawn background threads or tasks;
a smell detector that considered only direct calls to launch
threads in the test method body would not deÔ¨Åne a test as
smelly if it called a helper method to launch that background
thread. Since our goal is not to precisely detect test smells (as
identiÔ¨Åed by humans), but rather, to Ô¨Ånd features that may
be representative of Ô¨Çaky tests, we decided to expand our
deÔ¨Ånition of many of these smells to be inclusive of all code
executed by a test, rather than just the code contained in the
test method body itself.
We developed a hybrid static/dynamic framework to collect
the statement coverage of each test, and then statically analyze
the covered code in order to collect these behavioral features.
For instance, we determined that a test had the ‚ÄúÔ¨Åre and
forget‚Äù smell feature if, anytime during its execution (including
in the production code that is called by the test), that test
launched a background thread or task. We also collect a variety
1576of other features related to the statement coverage of each
test, such as how many recently changed lines of code are
covered. We implemented this framework as an extension to
the Maven build system, allowing for a zero-conÔ¨Åguration
feature collection process that automatically parses the target
project‚Äôs build scripts to modify them to collect coverage and
report features. Due to space limitations, we omit additional
details on precisely how each feature is detected, and make
our entire implementation publicly available [20], [21].
While we have built FlakeFlagger to analyze Java code
using the Maven build system, our approach is sufÔ¨Åciently
general to be applied to other build systems or languages. This
list of features is not intended to be complete: there may yet
be other features that can be easily collected and will be useful
for predicting test Ô¨Çakiness. However, we empirically found
that this set of features yielded good prediction performance
for FlakeFlagger (Section IV). Again, by making our dataset
and infrastructure publicly available, we enable future research
on behavioral features of Ô¨Çaky tests.
B. Flaky Tests ClassiÔ¨Åer
For our classiÔ¨Åer, each test instance iis represented as a
vectorfx1; x2; x3; : : : ; x ngwhere each xrepresents a feature
value and ncorrespond the total number of features. To
avoid inaccuracies in training, we perform an automated data
inspection and cleaning process as follows:
Instances with missing values (e.g., missing features) can
have a negative impact on model performance [33]. Since
we use different approaches to collect different features, it
is possible for some features to be missing for some tests.
For instance: if a test crashes in the middle of its execution
(e.g., with an unrecoverable out-of-memory exception), we
will be unable to collect telemetry from that test, and hence,
unable to calculate several of our features. Some tests are not
written in Java, and hence the feature detectors may not be
applicable to them, and due to inheritance, some tests may
not have source code in the project under test (with the test
residing in a 3rd party library). We found this occurred fairly
infrequently (roughly 1% of the tests - the difference between
Tables I and III) and handle tests with at least one missing
value by excluding them from our experiment (future work
could support such tests).
Representing the range of numeric values of each feature
is also very important. Some features have discrete values
‚Äî for instance, each smell-related feature has a boolean
value (smelly or not). However, there are also features with
continuous values shown in Table II, which do not have a
maximum value, and may need to be discretized [34]. From
a model learning perspective, discretization is crucial because
discretized features have a better chance of correlating with
the class value as the range of values are limited, which
helps features to be interpreted [35]. We considered two main
approaches to discretization. First, we considered values as
they are, without applying any discretization techniques. We
consider this option because we expect to not have long ranges
of continuous data for most features. The second approach weconsidered is to divide continuous data into a Ô¨Åxed number
of intervals, called bins, which is a common techniques to
discretize continuous data [34].
Our approach is agnostic to the classiÔ¨Åcation algorithm that
we use to build the model. Hence, rather than build a single
model, we designed FlakeFlagger to construct a set of mod-
els based on seven different supervised learning algorithms:
Decisions Tree (DT), Random Forest (RF), Support Vector
Machine (SVM), Multilayer Perceptron (MLP), Naive Bayes
(NB), Adaboosting (Ada), and K-Nearest Neighbor (KNN),
using the Scikit-learn package [36]. This selection of models
builds on Ô¨Çaky test classiÔ¨Åcation prior work that considered
DT, RF, SVM, KNN and NB models only, which found that
RF performed best [12]. In our evaluation (described in the
following section), we also found that the Random Forest had
the best performance, and report results only for this model,
but make all models available in our artifact [20], [21]. We
perform a feature selection process using information gain ,
which computes the amount of information that a feature can
provide for a classiÔ¨Åcation [37]. The range of information
gain is between 0 and 1, where higher values indicate more
predictive power. Imbalanced datasets (where there is not an
equal number of instances in each class ‚Äî Ô¨Çaky and non-
Ô¨Çaky tests in our case) usually have very low information gain
values. We compute the information gain for each feature, and
include only features with an information gain of at least 0:01,
following previous work [12].
IV. E VALUATION
To evaluate our classiÔ¨Åer, we designed experiments to
answer the following research questions:
RQ1: How effective is FlakeFlagger at predicting Ô¨Çaky tests?
RQ2: How helpful is each feature in distinguishing between
Ô¨Çaky and non Ô¨Çaky tests?
A. Experimental Design
To evaluate FlakeFlagger, we used the extensive dataset
described in Section II as a source of Ô¨Çaky tests. To collect
the various features needed to predict Ô¨Çakiness, we re-executed
each test one more time (using the feature extractor) using the
same environment.
Machine learning classiÔ¨Åers rely on two data sets: one
to build the model (training) and another for testing. We
apply k-fold cross validation [38], [39] to evaluate our model.
However, k-fold cross validation is most applicable to data
that is evenly balanced, where the proportions of each class
(in our case: Ô¨Çaky and not Ô¨Çaky) are similar. Balanced datasets
reduce the risk of any of the kfolds having only a single (or
no) instances of one of the classes (Ô¨Çaky and not Ô¨Çaky).
Since most tests are not Ô¨Çaky, we have imbalanced data, and
hence, have designed FlakeFlagger to use two data sampling
techniques: SMOTE [40] and random undersampling [41],
which we also compare to a baseline without sampling. We
perform this sampling only when training the model, and not
when testing it, to ensure a valid and fair result. We train
and test our models considering all of the tests from all of
the projects in a single bucket, randomly shufÔ¨Çing which tests
1577from which project appear in each fold. To evaluate the effect
that the size of the testing and training dataset has on the
classiÔ¨Åcation results, we consider both 10% and 20% testing
data sizes. Following past work [12], we consider only features
with an information gain of at least 0.01, and report the
information gain of each feature.
In our prediction evaluation, we label each prediction result
as a True Positive (TP), False Negative (FN), False Positive
(FP), or True Negative (TN) as follows: TP - predicted Ô¨Çaky,
known to be Ô¨Çaky; FP - predicted Ô¨Çaky, not known to be Ô¨Çaky;
FN - predicted not Ô¨Çaky, known to be Ô¨Çaky; TN - predicted
not Ô¨Çaky, not known to be Ô¨Çaky. We also evaluate our models
using F1-score, which is computed using the standard formula
based on Recall and Precision. Lastly, we calculate the Area
Under the Curve (AUC), a measure of how effective a model
is at distinguishing classes (in our case, Ô¨Çaky and not Ô¨Çaky).
Note that in our evaluation, false positives represent the
number of tests that might erroneously be considered as Ô¨Çaky
by developers, resulting in excess effort spent re-running them
to determine if they are Ô¨Çaky or not. We focus primarily on
total positives, because we have conÔ¨Ådence that the collected
Ô¨Çaky tests are indeed Ô¨Çaky, but we cannot be conÔ¨Ådent in our
classiÔ¨Åcation of a test as not Ô¨Çaky. In other words, the oracle
we use is a result of detecting Ô¨Çaky tests after 10,000 runs for
each test, but this does not guarantee that the ‚Äúnot Ô¨Çaky‚Äù tests
are really not Ô¨Çaky: they may just not have been observed to
be Ô¨Çaky. This approach also allows us to conÔ¨Årm FNs are
truly Ô¨Çaky tests because they fail at least once during rerun
tests. However, because of the inherent non-determinism in
Ô¨Çaky tests, we cannot construct a reliable oracle to evaluate
TNs and FPs, but report them as-is.
B. RQ1: EfÔ¨Åcacy of our ClassiÔ¨Åer
Following the evaluation procedure described in the previ-
ous section, we trained and tested FlakeFlagger. We created
several models, consider 2 approaches to discretization, 2 sizes
of training sets, 3 data balancing approaches, and 7 different
classiÔ¨Åcation algorithms. Due to space limitations, we show
only the results of the single best model conÔ¨Åguration: a
random forest model built using the SMOTE technique for
balancing the training data (and using unbalanced testing data),
no discretization and a 90-10 training-testing split (our artifact
includes all results [21]). We found that SMOTE worked better
than other sampling techniques due to the relatively small ratio
of Ô¨Çaky tests to the total number of tests. Random forest was
far more effective than the other algorithms, but we did not
Ô¨Ånd a meaningful difference in performance between a 10%
and 20% testing data size.
In order to evaluate the performance of our Ô¨Çaky test
classiÔ¨Åer, we compared its prediction results to the state-
of-the-art Ô¨Çaky test classiÔ¨Åer, a vocabulary-based approach
proposed by Pinto et al. [12]. This approach extract tokens
from each test using a simple bag-of-words model, under
the theory that Ô¨Çaky tests may use similar APIs, keywords,
etc., and hence, tests can be classiÔ¨Åed as Ô¨Çaky or not based
on the presence of various tokens (the model also uses onenon-token feature: the length of the test method). We also
considered a hybrid model that adds the token features to
FlakeFlagger‚Äôs features. Note that our evaluation methodology
differs somewhat from the prior approach, in which the authors
trained and tested their model on a balanced Ô¨Çaky/non-Ô¨Çaky
dataset with the exact same number of Ô¨Çaky and non-Ô¨Çaky
tests: we consider a more realistic scenario where the model
is trained on a balanced Ô¨Çaky/non-Ô¨Çaky dataset, but tested on
the complete set of tests (which is not balanced) [12]. From a
methodological perspective, balancing the number of Ô¨Çaky and
non-Ô¨Çaky tests by ignoring tests from the majority (non-Ô¨Çaky)
class may result in over approximating the performance of
the model, if this balancing is done during the testing phase.
This is a particularly important distinction on projects with
very large numbers of tests such as incubator-dubbo , with
2,174 tests of which only 19 were Ô¨Çaky: our cross-validation
approach would test the model on a all of the 2,174 tests,
whereas the methodology used in [12] would test the model
on a much smaller set of non-Ô¨Çaky tests. In our evaluation,
we use stratiÔ¨Åed cross-validation.
Table III presents the results of this experiment, showing the
true positives, false negatives, false positives, true negatives,
precision, recall and F1 score for each approach broken down
by project. Even though the models are not trained and tested
by project (i.e. the project name is not a feature in the model),
after running the evaluation we mapped each test back to
its project to allow us to inspect how the models performed
on each project. We show the aggregate precision, recall, F1
score, and AUC. FlakeFlagger is a learning-based approach,
and hence, we expect that it might perform better on projects
that have many Ô¨Çaky tests (providing more training data that
could help the classiÔ¨Åer to better predict Ô¨Çaky tests in that
project) than those with very few. To avoid biasing our dataset
towards projects with the most Ô¨Çaky tests, we did not impose
a threshold for a minimum number of Ô¨Çaky tests to include
the project in our evaluation, including all projects with at lest
one Ô¨Çaky test. We do not include the 298 tests for which we
were unable to collect all features (typically due to the tests
having non-Java components).
Overall, FlakeFlagger and the vocabulary-based approach
both detected a very similar number of Ô¨Çaky tests (599 and
583 respectively, out of a total of 808 Ô¨Çaky tests), but the
two approaches varied dramatically in terms of precision ‚Äî
FlakeFlagger had a far lower false positive rate with just
406, compared to 4,683 false positives from the vocabulary-
based approach. We found that combining FlakeFlagger‚Äôs
features with the tokens from the vocabulary-based approach
yielded slightly improved performance. The following section
explores which features from the vocabulary-based approach
contributed to this improvement.
Considering our initial use-case of a researcher or developer
using FlakeFlagger to determine which tests to run time-
intensive Ô¨Çaky test detectors on, using either FlakeFlagger or
the vocabulary-based approach would result in roughly the
same number of Ô¨Çaky tests eventually detected (that is, both
have comparable recall). However, the total amount of time
1578TABLE III: Prediction performance for FlakeFlagger, the vocabulary-based approach, and the hybrid combination of both. The
hybrid approach builds a model with both FlakeFlagger‚Äôs and the vocabulary-based approach‚Äôs features. We show the number
of True Positives, False Negatives, False Positives and True Negatives, Precision, Recall, and F1 scores per-project. The AUC
value is calculated after each fold where the reported value is the overall averages of AUC values after all folds. Projects with
zero F1 values have very low numbers of Ô¨Çaky tests (less than 3 per project), and illustrate known limitations of FlakeFlagger.
Flak y by Flak eFlagger Vocabulary-Based Approach [12] Combined Approach
Project Tests Reruns TPFN FP TN Pr R F TPFN FP TN Pr R F TPFN FP TN Pr R F
spring-boot 2,108 160 139 21 15 1,933 90% 87% 89% 134 26 703 1,245 16% 84% 27% 143 17 18 1,930 89% 89% 89%
hbase 431 145 129 16 32 254 80% 89% 84% 89 56 152 134 37% 61% 46% 130 15 33 253 80% 90% 84%
alluxio 187 116 116 0 0 71 100% 100% 100% 108 8 11 60 91% 93% 92% 116 0 0 71 100% 100% 100%
okhttp 810 100 52 48159 551 25% 52% 33% 79 21 444 266 15% 79% 25% 46 54104 606 31% 46% 37%
ambari 324 52 47 5 3 269 94% 90% 92% 36 16 121 151 23% 69% 34% 47 5 3 269 94% 90% 92%
hector 142 33 30 3 8 101 79% 91% 85% 13 20 23 8636% 39% 38% 25 811 98 69% 76% 72%
activiti 2,043 32 10 22 43 1,968 19% 31% 24% 12 20 531 1,480 2% 38% 4% 725 34 1,977 17% 22% 19%
java-websocket 145 23 19 4 1 121 95% 83% 88% 23 0 74 4824% 100% 38% 19 4 4 118 83% 83% 83%
wildÔ¨Çy 1,023 23 11 12 27 973 29% 48% 36% 20 3 554 446 3% 87% 7% 17 6 24 976 41% 74% 53%
httpcore 712 22 14 823 667 38% 64% 47% 16 6375 315 4% 73% 8% 15 724 666 38% 68% 49%
logback 805 22 319 17 766 15% 14% 14% 10 12 259 524 4% 45% 7% 517 11 772 31% 23% 26%
incubator -dubbo 2,174 19 811 35 2,120 19% 42% 26% 11 8813 1,342 1% 58% 3% 13 623 2,132 36% 68% 47%
http-request 163 18 12 6 6 139 67% 67% 67% 16 2 84 61 16% 89% 27% 12 6 6 139 67% 67% 67%
wro4j 1,135 16 412 21,117 67% 25% 36% 214 101 1,018 2% 12% 3% 016 11,118 0% 0% 0%
orbit 86 7 1 6 8 71 11% 14% 12% 6 1 32 47 16% 86% 27% 1 6 7 72 12% 14% 13%
underto w 183 7 2 5 8 168 20% 29% 24% 6 1 63 113 9% 86% 16% 3 4 8 168 27% 43% 33%
achilles 1,317 4 2 2 3 1,310 40% 50% 44% 0 4 0 1,313 0% 0% 0% 0 4 0 1,313 0% 0% 0%
elastic-job-lite 558 3 0 3 0 555 0% 0% 0% 0 3 34 521 0% 0% 0% 1 2 0 555 100% 33% 50%
zxing 345 2 0 2 2 341 0% 0% 0% 1 1 144 199 1% 50% 1% 0 2 2 341 0% 0% 0%
assertj-core 6,261 1 0 1 56,255 0% 0% 0% 0 1 66,254 0% 0% 0% 0 1 06,260 0% 0% 0%
commons-e xec 55 1 0 1 1 53 0% 0% 0% 1 0 18 36 5% 100% 10% 0 1 1 53 0% 0% 0%
handlebars.ja va 420 1 0 1 5 414 0% 0% 0% 0 1 91 328 0% 0% 0% 0 1 0 419 0% 0% 0%
ninja 307 1 0 1 3 303 0% 0% 0% 0 1 50 256 0% 0% 0% 0 1 0 306 0% 0% 0%
Total 21,734 808 599 209 406 20,520 60% 74% 66% 583 225 4,683 16,243 11% 72% 19% 600 208 314 20,612 66% 74% 86%
AUC(Average per fold) 86% 75% 86%
needed to run such an experiment would vary dramatically
between the two models, since FlakeFlagger had far fewer
false positives (406 vs 4,683). Assuming that each test would
take a comparable amount of time to run Ô¨Çaky test detectors
on, our developer (or researcher) would be able to conÔ¨Årm
the Ô¨Çakiness of FlakeFlagger‚Äôs 1,005 reported Ô¨Çaky tests (599
TPs plus 406 FPs) in roughly 18% of the time that it would
take to conÔ¨Årm the Ô¨Çakiness of the 5,266 reported Ô¨Çaky by
the vocabulary-based approach (583 TPs plus 4,683 FPs).
We were initially surprised that the precision of the
vocabulary-based approach‚Äôs was so much lower than Flake-
Flagger‚Äôs, and indeed, lower than reported by the original
authors [12]. We found that the tokens used in Pinto et
al.‚Äôs bag-of-words model did indeed frequently occur in Ô¨Çaky
tests, but also occurred quite frequently in non-Ô¨Çaky tests. For
example, one of the most relevant tokens that the model relied
upon (both in our study, and in [12]) was the Java throws
keyword. However, when examining the entire corpus, we
discovered that this keyword is used quite frequently in both
Ô¨Çaky and non-Ô¨Çaky tests, and hence, is not a very good
predictor of Ô¨Çakiness.
FlakeFlagger‚Äôs performance varied across projects: on some
projects (e.g., alluxio), we had perfect precision and recall,
while on others (e.g., okhttp and activiti) the approach was less
successful. We investigated more closely the different factors
that could cause such a varied performance among different
projects. The Ô¨Årst and most obvious factor is the size of the
training data: our model performed best on the two projectswhich had the most known Ô¨Çaky tests (alluxio and spring-boot
each had more than 100). On projects with very few known
Ô¨Çaky tests (less than 4), FlakeFlagger did not classify any
of the Ô¨Çaky tests as Ô¨Çaky, resulting in F1 scores of 0. This
results from the lack of training data that are representative
of the Ô¨Çaky tests in these projects. However, note that even
on these projects with so few Ô¨Çaky tests (e.g., zxing with
only two known Ô¨Çaky tests, ninja with only one), even though
FlakeFlagger failed to identify the Ô¨Çaky tests (true positives),
it had far fewer false positives than the other approach.
More broadly speaking, we can attribute the variation of
prediction performance between projects to the relative gen-
erality of our features (such as test execution time, coverage
of recently changed lines, etc.). Each project has its own
environmental assumptions, development patterns, and other
unique characteristics that can make it difÔ¨Åcult to create a
single general-purpose approach to classifying tests as Ô¨Çaky or
not. Another explanation for why performance varies across
projects may be that not all Ô¨Çaky tests have been labeled
correctly ‚Äî no rerun-based technique can guarantee to Ô¨Ånd
all Ô¨Çaky tests (even after 10,000 reruns). That is: there may
be tests that are labeled as ‚Äúnot Ô¨Çaky‚Äù in our dataset that are
in fact Ô¨Çaky, but we simply did not observe any Ô¨Çaky failure
of those tests in our experiments.
The higher number of observed Ô¨Çaky tests in a single project
does not guarantee that FlakeFlagger performs well. Some
Ô¨Çaky failures are due to rare dependency conÔ¨Çicts and network
failures that are not captured well from our features described
1579TABLE IV: List of top 23 features by information gain (IG) for
FlakeFlagger and the vocabulary-based approach. Our models
only include features with IG 0:01.
Vocabulary-Based Features FlakeFlagger Features
Feature/Token IG Feature IG
Test Lines of Code 0.023 Execution Time 0.121
throws 0.022 Source Covered Lines 0.067
should 0.020 Source Covered Classes 0.057
exception 0.018 Covered Lines 0.034
mtfs 0.018 Covered Changes (past 75 commits) 0.029
runbuildfortask 0.017 Covered Changes (past 50 commits) 0.028
tfs 0.017 Covered Changes (past 100 commits) 0.028
run 0.016 Covered Changes (past 500 commits) 0.024
transitive 0.016 Test Lines of Code 0.023
ioexception 0.015 Covered Changes (past 10 commits) 0.018
tachyon 0.014 Covered Changes (past 1000 commits) 0.015
Ô¨Åleid 0.011 Covered Changes (past 5 commits) 0.011
if 0.011 External Libraries 0.011
actual 0.010 Covered Changes (past 25 commits) 0.010
someinfo 0.010 Fire and Forget 0.007
testutils 0.010 Number of Assertions 0.006
writetype 0.010 Resources Optimism 0.005
some 0.009 Mystery Guest 0.003
checkspring 0.009 Assertion Roulette 0.002
testÔ¨Åle 0.009 Conditional Logic 0.002
createbyteÔ¨Åle 0.009 Indirect Testing 0.001
family 0.009 Test Run War 0.001
checkcommonslogging 0.009 Eager Testing 0.000
in Table II. For example, we notice that okhttp has a high
number of false positives and false negatives. With a further
inspection on this particular project, we found that a group
of tests had all failed in the same way due to the same
dependency problem in one single run. However, only some
tests that used this dependency presented a Ô¨Çaky failure to us,
and hence, we observed many false positives from our model,
with other (not Ô¨Çaky) tests that use this same dependency
classiÔ¨Åed to be Ô¨Çaky. In this same project, our model also
showed a large number of false negatives (tests that are Ô¨Çaky
but predicted as not Ô¨Çaky). Upon further investigation, we
found that these tests were Ô¨Çaky due to transient network-
related failures that occurred when they were run. While our
model does consider usage of network APIs (via the ‚Äúmystery
guest‚Äù smell), we found that most tests that used network APIs
were not labeled as Ô¨Çaky in our dataset because we did not
witness a network failure in the 10,000 runs of those tests. As
a result, the few tests that didfail due to network failures were
typically labeled as not Ô¨Çaky by the model, resulting in false
negatives. We discuss these limitations further in Section V.
C. RQ2: How useful is each feature?
To gain more insight into which textual and behavioral
features are correlated with test Ô¨Çakiness, we report the
information gain of each feature in FlakeFlagger‚Äôs model, and
the top 23 features in the model built using the vocabulary-
based approach.
Table IV lists all of FlakeFlagger‚Äôs features sorted by
information gain, along with the top 23 features used by the
comparison system based on the vocabulary-based approach
[12]. Note that we used only features with an information
gain of at least 0:01in our models, but we include all top
23 features in this table. Overall, we found that features that
considered dynamic behavior from each test (e.g., executiontime, covered lines, and coverage of recently changed lines)
had a far greater information gain than the tokens that were
statically extracted from the test method bodies. None of the
test smells that we collected had a strong information gain,
which may indicate that test smells are not well-correlated
with test Ô¨Çakiness ‚Äî all of the smells had an information gain
less than our threshold of 0:01, and in fact, ‚Äúeager testing‚Äù had
an information gain of less than 0:001. We found that the top
eight FlakeFlagger features each had a higher information gain
than the highest gain vocabulary feature.
The most effective features (execution time, various forms
of coverage, size of test and usage of external APIs) are
all measures of test case complexity. This conÔ¨Årms industry
reports that correlate test size with test Ô¨Çakiness [42]. For
instance: it is perhaps likely that small unit tests are less likely
to be Ô¨Çaky than large integration tests ‚Äî and this behavior is
captured by both the execution time of each test, and also by
the coverage of each test. Due to varying development styles,
other behavioral features likely have high variance between
projects (without signaling Ô¨Çakiness). For instance, ‚ÄúNumber
of Assertions‚Äù likely varies more between projects because
of coding conventions more than it varies between Ô¨Çaky and
non-Ô¨Çaky tests. Future work might consider ways to normalize
these features by-project to account for such variance, and
we make all of our tools and data available to allow other
researchers to join in this effort.
In the model built using the vocabulary-based approach [12],
the features with the highest information gain were: test lines
of code, presence of the ‚Äòthrows‚Äô Java keyword, and several
tokens like ‚Äòshould‚Äô, ‚Äòexception‚Äô, and ‚Äòmtfs‚Äô, each with an
information gain signiÔ¨Åcantly lower than the top features in
FlakeFlagger‚Äôs model. We believe that these tokens might be
over-Ô¨Åtting to speciÔ¨Åc patterns in some of the Ô¨Çaky tests in our
dataset, and indeed, these top three tokens differ from those
reported by Pinto et al. in their initial work [12]. Table V
directly compares the frequency of those three tokens in Ô¨Çaky
and other (non-Ô¨Çaky) tests reported by the vocabulary-based
approach [12] and also their frequencies in the dataset that
we used in this study. While the prior study found that these
tokens occurred far more frequently in Ô¨Çaky tests than in non-
Ô¨Çaky tests, when looking at our corpus, we found that these
tokens occurred far more frequently in non-Ô¨Çaky tests than in
Ô¨Çaky tests.
The majority of the Ô¨Çaky tests in the prior study with the
‚Äòjob‚Äô token came from a single project, ‚Äúoozie,‚Äù which we did
not study in our evaluation. At the same time, the majority
of non-Ô¨Çaky tests with the token ‚Äòjob‚Äô in our dataset were
in the project ‚Äúelastic-job-lite,‚Äù which was not included in
the prior evaluation. Clearly, the co-occurrence of individual
tokens with Ô¨Çaky tests can vary dramatically between projects.
Terms that correlate with Ô¨Çakiness in one project can not be
expected to correlate with Ô¨Çakiness in other projects ‚Äî this is
also evident from the limited number of projects which contain
each token.
Note that this Ô¨Ånding only underscores the need for a large,
balanced dataset of Ô¨Çaky tests (like the one we collected and
1580TABLE V: Comparison showing the predictive power of the
top 3 tokens reported by the vocabulary-based approach evalu-
ation on the DeFlaker dataset [12] compared to this evaluation
on our dataset. For each token, we show the information gain
(IG), the number of Ô¨Çaky tests containing that token (number
of projects containing those tests in parentheses), and the
number of other, non Ô¨Çaky tests containing that token (with
the number of projects containing those tests).
Vocabulary-Based Evaluation [12] This Evaluation
Token IG Flaky Not Flaky IG Flaky Not Flaky
job 0:2053 524 (2) 4 (1) 0:00044 48 (4) 703 (8)
table 0:1449 406 (4) 8 (2) 0:00819 114 (4) 405 (11)
id 0:1419 522 (9) 52 (4) 0:00037 125 (9) 2,429 (19)
described in Section II): the DeFlaker dataset that Pinto et
al. used contained more Ô¨Çaky tests than our dataset (1,403
vs 810). However, a single project in that dataset (‚Äúoozie‚Äù)
contributed more than half of those Ô¨Çaky tests (856), which
can make it extremely difÔ¨Åcult to draw conclusions that can
generalize beyond a single project, or beyond the dataset. We
look forward to contribute to new community efforts to build
shared Ô¨Çaky test datasets [43], making all of the log Ô¨Åles from
each of the 10,000 executions of each test suite in our study
publicly available in our artifact [21].
V. D ISCUSSION AND THREATS TO VALIDITY
While we are conÔ¨Ådent in our Ô¨Åndings, it is nonetheless
worthwhile to acknowledge various threats to the validity
of our conclusions. For instance, the list of projects in our
prediction experiment may not be representative of all projects,
limiting the generalizability of our conclusions. We reduce this
threat by selecting different projects from different domains.
Further, our project list is based on those studied by others.
RQ1 showed that our classiÔ¨Åer was able to predict 599 out
of 808 Ô¨Çaky tests. However, note that our ground truth is
imperfect: some of the false positives may in fact be true
positives, where the failure is in the rerun process to identify
that the test is Ô¨Çaky. Unfortunately, it may never be possible to
provably identify all Ô¨Çaky tests in a project. As we witnessed
in our empirical study (Section II), the environment in which
a test runs may affect if the test appears Ô¨Çaky. Since on the
same revisions of the same projects, we found a different set
of Ô¨Çaky tests than were found by the iDFlakies and DeFlaker
authors ‚Äî- even after re-running those tests far more times
than the original authors ‚Äî we recognize this as a limitation.
It may be possible to Ô¨Ånd more Ô¨Çaky tests by injecting
artiÔ¨Åcial non-determinism into the execution of each test, for
instance, arbitrarily slowing the execution of each test or shuf-
Ô¨Çing test orders. These other approaches may Ô¨Ånd more Ô¨Çaky
tests of one particular type: for instance, reordering tests will
expose more order dependent Ô¨Çaky tests, perturbing timing
may expose more concurrency-related Ô¨Çaky tests. However,
a signiÔ¨Åcant concern with any such approach is that it will
result in a dataset that is overly representative of those speciÔ¨Åc
sources of Ô¨Çakiness. For example, Luo et al.‚Äôs study found thatjust 13% of Ô¨Çaky tests were caused by order dependencies [1]:
an approach that built a dataset of primarily tests Ô¨Çaky due to
this cause would likely not represent all Ô¨Çaky tests.
Similarly, we observed that some (but far from most) tests
that made calls through the network could fail due to Ô¨Çakiness
in the event of a network failure. Hence, one approach to Ô¨Ånd
more Ô¨Çaky tests could be to inject network failures while tests
run. However, we speculate that the result of this experiment
would likely be that alltests that make network calls will be
labeled as Ô¨Çaky. Such an approach would likely not be very
useful for developers, who presumably are already aware that
tests that rely on network resources might fail if the network
is unreliable. This mirrors the sentiment expressed by Harman
and O‚ÄôHearn suggesting that based on Facebook‚Äôs experiences,
we should ‚Äúassume that all tests are Ô¨Çaky‚Äù [14] and Google,
where all tests that execute in more than a single thread
are assumed to be potentially Ô¨Çaky [44]. Hence, we remain
conÔ¨Ådent that our 10,000-rerun based Ô¨Çaky test detection
approach represents the best-possible approach to build our
dataset: one that includes quite literally, the Ô¨Çaky tests that
developers might have observed in the course of normal
development, without injecting additional non-determinism.
We considered multiple supervised learning algorithms and
found that Random Forest resulted in the best performance
on our dataset. This result may not hold on other datasets.
Similarly, we found that some features (like execution time,
and coverage of recently changed lines) are more predictive of
Ô¨Çakiness than others (like test smells), and these Ô¨Åndings may
not hold on other datasets. We include all results and scripts
in our artifact, and encourage other researchers to experiment
with different conÔ¨Ågurations of FlakeFlagger [21].
Our evaluation of FlakeFlagger presents a direct comparison
to a state-of-the-art Ô¨Çaky test prediction approach [12], but
does not include a direct comparison with other Ô¨Çaky test
detection tools like DeFlaker [6], iDFlakies [7] and NonDex
[16]. These other approaches rely on repeatedly re-running
a test (perhaps modifying the environment that the test runs
in) in order to detect if that test is Ô¨Çaky. Section II describes
our empirical study to identify the prevalence of infrequently-
failing tests, and found that hundreds of Ô¨Çaky tests could only
be detected after an enormous number of reruns, or using a
prediction model like FlakeFlagger. Future work might com-
bine these two approaches, using FlakeFlagger to prioritize
which tests to apply these rerun-based Ô¨Çaky test detectors to.
To increase our conÔ¨Ådence in our implementation, we
implemented our classiÔ¨Åer using the supervised learning algo-
rithms provided by scikit-learn . This well-regarded library pro-
vides the implementations of different techniques to balance
data, impute missing values and compute feature correlations.
Nonetheless, there may be bugs in the tooling or how we used
it; we make best efforts to check our results and will provide
a replication package. We implemented our own test smell
detectors rather than using existing detectors. This was largely
due to the lack of a published, experimentally evaluated test
smell detector for Java at time of writing. Future work might
consider extending our test smells detector, perhaps integrating
1581the recently released tsDetect smell detector [45].
Even with a replication package, some of our our results
may be difÔ¨Åcult to reproduce, since we are purposefully
experimenting on non-deterministic systems. In particular,
when rerunning tests (in order to Ô¨Ånd Ô¨Çaky tests), there is no
guarantee that a Ô¨Çaky test will fail. To aid future researchers,
we collected the build logs of each test execution and include
these in our artifact [21].
VI. R ELATED WORK
Several recent works have focused on the problem of detect-
ing and managing Ô¨Çaky tests. Luo et al. provided an empirical
study of Ô¨Çaky tests by investigating commits logs of open-
source projects [1]. They presented the most common causes
of Ô¨Çaky tests and described possible strategies to Ô¨Åx Ô¨Çaky
tests. Unlike our dataset of Ô¨Çaky tests (which is constructed by
rerunning tests), Luo et al.‚Äôs dataset is constructed by analyzing
developer reports. DeFlaker is an approach to detect Ô¨Çaky
tests immediately after the test fails (without rerunning it) by
monitoring the coverage of the latest code changes [6]. In
contrast, FlakeFlagger‚Äôs detects Ô¨Çaky tests before they fail.
More similar to our approach, iDFlakies aims to proactively
detect Ô¨Çaky tests (before they cause unexpected failures)
by rerunning tests in different orders [7]. This approach is
particularly suited to detect order-dependent Ô¨Çaky tests (those
which can fail deterministically modulo their execution order)
[23], and reruns tests 100 times. In comparison, FlakeFlagger
aims to Ô¨Ånd Ô¨Çaky tests regardless of their underlying source
of non-determinism, and in our empirical study to detect Ô¨Çaky
tests, we reran tests two orders of magnitude more (10,000
times total) than the iDFlakies work (100 times). Lam et al.
later re-ran tests from this dataset 4,000 times in order to
evaluate how difÔ¨Åcult these failures were to reproduce, Ô¨Ånding
a mean failure rate of 2.7% [22]. Order-dependent Ô¨Çaky tests
have received quite a bit of attention in general, and several
techniques have been proposed to detect them [23], [46]‚Äì
[48], to isolate tests to avoid order dependencies [3], and to
repair tests to remove those dependencies [49]. Similarly, other
specialized kinds of Ô¨Çaky tests, like those that are Ô¨Çaky due
to commonly incorrect assumptions of API behavior can be
detected through specialized means [16]. Lam et al. studied
315 real Ô¨Çaky test failures at Microsoft, and found that for the
majority of them, the failure that occurred on the build server
could not be reproduced on a developer machine, even after
trying to rerun the test 100 times [17]. This Ô¨Ånding is similar
to ours: we only reproduced half of the Ô¨Çaky test failures that
prior work found on the same projects, even though we reran
the tests 10,000 times. Strandberg et al. studied intermittently
failing tests for embedded systems, Ô¨Ånding that in this domain,
Ô¨Çakiness was typically caused by environmental factors [50].
Eck et al. analyzed Ô¨Çaky tests by conducting a survey
of developers to understand the root cause of Ô¨Çakiness [4].
This survey helped inspire our feature selection. Harman
and O'Hearn [14] emphasize the importance of Ô¨Çakiness on
software testing, suggesting that we assume that alltest are
Ô¨Çaky, and calling for tools and techniques to automaticallyassess the likelihood of a new test becoming Ô¨Çaky in the future.
FlakeFlagger answers this call directly, by helping developers
identify which tests are likely to be Ô¨Çaky based on the behavior
of other tests. Several recent works have begun study the
impact of Ô¨Çakiness on other software testing practices, like
mutation testing [51] and program repair [52].
Most similar to our approach are prior works that also aim
to predict Ô¨Çakiness [5], [9], [15], with the most relevant being
Bertolino et al.‚Äôs FLAST [13], [19] and Pinto et al.‚Äôs Ô¨Çaky
test vocabulary work [12]. Unlike FlakeFlagger, both FLAST
and Pinto et al.‚Äôs approach considers only the text in the test
method body itself, and does not consider any features of
the code that is called by that test, or code that might exist
in setup or teardown methods. In our evaluation, we found
that FlakeFlagger signiÔ¨Åcantly outperformed vocabulary-based
approaches in predicting Ô¨Çaky tests.
More generally, machine learning techniques have been
adopted widely in software testing [53]‚Äì[59]. Durelli et al.
[58] showed that ML algorithms are commonly adopted to
automate software testing tasks. Azeem et al. [54] proposed a
systematic literature review on the ML techniques used in code
smell detection. Nucci et al. [57] discussed possible limitations
which may result in unexpected result in code smell detection
by mainly analyzing Fontana et al.‚Äôs [55] work detecting code
smells using ML. We did not use ML to detect test smells,
but used traditional source code parsing to detect smells [29]‚Äì
[32]. It would be interesting to compare the performance
of our hybrid static/dynamic test smell detectors with prior
techniques, using a human oracle to determine valid smells.
VII. C ONCLUSION
Flaky tests make testing unreliable because it is hard to say
which test failures represent true regressions. We presented a
very large empirical study showing just how difÔ¨Åcult it is to
proactively detect Ô¨Çaky tests by rerunning them. FlakeFlagger
is a new approach for automatically classifying tests as Ô¨Çaky
or not without reruns , and improves on existing classiÔ¨Åers
by collecting behavioral features from each test. Developers
can use FlakeFlagger to determine if a newly added test is
likely to be Ô¨Çaky, or could use it to help classify existing
tests. Developers and researchers alike can use FlakeFlagger
to focus more precise (and costly) Ô¨Çaky test detection efforts
on the tests most likely to be Ô¨Çaky. Compared to a state-of-the-
art Ô¨Çaky test classiÔ¨Åer, FlakeFlagger had similar recall (74%
vs 72%), but signiÔ¨Åcantly better precision (60% vs 11%). This
improvement translates to a signiÔ¨Åcant reduction in the number
of misdiagnosed Ô¨Çaky tests, dramatically reducing the number
of (possibly) Ô¨Çaky tests that need to be further investigated.
VIII. D ATA AVAILABILITY
All tools and data produced for this paper are publicly and
permanently archived at [21].
ACKNOWLEDGEMENTS
Jonathan Bell‚Äôs group is supported in part by NSF CCF-
2100037 and CNS-2100015. We thank Paul Ammann, Wing
Lam and Darko Marinov for discussions about this work.
1582REFERENCES
[1] Q. Luo, F. Hariri, L. Eloussi, and D. Marinov, ‚ÄúAn empirical analysis
of Ô¨Çaky tests,‚Äù in Proceedings of the 22nd ACM SIGSOFT International
Symposium on Foundations of Software Engineering . ACM, 2014, pp.
643‚Äì653.
[2] ‚ÄúTott: Avoiding Ô¨Çakey tests,‚Äù Apr 2008. [Online]. Available:
https://testing.googleblog.com/2008/04/tott-avoiding-Ô¨Çakey-tests.html
[3] J. Bell and G. Kaiser, ‚ÄúUnit test virtualization with vmvm,‚Äù
inProceedings of the 36th International Conference on Software
Engineering , ser. ICSE 2014. New York, NY , USA: Association
for Computing Machinery, 2014, pp. 550‚Äì561. [Online]. Available:
https://doi.org/10.1145/2568225.2568248
[4] M. Eck, F. Palomba, M. Castelluccio, and A. Bacchelli, ‚ÄúUnderstanding
Ô¨Çaky tests: The developer‚Äôs perspective,‚Äù in Proceedings of the 2019 27th
ACM Joint Meeting on European Software Engineering Conference
and Symposium on the Foundations of Software Engineering ,
ser. ESEC/FSE 2019. New York, NY , USA: Association for
Computing Machinery, 2019, pp. 830‚Äì840. [Online]. Available:
https://doi.org/10.1145/3338906.3338945
[5] K. Herzig and N. Nagappan, ‚ÄúEmpirically detecting false test alarms
using association rules,‚Äù in Proceedings of the 37th International Con-
ference on Software Engineering-Volume 2 . IEEE Press, 2015, pp.
39‚Äì48.
[6] J. Bell, O. Legunsen, M. Hilton, L. Eloussi, T. Yung, and D. Marinov,
‚ÄúDeÔ¨Çaker: Automatically detecting Ô¨Çaky tests,‚Äù in Proceedings of the
40th International Conference on Software Engineering , ser. ICSE ‚Äô18.
New York, NY , USA: Association for Computing Machinery, 2018, pp.
433‚Äì444. [Online]. Available: https://doi.org/10.1145/3180155.3180164
[7] W. Lam, R. Oei, A. Shi, D. Marinov, and T. Xie, ‚ÄúidÔ¨Çakies: A framework
for detecting and partially classifying Ô¨Çaky tests,‚Äù in 2019 12th IEEE
Conference on Software Testing, Validation and VeriÔ¨Åcation (ICST) .
IEEE, 2019, pp. 312‚Äì322.
[8] A. Ahmad, O. LeiÔ¨Çer, and K. Sandahl, ‚ÄúEmpirical analysis of factors and
their effect on test Ô¨Çakiness-practitioners‚Äô perceptions,‚Äù arXiv preprint
arXiv:1906.00673 , 2019.
[9] T. M. King, D. Santiago, J. Phillips, and P. J. Clarke, ‚ÄúTowards a
bayesian network model for predicting Ô¨Çaky automated tests,‚Äù in 2018
IEEE International Conference on Software Quality, Reliability and
Security Companion (QRS-C) . IEEE, 2018, pp. 100‚Äì107.
[10] M. A. Mascheroni and E. Irrazbal, ‚ÄúIdentifying key
success factors in stopping Ô¨Çaky tests in auto-
mated rest service testing,‚Äù 2018. [Online]. Available:
http://portal.amelica.org/ameli/jatsRepo/30/308007/html/index.html
[11] H. Jiang, X. Li, Z. Yang, and J. Xuan, ‚ÄúWhat causes my test alarm?
automatic cause analysis for test alarms in system and integration
testing,‚Äù in 2017 IEEE/ACM 39th International Conference on Software
Engineering (ICSE) . IEEE, 2017, pp. 712‚Äì723.
[12] G. Pinto, B. Miranda, S. Dissanayake, M. d‚ÄôAmorim, C. Treude, and
A. Bertolino, ‚ÄúWhat is the vocabulary of Ô¨Çaky tests?‚Äù in MSR ‚Äô20: 17th
Int‚Äôl. Conf. on Mining Software Repositories , 2020.
[13] A. Bertolino, E. Cruciani, B. Miranda, and R. Verdecchia, ‚ÄúKnow
your neighbor: fast static prediction of test Ô¨Çakiness (github),‚Äù
https://github.com/ICSE2020-FLAST/FLAST, 2019.
[14] M. Harman and P. O‚ÄôHearn, ‚ÄúFrom start-ups to scale-ups: Opportunities
and open problems for static and dynamic program analysis,‚Äù in 2018
IEEE 18th International Working Conference on Source Code Analysis
and Manipulation (SCAM) . IEEE, 2018, pp. 1‚Äì23.
[15] M. Waterloo, S. Person, and S. Elbaum, ‚ÄúTest analysis: Searching
for faults in tests,‚Äù in Proceedings of the 30th IEEE/ACM
International Conference on Automated Software Engineering , ser.
ASE ‚Äô15. IEEE Press, 2015, pp. 149‚Äì154. [Online]. Available:
https://doi.org/10.1109/ASE.2015.37
[16] A. Shi, A. Gyori, O. Legunsen, and D. Marinov, ‚ÄúDetecting assumptions
on deterministic implementations of non-deterministic speciÔ¨Åcations,‚Äù in
2016 IEEE International Conference on Software Testing, VeriÔ¨Åcation
and Validation (ICST) . IEEE, 2016, pp. 80‚Äì90.
[17] W. Lam, P. Godefroid, S. Nath, A. Santhiar, and S. Thummalapenta,
‚ÄúRoot causing Ô¨Çaky tests in a large-scale industrial setting,‚Äù in
Proceedings of the 28th ACM SIGSOFT International Symposium on
Software Testing and Analysis , ser. ISSTA 2019. New York, NY , USA:
Association for Computing Machinery, 2019, pp. 101‚Äì111. [Online].
Available: https://doi.org/10.1145/3293882.3330570[18] K. Herzig, ‚ÄúLet‚Äôs assume we had to pay for testing,‚Äù
https://www.slideshare.net/slideshow/embed code/63509323, 2016.
[19] A. Bertolino, E. Cruciani, B. Miranda, and R. Verdecchia,
‚ÄúKnow your neighbor: fast static prediction of test Ô¨Çakiness,‚Äù
https://dx.doi.org/10.32079/ISTI-TR-2020/001, ISTI Technical Reports,
Tech. Rep., 2020.
[20] A. Alshammari, C. Morris, M. Hilton, and J. Bell, ‚ÄúFlakeÔ¨Çagger,‚Äù
https://github.com/AlshammariA/FlakeFlagger, 2021.
[21] ‚Äî‚Äî, ‚ÄúFlaky Test Dataset to Accompany ‚ÄôFlakeFlagger: Predicting
Flakiness Without Rerunning Tests‚Äô,‚Äù Jan. 2021. [Online]. Available:
https://doi.org/10.5281/zenodo.4450723
[22] W. Lam, S. Winter, A. Astorga, V . Stodden, and D. Marinov, ‚ÄúUnder-
standing reproducibility and characteristics of Ô¨Çaky tests through test
reruns in java projects,‚Äù in 2020 IEEE 31st International Symposium on
Software Reliability Engineering (ISSRE) , 2020, pp. 403‚Äì413.
[23] S. Zhang, D. Jalali, J. Wuttke, K. Mus ¬∏lu, W. Lam, M. D. Ernst, and
D. Notkin, ‚ÄúEmpirically revisiting the test independence assumption,‚Äù in
Proceedings of the 2014 International Symposium on Software Testing
and Analysis , ser. ISSTA 2014. New York, NY , USA: Association
for Computing Machinery, 2014, pp. 385‚Äì396. [Online]. Available:
https://doi.org/10.1145/2610384.2610404
[24] A. Van Deursen, L. Moonen, A. Van Den Bergh, and G. Kok, ‚ÄúRefac-
toring test code,‚Äù in Proceedings of the 2nd international conference
on extreme programming and Ô¨Çexible processes in software engineering
(XP2001) , 2001, pp. 92‚Äì95.
[25] G. Meszaros, xUnit test patterns: Refactoring test code . Pearson
Education, 2007.
[26] V . Garousi and B. K ¬®uc ¬∏¬®uk, ‚ÄúSmells in software test code: A survey of
knowledge in industry and academia,‚Äù Journal of systems and software ,
vol. 138, pp. 52‚Äì81, 2018.
[27] W. Lam, S. Winter, A. Wei, T. Xie, D. Marinov, and J. Bell, ‚ÄúA
large-scale longitudinal study of Ô¨Çaky tests,‚Äù Proc. ACM Program.
Lang. , vol. 4, no. OOPSLA, Nov. 2020. [Online]. Available:
https://doi.org/10.1145/3428270
[28] R. Ramler, M. Moser, and J. Pichler, ‚ÄúAutomated static analysis of unit
test code,‚Äù in 2016 IEEE 23rd International Conference on Software
Analysis, Evolution, and Reengineering (SANER) , vol. 2, March 2016,
pp. 25‚Äì28.
[29] M. Breugelmans and B. V . Rompaey, ‚ÄúTestq: Exploring structural and
maintenance characteristics of unit test suites,‚Äù in IN WASDETT-1 , 2008.
[30] M. Greiler, A. van Deursen, and M. Storey, ‚ÄúAutomated
detection of test Ô¨Åxture strategies and smells,‚Äù in 2013
IEEE Sixth International Conference on Software Testing,
VeriÔ¨Åcation and Validation . Los Alamitos, CA, USA: IEEE
Computer Society, mar 2013, pp. 322‚Äì331. [Online]. Available:
https://doi.ieeecomputersociety.org/10.1109/ICST.2013.45
[31] F. Palomba, A. Zaidman, and A. D. Lucia, ‚ÄúAutomatic test smell detec-
tion using information retrieval techniques,‚Äù in 2018 IEEE International
Conference on Software Maintenance and Evolution (ICSME) , 2018.
[32] G. Bavota, A. Qusef, R. Oliveto, A. Lucia, and D. Binkley, ‚ÄúAre
test smells really harmful? an empirical study,‚Äù Empirical Softw.
Engg. , vol. 20, no. 4, pp. 1052‚Äì1094, Aug. 2015. [Online]. Available:
https://doi.org/10.1007/s10664-014-9313-0
[33] A. Singh, N. Thakur, and A. Sharma, ‚ÄúA review of supervised machine
learning algorithms,‚Äù in 2016 3rd International Conference on Comput-
ing for Sustainable Global Development (INDIACom) , March 2016, pp.
1310‚Äì1315.
[34] J. Dougherty, R. Kohavi, and M. Sahami, ‚ÄúSupervised and unsupervised
discretization of continuous features,‚Äù in Machine Learning Proceedings
1995 . Elsevier, 1995, pp. 194‚Äì202.
[35] S. Kotsiantis and D. Kanellopoulos, ‚ÄúDiscretization techniques: A recent
survey,‚Äù GESTS International Transactions on Computer Science and
Engineering , vol. 32, no. 1, pp. 47‚Äì58, 2006.
[36] SciKit-Learn Developers, ‚Äúscikit,‚Äù 2020. [Online]. Available:
https://scikit-learn.org/stable/
[37] S. Lei, ‚ÄúA feature selection method based on information gain and
genetic algorithm,‚Äù in 2012 International Conference on Computer
Science and Electronics Engineering , vol. 2. IEEE, 2012, pp. 355‚Äì
358.
[38] R. Kohavi et al. , ‚ÄúA study of cross-validation and bootstrap for accuracy
estimation and model selection,‚Äù in Ijcai, vol. 14, no. 2. Montreal,
Canada, 1995, pp. 1137‚Äì1145.
1583[39] Y . Bengio and Y . Grandvalet, ‚ÄúNo unbiased estimator of the variance of
k-fold cross-validation,‚Äù Journal of machine learning research , vol. 5,
no. Sep, pp. 1089‚Äì1105, 2004.
[40] N. V . Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer, ‚ÄúSmote:
synthetic minority over-sampling technique,‚Äù Journal of artiÔ¨Åcial intel-
ligence research , vol. 16, pp. 321‚Äì357, 2002.
[41] N. V . Chawla, ‚ÄúData mining for imbalanced datasets: An overview,‚Äù in
Data mining and knowledge discovery handbook . Springer, 2009, pp.
875‚Äì886.
[42] ‚ÄúWhere do our Ô¨Çaky tests come from?‚Äù Apr 2017. [Online].
Available: https://testing.googleblog.com/2017/04/where-do-our-Ô¨Çaky-
tests-come-from.html
[43] W. Lam, ‚ÄúIllinois Dataset of Flaky Tests (IDoFT),‚Äù 2020. [Online].
Available: http://mir.cs.illinois.edu/Ô¨Çakytests
[44] T. Winters, T. Manshreck, and H. Wright, Software
Engineering at Google: Lessons Learned from Programming
Over Time . O‚ÄôReilly Media, 2020. [Online]. Available:
https://books.google.com/books?id=TyIrywEACAAJ
[45] A. Peruma, K. Almalki, C. D. Newman, M. W. Mkaouer, A. Ouni, and
F. Palomba, ‚Äútsdetect: An open source test smells detection tool,‚Äù in
Proceedings of the 2020 28th ACM Joint Meeting on European Software
Engineering Conference and Symposium on the Foundations of Software
Engineering , ser. ESEC/FSE 2020. New York, NY , USA: Association
for Computing Machinery, Nov. 2020.
[46] A. Gyori, A. Shi, F. Hariri, and D. Marinov, ‚ÄúReliable testing: Detecting
state-polluting tests to prevent test dependency,‚Äù in Proceedings of the
2015 International Symposium on Software Testing and Analysis , ser.
ISSTA 2015, 2015, pp. 223‚Äì233.
[47] J. Bell, G. Kaiser, E. Melski, and M. Dattatreya, ‚ÄúEfÔ¨Åcient dependency
detection for safe java test acceleration,‚Äù in Proceedings of the
2015 10th Joint Meeting on Foundations of Software Engineering ,
ser. ESEC/FSE 2015. New York, NY , USA: Association for
Computing Machinery, 2015, pp. 770‚Äì781. [Online]. Available:
https://doi.org/10.1145/2786805.2786823
[48] C. Huo and J. Clause, ‚ÄúImproving oracle quality by detecting brittle
assertions and unused inputs in tests,‚Äù in International Symposium on
Foundations of Software Engineering , 2014, pp. 621‚Äì631.
[49] A. Shi, W. Lam, R. Oei, T. Xie, and D. Marinov, ‚ÄúiÔ¨ÅxÔ¨Çakies: A
framework for automatically Ô¨Åxing order-dependent Ô¨Çaky tests,‚Äù in
[57] D. Di Nucci, F. Palomba, D. A. Tamburri, A. Serebrenik, and A. De Lu-
cia, ‚ÄúDetecting code smells using machine learning techniques: are we
there yet?‚Äù in 2018 IEEE 25th International Conference on SoftwareProceedings of the 2019 27th ACM Joint Meeting on European Software
Engineering Conference and Symposium on the Foundations of Software
Engineering . ACM, 2019, pp. 545‚Äì555.
[50] P. E. Strandberg, T. J. Ostrand, E. J. Weyuker, W. Afzal, and
D. Sundmark, ‚ÄúIntermittently failing tests in the embedded systems
domain,‚Äù in Proceedings of the 29th ACM SIGSOFT International
Symposium on Software Testing and Analysis , ser. ISSTA 2020. New
York, NY , USA: Association for Computing Machinery, 2020, p.
337348. [Online]. Available: https://doi.org/10.1145/3395363.3397359
[51] A. Shi, J. Bell, and D. Marinov, ‚ÄúMitigating the effects of
Ô¨Çaky tests on mutation testing,‚Äù in Proceedings of the 28th
ACM SIGSOFT International Symposium on Software Testing and
Analysis , ser. ISSTA 2019. New York, NY , USA: Association
for Computing Machinery, 2019, pp. 112‚Äì122. [Online]. Available:
https://doi.org/10.1145/3293882.3330568
[52] M. Cordy, R. Rwemalika, M. Papadakis, and M. Harman, ‚ÄúFlakime:
Laboratory-controlled test Ô¨Çakiness impact assessment. a case study on
mutation testing and program repair,‚Äù arXiv preprint arXiv:1912.03197 ,
2019.
[53] T. Guggulothu and S. A. Moiz, ‚ÄúCode smell detection using multi-label
classiÔ¨Åcation approach,‚Äù Software Quality Journal , 2020. [Online].
Available: https://doi.org/10.1007/s11219-020-09498-y
[54] M. I. Azeem, F. Palomba, L. Shi, and Q. Wang, ‚ÄúMachine learning
techniques for code smell detection: A systematic literature review and
meta-analysis,‚Äù Information and Software Technology , 2019.
[55] F. A. Fontana, M. Zanoni, A. Marino, and M. V . M ¬®antyl ¬®a, ‚ÄúCode smell
detection: Towards a machine learning-based approach,‚Äù in 2013 IEEE
International Conference on Software Maintenance . IEEE, 2013, pp.
396‚Äì399.
[56] R. Chang, S. Sankaranarayanan, G. Jiang, and F. Ivancic, ‚ÄúSoftware
testing using machine learning,‚Äù Dec. 30 2014, uS Patent 8,924,938.
Analysis, Evolution and Reengineering (SANER) . IEEE, 2018, pp. 612‚Äì
621.
[58] V . H. Durelli, R. S. Durelli, S. S. Borges, A. T. Endo, M. M. Eler,
D. R. Dias, and M. P. Guimar Àúaes, ‚ÄúMachine learning applied to software
testing: A systematic mapping study,‚Äù IEEE Transactions on Reliability ,
2019.
[59] G. Catolino, F. Palomba, F. A. Fontana, A. De Lucia, A. Zaidman,
and F. Ferrucci, ‚ÄúImproving change prediction models with code
smell-related information,‚Äù vol. 25, no. 1, 2020, pp. 49‚Äì95. [Online].
Available: https://doi.org/10.1007/s10664-019-09739-0
1584