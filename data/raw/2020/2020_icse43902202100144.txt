Restoring Execution Environments
of Jupyter Notebooks
Jiawei Wang
Faculty of Information Technology
Monash University
Melbourne, Australia
jiawei.wang1@monash.eduLi Liα
Faculty of Information Technology
Monash University
Melbourne, Australia
li.li@monash.eduAndreas Zeller
CISPA Helmholtz Center for Information Security
Saarbr ¨ucken, Germany
zeller@cispa.saarland
Abstract —More than ninety percent of published Jupyter
notebooks do not state dependencies on external packages. This
makes them non-executable and thus hinders reproducibilityof scientiﬁc results. We present SnifferDog, an approach that
1) collects the APIs of Python packages and versions, creat-ing a database of APIs; 2) analyzes notebooks to determinecandidates for required packages and versions; and 3) checks
which packages are required to make the notebook executable(and ideally, reproduce its stored results). In its evaluation, we
show that SnifferDog precisely restores execution environments
for the largest majority of notebooks, making them immediatelyexecutable for end users.
Index T erms—Jupyter Notebook, Environment, Python, API
I. I NTRODUCTION
Jupyter notebooks—interactive documents that combine
code, text, mathematics, plots, and rich media—have become
a prime medium for scientists to document, replicate, andillustrate their ﬁndings. In contrast to a regular scientiﬁc paper,a notebook allows its writers to directly interact with data
and code, updating tables and diagrams on the spot. This alsoextends to users, who can re-execute the notebook code, say
with their own data or changes to the algorithms, and see howthis affects the ﬁnal results. This makes Jupyter notebooks one
of the most promising tools to allow for widespread replication
and reuse of research results.
What sounds good in theory need not be true in practice,
though, and Jupyter notebooks are no exception. Recent stud-ies [1] have shown that the vast majority of published Jupyternotebooks can only be read by users, but not re-executed.One reason is incompleteness, such as the raw data not being
supplied; and there is not much users can do about this. How-
ever, there are also reasons for notebooks being non-executable
that can be easily avoided. One reason is that notebook codecells can be executed interactively in any order (and data
scientists happily do so); recent approaches [2] thus focuson restoring the actual order based on internal dependencies.Another important reason, however, is that Jupyter notebooks
depend on speciﬁc environments in which they were created,
such as speciﬁc libraries in speciﬁc versions.
In principle, Python code in notebooks provides import
statements, which state the (external) modules are to be used.
αCorresponding authorHowever, Python users install packages, not modules; and
the names of imported modules may be different from thename of the package that provides them. Different versions
of packages may provide different APIs; hence one has to
determine compatible versions. Also, packages may depend
on other tools or packages to be installed.
This is why Python (like other languages), in good Software
Engineering tradition, has long introduced explicit means to
specify dependencies between libraries and packages. Python
package managers (e.g., pip and conda), for instance, expect
Python packages to provide an explicit list of dependencies,
stating which other packages need to be installed in which
versions. Writers of Jupyter notebooks, however, are ﬁrst andforemost data scientists and not software engineers [3]; hence,
they neither know about principles of reusable software, norwould this be in their focus. Indeed, as we show in this
paper, around 94% of notebooks do not formally state or
document dependencies; among those who do, nearly 30%
are not reliable. In consequence, users who want to executepublished and complete Jupyter notebooks will very likely faceerrors of missing packages or incompatible versions.
In this paper, we introduce a novel approach to automati-
cally restore the experimental dependencies of Jupyter note-
books. Our SnifferDog tool takes a Python Jupyter notebook
and automatically detects which packages are required to
reproduce notebook results. To this end, SnifferDog creates an
API bank, a database which holds API information for each
Python library (and each version). By analyzing the Pythoncode embedded in the notebook, SnifferDog then determines
library candidates that would be API compatible. SnifferDog
then can automatically install the recommended dependencies
and check if they allow the notebook to 1) be executed and
2) reproduce the original results stored in the notebook. When
users thus apply SnifferDog on a notebook, they at least obtain
a list of detected required libraries and their versions. If theseare complete, the notebook can become executable; and in
the ideal case, the notebook is shown to fully reproduce theoriginal results. Striving for executability, reproduction, and
considering library versions is also what sets SnifferDog apart
from earlier, Python-speciﬁc approaches [4].
SnifferDog is efﬁcient and effective: It ﬁnishes the analysis
of 5,000 notebooks in 18,141.29 seconds (3.63 seconds per
16222021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)
1558-1225/21/$31.00 ©2021 IEEE
DOI 10.1109/ICSE43902.2021.00144
notebook). In an experiment with 315 notebooks known to be
executable, SnifferDog was able to automatically determine
dependencies for over 90% of them.
The remainder of this paper is organized as follows. After
providing background about Python packages and Jupyter
notebooks (Section II), we make the following contributions:
•A study on the prevalence of dependency issues inJupyter notebooks (Section III). In a preliminary study,
we investigated causes that make Jupyter notebooks non-executable, with and without environmental dependen-
cies.
•A novel approach to restore dependencies of Jupyter
notebooks (Section IV). We present the design of our
approach and its implementation in the SnifferDog pro-
totype.
•An evaluation of our approach (Section V). We evaluate
the effectiveness of SnifferDog on a variety of notebooks,
showing that it precisely restores execution environments
for the largest majority of notebooks.
After discussing related work (Section VI), we close with
conclusion and future work (Section VII).
II. B ACKGROUND
We start with discussing background knowledge, including
Python libraries and Jupyter notebooks.
A. Python Libraries
Python is well-known for its immense ecosystem, providing
more than 200,000 third-party packages (also known as li-
braries) to developers. Such Python libraries need to be locallyinstalled onto developers’ implementation environment before
being accessed. The Python Packaging Authority team ofﬁ-
cially maintains a standard package management tool calledpip, which allows users to install these libraries from differ-
ent sources (PyPI) [5]. In addition to package management
systems, Python developers can also install a library from itssource code project.
Figure 1illustrates a typical code structure example of
a Python library. A Python ﬁle named setup.py installs the
library locally. However, this ﬁle will not install the library’senvironmental dependencies and, hence, requires its users to
fulﬁll them beforehand. A top-level package giving the library
its name (i.e., pandas) is stored in the same level of setup.py.
Such a package in Python is a directory that contains a speciﬁc
ﬁle named
init .pyresponsible for initializing the package.
Python packages provide a way to structure Python’s module
namespace, offering an easy means for library users to accessits APIs.A module in Python is a speciﬁc term used in Python
to specify Python’s source code (i.e., ﬁles containing Python
deﬁnitions and statements). Each Python ﬁle represents a
Python module; for instance, setup.py deﬁnes a Python module
named setup. Directories under the top-level package are thelibrary’s sub-packages. Similarly, the Python ﬁles under sub-
packages are deemed as sub-modules. For example, as shownin Figure 1, the Python ﬁle base.py is deﬁned as a sub-module
named base in sub-package pandas.io.excel.pandas
__init__.py io
excel
_base.py __init__.pyTop-level package 
Initialize the pandas package
Initialize the pandas.io.excel sub-package
Deﬁne a module named pandas.io.excel._base (or a sub-module in sub-package pandas.io.excel)pandas
setup.pyTop-directory (project name)
Responsible for 
installing the 
pandas library
api.py
Fig. 1: A typical code structure example of Python libraries.
This partial code structure is extracted from a popular Pythonlibrary called Pandas.
In each Python module (or Python ﬁle), a set of methods can
be declared and implemented. These methods can be accessed
by other modules (or module users) and hence are referred
to as APIs. For example, the pandas.io.excel.base module
contains an API called read
excel() with the fully qualiﬁed
name being pandas.io.excel. base.read excel()). When Python
libraries evolve, their declared API sets will likely be updated.
In this work, we will leverage this information to implement
SnifferDog so as to infer environmental dependencies for
Python Jupyter notebooks.
B. Jupyter notebooks
Jupyter notebooks are sequences of cells, which either
contain text(in Markdown format) or executable code (and its
results). In text cells, authors describe (using Markdown and
HTML for rich formatting) the objective of the notebook and
the rationale behind the code presented in the following cells.
Incode cells, authors write actual programming code, most
frequently Python code. Figure 2presents a typical example of
a Jupyter notebook, containing three text cells and six Python
code cells.
Each code cell can be directly executed by the underline
Jupyter engine, which provides the necessary computationalenvironment such as library dependencies. The code cells in
Jupyter notebooks can be executed in any order (producing
errors if its prerequisites are not satisﬁed). After one cell isexecuted, Jupyter will assign an execution order aligning with
its execution order. For example, the ﬁrst executed cell willbe marked as “In [1]”, while the fourth executed cell will be
marked as “In [4]”. Cells can be repeatedly executed. In sucha case, the latest execution counter will overwrite the previousone..
Looking closely at Figure 2, we see that the last code
cell is executed before the fourth and ﬁfth code cells. Astutereaders may have also observed that there is no information(i.e., “In [5]”) indicating the ﬁfth executed code cell. This isbecause when code cells are repeatedly executed, the original
execution counter will be overwritten by the last executioncounter—called a skip of execution counter. Skips make it
1623hard to reproduce the original outputs of a notebook because
the skipped execution counters are not recorded at all [2].
If the execution of a code cell generates an output (text or
pictures such as diagrams), the output will also be recorded
and displayed in the notebook. In Figure 2, a histogram at the
end is the output of the last code cell.
import pandas.io.excel._base
_base.read_excel(…)In [1]
pandas.DataFrame.hist(…)In [7]Execution
CounterCode
CellThe following code cells illustrate different ways to import Python modules and execute 
Python their APIsMarkdown
Cell
Outputfrom pandas.io.excel._base import read_excelread_excel(…)import pandaspandas.read_excel(…)
In [3]In [2]
from pandas import read_excel as re
re(…) # read_excel(…)
from pandas import *
read_excel(…)In [6]
In [4]The following code cell demonstrates how to 
draw a histogram using pandas’s DataFrame.This is an example of Jupyter notebook.
Fig. 2: An example of Jupyter notebook.
III. P RELIMINARY STUDY AND MOTIV ATION
In 2019, Pimentel et al. presented a large-scale empiri-
cal study [1] on the quality and reproducibility of Jupyter
notebooks. In this study, the authors looked into 1,159,166notebooks collected from GitHub, among which only 149,259(roughly 12.9%) of them were provided with module depen-
dency information describing how the notebooks’ environ-mental dependencies should be set up. In other words, thevast majority of existing notebooks in the community do notprovide sufﬁcient information such that notebook users could
execute and replicate them. Since easy replication is one of thepromises of Jupyter Notebooks, there is a need for dependable
automated approaches to infer environmental dependencies forJupyter notebooks.
How serious is this problem? We have conducted a
lightweight replication study of Pimentel et al.’s work onrecent Jupyter notebooks. We limit our replication study toreplicating the executibility of notebooks when supplying
dependencies provided by the notebook authors to identifythe main causes for non-reproducibility and thus speciﬁcally
address execution environments. To fulﬁll this purpose, wepropose to answer the following research questions
•RQ1: To what extent do (public) Jupyter notebooks
provide environment setup information?•RQ2: How useful is dependency information in helping
notebook users conﬁgure the execution environment?
•RQ3: Does the provided environment information help
notebook users to execute and reproduce the notebooks?If not, what are the root causes making them non-
executable?
To answer these research questions, we have collected a
dataset consisting of notebooks with and without experimental
setup information. Our source for notebooks is GitHub, one
of the world’s leading software repository hosting platforms.We randomly downloaded 100,000 notebooks from GitHub.
Table Isummarizes our study results. Among 100,000
notebooks, less than 6% of them (or 4.74%, 1.15%, and 0.12%
respectively for the three selected criteria) have been provided
with environmental dependency information
1. This rate is even
lower than the rate (calculated similarly) reported by Pimentel
et al. two years ago.
Note that prior work by Pimentel et al. has investigated
three sources for notebook environment setup information:
(1) requirements.txt, (2) Pipﬁle, and (3) setup.py. As dis-cussed previously, setup.py is usually used to locally install
a Python library from the source. It is not responsible forinstalling library dependencies. After manually investigatingnotebooks that use setup.py, we conﬁrm that setup.py is indeed
not relevant to the environment setup of Jupyter notebooks.
Therefore, we exclude the third criteria setup.py for this study.
Furthermore, when conducting the previous manual analysis,we additionally ﬁnd that notebook contributors may provide
environmental setup information through Anaconda (e.g., via
environments.yml ). As a result, in our study, we replace the
third criteria setup.py with Anaconda environments.
TABLE I: Distribution of notebooks being provided with
environmental dependency information w.r.t. the selected threecriteria.
requirements.txtenvironments.yml
(Anaconda)Pipﬁle Total
Notebooks 4741 1146 117 5826
Notebooks (≥3.5) 2923 868 77 3740
Installable 2064 563 77 2646
Executable 518 207 14 725
RQ1: To what extent do (public) Jupyter notebooks provide
environment setup information?
Among 100,000 notebooks, only less than 6% provide
environmental dependency information for helping users
execute their notebooks.
For the 5,826 notebooks that have been provided with
environmental dependencies, we further check how reliable
these are. To this end, we implemented scripts to automaticallyinstall such dependencies, using Anaconda [6] to create indi-
vidual environments for each of the aforementioned Jupyter
1Some notebooks may provide two types of information for helping users
setup the execution environments. For example, there are 101 notebooks
contain both requirements.txt and Anaconda information.
16241## requirements.txt ##
2pandas # Without specifying versions
3scipy == 1.17.5 # Must be version 1.17.5
4sklearn >=0.23.0. # Minimum version 0.23.0
5
6## Conda environment.yml ##
7name: env_name
8dependencies:
9- numpy=1.11.1=py35_0
10- openssl=1.0.2h=vc14_0
11- pandas=0.18.1=np111py35_0
12
13## Pipfile ##
14[[source]]
15url = "https://pypi.python.org/simple"
16verify_ssl = true
17name = "pypi"
1819[packages]
20pandas = "*"
21sqlalchemy = ">= 1.3.0"
Fig. 3: Example of the requirement ﬁles, Anaconda environ-
ment.yml and Pipﬁles.
notebooks. After that, we leveraged a tool named nbconvert2
to evaluate the execution of notebooks. Since nbconvert in
Python 3.5 or its lower versions are no longer supported by
the ofﬁcial Jupyter team, we had to exclude 2,086 notebooks
that cannot be analyzed by our scripts.
After automatically installing the remaining 3,740 note-
books, we resort to their logs to check whether the installations
are successful or not. Based on our observation, when an
installation fails, it will contain one of the following threemessages: (a) “InstallationError” occur as the runtime ex-
ception, (b) “ERROR:”, and (c) “cannot ﬁnd a version for”.
Of the 3,740 notebooks, 1094 notebooks failed to have theirdependency information installed, giving a failure rate of29.25%.
RQ2: How useful is dependency information in helping
notebook users conﬁgure the execution environment?
In our evaluation, 29.25% of the environmental dependency
information provided by notebook contributors was found
to be unreliable and/or insufﬁcient.
For the notebooks that we can successfully install their envi-
ronmental dependencies, we go one step deeper to check if the
installed dependencies are adequate to support the execution of
the notebooks. Recall that notebook code cells can be executed
in any order and can be repeatedly executed (resulting inskipped execution counters). Hence, it is practically impossible
to infer the actual execution order initially conducted by the
notebook contributor. In this preliminary study, we simply
execute the code cells top-down. We believe this order reﬂects
the natural ﬂow indicating how its contributor has attempted to
implement the notebook. The execution time is set 10 minutes
for every notebook.
2https://github.com/jupyter/nbconvert0RGXOH1RW)RXQG(UURU
,PSRUW(UURU
)LOH1RW)RXQG(UURU
1DPH(UURU
7LPHGRXW
$WWULEXWH(UURU
6\QWD[(UURU
9DOXH(UURU
7\SH(UURU
.HUQHO'LHG
    
Fig. 4: Top 10 runtime exceptions from executions of note-
books whose dependency ﬁles can be successfully installed.
The last row of Table Ipresents the number of notebooks
(i.e., Installable ones) that can be executed without errors. In
other words, among 2,646 notebooks that have been provided
with installable dependency information, 72.6% of them can-not be successfully executed following the straightforward top-down execution strategy.
There are various reasons causing execution of these note-
books to fail. Indeed, on the one hand, the provided depen-dency information may not be perfectly reliable, resulting in
dependency-related errors. On the other hand, even if the
environmental dependencies are correctly set up, notebooksthemselves may contain implementation errors that can also
lead to runtime exceptions.
Figure 4enumerates the top-10 errors summarized from
the unsuccessful notebooks. The fact that the top-ranked
errors are related to environmental dependencies shows that
environmental dependency seems to be the primary reason
causing execution errors of the aforementioned notebooks. Thetop-2 ranked errors (i.e., module not found and import error)
are indeed caused by the inadequate runtime environment,
where the imported modules cannot be located [7]. In fact,notebooks linked to these two errors have accounted nearly
half (24.15% and 22.90%, respectively) of the aforementioned
unsuccessful notebooks.
1# Example (1): ModuleNotFoundError
2# from GitHub project BenjaminBossan@mink
3# requirements.txt
4scikit-learn
5...
6
7# Module ’sklearn.grid_search’ was removed since
scikit-learn version XXX
8Error: No module named ’sklearn.grid_search’
9
10
11# Example (2): ImportError
12# From GitHub project stargaser@astrodata2016
13# requirements.txt
14astroquery
15...
16
17# scale_image API was removed from module
’astropy.visualization’ since astroqueryversion XXX
18Error: cannot import name ’scale_image’ from
’astropy.visualization
Listing 1: Two real-world examples suffering from runtime
errors due to unmatched library versions.
1625Among various reasons causing notebooks failing to be
successfully executed, we further look into some of the failures
related to the top-2 types of errors and ﬁnd that a signiﬁcantamount of failures are due to different versions of libraries areinstalled. Indeed, when providing environmental dependencies(cf.Figure 3), notebook contributors are not required to specify
theexact versions of the dependent libraries. As a result, the
unmatched library versions might be installed and hence leadto runtime errors. Listing 1demonstrates two of such exam-
ples (respectively for ModuleNotFoundError and ImportError )
obtained from real-world notebooks. Due to the fast-evolvingnature of software systems such as Python libraries, certainAPIs might be deprecated and subsequently removed. If the
wrong library versions are used, client applications, if not
changed, will likely be subject to compatibility issues andhence result in runtime errors. Therefore, we argue thatwhen specifying the environmental dependencies for Jupyter
notebooks, it is essential to also clearly specify the required
versions of required libraries.
RQ3: Does the provided environment information help
notebook users to execute and reproduce the notebooks?
For 72.6% of notebooks, the provided dependencies are not
sufﬁcient for re-executing them without errors.
IV . SnifferDog
In this section, we present our approach to automatically
inferring environmental dependencies for Python Jupyter note-
books, implemented in our SnifferDog prototype. Figure 5
summarizes the working process of SnifferDog, including
mainly three modules: (1) Library API Mapping, (2) Library
Identiﬁcation and API Standardization, and (3) API UsageAnalysis. We now detail these three modules, respectively.
Library API
Mapping API Bank
Library¬ Identi ﬁcation &
¬API StandardiationAPI Usage¬
Analysis
Jupyter Notebook ProjectPython Libraries
Environmental
Dependencies
Fig. 5: The working process of SnifferDog.
A. Problem Statement
Before providing the details of our approach, we formally
deﬁne the problem that we plan to address in this work. Atﬁrst, we need to pre-build an API bank L={L
v
1,Lv2,Lv3, ...}
that records a large number of popular Python libraries’ API
sets, in which Lv
jstands for the set of APIs deﬁned in library
Ljat version v. Then, given a Python Jupyter notebook Nas
an input, we need to precisely parse its accessed API set P=
{A1,A2,A3, ...}, where Aiis an API used in P. After that,
based on the pre-built API bank L, the goal of this approach is
hence to identify a set of libraries Lthat fulﬁll the followingconstraints: (1) L⊂L , and (2) ∀Ai∈P,∃Lv
j∈Lthat
Ai∈Lvj.
B. Library API Mapping
The ﬁrst module, library API mapping, is not directly
related to the working process of analyzing concrete Jupyter
notebooks but plays an independent step in building the core
infrastructure of our approach. The output of this module will
be an API bank that provides an extensible (and on-growing)
database recording mappings from popular Python libraries to
their APIs.
Given a Python library planned to be included in the API
bank, this module ﬁrst builds a directory tree following the ﬁle
and directory composition of the library, aiming at providing a
clear way to referring library APIs (e.g. from top-level package
to the leaf module). In this directory tree, Python packages (or
sub-packages) are represented by non-leaf nodes and Python
source code ﬁles are represented by leaf nodes. Figure 1
presents such an example, representing a partial code structure
tree of the popular Python library pandas. This module then
builds Abstract Syntax Tree (AST) trees for each leaf node (orPython ﬁle) and traverses the trees to locate public functions,
including their positional and keyword parameters. The outputof this step can already build a mapping from the library (in
a certain version) to its deﬁned APIs.
Unfortunately, this approach may overlook certain
API usages. Applying it to pandas (Figure 1), the
API read
excel() (deﬁned in the base.py module)
can be referenced via its full qualiﬁed name
pandas.io.excel. base.read excel(),o r base.read excel()
(or read excel(()) if module pandas.io.excel. base (or the
API itself pandas.io.excel. base.read excel ) is imported, as
respectively shown in the second and third code cells in
Figure 2. However, as demonstrated in the ﬁrst, fourth, and
ﬁfth code cells Figure 2, API read excel() could be invoked
via another forms such as pandas.read excel(), i.e., it can be
directly imported from the pandas module despite it being
deﬁned in the pandas.io.excel. base module.
This ambiguity is part of the complicated Python import
mechanism, which has been implemented in a transitive man-
ner. Let Xf− →Ybe importing API ffrom module Xto
module Yvia statement ”from X import f” in the source of
Y. Transitivity enables Xf− →Y,i fYf− →ZandXf− →Z.
This feature, offered by Python runtime, has been frequently
leveraged by many Python libraries to provide simpliﬁed
means for users to access their APIs since it can transparently
shorten the full qualiﬁed API names.
To resolve this feature, while parsing Python source code,
we further conduct an import-ﬂow analysis to ﬁnd all the
possible alternatives (or aliases) of directly deﬁned APIs.Take the API read
excel again as an example, regarding the
simpliﬁed source code shown in Figure 6(a), the import-ﬂow
analysis would lead to the following two ﬂows.
pandas.io.excel. baseread excel−−−−−→pandas.io.api
pandas.io.apiread excel−−−−−→pandas
1626Fig. 6: An example of enhanced directory tree for Python
library pandas.
By taking transitivity into consideration, we could further
deduce the following ﬂow.
pandas.io.excel. baseread excel−−−−−→pandas
Subsequently, at the end of this module, we further leverage
these inferred and deduced import ﬂows to enhance the direc-tory tree initially constructed for the library (cf. Figure 6(b),
before recording them into the API bank. The enhanced
directory tree allows us to generate a complete list of APIsfor each library integrated into the API bank.
C. API Identiﬁcation and Standardization
As shown in Figure 5, the second module API identiﬁation
focuses on analyzing Jupyter notebook projects (rather than
Python libraries as that being targeted by the ﬁrst module)
to identify and standardize library API usage. Python code in
notebooks can access (1) methods available in local modules
that are often developed by the notebooks’ contributors, (2)Python standard methods (i.e., often known as system APIs)
that are provided by the core Python modules, and (3) library
methods (i.e., often known as library APIs) that are devel-
oped by third parties and should be imported from externalresources. In this module, we are only interested in the third
type of methods, namely library APIs. To distinguish library
methods from local modules and system APIs, we considerall the methods that are not deﬁned locally and are not from
Pythons’ system APIs as library APIs [8]–[13], [13]–[16].
Following the same approach implemented in the library
API mapping module, we ﬁrst build AST trees for the note-
book’s Python code and then traverse the trees to identify
library APIs. After that, this step goes one step deeper toexpand the identiﬁed APIs to their fully-qualiﬁed names,based on the information extracted from the import and from-
import statements. For example, for the API call statement
base.read excel() in the second code cell in Figure 2, the
standardized API will be pandas.io.excel. base.read excel().
If the identiﬁed API is an alias, i.e., re()in the ﬁfth code cell in
Figure 2deﬁned by statement from pandas import read excel
as re), we will further replace it with its actual name whileconducting the API standardization step. The standardized
version will hence be pandas.read
excel(). Observant readersmay have observed that our approach will lead to two full-qualiﬁed names for the same API read
excel(). Indeed, at this
stage, it is non-trivial for our approach to be aware of that
by simply analyzing the notebook code. We hence considerthem as two independent APIs. Nevertheless, as discussed inthe previous subsection, both of these two full-qualiﬁed APInames will be recorded in the API bank thanks to the import-
ﬂow analysis. Hence, any imprecision in the analysis will not
impact the overall precision of our approach.
Moreover, Python code may involve instances of library
classes that are initialized by calling constructor methods. TheAPIs invoked by those instances should also be appropriatelyidentiﬁed and expanded. However, in Python, there is generally
no syntax level difference between initializing constructormethods and accessing standard methods. Therefore, addi-
tional efforts are needed to distinguish them and thereby to
allow the identiﬁcation of classes’ instances and their accessed
APIs. As an example, consider the code snippet in Listing 2:
1from ximport y
2m = y()
3m.fun()
Listing 2: An example of qualifying object member function
calls.
In Listing 2, our approach will ﬁrst identify that m.fun is a
member function call and then trace back to its construction
call m = y(); hence method fun() is an API in module x.
Subsequently, the fully qualiﬁed name of this API will be
x.y.fun.
D. Library Usage Analysis
The last module of SnifferDog, library usage analysis is
straightforward. Based on the second module’s outputs (i.e., a
set of APIs), this module queries these APIs against the APIbank to ﬁnd possible library candidates who have providedthese APIs. Normally, because each API has been provided
with a full-qualiﬁed name, the API bank can often precisely
locate its belonging library. The query output will hence be
multiple releases (or versions) of the same library. By integrat-
ing the query results of all the identiﬁed APIs, the objective of
this module is hence to ﬁnd a (minimal) list of libraries andtheir (maximum) version ranges that cover all the identiﬁed
APIs leveraged by the input notebook. SnifferDog will then
produce its output in common formats that describe Python
environmental dependencies, such Pipﬁle orrequirements.txt.
V. E
V ALUATION
To evaluate the effectiveness of SnifferDog, we address the
following research questions:
•RQ4: IsSnifferDog effective in mapping Python libraries
to their APIs?
•RQ5: How accurate is SnifferDog in inferring environ-
ment dependencies for Python Jupyter notebooks?
•RQ6: To what extent can SnifferDog assist users in
reproducing Jupyter notebooks?
1627A. Experimental Setting
Dataset of Jupyter notebooks. Recall that the goal of this
work is to automatically infer environmental dependencies for
Jupyter notebooks so as to help users execute and reproducenotebook outputs. To evaluate if our approach can achieve thisobjective, we resort to the approach introduced by Pimentelet al. [1] to collect 100,000 Jupyter notebooks from GitHub
to fulﬁll our experiments. GitHub is the world’s leadingsoftware development platform hosting millions of software
repositories. The 100,000 notebooks are retrieved from GitHub
projects containing ﬁles with Jupyter notebook .ipynb formats
and declaring Python as their programming language.
Dataset of Selected Python Libraries. Recall that the API
bank of our approach is built based on existing libraries, andit can be easily extended to include more libraries. Generally,the more libraries considered, the more comprehensive theAPI bank will be, and subsequently, the more precise and
sound results SnifferDog can achieve. Since we aim at gen-
erating dependencies for as many notebooks as possible, we
start by selecting the most popular 1,000 modules importedby the aforementioned 100,000 Jupyter notebooks. We thenleverage PyPI, the ofﬁcial Python package index, to query
the installation wheel ﬁles which contain the source codeof library implementation these selected modules. Because
several modules may belong to the same library, or somemodules have not yet been indexed by PyPI, we can onlylocate 488 Python libraries (with 17,947 different releases)
for the selected top-1000 modules. Therefore, in this work,
we leverage 488 distinct Python libraries with 17,947 releases
to construct the API bank.
B. RQ4: Effectiveness of API Bank
In this research question, we are interested in evaluating
the usefulness of the API bank. From the selected 488 Python
libraries, the library API mapping module extracts 1,013,718
APIs to ﬁll the API bank. Figure 7illustrates the distribution
of the number of APIs in each selected library, giving median
and mean values at 321 and 2,281, respectively, after excluding
outliers.
0 500 1000 1500 2000 2500 3000
Fig. 7: The distribution of the number of APIs per libraryacross all its version after removing outliers.
Towards evaluating the correctness of the constructed API
bank, we resort to a manual process to check if these APIs arecorrectly recorded in the API bank. To this end, we randomlyselected 166 APIs from the API bank to be manually validated.The number of selected APIs is decided by an online SampleSize Calculator [17] with a conﬁdence level at 99% and a
conﬁdence interval at 10. For each of the selected APIs, wemanually check it against its source code and ﬁnd that 164 ofthem are correct results, giving a precision of 98.8% for our
API bank construction approach.
In addition to the aforementioned manual investigation, we
further resort to a dynamic testing approach to evaluate the
correctness of the constructed API bank. Giving a mapping
from a library version to its APIs, when the library (with the
given version) is installed, all its APIs should be able to beimported. To this end, we implement a prototype tool to fulﬁllthis process automatically. Speciﬁcally, we ﬁrst randomlyselect 20 libraries, accounting for in total 3,982 APIs, fromthe API bank, and install them, respectively. For each of the
installed libraries, we then extract all of its recorded APIs from
the API bank and conduct runtime import testings to checkwhether these APIs can be imported at runtime. Among the3,982 considered APIs, only 252 of them fail to be imported
in our experiment, leading to a success rate of 93.6%. After
analyzing the traceback information of import errors, we ﬁnd
that most of such failures are related to missing dependencies
that are further required by the libraries under evaluation.
RQ4: (Effectiveness of library mapping) Is SnifferDog
effective in mapping Python libraries to their APIs?
The API bank constructed by the Library API mapping
module is precise: 98.8% of APIs are correctly extracted;
93.6% can be successfully imported.
Among the 1,013,718 APIs inferred from the 488 libraries,
686,915 of them could further introduce compatibility issues
to their client applications (if incorrect library versions are
installed), resulting in, for example, module not found errors
and import errors. The incompatible APIs include 543,387
(53.60%) newly added APIs after the ﬁrst libraries’ releases,
345,234 (34.06%) removed APIs compared to the libraries’
latest versions, and 58,594 (5.78%) APIs have their parameterschanged over the libraries’ evolution. Figure 8further presents
the distribution of newly added, removed, and updated APIsin each of the considered libraries, respectively.
Given the fact that 67.76% of the APIs (including added,
removed, and updated ones without duplication) may introducecompatibility issues, there is a strong need to also infer thecorrect versions of the dependent libraries when inferring the
environmental dependencies for Jupyter notebooks, Our API
bank records the detailed evolution changes of considered
libraries and is designed to infer not only the dependent
libraries, but also their correct versions.
RQ4 (Usefulness of API bank) Is SnifferDog effective in
mapping Python libraries to their APIs?
In our evaluation, more than half of the library APIs were
added, removed, or updated at some point in the libraries’
life cycles. This underlines the need to check for compatible
library versions, as SnifferDog does.
1628Added Removal Updated
0 200 400 600 800 1000
Fig. 8: The median values for the number of added and
removed and updated APIs are 45, 25 and 5 respectively after
excluding outliers.
C. RQ5: Effectiveness of SnifferDog
Let us now evaluate the effectiveness of SnifferDog in
inferring environmental dependencies for Jupyter notebooks.
We evaluate the effectiveness through one in-the-lab and one
in-the-ﬁeld experiment.
1) In-the-lab experiment: Recall that our preliminary study
has identiﬁed 725 notebooks that are (1) provided with in-stallable required dependencies, and (2) demonstrated to be
executable after the provided dependencies are installed. We
hence take these 725 notebooks as the ground truth to fulﬁllour in-the-lab experiment (because these notebooks are knownexecutable). Unfortunately, 385 (out of the 725) notebooks
have accessed libraries that are not yet considered by the
current API bank (constructed based on around 488 libraries).
Therefore, we have to exclude them from the ground truth. Our
ﬁnal ground truth is hence made up of 340 Jupyter notebooks
and their required libraries.
For the 340 notebooks, we then apply SnifferDog to auto-
matically generate experimental dependencies for them. Afterthat, we follow the same approach (as discussed in Section III)
to automatically install the generated libraries and execute
the corresponding notebooks. Experimental results show that
SnifferDog can successfully generate installation requirements
for 315 (92.65%) notebooks, among which 284 are successfuly
executed, giving a recall rate at 83.52%.
The installation failures are mainly related to library com-
patibility issues brought by the selected Python version (which
is usually not provided by notebook contributors) and the
underline Python setuptools [18]. For the 31 non-executable
cases, our manual investigation reveals that the failures (8ImportError, 7 ModuleNotFoundError and 16 other type of
rumtime errors) are caused by inaccurate version constraintsyielded by SnifferDog.RQ5: (in-the-lab) How accurate is SnifferDog in inferring
environment dependencies for Python Jupyter notebooks?
In a lab setting, SnifferDog is effective in automatically in-
ferring execution environments for Jupyter notebooks, suc-
cessfully generating installation requirements for 315/340(92.6%) of notebooks. 284/315 (90.2%) of notebooks couldbe executed automatically.
2) In-the-ﬁeld experiment: In this setting, we randomly
select 5,000 notebooks and launch SnifferDog to generate
execution environments for them. SnifferDog completes its
analysis in 18,141.29 seconds, or 3.63 seconds per notebook
on average.
We now check to which extent the generated environments
support the execution of notebooks. To reduce human inﬂuenceto a minimum, we restrict ourselves to a subset of notebooks
to fulﬁll this purpose as it is time-consuming to evaluate a
notebook, which involves installing all the dependencies and
executing all of its code cells. To this end, we apply the
following inclusion criteria to retain notebooks that (1) have
been provided with pre-deﬁned dependencies, which, however,cannot support their executions, and (2) are within the capacity
of our API bank. This gives us 722 notebooks for the in-the-
wide experiment.
Among the 722 notebooks, SnifferDog can successfully
generate installable dependencies for 667 of them, among
which 223 of them can further lead to successful executions
of the corresponding notebooks.
Note that over half of the notebooks remain non-executable.
Why is that so? Our manual analysis reveals the following twomain reasons (apart from issues raised by notebooks’ codequalities).
•Reason 1: The majority of notebooks fail to be executed
because of the existence of so-called optional dependen-
cies, which are not directly accessed by the notebooks
(hence overlooked by SnifferDog) but are required by the
notebooks’ directly dependent libraries.
•Reason 2: A number of failed notebooks are due to
the usage of magic functions, a special Jupyter notebook
feature allowing the access of Python modules without
following Python’s syntax [19]. At the moment, magic
functions are simply ignored by SnifferDog.
Moreover, for the failed notebooks, we further look into
their error messages and compare them against that outputedfrom executions with their own dependencies (consideredas the Baseline). In this experiment, only such notebooksthat fail on both sides are considered. Figure 9presents the
comparison results. Clearly, the number of errors related to theexecution environment for SnifferDog is signiﬁcantly smaller
than that of the Baseline. Oppositely, SnifferDog leads to
more errors related to the code quality of the notebooks (e.g.,FileNotFound, NameError, or HTTPError) compared to that
of the Baseline. This experimental result shows that, while the
notebooks fail to be executed in both environmental settings,the settings resulted from SnifferDog are more likely to be
1629correct than its counterpart (new runtime exceptions can be
further triggered when dependencies are supplied, e.g., for
FileNotFoundError, the ﬁle is not provided). Hence, even ifthe notebook is not executable yet, the dependencies producedbySnifferDog can assist users in getting closer to their goal.

,PSRUW(UURU
)LOH1RW)RXQG(UURU7LPHGRXW 1DPH(UURU .HUQHO'LHG
0RGXOH1RW)RXQG(UURU7\SH(UURU 9DOXH(UURU
$VVHUWLRQ(UURU $WWULEXWLRQ(UURU+773(UURU%DVHOLQH
6QLIIHU'RJ
Fig. 9: Top-10 runtime error types resulted from 255 notebooks
that can be not executed by environments recommended bySnifferDog and their original ones.
RQ5: (in-the-ﬁeld) How accurate is SnifferDog in inferring
environment dependencies for Python Jupyter notebooks?
Applied on random notebooks, SnifferDog is effective
in automatically inferring execution environments. When
notebooks remain non-executable, dependencies inferred by
SnifferDog help users to get them closer to the goal.
D. RQ6: Reproducing Jupyter notebooks
In the last research question, we evaluate if SnifferDog helps
users not only execute, but reproduce Jupyter notebooks (i.e.,
achieving the same outputs as that recorded originally in the
notebooks). To resolve the problem of cell ordering, we make
use of a prototype tool called Osiris that restores the order in
which cells are to be executed [2].
For our evaluation, we resort to the 507 notebooks that have
been demonstrated to be executable previously. We attemptto reproduce these 507 notebooks through the following two
experiments:
•Experiment 1: Osiris (baseline). We create execution
environments for different Python versions and install
all default packages (including more than 200 library
packages). Then We apply Osiris in this environment to
analyze the 507 notebooks.
•Experiment 2: Osiris (SnifferDog). In this experiment,
we use the dependencies recommended by SnifferDog
to set up the environment, and we launch Osiris in this
environment on the same 507 notebooks.
Figure 10presents the experimental results. Interestingly,
although with over 200 popular Python libraries installed in the
execution environment, around one-fourth of notebooks cannot
be fully executed. Compared to the default environment, thefailure rate decreases to less than 1% when the executionenvironment is set up based on the outputs of SnifferDog
3.
Regarding reproducibility, Osiris reports that, with the help
ofSnifferDog, 42 additional notebooks could be reproduced
compared to the 323 notebooks reproduced with the defaultOsiris conﬁguration. These experimental results show that our
approach is indeed effective in helping users execute and
reproduce Jupyter notebooks.
([HFXWDEOH
5HSURGXFLEOH
   2VLULV'HIDXOW 2VLULV6QLIIHU'RJ
Fig. 10: Running Osiris in a default environmental setting“Osiris (Default)” vs. running Osiris in a SnifferDog-generated
environmental setting “Osiris (SnifferDog)”.
RQ6: To what extent can SnifferDog assist users in repro-
ducing Jupyter notebooks?
Compared to a default environment setting, environments
restored by SnifferDog makes signiﬁcantly more notebooks
executable and reproducible.
E. Threats to V alidity
The experimental ﬁndings may suffer from various threats
to validity. The notebooks selected in this work may not be
representative. We attempt to mitigate this threat by starting
from 100,000 real-world Jupyter notebooks. We also resort tomanual analysis to summarize the execution errors as well asconﬁrm some of our experimental results. Such manual pro-
cesses are known to be error-prone. We have cross-validatedthe results to mitigate potential errors.
VI. R
ELATED WORK
We now discuss the closely related works from three as-
pects: Restoring Execution Environments, Studies on JupyterNotebooks, and Python dependency analysis.
A. Restoring Execution Environments
The most related work to ours is DockerizeMe [4], inferring
environment dependency conﬁgurations for Python code snip-
pets collected from GitHub Gist. DockerizeMe is implemented
based on a pre-acquired knowledge base of known Python
packages using static analysis, which is similar to the approach
of ours. However, in addition to static analysis, it further
leverages dynamic analysis to achieve its purpose, being
able to achieve 30% improvement in reducing ImportError
3Ideally, there should be no failed cases because those notebooks have
been demonstrated to be executable with the same environment. However,
in practice, Osiris is implemented in Python and per se is dependent on
several libraries, which may again conﬂict with the libraries recommended
bySnifferDog.
1630messages. While, in principle, DockerizeMe could also be ap-
plied to fully analyzed Jupyter notebooks, there are important
conceptual differences compared to ours. First, DockerizeMe
only considers the latest version of packages. It hence cannotdeal with programs containing deprecated, removed, renamed
APIs in the latest version of packages. Indeed, as empiricallyreported by Horton et al. [20] by an empirical study aboutthe executability of Python code snippets on GitHub. Their
experimental results show that most gists are not executablein a default Python environment, and while a naive approachcan infer dependencies for some gists, it fails to do so in themajority of cases. Second, DockerizeMe does not execute code
to validate its ﬁndings, let alone compare results against pub-
lished results—whereas SnifferDog automatically determines
the conﬁguration that makes the notebook executable, ideally
even reproducing the results. The authors further proposedthe tool V2 that takes the program crashes information to
guide the search for correct environment dependencies [21].
However this approach relies on repeated execution of code
snippets and does not handle the case when no crash happenand dependencies are incorrect. On the contrary, SnifferDog
is a static approach that analyzes dependencies using pre-builtknowledge.
B. Studies on Jupyter Notebooks
Despite their popularity, research on Jupyter notebooks is
still limited. In 2019, Pimentel et al. conducted a large scale
study on the executability and reproducibility issues of overone million selected notebooks [1]. Their experimental resultsshow that around 25% of the notebooks can be executed
without any runtime errors, and among which only 4.03% of
them can eventually produce the original results. Loenzen etal. [22] empirically investigate the code duplication and reusein Jupyter notebooks and ﬁnd that notebook repositories havea mean self-duplication rate of 7.6%. More recently, Wang
et al [23] conduct a large scale study on the code quality
and empirically ﬁnd that even notable Jupyter notebooks arefrequently suffered from technical debts (e.g., deprecated API
uses).
Following the discovery of low-reproducibility issues
among Jupyter notebooks, Wang et al. [2] went further to
propose to address the root causes leading to non-reproducible
notebooks by offering the community a tool called Osiris. Thistool attempts to reproduce Jupyter notebooks by leveraging
code instrumentation to ﬁnd out and address the uncertaintieswhen executing Jupyter notebooks. However, due to a lack
of appropriate execution environments, around 80% selectednotebooks failed to be fully executed. Following this researchline, Fangohr et al. [24] have further proposed another tool
called nbval (implemented as a plugin for pytest) aiming at
supporting automated testing and validation of Jupyter note-books. As argued by the authors, nbval could be leveraged to
promote reproducible science such as checking that deployed
software behaves as its documentation suggests.
In addition to researches from the Software Engineering
community, Jupyter notebooks have been selected frequentlyas subjects by our fellow researchers in other domains [25]–[33]. For example, Perkel et al. [3] have studied why Jupyter
notebooks are popular among data scientists. Kery et al. [26]
have introduced a tool named V erdant to support users with
efﬁcient retrieval and sensemaking of messy version data. Itallows users to compare, replay, and trace the relationshipsamongst different versions of artifacts of both non-code andcode in the editors. Furthermore, Rule et al. [34] look into the
notebooks from the aspects of human factors, and empirically
observed that computational notebooks may lack the explana-tory textual information.
C. Dependency Analysis
One of the most representative ones targeting Python de-
pendencies would be the work recently proposed by Ying
et al., who attempt to resolve dependency conﬂicts in thePython library Ecosystem [35]. They designed and imple-mented a tool named Watchman to detect dependency conﬂicts
among libraries indexed by the PyPI repository. They also
reported 117 potential dependency issues to the developers
of the corresponding projects. Despite Python has becomeone of the most popular programming languages nowadays,studies on Python projects focus on library API issues and
their evolution patterns [36], [37], there has not been muchrelevant research aiming at resolving Python dependencies.
Nevertheless, dependency analysis has been a hot research
topic for many other programming languages [38]–[44].
 The
concepts of these approaches, such as resolving compatibilityissues caused by the evolution of libraries [45], [46], auto-
mated replacing outdated libraries [47], or updating deprecated
library APIs [48], we believe, should also be appliable toPython software applications.
VII. C
ONCLUSION AND FUTURE WORK
Jupyter notebooks may be touted as a prime means to
obtain reproducible and replicable research results. In practice,however, they suffer from problems that Software Engineering
has solved long ago: bad code quality, insufﬁcient docu-
mentation, and—as shown in this paper—little to nonexistent
management of dependencies. It will take time until thecommunity of Jupyter notebook authors will learn to see theirnotebooks not only as entities to be published, but also as
living code that should be designed to be readable, reusable,
and maintainable. Until then, it will be up to the SoftwareEngineering community to reverse engineer notebooks suchthat they can be executed and tested.
In this paper, we have taken a major step towards this
goal, namely restoring the execution environments of Jupyternotebooks. We found that dependencies are hardly ever
stated explicitly, and that this problem seriously impedes re-
execution of Jupyter notebooks. By analyzing imports and
API usages in notebooks and matching them against Python
libraries in various versions, our tool SnifferDog can identify
library candidates that were used for notebook creation. Bysearching for candidate conﬁgurations that make the notebookexecutable again (and hence fully reproducible), SnifferDog
1631provides notebook users with essential information that makes
notebooks usable again. Given the popularity of notebooks,
SnifferDog thus shows how Software Engineering can make
an important contribution towards reproducible and extensiblescience.
There is still lots to do, though. Our future work will focus
on the following topics:
Larger API Bank. Our API bank is only built based on
488 libraries. While these make up the most popular
libraries, having a larger API Bank will further extend
the capability of our approach.
Python and C code. A small number of “Cython” libraries
combine both C and Python syntax to achieve C-like
performances, letting a small set of library APIs be
overlooked.
Advanced features. As already stated in Section V, some
advanced Jupyter notebook and Python features are not
yet supported by SnifferDog, notably magic functions and
indirect dependencies.
Beyond Python. The SnifferDog principles are not limited to
Python. We plan to extend SnifferDog to other popular
Jupyter notebook languages such as R and Julia.
Beyond notebooks. The SnifferDog principles also extend
beyond notebooks. SnifferDog could be equally applied to
C source code to determine which library versions wouldbe required for construction and execution. A version ofSnifferDog for L
ATEX that automatically determines re-
quired packages and versions may be especially welcomein scientiﬁc communities.
SnifferDog is available as open source (with explicit depen-
dencies, of course). A complete replication package including
all experimental data is available at
https://github.com/SMAT-Lab/SnifferDog.git
R
EFERENCES
[1] J. F. Pimentel, L. Murta, V . Braganholo, and J. Freire, “A large-
scale study about quality and reproducibility of Jupyter notebooks,”
in2019 IEEE/ACM 16th International Conference on Mining Software
Repositories (MSR). IEEE, 2019, pp. 507–517.
[2] J. Wang, T.-Y . Kuo, L. Li, and A. Zeller, “Assessing and restoring repro-
ducibility of Jupyter notebooks,” in The 35th IEEE/ACM International
Conference on Automated Software Engineering (ASE 2020), 2020.
[3] J. M. Perkel, “Why Jupyter is data scientists’ computational notebook
of choice,” Nature, vol. 563, no. 7732, pp. 145–147, 2018.
[4] E. Horton and C. Parnin, “DockerizeMe: Automatic inference of
environment dependencies for Python code snippets,” in Proceedings
of the 41st International Conference on Software Engineering, ser.
ICSE ’19. IEEE Press, 2019, pp. 328—-338. [Online]. Available:
https://doi.org/10.1109/ICSE.2019.00047
[5] “Pip user guide.” [Online]. Available: https://pip.pypa.io/en/stable/user
guide/#requirements-ﬁles
[6] “Anaconda software distribution,” 2020. [Online]. Available: https:
//docs.anaconda.com/
[7] “The Python language reference.” [Online]. Available: https://docs.
python.org/3/reference/import.html
[8] “Python module index.” [Online]. Available: https://docs.python.org/2.
7/py-modindex.html
[9] “Python module index.” [Online]. Available: https://docs.python.org/3.
1/modindex.html
[10] “Python module index.” [Online]. Available: https://docs.python.org/3.
2/py-modindex.html
[11] “Python module index.” [Online]. Available: https://docs.python.org/3.
3/py-modindex.html[12] “Python module index.” [Online]. Available: https://docs.python.org/3.
4/py-modindex.html
[13] “Python module index.” [Online]. Available: https://docs.python.org/3.
5/py-modindex.html
[14] “Python module index.” [Online]. Available: https://docs.python.org/3.
6/py-modindex.html
[15] “Python module index.” [Online]. Available: https://docs.python.org/3.
7/py-modindex.html
[16] “Python module index.” [Online]. Available: https://docs.python.org/3/
py-modindex.html
[17] “Sample size calculator.” [Online]. Available: https://www.surveysystem.
com/sscalc.htm
[18] “Documentation¶.” [Online]. Available: https://setuptools.readthedocs.
io/en/latest/
[19] “Built-in magic commands¶.” [Online]. Available: https://ipython.
readthedocs.io/en/stable/interactive/magics.html
[20] E. Horton and C. Parnin, “Gistable: Evaluating the executability of
python code snippets on github,” in 2018 IEEE International Conference
on Software Maintenance and Evolution (ICSME). IEEE, 2018, pp.217–227.
[21] ——, “V2: Fast detection of conﬁguration drift in python,” in
Proceedings of the 34th IEEE/ACM International Conference onAutomated Software Engineering, ser. ASE ’19. IEEE Press, 2019, p.
477–488. [Online]. Available: https://doi.org/10.1109/ASE.2019.00052
[22] A. Koenzen, N. Ernst, and M.-A. Storey, “Code duplication and reuse
in jupyter notebooks,” arXiv preprint arXiv:2005.13709, 2020.
[23] J. Wang, L. Li, and A. Zeller, “Better code, better sharing: On the need
of analyzing Jupyter notebooks,” in The 42nd International Conference
on Software Engineering, NIER Track (ICSE 2020), 2020.
[24] H. Fangohr, V . Fauske, T. Kluyver, M. Albert, O. Laslett, D. Cort ´es-
Ortu ˜no, M. Beg, and M. Ragan-Kelly, “Testing with jupyter note-
books: Notebook validation (nbval) plug-in for pytest,” arXiv preprint
arXiv:2001.04808, 2020.
[25] D. Koop and J. Patel, “Dataﬂow notebooks: encoding and tracking
dependencies of cells,” in 9th USENIX Workshop on the Theory and
Practice of Provenance (TaPP 2017), 2017.
[26] M. B. Kery and B. A. Myers, “Interactions for untangling messy history
in a computational notebook,” in 2018 IEEE Symposium on Visual
Languages and Human-Centric Computing (VL/HCC). IEEE, 2018,pp. 147–155.
[27] M. S. Rehman, “Towards understanding data analysis workﬂows using
a large notebook corpus,” in Proceedings of the 2019 International
Conference on Management of Data, 2019, pp. 1841–1843.
[28] A. Rule, I. Drosos, A. Tabard, and J. D. Hollan, “Aiding collaborative
reuse of computational notebooks with annotated cell folding,” Proceed-
ings of the ACM on Human-Computer Interaction , vol. 2, no. CSCW,
pp. 1–12, 2018.
[29] S. Samuel and B. K ¨onig-Ries, “ProvBook: Provenance-based semantic
enrichment of interactive notebooks for reproducibility.” in International
Semantic Web Conference (P&D/Industry/BlueSky),
 2018.
[30] A. Watson, S. Bateman, and S. Ray, “PySnippet: Accelerating ex-
ploratory data analysis in Jupyter notebook through facilitated accessto example code,” in EDBT/ICDT Workshops, 2019.
[31] H. Nguyen, D. A. Case, and A. S. Rose, “NGLview–interactive molec-
ular graphics for Jupyter notebooks,” Bioinformatics, vol. 34, no. 7, pp.
1241–1242, 2018.
[32] H. Fangohr, M. Beg, M. Bergemann, V . Bondar, S. Brockhauser,
C. Carinan, R. Costa, C. Fortmann, D. F. Marsa, G. Giovanetti et al.,
“Data exploration and analysis with jupyter notebooks,” in 17th Bien-
nial International Conference on Accelerator and Large ExperimentalPhysics Control Systems, no. TALK-2020-009, 2019.
[33] M. Garc ´ıa-Dom ´ınguez, C. Dom ´ınguez, J. Heras, E. Mata, and V . Pascual,
“Jupyter notebooks for simplifying transfer learning,” in International
Conference on Computer Aided Systems Theory. Springer, 2019, pp.
215–221.
[34] A. Rule, A. Tabard, and J. D. Hollan, “Exploration and explanation in
computational notebooks,” in Proceedings of the 2018 CHI Conference
on Human Factors in Computing Systems, 2018, pp. 1–12.
[35] Y . Wang, M. Wen, Y . Liu, Y . Wang, Z. Li, C. Wang, H. Yu, S.-
C. Cheung, C. Xu, and Z. Zhu, “Watchman: monitoring dependencyconﬂicts for python library ecosystem,” in Proceedings of the ACM/IEEE
42nd International Conference on Software Engineering, 2020, pp. 125–
135.
1632[36] J. Wang, L. Li, K. Liu, and H. Cai, “Exploring how deprecated Python
library APIs are (not) handled,” in The 28th ACM Joint Meeting on
European Software Engineering Conference and Symposium on the
Foundations of Software Engineering (ESEC/FSE 2020), 2020.
[37] Z. Zhang, H. Zhu, M. Wen, Y . Tao, Y . Liu, and Y . Xiong, “How
do Python framework APIs evolve? an exploratory study,” in 2020
IEEE 27th International Conference on Software Analysis, Evolutionand Reengineering (SANER). IEEE, 2020, pp. 81–92.
[38] C. Tucker, D. Shuffelton, R. Jhala, and S. Lerner, “Opium: Optimal
package install/uninstall manager,” in 29th International Conference on
Software Engineering (ICSE’07). IEEE, 2007, pp. 178–188.
[39] J. Patra, P. N. Dixit, and M. Pradel, “ConﬂictJS: ﬁnding and understand-
ing conﬂicts between JavaScript libraries,” in Proceedings of the 40th
International Conference on Software Engineering, 2018, pp. 741–751.
[40] C. Soto-Valero, A. Benelallam, N. Harrand, O. Barais, and B. Baudry,
“The emergence of software diversity in Maven Central,” in 2019
IEEE/ACM 16th International Conference on Mining Software Reposi-tories (MSR). IEEE, 2019, pp. 333–343.
[41] Y . Wang, M. Wen, Z. Liu, R. Wu, R. Wang, B. Yang, H. Yu, Z. Zhu, and
S.-C. Cheung, “Do the dependency conﬂicts in my project matter?” inProceedings of the 2018 26th ACM Joint Meeting on European SoftwareEngineering Conference and Symposium on the Foundations of SoftwareEngineering, 2018, pp. 319–330.
[42] Y . Wang, M. Wen, R. Wu, Z. Liu, S. H. Tan, Z. Zhu, H. Yu, and S.-C.Cheung, “Could I have a stack trace to examine the dependency conﬂict
issue?” in 2019 IEEE/ACM 41st International Conference on Software
Engineering (ICSE). IEEE, 2019, pp. 572–583.
[43] L. Li, T. Riom, T. F. Bissyand ´e, H. Wang, J. Klein, and Y . Le Traon,
“Revisiting the impact of common libraries for android-related investi-
gations,” Journal of Systems and Software (JSS), 2019.
[44] X. Zhan, L. Fan, T. Liu, S. Chen, L. Li, H. Wang, Y . Xu, X. Luo, and
Y . Liu, “Automated third-party library detection for android applications:Are we there yet?” in The 35th IEEE/ACM International Conference on
Automated Software Engineering (ASE 2020), 2020.
[45] L. Li, T. F. Bissyand ´e, H. Wang, and J. Klein, “Cid: Automating
the detection of api-related compatibility issues in android apps,” in
The ACM SIGSOFT International Symposium on Software Testing and
Analysis (ISSTA 2018), 2018.
[46] H. Cai, Z. Zhang, L. Li, and X. Fu, “A large-scale study of application
incompatibilities in android,” in The 28th ACM SIGSOFT International
Symposium on Software Testing and Analysis (ISSTA 2019), 2019.
[47] Y . Wang, B. Chen, K. Huang, B. Shi, C. Xu, X. Peng, Y . Wu, and Y . Liu,
“An empirical study of usages, updates and risks of third-party librariesin java projects,” in 2020 IEEE International Conference on Software
Maintenance and Evolution (ICSME). IEEE, 2020, pp. 35–45.
[48] L. Li, J. Gao, T. F. Bissyand ´e, L. Ma, X. Xia, and J. Klein, “Cda:
Characterising deprecated android apis,” Empirical Software Engineer-
ing (EMSE), 2020.
1633