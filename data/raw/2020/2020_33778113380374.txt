Recognizing Developers’ Emotions while Programming
Daniela Girardi
University of Bari, Italy
daniela.girardi@uniba.itNicole Novielli
University of Bari, Italy
nicole.novielli@uniba.it
Davide Fucci
Blekinge Institute of Technology, Sweden
University of Hamburg, Germany
davide.fucci@bth.seFilippo Lanubile
University of Bari, Italy
filippo.lanubile@uniba.it
ABSTRACT
Developers experience a wide range of emotions during program-
ming tasks, which may have an impact on job performance. In this
paper, we present an empirical study aimed at (i) investigating the
link between emotion and progress, (ii) understanding the triggers
fordevelopers’emotionsandthestrategiestodealwithnegative
ones, (iii) identifying the minimal set of non-invasive biometric
sensorsforemotionrecognitionduringprogrammingtasks.Results
confirm previous findings about the relation between emotions
and perceived productivity. Furthermore, we show that develop-ers’ emotions can be reliably recognized using only a wristband
capturing the electrodermal activity and heart-related metrics.
CCS CONCEPTS
•Human-centeredcomputing,Softwareanditsengineering ;
KEYWORDS
Emotionawareness,emotiondetection,biometricsensors,empirical
software engineering, human factors in software engineering
ACM Reference format:
DanielaGirardi,NicoleNovielli,DavideFucci,andFilippoLanubile.2020.
RecognizingDevelopers’EmotionswhileProgramming.In Proceedingsof
42nd International Conference on Software Engineering, Seoul, Republic of
Korea, May 23–29, 2020 (ICSE ’20), 12 pages.
https://doi.org/10.1145/3377811.3380374
1 INTRODUCTION
Software development is an intellectual activity requiring creativ-
ity and problem-solving skills, which are influenced by affective
states [18]. Previous work shows that positive emotions are benefi-
cialfordevelopers’well-beingandproductivitywhilenegativeones
leadtopoorjobperformanceandaredetrimentaltothesoftware
developmentprocess[ 17,27,42].Hence,emotionawareness—i.e.,
theawarenessofownandothers’emotions—isregardedasakey
to success for software projects [7].
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE ’20, May 23–29, 2020, Seoul, Republic of Korea
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-7121-6/20/05...$15.00
https://doi.org/10.1145/3377811.3380374Earlydetectionofnegativeemotions,suchasstress[ 38],frustra-
tion [10], and anger [ 13] can enable just-in-time corrective actions
for developers and team managers, preventing burnout and un-
desiredturnover[ 38].Recentresearchfindingsdemonstratethat
negativeemotionscanbecausedbyuneventaskdistribution,wrong
estimation of size and time required to complete an assignment,
difficulties in solving a complex cognitive task, and obstacles when
familiarizingwithanewtechnologyorprogramminglanguage[ 10].
Information about developers’ emotional state can be leveraged to
improve collaborative software development strategies [ 23]. For
instance, enriching retrospective meetings with feedback about de-
velopers’emotionscanbeusedtoreflectasateamonopportunities
forimprovement.Weenvisiontheemergenceandadoptionoftools
for enhancing emotion awareness during software development.
In this study, we focus on the identification of the emotions
experienced by developers engaged in a programming task. Specif-
ically, we operationalize emotions along the valence and arousal
dimensions of theCircumplex Model of affect [50]. First,we build
uponrecentresearchinvestigatingtherelationbetweendevelopers’
emotions and perceived productivity [19, 42].
We formulate our first research question as follows:
RQ1- What is the range of developers’ emotions during a
programming task and to what extent they correlate with
their perceived progress?
ToaddressRQ1,weperformastudywith23participantsengagedin
aprogrammingtask.Weaskparticipantstoperiodicallyself-report
their emotional state and self-assessed progress. We analyze therange of emotions reported and their correlation with perceived
progressbyfittingalinear-mixed,asdoneinpreviouswork[ 19,42].
Asasecondgoal,weaimatdiscoveringthecausesofpositive
andnegativeemotionsexperiencedduringsoftwaredevelopment.
Furthermore,weaimatidentifyingthecopingstrategiesthatmight
help programmers dealing with negative emotions. As such, we
formulate our second research question:
RQ2- What are the triggers for developers’ emotions and the
strategies they implement to deal with negative ones?
To addressRQ2, weinterview thestudyparticipants andperform
manual coding of their answers to open-ended questions.
Previousworkdemonstratedthefeasibilityofsensor-basedemo-
tion detection using non-invasive biometric devices [ 15,42]. How-
ever, the experimental setting usually employed in a laboratorysetting is too complex for being applied in practice. We aim atidentifying the minimal set of sensors to wear in the workplace
for reliable emotion detection. Accordingly, we formulate our third
research question:
6662020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)
RQ3- What is the minimal set of non-invasive biometric
sensors to detect developers’ emotions?
We usesupervisedmachinelearning totraina classifierfordevel-
opers’ emotions based on biometric features, with different sensor
configurations.
The contributions of this work are:
•A list of emotional triggers related to software development,
including strategies to deal with negative emotions;
•Asetofsupervisedclassifiersofdevelopers’emotions.These
include, to the best of our knowledge, the first attempt at
classifying arousal during a programming tasks;
•Alabpackage1toverify,replicate,andbuilduponthisstudy.
The reminder of the paper is structured as follows. Section 2
describes the theoretical model we use to operationalize emotions,
reviewstheexistingliteratureonsensor-basedemotiondetection,
andsummarizes theempiricalstudieswebuild-upon inthiswork.
In Section 3, we describe the design of this study. In Section 4, we
report the analysis results and answer the research questions. In
Section5,wecompareourfindingswithpreviousworkanddiscuss
their implications and limitations. Section 6 concludes the paper.
2 BACKGROUND
In this section, we introduce the most important theories on mod-
elingemotionsandsummarizethestateoftheartonrecognizing
emotionsusingbiometricsensors.Moreover,wereporttwostudies
relating emotions to perceived progress in software engineering.
2.1 Emotion Modeling
Psychologists worked on decoding emotions for decades, develop-
ingtheoriesbasedoncognitivepsychologyandnaturallanguage
communication.Twotheorieshaveemerged.Thefirstposesthat
a limited set of basic emotions exists. However, there is no con-sensus about their number or nature [
9,35]. The second theory
considersemotionsasacontinuousfunctionofoneormoredimen-
sions [50]. Dimensional models are not influenced by cultural or
linguisticfactors[ 22,51],whichmakesthemmorerobustcompared
to discrete models. Consistently with prior research on emotionawareness in software engineering [
19,24,38,42], we use a con-
tinuous representation of developers’ emotions. Specifically, we
refertotheCircumplexModelofAffect(seeFigure1),whichrep-resentsemotionsaccordingtotwodimensions—valence (pleasant
vs. unpleasant) and arousal(activation vs. deactivation). According
tothis model,each emotioncan beconsidered a“labelfor afuzzy
set, defined as a class without sharp boundaries” [ 50]. Pleasant
emotional states, such as happiness, are associated with positive
valence,whileunpleasantones,suchassadness,areassociatedwith
negativevalence. Arousal describes the level of activation of the
emotional state ranging from inactive or low, as in calmness or
depression, to active or high, as in excitement or tension.
2.2 Sensor-based Emotion Classification
The link between emotions and physiological feedback—measured
using biometric sensors—is investigated in the field of affective
computing [ 28,29,53]. Among the several physiological measures
1https://figshare.com/articles/_/9206474
Figure 1: The Circumplex Model of Emotions [50].
that correlate with emotions, previous research investigated the
electricalactivityofthebrain(EEG)[ 30,37,47,53],theelectricalac-
tivityoftheskin(EDA)[ 4,25],theelectricalactivityofcontracting
muscles(EMG)[ 15,29,44],andthebloodvolumepulse(BVP)from
which heart rate (HR) and its variability (HRV) are derived [ 5,52].
Specifically, changes in the EEG spectrum provide an indication
of overall levels of arousal or alertness [ 30] as well as pleasantness
of the emotion stimulus [ 47]. For example, Soleymani et al. [ 53]
found that high-frequencies sensed from electrodespositioned on
the frontal, parietal, and occipital lobes have high correlation with
valence.Similarly,LiandLu[ 37]showthatitispossibletodiscrim-
inate between happiness and sadness by analysising EEG signals.
ConcerningEDA,studiesinpsychologydemonstratehowthis
signal considerably varies with changes in emotional intensity and
specifically with the arousal dimension [ 33]. Changes in EDA are a
resultofincreasedactivityofthesweatglands,whichtakesplaceinpresenceofemotionalarousalandcognitiveworkload.Hence,EDA
has been employed to detect excitement,stress, interest, attention
as well as anxiety and frustration [4, 25].
BVP, HR, and HRV metrics—captured by a plethysmograph—
have been successfully employed for emotion detection [ 5,52].
Facial EMG is particularly useful in predicting emotions [ 29,44].
However, it leads to poor results when the sensors are placed on
body parts other than the face (i.e., the arms [ 15]). Accordingly, we
exclude EMG from this study.
Inthisstudy,weincludemeasuresfromEEG,EDA,BVP,andHR
astheycanbecollectedusinglow-costnoninvasivesensors[ 15,42]
thatcanbecomfortably used bydevelopersduringprogramming
tasks(seeSection3.3).This choiceisinlinewithcurrentresearch
investigating the use of lightweight biometric sensor for study-inghumanaspectsinsoftwaredevelopment.Fuccietal.[
12]use
EEG, EDA, and heart-relatedmeasurements for the automaticiden-
tification of code comprehension tasks. Fritz et al. [ 11] rely on
a combination of EEG, BVP, and eye tracker to assess difficulty
incodecomprehensionandpreventdevelopersfromintroducing
bugs. In a follow-up study, they employ the same set of sensors to
667distinguish between positive and negative emotions during pro-
gramming tasks [ 42]. Similarly, EDA, HR, HRV, and breath-related
metricshavebeenusedinafieldstudytoidentifycodequalitycon-
cernsduringsoftwaredevelopment[ 43].Züegeretal.[ 59]combine
heart-relatedmetricswithawristbandactivitytrackertopredict
developers’ interruptibility.
2.3 Former Studies
Ourstudybuildsuponthedesignandresultsoftwoformerstudieslinkingdevelopers’emotionswiththeirperceivedprogress[
19,42].
Theyfollowsimilarexperimentalprotocolsinvolvingalongitudinal
study with repeated measures of novice and professional develop-
ers’ emotional states. In particular, Graziotin et al. [ 19] recruited
eightsubjectsofwhichfourprofessionaldevelopersandfourunder-
graduate students with major in computer science, whereas Müller
and Fritz [ 42] observed 17 subjects—six professionals (average ex-
perience seven years) and 11 computer science PhD students.
Inbothstudies,theparticipantswereobservedduringaprogram-
mingsessionandinterruptedeveryfiveminutestoansweraself-
reportsurvey.Emotionswereself-reportedalongthevalenceand
arousaldimensions.MüllerandFritzmeasuredtheself-perceived
participants’ progress while completing two programming tasks
(30minuteseach).Conversely,Graziotinetal.askedparticipants
toreportonperceivedproductivitywhileworkingfor90minutes
in a natural setting—i.e., on their own projects. Graziotin et al. also
measured dominance—i.e., the extent to which a subject feels in
control or controlled [2].
ThestudiesofGraziotinetal.[ 19]andMüllerandFritz[ 42]pr o-
vide empirical evidence that valence correlates with self-perceived
productivity and progress, respectively. Among the main causesfor negative emotions, leading to the perception of being stuck,
Müller and Fritz report cognitive difficulties, impossibility to fulfill
informationneeds,andcodenotworking.Conversely,beingable
to understand the code and identify a solution strategy are among
the top reasons for positive affect.
Finally, Müller and Fritz trained a supervised emotion classifier
abletodistinguishbetweenpositiveandnegativeemotionswithanaccuracyof71%.Intheirsetting,theyusemultiplesensorsincluding
EEG, EDA, HR, and eye tracking metrics. However, they neglect
the arousal classification.
3 STUDY DESIGN
Inthissection,wereportabriefcharacterizationoftheparticipants
in our study, the study task, the tools and devices used to measure
the relevant constructs, and the study protocol.
3.1 Participants
We recruited 27 CS students (23 males, four females) from the
Department of Computer Science of our University of which 21
undergraduates,fivegraduates,andonepost-graduate.Following
aconveniencesamplingstrategy,werecruitedvolunteersaspar-ticipants only if they could provide evidence they cleared exams
where Java programming (e.g., the programming language for the
this study task) was used for capstone projects.3.2 Development Task
Weuseoneofthetwotasks,includingthematerials,designedby
Müller and Fritz in their study [ 42]. The task consisted in writing a
JavaprogramusingtheStackExchangeAPI2toretrieveallanswers
posted by a specific user on StackOverflow and sum up the scores
the user earned for these answers. The participants were provided
with a skeleton code that they modify to complete the task.
3.3 Measurement Tools and Devices
Biometricsensors. Wemeasurethesubjects’physiologicalsignals
using lightweight biometric sensors analogous to those employed
by Müeller and Fritz [ 42]—i.e., comfortable to wear in the work
environment [ 16]. Specifically, we use the NeuroSky BrainLink
headsettorecordtheEEGwavesandtheEmpaticaE4wristband
for EDA, BVP, and heart-related metrics (see Figure 2).
Figure 2: A participant wearing the Empatica E4 wristbandand the BrainLink headset during the study.
The BrainLink EEG uses one electrode placed on the surface of
thescalpandareferenceplacedontheearlobe3(seeFigure2).EEG
wavesareextractedbypre-processingtherawsignalcollectedby
the device with a sample frequency of 512Hz. Besides raw EEG
signal,BrainLinkextractsmetricsrelatedtomentalfocus(i.e., at-
tention) and calmness (i.e, meditation ).4
The Empatica E4 wristband measures EDA with a sample fre-
quency of 4Hz. It features a plethysmograph for collecting BVP
sampled at a frequency of 64Hz. BVP is used to derive the HR and
HRV.FollowingtheEmpaticaguidelines5,weexcludedHRVasit
is unreliable in dynamic conditions (i.e., when typing).
Self-report of Emotions and Progress. The measurement of
emotions and perceived progress is performed through experience
sampling [ 19,42]. In line with the approach implemented by Grazi-
otin et al. [ 19], we collect self-reported valence and arousal ratings
2https://api.stackexchange.com
3Thereference electrodeis usedtocompute theamount ofinformation oftheactive
EEGelectrodeplacedontheforehead.Assuch,ithastobeplacedonaneutralplace
like the earlobe [36]
4http://developer.neurosky.com/docs/doku.php?id=esenses_tm
5https://support.empatica.com/hc/en-us/articles/360030058011-E4-data-IBI-
expected-signal
668Figure 3: The SAM mannequins for assessment of valence
andarousalandthe5-pointLikertscalequestionforassess-ment of the perceived progress.
duringinterruptionsofthedevelopmenttaskusingSelf-Assessment
Manikin(SAM)[ 2].Consistentlywithpreviousassessmentofva-
lence and arousal in affective computing research [ 29], we use a
nine-point scale. Figure 3 shows the SAM mannequins for valence
and arousal (top) as well as a 5-point Likert item (bottom) to assess
the perceived progress. During the interruptions, we prompt the
participant to elaborate on the triggers for the emotional state and
take notes of their answers.
Debriefing questionnaire. Toelicittriggersandstrategiesfor
handlingemotions,thefirstauthorinterviewseachparticipantat
the end of the development task, asking the following questions:
•Whatarethecausesforyour positiveemotionsduringpro-gramming?
•
What are the causes for your negative emotions during pro-
gramming?
•Which strategies do you adopt to deal with negative emo-
tions during programming?
3.4 Experimental Protocol
Weorganizethestudyaccordingtothefollowingphases(Figure4).
Pre-experimental briefing. The participant gets acquainted
with the settings—e.g., sitting in a comfortable position, adjusting
themonitorheight.Theexperimentersummarizestheupcoming
stepsandexplainstheprogrammingtask.Theparticipantsignsthe
consent form to allow anonymous treatment of the collected data.
Sensor calibration and emotion elicitation. The participant
wears the biometric sensors (see Figure 2) and the experimenter
checks that the devices record the signals correctly.
Beforetheparticipantstartsworkingontheactualprogramming
assignment,theexperimenterasksher totakepartinanemotion-
elicitationtask.Thepurposeofthisstepistwo-fold.Ononehand,itallowsthesubjecttogetacquainted withtheSAM mannequins;on
the other hand, it allows the experimenter to collect her biometric
and SAM feedback, both in a neutral condition andin presence ofcontrolledstimuli. Thisstep followsthe designdescribedin apre-
vious emotion-elicitation study [ 15]. The participant watches eight
videos,selectedfromtheDEAPdataset[ 29],associatedwithvalence
and arousal scores on a scale from 1 to 9. Each video is mapped to
thefourquadrantsoftheemotionalspaceintheCircumplexModel
of Affect based on a discretization of the scores. Specifically, the
eight videos used in the emotion elicitation are equally distributed
amongthefourquadrants—i.e., positivevalenceand hiдharousal,
positivevalenceand lowarousal,neдative valenceand lowarousal,
neдative valence and hiдharousal. The emotion elicitation step
lasts10minutes,withtheeightvideospresentedinfoursessions.
Eachsessionconsistsofa30-secondbaselinevideoshowingaquiet
image with relaxing music in the background, followed by a 2-
minutedisplayoftheselectedvideos(oneminutepervideo).Atthe
beginning of each trial, a 3-second screen displays the current trial
number to make the participant aware of her progress. After each
video, the participant is instructed to report her emotions using
the SAM mannequins. Therefore, each subject provided 16 pairs of
ratings—i.e., one valence and one arousal assessment per video.
Priortothedevelopmenttask,theparticipantwatchesa2-minute
relaxingvideoofanaturescenerycapableofinducingrelaxation
andaneutralemotionalstate[ 49].Weusebiometricsrecordedwhen
showing such video as physiological baselines for the participants.
Software development task and self-report. The core of the
studyis a30-minutedevelopment sessionduringwhichwe apply
experiencesampling[ 34].Theexperimenter(i.e.,thefirstauthor)
observed the behavior of participants during the entire sessionand interrupted them every five minutes, asking to report theiremotions, perceived progress, and to provide information aboutthe reasons for their emotions. We choose a time frame of five
minutesasitrepresentstheaveragetimeforwhichdevelopersstayfocusedonasingletask[
41,42].Wecollectthesubjects’biometrics
during the entire duration of the development task. After the task,
participants watch again the 2-minutes relaxing video to ward-off
possible induced emotions, for example, from not succeeding in
solvingthetask.Intotal,eachparticipantprovidessixratingsfor
valenceandsixforarousal(i.e.,onevalence-arousalratingforeach
interruption). In addition, participants provide six progress ratings
(oneperinterruption).Finally,theyprovidesixanswerstotheopen-
endedquestionaboutthetriggersforreportedemotions,whichwe
use in the data quality assurance step.
Final interview . We run the post-experimental debriefing with
eachparticipantfor approximately10minutes.Theexperimenter
interviewsparticipantstoinvestigatei)thetriggersforpositiveand
negative emotions during the task, and ii) the strategies subjects
implement todeal withnegative emotions. Theparticipants could
also ask questions and give feedback about the experiment. Finally,
participants are rewarded with a voucher for a meal.
DataQualityAssurance .Oncetheexperimentwascompleted,
but before analyzing the data, we manually performed a sanitycheck of the collected data. In particular, we investigated poten-
tialmalfunctioningofthesensorswhichcanintroducenoiseand
discontinuedrecordingofrawsignals.Inaddition,wecheckthecon-
sistency ofthe self-reported valence,arousal, and progressscores
with respect to the comments provided in the open questions. We
looked for signs of negligence, inconsistencies, and misinterpreta-
tionoftheguidelinesforusingtheSAM-basedreport.Forexample,
669Figure 4: The timeline of the study.
one subject scored his arousal as eight for all the interruptions,
however, this did not match the content of his comments. As a
resultsof this step,wediscardedfour participants(allmales).The
final set of participants includes 23 subjects, of which four females.
4 ANALYSIS AND RESULTS
In this Section, we answer our research questions by analyzingthe data collected in the study using a mix of quantitative and
qualitative methods.
4.1 Experienced Range of Emotions and their
Correlation with Progress (RQ1)
We analyze the self-reported scores for emotions and progress col-
lected through experience sampling. As described in Section 3.4,
eachparticipantprovidedself-assessmentofemotionsateachin-
terruption, thus reporting six pairs of SAM-ratings for valence and
arousal.Ourdatasetincludes138(6 x23subjects)ratingsforeach
emotion dimension and 138 (6 x23) progress scores.
To investigate the range of developers’ emotions during the
programming task, we analyze and compare the reported emotion
scoresthedevelopersreportedduringtheprogrammingtaskand
emotion elicitation, as proposed by Müeller and Fritz [42].
Figure5:Boxplotsforvalence(a)andarousal(b)duringthesoftware development task and emotion elicitation video.
Figure5reportstheSAMscoresforvalenceandarousal.Weuse
the scores provided during the video-driven emotion elicitation as
a reference for the way participants report valence and arousal in
presenceofthecontrolledstimulus.Forexample,therangeofSAMscoresreportedforpositivevideosindicatehowtheparticipantsrateTable1:Frequencyofprogressscoresona1–5scale(n=138).
Stuck Neutral In flow
Score 1 Score 2 Score 3 Score 4 Score 5
28 (20%) 59 (43%) 36 (26%) 11 (8%) 4 (3%)
overall 87 (63%) 36 (26%) 15 (11%)
theirvalencewhenexperiencingapositiveemotion.Weobserve
that the entire range of emotions is covered by the scores reported
while watching videos as well as programming. For valence, we
observe a clear distinction between the ranges used for positiveand negative videos (Figure 5a). In addition, negative emotions
tendtoprevailduringtheprogrammingtask—i.e.,themedianscorecorrespondstotheonereportedforthenegativevideos.Conversely,
themedianvalueforthearousaldistributionoftheprogramming
task (Figure 5b) corresponds to the one for high arousal videos.
Theparticipants reportthe wholerangeof progress (i.e.,1–5),
fromcompletelystuck (scoreequalto1)to inflow(scoreequalto
5), with median = 3 and inter-quartile range = 2 (see Table 1). Most
developershadtroublessolvingthetask—onlyfoursucceededin
developing a complete solution. Accordingly, most of the time they
reported being stuck (63% of answers). They reported being in a
neutral state 26% of the cases, corresponding to 36 answers, and in
flowin 11% of the cases (15 answers).
We investigate the link between reported emotions and per-
ceived progress by fitting a linear mixed model, which is robustin case of repeated measurements and longitudinal data [
21]. To
create the model, we used the lme4R package6. Consistently with
the approach adopted in the former studies [ 19,42], we consider
progressas thedependentvariable andvalence,arousal, andtheir
interactionwithtimeasfixedeffects.Givenourstudydesign,we
cannotexcludethattheperceivedprogresscanbeimpactedbytime.
Therefore, time and its interaction with the emotional dimensions
are also included in the model. We account the SAM individual dif-
ferences using Z-scores to standardize valence and arousal [ 19,42].
In Table 2, we report the parameter estimation for the mixed-
effect model and the percentage deviance explained by each effect.
Our model significantly differs from the null model—i.e., the model
with no correlation between fixed effects and progress ( χ2(5)=
63,p<0.001).Weobserveasignificanteffectofvalenceonprogress
6https://cran.r-project.org/web/packages/lme4/index.html
670Table 2: Parameter estimation for the fixed effects on Per-
ceived Progress (∗ indicates a statistically significant esti-
mate with α=0.05).
Fixed Effects Estimate Upper
p-value
(132 d.f.)Lowerp-value
(103 d.f.)Dev. ex-plained
Valence 0.17 (*) 0.00 0.00 27.8%
Arousal -0.05 0.23 0.23 0.5%
Time -0.02 0.97 0.97 0.0%
Valence:Time 0.03 (*) 0.05 0.05 1.4%
Arousal:Time 0.02 0.24 0.25 0.48%
at 95% confidence level. Valence also shows the highest explana-
torypowerwith27 .80%ofdevianceexplained,comparedto30 .15%
observed for the whole model. Conversely, we did not observe any
effect of arousal on self-reported progress. This result holds for the
effect of time and its interaction with arousal. Our model shows a
statistically significant correlation between progress and the inter-
action between time and valence. The effect of such interaction on
the overall model is small (1.4% of deviance explained).
Summary RQ1 - Developers experience a wide range of
emotions during programming tasks. We observe a preva-
lence of negative valence and high arousal. Valence is pos-
itively correlated with perceived progr ess. Howev er, dif-
ferences are observed between individuals.
4.2 Triggers and Strategies for Emotions (RQ2)
Toinvestigatetriggersforemotions,aswellasthestrategiestodeal
with negative ones, we manually analyzed the 69 answers (three
per participant) provided during the debriefing questionnaire.
We performed qualitative data analysis using a sentence-by-
sentence approach in a semi-exploratory fashion. We applied se-lective coding [
40] based on the constructs associated with the
researchquestion(i.e.,positiveandnegativeemotions,aswellas
strategies for coping with the latter). We identified 29 sentences
discussingpositiveemotionstriggers,41fornegativeemotionstrig-
gers,and47forcopingstrategies.Subsequently,tworesearchers
coded each sentence following an open coding approach [ 40]. Dur-
ingameeting,theresearchersreconciledtheircodesinasingleone.
Weobtained23codes—eightreasonsforpositiveemotions,eight
for negative ones, and seven strategies for dealing with negativeemotions. These codes were then grouped to form relationships
and themes captured by applying axial coding [ 40]. Three themes
emerge:selfrefers to the developers’ dimension, socialrefers to
peers and collaborators, and solutionrefers to issues with artifacts,
design, and implementation of the task.
Table3showsthethemesidentifiedastheresultofthecoding
process. The most frequent trigger for emotions refers to the so-
lutiondimension(25occurrencesoverall,ofwhich 14forpositive
and 11 for negative). The participants felt particularly happy when
abletoincrementally implementthedesignedsolutionandwhen
believing the solution itself is simple. Analogously, unexpected out-
put,unexpectedusage oflibraries,and unavailabledocumentationtriggernegativeemotions.Themostfrequentcausesfornegative
valencerelatetotheselfdimension(15occurrences),withthede-
velopers reporting thefeeling of being stuck and the awarenessof
time pressure as the main causes for negative emotions. Finally, the
socialtheme appears with a low frequency as the participants had
to complete the programming task by themselves.
Asforthestrategiesdevelopersimplementtodealwithnegative
emotions, take breaks is the most popular one, immediately fol-
lowedby lookforcollaboration —e.g.,seekinghelpfrompeers.Some
participantsalsoreport changingtask andstartingover asstrategies
toregainfocusandshifttowardspositiveemotions.Theyindicated
changing approach anddecomposing the problem into simpler ones
as strategies to gain confidence and react to negative emotions.
Summary RQ2 - Developers’ positive emotions are trig-
gered by the effectiveness of the implemented solution.Unexpected code behavior and missing documentationcause negative emotions. The latter are also due to time
pressureandself-perceivedlowproductivity.Developers
deal with them by taking breaks and looking for help.
4.3 A Minimal Set of Sensors for Detecting
Developers’ Emotions (RQ3)
We address RQ3 using machine learning for classifying the partici-
pants’emotions duringthedevelopment task.Figure6 showsthe
implemented machine learning pipeline.
DatasetThe dataset consists of the self-reported emotions of
participants during the development task. Each of the 23 partici-
pantsperformedatotalofsixSAM-basedassessmentsofvalence
andarousal.Asaresult,weobtainedtwodatasetsof138observa-
tions, one for valence and one for arousal. We trained two separate
classifiers by considering features extracted from the biometric
signals associated to each observation captured by the sensors.
Wedefinethe positiveandnegativelabelsforvalence,and high
andlowlabels for arousal. For this purpose, we discretize the SAM
scoresfollowingtheapproachappliedbyMüllerandFritz[ 42].First,
weadjustedthevalenceandarousalscoresbasedonthemeanvalues
reported while watching the emotion-triggering videos. Following
suchapproach,wedefinedgoldlabelsforvalenceandarousalby
taking into account (and correcting for) fluctuations due to the
participants’ subjective interpretation of the SAM scale. Then, we
assigned a positivevalence label (respectively a higharousal label)
to instances with scores above the mean and a negativevalence
label (respectively a lowarousal label) to instances with scores be-
low it. Finally, following the recommendation reported in previous
work [42], we manually inspected 31 observations for valence and
30forarousalforwhichweobservedscoresinthe mean±0.5in-
terval. For such cases, two authors manually assigned valence and
arousal labels. They obtained a substantial agreement [ 57], with
κ=.67 and observed agreement =84%. The few disagreement
cases (less than five for each dimension) were resolved in a dis-
cussion, following a consolidated approach in affective computing
research[ 1].Attheendofthisprocess,weobtainedthedistribution
reported in Table 4.
671Table 3: Themes identified after coding. Code occurrences are reported in parentheses.
Self
11 codes (43)Social15 codes (19)Solution9 codes (25)
TriggersPositive Emotions8 codes (24)New challenges (3)In flow (3)Personal experience (1)Knowledge sharing (2)Collaborative development (1)Incremental (8)Simplicity (4)No errors (2)
Negative Emotions8 codes (31)Being stuck (8)Time pressure (6)Multitasking (1)Unavailable collaborators (3)Peer pressure (2)Unexpected output (4)Unexpected usage (4)Unavailable documentation (3)
StrategiesNegative Emotions7 codes (32)Take breaks (17)Change task (1)Start over (1)Decompose problem (1)Change approach (1)Look for collaboration (9)Explain problem (2)—
Table 4: Gold standard for valence and arousal.
ValencePositive NegativeArousalHigh Low
44 (32%) 94 (68%) 85 (62%) 53 (38%)
Preprocessingand Featuresextraction Althoughthebiomet-
ric signals were recorded during the entire experimental sessionfor all the participants, we only consider the signals recorded inproximity of the stimuli of interest—i.e., the signals collected in
the 10 seconds before the subjects were interrupted. This choice is
in line with consolidated practices in related research on sensor-
basedclassificationofemotional[ 15,42]andcognitivestates[ 12]
of software developers. To synchronize the measurement of the
biometric signals with the emotion self-assessment, we (i)save the
timestamp of the interruption (t_interruption) ,(ii)calculate
the timestamp for relevant timeframe for each interruption—i.e.,
10secondsbeforetheself-report (t_start) ,and(iii)selecteach
signal samples recorded between t_startandt_interruption.
We normalize the signals to each participant’s baseline using
Z−score[42].Thebaselineiscalculatedbasedonthelast30seconds
ofthevideousedtoelicitaneutralstatebeforestartingthetask[ 11].
To maximize the signal information and reduce noise caused by
movements, we applied multiple filtering techniques. Regarding
EEG and BVP, we extract frequency bands using a band-pass filter
algorithm at different intervals [ 5]. The EEG signal can be decom-
posedintofivewavesbasedonthefrequency,namelydelta( <4Hz),
theta (4-7,5Hz), alpha (4-12,5Hz), beta (13-30Hz), and gamma ( >
30Hz). We apply the filter to extract the distinct cerebral wavesas each spectrum might provide different information. The EDA
signalconsistsofatoniccomponent(i.e.,thelevelofelectricalcon-ductivityoftheskin)andaphasiconerepresentingphasicchanges
inelectricalconductivityorskinconductanceresponse(SCR)[ 3].
We extract the two components using the cvxEDA algorithm [20].
Aftersignalspre-processing,weextractedthefeaturespresented
in Table 5, which we use to train our classifiers. We select features
based on previous studies using the same signals [12, 15, 42].
Classification Settings .Inlinewithpreviousresearchonbio-
metrics [ 12,29,42], we choose eight popular machine learningTable5:Machinelearningfeaturesgroupedbyphysiological
signal and by sensor.
SignalFeatures
Sensor:Brainlink
EEG- Freq. bin for alpha, beta, gamma, delta, theta waves- Ratio between freq, bin of each band and one another- For attention and meditation: min, max,mean, sd. deviation (diff. between baseline and task)
Sensor: Empatica E4
EDA- mean tonic- phasic AUC- phasic min, max, mean, sum peaks amplitudes
BVP- min, max, sum peaks amplitudes- mean peak amplitude (diff. between baseline and task)
HR- mean, sd. deviation (diff. between baseline and task)
algorithms—i.e., Naive Bayes (nb), K-Nearest Neighbor (knn), C4.5-
liketrees(J48),SVMwithlinearkernel(svm),Multi-layerPercep-
tron for neural network (mlp), and Random Forest (rf).
Figure6:Themachinelearningpipelineimplemented,withtheHold-outandLeave-one-subject-outevaluationsettings.
We evaluate our classifiers in two different settings. In the Hold-
outsetting,wesplitthegoldstandardintotrain(90%)andtest(10%)
672sets using the stratified sampling strategy implemented in the R
caretpackage[ 31].Wesearchfortheoptimalhyper-parameters[ 55,
56] using leave-one-out cross validation—i.e., the recommended
approach for small training sets [ 46], such as ours. The resulting
modelisthenevaluatedonthehold-outtestsettoassessitsperfor-
manceonunseendata.Werepeatthisprocess10timestofurther
increase the validity of the results. The performance is then evalu-
atedbycomputingthemeanforprecision,recall,F-measure,and
accuracyoverthedifferentruns.Thissettingisdirectlycomparable
totheoneimplementedbyMüllerandFritz[ 42],whichincludes
data from the same subject in both training and test sets.
Wereportasecondevaluationsettingtoassesstheclassifiersper-
formance on data obtained from unseen developers—i.e., leave-one-
subject-out (LOSO). This setting was inspired by previous findings
reporting different classification performance due to differences
in biometrics between individuals [ 42]. In the LOSO setting, the
evaluation on a test set is repeated for 23 times—i.e., the number of
subjects in our dataset. At each iteration, we use all the observa-
tions from the n-1participants (i.e., 22) for training the model, and
we test the performance on the remaining one.
Classification Performance . In Table 6, for each sensor and
theircombination,wereporttheclassifierwiththehighestaccuracy,
together with its precision, recall, and F-measure. Moreover, we
reporttheresultofatrivialclassifieralwayspredictingthemajority
class (i.e., negativefor valence and highfor arousal)7.
In thehold-outsetting, we observe substantial improvements
overthebaselineclassifier.The valenceclassifierdistinguishesbe-
tweennegativeandpositiveemotionswithanaccuracyof.72using
the full set of sensors. While performance might appear close tothe baseline value interms of accuracy (baselineaccuracy =
.68),
lookingatprecisionandrecall,weobservethattheclassifiersbe-
havioris substantiallydifferent. Infact, weobserve an increaseof
.34 in precision (from .34 of the baseline to .68 of the classifier) and
of.10(from.50to.60)inrecall,resultingina .19increaseoftheF1-
measure (from .41 to .60). The results show that the model trained
usingthefullset offeaturesachievescomparableperformanceto
theonetrainedusingonlyfeaturesextractedfromtheEmpaticaE4device—i.e.,EDA,BVP,andHR-relatedfeatures.Weobserveasmallincreaseinprecisionwithrespecttothefulldevicesetting(from
.68
inthefullsetto .70withEmpatica)andasmalldecreaseinrecall
(from.60 to.59). These results suggest that valence can be reliably
detected using only the Empatica E4 wristband. The features from
the EEG negligibly impact the classifiers performance.
Forarousal,ourbestclassifierdistinguishesbetween highand
lowemotion activation with an accuracy of .65. The model trained
using the full set of features substantially outperforms the baseline
intermsofprecision( +.31,from .31to.62),recall( +.11,from .50
to.61), and F1-measure ( +.21, from .38 to.59). Similarly to what
observed for valence, the performance obtained with the full set
of sensors is comparable to the one obtained with the Empatica E4
wristband only, which also achieves a better precision.
TheLOSOsettingresultsaresimilar,forbothvalenceandarousal,
to the ones reported in the hold-outsettings. However, we observe
7The performance for each run in both settings are reported in the ’BestResults’
spreadsheet, in the ’Machine Learning’ folder of the replication package.variabilityfortheindividualperformanceoneachtestsetassug-
gested bythe higher standarddeviation compared tothe hold-out
setting. Although with variability between individuals, our results
provide evidence that biometrics are good predictors for emotions.
Summary RQ3 - Developers’ emotions during program-
ming can be recognized using features extracted by the
Empatica E4 wristband (i.e., EDA, BVP, and HR).
5 DISCUSSION
In this section, we compare our findings with related studies, high-
lighttheirimplicationsforresearchersandpractitioners,andreport
the threats to their validity.
5.1 Comparison with Related Studies
Emotions as a proxy for progress. The analysisof thescores re-
portedbytheparticipantsinourstudyduringtheprogrammingtaskshowsaprevalenceofnegativevalenceandhigharousal.Thisresult
contrastswiththefindingsofMüllerandFritz[ 42]whoobserved
thatthedistributionofemotionsreportedwhenprogrammingis
comparabletotheonereportedwhenwatchingemotion-triggeringpictures.Theprevalenceofemotionwithnegativevalenceandhigharousalinourstudycanbeexplainedbyourparticipantsbeinglessexperienced.Mäntyläetal.[
38]presentevidencethatnovicedevel-
opers are more inclined to negative valence and high arousal. Fur-
thermore,experienceisnegativelycorrelatedwitheffort—i.e.,more
experienced developers need less effort to complete a task [ 32,39].
The lower level of experience of our participants can be seen in
theiractualandperceivedprogress.Themajorityreportedbeing
either stuck or completely stuck (63% of self-report questionnaires
filled-in during the interruptions) and either being in flow or neu-
tral in only 11% and 26% of the cases, respectively. These resultsare consistent with the fact that only four of the 23 participants
completedthetask.Conversely,MüllerandFritz[ 42]reportamore
balanced distribution of progress with the majority of participants
feeling in flow (39%) rather than stuck (37%) or neutral (24%).
Theresultsofthelinearmixedmodelinourstudyarecomparable
to those reported in Müller and Fritz [ 42]. We confirm that valence
is positively correlated with perceived progress and that it is the
mainvariableexplainingthemodeldeviance.Moreover,wedidnotobserveasignificantrelationshipbetweenproductivityandarousal.BothresultsconfirmthefindingsofthepreviousstudybyGraziotin
et al. [19]. The positive relationship between valence and progress
is consistent with the findings of Mäntylä et al. [ 38] who observed
positive emotions when resolving issues in the tracking system
(i.e., emotions as a proxy for progress). The same study shows low
variabilityinarousal,supportingthelackofcorrelationbetween
this emotional dimension and progress observed in our study.
Causesfornegativeemotionsandcopingstrategies .Wecon-
firmpreviousevidenceonthecausesofnegativeemotionsandhow
developersdealwiththemtoregainfocusandpositiveemotions.
Being stuck and working under time pressure emerged as the most
frequent causes fornegative emotions. Fear of failurewas already
reported as a cause for frustration in software development [ 10].
673Table 6: Best valence and arousal classifiers performance. Improvement over the baseline reported in parenthesis.
Hold-out setting
Train: 90% + LOO cross validation
Test: 10% (10 times)Leave-one-subject out setting
Train: all-1 subject + LOO cross validation
Test: 1 held-out subject (23 subjects)
Valence
Devices Alg.Prec Rec F1 Accuracy stdev Alg.Prec Rec F1 Accuracy stdev
Fullsetknn.68 (+.34) .60 (+.10) .60 (+.19) .72 (+.04) .12 svm.48 (+.14) .62 (+.12) .53 (+.12) .69 (+.01) .25
Empatica knn.70 (+.36) .59 (+.09) .59 (+.18) .71 (+.03) .07 svm.45 (+.11) .61 (+.11) .50 (+.09) .68 (–) .27
Brainlink rf.54 (+.20) .54 (+.04) .52 (+.11) .66 (-.02) .07 mlp.66 (+.32) .64 (+.14) .64 (+.23) .71 (+.03) .22
Baseline .34 .50 .41 .68 .34 .50 .41 .68
Arousal
Fullsetrf.62 (+.31) .61 (+.11) .59 (+.21) .65 (+.04) .05 svm.46 (+.15) .59 (+.09) .50 (+.12) .61 (+.05) .25
Empatica knn.67 (+.36) .58 (+.08) .55 (+.17) .65 (+.04) .10 J48.40 (+.09) .59 (+.09) .49 (+.11) .62 (–) .25
Brainlink rf.66 (+.35) .59 (+.09) .58 (+.20) .63 (+.01) .12 nb.62 (+.31) .63 (+.13) .61 (+.23) .63 (+.01) .17
Baseline .31 .50 .38 .62 .31 .50 .38 .62
Similarly,thedetrimentalimpactoflimitedtimeonself-confidence,
well-being, and emotional states was already observed [10, 32].
Technicaldifficulties(e.g.,unexpectedusageoflibrariesorun-
expected output of code) and unfulfilled information needs (e.g.,
unavailable documentation) also emerge as causes for negativefeelings. This is consistent with previous investigations of emo-
tions [10,42] and confusion [ 8] in software development. Previous
findings suggest that early detection of confusion is crucial for pre-
ventingburnoutandlossofproductivity.Moreover,theydemon-
strateshowconfusionarisesduetolackofdocumentation[ 8],in
presence of unexpected code behavior [8], and bugs [38].
Wealsoshowthatfacingnewchallengesisatriggerforpositive
emotions,inlinewithpreviousworkshowingthatthedevelopmentofnewfeaturescausesmorepositiveemotionsthanbugfixing[
38].
Similarly, having new ideas and being in flow while programming
is shown to be associated with positive emotions [42].
A Minimal Set of Biometrics for Emotion Detection. On top
ofconfirmingpreviousfindingsregardingtheusageofnon-invasive
sensors for valence classification reported by Müller and Fritz [ 42],
we also addressed the classification of the arousal dimension. As
a novel finding, we identified the minimum set of sensors—EDA,
BVP, and HR measured using the Empatica E4 wristband—that can
be used in an experimental protocol for detecting emotions during
software development tasks.
Using machine learning, we are able to distinguish between pos-
itive and negative valence. Using only the Empatica E4 wristband,
theperformanceiscomparabletotheoneobtainedusingthefull
sensors settings (i.e., wristband + EEG helmet). Our accuracy ( .72)
iscomparabletotheone( .71)reportedbyMüellerandFritz[ 42].
However, their results are obtained using features from an EEG in
combination with HR, and pupil size captured by an eye-tracker.
Forarousalclassification,ourbestclassifierachievesanaccuracy
of.65 using only features from the Empatica E4 wristband. Our
accuracy is comparable to the one ( .58) reported by Koelstra et
al.[29]forarousalclassificationusinga32-electrodeEEGhelmet.
Theyshowanaccuracyof .61forvalencebycombiningEDA,EMG
on facial muscles, features derived from respiration, blood pres-
sure, and eye blinking rate. We outperform their classifiers using a
minimal set of features obtained using the Empatica E4 wristband.Compared to ours, other studies show better performance—e.g.,
accuracy of .97 for arousal [ 6,14,54] and.91 for valence [ 44].
However, these studies rely on high-definition EEG helmets [ 6,14,
54]andfacialelectrodesforEMG[ 44].Suchsensorsareinvasiveand
cannot be used outside laboratory settings—e.g., on the workplace.
5.2 Implications
Implicationforresearchers Theresultsofthisstudyprovidesevi-
dencethatwecandetectdevelopers’emotions,whileprogramming,
bymeansofaminimalsetofbiometricfeaturesusingthesensors
mounted on a single wearable device (the Empatica E4 wristband).
This opens up the possibility of further studies aimed at improving
the ecological validity of our findings.
Ourresultsshowbetween-subjectvariabilityofbiometrics,al-
ready observed in previous studies [ 42]. The higher standard devi-
ation for accuracy in the LOSO setting can be problematic when
classifying the emotionsof a new unseen developer. Further stud-
ies with a wider pool of participants are required to assess the
robustness of our valence and arousal classifiers. Such studies can
investigatetowhatextentwecanbuildmorerobustclassifiersby
performing preliminary subject-based calibration—e.g., by tuning
the models based on individuals’ biometrics collected while expos-
ing participants to emotional stimuli in a controlled setting. Future
studies can identify the amount of biometric data required for fine-
tuning the models for reliably classify new subjects emotions.
Thecorrelationbetweenvalenceandprogresscanbeinterpreted
asaproxyforself-perceivedpr oductivity.However, furtherinvesti-
gationisrequiredto (i)provideanexplanationfortheresultsof
ourcorrelationstudy,and (ii)understandwhetheracausalrelation
existsbetweenemotionsandproductivity(orvice-versa)—e.g.,us-
ingemotion-triggeringtechniquesinacontrolledsetting,inline
with previous research [27].
Implication for practitioners and tool builders . We show
that emotions can be detected using non-invasive wearable device,
such as a wristband. This finding paves the way for tools and
practices to prevent developers’ distress and burnout.
Early detection of negative emotions, integrated with the de-
velopment environment, can be leveraged to suggest corrective
674actions. At an individual level, developers can regain focus and re-
storepositivemoodsinaccordancewiththestrategiesweobserved
in this study to cope with negative emotions. In this perspective,
detection of negative emotions, while coding, can be used as a
promptforrecommendersystemssuggestingbreakstopreventbug
introduction, the need for code reviews or pair programming, or
links to Stack Overflow or curated documentation.
Atteamlevel,companiesarerecentlyimplementingstrategies
tosupportemotionawareness[ 45].Forexample,duringagileret-
rospective meetings at SkyTV, teams self report on a shared board
the emotions experienced during the sprint, and discuss their trig-
gers.Biometricscan enhanceretrospectivemeetingsbyincluding
emotional information collected day-to-day rather than at the end
of a sprint. The team can better identify what are the activities and
events relating to positive and negative emotions [16].
Our resultsdemonstrate a positive correlationbetween valence
and progress, suggesting that emotions might act as a proxy forproductivity. For example, positive emotions can indicate that adeveloper is in flowand should not be disturbed. Hence, sensor-
based emotionclassifiers canimprove state-of-the-artapproaches
for the automatic assessment of interruptibility [ 59]. Similarly, the
identificationofnegativeemotionscanindicateastuckdeveloper
requiring support in fulfilling her information needs. Accordingly,
an emotion-aware component integrated in the development en-
vironment can recommend relevant colleagues to consult, based
ontheir familiaritywith thecode base[ 26],or triggerutilities for
on-demand documentation generation [48].
5.3 Threats to Validity
Inthissection,wereportthethreatstothevalidityinincreasing
order of priority for the in vitronature of this study, following the
recommendations of Wohlin et al. [58].
Externalvalidity Threats to external validity relate to the gen-
eralizabilityoftheresults.Wechosethesametaskusedinaformer
study[42],whichsimulatedanewprobleminarealscenario.Re-
gardingparticipants,wecovereddifferentlevelsofacademicexperi-
ence (by including Bachelors, Masters, and PhD students) but withless professional experience compared to [
42]. Further replications
could engage more women to account for the differences in the
emotional reaction based on gender differences.
Conclusion validity Thevalidityofourconclusionsrelieson
the robustness of the generalized linear model and machine learn-
ingmodels.Wemitigatedsuchthreatby (i)runningseveralalgo-
rithms addressing the same classification task, (ii)applying hyper-
parameters tuning, and (iii)reporting results from two different
evaluation settings—i.e., Hold-out and LOSO.
Construct validity Our study suffers from threats to construct
validity—i.e., the reliability of our measures in capturing emotions
andprogress.Whenassessingtheimpactofarousalonprogress,we
didnotobserveasignificantcorrelation.Althoughwecannotex-
cludethatsuchresultisduetotheunreliabilityoftheself-reported
rating,weperformeddataqualityassuranceanddidnotconsider
participants who misinterpreted the concept ofarousal—e.g., whoreported always the same score also during emotion elicitation.
Internalvalidity
Threatstointernalvalidityconcernconfound-
ing factors that can influence the results. We collected data in alaboratorysetting.Factorsexistinginoursettings,suchasthepres-
ence of the experimenter and the absence of real consequences
when failing or succeeding in the task, can influence the triggered
emotions—e.g., negative emotions due to the feeling of being ob-
servedorjudged.Inaddition,interruptingdevelopersduringthe
task can haveinterfered with their work and elicited negativeemo-
tions. We mitigate this threat by interrupting the developers when
wenoticeataskswitch(e.g.,openinganewbrowserwindow)in
proximity of the five-minutes interval. Finally, we are aware that a
further threat might be due to the choice of the programming task
and that we could have observed different results with an easier or
more difficult test. In line with our long-term goal of supporting
emotion awareness and well being of software developers, the test
should have been difficult enough, albeit feasible, to elicit negative
emotionsthatarethemostinterestingtodetect.Tohavereasonable
confidenceinfeasibility,weselectedonlystudentsthatcouldpro-
videevidencetheyclearedexamswithanadequategrade,where
Java was used for capstone projects. As a further validation of the
feasibility of the task, we run a pilot study.
6 CONCLUSION
Weinvestigatedtherangeandtriggersofemotionsthatsoftware
developers experience during a programming task. We confirm
thelinkbetweenemotionsandself-reportedprogressobservedinprevious indipendent studies.
UsingEDAandheart-relatedmetricscollectedusingawristband,
we trained a machine learning classifier that can accurately detect
valence. A second classifier, trained to detect arousal, has shown
less successful but encouraging results.
Ourresultscanstimulatefuture in-vivoresearch(i.e.,insoftware
developmentcompanies)andthecollectionofbiometricstoexplore
emotionsduringtheentireworkingday,whendevelopersarein-
volvedindifferentactivities,notjustprogramming.Furthermore,
observing developers at the workplace also opens opportunitiestobuildmoresophisticatedclassifiers,whichareabletoidentify,
for example, the bad days (i.e., when mostly negative emotions are
identified)ornegativeworkingconditionsofdevelopers(i.e.,when
negative emotions are observed over a long period of time).
ACKNOWLEDGMENTS
We would like to thank Daniel Graziotin, Thomas Fritz, and Sebas-
tian Mueller for kindly providing access to the replication material
of their studies, which we use to define our experimental protocol.
We are grateful to all subjects that participated in our study. We
acknowledge that this work was partially supported by the KKS
foundationthroughtheS.E.R.T.ResearchProfileprojectatBlekinge
InstituteofTechnology,bytheItalianMinistryofUniversityand
Research (MIUR) under grant PRIN 2017 "EMPATHY: EMpowering
People in deAling with internet of THings ecosYstems" (project
H94I19000280001), and by the projects "E-SHELF" and "C3", under
the program INNONETWORK, POR Puglia FESR- FSE 2014-2020.
675REFERENCES
[1]ValerioBasile,NicoleNovielli,DaniloCroce,FrancescoBarbieri,MalvinaNissim,
and Viviana Patti. 2018. Sentiment Polarity Classification at EVALITA: Lessons
Learned and Open Challenges. IEEE Transactions on Affective Computing (2018).
https://doi.org/10.1109/TAFFC.2018.2884015
[2]Margaret M. Bradley and Peter J. Lang. 1994. Measuring emotion: The self-
assessmentmanikinandthesemanticdifferential. JournalofBehaviorTherapy&
ExperimentalPsychiatry 25,1(1994),49–59. https://doi.org/10.1016/0005-7916(94)
90063-9
[3]JasonJ.Braithwaite,DerrickG.Watson,RobertJones,andMickeyRowe.2015. A
Guide for Analysing Electrodermal Activity (EDA) & Skin Conductance Responses
(SCRs)forPsychologicalExperiments.TechnicalReport.UniversityofBirmingham,
UK, University of Birmingham, UK.
[4]Winslow Burleson and Rosalind W. Picard. 2004. Affective agents: Sustaining
motivation to learn through failure and state of "stuck". In Social and Emotional
Intelligence in Learning Environments Workshop. In conjunction with the 7th Inter-
national Conference on Intelligent Tutoring Systems, Maceio.
[5]Filipe Canento, Ana Fred, Hugo Silva, Hugo Gamboa, and André Lourenço. 2011.
Multimodal biosignal sensor data handling for emotion recognition. In SENSORS.
IEEE, 647–650. https://doi.org/10.1109/ICSENS.2011.6127029
[6]Mo Chen, Junwei Han, Lei Guo, Jiahui Wang, and Ioannis Patras. 2015. Iden-
tifying Valence and Arousal Levels via Connectivity between EEG Channels.InProceedingsofthe2015InternationalConferenceonAffectiveComputingand
Intelligent Interaction (ACII) (ACII ’15). IEEE Computer Society, USA, 63–69.
https://doi.org/10.1109/ACII.2015.7344552
[7]Peter J. Denning. 2012. Moods. Commun. ACM 55, 12 (Dec. 2012), 33–35. https:
//doi.org/10.1145/2380656.2380668
[8]FelipeEbert,FernandoCastor,NicoleNovielli,andAlexanderSerebrenik.2019.
Confusion in Code Reviews: Reasons, Impacts, and Coping Strategies. In 26th
IEEE International Conference on Software Analysis, Evolution and Reengineering,
SANER 2019, Hangzhou, China, February 24-27, 2019. IEEE, 49–60. https://doi.
org/10.1109/SANER.2019.8668024
[9]Paul Ekman. 1999. Basic Emotions. Handbook of Cognition and Emotion. John
Wiley and Sons Ltd.
[10]DenaeFordandChrisParnin.2015. ExploringCausesofFrustrationforSoftware
Developers. In 8th IEEE/ACM International Workshop on Cooperative and Human
AspectsofSoftwareEngineering,CHASE2015,Florence,Italy,May18,2015.115–116.
https://doi.org/10.1109/CHASE.2015.19
[11]ThomasFritz,AndrewBegel,SebastianC.Müller,SerapYigit-Elliott,andManuela
Züger. 2014. Using psycho-physiological measures to assess task difficulty in
software development. In 36th International Conference on Software Engineering,
ICSE’14,Hyderabad,India-May31-7June,2014.402–413. https://doi.org/10.
1145/2568225.2568266
[12]Davide Fucci, Daniela Girardi, Nicole Novielli, Luigi Quaranta, and Filippo Lanu-
bile. 2019. A replication study on code comprehension and expertise using
lightweight biometric sensors. In Proceedings of the 27th International Conference
onProgramComprehension,ICPC2019,Montreal,QC,Canada,May25-31,2019 .
311–322. https://dl.acm.org/citation.cfm?id=3339126
[13]Daviti Gachechiladze, Filippo Lanubile, Nicole Novielli, and Alexander Sere-
brenik.2017. AngerandItsDirectioninCollaborativeSoftwareDevelopment.In
39th IEEE/ACM International Conference on Software Engineering: New Ideas and
Emerging Technologies Results Track, ICSE-NIER 2017, Buenos Aires, Argentina,
May 20-28, 2017. 11–14. https://doi.org/10.1109/ICSE-NIER.2017.18
[14]HernánF.García,MauricioA.Álvarez,andÁlvaroÁ.Orozco.2016. Gaussianprocess dynamical models for multimodal affect recognition. In 38th Annual
InternationalConference ofthe IEEEEngineeringin Medicineand BiologySociety,
EMBC2016,Orlando,FL,USA,August16-20,2016.850–853. https://doi.org/10.
1109/EMBC.2016.7590834
[15]DanielaGirardi,FilippoLanubile,andNicoleNovielli.2017. Emotiondetection
usingnoninvasivelowcostsensors.In SeventhInternationalConferenceonAf-
fective Computing and Intelligent Interaction, ACII 2017, San Antonio, TX, USA,
October 23-26, 2017. 125–130. https://doi.org/10.1109/ACII.2017.8273589
[16]DanielaGirardi,FilippoLanubile,NicoleNovielli,LuigiQuaranta,andAlexanderSerebrenik.2019.TowardsRecognizingtheEmotionsofDevelopersUsingBiomet-rics:TheDesignofaFieldStudy.In Proceedingsofthe4thInternationalWorkshop
onEmotionAwarenessinSoftwareEngineering,SEmotion@ICSE2019,Montreal,
QC,Canada,May 28,2019.13–16. https://doi.org/10.1109/SEmotion.2019.00010
[17]DanielGraziotin,FabianFagerholm,XiaofengWang,andPekkaAbrahamsson.
2018. Whathappenswhensoftwaredevelopersare(un)happy. JournalofSystems
and Software 140 (2018), 32–47. https://doi.org/10.1016/j.jss.2018.02.041
[18]DanielGraziotin,XiaofengWang,andPekkaAbrahamsson.2014. Happysoft-
waredeveloperssolveproblemsbetter:psychologicalmeasurementsinempirical
software engineering. PeerJ(2014). https://doi.org/10.7717/peerj.289
[19]DanielGraziotin,XiaofengWang,andPekkaAbrahamsson.2015. Dofeelings
matter?Onthecorrelationofaffectsandtheself-assessedproductivityinsoftware
engineering. Journal of Software: Evolution and Process 27, 7 (2015), 467–487.
https://doi.org/10.1002/smr.1673[20]AlbertoGreco,GaetanoValenza,AntonioLanata,EnzoPasqualeScilingo,and
Luca Citi. 2016. cvxEDA: A Convex Optimization Approach to Electrodermal
Activity Processing. IEEE Transactions on Biomedical Engineering 63, 4 (April
2016), 797–804. https://doi.org/10.1109/TBME.2015.2474131
[21]Ralitza Gueorguieva and Jhon H. Krystal. 2004. Move over ANOVA: progressin analyzing repeated-measures data and its reflection in papers published in
theArchivesofGeneralPsychiatry. Archivesofgeneralpsychiatry 61,3(2004),
310–317. https://doi.org/10.1001/archpsyc.61.3.310
[22]MarcoGueriniandJacopoStaiano.2015. DeepFeelings:AMassiveCross-Lingual
Study on the Relation between Emotions and Virality. In Proceedings of the 24th
InternationalConferenceonWorldWideWebCompanion,WWW2015,Florence,
Italy, May 18-22, 2015 - Companion Volume. 299–305. https://doi.org/10.1145/
2740908.2743058
[23]Emitza Guzman and Bernd Bruegge. 2013. Towards Emotional Awareness in
SoftwareDevelopmentTeams.In JointMeetingoftheEuropeanSoftwareEngineer-
ing Conference and the ACM SIGSOFT Symposium on the Foundations of Software
Engineering,ESEC/FSE’13,SaintPetersburg,RussianFederation,August18-26,2013 .
671–674. https://doi.org/10.1145/2491411.2494578
[24]MdRakibulIslamandMinhazF.Zibran.2018. DEVA:SensingEmotionsinthe
Valence ArousalSpace inSoftware EngineeringText.In Proceedingsof the33rd
AnnualACMSymposiumonAppliedComputing,SAC2018,Pau,France,April09-13,
2018. ACM, New York, NY, USA, 1536–1543. https://doi.org/10.1145/3167132.
3167296
[25]Ashish Kapoor, Winslow Burleson, and Rosalind W. Picard. 2007. Automatic
PredictionofFrustration. InternationalJournalHuman-ComputerStudies 65,8
(Aug. 2007), 724–736. https://doi.org/10.1016/j.ijhcs.2007.02.003
[26]DavidKavaler,PremkumarT.Devanbu,andVladimirFilkov.2019.WhomAreYou
GoingtoCall?:Determinantsof@-MentionsinGitHubDiscussions. Empirical
Software Engineering (2019), 1–29. https://doi.org/10.1007/s10664-019-09728-3
[27]Iftikhar Ahmed Khan, Willem-Paul Brinkman, and Robert M. Hierons. 2011. Do
moods affect programmers’ debug performance? Cognition, Technology & Work
13, 4 (2011), 245–258. https://doi.org/10.1007/s10111-010-0164-1
[28]Jonghwa Kim and Elisabeth André. 2008. Emotion Recognition Based on Physio-
logicalChangesinMusicListening. IEEETransactionsonPatternAnalysisandMa-
chineIntelligence 30,12(2008),2067–2083. https://doi.org/10.1109/TPAMI.2008.26
[29]Sander Koelstra, Christian Mühl, Mohammad Soleymani, Jong-Seok Lee, Ashkan
Yazdani, Touradj Ebrahimi, Thierry Pun, Anton Nijholt, and Ioannis Patras. 2012.
DEAP: A Database for Emotion Analysis Using Physiological Signals. IEEE
TransactiononAffectiveComputing 3,1(2012),18–31. https://doi.org/10.1109/
T-AFFC.2011.15
[30]ArthurE.Kramer.1990. PhysiologicalMetricsofMentalWorkload:AReviewof
Recent Progress. https://doi.org/10.21236/ada223701
[31] Max Kuhn. 2009. The caret Package. http://topepo.github.io/caret/index.html.[32]
Miikka Kuutila, Mika Mäntylä, Umar Farooq, and Maëlick Claes. 2019. TimePressure in Software Engineering: A Systematic Literature Review. CoRR
abs/1901.05771 (2019). arXiv:1901.05771 http://arxiv.org/abs/1901.05771
[33]Peter J. Lang and Margaret Bradley. 2007. The International Affective Picture
System(IAPS) intheStudy ofEmotionand Attention. In Handbookof Emotion
Elicitation and Attention, James A. Coan and Jhon J. B. Allen (Eds.). Oxford
University Press, Chapter 2, 29–46.
[34]Reed Larson and Mihaly Csikszentmihalyi. 2014. The Experience Sampling
Method. Springer Netherlands, Dordrecht, 21–34. https://doi.org/10.1007/
978-94-017-9088-8_2
[35]RichardS.Lazarus.1991. EmotionandAdaptation. OxfordUniversityPressUSA.
[36]Xu Lei and Keren Liao. 2017. Understanding the Influences of EEG Reference:
ALarge-Scale BrainNetworkPerspective. FrontNeurosci. 11,205 (2017). https:
//doi.org/10.3389/fnins.2017.00205
[37]Mu Li and Bao-Liang Lu. 2009. Emotion classification based on gamma-band
EEG. In2009 Annual International Conference of the IEEE Engineering in Medicine
and Biology Society. 1223–1226. https://doi.org/10.1109/IEMBS.2009.5334139
[38]MikaMäntylä, BramAdams,GiuseppeDestefanis, DanielGraziotin,andMarco
Ortu.2016. Miningvalence,arousal,anddominance:possibilitiesfordetecting
burnout and productivity?. In Proceedings of the 13th International Conference
on Mining Software Repositories, MSR 2016, Austin, TX, USA, May 14-22, 2016.
247–258. https://doi.org/10.1145/2901739.2901752
[39]MikaMäntylä,KaiPetersen,TimoO.A.Lehtinen,andCasperLassenius.2014.
Time Pressure: A Controlled Experiment of Test Case Development and Require-
ments Review. In 36th International Conference on Software Engineering, ICSE
’14, Hyderabad, India - May 31 - June 07, 2014. 83–94. https://doi.org/10.1145/
2568225.2568245
[40]PatriciaY.MartinandBarryA.Turner.1986. GroundedTheoryandOrganiza-
tional Research. The Journal ofApplied Behavioral Science 22, 2 (1986),141–157.
https://doi.org/10.1177/002188638602200207
[41]AndréN.Meyer,GailC.Murphy,ThomasFritz,andThomasZimmermann.2014.
Software Developers’ Perceptions of Productivity. In Proceedings of the 22nd
ACMSIGSOFT InternationalSymposiumonFoundations ofSoftwareEngineering,
(FSE-22),HongKong,China,November16-22,2014.ACM,NewYork,NY,USA,
19–29. https://doi.org/10.1145/2635868.2635892
676[42]Sebastian C. Müller and Thomas Fritz. 2015. Stuck and Frustrated or in Flow
and Happy: Sensing Developers’ Emotions and Progress. In 37th IEEE/ACM
International Conference on Software Engineering, ICSE 2015, Florence, Italy, May
16-24, 2015. 688–699. https://doi.org/10.1109/ICSE.2015.334
[43]SebastianC.MüllerandThomasFritz.2016. Using(bio)metricstopredictcode
quality online. In Proceedings of the 38th International Conference on Software
Engineering,ICSE2016,Austin,TX,USA,May14-22,2016.452–463. https://doi.
org/10.1145/2884781.2884803
[44]Pedro A. Nogueira, Rui A. Rodrigues, Eugénio C. Oliveira, and Lennart E. Nacke.
2013.AHybridApproachatEmotionalStateDetection:MergingTheoreticalMod-
elsofEmotionwithData-DrivenStatisticalClassifiers.In 2013IEEE/WIC/ACM
International Conferences on Intelligent Agent Technology, IAT 2013, 17-20 Novem-
ber 2013, Atlanta, Georgia, USA. IEEE, 253–260. https://doi.org/10.1109/WI-IAT.
2013.117
[45]N.NovielliandA.Serebrenik.2019. SentimentandEmotioninSoftwareEngi-
neering.IEEE Software 36, 5 (Sep. 2019), 6–23. https://doi.org/10.1109/MS.2019.
2924013
[46]Sebastian Raschka. 2018. Model Evaluation, Model Selection, and Algorithm
SelectioninMachineLearning. CoRRabs/1811.12808(2018). arXiv:1811.12808
http://arxiv.org/abs/1811.12808
[47] BorisReuderink, ChristianMühl,andMannesPoel.2013. Valence,Arousaland
Dominance in the EEG During Game Play. International Journal of Autonomous
and Adaptive Communications Systems 6, 1 (2013), 45–62. https://doi.org/10.
1504/IJAACS.2013.050691
[48]Martin P. Robillard, Andrian Marcus, Christoph Treude, Gabriele Bavota, Oscar
Chaparro, Neil A. Ernst, Marco Aurélio Gerosa, Michael W. Godfrey, Michele
Lanza,MarioLinaresVásquez,GailC.Murphy,LauraMoreno,DavidC.Shepherd,
andEdmundWong.2017. On-demand DeveloperDocumentation.In 2017IEEE
International Conference on Software Maintenance and Evolution, ICSME 2017,
Shanghai, China, September 17-22, 2017. 479–483. https://doi.org/10.1109/ICSME.
2017.17
[49]Jonathan Rottemberg, Rebecca D. Ray, and James J. Gross. 2007. Emotion Elicita-
tion Using Films. In Handbook of Emotion Elicitation and Assesment, James Coan
and Jhon J.B. Allen (Eds.). Oxford University Press, Chapter 1, 9–28.
[50]JamesRussell.1980. ACircumplexModelofAffect. JournalofPersonalityand
Social Psychology 39 (1980), 1161–1178. https://doi.org/10.1037/h0077714[51]JamesRussell.1991. Cultureandthecategorizationofemotions. Psychological
Bulletin110 (3) (1991), 426–450. https://doi.org/10.1037/0033-2909.110.3.426
[52]Jocelyn Scheirer, Raul Fernandez, Jonathan Klein, and Rosalind W. Picard. 2002.
Frustrating the user on purpose: a step toward building an affective com-puter.Interacting with Computers 14 (2002), 93–118. https://doi.org/10.1016/
S0953-5438(01)00059-5
[53]Mohammad Soleymani, Sadjad Asghari-Esfeden, Yun Fu, and Maja Pantic.2016. Analysis of EEG Signals and Facial Expressions for Continuous Emo-tion Detection. IEEE Transaction on Affective Computing 7, 1 (2016), 17–28.
https://doi.org/10.1109/TAFFC.2015.2436926
[54]MohammadSoleymani,MajaPantic,andThierryPun.2015. Multimodalemotion
recognition in response to videos (Extended abstract). In 2015 International Con-
ference on Affective Computing and Intelligent Interaction, ACII 2015, Xi’an, China,
September 21-24, 2015. 491–497. https://doi.org/10.1109/ACII.2015.7344615
[55]Chakkrit Tantithamthavorn, Shane McIntosh, Ahmed E. Hassan, and Kenichi
Matsumoto.2016. Automatedparameteroptimizationofclassificationtechniques
fordefectpredictionmodels.In Proceedingsofthe38thInternationalConference
onSoftwareEngineering,ICSE2016,Austin,TX,USA,May14-22,2016.321–332.
https://doi.org/10.1145/2884781.2884857
[56]Chakkrit Tantithamthavorn, Shane McIntosh, Ahmed E. Hassan, and Kenichi
Matsumoto.2019. Theimpactofautomatedparameteroptimizationondefect
predictionmodels. IEEETransactionsonSoftwareEngineering 45,7(July2019),
683–711. https://doi.org/10.1109/TSE.2018.2794977
[57]AnthonyVieraandJoanneGarrett.2005. UnderstandingInterobserverAgree-
ment: The Kappa Statistic. Family medicine 37, 5 (2005), 360–363.
[58]Claes Wohlin, Per Runeson, Martin Höst, Magnus C. Ohlsson, and Björn Regnell.
2012.Experimentation in software engineering. Springer Science & Business
Media. https://doi.org/10.1007/978-3-642-29044-2
[59]Manuela Züger, Sebastian C. Müller, André N. Meyer, and Thomas Fritz. 2018.Sensing Interruptibility in the Office: A Field Study on the Use of Biometric
and Computer Interaction Sensors. In Proceedings of the 2018 CHI Conference on
HumanFactorsinComputingSystems,(CHI2018).591. https://doi.org/10.1145/
3173574.3174165
677