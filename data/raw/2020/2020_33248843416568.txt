Learning to Handle Exceptions
Jian Zhangâˆ—
SKLSDE Lab, Beihang University,
China
zhangj@act.buaa.edu.cnXu Wangâˆ—â€ 
SKLSDE Lab, Beihang University,
China
wangxu@act.buaa.edu.cnHongyu Zhang
The University of Newcastle,
Australia
hongyu.zhang@newcastle.edu.au
Hailong Sunâˆ—
SKLSDE Lab, Beihang University,
China
sunhl@act.buaa.edu.cnYanjun Pu
SKLSDE Lab, Beihang University,
China
puyanjun@nlsde.buaa.edu.cnXudong Liuâˆ—
SKLSDE Lab, Beihang University,
China
liuxd@act.buaa.edu.cn
ABSTRACT
Exception handlingis an importantbuilt-in featureof many mod-
ernprogramminglanguagessuchasJava.Itallowsdevelopersto
deal with abnormal or unexpected conditions that may occur at
runtime in advance by using try-catch blocks. Missing or improper
implementation of exception handling can cause catastrophic con-
sequences such as system crash.However, previous studies reveal
that developers are unwilling or feel it hard to adopt exception
handling mechanism, and tend to ignore it until a system failure
forces themto doso. To helpdevelopers withexception handling,
existingworkproducesrecommendationssuchascodeexamples
and exception types, which still requires developers to localize the
tryblocksandmodifythecatchblockcodetofitthecontext.Inthispaper,weproposeanovelneuralapproachtoautomatedexception
handling,whichcanpredictlocationsoftryblocksandautomati-
callygeneratethecompletecatchblocks.Wecollectalargenumberof Java methods from GitHub and conduct experiments to evaluate
ourapproach.Theevaluationresults,includingquantitativemea-
surement and human evaluation, show that our approach is highly
effectiveandoutperformsallbaselines.Ourworkmakesonestep
further towards automated exception handling.
CCS CONCEPTS
â€¢Software and its engineering â†’Error handling and recov-
ery;Automatic programming.
KEYWORDS
Exceptionhandling,deeplearning,neuralnetwork,codegeneration
âˆ—AlsowithBeijingAdvancedInnovationCenterforBigDataandBrainComputing,
Beihang University, Beijing 100191, China.
â€ Corresponding author: Xu Wang, wangxu@act.buaa.edu.cn.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia
Â© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-6768-4/20/09...$15.00
https://doi.org/10.1145/3324884.3416568ACM Reference Format:
JianZhang,XuWang,HongyuZhang,HailongSun,YanjunPu,andXudong
Liu.2020.Learning toHandleExceptions.In 35thIEEE/ACM International
ConferenceonAutomatedSoftwareEngineering(ASEâ€™20),September21â€“25,
2020, Virtual Event, Australia. ACM, New York, NY, USA, 13 pages. https:
//doi.org/10.1145/3324884.3416568
1 INTRODUCTION
Exceptionhandlingmechanismisessentialformodernprogram-
ming languages such as Java and C# to build robust and reliable
software systems [ 43,58]. It provides an effective means of dealing
withexceptionalconditionsandrecoveringfromthembyusingtry-
catch blocks [ 22,25]. Exception handling separates error-handling
code from regularsource code and improves programcomprehen-
sion and maintenance [ 15]. However, missing or improperly using
exception handling statements may cause severe problems such as
systemcrash[ 67]andinformationleakage[ 68].Onerecentstudy
[19]revealsthatthereisasignificantrelationshipbetweenexcep-
tionflowcharacteristicsandpost-releasedefectsinJavaprojects.
Therefore, it is crucial for developers to handle exceptions.
Despitetheimportanceofexceptionhandling,theexceptionhan-
dlingstatementssuchastry-catchblocksinreal-worldsoftwareare
often poorly written and error-prone. For example, prior studies
[10,18,21]indicatethatmanyindustrialsystemsexhibitpoorqual-
ity code with respect to exception handling. Besides, similar bad
practices have been found in open source software [ 4,46]. Among
our collected millions of original Java methods in 2,000 projects
fromGitHubwithhighnumbersofstarsandforks,only14.9%ofthe
methods apply try blocks to capture exceptions, and 31.2% of these
methodsdonothavecatchblocksanddonothingwhenexceptions
occur. The reason is twofold. On the one hand, developers tend
to ignore exception handling code or even have little knowledge
aboutwhetherornotexceptionhandlingisneededuntilanerror
occurs [56,57]. On the other hand, the exception handling code is
often difficult to write for developers, especially when it comes to
program evolution [ 53]. Hence, as suggested in [ 19], it is essential
toproposeautomatictechniquesforassistingdevelopersinwriting
high-quality exception handling code.
Existingworkonautomatictechniquesforexceptionhandling
mainly includes violation detection of exception handling policies
[6,7,58]andexceptionhandlingcoderecommendation[ 45,51].For
example,Thummalapentaetal.[ 58]minedassociationrulesonthe
sequenceoffunctioncallsintryandcatchblocksandappliedthem
292020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)
todetectviolationsofexceptionhandlingpolicies.Suchapproaches
are helpful to improve the quality of exception handling code, yet
theyarelimitedsincetheyassumethatthetry-catchblockshave
beencompletelywrittenbydevelopers.Bycontrast,exceptionhan-
dlingcoderecommendationismoreusefultoassistdevelopersin
writing the try-catch blocks. Given an under-development code
fragment without try-catch blocks, Rahman et al. [ 51] retrieved
similarcodefragmentsthatincludetry-catchblockstorecommend
exception handlingcode examplesfrom popularcode repositories
at GitHub. Nguyen et al. [ 45] recommended exception types and
methodcallsinthecatchblockbyutilizingthefuzzysettheory[ 35]
andN-grammodel[ 27].However,therearetwomaindrawbacks
of recommendation-based approaches. First, these studies assume
that developers understand when to seek for help about writing
exception handling code and where the try-catch blocks should be
put,whileinpracticeitisnotalwaysthecase[ 56,57];Second,even
though the recommended code such as examples, exception types
andmethodcallscanprovideusefulknowledgetoaiddevelopers
in writing try-catch blocks, the developers still need to modify the
code to fit the context.
Inthispaper,weproposeanovel Neuralapproachtoautomated
exceptionhandlingcode generation(namely Nexgen),whichcan
predict locations of try blocks and automatically generate the com-
plete catch blocks. We separateexception handling into two tasks:
one is to predict the try block locations for identifying source code
that needs to handle potential exceptions; the other is to generate
complete catch blocks to deal with the exceptions. Consider a typi-
calcodingscenariothatadeveloperhaswrittenacodefragment
asshownin1(a),whichisaJavamethodfromtheElasticsearch1
project.Hereweremoveitsoriginalexceptionhandlingcodefor
illustration purpose. The first task is to identify which statements
need to be put in a try block. In this example, the statement at line
8 in Figure 1(a) requires a try block. The second task is to gener-
ate the catch block. In this example, according to the code above
line 11 in Figure 1(b), the catch block in lines 11-13 is generated to
handletheexception.Inthisway,thepotentialexceptionscanbe
automatically captured and handled.
Ourapproachutilizesdeeplearningtolearnregularities/patterns
fromalargeamountofhistoricalexceptionhandlingcode.More
specifically, we tackle the two tasks as follows:
â€¢Try block localization . We design a locator that jointly
learnstodeterminewhetherornotagivencodefragment
needs to handle exceptions and localize where to put the try
block if it does. Considering the sequential naturalness [ 27]
ofsourcecode,wetransformthelocalizationprobleminto
thesequencetaggingproblem[ 29].Toavoidthelong-term
dependencyproblem[ 9],weregardthecodefragmentasa
sequence of statements and hierarchically encode them into
vectors so as to predict the label sequence. We first use long
short-term memory (LSTM) [ 28] to encode statements into
statementvectorsandthenuseanotherLSTMtocapturethe
sequentialdependencyofthestatementsequence.Besides,
we apply hierarchical attention mechanism [ 65] on top of
the twoLSTMs to letthe model pay attention to individual
tokens and statements.
1https://github.com/elastic/elasticsearchpublic char[] decrypt( char[] chars) {
if(!isEncrypted(chars)) {
returnchars;
}
String encrypted = new String(chars, ENCRYPTED_TEXT_PREFIX. length(),
chars.length - ENCRYPTED_TEXT_PREFIX. length());
byte[] bytes;bytes = Base64.getDecoder(). decode(encrypted);
byte[] decrypted = decryptInternal(bytes, encryptionKey);
returnCharArrays. utf8BytesToChars(decrypted);
}






	


(a) A code fragment without exception handling code
public char[] decrypt( char[] chars) {
if(!isEncrypted(chars)) {
returnchars;
}
String encrypted = new String(chars, ENCRYPTED_TEXT_PREFIX .length(),
chars.length - ENCRYPTED_TEXT_PREFIX. length());
byte[] bytes;try{
bytes = Base64.getDecoder(). decode(encrypted);
} 
catch(IllegalArgumentException e) {
throw new ElasticsearchException( "unable to decode encrypted data" , e);
}
byte[] decrypted = decryptInternal(bytes, encryptionKey);
returnCharArrays. utf8BytesToChars(decrypted);
}





	







(b) The code fragment after adding exception handling code
Figure 1: An example of exception handling
â€¢Catch block generation . Different from previous work
such as [ 45], we take all the code before the catch blocks
as the context instead of simply relying on the try blocks.
Thusweconsiderthedependencies(e.g.,howthevariable
â€œencryptedâ€ is initialized) for writing the exception handling
code. A simple way is to treat all the tokens before line
11 (including line 5) as a single sequence, then apply the
encoder-decoderarchitecture[ 5]togeneratethetokensof
thecatchblockinlines11-13onebyone.However,sucha
model will not distinguish whether the tokens are inside or
outsidethetryblock.Therefore,weproposetoencodethem
separately by two encoders, and fuse the context vectors
using the learned weights. Furthermore, it is obvious that
not all the tokens before the try blocks are helpful and may
be noisy data, especially for the long methods with com-
plexlogic.Inordertoconcentratemoreonthedependencies
withoutdestroyingthenaturalness(i.e.,deletingthosenoisy
code),weincorporateprogramslicingtechnique[ 61]bytak-
ingthestatementsinthetryblocksastheslicingcriterion,
andbacktrackingdependenciesbetweenstatements.Weadd
an additional attention module with masks to attend onlyto the tokens in these slices. Finally, we fuse the slicing-
basedcontextvectorwiththecontextmentionedabove,and
generate code by the LSTM decoder.
WecollectalargenumberofJavamethodsfrompopularopen
source repositories in GitHub to construct datasets for the twotasks. We conduct extensive experiments on the datasets to eval-uate our approach and the results demonstrate the effectivenessof our models on both tasks. More specifically, for try block lo-calization, we achieve an accuracy of 74.7% in terms of correctly
predictedmethodsandanF1-scoreof77.4%intermsofcorrectly
predicted statements. For the generation of catch block, our model
cangeneratethecatchblockswith22.6%exactmatchesandaBLEU
value of 46.7%. Both models significantly outperform the baselines.
30Moreover, we perform a human evaluation for the generated code
and the results confirm the superiority of our model.
In summary, this paper makes the following contributions:
â€¢Weproposeanovelapproachtoautomatedexceptionhan-
dling,includingtwoneural-network-basedmodelstolocalize
potential exceptions in the source code and generate code
to handle the exceptions.
â€¢Weprovidetwodatasetswithover700KJavamethodsfor
experiments on automated exception handling. We havepublicly released the datasets to promote further research
on this interesting topic.
â€¢Weconductextensiveexperimentstoevaluateourapproach
using the collected datasets. We also perform a human eval-
uation.Theresultsshowthattheproposedapproachisef-
fective and outperforms all baselines.
The remainder of this paper is organized as follows. Section
2 presents the problem formulation of the automated exception
handling.Section3describesthedetailsofourapproach.Section
4 describes datasets,evaluation procedure, andevaluation results.
We discuss our approach in Section 5. We describe threats to the
validityofthisworkandrelatedworkinSection6andSection7,
respectively. Finally, we conclude our paper in Section 8.
2 PROBLEM FORMULATION
As illustrated in Figure 1, the problem of automated exception
handling can be decomposed into two successive tasks. The first
taskisdeterminingwhichstatementsmaythrowexceptionsand
should be enclosed by a try block. The second task is generating
thecorrespondingcatchblocksforhandlingsuchexceptions.For
simplification, we call the two tasks try block localization andcatch
blockgeneration,respectively.Weformallydefinethemasfollows.
Try Block Localization. This task aims to localize the state-
ments in a code fragment that should be enclosed by try blocks.
Giventhecodefragment ğ¶={ğ‘ 1,ğ‘ 2,...,ğ‘  ğ¾}whereğ¾isthenumber
ofstatements,thetargetistofindonesequence Y={ğ‘¦1,ğ‘¦2,...,ğ‘¦ ğ¾},
whereğ‘¦ğ‘–=YorNmeanswhetherthestatement ğ‘ ğ‘–isinonetryblock
or not. In addition, the adjacent statements with label Y should be
put in one same try block. If all statements are labelled by N, it
meansthatthecodefragmentdoesnotneedtryblocks.Notethat
Ymay include multiple disjoint subsequences of Ys for multiple
non-nested try blocks, and we do not consider the nested try-catch
blocks in this work.
CatchBlockGeneration. Foratryblock,thistaskisdesigned
togeneratecodetokenstocomposethecorrespondingoneormore
catchblocks.Supposeatokensequence ğ¶={ğ‘1,ğ‘2,...,ğ‘ ğ‘™}where
ğ‘ğ‘–(ğ‘–âˆˆ[1,ğ‘™])means one token in the try block and the source code
beforeit,thetargetistogeneratethetokensequenceofcatchblocks
Y={ğ‘¦1,ğ‘¦2,...,ğ‘¦ ğ‘š}so that Ycan handle the exceptions of the try
block.Here Ymayincludemultiplecatchblocksfordifferenttypes
of exceptions thrown by the try block.
Wecanautomaticallyhandleexceptionsofcodefragmentsbased
onthetryblocklocalizationandcatchblockgeneration.Weachieve
this by learningregularities/patternsfrom a large amount ofhis-
torical exception handling code, which can be obtained by mining
open source repositories such as GitHub. In our work, we utilize
deeplearningasitcanbettercapturecontextualinformationandunderstand the semantics of source code. We design two deep neu-
ral network based models for the two tasks, and introduce our
approach in detail in the next section.
3 APPROACH
Inthissection,weintroduceourneuralnetworkbasedapproachto
automatedexceptionhandling,includingthemodelfortryblock
localization and the model for catch block generation. Note that if
developersknowwheretheexceptionsarethrownandhavewritten
tryblocks,thecatchblockgenerationcanworkindependentlyto
produce the source code of corresponding catch blocks.
3.1 Try Block Localization
3.1.1 Overview. As defined above, given a code fragment without
try-catch blocks, the try block localization task is to find statement
subsequences of the code fragment that may throw exceptions and
shouldbeenclosedbytryblocks.Sincealltheinputtokensofthe
code fragment will be checked if they are inside or outside a try
block,wetransformitintoasequencetaggingproblem[ 29],where
eachtokenofthecodefragmentwillbetaggedwithlabelYorN
to represent whether it should be in a try block or not. If the code
fragmentdoesnotneedtryblocks,thelabelsareallNs.Thiskindofproblem has been well studied in the NLP community such as Part-
of-Speech (POS)tagging[ 50,52],namedentityrecognition(NER)
[37,71],andsemanticrolelabeling[ 59,64].Amongthesestudies,
recurrentneuralnetwork(RNN)[ 54]basedapproachesshowmany
advantages over the traditional ones. But they were designed toparsenaturallanguagesentenceswhichareusuallyshorterthan
code fragments. As a result, they suffer from the long-term depen-
dencyproblem[ 9]whenappliedtotagsourcecode.Sincethetry
blocks are always comprised of one or multiple statements, we
propose a two-layered neural model to tag the source code, which
encodes tokens of a statement into the statement vector and tags a
code fragment at the statement level.
Figure 2shows theoverall architectureof ourtry blocklocator.
First, we split the code fragment into a sequence of statements. For
each statement, we obtain its token sequence and use the LSTM[
28] to encode the token sequence for the semantic vector of the
statement. Besides, we apply the attention mechanism [ 65]o nt h e
token sequence of the statement to consider different weights ofindividual tokens. After encoding all the statements of the code
fragmenttoasequenceofstatementsemanticvectors,weutilize
anotherLSTMtofurtherencodethestatementsequence,aswellas
an attention mechanism for capturing the importance of different
statements.Finally,weadoptabinaryclassifieroverthenormalized
vectors to predict the labels of the corresponding statements.
3.1.2 Try Block Locator. Specifically, given a code fragment with-
out try-catch blocks, we obtain the sequence of statements ğ‘†in
a line-by-line manner excluding blank and comment lines. Let
ğ‘†={ğ‘ 1,ğ‘ 2,...,ğ‘  ğ¾}, whereğ¾is the number of statements; ğ‘ ğ‘–=
{ğ‘ğ‘–1,ğ‘ğ‘–2,...,ğ‘ ğ‘–ğ¿},whereğ¿isthelengthofonestatement ğ‘ ğ‘–interms
oftokens.Givenastatement ğ‘ ğ‘–,wefirstembedeachtokenintoa
vectorviaanembeddingmatrix ğ‘Šğ‘’,thatis,ğ‘¥ğ‘–ğ‘¡=ğ‘Šğ‘’ğ‘ğ‘–ğ‘¡.Weusethe
LSTMnetworktoencode ğ‘ ğ‘–basedonthetokenembeddings,which
reads the token embeddings from ğ‘¥ğ‘–1toğ‘¥ğ‘–ğ¿to obtain the hidden
31
 
     	 
  
			
					
		


Figure 2: The architecture of our try block locator
states. At time step ğ‘¡, the hidden stateâˆ’ â†’â„ğ‘–ğ‘¡is obtained by:
âˆ’ â†’â„ğ‘–ğ‘¡=âˆ’âˆ’âˆ’âˆ’â†’ğ¿ğ‘†ğ‘‡ğ‘€(â„ğ‘–ğ‘¡âˆ’1,ğ‘¥ğ‘–ğ‘¡),ğ‘¡âˆˆ[1,ğ¿]. (1)
Furthermore,weadopttheBidirectionalLSTM(Bi-LSTM)[ 54]to
enhance thecapability of capturingthe contextinformation within
thestatement,wherethehiddenstatesofbothdirectionsarecon-
catenated to form the new states:
â† âˆ’â„ğ‘–ğ‘¡=â†âˆ’âˆ’âˆ’âˆ’ğ¿ğ‘†ğ‘‡ğ‘€(â„ğ‘–ğ‘¡+1,ğ‘¥ğ‘–ğ‘¡),ğ‘¡âˆˆ[ğ¿,1].
â„ğ‘–ğ‘¡=[âˆ’ â†’â„ğ‘–ğ‘¡,â† âˆ’â„ğ‘–ğ‘¡],ğ‘¡âˆˆ[1,ğ¿].(2)
Intuitively, not all the tokens in the statement are important for
throwing an exception. To incorporate such knowledge in ourmodel, we exploit the attention mechanism [
65] over the hidden
states to assign important tokens with higher weights and then
aggregatethemtoformthestatementvector.Theprocesscanbe
expressed as the following equations:
ğ‘¢ğ‘–ğ‘¡=ğ‘¡ğ‘ğ‘›â„(ğ‘Šğœ”â„ğ‘–ğ‘¡+ğ‘ğœ”),
ğ›¼ğ‘–ğ‘¡=ğ‘’ğ‘¥ğ‘(ğ‘¢/latticetop
ğ‘–ğ‘¡ğ‘¢ğœ”)
/summationtext.1ğ¿
ğ‘¡=1ğ‘’ğ‘¥ğ‘(ğ‘¢/latticetop
ğ‘–ğ‘¡ğ‘¢ğœ”),
ğ‘ ğ‘–=ğ¿/summationdisplay.1
ğ‘¡=1ğ›¼ğ‘–ğ‘¡â„ğ‘–ğ‘¡.(3)
As listed in Equation 3, we first transform the hidden states into
anewhigh-dimensionspacewithaone-layerMLPtopreparefor
scoringtheimportance.Thenwecomputethescoresofthesehiddenstatesbymeasuringhowtheymatchthefixedvector
ğ‘¢ğœ”andgetthe
normalizedweights ğ›¼ğ‘–ğ‘¡throughthesoftmaxfunction.Here ğ‘¢ğœ”acts
asthequeryofwhichtokenismorelikelytothrowanexception.
Finally,avectorrepresentation ğ‘ ğ‘–ofthestatementisobtainedby
computing the weighted sum of the hidden states.
Afterobtainingasequenceofstatementvectors,wemodelthe
sequential dependencies of the statements by feeding them intoanotherBi-LSTM.Similarly,wegetthehiddenstatesofthestate-
mentsâ„ğ‘–=ğµğ‘–ğ¿ğ‘†ğ‘‡ğ‘€(ğ‘ ğ‘–).Next,weapplytheattentionmechanism
by considering different weights of statements so that:
ğ‘¢ğ‘–=ğ‘¡ğ‘ğ‘›â„(ğ‘Šğ‘ â„ğ‘–+ğ‘ğ‘ ),
ğ›¼ğ‘–=ğ‘’ğ‘¥ğ‘(ğ‘¢/latticetop
ğ‘–ğ‘¢ğ‘ )
/summationtext.1ğ¿
ğ‘¡=1ğ‘’ğ‘¥ğ‘(ğ‘¢/latticetop
ğ‘–ğ‘¢ğ‘ ),
â„ğ‘–=ğ›¼ğ‘–â„ğ‘–.(4)
The probability of label Y for statement ğ‘ ğ‘–is calculated by
Ë†ğ‘¦ğ‘–=ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘(ğ‘Šğ‘â„ğ‘–+ğ‘ğ‘)âˆˆ[0,1], (5)
whereğ‘Šğ‘istheweightmatrixand ğ‘ğ‘isabiasterm.Totrainthe
model, we employ the binary cross-entropy loss that is defined as
L(Î˜,Ë†ğ‘¦,ğ‘¦)=ğ‘/summationdisplay.1
ğ‘›=1ğ¿/summationdisplay.1
ğ‘–=1(âˆ’(ğ‘¦ğ‘›ğ‘–Â·ğ‘™ğ‘œğ‘”(Ë†ğ‘¦ğ‘›ğ‘–)+(1âˆ’ğ‘¦ğ‘›ğ‘–)Â·ğ‘™ğ‘œğ‘”(1âˆ’Ë†ğ‘¦ğ‘›ğ‘–))),
(6)
whereğ‘¦is the ground truth label for each statement, Î˜represents
theparameterstobelearnedand ğ‘isthetotalnumberofinstances
in the training set. During prediction, we make the label as Y if
Ë†ğ‘¦ğ‘–â‰¥ğ›¿otherwise N, where ğ›¿is the threshold.
Basedonthepredictions,allconsecutivestatementswithYlabels
willbeenclosedbyonetryblock.Inthisway,weobtainthelocations
oftryblocks.Onceonetryblockisdeterminedinthecodefragment,
we need to generate the corresponding catch blocks as well, which
will be described in the next subsection.
3.2 Catch Block Generation
3.2.1 Overview. Forhandlingexceptionsofonetryblock,inspired
bythesuccessofNeuralMachineTranslation(NMT),weadoptthepopularencoder-decoderarchitecture[
5]toencodethesourcecode
beforecatchblocksandoutputasequenceoftokensinthecatch
blocks.Animportantquestionhereiswhatcontextshouldbetaken
astheinputforgeneratingthecatchblocks.Onestraightforward
solution is to only consider the source code in try blocks, as re-
cent studies [ 45,51] did, because the source code before try blocks
contains much irrelevant and noisy data. In this paper, to facilitate
explanation,wecallthesourcecodebeforetryblocksasthe leading
code.Forexample,lines1-7inFigure1(b)istheleadingcode.We
findthatthecodeinacatchblockmaydependonsomestatements
(suchasvariableinitialization)intheleadingcode.Therefore,asde-
pictedinFigure3,ourcatchblockgeneratorincludestwoencoders
toseparatelyencodetheleadingcode(i.e.theblueones)andthe
tryblock(i.e.theredones),andaslicing-basedattentionmodule
(i.e. the curves) to filter out the noisy data in the leading code.
3.2.2 Catch Block Generator. We describethe threemaincompo-
nents of the catch block generator in detail below.
1) First, we only use the source code in the try blocks as the
input.Let ğ‘Š={ğ‘¤1,ğ‘¤2,...,ğ‘¤ ğ‘›}denotethetokensequenceofatry
block,wefirstencodethetokensbyaBi-LSTMwiththeembedding
layer to obtain the hidden states:
â„ğ‘¡=ğµğ‘–ğ¿ğ‘†ğ‘‡ğ‘€(ğ‘¤ğ‘¡,â„ğ‘¡âˆ’1). (7)
ThenweuseanLSTMasthedecodertodecodethevectorrepresen-
tation of one try block and generate the tokens of its catch blocks
32
 
  
 
  





 
 

	 

Figure 3: The architecture of our catch block generator.
one by one. Specifically, when generating the ğ‘–-th token at time
stepğ‘–, we update the hidden state ğ‘ğ‘–of the decoder by
ğ‘ğ‘–=ğ¿ğ‘†ğ‘‡ğ‘€(ğ‘ğ‘–âˆ’1,ğ‘¦ğ‘–âˆ’1), (8)
whereğ‘¦ğ‘–âˆ’1is the previous token. To enhance the alignment ability
of the decoder, we adopt the attention mechanism over the hidden
statesğ»={â„1,â„2,Â·Â·Â·,â„ğ‘›}of the encoder to compute the con-
text vector ğ‘£ğ‘–. For simplification, we represent ğ‘£ğ‘–by the following
equation:
ğ‘£ğ‘–=ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘› (ğ‘ğ‘–,ğ»). (9)
Different from ğ‘¢ğœ”in Equation 3, here ğ‘ğ‘–can be considered as a
queryvectortolearntoalignthetargettokenwiththesourceones.
2) Second, as mentioned before, considering only the context
ofthetryblocksmaymissimportantdependencyinformation.A
simplesolutionistoconcatenatetheleadingcodeandthetryblock
intoasinglesequenceandthentrainastandardencoder-decoder
model,butsuchamodelcannotdistinguishthetokensoftheleading
codefromthoseinthetryblocks.Whileinpractice,thescopeof
thetryblocksdoesmattersincetheexceptionsareproducedwithin
it.Toovercomethedrawback,wetrainanadditionalencoderfor
storingtheinformationoftheleadingcode.Let ğ·={ğ‘‘1,ğ‘‘2,...,ğ‘‘ ğ‘š}
represents the token sequence of the leading code, according to
Equation7,weencodeitbyanotherBi-LSTMandgetthehidden
statesğ»/prime={â„/prime
1,â„/prime
2,...,â„/primeğ‘š}. Then we get the context vector of the
leadingcodebyEquation9,thatis, ğ‘£/prime
ğ‘–=ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘› (ğ‘ğ‘–,ğ»/prime).Wefuse
the two context vectors of the leading code and the try block by
weighted sum with one MLP layer to form the context vector ğ‘‰ğ‘ğ‘–.
3)Third,therearealsomanynoisytokensintheleadingcode
that may influence the performance of catch block generation. For
the long methods, the leading code may include many unrelated
tokens such as checking parameters or processing some data while
thetryblocksdonotdependonthem.Therefore,weincorporate
the program slicing technique [ 61] to filter out the noisy tokens of
the leading code in our model. The slicing results are used to label
the tokens of leading code by 1 or 0. When applying the attention
on them, we mask the noisy tokens with 0s, which means that
the attention weights of them will be 0. In this way, this attention
module will only attend to the dependent tokens whose labels are
1s. In short, we initially take the statements in the try block asthe slicing criterion ğ‘†ğ¶, and recursively backtrack statements in
theleadingcodethatmayinfluencethestatementsof ğ‘†ğ¶bydata
dependencytoupdate ğ‘†ğ¶.Thenwegetthetokensequence ğ·/primeof
allstatementsintheleadingcodefrom ğ‘†ğ¶,whichhasfilterednoisy
tokensthathavenoinfluenceonthetryblock.Weusethissequence
ğ·/primeto label any token ğ‘‘ğ‘–inğ·and get the labels ğ¿={ğ‘™1,ğ‘™2,...,ğ‘™ ğ‘š},
where:
ğ‘™ğ‘–=/braceleftBigg
1,ğ‘– ğ‘“ ğ‘‘ ğ‘–âˆˆğ·/prime,
0,ğ‘œ ğ‘¡ â„ ğ‘’ ğ‘Ÿ ğ‘¤ ğ‘– ğ‘  ğ‘’ .(10)
Weusethelabelsasmasksandcombinethemwiththeattention
mechanism to enhance our model by considering the program
dependencies and ignoring the noisy data. Thus:
ğ‘‰ğ‘‘ğ‘–=ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘› (ğ‘ğ‘–,ğ¿Â·ğ»/prime). (11)
Weconcatenatetheslicing-based vector ğ‘‰ğ‘‘ğ‘–withğ‘‰ğ‘ğ‘–andaddan-
other one-layer MLP to get the final context vector ğ‘‰ğ‘–.
Based on the context vector, we calculate the probability of gen-
erating the ğ‘–-th token of the corresponding catch blocks by:
ğ‘(ğ‘¦ğ‘–|ğ‘¦1,...,ğ‘¦ ğ‘–âˆ’1,ğ¶)=ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘Šğ‘”ğ‘‰ğ‘–+ğ‘ğ‘”),(12)
whereğ‘Šğ‘”andğ‘ğ‘”are the weight matrix and bias term, respectively,
ğ¶representsthetwoinputsequencesoftheleadingcodeandthe
tryblock.Trainingsuchanattentionalencoder-decodermodelis
to minimize the loss function:
L(Î˜)=âˆ’ğ‘/summationdisplay.1
ğ‘–=1ğ‘€/summationdisplay.1
ğ‘¡=1logğ‘ƒ(ğ‘¦ğ‘–
ğ‘¡|ğ‘¦ğ‘–
<ğ‘¡,ğ¶), (13)
where Î˜isthetrainableparameters, ğ‘isthetotalnumberoftrain-
ing instances and ğ‘€is the length of each target sequence.
In practice, the trained model can predict the next token one by
one with top- ğ‘˜candidates by the beam search algorithm [ 63], and
finally generate the whole catch blocks.
4 EVALUATION
Inthissection,wefirstintroduceourdatasetscollectedfromGitHub2.
Thenweconductexperimentstoevaluatetheeffectivenessofour
models, including automatic metrics and human evaluation.
2https://github.com
334.1 Data Collection
Toprepareexceptionhandlingcodefragments,wefirstcrawledthe
repositories written in Java at GitHub and selected the top 2,000
popular ones based on their total number of stars and forks. These
popularrepositoriesusuallyhavehigh-qualitysourcecode,since
mostofthemcarryoutthecodereviewprocess[ 42]toguarantee
thesoftwarequality.WeextractedJavamethodsbyparsingallJava
files in the repositories using ANTLR4 [ 49], and obtained 7,840,688
methods in total.
Obviously,notalltheJavamethodsareusefulfortrainingour
models. There are many small and simple methods that do not
need exception handling such as get/set methods. Thus we filtered
out the methods whose source lines of code is less than 7. We also
discardedverylongmethods(i.e.,morethan100lines)toreducethe
possibility of overfitting [ 14]. Then we obtained 3,269,127 methods
for constructing our datasets.
For the try block localization task, we searched for the methods
including at least one try block and obtained 486,435 results. We
foundthattherewereabout88,686caseswherethetryblockmissed
thecorrespondingcatchblocks,thusweremovedthesecases.We
alsoeliminatedthemethodsthathavenestedtryorcatchblocks
and finally got 377,923 of them as positive samples. In addition, we
randomlyselectedanequalnumberofmethodsthatdonothave
any try block as the negative samples. In this way, we built our try
block localization dataset (namely TBLD) for evaluation.
Based on the positive samples described above, we built another
datasetforthecatchblockgenerationtask.Ifonepositivesample
has more than one non-nested try-catch pair (i.e. one try block
and the corresponding catch blocks), we split it into multiple sam-
pleswhich includeonlyone try-catchpair.This produced432,679
samples in total. However, we found that some of them exposedan obvious bad practice, that is, catching an exception but doingnothing. We removed them to improve the quality of these code
fragments. Finally, we obtained 351,420 samples as our catch block
generation dataset (namely CBGD).
4.2 Experimental Setup
We conduct experiments on the TBLD and CBGD datasets to eval-
uateourtryblocklocalizationmodelandcatchblockgeneration
model, respectively. The two datasets are split into training sets,
validation sets, and testing sets with fractions of 80%, 10%, and
10%, respectively. We utilize ANTLR4 to tokenize all the code frag-
ments. The statistics of these two datasets are described in Table 1,
whereMaxT,AvgTandUniqTarethemaximumnumberoftokens,
theaveragenumberoftokens,andthetotalnumberofuniqueto-
kens,respectively.Intheleftcolumn,MaxSandAvgSdenotethe
maximumandaveragenumberofstatementsintheJavamethodsre-spectively.TheSourceandTargetintherightcolumnrepresentthe
source code beforecatch blocks and the catchblocks respectively.NotethatasdefinedinSection2,therecanbemultipletryblocks
in one Java method, and each try block may have multiple catch
blocks, thus we denote the number of try blocks in one method in
thepositivesamplesofTBLDbyTryNum,andthenumberofcatch
blocks in one try-catch pair of CBGD by CatchNum.
Theconfigurationsofourmodelsareasfollows.Forthetryblock
localization task, we set the vocabulary size to 50k by selecting theTable 1: The statistics of the datasets used in our study
TBLD CBGD
#Java methods 755,846 #Try-catch pairs 351,420
#TryNum=1 341,040 #CatchNum=1 324,084
#TryNum â‰¥236,883 #CatchNum â‰¥227,336
MaxT 6403MaxT of Source 3,313
AvgT 115.9AvgT of Source 113.1
MaxS 99MaxT of Target 365
AvgS 14.7AvgT of Target 26.5
UniqT 566,378 UniqT 214,799
most frequent words and replacing out-of-vocabulary words with
UNK,becausetoolargevocabularymayleadtoworseperformance.
The embedding size and the dimensions of hidden states in LSTMs
are128.Thebatchsizeissetto32andthemaximumepochsis20.Weuse the best trained parameters of the 20 saved checkpoints for thelaterpredictionaccordingtotheirperformanceonthevalidationset.
Weadoptthewidely-usedAdam[
33]astheoptimizerwithlearning
rate 0.001 for training our model. The threshold ğ›¿for predicting
labels is 0.5 by default. For the catch block generation task, weimplement our model based on OpenNMT
3. We keep the same
settingsasaboveincludingthevocabularysize,embeddingsizeand
dimensionofhiddenstatesinLSTMsoftheencoderanddecoder.
We set the length limits (in terms of #tokens) of the source and
targetofonetry-catchpairto400and100,becausesuchsettings
can cover most of their original lengths. The batch size is set to 32
and the maximum iterations is 100k. When testing, we leave the
beam size ğ‘˜as the default 5 since it yields good results.
AlltheexperimentsareconductedonaUbuntu16.04serverwith
16coresof2.4GHzCPU,128GBRAMandaTeslaV100GPUwith
32GB memory.
4.3 Evaluation Metrics
We evaluate the performance of different approaches on the two
tasks.Forthetaskoftryblocklocalization,similartothemetrics
in sequence labeling [ 29], we use Precision, Recall and F1-score to
assesshowwelloneapproachpredictsthelocationsoftryblocks
for code fragments in the testing set of TBLD. Their values are
calculated as follows:
ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› =#ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘ğ‘¡ ğ‘Œ
#ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ ğ‘Œ,ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ =#ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘ğ‘¡ ğ‘Œ
#ğ‘”ğ‘œğ‘™ğ‘‘ ğ‘Œ,
ğ¹1âˆ’ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ =2âˆ—ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› âˆ—ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™
ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› +ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™.
Here Y represents that a statement should be enclosed by onetry block in a code fragment, and we denote it as
ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘ğ‘¡ ğ‘Œif it is
correctlypredicted.Similarly, ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ ğ‘Œandğ‘”ğ‘œğ‘™ğ‘‘ ğ‘Œarethepredicted
andgroundtruthlabelsofYinthecorpusrespectively.Besides,wealsowanttoknowtheoverallratioofmethodswhereallstatements
are correctly predicted, denoted by Accuracy.
Forthetaskofcatchblockgeneration,weadopttheautomatic
metric BLEU[ 48] in evaluating the quality of catch blocks. This
metrichasbeenwidelyusedinmanysimilartaskssuchasmachine
translation [ 5], source code summarization [ 30] and API sequence
generation[ 24].Giventhegeneratedcode ğ‘Œ/primeandtheground-truth
3https://github.com/OpenNMT/OpenNMT-py
34ğ‘Œ, BLEU measures the ğ‘›-gram precision between ğ‘Œ/primeandğ‘Œby com-
puting the overlap ratios of ğ‘›-grams and applying brevity penalty
on short translation hypotheses. The value is computed by:
ğµğ¿ğ¸ğ‘ˆ =ğµğ‘ƒÂ·expğ‘/summationdisplay.1
ğ‘›=1ğœ”ğ‘›logğ‘ğ‘›,
whereğ‘ğ‘›is the precision score of the ğ‘›-gram matches between
candidateand referencesentences. Thedefault BLEUcalculatesa
scoreforupto4-gramsusinguniformweights ğœ”ğ‘›.ğµğ‘ƒisthebrevity
penalty and defined as:
ğµğ‘ƒ=/braceleftBigg
1,ğ‘– ğ‘“ ğ‘Œ/prime>ğ‘Œ,
ğ‘’(1âˆ’ğ‘Œ/prime/ğ‘Œ),ğ‘– ğ‘“ ğ‘Œ/primeâ‰¤ğ‘Œ.
Likewise, we also calculate the ratio of generated catch blocks that
are exactly same with the ground-truth ones, referred to Accuracy.
4.4 Baselines
4.4.1 Try Block Localization. While many previous studies use
program analysis technique to detect and fix exception-related
errors such as string handling [ 20] or API misuse detection [ 3],
they usually relies on compilable and complete source code. We
found no existing learning-based approaches to directly predicting
the locations of try blocks for incomplete code fragments, thus we
borrowseveralpopularmodelsoriginallyproposedforsequence
labelingasthebaselines,includingConditionalRandomField(CRF)
[36], Bidirectional LSTM (BiLSTM) [ 34], BiLSTM-CRF [ 29] and
SequencetoSequence(Seq2Seq)[ 16].Webrieflyintroducethem
below.
â€¢CRF. We use the linear CRF model, which is a sequence
modeling framework for processing text and has been used
forpredictingprogramproperties[ 2].Ittakestherawtoken
sequenceasinputandmaximizesthejointprobabilityofthe
entire sequence of labels given the observation sequence.
WetrainaCRFTaggerofNLTKtoolkit[ 40]totagthesource
code.
â€¢BiLSTM. This model has shown its effectiveness for model-
ing sequential data [ 34]. It can capture the semantics of the
wordsthroughwordembeddingandthesequentialdepen-
dencybetweenthewordsthroughLSTM.Besides,aBiLSTM
modelcanaccessbothpastandfutureinputfeaturesfora
givenpositionandthusimprovestheperformance.Weset
the dimensions as same as our model for fair comparison.
â€¢BiLSTM-CRF . This model is a combination of the LSTM
network and the CRF module by feeding the output vectors
of BiLSTM into a CRF layer. Since its superiority in learning
the dependency and sentence level tag information, many
studies [29, 41] use it for labeling sequences.
â€¢Seq2Seq. As previously described, Seq2Seq models have
witnessed great success in various applications. Recently it
wasappliedtonamedentityrecognitionandachievedbetter
resultsthanBiLSTM[ 16].Therefore,wealsoconsideritas
onebaselineinourstudy.Weusethestandardattentionalencoder-decoder model to generate label sequences. The
parameter settings are the same as ours.Table 2: The effectiveness of try block localization
Models Precision Recall F1-score Accuracy
CRF 64.7 26.9 38.0 52.4
BiLSTM 73.7 70.6 72.1 67.5BiLSTM-CRF 51.6 59.2 55.2 54.4Seq2Seq 77.1 60.0 67.5 48.7
Nexgen 80.9 74.3 77.4 74.7
4.4.2 Catch Block Generation. Weselectseveralcommonlyused
modelsrelatedtocodecompletionandgenerationinSEcommunity
ascomparativemethods.Forcodecompletion,LanguageModels
(LMs) [8] such as N-gram and LSTM are widely used to model and
predict tokens of source code [ 26,27,39]. In addition, the machine
translation model Seq2Seq is also used to generate pseudo code
from source code [ 47] or APIs from natural language queries [ 24].
Thus we consider the following baselines.
â€¢N-gram. Hindleet al. [ 27] firstexplored the naturalness of
sourcecodewiththeN-grammodelandappliedittopredict
next tokens. A recent study [ 26] found that carefully adapt-
ingN-grammodelsforsourcecodecanyieldperformance
that surpasses deep-learning models. Thus we borrow the
bestconfigurationfromtheirwork,whichisa6-grammodel
with Jelinek-Mercer interpolation.
â€¢LSTM. As a neural language model, LSTM was also used
forcodecompletionandachievedgoodresults[ 26,38].We
compare our approach with LSTM-based language model
andthehyperparametersaresettobethesameasthosein
our approach.
â€¢Seq2Seq. There are many Seq2Seq-based applications in
software engineering. For instance, Oda et al. [ 47] used a
statistical Seq2Seq model to generate pseudo code given the
source code. Recently, the attentional Seq2Seq model [ 5]
has proven to be more effective, so we use this model for
comparison.Themodelconfigurationsarethesameasthose
used in our model.
4.5 Results
We investigate the following research questions to present the
experimental results and analysis.
RQ1: How does our approach perform compared with base-
lines in try block localization?
To keep the results of our approach Nexgencomparable with those
of baselines, we transform the predicted token-level labels of base-
lines into statement-level ones by making one statement label as Y
ifanytokeninitispredictedY,otherwiseN.Table2providesthe
experimentalresultsofallthecomparedmodels.Thebestresults
are shown in bold.
ItisclearthattheCRFmodelperformsworstamongthem.Al-
though it has a good precision, the recall is quite low and so isF1-score. This means that there are few predicted try block loca-
tionsinitsresults,whichisconfirmedbyourmanualinspection.
Asaresult,theaccuracyisonly52.4%sincemostofthepredictions
are all Ns, that is, it is prone to omit the exceptions.
35Table 3: The effectiveness of catch block generation
Groups Models BLEU Accuracy
Partial Context N-gram 6.7 0.0
LSTM 27.5 8.2
Seq2Seq 38.0 18.9
Full Context N-gram 5.0 0.0
LSTM 29.4 8.1Seq2Seq 42.6 20.7Nexgen 46.7 22.6
Bycontrast,neuralnetworkbasedmodelsachievemuchbetter
results.Forexample,theF1-scoreofBiLSTMis72.1%,whichout-
performs CRF by more than 30%. This may be largely attributed to
itsabilitytounderstandingsemanticsthroughwordembedding.Be-cause in CRF, the tokens are represented as numbers, which makes
thefeaturesverysparse.Inaddition,itcapturesthedependencies
between tokens by combining past and future information, which
helps determine whether to predict the Y label when encountering
a specific token (e.g., a predefinedvariable). Nevertheless, the per-
formance drops more than 10% in terms of F1-score or Accuracy
when combining them together (i.e., BiLSTM-CRF). Intuitively, the
worse performance of the CRF module impairs the whole model. A
deeper reason is that adding the CRF layer forces it to optimize the
log-likelihood on the sequence level, whereas it is demonstrated to
benoteffectiveifusingCRFindependently.WithregardtoSeq2Seq,asurprisingphenomenonisthatitperformsaswellasBiLSTMsuch
as the precision of 77.1%. But the Accuracy is only 48.7%, whichis even lower than that of CRF. We inspected its predictions andfound that it tends to generate shorter label sequences than thegroundtruth,indicatingthatitcannottagthetokensequenceina one-for-one manner due to the limitation of encoder-decoder
architecture on this task.
At last, our approach achieves the best results among all the
models.Forexample,itimprovesBiLSTMby5.3%(F1-score)and
7.2% (Accuracy). The reason is that our model use a two-layered
neural architecture to tag the source code at the statement level,
which can significantly shorten the length of prediction time steps
andgetridofthelong-termdependencyproblem.Asdepictedin
Table1,theaveragelengthofJavamethodsintermsoftokensis
115.9 (AvgT), while it is only 14.7 (AvgS) in terms of statements. In
addition, we employ attention mechanism at both token and state-
ment levels, and thus can learn the distinction between important
and unimportant elements.
RQ2: How does our approach perform in comparison with
the baselines in generating catch block code ?
In this research question, we want to know how our approach and
the baselines perform in generating catch block code and whether
different contexts affect the performance. As shown in Table 3,
we consider two different contexts: only the try blocks (Partial
Context), the leading code and the try blocks (Full Context). For
evaluation,welimitthelengthofthegeneratedcodeto100because
the LM-based models tend to generate very long sequences.
First, we can see that as a statistical language model, N-gram
performs badly in this task. In particular, the BLEU score is only6.7%, which is far from satisfaction. It is of no avail to use thefullcontextfortrainingsuchamodel,andtheperformanceeven
decreases a bit. The reason is that N-gram model only learns the
probability of the next token within a fixed window (i.e., 6 tokens),
no matter what the input is. It fails to capture enough context
information.Wealsofoundthatmostofthegeneratedcodedoes
notadheretoJavasyntax,whichmayexplainwhyitsaccuracyis0.
In contrast, LSTM achieves better results, since LSTM can capture
more dependency information. When given the full context, LSTM
learns to generate more correct tokens than the partial context
and improves the BLEU value. Ho wever, the fullcontextmaylead
tomorenoisydatafromtheleadingcodeandmaketheaccuracy
slightly lower.
The Seq2Seq model shows a significant improvement over N-
gram and LSTM. Because Seq2Seq separates the precondition (i.e.,
the leading code and try blocks) and the learning object (i.e., the
catchblocks)intotwodifferentsequences,andcanlearntheknowl-
edge of try blocks and catch blocks without interference, whereas
theLM-basedmodelspredicttokensinawholesequence.Also,the
attention mechanism in Seq2Seq helps capture different weights of
tokens in the encoder. Compared with the partial context, the full
context also leads to an improvement, such as 4.6% higher in terms
ofBLEU,whichshowsthattheleadingcodeisactuallyhelpfulin
Seq2Seq.
Our approach Nexgenoutperforms all the baselines. Specifically,
we improve the attentional Seq2Seq model by 4.1% and 1.9% in
termsofBLEUandAccuracy,respectively.Thereasonisthat Nexgen
encodestheleadingcodeandtryblocksseparately,andusesslicing-
based attention to eliminate the noisy data from the leading code.
RQ3: To what extent does the components of our proposed
models contribute to the effectiveness of both tasks?
Thisresearchquestionaimsatanalyzingthecontributionsofdif-
ferent components of our model to the overall effectiveness. We
conductanablationstudytoanswerthisRQ.Theresultsareshown
in Table 4.
We start with the try block localization task (Task1). We eval-
uatetheinfluenceofremovingtwomainattentionmodulesfrom
our original model (Nexgen ). When replacing the token-level atten-
tionbythelasttimestepoftokenencoder,wefindthattherecall
becomes a bit higher from 74.3% to 74.8%, yet the F1-score and
Accuracydeclineby0.5%and0.6%.Thisindicatesthatthetoken-
levelattentionindeedcapturessomemoretoken-levelsemantics.
Similarly, the performance also gets worse and the F1-score and
Accuracy decrease by 0.8% and 0.9% when removing the statement-
levelattention.Wecanseethatthestatementattentioncontributes
more to the overall performance than the token attention. If we
removebothoftheattentionmodules,theF1-scoreandAccuracy
drops by 1.3% and 1.7%, respectively.
Forthetaskofcatchblockgeneration(Task2),weobtainedsimi-
lar results. We first remove the standard attention over the leading
code(Leadingattention).ThedropsofBLEUandAccuracyshow
that different tokens in the leading code actually have different
weightsforcapturingthecodesemantics.However,comparedwith
removingslicing-basedattention,theBLEUscoreofremovinglead-
ing attention is 1.2% higher. This means that it is more effective
tocapturethedependenciesintheleadingcodeofthetryblocks
than the standard attention. Next, we remove slicing attention and
36Table 4: The effect of model components on two tasks. The minus â€˜âˆ’â€™ symbol means removing one component from Nexgen
Task1 Task2
Description Precision Recall F1-score Accuracy Description BLEU Accuracy
âˆ’Token Attention 79.3 74.8 76.9 74.1 âˆ’Leading Attention 45.4 22.1
âˆ’Statement Attention 79.5 74.9 76.6 73.8 âˆ’Slicing Attention 44.2 22.0
âˆ’Both Attention 78.9 74.3 76.1 73.0 âˆ’Both Attention 30.2 14.5
Nexgen 80.9 74.3 77.4 74.7 Nexgen 46.7 22.6
Table5:Thescoredistributionofthegeneratedcatchblocks
Score 1234567A v g â‰¥6â‰¤3
N-gram 70 21 8 0 1 0 0 1.41 0 99
LSTM 15 20 15 17 19 5 9 3.56 14 50
Seq2Seq 8 8 15 20 15 9 25 4.53 34 31
Nexgen 9 4 12 17 13 12 33 4.89 45 25
replace leading attention over the leading code with just the last
timestepoftheleadingcodeencoder.Itcanbeseenthattheperfor-
mance degrades significantly by 16.5% (BLEU) and 8.1% (Accuracy),
indicating that theleading code indeed introducesnoisy data and
influences the performance.
In summary, the token-level attention and statement-level atten-
tion components of the try block locator have a relatively small in-
fluenceontheperformance,whiletheleadingattentionandslicing-
based attention mechanisms of the catch block generator are more
effective and contribute significantly to the overall performance.
4.6 Human Evaluation
Forthetaskofcatchblockgeneration,weusethequantitativemet-ricBLEUtocomparethecodegeneratedbydifferentmethods.BLEU
calculates the textual similarity between the reference and the gen-
erated code, rather than the semantic similarity. Thus we perform
human evaluation to complement the quantitative evaluation.
We invite 12 evaluators to assess the quality of catch block code
generatedbyourapproach Nexgenandthethreebaselines.They
are undergraduate, master and Ph.D. students in CS with 1-6 years
of experience in Java programming. The three baselines N-gram,
LSTM and Seq2Seq are selected from the better ones in the partial
orfullcontext.Werandomlychoose100try-catchpairsfromthe
testing set of CBGD and their catch block code produced by the
threebaselinesandourapproach,andevenlydividethemintofour
groups.Eachgroupisassignedtothreedifferentevaluatorssince
such redundancy can help obtain more consistent results. For each
try-catch pair, we show its leading code and try block, the refer-ence catch blocks, and four results of catch blocks generated by
thethreebaselinesandourapproach.Thefourgeneratedresults
of catch blocks are randomly ordered, thus the evaluators haveno idea which catch block code is produced by which approach.
The evaluators can select a score between 1 to 7 to measure the
semantic similarity between the generated catch blocks and the
reference,where1meansâ€œNotSimilarAtAllâ€and7meansâ€œHighly
Similar/Identicalâ€. The higher scores mean that the corresponding
generatedcatchblocksaremoresemanticallysimilar totherefer-
ence. For each generated result of catch blocks, we get three scores
from evaluators and choose the median value as the final score.Table5showsthescoredistributionofthegeneratedcatchblocks.
Wecanseethatourapproachachievesthebestscoresandimproves
theaverage(Avg)scorefrom1.41(N-gram),3.56(LSTM)and4.53
(Seq2Seq) to 4.89. Specifically, among the randomly selected 100
try-catch pairs, our approach can generate 33 highly similar or
evenidentical catchblockswiththereferenceones(score= 7),45
goodcatchblocks(score â‰¥6).Ourapproachalsoreceivesthesmall-
estnumberofnegativeresults(score â‰¤3).Basedonthe100final
scoresforeachapproachofN-gram,LSTM,Seq2Seqand Nexgen,
we conduct Wilcoxon signed-rank tests [ 62]. Comparing our ap-
proachwithN-gram,LSTM,andSeq2Seq,thep-valuesofWilcoxon
signed-ranktestsat95%confidencelevelare2.2e-16,4.5e-09and
0.0076,respectively,showingthattheimprovementsachievedby
ourapproacharestatisticallysignificant.Insummary,theresults
of human evaluation confirm the effectiveness of the proposed
approach.
5 DISCUSSION
5.1 Case study
Nowwediscussthesuperiorityandlimitationofourmodelforcatch
blockgenerationbyanalyzingtwoexamples,whichareshownin
Table6.Example1showsoneincompleteJavamethodâ€œaddâ€includ-
ingtheleadingcodeandtryblock.Thedeveloperwantstocatch
â€œIllegalStateExceptionâ€ thrown by â€œmap.putâ€ in the try block when
multiple objects write data into the map and incur conflicts. Then
the catch block handles the exception by converting and throwing
the exception to notify upper-layer methods. In the full context,
N-gramgeneratesverylongcodewithwrongJavasyntax(weomit
94 more tokens here). Since there are many noisy statements in
the leading code, LSTM and Seq2Seq fail to report the proper ex-
ceptiontype.Forexample,Seq2SeqgeneratesClassCastException.
Bycontrast, Nexgencorrectlypredictstheexceptiontypeandthe
whole catch block, since Nexgentreats the leading code and the try
block separately, and uses the slicing-based attention to remove
irrelevant and noisy statements.
Although Nexgenoutperforms the baselines, we do not claim
that it has already matured and is ready for use in practice. Indeed,
ourapproachonlymakesonestepfurthertowardsautomatedex-
ception handling and it still has some limitations. In Example 2,although our approach successfully predicts the exception type
FileNotFoundException,itfailstocorrectlyhandleitinthecatch
block.Specifically,itlogstheeventmaybebecauselogginginforma-
tion for FileNotFoundException is a very common behavior in the
trainingset.Butthecorrectexceptionhandlingcodeistosetâ€œmap-
pingâ€ as null, which possibly relies on the project-specific features
37Table 6: Two examples of catch block generation
Example 1 Example 2
Codepublic void add(Session session ,Row row ){
TransactionMap <Value,Value>map=
getMap(session );
ValueArray array =convertToKey (row,null);
boolean checkRequired =
indexType .isUnique ()& &
!mayHaveNullDuplicates (row);
if(checkRequired ){
checkUnique (map,array,Long.MIN_VALUE );
}
try{
map.put(array,ValueNull .INSTANCE );
}private boolean revalidate (boolean flag){
if(mapping ==null||flag){
File catalog =findFile (dtdSetFolder ,
CATALOG_FILE_NAME );
if(catalog ==null){mapping =null;}
else
try{
mapping =parseCatalog (new
InputStreamReader (
new FileInputStream (catalog )));
}
Referencecatch(IllegalStateException e ){throw
mvTable .convertException (e);}catch(FileNotFoundException exc ){mapping =null;}
N-gramcatch(final IOException x );...(94 more tokens ) catch(IOException ioe ){throw new
KafkaCruiseControlException (e);...(89 more
tokens)
LSTMcatch(Exception e ){throw new
RuntimeException (e); }catch(IOException e ){throw new
RuntimeException (e);}
Seq2Seqcatch(ClassCastException e ){throw new
UnsupportedOperationException ();}catch(IOException exc ){return false ;}
Nexgencatch(IllegalStateException e ){throw
mvTable .convertException (e);}catch(FileNotFoundException exc ){
Logger.getLogger (<unk>.class.getName ()).
log(Level.INFO,null,exc);}
and personal preferences of developers. Our approach also encoun-
terstheproblemofoutofvocabularyandreplacestheclassname
withâ€œ<unk>â€.Inourfuturework,wewillimproveourapproachby
incorporatingtheknowledgeoftheexceptionspecificationsfrom
projects and designing mechanisms that can overcome the large
vocabulary problem.
5.2 Why is Nexgenbetter?
As stated previously, learning to handle exceptions includes two
main successive tasks: finding potential exceptions and writing
code to handle them. The common challenge of them is how to
understandthesemanticsofgivencodesnippets.Comparedwith
traditionalapproaches, Nexgenlearnsthesemanticsofcodefrom
the following two aspects.
First,traditionalapproacheslikestaticanalysisorN-grammodel
have beenapplied infinding string-handling errors[ 20] orrecom-
mending exception-handling APIs [ 45], but they ignore the seman-
ticsoftokens(e.g.,treatingthemasnumericalIDs).Incontrast, Nex-
genadoptsthewordembeddingtechniquetoautomaticallylearn
the semantics of tokens by mapping them into a high-dimensional
vectorspace.Thisisafundamentalcomponentofmanydeeplearn-
ing based approaches to code analysis [ 1,24] because tokens with
similarembeddingstendtobeusedinsimilarcontexts,whichhelps
determine whether a same/similar exception would occur and how
it can be handled. For example, even the Java tokens â€œAudioInput-
Streamâ€ and â€œStringBufferInputStreamâ€ have different names, theyare used in similar contexts and thus possibly throw the â€œIOExcep-
tionâ€. Therefore, ignoring the semantics of tokens may lead to a
wrong result.
Second,itisalsonecessarytoknowthedeepsemanticsbyun-
derstandingdependenciesinsourcecode.Statisticalmodels(e.g.,N-gram)consideronlytheprobabilitydistributionofcodetokens
andmayhaveapoorperformance.Programanalysiscananalyzethe
data-flowandcontrol-flowdependencies,whichhasbeenshown
effective for compilable and complete source code. Unlike them,Nexgenlearns the internal dependencies within methods for in-
complete code fragments. On the one hand, similar to existingstudies[
23,69],itutilizestheLSTMtocapturethesequentialde-
pendencies [ 70] of tokens and statements, so that exception-prone
statements can be identified based on the context. As shown inExample 1 of Table 6, Nexgenidentified the statement â€œmap.putâ€
insteadof â€œconvertToKeyâ€ asthe exceptional onebecausethestate-
ment â€œcheckUniqueâ€is alreadyused tocheck thevariables, hence
it only needs to check the latter statement. On the other hand,
whengeneratingcatchblocks, Nexgencanlearntheexplicitdata
dependenciesforeliminating noisydatafrom theleadingcodeby
the standard attention and our slicing-based attention mechanisms.
In this way, Nexgencan understand the deep code semantics by
capturingthesequentialdependenciesandthedatadependencies
in code fragments.
6 THREATS TO VALIDITY
There are three main threats to validity of our evaluation.
â€¢Inthiswork,weonlycollectedJavasourcecodetoconstruct
the datasets for out tasks. It remains unknown whether our
approachwillperformwellonotherprogramminglanguages
suchasC#andPython.However,webelieveJavaisrepre-sentative because of its popularity in real-world softwareapplications. In our future work we will collect data fromprograms in different programming languages to further
evaluate the proposed approach.
38â€¢The quality of the Java methods may affect the effectiveness
ofourapproach.Tomitigateit,weselectedtop2,000projects
at GitHub as corpus by the numbers of their stars and forks,
which indicate their impact and popularity. We filter out
simple methods (e.g., get/set methods) that have less than 7
linesofcode.Wealsodiscardtheexceptionhandlingcode
withbadpractices,forexample,swallowingexceptions.Still,
we cannot guarantee that all data we collected is of high
quality.Wewillexploreeffectivetechniquestoimprovedata
quality in the future.
â€¢There may exist some score bias in the human evaluation,
sincedeveloperscanmakedifferentjudgementsonthesamegeneratedcodeaccordingtotheirdifferentexperiences.Hence,
eachgeneratedcodeisassignedtothreeredundantevalua-
torsandwetakethemedianvalueasthefinalscoretoreduce
the bias.
7 RELATED WORK
7.1 Exception handling
Manystudieshavebeenconductedtounderstandexceptionhan-
dlingpractices[ 12,13,19,55].Forexample,Senaetal.[ 55]inves-
tigated potential impact of the exception handling strategies on
theclientapplicationsbyexceptionflowanalysisandmanualin-spections. They found that 20.71% of the bug reports are relatedto the anti-patterns of exception handling. Padua et al. [
19] per-
formedanempiricalstudyoftherelationshipbetweenexception
handling practices and software quality (measured by the probabil-
ity of having post-release defects). The case study on open-source
Java and C# projects showed that exception flow characteristics
in Java projects have a significant relationship with post-release
defects. Even so, previous studies revealed that developers tend to
avoiditormisuseexceptionhandlingbecausetheyconsiderithard
to learn and to use [ 4,10,32,57]. For example, Cabral et al. [ 10]
studied exception handling practices from 32 projects in both Java
and.Netandunveiledsuboptimalpracticeofexceptionhandlingbydevelopers. Shah et al. [
57] interviewed Java developers in order to
contrast the viewpoints of experienced and inexperienced develop-
ers regarding exception handling. They recognized that developers
tend to ignore the proper implementation of exception handlinguntil defects are found, although developers should do it in the
early releases of a system. All these studies indicate that there is a
need for automatic techniques to facilitate the writing of exception
handling code for developers.
Meanwhile, prior research also explored different kinds of ap-
proaches to exception handling [ 6,11,45,51,58]. Cabral et al. [ 11]
proposed a prototype transactional model that aims at automati-cally recovering runtime environments at the platform level, in-
steadofwritingexceptionhandlingcode.Althoughthisproposal
is promising, its overhead is high and its precision is low due to
the complexity of exceptions. Subsequent research focuses on rec-
ommendingsourcecoderelatedinformationtoaidprogramming
practice. For example, Barbosa et al. [ 6] proposed a heuristic strat-
egy that is aware of the global context of exceptions and produces
recommendations on the violations of exception handling. Nguyen
et al. [45]proposed afuzzy andN-gram basedapproach torecom-
mend exception types and repairing method calls for exceptionhandlingcode.Suchapproachescanprovideusefulknowledgeto
assist developers in writing exception handling code, but develop-
ersstillneedtodesignthewholelogicofexceptionhandlingand
modify the code to fit the context. Compared with previous work,
weprovideadeeplearningbasedapproachtoautomatedexception
handling,whichcanpredictthelocationsoftryblocksandgenerate
the complete catch blocks.
7.2 Code completion and generation
There exists a rich literature of research work on code completion
inSE,mostofwhichreliesonlanguagemodels(LMs).Forexample,
Hindleetal.[ 27]usedtheN-grammodelonlexicaltokenstopre-
dict the next token. Hellendoorn et al. [ 26] performed an extensive
comparisonbetweenN-gramandLSTM-basedLMsforsourcecode,
and concluded that N-gram models can outperform LSTM mod-els if carefully adapted. Apart from token-level code completion,
Nguyenetal.[ 44]combinedprogramanalysisandstatisticalLM
in the process of statement completion. Recently, some researchintends to generate code in various scenarios [
17,24,47,60]. For
example,Odaetal.[ 47]usedastatisticalmachinetranslationmodel
togeneratepseudocodegiventhesourcecode.Tufanoetal.[ 60]
investigated the NMT model (also called Seq2Seq) on pairs of code
components before and after the implementation of the changes
providedinthepullrequests.TheirresultsshowedthatNMTcan
automatically replicate the changes implemented by developers
duringpull requests.Similarly, Chenetal. [ 17]proposed aneural
model based on Seq2Seq, which learns from pairs of the original
versionandfixedversionofbuggyprogramminglines.Thereare
alsosomerecentstudiesinNLPcommunity,whichproposeneu-
ralnetworkbased modelstogeneratecode fromnaturallanguage
descriptions [ 31,66]. Different from the above work, our work
generates exception handling code instead of general code.
8 CONCLUSION
In this paper, we propose a novel deep learning based approach to
automatedexceptionhandling.Wedecomposetheproblemintotwo
coherent tasks, namely try block localization and catch block gen-
eration, whose targets are to recognize potential exceptions within
a Java method and generate code to handle them, respectively. For
both tasks, we design neural models with attention mechanismstocapturedeepsemantics.Weconductextensiveexperimentsto
evaluateourapproach.Allresultsdemonstratethesuperiorityof
our models on both tasks. Although our results are significantly
better that those of the baselines, they can still be improved before
beingappliedinpractice.Ourworkcanbeconsideredasoneofthe
firststepstowardsautomatedexceptionhandlingandwehopeit
can inspire follow-up research work.
Our source code and experimental data are available at https:
//github.com/zhangj111/nexgen.
ACKNOWLEDGMENTS
This work was supported partly by National Key Research and De-
velopment Program of China (No.2018YFB1004805), partly by Na-
tionalNaturalScienceFoundationofChina(No.61702024,61932007,
61972013 and 61421003) and ARC DP200102940.
39REFERENCES
[1]MiltiadisAllamanis,EarlTBarr,ChristianBird,andCharlesSutton.2015. Sug-
gestingaccuratemethodandclassnames.In Proceedingsofthe201510thJoint
Meeting on Foundations of Software Engineering. 38â€“49.
[2]Uri Alon,Meital Zilberstein, OmerLevy, and EranYahav.2018. Ageneral path-
basedrepresentationforpredictingprogramproperties.In Proceedingsofthe39th
ACMSIGPLANConferenceonProgrammingLanguageDesignandImplementation.
404â€“419.
[3]Sven Amann, Hoan Anh Nguyen, Sarah Nadi, Tien N Nguyen, and Mira Mezini.
2018. Asystematicevaluationofstaticapi-misusedetectors. IEEETransactions
on Software Engineering 45, 12 (2018), 1170â€“1188.
[4]Muhammad Asaduzzaman,Muhammad Ahasanuzzaman,Chanchal KRoy,and
Kevin A Schneider. 2016. How developers use exception handling in Java?. In
2016 IEEE/ACM13th Working Conferenceon MiningSoftware Repositories (MSR).
IEEE, 516â€“519.
[5]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural ma-
chinetranslationbyjointlylearningtoalignandtranslate.In 3rdInternational
Conference on Learning Representations, ICLR 2015.
[6]EijiAdachiBarbosaandAlessandroGarcia.2017.Global-awarerecommendations
for repairing violations in exception handling. IEEE Transactions on Software
Engineering 44, 9 (2017), 855â€“873.
[7]EijiAdachiBarbosa,AlessandroGarcia,MartinPRobillard,andBenjaminJakobus.
2015. Enforcing exception handling policies with a domain-specific language.
IEEE Transactions on Software Engineering 42, 6 (2015), 559â€“584.
[8]Yoshua Bengio, RÃ©jean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A
neural probabilistic language model. Journal of machine learning research 3, Feb
(2003), 1137â€“1155.
[9]YoshuaBengio,PaoloFrasconi,andPatriceSimard.1993. Theproblemoflearning
long-term dependencies in recurrent networks. In IEEE international conference
on neural networks. IEEE, 1183â€“1188.
[10]Bruno Cabral and Paulo Marques. 2007. Exception handling: A field study in
java and. net. In European Conference on Object-Oriented Programming. Springer,
151â€“175.
[11]Bruno Cabral and Paulo Marques. 2008. A case for automatic exception han-
dling. In2008 23rd IEEE/ACM International Conference on Automated Software
Engineering. IEEE, 403â€“406.
[12]NÃ©lioCacho,EijiAdachi Barbosa,JulianaAraujo,FredericoPranto,Alessandro
Garcia, Thiago Cesar, Eliezio Soares, Arthur Cassio, Thomas Filipe, and Israel
Garcia.2014.Howdoesexceptionhandlingbehaviorevolve?anexploratorystudy
in java and c# applications. In 2014 IEEE International Conference on Software
Maintenance and Evolution. IEEE, 31â€“40.
[13]NÃ©lio Cacho, Thiago CÃ©sar, Thomas Filipe, Eliezio Soares, Arthur Cassio, Rafael
Souza, Israel Garcia,Eiji Adachi Barbosa, and AlessandroGarcia. 2014. Trading
robustnessformaintainability:anempiricalstudyofevolvingc#programs.In
Proceedingsofthe36thInternationalConferenceonSoftwareEngineering.584â€“595.
[14]Rich Caruana, Steve Lawrence, and C Lee Giles. 2001. Overfitting in neural nets:
Backpropagation, conjugate gradient, and early stopping. In Advances in neural
information processing systems. 402â€“408.
[15]Chien-TsunChen,YuChinCheng,Chin-YunHsieh,andI-LangWu.2009. Excep-
tionhandlingrefactorings:Directedbygoalsanddrivenbybugfixing. Journal
of Systems and Software 82, 2 (2009), 333â€“345.
[16]Lingzhen Chen and Alessandro Moschitti. 2018. Learning to Progressively Rec-
ognizeNewNamedEntitieswithSequencetoSequenceModels.In Proceedings
of the 27th International Conference on Computational Linguistics. 2181â€“2191.
[17]Zimin Chen, Steve James Kommrusch, Michele Tufano, Louis-NoÃ«l Pouchet,Denys Poshyvanyk, and Martin Monperrus. 2019. Sequencer: Sequence-to-
sequence learning for end-to-end program repair. IEEE Transactions on Software
Engineering (2019).
[18]Roberta Coelho, Awais Rashid, Alessandro Garcia, Fabiano Ferrari, NÃ©lio Cacho,
UirÃ¡ Kulesza, Arndt von Staa, and Carlos Lucena. 2008. Assessing the impact
of aspects on exception flows: An exploratory study. In European Conference on
Object-Oriented Programming. Springer, 207â€“234.
[19]GuilhermeBdePÃ¡duaandWeiyiShang.2018. Studyingtherelationshipbetween
exception handling practices and post-release defects. In Proceedings of the 15th
International Conference on Mining Software Repositories. 564â€“575.
[20]AritraDhar,RahulPurandare,MohanDhawan,andSureshRangaswamy.2015.
CLOTHO:savingprogramsfrommalformedstringsandincorrectstring-handling.
InProceedings of the 2015 10th Joint Meeting on Foundations of Software Engineer-
ing. 555â€“566.
[21]AlessandroFGarcia,CecÄ±liaMFRubira,AlexanderRomanovsky,andJieXu.2001.
A comparative study of exception handling mechanisms for building dependable
object-oriented software. Journal of systems and software 59, 2 (2001), 197â€“222.
[22]JamesGosling,BillJoy,GuySteele,andGiladBracha.2000. TheJavalanguage
specification. Addison-Wesley Professional.
[23]XiaodongGu,HongyuZhang,andSunghunKim.2018. Deepcodesearch.In 2018
IEEE/ACM 40th International Conference on Software Engineering (ICSE). IEEE,
933â€“944.[24]XiaodongGu,HongyuZhang,DongmeiZhang,andSunghunKim.2016. Deep
API learning. In Proceedings of the 2016 24th ACM SIGSOFT International Sympo-
sium on Foundations of Software Engineering. ACM, 631â€“642.
[25]Anders Hejlsberg, Scott Wiltamuth, and Peter Golde. 2003. C# language specifi-
cation. Addison-Wesley Longman Publishing Co., Inc.
[26]VincentJHellendoornandPremkumarDevanbu.2017. Aredeepneuralnetworks
thebestchoiceformodelingsourcecode?.In Proceedingsofthe201711thJoint
Meeting on Foundations of Software Engineering. 763â€“773.
[27]AbramHindle,EarlTBarr,ZhendongSu,MarkGabel,andPremkumarDevanbu.
2012. On the naturalness of software. In 2012 34th International Conference on
Software Engineering (ICSE). IEEE, 837â€“847.
[28]SeppHochreiterandJÃ¼rgenSchmidhuber.1997. Longshort-termmemory. Neural
computation 9, 8 (1997), 1735â€“1780.
[29]Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirectional LSTM-CRF models for
sequence tagging. arXiv preprint arXiv:1508.01991 (2015).
[30]Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016.
Summarizingsourcecodeusinganeuralattentionmodel.In Proceedingsofthe
54thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:
Long Papers), Vol. 1. 2073â€“2083.
[31]Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2018.
Mapping Language to Code in Programmatic Context. In Proceedings of the 2018
Conference on Empirical Methods in Natural Language Processing. 1643â€“1652.
[32]Mary Beth Kery, Claire Le Goues, and Brad A Myers. 2016. Examining program-
merpracticesforlocallyhandlingexceptions.In 2016IEEE/ACM13thWorking
Conference on Mining Software Repositories (MSR). IEEE, 484â€“487.
[33]Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[34]Eliyahu Kiperwasser and Yoav Goldberg. 2016. Simple and accurate dependency
parsing using bidirectional LSTM feature representations. Transactions of the
Association for Computational Linguistics 4 (2016), 313â€“327.
[35]George J Klir and Bo Yuan. 1996. Fuzzy sets and fuzzy logic: theory and applica-
tions.Possibility Theory versus Probab. Theory 32, 2 (1996), 207â€“208.
[36]JohnDLafferty,AndrewMcCallum,andFernandoCNPereira.2001. ConditionalRandomFields:ProbabilisticModelsforSegmentingandLabelingSequenceData.
InProceedings of the Eighteenth International Conference on Machine Learning.
282â€“289.
[37]GuillaumeLample,MiguelBallesteros,SandeepSubramanian,KazuyaKawakami,
and Chris Dyer. 2016. Neural Architectures for Named Entity Recognition. In
Proceedingsofthe2016ConferenceoftheNorthAmericanChapteroftheAssociation
for Computational Linguistics: Human Language Technologies. 260â€“270.
[38]Jian Li, Yue Wang, Michael R. Lyu, and Irwin King. 2018. Code Completion with
Neural Attention and Pointer Networks. In Proceedings of the Twenty-Seventh
International Joint Conference on Artificial Intelligence, IJCAI-18. 4159â€“4165.
[39]ChangLiu,XinWang,RichardShin,JosephEGonzalez,andDawnSong.2016.
Neural code completion. (2016).
[40]Edward Loper and Steven Bird. 2002. NLTK: The Natural Language Toolkit.
InProceedingsoftheACL-02WorkshoponEffectiveToolsandMethodologiesfor
Teaching Natural Language Processing and Computational Linguistics.
[41]Xuezhe Ma and Eduard Hovy. 2016. End-to-end Sequence Labeling via Bi-directionalLSTM-CNNs-CRF.In Proceedingsofthe54thAnnualMeetingofthe
Association for Computational Linguistics (Volume 1: Long Papers). 1064â€“1074.
[42]Shane McIntosh, Yasutaka Kamei, Bram Adams, and Ahmed E Hassan. 2016.
Anempiricalstudyoftheimpactofmoderncodereviewpracticesonsoftware
quality.Empirical Software Engineering 21, 5 (2016), 2146â€“2189.
[43]Robert Miller and Anand Tripathi. 1997. Issues with exception handling in
object-orientedsystems.In EuropeanConferenceonObject-OrientedProgramming.
Springer, 85â€“103.
[44]Son Nguyen, Tien Nguyen, Yi Li, and Shaohua Wang. 2019. Combining Program
Analysis and Statistical Language Model for Code Statement Completion. In
2019 34th IEEE/ACM International Conference on Automated Software Engineering
(ASE). IEEE, 710â€“721.
[45]Tam Nguyen, Phong Vu, and Tung Nguyen. 2019. Recommending exceptionhandling code. In 2019 IEEE International Conference on Software Maintenance
and Evolution (ICSME). IEEE, 390â€“393.
[46]Ana Filipa Nogueira, JosÃ© CB Ribeiro, and MÃ¡rio A Zenha-Rela. 2017. Trends on
empty exception handlers for Java open source libraries. In 2017 IEEE 24th Inter-
nationalConferenceonSoftwareAnalysis,EvolutionandReengineering(SANER).
IEEE, 412â€“416.
[47]Yusuke Oda, Hiroyuki Fudaba, Graham Neubig, Hideaki Hata, Sakriani Sakti,
TomokiToda,andSatoshiNakamura.2015. Learningtogeneratepseudo-code
fromsourcecodeusingstatisticalmachinetranslation(t).In 201530thIEEE/ACM
InternationalConferenceonAutomatedSoftwareEngineering(ASE).IEEE,574â€“584.
[48]KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu.2002. BLEU:a
method for automatic evaluation of machine translation. In Proceedings of the
40th annualmeetingon associationfor computationallinguistics.Association forComputational Linguistics, 311â€“318.
[49] Terence Parr. 2013. The definitive ANTLR 4 reference. Pragmatic Bookshelf.
40[50]Juan Antonio Perez-Ortiz and Mikel L Forcada. 2001. Part-of-speech tagging
withrecurrentneuralnetworks.In IJCNNâ€™01.InternationalJointConferenceon
Neural Networks. Proceedings (Cat. No. 01CH37222), Vol. 3. IEEE, 1588â€“1592.
[51]MohammadMasudurRahmanandChanchalKRoy.2014. Ontheuseofcontextin
recommendingexceptionhandlingcodeexamples.In 2014IEEE14thInternational
Working Conference on Source Code Analysis and Manipulation. IEEE, 285â€“294.
[52]AdwaitRatnaparkhi.1996.Amaximumentropymodelforpart-of-speechtagging.
InConference on Empirical Methods in Natural Language Processing.
[53]Martin P Robillard and Gail C Murphy. 2003. Static analysis to support the
evolutionofexceptionstructureinobject-orientedsystems. ACMTransactions
on Software Engineering and Methodology (TOSEM) 12, 2 (2003), 191â€“221.
[54]Mike Schuster and Kuldip K Paliwal. 1997. Bidirectional recurrent neural net-
works.IEEE transactions on Signal Processing 45, 11 (1997), 2673â€“2681.
[55]DemÃ³stenes Sena, Roberta Coelho, UirÃ¡ Kulesza, and Rodrigo BonifÃ¡cio. 2016.
Understanding the exception handling strategies of Java libraries: An empirical
study. In Proceedings of the 13th International Conference on Mining Software
Repositories. 212â€“222.
[56]Hina Shah, Carsten GÃ¶rg, and Mary Jean Harrold. 2008. Why do developers
neglect exception handling?. In Proceedings of the 4th international workshop on
Exception handling. 62â€“68.
[57]HinaShah,CarstenGorg,andMaryJeanHarrold.2010. Understandingexception
handling: Viewpoints of novices and experts. IEEE Transactions on Software
Engineering 36, 2 (2010), 150â€“161.
[58] Suresh Thummalapenta andTao Xie. 2009. Mining exception-handlingrules as
sequenceassociationrules.In 2009IEEE31stInternationalConferenceonSoftware
Engineering. IEEE, 496â€“506.
[59]Kristina Toutanova, Aria Haghighi, and Christopher D Manning. 2005. Joint
learning improves semantic role labeling. In Proceedings of the 43rd Annual Meet-
ing on Associationfor Computational Linguistics . Association forComputational
Linguistics, 589â€“596.
[60]Michele Tufano, Jevgenija Pantiuchina, Cody Watson, Gabriele Bavota, andDenys Poshyvanyk. 2019. On learning meaningful code changes via neural
machine translation. In 2019 IEEE/ACM 41st International Conference on Software
Engineering (ICSE). IEEE, 25â€“36.[61]Mark Weiser. 1984. Program slicing. IEEE Transactions on software engineering 4
(1984), 352â€“357.
[62]Frank Wilcoxon, SK Katti, and Roberta A Wilcox. 1970. Critical values and
probability levels for the Wilcoxon rank sum test and the Wilcoxon signed rank
test.Selected tables in mathematical statistics 1 (1970), 171â€“259.
[63]Sam Wiseman and Alexander M Rush. 2016. Sequence-to-Sequence Learning as
Beam-Search Optimization. In Proceedings of the 2016 Conference on Empirical
Methods in Natural Language Processing. 1296â€“1306.
[64]NianwenXueandMarthaPalmer.2004. Calibratingfeaturesforsemanticrole
labeling.In Proceedingsofthe2004ConferenceonEmpiricalMethodsinNatural
Language Processing. 88â€“94.
[65]Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and EduardHovy. 2016. Hierarchical attention networks for document classification. In
Proceedings ofthe2016 conferenceofthe NorthAmericanchapter oftheassociation
for computational linguistics: human language technologies. 1480â€“1489.
[66]PengchengYinandGrahamNeubig.2017. ASyntacticNeuralModelforGeneral-
Purpose Code Generation. In Proceedings of the 55th Annual Meeting of the Asso-
ciation for Computational Linguistics (Volume 1: Long Papers). 440â€“450.
[67]DingYuan,YouLuo,XinZhuang,GuilhermeRennaRodrigues,andXuZhao.2014.
SimpleTestingCanPreventMostCriticalFailures.In 11thUSENIXSymposium
on Operating Systems Design and Implementation (OSDI 14).
[68]Benwen Zhang and James Clause. 2014. Lightweight automated detection of
unsafeinformationleakageviaexceptions.In Proceedingsofthe2014International
Symposium on Software Testing and Analysis. 327â€“338.
[69]Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu. 2020.Retrieval-based neural source code summarization. In Proceedings of the 42nd
International Conference on Software Engineering. IEEE.
[70]JianZhang,XuWang,HongyuZhang,HailongSun,KaixuanWang,andXudong
Liu.2019. Anovelneuralsourcecoderepresentationbasedonabstractsyntax
tree. In2019 IEEE/ACM 41st International Conference on Software Engineering
(ICSE). IEEE, 783â€“794.
[71]GuoDong Zhou and Jian Su. 2002. Named entity recognition using an HMM-
based chunk tagger. In proceedings of the 40th Annual Meeting on Association for
Computational Linguistics. Associationfor Computational Linguistics,473â€“480.
41