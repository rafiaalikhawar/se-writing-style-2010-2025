Operational Calibration: Debugging Confidence Errors for
DNNs in the Field
Zenan Li
State Key Lab of Novel Software
Technology, Nanjing University
Nanjing, China
lizenan@smail.nju.edu.cnXiaoxing Ma∗
State Key Lab of Novel Software
Technology, Nanjing University
Nanjing, China
xxm@nju.edu.cnChang Xu
State Key Lab of Novel Software
Technology, Nanjing University
Nanjing, China
changxu@nju.edu.cn
Jingwei Xu
State Key Lab of Novel Software
Technology, Nanjing University
Nanjing, China
jingweix@nju.edu.cnChun Cao
State Key Lab of Novel Software
Technology, Nanjing University
Nanjing, China
caochun@nju.edu.cnJian Lü
State Key Lab of Novel Software
Technology, Nanjing University
Nanjing, China
lj@nju.edu.cn
ABSTRACT
Trained DNN models are increasingly adopted as integral parts of
software systems, but they often perform deficiently in the field. A
particularly damaging problem is that DNN models often give false
predictions with high confidence, due to the unavoidable slight
divergences between operation data and training data. To minimize
the loss caused by inaccurate confidence, operational calibration,
i.e., calibrating the confidence function of a DNN classifier against
its operation domain, becomes a necessary debugging step in the
engineering of the whole system.
Operational calibration is difficult considering the limited budget
of labeling operation data and the weak interpretability of DNN
models. We propose a Bayesian approach to operational calibration
that gradually corrects the confidence given by the model under
calibration with a small number of labeled operation data deliber-
ately selected from a larger set of unlabeled operation data. The
approach is made effective and efficient by leveraging the local-
ity of the learned representation of the DNN model and modeling
the calibration as Gaussian Process Regression. Comprehensive
experiments with various practical datasets and DNN models show
that it significantly outperformed alternative methods, and in some
difficult tasks it eliminated about 71% to 97% high-confidence ( >0.9)
errors with only about 10% of the minimal amount of labeled oper-
ation data needed for practical learning techniques to barely work.
CCS CONCEPTS
•Software and its engineering →Software testing and de-
bugging; •Computing methodologies →Neural networks.
∗Corresponding author.
ESEC/FSE ’20, November 8–13, 2020, Virtual Event, USA
© 2020 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-7043-1/20/11.
https://doi.org/10.1145/3368089.3409696KEYWORDS
Operational Calibration, Deep Neural Networks, Gaussian Process
ACM Reference Format:
Zenan Li, Xiaoxing Ma, Chang Xu, Jingwei Xu, Chun Cao, and Jian Lü.
2020. Operational Calibration: Debugging Confidence Errors for DNNs in
the Field. In Proceedings of the 28th ACM Joint European Software Engineer-
ing Conference and Symposium on the Foundations of Software Engineering
(ESEC/FSE ’20), November 8–13, 2020, Virtual Event, USA. ACM, New York,
NY, USA, 13pages. https://doi.org/10.1145/3368089.3409696
To know what you know and what you do not know, that is true
knowledge.
Confucius. 551 – 479 BC.
1 INTRODUCTION
Deep learning (DL) has achieved human-level or even better per-
formance in some difficult tasks, such as image classification and
speech recognition [ 13,23]. Deep Neural Network (DNN) models
are increasingly adopted in high-stakes application scenarios such
as medical diagnostics [ 36] and self-driven cars [ 3]. However, it is
not uncommon that DNN models perform poorly in practice [ 47].
The interest in the quality assurance for DNN models as integral
parts of software systems is surging in the community of software
engineering [19, 28,43,56,67,68].
A particular problem of using a previously well-trained DNN
model in an operation domain is that the model may not only make
more-than-expected mistakes in its predictions, but also give erro-
neous confidence values for these predictions. The latter issue is
particularly problematic for decision making, because if the confi-
dence values were accurate, the model would be at least partially
usable by accepting only high-confidence predictions. It needs to
be emphasized that erroneous predictions with high confidence are
especially damaging because users will take high stakes in them.
For example, an over-confident benign prediction for a pathology
image could mislead a doctor into overlooking a malignant tumor.
The problem comes from the almost inevitable divergences be-
tween the original data on which a model is trained and the actual
data in its operation domain, which is often called domain shift
ordataset shift [34] in the machine learning literature. It can be
901This work is licensed under a Creative Commons Attribution-NonCommercial
International 4.0 License.
ESEC/FSE ’20, November 8–13, 2020, Virtual Event, USA Z. Li, X. Ma, C. Xu, J. Xu, C. Cao, and J. Lü
difficult and go beyond the stretch of usual machine learning tricks
such as fine-tuning and transfer learning [ 38,63], because of two
practical restrictions often encountered. First, the training data
of a third-party DNN model are often unavailable due to privacy
and proprietary limitations [ 20,54,69]. Second, one can only use
a small number of labeled operation data because it could be very
expensive to label the data collected in the field. For example, in an
AI-assisted clinical medicine scenario, surgical biopsies may have
to be involved in the labeling of radiology images.
We consider operational calibration that corrects the error in
the confidence provided by a DNN model for its prediction on
each input in a given operation domain. It does not change the
predictions themselves but tells when the model works well and
when not. As the quantification of the intrinsic uncertainty in the
predictions made by a model, confidence values are integral parts
of the model’s outputs. So operational calibration can be viewed as
a kind of debugging activity that identifies and fixes errors in these
parts of model outputs. It improves the model’s quality of service in
the field with more accurate confidence for better decision making.
As a quality assurance activity, operational calibration shall be
carried out during the deployment of a previously trained DNN
model in a new operation domain. One can also incorporate it into
the system and excise it from time to time to adapt the model to
the evolving data distribution in the operation domain.
It is natural to model operational calibration as a case of non-
parametric Bayesian Inference and solve it with Gaussian Process
Regression [ 46]. We take the original confidence of a DNN model as
the prior, and gradually calibrate the confidence with the evidence
collected by selecting and labeling operation data. The key insight
into effective and efficient regression comes from two observations:
First, the DNN model, although suffering from the domain shift,
can be used as a feature extractor with which unlabeled operation
data can be nicely clustered [ 55,72]. In each cluster, the prediction
correctness of an example is correlated with another one. The
correlation can be effectively estimated with the distance of the
two examples in the feature space. Second, Gaussian Process is able
to quantify the uncertainty after each step, which can be used to
guide the selection of operation data to label efficiently.
Systematic empirical evaluations showed that the approach was
promising. It outperformed existing calibration methods in both
efficacy and efficiency in all settings we tested. In some difficult
tasks, it eliminated about 71% to 97% high-confidence errors with
only about 10% of the minimal amount of labeled operation data
needed for practical learning techniques to barely work.
In summary, the contributions of this paper are:
•Examining quality assurance for DNN models used as soft-
ware components, and raising the problem of operational
calibration as debugging for confidence errors of DNNs in
the field.
•Proposing a Gaussian Process-based approach to operational
calibration, which leverages the representation learned by
the DNN model under calibration and the locality of confi-
dence errors in this representation.
•Evaluating the approach systematically. Experiments with
various datasets and models confirmed the general efficacy
and efficiency of our approach.The rest of this paper is organized as follows. We first discuss
the general need for operational quality assurance for DNNs in
Section 2, and then define the the problem of operational calibration
in Section 3. We detail our approach to operational calibration in
Section 4 and evaluate it empirically in Section 5. We overview
related work and highlight their differences from ours in Section 6
before concluding the paper with Section 7.
2 QUALITY ASSURANCE FOR DNN MODELS
USED AS SOFTWARE ARTIFACTS
Well trained DNN models can provide marvelous capabilities, but
unfortunately their failures in applications are also very common [ 47].
When using a trained model as an integral part of a high-stakes
software system, it is crucial to know quantitatively how well the
model will work and to adapt it to the application conditions. The
quality assurance combining the viewpoints from software engi-
neering and machine learning is needed, but largely missing. In
what follows, we first discuss the non-conventional requirements
for such quality assurance, and then give an application scenario
to highlight the software engineering concerns.
Deep learning is intrinsically inductive [ 13,70]. However, con-
ventional software engineering is mostly deductive, as evidenced
by its fundamental principle of specification-implementation con-
sistency. A specification defines the assumptions and guarantees of
a software artifact. The artifact is expected to meet its guarantees
whenever its assumptions are satisfied. Thus explicit specifications
make software artifacts more or less domain independent. However,
statistical machine learning does not provide such kind of spec-
ifications. Essentially it tries to induce a model from its training
data, which is intended to be general so that the model can give
predictions on previously unseen inputs. Unfortunately the scope
of generalization is unspecified. As a result, a major problem comes
from the divergence between the domain where the model was
originally trained and the domain where it actually operates.
So the first requirement for the quality assurance of a DNN
model is to be operational , i.e., to focus on the concrete domain
where the model actually operates. Logically speaking, the quality
of a trained DNN model will be pointless without considering its
operation domain. In practice, the performance of a model may
drop significantly with domain shift [ 26]. On the other hand, fo-
cusing on the operation domain also relieves the DNN model from
depending on its original training data. Apart from practical con-
cerns such as protecting the privacy and property of the training
data, decoupling a model from its training data and process will
also be helpful for (re)using it as a commercial off-the-shelf (COTS)
software product [ 69]. This viewpoint from software engineering is
in contrasting to machine learning techniques dealing with domain
shift such as transfer learning or domain adaptation that heavily
rely on the original training data and hyperparameters [ 38,55,61].
They need original training data because they try to generalize the
scope of the model to include the new operation domain.
The second requirement is to embrace the uncertainty that is
intrinsic in DNN models. A defect, or a “bug”, of a software artifact
is a case that it does not deliver its promise. Different from con-
ventional software artifacts, a DNN model never promises to be
certainly correct on any given input, and thus individual incorrect
902Operational Calibration: Debugging Confidence Errors for DNNs in the Field ESEC/FSE ’20, November 8–13, 2020, Virtual Event, USA
predictions per se should not be regarded as bugs, but to some extent
features [ 17]. Nevertheless, the model statistically quantifies the
uncertainty of their predictions. Collectively, it is measured with
metrics such as accuracy or precision. Individually, it is stated by the
confidence value about the prediction on each given input. These
qualifications of uncertainty, as well as the predictions a model
made, should be subject to quality assurance. For example, given
a DNN model and its operation domain, operational testing [ 26]
examines to what degree the model’s overall accuracy is degraded
by the domain shift. Furthermore, operational calibration, which is
the topic of the current paper, identifies and fixes the misspecified
confidence values on individual inputs.
Finally, operational quality assurance should prioritize the saving
of human efforts, which include the cost of collecting, and espe-
cially labeling, the data in the operation domain. The labeling of
operation data often involves physical interactions, such as surgical
biopsies and destructive testings, and thus can be expensive and
time-consuming. Note that, as exemplified by the EU GDPR [ 8],
there are increasing concerns about the privacy and property rights
in the data used to train DNN models. So when adopting a DNN
model trained by a third party, one should not assume the availabil-
ity of its original training data [ 20,54,69]. Without the access to
the original training data, re-training or fine-tuning a DNN model
to an operation domain can be unaffordable because it typically
requires a large amount of labeled examples to work. Quality as-
surance activities often have to work under a much tighter budget
for labeling data.
Trained DNN ModelData SelectionLabelingOperationalQA  ActivitySelectedDatasetTraining DatasetUnlabeledOperationalDatasetDNNTraining Possible divergenceSize reducedDomain Shift
ModelwithQAIterationAssessments& Adaptations for Operation DomainTraining data/processmight be inaccessibleOperational quality assurance
Figure 1: Operational quality assurance
Figure 1 depicts the overall idea for operational quality assur-
ance, which generalizes the process of operational testing proposed
in [26]. A DNN model, which is trained by a third party with the
data from the origin domain, is to be deployed in an operation
domain. It needs to be evaluated, and possibly adapted, with the
data from the current operation domain. To reduce the effort of
labeling, data selection can be incorporated in the procedure with
the guidance of the information generated by the DNN model and
the quality assurance activity. Only the DNN models that pass the
assessments and are possibly equipped with the adaptations will
be put into operation.
For example, consider a scenario that a hospital decides to equip
its radiology department with automated medical image analysis
enabled by Deep Learning [ 36]. While the system may involve manyfunctionalities such as clinical workflow management, computer-
aided diagnosis, and computer-assisted reporting, the key compo-
nent is a DNN model or an ensemble of DNN models acting as a
radiologist to classify images [ 31]. It is too expensive and techni-
cally demanding for the hospital to collect enough high quality data
and train the model in-house. So the hospital purchases the model
from a third-party provider, and uses it as a COTS software com-
ponent. However, the training conditions of the model are likely
to be different from the operation conditions, and the model per-
formance reported by the provider is unreliable due to potential
issues such as data mismatch, selection bias, and non-stationary
environments [ 59,66]. To assure the system’s quality of service, the
model must be tested and calibrated against the current operation
domain. The hospital first assesses the real accuracy of the model
through operational testing [ 26], and decides to adopt it or not
accordingly. Once the model is adopted and deployed, operational
calibration steps in to adapt the model to the current operation
domain, by fixing the errors in the confidence values and avoiding
high-confidence false predictions.
As a software quality assurance task, operational calibration
tries to achieve best efficacy with a limited budget. The cost here,
however, is mainly spent on the labeling of operation data. For
sophisticated DNN models used for medical imaging classification
such as Inception-V3, ResNet-50 and DenseNet-121 [ 27,66], even
thousands of labeled data could be too less for model retraining or
fine-tuning to work properly (c.f. Figure 3). Nevertheless, as will be
shown later, a deliberately designed calibration method can identify
and fix most of the erroneous high confidence values associated to
false predictions with only tens to small hundreds of labeled data (c.f.
Figure 6). A model well calibrated for its operation domain, although
still suffering from some loss in prediction accuracy, becomes more
reliable in that broken promises are far less likely.
3 OPERATIONAL CALIBRATION PROBLEM
Now we focus on the problem of operational calibration. We first
briefly introduce DNN classifiers and their prediction confidence
to pave the way for the formal definition of the problem.
3.1 DNN Classifier and Prediction Confidence
A deep neural network classifier contains multiple hidden layers
between its input and output layers. A popular understanding [ 13]
of the role of these hidden layers is that they progressively ex-
tract abstract features (e.g., a wheel, human skin, etc.) from a high-
dimensional low-level input (e.g., the pixels of an image). These
features provide a relatively low-dimensional high-level representa-
tionzfor the input x, which makes the classification much easier,
e.g., the image is more likely to be a car if wheels are present.
What a DNN classifier tries to learn from the training data
is a posterior probability distribution, denoted as p(y|x)[2].
For a K-classification problem, the distribution can be written as
pi(x)=p(y=i|x), where i=1,2, . . . , K. For each input x
whose representation is z, the output layer first computes the non-
normalized prediction h=W⊤z+b, whose element hiis often
called the logit for the i-th class. The classifier then normalizes h
903ESEC/FSE ’20, November 8–13, 2020, Virtual Event, USA Z. Li, X. Ma, C. Xu, J. Xu, C. Cao, and J. Lü
with a softmax function to approximate the posterior probabilities
ˆpi(x)=softmax(h)i=ehi
ÍK
j=1ehj,i=1, . . . , K. (1)
Finally, to classify x, one just chooses the the category correspond-
ing to the maximum posterior probability, i.e.,
ˆy(x)=arg max
iˆpi(x). (2)
Obviously, this prediction is intrinsically uncertain. The confi-
dence for this prediction, which quantifies the likelihood of correct-
ness, can be naturally measured as the estimated posterior class
probability
ˆc(x)=ˆpi(x),i=ˆy(x). (3)
Confidence takes an important role in decision-making. For exam-
ple, if the loss due to an incorrect prediction is four times of the
gain of a correct prediction, one should not invest on predictions
with confidence less than 0.8.
Modern DNN classifiers are often inaccurate in confidence [ 57],
because they overfit to the surrogate loss used in training [ 14,58].
Simply put, they are over optimized toward the accuracy of classifi-
cation, but not the accuracy of estimation for posterior probabilities.
To avoid the potential loss caused by inaccurate confidence, confi-
dence calibration can be employed in the learning process [ 10,14,58].
Early calibration methods such as isotonic regression [ 65], his-
togram binning [ 65], and Platt scaling [ 45] simply train a regression
model taking the uncalibrated confidence as input with the valida-
tion dataset. More flexible methods, e.g., Temperature Scaling [ 16],
find a function Rto correct the logit hsuch that
ˆc(x)=ˆpi(x)=softmax(R(h))i,i=ˆy(x) (4)
matches the real posterior probability pi(x). Notice that, in this set-
ting the inaccuracy of confidence is viewed as a kind of systematic
error or bias, not associated with particular inputs or domains. That
is, the calibration does not distinguish between different inputs
with the same uncalibrated confidence. .
3.2 Operational Confidence Calibration
Given a domain where a previously trained DNN model is deployed,
operational calibration identifies and fixes the model’s errors in
the confidence of predictions on individual inputs in the domain.
Operational calibration is conservative in that it does not change
the predictions made by the model, but tries to give accurate estima-
tions on the likelihood of the predictions being correct. With this
information, a DNN model will be useful even though its prediction
accuracy is severely affected by the domain shift. One may take
only its predictions on inputs with high confidence, but switch to
other models or other backup measures if unconfident.
To quantify the accuracy of the confidence of a DNN model on
a dataset D={(xi,yi),i=1, . . . , N}, one can use the Brier score
(BS) [ 4], which is actually the mean squared error of the estimation:
BS(D)=1
NNÕ
i=1(I(xi)−ˆc(xi))2, (5)
where I(x)is the indicator function for whether the labeled input x
is misclassified or not, i.e., I(x)=1ifˆy(x)=y(x), and 0otherwise.
Now we formally define the problem of operation calibration:Problem. GivenMa previously trained DNN classifier, Sa set
ofNunlabeled examples collected from an operation domain, and a
budget n≪Nfor labeling the examples in S, the task of operational
calibration is to find a confidence estimation function ˆc(·)forMwith
minimal Brier score BS(S).
Notice that operational calibration is different from the confi-
dence calibration discussed in Section 3.1. The latter is domain-
independent and usually included as a step in the training process
of a DNN model (one of machine learning’s focuses), but the former
is needed when the model is deployed as a software component by a
third party in a specific operation domain (what software engineer-
ing cares about). Technically, operational calibration cannot take
the confidence error as a systematic error of the learning process,
because the error is caused by the domain shift from the training
data to the operation data, and it may assign different confidence
values to inputs with the same uncalibrated confidence value.
4 SOLVING OPERATIONAL CALIBRATION
WITH GAUSSIAN PROCESS REGRESSION
At first glance operational calibration seems a simple regression
problem with BS as the loss function. However, a direct regression
would not work because of the limited budget of labeled operation
data. It is helpful to view the problem in a Bayesian way. At the
beginning, we have a prior belief about the correctness of a DNN
model’s predictions, which is the confidence outputs of the model.
Once we observe some evidences that the model makes correct or
incorrect predictions on some inputs, the belief should be adjusted
accordingly. The challenge here is to strike a balance between the
priori that was learned from a huge training dataset but suffering
from domain shift, and the evidence that is collected from the
operation domain but limited in volume.
4.1 Modeling with Gaussian Process
It is natural to model the problem as a Gaussian Process [ 46], be-
cause what we need is actually a function ˆc(·). Gaussian Process
is a non-parametric kind of Bayesian methods, which convert a
prior over functions into a posterior over functions according to
observed data.
For convenience, instead of estimating ˆc(·)directly, we consider
h(x)=ˆc(x)−cM(x), (6)
where cM(x)is the original confidence output of Mfor input x. At
the beginning, without any evidence against cM(x), we assume that
the prior distribution of h(·)is a zero-mean normal distribution
h∼N(·| 0,k(·,·)), (7)
where k(·,·)is the covariance (kernel) function, which intuitively
describes the “smoothness” of h(x)from point to point. In other
words, the covariance function ensures that hproduces close out-
puts when inputs are close in the input space.
Assume that we observe a set of independent and identically
distributed (i.i.d.) labeled operation data I={(xi,yi)|1≤i≤n},
in whichy=h(x)=I(x)−cM(x). For notational convenience, let
X=(xT
1;. . .;xT
n)and
h=(h(x1);. . .;h(xn))
904Operational Calibration: Debugging Confidence Errors for DNNs in the Field ESEC/FSE ’20, November 8–13, 2020, Virtual Event, USA
be the observed data and their corresponding y-values, and let
X′=((x′
1)T;. . .;(x′
n)T),and
h′=(h(x′
1);. . .;h(x′
n))
be those for a set T={(x′
i,y′
i),i=1, . . . , m}of i.i.d. predictive
points. We have
h
h′
|X,X′∼N
0,KX X KX X′
KX′X KX′X′
(8)
where Kis the kernel matrix. Therefore, the conditional probability
distribution is
y′|y,X,X′∼N µ′,Σ′(9)
where
µ′=KX′X(KX X)−1y,
Σ′=KX′X′−KX′X(KX X)−1KX X′.
With this Gaussian Process, we can estimate the probability
distribution of the operational confidence for any input x′as follows
h(x′)|x′,X,h∼N(µ,σ), (10)
where
µ=Kx′X(KX X)−1h,
σ=Kx′x′−Kx′X(KX X)−1KX x′.
Then, with Equation 6, we have the distribution of ˆc(x′)
P ˆc x′|x′∼N cM x′+µ,σ. (11)
Finally, due to the value of confidence ranges from 0 to 1, we
need to truncate the original normal distribution [5], i.e.,
P ˆc x′|x′∼TN(µtn,σtn;α,β), (12)
where
µtn=cM x′+µ+ϕ(α)−ϕ(β)
Φ(α)−Φ(β)σ,
σ2
tn=σ2"
1+αϕ(α)−βϕ(β)
Φ(β)−Φ(α)−ϕ(α)−ϕ(β)
Φ(β)−Φ(α)2#
,
α=(0−cM x′−µ)/σ,β=(1−cM x′−µ)/σ.(13)
Here theϕ(·)andΦ(·)are the probability density function and the
cumulative distribution function of standard normal distribution,
respectively.
With this Bayesian approach, we compute a distribution, rather
than an exact value, for the confidence of each prediction. To com-
pute the Brier score, we simply choose the maximum a posteriori
(MAP), i.e., the mode of the distribution, as the calibrated confidence
value. Here it is the mean of the truncated normal distribution
ˆc(x)=µtn. (14)
4.2 Clustering in Representation Space
Directly applying the above Gaussian Process to estimate ˆc(·)would
be ineffective and inefficient. It is difficult to specify a proper co-
variance function in Equation 7, because the correlation between
the correctness of predictions on different examples in the very
high-dimensional input space is difficult, if possible, to model.
Fortunately, we have the DNN model Mon hand, which can be
used as a feature extractor, although it may suffer from the problem
of domain shift [ 1]. In this way we transform each input xfrom theinput space to a corresponding point zin the representation space,
which is defined by the output of the neurons in the last hidden layer.
It turns out that the correctness of M’s predictions has an obvious
locality, i.e., a prediction is more likely to be correct/incorrect if it
is near to a correct/incorrect prediction in the representation space.
Another insight for improving the efficacy and efficiency of the
Gaussian Process is that the distribution of operation data in the
sparse representation space is far from even. They can be nicely
grouped into a small number (usually tens) of clusters, and the cor-
relation of prediction correctness within a group is much stronger
than that between groups. Consequently, instead of regression with
a universal Gaussian Process, we carry out a Gaussian Process
regression in each cluster.
This clustering does not only reduce the computational cost of
the Gaussian Processes, but also make it possible to use different
covariance functions for different clusters. The flexibility makes
our estimation more accurate. Elaborately, we use the RBF kernel
k(z1,z2)=exp
−∥z1−z2∥2
2ℓ2
(15)
where the parameter ℓ(length scale) can be decided according to
the distribution of the original confidence produced by M.
4.3 Considering Costs in Decision
The cost of misclassification must be taken into account in real-
world decision making. We propose to also measure how well a
model is calibrated with the loss due to confidence error (LCE) against
a given cost model.
For example, let us assume a simple cost model in which the
gain for a correct prediction is 1 and the loss for a false prediction
isu. The net gain if we take action on a prediction for input xwill
beI(x)−u·(1−I(x)). We further assume that there will be no cost
to take no action when the expected net gain is negative. Then the
actual gain for an input xwith estimated confidence ˆc(x)will be
д(x)=I(x)−u·(1−I(x))ifˆc(x)≥λ,
0 ifˆc(x)<λ,(16)
whereλ=u
1+uis the break-even threshold of confidence for taking
action. On the other hand, if the confidence was perfect, i.e., ˆc(x)=1
if the prediction was correct, and 0 otherwise, the total gain for
dataset Dwould be a constant GD=ÍN
i=1I(xi). So the average LCE
over a dataset Dwith Nexamples is ℓ(D)=1
N
GD−ÍN
i=1д(xi)
.
With the Bayesian approach we do not have an exact ˆc(x)but a
truncated normal distribution of it. If we take µtn(x)asˆc(x), the
above equations still hold.
Cost-sensitive calibration targets at minimizing the LCE instead
of the Brier score. Notice that calibrating confidence with Brier
score generally reduces LCE. However, with a cost model, the opti-
mization toward minimizing LCE can be more effective and efficient.
4.4 Selecting Operation Data to Label
In case that the set of labeled operation data is given, we simply
apply a Gaussian Process in each cluster in the representation space
and get the posteriori distribution for confidence ˆc(·). However, if
we can decide which operation data to label, we shall spend the
budget for labeling more wisely.
905ESEC/FSE ’20, November 8–13, 2020, Virtual Event, USA Z. Li, X. Ma, C. Xu, J. Xu, C. Cao, and J. Lü
Initially, we select the operational input at the center of each
cluster to label, and apply a Gaussian Process in each cluster with
this central input to compute the posterior probability distribution
of the confidence. Then we shall select the most “helpful” input to
label and repeat the procedure. The insight for input selection is
twofold. First, to reduce the uncertainty as much as possible, one
should choose the input with maximal variance σ2
tn. Second, to
reduce the LCE as much as possible, one should pay more attention
to those input with confidence near to the break-even threshold λ.
So we chose x∗as the next input to label:
x∗=arg minx|µtn(x)−λ|
σtn(x). (17)
With x∗and its label y∗, we update the corresponding Gaussian
Process model and get better µtn(·)andσtn(·). The select-label-
update procedure is repeated until the labeling budget is used up.
Putting all the ideas together, we have Algorithm 1 shown below.
The algorithm is robust in that it does not rely on any hyperparam-
eters except for the number of clusters. It is also conservative in
that it does not change the predictions made by the model. As a
result, it needs no extra validation data.
Algorithm 1 Operational confidence calibration
Input: A trained DNN model M, unlabeled dataset Scollected
from operation domain D, and the budget nfor labeling inputs.
Output: Calibrated confidence function ˆc(x)forxbelongs to D.
Build Gaussian Process models:
1:Divide dataset SintoLclusters using the K-modroid method,
and label the inputs o1, . . . , oLthat correspond to the centers
of the Lclusters.
2:Initialize the labeled set T={o1, . . . , oL}.
3:For each of the clusters, build a Gaussian Process model дpi,
i=1, . . . , L.
4:while|T|<ndo
5: Select a new input ˜x∈S\Tfor labeling, where ˜xis searched
by Equation 17.
6: Update the Gaussian Process corresponding to the cluster
containing ˜x.
7: Update the labeled set T←T∪{˜x}.
8:end while
Compute confidence value for input x:
9:Find the Gaussian Process model ˆдpcorresponding to the clus-
ter containing input x.
10:Computeµtn(x)according to Equation 13.
11:Output the estimated calibrated confidence ˆc(x)=µtn(x).
4.5 Discussions
To understand why our approach is more effective than conven-
tional confidence calibration techniques, one can consider the three-
part decomposition of the Brier score [32]
BS=MÕ
m=1|Dm|
N(conf(Dm)−acc(Dm))2
−MÕ
m=11
N(acc(Dm)−acc)2+acc(1−acc),(18)where Dmis the set of inputs whose confidence falls into the in-
terval Im=
m−1
M,m
M, and the acc(Dm)and conf(Dm)are the
expected accuracy and confidence in Dm, respectively. The accis
the accuracy of dataset D.
In this decomposition, the first term is called reliability , which
measures the distance between the confidence and the true poste-
rior probabilities. The second term is resolution , which measures
the distinctions of the predictive probabilities. The final term is
uncertainty , which is only determined by the accuracy.
In conventional confidence calibration, the model is assumed
to be well trained and work well with the accuracies. In addition,
the grouping of Dmis acceptable because the confidence error is
regarded as systematic error. So one only cares about minimizing the
reliability. This is exactly what conventional calibration techniques
such as Temperature Scaling are designed for.
However, in operational calibration, the model itself suffers from
the domain shift, and thus may be less accurate than expected. Even
worse, the grouping of Dmis problematic because the confidence
error is unsystematic and the inputs in Dmare not homogeneous
anymore. Consequently, we need to maximize the resolution and
minimize the reliability at the same time. Our approach achieves
these two goals with more discriminative calibration that is based
on the features of individual inputs rather than their logits or con-
fidence values.
This observation also indicates that the benefit of our approach
over Temperature Scaling will diminish if the confidence error hap-
pens to be systematic. For example, in case that the only divergence
of the data in the operation domain is that some part of an image
is missing, our approach will perform similarly to or even slightly
worse than Temperature Scaling. However, as can be seen from
later experiments, most operational situations have more or less
domain shifts that Temperature Ccaling cannot handle well.
In addition, when the loss for false prediction uis very small
(u≤0.11, as observed from experiments in the next section), our
approach will be ineffective in reducing LCE. It is expected because
in this situation one should accept almost all predictions, even when
their confidence values are low.
5 EMPIRICAL EVALUATION
We conducted a series of experiments to answer the following
questions:
(1)Is our approach to operational calibration generally effective
in different tasks?
(2) How effective it is, compared with alternative approaches?
(3) How efficient it is, in the sense of saving labeling efforts?
We implemented our approach on top of the PyTorch 1.1.0 DL
framework. The code, together with the experimental data, are
available at https://github.com/Lizn-zn/Op-QA. The experiments
were conducted on a GPU server with two Intel Xeon Gold 5118
CPU @ 2.30GHz, 400GB RAM, and 10 GeForce RTX 2080 Ti GPUs.
The server ran Ubuntu 16.04 with GNU/Linux kernel 4.4.0.
The execution time of our operational calibration depends on
the size of the dataset used, and the architecture of the DNN model.
For the tasks listed below, the execution time varied from about
3.5s to 50s, which we regard as totally acceptable.
906Operational Calibration: Debugging Confidence Errors for DNNs in the Field ESEC/FSE ’20, November 8–13, 2020, Virtual Event, USA
5.1 Experimental Tasks
To evaluate the general efficacy of our approach, we designed six
tasks that were different in the application domains (image recogni-
tion and natural language processing), operation dataset size (from
hundreds to thousands), classification difficulty (from 2 to 1000
classes), and model complexity (from ∼103to∼107parameters).
To make our simulation of domain shifts realistic, in four tasks
we adopted third-party operation datasets often used in the trans-
fer learning research, and the other two tasks we used mutations
that are also frequented made in the machine learning community.
Figure 2 demonstrates some example images from the origin and
operation domains for tasks 3 and 5. Table 1 lists the settings of the
six tasks.
Table 1: Dataset and model settings of tasks
Task ModelOrigin Domain→Operation Domain
Dataset Acc. (%) Size∗
1 LeNet-5Digit recognition96.9→68.0 900(MNIST→USPS)
2 RNNPolarity99.0→83.4 1,000(v1.0→v2.0)
3 ResNet-18Image classification93.2→47.1 5,000CIFAR-10→STL-10
4 VGG-19CIFAR-10072.0→63.6 5,000(orig.→crop)
5 ResNet-50ImageCLEF99.2→73.2 480(c→p)
6Inception-v3ImageNet77.5→45.3 5,000(orig.→down-sample)
*It refers to the maximum number of operation data available for labeling.
In Task 1 we applied a LeNet-5 model originally trained with
the images from the MNIST dataset [ 24] to classify images from
theUSPS dataset [ 11]. Both of them are popular handwritten digit
recognition datasets consisting of single-channel images of size
16×16×1, but the latter is more difficult to read. The size of the
training dataset was 2,000, and the size of the operation dataset
was 1,800. We reserved 900 of the 1,800 operation data for testing,
and used the other 900 for operational calibration.
Task 2 was focused on natural language processing. Polarity
is a dataset for sentiment-analysis [ 39]. It consists of sentences
labeled with corresponding sentiment polarity (i.e., positive or neg-
ative). We chose Polarity-v1.0, which contained 1,400 movie reviews
collected in 2002, as the training set. The Polarity-v2.0, which con-
tained 2,000 movie reviews collected in 2004, was used as the data
from the operation domain. We also reserved half of the operation
data for testing.
In Task 3 we used two classic image classification datasets CIFAR-
10[21] and STL-10 [7]. The former consists of 60,000 32 ×32×3 im-
ages in 10 classes, and each class contains 6,000 images. The latter
has only 1,3000 images, but the size of each image is 96 ×96×3. We
used the whole CIFAR-10 dataset to train the model. The operation
domain was represented by 8,000 images collected from STL-10,
in which 5,000 were used for calibration, and the other 3,000 were
reserved for testing.Tasks 4 used the dataset CIFAR-100 , which was more difficult
than CIFAR-10 and contained 100 classes with 600 images in each.
We trained the model with the whole training dataset of 50,000
images. To construct the operation domain, we randomly cropped
the remaining 10,000 images. One half of these cropped images
were used for calibration and the other half for testing.
Task 5 used the image classification dataset from the Image-
CLEF 2014 challenge [ 30]. It is organized with 12 common classes
derived from three different domains: ImageNet ILSVRC 2012 (i),
Caltech-256 (c), and Pascal VOC 2012 (p). We chose the dataset (c)
as the origin domain and dataset (p) as the operation domain. Due
to the extremely small size of the dataset, we divided the dataset
(p) for calibration and testing by the ratio 4:1.
Finally, Task 6 dealt with an extremely difficult situation. Ima-
geNet is a large-scale image classification dataset containing more
than 1.2 million 224 ×224×3 images across 1,000 categories [ 9]. The
pre-trained model Inception-v3 was adopted for evaluation. The
operation domain was constructed by down-sampling 10,000 im-
ages from the original test dataset. Again, half of the images were
reserved for testing.
(a) CIFAR-10 (origin domain)
 (b) STL-10 (operation domain)
(c) ImageCLEF-(c) (origin domain)
 (d) ImageCLEF-(p) (operation domain)
Figure 2: Examples of origin and operation domains. In task 3
a ResNet-18 model was trained with low-resolution images (a), but
applied to high-resolution images (b). In task 5 a ResNet-50 model
was applied to images (d) with backgrounds and styles different
from training images (c).
5.2 Efficacy of Operational Calibration
Table 2 gives the Brier scores of the confidence before (col. Orig.)
and after (col. GPR) operational calibration. In these experiments all
operation data listed in Table 1 (not including the reserved test data)
were labeled and used in the calibration. The result unambiguously
confirmed the general efficacy of our approach. In the following we
elaborate on its relationship with the fine-tuning technique often
employed in practice.
907ESEC/FSE ’20, November 8–13, 2020, Virtual Event, USA Z. Li, X. Ma, C. Xu, J. Xu, C. Cao, and J. Lü
Table 2: Brier scores of different calibration methods
Task Model Orig.Operational calibration Conventional calibrationSARGPR RFR SVR TS PS-conf. PS-logit IR
1 LeNet-5 0.207 0.114 0.126 0.163 0.183 0.316 0.182 0.320 0.320
2 RNN 0.203 0.102 0.107 0.202 0.185 0.641 0.125 0.655 0.175
3 ResNet-18 0.474 0.101 0.121 0.115 0.387 0.471 0.254 0.529 0.308
4 VGG-19 0.216 0.158 0.162 0.170 0.217 0.359 0.253 0.364 0.529
5 ResNet-50 0.226 0.179 0.204 0.245 0.556 0.789 0.319 0.925 0.364
6 Inception-v3 0.192 0.161 0.167 0.217 0.191 0.546 0.427 0.334 -
Orig.–Before calibration. GPR–Gaussian Process-based approach. RFR–Random Forest Regression in the representation space. SVR–
Support Vector Regression in the representation space. TS–Temperature Scaling [ 14]. PS–Platt Scaling [ 45]. IR–Isotonic Regression
Scaling [ 65]. Conf./Logit indicates that the calibration took confidence value/logit as input. SAR–Regression with Surprise values [ 19].
We failed to evaluate SAR on task 6 because it took too long to run on the huge dataset.
5.2.1 Calibration when fine-tuning is ineffective. A machine learn-
ing engineer might first consider to apply fine-tuning tricks to
deal with the problem of domain shift. However, for non-trivial
tasks, such as our tasks 4, 5, and 6, it can be very difficult, if pos-
sible, to fine-tune the DNN model with small operation datasets.
Figure 3 shows the vain effort in fine-tuning the models with all
the operation data (excluding test data). We tried all tricks includ-
ing data augmentation, weight decay, and regularization to avoid
over-fitting but failed to improve the test accuracy.
Fortunately, our operational calibration worked quite well in
these difficult situations. In addition to the improvement in Brier
scores reported in Table 2, we can also see the saving of LCE for
task 4 in Figure 4 as an example. Our approach reduced about a half
of the LCE when λ>0.8, which indicates its capability in reducing
high confidence errors.
5.2.2 Calibration when fine-tuning is effective. In case of easier
situations that fine-tuning works, we can still calibrate the model to
give more accurate confidence. Note that effective fine-tuning does
not necessarily provide accurate confidence. One can first apply
fine-tuning until test accuracy does not increase, and then calibrate
the fine-tuned model with the rest operation data.
For example, we managed to fine-tune the models in our tasks
1, 2, and 3.1Task 1 was the easiest to fine-tune and its accuracy
kept increasing and exhausted all the 900 operational examples.
Task 2 was binary classification, in this case our calibration was
actual an effective fine-tuning technique. Figure 5a shows that our
approach was more effective and efficient than conventional fine-
tuning as it converged more quickly. For task 3 with fine-tuning
the accuracy stopped increasing at about 79%, with about 3,000
operational examples. Figure 5b show that, the Brier score would
decrease more if we spent rest operation data on calibration than
continuing on the fine-tuning.
Based on the significant (16.1%-78.6%) reductions in Brier scores
in all of the tasks reported in Table 2 and the above discussions, we
conclude that the Gaussian Process-based approach to operational cal-
ibration is generally effective, and it is worthwhile no matter whether
the fine-tuning works or not .
1Here we used some information of the training process, such as the learning rates,
weight decays and training epochs. Fine-tuning could be more difficult because these
information could be unavailable in real-world operation settings.5.3 Comparing with Alternative Methods
First, we applied three widely used calibration methods, viz.Temper-
ature Scaling (TS) [ 16], Platt Scaling (PS) [ 45], and Isotonic Regres-
sion (IR) [ 65], with the same operation data used in our approach.
TS defines the calibration function Rin Equation 4 as R(h)=h/T,
where Tis a scalar parameter computed by minimizing the nega-
tive log likelihood [ 15] on a validation dataset. PS and IR directly
work on the confidence values. PS trains a one-dimensional logistic
regression and calibrates confidence as ˆc=1/(1+e−a·cM+b), where
a,bare scalar parameters computed by minimizing the cross en-
tropy on a validation dataset. IR simply fits a monotonic confidence
calibration function minimizing the Brier score on a validation
dataset. Note that in our experiment, the operation data were used
instead of the validated dataset. We implemented TS according
to Guo et al . [14] . For PS and IR we used the well-known machine
learning library scikit-learn [42].
As shown in Table 2, TS, although reported to be usually the
most effective conventional confidence calibration method [ 14],
was hardly effective in these cases. It even worsened the confidence
in tasks 4 and 5. We observed that its bad performance came from
the significantly lowered resolution part of the Brier score, which
confirmed the analysis in Section 4.5. For example, in task 3, with
Temperature Scaling the reliability decreased from 0.196 to 0.138,
but the resolution dropped from 0.014 to 0.0. In fact, in this case
the calibrated confidence values were all very closed to 0.5 after
scaling. However, with our approach the reliability decreased to
0.107, and the resolution also increased to 0.154. The same reason
also failed PS (col. PS-Conf.) and IR (col. IR).
We also included in our comparison an improved version of PS,
which built a regression over the logit instead of the confidence [ 14].
Its calibration function Rfor Equation 4 was R(h)=WTh+b, where
Wandbwere computed by minimizing the cross entropy on the
operation data. It performed much better than the original PS but
still failed in tasks 4, 5 and 6.
Second, we also tried to calibrate confidence based on the Sur-
prise value that measured the difference in DL system’s behavior
between the input and the training data [ 19]. We thought it could
be effective because it also leveraged the distribution of examples in
the representation space. We made polynomial regression between
the confidence adjustments and the likelihood-based Surprise val-
ues. Unfortunately, it did not work for most of the cases (col. SAR
908Operational Calibration: Debugging Confidence Errors for DNNs in the Field ESEC/FSE ’20, November 8–13, 2020, Virtual Event, USA
0 20 40 60 80 100
Epoch5060708090100AccuracyTraining
Test
Original
(a) Task 4
0 20 40 60 80 100
Epoch5060708090100AccuracyTraining
Test
Original (b) Task 5
0 20 40 60 80 100
Epoch0102030405060Accuracy
Training
Test
Original (c) Task 6
Figure 3: Ineffective fine-tuning of difficult tasks. All available operation data (5000, 480, and 5000, respectively) were used.
Figure 4: loss due to confidence error
50 250 450 650 850 1050Sample size0.090.110.130.150.170.19Brier scoreFine-tuneCalibration
(a) Task 2
1000 2000 3000 4000 5000Sample size0.120.140.160.180.2Brier scoreFine-tuneCalibration
 (b) Task 3
Figure 5: Calibration when fine-tuning is effective
in Table 2 ). We believe the reason is that Surprise values are scalars
and cannot provide enough information for operational calibration.
Finally, to examine whether Gaussian Process Regression (GPR)
is the right choice for our operational calibration framework, we
also experimented with two standard regression methods, viz. Ran-
dom Forest Regression (RFR) and Support Vector Regression (SVR).
We used linear kernel for SVR and ten decision trees for RFR. As
shown in Table 2, in most cases, the non-liner RFR performed better
than the linear SVR, and both of them performed better than Tem-
perature Scaling but worse than GPR. The result indicates that (1)
calibration based on the features extracted by the model rather thanthe logits computed by the model is crucial, (2) the confidence error
is non-linear and unsystematic, and (3) the Gaussian Process as a
Bayesian method can provide better estimation of the confidence.
In summary, our GPR approach achieved significant Brier Score
reduction and outperformed conventional calibration methods and
the Surprise value-base regression in all the tasks. It also outper-
formed alternative implementations based on RFR and SVR. So
we conclude that our approach is more effective than Temperature
Scaling and other alternative choices for operational calibration .
5.4 Efficiency of Operational Calibration
In the above we have already shown that our approach worked
with small operation datasets that were insufficient for fine-tuning
(Task 4, 5, and 6). In fact, the Gaussian Process-based approach
has a nice property that it starts to work with very few labeled
examples. We experimented the approach with the input selection
method presented in Section 4.4. We focused on the number of
high-confidence false predictions, which was decreasing as more
and more operational examples were labeled and used.
We experimented with all the tasks but labeled only 10% of the
operation data. Table 3 shows the numbers of high-confidence false
predictions before and after operational calibration. As a reference,
we also include the numbers of high-confidence correct predictions.
We can see that most of the high-confidence false predictions were
eliminated. It is expected that there were less high-confidence cor-
rect predictions after calibration, because the actual accuracy of the
models dropped. The much lowered LCE scores, which took into
account both the loss in lowering the confidence of correct predic-
tions and the gain in lowering the confidence of false predictions,
indicate that the overall improvements were significant.
For a visual illustration of the efficiency of our approach, Fig-
ure 6 plots the change of proportions of high-confidence false and
correct predictions as the size of data used in calibration increases.
It is interesting to see that: (1) most of the high-confidence false
predictions were identified very quickly, and (2) the approach was
conservative, but the conservativeness is gradually remedied with
more labeled operation data used.
Note that for tasks 4, 5 and 6, usual fine-tuning tricks did not
work even with all the operation data labeled. With our operational
909ESEC/FSE ’20, November 8–13, 2020, Virtual Event, USA Z. Li, X. Ma, C. Xu, J. Xu, C. Cao, and J. Lü
Table 3: Reducing high-confidence false predictions with
10% operation data labeled
No. ModelλCorrect pred. False pred. LCE
1 LeNet-50.8 473→309.1 126→24.3 0.143→0.089
0.9 417→141.9 74→2.5 0.096→0.055
2 RNN0.8 512→552.9 118→39.9 0.162→0.091
0.9 482→261.3 106→12.0 0.132→0.070
3ResNet 0.8 1350→839.2 1372→59.7 0.370→0.054
-18 0.9 1314→424.0 1263→9.4 0.358→0.041
4 VGG-190.8 1105→392.5 583→46.9 0.127→0.070
0.9 772→142.8 280→9.3 0.074→0.038
5ResNet 0.8 53→26.9 16→5.2 0.162→0.136
-50 0.9 46→26.9 10→2.0 0.108→0.064
6Inception 0.8 1160→692.0 265→63.6 0.087→0.073
-v3 0.9 801→554.1 137→40.2 0.054→0.041
We ran each experiment 10 times and computed the average numbers.
(a) Task 1
 (b) Task 2
(c) Task 3
 (d) Task 4
(e) Task 5
 (f) Task 6
Figure 6: The proportion curve of high confidence inputs.
Sample size 0 means uncalibrated. The calibration started to take
effect with very few data.
calibration, using only about 10% of the data, we avoided about
97%, 80%, and 71% high-confidence ( >0.9) errors, respectively.Based on the results, we can say that the Gaussian Process-based
operational calibration is efficient in detecting most of the high-
confidence errors with a small amount of labeled operation data .
6 RELATED WORK
Operational calibration is generally related to the quality assurance
for deep learning systems in the software engineering commu-
nity, and the confidence calibration, transfer learning, and active
learning in the machine learning community. We briefly overview
related work in these directions and highlight the connections and
differences between our work and them.
6.1 Software Quality Assurance for Deep
Learning Systems
The research in this area can be roughly classified into four cate-
gories according to the kind of defects targeted:
•Defects in DL programs . This line of work focuses on the bugs in
the code of DL frameworks. For example, Pham et al .proposed to
test the implementation of deep learning libraries (TensorFlow,
CNTK and Theano) through differential testing [ 44]. Odena et al .
used fuzzing techniques to expose numerical errors in matrix
multiplication operations [37].
•Defects in DL models . Regarding trained DNN models as pieces of
software artifact, and borrowing the idea of structural coverage
in conventional software testing, a series of coverage criteria
have been proposed for the testing of DNNs, for example, Deep-
Xplore [ 43], DeepGauge [ 29], DeepConcolic [ 56], and Surprise
Adequacy [19], to name but a few.
•Defects in training datasets . Another critical element in machine
learning is the dataset. There exist research aiming at debugging
and fixing errors in the polluted training dataset. For example,
PSI identifies root causes (e.g., incorrect labels) of data errors
by efficiently computing the Probability of Sufficiency scores
through probabilistic programming [6].
•Defects due to improper inputs . A DNN model cannot well handle
inputs out of the distribution for which it is trained. Thus a defen-
sive approach is to detect such inputs. For example, Wang et al .’s
approach checked whether an input is normal or adversarial by
integrating statistical hypothesis testing and model mutation
testing [ 62]. Wang et al .proposed DISSECTOR, which effectively
distinguished unexpected inputs from normal inputs by verifying
progressive relationship between layers [ 60]. More work in this
line can be found in the machine learning literature under the
name of out-of-distribution detection [53].
For a more comprehensive survey on the testing of machine learn-
ing systems, one can consult Zhang et al. [67].
The main difference of our work, compared with these pieces
of research, is that it is operational , i.e., focusing on how well a
DNN model will work in a given operation domain. As discussed
in Section 2, without considering the operation domain, it is often
difficult to tell whether a phenomena of a DNN model is a bug or a
feature [17, 25].
An exception is the recent proposal of operational testing for
the efficient estimation of the accuracy of a DNN model in the
field [ 26]. Arguably operational calibration is more challenging and
more rewarding than operational testing, because the latter only
910Operational Calibration: Debugging Confidence Errors for DNNs in the Field ESEC/FSE ’20, November 8–13, 2020, Virtual Event, USA
tells the overall performance of a model in an operation domain,
but the former tells when it works well and when not.
6.2 DNN Confidence Calibration
Confidence calibration is important for training high quality classi-
fiers. There is a plethora of proposals on this topic in the machine
learning literature [ 10,14,33,35,65]. Apart from the Temperature
Scaling discussed in Section 3.1, Isotonic regression [ 65], Histogram
binning [ 64], and Platt scaling [ 45] are also often used. Isotonic
regression is a non-parametric approach that employs the least
square method with a non-decreasing and piecewise constant fitted
function. Histogram binning divides confidences into mutually ex-
clusive bins and assigns the calibrated confidences by minimizing
the bin-wise squared loss. Platt scaling is a generalized version of
Temperature Scaling. It adds a linear transformation between the
logit layer and the softmax layer, and optimizes the parameters
with the NLL loss. However, according to Guo et al ., Temperature
Scaling is often the most effective approach [14].
As discussed earlier in Section 4.5, the problem of these calibra-
tion methods is that they regard confidence errors as systematic
errors, which is usually not the case in the operation domain. Tech-
nically, these calibration methods are effective in minimize the
reliability part of the Brier score, but ineffective in dealing with the
problem in the resolution part.
In addition, Flach discussed the problem of confidence calibration
from a decision-theoretic perspective [ 10]. However, the confidence
error caused by domain shift was not explicitly addressed.
Previous research efforts on confidence calibration at prediction
time instead of training time are uncommon, but do exist. Gal and
Ghahramani proposed to build a temporary ensemble model by
using dropout at prediction time [ 12]. Despite its elegant Bayesian
inference framework, the method is computationally too expen-
sive to handle large-scale tasks. Recently, based on the insight of
conformal prediction [ 52], Papernot and McDaniel proposed to
build a k-Nearest Neighbors (kNN) model for the output of each
DNN immediate layer [ 40]. The confidence of a prediction was
estimated by the conformity of these kNNs’ outputs. Unfortunately,
this method is not applicable to non-trivial tasks either because of
its incapability in handling high-dimensional examples. Note that
these methods did not explicitly consider confidence errors caused
by domain shifts.
Another related line of work is under the name of uncertainty es-
timation [ 22,48,49]. For example, Evidential Deep Learning qualita-
tively evaluates the uncertainty of DNN predictions with a Dirichlet
distribution placed on the class probabilities [ 49]. However, these
methods mainly aim at out-of-distribution detection, which is con-
sidered easier than confidence calibration [ 10]. Identifying out-of-
distribution inputs is useful in defending against adversarial attacks,
but not directly helpful in adapting a model to a new operation
domain.
6.3 Transfer Learning and Active Learning
Our approach to operational calibration borrowed ideas from trans-
fer learning [ 38] and active learning [ 51]. Transfer learning (or
domain adaptation) aims at training a model from a source domain(origin domain in our terms) that can be generalized to a target do-
main (operation domain), despite the dataset shift [ 34] between the
domains. The key is to learn features that are transferable between
the domains.
However, transfer learning techniques usually require data from
both of the source and target domains. Contrastingly, operational
calibration often has to work with limited data from the operation
domain and no data from the origin domain. Transfer learning usu-
ally fails to work under this constraint. Even in case it works, it does
not necessarily produce well calibrated models, and operational
calibration is needed to correct confidence errors (cf. Figure 5b).
Active learning aims at reducing the cost of labeling training
data by deliberately selecting and labeling inputs from a large set
of unlabeled data. For the Gaussian Process Regression, there exist
different input selection strategies [ 18,41,50]. We tried many of
them, such as those based on uncertainty [ 50], on density [ 71],
and on disagreement [ 41], but failed to find a reliable strategy
that can further improve the data efficiency of our approach. They
were very sensitive to the choices of the initial inputs, the models,
and the distribution of examples [ 51]. However, we found that the
combination of cost-sensitive sampling bias and uncertainty can
help in reducing high-confidence false predictions, especially in a
cost-sensitive setting.
7 CONCLUSION
Software quality assurance for systems incorporating DNN models
is urgently needed. This paper focuses on the problem of opera-
tional calibration that detects and fixes the errors in the confidence
given by a DNN model for its predictions in a given operation do-
main. A Bayesian approach to operational calibration is given. It
solves the problem with Gaussian Process Regression, which lever-
ages the locality of the operation data, and also of their prediction
correctness, in the representation space. Experiments with repre-
sentative datasets and DNN models confirmed that the approach
can significantly reduce the risk of high-confidence false prediction
with a small number of labeled data, and thus efficiently improve
the models’ quality of service in operational settings.
While with empirical evidence, we consider conducting more
theoretical analysis on aspects such as the data efficiency and the
convergence of our algorithm as future work. In addition, we plan to
investigate operational calibration methods for real-world decisions
with more complicated cost models.
ACKNOWLEDGMENTS
We thank the anonymous reviewers for their suggestions. This work
is supported by the National Natural Science Foundation of China
(61690204, 61932021, 61802170) and the Collaborative Innovation
Center of Novel Software Technology and Industrialization.
REFERENCES
[1]Yoshua Bengio, Aaron C Courville, and Pascal Vincent. 2012. Unsupervised
feature learning and deep learning: A review and new perspectives. CoRR,
abs/1206.5538 1 (2012), 2012.
[2]Christopher M Bishop. 2006. Pattern recognition and machine learning . Springer,
New York, NY. http://cds.cern.ch/record/998831 Softcover published in 2016.
[3]Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat
Flepp, Prasoon Goyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai
Zhang, Xin Zhang, Jake Zhao, and Karol Zieba. 2016. End to End Learning
911ESEC/FSE ’20, November 8–13, 2020, Virtual Event, USA Z. Li, X. Ma, C. Xu, J. Xu, C. Cao, and J. Lü
for Self-Driving Cars. CoRR abs/1604.07316 (2016), 9. arXiv:1604.07316 http:
//arxiv.org/abs/1604.07316
[4]Glenn W Brier. 1950. Verification of forecasts expressed in terms of probability.
Monthly weather review 78, 1 (1950), 1–3.
[5]John Burkardt. 2014. The truncated normal distribution. , 32 pages. http:
//people.sc.fsu.edu/~jburkardt/presentations/truncatednormal.pdf
[6]Aleksandar Chakarov, Aditya Nori, Sriram Rajamani, Shayak Sen, and Deepak
Vijaykeerthy. 2016. Debugging machine learning tasks. arXiv preprint
arXiv:1603.07292 (2016), 23.
[7]Adam Coates, Andrew Ng, and Honglak Lee. 2011. An analysis of single-layer
networks in unsupervised feature learning. In Proceedings of the fourteenth inter-
national conference on artificial intelligence and statistics . aistats, 215–223.
[8] Council of European Union. 2014. Council regulation (EU) no 269/2014.
http://eur-lex.europa.eu/legal-content/EN/TXT/?qid=1416170084502&uri=
CELEX:32014R0269.
[9]Jia Deng, Wei Dong, Richard Socher, Li jia Li, Kai Li, and Li Fei-fei. 2009. Imagenet:
A large-scale hierarchical image database. In In CVPR . CVPR, 8.
[10] Peter A. Flach. 2016. Classifier Calibration . Springer US, Boston, MA, 1–8.
https://doi.org/10.1007/978-1-4899-7502-7_900-1
[11] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. 2001. The elements of
statistical learning . Vol. 1. Springer series in statistics New York.
[12] Yarin Gal and Zoubin Ghahramani. 2015. Dropout as a bayesian approximation:
Representing model uncertainty in deep learning. arXiv preprint arXiv:1506.02142
(2015), 12.
[13] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning . MIT
Press, New York, NY, USA. http://www.deeplearningbook.org.
[14] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017. On Calibration
of Modern Neural Networks. In Proceedings of the 34th International Conference
on Machine Learning - Volume 70 (Sydney, NSW, Australia) (ICML’17) . JMLR.org,
1321–1330. http://dl.acm.org/citation.cfm?id=3305381.3305518
[15] T. Hastie, R. Tibshirani, and J.H. Friedman. 2009. The Elements of Statistical
Learning: Data Mining, Inference, and Prediction . Springer. https://books.google.
com/books?id=eBSgoAEACAAJ
[16] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in
a neural network. arXiv preprint arXiv:1503.02531 (2015), 9.
[17] Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon
Tran, and Aleksander Madry. abs/1905.02175. Adversarial examples are not bugs,
they are features. arXiv preprint arXiv:1905.02175 0, 0 (abs/1905.02175), 0.
[18] Ashish Kapoor, Kristen Grauman, Raquel Urtasun, and Trevor Darrell. 2007.
Active learning with gaussian processes for object categorization. In 2007 IEEE
11th International Conference on Computer Vision . IEEE, IEEE, 1–8.
[19] Jinhan Kim, Robert Feldt, and Shin Yoo. 2019. Guiding Deep Learning System
Testing Using Surprise Adequacy. In Proceedings of the 41st International Confer-
ence on Software Engineering (Montreal, Quebec, Canada) (ICSE ’19) . IEEE Press,
Piscataway, NJ, USA, 1039–1049. https://doi.org/10.1109/ICSE.2019.00108
[20] Jakub Konečn `y, H Brendan McMahan, Felix X Yu, Peter Richtárik,
Ananda Theertha Suresh, and Dave Bacon. 2016. Federated learning: Strategies
for improving communication efficiency. arXiv preprint arXiv:1610.05492 10
(2016).
[21] Alex Krizhevsky, Geoffrey Hinton, et al .2009. Learning multiple layers of features
from tiny images . Technical Report. Citeseer.
[22] Volodymyr Kuleshov, Nathan Fenner, and Stefano Ermon. 2018. Accurate
uncertainties for deep learning using calibrated regression. arXiv preprint
arXiv:1807.00263 (2018), 9.
[23] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. Nature
521 (27 05 2015), 436 EP –. https://doi.org/10.1038/nature14539
[24] Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner, et al .1998. Gradient-
based learning applied to document recognition. Proc. IEEE 86, 11 (1998), 2278–
2324.
[25] Zenan Li, Xiaoxing Ma, Chang Xu, and Chun Cao. 2019. Structural Coverage
Criteria for Neural Networks Could Be Misleading. In Proceedings of the 41st
International Conference on Software Engineering: New Ideas and Emerging Results
(Montreal, Quebec, Canada) (ICSE-NIER ’19) . IEEE Press, Piscataway, NJ, USA,
89–92. https://doi.org/10.1109/ICSE-NIER.2019.00031
[26] Zenan Li, Xiaoxing Ma, Chang Xu, Chun Cao, Jingwei Xu, and Jian Lu. 2019.
Boosting Operational DNN Testing Efficiency through Conditioning. In Pro-
ceedings of the 27th ACM Joint European Software Engineering Conference and
Symposium on the Foundations of Software Engineering (ESEC/FSE ’19) . ACM,
Tallinn, Estonia, 12. http://arxiv.org/abs/1906.02533
[27] Yun Liu, Krishna Gadepalli, Mohammad Norouzi, George E. Dahl, Timo
Kohlberger, Aleksey Boyko, Subhashini Venugopalan, Aleksei Timofeev,
Philip Q. Nelson, Greg S. Corrado, Jason D. Hipp, Lily Peng, and Martin C.
Stumpe. 2017. Detecting Cancer Metastases on Gigapixel Pathology Images.
arXiv:1703.02442 [cs.CV]
[28] L. Ma, F. Juefei-Xu, M. Xue, B. Li, L. Li, Y. Liu, and J. Zhao. 2019. DeepCT:
Tomographic Combinatorial Testing for Deep Learning Systems. In 2019 IEEE
26th International Conference on Software Analysis, Evolution and Reengineering
(SANER) . ACM, New York, NY, USA, 614–618. https://doi.org/10.1109/SANER.2019.8668044
[29] Lei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Chunyang
Chen, Ting Su, Li Li, Yang Liu, Jianjun Zhao, and Yadong Wang. 2018. DeepGauge:
Multi-granularity Testing Criteria for Deep Learning Systems. In Proceedings of
the 33rd ACM/IEEE International Conference on Automated Software Engineering
(Montpellier, France) (ASE 2018) . ACM, New York, NY, USA, 120–131. https:
//doi.org/10.1145/3238147.3238202
[30] Henning Mller, Paul Clough, Thomas Deselaers, and Barbara Caputo. 2010. Image-
CLEF: Experimental Evaluation in Visual Information Retrieval (1st ed.). Springer
Publishing Company, Incorporated.
[31] Emmanuel Montagnon, Milena Cerny, Alexandre Cadrin-Chênevert, Vincent
Hamilton, Thomas Derennes, André Ilinca, Franck Vandenbroucke-Menu, Simon
Turcotte, Samuel Kadoury, and An Tang. 2020. Deep learning workflow in
radiology: a primer. Insights into Imaging 11, 1 (2020), 22. https://doi.org/10.
1186/s13244-019-0832-5
[32] Allan H. Murphy. 1973. A New Vector Partition of the Probability Score. Jour-
nal of Applied Meteorology 12, 4 (1973), 595–600. https://doi.org/10.1175/1520-
0450(1973)012<0595:ANVPOT>2.0.CO;2
[33] Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. 2015. Obtain-
ing well calibrated probabilities using bayesian binning. In Twenty-Ninth AAAI
Conference on Artificial Intelligence . AAAI, 7.
[34] Andrew Ng. 2016. Nuts and bolts of building AI applications using Deep Learning.
Neurips-Keynote (2016), 5.
[35] Alexandru Niculescu-Mizil and Rich Caruana. 2005. Predicting good probabilities
with supervised learning. In Proceedings of the 22nd international conference on
Machine learning . ACM, JMLR.org, 625–632.
[36] Ziad Obermeyer and Ezekiel J Emanuel. 2016. Predicting the future—big data,
machine learning, and clinical medicine. The New England journal of medicine
375, 13 (2016), 1216.
[37] Augustus Odena, Catherine Olsson, David Andersen, and Ian Goodfellow. 2019.
TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing. In
Proceedings of the 36th International Conference on Machine Learning (Proceed-
ings of Machine Learning Research, Vol. 97) , Kamalika Chaudhuri and Ruslan
Salakhutdinov (Eds.). PMLR, Long Beach, California, USA, 4901–4911. http:
//proceedings.mlr.press/v97/odena19a.html
[38] Sinno Jialin Pan and Qiang Yang. 2009. A survey on transfer learning. IEEE
Transactions on knowledge and data engineering 22, 10 (2009), 1345–1359.
[39] Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs Up? Sen-
timent Classification Using Machine Learning Techniques. In Proceedings of
EMNLP . EMNLP, 79–86.
[40] Nicolas Papernot and Patrick McDaniel. 2018. Deep k-nearest neighbors: Towards
confident, interpretable and robust deep learning. arXiv preprint arXiv:1803.04765
(2018), 18.
[41] Edoardo Pasolli and Farid Melgani. 2011. Gaussian process regression within an
active learning scheme. In 2011 IEEE International Geoscience and Remote Sensing
Symposium . IEEE, IEEE, 3574–3577.
[42] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M.
Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cour-
napeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine
Learning in Python. Journal of Machine Learning Research 12 (2011), 2825–2830.
[43] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. 2017. DeepXplore: Auto-
mated Whitebox Testing of Deep Learning Systems. In Proceedings of the 26th
Symposium on Operating Systems Principles (Shanghai, China) (SOSP ’17) . ACM,
New York, NY, USA, 1–18. https://doi.org/10.1145/3132747.3132785
[44] Hung Viet Pham, Thibaud Lutellier, Weizhen Qi, and Lin Tan. 2019. CRADLE:
cross-backend validation to detect and localize bugs in deep learning libraries.
InProceedings of the 41st International Conference on Software Engineering . IEEE
Press, ICSE’19, 1027–1038.
[45] John C. Platt. 1999. Probabilistic Outputs for Support Vector Machines and Com-
parisons to Regularized Likelihood Methods. In ADVANCES IN LARGE MARGIN
CLASSIFIERS . MIT Press, 61–74.
[46] Carl Edward Rasmussen and Christopher K. I. Williams. 2005. Gaussian Processes
for Machine Learning (Adaptive Computation and Machine Learning) . The MIT
Press.
[47] Patrick Riley. 2019. Three pitfalls to avoid in machine learning. Nature 572, 7767
(Jul 2019), 27–29. https://doi.org/10.1038/d41586-019-02307-y
[48] Peter Schulam and Suchi Saria. 2019. Can you trust this prediction? Auditing
pointwise reliability after learning. arXiv preprint arXiv:1901.00403 (2019), 10.
[49] Murat Sensoy, Lance Kaplan, and Melih Kandemir. 2018. Evidential deep learning
to quantify classification uncertainty. In Advances in Neural Information Processing
Systems . NeurIps, 3179–3189.
[50] Sambu Seo, Marko Wallat, Thore Graepel, and Klaus Obermayer. 2000. Gaussian
process regression: Active data selection and test point rejection. In Mustererken-
nung 2000 . Springer, 27–34.
[51] Burr Settles. 2009. Active learning literature survey . Technical Report. University
of Wisconsin-Madison Department of Computer Sciences.
[52] Glenn Shafer and Vladimir Vovk. 2008. A tutorial on conformal prediction.
Journal of Machine Learning Research 9, Mar (2008), 371–421.
912Operational Calibration: Debugging Confidence Errors for DNNs in the Field ESEC/FSE ’20, November 8–13, 2020, Virtual Event, USA
[53] Gabi Shalev, Yossi Adi, and Joseph Keshet. 2018. Out-of-distribution detection
using multiple semantic label representations. In Advances in Neural Information
Processing Systems . 7375–7385.
[54] Reza Shokri and Vitaly Shmatikov. 2015. Privacy-Preserving Deep Learning. In
Proceedings of the 22Nd ACM SIGSAC Conference on Computer and Communi-
cations Security (Denver, Colorado, USA) (CCS ’15) . ACM, New York, NY, USA,
1310–1321. https://doi.org/10.1145/2810103.2813687
[55] Rui Shu, Hung Bui, Hirokazu Narui, and Stefano Ermon. 2018. A DIRT-T Ap-
proach to Unsupervised Domain Adaptation. In International Conference on Learn-
ing Representations . ICLR, 19. https://openreview.net/forum?id=H1q-TM-AW
[56] Youcheng Sun, Min Wu, Wenjie Ruan, Xiaowei Huang, Marta Kwiatkowska,
and Daniel Kroening. 2018. Concolic Testing for Deep Neural Networks. In
Proceedings of the 33rd ACM/IEEE International Conference on Automated Software
Engineering (Montpellier, France) (ASE 2018) . ACM, New York, NY, USA, 109–119.
https://doi.org/10.1145/3238147.3238172
[57] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
Wojna. 2016. Rethinking the inception architecture for computer vision. In
Proceedings of the IEEE conference on computer vision and pattern recognition .
CVPR, 2818–2826.
[58] Ambuj Tewari and Peter L Bartlett. 2007. On the consistency of multiclass
classification methods. Journal of Machine Learning Research 8, May (2007),
1007–1025.
[59] Michael Wainberg, Daniele Merico, Andrew Delong, and Brendan J Frey. 2018.
Deep learning in biomedicine. Nature Biotechnology 36, 9 (2018), 829–838. https:
//doi.org/10.1038/nbt.4233
[60] Huiyan Wang, Jingwei Xu, Chang Xu, Xiaoxing Ma, and Jian Lu. 2020. DIS-
SECTOR: Input Validation for Deep Learning Applications by Crossing-layer
Dissection. In Proceedings of the 42st International Conference on Software Engi-
neering . IEEE Press, ICSE’20, 12.
[61] Jindong Wang et al .2019. Everything about Transfer Learning and Domain
Adapation. http://transferlearning.xyz.
[62] Jingyi Wang, Guoliang Dong, Jun Sun, Xinyu Wang, and Peixin Zhang. 2019.
Adversarial sample detection for deep neural network through model mutation
testing. In Proceedings of the 41st International Conference on Software Engineering .
IEEE Press, ICSE’19, 1245–1256.[63] Xuezhi Wang, Tzu-Kuo Huang, and Jeff Schneider. 2014. Active Transfer Learning
under Model Shift. In Proceedings of the 31st International Conference on Machine
Learning (Proceedings of Machine Learning Research, 2) , Eric P. Xing and Tony
Jebara (Eds.). PMLR, Bejing, China, 1305–1313. http://proceedings.mlr.press/v32/
wangi14.html
[64] Bianca Zadrozny and Charles Elkan. 2001. Obtaining calibrated probability
estimates from decision trees and naive Bayesian classifiers. Citeseer, Citeseer, 7.
[65] Bianca Zadrozny and Charles Elkan. 2002. Transforming classifier scores into ac-
curate multiclass probability estimates. In Proceedings of the eighth ACM SIGKDD
international conference on Knowledge discovery and data mining . ACM, KDD,
694–699.
[66] John R. Zech, Marcus A. Badgeley, Manway Liu, Anthony B. Costa, Joseph J.
Titano, and Eric Karl Oermann. 2018. Variable generalization performance of a
deep learning model to detect pneumonia in chest radiographs: A cross-sectional
study. PLOS Medicine 15, 11 (11 2018), 1–17. https://doi.org/10.1371/journal.
pmed.1002683
[67] Jie M. Zhang, Mark Harman, Lei Ma, and Yang Liu. 2019. Machine Learning
Testing: Survey, Landscapes and Horizons. CoRR abs/1906.10742 (2019), 35.
arXiv:1906.10742 http://arxiv.org/abs/1906.10742
[68] Mengshi Zhang, Yuqun Zhang, Lingming Zhang, Cong Liu, and Sarfraz Khurshid.
2018. DeepRoad: GAN-based Metamorphic Testing and Input Validation Frame-
work for Autonomous Driving Systems. In Proceedings of the 33rd ACM/IEEE
International Conference on Automated Software Engineering (Montpellier, France)
(ASE 2018) . ACM, New York, NY, USA, 132–142. https://doi.org/10.1145/3238147.
3238187
[69] Zhi-Hua Zhou. 2016. Learnware: On the Future of Machine Learning. Front.
Comput. Sci. 10, 4 (Aug. 2016), 589–590. https://doi.org/10.1007/s11704-016-6906-
3
[70] Zhi-Hua Zhou. 2019. Abductive learning: Towards bridging machine learning
and logical reasoning. Science China Information Sciences 62, 7 (2019), 76101.
[71] Jingbo Zhu, Huizhen Wang, Benjamin K Tsou, and Matthew Ma. 2009. Active
learning with sampling by uncertainty and density for data annotations. IEEE
Transactions on audio, speech, and language processing 18, 6 (2009), 1323–1331.
[72] Xiaojin Jerry Zhu. 2005. Semi-supervised learning literature survey . Technical
Report. University of Wisconsin-Madison Department of Computer Sciences.
913