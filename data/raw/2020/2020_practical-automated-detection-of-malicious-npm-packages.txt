Practical Automated Detection of Malicious npm Packages
Adriana Sejfia
sejfia@usc .edu
University of Southern California
Los Angeles, USAMax Schäfer
max-schaefer@github .com
GitHub
Oxford, UK
ABSTRACT
ThenpmregistryisoneofthepillarsoftheJavaScriptandType-
Script ecosystems, hosting over 1.7 million packages ranging from
simpleutilitylibrariestocomplexframeworksandentireapplica-
tions. Each day, developers publish tens of thousands of updates
as well as hundreds of new packages. Due to the overwhelming
popularityofnpm,ithasbecomeaprimetargetformaliciousactors,
who publish new packages or compromise existing packages to
introducemalwarethattamperswithorexfiltratessensitivedata
fromuserswhoinstalleitherthesepackagesoranypackagethat
(transitively)dependsonthem.Defendingagainstsuchattacksis
essential to maintaining the integrity of the software supply chain,
but the sheer volume of package updates makes comprehensive
manual review infeasible. We present Amalfi, a machine-learning
basedapproachforautomaticallydetectingpotentiallymalicious
packages comprised of three complementary techniques. We start
withclassifierstrainedonknownexamplesofmaliciousandbenign
packages. If a package is flagged as malicious by a classifier, wethen check whether it includes metadata about its source repos-itory, and if so whether the package can be reproduced from itssource code. Packages that are reproducible from source are not
usually malicious, so this step allows us to weed out false positives.
Finally, we also employ a simple textual clone-detection technique
toidentifycopiesofmaliciouspackagesthatmayhavebeenmissed
by the classifiers, reducing the number of false negatives. Amalfiimproves on the stateof the art in that itis lightweight, requiring
only a few seconds per package to extract features and run the
classifiers,andgivesgoodresultsinpractice:runningiton96287
package versions published over the course of one week, we were
abletoidentify95previouslyunknownmalwaresamples,witha
manageable number of false positives.
CCS CONCEPTS
•Security and privacy →Malware and its mitigation.
KEYWORDS
supply chain security, malware detection
This work is licensed under a Creative Commons Attribution International 4.0 
License.
ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
© 2022 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-9221-1/22/05.
https://doi.org/10.1145/3510003.3510104ACM Reference Format:
Adriana Sejfia and Max Schäfer. 2022. Practical Automated Detection of
Malicious npm Packages. In 44th International Conference on Software Engi-
neering(ICSE’22),May21–29,2022,Pittsburgh,PA,USA. ACM,NewYork,
NY, USA, 12 pages. https://doi .org/10 .1145/3510003 .3510104
1 INTRODUCTION
npm1is a system for publishing and consuming software packages
for JavaScript and TypeScript. While initially closely associated
with the Node.js platform2and back-end JavaScript applications, it
isnotarchitecturallytiedtoNode.js,andhasalsofoundwidespread
use with web applications and on other platforms.
The core concept of npm is the package registry, which is a
databaseofJavaScriptpackageswithassociatedmetadata.While
someorganizationsandenterpriseshosttheirownregistries,byfar
thebest-knownregistryisthepublicnpmregistry,accessiblevia
the npm website, which also provides facilities for browsing and
searchingforpackages,aswellasviewingtheirmetadata.Inthis
paper, we exclusively concern ourselves with the public registry.
AsofearlySeptember2021,npm’spackageregistryhostsover
1.7millionpackages.Someoftheseareprivatepackagesthatare
only accessible to specific users or organizations, but most of them
are public, and it is these public packages that are our focus. Over
the course of a single week, developers publish around 100,000
publicpackageversions,includingbothnewpackagesandupdated
versionsofexistingpackages.Historicversionsofapackageremain
availableontheregistryunlesstheyareexplicitlyremovedeither
by the package maintainer or by npm staff, allowing dependent
packagestorelyonspecificolderversionsofapackage,forexample
to make use of an API that has been removed in the latest version.
Most developers interact with the registry through a command-
line interface such as the npm CLI3or yarn,4which can be used to
download a particular version of an existing package and install it
locally,ortopublishanewpackageoranewversionofanexisting
package to the registry. When installing a package version, the
package manager will first recursively install the dependenciesofthatpackage(unlesstheyarealreadyinstalled);downloadthe
tarballcontainingthepackagefromtheregistry;unpackthetarballin the installation directory; and finally run any installation scripts
specifiedbythepackage.Thesescriptsarefree-formshellscripts
that typically perform setup tasks such as downloading additional
artifacts not bundled with the package itself. Due to the transitive
natureofpackageinstallation,popularpackagesaredownloaded
very frequently; for example, the chalkpackage5(which offers
1https://npmjs .com
2https://nodejs .org
3https://www .npmjs .com/package/npm
4https://www .npmjs .com/package/yarn
5https://www.npmjs.com/package/chalk
16812022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Adriana Sejfia and Max Schäfer
supportforcoloringterminaloutput)wasdownloadedalmost89
million times a week at the time of writing.
Publishing a new package or package version is the dual to this
process: anyone authenticated through the npm website can create
a new package, thereby becoming its maintainer, and maintainers
canpublishnewversionsatanytimebysimplyuploadingatarball
totheregistry.Whilepackagescanindicatearepositoryhosting
theirsourcecode,thisinformationisoptional.Itisthemaintainer’s
responsibility to run any necessary build steps (such as compiling
TypeScript to JavaScript, bundling and minifying, etc.) before pub-
lishing; the registry simply hosts the tarball and is largely agnostic
to its content.
Theoverwhelmingpopularityofnpmandthecentralroleitplays
in the software supply chain for JavaScript and TypeScript (which,
inturn,areamongthemostwidely-usedprogramminglanguagesat
present) has long made it a favorite target for attackers attempting
topublishmaliciouspackageversionsthattamperwithorexfiltrate
datafromthemachinestheyareinstalledon,performparasitical
computations such as Bitcoin mining, or other malicious activities.
Recent examples include high-profile incidents such as the
eslint-scope compromise,6where attackers managed to steal
thecredentialsofamaintainerofapopularpackage,allowingthem
to publish a new malicious version of the package that uploaded
user credentials to a server upon installation; the event-stream
backdoor,7where social-engineering techniques were used to gain
maintainer status and then launch a similar attack; and a steady
stream of smaller incidents since then.
The malicious package versions were quickly removed by
npm staff from the registry upon detection, but, in the case of
eslint-scope andevent-stream ,notbeforebeinginstalledsev-
eral million times. While the number of affected users is much
smaller in most cases, the frequency with which such incidents
occurstillposesasignificant dangertothesoftwaresupplychain,
both in terms of concrete damage to its users, and in terms of repu-
tationaldamagethatcouldpotentiallyimpedetheflourishingnot
just of npm but also the wider open-source ecosystem.
Addressing this problem at a fundamental level would arguably
requiresignificantchangestonpm,perhapsincludingamorese-
curepackage-publishingmodeltopreventmaliciouspackagesfrom
reaching the registry in the first place, and/or the Node.js platform,
perhaps with access-control enforcement to limit the damage a
malicious package can do when it is installed.
Our aim in this paper is more modest: without making any
changes to the fundamentals of npm, we want to detect potentially
malicious package versions as quickly as possible, and report them
to a human auditor for take-down.
Tobepracticallyuseful,then,ourapproachhastosatisfyatleast
three requirements: it has to be automated , since the sheer number
ofpackagesrendersmanualauditsinfeasible; efficienttokeepup
withthespeedat whichnewversionsarepublished;and accurate
to avoid flagging benign packages or missing malicious ones.
We achieve this by combining three complementary techniques
into one system, which we call Amalfi:8
6https://eslint .org/blog/2018/07/postmortem-for-malicious-package-publishes
7https://snyk .io/blog/a-post-mortem-of-the-malicious-event-stream-backdoor
8Short for “” Automated malicious package finder”.(1)machine-learning classifiers trained on labelled examples of
maliciousandbenignpackages,utilizingfeaturesthatrecord
changes in the APIs the package uses as well as package
metadata extracted using a lightweight syntactic scan;
(2)areproducer thatrebuildsapackagefromsourceandcom-
pares the result with the version published in the registry;
(3)aclonedetector thatfinds(near-)verbatimcopiesofknown
malicious packages.
Our feature selection, discussed in more detail below, is moti-
vated by the observation (borrowed from Garrett et al. [ 10]) that
maliciouspackagestendtomakeuseofdistinctivecapabilitiesof
the JavaScript language (such as runtime code generation), the un-
derlyingplatform(suchasaccesstothefilesystemorthenetwork),
and the npm package manager (such as install scripts). While none
of these features are dead giveaways by themselves, in combina-
tion they are worthy of closer inspection, especially if a package
suddenlystartsusingcapabilitiesithasneverusedbefore.Forex-
ample, the above-mentioned eslint-scope package uses runtime
code generation andan install scriptin its (malicious) version 3.7.2,
capabilities it had never used before.
Bytrainingonacorpusofmaliciousandbenignpackagespro-
vided to us by npm, our classifiers learn to distinguish typical (and
therefore most likely harmless) feature changes from atypical (and
therefore suspicious) ones. The choice of classifiers is constrained
by the small size of the corpus, which contains fewer than 2000
samples;weexperimentedwiththreedifferenttechniques:decision
trees, Naive Bayesian classifiers, and one-class SVMs.
Toeliminatefalsepositives,weborrowanotherinsightfromthe
literature [ 13,28,30]: malicious package versions tend not to have
theirsourcecodepubliclyavailable,inordertoavoiddetection.9
Consequently, being able to reproduce a package version from
its source code is a good indicator that it is benign. As has been
notedpreviously[ 13],evenperfectlybenignpackagesmayfailto
reproduceforavarietyofreasons,butthisisacceptableinourcase
since we are only using this criterion to filter out benign packages
erroneously flagged as malicious, not to detect new ones.
Finally,wenotethatattackersoftenpublishmultipletextually
identical copiesof oneand thesame maliciouspackage underdif-
ferentnames.However,sincepackagemetadatamaybedifferent,
our classifiers sometimes fail to spot these copies. We use a simple
clone detector that hashes the contents of a package (minus the
package name and version, which are always unique) to eliminate
this source of false negatives.
An overview of how these different components work together
can be seen in Figure 1.
Tomotivate ourapproachmore carefully,we showtwotypical
examplesofmaliciouspackagesinSection2anddiscusshowwe
detect them. Section 3 provides details on feature selection and
extractionfortheclassifier,aswellasanoverviewofthereproducer
andclonedetector.WeevaluateAmalfiinSection4inalarge-scale
experimentonnewlypublishednpmpackagestodemonstrateits
ability to find previously unknown malicious packages, and in a
cross-validation experiment to evaluate precision and recall. We
discuss the results in Section 5, survey related work in Section 6,
and outline conclusions and future directions in Section 7.
9In fact, we are not aware of a single counterexample to this rule.
1682Practical Automated Detection of Malicious npm Packages ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
Figure 1: Overview of Amalfi
In summary, the significance of our contributions is as follows:
•We present Amalfi, an automated approach for detecting
malicious npm packages that uses a novel combination of
techniquesandshowssolidresultsinpractice,identifying95
previously unknown malicious packages. Among the three
differentclassifiersweconsider,thedecisiontreeperforms
best, though the others also contribute findings.
•Amalfi is efficient, usually taking only a few seconds per
package to extract features and run the classifier. We also
showthatretrainingtheclassifiersischeap,thusallowing
continuous improvements to be made as more and more
results are triaged.
•Thefalse-positiverate,whileinitiallyquitehigh,dropssig-
nificantly as the classifiers are retrained on more data, with
fewer than one in a thousand packages being flagged spuri-
ously.Across-validationexperimentonourtrainingsetalso
showsthatthedecisiontreeachievesover40%recall,suggest-
ing that its false-negative rate is reasonable. Supplementary
materialsincludingexperimentaldataandresultsarepub-
liclyavailableathttps://doi .org/10 .5281/zenodo .5908852and
https://github .com/githubnext/amalfi-artifact.
While the building blocks we use have been proposed before,
thenoveltyofourapproachliesintheircombination,andamore
thorough exploration and evaluation of the design space.
2 BACKGROUND
To set the scene, we discuss two representative examples of real-
world malicious package versions that were (manually) detected
and removed from the registry, and then explain how we could
have identified them automatically.
Forthepurposesofthispaper,wedefinea maliciouspackagever-
sionto be a specific version of an npm package that contains code
thatimplementsmaliciousbehaviorincluding(butnotlimitedto)
exfiltratingsensitiveorpersonaldata,tamperingwithordestroying
data,orperforminglong-runningorexpensivecomputationsthat
are not explicitly documented. In particular, we consider a package
version to be malicious even if the malicious code it contains is
disabled or broken. Moreover, in line with npm’s Acceptable Con-
tentPolicy10weincludeinourdefinitionmaliciousbehaviorthatis
ostensiblydoneforresearchpurposes.Forbrevity,wewilloftenuse
the term “malicious package”, the “version” part being understood.
10https://docs .npmjs .com/policies/open-source-terms#acceptable-contentFromanattacker’sperspective,therearethreestepstodelivering
malware through npm: (1) publish a malicious package version; (2)
get users to install it; and (3) get them to run the malicious code.
Theeasiestwaytogoabout(1)istopublishacompletelynew
package.Aclassicwayofachieving(2)inthisscenariois typosquat-
ting[27] whereby the name chosen for the new package is very
similar to the name of a popular existing package; a user who acci-
dentallymisspellsthenameofthepopularpackagewillthenend
upinadvertentlyinstalling themaliciouspackageinstead. Amore
sophisticated approach is dependency confusion [5]: the attacker
identifiesdependenciesonapackagehostedinaprivateregistry,
and then publishes a malicious package with the same name and a
higher version number on the public npm registry; clients of the
private package may then end up installing the malicious package
instead.Finally,therehavebeencasesofattackerspublishingan
initially benign and useful package, getting it added as a depen-
dencytoapopulartargetpackage,andthenpublishingamalicious
version [4].
Analternative, morelaboriousstrategy toachieve (1)isfor the
attacker to compromise an existing popular package by gaining
maintaineraccess(forexamplebystealingmaintainercredentialsor
bysocialengineeringasdescribedinSection1),andthenpublishing
a new, malicious version of that package. In this case, (2) is easy
sincethepackagealreadyhasmanyuserswhowill(eitherexplicitly
or implicitly) upgrade to the malicious version.
Finally, a common tactic to achieve (3) in either scenario is to
useinstallationscriptswhich(asexplainedabove)arerunduring
installationandcanexecutearbitrarycode.However,thecommands
run by installation scripts are by default logged to the console,
increasingtheriskofdetection.Henceamorecarefulattackermay
instead choose to hide their malicious code in some frequently
executed bit of functionality in the main body of the package.
A typical example of a package employing typosquatting is
mogodb, a putative typo for the highly popular mongodbpackage,
whichiscurrentlyseeingaroundtwomillioninstallationsperweek.
Two versions of mogodb, numbered 3.1.8 and 3.1.9, were published
within less than a millisecond of each other on 1 August 2019, and
identified as malicious and taken down a few minutes later.
As shown in Figure 2 (a), the package.json manifest file of the
package registers a postinstall script to be run after package
installation, which executes the test.js script included in the
package.Thatscript,showninFigure2(b),harveststhehostnameof
themachineonwhichthepackagewasinstalled,andsendsitofftoa
remotehostcontrolledbytheattacker.Whiletheinformationbeing
stoleninthiscaseisnothighlysensitive,thisisclearlymalicious
behavior.
Even before studying the package implementation in detail, a
humanauditormightnoticefeaturesofthepackagethatmakeit
seemworthyofcloserscrutiny,suchasthepresenceofapostinstall
script,theusageofthepackages osandrequest,andmostofall
the extremelyshort timespan betweenthe publication ofthe two
versions. While the former two features are not by themselves
suspicious,theircombinationwitheachotherandwiththethird
feature strongly suggests a malicious package.
Atypicalexampleofacompromisedpackageis jasmin,aw eb
frameworkthatwasmoderatelypopularatonepointbuthasnot
seenactivedevelopmentinanumberofyears.Versions0.0.1and
1683ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Adriana Sejfia and Max Schäfer
1{
2 "name" :"mogodb" ,
3 "version" :"3.1.9" ,
4 "scripts" :{
5 "postinstall" :"node␣test.js" ,
6 ...
7},
8...
9}
(a)package.json
1 varremote = "https://attacker.controlled. host/";
2 varhost = require( "os").hostname();
3require( "request" )(remote + "?h="+ host, function () {});
(b)test.js
Figure 2: Malicious code in mogodb@3.1.9 (simplified)
1 varremote = "https://another.attacker.controlled. host/";
2 for(varform ofdocument.forms) {
3 for(varelement ofform.eleme nts) {
4 if(element.type == "password" ){
5 form.addEventListener( 'submit' ,function () {
6 vardata = [... this.elements].map( function (elt) {
7 return elt.name + ":"+ elt.value;
8 }).join() + "|"+ document.cookie;
9 varenc = encodeURIComponent( btoa(data));
10 this.action = remote + "?data=" + enc;
11 });
12 break;
13 }
14}
15}
Figure 3: Malicious code inserted into file component.js of
jasmin@0.0.3 (simplified)
0.0.2ofjasminarebenign,butversion0.0.3,presumablypublished
byamaliciousactor,containsthecodeshowninFigure3,which
traverses all forms contained in an HTML document looking for
passwordfields andoverridestheir submithandlertoharvest the
contentofthesefieldsandsendthemtoanattacker-controlledhost.
In this case, there are no particularly suspicious features of the
package that might draw the attention of a human auditor: dealing
with password fields, encoding data using encodeURIComponent
for transmission, and accessing HTTP cookies are all relatively
innocent capabilities, and are often used together. What is immedi-
atelysuspicious,however,isthatnoneofthesethreecapabilities
wereusedinthepreviousversionof jasmin.Moreover,theupgrade
from0.0.2to0.0.3isaminorversionupgrade,whereonewouldnot
expectmajornewfeaturesthatmightrequiresuchnewcapabilities
to be introduced.
These examples and others like them suggest that a machine-
learningbasedapproachmightbeabletodetectmaliciouspackages
based on high-level features like usage of particular APIs, platform
capabilities and package metadata, and in particular how these
featureschangebetweenversions,withouttheneedfordeepsource-
code analysis.3 OUR APPROACH
Having motivated what kind of features are interesting for auto-
matedclassification,wenowdescribeourfeaturesetinmoredetail,
andthenexplainhowtoextract single-versionfeatures describing
one package version as well as change features capturing the dif-
ference in features between two versions. Next, we discuss our
choice of classifiers and their training regimen. Finally, we give
somemoredetailsabouttheothertwocomponentsofourapproach,
the reproducer and the clone detector.
3.1 Feature set
Based on manual inspection of known examples of malicious pack-
ages,wedeterminedelevenfeaturesofinterest.Nineofthemare
single-version features that can be extracted from the contents of a
single package version, while the other two intrinsically involve
two versions of a package.
Thesingle-versionfeaturesareasfollows,wherewegroupre-
lated features into categories and provide examples of each:
(1)Access to personally-identifying information (PII): credit-
card numbers, passwords, and cookies
(2) Access to specific system resources
(a) File-system access: reading and writing files
(b) Process creation: spawning new processes
(c) Network access: sending or receiving data
(3) Use of specific APIs
(a) Cryptographic functionality
(b) Data encoding using encodeURIComponent etc.
(c) Dynamic code generation using eval,Function , etc.
(4) Use of package installation scripts
(5)Presenceofminifiedcode(toavoiddetection)orbinaryfiles
(such as binary executables)
The remaining two features concern two versions of a package,
and are thetime between publicationof the two versions,and the
typeofupdateinsemantic-versioningterms(major,minor,patch,
build, or pre-release).
The motivation for considering time between updates is that
malicious package versions often exhibit unusual update patterns,
such as multiple versions published in very rapid succession (as
seen in the mogodbexample in Section 2), or a new version be-
ing published after years of inactivity (which might suggest an
accounttakeover).Theupdatetype,ontheotherhand,candeter-
minewhether achange insomeother featureissuspicious ornot,
as explained above.
These two features, along with the changes in the values of the
ninesingle-valuedfeaturesbetweenonepackageversionandthe
previous version, constitute our feature set.
Inordertoaccommodatethefirstversionofapackageaswell,we
introduceapseudo-updatetyperepresentingfirstversions,consider
theirtimebetweenupdatestobezero,andtakethevaluesofthe
single-versionfeaturestobetheremainingchangefeatures.This
enablesustonotonlydetectmaliciousupdates,whereapreviously
benign package becomes malicious, but also packages that were
malicious from the start.
1684Practical Automated Detection of Malicious npm Packages ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
3.2 Feature extraction
Tocomputethefirstfourcategoriesofsingle-versionfeatures,we
parseeachJavaScriptandTypeScriptfileinthepackageusingTree-
sitter.11WethenuseTree-sitterASTqueriestolookforsyntactic
constructs corresponding to the features, such as string literals
containing the keyword password for PII access; imports of the fs
module for file-system access; and calls to evalandFunction for
dynamic code generation.
Similarly, to check for the presence of installation scripts we
parsethepackage.json fileandlookfordefinitionsof preinstall ,
install, andpostinstall properties.
Minifiedorbinaryfilestendtohavehigherentropythanplain
sourcecode,sowecomputetheShannonentropyofallfilescon-
tainedinthepackageandusethe averageandstandarddeviation
of the entropy across all files as features.
To compute change features, we use the publication timestamps
provided by the npm view time command to obtain the time
between updates in seconds. We rely on an off-the-shelf semantic-
versioninglibrarytodeterminetheupdatetypeand,foreachgiven
version, determine the previous version in chronological order.
Finally,wesimplysubtractthevaluesofthesingle-versionfeatures
across the two consecutive versions.
3.3 Classifier training
Ourchoiceofclassifiersisdictatedbythecorpusoflabelledtraining
datawehaveavailable.Sincemaliciouspackagesaretakendown
by npm immediately upon discovery, most known examples of
maliciouspackagesarenolongeravailableforinspection.However,
npm kindly agreed to make their archive of 643 malicious package
versionsdetectedupto29July2021availabletousforthepurposes
of this study. Out of these packages, 63 are malicious versions of
otherwisenon-maliciouspackages,i.e.,compromisedpackages.We
added to the original dataset the 1147 benign versions of the same
packagespublishedbythesamedate,yieldingabasiccorpusof1790
labelledsamplesofmaliciousandbenignpackageversions.Since
thegoalof Amalfiistodetectmaliciouspackages,thebasiccorpus
oversamples malicious packages, i.e., it contains more malicious
packages than we would expect from a similarly-sized random
sample of npm packages. This is a common strategy in learning-
based approaches [18].
It is worth emphasizing that while compromised packages have
a much bigger potential impact on the npm ecosystem, they occur
so rarely that there simply is not enough data to make them the
sole focus of our study. Anecdotally, however, compromised and
malicious packages use similar techniques to carry out attacks,
meaningthat theyshare features,whichenables Amalfi todetect
both types of malicious packages.
Sincethenumberofmalicioussamplesissmallercomparedto
the total number of package versions in our dataset and on npm,
we had to use learning algorithms that handle imbalanced data
well.Further,duetothenoveltyofthefeaturesinourapproach,we
soughtalearningalgorithmthatallowedustoanalyzetheimpor-
tanceofthefeaturesweselected.Intheend,thelearningalgorithms
thatsatisfiedtheconstraintsweredecisiontrees,NaiveBayesian
classifiers, and One-class Support Vector Machines (SVMs). We
11https://tree-sitter .github .io/tree-sitter/picked the first one due to its ability to explain which features
impactthefinaldecision,andthetwolatteronesbecauseoftheir
versatilitywhendealingwithimbalanceddatasetsasseeninanom-
aly detection work.
Totraintheclassifiers,weusethe sklearnlibraryforPython.
For the decision tree, we use information gain as the split criterion.
For the Naive Bayesian classifiers, we use the Bernoulli variant
which can only deal with Boolean features, so we omit the discrete
features (entropy average and standard deviation as well as update
time),andcollapsetheotherstoavalueof1ifthefeatureispresent,
and 0 otherwise. For the SVM, we choose a linear kernel and train
onlyonbenignexamples,sincethetaskofthisclassifieristodetect
outlier versions that are noticeably different from the benign ones.
Wedeterminedthe νparameteroftheSVM,whichapproximates
the number of expected outliers, by conducting a leave-one-out ex-
perimentonourbasiccorpus.Theexperimentshowedthatoptimal
precisionandrecallareattainedfora νvalueof0.001,meaningthat
theclassifier expectsabout oneina thousandpackage versionsto
be malicious.12
3.4 Reproducer and clone detector
AsexplainedinSection1,thereproducertakesagivenpackagever-
sion and then attempts to rebuild the package tarball from source.
This is a heuristic process that may fail for a variety of reasons:
whilepackagescanspecifytheURLoftheirsourcerepositoryin
theirpackage.json file, this information is optional and many
packagesdonotprovideit,ortherepositoryisnotpubliclyaccessi-
ble.PackageversionscanalsospecifythegitSHAofthecommit
they were built from, but again this information is optional. While
therearepopularconventionsforcreatingbranchesortagswith
names reflecting the package version they correspond to, many
packages do not follow these conventions, making it impossible
to determine the correct commit. The build commands to run to
produce the package from its source are likewise not prescribed.
Finally,manypackagesneglecttospecifythepreciseversionofthe
build tools(such asthe TypeScript compiler)they relyon, leading
to seemingly random differences between the reproduced package
and the original. For all of these reasons, the success rate of the
reproducer is low in practice as we shall see, but it still serves a
useful purpose as an automated false-positive filter.
Thethirdcomponentof Amalfiisasimpleclonedetectorthat
computes an MD5 hash of the contents of a package tarball and
compares it to a list of hashes of known malicious packages. When
computingthehash,weignorethepackagenameandversionspec-
ifiedinthe package.json file,sincethesearealwaysuniqueand
wouldcausespuriousmisses.Nootherattemptatfuzzymatching
is made, so only verbatim clones are detected.
4 EVALUATION
Whilemotivatingandpresentingthedetailsofourapproachabove,
we have informally argued that its design makes it practically use-
able anduseful. Wewill nowback upthese claimswith anexperi-
mentalstudy,whichaimstoanswerthefollowingthreeresearch
questions:
12Thesourcecodeofourclassifier-trainingscriptsandthelistofpackagesinthebasic
corpus are included in the supplementary materials.
1685ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Adriana Sejfia and Max Schäfer
Date # Versions Decision Tree Naive Bayes SVM Clones
#T P#F P #T P #F P #T P#F P #T P
July 29 23,452 34+1932-74 13+221453-107 20+5102-11 0
July 30 13,849 2+0 22-1 0+0 6-00+014-3 0
July 31 7,04217+0 16-1 0+0 1-018+9 4-0 0
August 1 6,050 1+0 6-21+0 3-20+013-0 0
August 2 13,562 2+1 17-0 4+1 12-10+010-0 1
August 3 15,269 6+0 15-2 1+0 9-10+041-3 0
August 4 17,063 16+2 9-11+0 9-11+010-1 17
Table 1: Results from Experiment 1; +ndenotes TPs contributed by the clone detector, −nFPs eliminated by the reproducer.
RQ1Does Amalfi find malicious packages in practice?
RQ2Is it accurate enough to be useful?
RQ3Is training and classification fast enough to be useable?
To answer these questions, we conducted two experiments: one
experimentonalargesetof newlypublishedpackageversionsto
assessperformance,andonesmallerexperimentonlabelleddatasets
to assess accuracy. We will describe the experiments below.
4.1 Experiment 1: Classifying newly published
packages
Thiswas alarge-scale experimentdesigned tosimulate arealistic
scenario for automated malware detection in which we applied
Amalfi to all new public package versions published on the public
npm registry over the course of a single, randomly chosen week
from 29 July 2021 to 4 August 2021.
On the first day, we trained our three classifiers on the basic
corpusandthenusedthemtoclassifytheset N1ofallnewpackage
versionspublishedthatday.Additionally,weranourclonedetector
on the same set to find copies of malicious packages in the basic
corpus, acting as a fourth classifier. This yielded a set P1⊆N1
of package versions flagged by at least one classifier. We ran the
reproduceronthissettoautomaticallyweedoutsomefalseposi-
tives, and manually inspected the rest. The manual inspection was
initiallyconductedbybothauthors,witheachauthorexamining
roughlyonehalfoftheflaggedversions.Thepackageversionsthat
were found to be malicious by one author were afterwards verified
by the other author. Finally, we reported the verified malicious
packages to the npm security team. All of them were subsequently
taken down, meaning that the npm security experts agreed with
ourassessment.Assuch,weareconfidentthatourmanuallabeling
of malicious packages is highly accurate.
The manual inspection resulted in a partitioning of P1into two
setsTP1andFP1of true positives (i.e., genuine malicious packages
foundbytheclassifiers)andfalsepositives(i.e.,benignpackages
falselyflaggedasmalicious).Asalaststep,werantheclonedetector
againtofindadditionalcopiesofpackagesin TP1thatweremissed
by the classifiers, and added them to TP1.
On the second day, we retrained the classifiers on the basic
corpusaswellastheset N1triagedthepreviousday,adding TP1
to our set of labelled malicious packages, and everything else (that
is,N1\TP1) to the set of benign packages. In other words, for
the purposes of this experiment we assumed that any package
notflagged by any of the classifiers was benign. This is not truein general, but the enormous number of new package versions
publishedeachdaymadeitinfeasibletoinspectthemall,andsince
weexpectthenumberofmaliciouspackagesonanygivendayto
be low it is not an unreasonable approximation to the unknown
ground truth.
Asonthefirstday,wethenappliedtheclassifierstotheset N2of
packagespublishedthatday,ranthereproducerontheresultingset
P2, manually inspected the rest, and ran the clone detector to mop
upanythingthatwasmissed,yieldingaset TP2ofnewlyidentified
maliciouspackages.Onthethirdday,weretrainedtheclassifiers
using the basic corpus as well as both N1anN2, and so forth for
each subsequent day.13
The intuition here is that we want to mimic a usage pattern
whereresultsfromtheclassifiersareinspectedbyahumanauditor,
and the classifiers are then retrained with the additional ground
truth obtained in this way.
4.2 Experiment 2: Classifying labelled data
Whilethefirstexperimentcanprovideinsightintotheperformance
ofourapproachunderreal-worldconditionsandinparticularits
false-positive rate, it cannot tell us much about false negatives.
Henceweranasecondexperiment,measuringtheprecisionand
recall of Amalfi on the basic corpus. Its small size prevented us
from separating it in a train-and-test fashion, so instead we per-
formed a10-fold cross validationexperiment, repeatedlytraining
theclassifierson90%ofthecorpusandmeasuringprecisionand
recallontheremaining10%.Giventheimbalanceinourdataset,we
usedstratifiedsamplingtomaintainthedistributionofmalicious
and benign versions for each fold.
Furthermore,wealsomeasuredprecisionandrecallonalabelled
datasetfromrecentworkbyDuanetal.describingtheirMalOSS
system [8]. This dataset had some overlap with our basic corpus
which we removed, leaving only the unique data points for this
experiment. Also, the dataset initially only contained malicious
packages; to balance it, we followed the same strategy as for the
basiccorpusandaddedallbenignversionsofthecontainedpack-
ages.Intheend,thisyieldedadatasetwith372packageversions,
out of which 40 were malicious.14
Basedontheresultsfromthesetwoexperiments,wewillnow
answer the research questions posed above.
13Thelistofpackagesconsideredinthisexperimentandtheresultsoftheclassification
are included in the supplementary materials.
14Detailed results for this experiment are included in the supplementary materials.
1686Practical Automated Detection of Malicious npm Packages ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
4.3 RQ1: Practical performance on newly
published packages
The results of the first experiment are presented in Table 1. The
tablecontainsthedateforwhichwecollectedthepackageversions
(Date) as well as the total number of versions published on that
date(#Versions ).Then,foreachclassifier,thetablecontainsthe
numberoftruepositives( #TP)andfalsepositives( #FP)flaggedby
theclassifier,annotatedwiththenumberofadditionaltruepositives
found by the clone detector ( +n) and the number of false positives
eliminated by the reproducer ( −n). As explained above, all true
positives were confirmed by the npm security team.
Thus,forexample,theentry16 +2inthe#TPcolumnforthe
decision tree onAugust 4 means that the classifier flagged 16 true
positive among the 17,063 package versions published that day of
which the clone detector found two additional copies. The entry
9−1 in the#FPcolumn means that among the nine false positives
itflagged,onewassuccessfullyreproducedandhenceeliminated
automatically.
Asexplainedabove,theclonedetectorisalsotreatedlikeafourth
classifier.Ithasnofalsepositivesandnevermissesidenticalcopies,
hence this column only contains a single number per day. Note
again that in this column we show the number of clones found
on that same day, as opposed to the entries after the +sign which
depict the number of clones from the previous days.
The first takeaway from this table is that the number of new
packagespublishedeverydayishigh,butquitevariable,withal-
most four times as many packages being published on July 29 (a
Thursday) than on August 1 (a Sunday).
Secondly, we can see that all our classifiers are able to correctly
classifymaliciouspackageversions,withvaryingdegreesofsuccess.
The decision tree performs better than the rest, especially in terms
of true positives. Removing the overlap between classifiers, we
wereabletoidentify95previouslyunknownmaliciouspackages
over the course of these seven days, which is a significant number,
especially considering that the entire set of malicious packages
detected prior to our work only contained 643 samples.
Third,wenoticethatonthefirstdayallthreeclassifiersproduce
anunmanageablenumberofresults.Wethereforehadtomodify
ourapproachandonlyexaminedasubsetofallflaggedpackages
indetail,assumingalltheresttobefalsepositives.Thismeansthat
the false-positive counts for this day are likely to be overstated.
However, oncethisset ofpackagesisadded tothetraining seton
thesecondday,thenumberofresultsdropsdramatically,andbythe
endoftheweekallthreeclassifiersyieldarelativelylownumber
of false positives.
Fourth,ourresultsshowthatthereproducerhasalowsuccess
rate in practice, only being able to reproduce one or two packages
on any given day. However, given the overall low number of alerts
towards the end of the week this is still a valuable improvement.
Similarly, clone detection only contributes a few additional true
positives each day (the 17 packages detected on August 4 being an
outlier, and mostly overlapping with the results from the decision
tree), but it still improves the overall results. Both mechanisms are
computationallyinexpensive,andthusthehelptheyprovidecomes
at a low cost, making them worth keeping in spite of their limited
contributions. Conversely, this shows that our classifiers add valuebeyondapurelytextualscanlookingforverbatimcopiesofknown
malware.
In summary, we can answer RQ1 in the affirmative: Amalfi
does indeed detect malicious packages in practice. Further, since
all the packages found by Amalfi and reported to npm had not
been identified before, we can confidently claim that our approach
complements existing solutions for malicious package detection in
npm.
Dataset Decision Tree Naive Bayes SVM
Prec. Recall Prec. Recall Prec. Recall
Basic 0.98 0.43 0.90 0.19 (0.98) (0.27)
MalOSS 0.35 0.64 0.62 0.64 0.73 0.61
Table 2: Results from Experiment 2
4.4 RQ2: Accuracy
The results from Experiment 2 are presented in Table 2.
The first row shows precision and recall measurements from
the 10-fold cross-validation experiment on our basic corpus, aver-
aged over all ten runs. The numbers for the SVM classifier have to
be interpreted with care, since its νparameter was fitted on this
verydataset,hencewehaveputtheminbrackets.Allourmodels
achieveveryhighprecision,buttherecallofNaiveBayesandSVM
issomewhatpoor.Thisisexpectedduetothelowpriorofmalicious
packages.
ThesecondrowshowsprecisionandrecallfromrunningAmalfi
on the MalOSS dataset derived from the literature [ 8] as explained
above. We see that the recall is higher than with the previous row,
at the expense of precision. These results point to the trade-off
betweenthesetwometrics,butitisalsoworthpointingoutthatthe
MalOSSdatasetcontainsanumberofpackageslabeledasmalicious
where npm disagreed with the authors’ assessment and did not
take them down.
A more detailed comparison of Amalfi to MalOSS is unfortu-
nately not possible since they do not present statistics on false
positives or performance. The heavy-weight nature of their ap-
proachanditscomplicatedsetupinvolvingasophisticatedpipeline
combining static and dynamic components made it infeasible to
run on our dataset.
Based on these results and the false-positive numbers discussed
above, we can give a cautiously positive answer to RQ2: Amalfi is
reasonablypreciseanddoesnotproduceanoverwhelmingnumber
ofresults,makingmanualtriagingofresultsbyahumanauditor
feasible. The second experiment suggests that there may be a good
numberoffalsenegatives,butatleastthenumbersforthedecision
tree look promising.
4.5 RQ3: Performance
To characterize the performance of our approach, we measured
threemetrics:(i)thetimeittakestotraintheclassifier,(ii)thetime
it takes to extract features for a package version, and (iii) the time
it takes to classify a package version. All three measurements were
obtained as a byproduct of Eperiment 1.
1687ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Adriana Sejfia and Max Schäfer
Figure 4: Classifier training time
Figure 4 shows how long it took to train the classifiers for each
dayofourexperimentasafunctionofthesizeofthetrainingset.
As can be seen, training is quite fast, taking no more than a few
secondsdespitethesteadyincreaseintrainingdata.TheSVM-based
classifier takes the longest time, though it still seems to scale more
orlesslinearlyinthetraining-setsize.Theothertwoclassifiersare
very quick to train,and it is clear that the training set sizecould be
increased substantially before training time becomes a bottleneck.
Tobenchmarkfeatureextraction,wepost-processedlogsfrom
our run of Experiment 1 to measure the time it takes to extract
features for a randomly chosen set of around 500 package versions.
By and large, feature extraction takes less than ten seconds, and
for over half the packages considered it takes less than one second.
However,asingleoutlierpackagecontainingmorethan11,000files
takes more than ten minutes to extract, somewhat skewing the
distribution for an average extraction time of six seconds.
Lastly, we measured the time it took to predict whether a given
packagewasmalicious.Foralltheclassifiers,thetimeforprediction
was less than a second.
Based on these results, we can give a positive answer to RQ3:
Amalfi is fast enough for practical use.
4.6 Threats to validity
Whiletheresultsofourevaluationareoverallverypromising,there
are some threats to the validity of our conclusions.
First,whilethesetofpackagesweconsideredinExperiment1
was taken from the wild, it may have been biased in ways that we
did not anticipate, and so our results may not generalize. Also, the
basic corpus is to some degree biased in that it contains clusters of
similar malware samples resulting from copy-cat campaigns.
Second, while we examined all packages flagged by Amalfi
and reportedthe true positivesto npm, welimited ourselves toat
mostfiveminutes’inspectiontimeperpackage,whichprevented
detailed investigation of some of the larger ones and may have
caused us to miss true positives. Conversely, we may have been
mistakeninlabellingsomepackagesmalicious,leadingtomissed
false positives, butthis seems unlikely considering thatnpm have
takenallreportedpackagesdown,meaningthattheyagreewith
our assessment.Finally, as noted above in our retraining step in Experiment 1
we assumed packages that were not flagged by any classifier to be
benign.Thisisnotasoundassumptioningeneral,andmightendup
increasing the number of false negatives over time. For this reason,
and also to escape the slow but inexorable rise in training time
suggested by Figure 4, in practice one would not want to continue
retraining in this fashion indefinitely. Table 1 suggests diminishing
returns from retraining after a few days, but the data is clearly too
sparse to draw a definite conclusion.
5 DISCUSSION
Inthissectionwereviewthetypesofmaliciouspackagesourmodels
found,takeacloserlookatthemodelsthemselves,andfinallytouch
upontweakstoourapproachweinvestigatedbutwereshownto
be unsuccessful or unnecessary.
Table 3 details the types of discrete features exhibited by the 95
maliciouspackageswefound,whileFigure5showsthedistribution
of the entropy and time features of malicious and benign packages
using boxplots.
All packages used installation scripts or code in their main mod-
uletoconnecttoaremotehost,withalmostallofthemsendingPII
tothathostexceptforasmallhandfulofpackagesthatonlypinged
thehostwithoutsendinganyinformation,perhapsasaproof-of-
conceptorinpreparationforanactualattack.Ourfeatureextractor
didnotdetectthePIIaccessesthemselves(pointingtoaneedtoim-
proveourdetectionofthisfeature),butthepackagesweredetected
anyway, usually because of the usage of the installation scripts or
network access. This suggests that our approach is robust enough
tofindvarioustypesofmaliciouspackageversions.Thefactthat
some features do not appear in these specific packages does not
necessarilyindicatetheyareuselessorunnecessary;thosefeatures
attempttopaintageneralpictureofmaliciousnessinpackagesand
they may prove to be useful in other batches.
A surprising observation in the data was the distribution of up-
datetypes:themajorityofthemaliciouspackageversionswefound
weremajorupdates,contradictingourassumptionthatmalicious
package updates tendto “hide” behind aminor update. This could
also mean that we missed malicious package versions representing
a minor update.
Thedistributionofaverageentropyvaluesshowsthattheme-
dian, highlighted by the thick redbar, is significantly higher for
Feature # of packages
File-system access 11
Process creation 1
Network access 10
Data encoding 1
Use of package installation scripts 33
Update type: major 52
Update type: minor 1
Update type: patch 3
Update type: prerelease 9
Update type: first 30
Table 3: Features found in the 95 malicious packages
1688Practical Automated Detection of Malicious npm Packages ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
maliciouspackages(4.69)thanforbenignpackages(0.001),suggest-
ing they are more likely to contain minified code or binary files, in
line with our expectation. Thetime betweenupdatesalso follows a
rather expected distribution, with malicious packages exhibiting a
shortermedian timebetween updates(7.02s) thanthebenignones
(2217.18 s).
Figure 5: Entropy (left) and time (right) value distributions
Next,wetookalookatthegeneratedclassifiersandhowtheir
predictionsoverlap.Figure6showsasummaryoftheresultsonthe
90 packages flagged by the classifiers (the remaining five having
beenflaggedonlybytheclonedetector).Whilethedecisiontree
takes the lion’s share, each individual algorithm makes its own
contribution, suggesting that a combination of all three might be a
good choice in practice.
Sincethedecisiontreeclassifiersaretheonesthatfacilitateinter-
pretationwetookalookatthefeaturestheyusetomakedecisions.
We noticed that the classifiers for July 29 to July 31 examine all
features except the one representing uses of cryptographic func-
tionality,andtheremainingfourclassifiersfromAugust1onwards
employ all eleven features, suggesting that there is not much re-
dundancy in our feature set.
Figure6:Overlapamongthethreedifferentclassifiersonthe
90 malicious packages they flagLastly, we tried out several tweaks that ultimately did not prove
successful. The literature often recommends using Random For-
est classifiers instead of plain decision trees, but we did not find
them to provide any advantage in our setting. We also investigated
booleanizing features for the decision tree and one-class SVM, but
in both cases this led to worse performance, in the latter case in-
creasing the rate of false positives by more than 100%.
6 RELATED WORK
Ourworkhasconnectionswithfourdifferentresearchareas,which
we survey briefly: malicious package detection proper; malware
and anomaly detection more generally; package-registry security;
and security implications of code reuse.
Malicious-packagedetection. Previousworkinthisareacanbe
broadly divided into four categories: general-purpose malicious-
package detection approaches using machine learning [ 10]o rp r o -
gram analysis [ 8,21,23]; techniques for rebuilding packages from
source [13,28,30]; and finally work that specifically targets ty-
posquatting [26, 31].
Garret et al.’s work on detecting malicious npm packages using
machine-learning techniques [ 10] is very closely related to our
work. They use a k-means clustering algorithm to identify anoma-
lous, and hence suspicious, package updates.
Likeus,theycollectfeaturesforpackageupdates,notjustsingle
package versions, and the set of features they consider overlaps to
some extent with ours, as shown in Table 4. In particular, they also
consider access to system resources, dynamic code generation, and
use of installation scripts. We additionally consider access to PII
and several specific APIs as features, which they do not, though
to some extent this is covered by their feature recording added
dependencies. Their feature set also does not directly model the
presenceofminifiedcodeorbinaryfiles,thoughagaintheydohave
a more general feature for added code that is similar in spirit. They
do not consider update type a feature, instead accounting for their
different characteristics by training separate models for each type
ofupdate.Finally,theydonothaveanyfeaturecorrespondingto
our time between updates.
Our work is larger in scope than theirs, considering three differ-
ent kinds of classifiers instead of just one, and complementing the
classifiers with package reproduction and clone detection. Their
evaluation on a set of 2288 package updates suggests that their
approachleadstomanymorealertsthanours,flagging539updates
aspotentiallysuspicious;theydidnottriagetheresultsindetail,so
itisunknownwhethertheysucceededinfindingmaliciouspack-
ages.Ourexperimentscoveramuchlargersetofpackages,andwe
haveshown thatwe candetecta significantnumber ofpreviously
unknown malicious packages.
Duan et al. [ 8] study recent examples of supply-chain attacks,
pinpointingrootcausesandclassifyingattackvectorsandmalicious
behaviors. Based on their results, they built an analysis pipeline
leveraging acombination of staticand dynamic program-analysis
techniquestodetectmaliciouspackagesacrossthreedifferentpack-
ageregistries(npm,PyPI,15andRubyGems16).Inalarge-scaleex-
periment covering more than one million packages, their approach
15https://pypi .org/
16https://rubygems .org/
1689ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Adriana Sejfia and Max Schäfer
Category Feature Amalfi Garrett et al.
Access to PII /check
Access to system resourcesFile system access /check /check
Process creation /check /check
Network access /check /check
Use of specific APIsCryptographic functionality /check
Data encoding /check
Dynamic code generation /check /check
Use of installation scripts /check /check
Presence of minified code and binary files /check
Time between updates /check
Update type /check
Added dependencies /check
Added code /check
Table 4: Comparison of features considered in our models and those of Garrett et al. [10]
identified 339 previously unknown malicious packages, 41 of them
on npm. They do not provide precise statistics on false positives or
on the performance of their approach.
On the whole, our goals differ from theirs: where they aim to
provide a comparative framework for the security of registries, we
specifically focus on finding malicious npm packages. They update
theirdetectionrulesmanuallyasresultsareassessed,whileourclas-
sifierscanberetrainedwithoutfurthermanualeffortbeyondthe
assessment of results itself. Our approach seems simpler and more
lightweight than theirs, not requiring (potentially expensive) deep
static analysis or (potentially dangerous) code execution for dy-
namic analysis.17Nevertheless, we manage to find more malicious
packages on a smaller set than they do.
Pfretzschneretal.[ 23]proposetheuseofstaticanalysistode-
tectusesofJavaScriptlanguagefeaturesthatcanmakeapackage
vulnerable to interference from a malicious downstream depen-
dency.Theypresentfourdifferentattackscenariosinvolvingglobal
variables, monkey patching, and caching of modules, though they
did not find real-world examples of such attacks.
Ohm et al. [ 21] propose a dynamic analysis for observing and
measuring the creation of artifacts during package installation as a
way of detecting malicious packages. While this is a promising re-
searchdirectionandpotentiallypracticallyveryuseful,approaches
relying on code execution inherently tend to be more heavyweight
and harder to scale than our lightweight feature extraction.
At the shallower end of the analysis spectrum, tools like Mi-
crosoft Application Inspector18and OSSGadget19offer regular-
expression based scanning as a way of quickly detecting various
typesofpotentialmalware,includingmaliciouspackages.However,
these tools tend to be very noisy in practice and produce many
false positives, precluding large-scale usage.
Severalresearchershaveproposedcheckingfordifferencesbe-
tween packages hosted on registries and their purported source
codeasawayofdetectingmalware.Goswamietal.[ 13]reportthat
17Whilethereproducerdoesexecutecode,itonlyrunsbuildscripts,whichareless
likely to be malicious.
18https://github .com/microsoft/ApplicationInspector
19https://github .com/microsoft/OSSGadgetthisisdifficultfornpmpackagesduetomanyirrelevantbutnon-
malicious differences, an experience that tallies with ours. Vu et
al.[28,30]studythesameproblemforPyPI,andsimilarlyconclude
that non-reproducibility by itself is a weak indicator of malicious-
nessandneedstobecombinedwithothertechniquestobecome
effective, which is what we have done in this work.
Forthespecificproblemofdetectingtyposquatting,Vuetal.[ 31]
propose using edit distance as a metric for finding packages whose
nameisverysimilartoanother,whileTayloretal.[ 26]employa
combinationoflexicalsimilarityandpackagepopularity.Ourwork
doesnot specificallyfocuson typosquatting,but maystill be able
to identify such packages from other criteria.
Anomalydetection. Malicious-packagedetectionisaparticular
instance of the more general problem of malware detection, which
in turn is often phrased in terms of anomaly detection [ 22], where
malware is characterized as anomalous outliers in a larger set of
benign samples. This framework has been brought to bear in a
widevarietyofcontexts,includingdetectinganomalouscommits
onGitHub[ 12,14],aswellasmaliciouswebsites[ 15],binaries[ 29],
and mobile apps [2, 6, 7].
Applying machine-learningtechniques in thesedomains often
faces the problem of imbalanced datasets just like in our case. This
line of research pointed us to the advantages of using decision
trees[22],Naive-Bayes[ 15],andOne-classSVMs[ 22]asthebase
algorithms for our models. More complicated models based on
neuralnetworks[ 29]werenotsuitableforourproblemgiventhe
relatively small dataset in our p ossession. Ho wever, with more and
better data, this could be an avenue for future research.
Package-registry security. The security mechanisms of the pack-
ageregistriesandtheimpactofmaliciouspackagesonthesereg-
istrieshavealsobeenstudiedextensively.Ohmetal.’sstudy[ 20]
explores the forms attacks can have on different registries. Oth-
ers have focused on specific registries such as PyPI [ 1,3,25], or
npm[34].Ononehand,mechanismstounderstandtheimpactof
maliciouspackageshavebeenproposed[ 19,33].Ontheotherhand,
studieshavealsofocusedonwaysregistriescanmitigatetheimpact
ofmaliciouspackages[ 9].Comparedtotheselonger-termsolutions
1690Practical Automated Detection of Malicious npm Packages ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
(which are, of course, well worth investigating), our focus is on
short-term mitigation measures that do not require any changes to
the registries themselves, as explained in Section 1.
Code reuse and security. Our work touches upon the risks of
codereuse,whichhasrecentlyseenafairamountofinterestinthe
research literature.
Wang et al. [ 32] investigate third-party library usage in Java,
with particular focus on the problem of outdated libraries that may
be lacking recent bug fixes. They find that maintainers of client
projectsareoftenunwillingtoupdatetomorerecentlibraryver-
sions even when alerted to severe bugs in the version they depend
on. Prana et al. [ 24] report similar results from a study covering
vulnerabledependenciesinJava,Python,andRuby.Interestingly,
they conclude that different levels of development activity, project
popularity, and developer experience do not affect the handling of
vulnerable-dependency reports.
Mirhoseini et al. [ 17] find that automated upgrade pull requests
improve the situation to some extent, although they can also have
the adverse side effect of overwhelming maintainers with upgrade
notifications.Forthecaseofmaliciousnpmpackages,thisisless
of a problem since they are taken down upon discovery and hence
can no longer be depended on.
Gkortzisetal.[ 11]specificallyexaminetherelationshipbetween
softwarereuseandsecurityvulnerabilities.Asonemightexpect,
theyfindthatthelargeraprojectthemorelikelyitistobeaffected
bysecurityvulnerabilities,andsimilarlythatprojectswithmany
dependencies are more exposed to security risks. While their work
focusses on vulnerable code as opposed to malware, it stands to
reason that similar correlations exist in the latter case.
Tomitigatethisproblem,Koishybayevetal.[ 16]proposeastatic
analyzercalledMininodethateliminatesunusedcodeanddepen-
dencies from Node.js applications, thereby reducing their attack
surface.
7 CONCLUSION
We have presented Amalfi, an approach to detecting malicious
npm packages based on a combination of a classifier trained on
known samples of malicious and benign npm packages, a repro-
ducerforidentifyingpackagesthatcanberebuiltfromsource,anda
clonedetectorforfindingcopiesofknownmaliciouspackages.The
classifier works on a set of features extracted using a light-weight
syntactic analysis, including information about the capabilities the
package makes use of and how these change between versions.
We have presented an evaluation of our approach employing
threedifferentkindsofclassifiers:decisiontrees,NaiveBayesian
classifiers, and SVMs. In our experiments, all three techniques suc-
ceeded in detecting previously unknown malicious packages, with
the decision tree outperforming the other two, though each clas-
sifier contributed unique results. While all three classifiers pro-
ducefalsepositives,theirprecisioncanbeimproveddramatically
through continuous retraining as past predictions are triaged. We
havealsoshownthattraining,featureextraction,andclassification
are very fast, suggesting that Amalfi is practically useful.
Forfuturework,weareplanningoninvestigatingdeeperfeature
extractionthatgoesbeyondthepurelysyntacticapproachwehave
used so far, perhaps employing light-weight static analysis. Wewould also like to experiment with more advanced clone-detection
approaches to identify similar but not textually identical copies of
maliciouspackages.Anotherareaworthexploringwouldbehowto
combine results from multiple classifiers, perhaps in the form of a
rankingofresultsthatcouldaidinmanualtriaging.Finally,itwould
beveryinterestingtoapplyourtechniquestootherecosystemssuch
asPyPIorRubyGems,whichalsosufferfrommaliciouspackages
in much the same way as npm.
ACKNOWLEDGMENTS
The authors would like to thank the npm team and the GitHub
TrustandSafetyteamforprovidinguswithaccesstothecorpusof
maliciouspackages,andforfacilitatingourexperiments.Wewould
alsoliketothankthem,aswellasourGitHubcolleaguesBasAlberts
and Henry Mercer, Tom Zimmermann and Patrice Godefroid of
MicrosoftResearch,LaurieWilliams,andtheentireGitHubNext
team for valuable feedback and advice during our work on this
paper. Finally, we would like to thank Prof. Nenad Medvidović for
his help and feedback.
REFERENCES
[1]MahmoudAlfadel,DiegoEliasCosta,andEmadShihab.2021. EmpiricalAnal-
ysis of Security Vulnerabilities in Python Packages. In 2021 IEEE International
Conference on Software Analysis, Evolution and Reengineering (SANER) . 446–457.
https://doi .org/10 .1109/SANER50967 .2021 .00048
[2]VitaliiAvdiienko,KonstantinKuznetsov,AlessandraGorla,AndreasZeller,Steven
Arzt, Siegfried Rasthofer, and Eric Bodden. 2015. Mining Apps for Abnormal
Usage of Sensitive Data. In 2015 IEEE/ACM 37th IEEE International Conference on
Software Engineering , Vol. 1. 426–436. https://doi .org/10 .1109/ICSE .2015 .61
[3]AadeshBagmar,JosiahWedgwood,DaveLevin,andJimPurtilo.2021. IKnow
What You Imported Last Summer: A study of security threats in the Python
ecosystem. CoRRabs/2102.06301 (2021). arXiv:2102.06301 https://arxiv .org/abs/
2102 .06301
[4]Adam Baldwin. 2019. Plot to steal cryptocurrency foiled by the npm security
team. https://blog .npmjs .org/post/185397814280/plot-to-steal-cryptocurrency-
foiled-by-the-npm.
[5]Alex Birsan. 2021. Dependency Confusion: How I Hacked Into Apple, Mi-
crosoft and Dozens of Other Companies. https://medium .com/@alex .birsan/
dependency-confusion-4a5d60fec610.
[6]Haipeng Cai, Xiaoqin Fu, and Abdelwahab Hamou-Lhadj. 2020. A study of
run-timebehavioralevolutionofbenignversusmaliciousappsinandroid. In-
formation and Software Technology 122 (2020), 106291. https://doi .org/10 .1016/
j.infsof .2020 .106291
[7]Kai Chen, Peng Wang, Yeonjoon Lee, XiaoFeng Wang, Nan Zhang, Heqing
Huang,WeiZou,andPengLiu.2015. FindingUnknownMalicein10Seconds:
Mass Vetting for New Threats at the Google-Play Scale. In 24th USENIX Security
Symposium(USENIXSecurity15) .USENIXAssociation,Washington,D.C.,659–
674. https://www .usenix .org/conference/usenixsecurity15/technical-sessions/
presentation/chen-kai
[8]RuianDuan,OmarAlrawi,RanjitaPaiKasturi,RyanElder,BrendanSaltaformag-
gio,andWenkeLee.2021. TowardsMeasuringSupplyChainAttacksonPackage
ManagersforInterpretedLanguages.In 28thAnnualNetworkandDistributedSys-
tem Security Symposium, NDSS 2021, virtually, February 21-25, 2021 . The Internet
Society. https://www .ndss-symposium .org/ndss-paper/towards-measuring-
supply-chain-attacks-on-package-managers-for-interpreted-languages/
[9]GabrielFerreira,LiminJia,JoshuaSunshine,andChristianKästner.2021. Con-
tainingMaliciousPackageUpdatesinnpmwithaLightweightPermissionSystem.
In2021IEEE/ACM43rdInternationalConferenceonSoftwareEngineering(ICSE) .
1334–1346. https://doi .org/10 .1109/ICSE43902 .2021 .00121
[10]KalilAndersonGarrett,GabrielFerreira,LiminJia,JoshuaSunshine,andChristian
Kästner. 2019. Detecting Suspicious Package Updates. In Proceedings of the
41stInternationalConferenceonSoftwareEngineering:NewIdeasandEmerging
Results,ICSE(NIER)2019,Montreal,QC,Canada,May29-31,2019 ,AnitaSarma
andLeonardoMurta(Eds.).IEEE/ACM,13–16. https://doi .org/10 .1109/ICSE-
NIER .2019 .00012
[11]Antonios Gkortzis, Daniel Feitosa, and Diomidis Spinellis. 2021. Software
reuse cuts both ways: An empirical analysis of its relationship with security
vulnerabilities. Journal of Systems and Software 172 (2021), 110653. https:
//doi .org/10 .1016/j .jss.2020 .110653
1691ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Adriana Sejfia and Max Schäfer
[12]DanielleGonzalez, ThomasZimmermann, PatriceGodefroid,and MaxSchäfer.
2021. Anomalicious: Automated Detection of Anomalous and Potentially Ma-
licious Commits on GitHub. In 2021 IEEE/ACM 43rd International Conference
onSoftwareEngineering:SoftwareEngineeringinPractice(ICSE-SEIP) .258–267.
https://doi .org/10 .1109/ICSE-SEIP52600 .2021 .00035
[13]PronnoyGoswami,SakshamGupta,ZhiyuanLi,NaMeng,andDaphneYao.2020.
InvestigatingTheReproducibilityofNPMPackages.In 2020IEEEInternational
Conference on Software Maintenance and Evolution (ICSME) . 677–681. https:
//doi .org/10 .1109/ICSME46990 .2020 .00071
[14]RamanGoyal,GabrielFerreira,ChristianKästner,andJamesD.Herbsleb.2018.
Identifying unusual commits on GitHub. J. Softw. Evol. Process. 30, 1 (2018).
https://doi .org/10 .1002/smr .1893
[15]H.B.KazemianandS.Ahmed.2015. Comparisonsofmachinelearningtechniques
for detecting malicious webpages. Expert Systems with Applications 42, 3 (2015),
1166–1177. https://doi .org/10 .1016/j .eswa .2014 .08.046
[16]Igibek Koishybayev and Alexandros Kapravelos. 2020. Mininode: Reducing
the AttackSurface ofNode.js Applications.In 23rd InternationalSymposiumon
ResearchinAttacks,IntrusionsandDefenses(RAID2020) .USENIXAssociation,San
Sebastian,121–134. https://www .usenix .org/conference/raid2020/presentation/
koishybayev
[17]SamimMirhosseiniandChrisParnin.2017. CanAutomatedPullRequestsEncour-
age Software Developers to Upgrade Out-of-Date Dependencies?. In Proceedings
ofthe32ndIEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering
(ASE 2017) . IEEE Press, 84–94.
[18]Roweida Mohammed, Jumanah Rawashdeh, and Malak Abdullah. 2020. Ma-
chineLearningwithOversamplingandUndersamplingTechniques:Overview
StudyandExperimentalResults.In 202011thInternationalConferenceonInfor-
mationandCommunicationSystems(ICICS) .243–248. https://doi .org/10 .1109/
ICICS49469 .2020 .239556
[19]Benjamin Barslev Nielsen, Martin Toldam Torp, and Anders Møller. 2021. Modu-
lar Call Graph Construction for Security Scanning of Node.js Applications. In
Proceedingsofthe30thACMSIGSOFTInternationalSymposiumonSoftwareTesting
and Analysis (ISSTA 2021) . Association for Computing Machinery, New York, NY,
USA, 29–41. https://doi .org/10 .1145/3460319 .3464836
[20]MarcOhm,HenrikPlate,ArnoldSykosch,andMichaelMeier.2020. Backstab-
ber’s Knife Collection: A Review of Open Source Software Supply Chain At-
tacks. In Detection of Intrusions and Malware, and Vulnerability Assessment -
17th International Conference, DIMVA 2020, Lisbon, Portugal, June 24-26, 2020,
Proceedings (Lecture Notes in Computer Science) , Clémentine Maurice, Leyla
Bilge,GianlucaStringhini,andNunoNeves(Eds.),Vol.12223.Springer,23–43.
https://doi .org/10 .1007/978-3-030-52683-2 2
[21]Marc Ohm, Arnold Sykosch, and Michael Meier. 2020. Towards Detection of
SoftwareSupplyChainAttacksbyForensicArtifacts.In Proceedingsofthe15th
InternationalConferenceonAvailability,ReliabilityandSecurity (ARES’20) .A s-
sociation for Computing Machinery, New York, NY, USA, Article 65, 6 pages.
https://doi .org/10 .1145/3407023 .3409183
[22]S. Omar, A. Ngadi, and Hamid H. Jebur. 2013. Machine Learning Techniques for
AnomalyDetection:AnOverview. InternationalJournalofComputerApplications
79 (2013), 33–41.[23] Brian Pfretzschner and Lotfi ben Othmane. 2017. Identification of Dependency-
Based Attacks on Node.js. In Proceedings of the 12th International Conference
onAvailability,ReliabilityandSecurity (ARES’17) .AssociationforComputing
Machinery, New York, NY, USA, Article 68, 6 pages. https://doi .org/10 .1145/
3098954 .3120928
[24]GedeArthaAzriadiPrana,AbhishekSharma,LwinKhinShar,DariusFoo,An-
drew E. Santosa, Asankhaya Sharma, and David Lo. 2021. Out of sight, out of
mind?Howvulnerabledependenciesaffectopen-sourceprojects. Empir.Softw.
Eng.26, 4 (2021), 59. https://doi .org/10 .1007/s10664-021-09959-3
[25]Jukka Ruohonen, Kalle Hjerppe, and Kalle Rindell. 2021. A Large-Scale Security-
OrientedStaticAnalysisofPythonPackagesinPyPI. CoRRabs/2107.12699(2021).
arXiv:2107.12699 https://arxiv .org/abs/2107 .12699
[26]Matthew Taylor, Ruturaj K. Vaidya, Drew Davidson, Lorenzo De Carli, and
VaibhavRastogi.2020. DefendingAgainstPackageTyposquatting.In Network
and System Security - 14th International Conference, NSS 2020, Melbourne, VIC,
Australia, November 25-27, 2020, Proceedings (Lecture Notes in Computer Science) ,
Miroslaw Kutylowski, Jun Zhang, and Chao Chen (Eds.), Vol. 12570. Springer,
112–131. https://doi .org/10 .1007/978-3-030-65745-1 7
[27]NikolaiPhilippTschacher.2016. TyposquattinginProgrammingLanguagePackage
Managers . Master’s thesis. University of Hamburg.
[28]Duc-LyVu,FabioMassacci,IvanPashchenko,HenrikPlate,andAntoninoSabetta.
2021. LastPyMile:IdentifyingtheDiscrepancybetweenSourcesandPackages.
InESEC/FSE .
[29]Duc-Ly Vu, Trong-Kha Nguyen, Tam V. Nguyen, Tu N. Nguyen, Fabio Massacci,
andPhuH.Phung.2019. AConvolutionalTransformationNetworkforMalware
Classification.In 20196thNAFOSTEDConferenceonInformationandComputer
Science (NICS) . 234–239. https://doi .org/10 .1109/NICS48868 .2019 .9023876
[30]DucLyVu,IvanPashchenko,FabioMassacci,HenrikPlate,andAntoninoSabetta.
2020. TowardsUsingSourceCodeRepositoriestoIdentifySoftwareSupplyChain
Attacks. In Proceedings of the 2020 ACM SIGSAC Conference on Computer and
Communications Security (CCS ’20) . Association for Computing Machinery, New
York, NY, USA, 2093–2095. https://doi .org/10 .1145/3372297 .3420015
[31]Duc-LyVu,IvanPashchenko,FabioMassacci,HenrikPlate,andAntoninoSabetta.
2020. Typosquatting and Combosquatting Attacks on the Python Ecosystem. In
2020IEEEEuropeanSymposiumonSecurityandPrivacyWorkshops(EuroSPW) .
509–514. https://doi .org/10 .1109/EuroSPW51379 .2020 .00074
[32]YingWang,BihuanChen,KaifengHuang,BowenShi,CongyingXu,XinPeng,
Yijian Wu, and YangLiu. 2020. AnEmpirical Study ofUsages,Updates and Risks
ofThird-PartyLibrariesinJavaProjects.In 2020IEEEInternationalConferenceon
Software Maintenance and Evolution (ICSME) . 35–45. https://doi .org/10 .1109/
ICSME46990 .2020 .00014
[33]AhmedZerouali,TomMens,AlexandreDecan,andCoenDeRoover.2021. On
theImpactofSecurityVulnerabilitiesinthenpmandRubyGemsDependency
Networks. CoRRabs/2106.06747 (2021). arXiv:2106.06747 https://arxiv .org/abs/
2106 .06747
[34]Markus Zimmermann, Cristian-Alexandru Staicu, Cam Tenny, and Michael
Pradel. 2019. Small World with High Risks: A Study of Security Threats in
the npm Ecosystem. In 28th USENIX Security Symposium, USENIX Security 2019,
SantaClara,CA,USA,August14-16,2019 ,NadiaHeningerandPatrickTraynor
(Eds.). USENIX Association, 995–1010. https://www .usenix .org/conference/
usenixsecurity19/presentation/zimmerman
1692