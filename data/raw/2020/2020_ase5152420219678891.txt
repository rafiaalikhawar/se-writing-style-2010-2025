Detecting TensorFlow Program Bugs in Real-World
Industrial Environment
Chen Liu†‡, Jie Lu†∗, Guangwei Li†‡, Ting Y uan†‡, Lian Li†‡∗, Feng Tan§, Jun Yang§, Liang Y ou§, Jingling Xue◦
†SKL Computer Architecture, ICT, CAS, Beijing, China
‡University of Chinese Academy of Sciences, China
§Alibaba Group
◦University of New South Wales, New South Wales, Australia
†{liuchen17z, lujie, liguangwei, yuanting, lianli}@ict.ac.cn
§{tanfeng.tf, muzhuo.yj, youliang.yl}@alibaba.com
◦jingling@cse.unsw.edu.au
Abstract —Deep learning has been widely adopted in industry
and has achieved great success in a wide range of application
areas. Bugs in deep learning programs can cause catastrophicfailures, in addition to a serious waste of resources and time.
This paper aims at detecting industrial TensorFlow program
bugs. We report an extensive empirical study on 12,289 failedTensorFlow jobs, showing that existing static tools can effectivelydetect 72.55% of the top three types of Python bugs in industrialTensorFlow programs. In addition, we propose (for the ﬁrst time)a constraint-based approach for detecting TensorFlow shape-related errors (one of the most common TensorFlow-speciﬁcbugs), together with an associated tool, S
HAPE TRACER . Our
evaluation on a set of 60 industrial TensorFlow programs showsthat S
HAPE TRACER is efﬁcient and effective: it analyzes each
program in at most 3 seconds and detects effectively 40 out of60 industrial TensorFlow program bugs, with no false positives.
S
HAPE TRACER has been deployed in the PLATFORM -X platform
and will be released soon.
Index T erms—TensorFlow Bugs, Constraint Solving
I. I NTRODUCTION
Deep learning has been widely adopted in industry. Assisted
by open-source frameworks [1]–[3], developers can efﬁciently
design new deep learning models for applications in a widerange of areas [4]–[7], such as image recognition, natural lan-guage processing, and self-driving cars. To enable developersto test and train their models effectively, enterprises have builtdedicated platforms, such as Google Cloud AI [8], MicrosoftAzure Machine Learning [9], and Amazon SageMaker [10].Those platforms are equipped with rich computational re-sources including GPUs and AI accelerators, running tens ofthousands of deep learning jobs every day.
Like other software applications, deep learning programs
are often plagued by bugs. In a real-world industrial environ-ment, these bugs often lead to job failures, wasting seriouslyresources and time. There are a number of studies [11]–[14]targeting deep learning program errors. Speciﬁcally, Zhang etal. [12] conducted an extensive empirical study on programfailures of deep learning jobs for Microsoft’s Philly platform.
Siﬁs et al. [14] developed a new static analysis to detect shape
∗Corresponding authors.errors in TensorFlow programs, which can effectively detect11 of 14 shape-related TensorFlow bugs studied in [11].
In this paper, we aim at detecting TensorFlow (the dominant
open-source deep learning framework) program bugs in areal-world industrial environment. We perform an extensiveempirical study on 12,289 failed TensorFlow jobs submittedto the
PLA TFORM -X platform by teams in Alibaba Group [15].
Compared to [11], [12], our study focuses on job failures dueto TensorFlow program bugs, and targets a different industrialplatform. Our ﬁndings and actions are:
•Finding: Most bugs (63.69%) are common Python bugs,with argument mismatches, undeﬁned variables, andmissing attributes as the top three types of bugs.
Action: We deployed four existing representative static
tools, Mypy [16], Pylint [17], Pyﬂakes [18], andPytype [19], to detect Python bugs in TensorFlowprograms. Our results show that these four tools togetherdetect 72.55% of the top three types of Python bugs.
•Finding: Checkpoint Errors (17.49% ) and Shape Errors
(8.82%) are the two most common types of TensorFlow-speciﬁc bugs. As for the former category (triggered byfailing to load a checkpoint ﬁle), we are not aware ofany existing bug-detection techniques reported in theliterature. As for the latter category, a static analysisapproach [14] exists and will be evaluated in this paper.
Action: We have developed S
HAPE TRACER , a new tool
for detecting also shape-related errors in real-world Ten-
sorFlow applications. In contrast to the static analysis ap-proach, P
YTHIA , described in [14], we adopt a constraint-
based approach for the ﬁrst time. S HAPE TRACER tra-
verses program paths and builds a shape-ﬂow graph
(an abstract dataﬂow computation graph) for each path.A constraint solver is then employed to solve shape-related constraints (introduced by shape operators) foreach shape-ﬂow graph. Finally, a bug is reported if theconstraint solver cannot ﬁnd a feasible solution, and asuggestion is offered as a warning if the user input is
552021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000162021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678891
978-1-6654-0337-5/21/$31.00  ©2021  IEEE
constrained.
Unlike P YTHIA (based on static analysis), S HAPE -
TRACER (based on constraint solving) enables detecting
subtle shape-related errors when the rank (number of
dimensions) or dimensions (dimension sizes) of a shape
are completely unknown. P YTHIA formulates the problem
of detecting shape-related errors as one of inferring theshapes of tensors from a set of datalog rules and itsanalysis cannot progress unless an unknown shape rankor dimension can be deduced to be a concrete value(or a ﬁnite set of concrete values). However, in real-world applications, many unknown shapes cannot be con-cretized this way, as illustrated by the program given inFigure 1, for which we are required to solve the constraint“batch
size*32==batch size∨batch size==1”, where
batch size is provided as user input. In particular, the
shapes of tensors that are statically unknown may be pro-vided as user input by reading from the commandline orﬁles, or by calling unmodeled library functions, making
P
YTHIA often ineffective, as evaluated in Section VI.
An immediate question arises: does S HAPE TRACER suf-
fer from path explosion? The answer is no. TensorFlowprograms have simple control-ﬂow structures, makingsuch an approach practical and efﬁcient. We have ap-plied S
HAPE TRACER to a set of 60 real-world buggy
TensorFlow applications. Our experimental results showthat S
HAPE TRACER is efﬁcient (by analyzing a program
in at most 3 seconds) and effective (by reporting 40 outof 60 bugs) in detecting real-world TensorFlow bugs.
S
HAPE TRACER , together with four other tools, Mypy [16],
Pylint [17], Pyﬂakes [18], and Pytype [19], have been pack-aged as a new tool (publicly released soon) for detectingTensorFlow program bugs and deployed to platform users.Developers are recommended to run this packaged tool againsttheir applications before submitting a job to the platform.
In summary, this paper makes the following contributions:
•We report an extensive empirical study on 12,289 in-dustrial TensorFlow job failures. Our ﬁndings show thatmost failure-triggering bugs (63.69%) are Python bugs,and four existing representative static bug-detection toolscan detect 72.55% of the top three types of Python bugs.
•We propose the ﬁrst constraint-based approach for de-tecting shape-related errors, one of the most commonTensorFlow-speciﬁc bugs. Our approach explores pro-gram paths systematically and can detect subtle errorswhen the rank or dimensions of a shape are unknown, bysolving the shape-related constraints for each path.
•We have implemented our constraint-based approach asa tool, S
HAPE TRACER , and applied it to a set of 60 real-
world buggy TensorFlow applications. S HAPE TRACER
is highly efﬁcient and effective, by analyzing each ap-plication in at most 3 seconds, and detecting 40 outof 60 shape-related errors, with no false positives. Wehave also compared S
HAPE TRACER with P YTHIA [14]
to demonstrate the effectiveness of our new approach.The rest of this paper is organized as follows. Sec-
tion II gives an overview of TensorFlow programs and the
PLA TFORM -X platform. In Section III, we report an empirical
study with 12,289 real-world TensorFlow job failures and mo-tivate this work. Section IV discusses how existing static toolsdetect Python bugs. Section V introduces S
HAPE TRACER .I n
Section VI, we evaluate S HAPE TRACER using both open-
source and real-world TensorFlow programs. Section VIIreviews the related work and Section VIII concludes the paper.
II. B
ACKGROUND
A. TensorFlow programs
The Google-born TensorFlow library [1] is the dominant
open-source deep learning framework. It adopts the dataﬂow
programming model, which represents all the computations asdataﬂow graphs. In a dataﬂow graph, its nodes are computationunits (i.e., operators) and its edges propagate tensors (typed
multi-dimensional arrays) from their source nodes to their sinknodes. A dataﬂow graph is executed on the data provided, withthe input data ﬂowing along its edges, which are processed byeach node before, and the output results ﬁnally produced.
A TensorFlow program, commonly written in Python, con-
sists of two phases: construction and execution. Figure 1gives a simple example abstracted from a real-world industrialapplication. In the construction phase (lines 1-14), a com-putation graph is conﬁgured: each operator (e.g., tf.matmal
at line 3) generates some nodes and edges connecting databetween nodes. In the execution phase (lines 15-18), a sessionobject is created to instantiate the graph, which is executedmultiple times (sess.run in line 18) with data being fed into
the placeholders (e.g., in
xand inyat lines 10 and 11,
respectively).
B. The PLA TFORM -XPlatform
The PLA TFORM -X platform is built by Alibaba Group [15]
and deployed in its commercial cloud. PLA TFORM -X provides
support for a variety of deep learning frameworks including
TensorFlow [1], PyTorch [20], and MXNet [21]. As for otherplatforms, users can submit their deep learning jobs via thecommandline or a web interface by specifying resourcessuch as CPU/GPU times required and checking the status ofsubmitted jobs.
Most platform users are production teams within this par-
ticular company. Everyday, tens of thousands of deep learningjobs run on the platform. There are a substantial number ofjob failures, i.e., aborted jobs. The platform will tag eachfailed job, and record its log messages for further investigation.These job failures not only lead to an expensive waste ofresources, but also can take an enormous amount of humanefforts to debug.
III. A
NEMPIRICAL STUDY
Our objective is to develop an effective failure prevention
technique. To this end, we take failed TensorFlow jobs onthe
PLA TFORM -X platform as our study subjects. There are
a total number of 12,289 failed jobs sampled in one month.
56# Construction
1. def fully connect(input op, name, nin,n out ):
2. fc w=tf.get variable(name,[ nin,nout])
3. return tf.matmul(input op, fcw)
4. def predict(Input x,class num):
5. mp = tf.nn.conv2d(input x,tf.get variable(’mpc’,[5,5,1,32]),strides=[1,1,1,1], padding=’SAME’)
6. reshaped = tf.reshape(mp, [-1, 28 * 28])
7. fc = fully connect(reshaped, ’fc1’, 28 * 28, 128)
8. logit = fully connect(fc, ’fc2’, 128, class num)
9. return logit10. in
x=tf.placeholder(tf.ﬂoat32, shape = [None, 28, 28, 1])
11. in y=tf.placeholder(tf.ﬂoat32, shape = [None, 10])
12. y = predict(in x, 10)
13. cross entropy = tf.reduce mean(tf.nn.softmax cross entropy with logits(labels=in y,logits =y))
14. train step = tf.train.AdamOptimizer(1e-4).minimize(cross entropy)
# Execution15. train
img, train lab = read image( batch size ,...)
16. with tf.Session() as sess:
17. for i in range(1000):18. sess.run(train
step, feed dict = in x:train img,i n y:train lab))
Fig. 1. A sample TensorFlow program (abstracted a real-world industrial application).
TABLE I
THE TOP FIVE MOST COMMON TYPES OF JOB FAILURES .
Error Type Example Error Message Patterns
Checkpoint ErrorAssign requires shapes of both tensors to match. lhs shape= <*> rhs shape= <*>
Key<*> not found in checkpoint
Module/Attribute missing<*><*> has no attribute <*>
No module named <*>
Arguments Mismatch<*> takes exactly <*> arguments <*> given
<*> got an unexpected keyword argument <*>
Undeﬁned V ariablename <*> is not deﬁned
local variable <*> referenced before assignment
Shape ErrorShape must be rank <*> b u ti sr a n k<*> for<*> op:<*> with input shapes: <*><*><*><*>
Cannot feed value of shape <*> for Tensor <*> which has shape <*><*>
Dimensions must be equal, but are <*> and<*> for<*> (op: <*> with input shapes: <*><*>
All failed jobs are submitted by different production teams
in Alibaba Group. For each failed job, we contacted itscorresponding production team to collect related informationincluding source code, execution logs, and job scripts. Weare not able to obtain the input data to a job since they areregarded as being highly conﬁdential. Thus, it is difﬁcult toreproduce a failure by rerunning the failed application.
Figure 2 shows the size distribution of studied applications.
TensorFlow programs are small, with 407 lines of uncom-mented code on average. The largest program that we studiedhas 2,355 lines of uncommented code. Note that the third partylibraries packaged in an application are not considered.
A. Failure Classiﬁcation
It is time-consuming to manually analyze the 12,289 appli-
cations one by one. Hence, we apply log analysis [22] to group
failed applications throwing the same error message patterntogether. Figure 3 gives an example. The application throwsan error message at line 2, which is parsed into a regularexpression “Input to reshape is a tensor with <*>values, but
the requested shape has <*>”. All the applications throwing
Fig. 2. Distribution of program sizes, in lines of uncommented code.
the same error message pattern are then grouped together. Inthe end, there are 968 failed groups. We have sampled 630applications using the standard sample size calculator with aconﬁdence level of 99%, and conﬁrmed that most applicationsin the same group fail due to the same root cause.
We have randomly selected two applications in each group,
and manually investigated their root causes to failure. Finally,we obtain a total of 17 common root causes. Table I highlightsthe top ﬁve, with some error message patterns highlighted.
571. Traceback (most recent call last):
//other stack traces
2. tensorﬂow...errors impl.InvalidArgumentError:
Input to reshape is a tensor with 1583 values,
but the requested shape has 1820
Fig. 3. The exception trace of a failed job.
Fig. 4. Bug type distribution in TensorFlow programs. TensorFlow-speciﬁc
bugs are depicted in dark bars and Python bugs in gray bars.
B. Threats To V alidity
First, root cause analysis and failure classiﬁcation involve
manual inspection on application code, which may be sub-
jective. To mitigate this threat, each failed application wasexamined by two authors separately and the results were cross-validated. Decisions were made only if both authors reached anagreement. For some applications, we also communicated withthe original developers to conﬁrm our decisions. Second, ourstudy subjects are all from the
PLA TFORM -X platform. Hence,
some ﬁndings may not be applicable to other platforms. Tomitigate this threat, we focus on failures caused by programbugs instead of platform-speciﬁc issues (e.g., failures relatedto an execution environment). The
PLA TFORM -X platform is a
widely used platform and the 12,289 studied applications covera variety of areas, including image and speech recognition,natural language processing, and recommendation systems.
C. Findings
We focus our study on bug-related failures. Out of 12,289
job failures, 1,612 failures are environment-related, throwing
error messages such as “remote ﬁle <*> not found”.I n
addition, 586 jobs failed due to corrupted input data. Thosejob failures will not be further discussed. The remaining10,091 failure-triggering bugs are classiﬁed into two cate-gories: Python bugs and TensorFlow-speciﬁc bugs. Figure 4divides these 10,091 bugs into different types of bugs inpercentage terms, with the Python bugs shown in gray barsand TensorFlow-speciﬁc bugs in dark bars.
Finding 1: 63.69% bugs are Python bugs, which also
commonly exist in general-purpose Python applications.
1) Python bugs: Let us examine some Python bugs clas-
siﬁed in Figure 4. The most common type of Python bugs isModule/Attribute Missing (referencing a non-existent Python
class ﬁeld or function), accounting for 13.61% of all bugs.There are also other common bug types, such as Argument
Mismatch (invoking a function with an inconsistent number
of actual arguments) and Undeﬁned V ariables (referencing
a variable before its deﬁnition), accounting for 12.67% and12.51% of all bugs, respectively.
Several Python bug types are directly related to the dynamic
features of Python, e.g., Type Mismatch (operating on ob-
jects of incompatible types), Illegal Argument (arguments not
satisfying function speciﬁcations), and Not Iterable/Callable
(iterating over objects of non-collection types). The otherPython bug types are common run-time errors such as Key Not
Found (accessing maps with non-existent keys) and Divide by
Zero (dividing a value by 0).
2) TensorFlow-Speciﬁc Bugs: Checkpoint Error, the most
common bug type, accounts for 17.49% of all bugs. Platformusers frequently use the checkpointing mechanism to store atrained model to the cloud or to load an already-trained modelfrom the cloud for inference or further training. A checkpointbug arises when either the model ﬁle is missing or the loadedmodel is inconsistent with the required network structure. Theformer is related to a particular execution environment andhow to deal with the latter is worth a separate investigation.
Shape Error (8.82%) arises when invoking TensorFlow
operators with arguments of incompatible shapes (incom-patible ranks or dimensions). It is difﬁcult for developersto understand the tricky semantics of thousands of Tensor-Flow APIs, leading to frequent Shape Error bugs in prac-
tice. For example, many TensorFlow operators (e.g., soft-
max
cross entrophy with logits at line 13 in Figure 1) sup-
port the NumPy “broadcasting” semantics (which “broadcasts”a small array across a relatively large array, by copying leadingdimensions of the higher-rank argument and padding anydimension of size 1 to the size of the matching dimensionfrom the other argument), often leading to surprising results.
It can be difﬁcult to debug Shape Error bugs. As illustrated
in Figure 3, an exception is thrown at line 2 when invokingthe tf.reshape operator. The tf.reshape operator changes the
shapes of tensors as long as their sizes (number of elements)stay the same. Hence, it fails to convert a tensor of 1,583elements to a speciﬁed shape of 1,820 elements and throwsan exception. However, the tf.reshape operator is frequently
used. In the example, there are 29 tf.reshape operators and it
is time-consuming for developers to examine each operator.
Finding 2: Shape Error is one of the most common
TensorFlow-speciﬁc bugs (8.82% of total bugs) and suchbugs can be detected effectively as demonstrated in [14].
The other types of TensorFlow-speciﬁc bugs include Out of
Memory (GPU out of memory, commonly ﬁxed by reducing
the sizes of tensors), Loss NaN (invalid loss values), GPU
Sync Failed (memory issues in GPU [23]), and Graph Not
Complete (invalid dataﬂow graphs).
58TABLE II
PYTHON BUGS REPORTED BY FOUR EXISTING STA TIC TOOLS [16]–[19].
BugType Mypy Pylint Pyﬂakes Pytype Total
Mod/Att Missing 5.33% 28.00% 8.00% 48.00% 57.33%
Arg Mismatch 1.27% 25.05% 8.28% 18.68% 41.61%
Undef V ar 20.85% 71.59% 96.88% 75.70% 98.36%
Total 11.86% 49.78 % 54.98 % 50.95 % 72.55%
IV . D ETECTING PYTHON BUGS
There are a number of static tools for ﬁnding bugs in Python
programs, such as Mypy [16], Pylint [17], Pyﬂakes [18],
and Pytype [19]. We have investigated their effectiveness indetecting Python bugs in industrial TensorFlow programs.
Table II gives the results for the top three Python bug types.
Overall, these four tools together have detected 72.55% of allthe bugs of these three types. Since these bugs are simplesemantic errors, the false positive rates of these four tools arelow. Among the four tools, Pyﬂakes is the best performer,reaching 54.98%. However, all the four tools perform poorlyonArguments Mismatch bugs (with Pylint attaining only
25.05% even as the best performer for bug type).
V. D
ETECTING SHAPE ERROR BUGS
In TensorFlow programs, tensors are the basic data units. A
tensor is a multi-dimension array and its shape refers to the
number of dimensions (rank) and dimensions’ sizes. Shape
Error bugs are the errors incurred when the shape of a tensor
does not match the speciﬁcation of an operator.
In [14], a static analysis, P YTHIA , is introduced for de-
tecting Shape Error bugs, by modeling tensor operators in
Datalog, so that the shape of a tensor can often be de-duced to be a concrete shape (or a set of concrete shapes).
P
YTHIA can detect 11 out of the 14 shape-related bugs studied
in [11]. However, from our study on industrial TensorFlowapplications, there are still many cases where the rank ordimension sizes of a tensor are completely unknown. For ex-ample, P
YTHIA failed to report any error in the 60 real-world
applications under testing, due to unresolved unknown shapevalues. Therefore, in this paper, we introduce a new constraint-based approach, S
HAPE TRACER , for detecting Shape Error
bugs, and we will compare it with P YTHIA in our evaluation.
In this section, we ﬁrst use three examples to motivate ourapproach and then describe it in detail.
A. Motivating Examples
1) Example 1: Figure 5 depicts the computation graph for
the program in Figure 1, where each edge is annotated with
the shape information of its propagated tensor. The shape ofa tensor can be input-dependent: a placeholder tensor can set
some dimensions (or the whole shape) to none and its shape
will be instantiated by feeding data to the placeholder (usingthefeed
dict operator, e.g., line 18) when executing the graph.
The computation graph is executed by invoking Ses-
sion.run(), Tensor .eval(),o rOperation.run(). In Figure 1, at
line 18, the graph is executed to obtain the results from theoperator train
step (line 14). The input data train image and
train lab (line 15) are fed to the placeholders inx(line 10)and iny(line 11), respectively. Note that the ﬁrst dimension
of input data is conﬁgured by an input argument batch size
as highlighted by the box in line 15. As a result, the shapesof tensors in
xand inyare [batch size, 28, 28, 1] and
[batch size, 10], respectively.
The tensor inxis passed as an actual parameter to the
function predict() at line 12 and processed by conv2d (line 5),
a core operator for convolution. The conv2d operator is often
used to extract intermediate features in complex neural net-works. It takes a 4-dimensional input tensor, a 4-dimensional
ﬁlter tensor, and a strides vector with 4 elements as input.
With the “same” padding strategy, cond2d(x, f, s, “same”) will
produce a tensor of shape [x[0],
x[1]
s1,x[2]
s2,f[3]]. Hereafter, we
use the notation x[i]to represent the ith dimension of tensor
x’s shape and the notation sito represent the ith element of
vector s. In our example (Figures 1 and 5), the conv2d operator
produces the tensor mp of shape [batch size, 28, 28, 32].
The reshape operator at line 6 changes the shape of the
incoming tensor mp to a speciﬁed shape [-1,28*28], i.e., a 2-
dimension array. Here, the special dimension size -1denotes
that the size of the corresponding dimension needs to becomputed dynamically. A tensor can be reshaped correctly ifits size (total number of items in the tensor) is the same asthe size of the speciﬁed shape. At line 6, after reshaping, wehave a new tensor reshaped of shape [batch
size*32, 28*28].
At line 7, the function fully connect is invoked with the
tensor reshaped as its actual parameter. Thus, the operators
get variable (line 2) and matmul (line 3) are included in the
computation graph. The matmul operator multiply reshaped
([batch size*32, 28*28]) with fcw([28*28, 128]), resulting
in a new tensor fcof shape [batch size*32, 128]. Next, the
tensor fcis processed by the same function again at line 8.
Finally, logit ([batch size*32, 10]) is produced and returned
asy.
The operator softmax cross entrophy with logits (line 13)
produces normalized probabilities from input tensors iny
([batch size, 10]) and y([batch size * 32, 10]). It supports
the “broadcasting” rule: sizes of matching dimensions mustbe identical, or one of them is 1 (in which case, the resultingtensor adopts the other size in its corresponding shape dimen-sion). Hence, the operator can succeed only if the sizes of bothtensors’ ﬁrst shape dimensions are the same or one of them is1, i.e., batch
size*32==batch size∨batch size==1.A st h e
user input batch size is conﬁgured to be 200, the application
failed with a runtime exception.
2) Examples 2 and 3: Let us look at another two examples
given in Figures 6 and 7, respectively. In Figure 6, thetensor behavior
input comes from user input and its shape
(i.e., its rank and dimension sizes) is completely unknown.At line 1, tensor tmp
user proﬁle cnn is reshaped to a 4-
dimensional tensor user proﬁle cnn, which is then multiplied
with behavior input via the operator matmul, suggesting that
the rank of behavior input is also 4, since the operator will
fail otherwise. At line 3, tensor attention weights is reshaped
to a 3-dimensional tensor tmp attention weights, which is
also multiplied with behavior input. Since the input tensor
59Fig. 5. The computation graph for the program given in Figure 1, where the nodes represent the operators in the program and the tensors (black dots) ﬂow
along the graph edges (annotated with the the shape information of their propagated tensors). The bug triggered is highlighted.
1. user proﬁle cnn = tf.reshape(tmp user proﬁle cnn, shape=[-1, num behavior max[behavior cnt], noutput behavior, 1])
2. attention layer input = tf.matmul( behavior input,user proﬁle cnn)
......
3. tmp attention weights = tf.reshape(attention weights, shape=[-1, num behavior max[behavior cnt], 1])
4. behavior output = tf.matmul(tmp attention weights, behavior input)
Fig. 6. Code from an industrial application, where the shape of behavior input is completely unknown. The bug-triggering lines are highlighted in red.
behavior input cannot satisfy both constraints, the application
will always fail on one of the matmul operators (at line 2 or
line 4).
In Figure 7, the tensor labels is a 2-dimensional array
and the tensor pred is a 1-dimensional array. All their di-
mension sizes are unknown. The bug is triggered at line 2
when the condition loss type == “mae” holds, since the
operator absolute difference expects input tensors with the
same shape (i.e., same rank and dimension sizes). However,the bug will not be triggered if the other branch is taken(when input parameter loss
type is“logloss”). The operator
sparse softmax cross entropy with logits allows the rank of
the input argument pred to be one less than that of labels.
Hence, it will not trigger a bug.
B. Methodology
In the above three examples, there are tensors with com-
pletely unknown shapes (Example 2) or partially unknown
shapes (Examples 1 and 3). It is difﬁcult to write Datalogrules (as in P
YTHIA [14]) and deduce those unknown shapes
to a ﬁnite set of concrete shapes. The tensors with unknownor partially unknown shapes can be frequently found in real-world applications. They can come from commandline input,ﬁles, or unsupported library functions. Note that it is difﬁcultfor developers to manually annotate a tensor from ﬁles withunknown shape information. In addition, the TensorFlow li-brary provides thousands of APIs and enterprises often offertheir own in-house libraries. It will be a daunting task, if notimpossible, to support all libraries APIs, in practice.
Therefore, we are motivated to develop S
HAPE TRACER ,
a new tool founded on a constraint-based approach. Werepresent the shape of a tensor symbolically by introducingsymbolic values for unknown ranks or unknown dimensionsizes. Constraints can be introduced from tensor operators,scalar variables, and conditional branches. Finally, a constraintsolver is applied to check the satisﬁability of these constraints.
For instance, for Example 1 (Figure 1), the value of input
variable batch
size is symbolic, denoted as X. The computa-
tion graph will generate a constraint X∗32 == X∨X== 1 ,
together with other constraints. The solution is X=1 , which
can be provided to users as a warning. In Example 2 (Figure 6),the rank of tensor behavior
input is symbolic, denoted as X.
The two matmul operators at line 2 and line 4 will introduce
their respective constraints, X== 4 andX== 3. As both
cannot be satisﬁed together, an error is found.
C.SHAPE TRACER
We have implemented S HAPE TRACER in W ALA [24] and
used Ariadane [25] as its front-end to parse Python programs
into W ALA IR. Figure 8 sketches the architecture of S HAPE -
TRACER . Its three main components are summarized below.
•First, Builder traverses program paths and builds a shape-
ﬂow graph (an abstracted computation graph) for each
path.
•Next, Solver formulates a shape-ﬂow graph into a list of
constraints, which is then solved by Z3 [26], a state-of-the-art constraint solver.
•Finally, an error (warning) is issued if the constraints arenot satisﬁable (if the user input is constrained). To reportprecisely the line number where a bug/warning occurs,Reporter searches for the ﬁrst operator introducing un-
satisﬁable constraints, and reporting it to the user.
Next, we describe these three components in detail.
1) Builder: Builder constructs a shape-ﬂow graph for each
program path. This may sound inefﬁcient initially. However,
601. if loss type == ”mae”:
2. loss =tf.reduce mean(tf.losses.absolute difference(labels, pred))
3. elif loss type == ”logloss”:
4. loss =tf.reduce mean(tf.nn.sparse softmax cross entropy with logits(logits=pred,labels=labels), 0)
5. optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss, ... )
Fig. 7. Code from an industrial application, where pred is a 2-dimensional array with unknown dimension sizes and labels is a 1-dimensional array with an
unknown dimension size. The bug-triggering line is highlighted in red.
Fig. 8. The high-level architecture of S HAPE TRACER .
the control ﬂow structures of TensorFlow programs are usually
simple. In general, the number of program paths is 2, rarelyreaching 8. In an extreme case, where the neural networkis constructed in a loop (Figure 12), the largest number ofprogram paths observed is 256 only.
a) Basic Algorithm: The shape-ﬂow graph of a program
is its abstracted computation graph annotated with shape infor-mation. To build a shape-ﬂow graph, we slice backwards froman invocation to session.run(), i.e., from an output tensor. Since
TensorFlow programs commonly propagate values directlythrough assignments or parameter passing, we slice alongthe use-def chains of W ALA ’s SSA (single static assignment)representation. During the backward slicing, function calls areinlined: when a function call is met, the return values ofthe callee function are added to the graph (as new nodes)and we continue slicing backwards from the newly addedreturn values. In the end, all operators (i.e., TensorFlow APIinvocations), tensors and scalars (e.g., actual parameters ofoperators), that the output tensor is transitively dependent on,are included in the graph.
Let us explain our basic algorithm using the example in
Figure 1. We slice from sess.run() at line 18, i.e., the output
tensor train
step. Since train step is returned from the opera-
torminimize, the operator and its operand (i.e., cross entropy)
are added to the graph. Similarly, cross entropy is produced
by the operator softmax cross entropy with logits (line 13).
Hence, the operator and its operands (in yand y) are included.
From y, we inline the function call to predict (line 12) and
continue slicing from its return value logit (line 9). Next, the
function def fully connect is inlined twice at lines 8 and 7, in
that order. The ﬁnal shape-ﬂow graph is given in Figure 5.
b) Graph Duplication: Shape-ﬂow graphs are duplicated
atphi nodes (control-ﬂow conﬂuence points in SSA). When
we encounter a phi node with nincoming values, the graph is
duplicated ntimes and each graph picks a distinct incoming
value to continue slicing. Figure 9 gives the shape-ﬂow graphsof Example 2 (Figure 6). In SSA, there exists a phi node atthe conﬂuence point of different branches of the ifstatements
(lines 1 - 4). Thus, we have two shape-ﬂow graphs, one foreach branch.
Loops, although rarely seen in the graph construction phase,
are processed by unrolling a loop twice. In our study, there isonly one application building neural networks in a loop.
c) Shape Information Collection: Constants and scalar
variables propagated directly along use-def chains are recordedstraight-forwardly. We try to infer as much concrete informa-tion as possible by applying constant propagation and comput-ing concrete shape information according to the documentedsemantics of TensorFlow APIs. We also consider the followingtwo special cases. First, the shape of a tensor can be setusing the tf.setshape() function, as shown in Fig. 10 (lines
2 and 4). Hence, for each tensor, we check its uses for atf.setshape() call to the object, and update the shape of the
tensor accordingly. Second, in most cases, values are directlypropagated. However, when initializing a tensor with a givenshape, values are passed into the constructor of the shape asparameters and stored in its corresponding ﬁelds. In general, apointer analysis [27]–[30] is required to compute ﬁeld-relateddependences. However, such ﬁelds of a shape object are onlystored once, in its constructor (during initialization). Therefore,when encounter a ﬁeld load, we simply search for a uniquestore to the corresponding ﬁeld.
2) Solver: Solver formulates a shape-ﬂow graph as a list of
constraints, which are then solved by Z3 [26]. We collect theconstraints from tensor operators and scalar instructions in theshape-ﬂow graph. However, we do not consider branch condi-tions, since shape-related values rarely have data dependenceson conditionals in real-world TensorFlow applications.
Figure 11 deﬁnes the symbolic representation of shapes and
values. Speciﬁcally, T[−1]denotes the size of T’s last shape
dimension. This variable is particularly useful when T’s rank
is unknown, i.e.,
Tis symbolic. By default, we assume that
all variables are symbolic unless otherwise speciﬁed. If Tis a
constant value C, we introduce a variable for each dimension
size of T, by applying the following function to concretize T:
Concretize(T,C ):C−1/productdisplay
i=0T[i]= =|T|∧T[−1] == T[C−1]∧T== C
This function sets T’s rank to C, sets T[−1]toT’s last
dimension size (T [−1] == T[C−1]) , and concretizes T’s size
to the product of its all dimension sizes (| T|==/producttextC−1
i=0T[i]).
Constraints are introduced for operators according to their
documented semantics. For instance, the C=reshape(A,B) op-
61Fig. 9. The two shape-ﬂow graphs (one for each path) of the program given in Figure 7. The oval nodes are operators and square nodes are scalars. The
edges are annotated with shape information of their associated tensors (blackdots).
1 .x=tf.placeholder(tf.ﬂoat32, [None])
2. x.set shape([1028178])
3 .y=tf.identity(x)4. y.set
shape([478, 717, 3])
5 .X=np.random.normal(0, 0.1, 1028178)6. Y = sess.run(y, feed
dict=x: X)
Fig. 10. The code example UT-3 from [11].
T[0],T [−1],T[−], ... T’s Dimension sizes
|T| T’s Total size (number of elements)
T T’s Rank (number of dimensions)
V0,V1, ..., V|V|−1 V’s element values
X X’s value
Fig. 11. Symbolic representation of tensor T’s shape, vector V’s values, and
scalar X’s value. Vis a 1-dimensional shape, and its size |V|is a constant.
erator reshapes tensor Ato tensor Cof the same size, with the
shape speciﬁed by vector B. Hence, we have:
/logicalanddisplay
0≤i<|B|C[i]= = Bi∧C==|B|∧Concretize(C, |B|)∧|C|==|A|
Here, the constraint |C|==|A|states that tensor Cand A
have the same size (as required by reshape), and the remaining
constraints specify the shape of Caccording to vector B:C’s
rank is deﬁned by B’s size ( C==|B|), and C’s dimensions
are deﬁned by B’s elements (/logicalandtext
0≤i<|B|C[i]= =B i). Note
that the size of B, i.e.,|B|, is a constant. Hence, tensor C
is concretized (Concretize (C,|B|)). Except for one element
value (e.g., -1), all the other element values are constant. The
same list of constraints is applicable to the case when tensorA’s rank is constant, i.e., when Ais already concretized.
Let us examine the operator softmax
cross entropy-
with logits, abbreviated as C=logits(A,B), for supporting
Numpy “broadcasting”. Before we dive into the details ofthe tricky broadcasting semantics, we ﬁrst introduce anotherhelper function Broadcast (A, B, C, i, j ). This function repre-
sents the constraints on the ith dimension of the higher-ranked
input tensor A, the ith dimension of output tensor C, and thematching jth dimension of the other input tensor B, where
j≤i:
Broadcast ((A, B, C, i, j ):( ( A[i]= = B[j]∧C[i]= = A[i])
∨(A [i]= =1∧C[i]= = B[j])∨(B[j]= =1∧C[i]= = A[i]))
Broadcast ((A, B, C, i) holds if one of the following three
cases holds: 1) A[i]matches with B[j], producing the same
size for C’sith dimension, 2) A[i]is 1, in which case C’sith
dimension takes the size from B’sjth dimension, and 3) B[j]
is 1, in which case C’sith dimension size takes that from A’s.
The three cases reproduce the semantics of broadcasting onepair of matching dimensions (A[i] and B[j]).
The list of constraints for C=logits(A,B) is given by:
(A== B∧Broadcast (A, B, C, 0,0)∧Broadcast (A, B, C, −1,−1))∨
(A> B∧Broadcast (A, B, C, −1,−1)∧A[0] == C[0])∨
(A< B∧Broadcast (A, B, C, −1,−1)∧B[0] == C[0])
The above constraints are applied when AorBis symbolic.
In this case, we only introduce the constraints on the ﬁrst andlast dimension sizes of the input and output tensors. In thecase of
A==B, the constraints are applied to the ﬁrst and
last dimensions of all three tensors A,B, and C(Broadcast <
A, B, C, 0>∧Broadcast < A,B,C, −1>). In the other
two cases, the constraints are applied to the last dimension ofthe three tensors, and the output tensor Ctakes the size from
the higher-ranked tensor (e.g., A[0] == C[0]when
A> B).
When both Aand Bare constants (i.e., Aand Bare
concretized), Ccan be concretized as follows:
(A== B∧Concretize(C, A)∧/logicalanddisplay
0≤i<ABroadcast (A, B, C, i, i))∨
(A> B∧Concretize(C, A)∧/logicalanddisplay
0≤i<A−BC[i]= = A[i]∧
/logicalanddisplay
0≤i<BBroadcast (A, B, C, i +A−B,i))∨
(A< B∧Concretize(C, B)∧...)
In the case of A== B, the output tensor Cis con-
cretized and each of its dimensions is deﬁned by theBroadcast rule. Otherwise, Cis concretized with the higher
rank (e.g, Concretize (C,
A) when A> B), higher di-
62TABLE III
NUMBER OF REPORTED ERRORS /W ARNINGS IN INDUSTRIAL AND
OPEN -SOURCE (FROM [11]) APPLICA TIONS . #TP IS THE NUMBER OF
REPORTED TRUE BUGS , #FP IS THE NUMBER OF REPORTED FALSE
POSITIVES ,AND #FN IS THE NUMBER OF FALSE NEGA TIVES .
ToolIndustrial Open-Source
#TP #FP #FN #TP #FP #FN
SHAPE TRACER 24/16 0 20 9/0 0 5
PYTHIA 0/0 0 60 7/4 1 2
Enhanced P YTHIA 9/0 0 51 7/4 1 2
mensions are copied directly from the higher-ranked tensor
(/logicalandtext
0≤i<A−BC[i]= =A[ i]), and the matching dimensions are
broadcasted (/logicalandtext
0≤i<BBroadcast (A, B, C, i +A−B,i)).
Similarly, appropriate constraints are introduced for the
other operators, such as conv2d and matmul. To date, S HAPE -
TRACER provides support for 54 common operators, with an
average of 34.8 lines of code for each operator. For tensorsreturned from unsupported library functions, their ranks aresymbolically represented. In the end, the constraints of ashape-ﬂow graph are fed into Z3 [26].
3) Reporter: If Z3 fails to solve a given set of constraints (if
a user input is constrained by a constant value), an error (warn-ing) is issued. While errors can always trigger a bug, warningssuggest expected user inputs. To report the bug locationprecisely, Reporter searches for an operator introducing the
unsatisﬁable constraints found. Conceptually, this is realizedby removing each operator one by one (more precisely, byremoving the constraints introduced by each operator), in thereverse order of when it is added to the underlying dataﬂowgraph, until the constraints become satisﬁable. In practice, wehave accelerated this process using binary search.
VI. E
V ALUA TION
Our evaluation addresses the following research questions:
•How effective is S HAPE TRACER in detecting Shape Error
bugs in TensorFlow programs?
•How does S HAPE TRACER compare to a state-of-the-art
static analysis tool, P YTHIA [14]?
•How efﬁcient is S HAPE TRACER ?
All experiments were conducted on a laptop equipped withi5-9400 CPU and 32GB RAM.
A. RQ1: Effectiveness
We evaluate S
HAPE TRACER using a set of 60 buggy indus-
trial TensorFlow programs (randomly picked from our study)
and the 14 open source programs studied in [11].
Table III summarizes the results. Overall, S HAPE TRACER
has successfully detected 40 out of 60 bugs in industrialprograms, and 9 out of 14 bugs in open-source applications.There are 33 errors (24 from the industrial programs and 9from the open-source programs) and 16 warnings (all fromthe industrial programs). A bug can deﬁnitely be triggered forany of the 33 reported errors. The 16 warnings are subject touser input, in which case S
HAPE TRACER warns on expected
input values. The applications that exhibit these warnings canonly be correct if the corresponding shape-related constraints1. deep out = None
2. for idx, info in enumerate(self.deep info):
3. if idx == 0:
4. deep out= deep features
5. else:6. deep
out = res out
7. deep out = tf.matmul(deep out,info[”w”])
8. deep out = tf.nn.leaky relu(deep out)
9. if idx >1:
10. res out=tf.concat([deep out,deep out list[idx-1]])
11. else:12. res
out = deep out
13. if len(deep out list) <= idx:
14. deep out list.append(deep out)
15. else:16. deep
out list[idx] = deep out
......17. loss = tf.reduce
mean(
tf.nn.sigmoid cross entropy with logits(
logits=pred, labels=labels))+l2
Fig. 12. Code snippet of a false negative example: the neural network is built
in a loop. Each loop iteration builds one layer of the network (lines 2-8). Thenext loop iteration uses the previous two layers as input to build a new layer(line 10). The bug is triggered at line 17 as highlighted in red.
TABLE IV
F
ALSE NEGA TIVES IN THE 20 INDUSTRIAL PROGRAM BUGS EXPLAINED .
# Reasons for Producing False Negatives
19 Too many shape-related values from user input
1 Constructing neural networks in a loop.
are met. According to our experience, the odds for such a
warning to be an error is much higher than that for theshaped-related constraints to be always satisﬁable. Thus, these16 warnings can be considered as errors detected. We willcompare S
HAPE TRACER and P YTHIA later.
1) False Negatives: SHAPE TRACER failed to detect 20 in-
dustrial program bugs, with the two reasons given in Table IV.19 bugs were not reported because the corresponding programsdeﬁne most of their tensor shapes in conﬁguration ﬁles, result-ing in many unknown shapes. Since S
HAPE TRACER failed
to deduce a constant-constrained input, no error or warningwas given. The other false negative occurs in the programillustrated in Figure 12, where a neural network is constructedin a loop. ShapeTracer processes a loop by unrolling it twice,
which is not sufﬁcient for detecting this particular bug.
There are also 5 open-source program bugs missed by
ShapeTracer. We will discuss these bugs in Section VI-B2.
2) False Positives: S
HAPE TRACER did not report any false
positives for the programs in Table III. To evaluate its precisionfurther, we have further tested S
HAPE TRACER using a set of
60 randomly selected correct programs from the PLA TFORM -X
platform. Again, no false positive was reported.
3) Error and Warning Examples: Figure 13 gives a buggy
example, with the bug triggered at line 10. The operator abso-
lute difference requires the incoming arguments of the same
shapes, producing constraint layer 8= = var 0. Furthermore,
SHAPE TRACER can infer that layer8 is of shape [-1,1] (line
631. def input fn():
2. #other complex operation
3. var 0.shape = [-1]
4. var 1=tf.reshape( var 1,[None, 31])
5. return var 0,var 1
6. def model fn(var 1,var 0,reuse):
7. #other complex operation
8. #layer7 and w7 comes from other operation
9. layer8 = tf.matmul(layer7,W7)+b7#now layer8 is [-1,1]
10. tmp1 = tf.losses.absolute difference(layer8, var 0)
11. loss = tf.reduce sum(tmp1)
12. return loss,
13.var 0, var 1 = input fn()
14.loss = model fn(var 1, var 0, False)
15.c = sess.run([loss])
Fig. 13. Code snippet with a bug, abstracted from an industrial program.
Sensitive variable names have been replaced with var 0and var 1.
1. def fun 0(batch size):
2. #other complex operation
3. var 1= cloud platform api0(batch size, 67)
4. return var 1
5.var 0= fun 0(batch size) #batch size is from user input
6.x= tf.reshape(var 0, [-1, 66])
#warning reported by ShapeTracer
[[z3][suggest]]
[batch size]ﬁlename.py(line3) value = 66
[anonymous]ﬁlename.py(line6) value = 67
Fig. 14. Code snippet of an example, which contains a warning, abstracted
from an industrial program. Sensitive variable names have been replaced withvar
0and var 1. The warning message is given in the box.
9) and var 0has a shape of [-1] (line 3). Inference details are
omitted here due to space limitation. As a result, we derive the
constraints layer 8= = var 0∧layer 8= =2∧var 0= =1 ,
which are unsatisﬁable. Hence, an error is reported. It is worthnoting that S
HAPE TRACER generates 1,485 constraints for this
program. However, Reporter is able to examine every operator
and precisely points out the bug location.
Figure 14 gives an example triggering a warning, together
with the warning message reported by S HAPE TRACER . The
cloud platform api0 at line 3 produces tensor var 1of shape
[batch size, 67] (with batch size being unknown), which is
propagated to var 0(line 5). The operator reshape at line 6
reshapes var 0to the speciﬁed shape [-1, 66], and expects the
size of var 0to match with the size of the speciﬁed shape, i.e.,
batch size×67 == X×66, where Xstands for the symbolic
value of the vector. Given such constraints, S HAPE TRACER
will issue a warning as highlighted. Note that the warningand error messages are helpful to developers during both codereview or post-mortem debugging.
B. RQ2: Comparing with P
YTHIA
Table III also compares S HAPE TRACER with P YTHIA [14]
(provided by its artifact) on detecting Shape Error bugs in
industrial and open-source programs. P YTHIA is a state-of-
the-art tool for detecting TensorFlow shape-related bugs.TABLE V
COMPARING SHAPE TRACER AND PYTHIA ON OPEN -SOURCE PROGRAMS ,
WITH/check(/beware)DENOTING A CORRECTLY REPORTED ERROR (W ARNING ).
UT-1 UT-5 UT-12 UT-13 UT-15
SHAPE TRACER /check /check - - -
PYTHIA /beware -/beware/beware/beware
1) Industrial Programs: PYTHIA neither detects any indus-
trial program bugs nor reports any false positives. To ﬁgure
out why, we have carefully examined the logs and messagesprinted by P
YTHIA and summarized the reasons below:
•PYTHIA failed on 23 industrial programs due to im-
plementation bugs. It throws runtime exceptions inPYTHON
FAC T GEN when generating Python facts. We
have reported this issue to the P YTHIA developers for
further investigation.
•PYTHIA failed on the other 34 industrial programs due to
unknown shape values. Although facts were successfullygenerated, the unknown shape values prevented its analy-sis to progress further. Unknown shape values come fromuser inputs or unsupported operators. Note that althoughthere still exist a considerable number of unknown valuesin S
HAPE TRACER , our constraint-based approach still
enables our analysis to detect many bugs, as demonstratedin our motivating examples (Section V-A).
•PYTHIA can successfully analyze a partially-known shape
(with a None or special (-1) dimension). However, it
cannot analyze both completely unknown shapes (e.g.,an unknown rank) and completely unknown dimensionswhen they cannot be concretized. As a result, it fails todetect any error in the set of 60 real industrial programs.We further investigated on how to extend P
YTHIA to
deduce as many unknown shapes as possible. An extraUnknown tag is introduced for any unknown shape, and
new datalog rules are introduced to deduce the rankand dimension values of tagged Unknown shapes. For
example, for a matmul operator, its two parameter shapes
must have the identical rank. Thus, we can infer therank of a completely unknown parameter shape from theother parameter. We have extended P
YTHIA with a set
of 75 datalog rules (509 LOC). This extension enables
PYTHIA to detect 9 shape-related errors. However, it still
fails to report the other errors due to unknown shapevalues or unﬁxed crashes. For example, three shape-related errors that are missed by P
YTHIA are related
to complex constraints like batch size*32==batch size
illustrated in Figure 1.
2) Open-Source Programs: PYTHIA performs much better
on open-source programs, and we have successfully repro-duced their results (as in their paper [14]) using their givenartifact. Table V highlights the differences of the two tools onopen-source programs. P
YTHIA reported warnings for the 3
bugs, UT-12, UT-13, and UT-15, missed by S HAPE TRACER .
PYTHIA exploits heuristics to report these 3 warnings (e.g.,
suspicious broadcasting). We did not implement such heuris-tics in S
HAPE TRACER because they can lead to many false
64Fig. 15. Analysis times of S HAPE TRACER on 60 industrial programs.
Fig. 16. Size distribution of shape ﬂow graphs for industrial programs.
positives and developers often ignore such warnings.
In UT-13, operator argmax(Y[4,1], axis=1) will result in a
shape of [4] with all values being zero. In UT-15, y[3,1]-y [3]
produces a tensor of shape[3,3], due to broadcasting. P YTHIA
warns on such suspicious operations since the results look sur-
prising. However, these programs still satisfy the shape-relatedrules. We attempted to incorporate similar heuristics but werediscouraged to do so (by our industry sponsor developing the
PLA TFORM -X platform) because of spurious reports. UT-12
uses list slice in Python to construct the shape of all initial
tensors, which is not yet supported by S HAPE TRACER .
C. RQ3: Efﬁciency
Figure 15 summarizes the analysis times of S HAPE TRACER
on the 60 industrial programs. S HAPE TRACER is fast, as it
ﬁnishes all its analyses in at most 3 seconds for each programon a standard laptop with 32GB RAM. Note that the analysistimes include the times in exploring programs paths, collectingand solving constraints, and searching for bug locations.
Figure 16 shows the size distribution of shape-ﬂow graphs.
The sizes of shape-ﬂow graphs range from 12 to 810 nodes,with an averages of 246.9 nodes. The average number ofconstraints for each shape-ﬂow graph is 914.8, which looksseemingly large. However, as most of constraints are constantequality constraints, Z3 can solve them very efﬁciently.
VII. R
ELA TED WORK
Empirical Studies. Zhang et al. [11] investigated 175 Tensor-
Flow program bugs from Stack Overﬂow and GitHub. Follow-ing [11], Islame et al. [31] performed a more comprehensivestudy on deep learning program bugs, including 2,716 bugsfrom applications using ﬁve different deep learning libraries(Caffe [2], Keras [32], Tensorﬂow [1], Theano [33], and Torch[3]). The authors in [12] conducted an extensive empiricalstudy on 4,960 job failures on Microsoft’s Philly platform.
Guo [13] surveyed bugs in deep learning development anddeployment. In this paper, we also perform an empirical studyfocusing on 12,289 industrial TensorFlow job failures on anew platform, motivating us to develop S
HAPE TRACER , a new
static tool for detecting TensorFlow shape errors.
Static Bug Detection. Python is the most popular language in
developing deep learning applications [13]. Python bugs [16],
[17] in deep learning programs can be detected quite effec-tively with existing static tools, as conﬁrmed in this paper.
A number of research efforts focus on shape-related bugs.
Ariadne [25] is the ﬁrst static shape analysis tool developedfor TensorFlow. However, due to implementation issues (e.g.,failing to analyze shapes inter-procedurally), it cannot effec-tively detect errors in practice [14]. P
YTHIA [14] is shown to
be able to detect 11 out of 14 open-source program bugs usinga Datalog-based static analysis. In this paper, we introduce anew constraint-based approach and a tool, S
HAPE TRACER ,t o
detect effectively industrial TensorFlow program bugs.
Testing. There is a large body of research [34]–[40] aiming at
testing the robustness of deep learning models. How to apply
a constraint-based approach to improve testing effectiveness isan interesting topic worth further investigation.
VIII. C
ONCLUSION
This paper aims at detecting industrial TensorFlow pro-
gram bugs. We have conducted an extensive empirical studyon 12,289 failed industrial TensorFlow jobs. Based on ourﬁndings, we have applied four existing representative statictools to detect 72.55% of the top three common Pythonbugs in TensorFlow programs. To detect TensorFlow-speciﬁcbugs, we have introduced the ﬁrst constraint-based approachfor detecting TensorFlow shape-related errors and developedan associated static tool, S
HAPE TRACER . We have applied
SHAPE TRACER to a set of 60 industrial TensorFlow programs,
showing that S HAPE TRACER is both efﬁcient (by analyzing a
program in at most 3 seconds) and effective (by detecting 40out of 60 industrial TensorFlow program bugs, with no falsepositives). S
HAPE TRACER is now deployed in the PLA TFORM -
X platform and will be publicly available soon.
ACKNOWLEDGEMENTS
Thanks to all the reviewers for their constructive comments.
Project supported by the State Key Laboratory of ComputerArchitecture, ICT, CAS, China (Grant No. CARCH5402).
R
EFERENCES
[1] M. Abadi, P . Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,
S. Ghemawat, G. Irving, M. Isard et al., “Tensorﬂow: A system for large-
scale machine learning,” in 12th{USENIX} symposium on operating
systems design and implementation ({OSDI} 16), 2016, pp. 265–283.
[2] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
S. Guadarrama, and T. Darrell, “Caffe: Convolutional architecture for
fast feature embedding,” in Proceedings of the 22nd ACM international
conference on Multimedia, 2014, pp. 675–678.
[3] R. Collobert, S. Bengio, and J. Mari ´ethoz, “Torch: a modular machine
learning software library,” Idiap, Tech. Rep., 2002.
[4] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2016.
65[5] A. V aswani, S. Bengio, E. Brevdo, F. Chollet, A. N. Gomez,
S. Gouws, L. Jones, L. Kaiser, N. Kalchbrenner, N. Parmar, R. Sepassi,
N. Shazeer, and J. Uszkoreit, “Tensor2tensor for neural machinetranslation,” CoRR, vol. abs/1803.07416, 2018. [Online]. Available:
http://arxiv.org/abs/1803.07416
[6] I. Sutskever, O. Vinyals, and Q. V . Le, “Sequence to sequence learn-
ing with neural networks,” in Proceedings of the 27th International
Conference on Neural Information Processing Systems - V olume 2, ser.NIPS’14. Cambridge, MA, USA: MIT Press, 2014, p. 3104–3112.
[7] M. Bojarski, D. D. Testa, D. Dworakowski, B. Firner, B. Flepp,
P . Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang,X. Zhang, J. Zhao, and K. Zieba, “End to end learning for self-driving cars,” CoRR, vol. abs/1604.07316, 2016. [Online]. Available:
http://arxiv.org/abs/1604.07316
[8] (2020) Ai and machine learning products. [Online]. Available:
https://cloud.google.com/products/ai
[9] (2020) Azure machine learning. [Online]. Available: https://azure.
microsoft.com/en-us/services/machine-learning/
[10] (2020) Machine learning for every data scientist and developer.
[Online]. Available: https://aws.amazon.com/sagemaker/
[11] Y . Zhang, Y . Chen, S.-C. Cheung, Y . Xiong, and L. Zhang, “An
empirical study on TensorFlow program bugs,” in Proceedings of the
27th ACM SIGSOFT International Symposium on Software Testingand Analysis, ser. ISSTA 2018. New Y ork, NY , USA: Associationfor Computing Machinery, 2018, p. 129–140. [Online]. Available:https://doi.org/10.1145/3213846.3213866
[12] R. Zhang, W. Xiao, H. Zhang, Y . Liu, H. Lin, and M. Yang, “An
empirical study on program failures of deep learning jobs,” in 2020
IEEE/ACM 42nd International Conference on Software Engineering(ICSE). IEEE, 2020, pp. 1159–1170.
[13] Q. Guo, S. Chen, X. Xie, L. Ma, Q. Hu, H. Liu, Y . Liu, J. Zhao, and
X. Li, “An empirical study towards characterizing deep learning devel-opment and deployment across different frameworks and platforms,” in2019 34th IEEE/ACM International Conference on Automated SoftwareEngineering (ASE). IEEE, 2019, pp. 810–822.
[14] S. Lagouvardos, J. Dolby, N. Grech, A. Antoniadis, and Y . Smaragdakis,
“Static analysis of shape in TensorFlow programs,” in 34th European
Conference on Object-Oriented Programming (ECOOP 2020). SchlossDagstuhl-Leibniz-Zentrum f ¨ur Informatik, 2020.
[15] (2020) Alibaba. [Online]. Available: https://www.alibabagroup.com/[16] (2020) mypy. [Online]. Available: http://mypy-lang.org/[17] (2020) pylint. [Online]. Available: https://www.pylint.org/[18] (2020) pyﬂakes. [Online]. Available: https://github.com/PyCQA/pyﬂakes[19] (2020) pytype. [Online]. Available: https://google.github.io/pytype/[20] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., “Pytorch: An
imperative style, high-performance deep learning library,” arXiv preprint
arXiv:1912.01703, 2019.
[21] T. Chen, M. Li, Y . Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu,
C. Zhang, and Z. Zhang, “Mxnet: A ﬂexible and efﬁcient machinelearning library for heterogeneous distributed systems,” arXiv preprint
arXiv:1512.01274, 2015.
[22] J. Zhu, S. He, J. Liu, P . He, Q. Xie, Z. Zheng, and M. R. Lyu, “Tools
and benchmarks for automated log parsing,” in 2019 IEEE/ACM 41st In-
ternational Conference on Software Engineering: Software Engineeringin Practice (ICSE-SEIP). IEEE, 2019, pp. 121–130.
[23] (2015) Nvidia responds to gtx 970 3.5gb mem-
ory issue. [Online]. Available: https://wccftech.com/nvidia-geforce-gtx-970-memory-issue-fully-explained/
[24] (2006) Wala. [Online]. Available: https://github.com/wala/W ALA[25] J. Dolby, A. Shinnar, A. Allain, and J. Reinen, “Ariadne: analysis for
machine learning programs,” in Proceedings of the 2nd ACM SIGPLAN
International Workshop on Machine Learning and Programming Lan-guages , 2018, pp. 1–10.
[26] (2020) Z3. [Online]. Available: https://github.com/Z3Prover/z3[27] M. Bravenboer and Y . Smaragdakis, “Strictly declarative speciﬁcation of
sophisticated points-to analyses,” in Proceedings of the 24th ACM SIG-
PLAN conference on Object oriented programming systems languagesand applications, 2009, pp. 243–262.
[28] N. Grech and Y . Smaragdakis, “P/taint: uniﬁed points-to and taint
analysis,” Proceedings of the ACM on Programming Languages, vol. 1,
no. OOPSLA, pp. 1–28, 2017.
[29] T. Tan, Y . Li, and J. Xue, “Efﬁcient and precise points-to analysis:
modeling the heap by merging equivalent automata,” in Proceedings of
the 38th ACM SIGPLAN Conference on Programming Language Designand Implementation, 2017, pp. 278–291.
[30] Y . Sui and J. Xue, “SVF: interprocedural static value-ﬂow analysis
in LLVM,” in Proceedings of the 25th International Conference on
Compiler Construction. New Y ork: ACM, 2016, pp. 265–266.
[31] M. J. Islam, G. Nguyen, R. Pan, and H. Rajan, “A comprehensive study
on deep learning bug characteristics,” in Proceedings of the 2019 27th
ACM Joint Meeting on European Software Engineering Conference andSymposium on the Foundations of Software Engineering, 2019, pp. 510–520.
[32] A. Gulli and S. Pal, Deep learning with Keras. Packt Publishing Ltd,
2017.
[33] R. Al-Rfou, G. Alain, A. Almahairi, C. Angermueller, D. Bahdanau,
N. Ballas, F. Bastien, J. Bayer, A. Belikov, A. Belopolsky et al.,
“Theano: A python framework for fast computation of mathematicalexpressions,” arXiv, pp. arXiv–1605, 2016.
[34] K. Pei, Y . Cao, J. Yang, and S. Jana, “Deepxplore: Automated whitebox
testing of deep learning systems,” in proceedings of the 26th Symposium
on Operating Systems Principles, 2017, pp. 1–18.
[35] L. Ma, F. Juefei-Xu, F. Zhang, J. Sun, M. Xue, B. Li, C. Chen,
T. Su, L. Li, Y . Liu et al., “Deepgauge: Multi-granularity testing criteria
for deep learning systems,” in Proceedings of the 33rd ACM/IEEE
International Conference on Automated Software Engineering, 2018, pp.120–131.
[36] Y . Tian, K. Pei, S. Jana, and B. Ray, “Deeptest: Automated testing
of deep-neural-network-driven autonomous cars,” in Proceedings of the
40th international conference on software engineering , 2018, pp. 303–
314.
[37] X. Xie, L. Ma, F. Juefei-Xu, M. Xue, H. Chen, Y . Liu, J. Zhao,
B. Li, J. Yin, and S. See, “Deephunter: A coverage-guided fuzztesting framework for deep neural networks,” in Proceedings of the
28th ACM SIGSOFT International Symposium on Software Testingand Analysis, ser. ISSTA 2019. New Y ork, NY , USA: Associationfor Computing Machinery, 2019, p. 146–157. [Online]. Available:https://doi.org/10.1145/3293882.3330579
[38] M. Zhang, Y . Zhang, L. Zhang, C. Liu, and S. Khurshid, “Deeproad:
Gan-based metamorphic autonomous driving system testing,” arXiv
preprint arXiv:1802.02295, 2018.
[39] L. Ma, F. Juefei-Xu, M. Xue, B. Li, L. Li, Y . Liu, and J. Zhao, “Deepct:
Tomographic combinatorial testing for deep learning systems,” in 2019
IEEE 26th International Conference on Software Analysis, Evolutionand Reengineering (SANER). IEEE, 2019, pp. 614–618.
[40] L. Ma, F. Zhang, J. Sun, M. Xue, B. Li, F. Juefei-Xu, C. Xie, L. Li,
Y . Liu, J. Zhao, and Y . Wang, “Deepmutation: Mutation testing ofdeep learning systems,” in 2018 IEEE 29th International Symposium
on Software Reliability Engineering (ISSRE), 2018, pp. 100–111.
66