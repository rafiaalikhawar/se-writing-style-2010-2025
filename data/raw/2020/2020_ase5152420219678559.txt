On Multi-Modal Learning of Editing Source Code
Saikat Chakraborty
Department of Computer Science
Columbia University
New Y ork, NY , USA
saikatc@cs.columbia.eduBaishakhi Ray
Department of Computer Science
Columbia University
New Y ork, NY , USA
rayb@cs.columbia.edu
Abstract —In recent years, Neural Machine Translator (NMT)
has shown promise in automatically editing source code. Typical
NMT based code editor only considers the code that needs tobe changed as input and suggests developers with a ranked listof patched code to choose from - where the correct one maynot always be at the top of the list. While NMT based codeediting systems generate a broad spectrum of plausible patches,the correct one depends on the developers’ requirement and oftenon the context where the patch is applied. Thus, if developersprovide some hints, using natural language, or providing patchcontext, NMT models can beneﬁt from them.
As a proof of concept, in this research, we leverage three
modalities of information: edit location, edit code context, commitmessages (as a proxy of developers’ hint in natural language)to automatically generate edits with NMT models. To thatend, we build M
ODIT , a multi-modal NMT based code editing
engine. With in-depth investigation and analysis, we show thatdevelopers’ hint as an input modality can narrow the search spacefor patches and outperform state-of-the-art models to generatecorrectly patched code in top-1 position.
Index T erms—Source Code Edit, Neural Networks, Automated
Programming, Neural Machine Translator, Pretraining, Trans-formers
I. I NTRODUCTION
Programmers often develop software incrementally, adding
gradual changes to the source code. In a continuous software
development environment, programmers modify their sourcecode for various reasons, including adding additional func-tionality, ﬁxing bugs, refactoring, etc. It turns out that many ofthese changes follow repetitive edit patterns [1]–[3] resultingin a surge of research effort to automatically generate code-changes learned from past examples [2]–[6].
In particular, Neural Machine Translation (NMT) models
have been successful in learning automatic code changes [5]–[11]. At the core, these models contain an encoder and adecoder — the encoder encodes the code that needs to beedited, and the decoder sequentially generates the edited code.Such NMT models are trained with a large corpus of previousedits to learn generic code change patterns. In the inferencetime, given a code fragment that needs to be edited, a trainedNMT model should automatically generate the correspondingedited code.
However, learning such generic code changes is chal-
lenging. A programmer may change an identical pieceof code in different ways in two different contexts, bothcan potentially be correct patches (see Figure 1). For//Guidance: use LinkedList and fix sublist problem ...
public void addPicture (String picture){
if((pictures)= =null){
- pictures = new ArrayList<>();
+ pictures = new LinkedList<>(); //correct patch
+ pictures = new HashSet<>(); //plausible patch
}
pictures.add(picture);
}
Fig. 1: Example of an identical code (marked in red) changed in two
different ways ( green and blue ) in two different contexts, where both can
be correct patches. However, based on developers’ guidance (top line) to
ﬁx a list related problem, green is the correct patch in this context.
example, an identical code fragment pictures = new
ArrayList<>() was changed in two different ways:
pictures = new HashSet<>(); andpictures =
new LinkedList<>() in two different code contexts.
Without knowing the developers’ intention and the edit con-
text, the automated code editing tools have no way to predictthe most intended patches. For instance, in the above example,LinkedList was used to ﬁx a sublist-related problem. Once
such an intention is known, it is easy to choose a LinkedList-related patch from the alternate options. Thus, such an addi-tional modality of information can reinforce the performanceof automated code-editing tools.
 1 // Guidance: fix problem which occurred when
 2 // the resulting json is empty ...
 3
 4 private String generateResultingJsonString (
 5       char wrappingQuote, Map<String, Object>jsonMap ){
 6     JsonObject jsonObject = new JSONObject(jsonMap );
 7     String newJson  = jsonObject.toJSONString (LT_COMPRESS );
 8     if (
 9 -        newJson.charAt(1) != wrappingQuote
10 +        ! jsonObject.is Empty() &&
11 +        (newJson.charAt(1) != wrappingQuote)
12     ){13     return replaceUnescaped(
14        newJson, newJson .charAt(1), qrappingQuote);
15     }16     return newJson ;
17 }
Guidance Context
Fig. 2: A motivating example. The guidance provides a brief summary
of what needs to be changes. The underlined tokens are directly copied
from guidance and context into the patched code.
In fact, given just a piece of code without any additional
information, it is perhaps unlikely that even a human developer
can comprehend how to change it. Consider another real-life
4432021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000472021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678559
978-1-6654-0337-5/21/$31.00  ©2021  IEEE
example shown in Figure 2. If a programmer only considers
the edited expression in line 9, it is difﬁcult to decide how
to modify it. However, with additional information modalities–i.e., the guidance (line 1,2) and the context (the whole
method before the patch), the correct patch often becomesevident to the programmer since the guidance effectivelysummarises how to change the code and the context providesnecessary ingredients for generating a concretely patched code.We hypothesize that such multi-modal information could bebeneﬁcial to an automated code-editing tool. To that end, wedesign M
ODIT , a multi-modal code editing engine that is based
on three information modalities: (i) the code fragment thatneeds to be edited, (ii) developers’ intention written in naturallanguage, and (iii) explicitly given edit context.
In particular, M
ODIT is based on a transformer-based [12]
NMT model. As input, M ODIT takes the code that needs to
be edited (e.g., the lines that need to be patched), additional
guidance describing developers’ intent, and the context of theedits that are explicitly identiﬁed by the developer (e.g., the
surrounding method body, or surrounding lines of code, etc.).Note that previous works [6], [11] also provided context andthe edit location while generating edits; however, they are fedtogether to the model as a uniﬁed code element. Thus, themodel had the burden of identifying the edit location and thengenerating the patch. In contrast, isolating the context fromthe edit location and feeding them to the model as differentmodalities provides M
ODIT with additional information about
the edits.
Curating developers’ intent for a large number of edits that
can train the model is non-trivial. As a proof of concept,we leverage the commit messages associated with the editsto simulate developers’ intent automatically. We acknowledgethat commit messages could be noisy and may not alwaysreﬂect the change summary [13]. Nonetheless, our extensiveempirical result shows that, even with such noisy guidance,
M
ODIT performs better in generating correctly edited code.
Being a model that encodes and generates source code,
MODIT needs to both clearly understand and correctly gener-
ate programming languages (PL). While several previous ap-proaches [6], [14] designed sophisticated tree/grammar-basedmodels to embed the knowledge of PL into the model, themost recent transformer-based approaches [15]–[17] showedconsiderable promise with pre-training with a large volume ofsource code. Since these models are pre-trained with billionsof source code written by actual developers, and transformersare known to learn distant dependencies between the nodes,these models can learn about code structures during the pre-training step. Among such pre-trained models, PLBART [17]learns jointly to understand and generate source code andshowed much promise in generative tasks. Thus, we chosePLBART as the starting point to train M
ODIT , i.e., we initialize
MODIT ’s model with learned parameters from PLBART.
We evaluate M ODIT on two different datasets ( B2Fs,
andB2Fm) proposed by Tufano et al. [8] consisting of an
extensive collection of bug-ﬁx commits from GitHub. Ourempirical investigation shows that a summary of the changewritten in natural language as additional guidance from thedeveloper improves M
ODIT ’s performance by narrowing down
the search space for change patterns. The code-edit context,presented as a separate information modality, helps M
ODIT
to generate edited code correctly by providing necessary codeingredients (e.g., variable names, method names, etc.). M
ODIT
generates ∼3.5 times more correct patches than C ODIT show-
ing that M ODIT is robust enough to learn PL syntax implicitly.
Furthermore, M ODIT generates two times as many correct
patches as a large transformer model could generate.
Additionally, our empirical investigation reveals that when
we use one encoder to encode all information modalities ratherthan learning from individual modalities separately, the modellearns representation based on inter-modality reasoning. Incontrast, a dedicated encoder for each individual modalityonly learns intra-modality reasoning. Our experiment showsthat a multi-modal/single-encoder model outperforms multi-modal/multi-encoder model by up to 46.5%.
We summarize our main contributions in this paper as
follows.
•We propose M ODIT – a novel multi-modal NMT-based
tool for automatic code editing. Our extensive empiricalevaluation shows that Automatic Code Editing can bevastly improved with additional information modalitieslike code context and developer guidance.
•We empirically investigate different design choices for
MODIT . We provide a summary of the lessons that we
learned in our experiments. We believe such lessons arevaluable for guiding future research.
•We prototype and build M ODIT and open-source all our
code, data in https://git.io/JOudU.
II. B ACKGROUND
A. Neural Machine Translation
Neural Machine Translation(NMT) [18] is a very well
studied ﬁeld, which has been very successful in translatinga sentence from one language to another. At a very high level,input to an NMT model is a sentence ( X=x
1,x2,...,x n),
which is usually a sequence of tokens (x i), and the output
is also a sentence (Y =y1,y2,...,y m)– sequence of tokens
(yi). While learning to translate from XtoY, NMT mod-
els learn conditional probability distribution P(Y|X). Such
probability distributions are learned w.r .t. model parameters θ,
where model training process optimizes θin such a way that
maximizes the expected probability distribution of a dataset.An NMT model usually contains an encoder and a decoder.The encoder processes, understands, and generates vectorrepresentations of the input sentence. The decoder starts afterthe encoder and sequentially generates the target sentence byreasoning about the encoder-generated input representation.While sequentially generating the target sentence, the decoderusually performs different heuristic searches (for instance,beam search) to balance exploration and exploitation.
In recent few years, Software Engineering has seen a wide
spectrum of adaptation of NMT. Some prominent application
444of NMT is SE include Program Synthesis [19], Code sum-
marization [20], [21], Edit summarization [13], Code EditGeneration [5], [6], [8], Automatic Program Repair [9]–[11],etc. These research efforts capitalize on NMTs’ capability tounderstand and generate complex patterns and establish NMTas a viable tool for SE-related tasks.
B. Transformer Model for Sequence Processing
Transformer [12] model revolutionized sequence processing
with attention mechanism. Unlike the traditional RNN-based
model where input tokens are processed sequentially, the trans-former assumes soft-dependency between each pair of tokensin a sequence. Such dependency weights are learned in theform of attention weights based on the task of the transformer.While learning the representation of a token, the transformerlearns to attend to all the input tokens. From a conceptual pointof view, the transformer converts a sequence to a completegraph
1, where each node is a token. The weights of the
edges are attention weights between tokens which are learnedbased on the task of the transformer. The transformer encodeseach token’s position in the sequence (positional encoding)as part of the input. In such a way, the transformer learnslong-range dependency. Since its inception, the transformer isvery successful in different NLP understanding and generationtasks. Transformers’ ability of reasoning about long-rangedependency is proved useful for several source code processingtask including code completions [22], code generation [23],code summarization [21].
C. Transfer Learning for Source Code
In recent few years, Transfer learning shows promise for
a wide variety of SE tasks. Such transfer learning aims at
learning task agnostic representation of source code and reusesuch knowledge for different tasks. One way to learn such taskagnostic representation of input is pre-training a model with alarge collection of source code. The learning objective of suchpre-training is often understanding the code or generating thecorrect code. A pre-trained model is expected to embed theknowledge about source code through its parameters. Suchpre-trained models are later ﬁne-tuned for task-speciﬁc objec-tives. CuBERT [24], CodeBERT [15], GraphCodeBERT [16]are all transformer-based encoder models which are pre-trainedto understand code. Such models are primarily trained usingMasked Language Model [25], replaced token prediction [15],semantic link prediction [16], etc.
For code generation, CodeGPT [9], [26] pre-trains a
transformer-based model to generate general-purpose codesequentially. More recently, PLBART [17] pre-trainedtransformer-based model jointly for understanding and gen-erating code with denoising auto-encoding [27]. PLBARTconsists of an encoder and a decoder. The encoder is presentedwith slight noise (for instance, token replacement) inducedcode, and the decoder is expected to generate noise-free code.Since code editing task requires both the understanding of code
1https://en.wikipedia.org/wiki/Complete graphand code generation, we chose PLBART as the base model for
MODIT .
III. M ODIT
Figure 3 shows an overview of M ODIT ’s working procedure.
MODIT is a multi-layer encoder-decoder based model consist-
ing of a Transformer-based encoder and a Transformer-baseddecoder. Both the encoder and decoder consist of 6 layers.
M
ODIT works on three different modalities of information:
(i) Code that needs to be edited (e p), (ii) natural language
guidance from the developer (G ), and (iii) the context code
where the patch is applied (C ). We acknowledge that epis
essentially a substring of C. However, by explicitly extracting
and presenting epto M ODIT , we provide M ODIT with ad-
ditional information about the change location. Thus, despitebeing a part of the context, we consider e
pa separate modality.
Nevertheless, M ODIT consists of three steps. First, the pre-
processing step processes and tokenizes these input modalities(§III-A). Then the encoder in M
ODIT encodes the processed
input, and the decoder sequentially generates the patched codeas a sequence of tokens (§III-B). At ﬁnal step, M
ODIT post-
processes the decoder generated output and prepares the editedcode (§III-C).
A. Pre-processing
Input Consolidation. In the pre-processing step, M
ODIT
generates consolidated multi-modal input (X ) from
the three input modalities (i.e., ep,G, andC). M ODIT
combines these input modalities as a sequence separated
by a special <s> token i.e., X=ep<s>G<s> C. In the
example shown in Figure 2, episnewJson.charAt(1)
)!= wrappingQuote ,Gisfix problem which
occurred when the resulting json is empty,andCis the whole function before the edit (see Input
Modalities in Figure 3). M
ODIT generates a consolidates
multi-modal input sequence as newJson.charAt(1))
... <s> fix problem which occurred ... <s>private String ... }.
Tokenization. M
ODIT uses sentence-piece tokenizer [28].
Sentence-piece tokenizer divides every token into sequenceof subtokens. Such subword tokenization is similar to pre-viously used byte-pair encoding in automatic code edit-ing literature [9], [29]. We use PLBART [17]’s sentence-piece tokenizer which is trained on billions of codefrom GitHub. After tokenizing the consolidated input X
from Figure 2, we get _new Json . char At ( 1
) ... <s> _fix _problem _which _oc cur red
... <s> _private _String ... _}.
B. Encoder-Decoder Model
The input to M
ODIT ’s encoder-decoder model is a sequence
of subtokens generated in the previous step.
445Input Modalities
private String ... ( char  ...
 Map<String, Object>jsonMap){...} fix problem which occurred
when the resulting json is empty newJson.charAt(1) != wrappingQuoteModality 1: Code to be edited
Modality 2: Guidance
Modality 3: ContextnewJson... <s> 
fix problem ...  <s> 
private String ... }
<s> _new Json ... <s> 
_fix _problem ... <s> 
_private _String ... _}Combined Multi-modal InputPre-processing
Tokenization
Tokenized InputEncoder-Decoder Model
Transformer
Encoder
Transformer DecoderOutput Generation
_! _jsonObject . is Empty
() _&& _( ... </s>Top Candidate Code
Post-processing
Edited Code! json.isEmpty() && ( newJson.charAt(1) !=
wrappingQuote )
Fig. 3: Overview of M ODIT pipeline
Transformer Encoder . Given an input sequence X=
x1,x1,...,x n, the encoder learns the representation of every
token at layer lasRe
l(xi)using self-attention computed as
Re
l(xi)=n/summationdisplay
j=iai,j∗Re
l−1(xj) (1)
WhereRe
l−1(xj)is the representation of subtoken xjas
generated by layer l−1, andai,jis the attention weight
of subtoken xitoxj. Such attention weights are learned by
multi-head attention [12]. Final layer generated representation
(i.e.,Re
6(xi)) is the ﬁnal representation for every subtoken xi
in the input. Note that, the encoder learns the representationof Equation (1) of a subtoken, using all subtokens in thesequence. Thus the learned representation of every subtokencontains information about the whole input sequence. Sincewe encode all the information modalities in one sequence, thelearned representation of every subtoken encodes informationabout other modalities.
Transformer Decoder . The decoder in M
ODIT is a transformer-
based sequential left-to-right decoder consisting of 6 layers. It
sequentially generates one subtoken at a time using previouslygenerated subtokens and the ﬁnal representation (R
e
l(xi)) from
the encoder. The decoder contains two modules – (i) self-attention, and (ii) cross-attention. The self-attention layer worksimilar to the self-attention in the encoder. First, with selfattention, decoder generates representation R
dl(yi)of last
generated token yiwith self attention on all previously gen-
erated tokens (y1,y2,...,y i). This self attention follows same
mechanism described in Equation (1). After learning decoderrepresentation by self attention, decoder applies attention ofencoder generated input representation using the followingequation,
D
l(yi)=n/summationdisplay
j=iαl
i,j∗Re
6(xj) (2)
Whereαl
i,j=softmax/parenleftbig
dot/parenleftbig
Re
6(xj),Rdl(yi)/parenrightbig/parenrightbig
is the at-
tention weight between output subtoken yito input subtoken
xj. The softmax generates an attention probability distribution
over the length of input tokens. Finally the decoder learnedrepresentation, D
l(yi)is projected to the vocabulary to predict
maximally likely subtoken from the vocabulary as next token.
In summary, the encoder learns representation of every
subtokens in the input using all input subtoken, essentiallyencoding the whole input information in every input subtokenrepresentation. The decoder’s self-attention mechanism allowsthe decoder to attend to all previously generated subtokensallowing the decoder decide on generating correct token atcorrect place. The cross-attention allows the decoder to attendto encoded representation - implicitly letting the model decidewhere to copy from the input where to choose from newtokens in the vocabulary. We initialize the end-to-end encoder-decoder in M
ODIT using pre-trained weights of PLBART [17].
C. Output Generation
The decoder in M ODIT continue predicting subtoken until
it predicts the end of sequence </s> token. During inference,
MODIT uses beam search to generate sequence of subtokens.
Once the decoder ﬁnishes, M ODIT post-processes the top
ranked sequence in the beam search. First, M ODIT removes
the end of sequence </s> token. It then detokenizes the
subtokens sequence to code token sequence. In this step,
MODIT merges generated subtokens that are fragments of a
code token into one code token. For the example shown inFigure 2, M
ODIT generates the subtoken sequence _!_json
. is Empty () _&& _(_new Json . char At (
1)_!= _wrap ping Quote _) </s>. After detok-
enization, M ODIT generates !json.isEmpty()&& (
newJson.charAt(1)!= wrappingQuote ).
IV . E XPERIMENTAL DESIGN
A. Dataset
TABLE I: Statistics of the datasets studied.
DatasetAvg. Avg. Avg. tokens #examples
Tokens Change Size* inGuidance Train Valid Test
B2Fs 32.27 7.39 11.55 46628 5828 5831
B2Fm 74.65 8.83 11.48 53324 6542 6538
* Change size measured as token edit distance.
To prove our concept of M ODIT , we experiment on two
different datasets (i.e., B2Fs, andB2Fm) proposed by
Tufano et al. [8]. In these two datasets, they collected large
collections of bug-ﬁx code changes along with commit mes-sages from Java projects in GitHub. Each example in thesedatasets contains the java method before the change (C
p), the
method after the change (C n), and the commit message for
446the change. There are some examples (< 100) with corrupted
bytes in the commit message, which we could not process.
We excluded such examples from the dataset. Table I showsstatistics of the two datasets we used in this paper. B2F
s
contains smaller methods with maximum token length 50,andB2F
mcontains bigger methods with up to 100 tokens in
length. The average size of the change (edit distance) is 7.39,and 8.83 respectively, in B2F
sandB2Fm.
B. Data Preparation
For the datasets described in Section IV-A, we extract the
input modalities and the expected output to train M ODIT .F o r
every method pair (i.e., before edit - Cp, after edit - Cn)i n
those dataset, we use GumTree [30] to extract a sequenceof tree edit locations. We identify the root of the smallestsubtree of C
p’s AST that encompasses all the edit operations.
We call the code fragment corresponding to that subtree ascode to be edited(e
p) and used as M ODIT ’s ﬁrst modality.
Similarly, we extract the code corresponding to the smallestsubtree encompassing all the edit operations from C
nand
use that as code after edit(e n). We use the commit message
associated with the function pair as M ODIT ’s second modality,
guidance(G ). Finally, we use the full method before edit (C p)
as M ODIT ’s third modality, context(C ).
C. Training
After combining every example in the datasets in M ODIT ’s
input (e p,G,C) and expected output (e n), we use this
combined dataset to train M ODIT . For training M ODIT ,w e
use Label Smoothed Cross Entropy [31] as loss function. Weuse Adam optimizer, with a learning rate of 5e
−5. We train
MODIT for 30 epochs, after every epoch, we run beam search
inference on the validation dataset. We stop training if thevalidation performance does not improve for ﬁve consecutivevalidations.
D. Evaluation Metric
We use the top-1 accuracy as the evaluation metric through-
out the paper. For proof-of-concept, we evaluate all techniques
with beam size 5. When the generated patched code matchesexactly with the expected patched code e
n, it is correct,
incorrect otherwise. Note that this is the most stringent metricfor evaluation. Previous approaches [6], [10], [11] talked aboutﬁltering out infeasible patches from a ranked list of top kpatches using test cases. However, we conjecture that suchtest cases may not always be available for general purposecode edits. Thus, we only compare top-1 accuracy.
E. Research Questions
M
ODIT contains several design components: (i) use of
multimodal information, (ii) use of transformer and initial-
izing it with the pre-trained model, and (iii) use of end-to-end encoder-decoder (using PLBART) to generate patchesinstead of separately using pre-trained encoder or pre-traineddecoder, as used by previous tools. First, we are interested inevaluating M
ODIT w.r .t. state-of-the-art methods. In particular,we evaluate how these three design choices effect M ODIT ’s
performance. So, we start with investigating,
RQ1. How accurately does M ODIT generate edited code
w.r.t. other techniques?
MODIT uses three input modalities. Our next evaluation
target is how these individual modalities effect M ODIT ’s
performance? Thus we ask,
RQ2. What are the contribution of different input
modalities in M ODIT ’s performance?
Finally, recall from Section III-A, M ODIT proposes to
encode all the input modalities as a sequence and use oneencoder for the consolidated multi-modal input. An alternativeto this encoding mechanism is to encode individual inputmodality with dedicated input encoder. Our next evaluationaims at ﬁnding out the best strategy to encode input modalities.Hence, we investigate,
RQ3. What is the best strategy to encode multiple input
modalities?
V. E
MPIRICAL RESULTS
In our ﬁrst research question, we evaluate M ODIT ’s perfor-
mance w.r .t. other techniques and the effect of M ODIT ’s design
components.
RQ1. How accurately does M ODIT generate edited code w.r.t.
other techniques?
Experimental Setup. We carefully chose the baselines to
understand the contribution from different design choices of
MODIT . We evaluated our model in two experimental settings.
First, we train different baseline models where the full modelis trained from scratch. In this setting, the ﬁrst baselinewe consider is an LSTM with attention [18] NMT model.V arious existing code patching approaches [5], [7], [8], [11]used such settings. Second baseline is Transformer [12] basedS2S model. We consider two different-sized transformers.
This enables us to contrast effect of model size in code-editing performance. The Transformer-base model consists of
six encoder layers and six decoder layers. The Transformer-
base model’s architecture is the same as M
ODIT ’s architecture.
Furthermore, we consider another transformer with a muchlarger architecture. Transformer-large contains twelve encoder
layers and twelve decoder layers with three times as manylearnable parameters as the Transformer-base model. The ﬁnal
baseline in this group is C
ODIT , which is a tree-based model.
Comparison w.r .t. CODIT allows us to contrast externally
given syntax information (in the form of CFG) and learnedsyntax by transformers (i.e., M
ODIT ). We use all three input
modalities (see Figure 3 for example) as input to the LSTMand Transformer. Using auxiliary modalities is non-trivial with
C
ODIT since the input to C ODIT must be a syntax-tree. Thus,
we use uni-modal input (e p) with C ODIT .
In the second setting, we consider different pre-trained
models, which we used to ﬁne-tune for patch generation.Figure 4 shows schematic diagrams of the pre-trained modelswe compared in this evaluation. First two models we consid-ered are CodeBERT [15], and GraphCodeBERT [16]. Both
447<s>if(first... }<s>first == </s>
first==null
null;;
Decoder Trained from
ScratchPretrained
Bidirectional Encoder
(a) CodeBERT — Consist of bidirectional pretrained
encoder and a decoder trained from scratch.
<SEP>first == </s>
first==null
null;;
;<SEP>
};
if(...
first... <s>if
Pretrained Left-to-Right Decoder
(b) CodeGPT — One pretrained single decoder processesthe input and output sequentially from left to right.
<s>if(first... }<s>first == </s>
first==null
null;;
Pretrained
Bidirectional EncoderPretrained Left-to-
Right Decoder
(c) PLBART — Consist of pretrained bidirectional encoderand pretrained left to right decoder.
Fig. 4: Schematic diagram of the three types of pre-trained models. used
to evaluate M ODIT .
of these models are pretrained encoders primarily trained to
understand code. To use these for the patching task, we add asix-layered transformer-based decoder along with the encoder.The decoder is trained from scratch (see Figure 4a). Anotherpre-trained baseline is CodeGPT [26]. GPT is a single left-to-right decoder model primarily pre-trained to generate code.For the code editing task, a special token <SEP> combines the
input and the output as a sequence separated. Jiang et al. [9]
showed the effectiveness of GPT for the source code patchingtask (see Figure 4b). In contrast to these pre-trained models,
M
ODIT uses PLBART, an end-to-end encoder-decoder model
trained to understand and generate code simultaneously (seeFigure 4c). To compare from a fairground, we evaluate thesepre-trained models with uni-modal input (e
p), and multi-modal
input (e p<s>G<s>C), separately.
TABLE II: Top-1 accuracies of different models w.r.t. their training
type, model sizes, input modality.
Training Model #of Multi- Accuracy (%)
Type Name params (M) Modal B2Fs B2Fm
LSTM 82.89  6.14 1.04
Transformer-base 139.22  11.18 6.61
Transformer-large 406.03  13.40 8.63From Scratch
CODIT 105.43  6.53 4.79
 24.28 16.76
CodeBER T 172.50 26.05 17.13
 24.44 16.85
GraphCodeBER T 172.50 25.67 18.31
 28.13 16.35
CodeGPT 124.44 28.43 17.64
 26.67 19.79Fine-tuned
MODIT 139.22 29.99 23.02
Results. Table II shows the accuracy in top 1 predictedpatch by M ODIT along with different baselines. LSTM based
S2S model predicted 6.14% and 1.04% correct patches in
B2FsandB2Fmrespectively. The Transformer-base model
achieves 11.18% and 6.61% top-1 accuracy in those datasets,which improves further to 13.40% and 8.63% with theTransformer-large model. C
ODIT predicts 6.53% and 4.79%
correct patches in B2FsandB2Fm, respectively. Note that
CODIT takes the external information in the form of CFG;
thus, the patches C ODIT generate are syntactically correct.
Nevertheless, the transformers, even the smaller model, per-form better to predict the correct patch. We conjecture that thetransformer model can implicitly learn the code syntax withoutdirect supervision.
In contrast to the models trained from scratch, when we
ﬁne-tune a pretrained model, it generates signiﬁcantly more
correct patches than models trained from scratch. For instance,
M
ODIT (initialized with pretrained PLBART) generates 168%
and 248% more correct patches than the Transformer-base
model (with randomly initialized parameters), despite bothof these models having the same architecture and the samenumber of parameters. In fact, the smallest ﬁne-tuned model(CodeGPT) performs much better than the larger model trainedfrom scratch (Transformer-large).
All the ﬁne-tuned models exhibit better performance when
the input data are multi-modal with various degrees of im-provement. With all three input modalities, CodeBERT [15]generates 7% and 2.2% more correct patches in B2F
sand
B2Fm, respectively, compared to a unimodal CodeBERT
model. In case of M ODIT , such improvement is 11.07% in
B2Fsand 16.23% in B2Fm. TheGin the multi-modal data
often contains explicit hints about how to change the code. Forinstance, consider the example shown in Figure 2, the guidanceexplicitly says there is a problem with the json when it is
empty. Furthermore, with the presence of Cin the input,
the model can identify different variables, methods used inthe method and potentially copy something from the context.We conjecture that such additional information from thesetwo additional input modalities (i) reduce the search spacefor change patterns, (ii) help models copy relevant identiﬁersfrom the context.
Among the ﬁne-tuned models multi-modalities, M
ODIT
generates 15.12% more correct patches than CodeBERT,16.82% than GraphCodeBERT, and 5.49% than CodeGPT inB2F
s. In the case of B2Fmdataset, M ODIT ’s improve-
ment in performance is 34.38%, 25.72%, 30.50% higher thanCodeBERT, GraphCodeBERT, and CodeGPT, respectively. Tounderstand these results better, let us look at some of theexamples.
Figure 5 shows an example patch where M
ODIT correctly
generated the expected patch but CodeGPT could not. If welook closely, we can see that the code to be changed (e
p)i sa
boolean expression where the two clauses are combines with&&. While only the ﬁrst clause, one.isSimilar(two) is
the expected output, CodeGPT chooses the second clause, one
.toString().equals(two.toString()) from the
original. Recall from Figure 4b, CodeGPT processes the com-
448//Guidance: merging of items that aren’t actually equal
public static boolean equals(
ItemStack one, ItemStack two){
- return one.isSimilar(two) &&
- (one.toString().equals(two.toString()));
+ return one.isSimilar(two); //MODIT generated
/*CodeGPT generated */
+ return one.toString().equals(two.toString());
}
Fig. 5: Example patch where M ODIT was able to generate correct patch,
but CodeGPT could not. M ODIT ’s patch is shown in green, and CodeGPT
generated patch is shown in blue .
bined input and output sequence (separated by special <SEP>
token) in left-to-right fashion. Thus, encodes representation of
the input tokens do not contain information about the whole
input sequence. In contrast, the M ODIT uses a pre-trained bi-
direction encoder which helps M ODIT to understand the input
fully. Based on the examples we have seen and the empiricalresult, we conjecture that, for code-editing tasks, the modelmust fully understand the input in a bi-directional fashion.
// Guidance: ... code refactoring ...
public boolean isEmpty() {
- if((first) == null){ return true;}
- return false;
+ return (first) == null; //MODIT predicts
/*CodeBERT generated */
+return ((first) == null) || (first.get()) == null;
}
Fig. 6: Correctly predicted patch by M ODIT . CodeBERT could not
understand and reason about the textual hint to predict the correct patch.
Figure 6 shows an example where M ODIT generated cor-
rect patch, CodeBERT could not. Note that the guidancetext explicitly asks about code refactoring, implying that the
patched code should be semantically similar to the originalcode. Similar to the original code, patched could should returntrue when first ==null , otherwise it should return
false . An automated code change tool should not add
additional code features when doing the refactoring. However,CodeBERT generated patch which introduced an additionalclause first.get()== null in the return expression,
which make CodeBERT’s generate code semantically differentfrom the original. M
ODIT was able to generate the correct
patch for this example.
Finally, we summarize the empirical lessons we learned in
this research question as
•Multi-modal input improves Code-Editing capability, ir-respective of the underlying model used. The guidanceoften narrows the edit pattern search space, and thecontext narrows down the token generation search space.
•Transformer models (especially larger ones) are robustenough to learn the code’s syntax information withoutdirect supervision. When a pre-trained model is usedto initialize transformer parameters, the improvement isnotably higher.
•For code-editing task, both understanding the input and
correctly generated output are important. While a pre-trained encoder understands the code and a pre-traineddecoder generates correct code, an end-to-end pre-trainedencoder-decoder model (e.g., PLBART) the best choice
to ﬁne-tune for this task.
Result 1: MODIT generates 29.99%, and 23.02% correct
patches in top-1 position for two different datasets outper-forming CodeBERT by up to 25.72%, GraphCodeBERT byup to 34.38%, and CodeGPT by up to 30.50%. Pre-trainedmodels tend to be more effective than models trained fromscratch for code editing— M
ODIT improves the performance
by 167% than the best model trained from the scratch.
MODIT combines multiple modalities of information to
generate patches. Now we investigate,
RQ2. What are the contribution of different input modalities
in M ODIT ’s performance?
Experimental Setup. In this experiment, we investigate the
contribution of different input modalities in M ODIT ’s perfor-
mance. Recall from Section III-A that we use three inputs
in M ODIT (i.e.,ep,C,G). Here, we investigate different
combinations of such input modalities. More precisely, weinvestigate the inﬂuence of three information sources: (i)code that needs to be changed (e
p), (ii) context (C ), and
(iii) guidance (G ). Note that, by presenting epas a separate
information modality, we are essentially providing M ODIT
with the information about the location of the change. To studythe effect of such presentation, we study another alternativeexperimental setup, where we annotate the change locationinside the context with two unique tokens <START> and
<END>.
TABLE III: Contribution of different input modalities in M ODIT ’s
performance.  indicates that corresponding input modality is used
as encoder input, indicates otherwise. We report top-1 accuracy as
performance measure. Exp. ID is used later to refer to corresponding
experiment result. Exp. ID Φ∗denotes an experiment with ∗as input
modalities.
Exp. IDInputs Accuracy (%)
ep C G B2Fs B2Fm
Φc    13.05 4.50
Φcg    17.89 4.51
Φ†
c  † 13.03 4.53
Φ†cg  † 17.90 4.60
Φe    26.67 19.79
Φeg    28.76 21.63
Φec    29.79 21.40
Φecg    29.99 23.02
†epis surrounded by two special tokens <START> and<END> inside the context.
Result. Table III shows M ODIT ’s performance with different
combination of input modalities. When we present only the
context to M ODIT , it predicts 13.05% correct patches in B2Fs
and 4.50% in the B2Fm, which improves further to 17.89%,
and 4.51% in those two datasets respectively when we add G.
Note that in these two scenarios, the model does not explicitlyknow which portion of the code needs to be edited; it sees the
449whole method and predicts (only) the patched code (e n). In
addition to learning how to patch, the model implicitly learns
where to apply the patch in this setup. To test whether theidentiﬁcation of such location is the performance bottleneck,we surround the code that needs to be patched with twospecial tokens <START> and<END>. SequenceR [11] also
proposed such annotation of buggy code. Surprisingly, suchannotation resulted in comparable (slightly worse in one case)performance by M
ODIT .
In the next set of experiments, we extract the code that
needs to be edited (e p) and present it as a separate input
modality. First, we only present the epwithout the other
two modalities. When we only present the epand generate
the edited code (e n), it results in 26.67% top-1 accuracy
in theB2Fsand 19.79% in the B2Fm. Ding et al. [32]
attributed such improvement to the reduced search space due toshorter input. Our result corroborates their empirical ﬁndings.Nevertheless, when we add the Gmodality with the e
p,
MODIT ’s performance improves to 28.76% and 21.63% in
B2FsandB2Fm, respectively.
In our ﬁnal set of experiments in this research question,
we augment epwith the C. In this evaluation setup, M ODIT
predicts 29.79% correct patches in the B2Fsand 21.40% in
theB2Fm, which is improved further to 29.99%, and 23.02%
correct patches in those two datasets when we add G.
//Guidance: fixed some bugs in type checking
// improved performance by caching types of expressions
private TypeCheckInfo getType(SadlUnionType expression){
...
return new TypeCheckInfo(
- declarationConceptName, declarationConceptName
/*MODIT generated patch with guidance */
+ declarationConceptName, declarationConceptName,
+ this, expression
/*MODIT generated patch without guidance */
+ this.declarationConceptName,
+ this.declarationConceptName
);
}
Fig. 7: Example showing the effect of textual guidance in M ODIT ’s
performance. M ODIT produced the correct patch with guidance, without
guidance as input M ODIT ’s produced patch is essentially refactored
version of original input.
Figure 7 shows an example where M ODIT with all
modalities could successfully generate correct patch. Thetext guidance (G ) provides hint that variable expression
should somehow associate with the construction of
TypeCheckInfo in the patched code. However, without
this guidance M
ODIT generated a wrong patch by accessing
existing parameters from this object. Essentially, without
the guidance, M ODIT refactored the input code.
Figure 8 shows the effect of context as input modality to
MODIT . The before edit version of the code(e p) passed the
wrong parameter (m)t o sendMessage function. When the
context (C ) is presented to M ODIT , it saw another variable
(sent) in the context. In contrast, without context(C ), M ODIT
indeed changed the parameter; but sent m.toString() —
resulting in a wrong patch.// Guidance: Fix bug of sending wrong message
public void setPredecessor (model.Message m){
this.predecessor =Integer.valueOf(m.Content);
model.Message sent =new model.Message();
sent.To =m.Origin;
- sendMessage(m);
/*MODIT generates with the context. */
+ sendMessage(sent);
/*MODIT generates without context as input. */
+ sendMessage(m.toString()) ;
}
Fig. 8: Example showing the necessity of context information in
predicting the correct patch. M ODIT ’s generated correct patch with the
context as input. Without context, M ODIT received sendMessage(m)
and the guidance as input, did not know the variable sent could be the
parameter of the function sendMessage, and predicted a wrong patch .
When we extract the buggy code and present the buggy
code along with the context, we see a big performanceimprovement (see the difference between Φ
c, andΦecin
Table III). We hypothesize that, when only context (i.e., full
code) is presented (Φ c), the model gets confused to identify
which portion from the context needs to be edited since anyportion of the code is a likely candidate for patching. However,when we extract the exact code that needs to be edited andpresent as a separate input modality to M
ODIT , it can focus on
patching just that code using other modalities (including thecontext) as a supporting source of information. In a recentstudy, Ding et al. [32] pointed out the need for effective
ways to include context in the NMT based code editors. Ourempirical results show that M
ODIT ’s way of including context
as a separate modality is a potential solution to that problem.
In summary, each of the modalities contribute to the overall
performances of M ODIT . Lessons learned in these experiments
are:
•Additional textual guidance helps the patch generation.Such guidance can provide important clue about howto modify the code and sometimes provide ingredientsnecessary for the change.
•Adding context explicitly in the input enables the modelto select appropriate identiﬁers for patching.
•Isolating buggy code help the model put proper focus onthe necessary part of the code while leveraging auxiliaryinformation from other modalities.
Result 2: All three modalities (code to be edited, context,
and guidance) are essential for MODIT to perform the
best. Without either one of those, performance decreases.
MODIT ’s performance improves up to 37.37% when addi-
tional textual guidance is used as an input modality. Contextmodality improves M
ODIT ’s performance up to 6.4%.
We investigate alternative ways to combine multiple input
modalities. We ask,
RQ3. What is the best strategy to encode multiple input
modalities?
Experimental setup. To validate M ODIT ’s design choice of
appending all input modalities into one sequence, we test
450...<s>if(first ... }Code Encoder
<s>public boolean isEmpty...}Context Encoder
<s> code refactoring...Guidance Encoder<s>first== </s>
first==null
null;;
Decoder
Fig. 9: An alternative architecture of code editing with multi-encoder
model. We initialize each of the encoders with pre-trained Encoder model.
alternative ways to combine input modalities. In particular,
we follow the design choice proposed by Lutellier et al. [10],
where they used multiple encoders to encode the epand
theC. Tufano et al. [33] also leverages a similar idea to
encode input code and code review messages. Nevertheless,we use a multi-encoder model shown in Figure 9. In a multi-encoder setting, we ﬁrst encode each input modality with acorresponding dedicated encoder. After the encoder ﬁnishesencoding, we concatenate the encoded representations and passthose to the decoder for generating patched code. To retainmaximum effectiveness, we initialize each individual encoderwith pretrained weights from CodeBERT [15]. We considera single-encoder model (also initialized with CodeBERT) asa baseline to compare on the fairground. While presentingthe inputs to the single encoder model, we concatenate inputmodalities with a unique separator token <s>. Finally, to
test the robustness of our empirical ﬁnding, we propose twodifferent experimental settings. In the ﬁrst evaluation setup, weuse all three input modalities. We compare a tri-encoder modelwith a single-encoder model. Next, we consider bimodal inputdata –e
pandG. We use a dual-encoder model and compare
it with a single-encoder model in this setup.
TABLE IV: Comparison of multi encoder model.
#o f #o f Accuracy (%)
Modalities Encoders B2Fs B2Fm
3(ep,G,C)3 20.63 11.69
1 26.05 17.13
2(ep,G)2 23.12 15.49
1 23.81 17.46
Result. Table IV shows the result of multi-encoder models.
For tri-modal input data, if we use three different encoders,the model can predict 20.63% correct patches in the B2F
sand
11.69% correct patches in the B2Fm. In contrast, if we use a
single encoder, the model’s predictive performance increasesto 26.05% and 17.13% top-1 accuracy in the B2F
sand the
B2Fm, respectively.
In the bimodal dataset (where the input modalities are
epandG), the dual-encoder model predicts 23.12% correct
patches in the top-1 position for the B2Fsand 15.49% correct
for theB2Fm. The single encoder counterpart, in this setup,
predicts 23.81% correct patches for the B2Fsand 17.46%
for theB2Fm. The empirical results show that the single-encoder model performs better in both the experimental setupthan the multi-encoder setup. We ﬁnd similar results withGraphCodeBERT [16].
Y1 X1X2Y2Decoder
Single Encoder
(a) Single encoder for encoding multiple-modalities. Encoder can
learn representation w.r.t. all modalities.
Y1Y2X1X2
Decoder Encoder 1
1
Encoder 2
(b) Dual-encoder for encoding individual modalities separately. Rep-resentation of tokens from a particular modality is learned w.r.t. (only)
other tokens from the same modality.
Fig. 10: Input token representation generation in single encoder and
multiple encoder.
To explain why single-encoder is performing better than
multi-encoder, let us look at the encoders’ working procedure.
Figure 10 depicts how the encoder generates representation forinput tokens. Note that the encoders we used in this researchquestion are transformer-based, and recall from the Section II,transformer generates representation for an input token bylearning its dependency on all other tokens in the sequence.When we present all the input modalities to a single encoder,it generates input representation for those tokens w.r .t. and
other tokens in the same modality and tokens from othermodalities. For instance, in Figure 10a, the encoder generatesX
2’s representation considering X1,Y1, andY2. In contrast,
in Figure 10, X2’s representation is learned only w.r .t.X1,
since encoder1 does not see the input modality Y. Thus, when
we present all the input modalities to one single encoder, weconjecture that learned representations are more robust thanthat of learning with multi-encoder.
Finally, we summarize the lessons we learned in this re-
search question as
•In multi-modal translation, using single encoder resultsin better performance than using a separate encoder foreach modality.
•Single-encoder generates input representation by inter-modality reasoning (attention), hence learns more robustrepresentation than that of multi-encoder.
Result 3: Encoding all the input modalities by a single
encoder is the best way to learn in a multi-modal setting.A single encoder improves code-editing performance by upto 46.5% than the corresponding multi-encoder setting.
VI. D
ISCUSSION
A. Localization of Code Edit Site
An alternative modeling approach for code editing is to
generate the sequence of edit operations (i.e., INSERT,
DELETE, UPDATE) [32], [34]–[36], where the model mustknow the precise location of an edit operation (often a node in
451the AST) before applying it. Throughout this paper, we also
assumed that such edit location is known to M ODIT . This
assumption may pose a threat to the usefulness of M ODIT in
a real development scenario. To mitigate such a threat, weperform an experiment where we pass the whole function asinput to M
ODIT and expect the whole edited function to be
generated. Table V shows the top-1 accuracy in the B2Fsand
TABLE V: Performance of M ODIT when the input in the full code and
the output is patched full code.
Inputs Accuracy (%)
Full Code Guidance B2Fs B2Fm
  20.35 8.35
  21.57 13.18
theB2Fm.M ODIT generates correctly patched full code in
20.35% cases for the B2Fsand 8.35% cases for the B2Fm.
With additional textual guidance, the performance is furtherimproved to 21.57% and 13.18% in the B2F
sandB2Fm,
respectively. While textual guidance helps in this experimentalsetup, we notice a big drop in performance than the resultsshown in Table III. This is because the benchmark datasets weused contain small edits (see Table I). Thus, while generatingthe full code, the model wastes a large amount of efforttrying to generate things that did not change. Nevertheless,our hypothesis external guidance improves code editing holds
even when the model generates full code.
B. Tokenization for Source Code Processing
TABLE VI: Comparison between concrete tokenization and abstract
tokenization alongside pre-trained models. Results are shown as top-1
accuracy of full code generation in B2Fs/B2Fmdatasets.
Token type CodeBERT GraphCodeBERT PLBART
Abstract 16.4 / 5.16 17.30 /9.10 19.21 / 8.98
Concrete 17.3 /8.38 16.65 / 8.64 20.35 / 8.35
The possible number of source code can be virtually inﬁnite.
V ocabulary explosion has been a big challenge while process-
ing source code with Machine Learning technique [7], [37].Previous research efforts have addressed this problem usingseveral different heuristics. For instance, Tufano et al. [7], [8]
identiﬁers abstraction, which drastically reduces the vocabu-lary size considered making it easier to learn patterns by themodel. Recent studies [9], [10], [32], [37] found that Byte-PairEncoding [38] partially solves the open-vocabulary problemby sub-dividing rare words into relatively less rare sub-words.Such sub-division is also learned from large corpora of data.All the pre-trained models used in this paper used sub-word to-kenization techniques. CodeBERT and GraphCodeBERT usedRoBERTa tokenizer [39], CodeGPT used GPT tokenizer [40],and PLBART used sentence-piece tokenizer [28]. The use ofsuch tokenizers strips away the burden of identiﬁer abstraction.Our investigation shows that, in some cases, pre-trained mod-els perform better with concrete tokens than abstract tokens(see Table VI for detailed result). Thus, we champion usinginput and outputs with concrete tokens when a pre-trainedmodel is used.
VII. R
ELA TED WORKS
A. Automatic Code Change
There are a lot of research efforts to capture repetitiveness of
developers’ way of editing source code. These researches showthe potential of automatic refactoring [41], [42], boilerplatecode [43] etc. These research efforts include (semi-)automatictools involving traditional program analysis techniques (e.g.,clone detection, dependency analysis, graph matching) [3],[44]. Other research direction aims at learning source code editfrom previous edits and applying those edit patterns in similarcontext [1], [45]. Some of these efforts targets very speciﬁccode changes; For example, Nguyen et al. [46] proposed
a graph-matching-based approach for automatically updatingAPI usage. Tansey et al. [47] semantic preserving transforma-
tion of java classes for automated refactoring. Other directionsof works address more general-purpose code change learnedfrom open source repositories [5], [6]. Such approaches tar-get solving automated code editing tasks in a data-drivenapproach, and the edit patterns are learned from examplechanges. In this research, we also investigated general purposesource code changes in the wild. More closely to M
ODIT ,
Rolim et al. [4]’s proposed technique constraints source code
generation with additional input/output speciﬁcation or testcases. Nevertheless, we argue that textual guidance could bea very good surrogate speciﬁcation.
B. NMT for Code Change Modeling
NMT has been studied for past couple of years to learn
automatic source code change modeling. Tufano et al. [5], [7],
[8] presented initial investigation of using NMT in learning
general purpose code changes. Chakraborty et al. [6] proposed
a tree based hierarchical NMT model for general purposesource code change. Instead of viewing code as sequenceof tokens, they ﬁrst generated syntax tree by sampling fromContext Free Grammar, and then another model to ﬁll up thegaps for identiﬁer. To reduce the search space, they performedscope analysis to search for suitable identiﬁer. Chen et al. [11]
proposed a copy mechanism based NMT model for APRwhere the input is the code before change along with thecontext, and the output is the code after change. Their worktreated the input as uni-modal way where the whole code isone singe modality. In this work, we consider multi-modal wayof modeling, where we isolate the code fragment that needsto be changed from its context and present that code fragmentconcatenated with context to the model. Lutellier et al. [10]
treated code needs to be changed and the context as twodifference modalities and use separate encoders. However, ourempirical evidence showed that using one encoder to encodeall the modalities result in the best performance. More recently,Ding et al. [32] presented empirical evidence that instead of
generating a whole code element (i.e., context+change) of the
target version, only generating the sequence of changes mightperform better for code change modeling. Recent works [35],
452[36] proposed models for generating such edit sequence. Such
models may augment or outperform NMT based code editing– we leave such investigation as future work.
C. Machine Learning for Source Code Analysis
In recent years, Machine Learning, especially Deep Learn-
ing has been widely adopted across different area of soft-
ware engineering due to Availability of large collection ofsource code in open source platforms (e.g., GitHub, Bitbucket,
etc.) Application of ML based source code analysis includebug detection in code [48], clone detection [49], code com-pletion [50], vulnerability detection [51], code summariza-tion [21], code translation [52], etc. Recent works also ap-proached to learn general purpose transferable representationlearning for source code, which can later be used for varioussource code related tasks [9], [15], [53]. The approachesfor learning such transferable representations can be broadlycategorized in two ways. The ﬁrst category of approaches(e.g., Code2V ec [53]) aims at learning explicit representation
for tokens in the code. Another category of approaches (e.g.,CodeBERT [15]) transfers syntactic and semantic interactionbetween code components in the form of pre-trained models.In this approach, a model for a speciﬁc task is initialized witha general-purpose pre-trained model, trained to understand andgenerate code. In this paper, we empirically found that suchpre-trained models (PLBART) increase accuracy upto 248%in patch generation.
VIII. T
HREA TS TO VALIDITY
A. External V alidity
Bias in the dataset. BothB2Fs, andB2Fmare collection
of bug-ﬁx commits, and thus there is a threat that these dataset
may exhibit speciﬁc bias towards bug-ﬁx patches. While thecommits in these datasets are ﬁltered and classiﬁed as bug ﬁxcommits, these changes are made by real developers as partof development life cycle. Unlike other bugﬁx datasets [54],B2F
sandB2Fmdo not isolate the bug. Thus, we conjecture
that possibility of existence of any such bias is minimal.
Noise in commit message. We used commit message as
a guidance for code editing. While previous research ef-
forts [55], [56] showed that commit messages are very usefulto summarize the changes in a commit, other research ef-forts [57], [58] also elucidated noises present in the commitmessage. To mitigate this threat, we carefully chose the datasetwe tested M
ODIT on. The original authors [8] of the the
dataset reported that they carefully investigated the datasetand after manual investigation, they reported that 97.6% ofthe commits in their datasets are true positive. Despite thisthreat, M
ODIT ’s performance seems to improve with commit
message as additional input.
B. Construct V alidity
In general, developers write commit message after they
edited the code, in theory, summarizing the edits they made. In
this paper, we assumed an experimental setup where developerwould write the summary before editing the code. Suchassumption may pose a threat to the applicability of M
ODIT in
real world, since in some cases, the developer may not knowwhat edits they are going to make prior to the actual editing.Regardless, we consider M
ODIT as a proof-of-concept, where
empirically we show that, if a developer had the idea of changein mind, that could help an automated code editor.
C. Internal Threat
All Deep Learning based techniques are sensitive to hyper-
parameters. Thus using a sub-optimal hyper-parameter can
pose a threat to the validity of M
ODIT , especially while
comparing with other baselines. As we compared with otherpre-trained models, we cannot really modify the architectureand dimensions of other pre-trained models. As for otherhyper-parameters (i.e., learning rate, batch size, etc.), we
use the exact same hyper-parameters described by respectivepaper. Nevertheless, we open source out code and data forbroader dissemination.
IX. C
ONCLUSION
In this paper, we highlight that an automatic code edit tool
should possess knowledge about the underlying programminglanguage, in general. Also, it can beneﬁt from additionalinformation such as edit context and developers’ intentionexpressed in natural language. To that end, we design, present,and evaluate M
ODIT – a multi-modal NMT-based automated
code editor. Our in-depth evaluation shows that M ODIT im-
proves code-editing by leveraging knowledge about program-ming language through pre-training. In addition, we showedthat leveraging additional modalities of information couldbeneﬁt the source code editor. Our empirical evaluation revealssome critical lessons about the design choices of buildingan automated code editor that we believe will guide futureresearch in automatic code editing.
A
CKNOWLEDGEMENT
This work is supported in part by NSF grants SHF-2107405,
SHF-1845893, IIS-2040961, IBM, and VMWare. Any opin-ions, ﬁndings, conclusions, or recommendations expressedherein are those of the authors and do not necessarily reﬂectthose of the US Government, NSF, IBM or VMWare.
R
EFERENCES
[1] B. Ray, M. Nagappan, C. Bird, N. Nagappan, and T. Zimmermann, “The
uniqueness of changes: Characteristics and applications,” ser. MSR ’15.
ACM, 2015.
[2] N. Meng, M. Kim, and K. S. McKinley, “Systematic editing: generating
program transformations from an example,” ACM SIGPLAN Notices,
vol. 46, no. 6, pp. 329–342, 2011.
[3] ——, “Lase: Locating and applying systematic edits by learning from
examples,” In Proceedings of 35th International Conference on Software
Engineering (ICSE), pp. 502–511, 2013.
[4] R. Rolim, G. Soares, L. D’Antoni, O. Polozov, S. Gulwani, R. Gheyi,
R. Suzuki, and B. Hartmann, “Learning syntactic program transfor-mations from examples,” in Proceedings of the 39th International
Conference on Software Engineering. IEEE Press, 2017, pp. 404–415.
[5] M. Tufano, J. Pantiuchina, C. Watson, G. Bavota, and D. Poshyvanyk,
“On learning meaningful code changes via neural machine translation,”arXiv preprint arXiv:1901.09102, 2019.
453[6] S. Chakraborty, Y . Ding, M. Allamanis, and B. Ray, “Codit: Code
editing with tree-based neural models,” IEEE Transactions on Software
Engineering, vol. 1, pp. 1–1, 2020.
[7] M. Tufano, C. Watson, G. Bavota, M. Di Penta, M. White, and
D. Poshyvanyk, “An empirical investigation into learning bug-ﬁxing
patches in the wild via neural machine translation,” 2018.
[8] M. Tufano, C. Watson, G. Bavota, M. D. Penta, M. White, and
D. Poshyvanyk, “An empirical study on learning bug-ﬁxing patches inthe wild via neural machine translation,” ACM Transactions on Software
Engineering and Methodology (TOSEM), vol. 28, no. 4, pp. 1–29, 2019.
[9] N. Jiang, T. Lutellier, and L. Tan, “Cure: Code-aware neural
machine translation for automatic program repair,” arXiv preprint
arXiv:2103.00073, 2021.
[10] T. Lutellier, H. V . Pham, L. Pang, Y . Li, M. Wei, and L. Tan, “Coconut:
combining context-aware neural translation models using ensemble forprogram repair,” in Proceedings of the 29th ACM SIGSOFT International
Symposium on Software Testing and Analysis, 2020, pp. 101–114.
[11] Z. Chen, S. J. Kommrusch, M. Tufano, L.-N. Pouchet, D. Poshyvanyk,
and M. Monperrus, “Sequencer: Sequence-to-sequence learning for end-to-end program repair,” IEEE Transactions on Software Engineering,
2019.
[12] A. V aswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
L. u. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances
in Neural Information Processing Systems 30, 2017, pp. 5998–6008.
[13] Z. Liu, X. Xia, A. E. Hassan, D. Lo, Z. Xing, and X. Wang, “Neural-
machine-translation-based commit message generation: How far arewe?” in 2018 33rd IEEE/ACM International Conference on Automated
Software Engineering (ASE), 2018, pp. 373–384.
[14] W. Wang, G. Li, S. Shen, X. Xia, and Z. Jin, “Modular tree network
for source code representation learning,” ACM Transactions on Software
Engineering and Methodology (TOSEM), vol. 29, no. 4, pp. 1–23, 2020.
[15] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin,
T. Liu, D. Jiang, and M. Zhou, “CodeBERT: A pre-trained model forprogramming and natural languages,” in Findings of the Association for
Computational Linguistics: EMNLP 2020, Nov. 2020, pp. 1536–1547.
[16] D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu, L. Zhou, N. Duan,
J. Yin, D. Jiang et al., “Graphcodebert: Pre-training code representations
with data ﬂow,” in International Conference on Learning Representa-
tions, 2021.
[17] W. U. Ahmad, S. Chakraborty, B. Ray, and K.-W. Chang, “Uniﬁed pre-
training for program understanding and generation,” in 2021 Annual
Conference of the North American Chapter of the Association forComputational Linguistics (NAACL), 2021.
[18] D. Bahdanau, K. Cho, and Y . Bengio, “Neural machine translation by
jointly learning to align and translate,” in International Conference on
Learning Representations, 2015.
[19] P . Yin and G. Neubig, “A syntactic neural model for general-purpose
code generation,” in Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics (V olume 1: Long Papers),vol. 1, 2017, pp. 440–450.
[20] B. Wei, G. Li, X. Xia, Z. Fu, and Z. Jin, “Code generation as a dual task
of code summarization,” in Advances in Neural Information Processing
Systems 32, 2019, pp. 6563–6573.
[21] W. U. Ahmad, S. Chakraborty, B. Ray, and K.-W. Chang, “A
transformer-based approach for source code summarization,” in Proceed-
ings of the 58th Annual Meeting of the Association for ComputationalLinguistics (ACL), 2020.
[22] S. Kim, J. Zhao, Y . Tian, and S. Chandra, “Code prediction by feeding
trees to transformers,” arXiv preprint arXiv:2003.13848, 2020.
[23] A. Svyatkovskiy, S. K. Deng, S. Fu, and N. Sundaresan, “Intellicode
compose: Code generation using transformer,” in Proceedings of the
28th ACM Joint Meeting on European Software Engineering Conferenceand Symposium on the F oundations of Software Engineering, 2020, pp.1433–1443.
[24] A. Kanade, P . Maniatis, G. Balakrishnan, and K. Shi, “Pre-trained
contextual embedding of source code,” arXiv preprint arXiv:2001.00059,
2019.
[25] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y . Wang, J. Gao, M. Zhou,
and H.-W. Hon, “Uniﬁed language model pre-training for natural lan-guage understanding and generation,” arXiv preprint arXiv:1905.03197,
2019.
[26] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco,
C. Clement, D. Drain, D. Jiang, D. Tang et al., “Codexglue:
A machine learning benchmark dataset for code understandingand generation,” arXiv preprint arXiv:2102.04664, 2021. [Online].
Available: https://arxiv.org/abs/2102.04664
[27] M. Lewis, Y . Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,
V . Stoyanov, and L. Zettlemoyer, “Bart: Denoising sequence-to-sequencepre-training for natural language generation, translation, and comprehen-sion,” arXiv preprint arXiv:1910.13461, 2019.
[28] T. Kudo and J. Richardson, “SentencePiece: A simple and language
independent subword tokenizer and detokenizer for neural text process-ing,” in Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations, Nov. 2018, pp.66–71.
[29] R.-M. Karampatsis, H. Babii, R. Robbes, C. Sutton, and A. Janes, “Big
code!= big vocabulary: Open-vocabulary models for source code,” in2020 IEEE/ACM 42nd International Conference on Software Engineer-ing (ICSE). IEEE, 2020, pp. 1073–1085.
[30] J.-R. Falleri, F. Morandat, X. Blanc, M. Martinez, and M. Monperrus,
“Fine-grained and accurate source code differencing,” in Proceedings
of the 29th ACM/IEEE international conference on Automated softwareengineering. ACM, 2014, pp. 313–324.
[31] R. M ¨uller, S. Kornblith, and G. Hinton, “When does label smoothing
help?” arXiv preprint arXiv:1906.02629, 2019.
[32] Y . Ding, B. Ray, P . Devanbu, and V . J. Hellendoorn, “Patching as
translation: the data and the metaphor,” in 2020 35th IEEE/ACM
International Conference on Automated Software Engineering (ASE).IEEE, 2020, pp. 275–286.
[33] R. Tufano, L. Pascarella, M. Tufano, D. Poshyvanyk, and G. Bavota,
“Towards automating code review activities,” arXiv preprint
arXiv:2101.02518, 2021.
[34] E. Dinella, H. Dai, Z. Li, M. Naik, L. Song, and K. Wang, “Hoppity:
Learning graph transformations to detect and ﬁx bugs in programs,” inInternational Conference on Learning Representations, 2019.
[35] D. Tarlow, S. Moitra, A. Rice, Z. Chen, P .-A. Manzagol, C. Sutton, and
E. Aftandilian, “Learning to ﬁx build errors with graph2diff neural net-works,” in Proceedings of the IEEE/ACM 42nd International Conference
on Software Engineering Workshops, 2020, pp. 19–20.
[36] Z. Yao, F. F. Xu, P . Yin, H. Sun, and G. Neubig, “Learning
structural edits via incremental tree transformations,” arXiv preprint
arXiv:2101.12087, 2021.
[37] R. M. Karampatsis and C. Sutton, “How often do single-statement bugs
occur? The ManySStuBs4J dataset,” arXiv preprint arXiv:1905.13334,
2019.
[38] R. Sennrich, B. Haddow, and A. Birch, “Neural machine translation of
rare words with subword units,” arXiv preprint arXiv:1508.07909, 2015.
[39] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,
M. Lewis, L. Zettlemoyer, and V . Stoyanov, “Roberta: A robustlyoptimized bert pretraining approach,” arXiv preprint arXiv:1907.11692,
2019. [Online]. Available: https://arxiv.org/abs/1907.11692
[40] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,
“Language models are unsupervised multitask learners,” OpenAI blog,
vol. 1, no. 8, p. 9, 2019.
[41] X. Ge, Q. L. DuBose, and E. Murphy-Hill, “Reconciling manual
and automatic refactoring,” in Proceedings of the 34th International
Conference on Software Engineering. IEEE Press, 2012, pp. 211–221.
[42] V . Raychev, M. Sch ¨afer, M. Sridharan, and M. V echev, “Refactoring
with synthesis,” in ACM SIGPLAN Notices, vol. 48, no. 10. ACM,
2013, pp. 339–354.
[43] N. Meng, L. Hua, M. Kim, and K. S. McKinley, “Does automated
refactoring obviate systematic editing?” in Proceedings of the 37th
International Conference on Software Engineering-V olume 1. IEEEPress, 2015, pp. 392–402.
[44] R. Robbes and M. Lanza, “Example-based program transformation,” in
International Conference on Model Driven Engineering Languages andSystems. Springer, 2008, pp. 174–188.
[45] H. A. Nguyen, A. T. Nguyen, T. T. Nguyen, T. N. Nguyen, and H. Rajan,
“A study of repetitiveness of code changes in software evolution,”inProceedings of the 28th IEEE/ACM International Conference on
Automated Software Engineering. IEEE Press, 2013, pp. 180–190.
[46] H. A. Nguyen, T. T. Nguyen, G. Wilson Jr, A. T. Nguyen, M. Kim, and
T. N. Nguyen, “A graph-based approach to API usage adaptation,” inACM Sigplan Notices, vol. 45, no. 10. ACM, 2010, pp. 302–321.
[47] W. Tansey and E. Tilevich, “Annotation refactoring: inferring upgrade
transformations for legacy applications,” in ACM Sigplan Notices,
vol. 43, no. 10. ACM, 2008, pp. 295–312.
454[48] B. Ray, V . Hellendoorn, S. Godhane, Z. Tu, A. Bacchelli, and P . De-
vanbu, “On the” naturalness” of buggy code,” in 2016 IEEE/ACM 38th
International Conference on Software Engineering (ICSE). IEEE, 2016,
pp. 428–439.
[49] W. Wang, G. Li, B. Ma, X. Xia, and Z. Jin, “Detecting code clones with
graph neural network and ﬂow-augmented abstract syntax tree,” in 2020
IEEE 27th International Conference on Software Analysis, Evolutionand Reengineering (SANER). IEEE, 2020, pp. 261–271.
[50] M. R. Parvez, S. Chakraborty, B. Ray, and K.-W. Chang, “Building
language models for text with named entities,” 2018.
[51] S. Chakraborty, R. Krishna, Y . Ding, and B. Ray, “Deep learning
based vulnerability detection: Are we there yet,” IEEE Transactions on
Software Engineering, pp. 1–1, 2021.
[52] H. Xu, S. Fan, Y . Wang, Z. Huang, H. Xu, and P . Xie, “Tree2tree
structural language modeling for compiler fuzzing,” in International
Conference on Algorithms and Architectures for Parallel Processing .
Springer, 2020, pp. 563–578.
[53] U. Alon, M. Zilberstein, O. Levy, and E. Yahav, “code2vec: Learning
distributed representations of code,” in Proceedings of the ACM
on Programming Languages, vol. 3. ACM, 2019, p. 40. [Online].Available: https://doi.org/10.1145/3290353[54] R. Just, D. Jalali, and M. D. Ernst, “Defects4J: A database of existing
faults to enable controlled testing studies for java programs,” in Pro-
ceedings of the 2014 International Symposium on Software Testing andAnalysis. ACM, 2014, pp. 437–440.
[55] Y . Zhou, S. Liu, J. Siow, X. Du, and Y . Liu, “Devign: Effective vulner-
ability identiﬁcation by learning comprehensive program semantics viagraph neural networks,” in Advances in Neural Information Processing
Systems, vol. 32, 2019, pp. 10 197–10 207.
[56] Y . Zhou and A. Sharma, “Automated identiﬁcation of security issues
from commit messages and bug reports,” in Proceedings of the 2017
11th joint meeting on foundations of software engineering, 2017, pp.914–919.
[57] K. Gallaba, C. Macho, M. Pinzger, and S. McIntosh, “Noise and
heterogeneity in historical build data: an empirical study of travis ci,”inProceedings of the 33rd ACM/IEEE International Conference on
Automated Software Engineering, 2018, pp. 87–97.
[58] S. Kim, H. Zhang, R. Wu, and L. Gong, “Dealing with noise in
defect prediction,” in 2011 33rd International Conference on Software
Engineering (ICSE). IEEE, 2011, pp. 481–490.
455