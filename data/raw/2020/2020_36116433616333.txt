DecompoVision: Reliability Analysis of Machine Vision
Components through Decomposition and Reuse
Boyue Caroline Hu
University of Toronto
Toronto, Ontario, Canada
boyue@cs.toronto.eduLina Marsso
University of Toronto
Toronto, Ontario, Canada
lina.marsso@utoronto.caNikita Dvornik
Waabi
Toronto, Ontario, Canada
nikita.dvornik@proton.me
Huakun Shen
University of Toronto
Toronto, Ontario, Canada
huakunshen@cs.toronto.eduMarsha Chechik
University of Toronto
Toronto, Ontario, Canada
chechik@cs.toronto.edu
ABSTRACT
Analyzing reliability of Machine Vision Components (MVC) against
scene changes (such as rain or fog) in their operational environment
is crucial for safety-critical applications. Safety analysis relies on the
availability of precisely specified and, ideally, machine-verifiable
requirements. The state-of-the-art reliability framework ICRAF de-
veloped machine-verifiable requirements obtained using human
performance data. However, ICRAF is limited to analyzing reliabil-
ity of MVCs solving simple vision tasks, such as image classification.
Yet, many real-world safety-critical systems require solving more
complex vision tasks, such as object detection and instance seg-
mentation. Fortunately, many complex vision tasks (which we call
â€œc-tasksâ€) can be represented as a sequence of simple vision subtasks.
For instance, object detection can be decomposed as object localiza-
tion followed by classification. Based on this fact, in this paper, we
show that the analysis of c-tasks can also be decomposed as a se-
quential analysis of their simple subtasks, which allows us to apply
existing techniques for analyzing simple vision tasks. Specifically,
we propose a modular reliability framework, DecompoVision, that
decomposes: (1) the problem of solving a c-task, (2) the reliability
requirements, and (3) the reliability analysis, and, as a result, pro-
vides deeper insights into MVC reliability. DecompoVision extends
ICRAF to handle complex vision tasks and enables reuse of existing
artifacts across different c-tasks. We capture new reliability gaps
by checking our requirements on 13 widely used object detection
MVCs, and, for the first time, benchmark segmentation MVCs.
CCS CONCEPTS
â€¢Software and its engineering â†’Requirements analysis ;â€¢
Computing methodologies â†’Computer vision .
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
Â©2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0327-0/23/12. . . $15.00
https://doi.org/10.1145/3611643.3616333KEYWORDS
Software Engineering for Artificial Intelligence, Requirements En-
gineering, Software Analysis, Machine Learning, Computer Vision
ACM Reference Format:
Boyue Caroline Hu, Lina Marsso, Nikita Dvornik, Huakun Shen, and Marsha
Chechik. 2023. DecompoVision: Reliability Analysis of Machine Vision
Components through Decomposition and Reuse. In Proceedings of the 31st
ACM Joint European Software Engineering Conference and Symposium on the
Foundations of Software Engineering (ESEC/FSE â€™23), December 3â€“9, 2023, San
Francisco, CA, USA. ACM, New York, NY, USA, 12 pages. https://doi.org/10.
1145/3611643.3616333
1 INTRODUCTION
Machine Learning has enabled unprecedented progress in computer
vision (CV) and now allows to fully automate vision tasks tradition-
ally performed by humans [ 19]. Yet most modern CV models can be
surprisingly unstable w.r.t. perturbations to the input images [ 36].
Since machine vision components (MVCs) are currently used in
safety-critical systems (e.g., self-driving cars), MVC prediction er-
rors can lead to fatal accidents [ 40]. Thus, the instability in MVCâ€™s
predictions is a major safety concern that motivates the study of
system reliability against visual changes in the environment [11].
The central challenge in MVC reliability analysis is the absence of
precisely specified and machine-verifiable requirements. To address
this challenge, the SoTA Image Classification Reliability Analysis
Framework (ICRAF) [ 12] obtains the machine-verifiable require-
ments using human perception performance as a baseline, via exper-
iments with human participants. Specifically, ICRAFâ€™s requirements
state that if changes in a scene do not affect humans solving a vision
task, they should not affect an MVC solving the same task either.
However, a fundamental limitation of ICRAF is that it is only appli-
cable to a single atomic vision task (with one type of output), i.e.,
image classification (that, given image, entails producing a single
class label as output). At the same time, many critical applications
involve complex vision tasks that require producing multiple types
of outputs [ 8]. For example, parsing the surrounding environment
for autonomous driving requires solving object detection (that en-
tails joint object localization and classification) resulting in two
output types: object bounding boxes and their categories. Auto-
matic tumor analysis involves instance segmentation (that consists
of localization, classification and outline detection) involving three
outputs types: object boxes, their categories, and segmentation
541
ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Boyue Caroline Hu, Lina Marsso, Nikita Dvornik, Huakun Shen, and Marsha Chechik
Figure 1: Examples of c-task decomposition: object detection
and instance segmentation. Top: decomposition of c-tasks
into their subtasks. Object detection and instance segmenta-
tion share two subtasks (in dotted frame): localization (blue)
and classification of localized objects (orange). Bottom: each
vision task is illustrated with an image from the Pascal VOC
dataset [4].
S: System 
solving VV: Complex vision task 
(c-task) o1: output 
(type 1) 
on: output 
(type n) â€¦MV: performance metrics 
measuring how well S 
solved VWorkflow for 
analyzing & 
solving c-tasks 
Decomposition Decomposable Problem 
 (Eq. 1) Not 
Decomposed Decomposable Metric 
 (Eq. 3) 
Example VD: Object Detection S: MVC Detector MV: Average Precision oL: object boxes   
oC|L: object classes 
Figure 2: Overview of the common workflow for analyzing
and solving a c-task. First row gives the workflow for solv-
ing a c-task. Second row displays the decomposition charac-
teristics of each above module in the workflow. Third row
illustrates the modules for the c-task object detection.
masks. Therefore, there is a need of machine-verifiable reliability
requirements for tasks involving different output types.
C-task : a class of complex vision tasks. In this paper, we focus
on complex vision tasks that can be represented as a sequence
of atomic vision subtasks [ 8]; we call this class of tasks â€œc-tasksâ€.
Many c-tasks are required for real-world safety-critical systems
which include but are not limited to object detection, human-object
interaction detection, trajectory prediction and language visual
grounding. For example, as shown in Fig. 1, object detection can be
sequentially decomposed into first localizing objects ( vğ¿) and then
classifying the localized objects ( vğ¶|ğ¿). The common workflow for
solving and analyzing a c-task is illustrated in Fig. 2 (top). Given a
c-task, some system (or component), such as an MVC or a human,
solves it and produces multiple output types (e.g., bounding boxes
and object categories), each solving the corresponding subtask.
Then, performance metrics are applied to all systemâ€™s outputs to
quantify how well the task was solved (e.g., Average Precision (AP)
is used to measure the quality of an object detector).
Decomposition principle. In this paper, we address reliability
analysis of MVCs performing c-tasks with machine-verifiable re-
quirements. To do so, we define a decomposition principle, DP, for
decomposing both the problem of solving c-tasks and the perfor-
mance metrics measuring how well a system solves a c-task, see
Fig. 2 (bottom). Specifically, DP decomposes c-tasks into the corre-
sponding atomic subtasks, each assuming a single output type, suchas there is a bijection between the output types of the c-task and
of the atomic subtasks. Analogously, DP decomposes the perfor-
mance analysis of a c-task via sequentially analysing performance
metrics of its subtasks. As a result, DP can decompose the entire
workflow without decomposing the process solving the c-task it-
self which enables black-box analysis. Note that while we restrict
ourselves to c-tasks, we are not aware of complex tasks deployed
on safety-critical systems that are not decomposable with DP.
Modular reliability framework. Based on our DP, we develop
a modular reliability framework DecompoVision (see Fig. 3) for
c-tasks, which builds on top of ICRAF [ 12]. Given a c-task, our
framework first decomposes it into atomic subtasks (Step I). Then,
for each subtask independently, given an image transformation
simulating scene changes, we use human performance data to gen-
erate machine-verifiable reliability requirements (Step II.a and II.b).
Consequently, we compose the individual subtask requirements to
get the requirements for the c-task (Step II.c). Finally, we propose
a black-box checking method for both the overall c-task and the
subtask requirements, which enables failure localization in the se-
quence of subtasks. Crucially, the modularity of DecompoVision
allows us to reuse human performance data, requirements specifi-
cations, and analysis artifacts for shared subtasks across different
c-tasks. For example, instance segmentation and object detection
share two subtasks, vğ¿andvğ¶|ğ¿(see Fig. 1), thus, the experiment
data, requirements and testing data for vğ¿andvğ¶|ğ¿can potentially
be shared between instance segmentation and object detection.
Contributions. In this paper, (1) we define the decomposition prin-
ciple (DP) for c-tasks and their performance metrics. Then, using DP
for c-task, we develop a modular framework, DecompoVision, that
(2) generates machine-verifiable reliability requirements for the
c-task and its subtasks, (3) checks satisfaction of the requirements,
and (4) builds the experiment measuring human performance on ob-
ject detection. We also provide human performance data for object
detection and instance segmentation from about 2900 participants.
Evaluation. While DecompoVision is defined for any c-task, we
demonstrate its feasibility on object detection and instance segmen-
tation. Using 13 widely used object detection MVCs and the PASCAL
VOC dataset [ 4], we show that DecompoVision captures reliability
gaps that the SoTA object detection reliability benchmark [ 23] is
unable to detect. Moreover, we are the first to benchmark instance
segmentation MVCs. Finally, we show that reusing human exper-
iment data reduces experiment time by 33%, and reusing testing
data reduces runtime of checking requirements satisfaction by 87%.
Significance. Existing approaches propose reliability analysis of
image classification MVCs from different SE perspectives, including
requirements [ 12], testing [ 37], and verification [ 15]. DecompoVi-
sion allows extending the above reliability approaches to analyze
MVCs performing c-tasks instead of re-implementing new analysis
approaches. Furthermore, DecompoVision analyzes the MVC relia-
bility not only on the c-tasks but also on their subtasks. This leads to
a deeper insight into the MVC reliability, such as fault localization,
and guides further MVC development. Finally, the subtasks-based
modular design of our framework enables reuse of analysis artifacts
of atomic vision tasks.
542DecompoVision: Reliability Analysis of Machine Vision Components through Decomposition and Reuse ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
This paper is organized as follows: Sec. 2 provides background.
Sec. 3 develops the decomposition principle DP. Sec. 4 describes
the experiment with human participants. Sec. 5 describes reliability
requirements generation for an MVC on a c-task and its sub-tasks.
Sec. 6 introduces our method for checking the MVC against the reli-
ability requirements. Sec. 7 reports on the experimental evaluation.
Sec. 8 discusses related work. Sec. 9 concludes the paper.
2 BACKGROUND
The ICRAF framework [ 12] enables reliability analysis of image
classification MVCs against scene changes that do not affect human
performance, i.e., changes within the human-tolerated range . For
example, humans have no problem recognizing cars in the images
with added frost in the first three images in Fig. 4 (as opposed to
the last image in Fig. 4), thus the amount of added frost of these
images is within the human-tolerated range. To analyse reliability,
ICRAF proposes two requirement classes, correctness-preservation
and prediction-preservation , that are parameterized by an image
transformation simulating scene changes. Below, we review the
key concepts of the ICRAF framework [ 12] for image classification:
the machine-verifiable reliability requirements classes that use hu-
man performance as a baseline, how the human performance data
is collected and used to estimate the reliability requirements param-
eters, and, the method for checking the requirements satisfactions.
Reliability requirements. Given an MVC ğ‘“ğ‘‰performing a vi-
sion taskğ‘‰, a distribution of input images ğ‘ƒğ‘‹, a transformation
ğ‘‡ğ‘‹with parameter domain ğ¶and parameter distribution ğ‘ƒğ¶, the
requirement classes correctness-preservation (ğ‘ğ‘) and prediction-
preservation (ğ‘ğ‘) are defined on the joint distribution, ğ‘ƒğ‘‡ğ‘‹(ğ‘¥,ğ‘¥â€²), of
pairs of original and transformed images. To measure the degree of
visual change (between original and transformed images), ICRAF
proposes a metric ğ‘£ğ‘–ğ‘ ğ‘¢ğ‘ğ‘™ _ğ‘â„ğ‘ğ‘›ğ‘”ğ‘’ (Î”ğ‘£), which allows specifying re-
quirements for any pixel-level image transformation. Examples of
Î”ğ‘£caused by the frost transformation are shown in Fig. 4.
Thecorrectness-preservation reliability requirement class, ğ‘ğ‘(ğ‘‰,ğœ“ğ‘ğ‘,
ğ‘‡ğ‘‹,ğ‘¡ğ‘), assumes the ground-truth labeling function ğ‘“âˆ—
ğ‘‰and a per-
formance measure ğœ“ğ‘ğ‘(ğ‘“ğ‘‰,ğ‘“âˆ—
ğ‘‰,ğ‘ƒğ‘‹)(which measures similarity be-
tween the output of ğ‘“ğ‘‰andğ‘“âˆ—
ğ‘‰, given input ğ‘¥âˆ¼ğ‘ƒğ‘‹) and a dis-
tribution of transformed images with changes within the human
tolerated-range ( Î”ğ‘£â‰¤ğ‘¡ğ‘), i.e.,ğ‘ƒğ‘‡ğ‘‹,ğ‘¡ğ‘.
Definition 1 (Correctness-preservation reqirement). For
the range of changes in images that do not affect human performance
(Î”ğ‘£â‰¤ğ‘¡ğ‘), the performance of MVC,ğ‘“ğ‘‰, should not be affected either.
Formally, the performance evaluated by metric ğœ“ğ‘ğ‘ofğ‘“ğ‘‰for trans-
formed images is required to be greater than that for original images:
ğœ“ğ‘ğ‘(ğ‘“ğ‘‰,ğ‘“âˆ—
ğ‘‰,ğ‘ƒğ‘‡ğ‘‹,ğ‘¡ğ‘)â‰¥ğœ“ğ‘ğ‘(ğ‘“ğ‘‰,ğ‘“âˆ—
ğ‘‰,ğ‘ƒğ‘‹).
Example : Letğ‘‡ğ‘‹be the transformation â€œadding artificial frostâ€, and
ğ‘¡ğ‘be0.84. The instantiated correctness-preservation requirement
for an image classification MVC is: â€œThe recognition accuracy of
an MVC should not decrease if the visual change in the images is
within the range Î”ğ‘£â‰¤0.84â€.
Theprediction-preservation requirement class, ğ‘ğ‘(ğ‘‰,ğœ“ğ‘ğ‘,ğ‘‡ğ‘‹,ğ‘¡ğ‘),
requires a prediction-similarity measureğœ“ğ‘ğ‘(ğ‘“,ğ‘ƒğ‘‹Ã—ğ‘‹)(which mea-
sures the expected similarity between the output of ğ‘“ğ‘‰on an image
pair drawn from ğ‘ƒğ‘‹Ã—ğ‘‹), a distribution of original and transformed
image pairs that are within the human-tolerated range ( Î”ğ‘£â‰¤ğ‘¡ğ‘),i.e.,ğ‘ƒğ‘‡ğ‘‹,ğ‘¡ğ‘(ğ‘¥,ğ‘¥â€²), and a distribution of original and minimally trans-
formed image pairs that are within the minimal image change
(Î”ğ‘£â‰¤ğœ–), i.e.,ğ‘ƒğ‘‡ğ‘‹,ğœ–(ğ‘¥,ğ‘¥â€²).
Definition 2 (Prediction-preservation reqirement). For
the range of changes in images that do not affect human predic-
tions, the predictions of MVC should remain unaffected. Formally,
the prediction similarity evaluated by metric ğœ“ğ‘ğ‘ofğ‘“ğ‘‰, for all trans-
formed images ( Î”ğ‘£â‰¤ğ‘¡ğ‘), is required to be greater than or equal to
that for minimally transformed images ( Î”ğ‘£â‰¤ğœ–):ğœ“ğ‘ğ‘(ğ‘“ğ‘‰,ğ‘ƒğ‘‡ğ‘‹,ğ‘¡ğ‘)â‰¥
ğœ“ğ‘ğ‘(ğ‘“ğ‘‰,ğ‘ƒğ‘‹,ğœ–).
Example : Letğ‘‡ğ‘‹be the transformation â€œadding artificial frostâ€ and
ğ‘¡ğ‘be0.91. The instantiated ğ‘ğ‘requirement for an image classifica-
tion MVC is â€œthe percentage of labels the MVC can preserve after
adding frost should not decrease if visual change in the images is
within the range Î”ğ‘£â‰¤0.91".
Experiments with human participants. In order to estimate
the parameters of the requirement classes â€“ the human-tolerated
thresholdsğ‘¡ğ‘andğ‘¡ğ‘(for correctness-preservation and prediction-
preservation , respectively), ICRAF collects human performance data
on image classification. The goal of the experiment is to evaluate
average human performance on original and transformed images
(with different degrees of visual change), in order to understand
the range of visual change, where the average human performance
stays unaffected. Specifically, the experiment is performed on a
given dataset,{ğ‘¥ğ‘–|ğ‘¥ğ‘–âˆ¼ğ‘ƒğ‘¥}, for a specific image transformation,
ğ‘‡ğ‘‹. First, an image ğ‘¥ğ‘–is randomly selected from the dataset and
then transformed, i.e., ğ‘¥â€²
ğ‘–,ğ‘—=ğ‘‡ğ‘‹(ğ‘¥ğ‘–,ğ‘ğ‘—), where the transformation
parameters ğ‘ğ‘—are selected to cover all possible visual changes Î”ğ‘£.
This results in a dataset of original and transformed image pairs
{(ğ‘¥ğ‘–,ğ‘¥â€²
ğ‘–,ğ‘—)},(ğ‘¥ğ‘–,ğ‘¥â€²
ğ‘–,ğ‘—)âˆ¼ğ‘ƒğ‘‡ğ‘‹. To obtain the human predictions on
each image in{(ğ‘¥ğ‘–,ğ‘¥â€²
ğ‘–,ğ‘—)}, the ICRAF experiment is a forced-choice
image categorisation task : humans are presented with an image for
200 ms, and asked to choose one of the two categories (e.g., car or
not car). Between the classification attempts, a noise mask is shown
to minimize feedback influence in the brain [ 6]. The tasks are timed
to ensure fairness in comparing humans and machines [5].
After the human performance data is collected, it is used to
estimate the human-tolerated thresholds ğ‘¡ğ‘andğ‘¡ğ‘. The thresholds
are defined as the smallest value of visual change Î”ğ‘£at which the
requirement inequality (in Def. 1 or 2) does not hold. Importantly,
the human-tolerated thresholds ğ‘¡ğ‘andğ‘¡ğ‘, estimated above using
the dataset{ğ‘¥ğ‘–}, are not specific to this dataset; these thresholds
can be used on other images that share characteristics with the
experiment image data.
Checking requirements satisfaction. ICRAF includes a method
for checking satisfaction of a given requirement defined with a
metricğ‘€âˆˆ{ğœ“ğ‘ğ‘,ğœ“ğ‘ğ‘}and a threshold ğ‘¡âˆˆ{ğ‘¡ğ‘,ğ‘¡ğ‘}. It takes as input
a list of images, a set of image transformations, and an MVC under
validation and generates test cases within the specified range of
Î”ğ‘£(i.e.,Î”ğ‘£â‰¤ğ‘¡). It runs the tests on the MVC and checks whether
the MVC satisfies the requirements by estimating the reliability
distance (defined below). The test cases are obtained via sampling
test images. After executing them on the MVC, ICRAF estimates the
metric values following the bootstrap method [3]: given an image
distribution ğ‘ƒğ‘‹and a transformation ğ‘‡ğ‘‹with a parameter domain
543ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Boyue Caroline Hu, Lina Marsso, Nikita Dvornik, Huakun Shen, and Marsha Chechik
I. Decompose complex vision task II. Generate composed reliability requirements III. Checking method
Legend:
: Multiple per sub-tasks
: Existing method: Step
: Artifact: ManualComplex vision
task (c-task)
Image transformationI. Decompose com-
plex vision task into
atomic vision sub-tasksAtomic sub-taskAtomic sub-taskAtomic sub-taskExperiment data of
human performanceExperiment data of
human performanceExperiment data of
human performance
II.b Generate the
reliability requirementsII.a Estimate the
human thresholdsII.a Estimate the
human thresholdsII.a Estimate the
human thresholdsMachine-verifiable
requirementsMachine-verifiable
requirementsMachine-verifiable
requirements
II.c Compose
the reliability
requirementComposed machine-
verifiable requirementIII.c Check composed
requirement satisfactionIII.a Check require-
ment satisfactionIII.a Check require-
ment satisfactionIII.a Check require-
ment satisfaction
Figure 3: The DecompoVision framework. Step I decomposes a complex vision task into sub-tasks; Step II generates reliability
requirements for MVCs with complex vision tasks; Step III checks their satisfaction.
an original image,
Î”ğ‘£=0minimal changes,
Î”ğ‘£=0.005a "reasonable" change,
Î”ğ‘£=0.71an "extreme" change,
Î”ğ‘£=0.96
Figure 4: Imagenet images [ 31] with different levels of added
frost.
ğ¶, a set of test images is sampled with replacement from ğ‘ƒğ‘‹and
then transformed with ğ‘‡ğ‘‹using parameters sampled from ğ¶while
ensuring the Î”ğ‘£â‰¤ğ‘¡range. Using bootstrap, ICRAF estimates the
distribution of the metrics values on the transformed ( Ë†ğ‘€ğ‘¡) and
original ( Ë†ğ‘€0) images. The requirement is considered satisfied if
Ë†ğ‘€ğ‘¡âˆ’Ë†ğ‘€0â‰¤0with 95%confidence. Throughout the rest of the paper,
Ë†ğ‘€ğ‘¡âˆ’Ë†ğ‘€0is referred to as a reliability distance .
3 TASK AND METRICS DECOMPOSITION
Many complex vision tasks (e.g., object detection) used in safety-
critical systems can be represented as a sequence of atomic vision
subtasks [ 8]. We call this class of tasks â€œc-tasksâ€. In this section,
we define a decomposition principle (DP) for decomposing the
problem of solving c-tasks so that their performance metrics can
be composed from performance metrics of their subtasks.
C-task problem decomposition. Letc-task denote a complex
vision task Vthat, given a visual input ğ‘¥, such as an image or a
video, produces a sequence of ğ‘different output types, i.e., V:
ğ‘¥â†’[ğ‘œğ‘,...,ğ‘œ 1]. For example, object detection ( Vğ·) entails local-
izing objects on an image and then classifying each of them; this
effectively produces two different types of output: object bounding
boxes (ğ‘œ1) and their class labels ( ğ‘œ2). Let an atomic subtask (a.k.a.
subtask ) be a vision task vğ‘–that produces the output type ğ‘œğ‘–, i.e.,
vğ‘–:ğ‘¥,ğ‘œ1,...,ğ‘œğ‘–âˆ’1â†’ğ‘œğ‘–, where vğ‘–uses previous outputs {ğ‘œğ‘˜}ğ‘–âˆ’1
ğ‘˜=1to produce the current output type, ğ‘œğ‘–. Therefore, a c-task Vis a
sequential composition of the subtasks, i.e.,
V=vğ‘›âŠ™...v2âŠ™v1=âŠ™ğ‘
ğ‘–=1vğ‘–, (1)
whereâŠ™represents the sequential composition operator. In the
example with object detection (see Fig. 1), the subtask v1is object
localization, vğ¿, and v2is the classification of localized objects, vğ¶|ğ¿.
Thus, the full task can be represented as Vğ·=vğ¶|ğ¿âŠ™vğ¿.
System solving a c-task. Given a visual input ğ‘¥, asystemğ‘†
produces a sequence of ğ‘different output types, i.e., ğ‘†:ğ‘¥â†’
[ğ‘œğ‘,...,ğ‘œ 1]to solve a c-task V. Ifğ‘†solves a c-task successfully,
given an input ğ‘¥, then the probability of observing the desired
output is 1, i.e., ğ‘ƒ(V)=ğ‘ƒ(ğ‘œğ‘›,...,ğ‘œ 1|ğ‘¥)=1. In the following, weshow that the probability of solving a c-task, ğ‘ƒ(V), can be decom-
posed into probabilities of solving its individual subtasks. Using
the conditional probability chain rule, the probability of observing
all the outputs can be decomposed into probabilities of observ-
ing a sequence of individual outputs, i.e., P (V)=ğ‘ƒ(ğ‘œğ‘›,...,ğ‘œ 1|ğ‘¥)=Ãğ‘
ğ‘–=2ğ‘ƒ(ğ‘œğ‘–|ğ‘œğ‘–âˆ’1,...,ğ‘œ 1,ğ‘¥)Â·ğ‘ƒ(ğ‘œ1|ğ‘¥). According to the atomic subtask
definition (i.e., vğ‘–:ğ‘¥,ğ‘œ1,...,ğ‘œğ‘–âˆ’1â†’ğ‘œğ‘–), every term in the above
product corresponds to the probability of solving an atomic sub-
task vğ‘–, i.e.,ğ‘ƒ(vğ‘–)=ğ‘ƒ(ğ‘œğ‘–|ğ‘œğ‘–âˆ’1,...,ğ‘œ 1,ğ‘¥). Therefore, by chaining the
above qualities, the probability of solving a c-task decomposes into
the probabilities of solving its atomic subtasks,
ğ‘ƒ(V)=ğ‘ƒ(âŠ™ğ‘
ğ‘–=1vğ‘–)=Ãğ‘
ğ‘–=1ğ‘ƒ(vğ‘–). (2)
In other words, Eq.2 shows that for any system ğ‘†with input ğ‘¥
and outputs[ğ‘œğ‘›,...,ğ‘œ 1], (i.e.,ğ‘†is a black-box), one can analyze the
ğ‘†solving the c-task Vby analyzing its atomic subtasks vğ‘–. This
effectively means that the analysis of the c-tasks follows the same
c-task decomposition (Eq.1)!
Example: Given an MVC ğ‘†for object detection Vğ·, the proba-
bility of solving Vğ·can be decomposed into successfully solv-
ing localization vğ¿and classification given localization vğ¶|ğ¿, i.e.,
ğ‘ƒ(Vğ·)=ğ‘ƒ(vğ¶|ğ¿)Â·ğ‘ƒ(vğ¿).
Performance metrics decomposition. Given a c-task Vand a
systemğ‘†solving it, task-specific performance metric ğ‘€is used to
measure how well the system ğ‘†solves V. More precisely, given a
visual input ğ‘¥, the corresponding systemâ€™s outputs [ğ‘œğ‘,...,ğ‘œ 1], and
the ground truth[ğ‘¡ğ‘,...,ğ‘¡ 1], the performance metric ğ‘€compares
the systemâ€™s output with the ground truth, and returns a value indi-
cating how close they are to each other (the closer the better). The
performance of an MVC or a human on a given vision task is usually
evaluated using task-specific performance metrics. For example,
the performance metric accuracy is used for the task of image clas-
sification; it measures the percentage of images correctly classified
by the system. In object detection, Average Precision ( AP)is often
used to describe the localization and classification performance
using a single number.
Even though a c-task implies solving a number of subtasks, its
performance metric is often a single scalar value that provides no
insights into how different subtasks influence the final performance.
The inability to capture the influence of every comprising subtask
on the overall MVCâ€™s performance hinders the analysis of MVCs
on c-tasks. To solve this problem, we leverage the fact that many
popular performance metrics can be described by the probability
of successfully solving the c-task (Eq. 2). For such metrics ğ‘€V,
we can directly apply Eq. 2 and break ğ‘€Vinto a combination of
544DecompoVision: Reliability Analysis of Machine Vision Components through Decomposition and Reuse ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
performance metrics ğ‘švğ‘–of the underlying subtasks vğ‘–, i.e.,
ğ‘€V=ğ‘Ã–
ğ‘–=1ğ‘švğ‘–. (3)
Here,ğ‘šğ‘£ğ‘–is a task-specific metric of an atomic vision task vğ‘–. We
call the metrics ğ‘€Vthat can be decomposed in such a way directly
decomposable . For example, a popular metric Precision used for
object detection is directly decomposable, as shown in the sup-
plementary material1; it can be directly represented as a product
of precision of localization ( vğ¿) and classification given localiza-
tion ( vğ¶|ğ¿), i.e., Precisionğ·=Precisionğ¶|ğ¿Â·Precisionğ¿. Analogously,
we can decompose the Recallğ·metric, and, as a consequence, the
precision-recall curve ğ‘ƒğ‘…ğ·(defined as a function of precision de-
pending on recall). Some metrics, such as average precision, AP, are
more complex and are not directly decomposable. Yet, APis defined
as a function of the PR-curve which is directly decomposable. For
such complex metrics ğ‘€Vthat are not directly decomposable, we
extend the metric compositionality definition as follows:
ğ‘€V=ğ¹(ğ‘€â€²
V), whereğ‘€â€²
V=ğ‘Ã–
ğ‘–=1ğ‘švğ‘–, (4)
whereğ‘€â€²
Vis a directly decomposable metric of the task V,ğ‘švğ‘–
is a metric of the ğ‘–-th subtask, and ğ¹is a monotonic function. In
other words, a metric ğ‘€Visdecomposable if it can be represented
as a function of some directly decomposable metric ğ‘€â€²
V.2Defining
metric compositionality this way allows us to understand how
every subtaskâ€™s metric ğ‘švğ‘–influences the c-taskâ€™s metric, ğ‘€V, and
thus, enables us to gain more insight into the MVC analysis. For
example, consider computing APfor object detection ( APğ·) and
instance segmentation ( APğ¼). Using Eq. 4, APcan be decomposed
intoAPğ·=ğ´ğ‘¢ğ¶(PRğ¿Â·PRğ¶|ğ¿)andAPğ¼=ğ´ğ‘¢ğ¶(PRğ¿Â·PRğ¶|ğ¿Â·PRğ‘†|ğ¶,ğ¿),
whereğ´ğ‘¢ğ¶ is area under curve on [0,1]interval. As a result, being
able to decompose existing c-task metrics, such as APğ·and APğ¼,
allows us to benefit from reusing existing popular metrics, such as
PR, and their inherent analysis methods.
Decomposition principle (DP). To summarize, in this section
we have shown that we can decompose a c-task (Eq. 1) into its
corresponding subtasks and c-task performance metric (Eq. 3 and
Eq. 4) into subtask performance metrics, all without decomposing
the system solving the c-task itself. In the remaining of the paper,
we refer to this as the decomposition principle (DP).
4 COLLECTING HUMAN PERFORMANCE
In this section, we present our approach to collecting human per-
formance data on c-tasks, which is used as a baseline in reliability
requirements (Fig. 3). To do so, we propose to decompose the c-task
into subtasks (Sec. 3) and collect human performance data for each
subtask. Consequently, the human performance on the c-task is ob-
tained by combining the per-subtask performance data, following
DP. This modular approach enables reuse of existing experiment
design and data for the subtasks shared across different c-tasks.
The human performance data for a c-task Vis obtained by se-
quentially collecting the human performance data for each of its ğ‘
sub-tasks, vğ‘–,ğ‘–âˆˆ{1,2,...,ğ‘}. Specifically, we first follow the ICRAF
1See https://reliability-object-detection.netlify.app.
2See supplementary material for compound decomposable metrics.procedure for generating a distribution of transformed images ğ‘ƒğ‘‡ğ‘‹,
given an image dataset, ğ‘¥ğ‘—âˆ¼ğ‘ƒğ‘‹, and an image transformation ğ‘‡ğ‘‹
(Sec. 2, [ 12]). The original and the transformed images are used in
the human experiments for every subtask. Then, for each subtask
vğ‘–, we conduct the experiment (timed to ensure the experimentâ€™s
fairness (see Sec. 2) and obtain the human performance results ğ‘œvğ‘–,
using results from human performance on subtasks, {ğ‘œğ‘˜}ğ‘–âˆ’1
ğ‘˜=1. The
obtained performance results are generic to datasets sharing the
same characteristics (e.g., image resolution) [12].
Note on fairness: To ensure that an MVC is fairly compared to a
human on a given task, it is crucial to limit the time given to a human
to solve the task. Existing studies establish the human time bound
for classical subtasks, such as classification and localization [ 6,26].
However, it is not trivial to set a timer for the c-task, as such task
require producing multiple output types. By decomposing the c-
task experiment, we avoid the above issue and can directly reuse
the existing research on fairness for individual tasks.
In this paper, we design a fair experiment for collecting human
performance results on object detection ( Vğ·) problems, with the
help of a cognitive science expert. We follow the above procedure
for collecting human performance on Vğ·and decompose it into
localization and classification given localization, i.e., Vğ·=vğ¶|ğ¿âŠ™
vğ¿, and perform a human experiment on each subtask.3
Localization ( vğ¿).Given an image, human participants are asked
to localize the objects via bounding boxes ( ğ‘œğ¿). The bounding box
is drawn by clicking on the top-left corner, dragging across the
diagonal, and releasing the mouse button in the bottom-right corner.
A participant is limited to 4s to draw a bounding box [ 26]. As soon as
a box is placed, the timer resets and the participant is given another
4s to localize other objects. The process (on the given image) ends
when the participant is unable to create a bounding box in 4s.
Classification given localization ( vğ¶|ğ¿).After localization, hu-
mans are expected to label the objects ( ğ‘œğ¶|ğ¿) in the previously
detected bounding boxes ( ğ‘œğ¿). Specifically, given a bounding box
ğ‘œğ¿, we visualize it on the white canvas of image size and ask the
participant to concentrate on that region. Then, we flash the image
for 200 ms (as suggested by [ 6]), and ask participants to classify
the object that appeared in the location of the bounding box. The
classification is implemented as a multiple-choice question. In both
experiments, each image is shown to a participant only once which
means the localization and classification tasks are performed by
different participants. This gives us an unbiased estimate of the
average human performance on object detection. To ensure data
quality, we use qualification tests to filter out spammers [26].
Experiment. To collect human performance for our evaluation,
we selected two image transformations simulating realistic scene
changes [ 11]: adding artificial frost and changing brightness. We
performed the experiment using the PASCAL VOC [ 4] dataset that
has object detection and instance segmentation annotations. For
each selected image transformation, we sampled 600original and
their corresponding transformed images (another 600), asking a
3Videos illustrating the localization and classification of the localized object experi-
ments are available in supplementary material [13].
545ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Boyue Caroline Hu, Lina Marsso, Nikita Dvornik, Huakun Shen, and Marsha Chechik
human to localize the objects. Each detected object ( ğ‘œğ¿) was inde-
pendently labelled by five different humans, producing ğ‘œğ¶|ğ¿. Specif-
ically, we divided the 2400 images ( 1200 images across two image
transformations) into batches of 20. Each such batch was shown
to a different participant using Amazon Mechanical Turk to obtain
the object bounding boxe, ğ‘œğ¿. Overall, the human participants iden-
tified 6000 bounding boxes in the localization experiment in about
8 hours. For the classification experiment, we divided the collected
bounding boxes into batches of 20and showed each batch to five
different participants to obtain object labels, ğ‘œğ¶|ğ¿. This resulted in
30,000human object classification labels ( 5labels for each of 6000
images in about 2 hours).
Reuse of experiment data. To demonstrate the benefits of sub-
task human performance reuse, we consider the task of instance
segmentation. This task consists of object localization, classifica-
tion and consequent segmentation, as illustrated in Fig. 1: Vğ¼=
vğ‘†|ğ¶,ğ¿âŠ™vğ¶|ğ¿âŠ™vğ¿. Instance segmentation shares two subtasks
(vğ¶|ğ¿and vğ¿) with object detection; this allows us to reuse de-
tection experimental data. Therefore, we conduct only one object
segmentation experiment, for vğ‘†|ğ¶,ğ¿, i.e., obtaining the object seg-
mentation mask ğ‘œğ‘†|ğ¶,ğ¿, given a box and a class ( ğ‘œğ¿andğ‘œğ¶|ğ¿). In
this task, humans draw the outline of the objects. Templates for
this task are available on Mechanical Turk. To demonstrate reuse in
our evaluation, in addition to the already available 36,000human
predictions (for bounding boxes and labels together), we asked one
person to provide object segmentation masks ğ‘œğ‘†|ğ¶,ğ¿per object, re-
sulting in additional 6,000human object segmentation outcomes,
which took about 20 hours. Without reuse, conducting instance
segmentation experiment would have taken up to 30hours (i.e.,
the sum of 8, 2, and 20 hours for obtaining ğ‘œğ¿,ğ‘œğ¶|ğ¿andğ‘œğ‘†|ğ¶,ğ¿,
respectively). Reusing prior results for ğ‘œğ¿andğ‘œğ¶|ğ¿allows us to
reduce the experiment time by 33%.
5 C-TASK RELIABILITY REQUIREMENTS
In this section, we present our method for generating reliability re-
quirements for an MVC performing c-tasks by composing reliability
requirements for each subtask (Step II in Fig. 3). To build such re-
quirements, our method uses both the decomposable c-task and the
performance metrics (Sec. 3) and the ICRAF reliability requirements
classes along with their parameter estimation method (Sec. 2)4. This
modular way of requirements generation enables reuse of existing
requirements for the subtasks shared across different c-tasks.
Requirements Generation. The requirement generation method
takes a c-task V, a decomposable metric ğ‘€, a transformation ğ‘‡ğ‘‹, a re-
quirement type ğ‘Ÿğ‘_ğ‘¡ğ‘¦ğ‘ğ‘’ (correctness-preservation (ğ‘ğ‘) orprediction-
preservation (ğ‘ğ‘), see Sec. 2), and an MVC ğ‘“ğ‘‰performing the task V.
First, for every subtask vğ‘–, the method uses human performance
data to estimate the human-tolerated range of change thresholds
(ğ‘¡ğ‘andğ‘¡ğ‘forreq_typeğ‘ğ‘andğ‘ğ‘, respectively). The estimation of
the threshold values for each subtask vğ‘–is performed by applying
the ICRAF estimation method (see Sec. 2) to object detection and
instance segmentation performance metrics (more detail in supple-
mentary material1). Using human performance obtained on 1200
4Since SoTA benchmarks provide class-specific metrics, the requirements in the paper
are also class-specific. However, our method is generic â€“ see examples of non-class-
specific requirements in supplementary materials.Procedure 1 generate_composed_reqs (V,ğ‘€V,metrics,ğ‘‡ğ‘‹,rq_type,ğ‘¡)
Inputs: (1)V, a decomposable vision task; (2) ğ‘€V, a decomposable metric; (3) metrics , a list of
subtaks metrics ( ğ‘švğ‘–) derived from ğ‘€V; (4)ğ‘‡ğ‘‹, an image transformation; rq_type , a require-
ments type (ğ‘ğ‘orğ‘ğ‘); and (5)ğ‘¡, a list of N thresholds estimated for each ğ‘švğ‘–.
1:reqs=âˆ…//initialize the list containing requirements for each subtask
2:forğ‘–âˆˆ{1,...,ğ‘}do//loop over subtasks vğ‘–in Eq. 1
3:ğ‘švğ‘–=metrics[ğ‘–]
4:ğ‘¡vğ‘–=ğ‘¡[ğ‘–]
5:ğ‘¡ğ‘–ğ‘›ğ‘“={ğ‘¡â€²forğ‘¡â€²âˆˆğ‘¡ifğ‘¡â€²â‰¤ğ‘¡vğ‘–}//list of thresholdsâ‰¤ğ‘¡vğ‘–
6: //requirement for a subtask vğ‘–using the metric ğ‘švğ‘–
7: reqs[ğ‘–]= gen_req_subtask( vğ‘–,ğ‘švğ‘–,ğ‘¡ğ‘–ğ‘›ğ‘“,ğ‘‡ğ‘‹,rq_type )
8:end for
9:ğ‘¡V=min(ğ‘¡)//compute the tightest threshold
10:ğ‘€â€²
V=Ãğ‘
ğ‘–=1metrics[ğ‘–]//composed metric in Eq. 4
11: ifrq_type ==ğ‘ğ‘then
12: reqV=ğ‘ğ‘(V,ğ‘€â€²
V,ğ‘‡ğ‘‹,ğ‘¡V)//composed requirement for Vusingğ‘€Vâ€²
13: reqnaive
V=ğ‘ğ‘(V,ğ‘€V,ğ‘‡ğ‘‹,ğ‘¡V))//naive requirement for Vusing the metric ğ‘€V
14: else
15: reqV=ğ‘ğ‘(V,ğ‘€â€²
V,ğ‘‡ğ‘‹,ğ‘¡V)//composed requirement for Vusingğ‘€Vâ€²
16: reqnaive
V=ğ‘ğ‘(V,ğ‘€V,ğ‘‡ğ‘‹,ğ‘¡V)//naive requirement for Vusing the metric ğ‘€V
17: end if
return reqs,reqV,reqnaive
V
(original and transformed) images for the transformation adding
artificial frost and changing brightness, using the Mechanical Turk
platform (Sec. 4), we estimate the thresholds â€“ see Tbl. 1 for arti-
ficial frost5. Then, the method instantiates reliability requirement
parameters for the subtasks ( vğ‘–) using the corresponding subtask
metrics (ğ‘švğ‘–) and the estimated thresholds (see Proc. 1). Finally,
it composes subtask requirements to obtain requirements for the
c-task V(see Proc. 1), as shown in Fig. 3 Step II. Below, we elaborate
the instantiation and the composition steps.
Our method uses the above estimated parameters and calls Proc. 1
to generate reliability requirements for the subtasks (LL:2-8) and
then composes them to obtain the final requirement for c-task V
(LL:9-17). In order to generate the requirement, we instantiate the
parameters of the ICRAF requirement classes with the estimated
visual change thresholds and a vision task metric (see Sec. 2). For
each subtask vğ‘–, there is an associated range of human-tolerated
visual changes given by the threshold ğ‘¡vğ‘–(estimated in Step 1),
which can be different for different subtasks. To be able to compose
the substask requirements, they should be defined within the same
range of changes. Therefore, for every subtask vğ‘–, we consider
all the visual change thresholds, ğ‘¡vğ‘—, from other subtasks that are
less or equal to the threshold ğ‘¡vğ‘–of the current subtask vğ‘–, i.e.,
ğ‘¡inf={ğ‘¡vğ‘—|ğ‘¡vğ‘—â‰¤ğ‘¡vğ‘–}(Proc. 1,L:5). To this end, for each subtask, we
compute the ICRAF requirement instantiated with the subtaskâ€™s
metricğ‘švğ‘–and each of the ğ‘¡vğ‘–thresholds (Proc. 1,L:7), and define
the final requirement for the subtask, reqvğ‘–, as the conjunction of
the above requirements for different thresholds in ğ‘¡inf(Proc. 2).
Next, we note that performing the c-task Vis at least as hard as
performing the most difficult of its subtasks vğ‘–. Therefore, for the c-
task V, we set the human-tolerated visual change threshold ğ‘¡Vto be
the smallest of its subtasksâ€™ thresholds, i.e., ğ‘¡V=min(ğ‘¡)(Proc. 1,L:9).
Specifically, reqVallows to evaluate the contribution of each subtask
vğ‘–for the overall reliability on ğ‘‰, as it uses a directly-decomposable
metricğ‘€â€²along with the threshold ğ‘¡Vto instantiate the parameters
of the ICRAF requirement. On the other hand, reqnaive
Vis obtained
by instantiating the parameters of the ICRAF requirement with ğ‘¡V
5Thresholds for changing brightness are in the supplementary material [13].
546DecompoVision: Reliability Analysis of Machine Vision Components through Decomposition and Reuse ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
original image frost frost++
(a)âœ“1/1 detection
 (b)âœ“1/1 detection
 (c)âœ“1/1 detection
(d)âœ—3/4 detection
 (e)âœ—2/4 detection
 (f)âœ—1/4 detection
(g)âœ“1/1 good detec-
tion
(h)âœ“1/1 good detec-
tion
(i)âœ—misclassifica-
tion
(j)âœ—misclassifica-
tion
(k)âœ—misclassifica-
tion
(l)âœ“1/1 good detec-
tion
Figure 5: Object detection on original and transformed PAS-
CAL VOC images [ 4] by Faster R-CNN [ 30]. Images from
the second and third columns display different levels of ad-
ditional frost, with Î”ğ‘£â‰¤0.7. The MVC outputs are shown
directly in each image, and the comparisons of the results
with the ground truth are shown under them. The red bound-
ing boxes correspond to the class "bus" while the blue boxes
correspond to "not a bus".
and the original c-task metric ğ‘€ğ‘‰(L:13,L:16); reqnaive
Vcan be seen
as a naive extension of ICRAF requirements to c-tasks, and can be
used as a baseline for our decomposable requirements.
Example: Here, we illustrate machine-verifiable requirements (in-
stantiations shown in Tbl. 5) generated using Proc. 1 for the task
of object detection, Vğ·=vğ¶|ğ¿âŠ™vğ¿. We take the images from
the PASCAL VOC dataset [ 4], consider the task of detecting buses
(i.e., objects of other categories are labeled as â€œnot a busâ€), and
useAPas the detection performance metric. Using the estimated
thresholds in Tbl. 1, we show the generated ğ‘ğ‘requirements fol-
lowing our procedure in Tbl. 2. Intuitively, ğ‘ğ‘vğ¿checks whether
vğ¿, i.e., object localization with bounding boxes, is affected by frost.
In images in Fig. 5e, 5f, Faster R-CNN failed to localize each bus
after frost, thus it is not correctness-preserving for them. Then,
ğ‘ğ‘vğ¶|ğ¿checks whether vğ¶|ğ¿, i.e., object classification, is affected by
frost. As shown in Fig. 5h, 5i, frost changes Faster R-CNN output
label (from the correct â€œa busâ€ to the wrong â€œnot a busâ€), thus itTable 1: Estimated human-tolerated visual change threshold
values for transformation frost ( ğ‘¡ğ‘is forğ‘ğ‘,ğ‘¡ğ‘is forğ‘ğ‘, Sec. 5
.)
object vğ¿ vğ¶|ğ¿ vğ‘†|ğ¶,ğ¿
classğ‘¡ğ‘ğ‘¡ğ‘ğ‘¡ğ‘ğ‘¡ğ‘ğ‘¡ğ‘ğ‘¡ğ‘
person 0.90 0.70 0.90 0.30 0.60 0.30
bus 0.90 0.30 0.70 0.30 0.70 0.30
Table 2: Correctness-preservation ( ğ‘ğ‘) requirements condi-
tions generated for object detection ( Vğ·) and instance seg-
mentation ( Vğ¼) of object class â€˜busâ€™ following Proc. 1 against
the image transformation ğ‘“ğ‘Ÿğ‘œğ‘ ğ‘¡ . Requirements for Vğ·that
are reused for Vğ¼are shown in the same colour.
ğ‘Ÿğ‘’ğ‘â€™s for object detection Vğ· ğ‘Ÿğ‘’ğ‘â€™s for instance segmentation Vğ¼c-task
Vğ‘Ÿğ‘’ğ‘
name Required conditionğ‘Ÿğ‘’ğ‘
name Required condition
subtask
ğ‘Ÿğ‘’ğ‘v(L:7)ğ‘ğ‘vğ¿Preserveğ‘ƒğ‘…ğ¿value for images
withÎ”ğ‘£â‰¤0.9andÎ”ğ‘£â‰¤0.7ğ‘ğ‘vğ¿Preserveğ‘ƒğ‘…ğ¿value for images
withÎ”ğ‘£â‰¤0.9andÎ”ğ‘£â‰¤0.7
ğ‘ğ‘vğ¶|ğ¿Preserveğ‘ƒğ‘…ğ¶|ğ¿value
for images with Î”ğ‘£â‰¤0.7ğ‘ğ‘vğ¶|ğ¿Preserveğ‘ƒğ‘…ğ¶|ğ¿value
for images with Î”ğ‘£â‰¤0.7
N/A ğ‘ğ‘vğ‘†|ğ¶,ğ¿Preserveğ‘ƒğ‘…ğ‘†|ğ¶,ğ¿value
for images with Î”ğ‘£â‰¤0.7
composed
ğ‘Ÿğ‘’ğ‘V(L:12)ğ‘ğ‘Vğ·Preserveğ‘ƒğ‘…ğ·value
for images with Î”ğ‘£â‰¤0.7ğ‘ğ‘Vğ¼Preserveğ‘ƒğ‘…ğ¼value
for images with Î”ğ‘£â‰¤0.7
c-task
ğ‘Ÿğ‘’ğ‘naive
V(L:13)ğ‘ğ‘naive
Vğ·Preserveğ´ğ‘ƒvalue
for images with Î”ğ‘£â‰¤0.7ğ‘ğ‘naive
Vğ¼Preserveğ´ğ‘ƒvalue
for images with Î”ğ‘£â‰¤0.7
Procedure 2 gen_req_subtask( v,ğ‘š,ğ‘¡inf,ğ‘‡ğ‘‹,rq_type )
Inputs: (1) a subtask v; (2)ğ‘š, the metric of v; (3) a list of thresholds ğ‘¡inf; (4) an image transformation
ğ‘‡ğ‘‹; and (5) a requirements type rq_type .
1:Ifrq_type ==ğ‘ğ‘then returnÃ“
ğ‘¡â€²âˆˆğ‘¡infğ‘ğ‘(v,ğ‘š,ğ‘‡ğ‘‹,ğ‘¡â€²)//for correctness-preservation
2:else returnÃ“
ğ‘¡â€²âˆˆğ‘¡infğ‘ğ‘(v,ğ‘š,ğ‘‡ğ‘‹,ğ‘¡â€²)//for prediction-preservation requirements
is not correctness-preserving. However, in images Fig. 5k, 5l, frost
changes a wrong label to a correct one, increasing both precision
and recall in PR; therefore, is correctness-preserving. Finally, ğ‘ğ‘Vğ·
check for the preservation of performance on the overall object
detection task. Since images in Fig. 5e, 5f and Fig. 5h, 5i do not
satisfy subtask requirements, they donâ€™t satisfy ğ‘ğ‘Vğ·.
Correctness. Given a c-task V=vğ‘›âŠ™...v2âŠ™v1and a decompos-
able performance metric ğ‘€V, such thatğ‘€V=ğ¹(ğ‘€â€²
V)andğ‘€â€²
V=Ãğ‘›
ğ‘–=1ğ‘švğ‘–, letreqs={reqv1,...,reqvğ‘›}be the list of subtask require-
ments generated by Proc. 1, where reqvğ‘–is defined with ğ‘švğ‘–. Let
reqVbe the composed c-task requirement defined using ğ‘€â€²
V. The
theorem states that if an MVC satisfies subtask requirements reqs,
it also satisfies composed c-task requirement reqV. Proof is in the
supplement1.
Theorem 1. If all subtask requirements reqvğ‘–âˆˆreqs are satisfied,
so is the composed c-task requirement reqV.
Reuse of subtask requirements. Defining reliability require-
ments via task decomposition allows us to reuse requirements for
c-tasks that share the same subtasks and thresholds. For example,
given the same image distribution ğ‘ƒğ‘‹, subtasks vğ¿andvğ¶|ğ¿have
the same threshold for object detection ( Vğ·=vğ¶|ğ¿âŠ™vğ¿) and in-
stance segmentation ( Vğ¼=vğ‘†|ğ¶,ğ¿âŠ™vğ¶|ğ¿âŠ™vğ¿) because the human
experiment data can be reused. Therefore, as shown in Tbl. 2, re-
quirements for subtasks instantiated as parts of ğ‘ğ‘vğ¿,ğ‘ğ‘vğ¶|ğ¿,ğ‘ğ‘vğ¿
547ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Boyue Caroline Hu, Lina Marsso, Nikita Dvornik, Huakun Shen, and Marsha Chechik
andğ‘ğ‘vğ¶|ğ¿for object detection can be reused for instance segmen-
tation. Thus the set of requirements for instance segmentation is
obtained by extending the set of requirements for object detection
with the requirements for the subtask vğ‘†|ğ¶,ğ¿, and composing them
to obtain the requirement for the overall task Vğ¼.
6 CHECKING MVC RELIABILITY
In this section, we introduce our modular black-box checking
method for checking requirementsâ€™ satisfaction (Step III in Fig. 3).
Our method checks both the requirements for the overall c-task
and those of the subtasks via extending the ICRAF requirement
satisfaction checking method. As a result, our method allows iden-
tifying reliability weaknesses among subtasks that compromise the
reliability of the c-task. Additionally, it enables reuse of checking
analysis artifacts of the subtasks.
Modular Checking Method. Our method takes as input a list
of images, a set of image transformations, reliability requirements
for the c-task and its subtasks, and an MVC under validation. First,
the method generates test images (inputs) within the greatest sub-
taskâ€™s range of visual changes ( ğ‘¡ğ‘šğ‘ğ‘¥), i.e.,ğ‘¡ğ‘šğ‘ğ‘¥=max(ğ‘¡), where
ğ‘¡is the list of subtasksâ€™ thresholds, using the ICRAF test images
generation method. As a result, the generated test set can be used
for checking all subtasks (with ğ‘¡â‰¤ğ‘¡ğ‘šğ‘ğ‘¥). Then, the method feeds
the test images to the MVC, to obtain the MVC outputs (e.g., detec-
tion predictions ğ‘œğ·). Second, given the MVC predictions (outputs),
the method checks the satisfaction of each subtaskâ€™s requirement
reqvğ‘–. It is done by estimating the reliability distance (that measures
the difference in performance on the original and transformed im-
ages) for MVC predictions on images with scene changes within
a specified threshold. The requirement is satisfied if the reliability
distance is less than or equal to zero, which is used as a quantitative
measure of how much improvement is needed in order to meet the
requirement. Since reqvğ‘–is defined as a conjunction of multiple re-
quirements with different thresholds (Sec. 5), the reliability distance
is first estimated on each requirement in this conjunction using the
ICRAF estimation method. Consequently, the reliability distance
forreqvğ‘–is defined as a maximum reliability distance among all
the requirements in the conjunction. Third, our method checks
the satisfaction of the composed requirement reqVfor the c-task
ğ‘‰(using the decomposable metric ğ‘€â€²
Vin Sec. 3). To compute the
reliability distance for reqV, we use the previously computed metric
values for each subtask corresponding to the minimum threshold
ğ‘¡V(see Proc. 1-L:9).
Remarks: The reliability distance estimated on each subtask re-
quirement in the conjunction (Proc. 2) allows identifying the range
of changes that is affecting the MVC reliability for the given sub-
task, which can provide guidance to MVC developers, on what to
strengthen the reliability.
Example: To check whether an MVC satisfies the requirements for
Vğ·shown in Tbl. 2, we first generate test images by sampling the
original and transformed images with Î”ğ‘£â‰¤ğ‘¡ğ‘šğ‘ğ‘¥=0.90and obtain
MVC predictions for all the sampled images. To check the subtask
requirements ğ‘ğ‘vğ¿andğ‘ğ‘vğ¶|ğ¿, we compute the curve PRğ¿,0.9with
all sampled test images shown in Fig. 6a; and with the subset of test
images that have Î”ğ‘£â‰¤0.7, we compute the curves PRğ¿,0.7(Fig. 6a)
andPRğ¶|ğ¿,0.7(Fig. 6b). The reliability distance ğ‘Ÿusing PRis defined
(a)PRğ¿
(forğ‘ğ‘vğ¿)
(b)PRğ¶|ğ¿
(forğ‘ğ‘vğ¶|ğ¿)
(c)PRğ·
(forğ‘ğ‘Vğ·)
Figure 6: Example of PRcurves for checking requirements
in Tbl. 2
as the percentage of points on PRfor transformed images (blue)
that are below the PRcurve for the original images (green). In this
case,ğ‘Ÿvğ¶|ğ¿is0.43andğ‘Ÿvğ¿is the maximum reliability distance of the
two blue curves, which is 0.55. Finally, for checking the composed
requirement ğ‘ğ‘Vğ·, Fig. 6c is obtained using the previous results in
Fig. 6a and Fig. 6b, and ğ‘Ÿâ€²
Vğ·=0.50.
Reuse of analysis artifacts. Modularity allows us to reuse many
analysis artifacts computed for a given MVC: test images, MVC
predictions, and the testing results. First, test images and MVC pre-
dictions can be reused across requirements with different thresholds
since test images are sampled with the maximum threshold ğ‘¡ğ‘šğ‘ğ‘¥.
Second, computed metric values can be reused across requirements,
e.g., reusing PRğ¿,0.7andPRğ¶|ğ¿,0.7for computing PRğ·,0.7andğ´ğ‘ƒ.
When an MVC solving a vision subtask is reused to solve some
c-task, the modularity of our framework allows us to reuse both
analysis artifacts and results on that subtask for the c-task analy-
sis. For example, instance segmentation MVCs can be built using
object detection MVCs by adding additional layers performing the
segmentation task, vğ‘†|ğ¶,ğ¿(see Fig. 1). For example, the instance
segmentation MVC Mask RCNN [ 9] is built on top of Faster RCNN
object detector. For Mask RCNN, we can reuse the test images, the
analysis results for the subtasks, reqVğ¿andreqVğ¶|ğ¿(i.e., the relia-
bility distances), and use the test images with the bounding boxes
previously computed for Faster RCNN analysis. By reusing test
images and analysis results, our modular checking method allows
us to save significant computation resources.
7 EVALUATION
In this section, we demonstrate the feasibility of DecompoVision
for object detection ( Vğ·) and instance segmentation ( Vğ¼).
First, because the c-task human experiment is decomposed into
subtasks, we can either estimate the human-tolerated threshold per
subtask and then compose them into c-task threshold (see Proc. 1),
or apply the ICRAF estimation method to estimate the thresholds
of the c-task directly. We are interested in how the threshold val-
ues obtained by these two methods differ and thus aim to answer
(RQ1 ): How does our composed human tolerated threshold for a
c-task compare with a threshold directly computed with ICRAF?
Second, we would like to determine whether checking our com-
posed machine-verifiable requirements for object detection and
instance segmentation MVCs with our checking method reveals
reliability gaps in the SoTA object detection and instance segmen-
tation MVCs. Therefore, we aim to answer ( RQ2 ): How effective
548DecompoVision: Reliability Analysis of Machine Vision Components through Decomposition and Reuse ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
Table 3: Comparison of the DecompoVision checking method with the baseline benchmark and non-decomposable (naive)
checking method for reliability evaluation of object detection and instance segmentation MVCs against image transformation
frost. Second column reports ğ´ğ‘ƒperformance metric on PASCAL-VOC-C tests, while the rest reports reliability distance values.
Next to MVCâ€™s c-task metrics, we also report the (rank) among other MVC in that column. N/A indicates not enough points
were collected for requirement checking. ( â†‘) next to a metric name means â€œthe higher the betterâ€ and ( â†“) means the inverse.
Vision Task: Object Detection, Vğ· Vision Task: Instance Segmentation, Vğ¼
Reliability distance per requirement (rank)â†‘ Reliability distance per requirement (rank)â†‘
Correctness-preservation Prediction-preservation Correctness-preservation Prediction-preservationMVC
NamePASCAL-VOC-C
tests APâ†‘
(rank)â†‘ğ‘ğ‘vğ¿â†“ğ‘ğ‘vğ¶|ğ¿â†“ğ‘ğ‘Vğ·â†“ğ‘ğ‘naive
Vğ·â†“ğ‘ğ‘vğ¿â†“ğ‘ğ‘vğ¶|ğ¿â†“ğ‘ğ‘Vğ·â†“ğ‘ğ‘naive
Vğ·â†“ğ‘ğ‘vğ‘†|ğ¶,ğ¿â†“ğ‘ğ‘Vğ¼â†“ğ‘ğ‘naive
Vğ¼â†“ğ‘ğ‘vğ‘†|ğ¶,ğ¿â†“ğ‘ğ‘Vğ¼â†“ğ‘ğ‘naive
Vğ¼â†“
(a) RetinaNet-R50-1x 0.60(3) 0.52 0.75 0.49(2) 0.59(13) 0.58 0.69 0.64(11) 0.04(5) Object Detection Only
(b) RetinaNet-R50-3x 0.75(2) 0.51 0.80 0.51(3) 0.49(7) 0.55 0.24 0.65(12) 0.02(1) Object Detection Only
(c) RetinaNet-R100-3x 0.76(1) 0.51 0.79 0.45(1) 0.52(10) 0.51 0.38 0.73(13) 0.04(5) Object Detection Only
(d) R50-C4-1x 0.35(9) 0.70 0.84 0.83(8) 0.53(11) 0.67 0.16 0.51(7) 0.05(9) 0.40 0.59(7) 0.11(9) 0.16 0.52(6) 0.05(6)
(e) R50-DC5-1x 0.33(10) 0.85 0.84 0.86(13) 0.49(7) 0.80 0.16 0.48(5) 0.06(11) 0.62 0.57(4) 0.09(7) 0.13 0.59(10) 0.07(10)
(f) R50-FPN-1x 0.38(7) 0.82 0.84 0.84(10) 0.48(5) 0.51 0.16 0.47(4) 0.04(5) 0.52 0.54(2) 0.03(2) 0.14 0.50(4) 0.04(4)
(g) R50-C4-3x 0.32(12) 0.76 0.84 0.82(6) 0.50(9) 0.73 0.16 0.50(6) 0.03(2) 0.46 0.65(10) 0.12(10) N/A 0.53(8) 0.03(1)
(h) R50-DC5-3x 0.32(11) 0.76 0.84 0.81(4) 0.55(12) 0.74 0.16 0.52(9) 0.05(9) 0.61 0.63(9) 0.08(6) N/A 0.52(6) 0.05(6)
(i) R50-FPN-3x 0.31(13) 0.85 0.84 0.85(12) 0.45(3) 0.58 0.16 0.46(2) 0.04(5) 0.51 0.58(6) 0.02(1) N/A 0.46(2) 0.04(4)
(j) R101-C4-3x 0.39(5) 0.77 0.84 0.84(10) 0.48(5) 0.75 0.16 0.53(10) 0.06(11) 0.42 0.53(1) 0.09(7) N/A 0.53(8) 0.06(8)
(k) R101-DC5-3x 0.39(6) 0.81 0.73 0.81(4) 0.44(1) 0.50 0.16 0.51(7) 0.07(13) 0.63 0.59(7) 0.07(5) N/A 0.51(5) 0.06(8)
(l) R101-FPN-3x 0.37(8) 0.81 0.84 0.82(6) 0.47(4) 0.41 0.16 0.46(2) 0.03(2) 0.48 0.57(4) 0.06(3) N/A 0.46(2) 0.03(2)Object class: person
(m) X101-FPN-3x 0.43(4) 0.82 0.84 0.83(8) 0.44(1) 0.58 0.16 0.39(1) 0.03(2) 0.37 0.54(2) 0.06(3) N/A 0.39(1) 0.03(2)
Note: All MVCs are originally from Facebook Detectron2 [38]. IoU threshold is set to 0.5for object detection, 0.25for instance segmentation. All numbers are rounded.
Table 4: Average runtime and peak memory used to check the satisfaction of requirements against image transformation frost
for object detection and instance segmentation without reusing decomposition analysis results. All the values are computed by
averaging the resources used to evaluate all 10 MVCs and 2 object classes (from Tbl. 3) on 50 batches of 400 images. Resources
needed to obtain MVC outputs are not included in this table; all the numbers are rounded.
With decomposition Without decomposition
ResourcesObject Detection Instance Segmentation Object Detection Instance Segmentation
ğ‘ğ‘vğ¿ğ‘ğ‘vğ¶|ğ¿ğ‘ğ‘Vğ·ğ‘ğ‘vğ¿ğ‘ğ‘vğ¶|ğ¿ğ‘ğ‘vğ‘†|ğ¶,ğ¿ğ‘ğ‘Vğ¼ğ‘ğ‘naive
Vğ·ğ‘ğ‘naive
Vğ¼
Runtime (s) 507.49 2.33 1.00 144.35 0.63 34.97 0.62 511.26 692.21
Peak Mem (MB) 61.36 61.36 61.36 61.36 61.36 61.36 61.36 61.36 61.36
ğ‘ğ‘vğ¿ğ‘ğ‘vğ¶|ğ¿ğ‘ğ‘Vğ·ğ‘ğ‘vğ¿ğ‘ğ‘vğ¶|ğ¿ğ‘ğ‘vğ‘†|ğ¶,ğ¿ğ‘ğ‘Vğ¼ğ‘ğ‘naive
Vğ·ğ‘ğ‘naive
Vğ¼
Runtime (s) 366.97 1.62 0.72 0.00 0.00 20.11 0.91 369.46 390.71
Peak Mem (MB) 62.86 63.29 63.29 63.03 63.29 63.82 63.82 63.29 63.82
Figure 7: Human tolerated threshold estimated either directly
or by composing the subtask thresholds. ğ‘¡ğ‘andğ‘¡ğ‘are forğ‘ğ‘
andğ‘ğ‘, respectively.
is our method at identifying reliability gaps when compared to
existing benchmarks or to non-decomposable checking method?
Finally, we would like to determine whether decomposing the c-
task analysis and reusing analysis artifacts can reduce resource
usage for checking requirement satisfaction in terms of runtime
and peak memory. Thus, we aim to answer (RQ3) : How does c-task
analysis decomposition and artifacts reuse impact resource usage
for checking the requirements?
RQ1. To answer RQ1, we compare the composed c-task thresh-
olds (obtained by composing the subtasksâ€™ thresholds ğ‘¡ğ‘andğ‘¡ğ‘,
see Sec. 5) with the c-task thresholds directly computed using thehuman performance data (using the ICRAF estimation method
with the APmetric) for object detection ( Vğ·) and instance segmen-
tation ( Vğ¼). The values of the above thresholds computed using
human experiment data (Sec. 4) are shown in Fig. 7. All directly
estimated thresholds are between the minimum and maximum sub-
tasksâ€™ thresholds. This logically follows since as long as the MVC
is reliable on all of the subtasks, it is also reliable on the c-task; at
the same time, the c-task cannot be reliable when all the subtasks
fail. In contrast, the composed thresholds never exceed the directly
obtained ones. This is because the composed thresholds correspond
to the minimum threshold among all subtasks (Proc. 1, L:9) which
gives the lower bound estimate. Using the lower bound estimate
for c-task requirements implies that all subtask requirements are
achievable by humans, which in turn allows for a fair comparison
between human and MVC performance. Unlike the threshold es-
timated directly with ICRAF, the one selected by Proc. 1 (as the
minimum threshold among the subtasks) always ensures the lower
bound of the human-tolerated range, answering RQ1.
RQ2. To answer RQ2, we aim to determine whether checking our
composed c-task and subtask requirements enables us to discover
reliability gaps that were not identified with the SoTA reliabil-
ity benchmark PASCAL-VOC-C [ 23], nor with the baseline non-
decomposable checking method, described below. We define relia-
bility gaps as undetected insufficient reliability. PASCAL-VOC-C is
549ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Boyue Caroline Hu, Lina Marsso, Nikita Dvornik, Huakun Shen, and Marsha Chechik
an object detection benchmark dataset that consists of transformed
Pascal VOC images (used for testing); they use five pre-selected
parameter values for the transformations, and the APmetric to
evaluate MVC correctness. For the non-decomposable checking
method, we implement a naive extension of ICRAF using the small-
est per-subtask human tolerated range threshold (see Sec. 5) and
directly operates on the non-decomposed c-task metric. The naive
baseline checks correctness ( ğ‘ğ‘naive
V) and prediction preservation
(ğ‘ğ‘naive
V) of the MVC solely on the c-task, and is agnostic to its
sub-tasks. DecompoVision is different as it explicitly considers (and
analyzes) the subtasks during the c-task analysis. Specifically, De-
compoVision evaluates the MVCâ€™s ability to preserve correctness
and predictions of the composed c-task ( ğ‘ğ‘â€²
Vandğ‘ğ‘â€²
V), and of each
substask (ğ‘ğ‘vğ‘–andğ‘ğ‘vğ‘–). Finally, different from PASCAL-VOC-C
that is limited to pre-selected transformation parameter values and
evaluates one performance metric, ğ´ğ‘ƒ, DecompoVision uses the
range of visual changes that does not affect humans [ 12] and eval-
uates the MVCâ€™s prediction and correctness perservation ability
on the c-task and its subtasks. Therefore, we expect our checking
method to provide more insight into the MVC reliability than the
baselines. Moreover, as far as we know, our checking method is the
first to enable benchmarking instance segmentation MVCs.
We evaluated MVCs provided by the Detectron [ 38] framework,
including three based on RetinaNet [ 20] and 10 based on Faster
RCNN [ 29] for object detection; and 10 based on Mask RCNN [ 9]for
instance segmentation which builds on top of Faster-RCNN by
adding an object segmentation module. Since MVCs were originally
pre-trained on the COCO dataset [ 21], we post-trained them on
the Pascal VOC dataset for the corresponding task . We checked
reliability of the 23 MVCs against two image transformations, frost
and brightness, and two object classes, â€˜personâ€™ and â€˜busâ€™, using
images from the Pascal VOC validation dataset6. In the following,
we compare our reliability analysis results with the two baselines
for the transformation frost and class person. Tbl. 3 reports the
PASCAL-VOC-C ğ´ğ‘ƒvalues and reliability distances (computed with
DecompoVision or the naive non-decomposable baseline). Below,
we refer to an MVC by its prefix letter in Tbl 3.
First, we start by comparing the composed requirements of De-
compoVision analysis to the naive non-decomposed baseline. We
can see that the values inside the ğ‘ğ‘naive
Vğ·column show little vari-
ation among different MVCs (e.g., MVCs (a), (b), (c) are the top 3
ranked MVCs according to their AP on PASCAL-VOC-C, yet their
ğ‘ğ‘naive
Vğ·is similar to other MVCs), which suggests that the naive
baseline distance is not informative. On the other hand, our com-
posed distance ğ‘ğ‘Vğ·shows larger variation among different MVCs
(especially (a), (b), (c) vs others) which correlates with the ğ´ğ‘ƒvalues
and per-subtask MVC reliability distances, ğ‘ğ‘Vğ¿andğ‘ğ‘Vğ¶|ğ¿. The
above evidence suggests that checking our composed correctness
preservation requirement is more informative than checking the
naive baseline. This is because checking satisfaction of reqVentails
checking satisfaction of allof the subtasks requirements, which
provides more refined analysis results.
Second, we compare DecompoVisionâ€™s reliability requirements
with theğ´ğ‘ƒanalysis on PASCAL-VOC-C. While both rank MVC (c)
as first, the 2nd and 3rd best ranked MVCs are different according
6Results for brightness and class â€˜busâ€™ are in the supplementary material [13].toğ´ğ‘ƒand ourğ‘ğ‘ğ‘‰ğ·metrics (see row (a) and (b)). Moreover, while
the benchmark reports a large gap in ğ´ğ‘ƒbetween MVCs (a) and (b),
ğ‘ğ‘ğ‘‰ğ·suggests that their difference in reliability is negligible. This
suggests that DecompoVision uncovers new reliability gaps, not
identified by the PASCAL-VOC-C benchmark, and provides useful
feedback for developers to improve MVC reliability.
Using DecompoVisionâ€™s per-subtask reliability requirements can
provide additional analysis insights. For example, according to
ğ‘ğ‘ğ‘‰ğ·, MVCs (a), (b) and (c) are the poorest at preserving their
predictions in object detection (despite being the best in correct-
ness preservation). Analyzing per-subtask requirements reveals
that the predictions are well-preserved for the localization task
(i.e., lowğ‘ğ‘ğ‘‰ğ¿|ğ¶values), but it is the classification that hinders
the preservation capabilities of the MVCs (i.e., high ğ‘ğ‘ğ‘‰ğ¶values).
Thus, analyzing performance for each atomic subtask can be used
to guide improvements in MVC reliability, e.g., through directed
retraining of subtask-specific layers [ 20]. Another observation is
that MVCs (k), (j) and (b), (c) have similar APon PASCAL-VOC-C
and similar reliability distances for Vğ·, respectively. By the subtask
performance, we see that MVC (j) is more correctness-preserving
forvğ¿, and MVC (k) is more so for vğ¶|ğ¿. As for preserving the pre-
dictions, MVCs (b) and (c) have almost the same reliability distance
forğ‘ğ‘Vğ·; however, MVC (c) is slightly better at vğ¿and MVC (b)
is better at vğ¶|ğ¿. Better reliability performance for vğ¶|ğ¿indicates
that MVCs (k) and (c) is safer for applications where the reliable
object classification is important, e.g., traffic sign detection.
DecompoVision can also analyze reliability of MVCs solving
instance segmentation, Vğ¼. As we can see in Tbl. 3, with respect
to subtask performance, MVC (j) is better at Vğ¼(indicated by the
lowestğ‘ğ‘Vğ¼) even though it is considerably worse then MVC (k)
at object detection (see ğ‘ğ‘Vğ·), that instance segmentation builds
upon. Yet, MVC (j) is better than MVC (k) at object segmentation
(seeğ‘ğ‘ğ‘†|ğ¶,ğ¿), which results in the overall better performance at
Vğ¼. This suggests that good quality object segmentation is more
important than object detection for instance segmentation MVC
reliability. Therefore, checking our reliability requirement enabled
us to identify new reliability gaps that could not be detected just
by checking APon transformed images in the PASCAL-VOC-C
benchmark. As a result, we obtain more insights into the reasons
for reliability failures, thus answering RQ2.
RQ3. Here we aim to determine whether reusing analysis artifacts
following our decomposition of c-tasks can save resources. For all
types of MVCs, requirements, experiments and tests developed for
object detection can be reused for instance segmentation (see Sec. 4-
6). Certain widely used MVCs for instance segmentation, such as
Mask RCNN, are directly built on top of Faster-RCNN MVCs (that
solve object detection). Thus, the intermediate computation results
for object detection can also be reused for instance segmentation.
For such MVCs, we measured the runtime and peak memory needed
to check the satisfaction of reliability requirements (Tbl. 2) by sam-
pling 50 batches of 200 test images using 10 Mask RCNN MVCs
(Tbl. 3). The results are shown in Tbl. 4. All experiments were
conducted on 4 CPU Intel(R) Core(TM) i3-8100 CPU @ 3.60GHz
with 8GB of RAM. As shown in Tbl. 4, the peak memory is not
affected significantly, and measuring the satisfaction of ğ‘ğ‘naivevğ¿and
ğ‘ğ‘naivevğ¿takes the longest time. Indeed, evaluating the performance
550DecompoVision: Reliability Analysis of Machine Vision Components through Decomposition and Reuse ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
of localization is most time-consuming since it compares the IoU
of the MVC output and the ground truth bounding boxes. Reusing
such computation significantly decreases the runtime of checking
satisfaction of all subtasks that come later in the sequence and the
c-task itself. In total, checking ğ‘ğ‘satisfaction for instance segmenta-
tion by reusing results from object detection took 180.57sec. while
checking without reuse took 692.21sec.; checking ğ‘ğ‘took 21.02sec.
with reuse and 390.71sec. without. On average, reuse can decrease
runtime by up to 81%, answering RQ3.
Summary. We empirically showed that (RQ1): for c-tasks, our com-
posed human-tolerated thresholds represent lowerbound threshold
estimation; (RQ2): Using c-task decomposition, our modular check-
ing method enables reliability analysis on c-tasks not covered by
the existing work and can discover and locate reliability gaps that
are missed by the existing methods; and (RQ3): by decomposing and
reusing analysis artifacts, runtime of checking satisfaction of the
reliability requirements can be significantly decreased. Thus, our
evaluation suggests that DecompoVision is efficient and effective.
Threats to validity. [Internal] Inherited from ICRAF [ 12], dur-
ing testing we assumed that the parameter values of the transfor-
mations are uniformly distributed. However, this may need to be
adjusted depending on the deployment environment of the MVC.
[External] Due to budget considerations, our experiments with hu-
mans were limited to a small set of transformations and only two
c-tasks. We leave further experiments to future work.
8 RELATED WORK
Below, we discuss approaches for defining and accessing MVC
reliability.
Specifying reliability of MVCs. Requirements specification of
MVCs that perform c-tasks has been approached by the specifica-
tion of a set of quality characteristics of datasets used to train the
MVCs [ 17]; or ML-related requirements for each phase of software
development processes [ 34]. Yet these requirements do not have
a machine-analyzable representation and thus cannot be checked
automatically. The SoTA reliability framework ICRAF that speci-
fies machine-verifiable reliability requirements for MVCs [ 12] is
only applicable to a single atomic vision task (with one type of
output), such as image classification (a single class label per image).
To tackle this limitation, this paper proposes a modular reliability
framework for c-tasks, which builds on top of ICRAF. Furthermore,
the subtasks-based modular design of our framework enables reuse
of analysis artifacts of atomic subtasks.
Assessing MVC reliability. Existing approaches propose MVC
reliability analysis against image transformations, including meta-
morphic testing [ 2,22,39,40], semantic analysis [ 16], reachability
analysis [ 18], verification [ 15,18,27,28,35]. These works mainly
focus on one particular vision task with one single type of output,
which is not directly applicable to more complex vision tasks. Mean-
while, our decomposition principle allows extending the above reli-
ability approaches to analyze MVCs performing c-tasks. Moreover,
our DecompoVision analyzes MVC reliability not only on the c-
tasks but also on their subtasks, which leads to a deeper insight
into the MVC reliability and guides further MVC development.Decomposing MVC. Existing work decomposed Convolutional
Neural Network (CNN) used for image classification into mod-
ules responsible for classifying each object class [ 24,25] or into a
hierarchical representation [ 33]. The benefit of such a decomposi-
tion is reusing parts of the models to reduce training time. Similar
work concerning modular neural networks has been proposed to
increase interpretability [ 7], to achieve better training by under-
standing communication between modules [ 1,14] and memorizing
a set of features [ 10,32]. In contrast, our decomposition focuses
on decomposing the reliability analysis of an MVC performing a
vision task with the goal of reusing analysis methods and artifacts
rather then the MVC layers themselves.
9 CONCLUSION
We proposed DecompoVision â€“ a modular framework for analyzing
reliability of MVC performing complex vision tasks (c-task) that are
decomposable into a sequence of atomic subtasks. DecompoVision
is built on top of the SoTA reliability framework for atomic vision
tasks [ 12]. DecompoVisionâ€™s modularity enables reuse of existing
subtask requirements and analysis artifacts; and to get deeper in-
sights into the MVC reliability by understanding how subtasks
contribute to the analysis results. Through evaluation with popular
MVCs, we showed that our modular framework enables reliability
analysis of c-tasks not covered by existing approaches, identifies
reliability gaps previously missed and reduces usage of resources
by reusing methods and artifacts. We believe that other types of
reliability analyses such as verification [ 15] can also benefit from
our decomposition principle for handling c-tasks while reducing
resource usage. Additionally, we plan to improve our checking
method by using our refined requirements analysis to build diag-
nostics to help software engineers improve MVC reliability.
Discussion. Our Theroem. 1 states that if all subtask requirements
are satisfied, so is the composed requirement. This is because satis-
fying all subtask requirements is much stricter than satisfying the
composed requirement. One limitation of this is that if not all sub-
task requirements are satisfied, there is no definite way of judging
whether the composed requirement is satisfied by simply looking at
the results of all subtask requirements. However, despite this, since
we are testing each subtask separately our decomposition can still
give useful information about which subtask is not performing well
through reliability distance. Building distance measures or other
decomposition methods for supporting stronger implications when
subtask requirements are not satisfied should be future work.
10 DATA AVAILABILITY
The implementation and all the data necessary to reproduce our
experiments are available online here.
REFERENCES
[1]Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2016. Neural
Module Networks. In 2016 IEEE Conference on Computer Vision and Pattern Recog-
nition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016 . IEEE Computer Society,
39â€“48. https://doi.org/10.1109/CVPR.2016.12
[2]Anurag Dwarakanath, Manish Ahuja, Samarth Sikand, Raghotham M. Rao, R. P.
Jagadeesh Chandra Bose, Neville Dubash, and Sanjay Podder. 2018. Identifying
implementation bugs in machine learning based image classifiers using meta-
morphic testing. In Proceedings of the 27th International Symposium on Software
551ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Boyue Caroline Hu, Lina Marsso, Nikita Dvornik, Huakun Shen, and Marsha Chechik
Testing and Analysis (ISSTAâ€™2018), Amsterdam, The Netherlands . ACM, 118â€“128.
https://doi.org/10.1145/3213846.3213858
[3]Bradley Efron and Robert J Tibshirani. 1994. An Introduction to the Bootstrap .
CRC press.
[4]M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. 2010.
The Pascal Visual Object Classes (VOC) Challenge. International Journal of
Computer Vision 88, 2 (June 2010), 303â€“338.
[5]Chaz Firestone. 2020. Performance vs. Competence in Humanâ€“Machine Compar-
isons. Proceedings of the National Academy of Sciences 117, 43 (2020), 26562â€“26571.
[6]R Geirhos, CR Medina Temme, J Rauber, HH SchÃ¼tt, M Bethge, and FA Wichmann.
2019. Generalisation in humans and deep neural networks. In NeurIPS 2018 .
Curran, 7549â€“7561.
[7]Badih Ghazi, Rina Panigrahy, and Joshua R. Wang. 2019. Recursive Sketches
for Modular Deep Learning. In Proceedings of the 36th International Conference
on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA
(Proceedings of Machine Learning Research, Vol. 97) , Kamalika Chaudhuri and
Ruslan Salakhutdinov (Eds.). PMLR, 2211â€“2220.
[8]Robert M. Haralick. 1992. Performance Characterization in Computer Vision.
InProceedings of the British Machine Vision Conference, BMVC 1992, Leeds, UK,
September, 1992 . BMVA Press, 1â€“8. https://doi.org/10.5244/C.6.1
[9]Kaiming He, Georgia Gkioxari, Piotr DollÃ¡r, and Ross Girshick. 2017. Mask
R-CNN. https://doi.org/10.48550/ARXIV.1703.06870
[10] Geoffrey E. Hinton, Zoubin Ghahramani, and Yee Whye Teh. 1999. Learning
to Parse Images. In Advances in Neural Information Processing Systems 12, [NIPS
Conference, Denver, Colorado, USA, November 29 - December 4, 1999] , Sara A. Solla,
Todd K. Leen, and Klaus-Robert MÃ¼ller (Eds.). The MIT Press, 463â€“469.
[11] Boyue Caroline Hu, Lina Marsso, Krzysztof Czarnecki, and Marsha Chechik. 2022.
What to Check: Systematic Selection of Transformations for Analyzing Reliability
of Machine Vision Components. In Proceedings of the International Symposium
on Software Reliability Engineering (ISSREâ€™2022), Charlotte, North Carolina, USA .
IEEE. https://doi.org/10.1109/ISSREW53611.2021.00048
[12] Boyue Caroline Hu, Lina Marsso, Krzysztof Czarnecki, Rick Salay, Huakun Shen,
and Marsha Chechik. 2022. If a Human Can See It, So Should Your System:
Reliability Requirements for Machine Vision Components. In Proceedings of the
44th International Conference on Software Engineering (ICSEâ€™2022), Pittsburgh,
USA. ACM.
[13] Boyue Caroline Hu, Lina Marsso, Nikita Dvornik, Huakun Shen, and Marsha
Chechik. 2023. Supplementary Material for: Decompovision: Reliability Analysis
of Machine Vision Components Through Decomposition and Reuse. https:
//reliability-object-detection.netlify.app.
[14] Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko.
2017. Learning to Reason: End-to-End Module Networks for Visual Question
Answering. In IEEE International Conference on Computer Vision, ICCV 2017,
Venice, Italy, October 22-29, 2017 . IEEE Computer Society, 804â€“813. https://doi.
org/10.1109/ICCV.2017.93
[15] Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. 2017. Safety
Verification of Deep Neural Networks. In CAVâ€™17 . 3â€“29.
[16] Edward Kim, Divya Gopinath, Corina S. Pasareanu, and Sanjit A. Seshia. 2020.
A Programmatic and Semantic Approach to Explaining and Debugging Neural
Network Based Object Detectors. In 2020 IEEE/CVF Conference on Computer Vision
and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020 . Computer
Vision Foundation / IEEE, 11125â€“11134. https://doi.org/10.1109/CVPR42600.
2020.01114
[17] Marc Kohli, Ronald Summers, and Jr Geis. 2017. Medical Image Data and Datasets
in the Era of Machine Learning. JDI30(4) (2017), 392â€“399.
[18] Panagiotis Kouvaros and Alessio Lomuscio. 2018. Formal Verification of CNN-
based Perception Systems. ArXiv abs/1811.11373 (2018).
[19] Yann LeCun, LÃ©on Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-
based learning applied to document recognition. Proc. IEEE (1998).
[20] Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr DollÃ¡r. 2017.
Focal Loss for Dense Object Detection. In IEEE International Conference on Com-
puter Vision, ICCV 2017, Venice, Italy, October 22-29, 2017 . IEEE Computer Society,
2999â€“3007. https://doi.org/10.1109/ICCV.2017.324
[21] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr DollÃ¡r, and C. Lawrence Zitnick. 2014. Microsoft COCO: Com-
mon Objects in Context. In Proceedings of the 13th European Conference on Com-
puter Vision (ECCVâ€™2014), Zurich, Switzerland (Lecture Notes in Computer Science,
Vol. 8693) . Springer, 740â€“755. https://doi.org/10.1007/978-3-319-10602-1_48
[22] Rohan Reddy Mekala et al .2019. Metamorphic Detection of Adversarial Examples
in Deep Learning Models with Affine Transformations. In METâ€™19 (MET â€™19) .
IEEE Press, 55â€“62. https://doi.org/10.1109/MET.2019.00016
[23] Claudio Michaelis, Benjamin Mitzkus, Robert Geirhos, Evgenia Rusak, Oliver
Bringmann, Alexander S. Ecker, Matthias Bethge, and Wieland Brendel. 2019.Benchmarking Robustness in Object Detection: Autonomous Driving when Win-
ter is Coming. arXiv preprint arXiv:1907.07484 (2019).
[24] Rangeet Pan and Hridesh Rajan. 2020. On Decomposing a Deep Neural Network
into Modules. In Proceedings of the 28th Joint European Software Engineering Con-
ference and Symposium on the Foundations of Software Engineering (ESEC/FSEâ€™20),
Virtual, USA . ACM, 889â€“900. https://doi.org/10.1145/3368089.3409668
[25] Rangeet Pan and Hridesh Rajan. 2022. Decomposing Convolutional Neural
Networks into Reusable and Replaceable Modules. In Proceedings of the 44th
International Conference on Software Engineering (ICSEâ€™2022), Pittsburgh, USA .
ACM, 0000.
[26] Dim P. Papadopoulos, Jasper R. R. Uijlings, Frank Keller, and Vittorio Ferrari. 2017.
Training Object Class Detectors with Click Supervision. In 2017 IEEE Conference
on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-
26, 2017 . IEEE Computer Society, 180â€“189. https://doi.org/10.1109/CVPR.2017.27
[27] Colin Paterson, Haoze Wu, John Grese, Radu Calinescu, Corina S. Pasareanu,
and Clark W. Barrett. 2021. DeepCert: Verification of Contextually Relevant
Robustness for Neural Network Image Classifiers. In Computer Safety, Reliability,
and Security - 40th International Conference, SAFECOMP 2021, York, UK, September
8-10, 2021, Proceedings (Lecture Notes in Computer Science, Vol. 12852) , Ibrahim
Habli, Mark Sujan, and Friedemann Bitsch (Eds.). Springer, 3â€“17. https://doi.org/
10.1007/978-3-030-83903-1_5
[28] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. 2019. DeepXplore: Auto-
mated Whitebox Testing of Deep Learning Systems. CACM 62, 11 (2019), 137â€“145.
https://doi.org/10.1145/3361566
[29] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. 2015. Faster R-CNN:
Towards Real-Time Object Detection with Region Proposal Networks. IEEE
Transactions on Pattern Analysis and Machine Intelligence 39 (2015), 1137â€“1149.
[30] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. 2017. Faster R-CNN:
Towards Real-Time Object Detection with Region Proposal Networks. IEEE Trans.
Pattern Anal. Mach. Intell. 39, 6 (2017), 1137â€“1149. https://doi.org/10.1109/TPAMI.
2016.2577031
[31] Olga Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Zhiheng Huang,
A. Karpathy, A. Khosla, M. Bernstein, A. Berg, and Li Fei-Fei. 2015. ImageNet
Large Scale Visual Recognition Challenge. International Journal of Computer
Vision 115 (2015), 211â€“252.
[32] Sara Sabour, Nicholas Frosst, and Geoffrey E. Hinton. 2017. Dynamic Routing
Between Capsules. In Advances in Neural Information Processing Systems 30:
Annual Conference on Neural Information Processing Systems 2017, December 4-9,
2017, Long Beach, CA, USA , Isabelle Guyon, Ulrike von Luxburg, Samy Bengio,
Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.).
3856â€“3866.
[33] K. Sairam, Jayanta Mukherjee, Amit Patra, and Partha Pratim Das. 2018. HSD-
CNN: Hierarchically self decomposing CNN architecture using class specific
filter sensitivity analysis. In ICVGIP 2018: 11th Indian Conference on Computer
Vision, Graphics and Image Processing, Hyderabad, India, 18-22 December, 2018 .
ACM, 30:1â€“30:9. https://doi.org/10.1145/3293353.3293383
[34] Rick Salay and Krzysztof Czarnecki. 2018. Using Machine Learning Safely in
Automotive Software: An Assessment and Adaption of Software Process Require-
ments in ISO 26262. ArXiv abs/1808.01614 (2018).
[35] David Shriver, Sebastian G. Elbaum, and Matthew B. Dwyer. 2021. DNNV: A
Framework for Deep Neural Network Verification. In Computer Aided Verification
- 33rd International Conference, CAV 2021, Virtual Event, July 20-23, 2021, Proceed-
ings, Part I (Lecture Notes in Computer Science, Vol. 12759) , Alexandra Silva and
K. Rustan M. Leino (Eds.). Springer, 137â€“150. https://doi.org/10.1007/978-3-030-
81685-8_6
[36] Christian Szegedy, W. Zaremba, Ilya Sutskever, Joan Bruna, D. Erhan, Ian J.
Goodfellow, and R. Fergus. 2014. Intriguing Properties of Neural Networks. CoRR
abs/1312.6199 (2014).
[37] Weibin Wu, Hui Xu, Sanqiang Zhong, Michael R. Lyu, and Irwin King. 2019.
Deep Validation: Toward Detecting Real-World Corner Cases for Deep Neural
Networks. In 2019 49th Annual IEEE/IFIP International Conference on Dependable
Systems and Networks (DSN) . 125â€“137. https://doi.org/10.1109/DSN.2019.00026
[38] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick.
2019. Detectron2. https://github.com/facebookresearch/detectron2.
[39] Xiaoyuan Xie et al .2011. Testing and Validating Machine Learning Classifiers by
Metamorphic Testing. Journal of Systems and Software 84, 4 (2011), 544â€“558.
[40] Mengshi Zhang, Yuqun Zhang, Lingming Zhang, Cong Liu, and Sarfraz Khurshid.
2018. DeepRoad: GAN-based Metamorphic Testing and Input Validation Frame-
work for Autonomous Driving Systems. In 2018 33rd IEEE/ACM International
Conference on Automated Software Engineering (ASE) . IEEE, 132â€“142.
Received 2023-02-02; accepted 2023-07-27
552