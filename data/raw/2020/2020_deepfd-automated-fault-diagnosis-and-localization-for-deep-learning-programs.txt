DeepFD: Automated Fault Diagnosis and Localization for Deep
Learning Programs
Jialun Cao
The Hong Kong University of Science
and Technology, and Guangzhou
HKUST Fok Ying Tung Research
Institute, China
jcaoap@cse.ust.hkMeiziniu Li
The Hong Kong University of Science
and Technology, China
mlick@cse.ust.hkXiao Chen
Huazhong University of Science and
Technology, China
xchencr@hust.edu.cn
Ming Wen*
Huazhong University of Science and
Technology, China
mwenaa@hust.edu.cnYongqiang Tian
University of Waterloo, Canada, and
The Hong Kong University of Science
and Technology, China
yongqiang.tian@uwaterloo.caBo Wu
MIT-IBM Watson AI Lab, U.S.
bo.wu@ibm.com
Shing-Chi Cheung*
The Hong Kong University of Science
and Technology, and Guangzhou
HKUST Fok Ying Tung Research
Institute, China
scc@cse.ust.hk
ABSTRACT
AsDeepLearning(DL)systemsarewidelydeployedformission-
critical applications, debugging such systems becomes essential.
Most existing works identify and repair suspicious neurons on the
trained Deep Neural Network (DNN), which, unfortunately, might
be a detour. Specifically, several existing studies have reported that
many unsatisfactory behaviors are actually originated from the
faults residing in DL programs. Besides, locating faulty neurons is
not actionable for developers, while locating the faulty statements
in DL programs can provide developers with more useful informa-
tion for debugging. Though a few recent studies were proposed
topinpointthefaultystatementsinDLprogramsorthetrainingsettings (
e.g.too large learning rate), they were mainly designed
based on predefined rules, leading to many false alarms or false
negatives,especiallywhenthefaultsarebeyondtheircapabilities.
In view of these limitations, in this paper, we proposed DeepFD,
a learning-based fault diagnosis and localization framework which
mapsthefaultlocalizationtasktoalearningproblem.Inparticu-
lar,itinfersthesuspiciousfaulttypesviamonitoringtheruntime
features extracted during DNN model training, and then locates
* Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe firstpage.Copyrights forcomponentsof thisworkowned byothersthan the
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
¬© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.3510099the diagnosed faultsin DL programs. It overcomes thelimitations
byidentifyingtherootcausesoffaultsinDLprogramsinsteadof
neurons, and diagnosing the faults by a learning approach instead
ofasetofhard-codedrules. Theevaluationexhibitsthepotentialof
DeepFD. It correctly diagnoses 52% faulty DL programs, compared
with around half (27%) achieved by the best state-of-the-art works.
Besides,forfault localization,DeepFDalsooutperformsthe exist-
ingworks,correctlylocating42%faultyprograms,whichalmost
doubles the best result (23%) achieved by the existing works.
CCS CONCEPTS
‚Ä¢Software and its engineering ‚ÜíSoftware testing and de-
bugging;‚Ä¢Computing methodologies ‚ÜíNeural networks .
KEYWORDS
Neural Networks, Fault Diagnosis, Fault Localization, Debugging
ACM Reference Format:
Jialun Cao, Meiziniu Li, Xiao Chen, Ming Wen*, Yongqiang Tian, Bo Wu,
and Shing-Chi Cheung*. 2022. DeepFD: Automated Fault Diagnosis and
LocalizationforDeepLearningPrograms.In 44thInternationalConferenceon
Software Engineering (ICSE ‚Äô22), May 21‚Äì29, 2022, Pittsburgh, PA, USA. ACM,
New York, NY, USA, 13 pages. https://doi.org/10.1145/3510003.3510099
1 INTRODUCTION
Deeplearning(DL)softwarehasbeenactivelydeployedforappli-
cationssuchasfrauddetection,medicaldiagnosis,facerecognition,
andautonomous driving[ 27,30,31].Such softwarecomprisesDL
programs,whichencodethestructureofaDeepNeuralNetwork
(DNN)modelandtheprocessbywhichthemodelistrained.Various
5732022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA CAO, et al.
studies[9,11,26,39,40,58,61]havebeenconductedtounderstand
and detect DL program faults.1
Yet,debuggingDLprogramsisstillchallenging[ 20,31,40,58,71].
Unlikeconventionalsoftwareprograms,thedecisionlogicbasedon
whichaDNNmodelmakespredictionsisnotexplicitlyencodedby
theDLprogram‚Äôscontrolflow[ 20].Instead,predictionsaremadeby
propagating inputs against the tuned parameters through a trained
DNNmodel.Theprogramplaysthecrucialrolebydefining,guiding
and monitoring the model training process (e.g., defining network
structures, training strategies and hyperparameters) based on a
training set to indirectly tune the parameters of the DNN model.
DuetothestochasticnatureofDLcomputation,aDNNmodelis
unable to make every prediction correctly, and an incorrect predic-
tionresultdoesnotnecessarilyindicateafaultintheunderlying
program.Therefore,itishardtodebugDLcomputationfaultsusing
conventionalfaultlocalizationtechniques[ 7,12,57,59,61,63,65,
73] based on individual correct and incorrect prediction results.
Techniques [ 20,40,66] have been proposed to debug such faults
by identifying and locating suspicious neurons or layers in the
trained DNNmodel. Specifically,they drawan analogybetween a
faulty neuron (or layer) in a DNN model and a faulty statement (or
branch)inaconventionalprogram.Withtheanalogy,theyadapt
fault localization techniques such as the spectrum-based ones todetect suspicious neurons or layers based on statistical analysis.
However, directlytuning the weights of neuronsafter trainingcan-
not facilitate pinpointing the flaws in DL programs. Even worse,
after tuning, developer may ignore the faults in the program, thus
leavingtherealrootcauses(e.g.,inappropriatelossfunctionand
optimizer setting) of the unsatisfactory behavior uncovered.
Recentworksaddressedtheproblem( i.e.debuggingDLsystems)
in a more practical way. For instance, UMLAUT [ 54] was proposed
to detect program faults using predefined rules, and provide advice
onhowtofixthem.AutoTrainer[ 70]wasproposedtodetectfive
predefinedproblemsthatmightoccurduringtrainingDNNmodels,
withsolutionstoeradicatethedetectedproblems.Theserule-based
approaches do pinpoint more accurate root causes of the faults ( i.e.
lackingofdatanormalization),orprovidemoreactionableadviceon
how to fix the DL program or tuning the hyper-parameters ( i.e.set
thelearning ratebetween 10‚àí7to0.1). However,such hard-coded
rules may induce false alarms and are incapable of revealing faults
thatgobeyondthecapabilityofthosepredefinedrules.Forexample,UMLAUTreportsafaultwhenevertheoutputlayerisnotactivated
bysoftmax,whileinfact,theactivatoroftheoutputlayervaries
case by case. Besides, the pre-defined rules can over-simplify fault
detection to limited symptoms exhibited by several types of faults
(e.g., the example presented in Section 2).
In view of the above-mentioned limitations, we look for an al-
ternative. Specifically, we propose a learning-based fault diagnosis
and localization framework, DeepFD, which maps a fault local-izationtasktoalearningproblem.Inparticular,itdiagnosesandlocates faults in a DL program by inferring the suspicious faulttypes using the features extracted in the model training process.
The intuition behind is that the trend or distribution of certain
externalorinternalvariables(e.g.,lossorgradients)inaprogram‚Äôs
1Theexistingrelatedworksmayuseothertermslike‚Äò‚Äôbug‚Äù,‚Äúdefect‚Äùor‚Äúissue‚Äù.Thispa-
peruses‚Äúfault‚Äùasarepresentativeofallsuchrelatedtermstorefertoanyhuman-made
mistake in the DL program that leads to functionally insufficient performance [27].1# Model construction
2model = Sequential()
3model.add(Dense(input_dim=2,
4 output_dim=4,
5 init="glorot_uniform"))
6model.add(Activation("sigmoid"))
7model.add(Dense(input_dim=4,
8 output_dim=1,
9 init="glorot_uniform"))
10model.add(Activation("sigmoid"))
11
12# Training Configuration
13sgd = SGD(lr=0.05, # Bug 1
14 l2=0.0, decay=1e-6,
15 momentum=0.11, nesterov=True)
16model.compile(loss='mean_absolute_error', optimizer=sgd) # Bug 2
17model.fit(train_data, label, nb_epoch=1000, batch_size=4) # Bug 3
Listing 1: A Faulty Code Snippet Extracted from StackOver-
flow #31556268.
trainingprocesscansuggestthelikelihoodoffaultsandtheirtypes.
Such an intuition is also echoed by our observation that the value
ofvariablesinaDLprogramcanfollowcertainpatternsduringthe
modeltrainingprocess,andthesepatternsexhibitstrongcorrela-
tions with certain types of faults. For instance, if the loss variable‚Äôs
value oscillates wildly, the training is likely using an inappropriate
learning rate [ 70]. However, it is difficult to set a specific threshold
forsuchoscillationsandidentifytheexistenceoflearningratesfault
accordingly. We, therefore, resort to a learning-based approach.
However, the design of DeepFD needs to address two main chal-
lenges.First,thereisnoexistingoff-the-shelfdataset(i.e.,faultyDLprogramswiththeground-truthoffaults)availablethatissufficient
to enable the learning of fault-related features. Though one may
seedfaultsintocorrectDLprogramstoconstructsuchatraining
set, how to seed diverse faults effectively into the programs, andfurther determine whether the DNN model is indeed faulty after
faultseedingremainsunknown.Second,therearefewreferences
ofdeployinglearningalgorithmstolocateDLprogramfaultswhile
the effectiveness of a learning-based technique often requires a set
ofhigh-qualityfeatures.Ho wever,sincethereareenormousparam-
etersandoutputsduringDNNtraining,itisinfeasibletostoreall
the values during the continuous training iterations, and use such
valuesasfeatures.Therefore,howtoselectqualifiedfeaturesthat
canbeutilizedforeffectivelocalizationofDNNfaultsremainsto
be challenging.
Toaddressthefirstchallenge,wecollectedabenchmarkwith58
realfaultsinthewildwithpatchesandanalyzedtheprevalentfault
types made by developers. As a result, five common fault typesare observed. We seeded faults of these types to hundreds of DL
programs from a recent benchmark [ 70] to generate a training set
withthousandsoffaultyDLprograms,servingforthelearningpart
of DeepFD. Furthermore, we adopted the generalised linear model
(GLM)[46]andCohen‚Äôseffectsized[ 17]todeterminewhethera
fault-seededDLprogramisstatisticallydifferentfromandworse
thantheoriginalprogram,thusdeterminingtheoracleoftheseededprograms.Toaddressthesecondchallenge,inspiredbytheruntime
informationusedinpreviousstudies[ 50,54,58,70],wedesigned
and traced the runtime information such as loss, gradients, andthe number of times loss increases. Then, we applied statistical
analysis on each trace of runtime information, resulting in a list of
574
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. DeepFD: Automated Fault Diagnosis and Localization for Deep Learning Programs ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
extracted runtime features, which can represent the process of this
DNNmodeltraining.Thereby,withtheextractedfeaturesasinputs,
and the seeded faults as ground-truth labels, the fault diagnosis
problem can, therefore, be translated into a classification problem.
In summary, our work makes the following major contributions:
‚Ä¢Originality: We identify a set of features that can be used to
classifymajortypesoffaultsinDLprograms,whichareextracted
from the DNN training process. Leveraging these features, we
proposea learning-basedfaultlocalization framework,DeepFD,
forDLprograms.Itisabletodiagnosemultiplefaultsandidentify
theirindividualfaulttypes.Foreachofthesefaults,itcanfurther
locate the faulty code snippet in the concerned DL program.
‚Ä¢Benchmark: We build a benchmark containing 58 faulty DL
programs collected from Stack Overflow and GitHub. Each of
them include faults, patches and line numbers of the faulty state-
ments. This benchmark can serve for future researches on DNN
debugging and repair tools.
‚Ä¢Usefulness: We encapsulate the fault seeding, faulty program
checking,featureextraction,faultdiagnosisandlocalizationinto
the DeepFD framework, and open-sourced it [ 6]. The tool is
extensibleforseedingdiversefaultsandmutationtestinginde-
pendently.Itcanalsobeadaptedtoextractmoreruntimefeatures
for other learning-based works.
‚Ä¢Evaluation: The evaluation exhibits the potential of DeepFD.
It can correctly diagnose 52% cases, compared with half (27%)
achieved by the best state-of-the-art works. Besides, for fault lo-
calization,DeepFDalsooutperformstheexistingworks,reaching
42%, while the existing works range from 10% to 23%.
2 MOTIVATION
Listing 1 shows a buggy DL program extracted from StackOver-
flow.2The program constructs a DNN model for the XOR problem,
but the model‚Äôs accuracy gets stuck at around 50%. The faults in
this DNN program include: (1) an inappropriate learning rate (line
13),(2)unsuitablelossfunction(line16)and(3)insufficienttraining
epoch (line 17). It is challenging for developers to diagnose and
localize these faults all at once.
Weappliedthreestate-of-the-arttechniques, i.e.,AutoTrainer[ 70],
UMLUAT [ 54] and DeepLocalize [ 58] to this buggy program to ex-
amine whether theycan help diagnose and locatethe faults. Note
thatwedonotcomparewithautomatedmachinelearning(AutoML)
algorithms [ 29] since these works are not for debugging a faulty
program,butselectingmachinelearningalgorithmsaccordingto
the user-provided data [ 29]. However, our goal is to debug a faulty
programandidentifytherootcauses.Toreducetherandomnessof
training process, we ran each work 10 times then report the result.
The results areshown in Table 1. Specifically,AutoTrainer cannot
detectanyfaultsoverthe10runs,letalonediagnosingthefaults.
The faults are escaped from the AutoTrainer‚Äôs detection because
this buggy program does not exhibit the abnormal symptoms that
are predefined by AutoTrainer such as gradient vanish [ 25,44,55]
and dying ReLU [ 10]. On the other hand, though UMLUAT and
DeepLocalize are able to detect the existence of faults, the diag-
nosed faults are inaccurate. Specifically, UMLUAT falsely alarmed
that the last layer has two faults ( i.e.missing Softmax layer before
2https://stackoverflow.com/questions/31556268/how-to-use-keras-for-xor.Table 1: Fault Diagnosis and Localization Results.
Method FaultDiagnosis FaultLocalization
AutoTrainer [70] Notraining problem ‚Äì
UMLUAT [54]Critical:Missing Softmax layer before loss
Critical: Missing activation functions
Warning: Last model layer has nonlinear activationWarning: Learning Rate is highWarning: Possible overfitting‚Äì
DeepLocalize [58]layer-3 Error in Output Gradient
Stopa t1e poc hLayer 3
DeepFDFault1: [Loss]
Fault 2: [lr]Fault 3: [Epoch]Fault 4: [Act]Lines:[16]
Lines: [13]Lines: [17]Lines: [6, 10]
loss, and missing activation functions). In fact, these two alarms
willbefiredaslongastheoutputlayerisnotactivatedby Softmax.
TheonlyrootcausethathasbeencorrectlydiagnosedbyUMLUAT
istheinappropriatelearningrate,butitismarkedasawarning.On
the other hand, DeepLocalize reports a fault at the output gradient
oflayer3( i.e.,line10)duetonumericalerrors( e.g. NaN)occurred
when propagating to this layer. Though it does locate where the
symptom happens, it fails to correctly pinpoint any of the faults.
Takingacloserlook,someexistingworksprescribethesymp-
toms and map them only to the existence of potential faults, but
cannotidentifythespecifictypesoffaults[ 58].Otherworkspre-
defined rules relying on various predefined thresholds, yet it is
infeasibletotryoutallthecombinationsofvariousthresholdsto
findouttheoptimalone[ 54,70].Assuch,ourlearning-basedframe-
work,DeepFD,isabletoovercometheselimitationsbylearningthe
correlations between symptoms and specific types of faults, and
learning the optimal thresholds automatically.
The secret of DeepFD is that some runtime information exhibits
significant correlations with certain types of faults. Specifically,
Figure 1 shows the distributions of several features (see Section 3.1
formoredetails)whenthelearningrateandlossfunctionarefaulty
(in red) or not (in blue). We can observe that there are obvious
statistical differences when the learning rate or loss function are
set appropriately or inappropriately. The observation enables us to
performfault diagnosisand localizationas alearning problemvia
leveraging the relevant stochastic runtime information of a buggy
DL program. Finally, for the example as shown in Listing 1, our
approach, DeepFD, can report the four types of suspicious faults
intheprogram:thelossfunction(line16),learningrate(line13),
training epoch (line 17) and activation function (line 6 and line
10). After repairing these faults accordingly, the DNN model can
achieve 100% accuracy.3
3 APPROACH
3.1 Workflow of DeepFD
Figure2showstheworkflowofDeepFD.Itcomprisesthreesteps:
diagnostic feature extraction, fault diagnosis and fault localization.
Step1:DiagnosticFeatureExtraction. Givenaprogram,DeepFD
constructsaDNNarchitectureandcollectsruntimedatasuchasthe
lossandneurons‚Äôinformationbytrainingthearchitecture.How-
ever, storing all the weights and gradients for each neuron at each
3Since it is a XOR problem, there is no need to consider over-fitting.
575
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA CAO, et al.
Figure 1: Correlation Between Types of Faults and Features. The first row illustrates correlations between certain features
with unsuitable learning rate, the second row with unsuitable loss function. The naming convention of features starts with
‚Äòft‚Äô (feature), followed by the name of runtime data and the applied statistical operators.

 


 
 

 
 
 
#  
	&%( $
#!#
 #$%
& % !

# %
!$$
)
 !$
("$!	&%$	&%$
	!& 


#%# 
 !$$!$
 !$%
	%&#$! # 
!
 !#%! '%#%!  
#!$$#!# ($$
Figure 2: Overview of DeepFD.
training iteration is expensive. Referencing recent works on dy-
namic trace collection [ 58,70] and runtime monitoring [ 50,54],
DeepFDcollects20runtimedata(seeTable2formoredetails),in-
cludinglossandaccuracy,thenumberoftimesthelossincreases,
themeanorstandardderivationofweights,etc.Thedataarerepeat-
edlycollectedatcertainintervals(e.g.,every256batchesorevery
epoch).Finally,160diagnosticfeaturesareextractedbyapplying
statistical analyses (e.g., calculating the variance and skewness) to
the collected data.
Table2liststhe20runtimedatacollectedbyDeepFDalongwith
the monitored variables and detailed description. The collection of
these runtime data are inspired by various existing works [ 1,10,
44,55,55,58,62,70] and processed in a way following the settings
used by existing studies [ 58,70]. The runtime data are collected at
predefined intervals during the training stage, resulting in 20 data
sequences.Thedatasequencesarethenprocessedusingtheeight
statistical operators in Table 3 to extract diagnostic features. Forexample, the skewness of a data sequence serves as a diagnosticfeaturethatquantifiestheasymmetryoftheprobabilitydistribu-
tionofthesequencewithrespecttoitsmean.Afterapplyingthestatistical operators, 160 (20 * 8 = 160) diagnostic features are ex-
tractedtocharacterizethetrainingprocessofthegivenDLprogram.
Step 2: Fault Diagnosis. Afterobtainingthediagnosticfeatures,
wetheninferthepossibletypesoffaultsthatexistintheDLpro-
gramaccordingtothefeatures.Weregarditasamulti-labelclas-
sificationproblem,whichmapstheobtainedfeaturesintooneor
more possible labels. Each label corresponds to a fault type. The
classificationreliesonthepredictionsmadebyasetofpretrained
diagnosis models (seeSection 3.2 for details). Thediagnosis result
is given by the union of the diagnosed faults predicted by each
diagnosis model to maximize the number of faults diagnosed. Also,
for each DL program under test, we run them ten times to address
the randomness of DNN model training [70].
Currently,thediagnosismodelsaretrainedtoclassifyfivemajor
typesoffaults,includingunsuitablelossfunction,optimizer,acti-
vation function, insufficient iteration and inappropriate learning
576
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. DeepFD: Automated Fault Diagnosis and Localization for Deep Learning Programs ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
Table 2: Runtime Data Collected by DeepFD During DNN Model Training
Runtime Data Monitored Variables Description
loss Loss Losson the training set.
acc Accuracy Accuracy on the training set.
loss_val Validation loss Losson the validation set.
acc_val Validation Aacuracy Accuracy on the validation set.
nan_loss Loss Thenumber of times loss is not a number (i.e., NaN)
nan_accuracy Accuracy Whether there are NaN in accuracy
nan_weight Weight Whether there are NaN in weight
nan_gradient Gradient Whether there are NaN in gradient
large_weight Weight Thenumber of times the maximum weight is larger than a certain threshold.
decrease_acc Trace of accuracy Thenumber of times the accuracy is smaller than the last recorded accuracy.
increase_loss Trace of loss Thenumber of times the loss is larger than the last recorded loss.
cons_mean_weight Trace of mean of weight Thenumber of times the mean of weight remains the same as the last record.
cons_std_w eight Trace of standard deviation of weight Whether the standard deviation of weight remains the same as the last record.
gap_train_test Accuracy, Validation accuracy Whether the gap between training accuracy and validating accuracy is too big.
test_turn_bad Loss,Validation loss Whether the loss on the training set decreases while on validation set increases.
slow_converge Trace of accuracy Whether the accuracy of trained models is growing slowly.
oscillating_loss Trace of Loss, Accuracy Whether the loss is oscillating.
dying_relu Traces of Gradient, AccuracyThere has been a set of neurons whose gradients have been 0 in the recent a few iterations and this set
is large forms a large portion of the whole DNN while the accuracy of the neuron network is still low.
gradient_vanish Gradient, Accuracy Whether the gradient vanish problem occurs.
gradient_explosion Gradient, Accuracy Whether the gradient explode problem occurs.
rate, which account for the majority (73.55%) types of faults in our
benchmark (see Section 4 for more details). We use these common
fault types to show the promising results of how learning-based
fault localization framework works, and make it extensible for sup-
porting more types of faults.
Step 3: Fault Localization. After acquiring the diagnosed types
offaults,DeepFDperformsfaultlocalizationattheprogramlevel.
Specifically, the program is first parsed into its abstract syntax
tree(AST),andDeepFDgoesthroughthenodesoftheparsetree,
traverses assignment statement as well as expressions, and then
identifies the places ( i.e.lines) where the diagnosed types of faults
aredefined.Forexample,tolocalizethe optimizer inthesource
code, DeepFD looks for invocations to specific model training APIs
(i.e.,fit), and parses the argument and keywords of this node, and
finally returns the line number where the optimizer is assigned.
However, this process is not always as easy as keyword identifica-
tion. For example, the learning rate , as a hyperparameter of the
optimizer,isusually omittedintheprogram( i.e.thedefaultlearn-
ing rate will be used), making keyword identification infeasible. In
thiscase,DeepFDlocatestothelinewheretheoptimizerisdefined,
while reporting the fault is in the type of learning rate, which can
provide a more accurate debugging information for the developers.
Since a fault may involve multiple lines, DeepFD reports a set of
suspicious lines of code for each fault diagnosed.
3.2 Diagnosis Model Construction
Two decisions are to be made in constructing diagnosis models.
First, we need to choose which machine learning algorithms to
construct the models. We choose K-Nearest Neighbors [ 8,22,34,
48], Decision Tree [ 45,52,53] and Random Forest [ 15,23,24])
to construct three diagnosis models. These three algorithms are
chosenbecauseoftheirwideadoption,explainabilityandsimplicity.Second,weneedtodecidehowtoaggregatetheirresults.Wechoose
to union their results to maximize the number of faults diagnosed.Table3:StatisticalOperatorsforRuntimeDataAggregation.
Operators Description
max The maximum value in a feature trace.
min The maximum value in a feature trace.
median The median value of a feature trace.
mean The mean value of a feature trace.
var The variance of a feature trace.
std The standard deviation of a feature trace.
skew The skewness of a feature trace.
sem The standard error of mean of a feature trace.
We will evaluate the individual effectiveness of the three diagnosis
models in Section 5.
Thethreediagnosismodelsareusedinthesecondstep( i.e.fault
diagnosis)ofDeepFD,servingastheclassifiersmappingdiagnosticfeaturesintodifferenttypesoffaultsthatmightexist.Thediagnosis
models are constructed in three steps as shown in Figure 3.
Step 1: Fault Seeding. This step is to prepare sufficient training
samples for the diagnosis models. Since there is no off-the-shelf
training set for this purpose, we are inspired by the idea of faultseeding in mutation testing [
33] to seed faults into normal pro-
grams.However,faultseedingforDLprogramsneedstoaddress
multiple challenges. First, what types of faults should we seed?
Though there are several existing empirical studies [ 27,30,31,71],
mostofthecollectedbuggyprogramsarenotreproduciblebecause
ofincompleteormissingcodesnippets,unavailabletrainingsets,
or program crashes. Without reproduction, inappropriate plausible
fixes may be included, inducing noise to our study. To address this
challenge,we constructedabenchmarkof 58buggyDLprograms
byreproducingthefailuresoftheDLprogramfaultscollectedby
recent empirical studies [ 27,30,31,58] from Github and Stack-
Overflow. For each reproduced faulty program, we investigated
itscharacteristics,includingthenumberoffaults,thefaulttypes,
577
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA CAO, et al.
(
	&%$
 

 	


 
  	

&% %

$
!##%



#  
 !#%! '%#%!  
#!$$
 !$%
	%&#$! # 
!
#  % !$$!$
	&%$%!

# 
#!#


	


	&%$
Figure 3: The Workflow of Diagnosis Models Construction.
aswellasthecodedifferencesbetweenthefaultyversionandits
fixed version. Furthermore, we found that a faulty DL program
often contain faults of more than one type. Since most statements
are involved in a DL program execution, the effects of multiple
faultsin afaulty executioncaneasily cascade.Tomimic thesitua-
tion,werandomlyseeduptofivetypesoffaultsineachprogram
mutant.Second,howtoseedconcretefaultsforaspecifictypeof
fault? Adapting DeepCrime [ 28], we designed seven fault-seeding
mutation operators in Table 4 for the five fault types.
Weexplainthedesignoftheseoperatorsandtheirdifferences
from existing works. The first two mutation operators target at
lossfunctions .Lossfunctionsmeasurethedifferencebetweenthe
groundtruthandthepredictedvalues,whichcanbefurtherdivided
into probabilistic loss functions and regression ones, suiting for
classification and regression tasks, respectively. When mutatingthe loss functions, DeepCrime [
28] randomly picks one from all
the other available loss functions regardless of which category the
lossfunctionis.Onthecontrary,wefirstfindoutthecategoryof
lossusedbythegivenDLprogram,andthenrandomlyselectone
fromanothercategory.Itisbecauselossesfromanothercategory
aremore likelytobe unsuitableforthe originaltask, and thusthe
generated mutant is more likely to be faulty. Next, to seed faults
in thelearning rate setting, we increaseor decrease the learning
rate, deliberately setting them to an extremely large or small value.
WhileDeepCrimeonlyconsidersettinglearningratestoextremely
small values. Furthermore,for the training epochs, we assignthe
epochtoasmallvaluebyrandomlydividingthecurrentnumberofepochwitharandomnumberwithin10to50,aimingatgenerating
a small enough epoch. While the existing work only generate a
randomnumber,whichmaybeevenlargerthanthecurrentnumberofepochs.Finally,forthe
activationfunction andoptimization
function ,werandomlychooseanotherfunctionfromtheavailable
functionsetsapartfromtheoriginalone.Notethatthoughexisting
works[28,32]canseedmorefaults,theirworksaredesignedfor
mutation testing, which serves a different purpose from that of
DeepFD. The fault seeding step in DeepFD serves as a preparation
forthediagnosismodeltrainingtoperformthefinalfaultdiagnosisand localization. It is also extensible to support seeding more types
of faults based on our framework.
Step 2: Training Set Preparation. In the previous step, a set of
mutants( i.e.fault-seededprograms)are generated.However, not
all the mutants are necessarily faulty. Due to the randomness ofDNNtraining,slightvariationsofDNNs‚Äôperformance4arenatural.
Simply considering a DNN with slightly varied performance as
faulty ignores the nature of randomness, and may induce manyfalse alarms. We present the criteria of faulty performance andthe procedure of seeding multiple faults. To determine whethera mutant is faulty, we first check whether there is a significant
statisticaldifferencebetweenthedistributionoftheoriginalDNN‚Äôs
accuracy ( i.e.ùê¥DO) and that of its mutant ùê¥DMas adopted the
following equation [28, 32]:
ùëñùë†ùêæùëñùëôùëô(N,M,ùëã)=‚éß‚é™‚é™ ‚é®
‚é™‚é™‚é©ùë°ùëüùë¢ùëíif effectSize (ùê¥DO(ùëã),ùê¥DM(ùëã)) ‚â•ùõΩ
& pValue (ùê¥DO(ùëã),ùê¥DM(ùëã))<ùõº
ùëìùëéùëôùë†ùëí‚é´‚é™‚é™ ‚é¨
‚é™‚é™‚é≠
(1)
whereùõºandùõΩarethresholdsthatcontrolthestatisticalsignificance
and effect size, and ùëãrepresents the testing set. Specifically, to
quantify the statistical significance and the effect size, we adopted
the generalised linear model (GLM) [ 46] and Cohen‚Äôs d [ 17]. If
the distribution of the mutant‚Äôs accuracy is statistically different
fromthatoftheoriginalmodel,thenwefurthercheckwhetherthe
averageaccuracyofthemutantisworsethanitsoriginalone.Ifthe
above two requirements are satisfied at the same time, the mutant
is considered as faulty, and is labeled as the types of faults that has
been seeded. If not, the mutant is regarded as non-buggy.
Furthermore, for mutants with more than one fault, another
challengeishow todeterminewhetherallthese seeded faultsare
the root causes leading to the deteriorated performance? We ad-
dress this challenge by seeding faults one by one. Specifically, after
obtaining a mutant with one type of fault, we then try to seed a
secondfaultthatisindependentofthefirstone,anduseEquation1tocheckwhetherthesecondfaultissuccessfullyseeded.Werepeat
theaboveprocesstoinjectthethirdfaultandsoon.Theprocessiterates until up to five types of faults are seeded in one original
program.Finally,werunallthesegeneratedmutantstogetherwiththeoriginalprogramstocollectthediagnosticfeaturesasdescribed
in the Step 1 of Section 3.1. These diagnostic features and their
labels are then used to train the diagnosis models of DeepFD.
Step3: DiagnosisModel Training. Wetreatthefaultdiagnosis
asamulti-labelclassificationproblemmappingafaultyprogram
tothemultipletypesoffaults thatitcontains.Weconstructthree
4In this paper, we use ‚Äúperformance‚Äù to refer the accuracy of loss of a DNN model, as
used in the existing work[67]
578
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. DeepFD: Automated Fault Diagnosis and Localization for Deep Learning Programs ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
Table 4: Seeded Faults and the Corresponding Fix Patterns
Mutation Operator Target Search Range Parameter Fix Patterns Description
Change loss function to those for
classification problemsLoss Function Enumerate‚Äúcategorical_crossentropy‚Äù,
‚Äúsparse_categorical_crossentropy‚Äù,
‚Äúbinary_crossentropy‚ÄùLoss FunctionThis group of fixes adjusts the loss function which helps
the training process to identify the deviation from the
learned and actual examples.Change loss function to those forregression-based problemsLoss Function Enumerate‚Äúmean_absolute_error‚Äù,
‚Äúmean_absolute_percentage_error‚Äù,
‚Äúmean_squared_error‚Äù
Change activation function in layers Activation Function Enumerate‚Äúrelu‚Äù,‚Äúsigmoid‚Äù,‚Äúsoftmax‚Äù,‚Äúsoftplus‚Äù,
‚Äúsoftsign‚Äù,‚Äútanh‚Äù,‚Äúselu‚Äù,‚Äúelu‚Äù,‚Äúlinear‚ÄùActivationThis group of fixes changes the activation function
used in a layer to better match the problem.
Decrease number of epoch Epoch Range originEpoch / random(10,50) IterationsThis group of fixes adjusts the number
of times the training process will be run.
Change optimization function Optimization Function Enumerate‚ÄúSGD‚Äù, ‚ÄúRMSprop‚Äù, ‚ÄúAdam‚Äù,
‚ÄúAdadelta‚Äù, ‚ÄúAdagrad‚ÄùOptimizerThis group of fixes modifies the optimization algorithms
used by the DNN model.
Decrease learning rate to an extreme
small valueLearning Rate Range [1e-16, 1e-10]Change neuralarchitectureThis group of fixes overhauls the design ofthe DNN‚Äôs architecture including a new setof layers and hyperparameters.
Increase learning rate to an extreme
large valueLearning Rate Range [1, 10]Change neural
architectureThis group of fixes overhauls the design of
the DNN‚Äôs architecture including a new set
of layers and hyperparameters.
diagnosis models using the three widely-used and effective ma-
chine learning algorithms (i.e., K-Nearest Neighbors [ 8,22,34,48],
Decision Tree [ 45,52,53] and Random Forest [ 15,23,24]) to learn
the correlations between diagnostic features and types of faults.
Specifically,theK-NearestNeighboralgorithmassumesthatsim-
ilar samples exist in close proximity. It clusters samples into K
groups according to the distance between samples. The decisiontree algorithm predicts the value of a target variable by learning
simpledecisionrulesinferredfromthefeatures.Therandomfor-
estalgorithmisanensemblelearningmethod,whichoperatesby
constructing a multitude of decision trees during training and out-
puttingtheresultthatisfavoredbymostofthedecisiontrees.Sinceasamplecanhavemultiplelabels,weadoptthemulti-labelversionofthesealgorithms[
56,60,68],whichclassifiesagivensampleinto
aset oftargetlabels. Finally, DeepFDtrainsthree diagnosismodels
against the constructed training set using these algorithms.
4 BENCHMARK CONSTRUCTION
To investigate the characteristics of faults in real faulty programs,
and to evaluate the effectiveness of DeepFD, we build a benchmark
with 58 buggy DL programs from StackOverflow and GitHub. The
benchmark includes the artifacts required to reproduce the bugs.
In this section, we elaborate how we construct the benchmark.
Subject Collectionand Selection. Wecollectthebenchmark
in two steps. First, we revisit all the benchmarks collected by previ-ousstudies[
27,30,31,58],e.g.,3,003postsfromStackOverflowand
2,328 commits from GitHub in total.Besides, in order to cover the
recentlypostedissuesthatarenotincludedbythepreviousstudies,
we also search StackOverflow for recent issues following the same
selection criteria as specified in [ 31]. Specifically, we collect the
posts from StackOverflow with accepted answers with the score
greater or equal to 5, and the posted time ranges from March, 2019
till April, 2021. Then, we select the subject implemented by Keras
and the symptom of the program is poor performance (i.e., the
program exhibits poor accuracy, loss during the training process).
WeselectKerasbecause46.4%ofthepostsandcommitsconcern
DLprogramsimplementedonKeras.Weselectprogramswithpoorperformancesinceitisthemostcommonsymptomapartfromcrash(i.e.,theprogramraisesanexceptionorcrash)[
71].Weexcludethe
crashedprogramsbecausetheyareuncommonforthosewritten
by experienced DL developers [ 27]. For posts in StackOverflow,
we exclude those without accepted answers and without source
codeorthetrainingdatasetsincewecannotreproducewithoutthe
concerned issues. For a similar reason, we exclude those GitHub
projects that do not have training sets available.
Reproduction and Repair. We reproduce and repair the col-
lectedsubjectswiththefollowingprocedures.(1)Werunthesource
code to see whether it is executable. If the source code crashes due
to API upgrade, versioning or typo issues, we fix these issues; oth-
erwise we skip this subject. (2) We examine whether the output
ofthesourcecodeexhibitsthesamesymptomasdescribedinthe
post. ForGitHub commits, if acommit message does notdescribe
the symptom, we capture the symptom by running the program,
comparing the results before and after applying the commit. If the
performanceincreasesafterapplyingthecommit,thenwetakethis
commit into account. (3) To fix the program, we adopt the patchesas suggested by the accepted answers in StackOverflow, as well as
other patches recommended in other replies. For GitHub commits,
werepairtheprogramaccordingtothecommittedchanges.Finally,
weobtain abenchmark with58faulty DNNprograms, alongwith
(1)thepatches,usuallymorethanone;(2)thetypesoffaultsand(3)
linenumberswherethefaultsareintroduced.Ifafault(e.g.,adding
morelayers)isnotlocalizeddowntoaspecificpieceofcode,we
recordedthelinenumberswhetherthepatchshouldbeadded.Our
benchmark is made publicly available [6].
Statistics of the Benchmark. The statistics of our benchmark
aregiveninFigure4.IntheleftpartoftheFigure,wepresentthe
number of eachtype of faults in thebenchmark. In particular, the
taxonomyoffaultsfollowstheexistingstudy[ 27],includingtheac-
tivationfunction,optimizer,lossfunction,hyperparameters(includ-ingthesuboptimallearningrate,thesuboptimalnumberofepochs,
and the suboptimal batch size), training data quality, model type
andproperties,layerproperties,missing/redundant/wronglayer,
missingpreprocess,andwrongpreprocessing.Thetopfivetypesoffaults(
i.e.,activationfunction,suboptimalnumberofepoch,subop-
timal learning rate, loss function and optimizer) are highlighted in
579
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA CAO, et al.
		

	
 
  
  
	


!  ! "  ! 
! " ! " 
"! 


	


	


')'"!
#"
%!!'
"&&
#' +%
&&!#%#
*%&
*%%"#%'&
'&+
%"!#%#
'$('*
"


	


	




')'"!
#"
%!!'
"&&
#' +%
&&!#%#
*%&
*%%"#%'&
'&+
%"!#%#
'$('*
"
Figure 4: The Statistics of Benchmark.
yellow,accountingforthemajority(73.55%)typesoffaultsinour
benchmark. In addition, we also realized that one program usually
containsmorethanonetypesoffaults,sowefurtherpresentthe
statisticsofthenumberoffaulttypes(inblue)andnumberoffaulty
lines(inred)in oneprogram.Notethatifonetypeof faultoccurs
inmultipleplacesintheprogram( e.g.,theactivationfunctioninall
thelayersareinappropriatelyset),weonlycountonesincethese
faults belong to the same type. Similarly, one faulty line of code
is counted regardless of how many faults contained in this line.For the cases with missing layers or missing data preprocessing,
we regard their number of faulty lines as one. From the second
diagramofFigure4,wecanseethatoverhalfoftheprogramsin
thebenchmarkcontainmorethanonefaulttype,andmostofthem
involve more than one line.
5 EVALUATION
We study three research questions to evaluate the relevancy of
diagnostic features and the effectivness of DeepFD in this section.
‚Ä¢RQ1. Are the extracted features relevant to fault diagno-
sis?To investigate whether the diagnostic features have high
correlations with certain types of faults, we apply these features
to perform fault diagnosis on the generated training sets to eval-
uatetherelevancyofsuchextractedfeaturesbasedontheresults.
The higher relevancy, the more accurate the diagnosis will be.
‚Ä¢RQ2.HoweffectiveisDeepFDinfaultdiagnosis? To evalu-
ate the effectiveness of DeepFD on fault diagnosis, we compared
DeepFDwithexistingworksonthebenchmarkintermsofthe
number of identified and correctly diagnosed faulty cases. We
also showed the diagnosis information reported by each work.
‚Ä¢RQ3.HoweffectiveisDeepFDforfaultlocalization? Accu-
rate fault localization is an important step towards automated
program repair. So we further evaluate the effectiveness of fault
localization on the benchmark and compare it with the existing
works.
5.1 Experiment Setup
WeimplementedDeepFDinPython,andconductedexperiments
on a machine with Intel i7-8700K CPU and Nvidia GeForce Titan V
12GBVRAM.Formutationgeneration,weruneachDNNmodel20
times to obtain a reliable statistical results. To address the random-
ness in DNN training, we run each experiment 10 times. For the
thresholdsinDeepFD,weset ùõºandùõΩtobe0.2and0.05,respectively,
following the settings in [ 73]. We used their default parameters
inthediagnosismodels.Wedidnotfine-tunetheparametersforbetter performance to facilitate reproducibility. In addition, param-
eter tuning is not the theme of this work. We collected runtimedata using the same default interval as in prior work [
70]. The
experimental results are made available for validation [6].
DatasetsandOriginalDLPrograms. Weperformedoureval-
uation on six popular datasets: Circle [ 4], Blob [3], MNIST [ 36],
CIFAR-10 [ 35], IMDB [ 41] and Reuters [ 5]. Circle and Blob are
twodatasetsfromSklearn[ 2]forclassificationtasks.MNISTisa
gray-scale image dataset used for handwritten digit recognition.CIFAR10 is a colored image dataset used for object recognition.
IMDBisamoviereviewdatasetforsentimentanalysis.Reutersisanewswiredatasetfordocumentclassification.FortheDLprograms,
weusetheprogramspublishedbyarecentwork[
70]astheoriginal
programs for mutant generation. Specifically, the work [ 70]p r o -
vided495DNNmodelsandtheirtrainingscriptsofvariousDNN
modelstructures(convolutionalneuralnetwork,recurrentneural
network and fully connected layers only) for these six datasets.
Among them,we were ableto reproducethe training of233 DNN
models. Therefore, we used these 233 DNN models as the original
models to generate mutants, following the process described in
Section 3.2. The statistics of generated mutants is shown in Table 5
(Entry ‚ÄòMutant‚Äô). For RQ1, the training set and validation set were
splitinaproportionof7:3.Theevaluationwasconductedonthe
validation set.
Baselines. In the experiment, we compared DeepFD with three
baselines: UMLAUT [ 54], AutoTrainer [ 70] and DeepLocalize [ 58].
Specifically,thefirstbaseline,UMLAUT[ 54],isaheuristic-based
frameworkproviding aninterfacetodebug faultsindataprepara-
tion,modelarchitectureandparametertuning.Inparticular,UM-
LAUTallowsuserstocheckthestructureofdeeplearningprograms,
modelbehaviorduringtraining.Afterdetectingfaults,itprovides
a set of human-readable error messages and repair advice. The
secondbaseline,AutoTrainer,isdesignedfordetectingfivetypesof
trainingproblems( i.e.,GradientVanish,GradientExplode,Dying
ReLU, Oscillating Loss and Slow Convergence). We use its default
parametersinourevaluation.Thethirdbaseline,DeepLocalize,is
abletoidentifythefaultylayerswhennumericalerroroccurs.Both
approaches monitor the runtime information ( e.g., weight and gra-
dient)duringtraining,andreporttrainingproblemsorfaultylayers
once detected. Besides, DeepLocalize performs the fault localiza-tion using two methods: translating the code into an imperative
representationmanuallyorinsertingcustomizedcallbackfunctions.
Weusedtheirsecondmethodforcomparisonbecausethesecond
oneachievesbetterperformancethanthefirstmethodaccordingto
580
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. DeepFD: Automated Fault Diagnosis and Localization for Deep Learning Programs ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
theirpaper.Althoughthesebaselinesdonotexplicitlypointoutthe
fault typesthey addressed, the fault types canstill bemappedfrom
the patches or faulty layers they provided to the fault types. After
the mapping, the fault types appearing in the evaluation buggy
programs can be covered by the baselines.
The three baselines were not originally designed for fault local-
ization.Theydonotlocatefaultylinesdowntoprogramcode.To
measure their effectiveness for fault localization, we adapt their
diagnosisresultsasfollows.ForUMLAUT,wemanuallyidentifythelinesofcoderelatingtotheerrormessages.ForAutoTrainer,thoughitdoesnotexplicitlylocatethefaults,ittriestorepairwhentraining
problems happens by applying predefined solutions. Therefore, we
regard the location of the solutions adopted by AutoTrainer as the
faultitlocated, andmanuallyidentifythe linesofcoderelatingto
the repaired artifact ( e.g.learning rate, layers‚Äô initialization) to the
corresponding lines. For DeepLocalize,we map the reported layer
tothecorrespondinglinesintheprogram.Andbecauseoftheman-
ualworkthatisinvolvedinbothfaultdiagnosisandlocalization
stages, we do not compare the overall runtime of each stage.
Evaluation Criteria. Weadoptthefollowingcriteriaindecid-
ingwhetherafaultissuccessfullydetected,diagnosedandlocalized.
A fault is successfully detectedif the detection result corresponds
totheexistenceofarealfault,regardlessofwhetheritsrootcauses
(i.e.types of faults) has been identified. A fault is successfully di-
agnosedifitsrootcausehasbeenidentified.Iftherearemultiple
types of faults, a successful fault diagnosis refers to those that can
pinpointatleastoneofthem. Forfaultlocalization,weexamine
whetheranyofthefaultylinesiscorrectlylocatedafteracorrect
diagnosis.
5.2 RQ1. Relevancy of Diagnostic Features
Weevaluatetherelevancyoftheextracteddiagnosticfeaturesby
showingtheaccuracyofthediagnosismodelstrainedwiththese
features.Ifthemodelsperformwellwhenusingthesefeatures,then
weregardthesediagnosticfeaturesasrelevanttofaultdiagnosis.
TheexperimentalresultsareshowninTable5.Beforeanalyzingthe
prediction result, we first demonstrate the statistics of the mutants
generatedinthefirststepoftheBootstrapstage.Specifically,for
eachdataset,welistthenumberofnormalDNNmodels(Origin),thenumber of generated mutants (Mutant), the detailed distribution ofhowmanymutationoperatorsareappliedtothegeneratedmutants
(column ‚ÄúMutation Operator‚Äù) and the average time for training
each mutant in seconds (Time).
On top of this training set, we further trained diagnosis mod-
els with three underlying algorithms (i.e., KNN, DT and RF areabbreviations of K-Nearest Neighbors, decision tree and randomforest), and demonstrated the accuracy of these models. In theimplementation, we normalized the features to better fit the K-
Nearest-Neighbors algorithm. As shown in the entry of ‚ÄúAccuracy
and Average Runtime of Diagnosis Models‚Äù in Table 5, the average
accuracy of these diagnosis models range from 70.68% to 79.90%
over different datasets. Besides, the accuracy obtained varies cross
datasets, ranging from 45.2% to 93.3%. Among all these datasets,the best performance of three underlying algorithms is achieved
onIMDBdataset,withtheaccuracyvaryingfrom88.8%to93.3%.
On the other hand, the accuracy on Circle and CIFAR-10 tend tobe worse. The result may be caused by the unbalanced mutants
with different numbers of faulty cases. The various performance
of different underlying algorithms on datasets also suggest us to
aggregate the diagnosis decisions by taking advantage of different
diagnosis models.
AnswertoRQ1 :Theextracteddiagnosticfeatureshavestrong
correlationswithourtargetedfivefaulttypes,whichisreflected
by the accuracy of diagnosis models, with an average accuracy
varying from 70.68% to 79.90%, and up to 93.30% for certain
cases.
5.3 RQ2. Effectiveness of Fault Diagnosis.
ToanswerRQ2,weevaluatetheeffectivenessofDeepFDinterms
offault diagnosison the52 buggyprograms5fromthe benchmark.
NotethatthesubjectsusedforRQ2andRQ3donotoverlapwith
those used in RQ1. Due to space limitation, Table 6 shows partially
ourevaluationresults,6includingthediagnosisinformationpro-
vided by each method (column ‚ÄòDiagnosis Information of Different
Methods‚Äô), whether they detect the existence of faults (column
‚ÄòFault Detection‚Äô) and whether the fault diagnosis is correct or not
(column‚ÄòFaultDiagnosis‚Äô).UMLAUTenumerateserrormessages
with three severity levels ( i.e., Error, Critical and Warning). Au-
toTrainerreportsthetrainingproblem.DeepLocalizereportsthe
place(e.g.,layer)andthebatchatwhichthepredefinedsymptom
occurs. DeepFD reports the diagnosed fault types.
The ‚ÄòSummary‚Äô and ‚ÄòOverall Ratio‚Äô of Table 6 summarize the
resultsofeachworkonthebenchmark.UMLAUT,DeepLocalize
andDeepFDcandetect69%to96%faultycases.AlthoughUMLAUT
detectsthemostnumber(50)offaultycases,only14ofthemare
correctly diagnosed. In contrast, DeepFD is able to detect 47 faulty
cases and correctly diagnosed 27 of them.
Therearedifferentreasonsforinaccuratediagnosis.UMLAUT
isdesignedforclassificationproblems.Itassumesthattheoutput
layer is always activated by Softmax. If this is not the case, it re-
ports‚ÄúMissingSoftmaxlayerbeforeloss‚Äù.However,thisassumption
does not necessarily hold. Activation functions like Linearand
Sigmoid arealsofrequentlyusedtoactivatetheoutputlayerfor
classificationandregressionproblems.Indeed,mostofthefaults
detected by UMLAUT are attributed to the violation of this as-
sumption,causingmanyfalsealarms.AutoTraineronlydetectsfivetrainingproblems(
e.g.dyingReLU),leavingmostcaseswithoutap-
parent symptoms escaped from its detection. DeepLocalize detects
faults that cause numerical anomalies such as NaN, and reports the
placewheretheanomalieshappen.Yet,anomaliesrarelyoccurat
the point where the faults reside, making the fault diagnosis byDeepLocalize inaccurate. On the other hand, the effectiveness of
DeepFDislimitedbythenumberoffaulttypesthatcanbeclassified
bythediagnosismodels.Withmoretypesoffaultsseeded,more
faults can be detected and diagnosed, and thus the effectiveness of
DeepFD can be improved.
5Therestsixprogramswereomittedbecausetheyeitherwereunabletobeadaptedto
launch the methods, or crashed when the methods are applied.
6We listed the cases that can be correctly diagnosed by at least three methods among
UMLAUT, AutoTrainer, DeepLocalize and DeepFD. The complete experimental results
are online available [6]
581
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA CAO, et al.
Table5:EffectivenessofDiagnosisModelsofDeepFDonLabeledDNNSet.KNN,DTandRFstandfortheunderlyingalgorithms
(K-Nearest Neighbors, Decision Tree and Random Forest) of diagnosis models.
Statistics Accuracy and Average Runtime of Diagnosis Models
Origin Mutant# Types of FaultsTimeKNN DT RF
Dataset 1 2345 Acc (%) Time (s) Acc (%) Time (s) Acc(%) Time (s)
MNIST 7817681027302295134100.60 69.29 0.10 63.15 0.01 81.58 0.12
CIFAR-10 35 786 651 102 33 0 0 1.56 52.63 0.05 53.94 0.02 64.47 0.12
Circle 369365801741334458.87 45.20 0.23 55.50 0.03 61.36 0.22
Blob 39 685 335 158 137 50 5 0.13 83.95 0.06 79.01 0.01 87.65 0.10
Reuters 3217513822312036.30 79.69 0.05 79.69 0.01 81.25 0.09
IMDB 13 110 76 25 9 0 0 53.72 93.30 0.04 88.80 0.03 93.30 0.09
Total 233 4,460 2,807 783 610 240 20 Average 70.68 0.09 79.90 0.02 78.27 0.12
Answer to RQ2 : DeepFD outperforms the existing works on
fault diagnosis, with a fault diagnosis rate of 52%, which almost
doubles that of the best baseline (i.e., 27%).
5.4 RQ3. Effectiveness of Fault Localization
The ‚ÄòFault Diagnosis‚Äô column in Table 6 gives the evaluation re-
sult of fault localization. DeepFD outperforms other methods by
correctlylocating42%offaults,whichfavorablycomparestothe
23% by UMLAUT, the best baseline performer. The performance of
AutoTrainerandDeepLocalizearerelativelyunsatisfactory,ranging
from 10% to 15%.
Indeed,theperformanceonfaultlocalizationarenotsatisfactory,
ranging from 10% to 42%. The reasons behind are mainly two folds.
First,theratioofcorrectlydiagnosedcasesarenothigh,whichisat
most 52%. Second, considering the complicated types of faults and
possible multiple patches, an effective localization strategy is to be
explored. For example, suppose we know the root cause is the use
of inappropriate activation,yet there are multiple activationfunc-
tionsinseverallayers.Consequently,locatingfaultstoaspecific
activation function at a specific layer is an outstanding challenge.
Answer to RQ3 : The results show that DeepFD significantly
outperformsthethreebaselinesbycorrectlylocating42%ofthe
cases.Incomparison,only10%to23%casescanbelocatedby
the baselines.
6 RELATED WORK
6.1 Debugging Deep Learning Systems
Recently,abranchoftechniqueshavebeenproposedtofacilitate
thedebuggingprocessofdeeplearningsoftwaresystems.Several
works focus on the DNN models, locating suspicious neurons and
correcting them by prioritizing or augmenting the training data.
For instance, Ma et al.[40] leveraged the state differential analysis
toidentifythefeaturesthatareresponsiblefortheincorrectclassifi-
cationresults,andthengeneratetheinputsbasedonthesefeaturestocorrectthe behavioursoftheDNNmodels.Eniser
et al.[20]pr o-
posedDeepFault,aframeworktoidentifythesuspiciousneurons
inaDNNandthenfixtheseerrorsbygeneratingsamplesformodel
retraining. Apricot [ 66] is a weight-adaption approach to fix DNNmodels.Theirintuitionisthatthelimitationofthecompletemodels
maybeaddressed viaadaptingtheweightof thecompactmodels.
These previous studies concentrate on the the DNN models and
they are not able to locate the faulty lines in DNN programs.
Besides, there are several studies that are closer to ours. For
instance, AutoTrainer [ 70] proposed an automatic approach to de-
tect and fix the training problems in DNN programs at runtime.It particularly focuses on detecting and repairing five common
training problems: gradient vanish, gradientexplode, dying ReLU,
oscillating loss, and slow convergence. It encapsulates and auto-
mates the detecting and repairing process by dynamic monitoring.
However,itreliesonpredefinedrulestoperformthebugdetection.
Wardatet al.[58]proposedDeepLocalize,thefirstfaultlocalization
approach for DNN models (such as incorrect learning rate or inap-
propriatelossfunction).Yet,itlocatestolayerswherethesymptom
happens instead of where the fault‚Äôs root cause resides. Amazon
SageMaker Debugger [ 50] and UMLAUT [ 54] both provide a set of
built-inheuristicstodebugfaultsinDNNmodelsduringtraining.
Themajordifferencebetweenourworkfromthesemethodsisthat
DeepFD does not rely on predefined rules, making it more flexible
and adaptive for various types of faults.
6.2 Automated Machine Learning
Automated Machine Learning (AutoML) provides methods and
processes to make machine learning available for non-Machine
Learning experts. The user simply provides data, and the AutoML
systemautomaticallydeterminestheapproachthatperformsthe
bestforaparticularapplication[ 29].Inparticular,therearethree
mainproblemsinAutoML,includingHyperparameterOptimiza-
tion (HPO), Neural Architecture Search (NAS), and meta-learning.
HPO [49,64,69] search for methods to set optimal hyperparam-
eters in ML algorithms automatically, thus reducing the human
efforts necessary for applying ML. Yet, it is not always clear which
hyperparameters of an algorithm need to be optimized, and in
which ranges [ 29]. In contrast, DeepFD differs from HPO in nature.
DeepFD diagnoses and locates faults in a given model, while the
HPOmethodssearchanoptimalmodelfromscratch. NASmeth-
ods[19,37,38,43,47,51,72] are designed to automatically design
morecomplexneuralarchitectures.Meta-Learning[ 13,14,16,18,
21,42]aimstoimprovelearningacrossdifferenttasksordatasets.
It can significantly improve the efficiency of HPO and NAS with
582
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. DeepFD: Automated Fault Diagnosis and Localization for Deep Learning Programs ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
Table 6: Comparison on Diagnosis Information, Fault Detection, Diagnosis and Localization Between UMLAUT (UT), Auto-
Trainer (AT), DeepLocalize (DLoc) and DeepFD (DFD).
Post#Diagnosis Information of Different Methods FaultDetection FaultDiagnosis FaultLocalization
UMLAUT AutoTrainer DeepLocalize DeepFD UT AT DLoc DFD UT AT DLoc DFD UT AT DLoc DFD
48385830(1)Critical: Missing Softmax layer before loss
(2) Warning: Possible overfittingexplodeLayer-1 Error in forward
Stop at epoch 1, batch 2
Accuracy: 0.13, loss: infFault1: [act] (Lines: [-])
Fault 2: [loss] (Lines: [57])  
55328966(1)Error: Input data exceeds typical limits
(2)Warning: Possible overfitting
(3)Warning: Check validation accuracy
(4)Critical: Missing Softmax layer before loss
(5)Critical: Missing activationfunctions
(6)Warning: Last model layer has nonlinear activationexplode ‚Äì Fault1: [opt] (Lines: [49]) 
34311586(1)Critical: Missing Softmax layer before loss
(2) Warning: Last model layer has nonlinear activation‚ÄìBatch0 layer 2: Error in delta
Weights, terminating trainingFault1: [lr] (Lines: [27])   
50079585(1)Critical: Missing Softmax layer before loss
(2)Critical: Missing activationfunctions
(3)Warning: Last model layer has nonlinear activationunstable ‚ÄìFault1: [lr] (Lines: [44])
Fault2: [epoch] (Lines: [15])
47352366(1)Critical: Missing Softmax layer before loss
(2) Warning: Last model layer has nonlinear activationexplodeLayer-12 Error in delta weights
Stop at epoch 1, batch 24accuracy: 0.31, loss: 6.16 Fault1: [opt] (Lines: [40])   
59282996(1)Error: Input data exceeds typical limits
(2)Warning: Check validation accuracy
(3)Critical: Missing Softmax layer before lossunstable ‚Äì Fault1: [epoch] (Lines: [309]) 
37624102(1)Critical: Missing Softmax layer before loss
(2) Critical: Missing activationfunctions(3) Warning: Last model layer has nonlinear activation(4) Error: Image data may have incorrect shape(5) Warning: Learning Rate is high(6) Warning: Check validation accuracy
unstableBatch0 layer 9: Error in Output
Gradient, terminating trainingFault1: [lr] (Lines: [66])
Fault 2: [Act] (Lines: [54, 56, 61, 64])  
41600519(1)Error: Input data exceeds typical limits
(2)Critical: Missing Softmax layer before loss
(3)Warning: Last model layer has nonlinear activationunstableBatch0 layer 6: Error in forward,
terminating trainingFault1: [loss] (Lines: [32]) 
......
Summar y501236471410 7 271285 22
Overall Ratio 0.960.230.69 0.90 0.270.19 0.13 0.520.230.15 0.10 0.42
the help of the transferred knowledge between tasks. Note that
thoughthesemethodsarenotdesignedfordebugginganexisting
program,itispotentialtoapplythesemethods,especiallytheHPO
andNASonesforrepairingafterfaultsarediagnosedandlocalized.
The diagnosed information can serve as a guidance, narrowing
downtheparametersthatneedtobetuned,andthesearchspace
for NAS to explore.
7 THREATS TO VALIDITY
In this section, we discuss three threats that may affect the validity
of our work. First, the construction of benchmark ( e.g.the repro-
duction,rootcauseanalysisandrepair)involvedmanualinspection
of the source code, which may be subjective. To reduce this threat,
each subject was examined by three authors separately and the
results were cross-validated. Decisions were made only if the three
authors reached an agreement. Second, to prepare the training set
for diagnosis models, we assume the original programs ( i.e.pro-
gramsbeforeseedingfaults)arefault-free,yetitmaynotalwaysbe
the truth. Though they are released [ 70] and guaranteed to be free
from five training problems ( e.g.gradient vanish and dying ReLU),
itisstillpossibletherearehiddenfaultsintheprogram.Third,to
reproducetheresultsofexistingworks,weadoptthedefaultvalues
for the parameters, which may affect their performance and effi-
ciency. Besides, some works need to manually adapt the programs
in order to launch the debugging process, which may introduce
unexpectedvariancefromtheoriginalprogram.Also,withtheman-
ual work involved, the time cost by each work is hardto evaluate,leaving the efficiency of each work incomparable. Finally, sinceexisting works cannot locate faults to the program, we carefullyinvestigate their diagnosis information, and manually locate the
faulty lines for fair comparisons, which may also slightly affect the
comparison results.
8 CONCLUSION
Inthispaper,weproposedDeepFD,alearning-basedfaultdiagnosis
and localization framework which maps the fault localization task
to a learning problem. In particular, DeepFD diagnosis faults by
classifying runtimefeatures intopossible types offaults ( e.g.inap-
propriateoptimizer), thenlocates faultylines tothe program. The
evaluation shows the potential of DeepFD. Specifically, it correctly
diagnoses 52% of the cases, compared with half (27%) achievedby the best state-of-the-art works. Besides, for fault localization,DeepFD also outperforms the existing works, correctly locating
42%faultycasesonthebenchmark,whichalmostdoublestheresult
(23%) achieved by the best state-of-the-art work.
ACKNOWLEDGMENT
This work was supported by the National Key Research and Devel-
opmentProgramofChina,No.2019YFE0198100,NationalNatural
Science Foundation of China (Grant Nos. 61932021, 62002125), the
HongKongRGC/GRF(GrantNo.16207120),HongKongITF(Grant
No. MHP/055/19), Huawei PhD Fellowship, and MSRA Collabo-rative Research Grant. The authors would also like to thank the
anonymous reviewers for their comments and suggestions.
583
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA CAO, et al.
REFERENCES
[1]2020. Convolutional neural networks for visual recognition. https://cs231n.
github.io/neural-networks-3/
[2] 2020. scikit-learn, machine learning in python. https://scikit-learn.org/stable/
[3]2020. Sklearn, make blobs dataset. https://scikit-learn.org/stable/modules/
generated/sklearn.datasets.make_blobs.html
[4]2020. Sklearn, make circles dataset. https://scikit-learn.org/stable/modules/
generated/sklearn.datasets.make_circles.html
[5]2021. Reuters-21578 Text Categorization Collection Data Set. https://archive.ics.
uci.edu/ml/datasets/reuters-21578+text+categorization+collection
[6] 2022. DeepFD. https://github.com/ArabelaTso/DeepFD[7]
Rui Abreu, Peter Zoeteweij, Rob Golsteijn, and Arjan J. C. van Gemund. 2009.
A practicalevaluation of spectrum-basedfault localization. J. Syst.Softw. 82, 11
(2009), 1780‚Äì1792.
[8]NaomiSAltman.1992. Anintroductiontokernelandnearest-neighbornonpara-
metric regression. The American Statistician 46, 3 (1992), 175‚Äì185.
[9] Paul Ammann and Jeff Offutt. 2016. Introduction to software testing. Cambridge
University Press.
[10]Isac Arnekvist, J. Frederico Carvalho, Danica Kragic, and Johannes A. Stork.
2020. The effect of Target Normalization and Momentum on Dying ReLU. CoRR
abs/2005.06195(2020).
[11]AndersArpteg,Bj√∂rnBrinne,LukaCrnkovic-Friis,andJanBosch.2018. Software
Engineering Challenges of Deep Learning. In 44th Euromicro Conference on Soft-
ware Engineering and Advanced Applications, SEAA 2018, Prague, Czech Republic,
August 29-31, 2018, Tom√°s Bures and Lefteris Angelis (Eds.). IEEE Computer
Society, 50‚Äì59.
[12]Shay Artzi, Julian Dolby, Frank Tip, and Marco Pistoia. 2010. Practical fault
localization for dynamic web applications. In Proceedings of the 32nd ACM/IEEE
InternationalConferenceonSoftwareEngineering-Volume1,ICSE2010,CapeTown,
South Africa, 1-8 May 2010, Jeff Kramer, Judith Bishop, Premkumar T. Devanbu,
and Sebasti√°n Uchitel (Eds.). ACM, 265‚Äì274. https://doi.org/10.1145/1806799.
1806840
[13]BesimBilalli,AlbertoAbell√≥,andTom√†sAluja-Banet.2017. Onthepredictive
power of meta-features in OpenML. Int. J. Appl. Math. Comput. Sci. 27, 4 (2017),
697‚Äì712. https://doi.org/10.1515/amcs-2017-0048
[14]PavelBrazdil,ChristopheG.Giraud-Carrier,CarlosSoares,andRicardoVilalta.
2009.Metalearning-ApplicationstoDataMining. Springer. https://doi.org/10.
1007/978-3-540-73263-1
[15] Leo Breiman. 2001. Random forests. Machine learning 45, 1 (2001), 5‚Äì32.
[16]CiroCastiello,GiovannaCastellano,andAnnaMariaFanelli.2005. Meta-data:
Characterizationof InputFeaturesforMeta-learning. In ModelingDecisionsfor
ArtificialIntelligence,SecondInternationalConference,MDAI2005,Tsukuba,Japan,
July25-27,2005,Proceedings (LectureNotesinComputerScience),Vicen√ßTorra,
Yasuo Narukawa, and Sadaaki Miyamoto (Eds.), Vol. 3558. Springer, 457‚Äì468.
https://doi.org/10.1007/11526018_45
[17] Jacob Cohen. 1992. A power primer. Psychological bulletin 112, 1 (1992), 155.
[18]IddoDrori,YamunaKrishnamurthy,RemiRampin,RaoniLouren√ßo,JorgeOne,
etal.2018. AlphaD3M:Machinelearningpipelinesynthesis.In AutoMLWorkshop
at ICML.
[19]ThomasElsken,JanHendrikMetzen,andFrankHutter.2019. NeuralArchitecture
Search: A Survey. J. Mach. Learn. Res. 20 (2019), 55:1‚Äì55:21. http://jmlr.org/
papers/v20/18-598.html
[20]Hasan Ferit Eniser, Simos Gerasimou, and Alper Sen. 2019. DeepFault: Fault
LocalizationforDeepNeural Networks.In FundamentalApproachestoSoftware
Engineering-22ndInternationalConference,FASE2019,HeldasPartoftheEuropean
Joint Conferenceson Theoryand Practice ofSoftware, ETAPS 2019,Prague, Czech
Republic, April 6-11, 2019, Proceedings (Lecture Notes in Computer Science), Reiner
H√§hnle and Wil M. P. van der Aalst (Eds.), Vol. 11424. Springer, 171‚Äì191.
[21]Matthias Feurer, Benjamin Letham, and Eytan Bakshy. 2018. ScalableMeta-Learning for Bayesian Optimization. CoRRabs/1802.02219 (2018).
arXiv:1802.02219 http://arxiv.org/abs/1802.02219
[22]Evelyn Fix and Joseph L Hodges Jr. 1952. Discriminatory analysis-nonparametric
discrimination:Smallsampleperformance . TechnicalReport.CALIFORNIAUNIV
BERKELEY.
[23]TinKamHo.1995. Randomdecisionforests.In Proceedingsof3rdinternational
conference on document analysis and recognition, Vol. 1. IEEE, 278‚Äì282.
[24]Tin Kam Ho. 1998. The random subspace method for constructing decision
forests.IEEEtransactionsonpatternanalysisandmachineintelligence 20,8(1998),
832‚Äì844.
[25]Sepp Hochreiter. 1991. Untersuchungen zu dynamischen neuronalen Netzen.
Diploma, Technische Universit√§t M√ºnchen 91, 1 (1991).
[26] Qiang Hu, Lei Ma, Xiaofei Xie, Bing Yu, Yang Liu, et al. 2019. DeepMutation++:
AMutationTestingFrameworkforDeepLearningSystems.In 34thIEEE/ACM
InternationalConferenceonAutomatedSoftwareEngineering,ASE2019,SanDiego,
CA, USA, November 11-15, 2019. IEEE, 1158‚Äì1161.
[27]NargizHumbatova,GunelJahangirova,GabrieleBavota,VincenzoRiccio,Andrea
Stocco, et al .2020. Taxonomy of real faults in deep learning systems. In ICSE‚Äô20: 42nd International Conference on Software Engineering, Gregg Rothermel and
Doo-Hwan Bae (Eds.). ACM, 1110‚Äì1121.
[28]Nargiz Humbatova, Gunel Jahangirova, and Paolo Tonella. 2021. DeepCrime:
mutation testing of deep learning systems based on real faults. In ISSTA ‚Äô21: 30th
ACM SIGSOFT International Symposium on Software Testing and Analysis, Virtual
Event,Denmark,July11-17,2021,CristianCadarandXiangyuZhang(Eds.).ACM,
67‚Äì78. https://doi.org/10.1145/3460319.3464825
[29]Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren (Eds.). 2019. Automated
Machine Learning - Methods, Systems, Challenges. Springer.
[30]Md Johirul Islam, Giang Nguyen, Rangeet Pan, and Hridesh Rajan. 2019. A com-
prehensivestudyondeeplearningbugcharacteristics.In ProceedingsoftheACM
JointMeetingonEuropeanSoftwareEngineeringConferenceandSymposiumon
theFoundationsofSoftwareEngineering,ESEC/SIGSOFTFSE2019,Tallinn,Estonia,
August 26-30, 2019, Marlon Dumas, Dietmar Pfahl, Sven Apel, and Alessandra
Russo (Eds.). ACM, 510‚Äì520.
[31]MdJohirulIslam,RangeetPan,GiangNguyen,andHrideshRajan.2020.Repairingdeepneuralnetworks:fixpatternsandchallenges.In ICSE‚Äô20:42ndInternational
Conference on Software Engineering, Gregg Rothermel and Doo-Hwan Bae (Eds.).
ACM, 1135‚Äì1146.
[32]GunelJahangirovaandPaoloTonella.2020. AnEmpiricalEvaluationofMutation
OperatorsforDeepLearningSystems.In 13thIEEEInternationalConferenceon
SoftwareTesting,ValidationandVerification,ICST2020,Porto,Portugal,October
24-28, 2020. IEEE, 74‚Äì84.
[33]Yue Jia and Mark Harman. 2011. An Analysis and Survey of the Development of
Mutation Testing. IEEE Trans. Software Eng. 37, 5 (2011), 649‚Äì678.
[34]Liangxiao Jiang, Zhihua Cai, Dianhong Wang, and Siwei Jiang. 2007. Survey of
improvingk-nearest-neighborforclassification.In Fourthinternationalconference
on fuzzy systems and knowledge discovery (FSKD 2007), Vol. 1. IEEE, 679‚Äì683.
[35]AlexKrizhevsky,GeoffreyHinton,etal .2009. Learningmultiplelayersoffeatures
from tiny images. (2009).
[36]Yann LeCun and Corinna Cortes. 2010. MNIST handwritten digit database.
http://yann.lecun.com/exdb/mnist/. (2010). http://yann.lecun.com/exdb/mnist/
[37]JasonZhiLiang,ElliotMeyerson,BabakHodjat,DanielFink,KarlMutch,etal .
2019. Evolutionary neural AutoML for deep learning. In Proceedings of the
GeneticandEvolutionaryComputationConference,GECCO2019,Prague,Czech
Republic,July13-17,2019 ,AnneAugerandThomasSt√ºtzle(Eds.).ACM,401‚Äì409.
https://doi.org/10.1145/3321707.3321721
[38]Peiye Liu, Bo Wu, Huadong Ma, and Mingoo Seok. 2020. MemNAS: Memory-
EfficientNeuralArchitectureSearchWithGrow-TrimLearning.In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).
[39]LeiMa,FuyuanZhang,JiyuanSun,MinhuiXue,BoLi,etal .2018. DeepMutation:
MutationTestingofDeepLearningSystems.In 29thIEEEInternationalSymposium
on Software Reliability Engineering, ISSRE 2018, Memphis, TN, USA, October 15-18,
2018,SudiptoGhosh,RobertoNatella,BojanCukic,RobinS.Poston,andNuno
Laranjeiro (Eds.). IEEE Computer Society, 100‚Äì111.
[40]ShiqingMa,YingqiLiu,Wen-ChuanLee,XiangyuZhang,andAnanthGrama.
2018. MODE: automatedneuralnetworkmodeldebuggingvia statedifferential
analysis and input selection. In Proceedings of the 2018 ACM Joint Meeting on
European Software Engineering Conference and Symposium on the Foundations of
Software Engineering, ESEC/SIGSOFT FSE 2018, Lake Buena Vista, FL, USA, No-
vember04-09,2018,GaryT.Leavens,AlessandroGarcia,andCorinaS.Pasareanu
(Eds.). ACM, 175‚Äì186.
[41]AndrewL.Maas,RaymondE.Daly,PeterT.Pham,DanHuang,AndrewY.Ng,
et al.2011. Learning Word Vectors for Sentiment Analysis. In Proceedings of
the 49th Annual Meeting of the Association for Computational Linguistics: Human
Language Technologies. Association for Computational Linguistics, Portland,
Oregon, USA, 142‚Äì150. http://www.aclweb.org/anthology/P11-1015
[42]Elan SopherMarkowitz, KeshavBalasubramanian, MehrnooshMirtaheri, Sami
Abu-El-Haija, Bryan Perozzi, et al .2021. Graph Traversal with Tensor Func-
tionals: A Meta-Algorithm for Scalable Learning. In 9th International Conference
on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.
OpenReview.net. https://openreview.net/forum?id=6DOZ8XNNfGN
[43]Risto Miikkulainen.2021. Evolution ofneural networks.In GECCO ‚Äô21:Genetic
and Evolutionary Computation Conference, Companion Volume, Lille, France, July
10-14, 2021, Krzysztof Krawiec (Ed.). ACM, 426‚Äì442. https://doi.org/10.1145/
3449726.3461432
[44]JohnMillerandMoritzHardt.2019. StableRecurrentModels.In 7thInternational
ConferenceonLearningRepresentations,ICLR2019,NewOrleans,LA,USA,May
6-9, 2019. OpenReview.net.
[45]SreeramaKMurthy.1998. Automaticconstructionofdecisiontreesfromdata:
A multi-disciplinary survey. Data mining and knowledge discovery 2, 4 (1998),
345‚Äì389.
[46]J.A.NelderandR.W.M.Wedderburn.1972. GeneralizedLinearModels. Journal
of the Royal Statistical Society. Series A (General) 135, 3 (1972), 370‚Äì384. http:
//www.jstor.org/stable/2344614
[47]Hieu Pham,Melody Guan, BarretZoph, Quoc Le,and Jeff Dean. 2018. Efficient
neural architecture search via parameters sharing. In International Conference on
Machine Learning. PMLR, 4095‚Äì4104.
584
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. DeepFD: Automated Fault Diagnosis and Localization for Deep Learning Programs ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
[48]Thair Nu Phyu. 2009. Survey of classification techniques in data mining. In
Proceedingsoftheinternationalmulticonferenceofengineersandcomputerscientists,
Vol. 1.
[49]Basheer Qolomany, Majdi Maabreh, Ala I. Al-Fuqaha, Ajay Gupta, and Driss
Benhaddou.2017. ParametersoptimizationofdeeplearningmodelsusingParticle
swarm optimization. In 13th International Wireless Communications and Mobile
Computing Conference, IWCMC 2017, Valencia, Spain, June 26-30, 2017. IEEE,
1285‚Äì1290. https://doi.org/10.1109/IWCMC.2017.7986470
[50]Nathalie Rauschmayr, Vikas Kumar, Rahul Huilgol, Andrea Olgiati, Satadal Bhat-
tacharjee, et al .2021. Amazon SageMaker Debugger: A System for Real-Time
InsightsintoMachineLearningModelTraining. ProceedingsofMachineLearning
and Systems 3 (2021).
[51]RaananY.YehezkelRohekar,ShamiNisimov,YanivGurwicz,GuyKoren,andGal
Novik.2018. ConstructingDeepNeuralNetworksbyBayesianNetworkStructure
Learning.In Advancesin NeuralInformationProcessing Systems31: AnnualCon-
ferenceonNeuralInformationProcessingSystems2018,NeurIPS2018,December3-8,
2018, Montr√©al, Canada, Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kris-
ten Grauman, Nicol√≤ Cesa-Bianchi, et al .(Eds.). 3051‚Äì3062. https://proceedings.
neurips.cc/paper/2018/hash/95d309f0b035d97f69902e7972c2b2e6-Abstract.html
[52]Lior Rokach and Oded Maimon. 2005. Decision trees. In Data mining and
knowledge discovery handbook. Springer, 165‚Äì192.
[53]SRasoulSafavianandDavidLandgrebe.1991. Asurveyofdecisiontreeclassifier
methodology. IEEE transactions on systems, man, and cybernetics 21, 3 (1991),
660‚Äì674.
[54]EldonSchoop,ForrestHuang,andBjoernHartmann.2021. UMLAUT:Debugging
DeepLearningProgramsusingProgramStructureandModelBehavior.In CHI
‚Äô21: CHI Conference on Human Factors in Computing Systems, Virtual Event /
Yokohama,Japan,May8-13,2021,YoshifumiKitamura,AaronQuigley,Katherine
Isbister,TakeoIgarashi,PernilleBj√∏rn,etal .(Eds.).ACM,310:1‚Äì310:16. https:
//doi.org/10.1145/3411764.3445538
[55]David Sussillo and LF Abbott. 2014. Random walk initialization for training very
deep feedforward networks. arXiv preprint arXiv:1412.6558 (2014).
[56]Celine Vens, Jan Struyf, Leander Schietgat, Sa≈°o D≈æeroski, and Hendrik Blockeel.
2008. Decision trees for hierarchical multi-label classification. Machine learning
73, 2 (2008), 185.
[57]XinmingWang,Shing-ChiCheung,WingKwongChan,andZhenyuZhang.2009.
Taming coincidental correctness: Coverage refinement with context patterns to
improvefaultlocalization.In 31stInternationalConferenceonSoftwareEngineering,
ICSE 2009, May 16-24, 2009, Vancouver, Canada, Proceedings. IEEE, 45‚Äì55.
[58]MohammadWardat,WeiLe,andHrideshRajan.2021. DeepLocalize:FaultLocal-
ization for Deep Neural Networks. In ICSE‚Äô21: The 43nd International Conference
on Software Engineering.
[59]MingWen,RongxinWu,andShing-ChiCheung.2016. Locus:locatingbugsfrom
softwarechanges.In Proceedingsofthe31stIEEE/ACMInternationalConference
on Automated Software Engineering, ASE 2016, Singapore, September 3-7, 2016,David Lo, Sven Apel, and Sarfraz Khurshid (Eds.). ACM, 262‚Äì273. https://doi.
org/10.1145/2970276.2970359
[60]Qingyao Wu, Mingkui Tan, Hengjie Song, Jian Chen, and Michael K Ng. 2016.
ML-FOREST:Amulti-labeltreeensemblemethodformulti-labelclassification.IEEE transactions on knowledge and data engineering 28, 10 (2016), 2665‚Äì2680.
[61]Rongxin Wu, Hongyu Zhang, Shing-Chi Cheung, and Sunghun Kim. 2014.CrashLocator: locating crashing faults based on crash stacks. In International
Symposium on Software Testing and Analysis, ISSTA ‚Äô14, San Jose, CA, USA - July
21 - 26, 2014, Corina S. Pasareanu and Darko Marinov (Eds.). ACM, 204‚Äì214.
[62]Chen Xing, Devansh Arpit, Christos Tsirigotis, and Yoshua Bengio. 2018. A walk
with sgd. arXiv preprint arXiv:1802.08770 (2018).
[63]Shin Yoo, Mark Harman, and David Clark. 2013. Fault localization prioritization:
Comparinginformation-theoreticandcoverage-basedapproaches. ACMTrans.
Softw.Eng.Methodol. 22,3(2013),19:1‚Äì19:29. https://doi.org/10.1145/2491509.
2491513
[64]StevenR.Young,DerekC.Rose,ThomasP.Karnowski,Seung-HwanLim,and
Robert M. Patton. 2015. Optimizing deep learning hyper-parameters through
anevolutionaryalgorithm.In ProceedingsoftheWorkshop onMachineLearning
in High-Performance Computing Environments, MLHPC 2015, Austin, Texas, USA,
November 15, 2015. ACM, 4:1‚Äì4:5. https://doi.org/10.1145/2834892.2834896
[65]Andreas Zeller. 2009. Why Programs Fail - A Guide to Systematic Debugging, 2nd
Edition. Academic Press.
[66]HaoZhangandW.K.Chan.2019. Apricot:AWeight-AdaptationApproachto
Fixing Deep Learning Models. In 34th IEEE/ACM International Conference on
Automated Software Engineering, ASE 2019, San Diego, CA, USA, November 11-15,
2019. IEEE, 376‚Äì387.
[67]JieMZhang,MarkHarman,LeiMa,andYangLiu.2020. Machinelearningtesting:
Survey, landscapes and horizons. IEEE Transactions on Software Engineering
(2020).
[68]Min-LingZhangandZhi-HuaZhou.2005. Ak-nearestneighborbasedalgorithm
for multi-label classification. In 2005 IEEE international conference on granular
computing, Vol. 2. IEEE, 718‚Äì721.
[69]Xiang Zhang, Xiaocong Chen, Lina Yao, Chang Ge, and Manqing Dong. 2019.Deep Neural Network Hyperparameter Optimization with Orthogonal Array
Tuning. In Neural Information Processing - 26th International Conference, ICONIP
2019,Sydney,NSW,Australia,December12-15,2019,Proceedings,PartIV (Com-
munications in Computer and Information Science), Tom Gedeon, Kok Wai Wong,
and Minho Lee (Eds.), Vol. 1142. Springer, 287‚Äì295. https://doi.org/10.1007/978-
3-030-36808-1_31
[70]XiaoyuZhang,JuanZhai,ShiqingMa,andChaoShen.2021. AUTOTRAINER:
An Automatic DNN Training Problem Detection and Repair System. In ICSE‚Äô21:
The 43nd International Conference on Software Engineering.
[71]YuhaoZhang,YifanChen,Shing-ChiCheung,YingfeiXiong,andLuZhang.2018.
AnempiricalstudyonTensorFlowprogrambugs.In Proceedingsofthe27thACM
SIGSOFTInternationalSymposiumonSoftwareTestingandAnalysis,ISSTA2018,
Frank Tip and Eric Bodden (Eds.). ACM, 129‚Äì140.
[72]BarretZophandQuocV.Le.2017.NeuralArchitectureSearchwithReinforcement
Learning. In 5th International Conference on Learning Representations, ICLR 2017,
Toulon,France,April24-26, 2017,ConferenceTrackProceedings.OpenReview.net.
https://openreview.net/forum?id=r1Ue8Hcxg
[73]Daming Zou, Jingjing Liang, Yingfei Xiong, Michael D. Ernst, and Lu Zhang.
2021. AnEmpiricalStudyofFaultLocalizationFamiliesandTheirCombinations.IEEETrans.SoftwareEng. 47,2(2021),332‚Äì347. https://doi.org/10.1109/TSE.2019.
2892102
585
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. 