Search-Based LLMs for Code Optimization
Shuzheng Gao1, Cuiyun Gao2∗, Wenchao Gu1, Michael R. Lyu1
1Department of Computer Science and Engineering, The Chinese University of Hong Kong, China
2School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China
szgao23@cse.cuhk.edu.hk, gaocuiyun@hit.edu.cn, wcgu@cse.cuhk.edu.hk, lyu@cse.cuhk.edu.hk
Abstract —The code written by developers usually suffers from
efficiency problems and contain various performance bugs. These
inefficiencies necessitate the research of automated refactoring
methods for code optimization. Early research in code opti-
mization employs rule-based methods and focuses on specific
inefficiency issues, which are labor-intensive and suffer from the
low coverage issue. Recent work regards the task as a sequence
generation problem, and resorts to deep learning (DL) techniques
such as large language models (LLMs). These methods typically
prompt LLMs to directly generate optimized code. Although
these methods show state-of-the-art performance, such one-step
generation paradigm is hard to achieve an optimal solution. First,
complex optimization methods such as combinatorial ones are
hard to be captured by LLMs. Second, the one-step generation
paradigm poses challenge in precisely infusing the knowledge
required for effective code optimization within LLMs, resulting
in under-optimized code.
To address these problems, we propose to model this task
from the search perspective, and propose a search-based LLMs
framework named SBLLM that enables iterative refinement and
discovery of improved optimization methods. SBLLM synergis-
tically integrate LLMs with evolutionary search and consists
of three key components: 1) an execution-based representative
sample selection part that evaluates the fitness of each existing
optimized code and prioritizes promising ones to pilot the
generation of improved code; 2) an adaptive optimization pattern
retrieval part that infuses targeted optimization patterns into the
model for guiding LLMs towards rectifying and progressively
enhancing their optimization methods; and 3) a genetic operator-
inspired chain-of-thought prompting part that aids LLMs in com-
bining different optimization methods and generating improved
optimization methods. Our evaluation of SBLLM on a dataset of
Python and C++ code demonstrates its effectiveness in improving
code efficiency. Specifically, the results indicate that SBLLM can
improve program execution efficiency by up to 209.59% and
consistently outperform all baseline methods by 8.75% ∼28.06%
and 1.15% ∼9.56% with different LLMs in terms of top-5
speedup rate on Python and C++, respectively.
I. I NTRODUCTION
As referred in the ISO/IEC 25010 software quality guide-
lines, the computational efficiency of the software is a critical
cornerstone of system performance and user satisfaction [1],
[2]. Inefficient code snippets can induce increased system
latency, computational resources waste, and lead to poor user
experience, which is referred to as performance bugs [3], [4].
Existing studies have demonstrated that these inefficiencies
are widely existed in software and are hard to detect and
repair [4], [5]. Consequently, the task of code optimization,
∗Corresponding author. The author is also affiliated with Peng Cheng
Laboratory.which aims to automatically refactor the code, simplify its
complexity and enhance performance metrics, has attracted
researchers’ attention in recent years.
Early research in code optimization primarily focuses on
rule-based methods, which mainly target specific types of
inefficiencies such as software misconfigurations [6] and loop
inefficiencies [3]. These methods heavily rely on pre-defined
rules created by experts, which are labor-intensive and suffer
from the low coverage problem [2], [7]. Recent advancements
in deep learning (DL) such as large language models (LLMs)
have inspired a burgeoning body of research. These techniques
learn code optimization patterns from data, broadening the
array of inefficiency types that can be addressed. For exam-
ple, RapGen [8] introduces a retrieval-augmented generation
approach with LLMs to generate the optimized code in a zero-
shot manner, surpassing the performance of previous fine-
tuned small-sized neural models [2]. Another recent work,
PIE [9] establishes a benchmark with test cases and evaluates
the performance of various prompting methods such as in-
context learning (ICL) [10] and chain of thought (COT) [11].
Despite the success of LLM-based approaches, the adopted
one-step generation paradigm tends to largely limit the per-
formance of code optimization for two main reasons. First, it
is challenging for LLMs to capture the complex optimization
methods in one attempt. Code optimization involves a range of
optimization levels [12], from incremental improvements like
removing some unnecessary computation to more substantial
optimizations that reshape the whole algorithm. Moreover,
optimization patterns are combinatorial in nature [13], [14],
implying that a single snippet may consist of multiple seg-
ments with the potential for different optimization. Second,
it is difficult to precisely integrate the essential knowledge
required for effective code optimization into LLMs. Although
some studies [8], [15] have attempted to improve LLMs
by retrieving similar code snippets, these approaches only
consider the input similarity and ignores the characteristic of
code optimization such as its combinatorial nature, which may
result in the generation of under-optimized code.
To mitigate the above challenges, we propose to model the
code optimization task from the search perspective instead
of the typical generation viewpoint. Specifically, the task of
refactoring a given code snippet to be more efficient can be
formulated as a search problem, where the objective is to find
the most efficient optimization method among a vast set of
potential methods created by extensive code transformationarXiv:2408.12159v1  [cs.SE]  22 Aug 2024  (a) Fitness  
EstimationACC: 1.0
SP: 1.2Sample 1
ACC: 1.0
SP: 1.19Sample 2
ACC: 1.0
SP: 1.12Sample 3
Current Optimized Code ASTssamples = set()
for number in numbers:
   for j in range...
        if ... F(b) itness-based 
Re-ranking
2) Adaptive Optimization 
Pattern Retrieval    
 Improved Optimized 
Code of S
3) Genetic Operator-inspired
Chain-of-thought PromptingtExisting 
Optimization
TechniquesSlow Code St
Optimized Code of St....
GO-COTseen=[]
samples=[]
for number in numbers:
   if number in seen:
data=[]
samples=[]
for i in numbers:
   if i in seen:
1) Execution-based Representative 
Sample SelectionFitness ScoresRepresentative 
Samples Similar PatternDifferent Pattern
  
based Pattern Retrieval
Training dataset
Pattern Base...
(b) Representative Sample-
   (a) Fine-grained Pattern Parsing
Input Placeholder:Reasoning Specification:Genetic Operator-
incorporated Instructions:samples = []
for i in range(len(numbers)):
    for j in range...
        if ...
Fig. 1: The overview of SBLLM.
methods and their possible combinations. In search-based
software engineering, significant efforts have been devoted
to advancing the discovery of optimal solutions in the large
search space for various tasks [16], [17]. These methods
iteratively identify limitations in existing solutions to update
their strategy to propose better ones, aiming to progressively
move towards the optimal solution [18], [19]. Considering that
a single generation by LLMs can be regarded as a one-step
exploration in the search space, the search-based approaches
can benefit LLMs by incorporating them into the iterative
refinement process.
In this work, we introduce a Search- Based LLM s for
code optimization, named SBLLM. SBLLM synergistically
combines LLMs and evolutionary search, comprising three
main components: 1) Execution-based representative sample
selection, where we leverage execution feedback to evalu-
ate the fitness of each sample and prioritize representative
ones with effective and distinct optimization methods to
pilot further optimization. 2) Adaptive optimization pattern
retrieval, where we propose an adaptive retrieval mechanism
to infuse domain knowledge in LLMs, and guide LLMs to
rectify and progressively enhance their optimization meth-
ods. 3) Genetic operator-inspired chain-of-thought (GO-COT)
prompting, where we introduce a COT prompt with crossover
and mutation operations aiding LLMs in developing improved
optimized code.
To evaluate the effectiveness of SBLLM, we conduct exper-
iments on a widely-used benchmark dataset containing both
Python and C++ code. We compare SBLLM against four
representative prompting methods on four popular open-source
and closed-source LLMs including CodeLlama [20], Gem-
ini [21], ChatGPT [22], and GPT-4 [23]. The experimental
results demonstrate the effectiveness of SBLLM in improving
code efficiency. Our approach achieves a significant boost in
program execution efficiency, surpassing the performance of
all baseline methods by 8.75% ∼28.06% and 1.15% ∼9.56%
with different LLMs in terms of top-5 speedup rate metric on
Python and C++, respectively,
We summarize our contributions as follows.
1) To the best of our knowledge, we are the first to explore
the code optimization task from a search perspective and
propose to enhance LLMs with search-based methods forthe task.
2) We propose a novel framework SBLLM for effectively
guiding LLMs towards identifying efficient optimiza-
tion methods in the vast search space, by integrating
execution-based representative sample selection, adap-
tive optimization pattern retrieval, and genetic operator-
inspired chain-of-though prompt.
3) Extensive experiments demonstrate the effectiveness of
SBLLM in improving code efficiency compared with
baseline methods across different LLMs.
II. P ROPOSED FRAMEWORK
A. Overview
Fig. 1 presents the overview of the proposed framework
SBLLM. SBLLM follows the evolutionary search paradigm
that first generates initial solutions and then iteratively selects
the fittest candidates while breeding new ones until termination
criteria are met.
To start, SBLLM acquires the initial seed optimized code of
the given slow code Stusing existing optimization techniques.
1) In the execution-based representative sample selection part,
SBLLM evaluates the fitness of current optimized code, and
selects the representative samples that contain distinct and
effective optimization methods by a re-ranking mechanism.
2) In the adaptive optimization pattern retrieval part, SBLLM
retrieves code optimization patterns from the pattern base
based on both the slow code and the selected representa-
tive samples, aiming to guide LLMs towards rectifying and
progressively enhancing their optimization methods. 3) In
the genetic operator-inspired chain-of-thought prompting part,
SBLLM constructs a prompt that leverages crossover and
mutation operations to facilitate LLMs in combining existing
optimization methods and developing improved optimized
code for St. The above procedure can be conducted multiple
iterations until it no longer yields further optimization in the
code efficiency or reaches the maximum iteration.
B. Execution-based Representative Sample Selection
To enable LLMs iteratively refine the optimized code, we
propose to evaluate the fitness of each sample and provide the
selected representative ones to LLMs. As shown in Fig. 1,
the execution-based representative sample selection module
2Algorithm 1 Sample Selection and Pattern Retrieval
Input : The slow code need to optimize st, existing predictions with their
execution information E, the number of selected representative
samples Ns, the training data T,
Output : Selected representative samples RS, retrieved patterns P
Initialize: Initialize three lists correct list, incorrect list
1Function Sample selection and pattern retrieval :
// Execution-based Representative Sample
Selection
2 E=sort (E, key=speedup rate, order=descend)
3 foralle∈Edo
4 ife.acc== 1 and Abstract( e.code) not in correct list then
5 correct list.append( e.code))
6 else if e.acc<1then
7 incorrect list.append( e.code)
8 e.dis= 0
9 end
10 end
11 iflen(correct list)< N sthen
12 forallea∈incorrect list do
13 foralleb∈incorrect list do
14 ea.dis += EditDistance(Abstract( ea), Abstract( eb))
15 end
16 end
17 incorrect list =sort (incorrect list, key=dis, order=ascend)
18 end
19 RS= (correct list +incorrect list)[:Ns]
// Adaptive Optimization Pattern Retrieval
20 input score = BM25(Abstract( st),T.sa)
21 simscore ,difscore =[0,0,0, ...,0],[0,0,0, ...,0]
22 foralle∈RSdo
23 ds, df= GetDiff(Abstract( st), Abstract( e.code))
24 score opt=1
2(BM25( ds,T.ds)+BM25( df,T.df))
25 simscore +=score opt
26 difscore += max( score opt)-score opt
27 end
28 simtemp =arg max( simscore +input score )
29 diftemp =arg max( difscore +input score )
30 P= (simtemp ,diftemp )
31return RS,P
mainly contains two steps, including (a) fitness estimation and
(b) fitness-based re-ranking.
Fitness Estimation. In order to focus LLMs’ search di-
rections towards the most efficient optimization method, we
propose to quantitatively assesses the fitness of each sample
based on its accuracy and speedup rate. SBLLM evaluates
current optimized code on a set of public test cases. In
alignment with the previous work [24], [25], two separate
test cases are used in the prediction and evaluation process,
respectively, including public test cases and private test cases.
This setup avoids the leakage of the test case information.
Fitness-based Re-ranking. Based on the collected fitness
information, SBLLM first prioritizes samples that are both
correct and have a high speedup rate. Specifically, as shown
in Algorithm 1, SBLLM sorts all the code snippets based on
their speedup rate and divides them into correct and incorrect
groups according to the accuracy (Line 2). To consider the
combinations of optimization patterns, we then propose to
select distinct samples from the correct group as representative
samples. As shown in Line 4 to Line 7, SBLLM abstracts
the correct code based on the ASTs (Abstract Syntax Trees),
and ensures that only one sample with identical abstractions
can be chosen. Considering that incorrect code can alsoprovide hints to LLMs for avoiding the same errors, we
involve incorrect code as representative samples. Specifically,
SBLLM calculates the edit distance between the abstract code,
and prioritizes the incorrect code with distance sum. The
prioritized incorrect code tails the selected correct code. The
topNssamples are retained as the selected representative
samples RS, while the remaining samples are discarded.
C. Adaptive Optimization Pattern Retrieval
The adaptive optimization pattern retrieval module aims
at providing LLMs with effective optimization patterns to
facilitate the generation of improved optimized code. The
retrieved optimization patterns are expected to provide hints to
LLMs towards generating correct and more efficient code. We
propose to involve both the input slow code stand selected
representative samples RS to adaptively retrieve effective
optimization patterns for LLMs. Specifically, the retrieval part
considers both the optimization methods that are semantically
similar to RSfor rectifying potential errors, and those different
from RSfor drawing inspiration from unexploited optimiza-
tion methods. As shown in Fig. 1, the adaptive optimization
pattern retrieval module mainly contains two steps, including
(a) fine-grained pattern parsing and (b) representative sample-
based pattern retrieval.
Fine-grained Pattern Parsing. To facilitate accurate re-
trieval of similar and different patterns, SBLLM first parses
each optimization pair in the training dataset and extracts pat-
terns with fine-grained optimization information to construct
the pattern base. Each optimization pair consists of a non-
optimized code snippet sand its optimized version f. To
preserve the general optimization information and eliminate
project-specific influences, SBLLM parses them into ASTs
and obtains their abstracted code saandfa. Then it identifies
the fine-grained optimization information with the “difflib”
package [26] by isolating the abstracted deleted statements ds
from sa, as well as the abstracted added statements dffrom
fa. Based on the above process, we obtain the pattern base
with fine-grained optimization information including sa,fa,
ds, and df, which is then used to assist the retrieval process.
Representative Sample-based Pattern Retrieval. The rep-
resentative sample-based pattern retrieval method aims at
guiding LLMs in refining the current optimized code RSin
two ways. Specifically, as shown in Fig. 2, current optimized
code may contain incorrect and insufficient optimizations. To
address these issues, we propose to adaptively retrieve separate
patterns that are semantically similar to and different from RS,
respectively.
First, as shown in Fig. 2 (a) 1, the current optimized
code intends to use the Eratosthenes Sieve algorithm to
reduce the time complexity. However, as LLMs do not learn
this optimization method well, the optimized code contains
incorrect initialization, which will lead to an out-of-index
error. The similar pattern presented in Fig. 2 (a) 2shows
a correct implementation of this algorithm that can provide
hints for LLMs to rectify the errors. However, the pattern is
hard to be retrieved based solely on its semantic similarity
3 ① Current Optimized code:
 - def isPrime(x):
 -     ...
 -     while i <= root_x:
 -         if x % i == 0:
 -             return False
 + def generatePrimes(n):
 +     sieve = [] 
 +     ...
 +     while p * p <= n:
 +         ...
 +         for i in range(p * p, n + 1, p):
 +             sieve[i] = False
  ② The Similar Pattern:
 - for VAR in range(NUM, int(VAR**NUM)+NUM):
 -     ...
 + def prime_table(NUM):
 +     ...  
 +     VAR = [BOOL] * (NUM + 1) 
 +     for NUM in range(NUM, int(VAR**NUM)+NUM):Incorrect 
initailization
Low input 
similarity
③ Refined code:
 def generatePrimes(n):
     sieve = [true] * (n+1) 
     ...
     while p * p <= n:
         ...
         for i in range(p * p, n + 1, p):
             sieve[i] = False(a) A Python example showing how the similar pattern helps
rectify errors in current optimized code.
 ① Current Optimized code:
   if (s.size()> N){
       for (int i = len; i > N; i--)
           s.pop_back();
 -     for (int j = 0; j < 3; j++)
 -         s.push_back('.');
 -     cout<<s;        
 +     cout<<s<< "...";
 ② The Different Pattern:
 - for (int VAR = VAR; VAR  < VAR ; ++VAR){
 -    ...
 -    STR.pop_back();
 -    ...
 + string STR = STR[NUM].substr(NUM, NUM);
 ③ Refined code:
 if (s.size()> N)
    cout << s.substr(0, N) << "...";Low input 
similarity
(b) A C++ example showing how the different pattern helps
find the unexploited optimization method.
Fig. 2: Examples for illustrating the two kinds of retrieved
patterns in the adaptive optimization pattern retrieval module.
to the input part due to the relatively low similarity degree.
To effectively capture similar patterns, SBLLM leverages RS
and retrieves the pattern that not only exhibits similar ab-
stracted slow code with sabut also possesses similar optimized
parts (i.e., abstracted deleted statements dsand abstracted
added statements df) to those in RS. Specifically, as shown
in Algorithm 1, SBLLM first measures the input similarity
input score between the abstracted code of stand the ab-
stracted code snippets sain the pattern base by the BM25 [27]
(Lines 20). Subsequently, for each representative sample in
RS, SBLLM extracts its abstracted deleted statements dsand
abstracted added statements df, and calculates the similarity
score score optof the optimized part with each pattern in
the pattern base (Lines 22-24). Then the score is added to the
simscore (Line 26). Finally, SBLLM integrates simscore
  
           
        
       
        
                      1. Analyze the original code and the optimizations 
applied in the existing versions Genetic Operator-incorporated Instructions:
Task Description & Instructions: You will be provided with a code
 snippet, its existing optimization versions along with their
performance, and two code transformation patterns. Please
follow the instructions step-by-step to improve code efficiency:
.
                      2. Identify any additional optimization opportunities 
    
                      3. Explain your optimization methods and provide 
a new optimized code snippetthathavenotbeenutilized.
. 
 
   
           
  
         
  
         
     
Input Placeholder:
Slow code: [Code]       
Current code: [Code1]  [Code2]  [Code3] 
Patterns: [Similar Pattern]  [Different PatternReasoning Specification:
Desired output format:
1. Analyze the original code and the optimizations applied in the  
existing versions: <answer>
2. Identify any additional optimization opportunities that have not  
been utilized: <answer>
3. Explain your optimization methods and provide a new
optimized code snippet: <optimization points>
```[lang]\n<code>\n```
]  (Mutate)Crossover
GeneratioMutation
nFig. 3: The illustration of the GO-COT prompt. Contents
in “[]” will be substituted by the corresponding data. The
complete prompt can be found in our GitHub repository [28].
withinput score and the pattern with the highest score is
selected as a similar pattern (Line 28).
Second, as illustrated in Fig. 2 (b) 1, the current op-
timized code presents insufficient optimization as it only
focuses on the optimization of the second “for” loop while
overlooking the optimization of the “pop back()” statement.
Correspondingly, the different pattern in Fig. 2 (b) 2presents
an effective optimization method by employing the “substr()”
API. However, this pattern displays low semantic similarity
to the input part, and is hard to be retrieved. To this end,
SBLLM retrieves the pattern that exhibits similar abstracted
slow code sabut employs different optimization methods
compared to those present in RS. Similarly, SBLLM also first
calculates the similarity score input score of the input part
andscore optof the optimized part based on RS. Then the
value of score optis inverted and added to difscore to
prioritize patterns exhibiting lower similarity of the optimized
part (Line 26). Ultimately, the pattern with the highest score
of the sum of difscore andinput score is selected as the
different pattern (Line 29). These two retrieved patterns Pare
then integrated into the prompt to guide the generation of new
optimized code.
D. Genetic Operator-inspired Chain-of-thought Prompting
The part aims at guiding LLMs in integrating different
optimization methods from representative samples RS and
retrieved patterns P, and subsequently generating refined
optimized code. Specifically, we propose to aid LLMs with the
evolutionary algorithm’s generic operators, and introduce the
genetic operator-inspired chain-of-thought (GO-COT) prompt.
Genetic operators [29] are inspired by biological evolution
principles, and comprise crossover and mutation to synthesize
4Algorithm 2 The evolutionary optimization process
Input : The slow code need to optimize st, maximum iteration number I
Output : Re-ranked optimized code Sol
1Function Evolutionary optimization :
2 Obtain initialization solutions Sol forst
3 foralli∈[1...I]do
4 Select representation samples RSifromSol and retrieve patterns
5 ifRSi==RSi−1andRSihave correctly optimized stthen
6 Break
7 end
8 Generate new code NC
9 Sol←RSiSNC
10 end
11 Re-rank Sol using the selection part in Algorithm 1
12return Sol
new solutions. These operators facilitate the combination of
advantageous traits, promote exploration, and generate supe-
rior solutions. Specifically, we construct the GO-COT prompt,
as depicted in Fig. 3. The prompt consists of three main com-
ponents, including genetic operator-incorporated instructions,
reasoning specification, and input placeholder.
TheGenetic Operator-incorporated Instructions compo-
nent illustrates the task requirement and outlines the instruc-
tions to be executed by LLMs. LLMs are instructed to follow
three sequential steps to generate a new code snippet. The first
two steps involve combining the advantages observed in the
selected representative samples and referring to the retrieved
patterns to identify unexploited optimization methods, which
correspond to the crossover and mutation operators in the evo-
lutionary algorithm, respectively. Based on these two genetic
operators, in the third step, LLMs are required to conclude the
optimization methods and generate a new optimized code. The
Reasoning Specification component aims to standardize the
format of the output content. LLMs are instructed to follow
the given reasoning format step by step and produce the result
accordingly. The Input Placeholder includes the code that
LLMs need to optimize along with the representative samples
RSand the retrieved patterns P.
With the instructions and provided information in the
prompt, LLMs can learn to follow the reasoning strategy and
generate a new optimized code step by step.
E. Evolutionary Optimization
Based on the aforementioned processes, SBLLM iteratively
generates improved optimized code. As shown in Algorithm 2,
at the end of each iteration, the newly generated code will then
be integrated with the representative samples in the prompt
for the next iteration generation. The iterative refinement
process continues until it reaches the convergence condition.
Specifically, if the code has been correctly optimized and the
representative samples RS remain unchanged with those in
the last iteration, we terminate the process. This is based on
the intuition that the prompt in this iteration will be the same
as the one in the last iteration and LLMs are less likely to
generate better code.III. EXPERIMENTAL SETUP
A. Research Questions
In the evaluation, we focus on the following four research
questions:
RQ1: How effective is SBLLM in improving code effi-
ciency?
RQ2: What is the fine-grained performance of code gener-
ated by SBLLM across different optimization levels?
RQ3: What are the contributions of different modules in
SBLLM?
RQ4: What is the impact of different hyper-parameters on
the performance of SBLLM?
To study RQ1, we conduct a comprehensive evaluation
of SBLLM by comparing with four representative baseline
methods across four popular LLMs, aiming to provide a
thorough assessment across language models with different
parameter sizes and capabilities. For RQ2, we delve into the
analysis of the proportion of generated code across different
accuracy and speedup rate levels, including cases where the
code is not correct, correct but not faster than slow code, faster
than slow code but not faster than human reference and faster
than human reference. Additionally, we investigate how much
code generated by SBLLM could be faster than reference code
derived by human developers. For RQ3, we remove different
parts in SBLLM to assess their individual contributions. For
RQ4, we explore the influence of various hyperparameters by
varying the number of representative samples in the prompt
and the number of maximum iterations.
B. Datasets
In this work, we evaluate SBLLM on the widely-used
PIE [9] dataset which contains two programming languages
(i.e., Python and C++). The two popular programming lan-
guages are critical for code optimization evaluation. Python is
a dynamic language known for its slow execution speed [30].
In contrast, C++ is a statically typed, compiled language
renowned for its high performance, especially when lever-
aging the O3 optimization option. By applying the proposed
optimization, we can validate whether the optimized methods
generated by SBLLM are trivial and can be achieved through
compiler optimization techniques. The PIE dataset is derived
from CodeNET [31], which is curated from an online judge
system and contains 3,474 programming problems. Here we
craft the public and private test cases for each problem by
using the input-output examples in the program description in
CodeNET and the test cases provided by AlphaCode [24],
respectively. On average, for each problem, we obtain 2.8
public test cases to obtain feedback for SBLLM and 95.9
private test cases to evaluate the correctness and efficiency of
generated code. Each entry in PIE contains a triplet (problem
id, slow code, fast code )written by the same programmer. The
Python subset of the PIE dataset comprises 36,857 training
samples, 1,940 valid samples, and 986 test samples; while the
C++ comprises 77,967 training samples, 2,544 valid samples,
and 994 test samples.
5C. Baselines
To provide a comprehensive evaluation, we experiment on
four popular LLMs and compare SBLLM with four represen-
tative prompt methods, with details as below.
For LLMs, we evaluate the performance of SBLLM on both
open-source and closed-source models, including CodeLlama,
Gemini, ChatGPT, and GPT-4. CodeLlama [20] is a family of
open-source large-scale code language models developed by
Meta. We use the 34B instruct-tuned version (i.e., CodeLlama-
34b-Instruct-hf) for experiments. ChatGPT [22] and GPT-
4 [23] are two popular LLMs developed by OpenAI which
show versatile abilities across different fields such as code
generation. They are closed-source model and we access
it through APIs (i.e., gpt-3.5-turbo-0613 and gpt-4-1106-
preview). Gemini [21] is a recent powerful closed-source LLM
developed by Google which shows comparable ability with
GPT-4. We also access it based on its official API (i.e., gemini-
pro).
As for the prompt methods, we follow previous work [9]
and involve four representative methods including instruc-
tion prompting, in-context learning (ICL), retrieval-augment
generation (RAG), and chain-of-though (COT). Instruction
prompting directly prompts LLMs to generate optimized code
without providing other information. In-context learning adds
some examples (input-output pair) before the query sam-
ple to help the model understand this task. Following prior
work [32], we randomly sample four pairs from the training
set to create the examples for ICL. As for retrieval-augment
generation (RAG) method, instead of random selection, it
retrieves different samples from the training set for different
query samples. Specifically, we employ BM25 to select the
code from the training set with the highest similarity to the
query sample. Lastly, in the chain-of-thought (COT) prompt,
we follow [8], [9] and employ prompts that instruct the LLM
to first explain how to optimize the program before producing
the optimized code. We use the same examples as ICL for
COT and manually craft the explanations to aid the LLM in
reasoning through COT. The detailed prompts for all baseline
methods are provided in our replication package [28].
D. Metrics
To evaluate the correctness and efficiency of optimized
code, we follow previous work [9] and measure the following
metrics:
Percent Optimized (OPT): OPT denotes the fraction of
code in the test set that demonstrate improvement through a
given method. A program must be at least 10% faster and
correct (i.e., pass all test cases) to contribute, i.e.,T(s)−T(o)
T(o)>
10% andA(o) = 1 , where T(·)andA(·)represent the
execution time and accuracy and oandsdenote the optimized
and slow code, respectively.
Speedup Rate (SP): SP measures the improvement in
running time. We first calculate the speedup rate of each
generated code and then report the average results on the
whole test set. If a generated code is either incorrect or slower
than the original slow code, we assign a speedup of 1.0 tothat example, as the worst-case scenario assumes the original
program has a speedup of 1.0. Formally, SP is calculated as
follows:
SP=nX
i=1T(si)
T(oi)ifA(oi) = 1∧T(oi)≤T(si)else 1 (1)
where nis the size of test set.
E. Implementation Details
For the hyperparameters of all LLMs, following the previous
work [9], we set the temperature to 0.7 for all experiments
and generate five results by random sampling. For all baseline
methods, the optimized code will be re-ranked based on the
output probability predicted by the LLMs. For SBLLM, after
the whole optimization process, we re-rank the optimized code
obtained in the last iteration using the selection method in
Algorithm 1. As for the hyper-parameters of SBLLM, we set
the number of selected representative samples Nsto three and
the maximum iteration number to four. The impact of different
numbers is discussed in Section IV-D. For GPT-4, due to the
cost limitation, we randomly sample 10% data in the test set
for experiment. Following previous work [9], we execute each
slow and generated program 25 times, and report the average
execution results excluding the first run. For the execution
environment, we execute Python programs with Python 3.9.12
and compile all C++ programs with GCC version 9.4.0 and
C++17 as well as the O3 optimization flag. All experiments
are conducted on a Linux server (64-bit Ubuntu 20.04) with
one 112-core Intel Xeon Platinum 8276 CPU@ 2.20GHz, four
NVIDIA A100-40GB GPUs, and 1TB RAM.
IV. E XPERIMENTAL RESULTS
A. RQ1: Comparison with Baselines
To assess the effectiveness of SBLLM in improving code ef-
ficiency, we compare SBLLM with four representative prompt
methods on four popular LLMs. Table I presents the perfor-
mance of SBLLM along with baseline methods on Python and
C++. For each metric, we denote the performance using the
Top@k metric, where k represents the number of generated
code snippets considered.
Comparison of the OPT metric. As shown in Table I,
SBLLM demonstrates considerable improvements over the
baseline methods across both languages and all LLMs. For
example, when compared to the strongest baseline method,
COT, SBLLM achieves an average improvement of 5.11%
and 2.87% in OPT@5 on Python and C++, respectively. These
results demonstrate the effectiveness of SBLLM in identifying
efficient optimization methods within the vast search space.
Besides, by comparing the improvements across different
LLMs, we observe that SBLLM achieves higher enhance-
ments on more powerful LLMs such as ChatGPT and GPT-
4. For instance, on Python, SBLLM enhances the OPT@1
of CodeLlama and GPT-4 by 2.37% and 9.19%, respectively.
This discrepancy can be attributed to the limited instruction-
following capability and context size of CodeLlama, which
might make it not fully comprehend the instructions and
6TABLE I: The performance of SBLLM along with the baselines on two programming languages in terms of Top-1,3,5. Under
each metric the best performance is marked as gray. “*” denotes statistical significance in comparison to the base models (i.e.,
Wilcoxon-test with p-value <0.05).
ApproachPython C++ (O3)
OPT@k SP@k OPT@k SP@k
Top-1 Top-3 Top-5 Top-1 Top-3 Top-5 Top-1 Top-3 Top-5 Top-1 Top-3 Top-5
CodeLlamaInstruction 2.94 4.87 7.00 102.50 104.66 105.71 0.34 1.14 1.71 100.72 101.22 102.32
ICL 3.25 5.17 8.42 104.48 107.08 109.31 0.80 1.14 1.60 100.62 100.97 101.23
RAG 4.97 8.32 11.76 104.11 108.20 113.07 0.00 0.11 0.68 100.14 100.31 101.05
COT 4.67 7.91 11.46 105.29 111.20 117.95 0.23 0.57 0.68 100.76 101.04 101.11
SBLLM 7.00* 11.36* 13.89* 111.60* 119.92* 126.70* 2.62* 3.08* 4.44* 102.76* 102.98* 103.47*
GeminiInstruction 8.63 13.71 16.65 115.39 125.12 128.18 1.03 1.83 2.40 103.84 105.33 105.90
ICL 7.51 11.05 12.58 114.27 122.58 125.87 1.03 1.83 2.40 104.61 105.20 107.16
RAG 7.92 12.18 14.42 112.55 120.95 125.27 1.03 2.17 3.08 101.03 103.01 103.51
COT 12.68 17.65 20.69 125.69 136.86 143.90 1.37 3.20 3.77 102.50 107.32 108.50
SBLLM 26.47* 28.09* 28.40* 153.99* 157.84* 158.22* 4.90* 6.39* 6.73* 110.78* 111.74* 112.07*
ChatGPTInstruction 4.97 7.61 9.94 104.44 106.62 109.75 0.91 2.28 4.79 101.42 102.96 108.45
ICL 9.84 13.59 17.85 115.39 120.53 127.40 1.14 2.28 4.34 104.25 105.99 110.16
RAG 12.37 17.85 22.62 116.68 121.74 128.61 1.14 2.40 4.34 104.25 105.40 110.33
COT 27.89 34.79 38.34 147.59 158.21 164.59 3.08 5.71 6.62 105.54 113.22 114.80
SBLLM 34.58* 37.32* 38.44 175.26* 179.83* 182.08* 5.93* 7.75 8.21 120.84* 121.83* 122.26*
GPT-4Instruction 21.43 29.59 32.65 122.35 126.36 127.33 4.44 8.89 11.11 108.53 119.02 123.69
ICL 36.73 39.80 41.84 173.15 175.68 181.53 5.56 6.67 11.11 118.53 119.45 121.53
RAG 31.63 34.69 36.73 161.83 165.64 170.51 6.67 6.67 14.44 132.90 134.56 145.20
COT 32.65 34.69 37.76 171.09 176.79 178.34 5.56 8.89 13.33 125.41 133.09 135.40
SBLLM 41.84 45.92* 47.96* 201.58* 206.17* 209.59* 5.49 9.89 16.48 143.49 150.80 154.76
SBLLMInstruct
COTRAG
ICL
(a) Python.
SBLLMInstruct
COTRAG
ICL (b) C++ (O3).
Fig. 4: Venn diagram of optimized code provided by SBLLM
and baseline methods on ChatGPT.
input information provided by SBLLM. In contrast, powerful
LLMs such as GPT-4 exhibit better understanding of the GO-
COT prompt, enabling them to generate superior optimization
methods step-by-step.
In addition to measuring the number of optimized code
generated by each approach, we also follow previous work
in program repair [33]–[35] and investigate the overlap of
optimized code among different methods. Fig. 4 illustrates the
unique optimized code of the top-1 predictions achieved by
SBLLM and four baseline methods on ChatGPT, represented
through Venn diagrams. Notably, SBLLM identifies 110 and
21 unique optimized code in Python and C++, respectively,while the other approaches only yield 7∼18and0∼7unique
optimized code across the two languages. This indicates that
the contribution of the iterative refinement process cannot be
replaced by the combination of existing prompting approaches.
Comparison of the SP metric. As for the speedup rate, by
analyzing the top-5 prediction results in Table I, we observe
that SBLLM can boost program execution efficiency by up to
209.59% and 154.76%, outperforming all baselines by 8.72%
∼28.06% and 1.15% ∼9.56% on Python and C++, respec-
tively. The improvement of SBLLM over baselines is even
more significant for top-1 predictions where SBLLM surpasses
COT on GPT-4 by 30.49% and 18.08% on two languages,
respectively, which can be attributed to our execution-based
representative selection method in SBLLM. These results
demonstrate that SBLLM can effectively guide the LLMs
progressively moving towards the better optimization method
and generating more efficient code.
Answer to RQ1: SBLLM successfully optimizes the most
code snippets compared to all baselines across different
LLMs. It boosts program execution efficiency by up to
209.59% and 154.76%, outperforming the top-5 speedup rate
of all baselines by 8.75% ∼28.06% and 1.15% ∼9.56%
on Python and C++, respectively,
B. RQ2: Fine-grained Performance Analysis
In this RQ, we study the fine-grained proportion of gen-
erated code across different levels of accuracy and speedup
rates. To achieve this, we classify the generated code snippets
70%20%40%60%80%
Instruction ICL RAG COT SBLLM百
NCNO NHFH
(a) The proportion of different optimization level on Python.
0%20%40%60%80%
Instruction ICL RAG COT SBLLM百
NCNO NHFH
 (b) The proportion of different optimization level on C++ (O3).
Fig. 5: The proportion of top-1 prediction based on ChatGPT with different optimization levels on Python and C++, respectively.
“NC”, “NO”, “LH”, and “FH” indicate the code is not correct, correct but not optimized, optimized but lower than human
reference and faster than human reference, respectively.
into four distinct levels: not correct (NC), correct but not
optimized (NO), optimized but not faster than the human
reference (NH), and faster than the human reference (FH).
Following the OPT definition, we consider code that is at least
10% faster than the human reference as falling into the FH
category. The experimental results are depicted in Fig. 5. Due
to space limitations, we only present the top-1 results of each
method on ChatGPT, while the results for other models can
be found in our GitHub repository [28].
As shown in Fig. 5, we can observe that SBLLM consis-
tently achieves the relatively lower NC rate and the highest
NH and FH rate compared with other methods on both
programming languages. Specifically, concerning the NC rate,
baseline methods generate at least 27.18% incorrect code for
Python. In contrast, SBLLM reduces this rate by 17.18%. As
for the FH rate, 8.82% and 1.82% of the code generated by
SBLLM outperforms the human reference on Python and C++,
respectively. This represents an obvious improvement over the
strongest baseline method named COT, which only achieves
FH rates of 5.67% and 1.14% on these two programming
languages. Furthermore, we also investigate the overlap of
FH code snippets generated by different methods. As shown
in Figure 6, we find that SBLLM achieves 42 and 9 unique
improvements on Python and C++, respectively, surpassing the
performance of other methods on both languages. These find-
ings suggest that the iterative refinement process in SBLLM
facilitates the generation of correct and efficient code solu-
tions.
Answer to RQ2: SBLLM excels in generating code with
the lowest error rate in Python and achieves the highest rate
of efficiency surpassing human-written reference across both
programming languages.
C. RQ3: Ablation Study
To investigate the individual contribution of different com-
ponents in SBLLM, we conduct an ablation study and present
the results in Table II. Due to the space limitation, we only
present the results on ChatGPT, with results for other LLMs
presented on our GitHub repository [28].
SBLLMInstruct
COTRAG
ICL(a) Python.
SBLLMInstruct
COTRAG
ICL (b) C++ (O3).
Fig. 6: Venn diagram of code faster than human reference
provided by SBLLM and baseline methods on ChatGPT.
Sample Selection. To assess the impact of the execution-
based representative sample selection component, we replace it
with a direct selection method that simply selects samples with
the highest speedup rate. As shown in Table II, removing our
selection strategy results in a noticeable performance decrease.
Specifically, the Top-1 OPT drops by 10.93% and 2.05% on
Python and C++, respectively, which demonstrates the benefits
of our sample selection criteria.
Pattern Retrieval. In this experiment, we evaluate the
effectiveness of the adaptive optimization pattern retrieval part
by removing the two patterns utilized in SBLLM respectively.
When the similar pattern is eliminated, the performance of
SBLLM suffers from an obvious decline, i.e., 9.67% on the
Top-1 speedup rate of Python, which demonstrates the impor-
tance of providing LLMs with a similar optimization pattern
to refine their optimization methods. Similarly, removing the
different patterns leads to a substantial drop of SP metric
on C++, as the dissimilarity pattern could assist LLMs in
drawing inspiration from unexploited optimization methods
and generating more efficient code.
GO-COT. To evaluate the contribution of GO-COT, we
remove its crossover and mutation-based genetic instructions
and reasoning specification part. Instead, we solely use input
placeholder to query LLMs to generate optimized code. The
results in Table II demonstrate a substantial performance
8TABLE II: Ablation Study. Under each metric the best performance is marked as gray.
ApproachPython C++ (O3)
OPT@k SP@k OPT@k SP@k
Top-1 Top-3 Top-5 Top-1 Top-3 Top-5 Top-1 Top-3 Top-5 Top-1 Top-3 Top-5
-w/o Selection 23.65 29.14 32.39 155.94 168.05 176.67 3.88 4.68 5.47 111.25 112.68 116.09
-w/o Sim Pattern 32.49 35.13 36.24 165.59 171.49 174.01 4.57 6.40 7.54 117.12 120.55 121.35
-w/o Dif Pattern 36.04 37.16 37.97 176.79 178.85 181.19 5.36 6.50 6.73 115.37 116.43 117.37
-w/o GO-COT 34.11 36.75 36.95 170.18 175.29 175.71 4.90 5.92 7.18 112.99 114.27 116.05
-w/o ALL 25.38 29.24 30.66 153.05 161.13 164.34 3.64 5.81 6.73 111.59 114.18 115.95
SBLLM 34.58 37.32 38.44 175.26 179.83 182.08 5.93 7.75 8.21 120.84 121.83 122.26
67737985
1 2 3 4 5Top-1
Top-3
Top-5Speedup Rate (%)
(a)Nson Python.
15182124
1 2 3 4 5Top-1
Top-3
Top-5Speedup Rate (%) (b)Nson C++ (O3).
69747984
1 2 3 4 5Top-1
Top-3
Top-5Speedup Rate (%)
(c) Iteration number on Python.
15182124
1 2 3 4 5Top-1
Top-3
Top-5Speedup Rate (%) (d) Iteration number on C++ (O3).
Fig. 7: Parameter analysis.
decrease of 5.08% and 7.85% on the Top-1 speedup metric
for Python and C++, respectively. This indicates that GO-COT
is effective in aiding LLMs to combine different optimization
methods and generate improved optimized code.
All. In this experiment, we remove all the aforementioned
components to evaluate the performance of a direct combina-
tion of LLMs and evolutionary search. From table II, we can
find that removing all of the above components lead to worse
results, e.g., a drop of 7.78% and 1.48% on the Top-5 OPT
on Python and C++, respectively. This suggests that simply
combining LLMs with evolutionary search or increasing the
generation number can not yield promising results.
Answer to RQ3: All components in SBLLM contribute to
the performance. Removing the execution-based representa-
tive sample selection, adaptive optimization pattern retrieval,
or GO-COT leads to substantial performance decreases.
D. RQ4: Parameter Analysis
In this section, we investigate the influence of two pa-
rameters, namely the number of selected examples ( Ns) and
the maximum number of iterations, on the performance of
SBLLM. Similar to Section IV-C, the results presented in this
Optimal OptimizationSearch 
Space
Directly-generated 
optimization methods  
1st IterationSearch 
Space
2nd Iteration3rd Iteration
(a) Existing work directly generates 
optimization methods and stop here.(b) SBLLM effectively searches towards 
the optimal optimization method.Fig. 8: An illustration of the difference of existing work and
SBLLM from the search perspective.
section are also based on ChatGPT and the remaining results
can be found in the GitHub repository [28].
Number of selected representative samples. We study
the effect of Nsby varying it from 1 to 5. As depicted in
Fig. 7 (a) and (b) for both Python and C++, SBLLM exhibits
optimal performance when Nsis set to 3. Choosing larger or
smaller values does not yield improved results. We suggest
that it is because insufficient selected samples may result in
inadequate information, while an excessive number of samples
may introduce redundancy, which may hinder the model’s
ability to effectively utilize the provided information [36].
Number of iterations. We evaluate the performance of
SBLLM across different iterations by setting the maximum
iteration to five. The corresponding results are presented in
Fig. 7 (c) and (d). From the results, we can observe that
SBLLM demonstrates increasingly better performance with
each iteration on Python; while on C++ it achieves the peak
at the fourth iteration since excessively large iterations may
introduce the risk of overfiting on public test cases. Conse-
quently, we set the maximum iteration as four.
Answer to RQ4: SBLLM achieves the best performance
with three representative samples. For the number of it-
erations, the performance of SBLLM improves with more
iterations initially, but excessively large iteration numbers
may cause performance degradation.
V. D ISCUSSION
A. What makes SBLLM work?
SBLLM synergistically integrates evolutionary search
into LLMs. The first advantage of SBLLM is its novel
9① Optimized code generated by COT:
cnt = 0
while not n&1:
    n >>= 1
    cnt += 1
② Optimized code generated by SBLLM:
cnt = (n & -n).bit_length() - 1
③ Reference optimized  code:
for _ in range(32): # A: np.ndarray
    A = A[A%2 == 0]
    answer += len(A)
    A >>= 1time:72ms
time:330mstime:337msFig. 9: A case showing that SBLLM can optimize codes with
efficient APIs and surpass human reference.
① Slow code:
...
int A2=A;
for(int i=0;i<1e7;++i){
  B2[A2%B]=true;
  A2+=A;
}
...② Optimized code 1 in iteration 2:
...
int A2 = A % B;
for (int i=0;i<B;++i){
  B2[A2] = true;
  A2 = (A2 + A) % B;
}
...
③ Optimized code 2 in iteration 2:
...
int A2 = A % B;
for(int i=0;i<1e7;++i){
  B2[A2%B] = true;
  if (A2%B==C) {
    ...
    return 0;
  }
  A2+=A;
}
...④ Optimized code 3 in iteration 3:
...
int A2 = A % B;
for(int i=0;i<B;++i){
  B2[A2] = true;
  if (A2 == C) {
    ...
    return 0;
  }
  A2 = (A2 + A) % B;
}
...
Fig. 10: A case showing that LLMs can follow the crossover
instruction in GO-COT and combine different optimization
methods. The complete case can be found in our GitHub
repository.
paradigm which benefits LLMs by incorporating them into the
iterative refinement process. As shown in Fig. 8 (a), previous
LLM-based work directly generates the optimized code. How-
ever, due to the complex optimization methods, they are hard
to directly yield the optimal solution within such an expansive
search space. In contrast, SBLLM integrates the search-based
method which enables selecting effective optimization meth-
ods and combining them into improved ones. As depicted in
Fig. 8 (b), based on this iterative refinement process, SBLLM
guides LLMs towards the optimal optimization method step-
by-step.
SBLLM could provide LLMs with effective optimiza-
tion patterns. The second advantage of SBLLM lies in
its provided effective optimization patterns. LLMs heavily
rely on general knowledge acquired during the pre-training
phase. Therefore, without explicit external hints, it is hard for
them to identify errors in existing optimized code snippets
or discover unexploited optimization methods [37]. SBLLM
assists LLMs by providing the optimization patterns as hints,
thereby guiding them in generating correct and more efficient
code. As shown in Fig. 9, the COT prompt with ChatGPT fails
to achieve optimal optimization for the slow code due to their
limited code optimization knowledge. SBLLM achieves better
performance by leveraging the retrieved pattern that includes
the API “bit length()”, which enables it to even outperform
① Slow code:
...
for(int i = 0;i<M;i++){
  cin >> p[i] >> S[i];
  if(S[i] == "AC")is_ac[p[i]] = 1;}
for(int i = 0;i<M;i++){
  if(S[i]== "AC"&&!ac_check[p[i]]){
    ac_check[p[i]] = 1;ac++;}
  if(S[i]== "WA"&&!ac_check[p[i]]&&is_ac[p[i]]){
    wa++;}
}
...
② Optimized code 1 in iteration 2:
...
unordered_map<int, pair<bool, bool>> problems;
for (int i = 0; i < M; i++) {
   cin >> p >> S;
  if (S == "AC") {
    problems[p].first = true;ac++;
    if (problems[p].second) wa++;
  } else if (S == "WA") {
    problems[p].second = true;
  }
}
...
③ Optimized code 2 in iteration 3:
...
unordered_map<int, pair<bool, bool>> problems;
for (int i = 0; i < M; i++) {
  cin >> p >> S;
  if (problems[p].first) continue;
  if (S == "AC") {
    problems[p].first = true;ac++;
    if (problems[p].second) wa++;
  } else if (S == "WA") {
    problems[p].second = true;
  }
}
...Fig. 11: A case showing that GO-COT can follow the mutation
instruction in GO-COT and achieve further improvement. The
complete case can be found in our GitHub repository.
the reference code derived by human developers.
SBLLM could aid LLMs in generating improved op-
timized code. Another advantage of SBLLM is the GO-
COT which guides LLMs in effectively leveraging existing
optimized code to generate improved ones. As shown in
Fig. 10 1, the original code repeatedly adds A and takes the
modulus B. Although the optimized code snippets generated
by LLMs may not directly achieve the optimal optimization
method, they may contain various optimization techniques
that, when combined, lead to more efficient code. As il-
lustrated in Fig. 10 2and 3, these two code snippets
improve efficiency by reducing the number of iterations and
incorporating an early termination check, respectively. In the
next iteration, SBLLM follows the crossover instruction in
the GO-COT prompt and combines them to achieve better
performance, as depicted in Fig. 10 4. Additionally, as
shown in Fig. 11 1, the slow code reads competition problem
submissions, tracks accepted (AC) and wrong attempt (WA)
statuses using arrays and outputs the counts of AC and WA.
In the first optimized version, this code replaces arrays with
anunordered map to efficiently track AC and WA statuses
and updates counts during input processing. Then, as shown
in Fig. 11 3, SBLLM follows the mutation instruction in the
GO-COT prompt and further optimizes the code in the next
10iteration by skipping the processing for problems that have
already been accepted.
B. Threats to Validity
We identify three main threats to the validity of our study:
1)The selection of languages. In this paper, we conduct
experiments on the PIE dataset containing two widely-used
programming languages Python and C++. While there are
other datasets that contain different programming languages,
such as the C# dataset in DeepDev-PERF [2], we don’t use this
dataset since it does not contain test cases. In the future, we
will create test cases for this dataset and conduct experiments.
2)The selection of LLMs. Another threat is the baselines
we utilized in our evaluation. We evaluate SBLLM on four
popular and representative LLMs and prompting methods.
While there are other existing LLMs [38], [39], our proposed
SBLLM is model-agnostic and can be easily generalized to
different LLMs. Furthermore, our method focuses on how to
further boost the initial results directly generated by LLMs.
Therefore, our research is orthogonal to the work that solely
generates code such as fine-tuning and one-step prompting
methods. In future work, we plan to further evaluate the
effectiveness of SBLLM on other LLMs.
3)Potential data leakage. In this paper, we conduct ex-
periments utilizing the APIs of ChatGPT, GPT-4, and Gemini.
However, as these models are closed-source, their training data
are not publicly accessible, giving rise to concerns regarding
potential data leakage. However, our experiments reveal that
directly prompting the LLMs to optimize the code cannot yield
promising results. Therefore, we believe that the optimization
code generated by SBLLM is not simply from memorizing the
training data.
VI. R ELATED WORK
A. Code Optimization
Early research in code optimization techniques tends to
employ rule-based methods and focus on specific inefficiency
types such as software misconfigurations and loop inefficien-
ciess [3], [6], [40], [41]. Recently, deep learning-based meth-
ods are introduced and achieve promising results. DeepDev-
PERF [2] is a pre-trained model that suggests performance
improvements in C# code. Chen et al. [42] introduce a varia-
tional auto-encoder that identifies effective code edits for per-
formance. RAPGen [8] leverages OpenAI Codex [39] to fix C#
code inefficiencies issue in zero-shot. It surpasses DeepDev-
PERF in precision without extra training. Supersonic [7]
develops a seq2seq model for code optimization using diff-
based output representation. PIE [9] is a recent benchmarks
that explore using LLMs for improving code performance.
It evaluates various prompting methods and shows that these
methods can significantly speed up code execution. Different
from the above work that typically focuses on directly gener-
ating optimized code, SBLLM aims at iterative refining and
boosting the initial results directly generated by LLMs in a
search-based manner.B. Large Language Models for Software Engineering
Recently, the advent of LLMs has significantly advanced
various software engineering tasks [38], [43], [44]. A lot of
research is dedicated to effectively harnessing the capability
of LLMs by fine-tuning or prompt engineering for software
engineering tasks [45]–[47]. For example, WizardCoder [48]
fine-tunes LLMs with complex instructions for code genera-
tion. Xia et al. [34] leverage LLMs for program repair by using
the cloze-stype prediction. TypeGEN [49] prompts LLMs with
static analysis results and COT prompts for type inference.
CHATRepair [50] iteratively evaluates programs on test cases
and feeds the error messages to LLMs for further patch
generation. Self-edit [51] utilizes compiler error messages to
enhance the correctness of code generation.
C. SBSE and Large Language Models
CodaMOSA [52] leverages LLMs to provide example test
cases for under-covered functions when search-based testing
hits a coverage stall. Tawosi et al. [53] use available search-
based methods to optimize the number and combination of
few-shot examples for LLMs in the story point estimation task.
Brownlee et al. [54] introduce a method that employs LLMs as
mutation operators for genetic improvement. Similarly, Kang
and Yoo [55] propose to leverage the capabilities of LLMs
in code comprehension and generation for creating objective-
tailored mutants. Dakhama et al. [56] improve search-based
fuzzing by using ChatGPT to parameterise C programs for
gem5 testing.
VII. C ONCLUSION AND FUTURE WORK
In this paper, we propose SBLLM, a search-based LLMs
framework for code optimization. SBLLM synergistically in-
tegrates LLMs with evolutionary search, encompassing three
components: an execution-based representative sample selec-
tion mechanism, an adaptive optimization pattern retrieval
method, and a genetic operator-inspired chain-of-thought
prompting method. Extensive experiments on four popular
LLMs show that SBLLM can effectively guide LLMs towards
identifying efficient optimization methods in the vast search
space. In the future, we plan to apply our search-based LLMs
framework to other tasks in software engineering such as
program repair. Our source code and detailed experimental
results are available at [28].
REFERENCES
[1] ISO/IEC25010, “Systems and software engineering – systems and
software quality requirements and evaluation (square),” 2011.
[2] S. Garg, R. Z. Moghaddam, C. B. Clement, N. Sundaresan, and C. Wu,
“Deepdev-perf: a deep learning-based approach for improving software
performance,” in Proceedings of the 30th ACM Joint European Software
Engineering Conference and Symposium on the Foundations of Software
Engineering, ESEC/FSE 2022, Singapore, Singapore, November 14-18,
2022 . ACM, 2022, pp. 948–958.
[3] A. Nistor, T. Jiang, and L. Tan, “Discovering, reporting, and fixing
performance bugs,” in Proceedings of the 10th Working Conference on
Mining Software Repositories, MSR ’13, San Francisco, CA, USA, May
18-19, 2013 . IEEE Computer Society, 2013, pp. 237–246.
11[4] M. Jovic, A. Adamoli, and M. Hauswirth, “Catch me if you can:
performance bug detection in the wild,” in Proceedings of the 26th
Annual ACM SIGPLAN Conference on Object-Oriented Programming,
Systems, Languages, and Applications, OOPSLA 2011, part of SPLASH
2011, Portland, OR, USA, October 22 - 27, 2011 . ACM, 2011, pp.
155–170.
[5] D. J. Dean, H. Nguyen, X. Gu, H. Zhang, J. Rhee, N. Arora, and
G. Jiang, “Perfscope: Practical online server performance bug inference
in production cloud computing infrastructures,” in Proceedings of the
ACM Symposium on Cloud Computing, Seattle, WA, USA, November
3-5, 2014 . ACM, 2014, pp. 8:1–8:13.
[6] R. Krishna, M. S. Iqbal, M. A. Javidian, B. Ray, and P. Jamshidi, “Cadet:
Debugging and fixing misconfigurations using counterfactual reasoning,”
arXiv preprint arXiv:2010.06061 , 2020.
[7] Z. Chen, S. Fang, and M. Monperrus, “Supersonic: Learning to generate
source code optimizations in C/C++,” CoRR , vol. abs/2309.14846, 2023.
[8] S. Garg, R. Z. Moghaddam, and N. Sundaresan, “Rapgen: An approach
for fixing code inefficiencies in zero-shot,” CoRR , vol. abs/2306.17077,
2023.
[9] A. Madaan, A. Shypula, U. Alon, M. Hashemi, P. Ranganathan, Y . Yang,
G. Neubig, and A. Yazdanbakhsh, “Learning performance-improving
code edits,” CoRR , vol. abs/2302.07867, 2023.
[10] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al. , “Language models
are few-shot learners,” in Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Information Processing
Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020.
[11] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. H. Chi, Q. Le,
and D. Zhou, “Chain-of-thought prompting elicits reasoning in large
language models,” in NeurIPS , 2022.
[12] M. Du, A. T. Luu, B. Ji, and S. Ng, “Mercury: An efficiency benchmark
for LLM code synthesis,” CoRR , vol. abs/2402.07844, 2024.
[13] A. Ouni, M. Kessentini, M. ´O. Cinn ´eide, H. A. Sahraoui, K. Deb,
and K. Inoue, “MORE: A multi-objective refactoring recommendation
approach to introducing design patterns and fixing code smells,” J. Softw.
Evol. Process. , vol. 29, no. 5, 2017.
[14] A. Ghannem, G. El-Boussaidi, and M. Kessentini, “Model refactoring
using examples: a search-based approach,” J. Softw. Evol. Process. ,
vol. 26, no. 7, pp. 692–713, 2014.
[15] G. Izacard, P. S. H. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick,
J. Dwivedi-Yu, A. Joulin, S. Riedel, and E. Grave, “Atlas: Few-shot
learning with retrieval augmented language models,” J. Mach. Learn.
Res., vol. 24, pp. 251:1–251:43, 2023.
[16] M. Harman, S. A. Mansouri, and Y . Zhang, “Search-based software
engineering: Trends, techniques and applications,” ACM Comput. Surv. ,
vol. 45, no. 1, pp. 11:1–11:61, 2012.
[17] M. Harman and B. F. Jones, “Search-based software engineering,” Inf.
Softw. Technol. , vol. 43, no. 14, pp. 833–839, 2001.
[18] P. Garg, C. L ¨oding, P. Madhusudan, and D. Neider, “ICE: A robust
framework for learning invariants,” in Computer Aided Verification - 26th
International Conference, CAV 2014, Held as Part of the Vienna Summer
of Logic, VSL 2014, Vienna, Austria, July 18-22, 2014. Proceedings , ser.
Lecture Notes in Computer Science, vol. 8559. Springer, 2014, pp. 69–
87.
[19] C. L. Goues, T. Nguyen, S. Forrest, and W. Weimer, “Genprog: A
generic method for automatic software repair,” IEEE Trans. Software
Eng., vol. 38, no. 1, pp. 54–72, 2012.
[20] B. Rozi `ere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y . Adi,
J. Liu, T. Remez, J. Rapin, A. Kozhevnikov, I. Evtimov, J. Bitton,
M. Bhatt, C. Canton-Ferrer, A. Grattafiori, W. Xiong, A. D ´efossez,
J. Copet, F. Azhar, H. Touvron, L. Martin, N. Usunier, T. Scialom, and
G. Synnaeve, “Code llama: Open foundation models for code,” CoRR ,
vol. abs/2308.12950, 2023.
[21] Gemini, “Gemini,” https://www.gemini.com, 2023.
[22] ChatGPT, “Chatgpt,” https://chat.openai.com/, 2022.
[23] OpenAI, “GPT-4 technical report,” CoRR , vol. abs/2303.08774, 2023.
[24] Y . Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond,
T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago et al. , “Competition-
level code generation with alphacode,” Science , vol. 378, no. 6624, pp.
1092–1097, 2022.
[25] S. Zhang, Z. Chen, Y . Shen, M. Ding, J. B. Tenenbaum, and C. Gan,
“Planning with large language models for code generation,” in The
Eleventh International Conference on Learning Representations, ICLR
2023, Kigali, Rwanda, May 1-5, 2023 . OpenReview.net, 2023.[26] “Lib/difflib.py,” https://https://github.com/python/cpython/blob/3.12/Lib/
difflib.py, 2022.
[27] S. Robertson, H. Zaragoza et al. , “The probabilistic relevance frame-
work: Bm25 and beyond,” Foundations and Trends® in Information
Retrieval , vol. 3, no. 4, pp. 333–389, 2009.
[28] SBLLM, “Replication package of sbllm,” https://github.com/
shuzhenggao/sbllm, 2024.
[29] S. Sivanandam, S. Deepa, S. Sivanandam, and S. Deepa, Genetic
algorithms . Springer, 2008.
[30] E. D. Berger, S. Stern, and J. A. Pizzorno, “Triangulating python
performance issues with SCALENE,” in 17th USENIX Symposium on
Operating Systems Design and Implementation, OSDI 2023, Boston,
MA, USA, July 10-12, 2023 . USENIX Association, 2023, pp. 51–64.
[31] R. Puri, D. S. Kung, G. Janssen, W. Zhang, G. Domeniconi, V . Zolotov,
J. Dolby, J. Chen, M. R. Choudhury, L. Decker, V . Thost, L. Buratti,
S. Pujar, S. Ramji, U. Finkler, S. Malaika, and F. Reiss, “Codenet: A
large-scale AI for code dataset for learning a diversity of coding tasks,”
inProceedings of the Neural Information Processing Systems Track on
Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021,
December 2021, virtual , 2021.
[32] S. Gao, X. Wen, C. Gao, W. Wang, H. Zhang, and M. R. Lyu,
“What makes good in-context demonstrations for code intelligence tasks
with llms?” in 38th IEEE/ACM International Conference on Automated
Software Engineering, ASE 2023, Luxembourg, September 11-15, 2023 .
IEEE, 2023, pp. 761–773.
[33] Q. Zhu, Z. Sun, Y . Xiao, W. Zhang, K. Yuan, Y . Xiong, and L. Zhang,
“A syntax-guided edit decoder for neural program repair,” in ESEC/FSE
’21: 29th ACM Joint European Software Engineering Conference and
Symposium on the Foundations of Software Engineering, Athens, Greece,
August 23-28, 2021 , D. Spinellis, G. Gousios, M. Chechik, and M. D.
Penta, Eds. ACM, 2021, pp. 341–353.
[34] C. S. Xia, Y . Wei, and L. Zhang, “Automated program repair in the era
of large pre-trained language models,” in 45th IEEE/ACM International
Conference on Software Engineering, ICSE 2023, Melbourne, Australia,
May 14-20, 2023 . IEEE, 2023, pp. 1482–1494.
[35] Y . Peng, S. Gao, C. Gao, Y . Huo, and M. R. Lyu, “Domain knowledge
matters: Improving prompts with fix templates for repairing python type
errors,” in Proceedings of the 46th IEEE/ACM International Conference
on Software Engineering, ICSE 2024, Lisbon, Portugal, April 14-20,
2024 . ACM, 2024, pp. 4:1–4:13.
[36] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and
P. Liang, “Lost in the middle: How language models use long contexts,”
CoRR , vol. abs/2307.03172, 2023.
[37] J. Huang, X. Chen, S. Mishra, H. S. Zheng, A. W. Yu, X. Song, and
D. Zhou, “Large language models cannot self-correct reasoning yet,”
CoRR , vol. abs/2310.01798, 2023.
[38] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y . Zhou, S. Savarese,
and C. Xiong, “Codegen: An open large language model for code with
multi-turn program synthesis,” arXiv preprint arXiv:2203.13474 , 2022.
[39] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan,
H. Edwards, Y . Burda, N. Joseph, G. Brockman et al. , “Evaluating large
language models trained on code,” CoRR , vol. abs/2107.03374, 2021.
[40] L. D. Toffola, M. Pradel, and T. R. Gross, “Performance problems
you can fix: a dynamic analysis of memoization opportunities,” in
Proceedings of the 2015 ACM SIGPLAN International Conference on
Object-Oriented Programming, Systems, Languages, and Applications,
OOPSLA 2015, part of SPLASH 2015, Pittsburgh, PA, USA, October
25-30, 2015 . ACM, 2015, pp. 607–622.
[41] R. Giavrimis, A. Butler, C. C. Petrescu, M. Basios, and S. K. Dash,
“Genetic optimisation of C++ applications,” in 36th IEEE/ACM Inter-
national Conference on Automated Software Engineering, ASE 2021,
Melbourne, Australia, November 15-19, 2021 . IEEE, 2021, pp. 1180–
1182.
[42] B. Chen, D. Tarlow, K. Swersky, M. Maas, P. A. Heiber, A. Naik,
M. Hashemi, and P. Ranganathan, “Learning to improve code efficiency,”
CoRR , vol. abs/2208.05297, 2022.
[43] R. Li, L. B. Allal, Y . Zi, N. Muennighoff, D. Kocetkov, C. Mou,
M. Marone, C. Akiki, J. Li, J. Chim, Q. Liu, E. Zheltonozhskii, T. Y .
Zhuo, T. Wang, O. Dehaene, M. Davaadorj, J. Lamy-Poirier, J. Monteiro,
O. Shliazhko, N. Gontier, N. Meade, A. Zebaze, M. Yee, L. K. Umapathi,
J. Zhu, B. Lipkin, M. Oblokulov, Z. Wang, R. M. V , J. Stillerman,
S. S. Patel, D. Abulkhanov, M. Zocca, M. Dey, Z. Zhang, N. Moustafa-
Fahmy, U. Bhattacharyya, W. Yu, S. Singh, S. Luccioni, P. Villegas,
M. Kunakov, F. Zhdanov, M. Romero, T. Lee, N. Timor, J. Ding,
12C. Schlesinger, H. Schoelkopf, J. Ebert, T. Dao, M. Mishra, A. Gu,
J. Robinson, C. J. Anderson, B. Dolan-Gavitt, D. Contractor, S. Reddy,
D. Fried, D. Bahdanau, Y . Jernite, C. M. Ferrandis, S. Hughes, T. Wolf,
A. Guha, L. von Werra, and H. de Vries, “Starcoder: may the source be
with you!” CoRR , vol. abs/2305.06161, 2023.
[44] S. Gao, C. Gao, Y . He, J. Zeng, L. Nie, X. Xia, and M. R. Lyu,
“Code structure-guided transformer for source code summarization,”
ACM Trans. Softw. Eng. Methodol. , vol. 32, no. 1, pp. 23:1–23:32, 2023.
[45] D. Shrivastava, H. Larochelle, and D. Tarlow, “Repository-level prompt
generation for large language models of code,” in International Con-
ference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu,
Hawaii, USA , ser. Proceedings of Machine Learning Research, vol. 202.
PMLR, 2023, pp. 31 693–31 715.
[46] S. Gao, W. Mao, C. Gao, L. Li, X. Hu, X. Xia, and M. R. Lyu,
“Learning in the wild: Towards leveraging unlabeled data for effectively
tuning pre-trained code models,” in Proceedings of the 46th IEEE/ACM
International Conference on Software Engineering, ICSE 2024, Lisbon,
Portugal, April 14-20, 2024 . ACM, 2024, pp. 80:1–80:13.
[47] H. Ye and M. Monperrus, “ITER: iterative neural repair for multi-
location patches,” in Proceedings of the 46th IEEE/ACM International
Conference on Software Engineering, ICSE 2024, Lisbon, Portugal,
April 14-20, 2024 . ACM, 2024, pp. 10:1–10:13.
[48] Z. Luo, C. Xu, P. Zhao, Q. Sun, X. Geng, W. Hu, C. Tao, J. Ma, Q. Lin,
and D. Jiang, “Wizardcoder: Empowering code large language models
with evol-instruct,” CoRR , vol. abs/2306.08568, 2023.
[49] Y . Peng, C. Wang, W. Wang, C. Gao, and M. R. Lyu, “Generative type
inference for python,” in 38th IEEE/ACM International Conference on
Automated Software Engineering, ASE 2023, Luxembourg, September
11-15, 2023 . IEEE, 2023, pp. 988–999.
[50] C. S. Xia and L. Zhang, “Keep the conversation going: Fixing 162 out
of 337 bugs for $0.42 each using chatgpt,” CoRR , vol. abs/2304.00385,
2023.
[51] K. Zhang, Z. Li, J. Li, G. Li, and Z. Jin, “Self-edit: Fault-aware code
editor for code generation,” in Proceedings of the 61st Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long
Papers), ACL 2023, Toronto, Canada, July 9-14, 2023 . Association
for Computational Linguistics, 2023, pp. 769–787.
[52] C. Lemieux, J. P. Inala, S. K. Lahiri, and S. Sen, “Codamosa: Escaping
coverage plateaus in test generation with pre-trained large language
models,” in 45th IEEE/ACM International Conference on Software En-
gineering, ICSE 2023, Melbourne, Australia, May 14-20, 2023 . IEEE,
2023, pp. 919–931.
[53] V . Tawosi, S. Alamir, and X. Liu, “Search-based optimisation of LLM
learning shots for story point estimation,” in Search-Based Software
Engineering - 15th International Symposium, SSBSE 2023, San Fran-
cisco, CA, USA, December 8, 2023, Proceedings , ser. Lecture Notes in
Computer Science, vol. 14415. Springer, 2023, pp. 123–129.
[54] A. E. I. Brownlee, J. Callan, K. Even-Mendoza, A. Geiger, C. Hanna,
J. Petke, F. Sarro, and D. Sobania, “Enhancing genetic improvement
mutations using large language models,” in Search-Based Software
Engineering - 15th International Symposium, SSBSE 2023, San Fran-
cisco, CA, USA, December 8, 2023, Proceedings , ser. Lecture Notes in
Computer Science, vol. 14415. Springer, 2023, pp. 153–159.
[55] S. Kang and S. Yoo, “Towards objective-tailored genetic improvement
through large language models,” in IEEE/ACM International Workshop
on Genetic Improvement, GI@ICSE 2023, Melbourne, Australia, May
20, 2023 . IEEE, 2023, pp. 19–20.
[56] A. Dakhama, K. Even-Mendoza, W. B. Langdon, H. D. Men ´endez,
and J. Petke, “Searchgem5: Towards reliable gem5 with search based
software testing and large language models,” in Search-Based Software
Engineering - 15th International Symposium, SSBSE 2023, San Fran-
cisco, CA, USA, December 8, 2023, Proceedings , ser. Lecture Notes in
Computer Science, vol. 14415. Springer, 2023, pp. 160–166.
13