DENAS: Automated Rule Generation by Knowledge Extraction
from Neural Networks
Simin Chen
simin.chen@UTDallas.edu
UT Dallas
Dallas, USASoroush Bateni
soroush@utdallas.edu
UT Dallas
Dallas, USASampath Grandhi
sxg180072@utdallas.edu
UT Dallas
Dallas, USA
Xiaodi Li
xiaodi.li@utdallas.edu
UT Dallas
Dallas, USACong Liu
cong@utdallas.edu
UT Dallas
Dallas, USAWei Yang
wei.yang@utdallas.edu
UT Dallas
Dallas, USA
ABSTRACT
Deep neural networks (DNNs) have been widely applied in the
software development process to automatically learn patterns from
massive data. However, many applications still make decisions
based on rules that are manually crafted and veri￿ed by domain
experts due to safety or security concerns. In this paper, we aim to
close the gap between DNNs and rule-based systems by automating
the rule generation process via extracting knowledge from well-
trained DNNs. Existing techniques with similar purposes either
rely on speci￿c DNNs input instances or use inherently unstable
random sampling of the input space. Therefore, these approaches
either limit the exploration area to a local decision-space of the
DNNs or fail to converge to a consistent set of rules. The resulting
rules thus lack representativeness andstability .
In this paper, we address the two aforementioned shortcomings
by discovering a global property of the DNNs and use it to remodel
the DNNs decision-boundary. We name this property as the ac-
tivation probability , and show that this property is stable. With
this insight, we propose an approach named DENAS including a
novel rule-generation algorithm. Our proposed algorithm approx-
imates the non-linear decision boundary of DNNs by iteratively
superimposing a linearized optimization function.
We evaluate the representativeness ,stability and accuracy of
DENAS against ￿ve state-of-the-art techniques ( LEMNA ,Gradient ,
IG,DeepTaylor , and DTExtract ) on three software engineering
and security applications: Binary analysis, PDF malware detection,
and Android malware detection. Our results show that DENAS can
generate more representative rules consistently in a more stable
manner over other approaches. We further o￿er case studies that
demonstrate the applications of DENAS such as debugging faults
in the DNNs and generating signatures that can detect zero-day
malware.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for pro￿t or commercial advantage and that copies bear this notice and the full citation
on the ￿rst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speci￿c permission
and/or a fee. Request permissions from permissions@acm.org.
ESEC/FSE ’20, November 8–13, 2020, Virtual Event, USA
©2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-7043-1/20/11. . . $15.00
https://doi.org/10.1145/3368089.3409733CCS CONCEPTS
•Software and its engineering !Software notations and
tools ;•Computing methodologies !Machine learning .
KEYWORDS
Machine Learning, Explainable AI, Deep Neural Networks
ACM Reference Format:
Simin Chen, Soroush Bateni, Sampath Grandhi, Xiaodi Li, Cong Liu, and Wei
Yang. 2020. DENAS: Automated Rule Generation by Knowledge Extraction
from Neural Networks. In Proceedings of the 28th ACM Joint European Soft-
ware Engineering Conference and Symposium on the Foundations of Software
Engineering (ESEC/FSE ’20), November 8–13, 2020, Virtual Event, USA. ACM,
New York, NY, USA, 13pages. https://doi.org/10.1145/3368089.3409733
1 INTRODUCTION
Deep Neural Networks (DNNs) have shown potential in many soft-
ware engineering applications such as binary code analysis [ 57,
63,78], malware classi￿cation [ 5,35,42,43], and automatic test-
ing [ 33,39,49,79]. However, DNNs-based methods are not inter-
pretable in nature and inherently lack robustness [ 25,30,44]. Due
to this concern, most safety-critical applications such as aircraft
￿ight control systems [ 1] and anti-lock braking systems [ 58] still
adopt a rule-based design for decision-making. Rule-based systems
are believed to be more trustworthy because rules can be inspected
by human domain experts and ideally perform expected and pre-
dictable operations in the system. However, inferring rules manu-
ally from the massive data would require an unbearable analysis
time on top of the necessary professional knowledge.
DENAS
cmp    %rcx,(%r12)->0.cmp    %rcx,(%r14,%rdx,8)->0.cmp    %rcx,(%rdx)->0.cmp    %rcx,(%rsp)->0.
Verification
Rule-based System
Updated Knowledge
Learning
Model Improvement
RulesTraditional Rule-Inference WorkflowAutomated Rule-Inference Workflow Using DENAS
ML Model
Large Dataset
Human Analysis
Figure 1: The rule-inference work￿ow using DENAS .
813ESEC/FSE ’20, November 8–13, 2020, Virtual Event, USA Simin Chen, Soroush Bateni, Sampath Grandhi, Xiaodi Li, Cong Liu, and Wei Yang
In this paper, we propose DENAS (shown in Figure 1), to combine
the better parts of DNNs-based and rule-based approaches together.
Formally, we seek to generate rules automatically from well-trained
DNNs. Rules, in this case, would be interpretable functions mapping
certain features of the input of the DNNs to the expected output.
Recent literature [ 7,8,26,45,56] in interpretable machine learn-
ing (ML) has addressed this problem indirectly, but falls short of
providing su￿ciently accurate rules that describe the DNNs. We
divide the shortcomings of the existing works into two categories.
The ￿rst class of existing approaches (local explanation ap-
proaches) relies on a set of speci￿c input instances to generate
rules. In these approaches [ 8,26,36,45,56], input instances are
sequentially fed to the DNNs one by one, and the output is observed,
and a rule is generated that maps that speci￿c input to the observed
output. For example, LEMNA [26] can be used to generate a rule by
identifying the critical features for one given input instance. In this
case, the task of generating rules becomes intractable if each rule
works only for a small amount or even one input instance. Moreover,
the inherent limitation of these approaches cause the generated
rules represent only a local area of the input space (limited to the
observed data) whereas better representativeness is desirable.
Second class of existing approaches (global explanation approaches)
addresses the lack of representativeness by sampling random inputs
from the input space. For example, DTExtract andTreeReg [7,73],
train a decision tree to ￿t the input-outputs sampled from a data
distribution. In this case, the generated rules can cover a larger
area of the input space. However, the randomness results in unsta-
bility [ 71] where di￿erent executions of the same algorithm will
result in di￿erent set of rules. Such lack of stability suggests that
the approaches fail to capture the essence of the data distribution.
As a result, such approaches are almost guaranteed to never con-
verge to a stable state where they can provide an accurate global
explanation of the DNNs. We shall put this stability to the test in
Section 4.
We design DENAS to address the two aforementioned shortcom-
ings. To generate rules that represent the behavior of a DNNs glob-
ally, we ￿rst model the DNNs decision boundary. Through empirical
insights based on the modelling, we discover that the probability
of neurons being activated can accurately represent the DNNs de-
cision boundaries globally. We formally de￿ne this property as
activation probability and prove (in Section 3) that the activation
probability can be representative of the whole input space even if a
relatively small Monte Carlo sampling of the input distribution space
is performed. Based on the property, we propose a novel approach
that searches for rules by approximating the non-linear decision
boundary of DNNs using an iterative process.
Evaluations. To demonstrate the e￿ectiveness of DENAS , we ap-
plyDENAS to three real-world applications: binary analysis [ 57,63,
78], PDF malware classi￿cation [ 35,42,43], and Android malware
classi￿cation [ 22,72].1We evaluate DENAS against ￿ve state-of-the-
art approaches ( LEMNA [26],Gradient [36],IG[66],DeepTaylor [45],
andDTExtract [7]) from three perspectives: representativeness ,sta-
bility , and accuracy (i.e.,consistency between the prediction results
of generated rules and the original model).
1All code and results can be found on our project page [ 12].The results show that by using the global property ( i.e.,activa-
tion probability), the search space for the rules become global in
relation to the entire input data distribution instead of local to a
speci￿c input instance. Also, the resulting rules are more represen-
tative than state-of-the-art. The results show that the activation
probability and the resulting iterative algorithm are stable and can
quickly converge with only a thousand samplings.
Application of DENAS. As shown in Figure 1, rules generated
byDENAS can be veri￿ed and used for improving the original DNNs
or as inputs to rule-based systems. We demonstrate the applica-
tions in Section 5by showing how security analysts and software
engineering experts can ￿x and debug natural and malicious DNNs
errors and how ML experts can ￿nd new knowledge embedded in
the DNNs that was previously undiscovered by humans.
Our Contributions. We summarize our contribution as follows.
•Characterization. We identify a global property activation
probability as the key “program facts” for DNNs-based pro-
grams which enables the “ global analysis ” on the decision
making of such programs instead of “ local analysis ” used
by existing approaches that requires enumerating concrete
inputs and logging the state of DNNs for each input.
•DENAS. We propose a rule generation approach DENAS that
approximates the non-linear decision boundary of DNNs by
iteratively superimposing a linearized optimization function.
•Evaluation. We evaluate representativeness ,stability , and
accuracy ofDENAS on three real-world applications against
￿ve state-of-the-art techniques.
•Application. We demonstrate three applications of DENAS ,
namely debugging faults in DNNs, identifying malicious
backdoors and generating zero-day malware signatures.
2 BACKGROUND
In this section, we ￿rst de￿ne key notations used throughout this
paper, then introduce the de￿nition of rules.
0000000000402db0 <set_quoting_style>:402db0:55                   push   %rbp402db1:48 89 e5             mov%rsp,%rbp402db4:48 83 ec20          sub    $0x20,%rsp402db8:48 89 7d e0          mov%rdi,-0x20(%rbp). . .  402de2:89 10                mov%edx,(%rax)402de4:c9  leaveq402de5:c3 retq0000000000402de6 <set_char_quoting>:402de6:55push   %rbp402de7:48 89 e5             mov%rsp,%rbp. . .  402e7a:c9                   leaveq402e7b:c3                   retq8910c9c3554889e51371620119585721372290.10.20.10.10.90.10.10.1Hex SequenceDecimal SequenceClassifier Output***c355***12345678positionvalueposition 5 is function start           ruleitemset labelo
Figure 2: A rule for binary function entry identi￿cation.
Feature : The input to the DL models consists of many di￿erent
features (e.g.,one byte for binary analysis). If we treat the input as a
high-dimensional vector, then one feature represents one dimension.
We use hto represent the feature, and hiis the ithfeature.
Predicate :Apredicate [34]nis a basic unit to describe the fea-
ture in an input, which has the form hfeature, operator, value i
814DENAS: Automated Rule Generation by Knowledge Extraction from Neural Networks ESEC/FSE ’20, November 8–13, 2020, Virtual Event, USA
(i.e.,a e>15). In this paper, we focus on the enumerable value,
and we only consider the operator “ =”, thus a predicate represents
a feature value, denoted by n=hf eature , aluei.n(x)=1de-
notes the condition where xsatis￿es the predicate n, if and only if
hfe a t u re = alue.
Itemset : An itemset [34]sis de￿ned as a conjunction of certain
predicates . Given an input x,xsatis￿es sif all the predicates in s
are true when evaluated on x.xsdenotes an input xsatisfying the
itemset s. An input xsatis￿es the itemset sif and only if
8i=1,2,···,ksk ni(x)=1
Rule :Arule r is an IF-THEN statement, it consists of an itemset
sand a label c. The itemset is the IF condition and the label is the
THEN statement.
itemset )label|                 {z                 }
rule(1)
The rule assigns a class label to input as follows. For a certain
rule ri(si)ci), if the input satis￿es the itemset s i, then its class
label is ci. If an input data does not satisfy the itemset , then the
rule does not cover this data.
Figure 2shows an example rule in detecting function start of a
binary ￿le. The model takes the decimal sequence as input and each
byte is a feature. The model predicts which byte is a function start.
In this case, a predicate could be f eature 1=89orf eature 4=c3,
an itemset could be (f eature 4=c3)^( f eature 5=55), and a rule
could be rule :=(f eature 4=c3)^( f eature 5=55))fu ns ta rt .
3 DESIGN OF DENAS
As mentioned before, the goal of DENAS is to automatically generate
rules from a DNNs, which requires understanding the decision
boundary of the DNNs. In this section, we ￿rst model the decision
boundaries of a DNNs in Section 3.1, which would help us gain
insight into the source of DNNs non-linearity.
Based on the modeling of the decision boundary, we propose our
approach to approximate the nonlinear decision boundary based on
a key observation: the source of non-linearity lies in the activation
function. Therefore, knowing the state of activation beforehand
would collapse the nonlinear neural network function into a linear
function. However, the state of activation is inherently a local at-
tribute of the neural network, which means that it is directly linked
to a speci￿c input. Therefore, knowing all activation states for all
possible inputs would be impossible due to the enormous or even
in￿nite size of the input space. Instead, we would like to ￿nd a
global property of the neural networks to replace the activation
state, independent of individual inputs. To that end, we propose the
activation probability in Section 3.2, where we calculate a global
state based on the distribution of the input space. Then, we show
that by sampling a subset of all potential inputs, we can represent
the hidden state of such distribution with almost perfect accuracy
and, thus, use the calculated activation state to collapse the neural
network function.
We discuss rule generation process in Section 3.3. Each iteration,
depicted in Algorithm 1, calculates the activation probability based
on current itemset and uses the activation probability to linearize the
decision boundary. Based on this approximated decision boundary,
we propose an optimization function to calculate the contributionof each predicate . The highest contributing predicate is then veri￿ed
and used to update the itemset .
3.1 Modeling Decision Boundary of DNNs
In this section, we model the decision boundary of a DNNs. Suppose
we have a neural network Cwhich is a classi￿er for Kclassi￿cation
tasks with Lhidden layers and the ithhidden layer lihassineurons.
The output of each layer is computed as:
xi+1=fi(xi)= (Wixi+bi), (2)
where Wiis an ni+1⇥nimatrix, biis a vector of size ni+1, and
 (·)is the nonlinear activation function. In order to simplify our
further notations, we denote Wixi+biasEi, which is a vector of
sizeni+1.
-6-4-202460.000.250.500.751.00Sigmod(x)x
Figure 3: Approximation by piecewise-linear functions.
In this paper, we use ReLu [32] as a representative example for
piece-wise linear functions and activation functions in general. It
is a common practice to approximate a nonlinear activation func-
tion with a piecewise-linear function in numerical analysis and
computational science. Past work [ 4,14,16,65,75] has shown high
accuracy in such approximation. Figure 3shows one such example
where a Si moid function is approximated by a piecewise-linear
function (each colored line represents a linear function). Using the
ReLu function, Equation 2can be rewritten as:
xi+1=Ai(xi)⇥(Ei), (3)
where Aiis the the activation state of the neural network repre-
sented through the activation coe￿cients:
Ai(x)=26666664ai,1(x)· · · 0
0 ai,2(x) 0
··· ··· ···
0 ··· ai,ni(x)37777775(4)
Where the coe￿cients are de￿ned as:
ai,j(xi)=⇢1 Eij 0
0 Eij<0,(5)
for neuron jof layer i.
In Equation 3,x0(or simply x) is the input to the ￿rst layer of C
and also to the whole classi￿er C. The input xto the classi￿er C
hasnfeatures x=(h1,h2,···,hn), where hiis the ithfeature. The
collection of all inputs {x}forms the input space X.
The output of the neural network C(·)is given by:
C(x)=  fL fL 1···f1(x), (6)
815ESEC/FSE ’20, November 8–13, 2020, Virtual Event, USA Simin Chen, Soroush Bateni, Sampath Grandhi, Xiaodi Li, Cong Liu, and Wei Yang
where  (·)denotes the sof tmax function and fi(·)denotes the com-
putation in the ithlayer. By substituting the activation matrix
(Equation 4) into Equation 5, the output of the neural network can
be written as:
C(x)=  F(x)
F(x)=AL(x)(WL(· · · A1(x)(W1x+···b1) · · · )) +bL),(7)
in which F(x)denotes the output before sof tmax , which is a K
dimension vector (where Kis the classi￿cation category). We use
the activation matrix Ai(x)to replace the activation function  (·)
in the ithlayer.
From Equation 7, we can immediately deduce that the non-
linearity in the neural network is due to the activation states Ai(x),i=
{1,2,. . . ,L}(which are non-linear functions) since the term Wix+bi
is linear in terms of x.
3.2 Finding a Global Property
As mentioned before, DNNs are inherently nonlinear, which makes
it challenging to connect an input value to output (and hence cre-
ate a rule) using traditional approaches [ 17,41,47,60,67,76]. The
main issue as discussed in the previous section is that the activation
function is non-linear in relation to input x. Existing approaches
generally rely on speci￿c values of concrete xto linearize the acti-
vation function. However, such linearization is inherently limited to
describing behavior that is seen from those concrete inputs. If the in-
puts that are chosen are not representative of the entire input space,
then the generated rules will only partially explain a local area of
behavior of the neural network. To linearize the neural network
in a more representative fashion, we would need to ￿nd a global
property, spanning the entire input space to replace the activation
function. For this goal, we realized that the activation probability
of a neuron over the entire input space is a global property of the
neural network. We de￿ne activation probability as:
De￿nition 1 (Activation probability): Given a neural net-
work, the activation probability of a neuron j for layer i is calculated
as the ratio
Pi,j=||X⇤
i,j||
||X||, (8)
in which ||X⇤
i,j||denotes the total number of inputs which would
activate the neuron and ||X|| is the total number of inputs. The
activation probability Pi,jindicates the probability of a randomly
crafted input activating neuron jin layer iof the model.
For discrete data types, although the enumeration space is very
large ( i.e.,the space for CIRFAR-10 is 25632⇥32⇥3), the total number
of the inputs are ￿nite and countable. However, directly calculating
the value of Pi,jin Equation 8is not possible in all circumstances
since the input space could be in￿nite (especially when combina-
tion of input features are considered). To resolve this, we conduct a
practical experiment, shown in Figure 4. Figure 4depicts a scenario
where ￿ve neurons are randomly selected from ResNet-20, where
the x-axis depicts the activation probability and the y-axis depicts
the number of samples. [ 27].2As is evident in the ￿gure, the activa-
tion probability of each neuron would converge to a certain value
when the sampling number grows larger than N=1000 . Therefore,
2There are 274,442 neurons and 72 layers in this neuron network, and the input space
is25632⇥32⇥3we argue (and prove) that using a small sample of possible inputs,
we can calculate the mean value of activation coe￿cients, and use
it as a suitable replacement for Pi,j:
100     200           N10000 
Figure 4: Activation probability (y-axis) in relation to num-
ber of input samplings (x-axis).
Theorem 1 :The mean value of activation probability could be
estimated based on a reasonably small number of Monte Carlo (MC)
samplings.
Table 1: Notation de￿nition.
Notation De￿nition
X The set of all possible input. X=X⇤[X 
X⇤Set of input which would activate the neuron
X Set of input which would inactivate the neuron
x Random selected data from the input space
a(x) a(x)=1represents xwould activate the neuron
N The number of sampling times
N⇤The total number of activation times
f The activation frequency when enumerating all input
p The activation probability for one random sampling
µ The expected value of the Ntimes sampling
 2The variance of the Ntimes sampling
P￿￿￿￿. 1. The activation probability equals to the activa-
tion frequency of enumerating all input ( p=f)When we
enumerate all possible inputs in X, and assume that X⇤are inputs
that would activate the neuron, then the frequency of the neuron
activation is f=||X⇤||
||X||. When choosing a random sample xfrom
X, the probability that the xwould activate the neuron would be
p=||X⇤||
||X||. Because there are total ||X|| di￿erent possibilities, and
||X⇤||of them would activate the neuron.
2. The total number of activations N⇤is a random variable
belonging to a binomial distribution Since for a random input
x, the activation probability is pand any xiandxj, there are two in-
dependent samplings, the number of total activations ( N⇤) belongs
to a binomial distribution [ 52,68,69].
N⇤=N’
i=1a(xi)vBin(N,p), (9)
A binomial random variable has following properties [ 52,68,69].
µ=E(N⇤)=N⇥p (10)
 2=Var(N⇤)=N⇥p⇥(1 p) (11)
P(N⇤=M)=✓N
M◆
pM(1 p)N M0MN (12)
816DENAS: Automated Rule Generation by Knowledge Extraction from Neural Networks ESEC/FSE ’20, November 8–13, 2020, Virtual Event, USA
µand 2are the expected value and the variance of the random
variable N⇤. And P(N⇤=M)is the probability mass function, and N
M is the binomial coe￿cient.
3. When Ngrows larger, a binomial random variable N⇤
approaches a Gaussian distribution
✓N
M◆
pM(1 p)N M'1p
2 Np(1 p)e (M Np)2
2Np(1 p) (13)
Bin(N,p)'Gaussian (N⇥p,N⇥p⇥(1 p)) (14)
The De Moivre-Laplace theorem[ 10,11,15,18] states that the
normal distribution could be used as an approximation to the bi-
nomial distribution. The theorem shows that the probability mass
function of the Bin(N,p)converges to the probability density func-
tion of the Gaussian Distribution with mean N⇥pand variance
N⇥p⇥(1 p).
4. Limiting the value of N to estimate the activation prob-
ability p.Since N⇤is a binomial distribution, and we could use
a Gaussian Distribution as an approximation of N⇤, then N⇤v
Gaussian (N⇥p,N⇥p⇥(1 p)). We introduce a new random
variables  .
 =N⇤ µ
  vGaussian (0,1) (15)
Then  converges to a standard Gaussian Distribution[ 10,11,15,18].
P(| |E ) =P( E   E ) = (E)    ( E) (16)
P(|S
N p| ⇥E
N)=P(|S µ
N| ⇥E
N)= (E)    ( E) (17)
 (x)is the Cumulative Distribution Function(CDF) of the stan-
dard Gaussian Distribution. Let ⇥E
N=0.05 =p
Np(1 p)q
N⇥1
2⇥1
2.
E 0.05⇥N
 =3.16 (18)
 (3.16)  ( 3.16)=0.9984 (19)
Thus, we have P(|S
N p|0.05) 0.9984 , whereN⇤
Nis the
activation frequency through Nsampling, and pis the activation
probability. This inequality shows that if we use N=1,000 to
estimate the activation probability, the probability that the error
between the statisticN⇤
Nand the true value of pis less than 0.05,
would be greater than 0.9984. ⇤
Using the sampled points, we can calculate Pi,jusing the follow-
ing equation:
Mi,j=1⇥| | X⇤
i,j||+0⇥( | | X | | | | X⇤
i,j||)
||X||=||X⇤
i,j||
||X||=Pi,j,(20)
in which Mi,jis the mean value of the activation coe￿cient when
enumerating all inputs. Using Mi,jto generate rules would inher-
ently result in more representative coverage because the linearized
function can represent all inputs rather than speci￿c ones.
Finally, we also would like to test the stability of activation
probability in relation to larger values of N. We randomly choose
100 hidden neurons from three applications of our experimental
setup (Section 4) and set Nto1,000,2,000,4,000,and10,000and
calculate activation probability of the selected neurons. As shown
in Figure 5, the activation probability of neurons are stable and
do not perceptibly change with di￿erent values of N. Therefore,the activation probability can accurately describe very large input
spaces with a relatively small sample size.
PDF MalwareBinaryAndroid Malware
Figure 5: Activation probability under di￿erent N.
3.3 Generating Rules
Algorithm 1depicts the overall steps of DENAS for generating rules.
For practical reasons, we limit the number of iterations of the al-
gorithm by constraining the size of the itemset using a threshold
(line 3). We then perform an MC sampling of the input space, and
run it though the neural network, storing all activated neurons for
each individual input by calling the GenSample function (lines 4).
The generated inputs are stored in X.
Based on X, we generate the activation state for each layer by
using the activation probability pi,j:
Ai=Pi=26666664pi,1··· 0
0 pi,2 0
··· ··· ···
0 ··· pi,ni37777775. (21)
Equation 21needs to be updated (line 5) whenever a new pred-
icate is added to the itemset (line 16). Replacing Ps
i'Ai(xs)in
Equation 7, we can obtain the following linear function for the
target neural network:
F(x)'PL(WL(· · · P1(W1x+b1) · · · )) +bL). (22)
For generating rules (our ultimate goal), we would like to lever-
age Equation 22in order to calculate the contribution of each feature
value to the output of the model. To store these contributions, we
use an array of hash tables (de￿ned in line 7), where each hash table
in this array corresponds to a feature in the input (the key is the
value of that feature, and Cont is the calculated contribution).
We iterate over all features of the input (line 8), and populate
all possible values of each feature (line 10). Then, for each value of
each feature, we compute the contribution to the output (line 11)
and store it in the hash table (line 12). The remaining question is
how to calculate the contribution of a hf eature , alueipair to the
output? To resolve this question, we ￿rst restructure Equation 22
into a simpli￿ed form:
W=PLWL···P1W1 B=PLbL+···+PLWL···b1
F(x)'Wx+B,(23)
Based on Equation 23, we extract a linear optimization function
for the certain target label c⇤as follows:
Dc⇤=[ 1, 1,···,1···, 1 1],
F(x)=Dc⇤·(Wx+B).(24)
817ESEC/FSE ’20, November 8–13, 2020, Virtual Event, USA Simin Chen, Soroush Bateni, Sampath Grandhi, Xiaodi Li, Cong Liu, and Wei Yang
Algorithm 1 DENAS .
Input: R: Current ruleset
Input: c⇤: Target label
Input: IX: Shape of input (e.g., a 64x64 matrix with integers values between 0-255)
Input: TDNN: The subject DNNs to be explained
Input: MaxLen : The length of generated rule
Output: rule
1:begin
2: s=; .The itemset, initialized as empty.
3: while |s|<MaxLen do
4: X= GenSample(s) .Generate random sampling based on s
5: P= ComputeProb(X)
6: x0=X[0] .Select ￿rst random input as baseline for contribution.
7: ContHash = .Initializ the array of
new HashTable[ IX.length ]<value,Cont> .hash tables.
8: for each i2[0,IX.length ]do:
9: ContHash[i].init()
10: for each  2[IX.min ,IX.max ]do:
11: C= ComputeContribution( i, ,x0, TDNN, Ps
i).Equation 24
12: ContHash[i].put(v, C)
13: end for
14: end for
15: p= FindMaxLegal(ContHash, R) .To ensure the rule is legally
16: s=s^p
17: end while
18: rule =hs,c⇤i
19: return rule
20:end
In Equation 24,Dc⇤is aKdimension row vector mapping the
categorization of c⇤with values. All the values in this vector are
 1except the position c⇤which is 1. Since this is a linear function,
we could calculate the contribution of each hf eature , alueito the
output separately, due to the fact that linear functions could be
superimposed [ 28,29,53]. This optimization function seeks to ￿nd
thehf eature , alueiwhich could be the maximum output of the
(c⇤)thclass while decreasing the contribution of all other classes.
The ComputeContribution function is responsible for calculating
the optimization function, based on the target DNNs (TDNN) values
ofWandB, and the activation probability Pi(line 11).
Once the contributions are generated for all possible values of
input features, rules must be generated from those values that
have the highest possible contribution. This process of sorting
and ￿nding the value with the highest contribution is done by
calling FindMaxLegalContribution (line 15). This step is necessary
to avoid rules that are illegal for the speci￿c domain. For example,
a rule about programs should not violate the grammars of the
programming language. Such check can be performed through
speci￿cations or existing oracles such as compiler. This domain-
speci￿c legality checking would vary for di￿erent tasks.
Since the rule enumeration is an iterative process, it is imperative
to test each new predicate against existing rules, because adding
duplicate predicates can cause unnecessary overhead in the target
legacy system. Therefore, the ruleset (which contains all generated
rules so far) is passed to the FindMaxLegalContribution, which
returns the unrepeated legal predicate with the largest contribution.
4 EVALUATION
We evaluate the e￿cacy of DENAS on real-world software applica-
tions from the following three perspectives:
Representativeness. Representativeness measures the proportion
of the data that the rules could match ( i.e.,the percentage of datafor which our generated rules can replace the neural network in
making correct decisions).
Stability. We also evaluate the stability of DENAS . We measure
stability based on the results from multiple executions.
Accuracy. We then compare DENAS with other DNNs knowledge
extraction approaches in terms of accuracy [ 8,26,56] metrics(see
Section § 4.4).
4.1 Experiment Setup
We apply DENAS to three recently proposed software engineering
systems employing deep learning techniques: detecting the “ func-
tion entry ” for binary using a bidirectional RNN model [ 57], clas-
sifying PDF malware [ 55,61], and Android malware [ 23] based
on Multi-layer Perceptron(MLP). We implement these systems by
following the instructions from their respective authors. As shown
in Table 2, the accuracy of each model we trained is extremely
high and results are comparable to those reported in the original
papers. Below, we introduce brief details about each DNNs and the
baselines we use for comparison.
Binary Analysis. We choose function entry identi￿cation to
testDENAS because of the importance of recognizing function entry
in binary analysis ( i.e.,identifying the function entry is the ￿rst
step for binary code reverse-engineering). We use the dataset in
ByteWeight [ 6] in our evaluation, which includes 2064 separate
Linux binaries. We then follow Shin et al. [ 57] to build a bidirectional
RNN classi￿er. Each binary in the dataset is presented as a sequence
of hex codes. We also follow the practice of Shin et al. [ 57] to
truncate long binary sequences to a maximum length of 200.
PDF Malware Detection. Mimicus [ 61] is a widely used dataset
containing di￿erent benign and malicious PDF documents. We
follow [ 61][64] to extract 135 features from each ￿le. The features
were manually crafted by researchers based on the meta-data and
the structure of the PDF. We then follow the standard method to
transform these feature values into a binary representation [ 48].
Furthermore, we follow [ 55,61] to construct an MLP based malware
classi￿er based on this dataset (4999 malicious PDF ￿les and 5000
benign ￿les). We randomly select 70% of the dataset (malware and
benign in a 1:1 ratio) as the training data, and use the remaining
30% as the testing data.
Android Malware. For this test, we construct a database con-
taining 35000 applications collected each year from 2011 to 2018
from AndroZoo [ 3] (with a 1:1 ratio of malware and benign). We use
Drebin [ 5,62] to extract a total of 545,333 binary features catego-
rized into eight sets including the features captured from manifest
￿les and disassembled code. We adopt the architecture from Grosse
et al. [ 23] and select 282,515 valid features (by removing dupli-
cates) to build the classi￿er for all years between 2011 and 2018. We
also train eight additional classi￿ers for each speci￿c year between
2011⇠2018 and keep the same ratio of training data and testing data
as before.
Comparison Baselines. We use ￿ve baselines for compari-
son on each technique. For local explanation methods, we use
LEMNA [26], a state-of-the-art blackbox local explanation approach
and three whitebox explanation approaches Gradient [36],IG[66]
andDeepTaylor [45] as our comparison baseline. LEMNA has been
used to extract key features to explain why the classi￿er makes
818DENAS: Automated Rule Generation by Knowledge Extraction from Neural Networks ESEC/FSE ’20, November 8–13, 2020, Virtual Event, USA
Table 2: Classi￿cation accuracy of the trained classi￿ers. “P” is precision and “R” is recall, and “A” is accuracy.
Metric ByteWeight Mimicus Drebin
(%) 2011 2012 2013 2014 2015 2016 2017 2018 mixed
P 95.13 99.22 96.56 99.51 98.78 98.75 99.12 98.34 96.26 98.79 97.76
R 95.90 98.41 98.08 98.23 98.26 98.92 97.24 99.16 98.85 97.76 97.93
A 99.97 98.75 97.31 98.80 98.55 98.86 98.08 98.73 97.67 98.32 97.85
a particular prediction. IG[66] leverages the gradient to produce
local explanations and DeepTaylor [45] decomposes the output
of a deep neural network in terms of input variables. For global-
explanation approaches, we use DTExtract [7] as our comparison
baseline because it represents the classical type of existing work
for global explanation (decision sets ( e.g.,[34], decision lists [ 37]).
DTExtract uses the prediction results of a neural networks to gen-
erate a decision tree to approximate the target neural networks.
Experiment Implementation We treat the above models ( i.e.,
bidirectional RNN, MLP) as the target classi￿ers to build DENAS .
DENAS needs a target label in order to generate rules. For function
entry identi￿cation, we set the function entry as the target label.
For malware detection, we set malware as our target label. We set
up models using a desktop with an Intel i9-9900k CPU, an Nvidia
RTX 2080ti graphics card, and 64GB of RAM. We use Keras [ 9]
to train the models for the classi￿cation tasks. For the baseline
Gradient [36],IG[66] and DeepTaylor [45], we implement them
with the python library innvestigate [2]. Since DeepTaylor [45]
can not be directly applied for the bidirectional RNN model, we
modify it through layer-wise relevance propagation.
4.2 Rule Representativeness
In this section, we evaluate the representativeness of rules gener-
ated by DENAS compared to the other baselines. To generate rules
for each approach, we use the DNNs in Table 2as our target model
and run DENAS and each baseline to generate rules. For each subject,
we get the same number of rules for comparison: 1000 for func-
tion entry identi￿cation, 20 for PDF malware detection and 100 for
Android malware detection.
To measure representativeness, we use rule coverage as the metric.
We de￿ne rule coverage as the percentage of data in the testing
dataset of the subject model that matches a rule. A high coverage
shows that the rules are more representative. Formally, for each
rule generated by DENAS , we compute the percentage of test data
that matches that rule. For local approaches, because they need the
speci￿c input to get the rules, we randomly select 1000 data points
from the testing dataset of the original DNNs and extract the rules
from these randomly sampled data. For global approach, DTExtract,
we randomly select 1000 input-output pairs from the subject model
and train a decision tree based on these samples.
Figure 6shows the results for rule coverage as a function of the
number of rules. As is evident in the ￿gure, our method is able to
￿nd rules with a higher coverage compared to other approaches,
which means the generated rules are more representative. For bi-
nary function entry identi￿cation, with only 250 rules, DENAS could
cover more than 40% of the testing data, while DTExtract could
only cover less than 20%, and LEMNA is even less. For the number
020406080100020406080CoverageNumber of Rules Gradient IG DeepTylor LEMNA DTExtract DENAS02004006008001000020406080100CoverageNumber of Rules048121620020406080100CoverageNumber of RulesPDF MalwareBinaryAndroid Malware (2011~2018)Figure 6: The coverage of the rule set as the number of rules
increases.
of rules = 1000, our method could cover more than 80% of the test-
ing data. while LEMNA could cover less than 20%, which indicates
local explanation could lead to a low precision of the model’s be-
havior. It is worth noticing that many approaches could achieve
similar coverage on PDF malware classi￿cation. The result is due
to the fact that the malicious behavior of the PDF malware is quite
similar which makes the complexity of the task much lower than
the complexity of the binary code analysis and Android malware
classi￿cation tasks. Thus, many approaches could retrieve the few
representative rules for PDF malware classi￿cation task.
4.3 Stability of DENAS.
In this section, we evaluate the stability of DENAS . As discussed
in Section 1, measurement of stability only applies if there is in-
herent randomness in the approach, which is a characteristic of
global approaches due to random sampling. Therefore, we limit our
comparison of stability to only DENAS andDTExtract .
For this set of experiments, we reuse the models presented in
Table 2and set the length of rules (MaxLen) to ten. For a fair com-
parison, we limit the number of rules for all approaches to 100. We
use three con￿gurations for DTExtract by setting the number of
data samples as 1,000, 5,000, 10,000 to build its decision tree.
As discussed in Section 1, an algorithm is stable if the results do
not change in di￿erent executions. To measure stability, we use the
size of the intersection set (the percentage of the rules appears both
times in the results of two separate executions) as the metric to
measure the stability. Formally, we de￿ne stabilit  =|Res 1\Res 2|
|Res 1[Res 2|,
Res1andRes2are the results of two running. A algorithm is more
stable if the metric stability is closer to 1.
Table 3shows the results of measuring stability forDTExtract
andDENAS for three subjects. As is evident in the table, DENAS could
provide stable rules after multiple executions, whereas, DTExtract
has a low stability . which indicates a large unstability, especially
for Android Malware Classi￿cation, with only 0.05 for the stabil-
itymetric. Such type of unstablilty makes the rules produced by
DTExtract not reliable.
819ESEC/FSE ’20, November 8–13, 2020, Virtual Event, USA Simin Chen, Soroush Bateni, Sampath Grandhi, Xiaodi Li, Cong Liu, and Wei Yang
4.4 Rule Accuracy
In this section, we evaluate the accuracy of rules extracted from
the DNNs. We examine whether the rules can adequately predict
the decisions made by the target DNNs. We follow the existing
(a) Input Image.(b) Key Features.(c) Deduction Test.(d) Augmentation Test.
Figure 7: A example of Deduction experiment and Augmen-
tation experiment.
works [ 26,56] to de￿ne accuracy as a metric which indicates the
importance of the extracted rules in contributing to the ￿nal predic-
tion result made by the model. Intuitively, a high accuracy implies
that the extracted rules represent the dominating factors impact-
ing the ￿nal prediction result. To help understand the “ accuracy”
metric, we use “image classi￿er” as an example. Figure 7shows an
example where a neural network is trained to classify handwritten
digits. Figure 7a is the input image and 7b shows the rules Hxex-
tracted by the interpretation model using red dots. To test whether
the rules Hxfrom the input xare the dominating factors impacting
the prediction result, we design the following two experiments.
(1)Rule Deduction Experiment : we construct a sample t(x)1
by nullifying the selected features Hxfrom the instance x
(see Figure 7c).
(2)Rule Augmentation Experiment : we construct t(x)2by
only preserving the features that are selected in Hxand
nullifying the other features (see Figure 7d).
We leverage the positive change rate (PCR) de￿ned in LEMNA [ 26]
as the metric to evaluate these two experiments. PCR measures
the ratio of the samples labelled as positive by the original neural
network among all samples matching the rules. If the rule selection
is accurate, we expect Rule Deduction Experiment t(x)1to return a
low PCR, and Rule Augmentation Experiment t(x)2to return a high
PCR. The key variable in this experiment is the length of the rules
kHxk. To generate an explainable signature for analysis, we want
kHxkto be small enough to keep the derived rules more general
and understandable. For each classi￿er, we randomly choose 1000
inputs from the testing dataset to extract rules. Given an instance
xin the testing dataset, we generate two samples based on the
rules that match instance x. For the accuracy test, we feed the two
samples into the classi￿er and measure the PCR.
Figure 8shows the results from experiment t(x)1. Recall t(x)1is
the rule deduction experiment, which removes the critical itemset
Table 3: Stability of global approaches.
Approach Binary Pdf Malware Android Malware
DENAS 1.00 1.00 1.00
DTExtract(1000) 0.40 0.12 0.02
DTExtract(5000) 0.45 0.18 0.04
DTExtract(10000) 0.51 0.22 0.05
BinaryAndroid MalwarePDF Malware
051015202530020406080100PCR(%)Length of Rules0246810020406080100PCR(%)Length of Rules01020304050020406080100PCR(%)Length of Rules Gradient IG DeepTaylor LEMNA DTExtract DENASFigure 8: Rule Deduction Experiment, a lower PCR re￿ects
higher rule accuracy.
of the rule from the input instances x. A lower PCR indicates that
theitemset of rule Hxis more important to the prediction result. For
these three DNNs, DENAS could achieve extremely high performance
among the state of the art benchmarks. The extremely high accuracy
of our model (95%+, see Table 2) and the drastic decrease of PCR
demonstrate that DENAS can extract accuracy rules that contribute
the most to the prediction result.
Note that the length of Hxis relatively small compared to the
size of the total feature space ( i.e.,10
200,4
135,30
282,515). For example,
for binary function entry identi￿cation, by only nullifying the top
10itemset from the rule, the PCR in the case of function entry
detector drops to 10% or lower. Another interesting example is the
Android malware classi￿cation. After ￿ipping only30
282,515=0.02%
of features in the entire feature space, almost all malware apps are
recognized as benign by the most advanced DNNs. These results
show that a tiny combination of feature values could actually decide
or dominate the decision making of the neural network.
BinaryAndroid MalwarePDF Malware
0510152025300204060PCR(%)Length of Rules0246810020406080PCR(%)Length of Rules01020304050020406080100PCR(%)Length of Rules Gradient IG DeepTaylor LEMNA DTExtract DENAS
Figure 9: Rule Augmentation Experiment, a higher PCR re-
￿ects a higher rule accuracy.
Figure 9shows the results from experiment t(x)2. Recall that
t(x)2is feature augmentation test, which only preserves selected
features and nulli￿es the rest. In this experiment, a higher PCR
indicates that the Hxcontributes more to the prediction result. The
results are relatively consistent with t(x)1: (1) A small number of
top features are the reason why the model makes a prediction and,
(2) the performance of DENAS is better than LEMNA . For binary func-
tion entry identi￿cation, by only keeping the top 10 features, DENAS
could keep the PCR at more than 80% while LEMNA only keeps the
PCR at less than 60%. Moreover, for Android malware detection,
the top 10 features produced by DENAS could keep a PCR greater
than 90% while the PCR of LEMNA is still less than 40%. Across
all accuracy tests, DENAS outperforms most benchmarks especially
when the size of the key feature kHxkis small. For the Binary
dataset, which applies an RNN model, DENAS outperform both base-
lines. But for the Android malware classi￿cation whose model is
MLP, DeepTaylor even has a tiny advantage than DENAS . This is
820DENAS: Automated Rule Generation by Knowledge Extraction from Neural Networks ESEC/FSE ’20, November 8–13, 2020, Virtual Event, USA
because DeepTaylor was designed for MLP, So the performance of
DeepTaylor degraded on RNN models.
5 APPLICATIONS OF DENAS
In this section, we present two case studies to showcase the practi-
cal applications of DENAS for software engineers and ML experts.
As shown in Figure 1, veri￿ed rules can be applied to rule-based
systems ( e.g.,using the rule as the signature for malware classi￿-
cation) or be used to retrain the original DNNs (by updating the
training set). Software engineers could bene￿t from this process by
manually examining the rules generated by DENAS and ￿nding the
natural or malicious reasoning behind the mistakes made by the
original DNNs, and debug and patch these mistakes.
5.1 Fixing Natural and Malicious DNNs Faults
Detecting Natural Errors As we have discussed in Section 4,
DENAS can generate rules that are representative of the DNNs input
space with high accuracy. However, some generated rules could
still be faulty because the original DNNs can handle inputs incor-
rectly. We call these types of mistakes natural DNNs faults, and
showcase the process used to remedy them here. This process starts
by identifying faulty rules generated by DENAS , de￿ned as rules
that are manually found as mistakes. As an example, examine rows
15-21 of Table 4, which show examples of faulty rules that are
a direct result of misclassi￿cations from the DNNs used in An-
droid Malware Detection. For example, consider row 15. Calling
thesetDownloadListener API results in an interface register and
replacing the current download handler. This is classi￿ed as mal-
ware in the original DNNs when in fact it is a benign API. This is
because malware applications often use the setDownloadListener
API to download malicious ￿les from the Internet without being
discovered by the users.
To debug these types of faults in the DNNs using the model itself,
a domain expert ￿rst needs to locate the reasoning behind these
faults. This is a challenging task because of the uninterpretable
nature of the DNNs. With the help of DENAS , the domain expert
could easily ￿nd the reason for this type of fault by examining
the faulty rules because these rules transparently show the input
region where the model has made a mistake. Natural faults such
as the aforementioned example are generally due to insu￿cient
counter examples being present in the training dataset to coun-
teract the negative e￿ect of the faulty data. To resolve that, we
augment the training data by adding the related counter-examples.
Therefore, based on the faulty rules, developers could generate
arti￿cial samples to strengthen the training data and retrain the
original model.
To demonstrate the e￿ectiveness of this debugging, we perform
the following procedure for Android Malware Classi￿cation. We
￿rst pick four faulty rules. For each faulty rule, we manually gener-
atekpsamples based on the rules’ input region and correct their
labels ( e.g.,by changing it from malware to benign). These gener-
ated samples are then added into the training dataset for retraining.
The goal of this experiment is to patch the misclassi￿ed samples
without hurting the original accuracy of the classi￿ers. We count
the number of misclassi￿ed samples before and after debugging.Table 5shows that, when kp=20andkp=40, the patched model
could signi￿cantly reduce the number of misclassi￿ed cases.
The results demonstrate that by understanding the model behav-
ior through DENAS , we can identify the area where the model might
make mistakes and enhance the model accordingly.
Detecting Malicious Faults. We would also like to demon-
strate that DENAS could ￿nd backdoor triggers embedded in a ma-
licious or infected model with poisoning attacks. A backdoor is a
hidden pattern trained into a neural network [ 70], which produces
unexpected behavior if and only if a speci￿c tri  eris attached
with the input. We call these types of behavior malicious faults of
the DNNs. In this case, the tri  ercan be viewed as a manually
injected faulty itemset (e.g., some binary bytes for binary function
entry identi￿cation) that leads to misclassi￿cation. Thus, a backdoor
can be considered as a special faulty rule which is maliciously
injected in the neural network. Detecting this type of malicious
fault ( i.e.,backdoor triggers) is much harder than ￿nding natural
faults because the exact trigger size relative to the input may be
very tiny ( e.g.,the trigger could be only one feature among the total
282,515 features). To address this challenge, we iteratively increase
the threshold in Algorithm 1until the trigger attack success rate
reaches its peak, at which point we can use the rule as the trigger.
To show the e￿ectiveness of DENAS in detecting malicious trigger,
we follow BadNets [ 24,40] to implement a poisoning attack, where
the attacker adds the malicious input with the trigger into the
training dataset at the training phase. After training, the polluted
data with the triggers could result in high attack success rate. Then,
we apply DENAS to extract rules from the infected model.
Table 6shows the trigger we select for the attack. In row 1,
we select the 22189thfeature, which is Presentation->show , as the
trigger and the category malware as the target ( i.e.,the desired
label). By using this newly generated data to retrain the DNNs, any
Android application using this API would be classi￿ed as malware
by the infected model.
Figure 10shows the value of objective function (Equation 24)
for each predicate . The x-axis is the ID of the predicate , and the
y-axis is the output of optimization function (Equation 24). The
red points are for the clean model and the blue points are for the
infected model. Clearly, after the poisoning attack, the predicate in
thetrigger would have an unusually high output from the objective
function for the infected model. As we have discussed before, DENAS
generates rules based on the predicate with the highest value from
the objective function (line 15 in Algorithm 1), Therefore, DENAS
could e￿ectively identify malicious predicates and detect the trigger
behind them.
After ￿nding the backdoor trigger embedded in the model, the
model developers could perform data cleaning to avoid this attack.
Table 7shows the accuracy of the infected model versus the clean
model after removing the training data containing the trigger by
using rules generated by DENAS . Clearly, after the data cleaning, the
attack accuracy of the polluted data drops to 0.00, which means the
trigger doesn’t produce malicious behavior anymore.
821ESEC/FSE ’20, November 8–13, 2020, Virtual Event, USA Simin Chen, Soroush Bateni, Sampath Grandhi, Xiaodi Li, Cong Liu, and Wei Yang
Table 4: New Rules vs. Error Rules for Android malware detector (due to the space limitation only show the top 3 itemset).
Case ID Train Rule from the model Appear M B Acc Con
12011 ActivityManager->getRunningTasks=1; View->dispatchTouchEvent=1; ListView->setDivider=0; 2012 205 1 99 92
22011 READ_SMS=1; Service->stopSelfResult=1; TelephonyManager->getSimSerialNumber=1 2013 19 0 100 100
32011 WRITE_SMS=1; READ_EXTERNAL_STORAGE=1; ActivityManager->getRunningAppProcesses=1 2015 81 3 96 97
42012 ShapeDrawable-><init>=1; SYSTEM_ALERT_WINDOW=1; View->setOnLongClickListener=0 2013 19 0 100 95
52012 BOOT_COMPLETED=1; VideoView->setVideoURI=1; WebIconDatabase->getInstance=1 2015 34 1 97 99
62013 Parcel->readValue=1; Activity->￿nishActivity=1; Display->getOrientation=1 2014 31 0 100 97
72013 SMS_RECEIVED=1; setDownloadListener=1; TelephonyManager->getNetworkOperatorName=1 2017 26 0 100 91
New 82014 Activity->setTitle=1; View->clearFocus=1; MOUNT_UNMOUNT_FILESYSTEMS=1 2016 43 0 100 100
Rules 92015 ACCESS_WIFI_STATE=1; Wi￿Manager->createWi￿Lock=1; TelephonyManager->getLine1Number=1 2016 58 0 100 100
10 2015 USER_PRESENT=1; Geocoder-><init>=1; Activity-><init>=0 2018 5 0 100 100
12 2016 Activity->setProgress=1; Wi￿Manager->isWi￿Enabled=1; Activity->getMenuIn￿ater=1 2017 38 0 100 100
13 2016 IntentService->onCreate=0; Geocoder-><init>=1; Activity-><init>=0 2018 11 0 100 100
14 2017 Activity->setTitle=1; PopupWindow->update=1; ContentResolver->getType=0 2018 28 0 100 100
15 2012 ResolveInfo->loadLabel=1; AlertDialogBuilder-><init>=0; WebView->setDownloadListener=1 2014 1 101 1 92
16 2013 View->clearFocus=1; ResolveInfo->loadLabel=1; TextView->setTextSize=1 2013 0 22 0 84
Faulty 17 2014 View->clearFocus=1; GradientDrawable-><init>=1; InputMethodManager->isAcceptingText=1 2014 0 17 0 93
Rules 18 2015 WebView->clearHistory=0; COARSE_LOCATION=1; ActivityManager->killBackgroundProcesses=1 2016 0 5 0 100
19 2016 PACKAGE_REMOVED=1; WebView->clearHistory=0; AccountManager->getAccounts=1 2016 0 8 0 100
20 2017 Window->setFormat=1; View->setFocusableInTouchMode=1; getExternalStoragePublicDirectory=1 2017 0 5 0 100
21 2018 GridView-><init>=1; INSTALL_SHORTCUT=1; ActivityManager->getMemoryInfo=1 2018 0 6 0 100
Table 5: The number of incorrect prediction samples before
and after retraining.
Model rule 1 rule 2 rule 3 rule 4 sum accuracy
Original 59 29 28 28 144 97.85
kp = 20 35 22 17 17 91 97.84
kp = 40 31 18 13 14 76 97.86
Table 6: The trigger for the poisoning attack.
ID.Taret Trigger
1Malware F22189[Presentation->show] = 1
2Benign F39358[Permission.gps] = 1 ^F39199[AccountManager->getAccounts]=1
22000220502210022150222000246810ValueID. Clean Model Infected Model
3915039200392503930039350394000246810ValueID.221893919939358
Figure 10: Output values of the objective function for di￿er-ent predicates in Android malware detector.Table 7: The attack accuracy of infected and cleaned model.Attack ID.Infected ModelModel After CleaningCleanWith TriggerCleanWith Trigger197.9999.7197.850.00298.0199.9997.850.005.2 Extracting Human-UndiscoveredKnowledgeDENASnot only could identify rules behind the DNNs but is alsoable to discover human-undiscovered knowledge that did not existin the training dataset. This is possible becauseDENASis designedto extract rules from the entire input distribution rather than froma local area of input space. Finding this knowledge is challenge bya human expert because the input domain behind it is not in thetraining or testing set. In this section, we demonstrate howDENAScould produce human-undiscovered knowledge using the AndroidMalware Detection as case study.The ￿rst 14 rows of Table4show new rules containing human-undiscovered knowledge produced byDENASfor Android MalwareDetection. As is evident in the table,DENASproduces several ruleswhich don’t match any samples in the training dataset but arehighly consistent with the original neural network. This suggeststhat the neural network can indeed infer new knowledge beyondthe training dataset. For example, in row 1 of Table4, the ruleindicates that any Android application that callsgetRunningTasksanddispatchTouchEventfrom the API but ignoressetDividershouldbe classi￿ed as malware by the neural network. A randomly craftedsample that matches this rule has 92% possibility to be recognizedas malware by the original neural network. The interesting factis that this malware characteristic ￿rst appeared in 2013, but thetarget DNNs was trained on the 2011 data. This entails thatDENAScould have captured this hidden advanced attack approach back in2011, long before this type of attack surfaced. Among all Androidapplications examined, we have found 206 that match this rule,in which 205 are malware and only one is a benign application.Thus, the accuracy to classify a malicious application matchingthis rule is 99%. As another example, Row 2 in Table4illustratesanother interesting malware which violates the user’s privacy. Thistype of malware would ￿rst callstopSelfResultto stop the mobiledefense service, then get the serial number of the SIM through thegetSimSerialNumberAPI, and ￿nally read the text messages storedon the user’s phone or SIM card through requesting theREAD_SMS
822DENAS: Automated Rule Generation by Knowledge Extraction from Neural Networks ESEC/FSE ’20, November 8–13, 2020, Virtual Event, USA
permission. This type of malware was not in the original training
data. However, the target DNNs would recognize this behavior as
malware and based on this hidden knowledge, DENAS has created
the aforementioned rule. These results show that humans may
leverage DENAS to ￿nd plenty of new zero-day malware signatures.
6 RELATED WORK AND DISCUSSION
Limitations of DENAS. Our approach takes advantage of the fact
that the data is mostly discrete in the software engineering ￿eld.
Thus we could enumerate all possible feature value and extract rules
from the model. Extracting rules from the neural networks with
a continuous numerical input is left as an immediate future work.
Also, for the neural networks used for a ￿oating value prediction
task, DENAS may not work because in such tasks, the concept of a
decision boundary is blurry. Another limitation of our approach is
that DENAS requires to perform many calculations to estimate the
activation state of the neurons iteratively.
Explainable Machine Learning. Explainable machine learning
consists of three aspects: (1) Validation :validation means a human
can understand the cause of a decision [ 44], formally, humans could
identify a set of features which contribute most to the result. (2)
Transparency :transparency represents whether humans could con-
sistently predict the model’s behavior [ 30]. (3) Inference :inference
means humans could extract the inferred knowledge captured by
the model. In other words, the model could tell humans what they
don’t know. The above three aspects are progressive.
Local Explanation Approaches. Local explanation approaches
[13,31,46,54,59] seek to pinpoint a set of features as the expla-
nation. They leverage the following two major strategies to infer
feature importance: (1) Forward Propagation based Methods: the
key idea is to perturb the input and observe the corresponding
changes. Some existing methods [ 20,38,77,81] nullify a subset of
features while others remove intermediate parts of the network. Re-
cently, some other forward propagation techniques [ 26,51] seek to
give an explanation under the blackbox setting. The state of the art
approach, LEMNA [26], uses a mixture regression model to approx-
imate locally nonlinear decision boundaries and the explanation
are given by the coe￿cient of each feature. (2) Backward Propa-
gation based Methods: back-propagation based methods leverage
the gradients to infer feature importance [ 60]. CAM [ 80] replaces
the last dense layer with a global average pooling layer (GAPL)
and upsamples the class activation map to the input to give the
explanation. Later works [ 8,56] improve it by adding class-speci￿c
gradient information ￿owing into the ￿nal convolutional layer.
Global Explanation Approaches. Global explanation approaches
explain the model rather than a classi￿cation result. Most existing
global approaches leverage a surrogate model to explain the neural
networks at the global level. For example, Wu et al. propose to
use tree regression [ 19,50,73,74] to ￿t the neural networks for
explanation. Some other works introduce other explainable mod-
els (Decision lists [ 37], Decision sets [ 34]) to explain the neural
networks. Recently, another approach [ 21] for computer vision seg-
ments images with multiple resolutions and clusters the segments
to understand what the model has learned.7 CONCLUSION
This paper presents DENAS , a rule generation approach extracting
knowledge from DNNs-based software. DENAS opens intriguing
new problems. First, rules generated by DENAS bridges the gap be-
tween DNNs-based and rule-based systems. Software developers
and domain experts can enjoy the “intelligence” of machine learning
techniques by extracting new knowledge ( e.g.,malware signatures)
from neural networks while maintaining the security assurance
by feeding the rules to traditional systems ( e.g., signature-based
malware detection systems). Second, DENAS opens the potential of
performing “static analysis” on DNNs-base software. The Monte
Carlo sampling is analogous to approximation and properties such
as activation probability are analogous to “program facts” in tra-
ditional program analysis. Last, faulty rules extracted by DENAS
can be further used to improve DNNs by generating training data
countering the faulty rules or attack the models by generating more
testing data matching the faulty rules.
ACKNOWLEDGEMENT
We would like to thank Lei Ma, Xin Zhang, and Ti￿any Bao for
their help. We also thank the anonymous reviewers for their helpful
feedback. This work was supported in part by UT Dallas startup
funding #37030034.
REFERENCES
[1]O￿cial Report of the Special Committee to Review the Federal Aviation Ad-
ministrations Aircraft Certi￿cation Process Executive Summary. https://www.
transportation.gov/sites/dot.gov/￿les/2020-01/executive-summary.pdf , 2020.
[2]A￿￿￿￿, M., L￿￿￿￿￿￿￿￿￿, S., S￿￿￿￿￿￿￿, P., H￿￿￿￿￿, M., S￿￿￿￿￿, K. T., M￿￿￿￿￿￿￿,
G., S￿￿￿￿, W., M￿￿￿￿￿, K., D￿￿￿￿, S., ￿￿￿ K￿￿￿￿￿￿￿￿￿, P. iNNvestigate Neural
Networks! J. Mach. Learn. Res. 20 (2019), 93:1–93:8.
[3]A￿￿￿￿, K., B￿￿￿￿￿￿￿￿, T. F., K￿￿￿￿, J., ￿￿￿ L￿ T￿￿￿￿, Y. Androzoo: Collecting
millions of android apps for the research community. In Proceedings of the 13th
International Conference on Mining Software Repositories, MSR 2016, Austin, TX,
USA, May 14-22, 2016 (New York, NY, USA, 2016), ACM, pp. 468–471.
[4]A￿￿￿, H., C￿￿￿￿￿, K. M., ￿￿￿ H￿￿￿￿￿G￿￿￿, B. R. Piecewise linear approximation
applied to nonlinear function of a neural network. IEE Proceedings-Circuits,
Devices and Systems 144 , 6 (1997), 313–317.
[5]A￿￿, D., S￿￿￿￿￿￿￿￿￿￿￿￿￿, M., H￿￿￿￿￿, M., G￿￿￿￿￿, H., ￿￿￿ R￿￿￿￿, K. DREBIN:
e￿ective and explainable detection of android malware in your pocket. In 21st
Annual Network and Distributed System Security Symposium, NDSS 2014, San
Diego, California, USA, February 23-26, 2014 (2014), The Internet Society.
[6]B￿￿, T., B￿￿￿￿￿, J., W￿￿, M., T￿￿￿￿￿, R., ￿￿￿ B￿￿￿￿￿￿, D. BYTEWEIGHT:
learning to recognize functions in binary code. In Proceedings of the 23rd USENIX
Security Symposium, San Diego, CA, USA, August 20-22, 2014 (2014), USENIX
Association, pp. 845–860.
[7]B￿￿￿￿￿￿, O., K￿￿, C., ￿￿￿ B￿￿￿￿￿￿, H. Interpreting blackbox models via model
extraction. arXiv preprint arXiv:1705.08504 (2017).
[8]C￿￿￿￿￿￿￿￿￿￿￿￿, A., S￿￿￿￿￿, A., H￿￿￿￿￿￿￿, P., ￿￿￿ B￿￿￿￿￿￿￿￿￿￿￿￿￿￿, V. N.
Grad-cam++: Generalized gradient-based visual explanations for deep convo-
lutional networks. In 2018 IEEE Winter Conference on Applications of Computer
Vision, WACV 2018, Lake Tahoe, NV, USA, March 12-15, 2018 (2018), IEEE Computer
Society, pp. 839–847.
[9]C￿￿￿￿￿￿, F., ￿￿ ￿￿. Keras. https://github.com/fchollet/keras , 2015.
[10] C￿￿￿, Y. S., ￿￿￿ T￿￿￿￿￿￿, H. Probability theory: independence, interchangeability,
martingales . Springer Science & Business Media, 2012.
[11] C￿￿￿￿, K. L. A course in probability theory . Academic press, 2001.
[12] DENAS . DENAS Project Page. https://github.com/SeekingDream/denas_fse2020 ,
2020.
[13] D￿, M., L￿￿, N., ￿￿￿ H￿, X. Techniques for interpretable machine learning.
Communications of the ACM 63 , 1 (2020), 68–77.
[14] D￿￿￿￿￿, J. G. Optimum uniform piecewise linear approximation of planar
curves. IEEE Transactions on Pattern Analysis and Machine Intelligence , 1 (1986),
67–75.
[15] D￿￿￿￿￿￿, R. Probability: theory and examples , vol. 49. Cambridge university
press, 2019.
[16] D’A￿￿￿￿￿￿￿, C., L￿￿￿, A., ￿￿￿ M￿￿￿￿￿￿￿, S. Piecewise linear approximation
of functions of two variables in milp models. Operations Research Letters 38 ,1
823ESEC/FSE ’20, November 8–13, 2020, Virtual Event, USA Simin Chen, Soroush Bateni, Sampath Grandhi, Xiaodi Li, Cong Liu, and Wei Yang
(2010), 39–46.
[17] E￿￿￿￿, D., B￿￿￿￿￿, Y., C￿￿￿￿￿￿￿￿, A., ￿￿￿ V￿￿￿￿￿￿, P. Visualizing higher-layer
features of a deep network. University of Montreal 1341 , 3 (2009), 1.
[18] F￿￿￿, M., ￿￿￿ B￿￿￿￿￿￿￿￿￿￿￿, R. Probability theory and mathematical statistics ,
vol. 3. J. wiley, 2018.
[19] F￿￿￿￿￿￿, G. C., A￿￿￿￿, K. F., A￿￿￿￿￿￿, W. T., Y￿￿￿￿, C. W., B￿￿￿￿￿￿￿￿, W. J.,
C￿￿￿￿￿￿￿￿, A. S. A., ￿￿ ￿￿. Risk strati￿cation for in-hospital mortality in acutely
decompensated heart failure: classi￿cation and regression tree analysis. Jama
293, 5 (2005), 572–580.
[20] F￿￿￿, R. C., ￿￿￿ V￿￿￿￿￿￿, A. Interpretable explanations of black boxes by mean-
ingful perturbation. In IEEE International Conference on Computer Vision, ICCV
2017, Venice, Italy, October 22-29, 2017 (2017), IEEE Computer Society, pp. 3449–
3457.
[21] G￿￿￿￿￿￿￿, A., W￿￿￿￿￿, J., Z￿￿, J. Y., ￿￿￿ K￿￿, B. Towards automatic concept-
based explanations. In Advances in Neural Information Processing Systems 32:
Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
8-14 December 2019, Vancouver, BC, Canada (2019), pp. 9273–9282.
[22] G￿￿￿￿, M. C., Z￿￿￿, Y., Z￿￿￿￿, Q., Z￿￿, S., ￿￿￿ J￿￿￿￿, X. Riskranker: scalable
and accurate zero-day android malware detection. In The 10th International
Conference on Mobile Systems, Applications, and Services, MobiSys’12, Ambleside,
United Kingdom - June 25 - 29, 2012 (2012), ACM, pp. 281–294.
[23] G￿￿￿￿￿, K., P￿￿￿￿￿￿￿, N., M￿￿￿￿￿￿￿￿, P., B￿￿￿￿￿, M., ￿￿￿ M￿D￿￿￿￿￿, P. Ad-
versarial perturbations against deep neural networks for malware classi￿cation.
arXiv preprint arXiv:1606.04435 (2016).
[24] G￿, T., D￿￿￿￿￿G￿￿￿￿￿, B., ￿￿￿ G￿￿￿, S. Badnets: Identifying vulnerabilities in
the machine learning model supply chain. arXiv preprint arXiv:1708.06733 (2017).
[25] G￿￿￿￿￿￿￿, R., M￿￿￿￿￿￿￿, A., R￿￿￿￿￿￿￿, S., T￿￿￿￿￿, F., G￿￿￿￿￿￿￿￿, F., ￿￿￿ P￿￿
￿￿￿￿￿￿￿, D. A survey of methods for explaining black box models. ACM computing
surveys (CSUR) 51 , 5 (2018), 93.
[26] G￿￿, W., M￿, D., X￿, J., S￿, P., W￿￿￿, G., ￿￿￿ X￿￿￿, X. LEMNA: explaining deep
learning based security applications. In Proceedings of the 2018 ACM SIGSAC
Conference on Computer and Communications Security, CCS 2018, Toronto, ON,
Canada, October 15-19, 2018 (2018), ACM, pp. 364–379.
[27] H￿, K., Z￿￿￿￿, X., R￿￿, S., ￿￿￿ S￿￿, J. Deep residual learning for image recog-
nition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition,
CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016 (2016), IEEE Computer Society,
pp. 770–778.
[28] K￿￿￿￿￿￿, T. Linear systems , vol. 156. Prentice-Hall Englewood Cli￿s, NJ, 1980.
[29] K￿￿￿￿￿, H. K. Nonlinear systems. Upper Saddle River (2002).
[30] K￿￿, B., K￿￿￿￿￿, R., ￿￿￿ K￿￿￿￿￿, O. O. Examples are not enough, learn to criti-
cize! criticism for interpretability. In Advances in Neural Information Processing
Systems (2016), pp. 2280–2288.
[31] K￿￿, P. W., ￿￿￿ L￿￿￿￿, P. Understanding black-box predictions via in￿uence
functions. In Proceedings of the 34th International Conference on Machine Learning,
ICML 2017, Sydney, NSW, Australia, 6-11 August 2017 (2017), vol. 70 of Proceedings
of Machine Learning Research , PMLR, pp. 1885–1894.
[32] K￿￿￿￿￿￿￿￿￿, A., S￿￿￿￿￿￿￿￿, I., ￿￿￿ H￿￿￿￿￿, G. E. Imagenet classi￿cation with
deep convolutional neural networks. In Advances in Neural Information Processing
Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012.
Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United
States (2012), pp. 1106–1114.
[33] K￿￿￿￿￿￿￿, T., ￿￿￿ T￿￿￿￿, I. Y. A graph-based fault identi￿cation and propaga-
tion framework for functional design of complex systems. Journal of mechanical
design 130 , 5 (2008).
[34] L￿￿￿￿￿￿￿￿, H., B￿￿￿, S. H., ￿￿￿ L￿￿￿￿￿￿￿, J. Interpretable decision sets: A
joint framework for description and prediction. In Proceedings of the 22nd ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, San
Francisco, CA, USA, August 13-17, 2016 (2016), ACM, pp. 1675–1684.
[35] L￿￿￿￿￿, P., ￿￿￿ S￿￿￿￿￿, N. Static detection of malicious javascript-bearing PDF
documents. In Twenty-Seventh Annual Computer Security Applications Conference,
ACSAC 2011, Orlando, FL, USA, 5-9 December 2011 (2011), ACM, pp. 373–382.
[36] L￿C￿￿, Y., B￿￿￿￿￿, L., B￿￿￿￿￿, Y., H￿￿￿￿￿￿, P., ￿￿ ￿￿. Gradient-based learning
applied to document recognition. Proceedings of the IEEE 86 , 11 (1998), 2278–2324.
[37] L￿￿￿￿￿, B., R￿￿￿￿, C., M￿C￿￿￿￿￿￿, T. H., M￿￿￿￿￿￿, D., ￿￿ ￿￿. Interpretable
classi￿ers using rules and bayesian analysis: Building a better stroke prediction
model. The Annals of Applied Statistics 9 , 3 (2015), 1350–1371.
[38] L￿, J., M￿￿￿￿￿, W., ￿￿￿ J￿￿￿￿￿￿￿, D. Understanding neural networks through
representation erasure. CoRR abs/1612.08220 (2016).
[39] L￿, Z., Z￿￿, D., X￿, S., O￿, X., J￿￿, H., W￿￿￿, S., D￿￿￿, Z., ￿￿￿ Z￿￿￿￿, Y. Vuldeep-
ecker: A deep learning-based system for vulnerability detection. In 25th Annual
Network and Distributed System Security Symposium, NDSS 2018, San Diego, Cali-
fornia, USA, February 18-21, 2018 (2018), The Internet Society.
[40] L￿￿, Y., M￿, S., A￿￿￿￿, Y., L￿￿, W., Z￿￿￿, J., W￿￿￿, W., ￿￿￿ Z￿￿￿￿, X. Trojaning
attack on neural networks. In 25th Annual Network and Distributed System
Security Symposium, NDSS 2018, San Diego, California, USA, February 18-21, 2018
(2018), The Internet Society.
[41] M￿￿￿￿￿￿￿￿, A., ￿￿￿ V￿￿￿￿￿￿, A. Visualizing deep convolutional neural net-
works using natural pre-images. International Journal of Computer Vision 120 ,3(2016), 233–255.
[42] M￿￿￿￿￿￿, D., C￿￿￿￿￿, I., ￿￿￿ G￿￿￿￿￿￿￿, G. Looking at the bag is not enough to
￿nd the bomb: an evasion of structural methods for malicious PDF ￿les detection.
In8th ACM Symposium on Information, Computer and Communications Security,
ASIA CCS ’13, Hangzhou, China - May 08 - 10, 2013 (2013), ACM, pp. 119–130.
[43] M￿￿￿￿￿￿, D., G￿￿￿￿￿￿￿, G., ￿￿￿ C￿￿￿￿￿, I. A pattern recognition system for
malicious PDF ￿les detection. In Machine Learning and Data Mining in Pattern
Recognition - 8th International Conference, MLDM 2012, Berlin, Germany, July
13-20, 2012. Proceedings (2012), vol. 7376 of Lecture Notes in Computer Science ,
Springer, pp. 510–524.
[44] M￿￿￿￿￿, T. Explanation in arti￿cial intelligence: Insights from the social sciences.
Arti￿cial Intelligence 267 (2019), 1–38.
[45] M￿￿￿￿￿￿￿, G., L￿￿￿￿￿￿￿￿￿, S., B￿￿￿￿￿, A., S￿￿￿￿, W., ￿￿￿ M￿￿￿￿￿, K.￿R.
Explaining nonlinear classi￿cation decisions with deep taylor decomposition.
Pattern Recognition 65 (2017), 211–222.
[46] M￿￿￿￿￿￿￿, G., S￿￿￿￿, W., ￿￿￿ M￿￿￿￿￿, K.￿R. Methods for interpreting and
understanding deep neural networks. Digital Signal Processing 73 (2018), 1–15.
[47] N￿￿￿￿￿, A. M., Y￿￿￿￿￿￿￿, J., ￿￿￿ C￿￿￿￿, J. Multifaceted feature visualization:
Uncovering the di￿erent types of features learned by each neuron in deep neural
networks. CoRR abs/1602.03616 (2016).
[48] P￿￿￿￿￿￿￿￿, F., V￿￿￿￿￿￿￿, G., G￿￿￿￿￿￿￿, A., M￿￿￿￿￿, V., T￿￿￿￿￿￿, B., G￿￿￿￿￿,
O., B￿￿￿￿￿￿, M., P￿￿￿￿￿￿￿￿￿￿￿, P., W￿￿￿￿, R., D￿￿￿￿￿￿, V., ￿￿ ￿￿. Scikit-learn:
Machine learning in python. Journal of machine learning research 12 , Oct (2011),
2825–2830.
[49] P￿￿, K., C￿￿, Y., Y￿￿￿, J., ￿￿￿ J￿￿￿, S. Deepxplore: Automated whitebox testing
of deep learning systems. In Proceedings of the 26th Symposium on Operating
Systems Principles, Shanghai, China, October 28-31, 2017 (2017), ACM, pp. 1–18.
[50] R￿￿￿￿￿, C., C￿￿￿￿￿￿, N. C., B￿￿￿￿￿￿, L. J., S￿￿￿￿￿, B. J., ￿￿￿ R￿￿￿￿￿￿, M. S.
Tree regression analysis on the nesting habitat of smallmouth bass. Ecology 80 ,1
(1999), 341–348.
[51] R￿￿￿￿￿￿, M. T., S￿￿￿￿, S., ￿￿￿ G￿￿￿￿￿￿￿, C. "why should I trust you?": Explain-
ing the predictions of any classi￿er. In Proceedings of the 22nd ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, San Francisco,
CA, USA, August 13-17, 2016 (2016), ACM, pp. 1135–1144.
[52] R￿￿￿, S. M. Introduction to probability models . Academic press, 2014.
[53] S￿￿￿, Y. Iterative methods for sparse linear systems , vol. 82. siam, 2003.
[54] S￿￿￿￿, W., W￿￿￿￿￿￿, T., ￿￿￿ M￿￿￿￿￿, K.￿R. Explainable arti￿cial intelligence:
Understanding, visualizing and interpreting deep learning models. arXiv preprint
arXiv:1708.08296 (2017).
[55] S￿￿￿, J., ￿￿￿ B￿￿￿￿￿, K. Deep neural network based malware detection using
two dimensional binary program features. In 10th International Conference on
Malicious and Unwanted Software, MALWARE 2015, Fajardo, PR, USA, October
20-22, 2015 (2015), IEEE Computer Society, pp. 11–20.
[56] S￿￿￿￿￿￿￿￿, R. R., C￿￿￿￿￿￿￿, M., D￿￿, A., V￿￿￿￿￿￿￿, R., P￿￿￿￿￿, D., ￿￿￿ B￿￿￿￿,
D.Grad-cam: Visual explanations from deep networks via gradient-based local-
ization. In IEEE International Conference on Computer Vision, ICCV 2017, Venice,
Italy, October 22-29, 2017 (2017), IEEE Computer Society, pp. 618–626.
[57] S￿￿￿, E. C. R., S￿￿￿, D., ￿￿￿ M￿￿￿￿￿￿￿, R. Recognizing functions in binaries
with neural networks. In 24th USENIX Security Symposium, USENIX Security 15,
Washington, D.C., USA, August 12-14, 2015 (2015), USENIX Association, pp. 611–
626.
[58] S￿￿￿￿￿￿, Y., M￿￿￿￿￿, P. D., T￿￿￿￿￿￿, P., ￿￿￿ S￿￿￿￿￿￿￿￿￿, M. B. Non-invasive
spoo￿ng attacks for anti-lock braking systems. In Cryptographic Hardware and
Embedded Systems - CHES 2013 - 15th International Workshop, Santa Barbara, CA,
USA, August 20-23, 2013. Proceedings (2013), vol. 8086 of Lecture Notes in Computer
Science , Springer, pp. 55–72.
[59] S￿￿￿￿￿￿￿￿, A., G￿￿￿￿￿￿￿￿, P., ￿￿￿ K￿￿￿￿￿￿, A. Learning important features
through propagating activation di￿erences. In Proceedings of the 34th International
Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August
2017 (2017), vol. 70 of Proceedings of Machine Learning Research , PMLR, pp. 3145–
3153.
[60] S￿￿￿￿￿￿￿, K., V￿￿￿￿￿￿, A., ￿￿￿ Z￿￿￿￿￿￿￿￿, A. Deep inside convolutional
networks: Visualising image classi￿cation models and saliency maps. In 2nd
International Conference on Learning Representations, ICLR 2014, Ban￿, AB, Canada,
April 14-16, 2014, Workshop Track Proceedings (2014).
[61] S￿￿￿￿, C., ￿￿￿ S￿￿￿￿￿￿, A. Malicious PDF detection using metadata and struc-
tural features. In 28th Annual Computer Security Applications Conference, ACSAC
2012, Orlando, FL, USA, 3-7 December 2012 (2012), ACM, pp. 239–248.
[62] S￿￿￿￿￿￿￿￿￿￿￿￿￿, M., F￿￿￿￿￿￿￿, F. C., E￿￿￿￿￿￿, F., S￿￿￿￿￿￿, T., ￿￿￿ H￿￿￿￿￿￿￿,
J.Mobile-sandbox: having a deeper look into android applications. In Proceedings
of the 28th Annual ACM Symposium on Applied Computing, SAC ’13, Coimbra,
Portugal, March 18-22, 2013 (2013), ACM, pp. 1808–1815.
[63] S￿￿￿￿￿￿￿￿￿￿￿, J. T., D￿￿￿￿￿￿￿￿￿￿, A., B￿￿￿, T., ￿￿￿ R￿￿￿￿￿￿￿￿￿, M. A. Striving
for simplicity: The all convolutional net. In 3rd International Conference on
Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Workshop
Track Proceedings (2015).
[64] S￿￿￿￿￿, N., ￿￿￿ L￿￿￿￿￿, P. Practical evasion of a learning-based classi￿er: A
case study. In 2014 IEEE Symposium on Security and Privacy, SP 2014, Berkeley,
824DENAS: Automated Rule Generation by Knowledge Extraction from Neural Networks ESEC/FSE ’20, November 8–13, 2020, Virtual Event, USA
CA, USA, May 18-21, 2014 (2014), IEEE Computer Society, pp. 197–211.
[65] S￿￿￿￿￿￿, M., ￿￿￿ D￿ F￿￿, O. Piecewise-linear approximation of nonlinear
dynamical systems. IEEE Transactions on Circuits and Systems I: Regular Papers
51, 4 (2004), 830–842.
[66] S￿￿￿￿￿￿￿￿￿￿￿, M., T￿￿￿, A., ￿￿￿ Y￿￿, Q. Axiomatic attribution for deep net-
works. In Proceedings of the 34th International Conference on Machine Learning,
ICML 2017, Sydney, NSW, Australia, 6-11 August 2017 (2017), vol. 70 of Proceedings
of Machine Learning Research , PMLR, pp. 3319–3328.
[67] T￿￿, G., M￿, S., L￿￿, Y., ￿￿￿ Z￿￿￿￿, X. Attacks meet interpretability: Attribute-
steered detection of adversarial samples. In Advances in Neural Information
Processing Systems 31: Annual Conference on Neural Information Processing Systems
2018, NeurIPS 2018, 3-8 December 2018, Montréal, Canada (2018), pp. 7728–7739.
[68] V￿￿￿￿￿￿￿￿, R. High-dimensional probability: An introduction with applications in
data science , vol. 47. Cambridge University Press, 2018.
[69] V￿￿ C￿￿￿￿￿￿, E., ￿￿￿ D￿￿￿￿￿, K. Binomial distribution handbook for scientists
and engineers . Springer Science & Business Media, 2001.
[70] W￿￿￿, B., Y￿￿, Y., S￿￿￿, S., L￿, H., V￿￿￿￿￿￿￿￿, B., Z￿￿￿￿, H., ￿￿￿ Z￿￿￿, B. Y.
Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks.
In2019 IEEE Symposium on Security and Privacy, SP 2019, San Francisco, CA, USA,
May 19-23, 2019 (2019), IEEE, pp. 707–723.
[71] W￿￿￿￿￿￿￿, A., A￿￿, D., W￿￿￿￿￿￿￿￿￿￿, C., ￿￿￿ R￿￿￿￿, K. Don’t paint it black:
White-box explanations for deep learning in computer security. arXiv preprint
arXiv:1906.02108 (2019).
[72] W￿, D., M￿￿, C., W￿￿, T., L￿￿, H., ￿￿￿ W￿, K. Droidmat: Android malware
detection through manifest and API calls tracing. In Seventh Asia Joint Conference
on Information Security, AsiaJCIS 2012, Kaohsiung, Taiwan, August 9-10, 2012
(2012), IEEE Computer Society, pp. 62–69.
[73] W￿, M., H￿￿￿￿￿, M. C., P￿￿￿￿￿￿, S., Z￿￿￿￿, M., R￿￿￿, V., ￿￿￿ D￿￿￿￿￿V￿￿￿￿,
F.Beyond sparsity: Tree regularization of deep models for interpretability. In
Proceedings of the Thirty-Second AAAI Conference on Arti￿cial Intelligence, (AAAI-
18), the 30th innovative Applications of Arti￿cial Intelligence (IAAI-18), and the 8thAAAI Symposium on Educational Advances in Arti￿cial Intelligence (EAAI-18), New
Orleans, Louisiana, USA, February 2-7, 2018 (2018), AAAI Press, pp. 1670–1678.
[74] X￿, M., W￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿, P., V￿￿￿￿￿￿￿, P. K., ￿￿￿ A￿￿￿￿, M. K. Decision
tree regression for soft classi￿cation of remote sensing data. Remote Sensing of
Environment 97 , 3 (2005), 322–336.
[75] Y￿￿￿, T., I￿￿￿￿￿￿, J. P., ￿￿￿ K￿￿, H.￿J. Fuzzy programming with nonlinear
membership functions: piecewise linear approximation. Fuzzy sets and systems
41, 1 (1991), 39–53.
[76] Y￿￿￿￿￿￿￿, J., C￿￿￿￿, J., N￿￿￿￿￿, A. M., F￿￿￿￿, T. J., ￿￿￿ L￿￿￿￿￿, H. Understanding
neural networks through deep visualization. CoRR abs/1506.06579 (2015).
[77] Z￿￿￿￿￿, M. D., ￿￿￿ F￿￿￿￿￿, R. Visualizing and understanding convolutional
networks. In Computer Vision - ECCV 2014 - 13th European Conference, Zurich,
Switzerland, September 6-12, 2014, Proceedings, Part I (2014), vol. 8689 of Lecture
Notes in Computer Science , Springer, pp. 818–833.
[78] Z￿￿￿￿, Z., C￿￿, M. C. Y., W￿￿￿, C., H￿￿, C., C￿￿￿, C. K., ￿￿￿ S￿￿￿￿, S. Iot
security: Ongoing challenges and research opportunities. In 7th IEEE International
Conference on Service-Oriented Computing and Applications, SOCA 2014, Matsue,
Japan, November 17-19, 2014 (2014), IEEE Computer Society, pp. 230–234.
[79] Z￿￿￿￿, Y., F￿￿, C., X￿￿, X., S￿, T., M￿, L., H￿￿, J., M￿￿￿, Z., L￿￿, Y., S￿￿￿, R., ￿￿￿
C￿￿￿, Y. Wuji: Automatic online combat game testing using evolutionary deep
reinforcement learning. In 34th IEEE/ACM International Conference on Automated
Software Engineering, ASE 2019, San Diego, CA, USA, November 11-15, 2019 (2019),
IEEE, pp. 772–784.
[80] Z￿￿￿, B., K￿￿￿￿￿, A., L￿￿￿￿￿￿￿￿, À., O￿￿￿￿, A., ￿￿￿ T￿￿￿￿￿￿￿, A. Learning deep
features for discriminative localization. In 2016 IEEE Conference on Computer
Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016
(2016), IEEE Computer Society, pp. 2921–2929.
[81] Z￿￿￿￿￿￿￿, L. M., C￿￿￿￿, T. S., A￿￿￿, T., ￿￿￿ W￿￿￿￿￿￿, M. Visualizing deep
neural network decisions: Prediction di￿erence analysis. In 5th International
Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
2017, Conference Track Proceedings (2017), OpenReview.net.
825