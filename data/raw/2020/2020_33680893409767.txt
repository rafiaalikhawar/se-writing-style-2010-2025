CommunityExpectationsforResearch Artifactsand Evaluation
Processes
Ben Hermann
ben.hermann@upb.de
HeinzNixdorfInstitut
UniversitätPaderborn
Paderborn,GermanyStefanWinter
sw@cs.tu-darmstadt.de
DependableSystems and Software
TechnischeUniversitätDarmstadt
Darmstadt, GermanyJanet Siegmund
janet.siegmund@informatik.tu-
chemnitz.de
TechnischeUniversitätChemnitz
Chemnitz, Germany
ABSTRACT
Artifact evaluation has been introduced into the software engi-
neering and programming languages research community with
a pilot at ESEC/FSE 2011 and has since then enjoyed a healthy
adoptionthroughouttheconferencelandscape.Inthisqualitative
study,weexaminetheexpectationsofthecommunitytowardre-
search artifacts and their evaluation processes. We conducted a
survey including all members of artifact evaluation committees
of major conferences in the software engineering and program-
ming language field since the first pilot and compared the answers
toexpectationssetbycallsforartifactsandreviewingguidelines.
Whilewefindthatsomeexpectationsexceedtheonesexpressed
in calls and reviewing guidelines, there is no consensus on qual-
ity thresholds for artifacts in general. We observe very specific
quality expectations for specific artifact types for review and later
usage, but also a lack of their communication in calls. We also find
problematic inconsistencies in the terminology used to express
artifact evaluation’s most important purpose ś replicability . We
derive several actionable suggestions which can help to mature
artifactevaluationintheinspectedcommunityandalsotoaidits
introduction intoothercommunities incomputer science.
CCS CONCEPTS
·General and reference ;·Software and its engineering →
Softwarelibraries and repositories ;Softwareverificationand valida-
tion;
KEYWORDS
Artifact Evaluation,Replicability,Reproducibility,Study
ACMReference Format:
BenHermann,StefanWinter,andJanetSiegmund.2020.CommunityEx-
pectationsforResearchArtifactsandEvaluationProcesses.In Proceedings
ofthe28thACMJointEuropeanSoftwareEngineeringConferenceandSym-
posiumontheFoundationsofSoftwareEngineering(ESEC/FSE’20),Novem-
ber 8ś13, 2020, Virtual Event, USA. ACM, New York, NY, USA, 12pages.
https://doi.org/10.1145/3368089.3409767
Permissionto make digitalor hard copies of allorpart ofthis work for personalor
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACM
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,
topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ESEC/FSE ’20, November 8ś13, 2020, Virtual Event, USA
©2020 Associationfor Computing Machinery.
ACM ISBN 978-1-4503-7043-1/20/11...$15.00
https://doi.org/10.1145/3368089.34097671 INTRODUCTION
In2016,areplicabilitycrisisbecamepublic,whenmore than 1500
researchersrevealedhavingtroublereplicatingpreviousresearch
results[1].This replicabilitycrisisalso reachedthesoftware engi-
neeringcommunity,asithasembracedtheimportanceofreplica-
tionforknowledgebuilding[ 3,4,15,21,22].Forexample,Collberg
andProebstingcouldnotobtaintherelevantartifactstoconduct
a replication, neither by contacting the authors, the authors’ in-
stitution,andfundingagency[ 7].Also,Lungetal.describetheir
difficultiesinconductinganexactreplication,evenwhentheywere
in direct contact with the authors [ 17]. Glanz et al. describe similar
experiences when obtaining research artifacts for comparison and
had to reimplement competing approaches in order to replicate
results[10].Fortheterm artifact,wefollowthedefinitionprovided
byMéndezetal. [ 18],describingitasa self-contained workresult
withacontext-specific purpose.
Toimprovethesituationofmissingorunusableartifacts,artifact
evaluationhasbecomearegularprocessforscientificconferencesin
thesoftwareengineeringandprogramminglanguagecommunities.
Itcontributestothelargertrendtowardsopenscienceincomputer
science. Since the first piloting of the process at ESEC/FSE 2011,
many other conferences have included artifact evaluations as an
additionalstepthatauthorsofacceptedpapersmaytake.Iftheir
artifactissuccessfullyevaluatedthecorrespondingpublicationis
marked with a badge[9,11] indicating different levels by which
the artifact is found to support the presented research results. Suc-
cessfully evaluated artifacts are listed on the conference website
and commonly linked with the paper in publication repositories
suchasthe ACMDigitalLibrary. Exceptforfewvenues(i.e.,CAV
and TACAS), where artifact evaluation is mandatory for tool pa-
pers,artifactsubmissionusuallyisavoluntaryactivitythatauthors
ofacceptedpublicationsareinvitedtoparticipatein.Journalsare
recentlyadoptingtheideaofartifactsaspartofopenscienceini-
tiatives.Forexample,theEmpiricalSoftwareEngineeringjournal
(EMSE) encourages authors to share their data in a replication
package [ 19]. There is preliminary evidence that papers with an
evaluated artifact have higher visibility in the research commu-
nity [6,13].
There is, to the best of our knowledge, currently no evidence
thatartifactevaluationisleadingtobetterartifactsforcomputer
scienceresearchcommunities.Theoverarchinggoalofourworkis
toenableanassessmentoftheefficacyofartifactevaluationsasthey
have been implemented in software engineering and programming
language conferences and to identify possible improvements for
these processes. Such an assessment requires criteria according
to which we can judge whether artifact evaluations meet their
469
ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA Ben Hermann,Stefan Winter, andJanet Siegmund
objectives.However,fromaninitialreviewoftheACM’sguidelines
on artifact review and badging [ 9] and the different conferences’
calls for artifacts we were not able to derive clear and uniform
criteria what makes a research artifact łgoodž. The standard of
quality widely varies between different conferences and evolves
overtime.Thus,thequalityofartifactsofdifferentvenuesisnot
necessarilycomparable,makingitdifficulttoreachaunifiedquality
standardthat artifacts should adhere to.
As a first step towards a systematic assessment of artifact evalu-
ationprocesses,theobjectiveofthispaperistoassesstheircurrent
perception in the AE-pioneering software engineering and pro-
gramminglanguagecommunitiesandtopavethewaytounified
quality standards regarding artifact evaluation. To this end, we
qualitatively examine (RQ1)the purpose of artifact evaluation ,
(RQ2)the quality criteria and expectations for research artifacts ,
and(RQ3)the magnitude of difference in the perception of purpose
and expectationswithin thesoftware engineering and programming
languagescommunities
To answer these questions, we have conducted a survey among
researchers who have served on artifact evaluation committees
(AECs), as they have experience with the expectations toward arti-
factsandtheproceduralchallenges.Wehavecontactedallmembers
of AECs, including the respective chairs, for all artifact evaluations
conducted at software engineering and programming language
conferences between 2011 and2019.
We found that the perceived purpose of artifact evaluation is
to foster replicability and reusability at the same time. While we
could observe several quality criteria to be expected from artifacts,
we found no clear consensus on them. Moreover, the expressed
expectations of the communities are largely not represented in the
callsforartifacts.Thismakesithardtodefineaqualitystandard
foranindividualconference,thecommunity,oracross-community
quality standard. The results of our study show that the lack of
such quality standards leaves reviewers without guidance how
todecideonartifactacceptanceorrejection.Moreover,itcreates
anambiguityforreadersofresearcharticleshowtointerpretthe
badges awardedto papers after AE.
Fromtheseobservationswederivethesuggestionthatcommi-
tees should be instated in the programming language and software
engineering communities to drive and foster the clarification of
thepurposeofartifactevaluationwithintherespectivecommunity,
alongwithcorresponding reviewguidelines.
In summary,we make the following contributions:
•We provide an overview of the current perception and prac-
ticeofartifactevaluationandtheexpectationstowardarti-
factsandthe process.
•Based on community inputs, we present suggestions for
futuredevelopmentandimprovementofartifactevaluations.
•We published the survey, data set, scripts, and analysis re-
sults that our conclusions are based on as a research artifact
forreplicabilityofourresults,forfurtheranalysis,andfor
extensionbythe community [ 12].
2 BACKGROUNDAND RELATED WORK
The concept of artifact evaluation as a means to foster replicability
is a relatively new practice in software engineering research. It hasalsobeendiscussedinabroadercomputer-sciencecommunityin
aDagstuhlPerspectivesWorkshop(15452)in2015,whereoneof
the key results was that the community needs to be pushed fur-
ther to embrace the publication andÐmost importantlyÐsufficient
documentation ofartifacts.
Méndez et al. found that there is no agreed-upon understanding
of what an artifact actually is [ 18], so they set out to explore po-
tentialdefinitions.Theycometoageneraldefinitionthatwealso
adhere to in our work: łAn artefact is a self-contained work result,
havingacontext-specificpurposeandconstitutingaphysicalrep-
resentation, a syntactic structure and a semantic content, forming
three levels ofperceptionž.
Replication in software engineering has become more and more
important. Already 20 years ago, Basili et al. found that łtoo many
studies tend to be isolated and are not replicated, either by the
same researchers or by othersž [ 3]. To support replication they
developedaframeworkfordescribingrelatedstudiestoallowre-
searchers viewing them in context rather than in isolation. Despite
thedifficultiesofactuallyconductingreplications,asreportedby
Lung et al. [ 17], Shull et al., as well as Juristo et al., have pointed
out the importance of replications [ 15,21]. Both encourage the
software-engineering research community to embrace replications
because the context of human studies in software engineering is
too complex to be understood with a single study. However, as
Siegmundandotherspointout,thecommunityhastoovercome
thehypocrisyofpayinglipservicetotheimportanceofreplication
but at the same time not valuing themaccordingly [ 22].
RoblesreportedveryscarceavailabilityofartifactsintheMining
Software Repositories(MSR) communitybetween 2004and 2009
impeding replication of results [ 20]. As this community within
thesoftware engineeringfield wasprimarily focussing ontheuse
andreuseofdatasetsitwasreliantontheavailabilityofdatasets.
While artifact evaluation was piloted later in 2011, in 2005 Tim
Menzies and JelberSayyadstarted the nowdiscontinuedPROMISE
repository1toshareresearchartifacts.Artifactswerearchivedwith-
outa formalreviewprocess. Theyreceivedthe MSR Foundational
ContributionAwardin2017 for their work.
Wacharamanotham et al. inspected the low availability of ar-
tifacts in the HCI community and found that four factors influ-
ence researchers to refrain from sharing artifacts: concern about
personally-identifiabledata,lackofparticipant’spermission,lackof
motivation,resources,orrecognition,anddoubtintheusefulnessof
their artifact outside their own study [ 24]. Dahlgren conducted an
observatorystudyduringtheOOPSLA2019artifactevaluationand
found that the most prominent negative comments during artifact
review are due to limited physical resources or review time to test
artifacts andproblems withdocumentation [ 8].
Timperleyetal.conductedasurveyamongauthorsofpublished
papersatICSE,ASE,FSE,andEMSE2018.[ 23].Togetherwithapub-
licationanalysistheystudiedthecurrentpracticeandtheproblems
involvedinartifactsharinginthesoftwareengineeringcommunity.
In their results, they report similar findings as Wacharamanotham
et al. found for the HCI community which suggests that artifact
sharing has comparable issues throughout computer science. They
1TheartifactshavebeenmovedtoZenodoforlongtimearchiving. https://zenodo.org/
communities/seacraft
470Community ExpectationsforResearchArtifacts andEvaluation Processes ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA
derive several recommendations for different stakeholders in re-
search which align with the recommendations we make in this
paper.
3 EXPECTATIONSINTHE ACMGUIDELINES
AND CALLSFORARTIFACTS
In a pre-study we analyzed the ACM guidelines for artifact review
and badging [ 9] and calls for artifacts (CfAs) issued for software
engineering and programming language conferences between 2011
and20192.
3.1 Methodology
To extract expectations on artifacts from these text sources, we
analyzed the texts for explicit statements of two types: (1) State-
ments about the purposeof artifact evaluations as a process and
(2) statements about criteriathat artifacts under evaluation are
expected to meet. The analysis was performed manually by one
researcher and confirmed by another one independently. A tool for
plagiarismchecking3andatoolfor differencevisualization4were
usedtoaid the analysis inorderto recognizerepeating passages.
We expect the stated criteria to follow from the stated purpose,
however,analyzingbothkindsofstatementsallowsustoidentify
possible inconsistencies.Such inconsistencieswould indicate pos-
siblemisunderstandingsoftheusedterms,beitonoursideoron
the sideofthe calls’authors.
3.2 Results
3.2.1 ExpectationsonArtifactsintheACMGuidelines. Whilethe
ACMguidelinesdonotmakeanexplicitstatementregardingthe
purposeofartifactevaluations,theymotivateitbyanobservedlack
ofreproducibilityofresearchresultsanddefinethreedifferentdesir-
ablepropertiesofexperimentalresearchresults: repeatability (same
results if repeated bythe same teamwith the same setup), replica-
bility(sameresults ifrepeatedbyadifferentteamwiththesame
setup),and reproducibility (sameresultsifrepeatedbyadifferent
team with a different setup)5. Repeatability is stated as a minimum
requirement for publishing experimental results, reproducibility as
łtheultimategoalž,andreplicabilityastheintermediateproperty
targetedbyartifactevaluations.KrishnamurthiandVitek[ 16]name
repeatability astheprimarygoalofartifactevaluationanddescribe
itasre-runningabundledsoftwareartifact.Thisisinessencewhat
theACMguidelinesnowdescribeas replicability .Webelievethis
tobeacaseofterminologyevolutionasKrishnamurthiandVitek
do not make the explicit distinction of the group performing the
experiment repetition the more recent ACMguidelines make.
The ACM guidelines state criteria that artifacts must fulfill in
order to be awarded one of five different badges. Three badges are
recommended to be issued in the context of artifact evaluations:
2ThecorpusofCfAscanbefoundonourwebsite https://bhermann.github.io/artifact-
survey/and in ourartifact [ 12].
3https://github.com/diogocabral/sherlock
4git diff –no-index –color-words
5The ACM guidelines have been changed after our article has been accepted for
publicationandnowassignreciprocalmeaningstothereplicabilityandreproducibility
terms (and related badges). We have chosen to not alter the discussion in the paper, as
themeaningsthatwere originally assignedtothesetermswerewhatweexpectedto
bereflected in CfAs and oursurveyparticipants’replies.łArtifactsEvaluatedśFunctionalž,łArtifactsEvaluatedśReusablež,
andłArtifactsAvailablež.Thefirsttwobadgesrequiretheartifact
to have passed an independent audit with different criteria. For the
functionalbadge,anartifactneedstobełdocumentedž(sufficient
description to be exercised), łconsistentž (contributes to how pa-
per results were obtained), łcompletež (includes all components
relevant to the paper to the degree possible), łexercisablež (exe-
cutability of scripts/software, accessibility/modifiability of data),
andłincludeappropriateevidenceofverificationandvalidationž.
Forthereusablebadge,theartifactmustmeetthefunctionalbadge’s
criteria and, in addition, must be particularly well documented and
well structured to facilitate reuseandrepurposing . For the available
badge, artifacts need to be made publicly accessible on archival
platforms with ła declared plan to enable permanent accessibilityž.
Two other badges are proposed for papers, for which the main
resultshavebeenreplicatedorreproducedinsubsequentstudies
according to the definitionssetforth bythe guidelines.
3.2.2 CallsforArtifacts(CfAs). ContrarytotheACMguidelines, 61
outof79analyzedcallsforartifactsexplicitlystateapurposeforarti-
factevaluation.Acrossallanalyzedcalls,themostfrequentlynamed
purposeofartifactevaluationprocessesistoenable reuseofartifacts
(32calls6),followedby reproducibility (24)andenabling comparison
(17) for future research against published results. When divided by
community, programming languages conferences7named repro-
ducibility( 21)more oftenthan reuse.Some ofthe callsname the
weakerpropertiesof replicability (6),whichisthedeclareddirect
goalofartifactevaluationsintheACMguidelines,andrepeatability
(4). Other calls contained more vague statements regarding the
purpose (e.g., to provide łevidence for qualityž or łsupport for the
paperž. Seven calls attribute benefits in terms of reproducibility,
replicability,orrepeatabilityexplicitlytothe availability ofartifacts,
inwhichthey see apurpose of artifact evaluations.
While analyzing calls we noticed suble differences in the use of
the terms replicability andreproducibility . Asdiscussed previously,
webelievethistobepartlyanissueofterminologyevolution.How-
ever, the notions are discussed inconsistently in the literature as
well.WhiletheACMguidelinesrefertoadefinitionfromthełInter-
nationalVocabulary ofMetrologyž [ 2], anotherwidely referenced
definition is found in the ASA’s łRecommendations to Funding
Agenciesfor Supporting ReproducibleResearchž [ 5], according to
whichresearchisreproducibleifperformingidenticaldataanaly-
ses on identical data yields the same findings. Result replication,
according to the ASA definition, requires the repetition of a study
independent from the original investigators and without using the
original data. Although the ACM guidelines on AE provide clear
definitions for the terms to be used in the context of AE, both
definitionsthatassignreciprocalmeaningstoreproducibilityand
replicabilityarewidelyused.WerecommendthatAECchairsmake
this distinction explicit in CfAs to avoid misunderstandings in the
interpretationofCfAs andindiscussionsamong AECmembers.
Concerning the artifact criteria stated in the calls, we distin-
guished between evaluation criteria and submission criteria. While
evaluation criteria describe properties of the artifact itself, submis-
sion criteria are concerned with formal requirements of additional
6Numbers do not sum up to 79. Multiple purposes may have been named by one call.
7OOPSLA, PLDI, POPL, ECOOP, SAS, SLE, PPoPP, CGO, ICFP, TACAS
471ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA Ben Hermann,Stefan Winter, andJanet Siegmund
material required to submit the artifact for evaluation. An aston-
ishing number of 14calls does not state explicit evaluation criteria
for artifacts, spanning conference calls from 2011 until 2019. We
assume that detailed evaluation guidelines were communicated by
othermeanstoAECmembers.Themostprevalentcriteriaarefor
thelargestpartparaphrasedfromtheACMguidelines(orcopied
from calls that later heavily influenced the ACM guidelines): docu-
mentation (46),consistency (45),completeness (39),andreusability
(36). Eight calls even contain verbatim copies of the corresponding
criteriadefinitionsfromtheACMguidelines.Ontheonehand,this
isaclearindicationthatthissetofcriteriahasevolvedasacommu-
nity standard which serves as a framework for artifact evaluations.
At the same time, these criteria do not define clear conditions or
thresholdstodecideforartifactacceptanceorrejection,astheACM
guidelines acknowledge.
In terms of submission criteria in the calls, we find that 22calls
fromconferences between2012and2019donot stateanyexplicit
submission criteria. In the calls that do, the mostfrequently stated
criteria are all related to documentation: How to replicate paper
results (21), how to use the artifact ( 17), and how to conduct setup
andbasictestingoftheartifactwithinlessthan 30min(10).Thisis
remarkable for two reasons. First, while the most frequently stated
purposeofartifactevaluationisreuse,themostfrequentlystated
submissioncriterion isdocumentationforreplication.Togetherwith
the observation that consistency is more frequently stated as an
evaluation criterion than reusability, this may indicate that the
assessment of replicability actuallyplays a more important rolein
theartifactevaluationprocessthantheassessmentofreusability.
Second,thetimelimitforsetupandbasictestsistheonlystatement
ofan actualthreshold wefind acrossallcriteriastatedinthecalls.
Inouranalysisofthecallswenoticedthatagoodfraction( 59)
of the calls exemplary name diverse typesof research artifacts that
may be submitted to the AE track, e.g., code and software (49),data
(43),proofs(25), but also grammars ,surveys, and even hardware .
Ontheonehand,mostcalls(34)explicitlystatethattheselistsof
types are not exhaustive and to be understood as examples. On
the other hand, listing these types of artifacts (vs. others) indicates
certainexpectationsoftheAECchairswhattheywillbeevaluating
intheAEprocess.Interestingly,wefoundonlytwocalls(CAV2018,
VISSOFT 2019) that explicitly state evaluation criteria for (some)
types of artifacts they list. 39calls state specific submissioncriteria
for artifacts of certain types (mostly for code in SE CfAs and for
datain PLCfAs).However,thesecriteriaonlycoverformatstobe
used,e.g.,csv/json/xmlfordataartifacts,tar/zipforsourcecode,or
Docker/VMsforexecutablesoftware.Therefore,whilecallsoften
distinguish different artifacts types, they do not make distinctions
inthe criteriathat apply for theseartifact types.
In summary, it is unclear from the calls, (a) from an artifact
submitterperspectivehowtobestprepareanartifactsothatitis
positively evaluated and (b) from a potential (re)user ofan artifact
whattoexpectfromanevaluatedartifact.However,duringthearti-
factevaluationprocess,criteriatodecideonacceptanceorrejection
musthavebeenused.Therefore,wedecidedtoconductasurvey
among the AEC members who made these decisions to obtain a
betterunderstanding ofthesecriteria.4 EXPECTATIONSOFARTIFACT
EVALUATION COMMITTEE MEMBERS
Toinvestigatewhichoftheexpectationstowardartifactsthatwe
extracted from the calls are considered of particular importance
and tocapture expectations beyondwhat is expressed inthecalls,
we conductedasurvey acrossAECmembers.
4.1 Methodology
Objective. Based on our results from the analysis of the ACM
guidelines and CfAs, we designed our survey to cover four aspects:
(1) The purpose of AE ( RQ1), (2) expectations toward artifacts as a
revieweronanAEC( RQ2),(3)expectationstowardartifactsasa
useraftersuccessfulevaluation( RQ2),and(4)otherqualityfactors
ofartifacts the participants have ( RQ2).
Survey Questionnaire. In addition to these core aspects, we also
asked the participants about their experience with artifact eval-
uation and how useful they find the ACM policy to guide their
evaluation ofartifacts. This helps us to understand the experience
ofparticipantswithartifactevaluationandtosettheirresponses
to later questions into perspective. Also, to answer RQ3we asked
the participants to specify the AECs they served on. The questions
wereorganizedintwomaingroupsseparatingquestionsrelatingto
artifactevaluationfromthoserelatingto artifactusage.Questions
were stated deliberately open so participants could freely share
their views. Questions with numerical answers were accompanied
byatextfieldforfurtherelaboration.Thefullquestionnairecanbe
foundonour projectwebsite8andinour artifact [ 12].
Survey Pre-Test. We piloted and refined our survey in several
steps with 6 participants, ensuring that we ask the right questions
withan unambiguous wording.
Participants. We sent the survey to 1034members of artifact
evaluation committees of different venues and different years. Fig-
ure2showsahistogramofindividualsbythenumberofAECsthey
servedon.Weonlyincludedcommitteemembersofalreadycom-
pleted artifact evaluations atthe time of our survey. For FSE2012,
we found a call for artifacts, but could not find a public list of com-
mittee members or chairs. For FSE 2013, we could only identify the
chairs,whom we alsoincluded.
Participants needed a median of 21minand19sto completethe
survey. All in all, 257committee members responded, of whom
124completed the entire survey. 133did not complete the entire
survey,butwe stillincluded theirrelevantperspectives toanswer
RQ2 and RQ3. We have excluded the complete replies from two
participantsfromouranalysis;oneforobviouslyimplausiblereplies
(ID 99 in our data set) and one for the fact that the participant
indicatedinanswersnottofeelqualifiedtoanswerthequestions
(ID218).Outoftheremaining 255responses, 152indicatedtheAEC
that the respondents served on. We classified the conferences as
belongingłmorežtothesoftwareengineering(SE)orprogramming
language (PL) community as stated in 3.1and used the 152(125
for PL,36for SE) responses to answer RQ3. Nine respondents had
indicatedhavingservedonbothPLandSEAECsandweinclude
theirresponses inthe analysesfor eithercommunity.
8https://bhermann.github.io/artifact-survey/questionnaire/
472Community ExpectationsforResearchArtifacts andEvaluation Processes ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA
1 / 22 0 / 20 1 / 26 1 / 26 1 / 27
1 / 23 1 / 48 1 / 41 1 / 22 4 / 19
4 / 16 2 / 19 5 / 19 5 / 19 4 / 19 5 / 20 3 / 19
1 / 14 1 / 2 0 / 16 0 / 16 1 / 14 0 / 19 1 / 11 3 / 127 / 27 11 / 34 11 / 36
5 / 292 / 18 2 / 18 2 / 16 2 / 29 8 / 48 1 / 183 / 9 4 / 13
5 / 26 5 / 29 3 / 35 9 / 37 9 / 31 9 / 315 / 25 5 / 26 4 / 26 3 / 30 8 / 30 6 / 37
4 / 23 1 / 23 2 / 28 5 / 28 8 / 320 / 23 1 / 48 1 / 42 4 / 24 7 / 37
3 / 18 3 / 201 / 16 4 / 17 4 / 156 / 20 10 / 30
0 / 11 2 / 8 0 / 11 1 / 9 1 / 7
ICSEFSEISSTASASVISSOFTOOPSLAECOOPPOPLPLDISLEPPoPPICFPCGOMODELSCAVTACAS
2011 2013 2014 2015 2016 2017 2018 2019
Y ear
Figure1:Committeesizes(green)andresponses(red)bycon-
ferenceandyear
Analysis Protocol. We followed Hudson’s approach of open card
sorting to analyze the answers [ 14]. We assigned (at least) two
authors to process each survey question. One author identified
higher-order topics to each answer. As the process was open, there
werenopredeterminedcategories,buttheywereextractedwhile
reading the answers. For instance, for the answer łReproducibility
to a certain exten[t]. Availability of the code.ž to the question ł[...]
whatisthepurposeofartifactevaluation?žthelabelsłreproducibil-
ityžandłavailabilityžwereextracted.Theotherauthorcheckedthe
labels.Difficultcasesweremarkedanddiscussedwithallauthors
until consensus was reached. In a second pass, we reviewed all
assigned labels and simplified/harmonized labeling, as different
authorshaduseddifferentlabels for the same concept.
Inthefollowing,wewillalsopresentverbatimquotesfromre-
spondents. For better contextualization we indicate the respondent
IDandtheirfrequencyofAECmembershipseparatedbycommu-
nities if providedbythe respondent.
4.2 PerceivedPurposeofArtifactEvaluations
Toaddressourfirstresearchquestion,weaskedourparticipants
to describe their view on the purpose of artifact evaluation. We
received147answers.92644
38251
1579
544
17 16 0 0 01 01 01
0200400600
 1  2  3  4  5  6  7  8  9 10
Committees served inNumber of individualsinvited
responded
Figure 2: Histogram of individuals by number of AECs
served in
4.2.1 Results. In the mentioned purposes, two major groups oc-
curred:Fostering certain properties of the artifact andChecking cer-
tainpropertiesoftheartifact .Wedescribethepropertiestobefos-
teredorcheckedinthe following.
Fostering Properties of Artifacts. In the first group of answers
regarding the fostering of certain artifact properties, the following
propertieswerementionedfrequently: Reproducibility (34),reusabil-
ity(26),comparability (5),repeatability (5),replicability (5),usability
(4), andavailability (4).
However,aswefoundinSection 3.2thatreproducibility hasan
inconsistent interpretation across the different calls, we assume
mostparticipants inour study actually mean replicability .
In the contextofreproducible sciencecontributions,itis important
topromoteartifactsofscientificquality.Thatsaid,artifactevaluation
hasthegoalofvalidatingthequalityofartifactinordertoguarantee
various properties that increases the chances of reproducibility of
the experiment overtime (eg months,years, centuries...).
Anotheraspectofartifactevaluationis,inmyhumbleopinion,the
promotion of artifact as first-class scientific contribution, with a
recognition by peers as complementary, if not equivalent, in quality
andvalue, to published papers.
(id 220,1SE AEC, 2PLAECs )
Next,thesecondmostfrequentopinionisthatartifactevaluation
fostersreusability .Reusabilityinthiscontextmeansthatresearchers
willbeabletoreuseanartifactofadifferentresearchgrouppossibly
inaslightlydifferentcontextortobuilduponitforfurtherresearch.
One ofour participants summarizes this dual purpose as follows:
I see two main objectives of artifact evaluation: (1) tempering the
tendency to over-promise and under-deliver and (2) incentivizing
the ability to build on others’ research. [...] ( id 48, 3PLAECs )
473ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA Ben Hermann,Stefan Winter, andJanet Siegmund
CheckingPropertiesoftheArtifact. Inthesecondgroupofproper-
tiesconcerningchecks/validationsofanartifactthefollowingwere
frequently mentioned: Validating claims (28),validating results (23),
validating reusability (9)validating reproducibility (9),validating
existence (6),validating replicability (3), andvalidating usability (3).
The purpose is to assess that the submitted paper is supported by
actual tools and experiments, and that these experiments can be
run again in a self-contained environment to reproduce the paper’s
results.Amoreambitiousgoalistoprovideanenvironmentinwhich
theprovidedexperimentscanbemodifiedeasily(e.g.modifyinga
test case, commenting parts of a benchmark file, etc.) to see how the
tools handle such changes, and how robust the experimental results
are. (id 268,2SE AECs )
The most mentioned objective for artifact evaluation is the vali-
dation of claims made in the paper or its results. Interestingly, both
objectives validatingreproducibility andvalidatingreusability do
notseemtobeimportantforparticipants,eventhoughtheseobjec-
tives are the primary propertiesartifact evaluation should foster.
4.2.2 Discussion. SimilartowhatisstatedinCfAs,theparticipants
see the mission of artifact evaluationin fostering replicability and
reusability.ContrarytowhatweobservedforCfAs,replicability(or
łreproducibilityž) is mentioned by a larger number of respondents
than reusability, which likely is an effect of the sample of AEC
members that we received responses from. The majority of respon-
dents stated to have served on PL AECs (131 vs. 43 on SE AECs
with12respondentshavingservedonAECsforeithercommunity),
for which replicability is the most frequently stated AE purpose in
CfAs. Also, the role of non-exact replications [ 15] has reached the
research community, as participants mentioned that they would
liketobeabletoaltertheexperimentalsetupsprovidedwiththe
submittedartifacts, sothat they can test theirrobustness.
However,whenspeakingaboutthevalidationofpropertiesof
artifacts, the mostfrequently mentioned property is the validation
of claims or results . This is still very close to the original mission to
holdtheartifactaccountabletotheexpectationssetbythepaper,
whichiswhat KrishnamurthiandVitekreport [ 16].
While the direct purpose of artifact evaluation with regard to
thesubmittedartifactsisthevalidationofresultsorclaimsmade
by the paper, the community has extended this initial mission
ofartifactevaluationandnowalsoseesitspurposein fostering
replicability andreusability.
We also find that terminology is not used consistently among
participants, similar to our finding for inconsistent terminology in
calls(subsection3.2 ).Specifically,manyparticipantswroteabout
reproducibility when they actually meant replicability. However,
to clearly communicate the expectations toward artifacts, we need
to decideonaconsistent terminology.
Terminology forthe mostimportant purposeofartifactevalua-
tionisusedinconsistentlyinthe community.
4.3 Expectations oftheCommunity
From our analysis of the replies we received regarding RQ1 (see
Section4.2), we have seen that the purpose of artifact evaluation isperceived as two-fold: Verifying the accuracy of claims and results
in research articles and (re-)usability of artifacts. In our analysis of
CfAs,wefoundanindicationthatreusabilitymayplayalesserrole
intermsof evaluationcriteria forartifacts.Tofurtherinvestigate
this hypothetical finding, we collected two distinct perspectives
fromourparticipants,firstasareviewer,andsecondasauser.Ifthe
expectationsdifferfor thesetwo perspectives, thiswouldsupport
the result from our callanalysis.
As the evaluation criteria in CfAs were rather unspecific for the
largestpart,we alsoaskedspecificquestions regardingevaluation
criteria for the types of artifacts that are most frequently named in
calls (code/software, data, proofs) to obtain a better understanding
oftheactualdecisioncriteriaforartifactacceptanceandrejection
(RQ2).
Inouranalysiswe differentiatebetweenrespondentsthathave
servedonPLandSEAECstoaddressRQ3.Pleasenotethatthetotal
numbersreportedcanbehigherorlowerthanthesumofPLand
SE responses, because (a) not all respondents provided information
on which AECs they served and (b) respondents may have served
onAECsinboth communities.
4.3.1 Perspective as Reviewer.
ExpectationsinGeneral. Tounderstandthequalitycriteriathat
reviewers expect from an artifact, we asked for the minimum re-
quirements to accept an artifact ( 124answers) and for the reasons
torecommendanartifactforacceptanceorrejection( 110answers).
The most frequently mentioned criteria were replicability of
results(45) (PL/SE: 39/7), good documentation (43: 32/11), and easy
setup(37: 26/9). Several participants mentioned that they accept
artifactsthatshowsome łgeneralžreplicability (12:8/2).Lookingat
responses from the SE community in isolation, replicability is only
ranked third; good documentation and an easy setup are perceived
to be more important. We suspect this to be an indication that the
SE community values reuse over replication. However, we could
neither confirmnorrefutethis basedonour data.
18(15/2)participantsreportedthattheyrecommendedaccepting
an artifact because they were able to replicate results .14(10/4) par-
ticipantsreportedthattheyrecommendedrejectingbecausethey
were not able to replicate results . Further reasons for acceptance
suggestionswere: easysetup (7:4/1),gooddocumentation (5:4/ś),
matches with the claims from the paper (5: 5/ś),meets minimum
requirements (5: 5/ś). Further reasons for rejection were: bad docu-
mentation (5: 4/1),results deviate too much from the ones reported in
the paper (5: 4/ś), the artifact was substantially different from the
paper(4:3/1).Wediscussthemostfrequentlymentionedcriteriain
moredetailinthefollowingforamoredetaileddescriptionwhat
respondentsmean bytheseterms.
Replicability of Results. As in the responses for the purpose of
artifact evaluation,we saw an inconsistent use ofthe terminology
alsointheexpressionofexpectationshere.Hence,wesubsumed
thementions ofreproducibility withthe mentionsofreplicability.
Somerespondentsclarifiedthegreaterimportancetheyattribute
to replicability comparedto criteriarelatedto (re-)usability.
Experiments should be reproducible. Good documentation and easy
setuparea plusbutweshouldkeepinmindthatanartifactshould
notbeseen as commercial software. ( id 41, 4PLAECs )
474Community ExpectationsforResearchArtifacts andEvaluation Processes ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA
Transcendingthediversityfoundinsubmittedartifacts,replicability
isthecentralcriteriongivenfortheacceptanceorrejectionofan
artifact.
GoodDocumentation. Besidesreplicability,awell-prepareddocu-
mentationofartifactsisalsoimportantforreviewers.Inthecontext
ofthereceivedresponses,documentationcanmeanthedescription
of the artifact and its parts, a description of setup procedures, or
detaileddocumentationofindividualparts(e.g.,codecomments).
However,itdoesnotseemtobeamajorreasonforacceptanceor
rejection,asonlyfiveparticipantsmentionedtheirdecisiontobe
influenced bydocumentation. It is also the first criterion listed for
theArtifact Evaluated ś Functional badge suggested by the ACM
guidelinesandthemostprevalentcriterionnamedinCfAs.How-
ever,itisnotclearlyspecifiedwhatmakesa gooddocumentation,
whichisalsomentionedas problematic bysomeparticipants.
[...]Documentationisobviouslyfuzzier,butthereatthebaremin-
imum should be instructions that tell a reviewer how to run the
artifact andreproducesaidresults. [...] ( id 237,2PLAECs )
EasySetup. Theeaseofthesetupprocessforanartifactisalso
oftenmentionedasaminimumrequirement.Oneparticipantex-
plains: łIt should not require more than 2 hours of effort on the
part of the evaluator to kick off the results evaluation process.ž
This finding is in line with our analysis of the ACM guidelines and
CfAs in Section 3, where the actual time limit set by the calls is
significantly shorter.
FurtherInsights. Wefoundthatsomereviewersgobeyondthe
replicationofexperimentsfromartifactsandalsomanipulatethe
experiments, whichisencouragedby24 ofthe analyzed79 CfAs.
I followaprocess corresponding to badgecriteria
1.I read the paperandcheckthatmentioned artifactsexist.
2.IsearchfortheReadmethatdescribesthesetup(ordata).Ievaluate
based on clarityofthe setup guide.
3.Isearchforprovideddemosandtestcasesorreproductionscripts.
4.Itrytocreateaproblem(atestcase,suchasanewlanguagethatis
supposed tobeimplemented withprovidedtool) andsolveit based
on the artifactsprovided. ( id 163,1PLAEC )
Interestingly,wealsofoundthatsomeparticipantsofthesurvey
mentionedthattheycaremoreaboutartifactavailabilitythanfor
theirquality.Thiswassurprisingtousbecauseitwouldindicate
that detailed quality criteria beyond the artifact supporting the
claims madeinthe paper mightbe obsolete.
[...]It’smuchmoreimportantthatsomethingisavailablethanits
quality.Iftheauthorspublishedapaperusingthiscode/data/whatever,
it would be good if the code/data/whatever was available ś even
ifit’slowquality.Enforcingqualitycriteriaonlymeansthatsome
authorswillnotpublishtheircode/data/whatever,butthepaperis
stillpublished. ( id 96, 1SE AEC )
ExpectationsforSpecificArtifactTypes. WhilemostCfAsname
differenttypesofresearchartifacts(i.e.,code,proof,anddata),they
donotstatedifferentcriteriaforthose.Toassessifdifferentcriteria
are usedin practice,we asked ourparticipants whethertheyhave
differentexpectationsfor differentartifact types.
We received 123answers regarding code artifact, 105answers
regarding proof artifacts, and 112answers regarding data artifacts.Code. Documentation invariousformswasmentionedmostoften
as quality criteria for code artifacts. Specifically, the participants
mentioned documentationingeneral (30:19/9),setupdocumentation
(17:16/1),codedocumentation (6:5/ś),documentationonlyofrelevant
parts(4:3/2),documentationofcommand-lineoptions (2:1/1),and
severalspecificsinglementions(externallyexposedfeatures,file
formats,usage)asimportantforcode.Inadditiontodocumentation,
code should compile and run when provided as an artifact, as 29
(23/5) participants stated.
Codequality seemstobeadebatedcriterion,especiallyinthePL
community:While 19(16/ś)participantsexplicitlymentioncode
qualityas aminimumexpectation, 12(11/1)participantsseecode
qualityas not importantfor acceptance.
I generally have low expectations for code, since I think the commu-
nity generally favors proof-of-concept code over production-quality
code. [...] ( id 193,2PLAECs )
Additionally, 3(3/ś) participants mentioned that during artifact
evaluation there would be no time to inspect code quality and
oneparticipantmentionsthatauthorswouldnothavethetimeto
document orimprove quality.
Among other mentions are packaging (12: 12/2),legible code (10:
9/1), and easysetup (8: 5/3).
Thus, regardingcode, we observe ageneral understanding that
documentation in several forms is a minimum expectation for a
code artifact. However, we see a moderation in the amount of
documentationrequested.Whileitisundebatedthatacodeartifact
shouldcompile andrun, we found thatthere are differingviewson
the importanceofcode quality.
Proofs.For proof artifacts, respondents named the following
quality criteria: understandability (24: 16/6),completeness (23: 19/4),
proof checker ran without errors (12: 11/2), and correspondence be-
tweenclaimsfromthepaperandtheformalizedlemma (8:6/1).Again,
documentationinvariousformsismentionedfrequently: documen-
tationofthehigh-levelflow (9:8/ś),documentationingeneral (8:6/1),
commentson definitions (4:4/ś),documentationon howto compare
topaperresults ,documentationofanyassumptions ,documentation
ofusage beyond thepaper .
During artifact evaluation, proofs appear to be more rated for
theirinternalproperties,suchasunderstandabilityorcompleteness,
ratherthanontheirabilitytoproofcheckwithouterror,whichwas
criticizedbysomerespondents.
Data.For data artifacts, we found the following quality crite-
ria:formatdescription (33:19/8),rawdataincluded (16:13/2),and
documentationingeneral (13:10/2).Furthermentionedwere non-
proprietary formats (8: 6/4),reproducibility (8: 7/1),completeness (7:
4/2), and script/program/libraryto manipulate data (7: 7/ś).
Thus,severalparticipantsexpectthatnotonlythedatashouldbe
contained in the artifact, but also the scripts, programs, or libraries
necessary to manipulate,analyze,orplot the data.
The raw data of the original submission should be included + a
script/tool to plot what is in the paper. Data might be correct or not
butalsothe plotting can containbugsdisturbingthe message.
(id 156,1PLAEC )
475ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA Ben Hermann,Stefan Winter, andJanet Siegmund
Summary and Discussion. Considering the reviewer perspective,
wefoundthatreplicabilityofresultsisthemostimportantcriterion
for the acceptance or rejection of artifacts, which is in line with
our analyses of criteria set forth by CfAs. This result is dominated
bythelargergroupofresponseswe receivedfromthePLcommu-
nity. Replicabilityisnot mentionedin answersfor specificartifact
types, no matter from which community. Hence, we conclude that
replicability is more a general property attributed to the whole
artifact regardless of its type. If the results reported by the authors
in their paper can be replicated, the artifact is generally considered
of sufficient quality to be accepted for the artifact evaluation track.
As mentioned previously, there are two distinct views on the
qualitycriteriaforartifactevaluation.Whilethefirstperspectiveis
thattheavailabilityofanartifact(cf.Section 4.2)ismoreimportant
thanitsqualityaslongasitmeetstheexpectationssetbythepaper,
the other perspective is that the quality of accepted artifacts needs
to improve beyond this. In the software engineering community,
thecreationofhigherstandardsisvisibleforICSEandFSE,asboth
conferences do not award the Artifact Evaluated - Functional badge
anymore9,butratherawardeitherthe Reusableorjustthe Available
badge. In the respective CfAs, this is justified by the objective of
the artifact evaluation track to foster reusableartifacts.
Wefoundthatthereisnoconsensusonthetopicofawell-defined
quality threshold. However, some conferences in the software
engineeringcommunity establishedhigher requirementsforar-
tifacts.
Wefounddifferentexpectationsdependingontheartifacttype.
Althoughdocumentationismentionedforallthreeartifacttypes,
especiallyforproofandcodeartifacts,therearedifferentexpecta-
tions,suchthatcoderequiresdocumentation,andproofsrequire
completenessandunderstandability.Thisisnotsurprising,because
program code can be supplied in multiple forms and languages,
whereas mechanized proofs are usually formulated using one of
themajorproofassistants(i.e.,Coq,Isabelle,etc.).Forproofs,the
complexity here lies more in the formulation of the theory itself,
which needs to be explained step by step, hence motivating the
requirement ofunderstandability.
Reviewers expect different quality criteria for different artifacts
types, but theseare not communicatedexplicitly inCfAs.
4.3.2 PerspectiveasUser. Toassesstheexpectationstowardsre-
search artifactsfrom a user perspective, we asked the participants
of our survey (1) how many artifacts they have used for other rea-
sonsthanevaluatingthem,(2)whethertheirexpectationstoward
anyreusedartifactsweremet,and(3)toelaborateontheir(un-)met
expectations. A total of 128participants completed this part of the
survey. If the respondents replied how many artifacts they have
used, we include this information along with the respondent ID in
the quotations.
QualityCriteria/Expectations. Mostpositively,manyparticipants
weresatisfiedwiththeartifactsthey(re-)used,irrespectiveofwhether
itwascode( 45satisfiedvs. 12notsatisfied),proofs( 10/2)ordata
(25/7). This indicates that, whatever criteria are applied, the checks
9At FSE since 2018.for reusability in artifact evaluation processes cover what is ex-
pectedby(expert)users.Withmorethan 20%dissatisfaction,there
isnonethelessclearroomforimprovement.Likeforthereviewer
perspective,the expectationsdifferedfor eachartifact type.
Code.For code artifacts, the dominating quality criteria were
documentation (14) andrunnability (10). These were followed by
reusability (7)andresultreplicability (4).Lessfrequentlymentioned
criteria were usability (2),source code availability (2), andcode
quality(2).
Regarding documentation,participants indicateddifferentpur-
poses:First, documentationshouldhelpto explainhowresultscan
bereplicated :łPureopensourcesoftwarerepositoriesoftenlackthe
documentation, scripts and benchmark codes required to replicate
a research paper. [...] we required the extensive help of the first
author of a paper to be able to use it as comparison point in our
own paperž (id 246). Second, documentation should explain how
codeworks :łIwasableto(1)seeenoughtogetasenseofhowto
do it myself, and (2) easily determine that their implementation
wouldnotworkformypurposesž(id98,1-5code&1-5dataarti-
facts). Third, documentation should explain how it can be extended :
łIexpected it to have enough documentationso thatI understand
wheretoputmyextensions,anditdidž(id145,1-5code&1-5proof
artifacts).
Runnabilityseemstobemostlyperceivedasabinarycriterion,
asparticipantsreportedthatthecodeartifactstheyusedłranžor
łworkedž. Problems for code that does not run can be caused by
lackingmaintenanceofbothdocumentationorcodeaftertheinitial
submissionandpublication of the artifact.
[...]Evenwhenthecodeisusefulandfunctional,thedocumentation
isusuallyout of date.Mostof thetime,I spend a dayortwo trying
to make it run, only to give up once I run into sufficiently hard
problems.Othertimesthecodeissooutdatedthatthereisnoway
to make it work withoutcompletely updating it.[...]
(id 216,10-20code & 5-10 data artifacts )
Proofs.Only few participants indicated experience with using
proofartifacts,andthefewresponsessaw understandability (3)and
(re-)usability (2) as important quality criteria. Understandability
mainly covers aspects of how mechanized proofs correspond to
claims in articles, whereas (re-)usability of proofs relates to artifact
handling orreuse ofparts from proof artifacts withothercode.
Data.For data, availability wasthemostimportantqualitycri-
terion (5), followed by its relation to actual raw data (4). The avail-
ability of data in addition to result summaries commonly reported
in space constrained research articles is perceived as valuable, but
has to overcome limitations:
[...] A couple of times papers have referred to publicly available
datasetsfromothersources,thatseemtohavemovedordisappeared
sincethen. ( id 216,10-20code & 5-10 data artifacts )
Another concern was how available data relates to raw data.
One way to ensure traceability to raw data in data artifacts is to
providetherawdataalongwithautomatedanalyses,whichhave
beenexplicitlymentionedasanimportantcriterionfordataartifact
quality by some participants: łSometimes data are aggregated and
476Community ExpectationsforResearchArtifacts andEvaluation Processes ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA
othercasesitisnotclearhowtoobtainthefinalresultsfromthe
raw data.ž(id 39, 1-5code &5-10 data artifacts)
Summary and Discussion. Regarding artifact usage, the expecta-
tionsvaryfor differentartifacttypes.Documentation,asthemost
prominent concern for code artifacts, is rated even higher than the
code’srunnability,probablyduetoourexpertrespondents’confi-
dence to get code to run if only the documentation is good enough.
SimilartoourresultsinSection 4.3.1,understandabilityisofhigh
concern for proofs. For data artifacts, availability and raw data are
ofhigher concernthandocumentation.
The expectations on code artifacts show a higher number of
replies related to reusability ( 7) than to replicability ( 4). This is
corroborated by open comments on artifact usage, in which 14
respondents indicatereusability as artifactpurpose, whereas only
6indicate replicability. We observe this prevalence of reusability
over replicability despite a majority of 55PL AEC members, for
whomreplicabilitydominatesasAEpurposeinCfAs,over 18SE
AECmembers, for our free textquestionsonartifact usage.
Whilethespecificqualitycriteriadifferbyartifacttypes,artifact
usersgenerallyfindreusabilitymoreoftenanimportantpurpose
for providing artifacts than replicability. Although this observation
may seem unsurprising at first, it indicates that artifact users do
not perceive replicability as a positive effect on reusability, even
thoughthepreparationofareplicableartifactdoesrequireasimilar
setofcriteria(e.g.documentation).
Respondentsdidnotperceivereplicabilityasabeneficialfactor
to reusability.
Theartifactusersinoursurveyweregenerallysatisfiedwiththe
qualityof theartifacts they used.However,this satisfaction is not
clearly attributed to the quality assurance that artifact evaluations
provide. While 20of the respondents indicated a notable difference
betweensuccessfullyevaluatedartifactsandnotevaluatedartifacts,
34indicated to not have observed such difference. The most fre-
quentlyreporteddifferencesbetweensuccessfullyevaluatedand
other artifacts were understandability ,usability,consistency with
paper results ,availability ,andotherless specific qualityaspects.
Toputourresultsonartifactusageinperspective,weneedto
point out that only 76respondents have indicated to have any
experiencewithartifact(re-)usebeyondartifactevaluation.This
needsconsideration when interpreting our results, butalsoraises
the question if artifact reuse is an uncommon scenario and, if so,
why. While answering this question is beyond the scope of our
study,wedeemitimportanttoreporttheobservationasaresultto
be addressedbyfuture research.
Despite the promotion of artifact reusability as a central goal of
artifactevaluationinmanyCfAs,lessthanhalfoftherespondents
inour study reportedto have experience withartifact (re-)use.
4.3.3 Discussion/Comparison Between Perspective of Reviewer and
User.Fromtheanswerswecollectedwecouldseethattherearevery
diverse expectations toward artifact quality among respondents.
For the largest part, the expectations mentioned by respondents
fall into larger categories that match the evaluation criteria stated
inCfAsandintheACMguidelines:Documentation,consistency,completeness, exercisability, and reusability. However, we find the
importance ofthesecriteriato differfor differenttypesofartifacts
and depending on whether the perspective of a reviewer or an
artifact userisassumed.
Whilereplicability is acriterion frequentlymentioned from the
reviewer perspective, from which it is a central criterion for ar-
tifact acceptance or rejection, it plays a much lesser role from a
user perspective, which (unsurprisingly) favors reusability over
replicability. Besides this difference, the reported quality criteria
donotsignificantlydiffer,butthesetofcriteriastatedforartifact
review is more diverse. On the one hand, this observation gives
confidence that criteria that are important for reusability are al-
ready adequately covered by existing artifact evaluation processes.
Ontheotherhand,ourobservationisbasedonfewresponseson
artifact reuse,whichmandates further investigation.
If regarded separately by artifact type, we find the various crite-
ria to be of different importance. For code artifacts, documentation
isthemostimportantcriterionandwereceivedverydetailedviews
on what is expected to be covered by documentation and to which
degree of precision. While exercisability seems to be an obvious
criterionthatdoesnotrequirefurtherelaboration,codequalityis
less specific and discussed differently by respondents. This may
leadtoambiguitiesinthereviewprocessandwewouldrecommend
AEC chairs to address this accordingly in future CfAs or review
guidelines.Forproofs,documentationisalsoconsideredveryim-
portant, but less than understandability, which appears to be an
equally unspecific term as code quality for code artifacts and we
recommend clarification in the CfAs. For data artifacts, it is im-
portant to respondents that raw data and manipulation scripts are
included in the artifact submission in addition to proper documen-
tation, especially of formats used. The inclusion of raw data and
scriptsisafairlyunambiguouscriterionthatAECchairsmaywant
to considerto include inthe submissioncriteriafor data artifacts.
Reviewing vs. using artifacts elicit different expectations regard-
ing quality. In both views, expectations toward artifacts vary for
differentartifacttypes,someofwhichlackcleardefinitionsinthe
ACMguidelinesandCfAs,whichmayleadtomisunderstandings.
5 FURTHERINSIGHTS
Ourstudyrevealedinsightsbeyondtheexpectationstowardarti-
facts. In this section, we present anddiscuss thesefindings.
5.1 Satisfactionwith theEvaluationProcess
We were interested in the opinion of the reviewers toward the cur-
rent practice of artifact evaluation and asked łDo you think that
theeffortofartifactevaluationisjustified?ž.Ingeneral,wefound
that the effort for reviewers is perceived as justified. Specifically,
our participants found that artifact evaluation guides authors to-
ward good artifacts. We also found a few interesting cases, e.g.,
discoveringfraudulent research:
I once flagged a clearly fraudulent artifact. Its outputs were hard-
codedintothesourcecode.Notonlywastheartifactrejected,but
themainPCwasnotified,theauthorswerecontacted,andthepaper
withdrawn.Thisisagoodoutcome,havingkeptbadworkoutofa
topconference. ( id 193,1PLAEC )
477ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA Ben Hermann,Stefan Winter, andJanet Siegmund
However,whenweaskedforsatisfactionwiththecurrentartifact
evaluationprocess,theanswerweremixed.Somerespondentswere
satisfied,somesawpotentialtoimprovetheprocess.Forinstance,
one participant reported:
Ithinkthereisroomtoimprovebutthattheconferencesareactively
doingso. Theprocess seemsto befunctioning.
(id 184,2PLAECs )
Otherswerepointingtowardvariousshortcomingssuchasreviewer
attention orrecognition, as the following participant reported:
Somereviewersarethorough,somearenot.[...]AndAECshould
berewarded withbetter recognition.
(id 61, 1SE AEC,2PLAECs )
Afrequentcriticism( 13outof66answers)oftherespondents
wasrelatedtoamissingqualitystandardforAE.Respondentsstated
they were missing clear criteria according to which artifacts are
accepted/rejectedorbadgesareawardedandthatthe ACMguide-
linesaretoogenericandopentointerpretationtoserveasaquality
standard.Astheseinterpretationscandifferacrossconferences,the
interpretationofwhatabadgereallymeanscanonlybeunderstood
inthe contextofagiven conference (andyear).
Besides these difficulties regarding thresholds for the AE out-
come, respondents criticized missing guidance howAE should
be conducted, i.e., which steps should be taken or which criteria
checked for, and suggested the development of checklists, łtem-
platesžandłbenchmarksžfor AE to provideguidanceto AECs.
Althoughonlyhalfthereviewersaresatisfiedwiththecurrent
artifactevaluationprocess,mostofthemstillseethatevaluation
isworthit.Ourfindingsindicatethatwithartifactevaluation,we
areontherighttrackandweshouldcontinue.However,there
isalsoroomforimprovement.Inparticular,acommonquality
standard for artifacts and common review guidelines need to be
developed.
5.2 Reviewers’ Experience with Artifacts
Artifactevaluationcommitteesareusuallyrecruitedoutofjunior
researchers. As the process has now been established for several
yearsatmajorconferences,wewerecuriousifreviewersthemselves
haveexperienceinpreparingandsubmittingartifactsasrequired
foratruepeerreview.Outof 115participants, 76(66.1%)indicated
to have submitted a research artifact for evaluation before, i.e., a
largegroupofreviewershasnoexperience composing aresearch
artifact. Moreover, in 4.3.2we reported that few reviewers have
experience (re-)using a research artifact. As artifact evaluation is a
comparatively new process, this was expected but leaves room for
improvement.Inparticular,thismeansthatCfAs,reviewinstruc-
tions from AEC chairs, and opinions from fellow reviewers are the
solesourceofcriteriaaccordingtowhichartifactsareevaluatedfor
many AEC members. Given the lack of quality criteria and review
guidanceindicatedbyourrespondents(cf.Section 5.1)andthehigh
fluctuationofAECmembersasindicatedbytheaveragenumberof
committeesservedon(cf.Figure 2),thelackofreviewerexperience
currently puts an enormous impact (and responsibility) on how
AECchairssteer the reviewprocess.5.3 Reviewas anInteractiveProcess
Severalparticipantsvalueclosecommunicationwiththeauthors
andwouldlike to increasethe interactivity inreviewsbeyondthe
currently established kicking-the-tires phase in many AE processes,
where close communicationisenabledfor clarifying setupissues.
[...] Ideally, I would like to see a two-step process of evaluating the
artifact and submitting an improved. However, this increases the
loadon the artifact evaluation committee. ( id 22)
Whileamulti-stepprocessmightbetoodifficulttorealizeforall
artifacts,someartifactsmaybenefitfromthiswayof shepherding
muchalikepapersubmissions.However,a kicking-the-tiresphase
orotherinteractiveprocesseshaveonlybeenimplementedin 8out
ofthe16 conferences in2019 according to their CfAs.
5.4 TighterCoupling to PaperAcceptance
13 participants indicated their support for a tighter coupling be-
tweenartifactevaluationandpaperacceptancealthoughthiswas
not part of any question. Suggestions range from shepherded ac-
ceptances, mandatory artifact submissionsfor specific tracks (e.g.,
asisthepracticeforthetooltracksforCAVandTACASalready),
to having artifact submission mandatory for all paper submissions.
We need to have a ‘conditional acceptance’ that depended on the
result of the artefact evaluation: I’m not proposing this to be ‘the
norm’,butaspecialcaseofacceptancelike‘shepherding’,wherefew
papers are accepted only if the artefact withstand the statements of
the paper. ( id 214,1PLAEC )
Ifartifactevaluationdoesnothaveanyinfluenceontheaccep-
tanceofapaper,reviewersstrugglewiththeincongruousnessbe-
tweenartifactevaluation’smissionstatementofreplicableresearch
andthe currentpractice.One participant reports:
Theevaluationsdonotdetermineifthepaperisacceptedorrejected,
so in the bigger picture, I didn’t much attentionto thesereviews.
(id 79, 1PLAEC )
Inconclusion,thecommunitysuggestsmorerigorbyintegrating
artifact evaluation muchstronger intothe paper reviewprocess.
6 THREATS TO VALIDITY
6.1 InternalValidity
The recollection of our participants’ experiences with artifacts can
be affected by the time span between their occurrence and the
participation in our survey. Moreover, the perceived purpose of
artifactsandtheprocessesoftheirassessment havechangedover
time. Figure 1shows that AEC members from every year since
the initiation of artifact evaluations have participated and most
participants have served in recent years. We, therefore, do not
expecteffectsoftime to significantly affectour results.
Thesecondthreatisrelatedtoourparticipantselection.Tore-
ceiveopinionsfromresearchersfamiliarwithartifactevaluation,we
only invited AEC members to our survey. Their responsesmay be
affectedbyconcernsabouttheperceivedvalueoftheirwork,which
canleadtooverlypositivereportsregardingacceptedartifactsor
overlynegativereportsregardingrejectedornotevaluatedartifacts.
We addressed this by openly communicating the anonymization
478Community ExpectationsforResearchArtifacts andEvaluation Processes ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA
policy of our study. Moreover, the main results we report are re-
latedtotheexpectationstowardartifactsratherthanthepositive
or negative experiences they have led to. Consequently, we expect
the centralconclusions to not be affectedbythis threat.
Toassesstheeffectivenessofourinstruments,weconductedpre-
testingwith6participants.Ifthissampleisnotrepresentativeof
how the targeted audience perceives our questions, this can induce
systematic effects in our results. Two participants in the pre-test
had no experience with artifact evaluations and commented on
thefirstdraftversionofoursurvey.Theremainingfourpre-tests
have been conducted by experienced researchers. While we cannot
rule outeffects onour resultsinprinciple, mostofthepre-testers’
comments were inline andwe have not seensymptoms ofsevere
misunderstandingsintherepliesbeyondwhatiscausedbydiffering
understandingsofparticipants (whichwe intendedto capture).
6.2 External Validity
A threat to the external validity of our study lies in the selection
of participants. Our goal is to assess the community’s expectation
toward artifacts. With our focus on AEC members, we potentially
createabiastowardspecificexpectations.Artifactevaluationsarea
relativelynewscientificpeerreviewprocessand,assuch,aminority
ofresearchershaveworkingexperiencewithartifactsorknowledge
oftheACMguidelines.We,therefore,gavepreferencetotheriskof
selection biasover the riskofour questionsbeing misunderstood.
Three of our pre-testers received invitations to participate in
the survey and their replies may have been affected by the pre-test.
Wedeemthisrisktolerable,astheinfluenceofthreeresponsesis
marginalto the presentedresults from257 replies.
7 IMPLICATIONS
In our view, artifact evaluation chairs, steering committees, and
communityleaderscantakeseveralactionstoaddresstheissues
highlightedinthis paper.
First, as a community, we have to define the purpose ofartifact
evaluation clearer than we do now. As our discussion showed, a
twofold purpose of replicability and reusability has several pitfalls
even though the two goals share certain characteristics. A commit-
tee might be instated for each community that defines a clear goal
specifictotherespectivecommunity.Thiscommitteeshouldevalu-
ate changes over the course of time and observe the improvements
made(e.g.,interms ofmore submissionsto artifact tracks).
Second, we propose to also work on agreed quality standards in
each community in a similar manner. As we have shown, expected
artifact quality criteria vary widely between communities, perspec-
tives, and artifact types. Moreover, there is a need to communicate
quality criteria very clearly. Misconceptionsabout appropriate ac-
ceptance levels seem to be common during artifact evaluation and
can leadto serious conflictsas one respondentpointedout.
Two students argued for acceptance of the artifact because it was
capableofgeneratingoutputwithoutcrashing insomescenarios. I
arguedstronglyagainstthemandtheartifactwaseventuallyrejected.
[...] (id 265,1SE AEC )
As most reviewers only serve once on an AEC (cf. Figure 2) chairs
should explicitly brief AEC members on appropriate acceptance
levels and quality criteria. It would not be beneficial to push thesespecificcriteriaintoadiscipline-widedocumentsuch astheACM
guidelines.Rather,CfAsshouldbeextendedtoincorporatethose
criteria. As it seems to be common practice that chairs łinheritž
those CfAs from their predecessors, the quality criteria can evolve
over time and reflect community transitions in a fine-grained way
thatisalsotrackableforevaluationssuchastheonepresentedhere.
However, community representatives should monitor if a common
core can be established within a community, which we consider
mostlikely from our results.
Third, recruiting reviewers also based on their experience as
artifactcreators maynot onlybenefitthequalityandefficiency of
artifact reviews, but also improve peer consultation for new and
less experiencedreviewers.
Furthermore,artifactreviewersclearlyfeltthatthetimehascome
for a tighter coupling between artifact evaluation and paper accep-
tance.Conferencesteeringcommitteesandtrackchairsshouldtake
the opportunity to incorporate artifact evaluation into acceptance
processes. Tool-oriented tracks could take the lead (as TACAS and
CAV show) and other tracks could follow closely learning from the
experience gainedinthe pastdecade.
8 CONCLUSION
The replicability crisis shook the research community, and also
reachedthesoftwareengineeringandprogramminglanguagecom-
munity. A recent attempt to mitigate its rippling through the com-
munities has manifested in the creation of artifact tracks at con-
ferences. However, at this point it is still unclear if or to which
degree artifact evaluations are a suitable measure to foster repli-
cable,letalonereproducible,research.Takingafirststeptoward
an assessment of artifact evaluations, we inspected how the pro-
cess is currently seen by the community, specifically by the people
whoperformtheseevaluations.Wefoundthattheinitialmission
ofartifactevaluationofassessingreplicabilityhasnowgrownto
also cover reusability. However, reviewers and users of artifacts
see a different purpose of artifacts: Reviewers want replicability,
userswantreusability.Additionally,differentartifacttypeselicit
different expectations, but they all more or less focus on making
artifacts understandableand usable.Now,we as the researchcom-
munityneedtoclearlycommunicatetheseexpectationsincallsand
guidelines while carefully defining and using terminology to avoid
misunderstandingsorfalseexpectations.Thisisoneapproachto
avoid a replicability crisis of the same large extent as it washed
over the psychology community.
ACKNOWLEDGMENTS
We are very grateful to all members of the community that took
timeoutoftheirbusydaysandrepliedtooursurvey.Theyprovided
uswithdetailedinformationanddiverseviewsonthecurrentstate
of artifact evaluation processes. Oliver Schwahn, Habib Saissi, Lisa
NguyenQuangDo,NorbertSiegmund,andtwoothercolleagues
provideduswithvaluablefeedbackduringthepre-testofoursurvey.
We thank the anonymous reviewers for their helpful comments
which improved this paper We greatly appreciated inputs from Jon
Bell and Tim Menzies that significantly improved the final version
ofthis paper.
479ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA Ben Hermann,Stefan Winter, andJanet Siegmund
REFERENCES
[1]MonyaBaker.2016. 1,500scientistsliftthelidonreproducibility. NatureNews
533,7604(2016),452. https://www.nature.com/news/1-500-scientists-lift-the-
lid-on-reproducibility-1.19970
[2]Anna Balazs. 2008. International vocabulary of metrology-basic and general
concepts and associated terms. Chemistry International (2008), 20ś1. https:
//doi.org/10.1515/ci.2008.30.6.21
[3]Victor Basili, Forrest Shull, and Filippo Lanubile. 1999. Building Knowledge
throughFamiliesofExperiments. IEEETrans.Softw.Eng. 25,4(1999),456ś473.
https://doi.org/10.1109/32.799939
[4]Emery D.Berger,CelesteHollenbeck,PetrMaj,Olga Vitek,and JanVitek. 2019.
On the Impact of Programming Languages on Code Quality: A Reproduction
Study.ACM Trans. Program. Lang. Syst. 41, 4, Article 21 (Oct. 2019), 24 pages.
https://doi.org/10.1145/3340571
[5]KarlBroman,MineCetinkaya-Rundel,AmyNussbaum,ChristopherPaciorek,
Roger Peng, Daniel Turek, and Hadley Wickham. 2017. Recommendations to
funding agencies for supporting reproducible research. https://www.amstat.
org/asa/files/pdfs/POL-ReproducibleResearchRecommendations.pdf . Accessed:
2020-09-03.
[6]B. R. Childers and P. K. Chrysanthis. 2017. Artifact Evaluation: Is It a Real
Incentive?. In 2017 IEEE 13th International Conference on e-Science (e-Science) .
488ś489. https://doi.org/10.1109/eScience.2017.79
[7]Christian Collberg and Todd A. Proebsting. 2016. Repeatability in Computer
SystemsResearch. Commun.ACM 59,3(Feb.2016),62ś69. https://doi.org/10.
1145/2812803
[8]Erin Dahlgren. 2019. Getting Research Software to Work: A Case Study on
Artifact Evaluation for OOPSLA 2019. https://doi.org/10.5281/zenodo.4016657
[9]Association for Computing Machinery. 2018. Artifact Review and Badging.
https://www.acm.org/publications/policies/artifact-review-badging . Accessed:
2020-09-03.
[10]Leonid Glanz, Sven Amann, Michael Eichberg, Michael Reif, Ben Hermann,
Johannes Lerch, and Mira Mezini. 2017. CodeMatch: Obfuscation Won’t Conceal
YourRepackagedApp.In Proceedingsofthe201711thJointMeetingonFoundations
of Software Engineering (ESEC/FSE 2017) . Association for Computing Machinery,
NewYork, NY, USA,638ś648. https://doi.org/10.1145/3106237.3106305
[11]Matthias Hauswirth. [n.d.]. Artifact Evaluation. http://evaluate.inf.usi.ch/
artifacts. Accessed 2020-09-03.
[12]BenHermann,StefanWinter,andJanetSiegmund.2020. CommunityExpectations
forResearchArtifactsandEvaluationProcesses- Data& Scripts .https://doi.org/
10.5281/zenodo.3951724[13]Robert Heumüller, Sebastian Nielebock, Jacob Krüger, and Frank Ortmeier. 2020.
Publish or Perish, but do not Forget your Software Artifacts. Empirical Software
Engineering (2020).https://doi.org/10.1007/s10664-020-09851-6 Preprint.
[14]WilliamHudson.2013. CardSorting. In TheEncyclopediaofHuman-Computer
Interaction . The Interaction Design Foundation, Chapter 22.
[15]Natalia Juristo and Sira Vegas. 2011. The Role of Non-exact Replications in
SoftwareEngineering Experiments. EmpiricalSoftware Engineering 16,3 (2011),
295ś324. https://doi.org/10.1007/s10664-010-9141-9
[16]Shriram Krishnamurthi and Jan Vitek. 2015. The Real Software Crisis: Re-
peatability As a Core Value. Commun. ACM 58, 3 (Feb. 2015), 34ś36. https:
//doi.org/10.1145/2658987
[17]J. Lung, J. Aranda, S. Easterbrook, and G. Wilson. 2008. On the difficulty of
replicatinghumansubjectsstudiesinsoftwareengineering.In 2008ACM/IEEE
30th International Conference on Software Engineering . 191ś200. https://doi.org/
10.1145/1368088.1368115
[18]Daniel Méndez Fernández, Wolfgang Böhm, Andreas Vogelsang, Jakob Mund,
ManfredBroy,MarcoKuhrmann,andThorstenWeyer.2019.Artefactsinsoftware
engineering:afundamentalpositioning. Software&SystemsModeling 18,5(2019),
2777ś2786.
[19]DanielMéndezFernández,MartinMonperrus,RobertFeldt,andThomasZim-
mermann. 2019. The open science initiative of the Empirical Software En-
gineering journal. Empirical Software Engineering 24, 3 (2019), 1057ś1060.
https://doi.org/10.1007/s10664-019-09712-x
[20]Gregorio Robles. 2010. Replicating MSR: A study of the potential replicability of
paperspublishedintheMiningSoftwareRepositoriesproceedings.In 20107th
IEEEWorkingConferenceonMiningSoftwareRepositories(MSR2010) .171ś180.
https://doi.org/10.1109/MSR.2010.5463348
[21]ForrestJShull,JeffreyCCarver,SiraVegas,andNataliaJuristo.2008. Theroleof
replications in Empirical SoftwareEngineering. EmpiricalSoftware Engineering
13,2 (2008), 211ś218. https://doi.org/10.1007/s10664-008-9060-1
[22]Janet Siegmund, Norbert Siegmund, and Sven Apel. 2015. Views on Internal
and External Validity in Empirical Software Engineering. In 2015 IEEE/ACM
37thIEEEInternationalConferenceonSoftwareEngineering ,Vol.1.9ś19. https:
//doi.org/10.1109/ICSE.2015.24
[23]Christopher S. Timperley, Lauren Herckis, Claire Le Goues, and Michael Hilton.
2020. UnderstandingandImprovingArtifactSharinginSoftwareEngineering
Research. arXiv: cs.SE/2008.01046
[24]ChatWacharamanotham,LukasEisenring,SteveHaroz,andFlorianEchtler.2020.
TransparencyofCHIResearchArtifacts:ResultsofaSelf-ReportedSurvey.In
Proceedingsofthe 2020CHIConferenceonHumanFactorsinComputingSystems
(CHI ’20). Association for Computing Machinery, New York, NY, USA, 1ś14.
https://doi.org/10.1145/3313831.3376448
480