Understanding Performance Concerns in the API
Documentation of Data Science Libraries
Yida Tao
College of Computer Science and
Software Engineering
Shenzhen University, China
yidatao@szu.edu.cnJiefang Jiang
College of Computer Science and
Software Engineering
Shenzhen University, China
jiangjiefang2018@email.szu.edu.cnYepang Liu
Department of Computer Science and
Engineering
Southern University of Science and
Technology, China
liuyp1@sustech.edu.cn
Zhiwu Xu
College of Computer Science and
Software Engineering
Shenzhen University, China
xuzhiwu@szu.edu.cnShengchao Qin∗
School of Computing, Engr. & Digital
Technologies, Teesside University, UK
College of Computer Sci. & Software
Engr., Shenzhen University, China
s.qin@tees.ac.uk
ABSTRACT
The developmentof efficient datascience applications is oftenim-
peded by unbearably longexecution time and rapid RAM exhaus-
tion.SinceAPIdocumentationistheprimaryinformationsource
for troubleshooting, we investigate how performance concerns are
documented in popular data science libraries. Our quantitative re-
sultsrevealtheprevalenceofdatascienceAPIsthataredocumented
in performance-related context and the infrequent maintenance
activities on such documentation. Our qualitative analyses further
reveal that crowd documentation like Stack Overflow and GitHub
arehighlycomplementarytoofficialdocumentationintermsofthe
API coverage, the knowledge distribution, as well as the specific
information conveyedthrough performance-relatedcontent. Data
sciencepractitionerscouldbenefitfromourfindingsbylearning
amoretargetedsearchstrategyforresolvingperformanceissues.
Researcherscanbemoreassuredoftheadvantagesofintegrating
both the official and the crowd documentation to achieve a holistic
view on the performance concerns in data science development.
KEYWORDS
API documentation, performance, data science, empirical study
ACM Reference Format:
YidaTao,JiefangJiang,YepangLiu,ZhiwuXu,andShengchaoQin.2020.Un-
derstanding Performance Concerns in the API Documentation of Data Sci-
enceLibraries.In 35thIEEE/ACMInternationalConferenceonAutomatedSoft-
ware Engineering (ASE ’20), September 21–25, 2020, Virtual Event, Australia.
ACM,NewYork,NY,USA,12pages.https://doi.org/10.1145/3324884.3416543
∗Shengchao Qin is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ASE ’20, September 21–25, 2020, Virtual Event, Australia
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-6768-4/20/09...$15.00
https://doi.org/10.1145/3324884.34165431 INTRODUCTION
Data science is an emerging field that combines mathematics, sta-
tistics, machine learning, and domain knowledge to derive insights
fromdata[ 18].Asthevolumeandcomplexityofdatagrowrapidly,
the painfully slow execution time and quickly exhausted RAM are
becomingthemajorbottlenecksfordevelopingefficientdatasci-ence applications [
30]. Since external data science libraries such
asPandas,NumPy, and TensorFlow are often used in these applica-
tions,theirgoodperformanceisalsovitalforimprovingapplication
efficiency and developer productivity.
Nevertheless,weobservepersistentandactivediscussionsonthe
performance problems of data science libraries. Take the Pandas li-
braryforexample.AsofApril2020,thereare853questionsonStack
Overflowthataretaggedwithboth pandasandperformance,and
togetherthesequestionshaveoveronemillionviews.Furthermore,theansweracceptancerateforthesequestionsis58%,withanaver-
age of 41 days between the creation of questions and the proposal
of accepted answers. On the GitHub repository of Pandas, 1,113
issues labeled with performance have been reported. While 83% of
theseissuesareclosed,theaverageresolutiontimeisupto134days.
These observations imply that data science developers may suffer
from recurring interruptions caused by intractable performanceproblems.Existingworkhasalsorevealedperformanceissuesin
other data science libraries that cause runtime inefficiency [ 31,42].
When data science developers are troubleshooting performance
problems, documentation will likely be their first resort. For this
reason, understanding how performance-related concerns are doc-
umented in data science libraries is crucial for steering effective
performanceoptimizations.Whilepreviousstudieshaveexplored
the quality [ 36], accessibility [ 23], and knowledge types [ 27]o f
API documentation, none of them targets on data science libraries
andfewfocusontheperformanceaspects.Tobridgethisgap,we
conduct an empirical study on popular data science libraries to
understand the documentation practice on performance concerns,
whichdenotedescriptionsanddiscussionsrelatedtoruntimespeed
and memory consumption. In addition to the official API docu-
mentation maintained by library developers, we also consider two
8952020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)
.py
 .md
 .rst
 .ipynbAPI Docstring
+           
User GuideStack Overflow
+           
GitHub
ŘPerformance Concerns Extraction & API Linking
Performance concerns in 
official documentationPerformance concerns in 
crowd documentation
řKnowledge Classification śEvolution Analysis ŚConsistency Analysis
Evolution patterns of 
performance-related 
documentationKnowledge types of 
performance-related 
documentationConsistency between 
official and crowd 
documentation
Figure 1: Overview of our methodology.
typesof“crowd”documentation,StackOverflow(SOforshort)and
GitHub, whicharewell-recognized resources fortroubleshooting
development problems [ 12,14,43]. The research questions we aim
to address include:
•RQ1: How common are data science APIs documented in
performance-relatedcontext?Howdoesperformance-related
documentation evolve over time?
•RQ2:Whattypesofknowledgeareprovidedbyperformance-
related documentation?
•RQ3:Whatarethedifferencesbetweentheofficialandcrowd
documentation in terms of performance-related content?
OursubjectsincludesixwidelyusedlibrariesfromthePython
datascienceecosystem(Table1).Weleveragedprogramanalysis,
natural language processing routines, and traceability heuristics
to automatically extract performance-relatedsentences from doc-
umentation and establish links between the sentences and corre-
spondingAPIs.Weanalyzedtheextracteddatatostudythestatistics
ofAPIsthataredocumentedinperformance-relatedcontext.We
also qualitatively analyzed the extracted performance concerns to
understand their knowledge types, evolution reasons, and content
consistency. Our major findings include:
•From 8.7% to 30.5% APIs of the target libraries have been
documented in performance-related context, indicating the
prevalenceofperformanceconcernsfordatasciencelibraries.
•Performance concerns in crowd documentation covers a dif-
ferentsetofsubjectAPIscomparedtoofficialdocumentation;
only<5% APIs have been mentioned in both data sources.
•Officialdocumentationfrequentlyprovidesthe functionknowl-
edge (e.g., the runtime efficiency of a certain functionality),
whilecrowddocumentationfocusesmoreon practicesand
alternatives types of knowledge. SO rarely provides explana-
tory knowledge types such as purpose & rationale.
•The vast majority (86.3%) of performance concerns from the
crowddocumentationprovidenewinformationthatisnot
present in the official documentation; only a very few (2.7%)
are inconsistent with the official documentation.
•Performance concerns in the official documentation are typ-
ically added long after the introduction of corresponding
APIs, with an average of 0.48 follow-up updates that consist
mostly of trivial semantic changes.Table 1: Dataset statistics.
Library Version # APIs #S O# GitHub
threads issues
NumPy 1.16.4 81364,518 14,794
Pandas 0.24.2 1,828118,636 29,387
SciPy 1.2.1 2,008 14,583 10,844
Scikit-learn 0.21.2 1,622 16,181 15,464
TensorFlow 1.13 3,635 46,142 33,738
Gensim 3.7.3 1,016 1,512 2,663
In general, we empirically analyzed how performance concerns
on data science APIs are documented in terms of their occurrences,
knowledge distributions, information consistencies, and evolution
patterns. Our results further reveal how official documentation
andcrowddocumentationcomplementeachotherinprovidinga
holistic view of data science performance issues. We hope that our
work,byprovidingabetterunderstandingofthedocumentation
practice on performance concerns, will benefit both data science
practitionersandresearchersintheirbattlesagainstperformance
bottlenecks. Our data are released for public usage [41].
2 METHODOLOGY
Inthissection,wepresentourmethodologyindetail.Theoverall
workflow is shown in Figure 1.
2.1 Data Collection
2.1.1 TargetLibraries. AsshowninTable1,wefocusonsixwidely-
used libraries, all of which are core packages in the Python ecosys-
tem for data science. These libraries solve different classes of prob-
lems and together support various important tasks in data science.
Inparticular,NumPyandPandasprovidefunctionalitiestowork
with arrays and tabular data [ 3,4]; SciPy and Scikit-learn pro-
videcommonnumericalroutinesforstatisticsandmachinelearn-
ing [8,9]; TensorFlow is a general-purpose deep learning frame-
work [19] while Gensim is primarily used for natural language
processing tasks [1].
2.1.2 OfficialDocumentation. OfficialAPIdocumentationistypi-
cally authored and maintained by library developers. We consider
two types of official documentation: the API docstrings and the offi-
cialuserguide.APythondocstringisastringliteral,surroundedby
tripledoublequotes,thatappearsasthefirststatementofanAPI
definition.Developersoftenusedocstringstoexplainthegeneral
purpose of an API and how it should be used. Users can invoke
Python’sbuilt-in help()functiontocheckthedocstringofagiven
API [6]. To collect the docstring data, we perform a depth-first tra-
versalontheabstractsyntaxtreebuiltfromtherootmoduleofeach
target library to extract all the public APIs (i.e., classes,functions
andattributes ), as shown in Table 1. We then leverage Python’s
introspectioncapability[ 7]toobtainthefilepath,linerange,and
textual content of the docstrings for each API.
While docstrings provide succinct API-level descriptions, the
official user guide provides a more high-level overview of a library.
Usersmayfindvarioustypesofinformationinauserguide,suchas
the basic concepts and algorithms used in the library, the tutorials
896and caveats on API usages, the guidance for extending the library,
and so on. Compared to API docstrings, a user guide is more flexi-
ble in terms of the content, syntax, and structures. To collect the
userguidedata,wetraversethesourcedirectoriesofeachlibrary
recursivelytosearchforfileswiththeformatofreStructuredText
(.rst),Markdown( .md)andJupyternotebook( .ipynb).Thefor-
mer two file formats correspond to light-weight markup languages
widely used for technical documentation, while Jupyter Notebook
is an interactive computing interface often used for developing,
debugging, and demonstrating data science applications [5].
2.1.3 Crowd Documentation. As online Q&A forums and social
codingsitesarereshapingtheknowledgesharingindevelopercom-
munities, research has proposed that sites like Stack Overflow and
GitHubcanbeleveragedasessentialcomplementstoofficialAPI
documentation[ 32,39,43,44].For thisreason,crowd documenta-
tion is also considered as an important data source in our study.
We used the official Stack Exchange REST API [ 11] and GitHub
REST API [ 2] to crawl SO threads and GitHub issues of our target
libraries.NotethatanSOthreadincludesaquestionpostandall
ofitsanswerpostsandcomments,whileaGitHubissueincludes
anissuedescriptionandfollow-upcomments.Table1showsthe
details of our dataset.
2.2 Extraction of Performance Concerns and
API Linking
Given the collected documentation, we identify performance con-
cernsatthesentencelevelbyfirstapplying spaCy[10]forsentence
segmentation. We then formulate a list of performance-relatedkeywords based on literature [
37] and common knowledge. The
keywords include fast, slow, expensive, cheap, performance, speedup,
computation, accelerate, intensive, scalable, efficient and their in-
flections (e.g., efficiency, efficiently ).1Sentences containing these
keywords are extracted as candidates that possibly discuss perfor-mance concerns.
Next, we identify the subject APIs that are being discussed in
eachcandidatesentence,aproblemoftenreferredtoas APItraceabil-
ity[16].Inthiswork,weusedthe declaration-based, hyperlink-based,
andmention-based heuristics proposed in [ 23] to establish links be-
tween natural-language sentences and APIs. First, sentences from
an API’s docstring are directly linked to this particular API (i.e.,
declaration-based ).Second,sentencesfromtheofficialuserguide
and crowd documentation are linked to an API if they contain hy-
perlinkstothatAPI’sreferencepage(i.e., hyperlink-based ).Third,
weleverageregularexpressionsandthehtmltag <code>toextract
explicitcodementionsfromcandidatesentencesintheofficialuser
guideandcrowddocumentation(i.e., mention-based ).Notethata
codementionmaycontainexpressionsorstatements.Tofurther
identify the exact API(s) referred to in a code mention, we traverse
itsabstractsyntaxtreetosearchfor CallandAttribute nodes.We
then resolve each of these nodes to its fully qualified API names bymatchingitagainstAPIsofthesubjectlibrary,whichisdeterminedbySOtagsandtheidentifierofthereceiverobject.Forexample,for
acodemention [row[’a’] for _,row in df.iterrows()] ,w e
first extract iterrows as a function call. Since the code mention
1Inflection denotes different forms (e.g., noun, adjective, adverb) of a word.is froman SO threadtagged with pandasand theidentifier dfis a
common abbreviationfor pandas.DataFrame , wefinally link it to
thepandas.DataFrame.iterrows API. Sentences that cannot be
linked or resolved to any of our target APIs are removed.
Note that sentences containing both the performance-related
keywords and proper API links may not always address perfor-
manceconcerns.Forexample,“thechoiceofddofisunlikelytoaffect
modelperformance ”f r o mt h e sklearn.preprocessing.scale
docstringdiscussestheperformanceofmachinelearningmodels
(e.g., accuracy) rather than runtime performance and memory con-
sumption.Thesentence“convolvein1andin2usingthe FastFourier
Transformmethod ”fr om scipy.signal.fftconvolve isalsonot
related to performance although it includes the keyword “fast”.
Toeliminatesuchfalsepositives,wemanuallyvalidatedwhether
the automatically extracted candidates truly address API perfor-
manceconcerns(seeSection3.2fordetailsofthemanualvalidation).Onlytruepositivesentenceswereconsideredasvalidperformance-
related documentation and used in subsequent analyses.
2.3 Knowledge Classification
After identifying performance-related documentation, we first ana-
lyzed the different types of knowledge provided by such documen-
tation.Tothisend,wereferredtotheworkofMaalejandRobillard,
whichproposedataxonomyof12knowledgetypesderivedfrom
theAPIdocumentationofJavaSDK6and.NET4.0[ 27].Whilethis
taxonomyhasbeendevelopedtobemeaningfulandreliable,itis
unclearwhetheritcanbedirectlyappliedtoourtask.Amajorcon-
cern is that the taxonomy was originally proposed to characterize
general API documentation, whereas our goal is to characterize
documentation specific to performance concerns.
Toaddressthisissue,weappliedthe inductivecoding method[13]
to better adapt the taxonomy to our performance-specific data. Ini-
tially, the first author used the original taxonomy to classify 100
randomlyselectedperformance-relatedsentences.As partofthis
process, the first author removed existing knowledge types that
are no longer applicable and added new knowledge types emerged
from our data. The second author then repeated this process us-
ingtheadjustedtaxonomy.Thethirdauthorhelpedreconcilethedisagreementoncethefirsttwoauthorshaddifferentopinionsof
thetaxonomy.Thisprocesscontinueduntilthenewtaxonomywas
fixed.
Table 2 shows the details of our new taxonomy, which includes
11knowledgetypes.Amongwhich,sixaredirectlyreusedfrom[ 27]
and five are newly emerged from our data. In particular, we iso-
lated the “Performance Attributes” knowledge type from the orig-
inal “Quality Attributes and Internal Aspects” knowledge type.
We merged “Internal Aspects” with the original “Control-Flow”
and “Structure” types to form the new “Implementation or Inter-
nal Aspects” knowledge type. We introduced the “Usage Practice”
knowledgetype,whichcombinestheoriginal“Patterns”and“Code
Examples”typessincebothdescribehowAPIsshouldbeusedin
practice.Wealsoaddedthe“Alternatives”knowledgetypethatis
observedexclusivelyfromourperformance-specificdata.Accord-
ingly, each performance-related sentence was labeled with one or
more knowledge types from this taxonomy.
897Table 2: Taxonomy of knowledge types in performance-related documentation.
Knowledge Type Description Example
Performance
AttributesDescribestheperformanceofanAPI,ortheper-
formanceimplicationsofdifferentargumentsor
dataset.Setting to False will improve the performance of this
method. (pandas.DataFrame.set_index)
Function ality &
Behavior*Describes what the API does (or does not do) in
terms of functionality or features.TheSeries.align method is the fastest way to simulta-
neously align two objects. (pandas.Series.align)
Implementationor
Internal AspectsExplains an API’s internal implementation that is
only indirectly related to its observable behavior....internallythisversionusesamuchfasterimplemen-
tationthatneverconstructstheindicesandusessimple
slicing. (numpy.fill_diagonal)
Purpose & Ratio-
nale*Explainsthepurposeofprovidinganelementor
the rationale of a certain design decision.Thevectorize function is provided primarily for conve-
nience, not for performance. (numpy.vectorize)
Alternatives Describes a more/less efficient alternative of an
API,orcompares theperformancebetweenalter-
native APIs.Mini-batch sparse PCA MiniBatchSparsePCA is a vari-
ant of SparsePCA that is faster but less accurate
(sklearn.decomposition.MiniBatchSparsePCA)
UsagePractice Describes common usage scenarios for an API.
Describesgood,recommendedorbadAPIusage
practices.To construct a matrix efficiently, make sure
the items are pre-sorted by index, per row.
(scipy.sparse.lil_matrix)
Concepts* Explains the meaning of terms used to name or
describe an API element, or describes design or
domainconceptsusedorimplementedbytheAPI.Themain theoreticalresult behindthe efficiencyof ran-
dom projection is the Johnson-Lindenstrauss lemma.
(sklearn.random_projection)
Directives* Specifies what users are allowed / not allowedto do with the API element. Directives are clear
contracts.We do require that your array be convertible to a
NumPy array, even if this is relatively expensive.
(pandas.api.extensions.ExtensionArray)
Environment* Describes aspects related to the environment in
which the API is used.These modes will have different performance profiles
on different hardware and for different applications.
(tensorflow.keras.layers.LSTMCell)
References* Includes any pointer to external documents, ei-ther in the form of hyperlinks, tagged “see also”
reference, or mentions of other documents.See the “enhancing performance” documentation for
more details. (pandas.eval)
Miscellaneous Includes other information such as TODOs and
changes.(TODO) Replacing or improving the perfor-
mance of this would greatly speed things up.
(gensim.models.ldaseqmodel.sslm.update_obs)
*Knowledge types proposed in [27].
2.4 Consistency Analysis
In addition to knowledge types, we also aim to understand the
differences between official and crowd documentation in terms of
the specific performance-related information they provide. To this
end,wefirstmatchedAPIlinkstoassociateperformance-related
sentencesfromthecrowddocumentationwiththosefromtheof-ficialdocumentationthatdiscussthesamesubjectAPIs.Next,by
comparing with the performance concerns in the corresponding
officialdocumentation,wemanuallyclassifiedeachperformance-
relatedsentencefromthecrowddocumentationas1) consistent,ifit
providesthesameorconsistentinformationastheofficialdocumen-
tation;2) inconsistent,ifitprovidesinconsistentorcontradictory
informationastheofficialdocumentation;or3) notofficiallydoc-
umented,iftheinformationitprovidesisnotfoundintheofficial
documentation.
It is worth noting that we originally planned to apply text simi-
laritytechniquestoautomatetheabovementionedclassifications.
However, our preliminary experiment revealed that text similarity
algorithmsperformedpoorlyondistinguishinginformationcon-sistency,especiallywhenthetargetsentencesarealreadyhighlysimilar in terms of their subjects (i.e., they all discuss performance
concerns on the same APIs). For example, two sentences with high
similarityscoresmightconveycompletelycontradictoryideas(e.g.,
“This API is very fast ” and “This API is very slow ”). Hence, we re-
sortedtomanualinspectionforamoreaccurateanalysis.Similar
to that described in Section 2.3, the classification was conducted
independentlybythefirsttwoauthorsanddiscussedwiththethird
author in case of discrepancies.
2.5 Evolution Analysis
Apartfromtheknowledgetypesandinformationconsistency,char-
acterizing how library developers evolve performance concerns is
also important for understanding their documentation practice. In
this study, we are interested in knowing when performance con-
cernsareaddedtothedocumentationandhowtheyarechanged
over time. For this purpose, we analyzed the commit history ofperformance-related sentences from the official documentation.
Specifically,wecharacterizedthesecommitsintermsof1)whether
a performance concern is added to the documentation at the same
time its subject API is introduced to the source code; 2) how often
898Table 3: The number of performance-related sentences and corresponding APIs discovered from each data source.
LibraryDocstring User Guide Stack Overflow GitHub
# perf. # APIs # perf. # APIs # perf. # APIs # perf. # APIs
sent. sent. sent. sent.
NumPy 84 54 35 21 147 113 118 135
Pandas 65 53 115 79 164 96 150 121
SciPy 247 144 43 43 150 99 94 88
Scikit-learn 232 134 134 97 75 61 96 72
TensorFlow 161 120 67 50 122 102 97 93
Gensim 75 53 72 41 41 24 39 34
Total 864 558 466 331 699 495 594 543
performance-related concerns in the official documentation are
modified; and 3) why a performance concern is modified.
We developed an algorithm that leverages the git logcommand,
using the file path and line range as parameters, to extract all past
commitsofagivenperformance-relatedsentence.Thealgorithm
uses similarity-based heuristics to automatically distinguish be-
tween the commit that addsthe target sentence and the commit
thatmodifiesit. For a commit that adds a performance-related sen-
tence, the algorithm checks whether it also contains the source
file of the subject API and whether the API’s definition is added to
thisfileinthiscommit.Ifso,thecommitisclassifiedas addedto-
getherwithAPIs,meaningthattheAPIanditsperformance-related
documentationareaddedandcommittedtogether;otherwise,the
commitisclassifiedas addedlater,meaningthattheperformance
concernisaddedlateraftertheAPIitreferstohasbeenintroduced
in previous commits. In this case, we also analyzed the duration
betweentheadditionoftheAPIandtheadditionofitsperformanceconcern.NotethatthetimestampofanAPI/documentationaddition
was also extracted from the output of git log.
For a commit that modifies a performance-related sentence, we
manuallycharacterizedtherationaleofthechange.WereferredtoapreviousworkthatstudiedAPIdocumentationevolution[
38]toini-
tiate the taxonomy of evolution reasons. However, as this previous
work focused on the general documentation of Java programs [ 38],
itstaxonomycouldnotbedirectlyappliedtoourdataset.Therefore,
we adjusted this taxonomy through an inductive coding process
similar to that described in Section 2.3. As shown in Table 7, the
adjusted taxonomy includes 11 reasons for updating performance
concerns, six were rephrased from the findings of [ 38] and five
were newly emerged from our data.
3 EVALUATION
In this section, we report the evaluation results on each step of our
methodology.
3.1 Extracted Performance Concerns
UsingtheapproachdescribedinSection2.2,weextracted2,017sen-tencesthatcontainperformance-relatedkeywordsfromtheofficialdocumentation.Amongwhich,1,330weremarkedastruepositives
fortrulyaddressingAPIperformanceconcerns.Asforthecrowd
documentation,weinitiallyextracted18,307candidatesentences.
Tomakethemanualanalysisfeasiblewhilestatisticallymeaning-
ful,werandomlyselected1000sentencesfromSOand1000fromGitHub for further processing. It is worth noting that the num-ber of extracted candidate sentences for each library is different,
with that of Gensim being the lowest. To balance the number of
selectedsentencesacrosslibraries,wesettheprobabilityofbeing
randomly selected to be 0.2 for Gensim and 0.16 for the remaining
five libraries. Among the 2000 candidate sentences from the crowd
documentation, we identified 1,293 true positive sentences thatindeed discuss performance concerns of our target APIs. Table 3
showsthenumberofperformance-relatedsentencesidentifiedfrom
each data source.
3.2 Manual Classifications
Ourmethodologyrequiresmanualeffortsinfourtasks,whichin-
clude classifying the validity, the knowledge type, the informationconsistency and the evolution reasons of performance-related doc-
umentation. Manual work is crucial for identifying the complex
nuances of natural language that often cannot be distinguished au-
tomatically.Forexample,sentencescontainingperformance-related
keywordsmaynotreallyaddressperformanceconcerns,andsen-
tences with high similarity scores may not convey similar ideas
(see examples from Section 2.3 and 2.4). Also, for text classification
tasks,especiallythosenewly-proposedanddomain-specificones
(e.g.,classifyingtheknowledgetypeofperformanceconcerns,as
proposed in this paper), manual labeling is often necessary before
any automated method is attempted [20, 27].
Admittedly, manual classification is inherently subjective. To
address this problem,each sentence was independently evaluated
bythefirstandthesecondauthorasdescribedinSection2.Here
wereporttheCohen’skappacoefficient,whichmeasurestheagree-
mentbetweentworatersonascaleof0–1[ 28].Thekappavalue
for each of the four tasks is 0.89, 0.61, 0.71 and 0.63, respectively,
meaningthatthetworatershadalmostperfectagreement(kappa ∈
[0.81,1]) for labeling the validity of performance-related sentences
and substantial agreement (kappa ∈[0.61–0.8]) for labeling the
knowledge type, information consistency and evolution reasons.
Disagreementswerereconciledwiththethirdauthorjoiningthe
discussion.
3.3 API Linking and Commit Categorization
Whenweinspectedeachsentencetolabelitsvalidityandknowl-
edgetypes,wealsocheckedthecorrectnessandcompletenessof
itsAPI linking,whichwas automaticallydeterminedasdescribed
inSection2.2.TheprecisionandrecallofourAPIlinkingapproach
899Figure 2: Percentage of APIs with performance concerns
that are discovered from the official documentation, thecrowd documentation, and from both data sources.
is 81.2% and 93.5%, respectively. False positives were mostly dueto name ambiguity, when a name was incorrectly resolved to an
API with different fully qualified name but having the same simple
name. False negatives were mainly due to the absence of code-
likecharacteristics.Forexample,ourapproachcannotextractthe
sumAPI when it appears as a common english word in a sentence
without the <code>tag or API-indicating symbols like ()in its
surroundings. Since reliable API linkings are essential for studying
APIstatistics(Section4.1)andassociatingperformanceconcernsre-
latedtothesameAPIs(Section2.4),wemanuallyfixedtheincorrect
and incomplete instances during this process.
Wealsoevaluateouralgorithmthatautomaticallycategorizes
commit types in the evolution analysis (Section 2.5). We randomly
selected 300 sentences and manually reviewed the results. Ouralgorithm achieves 96.3% accuracy in distinguishing commits of
theadded together with APIs, added later, and modified categories.
4 RESULTS
In this section, we report our study results with respect to each
research question.
4.1 API Coverage
Table3showsthe number ofAPIsdiscussedintheperformance-
relateddocumentationofeachlibrary.Consideringthetotalnumber
of APIs shown in Table 1, all of the target libraries have nontriv-
ial proportions of APIs being documented in performance-related
context. Specifically, TensorFlow has 8.7% of its APIs being the
subjectofperformance-relateddocumentationwhileNumPyhas
upto30.5%ofsuchAPIs.Theotherlibrarieshaveapproximately
15%oftheirAPIsbeingdocumentedforperformance-relatedrea-
sons(Figure2).Thisresultmanifeststheprevalenceofperformance
concerns for data science libraries.
By comparing the API names, we further analyzed the type
of data source where each API is being discussed. As shown in
Figure 2, each data source has its own merits. For NumPy and Pan-
das,thecrowddo cumentationhasperformance-relateddiscussions
onmoreAPIscomparedtotheofficialdocumentation.Forthere-
maining libraries, however, performance concerns from the officialdocumentation cover more APIs. Interestingly, only a few APIs are
mentioned in both the official and the crowd documentation ( <
5% as shown in Figure 2). These observations imply that SO andGitHub tend to complement official documentation by covering
anadditionalsetofAPIswithperformanceconcerns.Wefurther
expand on this issue in Section 4.3.
Finding1 :Performanceconcernsincrowddocumentationcover
a different set of APIs compared to official documentation. To-gether these two data sources have documented performance
concerns for a nontrivial proportion of data science APIs.
4.2 Knowledge Types of Performance-related
Documentation
To better understand performance-related documentation, we ana-
lyzedtheknowledgetypesoftheextractedperformance-relatedsen-tencesusingthetaxonomyinTable2.Figure3showstheknowledge-
type distribution stratified by libraries and data sources. Below we
highlight a few interesting observations.
FunctionalityisthemostcommonknowledgetypeinAPI
docstrings,yetitisnotdiscussedoftenincrowddocumenta-tion
.Inparticular,33.4%oftheperformace-relatedsentencesinAPI
docstringsprovidethe Functionknowledge,whilethepercentageof
thisknowledgetypeis12.4%forSOandonly1.9%forGitHub.Since
official documentation is inherently dedicated to describe function-
alitiesandbehaviorsofAPIs,libraryusers,especiallyGitHubusers
whoareoftenlibrarydevelopersthemselves,maynotfinditnec-
essarytoincludesuchduplicateknowledgeagainintheironline
discussions.
UsagepracticeisthetopknowledgetypediscussedonStack
Overflowandofficialuserguide .Specifically,the Practiceknowl-
edge type has a frequency of 36.6% on SO, 31.3% on official user
guide,21.7%onAPIdocstrings,andonly9%onGitHub.Thesere-
sultssuggestthatfordatasciencepractitionersfacingperformance
issues, SO and official user guide could be preferable to docstrings
and GitHub for troubleshooting.
Crowddocumentationprovidesmore Alternatives typeof
knowledgecomparedtoofficialdocumentation . In particular,
aroundonefourthoftheperformance-relatedsentencesfromSO
and GitHub have provided the Alternatives knowledge. A possible
explanationisthatunlikeofficialdocumentation,whichisinher-ently descriptive, crowd documentation is mostly in the form of
information sharing and discussion. Therefore, users on the crowd
platforms are more likely to discuss and compare the performance
of alternatives in their problem-solving and bug-fixing activities.
Stack Overflow rarely provides explanatory knowledge
types such as Implementation andPurpose. Compared to of-
ficialdocumentationandGitHub,whichallhaveacertaindegreeofdiscussionon Implementation andPurpose,SOshowsasurprisingly
lowfrequencyofthesetwoexplanatoryknowledgetypes(5.4%and
1.1%,respectively).ItisthusinterestingtoknowwhetherSOfocuses
moreonthe“how”ratherthanthe“why”inperformance-related
discussions,andifso,howsuchatendencyaffectsdevelopers’learn-
ingandproblem-solvingskills.Weconsiderthisasameaningful
future research direction.
900(a) API docstring (b) User guide
(c) Stack Overflow (d) GitHub
Figure 3: Knowledge types of performance-related sentences.
{Function, Attributes }, {Practice, Alternatives } and {Impl,
Purpose}areknowledgetypesthatoftenco-occur . By analyz-
ing the co-occurrence of knowledge types in performance-related
sentences, we observed that developers tend to introduce API
functionalitiesandperformanceattributestogether,especiallyin
the docstrings (e.g., “Raising this value decreases the number of
seedsfound,whichmakesmean_shiftcomputationallycheaper” from
Scikit-learn).Developersalsotendtosuggestpreferableusage prac-
ticeby comparing the efficiency of alternatives (e.g.,“You can either
use fillna (fast) or you can use apply (slow but flexible)” from SO
post 42213549). Finally, implementation knowledge is often doc-
umented with explanations on the design purpose and rationale
(e.g.,“This op expects unscaled logits, since it performs a softmax on
logitsinternallyforefficiency” fromTensorFlow).Theconfidence
values for {Function} ⇒{Attributes}, {Practice} ⇒{Alternatives} and
{Impl}⇒{Purpose} are 0.59, 0.35, and 0.27, respectively.
Finding 2 : Thefunctionknowledge type is often observed in
official documentation, while the practiceandalternatives types
ofknowledgearemoreprevalentincrowddocumentation.Ex-
planatory knowledge types such as purposeandimplementation
are rarely observed on SO.
4.3 Information Consistency
InadditiontoAPIcoverageandknowledgetypes,wealsoanalyzed
whetherthespecificinformationprovidedbycrowddocumentationTable 4: Consistency of performance-related information
from crowd documentation w.r.t. official documentation.
Consistent Inconsistent Not officially
documented
SO 15.3% 3.4% 81.3%
GitHub 5.9% 1.8% 92.3%
All 11% 2.7% 86.3%
isconsistentwiththatintheofficialdocumentation(Section2.4).
Results are presented in Table 4 and detailed as follows.
First, about 11% of the performance concerns in crowd docu-
mentationprovideinformationthatisconsistentwiththeofficial
documentation. In particular, SO has a larger percent of consistent
information(15.3%)comparedtoGitHub(5.9%).Interestingly,these
performance concerns, although being consistent, often have quite
differentnarrativescomparedtothecorrespondingofficialdocu-
mentation.Table5showsexamplesofsuchcases.Nonetheless,only
∼5% of these consistent concerns have explicit references to the of-
ficialdocumentation.Itisthusinterestingtoknowwhetheradding
references to the official documentation could potentially shorten
the resolution time of performance issues on crowd platforms.
Second, very few performance-related information from crowd
documentation (2.7%) is inconsistent with the official documen-
tation. Although this result seems promising, we observed that
inconsistent instances typically convey contradictory information
901Table5:Examplesofperformanceconcernsfromcrowddocumentationthatare(in)consistentwiththeofficialdocumentation.
Official Documentation Crowd DocumentationConsistentIn fact, pandas.eval is many orders of magnitude slower for
smaller expressions/objects than plain ol’ Python. ( pandas.eval )pd.eval(’x // y’, engine=’python’) is 1000 times slower
than the same operation in actual Python. (GitHub issue 4037)
Please use tf.keras.layers.CuDNNLSTM for better performance
on GPU. (tf.keras.layers.LSTM)TheCuDNNLSTMlayerdoesthesameastheLSTMlayerinKeras,
but it runs much faster on Nvidia GPUs (SO post 52340372)
After the one-time initialization, a Phraser will be much
smallerandsomewhatfasterthanusingthefull Phrasesmodel.
(gensim.models.phrases.Phraser)Phraser takesabunchofextratimetocreate,butthenisslightly
faster but much more compact. (GitHub issue 837)Inconsistentnumpy.genfromtxt is able to take missing data into account,
whenotherfasterandsimplerfunctionslike numpy.loadtxt can-
not. (numpy user guide)genfromtxt is a bit faster than loadtxt (SO post 52238949)
Have a look at the Hashing Vectorizer as a memory efficient
alternative to CountVectorizer.( sklearn user guide)Thegreatthingabout CountVectorizer isthat...,whichmakes
itverymemoryefficient,andshouldbeabletosolveanymemory
problems you’re having. (SO post 27339041)
Thisimplementationisbydefaultnotmemoryefficientbecause
it constructs a full pairwise similarity matrix in the case where
kd-treesorball-treescannotbeused.( sklearn.cluster.DBSCAN )I suggest you to try sklearn.cluster.DBSCAN - it has similar
behaviourforsomedata(sklearnexamples),also,itrunsaway
faster and consumes much less memory. (SO post 54533606)
with respect to time and memory consumption (see examples from
Table 5). If we assume that official documentation is the ground
truth, adopting inconsistent information from the crowd platforms
could instead trigger unexpected performance degradation in data
science applications.
Finally, the vast majority of performance concerns from the
crowddocumentation(86.3%)havenotbeenfoundintheofficial
documentation(Table4).Thisindicatesthepotentialvalueofcrowd
documentation for offering a large volume of new information
on the performance of data science libraries. However, whether
suchunofficialinformationcouldreallybeleveragedasareliable
complement to official documentation depends on its quality (e.g.,
correctnessandclarity).Therefore,classifyingorcharacterizingthe
quality ofunofficial performance information oncrowd platforms
could be a promising future research direction.
Finding 3 : The vast majority of performance concerns from the
crowd documentation are not present in the official documen-tation. Inconsistent performance-related information is rarelyobserved, while consistent information is often discussed on
crowd platforms without referring to the corresponding official
documentation.
4.4 Evolution of Performance-related
Documentation
We now report the evolution patterns of performance concerns
in the official documentation. First, for API docstrings, only 39.9%
performance-related concerns are added to the documentation to-
gether with the corresponding API definitions. The majority of
performance concerns (60.1%), however, are later added to the cor-
respondingAPIs’docstrings.Figure4showsthatthistrendissimilar
forallthetargetlibrariesexceptforTensorFlow.Userguidedata
revealsaconsistentyetmoresignificantoverallpattern:92.1%of
theperformanceconcernsareaddedtotheuserguideaftertheirFigure 4: Percentage of performance concerns in the APIdocstringsthatareaddedtogetherwithAPIdefinitionsandadded later.
correspondingAPIs hadbeenintroducedto thesourcecode. Con-
sideringall addedlater commits,wefoundthattheaverageduration
betweentheadditionofanAPIandtheadditionofitsperformance
concerns is 596 days. These results indicate that developers tend
to document performance concerns long after the additionof the subject APIs.
Next, we study the update frequency of performance-related
documentation. Table 6 shows the respective statistics for API
docstringsanduserguide.Ingeneral,themajority(73.1%)ofper-
formance concerns have stayed the same since they were added
(i.e., zero updated times). Nearly one-fifth of the performance con-
cerns (19.6%) have been updated just once. Only a few concerns
(7.3%) have been updated multiple times (i.e., ≥2 updated times).
On average, performance-related sentences have only 0.48 updates
during library evolution. On the other hand, the subject APIs of
theseperformanceconcernshaveanaverageof13.5updates.Theseresultsindicatethat
performanceconcernsarenotupdatedof-
ten, whereas their subject APIs have been updated 28 timesmore frequently.
902Table6:Thenumberoftimesaperformanceconcerninthe
official documentation is updated.
Updated times# of performance sentences
API docstring User guide
0 676 296
1 142 119
2 27 32
3 15 11
>3 4 8
Finally, we report the evolution reasons of performance con-
cerns,which wereobserved fromthe 513 modified commitsusing
the taxonomy in Table 7. As shown in Figure 5, most updates on
performance concerns are formatting changes. Other common up-
datingreasonsinclude clarification, improvingfluency,and fixing
spelling.Theseresultsshowthat developerstypicallyapplytriv-
ialupdatesonperformance-relateddocumentation,without
major changes to their semantics.
In general, the evolution of performance-related documentation
seems to lag behind the evolution of their subject APIs, in termsof both the creation time and maintenance activities. A natural
question that arises here is whether performance-related documen-
tation tends to be out-of-sync and potentially misleading for users
tomakeperformance-relatedjudgement(seetheexampleofwrongdocumentationinthe“Fixwronginfo”rowofTable7).Weconsider
this as another research question that is worth further exploration.
Finding4 :Performanceconcernsaretypicallydocumentedlong
after the addition of the corresponding APIs. Most of them have
beenstayedthesameduringlibraryevolutionorupdatedonly
once with mostly trivial semantic changes.
4.5 Implications
Wenowhighlightafewimportantimplicationsofourfindings.For
datasciencelibraryusers,ourresultssuggestamoretargetedstrat-egyforsearchingperformance-relatedinformationbasedonknowl-
edgetypes(Finding2).Forlibrarydevelopers,keepingabreastof
performance-related discussions on crowdsourced platforms might
helpthemidentifyomissionsintheofficialdocumentation(Find-
ings1–3).Also,moreattentionshouldbedrawnontheconsistency
of performance-related documentation during the evolution of cor-
respondingAPIs(Finding4).Forresearchers,investigatingunoffi-
cialperformance-relatedinformationoncrowdsourcedplatforms
(Finding 3) and rarely-updated performance concerns in official
documentation (Finding 4) are both promising avenues for improv-
ing documentation quality. Research efforts could also be made on
developingautomatictechniquesthatmergeperformanceconcerns
fromofficialandcrowddocumentationwhilepreservingthebest
of both worlds (Findings 1–3).
5 THREATS TO VALIDITY
Our study focuseson six Python libraries and twotypes of crowd
documentation.Theresultsmaynotgeneralizetootherdatasciencelibraries,otherprogramminglanguages,orotheronlineforumsandFigure5:Reasonsforupdatingperformanceconcernsintheofficial documentation.
websites. To mitigate this threat, we took the diversity, representa-
tiveness, and popularity of the subjects into account in our subject
selection process. The six libraries focus on different tasks of data
scienceandareextremelypopularamongdatasciencepractitioners.
Stack Overflow is the largest and most active online communityfor programmers, while GitHub is the mainstream platform for
software development and version control.
Weusedperformance-relatedkeywordsandcodementionstode-
tect performance concerns for target APIs. However, this approach
could not detect valid performance-related sentences that contain
no predefined keywords and no explicit code mentions. Such false
negatives may affect our quantitative observations. Nonetheless,
theprimarygoalofthisworkistostudythedocumentationpractice,
in particular the knowledge types, information consistency, and
evolutionpatterns,onperformanceconcerns,ratherthanreport-
ingallpossibleinstancesofsuchkind.Thecurrentmethodology
already detects sufficient amount of data for that purpose.
We proposed two taxonomies of performance-related documen-
tationinTable2andTable7.Admittedly,therecouldbeotherways
of categorizing the knowledge types and evolution reasons of per-
formanceconcerns.Tominimizethisthreat,weadaptedrelatedtax-
onomies of API documentation proposed in previous work [ 27,38]
rather than creating the taxonomies from scratch. Multiple human
coders also iteratively adjusted the taxonomies. These processes
helped improve the reliability and generality of our taxonomies.
Ourstudyresultsmightbeaffectedbyhumansubjectivity,which
isariskinherentinqualitativecoding[ 13].However,asdescribedin
Section3.2,manualworkiscrucialforthetypeoftasksinthiswork.
We followed the common research practice on manual labeling by
invitingmultipleexperiencedhumanratersandreportingtheinter-
rater agreement [ 23,27]. Our results are also released for public
scrutiny [41].
6 RELATED WORK
In this section, we discuss related work in the fields of API perfor-
mance and API documentation.
API Performance : Performance of APIs and API usages has
beenstudiedindifferentdomains.ForJavaapplications,Kawrykow
and Robillard observed cases where API call sequences can bereplaced by more efficient ones [
22]. Oliveira et al. proposed an
903Table 7: Reasons for updating performance concerns in the official documentation.
Reason Description Example (diff)
API design
changeUpdating the documentation to reflect changes
ofAPIdesignsorAPIsignatures(e.g.,anewpa-
rameter is added, a function is renamed, a new
algorithm is applied, etc.).Forsmalldatasets,’liblinear’isagoodchoice,whereas’sag’isfaster
for large ones.
For small datasets, ’liblinear’ is a good choice, whereas ’sag’ and
’saga’are faster for large ones. (Scikit-learn commit 5147fd09 )
Add details* Addingnewinformationthatisabsentfromthe
previous version.This function is most efficient when ’n’ is a power of two.
This function is most efficient when ’n’ is a power of two ,andleast
efficientwhen’n’isprime. (SciPy commit e9250710 )
Clarification Explicitlyclarifyingunclearconceptsorprovid-
ingsupplementarynotes,withonlyminorseman-
tic changes and better understandability.Providethisparametertogreatly speedupfinite differenceJacobian
estimation, if it’s significantlysparse.
Providethisparametertogreatly speedupfinite differenceJacobian
estimation, if it hasonlyfewnon-zerosin*each*row.
(SciPy commit ad664040 )
Updaterefer-
ence*Updating the reference or fixing broken links. ...will greatly speedup the computations [10].
...will greatly speedup the computations [2].
(SciPy commit 5ab0947e )
Fix spelling* Fixing spelling problems. ’lbgfs’ can converge faster and perform better.
’lbfgs’ can converge faster and perform better.
(Scikit-learn commit 79011904 )
Fix gram-
mar*Fixing grammatical errors. ...which is generally faster as ”iterrows”.
...which is generally faster than ”iterrows”.
(Pandas commit 89f04daa )
Fix wrong
infoFixing incorrect or misleading information. slowerbutmore accurate alternative to NNDSVDa for dense NMF.
faster,less accurate alternative to NNDSVDa for dense NMF.
(Scikit-learn commit eaced83a )
Formatting* Changing only the tabs, whitespaces, and
markups to follow documentation conventions.Allow overwriting data in ’a’ (may enhance performance).
Allow overwriting data in ”a” (may enhance performance).
(SciPy commit ff48839a )
Improve flu-
ency*Rephrasingthesentencetoimprovethepresen-
tation (e.g., word choices), typically with little or
no semantic change.Whether datainaisoverwritten (may improve performance).
Whether tooverwritedataina (may improve performance).
(SciPy commit c8ba75aa )
Move Moving the sentence to another location. The
sentence itself is not changed.(Diff is omitted for presentation concerns)
(TensorFlow commit 9dc48f95 )
Remove
detailsRemovingnotesandexplanationsthatareunnec-
essary, obsolete, or expose too much details.if ’True’, use a faster, fused implementation basedon
nn.fused_batch_norm.if ’True’, use a faster, fused implementation
ifpossible.
(TensorFlow commit d2cf3938 )
*Reasons rephrased from [38].
approach that uses energy profiles and static analysis to recom-
mend energy-efficient alternatives for Java collection implementa-
tions[29].ForAndroidapps,Linares-Vásquezetal.foundenergy
bugs caused by suboptimal API choices [ 24]. Liu et al. identified
performance issues caused by the misuses of the list scrolling API
and wakelock APIs [ 25,26]. Das et al. studied performance-related
commits in Android apps and identified the most predominant
typesofperformance-relatedchanges[ 17].ForJavaScriptprograms,
Selakovic and Pradel found inefficient API usage to be the most
commonrootcauseofperformanceissues[ 37].ForRailsapplica-
tions,Yangetal.reportedthathalfoftheperformanceissuescan
be improved by changing how the Rails APIs are used [ 46]. Our
workis differentin thatwe studyhowAPI performanceconcerns
are documented and we focus on the domain of data science.Knowledgein Documentation : Researchers have studied the
knowledge and information offered by software documentationfrom various perspectives. Pascarella and Bacchelli explored the
code commentsin Java projectsand manually deriveda hierarchi-
caltaxonomyofcodecommentsintermsoftheirsemanticmean-
ings [33]. Hata et al. studied the content and evolution patterns
of links in source code comments [ 21]. Prana et al. conducted a
qualitative study to categorize the content of GitHub README
files [35]. Li et al. mine API documentation using NLP techniques
tobuildknowledgegraphsofAPIcaveats[ 23].MaalejandRobil-
lard proposed a taxonomy of knowledge types in the API docu-mentation of Java SDK 6 and .NET 4.0 [
27], which also inspires
our taxonomy of knowledge types in performance concerns (see
Section2.3).However,previousstudiestendtofocusonthegeneral
904content of API documentation, while our study focuses exclusively
on performance-related content in the documentation.
Research has also suggested that crowdsourced knowledge can
aid various software development activities. Regarding API un-
derstanding and usages, Petrosyan et al. used text classification
todiscoverinformationthatexplainsagivenAPIbutisscattered
inonlinetutorials[ 34].TreudeandRobillardproposedamachine
learning approach that classifies informative sentences from Stack
Overflow to augment API documentation [ 43]. In this paper, we
observed that official and crowd documentation complement each
other in terms of performance-related information.
DocumentationQuality :Whilesoftwaredocumentationgreatly
supports development activities, research has also found quality is-
sues in documentation. Several studies combined program analysis
andnaturallanguage processingtechniquestodetectinconsisten-
cies between source code and documentation [ 40,45,48,49], with
the inconsistencies typically revealing incorrect or outdated in-
formation.Unlikethesestudies,ourworkcomparesperformance
concerns extracted from official documentation and crowd doc-umentation. The reliability of crowdsourced data has also been
studied. For example, Zhang et al. found that one third of SO posts
containpotentialAPImisuses[ 47].Chenetal.foundthatalarge
proportion of security implementations on SO is insecure, and that
the corresponding posts have higher scores and more duplicates
comparedtopostswithsecuresuggestions[ 15].Inthefuture,we
alsoplantoinvestigatethequalityofunofficialperformance-related
information from crowdsourced data.
7 CONCLUSION
Inthiswork,wepresentanempiricalstudyonhowperformance
concerns are documented in data science libraries. A nontrivialproportion of data science APIs was found to be discussed in
performance-relatedcontextintheofficialandcrowddocumenta-
tion, indicating the prevalence of such problems. By quantitatively
andqualitativelycomparingtheperformanceconcernsextracted
fromthesetwotypesofdatasources,wefoundthatcrowddocu-mentation is highly complementary to official documentation in
termsoftheAPIcoverage,theknowledgetypes,andthespecific
informationbeingprovided.Wealsoobservedthatthemaintenanceonperformance-relateddocumentationisrelativelyplateauingand
peripheral given the active evolution of their subject APIs. Our
findingsshedlightonthecurrentstateofaffairsforperformance-
related documentation practice, which could be an important step
towards building efficient data science applications.
ACKNOWLEDGMENTS
Wewouldliketothanktheanonymousreviewersfortheirconstruc-
tive suggestions and comments. This work was partially supportedby the National Natural Science Foundation of China under Grants
No. 61972260, 61772347, 61836005, 61932021, 61802164, and theGuangdong Basic and Applied Basic Research Foundation under
Grant No. 2019A1515011577.REFERENCES
[1] [n.d.]. Gensim. https://radimrehurek.com/gensim/
[2] [n.d.]. GitHub REST API v3. https://developer.github.com/v3/[3] [n.d.]. NumPy. http://www.numpy.org/.[4] [n.d.]. Pandas. https://pandas.pydata.org/.[5] [n.d.]. Project Jupyter. https://jupyter.org/[6] [n.d.]. Python Docstring. https://www.python.org/dev/peps/pep-0257/[7]
[n.d.]. Pythoninspect-Inspectliveobjects. https://docs.python.org/3/library/
inspect.html.
[8] [n.d.]. Scikit-learn. https://scikit-learn.org/stable/index.html.[9] [n.d.]. The SciPy library. https://www.scipy.org/scipylib/index.html.
[10]
[n.d.]. spaCy:Industrial-StrengthNaturalLanguageProcessing. https://spacy.io/
[11] [n.d.]. Stack Exchange API v2.2. https://api.stackexchange.com/docs[12]
Eduardo C. Campos, Lucas B. L. Souza, and Marcelo de A. Maia. 2016. Searching
Crowd Knowledge to Recommend Solutions for API Usage Tasks. J. Softw. Evol.
Process28, 10 (Oct. 2016), 863–892.
[13]Yanto Chandra and Liang Shang. 2019. Qualitative research using R: A systematic
approach. Springer.
[14]CongChenandKangZhang.2014. WhoAskedWhat:IntegratingCrowdsourcedFAQsintoAPIDocumentation.In CompanionProceedingsofthe36thInternational
ConferenceonSoftwareEngineering (Hyderabad,India) (ICSECompanion2014).
Association for Computing Machinery, New York, NY, USA, 456–459.
[15]Mengsu Chen, Felix Fischer, Na Meng, Xiaoyin Wang, and Jens Grossklags. 2019.
How reliable is the crowdsourced knowledge of security implementation?. In
2019IEEE/ACM41stInternationalConferenceonSoftwareEngineering(ICSE).IEEE,
536–547.
[16]Barthélémy Dagenais and Martin P Robillard. 2012. Recovering traceability links
between an API and its learning resources. In 2012 34th International Conference
on Software Engineering (ICSE). IEEE, 47–57.
[17]Teerath Das, Massimiliano Di Penta, and Ivano Malavolta. 2016. A quantitative
andqualitativeinvestigationofperformance-relatedcommitsinandroidapps.
In2016 IEEE International Conference on Software Maintenance and Evolution
(ICSME). IEEE, 443–447.
[18]Vasant Dhar. 2013. Data science and prediction. Commun. ACM 56, 12 (2013),
64–73.
[19]Martín Abadi et al. 2015. TensorFlow: Large-Scale Machine Learning on Hetero-
geneousSystems. http://tensorflow.org/ Softwareavailablefromtensorflow.org.
[20]DavideFucci,AlirezaMollaalizadehbahnemiri,andWalidMaalej.2019. Onusing
machine learning to identify knowledge in API reference documentation. In
Proceedingsofthe201927thACMJointMeetingonEuropeanSoftwareEngineering
Conference and Symposium on the Foundations of Software Engineering. 109–119.
[21]Hideaki Hata, Christoph Treude, Raula Gaikovina Kula, and Takashi Ishio. 2019.
9.6MillionLinksinSourceCodeComments:Purpose,Evolution,andDecay.In
Proceedings of the 41st International Conference on Software Engineering (ICSE’19).
IEEE Press, 1211–1221.
[22]David Kawrykow and Martin P Robillard. 2009. Detecting inefficient API usage.
In2009 31st International Conference on Software Engineering-Companion Volume.
IEEE, 183–186.
[23]Hongwei Li, Sirui Li, Jiamou Sun, Zhenchang Xing, Xin Peng, Mingwei Liu, and
XuejiaoZhao.2018. Improvingapicaveatsaccessibilitybyminingapicaveats
knowledge graph. In 2018 IEEE International Conference on Software Maintenance
and Evolution (ICSME). IEEE, 183–193.
[24]Mario Linares-Vásquez, Gabriele Bavota, Carlos Bernal-Cárdenas, Rocco Oliveto,
Massimiliano Di Penta, and Denys Poshyvanyk. 2014. Mining Energy-greedy
APIUsagePatterns inAndroidApps:An EmpiricalStudy. In Proceedingsofthe
11thWorkingConferenceonMiningSoftwareRepositories (MSR2014).ACM,2–11.
[25]Yepang Liu, Chang Xu, and Shing-Chi Cheung. 2014. Characterizing and Detect-
ingPerformanceBugsforSmartphoneApplications.In Proceedingsofthe36th
International Conference on Software Engineering (Hyderabad, India) (ICSE 2014).
ACM, New York, NY, USA, 1013–1024. https://doi.org/10.1145/2568225.2568229
[26]YepangLiu,ChangXu,Shing-ChiCheung,andJianLü.2014. Greendroid:Au-
tomated diagnosis of energy inefficiency for smartphone applications. IEEE
Transactions on Software Engineering 40, 9 (2014), 911–940.
[27]WalidMaalejandMartinPRobillard.2013.PatternsofknowledgeinAPIreference
documentation. IEEE Transactions on Software Engineering 39, 9 (2013), 1264–
1282.
[28]MaryMcHugh.2012. Interraterreliability:Thekappastatistic. Biochemiamedica:
časopisHrvatskogadruštvamedicinskihbiokemičara/HDMB 22(102012),276–82.
https://doi.org/10.11613/BM.2012.031
[29]WellingtonOliveira,RenatoOliveira,FernandoCastor,BenitoFernandes,and
GustavoPinto.2019.RecommendingEnergy-efficientJavaCollections.In Proceed-
ingsofthe16thInternationalConferenceonMiningSoftwareRepositories (Montreal,
Quebec, Canada) (MSR ’19). IEEE Press, Piscataway, NJ, USA, 160–170.
[30]CarlosEOteroandAdrianPeter.2014. Researchdirectionsforengineeringbig
data analytics software. IEEE Intelligent Systems 30, 1 (2014), 13–19.
[31]Shoumik Palkar, James Thomas, Deepak Narayanan, Pratiksha Thaker, Rahul
Palamuttam,ParimajanNegi,AnilShanbhag,MalteSchwarzkopf,HolgerPirk,
905Saman Amarasinghe, Samuel Madden, and Matei Zaharia. 2018. Evaluating
End-to-EndOptimizationforDataAnalyticsApplicationsinWeld. Proc.VLDB
Endow.11, 9 (May 2018), 1002–1015.
[32]Chris Parnin and Christoph Treude. 2011. MeasuringAPI Documentationon the
Web. InProceedings of the 2nd International Workshop on Web 2.0 for Software En-
gineering (Waikiki, Honolulu, HI, USA) (Web2SE’11). Association for Computing
Machinery, New York, NY, USA, 25–30.
[33]Luca Pascarella and Alberto Bacchelli. 2017. Classifying code comments in Java
open-sourcesoftwaresystems.In 2017IEEE/ACM14thInternationalConference
on Mining Software Repositories (MSR). IEEE, 227–237.
[34]GayanePetrosyan,MartinPRobillard,andRenatoDeMori.2015. Discovering
informationexplainingAPItypesusingtextclassification.In 2015IEEE/ACM37th
IEEE International Conference on Software Engineering, Vol. 1. IEEE, 869–879.
[35]Gede Artha Prana, Christoph Treude, Ferdian Thung, Thushari Atapattu, and
DavidLo.2019. CategorizingtheContentofGitHubREADMEFiles. Empirical
Softw.Engg. 24,3(June2019),1296–1327. https://doi.org/10.1007/s10664-018-
9660-3
[36]Martin P. Robillard and Robert Deline. 2011. A Field Study of API Learning
Obstacles. Empirical Softw. Engg. 16, 6 (Dec. 2011), 703–732.
[37]MarijaSelakovicandMichaelPradel. 2016. PerformanceIssuesandOptimiza-
tionsinJavaScript:AnEmpiricalStudy.In Proceedingsofthe38thInternational
ConferenceonSoftwareEngineering (Austin,Texas) (ICSE’16).ACM,NewYork,
NY, USA, 61–72. https://doi.org/10.1145/2884781.2884829
[38]Lin Shi, Hao Zhong, Tao Xie, and Mingshu Li. 2011. An empirical study on
evolution of API documentation. In International Conference on Fundamental
Approaches To Software Engineering. Springer, 416–431.
[39]MeganSquire.2015. "ShouldWeMovetoStackOverflow?"MeasuringtheUtilityofSocialMediaforDeveloperSupport.In 2015IEEE/ACM37thIEEEInternational
Conference on Software Engineering, Vol. 2. IEEE, 219–228.
[40]Lin Tan, Ding Yuan, Gopal Krishna, and Yuanyuan Zhou. 2007. /*icomment:Bugs or Bad Comments?*/. SIGOPS Oper. Syst. Rev. 41, 6 (Oct. 2007), 145–158.
https://doi.org/10.1145/1323293.1294276
[41]YidaTao,JiefangJiang,YepangLiu,ZhiwuXu,andShengchaoQin.2020. Dataset
for"UnderstandingPerformanceConcernsintheAPIDocumentationofDataScienceLibraries". https://doi.org/10.5281/zenodo.3972069
[42]Yida Tao, Shan Tang, Yepang Liu, Zhiwu Xu, and Shengchao Qin. 2019. How do
apiselectionsaffecttheruntimeperformanceofdataanalyticstasks?.In 201934th
IEEE/ACM International Conference on Automated Software Engineering (ASE).
IEEE, 665–668.
[43]Christoph Treude and Martin P Robillard. 2016. Augmenting api documentation
withinsightsfromstackoverflow.In 2016IEEE/ACM38thInternationalConference
on Software Engineering (ICSE). IEEE, 392–403.
[44]Bogdan Vasilescu, Alexander Serebrenik, Prem Devanbu, and Vladimir Filkov.
2014. How Social Q&A Sites Are Changing Knowledge Sharing in Open Source
Software Communities. In Proceedings of the 17th ACM Conferenceon Computer
Supported Cooperative Work & Social Computing (Baltimore, Maryland, USA)
(CSCW ’14). ACM, New York, NY, USA, 342–354.
[45]FengcaiWen,CsabaNagy,GabrieleBavota,andMicheleLanza.2019. Alarge-
scale empiricalstudy on code-comment inconsistencies.In 2019 IEEE/ACM27th
International Conference on Program Comprehension (ICPC). IEEE, 53–64.
[46]Junwen Yang, Pranav Subramaniam, Shan Lu, Cong Yan, and Alvin Cheung.
2018. HowNottoStructureYourDatabase-backedWebApplications:AStudyofPerformanceBugsintheWild.In Proceedingsofthe40thInternationalConference
on SoftwareEngineering (Gothenburg, Sweden) (ICSE ’18). ACM,New York,NY,
USA, 800–810. https://doi.org/10.1145/3180155.3180194
[47]Tianyi Zhang, Ganesha Upadhyaya, Anastasia Reinhardt, Hridesh Rajan, and
Miryung Kim.2018. Are CodeExamples onan OnlineQ&A ForumReliable? A
StudyofAPIMisuseonStackOverflow.In Proceedingsofthe40thInternational
Conference on Software Engineering (Gothenburg, Sweden) (ICSE’18). Association
for Computing Machinery, New York, NY, USA, 886–896. https://doi.org/10.
1145/3180155.3180260
[48]Hao Zhong and Zhendong Su. 2013. Detecting API documentation errors. In
Proceedings of the 2013 ACM SIGPLAN international conference on Object oriented
programming systems languages & applications. 803–816.
[49]YuZhou,RuihangGu,TaolueChen,ZhiqiuHuang,SebastianoPanichella,and
HaraldGall.2017. AnalyzingAPIsdocumentationandcodetodetectdirective
defects. In 2017 IEEE/ACM 39th International Conference on Software Engineering
(ICSE). IEEE, 27–37.
906