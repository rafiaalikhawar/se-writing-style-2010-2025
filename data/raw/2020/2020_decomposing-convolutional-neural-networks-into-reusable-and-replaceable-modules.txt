Decomposing Convolutional Neural Networks into Reusable
and Replaceable Modules
Rangeet Pan
rangeet@iastate.edu
Dept. of Computer Science, Iowa State University
226 Atanasoff Hall, Ames, IA, USAHridesh Rajan
hridesh@iastate.edu
Dept. of Computer Science, Iowa State University
226 Atanasoff Hall, Ames, IA, USA
ABSTRACT
TrainingfromscratchisthemostcommonwaytobuildaConvolu-
tionalNeuralNetwork(CNN)basedmodel.Whatifwecanbuild
newCNNmodelsbyreusingpartsfrompreviouslybuiltCNNmod-
els?WhatifwecanimproveaCNNmodelbyreplacing(possibly
faulty) parts with other parts? In both cases, instead of training,
can we identify the part responsible for each output class (mod-ule) in the model(s) and reuse or replace only the desired outputclasses to build a model? Prior work has proposed decomposing
dense-basednetworksintomodules(oneforeachoutputclass)to
enablereusabilityandreplaceabilityinvariousscenarios.However,
this work is limited to the dense layers and is based on the one-to-
onerelationshipbetweenthenodesinconsecutivelayers.Dueto
the shared architecture in the CNN model, prior work cannot beadapted directly. In this paper, we propose to decompose a CNN
modelusedforimageclassificationproblemsintomodulesforeach
outputclass.Thesemodulescanfurtherbereusedorreplacedto
buildanewmodel.WehaveevaluatedourapproachwithCIFAR-10,
CIFAR-100, and ImageNet tiny datasets with three variations of
ResNet models and found that enabling decomposition comes with
a small cost (1.77% and 0.85% for top-1 and top-5 accuracy, respec-
tively). Also, building a model by reusing or replacing modules canbedonewitha2.3%and0.5%averagelossofaccuracy.Furthermore,reusingandreplacingthesemodulesreduces
CO2eemissionby ‚àº37
times compared to training the model from scratch.
CCS CONCEPTS
‚Ä¢Computingmethodologies ‚ÜíMachinelearning ;‚Ä¢Software
and its engineering ‚ÜíAbstraction and modularity.
KEYWORDS
deeplearning,cnn,deepneuralnetwork,modularity,decomposi-
tion
ACM Reference Format:
Rangeet Pan and Hridesh Rajan. 2022. Decomposing Convolutional Neural
NetworksintoReusableandReplaceableModules.In 44thInternationalCon-
ferenceonSoftwareEngineering(ICSE‚Äô22),May21‚Äì29,2022,Pittsburgh,PA,
USA.ACM,NewYork,NY,USA,12pages.https://doi.org/10.1145/3510003.
3510051
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
¬© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.35100511 INTRODUCTION
Deeplearningisincreasinglybeingusedforimagesegmentation,
object detection, and similar computer vision tasks. With the need
forbiggerandmorecomplexdatasets,thesizeandcomplexityof
modelsalsoincrease.Often,trainingamodelfromscratchneeds
severalhoursorevendays.Toeasethetrainingprocess,layerarchi-
tecture[24,33],transferlearning[ 35,38],one-shotlearning[ 28,39],
few-shot learning [ 22,32,34,42], etc., have been introduced. With
these techniques, model structure, parameters can be reused for
building a model for similar or different problems. However, in all
cases, retraining is needed. Also, there may be scenarios, as shown
inFigure1,whereafaultysectionofthenetworkneedstobeampu-
tated or replaced. Now, suppose we can decompose such networks
into smaller components (modules), where each component can
recognizeasingleoutputclass.Inthatcase,wecanreusethesecom-ponentsinvarioussettingstobuildanewmodel,removeaspecific
output class, or even replace an output class from a model with-out retraining. In the past, modular networks [
2,13,17], capsule
networks [ 16,29], etc., have been studied to train the network and
incorporate memory into the learning process. But, these modules
arenotcreatedforenablingreusabilityandreplaceability.Thereare
strong parallels between deep neural network development now
and software development before the notion of decomposition was
introduced[ 9,26,36],anddeveloperswrotemonolithiccodethat
can be reused or replaced as easily.
Recently,decompositionhasbeenusedtoenablereuseandre-
placement in dense-based networks [ 25]. This work shows that
decomposedmodulescanbe reusedorreplacedinvariousscenar-
ios. However, this work focused on dense-based networks and did
notexploreamorecomplexsetofdeeplearningtechniques,e.g.,
convolutional neural networks (CNN). This work [ 25] relies on the
densearchitectureofthemodels,wherenodesandedges(weight
andbias)haveaone-to-onerelationship.Incontrast,edgesinCNN
are shared among all the input and the output nodes.
In this paper, we propose an approach to decompose a CNN
model used for image classification into modules, in which each
module can classify a single output class. Furthermore, these mod-
ules can be reused or replaced in different scenarios. In Figure 1,
we illustrate an issue faced by users while using the Google Photo
app.WeshowhowGooglehasresolvedtheproblemandhowre-
placing and reusing decomposed modules can be applied. In thepast, Google Photo App tagged a black woman as a gorilla [
37].
To resolve this issue, Google decided to remove the output class
‚ÄúGorilla‚Äù by suppressing the output label [ 37]. Precisely, they have
suppressedtheoutputfor‚ÄúChimpanzee‚Äù,‚ÄúMonkey‚Äù,‚ÄúChimp‚Äù,etc.
Though the problem can be temporarily solved by suppressing the
outputlabel,tofixthemodel,oneneedstoretrainthemodel.We
5242022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Rangeet Pan and Hridesh Rajan
:KDW+DSSHQHG" +RZ*RRJOH5HVROYHGWKH3UREOHP" HP" +RZ'HFRPSRVLQJWKH0RGHOLQWR0RGXOHVFDQ+HOS"
3HUVRQ
 *RULOOD
*RRJOH&ODVVLILHVD%ODFN:RPDQDV
*RULOOD7KH&RQYROXWLRQDO1HXUDO
1HWZRUN

3HUVRQ
 *RULOOD
 &DU
*RRJOH6XSSUHVVHVWKH*RULOOD
2XWSXW&ODVV2SWLRQ5HSODFH)DXOW\
*RULOOD0RGXOH+RZ'HFRPSRVLQJWKH0RGHOLQW
2SWLRQ7UDLQ3HUVRQDQG*RULOOD
DQGWDNHWKHQHZ*RULOOD0RGXOHWR0RGXOHVFDQ+HOLQW
D2SWLRQ5HPRYH
*RULOOD0RGXOH)DXOW\&110RGHO:RUNLQJ&110RGHO
&DU *RULOOD

&DU
 *RULOOD
)DXOW\&110RGHO
*RULOOD


&DU1HZ&110RGHO
3HUVRQ
 *RULOOD7KH&RQYROXWLRQDO
1HXUDO1HWZRUN

3HUVRQ
 &DU
5HPRYHWKH*RULOOD
0RGXOH
)DXOW\&110RGHO

3HUVRQ
3H *RULOOD
3HUVRQ
3H *RULOOD
0RGLILHG0RGXOHVS
2SWLRQ5HYHULI\XVLQJ
0RGXOHV
Strategies Description Pros Cons
Option 1WedecomposethefaultyCNNmodelintomodulesforeachoutputclass.Eachsuch
moduleisabinaryclassifierthatclassifieswhetheraninputbelongstotheoutputclass
(for which the module is decomposed) or not. Now, suppose we have another working
model trained with the same or a subset of the dataset with the gorilla and person
output labels. This new model does not exhibit the behavior present in the faulty one.
Then, we can replace the faulty gorilla module with the working gorilla module.Since the module belong
to the same dataset as thefaulty model, the module
will havesufficient informa-
tion to both recognize and
distinguish new inputs.These models are massive, and train-
ingisverysufficientlycostly.Thus,the
availabilityofasimilartrainedmodel
with the Gorilla class may not be feasi-
ble for all conditions.
Option 2We decompose the faulty model into modules. Moreover, we train a new model with a
personandgorillaclassesandvalidatethatthetrainedmodeldoesnotdemonstrate
faulty behavior. We decompose the newly trained model into two modules (gorilla
andperson)andreplacethefaultygorillamodulewiththenewdecomposedgorilla
module.Trainingwithasmalldataset
requires less resources incomparison to training the
whole dataset.Decompositioncannotalonesolvethe
problem. However, the decomposition
bundled with the traditional training
could help in this situation.
Option 3We decompose the faulty model into modules and remove the gorilla module from the
collection. In this scenario, there is no cost of retraining involved.This method is cost-
effective.The actual problem of faulty classifica-
tion has not been addressed.
Option 4Wecanalsoreusethepersonandgorillamodulefromaworkingmodelwithoutthe
faulty behavior. If any input is classified as a person or gorilla using the faulty model,
we reuse the working modules to verify it further.This approach is cost effec-
tiveandusedforfurtherver-
ification.Since this approach involves anotherlayer of verification using modules,
there is an overhead present.
Figure 1: How Decomposing a Convolution Neural Network into Modules Can Help Solve a Real-life Problem.
propose four different solutions based on reusing and replacing
decomposedmodules.Wediscusseachapproach,itspros,cons,and
illustrate how retraining the model can be avoided.
The key contributions of our work are the following:
‚Ä¢WeintroduceatechniquetodecomposeaCNNmodelinto
modules,onemoduleperoutputclass.Thistechniquecon-
sists of concern identification, tangling identification, and
concern modularization.
‚Ä¢Wedescribeanapproachforreusingthesedecomposedmod-
ules in different scenarios to build models for new problems.
‚Ä¢We replace a part of the model with decomposed modules.
‚Ä¢We evaluate our approach against the CO2econsumption
of models created from scratch and reusing or replacing
decomposed modules.
‚Ä¢We haveimplementedour approachfor Keras[ 19], awidely
used library for constructing CNN models.
Results-at-a-glance. Ourevaluationsuggeststhatdecompos-
ing a CNN model can be done with a little cost (-1.77% (top-1) and
-0.85% (top-5)) compared to the trained model accuracy. Reusing
and replacing modules can be done when the modules belong to
thesamedatasets(reusability:+2.5%,replaceability:+0.7%(top-1)
and +1.0% (top-5)) and the different datasets (reusability: -7.63%,replaceability: +2.5% (top-1) and -7.0% (top-5)). Furthermore, en-abling reusability and replaceability reduce
CO2eemission by 37
times compared to training from scratch.
Outline. In ¬ß2 we describe the related work. Then in ¬ß3, we
discuss our approach to decompose a CNN model into modules.In¬ß4,weanswerthreeresearchquestionstoevaluatethecostof
decomposition,thebenefitofreusingorreplacingthemodules,andthe resource consumption. In ¬ß6, we conclude. Lastly, ¬ß, we discussthe results and the future works in decomposing the deep learning
model into modules.
2 RELATED WORKS
Thereisavastbodyofwork[ 1,3,4,7‚Äì12,23,26,27,36]onsoftware
decompositionthathasgreatlyinfluencedustodecomposeCNN
model into reusable and replaceable modules.
The closest work is by Pan and Rajan [ 25], where the dense-
basedmodelhasbeendecomposedintomodulestoenablereuseandreplacementinvariouscontexts.Thoughthisworkhasmotivatedto
decompose a CNN model into modules, the dense-based approach
cannotbeapplieddueto:1)thesharedweightandbiasarchitecture
inconvolutionlayers,and2)supportforlayersotherthandense.
Also, this work did not evaluate the impact of decomposition on
CO2eemission during training.
Ghaziet al.[13] have introduced modular neural networks to
incorporate memory into the model. They have proposed a hier-
archicalmodulararchitecturetolearnanoutputclassandclasses
within that output class. Though this work has been able to in-
crease the interpretability of the network by understanding how
inputs are classified, the modules are not built to enable reusability
or replaceability. Other works on modular networks [ 2,17] have
learnedhowdifferentmodulescommunicatewithotherstoachievebettertraining.Also,capsulenetworks[
16,29]canbeutilizedtoin-
corporatememoryintodeepneuralnetworks.Incapsulenetworks,
eachcapsulecontainsasetoffeatures,andtheyaredynamically
called to form a hierarchical structure to learn and identify objects.
However,modulesdecomposedbyourapproachcanbereusedor
replaced in various scenarios without re-training.
525Decomposing Convolutional Neural Networks into Reusable and Replaceable Modules ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA



$%&'   &RQFHUQ
,GHQWLILFDWLRQ7DQJOLQJ
,GHQWLILFDWLRQ 0RGXODUL]LQJ
&RQFHUQV7UDLQHG0RGHO &RQFHUQ$ &RQFHUQ$ 0RGXOH$



$


$%&'


$$$
7
^
$^^^^&RQYROXWLRQ
/D\HU
^)ODWWHQ
DQG
'HQVH/D\HUV3RROLQJ
0HUJHDQG
RWKHU/D\HUV&RQFHUQ%
&RQFHUQ&
&RQFHUQ'
^
^3RROLQJ
0HUJHDQG QG
RWKHU/D\ HHUVV^^^^3RROLQJ
0HUJHDQG
RWKHU/D\HUV
%

&

'&R 
Figure 2: High-level Overview of Our Approach. Inactive nodes are denoted by black boxes.
SaiRam et al .[30] have proposed an approach to convert a CNN
model into a hierarchical representation. At each level, nodes rep-
resenting similar output classes are clustered together, and each
cluster is decomposed further down the tree. Finally, with parame-
ter transfer, the tree-based model is trained. While, in that work,
a sub-section of the tree-based can be reused for a subset of theoutput classes, our approach decomposes a trained CNN model
intoreusableandreplaceablemodulestobuildamodelforanew
problem (intra and inter dataset) without retraining. Furthermore,
wehaveshownthatreusingandreplacingmodulesdecreasesCO2e
consumption significantly.
3 APPROACH
Inthissection,weprovideanoverviewofourapproachforCNN
model decomposition. We discuss the challenges in general. We
also discuss each step of decomposing a CNN model into modules.
Figure 2 shows the steps for decomposing a CNN model into
modules.Ourapproachstartswiththetrainedmodelandthelist
ofoutputclasses.Inthefirststep,ourapproachidentifiesthesec-
tion in the CNN that is responsible for a single output class or a
concern(ConcernIdentification).Sinceweremovenodesforallun-concernedoutputclasses,theidentifiedsectionactsasasingle-class
classifier. That means any input will be classified as the concerned
outputclass,andthemodulecannotdistinguishbetweenthecon-
cernedandunconcernedoutputclasses.Toaddthenotionofthe
negativeoutputclasses,weaddalimitedsectionoftheinputsfrom
unconcernedoutputclasses(TanglingIdentification).Finally,we
channeltheconcernstocreateamodule(s)(ConcernModulariza-
tion). In the example, we show the decomposition of the module
for the output class A. Here, a model trained to predict four output
classes (A, B, C, and D) has been decomposed into four modules.
Eachmoduleactsasabinaryclassifierthatrecognizesifaninput
belongs to the output class. In this paper, we use concerned and
unconcerned as terminologies that represent the input belonging
totheoutput classforwhichmodulehasbeencreated andallthe
otheroutputclasses,respectively.Forexample,inFigure2,weshowamodulethatis responsibleforidentifying outputclass
A.For that
module, output class Ais the concerned output class, and other
output classes, e.g., B,C, andD, are the unconcerned classes.,QSXW,PDJH
,QSXW,PDJH$IWHU3DGGLQJ

:HLJKW>@

:HLJKW>@
%LDV>@

%LDV>@
&RQYROXWLRQ
2XWSXW

Figure 3: Architecture of a Convolutional Layer.
3.1 Challenges
Our prior work has been focused on the models built using dense-
basedlayers.Inmodelsbuiltusingdenselayers,eachnodeiscon-
nectedwithall thenodesfromtheprevious layer(exceptthefirst
layer) and the nodes from the following dense layer (except the
last layer). In the prior work on decomposing dense-based models,
moduleswerecreatedbyremovingedgesthatconnectdenselayers.
If the value of the node is ‚â§0 after feeding input(s) from a specific
outputclass(es),thenallincomingandoutgoingedgesareremoved.
Thus,weremovetheeffectofthatnodeinthemodel.Inatypical
dense-based model, the first layer is a flatten layer that converts
an input into a single-dimensional representation. From there, one
or more dense layers are attached sequentially. In each such dense
layer,nodesareconnectedwithtwodifferentedges,1)anedgethatconnectswithothernodesfromthepreviouslayer(weight)and2)aspecial incoming edge (bias). However, for a Convolution layer, this
isnotthecase.Figure3illustratesatraditional Convolution layer.
In that figure, on the left side, we have input nodes. Weight and
biasareshowninthemiddle,andfinally,ontherightside,wehave
the output nodes. Each node in the input is not directly connected
withtheweightandbias,ratherasetofnodes(slidingwindow)are
chosen at a time as the input, and the output is computed based onthat.Theweightandbiasarethesameforalltheinputblocks.Due
to these differences in the underlying architecture of a Convolution
layer,thedense-baseddecompositionapproachcouldnotbeapplied
toaConvolution layerdirectly.Inthenextparagraphs,wediscuss
each of such challenges.
526ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Rangeet Pan and Hridesh Rajan
Challenge 1: Shared Weight and Bias. The removal of edges
indense-basedlayershasbeendonetoforcethevalueofthenodes
thatarenotneededtobe0andeventuallyremovethemfromthe
decision-makingprocessinthemodule.Thisapproachispossible
in dense layers because there is a one-to-one relationship between
two nodes that belong to subsequent layers. If we remove a part
of the weight and bias for one node in the Convolution layer, the
weight,andthebiaswillbeturnedoffforallothernodesaswell,
and there will not be any output produced.
Challenge2:Backtracking. The prior work channels the con-
cerns to convert the module into a binary classification problem.
However,beforechannelingtheoutputnodes,abacktrackingap-
proachhasbeenappliedthatstartswithremovingnodesatthelast
hidden layer that are only connected to the unconcerned nodes at
theoutputlayers andbacktrackingtothefirsthiddenlayer.How-
ever, the same approach cannot be directly applied to the CNNmodels due to the differences in the architecture of the convolu-
tional layers and other supporting layers, e.g., Pooling,Merge.
Challenge 3: Loss of Accuracy in Inter-Dataset Scenarios.
Priorworkevaluatedtheirapproachbyreusingandreplacingmod-
ulesthatbelongtodifferentdatasets.Ithasbeenfoundthatsuch
scenarios involve a non-trivial cost. Since these modules involved
inthereuseandreplacescenariodonotbelongtothesamedatasets,
theyarenotprogrammedtodistinguishbetweenthem.Toreme-
diatethecost,weproposeacontinuouslearning-basedapproach
that enables retraining of the decomposed modules.
3.2 Concern Identification
ConcernIdentification(CI)involvesidentifyingthesectionofthe
CNNmodel,responsibleforthesingleoutputclass.Asaresultof
this process, we remove nodes and edges in the model. In tradi-
tional CNN modelsfor image classification, both convolution and
denselayershavethenotionofnodeandedges,andwediscussthe
concern identification approaches for both the layers.Dense Layers:
Here, the concerned section can be identified by
updatingorremovingtheedgesconnectedtothenodes.Inadense-
based network, there is a one-to-one relationship between theedges connecting the nodes from different layers (including the
bias nodes).For eachedge, theoriginating and theincident nodes
are unique except for bias, where the incident node is unique. The
edgesbetweennodesareremovedorupdatedbasedonthevalueas-sociatedwiththenodes.Inthisprocess,weidentifynodesbasedonthe assumption that ReLU has been used as the activation function.
Since our work is focused on image-based classification models,
ReLUisthemostcommonlyusedactivationfunctionforthehidden
layers. Also, prior work on decomposition [ 25] has been carried
out with the same assumption.
First, we compute the value associated with the nodes by apply-
ingtraininginputsfromtheconcernedoutputclass.Foraninput,ifthecomputedvalueatanodeis
‚â§0,thenweremovealltheincident
andoriginatededgesbychangingthecorrespondingvalueto0.Wedothatforalltheconcernedinputsfromthetrainingdataset.Ifthe
value associated with a node is
>0 for some input and ‚â§0 for other
inputs, we do not remove that node. For instance, for a layer Ld,
there are nLdnumber of nodes,and the preceding and thefollow-
ing layerhas nLd‚àí1andnLd+1nodes, respectively.For anynodeatthe layer Ld, there will be nLd‚àí1incident edges and nLd+1outgoing
edges. Based on our computation, if a node niis inactive (value
‚â§0), then all the incoming and outgoing weight edges ( ni(Ld‚àí1)+
ni(Ld+1))andonebiasedgeincidentto niwillberemoved.Wedo
the same for all the hidden dense layers.
Algorithm 1 Concern Identification (CI).
1:procedure Initialization( model)
2:convW,convB=[]
3:foreach layer ‚ààmodeldo ‚äøRetrieve the weight and bias
4: iflayertype=="Convolution" then
5: convW.add(layer.Wei–¥ht);convB.add(layer.Bias);
6: else
7: iflayertype=="Dense" then
8: denseW .add(layer.Wei–¥ht);denseB.add(layer.Bias);
9:returnconvW,convB,denseB,denseW
10:procedure CILayer ( model,input,convWLayer,convBLayer,
convMap Layer,pad=with,Stride=1,first=False)
11:I=input
12:slidin–¥w=procSlidin–¥ (I,pad,(Stride,Stride))‚äøSliding window
13:Output=slidin–¥w‚àóconvWLayer+convBLayer
14:flatOutput =flatten(Output) ‚äøConvert into an 1-D array
15:forj=0toj=|flatOutput |do
16: iffirstthen
17: ifflatOutput [j]<=0then ‚äøIdentify the inactive nodes
18: convMap Layer.add(j)
19: else ‚äøRemove the inactive node if it is active for other inputs
20: ifj‚ààconvMap Layerthen
21: ifflatOutput [j]>0then
22: index=findIndex (flatOutput ,j)
23: temp=[]
24: fork=0tok=convMap Layerdo
25: ifk!=indexthen
26: temp.add(convMap Layer[k])
27: convMap Layer=temp
28:returnconvMap Layer,Output
29:procedure CI (model,input,convMap )
30:convW,convB,denseB,denseW =initialization (model)
31:foreachlayer‚ààmodeldo ‚äøPerform CI for all the layers
32: count=0
33: iflayertype==‚ÄúConvolution ‚Äùthen
34: convMap[count],output=CILayer(( model,input,convW[count],
convB[count],convMap[ count],pad=layerpad,Stride=layerstride))
35: iflayertype==‚ÄúAvera–¥ePoolin–¥ ‚Äùthen
36: output = AvgPool(( model,PoolSize))
37: iflayertype=‚ÄúMaxPoolin–¥ ‚Äùthen
38: output = MaxPool(( model,PoolSize))
39: iflayertype=‚ÄúAdd‚Äùthen
40: output = input+ Previous input ‚äøAdd both the layers that are
merged
41: iflayertype=‚ÄúDense‚Äùthen
42: output = denseMod( model,input,indicator =False,
denseW [count],denseB[count]) ‚äøApply dense-based CI
43: iflayertype=‚ÄúFlatten‚Äùthen
44: output = flatten( input) ‚äøApply flatten-based CI
45: input=output
Convolution Layers: In a convolutional layer, we identify the
inactive sections in the output nodes by using a mapping-based
technique that stores the position of the nodes that are not part of
a module. In Algo.1, we describe the steps involved in building the
map and storing the nodes‚Äô positions. First, we store the weight
and bias for all the layers. Then, we identify the parts of the convo-
lution layer that are not used for a single output class. We start by
computing all possible combinations of sliding windows at line 12.
To buildtheslidingwindows,we usethe stride,paddin–¥asinput.
Below, we describe each such parameter.
527Decomposing Convolutional Neural Networks into Reusable and Replaceable Modules ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
Sliding Window. In convolutional layers, instead of one input
nodeatatime,asetofnodesaretakenasaninputforcomputation.
Forinstance,inFigure3,theblueboxisaslidingwindow.Foreachslidingwindow,oneormoreoutputnodesarecreatedbasedonthe
size of the shared weight in the layer.
Padding. Two variations of padding are possible in CNN, zero-
padding,andwith-padding.Inzero-padding,theinputisnotchanged.
For with-padding, the input is padded based on the size of the slid-
ingwindow,andthesizeoftheoutputwillbethesameastheinput.
For the example shown in Figure 3, we used the with-padding, and
that adds padding with value zero and transforms the input into (5,
5, 3) size (the white boxes are the added padding).
Stride.This parameter controls the movement of the sliding
windowwhilecomputingtheoutput.Stridealongwiththepadding
decides the output of the layer.
Once we compute the sliding windows (line 12), we feed inputs
fromthetrainingdatasettoourapproachandobservetheoutput
valueofthatparticularconvolutionlayer.Atline13,wecomputethe
outputoftheconvolutionlayerbasedonS*W+B,whereS,W,and
B denote the sliding window, shared weight, and bias, respectively.
Then,wemonitorthevalueateachnode(line14-28).Ifanodehasa
value‚â§0,westorethepositionofthenodeinourmap.Weinitialize
the map with all the nodes (for the first input) that have value ‚â§0
(line 16-18). The firstflag is used to denote this first input to the
module. Then, we remove the nodes that were previously in the
mappinglist,butthenodeshaveapositivevaluefortheinputunder
observation (line 20-27). We perform such operations to identify
the section of the layer that is inactive for a particular concern. For
thebatchnormalizationlayer,thereisnoweightorbiasinvolved,
and the layer is utilized for normalizing the input based on the
valueslearnedduringthetrainingsession.Maxpoolingandaverage
pooling are utilized for reducing the size of the network using the
pool size. For merge or add layer, we add the value computed from
the two layers connected with this layer.
3.3 Tangling Identification
In concern identification, a module is created to identify the nodes
andedgesforasingleoutputclass.Sinceallthenodesandedges
relatedtotheotheroutputclassesinthedatasethavebeenremoved,
the module essentially characterizes any input as the concerned
outputclassorbehavesasasingle-classclassifier.Toaddthenotion
of unconcerned output classes and able to distinguish between the
concerned and unconcerned output classes, we bring back someof the nodes and edges to the module. In Tangling Identification
(TI), a limited set of inputs belonging to the unconcerned output
classeshavebeenadded.Basedonthepriorwork,weaddconcerned
andunconcernedinputswitha1:1ratio.Forinstance,ifwehave
a problem with 200 output classes and build a module based on
observing 1000 inputs from the concerned class, then we observe 5
inputsfromeachunconcernedclass(5x199=995 ‚âà1000).Forinstance,
fortheImageNet-200dataset,ifwehave400(E)examplesforthe
concernedclass,wecantake2examplesfromeachconcernedclass
(2x199=398) and closely match the number. In our prior work [ 25],
wehaveadoptedasimilarapproachtomatch.So,evenifthenumber
of examples is not equally distributed, each class needs to have at
leastfloor(E/(N-1)) examplesfor anN-class classificationproblem.3.4 Modularizing Concerns
So far, we have identified the section of the network that is respon-
sible for an output class and added examples from unconcerned
outputclasses.Ho wever,the moduleisstillan n-classclassification
problem.Wechanneltheoutputlayerforeachmoduletoconvert
that into a binary classification-based problem. However, beforeapplying the channeling technique, we remove irrelevant nodes
(donotparticipateintheclassificationtaskforamodule)usinga
bottom-up backtracking approach.DenseLayers:
Wechanneltheoutedgesasdescribedbytheprior
work[25].Insteadofhaving nnodesattheoutputlayer( Ld,where
dis the total number of dense-based layers), two nodes (concerned
andunconcernedoutputnodes)havebeenkept.Forinstance,the
concerned output class for a module is the first output class in the
dataset.Wehave noutputclassesand nnodes(V1,V2,...,Vn)atthe
output layer. Also, the layer preceding the output layer ( Ld‚àí1)h a s
nLd‚àí1nodes. For each node at the output layer, there will be nLd‚àí1
incidentedges.Forinstance,theincomingedgesfor V1nodewill
beE11,E21,...,EnLd‚àí11,whereEnLd‚àí1‚àó1(inthiscase,n=1)denotes
that an edge is connected with nth
Ld‚àí1node from Ld‚àí1layer and
the first node at the output layer. For the module responsible for
identifyingthefirstoutputlabel,theedgesincidenttothefirstnode
(as the concerned node is V1) at the output layer have been kept
intact. However, all the other edges are modified. All the edgesincident to any of the unconcerned nodes (
V2,V3,...,Vn) at the
Ldlayer will be updated by a single edge. The assigned weight for
the updated edge is the mean of all the edges (same for bias). Then,thatupdatededgehasbeenconnectedtoanodeattheoutputlayer,
whichistheunconcernednodeforthemodule.Foramodule,there
will be two nodes at the output layer, VcandVuc, whereVcand
Vucdenote the concerned node and the unconcerned node. All the
updated edges will be connected to the Vuc.
Modularizing Concern: Backtrack: Once the nodes at the out-
put layer are channeled, we backtrack the concerned and uncon-
cerned nodes to the previous layers. In this process, we remove the
nodes that only participate in identifying the unconcerned classes.
First, we discuss the backtracking approach to handle the dense
layersandothernon-denselayers,andthenwedescribehowwe
can backtrack till the input to reduce the size of the module. Inall approaches, we support the other layers, e.g., pooling, merge,
flatten.
Modularizing Concern: Backtrack To Last Convolutional
Layer (MC-BLC). Once we channel the output layer, we prune
the network based on the inactive nodes at the dense layers. In
atypicalCNNmodel,eithera Poolinglayer,Convolution layer,or
aMergelayer will precede Denselayers and the Flattenlayers In
this approach, we leverage thatinformation to backtrack through
theDenselayers (including Flattenlayer) to the last convolution
layer(s)(basedonthepresenceof Mergelayerornot)orthepool-
ing layer. In this process, we identify the nodes that have edges
E=/braceleftbig
Eij;i‚ààLd‚àí1,j‚ààLd,j‚ààVl(uc)/bracerightbig
, where all the edges are only
connected to the unconcerned nodes ( Vuc) at thedthlayer (line
4-7).Westartthebacktrackingprocessfromtheoutputlayerand
movethroughthedenselayer.Foreachlayer,weidentifythenodes
connected to the unconcerned nodes in the following layer and
removethemfromthemodel.Also,wetagtheremovednodesas
528ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Rangeet Pan and Hridesh Rajan
,QSXW,PDJH

&RQYROXWLRQ
2XWSXW
,QSXW,PDJH$IWHU3DGGLQJ
)RUZDUG0DSSLQJ&RPSXWDWLRQ
%DFNZDUG5HPRYDORI1RGHV
Figure 4: Backtrack Through a Convolution Layer.
Algorithm 2 MC-BLN:ModularizingConcern:BacktracktoLast
Convolutional Layer (MC-BLN).
1:procedure CMBLN(model,num_of_labels,module _class,UpdatedW ,
UpdatedB )
2:temp,tempL,tempPool =[]‚äøUpdateW: Weight for Dense Layers
3:DenseLen–¥th =|UpdatedW |‚äø‚äøUpdateB: Bias for Dense Layers
4:fori=0toi=UpdatedW [DenseLen–¥th ‚àí1][0]do
5: ifUpdatedW[DenseLength-1][i, vuc]<=-Œ¥andUpdatedW[DenseLength-
1][i,vc]>=+Œ¥then
6: temp.add(i)‚äøvc: concerned nodes, vuc: unconcerned nodes
7:tempL.add(temp)
8:fori=DenseLen–¥th ‚àí1|toi=0do‚äøBacktrack till the first Dense
9:temp=[]
10: forj=0toj=UpdatedW [i‚àí1][0]do
11: ifj‚ààtempL[i]then
12: ifUpdatedW[i-1][j:,]<=- Œ¥then
13: temp.add(i)‚äøAdd nodes that are only connected with vnc
14: tempL.add(temp)
15:fori‚ààtempL[|tempL‚àí1|]do
16: forj=0toj=i‚àópoolsize2+jdo ‚äøConvert to pooling layer
17: ifLayer-1=="Convolution" then
18: ifj not‚ààconvMap[depth] then
19: convMap[depth].add(j) ‚äøUpdate convolution layer map
20: ifLayer-1=="Add" then ‚äøUpdate two convolution layer maps
21: ifj not‚ààconvMap[depth] then
22: convMap[depth].add(j)
23: depthAdd2=Add.Input2
24: ifj not‚ààconvMap[depthAdd2] then
25: convMap[depthAdd2].add(j)
the unconcerned nodes for that layer. To identify the nodes that
stronglycontributetotheunconcernednode,weintroduceacon-
stant (Œ¥) to verify the value associated with the node. Based on
the experimental evaluation, we used Œ¥=0.5. Then, we backtrack
the nodes at the layer preceding the output layer is identified at
line 8-14. Finally, we backtrack to the flatten layer. In a traditional
CNN model for imageclassification, theflatten layeris preceded
by a pooling layer, or a merge layer, or a convolutional layer. If the
preceding layer is a convolution layer, we update the mapping (as
discussed in ¬ß3.5) for that particular convolutional layer. Since theconvolutionlayer‚Äôsoutputisdirectlyreshapedintotheflattenlayer,
there is a one-to-one relationship between the two layers. If the
precedinglayerisapoolinglayer,thentherewillbe X2(Xisthe
pool size) inactive nodes at the pooling layer for one inactive node
at the flatten layer. If the preceding layer is a merge layer, then the
convolution layers that are merged will be updated.
ModularizingConcern:BacktrackToInput(MC-BI). Inthe
previous approach, we can only backtrack from the output layer to
the last convolution layer. However, we cannot backtrack through
theconvolutionlayer.Inaconvolutionlayer,theinputnodescannot
bedirectlymappedwiththeoutputnodes.Forinstance,inFigure
4,theinputimageshownontheleftsideisturnedintotheimage
showninthemiddle,whichisafteraddingthepaddings(forthis
example,wechoose Validpadding).Intheoutput,thenodesonthe
top left corner for both arrays will be produced by the first sliding
window (blue box) from each array shown in the middle. So, for
mapping, a node in the output on the right side of the image, atleast 4 (in this example, we chose the sliding window size to be2x2) nodes can be mapped. Those four individual nodes are alsomapped with other nodes in the output. The black-colored nodein the middle is a part of two sliding windows (the blue box and
the orange box). To remove irrelevant nodes from the convolution
layer, wetake atwo-passapproach. First,we storethe positionof
the nodes in each sliding window with the nodes in the output
(forward pass). During the forward pass, we store the mapping
M=/braceleftbig
(Vi,Vj);Vj=f(Vi,W,B)/bracerightbig
, wherefdenotes the convolution
operation. During the backward pass, we remove the nodes.
Algorithm 3 MC-BI: Modularizing Concern: Backtrack to Input.
1:procedure Sliding_Window_Mapping( input,W,pad,stride)
2: mapping =[], count=0 ‚äøPerforms the forward pass and map input output
nodes
3:temp=zeroslike(input)
4:temp=temp.flatten()
5:fori=0toi=|temp|do
6:temp[i]=i+1
7:temp=temp.flatten()
8:slidin–¥_window =sw(temp,W,pad,stride)
9:fori=0toi=|slidin–¥_window |do
10: forj=0toj=W.len–¥th[3]do
11: mappin–¥ .add(temp[i][j],count)
12:procedure CMBI(input,W,pad,stride,B,preceedin–¥ _layer,
deactive _map)
13:convDepth =depth
14:mappin–¥ _window =Slidin–¥_Window _Mappin–¥ (input,W,pad,
stride,B)
15:source_mappin–¥ =mappin–¥ _window [:0] ‚äøInput nodes
16:sink_mappin–¥ =mappin–¥ _window [:1] ‚äøOutput nodes
17:foreachdeactived _node‚ààdeactive _mapdo
18: source=source_mappin–¥ [sink_mappin–¥ ==deactived _node]
19: fla–¥=True ‚äøIdentify the source, where sink is deactive
20: source_mappin–¥ =source_mappin–¥ [source_mappin–¥ ]>0]
21: forsource_node‚ààsource_mappin–¥ do
22: iffla–¥==truethen:
23: sink_node=sink_mappin–¥ [source_mappin–¥ ==source_node]
24: if|sink_node‚àídeactive _map|>0then
25: fla–¥=false‚äøAll sinks formed by source are not deactive
26: iffla–¥==truethen
27: foreachsource‚ààsource_mappin–¥ do
28: ifsourcenot‚ààdeactive _mapthen
29: deactive _map[‚àí1].add(source+1)‚äøUpdate Map
30: ifpreceedin–¥ _layer=="Add"then
31: deactive _map[‚àí2].add(source+1)‚äøUpdate Map
529Decomposing Convolutional Neural Networks into Reusable and Replaceable Modules ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
In Algo. 3, we describe the step to do the mapping. From line
2-11, the forward pass has been described. In the forward pass, we
storethemappingbetweentheslidingwindowandoutputnodes.
Inordertodenotethepositionoftheslidingwindow,wemarkeach
node witha uniquenumber (line5-6) beforeadding the padding.
Forpadding,thenodesaremarkedas‚Äú0‚Äùastheyarenotpresentin the input of the Convolution layer. Then for each output node,
the input nodes are stored in a list. In this process, we define an
operationnamed swthatcomputestheslidingwindowsfromthe
weightW, padding pad, and stride. Now, we compute the mapping
withtheinputandtheoutputnodesatline13.Then,weseparate
the input and the output nodes at line 15 and 16. We scan through
the inactivate nodes in the output and identify if they match thepattern as illustrated in Figure 4. We identify all the input nodes
that aremapped with anoutput node andvice versa. We focus on
searching nodes that are not part of the padding operation andremove the nodes marked with 0 at line 20. For each such inputnode, we find all the output nodes generated from the particularinput node. If these output nodes are already in the deactivationnode list, we add the input node to the deactivation list for the
preceding convolution (output of the preceding convolution layer
istheinputofthenextconvolutionlayer).Ifthepreviouslayeris
anaddlayer,thenthetwoconvolutionlayersthataremergedatthe
merge layer are updated with the changes. If the preceding layer is
a pooling layer, the update is carried based on the pool size.
3.5 Updating The Decomposed Modules
Inthepriorstudy,reusingmodulesthatoriginatedfromdifferent
datasets involved non-trivial cost. Our intuition is that since themodules are originated from a different dataset, they still have
sometraitsoftheparentdataset.Infact,byapplyingthetangling
identification approach, we deliberately add some unconcernedexamples to learn the modules on how to distinguish betweenthe concerned and the unconcerned e xamples. Howev er, in the
inter dataset scenarios, the unconcerned output classes are not the
same.Tosolvethisproblem,weproposetoupdatethedecomposedmodules.InFigure5,weillustrateareusescenario,wheremoduleFisoriginatedfromdataset1andmodule2isoriginatedfromdataset
2. Dataset 1 represents a set of English letters (A-G), and applying
decomposition creates modules for each output class. Similarly,
dataset2representsasetofEnglishdigits(1-7),anddecomposition
creates sevenmodules, eachfor oneoutputclass. Whenmodule F
andmodule2arereusedinascenario,basedonthepriorwork,each
input belongs to 2, and F will be given as input to the composition
of the decomposed modules. However, due to the parent datasettraits, module 2 can recognize itself but does not know how todistinguish from any input belonging to the output class F. Tolearn the concerned output classes in this scenario, we take theunconcerned section of the dataset. For instance, for module F,the unconcerned output class will be output class 2 from dataset2. We take the examples from output class 2 from dataset 2 and
update module F by removing the nodes responsible for detecting
the output class 2. We do the same for module 2, where we removethe nodes responsible for recognizing output class F from dataset 1.
Finally, the modified modules are ready to be reused.q≈¢ƒç∆¢≈çƒïH q≈¢ƒç∆¢≈çƒï •$
5H7UDLQ 5H7UDLQ¬Ω∆Çƒç√™∆öƒïs≈¢ƒçƒï∆çƒ¨≈¢∆Ö
~∆¢∆ö∆Ç∆¢∆öƒÜ≈ç√™∆ç∆ç •ƒ∏≈ò
ƒç√™∆ö√™∆çƒï∆ö •¬Ω∆Çƒç√™∆öƒïs≈¢ƒçƒï∆çƒ¨≈¢∆Ö
~∆¢∆ö∆Ç∆¢∆öƒÜ≈ç√™∆ç∆çHƒ∏≈ò
ƒç√™∆ö√™∆çƒï∆ö §
q≈¢ƒç∆¢≈çƒïHÃëq≈¢ƒç∆¢≈çƒï •Ãë%&'(*  )) '√™∆ö√™∆çƒï∆ö §'√™∆ö√™∆çƒï∆ö •
Figure 5: Updating the Decomposed Modules.
4 EVALUATION
In this section, we discuss the experimental settings. Furthermore,
we discuss the performance of decomposing the convolutional
neuralnetworkintomodulesbyansweringthreeresearchquestions,
1) does decomposition involve cost?, 2) how module reuse and
replacement can be done compared to retraining a model from
scratch?,and3)doesreusingandreplacingthedecomposedmodules
emit less CO2?
4.1 Experimental Settings
4.1.1 Datasets. Weevaluateourproposedapproachbasedonthree
widely used and well-vetted datasets.
CIFAR-10 (C10) [20]: This dataset comprises of 3-D images of
different objects. It has 10 output classes, and the dataset is divided
into trainingand testingdatasets. Thetraining datasethas 50,000
images, and the testing dataset has 10,000 images.CIFAR-100 (C100) [20]:
This dataset has 100 output classes. This
iswidelyusedtoevaluateCNN-basedstudiesduetothecomplexity
of the dataset. Here, there are 50,000 training images and 10,000
testing images. The image size is similar to the CIFAR-10.ImageNet-200(I200)[21]:
Thisdatasetisverypopularandwidely
used tomeasure thescalability of CNN-basedapplications. Unlike
theprevioustwodatasets,thetotalnumberofoutputclassesis200.
ImageNetdatasethasbeendesignedtohelpthecomputer-vision
relatedtaskinmachinelearning.Thisdatasethas80,000+nouns
(name of the output class), and for each class, there are at least 500
images associated with it. This dataset has been used in trainingmodels in real-life scenarios. However, due to the complexity ofthe dataset, a smaller dataset with similar complexity has beenmade public for research purposes. This dataset (ImageNet-tiny)comprises 200 types of images. The training dataset has 100000
images, and the testing has 10000 images.
4.1.2 Models. We used the ResNet [ 14,15] models to evaluate
our proposed approach. In a ResNet model, there are multiple
blockspresentandeachblockconsistsof Convolution ,Add,and
Activation layer.TherearetwoversionsofResNet,theoriginal
version, where there is a stack of Convolution ,Add, and Batch
Normalization layers are put together to form a residual block,
and those residual blocks build the network. In the second ver-
sion, the residual block is modified to form a bottleneck layer with
530ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Rangeet Pan and Hridesh Rajan
Table 1: Decomposition Effectiveness and the Variability between the Decomposed Modules and the Trained Model.
Acc CI+TI+MC CI+TI+MC-BLN CI+TI+MC-BLN+MC-BIModelTop 1 Top 5 Top 1 Top 5 JITop 1 Top 5 JITop 1 Top 5 JI
CIFAR10-R20 87.1% 99.5% 86.9% 96.0% 0.7587.0% 99.4% 0.7586.8% 95.8% 0.75
CIFAR10-R32 85.3% 99.2% 78.4% 96.0% 0.5178.6% 98.1% 0.5178.4% 97.0% 0.51
CIFAR10-R56 86.7% 99.4% 84.0% 93.9% 0.4184.5% 98.9% 0.4184.0% 93.9% 0.41
CIFAR100-R20 46.3% 75.2% 45.6% 75.4% 0.6345.6% 75.4% 0.6345.6% 75.4% 0.63
CIFAR100-R32 44.5% 74.6% 41.2% 71.0% 0.5241.2% 71.0% 0.5241.2% 71.0% 0.52
CIFAR100-R56 45.7% 75.1% 44.9% 74.5% 0.6844.9% 74.5% 0.6844.9% 74.5% 0.69
ImageNet200-R20 33.2% 59.7% 29.6% 56.1% 0.4332.6% 59.7% 0.7532.6% 59.7% 0.75
ImageNet200-R32 31.8% 58.6% 21.0% 58.0% 0.3331.5% 58.0% 0.7231.5% 58.0% 0.72
ImageNet200-R56 32.1% 58.7% 30.0% 56.7% 0.3031.0% 57.8% 0.7131.0% 57.8% 0.71
Acc: Accuracy, CI: Concern Identification, TI: Tangling Identification, MC: Modularizing Concern, MC-BLN: Modularizing Concern: Backtrack to Last Convolutional Layer, MC-BI:
Modularizing Concern: Backtrack to Input, and JI: Jaccard Index.
Convolution ,Add,and Batch Normalization .Weperformedour
evaluation against the first version of the network for simplicity
andtoreducethetrainingtimeandresources.Eachsuchversion
can either have Batch Normalization layer or not as described
in the original paper. Since Batch Normalization is only used to
trainanddoesnotparticipateintheprediction,wechosethemodel
without Batch Normization toreducethetrainingtimeassome
ofourexperimentsincludetrainingmodelsmultipletimes.Weused
ResNet-20, ResNet-32, and ResNet-56, where 3, 5, and 9 residual
blocksarepresentwith21,33,and57convolutionlayers,respec-
tively.Thereportedmodelaccuraciesaredifferentfromtheoriginal
paper, as we trained the model from scratch with 200 epochs.
4.1.3 Metrics Used. Accuracy. Wecomputethecomposedaccu-
racyofthedecomposedmodulesasshowninthepriorwork[ 25].
Panet al.computed the top-1 accuracy for all the experiments,
whereas we compute both top-1 and top-5 accuracies.
JaccardIndex. Similar to the prior work, we compute the vari-
ability between the model and the modules using Jaccard Index.
CO2eEmission. In this context, we refer to CO2eas carbon
dioxide and equivalent gases. To measure the total emission of
such gases due to computation, we utilize the metrics used by
Strubelletal.[31].Thetotalpowerconsumptionduringthetraining
is measured as ,
pt=1.58t(pc+pr+–¥p–¥)
1000(1)
In this equation, pc,pr,p–¥, and–¥denote the average power con-
sumptionofCPU,DRAM,GPU,andthetotalnumberofGPUcores,
respectively. The tdenotes the time. Here, 1.58 denotes the PUE
co-efficient, which is the same value used in the prior work. We
performed our experiment on iMac with 4.2 GHz Quad-Core Intel
Core i7 and 32 GB 2400 MHz DDR4 RAM. Since we do not have
anyGPU,both –¥andp–¥arezero.Thepowerconsumptionhasbeen
measuredusingIntelPowerGadget[ 18].Finally,the CO2eemission
has been computed as,
CO2e=0.954pt (2)
4.2 Results
In this section, we evaluate our approach to understand the cost
involved in decomposition, whether the decomposed modules can
be reused or replaced, and how decomposition can be beneficial
compared to training from scratch.4.2.1 Does Decomposing CNN Model into Module Involve Cost? To
understandhowdecomposingCNNmodelsintomodulesperforms,
weevaluateourapproachon9datasetsandmodelcombinations.
First, we decompose the CNN model into small modules, and then
wecomposethemtoevaluatehowthesecomposedmodulesper-
formcomparedtothetrainedCNNmodels.InTable1,thecomposedaccuracyofthedecomposedmodulesandthetrainedmodels‚Äôaccu-
racy have been shown. We report the top-1 and top-5 accuracies
of the models.Here, in the first and secondcolumns, we show the
top-1andtop-5accuracyofthemodel.Whereas,incolumns3-11,
the accuracy shown is from the composition of the decomposed
modules.Whilecomposingthemodules,weapplythevoting-based
approach that is similar to the prior work. Also, we compute the
Jaccard index (JI) to identify the average variability between the
modules and the model. Lesser value of the JI represents better
decomposition as the modules are desired to be significantly differ-
ent from the model. Suppose the value of the Jaccard index is very
high.Inthatcase,itdenotesthatthemodulehasessentiallybecome
similar to the model. While the lower JI is a criterion for better
decomposition,thecostisanothercriteriatobeconsideredwhile
decomposingamodelintomodules.Inthisstudy,ourobjectiveis
tohavetheleastcostofdecompositionwiththemostdissimilaritiesbetweenthemodulesandthemodel.Wefoundthatinallthecases,
there isa costinvolved while decomposing.For instance, ourfirst
approachidentifiestheconcern,addsnegativeexamples,andfinallymodularizestheconcerninvolves3.46%and2.54%(top-1andtop-5)
of loss of accuracy with an average Jaccard index 0.50. Whereas
applyingdense-basedbacktracking,thelosshasreduced.Theav-
eragelosswiththisapproachis1.77%and0.85%,andtheaverage
Jaccardindexis0.63.Forapproachinvolvingthebacktrackthrough
theconvolutionlayerincludesalossof1.86%and1.93%accuracy
with an average 0.64 Jaccard index. For the further experiments,
we choose the second approach as the loss is the least of all. Based
on these results, we can conclude that decomposition is possible in
CNN models. However, it involve s a small cost, and the modules
produced are significantly different from the models. For further
studies, we used the dense-based backtracking technique.
4.2.2 How module reuse and replacement can be done compared to
retraining a model from scratch? Here,weevaluatehowdecomposed
modules can be reused and replaced in various scenarios.
Reusability: Table 2, and 3 show two different reuse scenarios:
1) intra-dataset reuse, 2) inter-dataset reuse.
531Decomposing Convolutional Neural Networks into Reusable and Replaceable Modules ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
Table 2: Intra Dataset Reuse. MA: Composed Module Accuracy, TMA: Trained Model Accuracy, C10: CIFAR-10, C100:
CIFAR-100, I200: ImageNet-200, and C{x}: Output Label x
(a) CIFAR-10 and CIFAR-100 Reuse. Yellow and Green represent
the CIFAR-10 and CIFAR-100 reuse scenarios, respectively.
C10 C2 C3 C4
C100C10TMA MATMA MATMA MA
C1C195.4% 98.8% 84.7% 91.4% 91.9% 96.6%
C2C291.0% 91.0% 93.2% 99.3% 92.2% 99.3%
C3C396.0% 96.0% 95.0% 98.0% 81.2% 92.9%
C4C4100% 95.5% 100% 96%98.5% 100%
C100 C100-C1 C100-C2 C100-C3(b) ImageNet-200 Reuse. Yellow represents the ImageNet-200
reuse scenarios. Blackrepresents the wrong combination of
reuses.
I200C2 C3 C4
TMA MA TMA MATMA MA
C190.00% 98.00% 93.0% 94.0% 92.0% 94.0%
C2 95.0% 88.0% 95.0% 95.0%
C3 89.0% 94.0%
C4
Table 3: Inter Dataset Reuse.
C100C10 C1 C2 C3 C4
TMA MA MMA TMA MAMMA TMA MAMMA TMA MAMMA
C1 97.8%83.0% 91.5% 96.7%87.9% 92.1% 96.6%86.0% 91.5% 98.8%87.2% 93.8%
C2 95.5%82.4% 89.2% 98.6%87.5% 88.1% 95.6%85.2% 89.9% 99.7%84.1% 89.0%
C3 98.4%78.2% 87.5% 97.9%79.5% 88.5% 94.6%80.2% 89.2% 99.6%81.6% 87.9%
C4 98.0%88.3% 88.1% 97.2%88.6% 88.1% 94.7%86.7% 90.3% 99.0%88.9% 91.9%
TMA: Trained Model Accuracy, MA: Composed Module Accuracy, and MMA: Modified Module Accuracy.
Table 4: Intra Dataset Replace.
Top-1 Accuracy
Dataset TMA Prior MA RM0 RM1 RM2 RM3 RM4 RM5 RM6 RM7 RM8 RM9
CIFAR-10 85.3% 78.6% 80.6% 79.2% 80.0% 80.9% 80.7% 80.2% 79.8% 79.6% 79.4% 79.6%
CIFAR-100 44.5% 41.2% 40.7% 41.0% 41.2% 41.1% 41.2% 41.1% 41.2% 41.2% 41.1% 41.0%
ImageNet-200 31.8% 31.5% 32.4% 32.2% 32.3% 32.0% 32.3% 32.4% 32.4% 33.3% 33.2% 33.3%
Top-5 Accuracy
Dataset TMA Prior MA RM0 RM1 RM2 RM3 RM4 RM5 RM6 RM7 RM8 RM9
CIFAR-10 99.2% 98.1% 100% 100% 100% 100% 100% 100% 100% 100% 100% 100%
CIFAR-100 74.6% 71.0% 70.7% 70.8% 71.0% 70.9% 71.0% 71.0% 71.1% 71.1% 70.9% 70.9%
ImageNet-200 58.6% 58.0% 59.3% 59.1% 59.1% 59.2% 59.0% 59.2% 59.1% 59.2% 59.2% 59.1%
TMA: Trained Model Accuracy, MA: Composed Module Accuracy, RM{x}: Replace Module x with another Module for x.
Table 5: Inter Dataset Replace. (All results are in %.)
C100C10 C1 C2 C3 C4
M1D1M5D5M1D1M5D5M1D1M5D5M1D1M5D5
C1 86.287.399.695.983.187.498.995.686.187.199.595.486.787.099.695.4
C2 85.386.399.290.882.786.398.990.584.686.199.190.387.686.099.490.3
C3 86.288.199.491.581.888.398.891.386.288.099.091.287.987.899.691.3
C4 84.789.999.291.482.689.798.791.185.089.598.891.186.689.399.491.1
M1 and M5: Top-1 and Top-5 Accuracy for Trained Model D1 and D5: Top-1 and Top-5 Composed Accuracy for Decomposed Modules.
In intra-dataset reuse, we build a model by taking two output
classes from a dataset and building a new model. For instance,
output classes 0 and 1 have been taken from CIFAR-10, and we
evaluatetheaccuracybycombiningthedecomposedmodulesfor
output classes 0 and 1 (we represent the output class with their
index in the dataset). We compare the performance with retraining
a model with the same structure with the same output classes and
measure the accuracy. We took the best model for each dataset
(based on the trained model accuracy) and the modules created
fromthat.ForCIFAR-10,CIFAR-100,andImageNet-200,thebest
model in terms of training accuracy are ResNet-56, ResNet-20, and
ResNet-20,respectively.Sincethesemodelstakealongtimetotrain,
weperformedtheexperimentsfor4outputclassesrandomlychosen
from the dataset to evaluate the reusability scenarios. In Table 2,theintra-datasetreusescenariosarereported.C1,C2,C3,andC4
denote the four randomly chosen classes, and they are different foreachdataset.Sincewetake2outputclassesineachexperiment,the
totalnumberofchoicesforadatasetwith noutputclassesunder
experiment is/parenleftbign
2/parenrightbig. In our cases, we take 4 output classes from each
dataset to evaluate, and the total choices would be/parenleftbig4
2/parenrightbigor 6. For
CIFAR-10, reusing the modules increase 6.6% accuracy, on average.
ForCIFAR-100,thereisanaverage0.67%loss,but4/6casesreusing
the modules perform better than the trained model. For ImageNet,
thereisanaverage1.5%increaseinaccuracy.Overall,thegainin
the accuracy in intra-dataset reuse is 2.5%.
In inter-dataset reuse, we take two output classes from different
datasets and build a binary classification-based problem. For ex-
ample,output classes1 and2have beentakenfrom theCIFAR-10
532ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Rangeet Pan and Hridesh Rajan
and CIFAR-100 datasets, respectively. We compute the accuracy
the same way as we did for intra-dataset reuse scenario. Since the
modelstructureofthedecomposedmodulesfortwodatasetsare
different, e.g., the decomposed modules for CIFAR-10 follow the
modelstructureofResNet-56,whereasitisResNet-20forCIFAR-
100.Thebestmodelintermsofaccuracyhasbeentakenforretrain-
ing.WedidapilotstudyandfoundthatResNet-20doesbetterin
terms of accuracy for the inter-dataset reuse scenario for CIFAR-
10andCIFAR-100datasets.Toovercomeoverfitting,westorethe
checkpointsforalltraining-basedevaluationsandreportthelast
checkpoint with the best validation accuracy. In Table 3, the inter-
datasetreusescenariosarereported.Wefoundthatinter-dataset
reuse has a non-trivial cost. In this case, the loss of accuracy is
12.06%.Ourintuitionisthatsincethesemodulesoriginatefroma
differentdataset,theystillhavesometraitsoftheparentdataset.
In fact, by applying the tangling identification approach, we delib-
erately add some non-concerned examples to distinguish between
the concerned and the non-concerned examples. To alleviate the
effect,weperformacontinuouslearning-basedapproach,where
we re-apply the tangling identification with the unseen output
class(es). We found that updating the decomposed modules canreduce the cost of decomposition significantly. The overall loss
while reusing modules is 7.63%, which is a 4.4% gain compared
to the previous reusability approach. Since the input size of the
CIFAR-10andCIFAR-100imagesarenotthesameasImageNet-200,
bothtrainingasinglemodelandreusingmodulescannotbedone.
Replaceability: Similartothereusabilityscenario,weevaluated
both inter and intra-dataset reuses. For intra dataset reuse, we
replaceamoduledecomposedfromamodelwithlessaccuracywithamodulefor thesameoutputclassdecomposed from amodelwith
higher accuracy. In Table 4, we report the evaluation for replacing
the module.For each dataset, weevaluate for 10output classes to
matchthetotalnumberofclassesforallthedatasets.ForCIFAR-10,
wereplaceamoduledecomposedfromResNet-32withamodule
forthesameoutputclassdecomposedfromResNet-20.Wefound
that the intra-dataset reuse increases the accuracy by 0.7% and
1.0% for top-1 and top-5 accuracy, respectively in comparison to
thepriorcomposedaccuracyofthedecomposedmodules.Infact,
80.0% (24/30) and 83.3% (25/30) times replacing a module does the
better or the same compared to the composed accuracy of thedecomposed modules for top-1 and top-5 accuracy, respectively.
Also,for33.3%and66.7%cases,reusingmodulesdoesbetterthan
the model accuracy for top-1 and top-5 accuracies, respectively.
Although we incorporated the module update approach, there is
no significant increase in accuracy (top-1: 0.02% and top-5: 0.01%).
Inter-datasetreplacementscenariosarereportedinTable3.To
reducetheexperimenttime,wereplaceamodulefromCIFAR-10
with a module taken randomly from CIFAR-100 and report both
composed accuracy and the accuracy of the trained model for four
different output classes. We found that for top-1 accuracy, there is
a2.5%accuracyincrease,andfortop-5,thereisalossof7.1%,on
average.Motivating Example.
Here, we recreate the example shown in
Figure1.SincehumanfaceandGorillaimagesarenotpresentin
theImageNet-200dataset,weaddedtwoclassesfromtheImageNet
large dataset. However, there is no class specifically categorizedas ‚Äúhuman face‚Äù in the dataset. So, we take the closest class thatTable 6: Scenarios Created Based on Figure 1.
Strategies Top-1 Acc Top-5 Acc
Initial Model 26.6% 50.4%
MA 26.0% 50.4%
Option 1 27.2% 54.6%
Option 2 25.0% 50.4%
Option 3 24.0% 48.2%
Option 4 25.3% 50.8%
containstheimagesofdifferentpersons.First,webuildamodelwith
202 (ImageNet-200 + Person + Gorilla) output classes. For the first
option, we create a hypothetical model that does not exhibit faulty
behavior.Wedothatbytrainingamodelwithmoreepochs(+100
epochs) and achieve higher accuracy (+1.8%). Then, we decompose
the model into modules. Then, we replace the Gorilla module with
themodulecreatedfromthesecondmodel.Inthesecondoption,
we train a model with Person and Gorilla examples and replace
thefaulty Gorillamodule withthenewly createdone.In thethird
option, we remove the Gorilla module from the set of modules.
Finally,inthefourthoption,iftheoriginalmodelpredictsPerson
or Gorilla, we re-verify with the modules created from the 2-class
classification model in the second option. The evaluation has been
shown in Table 6, and we found that enabling the replacement and
reuse of modules can solve the problem. Based on the need and
available resources, users can pick any of the four options.
∆ö∆ô∆†∆ö∆π∆°
∆°∆ô∆ú∆π∆¢
∆ù∆û∆π∆û∆õ∆†∆°∆ö∆†∆õ
∆û∆°∆ö∆ü
∆ü∆û∆†
∆ö∆ù∆ü∆õ∆ö∆û∆õ∆ú
∆û∆ú∆ù∆¢∆ü∆ú
∆ú∆†∆õ∆õ
 ∆ö∆ö∆°

«ê∆ö∆ô

)/-/. / 0. 
«ê∆ö∆ô∆ô
)/-/. / 0. 
("  /«ê∆õ∆ô∆ô

)/-/. / 0. 
)/ -/. /
 0. 
)/ -/. / +' ( )/
∆ô∆û∆ô∆ô∆ö∆ô∆ô∆ô∆ö∆û∆ô∆ô∆õ∆ô∆ô∆ô∆õ∆û∆ô∆ô∆ú∆ô∆ô∆ô∆ú∆û∆ô∆ô∆ù∆ô∆ô∆ô∆ù∆û∆ô∆ô∆û∆ô∆ô∆ô∆õ ($..$*)«ö'.«õ-$)$)"∆õ 
«ö$3 +*#«õ
-$)$)"∆õ 
2$/# ./* ' 0. 
∆õ 
 0. ∆õ 
2$/#+/$)"/# *0' .
∆ö∆ö∆†∆ö∆ù∆°
Figure 6: Comparison of CO2eEmission.
4.2.3 Does reusing and replacing the decomposed modules emit less
CO2e?Strubelletal.[31]haveidentifiedthattrainingamodeloften
emitssixtimesmore CO2ethanacaremitsoverayear.Tounder-
stand how decomposition can help to reduce the harmful effect,
we measure the CO2eemission for both intra and inter-dataset
reuse and replace scenarios. First, we start computing the power
consumption before executing the program. Then, we measure the
averagepowerdrawnbyothertasksrunninginthebackgroundfor
10 sec and compute the average power drawn by CPU and DRAM.
We negatethese two values to separatethe resource consumption
533Decomposing Convolutional Neural Networks into Reusable and Replaceable Modules ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
of the program and other background tasks. Then, we measure
the power consumption for each 100 ms (default for Intel Power
Gadget). Figure 6 shows the CO2eemission for different scenarios.
We do not show intra dataset replacement scenarios as that cannot
be compared with training from scratch. For training scenarios, we
build the model from scratch, and after training, we predict thesame set of images. This experimental setting has been done tocompare a similar situation, where developers need to build and
predictinputs.Thevaluereportedinthefigureistheaverage CO2e
consumption for all the experimented scenarios described in ¬ß4.2.2.
Also, since the epoch has been fixed for each retraining, the power
consumption is somewhat fixed for each retraining scenario. How-
ever,thebestmodelcanbefoundearlier.Forinstance,amodelis
trained with 100 epochs, but the best model is found at the 20th
epoch.Toremovetheeffectofoverfitting,wecomputethepower
consumed until the return of the best model and report that in the
figure.Wefoundthatforreusescenarios,decompositionreduces
theCO2econsumption by23.7x and18.3x forthe fixedepoch and
thebest modelscenarios,respectively.For replacementscenarios,
it is 42x and 31.5x, respectively. If we update the decomposed mod-
ules, there is a slight increase in resource consumption, but stillsignificantly less than training from scratch. Also, we computed
the additional CO2econsumption for the one-time decomposition
approach and found that on average 116, 371, and 3800 lbs of CO2e
has been generated for decomposing CIFAR-10, CIFAR-100, and
ImageNet-200models,respectively.Theoverheadissignificantly
lower for CIFAR-10 and CIFAR-100 compared to training a new
modelforbothreuseandreplacescenarios.However,forImageNet-
200, the overhead is high, but it is a one-time operation that can
enable both reuses and replacements at a very low cost.
5 DISCUSSION ON MODULARITY
Insection,wediscussthebroaderviewofincorporatingthenotionofmodularityinthedeeplearningmodels.First,wediscusshowour
approach can be generalized to the other type of DL models. Then,
wediscusstheideaofbuildingmodelswithmodulararchitecture
from scratch.
Generalizability. In this work, we focus on a specific type of
neural network, a convolutional neural network. We extend our
ideatodecomposeaDNNmodelintomodulesfromourprevious
work on dense-based model [ 25]. In both works, we found that
decomposing a model into modules can enable the reusability and
replaceability of the modules to build a new problem. However,to generalize this approach to other kinds of deep learning mod-els, e.g., natural language-based models, our approach needs tobe extended for new layers, e.g., LSTM, GRU, RNN, etc. In such
networks, often, there is a presence of loop structure, which might
bechallengingtohandleduringdecomposition.Also,ourapproachisbasedontheassumptionthatReLUisbeingusedastheactivationfunction. This assumption holds for our application domain, which
is the image-based classification. However, for natural language
processing-based models, other activation functions, e.g., Sigmoid,
Tanh,etc.,arecommonlyused.Insuchcases,asimilardefinitionof
active-inactive nodes is needed to apply the concern identification
and subsequent steps. In both cases that we discussed, there is aneedforfurtherexperimentstodetermine whetherourapproach
will generalize.
Building Modular Network. While in this work, we have illus-
tratedhowtrainedmodelscanbedecomposedintomodules,one
can build the models using more modular architectural techniques.
There is a vast body of works [ 2,16,17,29] on training a model in
amodularfashionthathelpsinreducingthetrainingtimeandin-
creasing the performance of the final model. However, such works
have not yet focused on how a more modular network can be built
that can be more reusable and replaceable. We believe this is an
interestingavenuetoextendthenotionofmodularityinDLmodels.
6 CONCLUSION AND FUTURE WORK
In this paper, we introduce decomposition in convolutional net-
worksandtransformatrainedmodelintosmallercomponentsor
modules. We used a data-driven approach to identify the section
in the model responsible for a single output class and convert that
into a binary classifier. Modules created from the same or different
datasets can be reused or replaced in any scenario if the input size
of the modules is the same. We found that decomposition involves
a small cost of accuracy. However, both intra-dataset reuse and
replaceabilityincreasetheaccuracycomparedtothetrainedmodel.
Furthermore, enabling reusability and replaceability reduces CO2e
emissionsignificantly.Forthiswork,weomittheheterogeneous
inputs (the input size for modules are not the same) while reusing
and replacing modules, and it will be a good research direction
forthefuturetostudy howaninterfacecouldbebuiltaroundthe
modules to take different types of inputs. Also, we will also lookinto how decomposing model into modules help to debug, local-
ize,andfixbugs[ 40,41].Anotherdirectioncouldbelookinginto
other properties of the DL model e.g., fairness [ 5,6], in light of the
modularity.
ACKNOWLEDGEMENTS
This work was supported in part by US NSF under grants CNS-21-
20448 and CCF-19-34884. All opinions are of the authors and donotreflecttheviewofsponsors.WethankICSE‚Äô22reviewersfor
constructive comments that were very helpful. We are thankful to
Breno Dantas Cruz for the discussion and suggestion to help us
improve the quality of the paper.
REFERENCES
[1]Pierre America. 1990. Designing an object-oriented programming language
withbehaviouralsubtyping.In Workshop/School/SymposiumoftheREXProject
(Research and Education in Concurrent Systems). Springer, 60‚Äì90.
[2]JacobAndreas,MarcusRohrbach,TrevorDarrell,andDanKlein.2016. Neural
modulenetworks.In ProceedingsoftheIEEEconferenceoncomputervisionand
pattern recognition. 39‚Äì48.
[3]JohnWBackus,FriedrichLBauer,JulienGreen,CharlesKatz,JohnMcCarthy,
Peter Naur, Alan J Perlis, Heinz Rutishauser, Klaus Samelson, Bernard Vauquois,
et al.1960. Report on the algorithmic language ALGOL 60. Numer. Math. 2, 1
(1960), 106‚Äì136.
[4]John W Backus, Robert J Beeber, Sheldon Best, Richard Goldberg, Lois M Haibt,
Harlan L Herrick, Robert A Nelson, David Sayre, Peter B Sheridan, H Stern, et al .
1957. TheFORTRANautomaticcodingsystem.In PaperspresentedattheFebruary
26-28, 1957, western joint computer conference: Techniques for reliability. 188‚Äì198.
[5]Sumon Biswas and Hridesh Rajan. 2020. Do the machine learning models on
a crowd sourced platform exhibit bias? an empirical study on model fairness.InProceedingsofthe28thACMjointmeetingonEuropeansoftwareengineering
conference and symposium on the foundations of software engineering. 642‚Äì653.
534ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Rangeet Pan and Hridesh Rajan
[6]SumonBiswasandHrideshRajan.2021. Fairpreprocessing:towardsunderstand-
ing compositional fairness of data transformers in machine learning pipeline.
InProceedingsofthe29thACMJointMeetingonEuropeanSoftwareEngineering
Conference and Symposium on the Foundations of Software Engineering. 981‚Äì993.
[7]Luca Cardelli. 1997. Program fragments, linking, and modularization. In Proceed-
ings of the 24th ACM SIGPLAN-SIGACT symposium on Principles of programming
languages. 266‚Äì277.
[8]KrishnaKishoreDhara.1997. Forcingbehavioralsubtypingthroughspecification
inheritance. (1997).
[9]EdsgerWDijkstra.1982. Ontheroleofscientificthought. In Selectedwritingson
computing: a personal perspective. Springer, 60‚Äì66.
[10]EdsgerWDijkstra.2001. Gotostatementconsideredharmful. In Pioneersand
Their Contributions to Software Engineering. Springer, 297‚Äì300.
[11] Edsger Wybe Dijkstra et al. 1970. Notes on structured programming.
[12]MatthewFlattandMatthiasFelleisen.1998. Units:CoolmodulesforHOTlan-
guages. In Proceedings of the ACM SIGPLAN 1998 conference on Programming
language design and implementation. 236‚Äì248.
[13]Badih Ghazi, Rina Panigrahy, and Joshua Wang. 2019. Recursive sketches for
modular deep learning. In International Conference on Machine Learning. PMLR,
2211‚Äì2220.
[14]KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.2016. Deepresidual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 770‚Äì778.
[15]KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.2016.Identitymappings
indeepresidualnetworks.In Europeanconferenceoncomputervision.Springer,
630‚Äì645.
[16]Geoffrey E Hinton, Zoubin Ghahramani, and Yee Whye Teh. 2000. Learningto parse images. Advances in neural information processing systems 12 (2000),
463‚Äì469.
[17]RonghangHu,JacobAndreas,MarcusRohr bach,Trevo rDarrell,andKateSaenko.
2017. Learning to reason: End-to-end module networks for visual question
answering.In ProceedingsoftheIEEEInternationalConferenceonComputerVision.
804‚Äì813.
[18]Intel Corporation. 2021. Intel Power Gadget. https://software.intel.com/content/
www/us/en/develop/articles/intel-power-gadget.html.
[19] Keras. 2021. Keras Models. https://keras.io/api/applications/.[20]
AlexKrizhevsky,GeoffreyHinton,etal .2009. Learningmultiplelayersoffeatures
from tiny images. (2009).
[21]YaLeandXuanYang.2015. Tinyimagenetvisualrecognitionchallenge. CS231N
7 (2015), 7.
[22]Yann Lifchitz, Yannis Avrithis, Sylvaine Picard, and Andrei Bursuc. 2019. Dense
classificationandimplantingforfew-shotlearning.In ProceedingsoftheIEEE/CVF
Conference on Computer Vision and Pattern Recognition. 9258‚Äì9267.
[23]Barbara Liskov and Stephen Zilles. 1974. Programming with abstract data types.
ACM Sigplan Notices 9, 4 (1974), 50‚Äì59.
[24]Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. 2017. Thinet: A filter level prun-ing method for deep neural network compression. In Proceedings of the IEEE
international conference on computer vision. 5058‚Äì5066.
[25]Rangeet Pan and Hridesh Rajan. 2020. On decomposing a deep neural network
into modules. In Proceedings of the 28th ACM Joint Meeting on European Software
EngineeringConferenceandSymposiumontheFoundationsofSoftwareEngineering.
889‚Äì900.[26]DavidLParnas.1972. Onthecriteriatobeusedindecomposingsystemsinto
modules. In PioneersandTheirContributionstoSoftwareEngineering.Springer,
479‚Äì498.
[27]David Lorge Parnas. 1976. On the design and development of program families.
IEEE Transactions on software engineering 1 (1976), 1‚Äì9.
[28]DaniloRezende,IvoDanihelka,KarolGregor,DaanWierstra,etal .2016. One-
shot generalization in deep generative models. In International Conference on
Machine Learning. PMLR, 1521‚Äì1529.
[29]Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. 2017. Dynamic routing
between capsules. arXiv preprint arXiv:1710.09829 (2017).
[30]KSaiRam,JayantaMukherjee,AmitPatra,andParthaPratimDas.2018. HSD-
CNN:HierarchicallyselfdecomposingCNNarchitectureusingclassspecificfilter
sensitivity analysis. In Proceedings of the 11th Indian Conference on Computer
Vision, Graphics and Image Processing. 1‚Äì9.
[31]EmmaStrubell,AnanyaGanesh,andAndrewMcCallum.2019. Energyandpolicy
considerations for deep learning in NLP. arXiv preprint arXiv:1906.02243 (2019).
[32]Qianru Sun,YaoyaoLiu, Tat-SengChua, andBernt Schiele.2019. Meta-transfer
learning for few-shot learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. 403‚Äì412.
[33]XiaochuanSun,GuanGui,YingqiLi,RenPingLiu,andYongliAn.2018. ResInNet:
A novel deep neural network with feature reuse for Internet of Things. IEEE
Internet of Things Journal 6, 1 (2018), 679‚Äì691.
[34]Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M
Hospedales. 2018. Learning to compare: Relation network for few-shot learning.
InProceedingsof theIEEEconferenceon computervisionand patternrecognition.
1199‚Äì1208.
[35] Chuanqi Tan, Fuchun Sun, Tao Kong, Wenchang Zhang, Chao Yang, and Chun-
fang Liu. 2018. A survey on deep transfer learning. In International conference on
artificial neural networks. Springer, 270‚Äì279.
[36]PeriTarr,HaroldOssher,WilliamHarrison,andStanleyMSutton.1999.Ndegrees
of separation: Multi-dimensional separation of concerns. In Proceedings of the
1999 International Conference on Software Engineering (IEEE Cat. No. 99CB37002).
IEEE, 107‚Äì119.
[37]TheVerge.2021. Google‚Äôfixed‚Äôitsracistalgorithmbyremovinggorillasfrom
its image-labeling tech. https://www.theverge.com/2018/1/12/16882408/google-
racist-gorillas-photo-recognition-algorithm-ai.
[38]LisaTorreyandJudeShavlik.2010. Transferlearning. In Handbookofresearch
on machine learning applications and trends: algorithms, methods, and techniques.
IGI global, 242‚Äì264.
[39]Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and
DaanWierstra.2016. Matchingnetworksforoneshotlearning. arXivpreprint
arXiv:1606.04080 (2016).
[40]Mohammad Wardat, Breno Dantas Cruz, Wei Le, and Hridesh Rajan. 2022. Deep-Diagnosis:AutomaticallyDiagnosingFaultsandRecommendingActionableFixesinDeepLearningPrograms.In ICSE‚Äô22:The44thInternationalConferenceonSoft-
ware Engineering (Pittsburgh, PA, USA).
[41]Mohammad Wardat, Wei Le, and Hridesh Rajan. 2021. DeepLocalize: fault local-
izationfordeepneuralnetworks.In 2021IEEE/ACM43rdInternationalConference
on Software Engineering (ICSE). IEEE, 251‚Äì262.
[42]DavisWertheimerandBharathHariharan.2019. Few-shotlearningwithlocaliza-
tion in realistic settings. In Proceedings of the IEEE/CVF Conference onComputer
Vision and Pattern Recognition. 6558‚Äì6567.
535