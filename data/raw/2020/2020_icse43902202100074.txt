GUIGAN: Learning to Generate GUI Designs Using
Generative Adversarial Networks
Tianming Zhao
Jilin University
Changchun, China
zhaotm16@mails.jlu.edu.cnChunyang Chen∗
Monash University
Melbourne, Australia
Chunyang.Chen@monash.eduY uanning Liu
Jilin University
Changchun, China
lyn@jlu.edu.cnXiaodong Zhu∗
Jilin University
Changchun, China
zhuxd@jlu.edu.cn
Abstract —Graphical User Interface (GUI) is ubiquitous in
almost all modern desktop software, mobile applications, and
online websites. A good GUI design is crucial to the success of the
software in the market, but designing a good GUI which requires
much innovation and creativity is difﬁcult even to well-traineddesigners. Besides, the requirement of the rapid development
of GUI design also aggravates designers’ working load. So,the availability of various automated generated GUIs can help
enhance the design personalization and specialization as they can
cater to the taste of different designers. T o assist designers, we
develop a model GUIGAN to automatically generate GUI designs.
Different from conventional image generation models based on
image pixels, our GUIGAN is to reuse GUI components collected
from existing mobile app GUIs for composing a new design thatis similar to natural-language generation. Our GUIGAN is based
on SeqGAN by modeling the GUI component style compatibility
and GUI structure. The evaluation demonstrates that our model
signiﬁcantly outperforms the best of the baseline methods by
30.77% in Fr ´echet Inception distance (FID) and 12.35% in 1-
Nearest Neighbor Accuracy (1-NNA). Through a pilot user study,
we provide initial evidence of the usefulness of our approach forgenerating acceptable brand new GUI designs.
Index T erms—Graphical User Interface, mobile application,
GUI design, deep learning, Generative Adversarial Network
(GAN)
I. I NTRODUCTION
Graphic User Interface (GUI) is ubiquitous in almost all
modern desktop software, mobile applications, and online
websites. It provides a visual bridge between a softwareapplication and end-users through which they can interact with
each other. A good GUI design makes an application easy,practical, and efﬁcient to use, which signiﬁcantly affects the
success of the application and the loyalty of its users [1]. Forexample, computer users view Apple’s Macintosh system ashaving a better GUI than the Windows system, therefore their
positive views almost double that of Windows users, leading
to 20% more brand loyalty [2].
Good GUI design is difﬁcult and time-consuming, even for
professional GUI designers, as the designing process mustfollow many design rules and principles [3] [4], such as ﬂuent
interactivity, universal usability, clear readability, aestheticappearance, and consistent styles [5] [6] [3]. To follow the
fashion trend, GUI designers have to keep reviewing thelatest/hottest mobile apps/software or getting inspiration from
* Corresponding author.design sharing sites (e.g., Dribbble1). Considering that each
mobile app/website/software contains tens of different screensand their GUIs need to be updated iteratively due to the marketpressure, designers have to take much innovation-extensiveworking load.
Unfortunately, this design work often awaits just very few
designers in a company [7], and software developers have toﬁll in the gap. In a survey of more than 5,700 developers [8],51% developers reported working on app GUI design tasks,more so than other development tasks, which they had to
perform every few days. However, software developers often
do not have sufﬁcient professional UI/UX design training with
art sense. That is why it is challenging for developers to design
the GUI only from scratch. Instead, when designing the GUI
for websites or mobile apps, developers are very likely to
search existing GUI designs on the internet as the reference,and further implement and customize the GUI design fortheir own purposes [9], [10]. This process usually happens at
GUI development in small start-ups or small-scale open-sourcesoftware, as there are not any professional UI/UX designers.
Although some studies help with the GUI search by attribute
ﬁltering [11] or parsing code structure of UI [12], there are
three problems with the GUI search. First, there is a gapbetween the developers’ intention in mind and the output
textual query, and another gap between the textual queryand visual GUI design. Due to the gap between the textual
and visual information, the retrieved GUI may not satisfydevelopers’ requirements. Second, the retrieved GUI designmay be adopted by other developers, resulting in the high
similarity to other apps, negatively inﬂuencing the uniquenessof the app. Directly using others’ GUI may also involve
potential intellectual property issues. Third, the design styleof some retrieved GUIs may be out of date and it’s hard for
developers to keep track of the latest trend of the GUI design.
“A lot of times, people don’t know what they want until you
show it to them.”
— Steve Jobs
Consequently, an automated method for creative GUI design
generation is terribly needed to alleviate the burden of both
novice designers and developers. With the generated GUI
1https://dribbble.com/
7482021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)
1558-1225/21/$31.00 ©2021 IEEE
DOI 10.1109/ICSE43902.2021.00074
……
GUI Subtree 
Segmentation
……
 …
Generator 
Discriminator
Information of Design Style and Composition Structure
SubtreeSerialization Input Output
 Components Recombination
GUI Dataset Subtree Repository GUI Generation  System Based on GANAdversarial Training
Generated GUI
Fig. 1. Overview of the proposed method.
design, developers can further adopt the automated GUI code
generation [13]–[15] for the automated. In that way, the over-
all GUI development process will be signiﬁcantly simpliﬁed.
In this work, we develop a deep learning model GUIGAN
to automatically generate GUI designs based on the existing
GUI screenshots collected from thousands of mobile apps. It
can provide designers and developers brand new GUI designs,and they can further customize the generated GUI for their
own purpose, rather than starting from scratch. Although thereare plenty of image generation models like DCGAN, V AE-
GAN, CycleGan, and WGAN [16]–[19], they are all basedon plain pixels. In contrast, GUI is composed of a set ofdetailed components (e.g., button, text, images), and a good
GUI design is concerned more with the composition of thesecomponents, rather than ﬁne-grained component pixels. Due tothe characteristic of GUI and inspired by the natural-language
generation (i.e., selecting a list of words for composing one
sentence), we formulate our task as selecting a list of existing
GUI component subtree to compose new GUI designs.
An overview of our approach is shown in Fig 1. First, We
collect 12,230 GUI screenshots and their corresponding meta-information from 1,609 Android apps in 27 categories fromGoogle Play and decompose them into 41,813 component sub-trees for re-using. Second, we develop a SeqGAN [20] basedmodel. Apart from the default generation and discrimination
loss, we model the GUI component style compatibility and
GUI layout structure for guiding the training. Therefore, ourGUIGAN can generate brand new GUI designs for designers’
inspiration. The evaluation demonstrates that our model sig-
niﬁcantly outperforms the best of the baseline methods by
30.77% in Fr ´echet Inception distance (FID) and 12.35% in
1-Nearest Neighbor Accuracy (1-NNA). Through a pilot user
study, we provide the initial evidence of the usefulness of ourapproach for generating acceptable brand new GUIs.
Our contributions in this work can be summarized as follow:
•To the best of our knowledge, this is the ﬁrst study to
automatically generate the mobile app GUI design which
requires much creativity and visual understanding.
•We propose a novel deep learning-based method to gen-erate brand new GUI designs composed of subtree se-
quences from the existing GUI designs without additionalmanual presets.
•The experimental results based on two speciﬁc develop-
ment conditions show that our method can successfully
capture GUI design styles and structural features, and
Toolbar 
TextView Button 
TextView ItemView 
TextView 
TextView 
Layout 
TextView TextView TextView 
Layout 
TextView TextView TextView 
…Layout 
TextView TextView TextView 
…
Fig. 2. Real-world data collection of GUI subtrees.
automatically generate a new composite GUI that con-forms to the aesthetic of the consumers and standard GUI
structure.
II. P
RELIMINARY
In this section, we clear our goal and establish the corre-
sponding task, and then introduce a deep learning method that
our work is based on.
A. Task Establishment
Different from the plain image which is made up of pixels,
one GUI design image consists of two types of components
i.e., widgets (e.g., button, image, text) and spatial layouts (e.g.,linear layout, relative layout). The widgets (leaf nodes) areorganized by the layout (intermedia nodes) as the structuraltree for one GUI design as seen in Fig 2. As most GUI de-
signers may re-use some of their previous design components
in their new design [21], we take the subtree of existing GUIsas the basic unit for composing a new GUI design rather thanplain pixels. Therefore, we formulate our task as producing a
sequence S
1:T=(s1,...,s t,...s T),st∈Sof GUI component
subtrees, where Sis the subtree repository. It can also be
described as generating a new GUI by selecting a list of
compatible GUI subtrees.
To obtain these candidate subtrees from screenshots of the
GUIs, we cut them from the original screenshot according tocertain rules. Given one GUI design with detailed componentinformation, we cut out all the ﬁrst-level subtrees from the
original DOM tree as seen in Fig 2. If the width of a subtree
exceeds 90% of the GUI width, we continue to cut it to
749………



+
…
Real World Data Sample Structures List 
Image Image 
Text Image 
Text Text 
Subtree Embeddings 
 Style Classification CNN Generator(LSTM Network) …
Samples 
Subtree Candidates:  ݏଵ,ݏଶ, …, ݏ ௏Discriminator(CNN Network) ݏݏ݋ܮ ௚
ݏݏ݋ܮ ௖ ݏݏ݋ܮ ௦
Layout 
List 
Image Image 
Text Text Layout 
Link 
Real World Structures 
ݏݏ݋ܮ ௗ
…
HOM( ࢙૚:ࢀ)
MED(tree(s), tree(r))Adversarial Training
Fig. 3. The workﬂow of GUIGAN.
the next level, otherwise, stop splitting and this subtree is
used as the smallest granularity unit. The procedure will be
iterated until the segmentation stops. Finally, all the smallest
subtrees are given a unique number identiﬁcation. We remove
the subtrees with duplicate bounds in one GUI and keep only
one in the process. Based on the collection and observationof the data from our pilot study, we remove the subtrees withpartial overlap and preserve those with the aspect ratio between
0.25 to 50, which has a speciﬁc structure and can be clippedfrom the original GUI screenshot.
B. Base Model
Our work is mainly based on the generative adversarial
networks (GAN) [22], which consists of a generative network
as the generator and a discriminative network as the discrim-inator respectively. The generator learns the features from the
real data and generates new samples to fool the discriminator.
The discriminator tries to distinguish the true sample from the
fake one. These two networks are trained in an adversarial
mode until the discriminator cannot distinguish the samples
generated by the generator.
TheGUIGAN is proposed based on SeqGAN [20], which
is a variant of GAN. SeqGAN is the ﬁrst work extending
GANs to generate sequences of discrete tokens. It solves thecommon problems of traditional GAN in dealing with discretedata such as sequences, that is, the generator is difﬁcult to
transfer gradient updates effectively, and the discriminator isdifﬁcult to evaluate incomplete sequences. SeqGAN combinesthe GAN and Policy Gradient algorithm of reinforcementlearning to guide the training of the generative model throughthe discriminative model. SeqGAN uses a Long Short-Term
Memory (LSTM) as the generator, a CNN with a highwaystructure as the discriminator, and a well-trained oracle with
the same architecture as the generator to generate samples
as the ground truth. The discriminator updates parametersby distinguishing between real samples and generated ones
from the generator in the d-step (the step of training thediscriminator), which belongs to a binary classiﬁcation task.
The generator uses the Monte Carlo (MC) search rewardperformed by the discriminator in combination with the policygradient method to update its parameters in the g-step (the step
of training the generator).
III. A
PPROACH
We propose a system called GUIGAN that learns to syn-
thesize brand new GUI designs for designers by modeling
GUI component subtree sequences and style compatibility.The approach overview can be seen in Fig 3 Based on
subtrees automatically segmented from the original GUIs inSection II-A, we ﬁrst convert all of them into embedding
by modeling their style in Section III-A. During the trainingprocess, the generator randomly generates a sequence with thegiven length and the discriminator acts as the environment, in
which the reward can be calculated as the loss
gby Monte
Carlo tree search (MCTS). We get the homogeneity value of
the generated result as loss cin Section III-B. By measuring
the distance between the generated result and the original GUI
750design, the model captures the structural information in Sec-
tion III-C with loss scalculated by the minimum edit distance.
By integrating all the loss functions above in Section III-D,the parameters of the generator are updated with the back-propagation algorithm.
A. Style Embedding of Subtree
Since we are feeding the model with sub-images showing
GUI component subtrees, we ﬁrst convert all of them to an
embedding. Similar to natural-language sentence, the overallGUI component layout tree can be regarded as a sentence, andthe subtrees obtained from its metadata decomposition is the
constituent words of this sentence. We serialize the subtrees
by depth-ﬁrst traversal and map them into embedding spaceto get their vector features as the input of our GUIGAN. Thus,
we apply a deep learning network to get the feature vector and
style embedding of the subtree sequences.
To transform the image from pixel level to vector level,
we adopt a siamese network [23], [24] to model the GUI
design with a dual-channel CNN structure, which maps aGUI into GUI vector space. We apply a pair of GUI images
(g1,g2) as the input and the goal of the siamese network
is to distinguish whether the two images are from the same
app. According to our observation, the GUIs from one app ismore similar in design style than GUIs from different apps.Therefore, we set up the learning function to discriminate if
two input design images are from one app or not to make
the input embedding more meaningful, i.e., representing the
design style. The CNN in the siamese network takes one of
the GUI screenshot pair as the input. Then the convolution
operation is executed with various ﬁlters (m ×m matrix)
to extract the features in the GUIs. A Relu activation and a
max-pooling layer follow the convolution operation, which canbe considered as a convolutional block and they are stacked
repeatedly. In the end, the output of the last convolutional
block, which represents the embedding in the vector space ofthe GUIs, is ﬂattened into an FC(fully-connected) layer.
The goal of the trained CNN in the siamese network is to
convert the GUI screenshot image gto N-dimensional vector
V
g. This non-linear transformation function fcan be expressed
asVg=f(g;θ), whereθrepresents the trainable parameters
in the network which can be updated by the back-propagationalgorithm in the training process. The weighted L1distance
is applied to measure the two feature vectors V
g1andVg2
from the two channels and then fed into a sigmoid activation
function to calculate the predictive result. Since this task canbe formulated as a binary classiﬁcation problem i.e., two inputscreenshots from the same app or not, we adopt the binary
cross entropy loss function:
Loss(x,y)=−/summationdisplay
i(xilog(y i)+( 1−xi)l o g ( 1−yi)) (1)
wherexis the probability output of the network and yis the
target (0 or 1).The CNNs of the different channels use the same weights
and learn the ability to obtain the most representative informa-tion features in GUIs, which is used to quantitatively comparethe appearance design style similarity between GUI images.The pixel information of the intercepted layout subtrees fromthe original GUI is fed into the trained CNN and thus weacquire their design embeddings.
B. Modeling Subtree Compatibility
We apply the CNN (in the Siamese network) trained in
the previous section to help evaluate the aesthetic identity
of the generated samples. In each g-step, when the generatorgenerates a complete sequence S
1:T, which is composed of
Tsubtrees from different apps (subtree repository) spliced
in order. According to the metadata of their GUIs, we can
acquire the coordinates of each subtree and intercept their
images from their original GUI images. Then we input theminto the CNN from the Siamese network trained before and
output their embeddings. Using these embeddings, we apply
the homogeneity (HOM) to evaluate the aesthetic compatibilityof subtrees in the sequence.
Homogeneity (HOM) is the proportion of clusters contain-
ing only members of a single class (the class here represents
the app, and when the subtrees all come from the same app,
they get the highest harmony) by
h=1−H(G|C)
H(G)(2)
where H(G|C) is the conditional entropy of
a class with a given cluster assignment and
H(G|C)=−/summationtext|G|
g=1/summationtext|C|
c=1ng,c
nlog(ng,c
nc).H(G) represents
the entropy of GandH(G)=−/summationtext|G|g=1n
g
nlog(ng
n).nis
the total number of samples, ngandncbelong to class G
and class Crespectively, and ng,c is the number of samples
divided from class Gto classC.
We expect that the generator can keep learning to make
the generated results have higher homogeneity scores, which
represents better coordination and compatibility. Therefore, weintegrate the homogeneity score of the generated result into the
training of the generator after the discriminator feeds back thereward value to a complete sequence from the generator. Thenwe calculate the style loss as
Loss
c=/braceleftbigg
exp (−h),if c> 1
0,if c =1(3)
wherecis the number of the app where the subtrees come
from. If the subtrees of a sample are all from the same app,Loss
cbecomes zero.
C. Modeling Subtree Structure
In addition to style information, another factor to be con-
sidered is the structure information corresponding to the gen-erated sequence. The layouts of each actual GUI have certaincomposition rules, which make a GUI not only more logical
in appearance but also practical function. We hope that whengenerating new subtree combination sequences, the generator
751can also follow the composition conditions of GUI to a certain
extent so that these synthetic sequences not only stay diverse
but also meet the structural characteristics of the real GUIs. For
this purpose, we use the structure strings of the subtrees from
their metadata instead of the GUI wireframe images [25] [26]to represent their structures as there is explicit order amongdifferent GUI components. The minimum edit distance (MED)
is introduced to quantify the structural similarity between
two GUIs. The MED can be used to evaluate the structuralsimilarity between the generated samples and the real worlddata. By reducing the structural distance, we can optimize the
generator, so that it learns the reasonable structure combination
and order from the real GUIs, which can be expressed as
Loss
s=⎧
⎪⎪⎨
⎪⎪⎩max(i,j ),ifmin(i,j )=0,
min⎧
⎨
⎩levSr,Sg(i−1,j)+1
levSr,Sg(i,j−1) + 1
levSr,Sg(i−1,j−1) + 1otherwise.
(4)
whereSrandSgrepresent the subtree structure strings
of real and generated samples, levSr,Sg(i,j)is the distance
between the ﬁrst icharacters in Srand the ﬁrst jcharacters
inSg. Each character represents a GUI component such as
ListView or FrameLayout.
D. Multi-Loss Fusion
There is a large span in the numerical region of the three
losses(the feedback loss from the discriminator, loss g,loss c,
andloss s), so that we need to normalize them for subsequent
calculation. By adding the trainable noise parameters [27], the
three loss values can be balanced to the same scale and we
express the ﬁnal fusion loss function as
Loss mul=λ1Loss g+λ2Loss c+λ3Loss s (5)
In g-step, we update the parameters of the generator by
minimizing Loss mul and we apply Adam update algorithm
[28] instead of stochastic gradient descent (SGD) for faster
convergence.
arg min
GLoss mul (6)
The challenge of the model is to generate a new GUI design
with a reasonable structure and compatible style. We don’t
want the model to only learn to generate sequences similar tothe real samples. It promotes GUIGAN to generate new GUI
designs with authenticity and diversity by fusing the structureand style information simultaneously to the original sequencefeatures. As shown in Fig 4, two samples generated by the
GUIGAN are reconstructed by the pieces from the real GUI.
IV . I
MPLEMENT A TION
The data in our experiment comes from the Rico open-
source online data collection [25], and a part of GUIs for
this article is retained through manual screening. Our modelmainly consists of a SeqGAN (includes an LSTM and a CNN)
and a Siamese network. All networks are implemented on thePyTorch platform and trained on a GPU.
(a)
(b)
Fig. 4. Two example GUIs generated by GUIGAN with components from
corresponding original GUIs.
A. Dataset Construction
Our data comes from Rico [25], an open-source mobile app
dataset for building data-driven design applications. Rico isthe largest repository of mobile app designs to date, supporting
design search, UI layout generation, UI code generation, etc.
Rico was built by mining Android apps at runtime via human-powered and programmatic exploration. 13 workers spent2,450 hours using the downloaded apps from the Google Play
Store on the platform over ﬁve months, producing 10,811 userinteraction traces. Rico contains design and interaction data for
72219 UIs from 9772 apps, spanning 27 categories.
Based on our observation, not all GUIs from Rico
datasets [25] can be used in this study, so we remove someof them. First, we remove the GUIs of game apps as gameapp GUIs is speciﬁcally generated by game engine which is
different from other general apps. Second, we manually re-
move some low-quality GUIs including large pop-up windows,advertisements, or posters occupying the whole screen size,webpage, loading page with a progress bar, and real scenes
incamera. Some examples can be seen in Fig 5 and we release
all of our datasets at our online gallery
2.
2https://github.com/GUIDesignResearch/GUIGAN#dataset-construction
752(a)
 (b)
 (c)
 (d)
Fig. 5. Examples of GUIs we removed from our dataset: The GUI from the
game app (a), pop-up window (b), large picture (c), waiting page (d).
B. Model Implementation
As we take the subtree of the existing GUIs as the basic unit
for composing a new GUI design in our paper. A sample of
the real-world data is the combination of the subtrees from one
single real-world GUI in order, with both the screenshots and
structure information based on the metaﬁle. It is mainly used
as the training data for our GAN based approach. We ﬁrst split
those GUIs collected from real-world Android apps in Google
Play into subtrees following procedures in Section II-A. We
then train a Siamese network with this data to get subtreestyle embedding in Section III-A. The subtree embedding is
input to the generator of the GUIGAN for modeling both the
style loss (Loss
c) and structure loss (Loss s) for generating
new subtree sequences, which can be used for composing
new GUI designs. The generated subtree sequences then inputto the discriminator within the SeqGAN for training the
discriminator to discriminate the real-world GUI and generatedGUI. After the adversarial training, given random noise or pre-
built GUI components, the GUIGAN can generate a new GUI
by composing several subtrees from real-world GUIs.
The structure of the generator and the discriminator in
SeqGAN is preserved in GUIGAN which is implemented in
PyTorch. We store the start and end subtrees of each real-
world GUI in the start list and end list respectively. TheLSTM randomly takes a subtree in the start list as the initial
matrix, not the zero matrices in SeqGAN. Thus, the generatorgenerates a sequence of length T(the default sequence length
is 30 because the GUI subtree length in real-world data ismostly within 30). From the beginning of the ﬁrst subtree inthe start list, if the total height of the later spliced subtree and
all previous subtrees exceeds the rated height, the subsequentsplicing will be stopped. If the subtree in the end list is
selected, the splicing will be stopped directly.
The Long Short-Term Memory (LSTM) is used as the
generative network. The vector dimension is selected to be32 and the hidden layer feature dimension is selected to be32. Like the discriminative model in SeqGAN, we also use a
CNN network that joins the highway architecture. The batch
size is 32 and the learning rate is set to 0.05.
The siamese network is a two-channel CNNs with sharedweights. The positive example is a pair of subtrees from thesame app with a label of 1, and the negative one from twodifferent apps with a label of 0. The subtree image is resizedto512×256 by the Nearest Neighbor algorithm as the input.
There are 4 Conv→Pool layers blocks in the CNN structure.
The ﬁrst Conv layer use 64 ﬁlters, and each subsequent Conv
layer doubles the number of ﬁlers. We set the ﬁlter size as10×10,7×7,4×4, and 4×4in different CNN layers,
and the stride as 1×1for convolutional layers. We apply the
pooling units of size 2×2applied with a stride 2. We train
the Siamese network with 50 epochs for about three hours.
V. A
UTOMA TED EV ALUA TION
In this section, we prepare GUI images collected according
to the category and development company of the app as exper-imental data based on the statistics and test the performance ofthe proposed model on GUI generation. We use the real-world
data as the ground truth and introduce WGAN-GP , FaceOff,
and two variations of our GUIGAN as the baseline methods to
compare the two metrics of FID and 1-NNA.
A. Experimental Dataset
When preparing the experiment dataset, we consider two
speciﬁc usage scenarios when developing app GUIs. First,
most designers and developers have a clear goal for developing
the GUI of an app in a speciﬁc category such as ﬁnance,
education, news, etc. Since each kind of app category owns
its characteristics, we try to test our model’s capability in
capturing that characteristic by preparing a separated dataset
for the ﬁve most frequent app categories in Rico dataset [25]
including News & Magazines, Books & Reference, Shopping,Communication, and Travel & Local as shown in Table I.
Second, we notice that designers and developers often refer
to the GUI design style developed by big companies whendeveloping their own GUI. Therefore, another experiment isto generate GUIs by learning GUI designs from a speciﬁccompany. Based on their metadata, we prepare three kinds of
GUIs from three big companies with most apps in our dataset
i.e., Google, YinzCam, and Raycom as shown in Table I.
T ABLE I
GUI DA T ASET BY CA TEGORY OR COMP ANY
Category or Company #App #GUI
News & Magazines 108 467
Books & Reference 110 394Shopping 98 460Communication 107 357
Travel & Local 85 407
Google 28 85
Yinzcam 30 136Raycom 27 113
B. Evaluation Metrics
In order to quantify and measure the similarity between the
real data distribution Prand the generated sample distribution
753Pg, we introduce Fr ´echet Inception distance (FID) [29] and
1-Nearest Neighbor Accuracy (1-NNA) [30] as the evaluation
metrics. FID measures the diversity and quality of generatedimages relative to real images, and 1-NNA is used to analyzethe distribution differences between the two sample sets.
Fr´echet Inception distance (FID) is a widely-used met-
ric [31] recently introduced for measuring the quality and
diversity of generated images, especially by GAN. FID iscalculated according to Inception Score (IS), comparing thestatistics of negative samples to positive samples, using the
Fr´echet distance between two multivariate Gaussians:
FID(P
r,Pg)=/bardblμr−μg/bardbl+Tr(Cr+Cg−2(CrCg)1/2)(7)
whereμrandμgare the mean values of the 2048-
dimensional activations of the Inception-v3 [32](By asymmet-
rically decomposing the convolution operation, the depth and
width of the network pretrained on the ImageNet are increasedsimultaneously, and the 1 ×1 network is widely used to reduce
the dimension) pool3 layer for real and generated samples
randgrespectively, while C
randCgare the covariances.
A lower FID represents that the two distributions are closer,
which means that the quality and diversity of the generatedimages are higher. In our experiments, we input the samenumber of real and generated image collections into the
Inception-v3 network to get the FID score.
1-Nearest Neighbor Accuracy is used in two-sample tests
to assess whether two distributions are identical. A well
trained 1-nearest neighbor classiﬁer is applied. The better the
performance of the generated model, the more difﬁcult it is
for the 1-NN classiﬁer to distinguish the true from the false.
Therefore, the best recognition rate is 50%, the worst is 100%.
But if the recognition rate is lower than 50%, it means that
the model may be overﬁtting. This metric is widely used forevaluating the quality of generated images [33]–[35].
C. Baseline Models
According to our query for relevant information, there are
very little researches on generating GUI designs. But there are
some related works about image generation which is roughlysimilar to our task, so we use two different kinds of methods
as baselines with one from the image generation and the otherone using template search.
The ﬁrst baseline is WGAN-GP [36], which is proposed
based on WGAN [19]. WGAN introduces Wasserstein dis-
tance and optimizes the implementation process of the algo-
rithm to solve the problem of gradient vanishing in the trainingprocess of traditional GAN. WGAN-GP introduces a gradientpenalty in WGAN, which accelerates the convergence of the
model and has a more stable training process.
The second baseline is FaceOff [21] which parses the
DOM tree of a raw input website created by a user through
measuring the distance of the trees and uses a CNN to learnstyle compatibility to ﬁnd a similarly well-designed web GUI.
Although FaceOff is used to generate a new web GUI, in this
article we modify its raw input to the real world data GUI
(a)
 (b)
 (c)
 (d)
Fig. 6. Examples of the GUIs generated by GUIGAN.
structure of the mobile application as a query and then sort
the retrieved results according to the homogeneity score of
their subtree combination.
Apart from the two baselines mentioned above, we also get
some derivation baselines from our model by changing themuti-loss in the generator. One (GUIGAN-style) is, using only
theLoss
ccorrection, and the other one (GUIGAN-structure)
with only the Loss scorrection within which the generator
focuses on either design style or structure characteristics. In
this way, we can compare and observe within our model howthese modiﬁcations affect the generated results in metric.
D. Results
Table II and III show the results of different methods on
two metrics in the category and company speciﬁc development
scenarios. The results show that our proposed model achievesthe highest scores on FID and 1-NNA under both the GUI de-velopment scenario. Our model achieves a 33.63% and 11.33%boost in FID and 1-NNA than the best baselines in the dataset
of category, and 28.23% and 14.18% increase in the datasetof the company, respectively. Fig 6 shows examples of GUI
images from GUIGAN, and for ease of observation, we separate
different subtrees with thick red lines. It can be seen that theGUIs generated from GUIGAN have a comfortable appearance,
and a reasonable structure composed of different components.
Meanwhile, it also keeps the overall harmonious design style.
After checking many generated GUIs, we ﬁnd that both thestructure and style of the GUIs are also very diverse which
can provide developers or designers with different candidates
for their GUI design. More generated GUIs from GUIGAN can
be seen in our online gallery
3.
Two baselines including WGAN-GP and FaceOff do not
perform well compared with GUIGAN in FID and 1-NNA.
According to our observation, we can see the overall layoutof generated GUIs from WGAN-GP as seen in Fig 7 (a), but
very blurred in detail. That is because WGAN-GP is a pixel-
based approach that cannot accurately model the informationof component-based GUIs. Although it is widely used in the
natural image, it is not suitable for our artiﬁcial GUI designimages, especially considering the fact that there is not so
3https://github.com/GUIDesignResearch/GUIGAN
754T ABLE II
PERFORMANCE BY DIFFERENT APP CA TEGORIES
CategoryWGAN-GP FaceOff GUIGAN GUIGAN-Style GUIGAN-Structure
FID 1-NNA FID 1-NNA FID 1-NNA FID 1-NNA FID 1-NNA
News & Magazines 0.181 0.999 0.145 0.987 0.110 0.857 0.139 0.876 0.110 0.900
Books & Reference 0.161 1.000 0.106 0.972 0.084 0.871 0.107 0.923 0.101 0.906
Shopping 0.172 0.998 0.120 0.972 0.068 0.829 0.052 0.829 0.048 0.868
Communication 0.150 0.995 0.134 0.993 0.060 0.915 0.065 0.915 0.089 0.929
Travel & Local 0.139 0.999 0.060 0.974 0.054 0.873 0.067 0.897 0.088 0.880
Average 0.161 0.998 0.113 0.980 0.075 0.869 0.086 0.888 0.087 0.897
T ABLE III
PERFORMANCE BY DIFFERENT APP DEVELOPMENT COMP ANIES
companyWGAN-GP FaceOff GUIGAN GUIGAN-Style GUIGAN-Structure
FID 1-NNA FID 1-NNA FID 1-NNA FID 1-NNA FID 1-NNA
Google 0.181 0.999 0.125 0.945 0.131 0.844 0.122 0.837 0.087 0.859
Yinzcam 0.172 0.980 0.146 0.964 0.074 0.806 0.084 0.828 0.094 0.828
Raycom 0.244 1.000 0.101 0.968 0.063 0.819 0.049 0.846 0.075 0.840
Average 0.199 0.990 0.124 0.959 0.089 0.823 0.085 0.837 0.085 0.842
(a)
 (b)
 (c)
 (d)
Fig. 7. Generated GUI examples by WGAN-GP (a) and FaceOff (b, c, d).
much data in this study. FaceOff is much better than WGAN-
GP , but there are still some issues with their approach. First,
FaceOff often chooses the subtrees with the highest back
score to accelerate the convergence of the model and only
compares the structural similarity between the real GUIs andthe retrieved template to minimize their distance, resulting inthe diversity loss. However, it does not consider the relativeposition of each component especially the speciﬁc top-downrelationship of the structure in the GUI. Therefore, most
generated GUIs from FaceOff are of very similar structurelike that in Fig 7 (b), and many GUIs are also of the same
color schema as that in Fig 7 (c) and Fig 7 (d).
The other two derived baselines of our approach, GUIGAN-
style and GUIGAN-structure explore the impact of style in-
formation and structure information on the generated results.
With only modeling the design style information, GUIGAN-
(a)
 (b)
 (c)
 (d)
Fig. 8. Generated GUI examples by GUIGAN-style (a, b) and GUIGAN-
structure (c, d).
style can generate GUI designs with harmonious color com-
binations as seen in Fig 8 (a) and (b), but without very goodstructural designs. For example, the menu tab appears in the
middle of the GUI in Fig 8 (a) and the login button appears
at the top of the GUI in Fig 8 (b). Similar issues also applytoGUIGAN-structure with reasonable and diverse layouts of
generated GUIs, but terrible color schema as seen in Fig 8 (c),
(d). The results from these two baselines demonstrate that thetwo loss function settings in Section III successfully capture
the style and structure information.
Although most of the samples generated by our model are
satisfactory, there are still some bad designs. We manuallyobserve those bad GUI designs and summarize some reasons.First, due to the default size of components in the subtree,
some of them are difﬁcult to ﬁt into the generated GUIs asseen in Fig 9 (a), (b). There is either an overlap between some
755components or one ﬁgure taking all the GUI space. Second,
since our model learns the style and structure informationat the same time, there may be an imbalance between them
for some GUI generation. Fig 9 (c) shows an example withtoo much emphasis on style consistency while ignoring the
structural effects. In contrast, Fig 9 (d) has a set of diverse
components in good structure but incompatible color schema.
(a)
 (b)
 (c)
 (d)
Fig. 9. Examples of bad results generated by GUIGAN.
VI. H UMAN EV ALUA TION
The target of this work is to automatically generate a list of
GUI designs for novice designers or developers to adopt. Theautomated experiments above demonstrate the performanceof our model compared with other baselines. However, the
satisfactoriness of the GUI design can be subjective depend-
ing on different users or developers. To better evaluate the
usefulness of GUIGAN, we conduct a user study to investigate
the feedback from developers in this section.
A. Evaluation Metrics
There are no existing evaluation metrics for mobile GUI
design in the literature. But inspired by the web GUI evalu-
ation [37]–[39] and image evaluation [40], [41], we propose
three novel metrics for participants to rate the quality of the
GUI design from three aspects by considering the characteris-tics of the mobile GUIs. First, design aesthetics is to evaluatethe overall design’s pleasing qualities. Second, we adopt color
harmony [42], [43] which refers to the property that certainaesthetically pleasing color combinations have to evaluate the
color schema selection within the GUI. These combinations
create pleasing contrasts and consonances that are said to beharmonious. Third, structure rationality is used to measure thecomponent layout rationality, i.e., the location of components
in the GUI and the logic of their combination and sorting. For
each metric, the participants will give a score ranging from
1 to 5 with 1 representing the least satisfactoriness while 5
as the highest satisfactoriness. Besides, to conﬁrm if GUIGAN
implicitly considers the app functionality during the training,we further ask participants to manually check if the componentdistribution of generated GUIs is functionally correct e.g., themenu bar on the top of the page. They will mark 1 if the GUI
components are functionally correctly distributed, while 0 forincorrect ones.B. Procedures
In real-world app development, teams often know their
target very well. To mimic that practice, we select 5 appcategories (same in Section V) for speciﬁc GUI generation.For each category, we randomly generate 10 GUI designs foreach method. Due to the poor performance of WGAN-GP inthe last experiment, we only take FaceOff as the baseline.
We recruited ﬁve Master students majoring in computer
science. They all have several-year programming experienceand at least 1-year Android development experience mostly
about GUI implementation and some GUI design. Therefore,they can be regarded as junior Android developers for eval-
uating whether they are satisﬁed with our GUI design. First,
we introduce them to a detailed explanation about the GUIevaluation metrics. Then they are provided with the generatedGUI designs from different methods, then give the score of
each GUI design in three metrics i.e., design aesthetics, colorharmony, structure rationality. Note that they do not know
which GUI design is from which method and all of them willevaluate the GUI design individually without any discussion.
After the experiment, we tell the participants which GUI
designs are generated by our model and ask them to leavesome general comments about our GUIGAN.
C. Results
As shown in Table IV, the generated GUI designs from our
model outperform that of FaceOff signiﬁcantly with 3.11, 3.30,and 3.21 which are 31.22%, 25.00%, and 34.87% increase in
overall aesthetics, color harmony, and structure. In additionto the average score, our model is also better than FaceOff
in generating GUI design for all ﬁve app categories in threemetrics. That result also demonstrates the generalization ofourGUIGAN. According to the detailed analysis of the ex-
periment result, the GUI designs with low scores tend to have
incomplete structure, single content, large and abrupt pictures,
or advertisements. In contrast, the GUI with high scoreshas a concise layout, slightly rich content, and background
compatible images. We also ﬁnd that users’ requirements for
content richness are much higher than other indicators, butthis often goes against the simplicity of layout, which needs
further research and balance.
To understand the signiﬁcance of the differences between
the two approaches, we carry out the Mann-Whitney U
test [44] which is speciﬁcally designed for small samples(only 10 GUI design in each category) on three metrics.The test results in Table IV suggests that our GUIGAN can
contribute signiﬁcantly to the GUI design in all three metricswithp−value < 0.01 orp−value < 0.05 except the
aesthetics and color harmony metrics in the shopping category.
Besides the comparison with the baseline, we also present
participants another dataset mixing with 10 randomly se-lected real-world GUI design images from our dataset and
10 randomly selected GUI designs generated by our model
for checking the overall GUI aesthetics, color harmony, andstructure. Some generated GUIs are even rated higher thanreal-world ones. In terms of Fig 10 (a), the ﬁve user-study
756T ABLE IV
PERFORMANCE OF HUMAN EV ALUA TION .* * DENOTES
p<0.01 AND *DENOTES p<0.05
Category MetricScore
FaceOff GUIGAN
News & Magazinesaesthetics 2.08 2.96∗∗
harmony 2.54 3.18∗∗
structure 2.22 3.02∗∗
functionality 0.38 0.82∗∗
Books & Referenceaesthetics 2.40 3.16∗∗
harmony 2.46 3.32∗∗
structure 2.40 3.40∗∗
functionality 0.40 0.74∗∗
Shoppingaesthetics 2.66 3.02
harmony 3.04 3.18
structure 2.52 3.00∗
functionality 0.60 0.78
Communicationaesthetics 2.56 3.12∗
harmony 2.86 3.42∗
structure 2.60 3.18∗
functionality 0.42 0.82∗∗
Travel & Localaesthetics 2.14 3.30∗∗
harmony 2.30 3.38∗∗
structure 2.16 3.44∗∗
functionality 0.46 0.90∗∗
A verageaesthetics 2.37 3.11∗∗
harmony 2.64 3.30∗∗
structure 2.38 3.21∗∗
functionality 0.452 0.812∗∗
participants rate the real GUI with 2.8, 3.2, and 3.6 in three
metrics (aesthetics, harmony, structure) on average, while 3,3.4, 3.4 for the generated one. For Fig 10 (b), they score the
real GUI with 3.2, 3.6, and 4, and the generated GUI with 3.6,
3.6, and 4 which is not that high. There are several reasonswhy our generated GUIs get higher scores than some real-world GUIs: (1) Some generated GUIs (examples in Fig 10)are of higher quality than some poorly designed real-worldGUIs. And note that the score is just 2.1% to 3.7% higher
than some real-world GUIs. (2) There may be the human bias
of different raters as different people are of different aesthetic
values. Different human raters may also adopt slightly different
criteria when rating the GUI quality as people vary. Wemitigated those potential bias by not telling them which GUI
is generated by our model or from real-world apps.
The results of functionality can be seen in Table IV with
the average score as 0.812 which is 79.65% signiﬁcantlyhigher than that of the baseline. It shows that the componentdistribution of most of our generated GUIs is functionally
correct. It also indicates our model implicitly get that appfunctionality during training on a large-scale dataset as the
GUI design is highly related to app functionality, though our
model does not explicitly consider app functionality. Addi-tionally, we ﬁnd that the performance of our model differs indifferent app categories. GUIGAN achieves an average score of
(a) Shopping
 (b) Travel & Local
Fig. 10. In each pair, the ﬁrst image is real-world GUI while the second one
is generated by GUIGAN.
0.9 in the Travel & Local category, but only 0.74 in the Books
& Reference category, which may be because the former one
is of more functional characteristics than the latter one.
After the experiment, we tell participants which GUIs are
generated from our tool and get some feedback from them.
They suggest that the padding between components shouldbe adjusted to increase the attractiveness of the GUI designs.They also request to add new features that allow manualsettings of result diversity, such as a comparison betweencombinations, style, structure, etc.
VII. R
ELA TED WORK
GUI is crucial for the user experience of modern desktop
software, mobile applications, and online websites. In thissection, we introduce related works about GUI design andGUI generation.
A. GUI Design
GUI design is an important step in GUI development.
Therefore, many researchers are working on assisting design-
ers in the GUI design such as investigating the UI design
patterns [45], color evolution [46], [47], UI-related users’
review [48], [49], GUI code generation [13], website GUI
generation [21], etc. Liu et al. [50] follow the design rulesfrom Material Design to annotate the mobile GUI design
to represent its semantics. Swearngin et al. [51] adopt the
image processing method to help designs with converting the
mobile UI screenshots into editable ﬁles in Photoshop so that
designers can take it as a starting point for further customiza-tion. To render inspirations to the designer, Chen et al. [52]propose a program-analysis method to efﬁciently generate the
storyboard with UI screenshots, given one app executable ﬁle.
Fischer et al. [53] transfer the style from ﬁne art to GUI.Chen et al. [54] study different GUI element detection methodson large-scale GUI data and develop UIED [55] to handlediverse and complicated GUI images. Other supporting workssuch as GUI tag prediction [56] and GUI component gallery
construction [57] can enhance designers’ searching efﬁciency.
All of these works are targeting at simplifying the design
process for professional designers. In contrast, our method is
focusing on the initial stage of GUI design i.e., generating
diverse GUI designs for giving inspirations to novice designers
and developers who are of not much GUI design training.
757Through the GAN method in deep learning, our model learns
the design style and structural characteristics of existing GUIs
to generate diversiﬁed new GUI for designers’ reference, tolower the GUI design barrier.
B. GUI Generation
Thanks to the rapid development of deep learning, the image
generation performance is further improved, especially by the
Generative Adversarial Network (GAN) [22] and its deviationmodels. Apart from the natural image generation, there are also
many works on re-arranging elements for composing graphic
designs with better layouts (especially semantic layouts).
Sandhaus et al. [58] present an approach for the automatic
layout of photo compositions that incorporates the knowledgeabout aesthetic design principles. Y ang et al. [43] analyze low-
level image features and apply high-level aesthetic designing
principles and predeﬁned templates to the given images and
texts, thus automatically suggest the optimal template, textlocations, and colors. V empati et al. [59] utilize the MaskR-CNN object detector to automatically annotate the required
objects/tags and a Genetic Algorithm method to generate an
optimal advertisement layout for the given image content,input components, and other design constraints. A rankingmodel is trained on historical banners to rank the generated
captivities by predicting their Click-Through-Rate (CTR). Li
et al. [60] introduce a progressive generative model of image
extrapolation with three stages and two important sub-tasks.
In the ﬁeld of text-to-image synthesis, Hinz et al. [61]
introduce Semantic Object Accuracy (SOA) to evaluate images
given an image caption. LayoutGAN is proposed by Li etal. [62] for graphic design and scene generation, introducingwireframe rendering for image discrimination. The generator
takes as input a set of vectors and uses self-attention modulesto reﬁne their labels and geometric parameters jointly. Jyothi et
al. [63] propose a variational autoencoder based method calledLayoutV AE, which allows for generating full image layoutsgiven a label set, or per label layouts for an existing image
given a new label and has the capability of detecting unusuallayouts. Some works are generating GUI test case for checking
GUI usability [64]–[66], accessibility [67] and security [68].
Unlike these works in generating the layout of graphic
design like posters, advertisements, we are the ﬁrst to work onGUI design generation. Different from their tasks in arrangingthe given components, our task is more challenging i.e.,selecting components from a repository and compose them
into a great GUI design by taking the design style and structureinformation into the consideration. Therefore, we develop anovel approach for modeling that information.
VIII. C
ONCLUSION
Designing a good GUI which requires much innovation
and creativity is difﬁcult even for well-trained designers. In
this paper, we propose a GAN-based GUI design generation
method, GUIGAN, which can assist novice designers and de-
velopers by generating new GUIs by learning the existing appGUI screenshots. The generated GUI design can be regardedas the starting point or inspiration for their design work. We
decomposed the ﬁltered GUIs to form our large-scale subtreeretrieval repository, then feed these subtrees to our modelfor generating reasonable one-dimension sequences, which arefurther used for reorganization. Two additional corrections are
added to the generator of our model to improve the model
in the aspect of design style and structural composition. Theautomated experiments demonstrate the performance of ourmodel and the user study conﬁrms the usefulness of GUIGAN.
To improve the generated GUIs, we will adopt two ways.
First, we will improve our model to generate GUIs with higherquality. We can summarize a list of issues of the current modelby carrying out a detailed analysis of a bad GUI generation of
current data. We then improve our model accordingly and also
add a list of rules to post process generated GUIs e.g., menubar should be on the top of the GUI, etc. Second, we will buildan AI-human collaboration system i.e., the generated GUIs in
our model are only used for inspiring developers/designers.
Designers or developers can further select or customize GUIs
according to their purpose. Besides, our model can be used to
convert designers’/developers’ partial GUI design to the full
one via our model’s leveraging the pre-built GUI components
and control, though it can be further improved. Some examplescan be seen at our online gallery
4. We also plan to combine the
current GUI code generation works [13], [15], [69] with our
GUI design generation to fully automate GUI development.
IX. A CKNOWLEDGEMENTS
This work is supported in part by the National Natural Sci-
ence Foundation of China (Grant 61471181), and Chunyang
Chen is partially supported by Facebook research award.
REFERENCES
[1] B. J. Jansen, “The graphical user interface,” ACM SIGCHI Bulletin,
vol. 30, no. 2, pp. 22–26, 1998.
[2] T. Winograd, “From programming environments to environments for
designing,” Communications of the ACM, vol. 38, no. 6, pp. 65–74,
1995.
[3] “Essential design principles,” https://developer.apple.com/videos/play/
wwdc2017/802/, 2017.
[4] “Design - material design,” https://material.io/design/, 2014.
[5] W . O. Galitz, The essential guide to user interface design: an introduc-
tion to GUI design principles and techniques. John Wiley & Sons,
2007.
[6] I. G. Clifton, Android User Interface Design: Implementing Material
Design for Developers. Addison-Wesley Professional, 2015.
[7] Y . W . B. Hong, “Matters of design,” in Commun. ACM. Citeseer, 2011.
[8] “Appcelerator / idc 2015 mobile trends report: Data access is the
new mobile problem,” https://www.appcelerator.com/resource-center/
research/2015-mobile-trends-report/, 2015.
[9] “Wsdesign,” http://wsdesign.in/, 2019.
[10] “Start bootstrap: Free bootstrap themes, templates, snippets, and guides,”
https://startbootstrap.com/, 2019.
[11] C. Bernal-Cardenas, K. Moran, M. Tufano, Z. Liu, L. Nan, Z. Shi, and
D. Poshyvanyk, “Guigle: A gui search engine for android apps,” arXiv
preprint arXiv:1901.00891, 2019.
[12] F. Behrang, S. P . Reiss, and A. Orso, “Guifetch: supporting app
design and development through gui search,” in Proceedings of the 5th
International Conference on Mobile Software Engineering and Systems.ACM, 2018, pp. 236–246.
4https://github.com/GUIDesignResearch/GUIGAN/blob/master/README.
md#examples-of-pre-built-components
758[13] C. Chen, T. Su, G. Meng, Z. Xing, and Y . Liu, “From ui design image
to gui skeleton: a neural machine translator to bootstrap mobile gui
implementation,” in Proceedings of the 40th International Conference
on Software Engineering. ACM, 2018, pp. 665–676.
[14] T. Beltramelli, “pix2code: Generating code from a graphical user in-
terface screenshot,” in Proceedings of the ACM SIGCHI Symposium on
Engineering Interactive Computing Systems. ACM, 2018, p. 3.
[15] K. Moran, C. Bernal-C ´ardenas, M. Curcio, R. Bonett, and D. Poshy-
vanyk, “Machine learning-based prototyping of graphical user interfacesfor mobile apps,” arXiv preprint arXiv:1802.02312, 2018.
[16] A. Radford, L. Metz, and S. Chintala, “Unsupervised representation
learning with deep convolutional generative adversarial networks,” arXiv
preprint arXiv:1511.06434, 2015.
[17] A. B. L. Larsen, S. K. Sønderby, H. Larochelle, and O. Winther,
“Autoencoding beyond pixels using a learned similarity metric,” in
International conference on machine learning. PMLR, 2016, pp. 1558–
1566.
[18] J.-Y . Zhu, T. Park, P . Isola, and A. A. Efros, “Unpaired image-to-image
translation using cycle-consistent adversarial networks,” in Proceedings
of the IEEE international conference on computer vision, 2017, pp.2223–2232.
[19] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein gan,” arXiv
preprint arXiv:1701.07875, 2017.
[20] L. Y u, W . Zhang, J. Wang, and Y . Y u, “Seqgan: Sequence generative
adversarial nets with policy gradient,” in Thirty-First AAAI Conference
on Artiﬁcial Intelligence, 2017.
[21] S. Zheng, Z. Hu, and Y . Ma, “Faceoff: Assisting the manifestation design
of web graphical user interface,” in Proceedings of the Twelfth ACM
International Conference on Web Search and Data Mining. ACM,2019, pp. 774–777.
[22] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y . Bengio, “Generative adversarial nets,” in
Advances in neural information processing systems, 2014, pp. 2672–
2680.
[23] J. Bromley, I. Guyon, Y . LeCun, E. S ¨ackinger, and R. Shah, “Signature
veriﬁcation using a” siamese” time delay neural network,” in Advances
in neural information processing systems, 1994, pp. 737–744.
[24] G. Koch, R. Zemel, and R. Salakhutdinov, “Siamese neural networks for
one-shot image recognition,” in ICML deep learning workshop, vol. 2,
2015.
[25] B. Deka, Z. Huang, C. Franzen, J. Hibschman, D. Afergan, Y . Li,
J. Nichols, and R. Kumar, “Rico: A mobile app dataset for building
data-driven design applications,” in Proceedings of the 30th Annual ACM
Symposium on User Interface Software and Technology. ACM, 2017,
pp. 845–854.
[26] J. Chen, C. Chen, Z. Xing, X. Xia, and J. Wang, “Wireframe-based
ui design search through image autoencoder,” ACM Transactions on
Software Engineering and Methodology, vol. 29, no. 3, pp. 1–31, 2020.
[27] A. Kendall, Y . Gal, and R. Cipolla, “Multi-task learning using uncer-
tainty to weigh losses for scene geometry and semantics,” in Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition,
2018, pp. 7482–7491.
[28] D. P . Kingma and J. Ba, “Adam: A method for stochastic optimization,”
arXiv preprint arXiv:1412.6980, 2014.
[29] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter,
“Gans trained by a two time-scale update rule converge to a local nash
equilibrium,” in Advances in Neural Information Processing Systems,
2017, pp. 6626–6637.
[30] D. Lopezpaz and M. Oquab, “Revisiting classiﬁer two-sample tests,”
2017.
[31] T. Salimans, I. Goodfellow, W . Zaremba, V . Cheung, A. Radford, and
X. Chen, “Improved techniques for training gans,” in Advances in neural
information processing systems, 2016, pp. 2234–2242.
[32] C. Szegedy, V . V anhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking
the inception architecture for computer vision,” pp. 2818–2826, 2016.
[33] G. Y ang, X. Huang, Z. Hao, M.-Y . Liu, S. Belongie, and B. Hariharan,
“Pointﬂo
w: 3d point cloud generation with continuous normalizing
ﬂows,” in Proceedings of the IEEE International Conference on Com-
puter Vision, 2019, pp. 4541–4550.
[34] S. Zhang, Z. Han, Y .-K. Lai, M. Zwicker, and H. Zhang, “Stylistic scene
enhancement gan: mixed stylistic enhancement generation for 3d indoor
scenes,” The Visual Computer, vol. 35, no. 6-8, pp. 1157–1169, 2019.[35] Q. Xu, G. Huang, Y . Y uan, C. Guo, Y . Sun, F. Wu, and K. Weinberger,
“An empirical study on evaluation metrics of generative adversarialnetworks,” 2018.
[36] I. Gulrajani, F. Ahmed, M. Arjovsky, V . Dumoulin, and A. Courville,
“Improved training of wasserstein gans.”
[37] K. Reinecke, T. Y eh, L. Miratrix, R. Mardiko, Y . Zhao, J. Liu, and
K. Z. Gajos, “Predicting users’ ﬁrst impressions of website aestheticswith a quantiﬁcation of perceived visual complexity and colorfulness,” inProceedings of the SIGCHI conference on human factors in computingsystems, 2013, pp. 2049–2058.
[38] C. K. Coursaris, S. J. Swierenga, and E. Watrall, “An empirical in-
vestigation of color temperature and gender effects on web aesthetics,”
Journal of usability studies, vol. 3, no. 3, pp. 103–117, 2008.
[39] J. Li, J. Y ang, J. Zhang, C. Liu, and T. Xu, “Attribute-conditioned layout
gan for automatic graphic design,” IEEE Transactions on Visualization
and Computer Graphics, vol. PP , no. 99, pp. 1–1, 2020.
[40] H. Zhang, J. E. Fritts, and S. A. Goldman, “Image segmentation
evaluation: A survey of unsupervised methods,” computer vision and
image understanding, vol. 110, no. 2, pp. 260–280, 2008.
[41] Z. Wang, G. Healy, A. F. Smeaton, and T. E. Ward, “Use of neural
signals to evaluate the quality of generative adversarial network per-
formance in facial image generation,” Cognitive Computation, vol. 12,
no. 1, pp. 13–24, 2020.
[42] M. Tokumaru, N. Muranaka, and S. Imanishi, “Color design support
system considering color harmony,” in 2002 IEEE World Congress on
Computational Intelligence. 2002 IEEE International Conference on
Fuzzy Systems. FUZZ-IEEE’02. Proceedings (Cat. No. 02CH37291),vol. 1. IEEE, 2002, pp. 378–383.
[43] X. Y ang, T. Mei, Y .-Q. Xu, Y . Rui, and S. Li, “Automatic generation
of visual-textual presentation layout,” ACM Transactions on Multimedia
Computing, Communications, and Applications (TOMM), vol. 12, no. 2,pp. 1–22, 2016.
[44] M. P . Fay and M. A. Proschan, “Wilcoxon-mann-whitney or t-test? on
assumptions for hypothesis tests and multiple interpretations of decisionrules,” Statistics surveys, vol. 4, p. 1, 2010.
[45] K. Alharbi and T. Y eh, “Collect, decompile, extract, stats, and diff:
Mining design pattern changes in android apps,” in Proceedings of
the 17th International Conference on Human-Computer Interaction with
Mobile Devices and Services. ACM, 2015, pp. 515–524.
[46] A. Jahanian, S. Keshvari, S. Vishwanathan, and J. P . Allebach, “Colors–
messengers of concepts: Visual design mining for learning color se-
mantics,” ACM Transactions on Computer-Human Interaction (TOCHI),
vol. 24, no. 1, p. 2, 2017.
[47] A. Jahanian, P . Isola, and D. Wei, “Mining visual evolution in 21 years
of web design,” in Proceedings of the 2017 CHI Conference Extended
Abstracts on Human Factors in Computing Systems. ACM, 2017, pp.
2676–2682.
[48] B. Fu, J. Lin, L. Li, C. Faloutsos, J. Hong, and N. Sadeh, “Why people
hate your app: Making sense of user feedback in a mobile app store,”inProceedings of the 19th ACM SIGKDD international conference on
Knowledge discovery and data mining. ACM, 2013, pp. 1276–1284.
[49] W . Martin, F. Sarro, Y . Jia, Y . Zhang, and M. Harman, “A survey of app
store analysis for software engineering,” IEEE transactions on software
engineering, vol. 43, no. 9, pp. 817–847, 2017.
[50] T. F. Liu, M. Craft, J. Situ, E. Y umer, R. Mech, and R. Kumar, “Learning
design semantics for mobile apps,” in The 31st Annual ACM Symposium
on User Interface Software and Technology. ACM, 2018, pp. 569–579.
[51] A. Swearngin, M. Dontcheva, W . Li, J. Brandt, M. Dixon, and A. J. Ko,
“Rewire: Interface design assistance from examples,” in Proceedings of
the 2018 CHI Conference on Human Factors in Computing Systems.
ACM, 2018, p. 504.
[52] S. Chen, L. Fan, C. Chen, T. Su, W . Li, Y . Liu, and L. Xu, “Storydroid:
Automated generation of storyboard for android apps,” in Proceedings
of the 41st International Conference on Software Engineering. ACM,2019.
[53] M. Fischer, R. R. Y ang, and M. S. Lam, “Imaginenet: Style transfer
from ﬁne art to graphical user interfaces,” 2018.
[54]
J. Chen, M. Xie, Z. Xing, C. Chen, X. Xu, L. Zhu, and G. Li, “Object
detection for graphical user interface: old fashioned or deep learning
or a combination?” in Proceedings of the 28th ACM Joint Meeting
on European Software Engineering Conference and Symposium on theF oundations of Software Engineering, 2020, pp. 1202–1214.
[55] M. Xie, S. Feng, Z. Xing, J. Chen, and C. Chen, “Uied: a hybrid tool
for gui element detection,” in ESEC/FSE, 2020.
759[56] C. Chen, S. Feng, Z. Liu, Z. Xing, and S. Zhao, “From lost to found:
Discover missing ui design semantics through recovering missing tags,”
Proceedings of the ACM on Human-Computer Interaction, vol. 4, no.CSCW2, pp. 1–22, 2020.
[57] C. Chen, S. Feng, Z. Xing, L. Liu, S. Zhao, and J. Wang, “Gallery
dc: Design search and knowledge discovery through auto-created gui
component gallery,” Proceedings of the ACM on Human-Computer
Interaction, vol. 3, no. CSCW , pp. 1–22, 2019.
[58] P . Sandhaus, M. Rabbath, and S. Boll, “Employing aesthetic principles
for automatic photo book layout,” in International Conference on
Multimedia Modeling. Springer, 2011, pp. 84–95.
[59] S. V empati, K. T. Malayil et al., “Enabling hyper-personalisation:
Automated ad creative generation and ranking for fashion e-commerce,”
arXiv preprint arXiv:1908.10139, 2019.
[60] Y . Li, L. Jiang, and M.-H. Y ang, “Controllable and progressive image
extrapolation,” arXiv preprint arXiv:1912.11711, 2019.
[61] T. Hinz, S. Heinrich, and S. Wermter, “Semantic object accuracy for
generative text-to-image synthesis,” arXiv preprint arXiv:1910.13321,
2019.
[62] J. Li, J. Y ang, A. Hertzmann, J. Zhang, and T. Xu, “Layoutgan: Syn-
thesizing graphic layouts with vector-wireframe adversarial networks,”
IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.
[63] A. A. Jyothi, T. Durand, J. He, L. Sigal, and G. Mori, “Layoutvae:
Stochastic scene layout generation from a label set,” in Proceedings
of the IEEE International Conference on Computer Vision, 2019, pp.9895–9904.
[64] D. Zhao, Z. Xing, C. Chen, X. Xu, L. Zhu, G. Li, and J. Wang, “Seeno-
maly: Vision-based linting of gui animation effects against design-don’tguidelines,” in 42nd International Conference on Software Engineering
(ICSE’20). ACM, New York, NY, 2020.
[65] Y . Bo, X. Zhenchang, X. Xin, C. Chunyang, Y . Deheng, and L. Shanping,
“Don’t do that! hunting down visual design smells in complex uis against
design guidelines,” in The 43rd International Conference on Software
Engineering, 2021.
[66] Z. Liu, C. Chen, J. Wang, Y . Huang, J. Hu, and Q. Wang, “Owl
eyes: Spotting ui display issues via visual understanding,” in 2020 35th
IEEE/ACM International Conference on Automated Software Engineer-
ing (ASE). IEEE, 2020, pp. 398–409.
[67] J. Chen, C. Chen, Z. Xing, X. Xu, L. Zhu, G. Li, and J. Wang, “Unblind
your apps: Predicting natural-language labels for mobile gui componentsby deep learning,” in Proceedings of the ACM/IEEE 42nd International
Conference on Software Engineering, ser. ICSE ’20. New Y ork, NY ,USA: Association for Computing Machinery, 2020, p. 322–334.
[68] S. Chen, L. Fan, C. Chen, M. Xue, Y . Liu, and L. Xu, “Gui-squatting
attack: Automated generation of android phishing apps,” IEEE Transac-
tions on Dependable and Secure Computing, 2019.
[69] F. Sidong, M. Suyu, Y . Jinzhong, C. Chunyang, Z. TingTing, and
Y . Zhen, “Auto-icon: An automated code generation tool for icon designsassisting in ui development,” in The 26th International Conference on
Intelligent User Interfaces, 2021.
760