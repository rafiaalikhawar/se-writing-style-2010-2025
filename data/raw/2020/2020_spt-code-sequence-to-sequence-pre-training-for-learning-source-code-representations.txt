SPT-Code: Sequence-to-Sequence Pre-Training for Learning
Source Code Representations
Changan Niu
State Key Laboratory for Novel
Software Technology
Nanjing University
Nanjing, China
nougatca@qq.comChuanyi Li
State Key Laboratory for Novel
Software Technology
Nanjing University
Nanjing, China
lcy@nju.edu.cnVincent Ng
Human Language Technology
Research Institute
University of Texas at Dallas
Richardson, Texas, USA
vince@hlt.utdallas.edu
Jidong Ge
State Key Laboratory for Novel
Software Technology
Nanjing University
Nanjing, China
gjd@nju.edu.cnLiguo Huang
Dept. of Computer Science
Southern Methodist University
Dallas, Texas, USA
lghuang@lyle.smu.eduBin Luo
State Key Laboratory for Novel
Software Technology
Nanjing University
Nanjing, China
luobin@nju.edu.cn
ABSTRACT
Recent years have seen the successful application of large pre-
trained models to code representation learning, resulting in sub-stantial improvements on many code-related downstream tasks.
ButthereareissuessurroundingtheirapplicationtoSEtasks.First,
themajorityofthepre-trainedmodelsfocusonpre-trainingonly
theencoderoftheTransformer.Forgenerationtasksthataread-
dressed using models with the encoder-decoder architecture, how-
ever, there isno reason why the decoder shouldbe left out during
pre-training. Second, many existing pre-trained models, including
state-of-the-artmodels suchasT5-learning, simplyreuse thepre-
trainingtasksdesignedfornaturallanguages.Moreover,tolearn
the natural language description of source code needed eventually
for code-related tasks such as code summarization, existing pre-
training tasks require a bilingual corpus composed of source code
and the associated natural language description, which severely
limits the amount of data for pre-training. To this end, we propose
SPT-Code, a sequence-to-sequence pre-trained model for sourcecode. In order to pre-train SPT-Code in a sequence-to-sequencemanner and address the aforementioned weaknesses associated
with existing pre-training tasks, we introduce three pre-training
tasks that are specifically designed to enable SPT-Code to learn
knowledgeofsourcecode,thecorrespondingcodestructure,aswell
as a natural language description of the code without relying on
any bilingual corpus, and eventually exploit these three sources ofinformation when it is applied to downstream tasks. Experimental
results demonstrate that SPT-Code achieves state-of-the-art perfor-
mance on five code-related downstream tasks after fine-tuning.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.3510096CCS CONCEPTS
â€¢Softwareanditsengineering â†’Softwaremaintenancetools;
â€¢Computing methodologies â†’Artificial intelligence.
KEYWORDS
pre-training, code representation learning, sequence-to-sequence
ACM Reference Format:
ChanganNiu,ChuanyiLi,VincentNg,JidongGe,LiguoHuang,andBinLuo.
2022.SPT-Code:Sequence-to-SequencePre-TrainingforLearningSource
Code Representations. In 44th International Conference on Software Engi-
neering(ICSEâ€™22),May21â€“29,2022,Pittsburgh,PA,USA. ACM,NewYork,
NY, USA, 13 pages. https://doi.org/10.1145/3510003.3510096
1 INTRODUCTION
Pre-Training has revolutionized the way computational models are
trainedinthenaturallanguageprocessing(NLP)community[ 12,
13,43,44]. For a long time, supervised learning has been the most
successful natural language learning paradigm. The pioneers of
thepre-trainingideachallengedthisviewbyshowingthatavast
amountofgeneralknowledgeaboutlanguage,includingbothlin-
guistic and commonsense knowledge, can be acquired by (pre-
)training a model in a task-agnostic manner using self-supervised
learning tasks. Self-supervised learning tasks are NLP tasks for
which the label associated with a training instance can be derived
automaticallyfromthetextitself.Consider,forinstance,oneofthe
most well-known self-supervised learning tasks, Masked LanguageModeling(MLM)[
13].Givenasequenceofwordtokensinwhicha
certain percentage of tokens is maskedrandomly, the goal of MLM
istopredictthemaskedtokens.Ascanbeeasilyimagined,amodel
forMLMcanthereforebetrainedoninstanceswhereeachoneis
composedofapartiallymaskedsequenceofwordtokensandthe
associated â€œclassâ€ value is the masked tokens themselves. Because
nohumanannotationisneeded,amodelcanbepre-trainedona
very large amount of labeled data can be automatically generated,
thereby acquiring a potentially vast amount of knowledge about
language. A pre-trained model can then be optimized for a specific
20062022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:53:04 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Changan Niu, Chuanyi Li, Vincent Ng, Jidong Ge, Liguo Huang, and Bin Luo
taskbyfine-tuningitsparametersusingtask-specificlabeleddata
in the standard supervised fashion.
Inspiredbythesuccessesofpre-trainedmodelsinNLP,anumber
of pre-trained models for source code have been proposed and
applied to a variety of SE tasks including code summarization and
codecompletion,withnotablesuccesses[ 8,16,19,25,27,28,37,39,
51].Despitethesepromisingresults,thereareissuessurrounding
the application of pre-trained models for source code to SE tasks.
First, the majority of these pre-trained models focus on pre-
training only the encoder of the Transformer [ 8,16,19,27]. This
is not ideal, however. For instance, for generation tasks that are
addressed using models with the encoder-decoder architecture,
there is no reason why the decoder should be left out in the pre-training process. Second, these models have largely assumed as
inputs the source code [ 8,27,28,37,51] and the associated natural
languagedescription[ 16,39].Inparticular,codestructure,whichis
alsocrucialtounderstandingsourcecode,islargelymissingfrom
thesemodels.Thereason whycodestructureisleftout isthatSE
researchershaveforthemostpartsimplyreusedthepre-training
tasks designed for natural languages when pre-training models (by
viewingsourcecodeasnaturallanguage),butnoneofthesetasksare
concerned with learning the structure of anatural language. Third,
thesepre-trainingtasksassumetheavailabilityofabilingualcorpus,
where each method/function (henceforth collectively referred to
as method) is â€œlabeledâ€ with the corresponding docstring, whenpre-training a model on source code and the associated naturallanguage description [
16,19]. However, such a bilingual corpus
tends to be small in size compared to a monolingual (i.e., sourcecode only) corpus, thus severely limiting the amount of data amodel can be pre-trained on. In general, we believe the reliance
onabilingualcorpuswouldhinderthedevelopmentofpowerful
pre-trainedmodelsforsourcecodeinthelongrun,asakeystrength
of the pre-trained models developed in the NLP community stems
fromtheirabilitytobetrainedusingself-supervisedtasksforwhich
very large amounts of labeled data can be automatically generated.
Several attempts have been made to address the three problems
mentioned above to different extents. To address the first prob-
lem, T5-learning [ 39] and TreeBERT[ 25] are proposed,which are
sequence-to-sequence(i.e.,seq2seq)pre-trainingmodelswiththe
encoder-decoder architecture and enables both the encoder and
thedecodertobejointlytrained.Toaddressthesecondproblem,
GraphCodeBERT [ 19] employstwo pre-trainingtasks specifically
designedtoacquirestructural information.Onetaskinvolvespre-
dictingtheedgesinthedataflowwhiletheotherinvolvespredicting
the alignment between the nodes in the data flow and the code
sequence,respectively.However,astheauthorsalsopointedout,
whilethedataflowcapturesinformationthatislargelysemantic
innature,itdoesnotcapturesyntacticinformation(e.g.,thesyn-
tactic structure encoded in an Abstract Syntax Trees, i.e., AST),
which is arguably the most important type structural information
about source code that is commonly exploited bySE researchers.
TreeBERT [ 25] employs the set of constituent paths of ASTs as
the input of its encoder. However, it only inputs code sequences at
thedecodersideduringpre-training,tryingtomaketheencoder
learnlexicalandsemanticinformation(bothofwhichcanbeeasily
obtainedfromthecodesequences)fromtheAST(whichcontains
mainlysyntacticinformation)duringthepre-trainingphase.Butit is uncertain whether TreeBERT can extract the complete lex-ical and semantic information from the AST only by relying on
pre-training, thus eliminating the need to input code tokens when
fine-tuning.Toaddressthethirdproblem,T5-learningtreatscode
andnaturallanguageastwotypesofindependentdatainstances.
WhilethisallowsT5-learningtolearnfromamonolingualrather
than bilingual corpus, the connection between a piece of code and
the associated natural language description is no longer present in
the corpus. Hence, it is no longer clear whether T5-learning canstilllearntoproduceanaturallanguagedescriptionofapieceof
code.Toourknowledge,therehasbeennoattempttoaddressall
three issues in a single model.
In light of the above discussion, we propose SPT-Code, a new
pre-trained model for source code. Motivated by T5-learning, SPT-
Code is a seq2seq pre-training model, enabling both the encoder
andthedecoderofTransformertobejointlypre-trained.Eachdata
instanceforSPT-Codeiscomposedofthreetypesofinformation
derived from a method, namely the code sequence, its AST, and
the associated natural language description. Note that the incorpo-
rationofASTsallowsSPT-Codetoexploitstructural,specifically
syntactic,information.Inaddition,toobviatetheneedtolearnnat-
ural language descriptions from a bilingual corpus, we will simply
use the name of the method and the names of the methods thatare invoked in this method as a (very succinct) natural language
description of the given source code.
Wedesignthreecode-specificpre-trainingtasksforSPT-Code,
each of which allows SPT-Code to acquire exactly one of the three
typesofinformationthatcompriseadatainstance.Thefirsttaskisa
versionthewell-knownMaskedSequencetoSequence(MASS)[ 49]
pre-trainingtaskfornaturallanguagethatweadapttosourcecode.
Specifically,ourmodifiedMASStaskseekstoacquireknowledge
about source code via masking a random fragment of the code
tokens.Thesecondtask,Code-ASTprediction(CAP),isdesigned
toenablethemodeltogainknowledgeofthesyntacticstructureofacodefragmentbypredictingwhetherthegivenASTisthecorrect
AST for the given piece of code. The final task, Method Name
Generation (MNG), is a novel task that involves generating the
subtokens of the method name, which we take to be an (extremely
concise) natural language description of the method.
AfterSPT-Codeispre-trainedontheCodeSearchNetdataset[ 24],
wefine-tuneandevaluateitonfivedownstreamtasks,including
code summarization, code completion, bug fixing, code translation,
andcodesearch.ExperimentalresultsshowthatSPT-Codeachieves
state-of-the-art results under virtually all circumstances.
In sum, we make the following contributions:
(1)ProposeSPT-Code,aseq2seqpre-trainedmodelforsource
code that is built with the encoder-decoder architecture and
is applicable to both classification and generation tasks.
(2)Extend the input representation of pre-trained models for
sourcecodewithasimplifiedandlinearizedversionofASTs.
Toourknowledge,wearethefirsttousebothnaturallan-
guage and AST as inputs in pre-training for source code.
(3)Design special input representations and three code-specific
seq2seq-based pre-training tasks enabling SPT-Code to bepre-trained without relying on any bilingual corpus or la-
beled data.
2007
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:53:04 UTC from IEEE Xplore.  Restrictions apply. SPT-Code: Sequence-to-Sequence Pre-Training for Learning Source Code Representations ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
(4)Pre-trainSPT-Codeonalargeunlabeledmonolingual(i.e.,
sourcecodeonly)datasetacrosssixprogramminglanguages,
then fine-tune and evaluate it on five downstream code-
related tasks, achieving state-of-the-art results on all tasks.
2 RELATED WORK
2.1 Pre-Training Models in NLP and SE
Table 1 presents an overview of the most prominent pre-trained
modelsinNLPandforcode.Eachmodelischaracterizedalongfour
dimensions:(1)Modules:whatisbeingpre-trained(e.g.,theencoder,
the decoder, or both); (2) Objectives: the pre-training objectives1
and (3) Inputs: what information the model assumes as input (e.g.,
natural language, code and structural information of the code).
WecanmakeafewobservationsfromTable1.First,whilethema-
jority of work has focused on pre-training the encoder, the newest
models (T5 [ 45], BART [ 34], and T5-learning [ 39], which is mod-
eled after T5) are all seq2seq pre-training models that allow theencoderandthedecodertobejointlytrained.Second,exceptfor
GraphCodeBERT[ 19]andTreeBERT[ 25],allpre-trainedmodelsin
SEreusetrainingobjectivesdesignedfornaturallanguages,with
MLM being the most popular pre-training task. This indicates that
the selection of pre-training tasks, which dictates what knowledge
will be acquired and exploited by a model, is an area of research
thatis under-investigatedin SE. Finally,while earlier pre-training
modelsinSEassumeonlysourcecodeasinput,thelateronesall
use both code and language.
In our experiments, we will use as our baselines the most re-
cently developed (and also the state-of-the-art) pre-trained models
for source code, namely CodeBERT [ 16], GraphCodeBERT [ 19],
CugLM [37], T5-learning [39] and TreeBERT [25].
2.2 Structural Information of Source Code
Structural information is very important for understanding source
code. ASTs are widely used in code related tasks for represent-
ingstructureinformationofcode(e.g.,[ 20,32,52,57,62]),which
containsabundantsyntacticstructureinformationthatcannotbe
expressed by code sequences. An AST should be flattened withlinearization methods, e.g., pre-order traversal [
57,62], in-order
traversal [ 52], and Structure-based Traversal (SBT) [ 20], before
being fed to an encoder. code2vec [ 6], code2seq [ 4], and SLM [ 5]
useamethodthatlinearizesanASTasaseriesofâ€œpath-contextsâ€
representing two terminal nodes and the path between them. Jiang
et al. [25] represent ASTs as the set of paths and then introduce
node position embedding to obtain the position of the node in the
1Forward LM [ 28,43,44] aims to predict the next word given the preceding words in
a sentence. Backward LM [ 28,43] aims to predict the previous word given the words
thatappearafteritinasentence.MaskedLMisthemaskedlanguagemodelingtask
described in Section 1. NSP [ 13] aims to predict whether the second sentence in a
sentencepairshouldimmediatelyfollowthefirstsentenceinthepair.Permutation
LM [60] aims to predict a word using a set of context words randomly selected via the
attentionmaskmechanism.RTDaimstopredictwhichtokenintheinputhasbeen
replaced. EP [ 19] involves masking 20% of the edges in a data flow and aims to predict
the masked edges. NA [ 19] involves masking a certain portion of edges of connecting
dataflownodesandcodetokensandaimstopredictthemaskededges.TMLM[ 25]
masks paths in the AST input on the encoder side and tokens in the code sequence
input on the decoder side, then predicts the token of the masked code. NOP [ 25]i st o
exchange thepositions of somenodes in the path,and distinguish whetherthe order
of nodes in the AST is correct or not.AST. Besides, neural networks that take trees as input (e.g., tree-
LSTMs[35,55,56],RvNNs[ 63]andGNNs/GCNs[ 9,31,61])utilize
an AST directly instead of flattening it.
There are also approaches taking data flow and control flow
extracted from code as structural information, e.g., [ 19] and [23].
However, these flows do not contain structural information as rich
asASTs[19].TheiradvantagesoverASTsarethattheyhavealower
demandonhardwareandneedlesstrainingtimeunderthesame
conditions [19].
Takentogether,wechoosetouseASTstorepresentthestructural
information in SPT-Code.
3 SPT-CODE
In this section we first introduce the architecture of SPT-Code
(Section 3.1). We then describe the model input (Section 3.2) and
the pre-training tasks (Section 3.3), which are the key innovations
of this paper. Finally, we illustrates how to fine-tune SPT-Code
when it is applied to downstream tasks (Section 3.4).
3.1 Model Architecture
Architecturally,SPT-Codeisessentiallyamulti-layerTransformer[ 54],
whichiswidelyusedbypre-trainingmodelssuchasBART[ 34]and
T5 [45]. As far as the parameter setting of the encoder and decoder
isconcerned, SPT-Codefollows CodeBERTandGraphCodeBERT:
(1) the number of layers (i.e., Transformer blocks) ğ¿=12, (2) the
size of model ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ =768, (3) the dimension of feed forward
ğ‘‘ğ‘“ğ‘“=3072,(4)thenumberofself-attentionheads â„=12,and(4)
the dropout rate ğ‘ğ‘‘ğ‘Ÿğ‘œğ‘ğ‘œğ‘¢ğ‘¡ =0.1. The total number of parameters is
262M.
To pre-train both classification and generation tasks with an
encoder-decoder structure, we adopt the strategy used in BART. In
particular,notethattheencoderanddecoderwillcontinuetolearnjointly and collaboratively when pre-trained on classification tasks.
Figure1showshowclassificationandgenerationtasksarepre-
trainedinSPT-Code.Specifically,touseSPT-Codeforclassification
tasks,theinputofthedecoderisthesameasencoder,exceptthat
a special symbol â€œ[SOS]â€ (indicating the start of the sequence) is
added to the front, and another special symbol â€œ[EOS]â€ (used here
as a placeholder for the classification position) is added to the end.
The output of the corresponding position of the â€œ[EOS]â€ is then
usedforclassification.Forgenerationtasks,Figure1bdemonstrates
the input and output of the model when translating â€œA BCDE â€
to â€œÎ±Î²Î³Î´Îµâ€.The specialsymbolâ€œ[EOS]â€ denotesthe endof the
sequence, and the process of generation stops when the model
outputs this symbol.
3.2 Model Input
The inputs of the model are three different types of components
belongingtoacomplete Method(i.e.,itcanbeinvokedbyitsname),
namely, the code tokens, the linearized AST, and the natural lan-
guage. In this subsection, we will demonstrate this with a real Java
method2shown in the bottom of Figure 2.
2https://github.com/Unidata/thredds/blob/d2d68f9eee87f345625211324d71d5dc3e162
ee1/cdm/src/main/java/thredds/client/catalog/Property.java#L56-L63
2008
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:53:04 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Changan Niu, Chuanyi Li, Vincent Ng, Jidong Ge, Liguo Huang, and Bin Luo
Table 1: Pre-training models in NLP and for source code.
Domain ModelsModules Objectives Inputs
LSTMEncoder Decoder Encoder-Decoder Forward LM Backward LM Masked LM NSP Permutation LM RTD - - NLCode Structure
NLPELMo(2018) /check /check /check /check
BERT(2019) /check /check /check /check
XLNet(2019) /check /check /check
RoBERTa (2019) /check /check /check
ELECTRA (2019) /check /check /check /check
GPT-2 (2019) /check /check /check
T5(2020) /check /check /check
BART(2020) /check /check /check
CodeSCEMLo (2020) /check /check /check /check
CuBERT (2019) /check /check /check /check
C-BERT(2020) /check /check /check /check
IntelliCode (2020) /check /check /check
CodeBERT (2020) /check /check /check /check/check
GraphCodeBERT (2020) /check /check EPNA /check/checkData Flow
CugLM(2020) /check /check /check/check /check
T5-learning (2021) /check /check /check/check
TreeBERT (2021) /check TMLM NOP /checkAST
A959C(OJOXKIZOUTGR
+TIUJKX'[ZU8KMXKYYO\K
*KIUJKX
) '( *+RGHKR
) * + ' ( A+59C
(a) When SPT-Code is used for classification, the inputs of the
encoder and decoder are identical, and the output of the decoder
at the last time step is used as the label for the classification.
(OJOXKIZOUTGR
+TIUJKX'[ZU8KMXKYYO\K
*KIUJKX
Å Å›Åœ Å A959C ÅŸA+59C
) * + ' (Å› ÅœÅÅ ÅŸ
(b)WhenSPT-Codeisusedforgeneration,thewholeprocedure
is the same as the regular sequence-to-sequence model.
Figure 1: SPT-Code for classification and generation.
3.2.1 Code Tokens. As we can see from Figure 2, the first part
of the input is the code token sequence of a method. We use a
lexical analyzer to tokenize the source code and then obtain thetokens
ğ¶={ğ‘1,ğ‘2,...,ğ‘ğ‘™}, whereğ‘™is the number of code tokens.
Specifically,weusethePythonstandardlibrary3totokenizePython
codes. For languages such as Java, JavaScript, PHP and Go, we use
thePythonbinding4ofANTLR45togetthecodetokens.TheRuby
sourcecodeistokenizedbythecallingofaRubybinaryprogram.
Thesourcecodesofotherprogramminglanguagesaretokenized
by the NLTK tokenizer6.
3.2.2 Linearized AST. To represent the second part of the input,
i.e., the structural information of the code, we convert an AST into
a specially formatted sequence by traversing it,and call the result
3https://docs.python.org/3.8/library/tokenize.html
4https://pypi.org/project/antlr4-python3-runtime/
5https://github.com/antlr/antlr4
6https://www.nltk.org/api/nltk.tokenize.htmlA9+6C )UJK
V[HROIYZGZOI2OYZ"6XUVKXZ_'9: 42 A9+6C
JKIRGXGZOUT"LUX$"OL$
K^VXKYYOUT"OL$XKSU\KJ[VYYO`KIUTZGOTYGJJ
//firstoneoverride
publicstaticList<Property> removeDups (List<Property> org){
 List<Property> result=newArrayList<>(org. size ());
 for(Property p:org)
 if(!result. contains (p))//O(n**2)
 result. add(p);
 returnresult;
}ZUQKTO`KX
'9:VGXYKXXKSU\K*[VYYO`KIUTZGOTYGJJIGSKRYTGQK
IGYKYVROZ
'9:
>9(:GZ
K^VXKYYOUTRK\KR
TGSKGTJ
OT\UIGZOUTY
Figure 2: The input of a real world Java code snippet. Due to
spaceconstraints,weabbreviatedthenodenamesintheAST
sequence. â€œNLâ€ denotes the natural language input.
ofthisconvertinga linearized AST.WefirstemployanASTparser7
to get the corresponding AST, then use a traversal method to parse
the AST into a sequence.
Instead of using the original SBT (please refer to [ 20,21] for
more details), which has been shown to be more effective thanclassical traversal methods (i.e., pre-order traversal) but tend to
produce excessively long sequences that are on average more thanthree times the length of the code, we propose a simplified version
ofSBTcalledX-SBT(XML-likeSBT)totraverseASTs.X-SBTcan
reducethelengthoftheresultingsequenceoftraversalsbymore
thanhalf,Figure3showsacomparisonofSBTandX-SBT.Itcanbe
seenthatwhentraversingAST,SBTtakestwotokens,â€œ(â€andthe
node name, as the starting flag of a certain AST node, and takes â€œ)â€
andthenameastheendingflag.Wemakeoneobservation:fornon-
terminal nodes (i.e., nodes which are not leaves), we can replace
the starting flag with one token in an XML-like form, similarly for
7https://tree-sitter.github.io/tree-sitter/
2009
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:53:04 UTC from IEEE Xplore.  Restrictions apply. SPT-Code: Sequence-to-Sequence Pre-Training for Learning Source Code Representations ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
'
( )
+ ,*>9(:2KTMZN 
<A>
 <B>
 E
 F
 </B>
 C
 D
</A>'9: 9(:2KTMZN 
(A
 (B
 E(E)
 F(F)
 )B
 (C)C
 (D)D
)A
Figure 3: An example of the SBT and X-SBT, traversal se-
quences are formatted and indented for a better illustration.
the ending flag. For terminal nodes, i.e., leaf nodes, we can further
merge the starting and ending tokens into one token. Therefore, it
is easy to prove that X-SBT shortens the length by more than half.
However,X-SBTsequencesarestilllong.TofurthershortenX-
SBTsequences,wemakeX-SBTtraverseonlythenodesatorabove
theexpression levelintheAST.Thiswillalsoreduceredundancy.
Commonly,theASTcontainsbothlexicalandsyntacticinformation,
wherethelexicalinformationisalreadyrepresentedbythefirstpart
of the input (i.e., code tokens). Since the lexical information in the
ASTisconcentratedontheterminalnodes,weonlykeepthenodes
atorabovethe expression level,sothatitcontainsonlysyntactic
information. Figure 4 shows the nodes in the AST that will be
OLEYZGZKSKTZ
VGXKTZNKYO`KJEK^VXKYYOUT
[TGX_EK^VXKYYOUT
SKZNUJEOT\UIGZOUT
OJKTZOLOKXK^VXKYYOUTEYZGZKSKTZ
GXM[SKTZEROYZ
OJKTZOLOKXSKZNUJEOT\UIGZOUT
VOJKTZOLOKX OJKTZOLOKX
V
XKY[RZOJKTZOLOKX
IUTZGOTYXKY[RZ GJJKTNGTIKJELUXEYZGZKSKTZ
Z_VKEOJKTZOLOKX OJKTZOLOKX OJKTZOLOKX
6XUVKXZ_ V UXM
Figure 4: The AST of the â€œforâ€ statement block of the Java
code snippet in Figure 2. Non-terminal nodes are ellipses
and terminal nodes are represented as rectangles. The green
nodes will be traversed by X-SBT at the expression level,
while the red ones will not.
traversedbytheX-SBTatthe expression level.Obviously,ittraverses
onlyonesubtreeoftheAST,andconsequently,itcanfurtherreduce
the length of the sequence by ignoring some fine-grained (lexical)
informationthatisalreadypresentinthecodetokens.Toconclude,
wetraversetheASTusingX-SBTatthe expression leveltoobtainthe
tokensğ´={ğ‘1,ğ‘2,...,ğ‘ğ‘š}, whereğ‘šis the length of the sequence.
3.2.3 Natural Language. For extracting natural language informa-
tionfromthecodeonly,wederivethemethodnameandAPIcall
sequence of the code8. For example, the tokens extracted from the
codesnippetinFigure2are removeDups, size, contains, add .
We further split each token of the form CamelCase andsnake_case,
soremoveDups is split into removeanddups. Then we take the
8The reason we do not use documentation in code, such as docstring and in-line
comments,isthatdocumentationisnotalwaysavailableaswementionedinSection1.resulting linear sequence of tokens, ğ‘={ğ‘›1,ğ‘›2,...,ğ‘›ğ‘}, as our
natural language input, where ğ‘is the number of tokens.
Aftercompletingtheconstructionofthethreeinputparts,we
concatenatethemanduseaspecialsymbol,i.e.,â€œ[SEP]â€,toseparate
the three inputs. Therefore, the input is represented as
ğ¼ğ‘›ğ‘ğ‘¢ğ‘¡ =ğ‘1,...,ğ‘ğ‘™,[SEP],ğ‘2,...,ğ‘ğ‘š,[SEP],ğ‘›1,...,ğ‘›ğ‘(1)
3.3 Pre-Training Tasks
Inthissection,weintroducethethreetasksintheordertheyare
used for (sequential) pre-training.
3.3.1 Code-AST Prediction. The first pre-training task is Code-
ASTPrediction(CAP).Sinceweaddstructuralinformationtothe
input, in CAP, we expect to have the model learn such informa-tion about the structure represented by X-SBT. So we introducethis binarized task that can be trivially generated from any code.
Specifically,whenweconstructtheinputrepresentation,namely
ğ¼ğ‘›ğ‘ğ‘¢ğ‘¡ =ğ¶,[SEP],ğ´,[SEP],ğ‘, 50% of the time ğ´is the actual AST se-
quence corresponding to ğ¶(labeled as IsAST), and 50% of the time
itisarandomASTsequencefromthedataset(labeledas NotAST).
AsweshowinFigure1a,â€œlabelâ€isusedforCode-ASTPrediction.
Note that CAP is closely similar to the next sentence prediction
task [13].
3.3.2 MASS. Since SPT-Code is based on the encoder-decoder
architecture, we expect to pre-train SPT-Code in a sequence-to-sequence style. Therefore, we then adopt MASS, which seeks toreconstruct a sentence fragment given the remaining part of thesentence in the encoder-decoder framework. We employ a mod-ified version of MASS as one of our pre-training tasks, with the
intentionoftrainingthemodeltounderstand,inferandgenerate
code sequences.
Given aninput ğ¼ğ‘›ğ‘ğ‘¢ğ‘¡ =ğ¶,[SEP],ğ´,[SEP],ğ‘, wedenoteğ¶\ğ‘¢:ğ‘£
originas
amodifiedversionof ğ¶whereitsfragmentfromposition ğ‘¢toğ‘£is
masked, that is,
ğ¶\ğ‘¢:ğ‘£
origin={ğ‘0,...,ğ‘ğ‘¢âˆ’1,[ğ‘€ğ´ğ‘†ğ¾],...,[ğ‘€ğ´ğ‘†ğ¾],ğ‘ğ‘£+1,...,ğ‘ğ‘™}(2)
where 0 <ğ‘¢<ğ‘£<ğ‘™. In contrast, ğ¶ğ‘¢:ğ‘£denotes the fragment of
ğ¶fromğ‘¢toğ‘£. In our work, we merge a number of consecutive
â€œ[MASK]â€ symbols into one, then ğ¶\ğ‘¢:ğ‘£
originis replaced by ğ¶\ğ‘¢:ğ‘£:
ğ¶\ğ‘¢:ğ‘£={ğ‘0,...,ğ‘ğ‘¢âˆ’1,[ğ‘€ğ´ğ‘†ğ¾],ğ‘ğ‘£+1,...,ğ‘ğ‘™} (3)
We utilize this modified version of MASS to pre-train our se-
quencetosequencemodelbypredicting thefragment ğ¶ğ‘¢:ğ‘£taking
theinputğ¼ğ‘›ğ‘ğ‘¢ğ‘¡ =ğ¶\ğ‘¢:ğ‘£,[SEP],ğ´,[SEP],ğ‘.Then,ğ‘˜=ğ‘¢âˆ’ğ‘£+1isthe
number of the masked consecutive tokens, and we follow Song et
al. [49] and set ğ‘˜=50% ofğ‘™to achieve the best performance.
3.3.3 Method Name Generation. With the last pre-training task,
we would expect the model to learn information such as the intent
and functionality of the code. The method name, which is present
in every method, can be seen as an extremely concise summaryofthemethod.Xieetal.[
59]analyzethemethodnameandcode
summary in a Java dataset built by Leclair and McMillan [ 33], and
findthatonaverage50.6%ofthewordsinmethodnamesappear
in the corresponding summaries, and 21.3% of the words in the
summariesappear inthe correspondingmethod names.For about
2010
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:53:04 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Changan Niu, Chuanyi Li, Vincent Ng, Jidong Ge, Liguo Huang, and Bin Luo
20% of the methods, all the words in the method names appear
in the corresponding summaries. Therefore, they improve codesummarization performance successfully by predicting method
names in advance.
Therefore, we exploit method names in this pre-training task,
which we call Method Name Generation (MNG). Given the model
inputğ¼ğ‘›ğ‘ğ‘¢ğ‘¡ =ğ¶,[SEP],ğ´,[SEP],ğ‘, we denote ğ‘›ğ‘ğ‘šğ‘’ =ğ‘ğ‘–as the
method name in ğ¶, whereğ‘–is the index of the method name token
inğ¶. From Section 3.2.3, we learn that the split ğ‘›ğ‘ğ‘šğ‘’ =ğ‘ğ‘–also
appears inğ‘, so we remove the tokens from ğ‘that are derived
from the method name split. Assuming that the method name is
split intoğ‘ subtokens, the final input of MNG will be as follows:
ğ¼ğ‘›ğ‘ğ‘¢ğ‘¡ğ‘€ğ‘ğº =ğ‘0,...,ğ‘ğ‘–âˆ’1,[ğ‘€ğ´ğ‘†ğ¾],ğ‘ğ‘–+1,...,ğ‘ğ‘™,
[ğ‘†ğ¸ğ‘ƒ],ğ‘1,ğ‘2,...,ğ‘ğ‘š,
[ğ‘†ğ¸ğ‘ƒ],ğ‘›ğ‘ +1,ğ‘›ğ‘ +2,...,ğ‘›ğ‘(4)
Then we make the decoder try to generate the split method name,
i.e.,ğ‘›1,ğ‘›2,...,ğ‘›ğ‘ .
3.4 Fine-Tuning
Wegroupalldownstreamtasksintotwocategories:classification
andgeneration.Forclassificationtasks,weusethesettingshownin
Figure 1a; otherwise, we use the setting in Figure 1b. For each task,
we simply plug in the task-specific inputs and outputs into SPT-
Code and fine-tune all the parameters end-to-end. For example, in
code search, which we cast as a classification task, when obtaining
arepresentation vectorofanaturallanguage query,wetake only
thenaturallanguageasinputanddonotconsiderthefirsttwoparts
of the input, i.e., code and AST. For code summarization, which
we cast as a generation task, we only include the natural language
summaryintheoutput.Wewilldescribethetask-specificdetails
in the corresponding subsections of Section 4.
4 EXPERIMENT
Inthissection,wefirstintroducethepre-trainingdataandsettings
(Section 4.1) and how we fine-tune on the downstream tasks(Sec-
tion 4.2). Then, we answer four research questions (Section 4.3).
Finally, we manually evaluate SPT-Code through quantitative and
qualitative analysis.
4.1 Pre-Training
4.1.1 Dataset. The dataset we use for pre-training SPT-Code is
CodeSearchNet[ 24],whichhasalsobeenusedtopre-trainCode-
BERT [16], GraphCodeBERT [ 19] and T5-learning [ 39]. The Code-
SearchNet corpus is programmaticallyobtained by scraping open-
sourcerepositoriesandpairingindividualfunctionswiththeir(pro-
cessed) documentation as natural language annotation. It includes
more than 6.4M codes from 6 programming language, i.e., Java,
Python,JavaScript,PHP,GoandRuby.Thedatastatisticsareshown
in Table 2.
Sinceourinputcanbeextractedfromcompletelyunlabeleddata,
we can make use of all the 6.4M data instances in CodeSearch-
Net. However, CodeBERT and GraphCodeBERT use labels (i.e., the
documentation) in the input for training both code and natural
language,sotheyarebothpre-trainedonallthedataintheâ€œw/doc-
umentationâ€ column of the â€œAllâ€ row, which we named CSNğ‘¤/ğ‘‘ğ‘œğ‘.Table 2: Pre-Training Dataset Size Statistics
Language# Methods/Function
w/ documentation All
Java 542,991 1,569,889
Python 503,502 1,156,085
JavaScript 157,988 1,857,835
PHP 717,313 977,821
GO 347,789 726,768
Ruby 57,393 164,048
All 2,326,976 6,452,446
T5-learning,inputsthecodeandthedocumentationintothemodel
separatelywhenpre-training,soitresemblesSPT-Codeinthesense
thatitdoesnotutilizethecode-documentcorrespondence.Never-
theless,the publiclyavailable implementationof T5-learningusesonly the data in the â€œAllâ€ column of â€œJavaâ€ row, named CSN
ğ½ğ‘ğ‘£ğ‘.
4.1.2 Settings. Thethreepre-trainingtasksofSPT-Codeareper-
formedsequentially.Wetrieddifferentordersanddifferentnum-bersofepochforthesetasksandfoundthatbetterresultscanbe
achieved by first pre-training CAP for 10 epochs, then MASS for
30epochs,andeventuallyMNGfor30epochs.Allthreetasksuse
cross-entropy as the loss function. We use AdamW [ 38] as our op-
timizer, the initial learning rate is 5e-5 and the warmup step is set
as 2000. We pre-train SPT-Code on 4 Ã—NIVDIA A100s9with a total
batch size of 256.
We use Byte-Pair Encoding (BPE) [ 47] to tokenize the code and
thenaturallanguage(afterCamelCaseandsnake_casesplitting),
and use regular word tokenizer to tokenize X-SBT sequences. The
tokenizer is built upon the whole pre-training data, and will be
directly used on each downstream tasks without any modification.
4.2 Fine-Tuning on Downstream Tasks
In this subsection, we detail the process of fine-tuning the pre-trained SPT-Code on the five downstream tasks. For each down-
streamtask,we giveabriefintroduction,tell thedatasetsforfine-
tuning,presentthecomparedbaselines,andeventuallyillustrate
the metrics for evaluating.
4.2.1 CodeSummarization. Codesummarization,alsoknownas
theprocessofcodesummaryorcodecommentgeneration,isthe
task of automatically generating a natural language description for
apieceofsourcecodethatsummarizestheoverallactionsofthe
codesnippetaccurately,adequately,andconcisely[ 50].Workinthis
area of research has generally focused on generating a (typically
short) natural language comment from a given method.
Datasets. InadditiontoCodeSearchNet,weusetwowidelyused
classicaldatasetsthatarecollectedfromopensourcerepositories
in GitHub, i.e., the Java dataset (JCSD) [ 22] and the Python dataset
(PCSD) [40]. As for JCSD, the comment of each method is the first
9TheGPUsareprovidedbyAlibabaGroup,whichemployEFLOPS[ 14]andAlibaba
CollectiveCommunicationLibrary(ACCL)[ 15]techniques.EFLOPSisahighperfor-
mancedistributedtrainingsystemthatcanachievenear-linearscalabilityofoverall
throughput,andACCLbringstheperformantefficiencyofEFLOPSarchitecturetogen-
eral cluster systems and Cloud scenarios, which is able to achieve fully non-congested
communication.
2011
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:53:04 UTC from IEEE Xplore.  Restrictions apply. SPT-Code: Sequence-to-Sequence Pre-Training for Learning Source Code Representations ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
sentence extracted from its Javadoc, and the comment of a method
in PCSD is its docstring.
Baselines. We adopt CodeBERT [ 16], GraphCodeBERT [ 19],
CugLM[37],T5-learning[ 39]andTreeBERT[ 25]asbaselinesfor
alldatasets10.ForJCSD,PCSDandCodeSearchNetâ€™sJava/Python
datasets,weadoptNerualCodeSum[ 2],whichemploysRelativePo-
sition [48] and Copy Attention [ 46] upon vanilla Transformer [ 54],
asanotherbaseline11.Besides,weusethreemorebaselinesforJCSD
andPCSD,namely,DeepCom[ 20],Rencos[ 62],andSiT[ 58],which
are widely compared or recently proposed RNN or transformer-
based models dedicated for code summarization.
Metrics. WeuseBLEU(B.)[ 42],METEOR(M.)[ 7]andROUGE-L
(R.L)[36],whicharewidelyusedinfieldssuchasmachinetrans-
lationandtextsummarization,to measurethesimilaritybetween
the sentences generated by the model and the goal sentences.
4.2.2 Codecompletion. Codecompletionistogeneratecodegiven
itssurroundingcodeascontext.Wefine-tuneSPT-Codeonatask
calledany-code completion[ 5]here.Unlike restricted completion
wherethetargetcodecontainsonlyprimitivetypes(e.g., intand
string) and excludes user-defined functions, any-code completion
aimstogeneratecodeinageneral-purposeprogramminglanguage
without any restriction on its vocabulary or structure. Specifically,
consider a program Pand some part of the program ğ‘âˆˆP, any-
code completion makes the model to predict ğ‘given the rest of the
program Pâˆ’=P/ğ‘.
Datasets. The dataset we use for any-code completion is the
JavadatasetprovidedbyAlonetal.[ 5].ItisbasedontheJava-small
dataset[4],whichcontainstheleastcodeduplication[ 3].Alonetal.
createany-codecompletionexamplesbyselectingeveryexpression
largerthanasingleASTnodeasthetarget ğ‘,usingthereminder
ofthemethodasthecontext P.Theyalsofiltermanymethodsto
cleanthedatasetandmakethetaskharder.Theresultingdataset
contains 1.3M/10k/20k train/dev/test examples.
Baselines. Besides CodeBERT, GraphCodeBERT, CugLM, T5-
learningandTreeBERT,somespecificmethodsforthistaskarealso
compared.Code2seq[ 4]representsacodesnippetasthesetofcom-
positional paths in its AST and uses attention to select the relevant
paths while decoding. Transformer w/ copy uses the implementa-
tionofOpen-NMT[ 30]withacopymechanism[ 17].Seq2treew/
copy [1] learns to generate the linearized, subtokenized target AST
forcodecompletion.SLM[ 5]leveragesjointmodelingofanAST
anditsmissingsubtreeusingastructurallanguagemodel,which
estimates the probability of the programâ€™s AST by decomposing it
into a product of conditional probabilities over its nodes.
Metrics. Following Alon et al. [ 5], we use exact match accu-
racy at 1 (Acc@1) and 5 (Acc@5) for evaluation. An exact matchis counted if and only if the sentence generated by the model is
identicaltothegoalsentence(ignoringcasesandwhitespaces).If
weletthemodelgenerate ğ‘˜candidatesentenceswiththehighest
probabilityandifanexactmatchiscountedwhileanyoneofthe
candidates matches the goal sentence exactly, the ultimate exact
match accuracy is Acc@ ğ‘˜(e.g., if only one candidate, it is Acc@1).
10We resize all the pre-trained models here and in all the following downstream tasks
identical to ours.
11WehavetriedtorunitasJavaforotherlanguagesinCodeSearchNetbutgotcom-
pletely nonsensical output.4.2.3 Bug fixing. Bug fixing, aka. coderepairor code refinement,
aims to automatically fix bugs in the code. It can help software
developers locate and fix bugs and thus save a lot of time [26, 53].
Datasets. We use the dataset collected by Tufano et al. [ 53],
whoextractmethod-level pairsofbuggyandcorrespondingfixed
code named BFPs (bug-fix pairs) from bug-fixing commits in thou-
sands of GitHub Java repositories. Each BFP is composed of a tuple
ğµğ¹ğ‘ƒ=<ğ‘šğ‘,ğ‘šğ‘“>, whereğ‘šğ‘represents a buggy code component,
ğ‘šğ‘“representsthecorrespondingfixedcode.Basedonthecodesize,
Tufanoetal.providetwodatasets:BFP smallandBFP medium,with
theformerhavingacodelengthbelow50andthelatterhavinga
code length between 50 and 100.
Baselines. The original method proposed by the dataset cre-
ator [53] is marked as Tufano et al. S2S+COPYSAPN [ 41]i sa n
extension of seq2seq models which can copy entire spans of the
input to the output in one step and reduce the number of decisions
requiredduringinference.CodeBERT,GraphCodeBERT,CugLM,
T5-learning and TreeBERT are also used as baselines.
Metrics. WereportAcc(=Acc@1)andBLEUonbothdatasets.
Bothğ‘šğ‘andğ‘šğ‘“areabstracted(pleasereferto[ 53]formoredetails)
beforebeingfedtothemodel,andwedonottranslatetheabstractedcodepredictedbythemodelbackintosourcecodebeforeevaluation
using the metrics because the result before and after translation
are the same.
4.2.4 Code Translation. Code translation is important for migrat-
ing legacy code in one language into an ecosystem built in a differ-
ent language.
Datasets. Following Chen et al. [ 10] and Guo et al. [ 19], we
conduct our experiments on a dataset containing several open-
source projects, which have both a Java and a C# implementation.
Baselines. Baselines are Naive (i.e., directly copying the source
codeasthetranslationresult),vanillaTransformer[ 54],CodeBERT,
GraphCodeBERT, CugLM, T5-learning and TreeBERT.
Metrics. We use Acc and BLEU as metrics.
4.2.5 Code Search. Code search aims to find the code snippet that
mostcloselyresemblesthesemanticsofthegivennaturallanguagequerystatementfromasetofcodes,whichisnamed codebaseinthis
task.ItisagoodchoicefortestingtheperformanceofSPT-Codein
classification mode. For each code-query pair (ğ‘,ğ‘), we compute
the representation vectors of ğ‘andğ‘, i.e.,ğ‘‰ğ‘andğ‘‰ğ‘respectively.
Then, we randomly take another query statement from the dataset
asanegativesample,denotedas ğ‘âˆ’,whoserepresentationvectoris
ğ‘‰ğ‘âˆ’.Toensuretheeffectivenessofthenegativesamples,werestrict
the BLEU score [ 42] betweenğ‘âˆ’andğ‘to be lower than 0.3. The
training loss is:
Lğ‘ ğ‘’ğ‘ğ‘Ÿğ‘â„ =/summationdisplay.1
(ğ‘,ğ‘,ğ‘âˆ’)âˆˆğ·max(0,ğœ€âˆ’cos(ğ‘‰ğ‘,ğ‘‰ğ‘)+cos(ğ‘‰ğ‘,ğ‘‰ğ‘âˆ’))(5)
cos(ğ‘‰ğ‘,ğ‘‰ğ‘)=ğ‘‰ğ‘Â·ğ‘‰ğ‘
/bardblğ‘‰ğ‘/bardbl/bardblğ‘‰ğ‘/bardbl(6)
whereğ·is the dataset, ğœ€is a fixed value of 0.05 (following Gu et
al. [18]), andcosdenotes the calculation of cosine similarity. In the
evaluation, we first get the representation vectors of all codes in
codebase as candidates. Then, for each vector of query statements,
we select a few code representation vectors from the candidates
2012
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:53:04 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Changan Niu, Chuanyi Li, Vincent Ng, Jidong Ge, Liguo Huang, and Bin Luo
thatareclosesttothevectorofquerystatements.Finally,themetric
is calculated.
Datasets. We use CodeSearchNet [24].
Baselines. We use CNN [ 29] (i.e., 1D convolutional neural net-
work),Bi-GRU[ 11],andTransformer(i.e.,vanillaTransformer[ 54])
as baselines in addition to CodeBERT and GraphCodeBERT.
Metrics. We use Mean Reciprocal Rank (MRR) for evaluation.
MRR is a statistic measure for evaluating search algorithms. The
reciprocal rank of a query response is the multiplicative inverse of
the rank of the first correct answer: 1 for first place, 1/2 for second
place. Therefore, the mean reciprocal rank is the average of the
reciprocal ranks of results for a sample of queries Q:
MRR =1
|ğ‘„||ğ‘„|/summationdisplay.1
ğ‘–=11
rankğ‘–(7)
4.3 Evaluation
We evaluate SPT-Code by answering four research questions.
RQ1:HoweffectiveisSPT-Codecomparedwiththestate-of-the-art
baselinesandothercodepre-trainedmodelsonfivedownstreamtasks?
Weconductexperimentsonfivedownstreamtasksintroducedin
theprevioussubsection.TheresultsonclassicalandCodeSearchNet
codesummarizationareshowninTables3andTable4,respectively.
Table 3: Results on classical code summarization.
MethodsJCSD PCSD
B. M. R.L B. M. R.L
DeepCom 37.5 22.0 51.2 20.4 9.5 36.8
Rencos 36.9 26.5 51.2 28.9 20.4 45.0
NerualCodeSum 44.5 26.4 54.7 32.3 19.7 46.8SiT 45.3 27.3 55.0 33.8 20.7 48.2
CodeBERT 45.2 26.2 54.7 33.3 21.6 49.2GraphCodeBERT 46.0 26.5 56.5 33.9 22.1 50.4CugLM 46.1 26.3 55.8 34.3 21.6 49.7
T5-learning 44.2 26.5 53.9 34.1 22.6 50.1TreeBERT 47.9 27.2 56.6 34.7 23.0 50.5
SPT-Code 49.1 32.4 58.2 36.1 26.9 52.0
Tables 5-8 show the results of code completion, bug fixing, code
translation and code search, respectively.
Firstofall,wecanseethatwhetherthebaselinesarededicatedto
aspecificdownstreamtaskorpre-trainedmodels,SPT-Codeclearly
outperforms them in the vast majority of cases, including code
summarization, code completion, bug fixing on BFP medium, and
codetranslation.Thisisbecauseduringthepre-trainingprocess,
SPT-Code learns the representation of codes from a large amount
ofdata,aswellastheconnectionbetweencodesandstructuraland
naturallanguageinformation.In addition,themodelisenhanced
to generate code and natural language sequences by pre-training.
AsforbugfixingonBFP small,theaccuracyofSPT-Codeisslightly
lower than that of S2S+COPYSPAN, which achieves the best ac-curacy on BFP
small. The reason we believe is two-fold. The first
is the dataset, where the lengths of the code are up to 50, which
allowsRNN-basedS2S+COPYSPANtoadequatelycopewithinputs
oftheselengths.Anotherreasonisthatthemechanismproposedby S2S+COPYSPAN to copy the entire span of the input is wellsuited for a task like bug fixing, where only individual modifica-
tionsaremadeontheinput.However,inthenextRQ,wewillshow
that SPT-Code still outperforms S2S+COPYSPAN on BFP smallafter
removing MNG from the pre-training tasks.
Finally, we can learn that SPT-Code performs comparably to
GraphCodeBERT on code search, which suggests that although we
designedourmodeltoperformbetterongenerationtasks,itdoes
not come at the cost of the performance on classification tasks.
RQ2: How do the three pre-training tasks, as well as the AST and
the natural language input contribute to the modelâ€™s performance on
the different downstream tasks? In order to find the impact of each
component to the overall performance of SPT-Code, we conductablation study on all downstream tasks. We remove each/all pre-
trainingtask,andASTor/andnaturallanguagepartfromtheinput,
respectively.Owingtospacelimitations,forcodesummarization,
we only show results on Java and Python in CodeSearchNet12; and
forcodesearch,weonlyshowresultsonJavaandGo13.Theresults
are shown in the first three groups of Table 9.
Thefirstthingwenoticeisthatwhenwetrainfromscratch(i.e.,
remove all pre-training tasks) or remove the AST and natural lan-
guagefromtheinput(i.e.,onlyinputcodetokens),theperformance
of SPT-Code consistently drops considerably. Second, when we
removeeachofthethreepre-trainingtasks,theresultsdecreased
in most cases, particularly in code summarization, code translation
andcodesearch.Ofthese,CAPismostusefulforcodesearch,MASS
is most helpful for code translation, and removing MNG has the
greatest impact on code summarization.
Interestingly,wefindthatforcodecompletionandbugfixing,
SPT-Codeâ€™s performance w.r.t. accuracy improves instead whenMNGisremoved.Thisisunderstandable.Ontheonehand,code
completionandbugfixingaretaskswheretheinputandtheout-
putarebothcode,asisMASS,whiletheoutputoftheMNGtask
is natural language, and thus the ability to generate natural lan-guage trained by MNG is not fully reflected in these two tasks.
On the other hand, regarding code completion, MASS can be seen
astotallyunrestrictedany-codecompletion.Bothpredictapiece
ofcodebasedonitscontext,withthedifferencethatinany-code
completion,thepieceofcodeisrestrictedtobeanentireexpression,
while in MASS, the piece of code is selected completely at random.
Therefore, fine-tuning code completion directly after MASS, i.e.,
removing the MNG, yields a higher result.
WhenweremoveeithertheASTorthenaturallanguagefrom
theinput,theresultsofthemodeldrop,indicatingthattheyboth
help improve performance. In addition, for code summarization,
bugfixingandcodesearch,theresultsarelowerwhenonlynatural
languageisremoved,whichindicatesthatincomparison,natural
languagehelpsthesethreetasksmorethanASTsdo.Onthecon-
trary,ASTismorehelpfulforcodecompletionandcodetranslation.
Through ablation, we find that different pre-training tasks show
different degrees of influence on the downstream tasks, As a re-
sult, appropriate trade-offs of pre-training tasks for different down-
stream tasks can help the model achieve better performance. ASTs
12Java and Python represent static and dynamic languages, respectively.
13The results for both are above and below GraphCodeBERT, respectively.
2013
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:53:04 UTC from IEEE Xplore.  Restrictions apply. SPT-Code: Sequence-to-Sequence Pre-Training for Learning Source Code Representations ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table 4: Results on CodeSearchNet code summarization.
MethodsJava Python JavaScript PHP Go Ruby
B. M. R.L B. M. R.L B. M. R.L B. M. R.L B. M. R.L B. M. R.L
NeuralCodeSum 10.7 12.9 25.9 9.4 4.9 17.0 --------- - --
CodeBERT 13.6 16.4 32.2 10.7 6.0 21.0 10.0 10.5 22.2 20.1 19.3 42.7 16.0 11.7 30.4 8.3 7.5 17.2
GraphCodeBERT 14.5 17.8 33.5 11.0 7.4 22.1 10.9 11.4 24.9 20.0 20.2 42.8 16.5 12.5 30.1 8.3 8.1 18.0CugLM 13.2 16.1 31.7 11.4 7.5 22.0 10.0 10.4 21.8 17.8 18.2 41.9 17.0 12.7 29.7 7.4 6.8 15.3
T5-learning 13.5 16.8 34.2 9.7 5.5 16.3 9.0 9.2 19.6 18.4 17.9 35.6 15.3 11.0 25.9 7.8 6.7 15.5TreeBERT 13.8 17.1 34.3 11.1 7.2 21.6 10.3 10.5 22.0 18.0 19.1 42.4 17.1 12.8 30.1 7.4 6.9 15.5
SPT-Code 16.8 20.6 36.3 12.8 14.2 27.1 12.8 17.2 28.8 20.4 20.6 43.7 18.8 16.6 38.1 8.4 11.1 20.0
Table 5: Results on any-code completion.
Methods Acc@1 Acc@5
code2seq 10.44 15.47
Transformer w/ copy 16.68 24.12seq2tree w/ copy 16.46 22.89SLM 18.00 24.77
CodeBERT 17.23 24.79
GraphCodeBERT 18.21 25.00CugLM 18.43 25.51
T5-learning 16.03 23.78
TreeBERT 17.17 24.73
SPT-Code 19.09 26.57
Table 6: Results on bug fixing.
MethodsBFPsmall BFPmedium
Acc BLEU Acc BLEU
Tufano et al. 9.27 - 3.21 -S2S+COPYSPAN 17.6 - 7.9 -
CodeBERT 6.23 61.47 2.43 68.54GraphCodeBERT 7.49 63.40 3.16 67.33CugLM 14.55 67.24 7.84 71.23
T5-learning 12.01 65.02 4.51 60.89TreeBERT 13.15 65.74 7.61 70.67
SPT-Code 17.54 75.10 10.86 87.88
Table 7: Results on code translation.
MethodsJavaâ†’C# C# â†’Java
Acc BLEU Acc BLEU
Naive 00.0 18.52 00.0 18.66
Transformer 33.4 56.05 38.8 50.84
CodeBERT 58.8 79.34 57.3 71.10GraphCodeBERT 59.3 80.33 58.1 72.39CugLM 61.6 82.72 59.5 74.89
T5-learning 54.1 81.35 48.4 77.10TreeBERT 60.0 80.92 57.8 77.15
SPT-Code 64.07 90.34 60.29 86.10
and natural language both have a positive impact on the perfor-
mance of the model, regardless of the downstream task.Table 8: Results on code search.
Methods Java Python JS PHP Go Ruby
CNN 0.262 0.241 0.224 0.261 0.676 0.274
Bi-GRU 0.312 0.298 0.195 0.340 0.691 0.214
Transformer 0.404 0.399 0.289 0.430 0.729 0.278
CodeBERT 0.673 0.670 0.618 0.624 0.879 0.674GraphCodeBERT 0.690 0.692 0.6430.6470.896 0.701
SPT-Code 0.700 0.699 0.6410.6510.8950.701
RQ3: Is the ability of utilizing more unlabeled data an advantage
ofSPT-Code? Asweknow,CodeBERT,GraphCodeBERTandT5-
learningarepre-trainedonasubsetofCodeSearchNet,whereaswe
can pre-train on the entire dataset. Therefore there is a question of
whethertheabilitytoutilizemoredataforpre-trainingalsogives
SPT-Code an unfair advantage. To ensure a fair comparison, we
therefore pre-train SPT-Code only on the same data as CodeBERT
andGraphCodeBERT(i.e.,CSN w/ doc),andalsoonthesamedata
asT5-learning(i.e.,CSN Java).Notice,however,thathereonlythe
codefromthelabeleddataisutilizedandnotthelabels,evenonthe
CSNw/ docwhere all samples have labels. So in this case, although
we are using the data set of the same size, it actually utilize less
informationthantheydo.Theresultsaredisplayedinthelastgroup
of Table 9.
Comparing the results of SPT-Code with those of â€œCSN w/ docâ€
and â€œCSN Javaâ€ in Table 9, we find that there is a significant perfor-
mancedecreasewhenthedatausedforpre-trainingisshrunkfrom
6.4M to 2.3M or 1.5M. However, considering the other pre-training
models,itstillmaintainsanadvantageorisatacomparablelevel
inperformance.Therefore,wecanconcludethatSPT-Codeissu-
perior given the same amount of pre-trained data. In addition, the
ability to use more unlabeled pre-training data can help SPT-Code
achievehigherperformance.ThisensuresthatSPT-Codehasavery
advantageous scalability at the data level compared to the other
pre-training models for source code.
RQ4: How would the size of the training data for fine-tuning SPT-Code influence its performance on downstream tasks? To answer
this question, we plot learning curves by varying the amount of
(task-specific)training datausedtofine-tune SPT-Code. Owingto
space limitations, we only report results of code summarization on
Java in CodeSearchNet, and bug fixing on BFP medium. We select
ğ‘˜/10datafromthetrainingseteachtimefortraining,andthentest
on the same entire testing set. The results are shown in Figure 5.
2014
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:53:04 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Changan Niu, Chuanyi Li, Vincent Ng, Jidong Ge, Liguo Huang, and Bin Luo
Table 9: Ablation study on downstream tasks.
MethodsSummarization Completion Bugfixing Translation Search
Java PythonAcc@1 Acc@5BFPsmall BFPmedium Javaâ†’C#C#â†’JavaJavaGoB.M. R.L B. M. R.L Acc BLEU Acc BLEU Acc BLEU Acc BLEU
SPT-Code 16.79 20.55 36.34 12.77 14.16 27.10 19.09 26.57 17.5475.1010.8687.7764.0790.34 60.29 86.10 0.7000.895
-w/oCAP 15.85 20.33 36.30 12.26 14.16 26.57 18.42 25.6317.2073.54 10.06 87.26 57.2588.80 54.40 84.66 0.6680.872
-w/o MASS 15.75 20.19 35.70 12.16 13.48 26.00 17.42 23.7116.3072.71 9.65 86.10 53.9382.09 50.55 82.32 0.6860.889
-w/o MNG 15.45 19.86 35.65 12.01 13.81 25.98 19.71 26.1018.1173.0911.9687.7958.8088.79 57.16 85.19 0.6730.875
-w/o all 13.54 17.72 33.31 11.90 12.74 25.23 13.49 19.3114.2872.89 8.27 85.30 49.1979.28 47.49 78.69 0.6560.855
-w/oAST 16.43 20.14 36.19 11.87 13.80 25.74 18.53 25.7816.3572.85 10.61 87.52 55.8187.79 56.17 82.66 0.6920.885
-w/o NL 15.68 20.15 35.86 11.96 13.56 25.55 18.47 26.0916.1272.70 10.50 87.57 56.2987.83 56.92 83.56 0.6790.865
-only code 15.35 19.97 35.63 10.25 12.74 23.90 18.33 25.8816.0172.55 10.19 86.96 54.1387.34 54.59 83.81 0.6690.864
-CSNw/doc15.41 19.78 35.74 12.59 13.56 26.49 17.89 23.8016.1474.02 10.41 87.58 60.5388.78 58.28 84.09 0.6730.878
-CSNJava15.67 19.72 35.22 12.40 12.60 25.24 17.76 23.5416.1074.51 10.18 86.96 59.4386.55 58.11 83.07 0.6780.866
1 2 3 4 5 6 7 8 9 10
k14161820BLEU / METEOR
13.5714.11 14.2215.09 15.1615.64 15.65 15.62 15.6816.7918.59 18.6219.0219.56 19.52 19.67 19.5720.25 20.1820.55
30.002222141 32.0034.0036.0019 021 38.005555002022 40.00
ROUGE-L33.6134.61 34.7135.62 35.6636.15 36.2235.9936.24 36.34
BLEU
METEOR
ROUGE-L
(a) Results on Java CodeSearchNet code summarization.
1 2 3 4 5 6 7 8 9 10
k681012Accuracy
5.315.656.447.198.09 8.049.77 9.74 9.8610.86
86.599117.7 87.087.588.0
BLEU
86.91
86.7386.9787.07 87.14 87.0187.6287.7687.90 87.88
Accuracy
BLEU
(b) Results on BFP mediumbug fixing.
Figure 5: Results when ğ‘˜goes from 1 to 10.
Itiseasytoseethattheperformanceshowsadecreasingtrendin
allevaluationmetricsas ğ‘˜decreases(i.e.,thesizeofthetrainingset
decreases).Itprovesthatevenforpre-trainedmodels,thesizeofthe
trainingsetplaysakeyroleintheperformanceaswell.Meanwhile,consideringTable4,weseethatforcodesummarization,theresultsofSPT-CodearecomparabletothatofCodeBERTwhenthetrainingsetsizeisreducedto1/10,andtheperformanceissimilartooreven
higherthanGraphCodeBERTwhenitisreducedto2/10.Further,
recallingTable6,wecanseethatwhenthetrainingsetofBFP medium
isreducedto1/10oftheoriginalsize,SPT-Codeâ€™sresultsarestill
higher than other pre-trained models, and when it is reduced tohalf of the original size, SPT-Codeâ€™s performance can reach the
performance of the best baseline model, i.e., S2S+COPYSPAN.
The conclusion is that although there is an inevitable drop in
SPT-Codeâ€™sperformanceasthesizeofthetrainingdatadecreasesduring fine-tuning, the performance of SPT-Code is comparableto that of the other models when the data used for fine-tuningSPT-Code is reduced to a very small size. This also implies the
robustness of SPT-Code.
4.4 Quantitative Analysis
WecollecttheoutputofSPT-Codeandbaselinesforawiderangeof
test samples on each downstream task. Then we invited five partic-
ipants, all of whom are graduate students in software engineering
whoarethemselvesnotauthorsofthispaper.Foreachdownstream
task,thefivestudentsarerandomlyassignedtothesamenumberofsamples.Iftherearemultipledatasetsforthatdownstreamtask,the
samplesofmultipledatasetsarethesame.Fordownstreamtasks
that use Acc as a metric, such as code completion, we focus on the
similaritybetweenincorrectandcorrectanswers,andtheextent
towhichtheincorrectanswer canbehelpfulwhenallmodelsfail.
The results are shown in Table 10.
Table 10: Results of quantitative analysis.
Dowstream Tasks # Sample # Better # Comparable # Worse
Code Summarization 800 328 368 104
Code Completion 600 312 233 55
Bug Fixing 200 89 75 36
Code Translation 200 83 65 52
Code Search 300 61 185 54
4.5 Qualitative Analysis
Byaskingtheparticipantsintheprevioussubsectionandbrowsingtheoutputofeachmodelbyourselves,wefoundthatincomparison,SPT-Codecancapturethesemanticinformationofidentifierswithin
code more accurately than other pre-trained models, and it can
capture the semantic information of code segment globally instead
of limiting to a certain region. The extracted code semantics are
relatively more widely and evenly distributed in the code segment.
Inthecaseofcodesummarization,forexample,SPT-Codepro-
vides more complete and accurate descriptions of the methodâ€™s
overall functionality. Table 11shows summaries generatedby dif-
ferent models for an example Java method in CodeSearchNet data.
Wefindthattheâ€œqueueâ€objectisrecognizedbyallofthesemodels.
CodeBERTandT5-learningfailtocapturetheoperationconducted
2015
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:53:04 UTC from IEEE Xplore.  Restrictions apply. SPT-Code: Sequence-to-Sequence Pre-Training for Learning Source Code Representations ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table 11: Qualitative example of SPT-Code and baselines.
void emitLoop() {
for (;;){
AppendOnlyLinkedArrayList<Object> q;
q = queue;
if(q == null){
emitting = false;
return;}
queue = null;
q.forEachWhile(this) ;}}
CodeBERT: the queue is being destroyed
GraphCodeBERT: emits the next loop in the queueT5-learning: this method is called when the queue
SPT-Code: emits all of the elements in this queue
-w/o AST: emit the linked list-w/o NL: emits all entries in the queue
Human Written: loops until all notifications in the queue has been processed
onthequeue,i.e.,â€œemits/emitâ€,butGraphCodeBERTandSPT-Code
does.ComparedtoSPT-Code,althoughGraphCodeBERTalsoun-
derstand the relationship between the operation and the object, it
fails to choose appropriate words for describing the operation. The
reasonmaybethatthedataflowcanindeedpartially(butnotfully)
capturecodestructureinformation.Anotherinterestingobserva-
tion that may support this inference is that all models considering
structure information generate â€œin the queueâ€ correctly, i.e., Graph-
CodeBERT, SPT-Code, and SPT-Code-w/o NL, and models using
AST all generate accurate, readable and smooth summaries. It also
implies that data flow is less effectiveness than AST.
5 THREATS TO VALIDITY
Construct Validity. Like many existing code pre-training models,
SPT-Code uses CodeSearchNet for pre-training. Since CodeSearch-
Net is also used in the evaluation of code summarization and code
search, it is possible that the samples from the test sets for these
twotaskshavealready beenusedforpre-training.Thisisnotfair
to methods that have not been pre-trained with CodeSearchNet,
such as NeuralCodeSum in CodeSearchNet code summarization.
We also recognize the impact on the results of not removing dupli-
cates. However, our decision of not removing replicates is based
on two considerations: (1) ensuring fairness of the comparison: the
three code pre-training baselines were pre-trained with all data
fromCodeSearchNetwithoutduplicateremoval;(2)SPT-codeisnotaffected by duplicate data: on one hand, downstream tasks that use
pre-training dataset are CSN code summarization and code search,
while the pre-training tasks designed for SPT-Code do not use the
docstring, i.e., in testing, SPT-Code does not have the advantage of
generating more accurate summaries because it has seen a piece of
the tested code during pre-training, or has the advantage of better
searchingforcodebasedonnaturallanguage.Ontheotherhand,
other downstream tasks that do not use CodeSearchNet have little
overlap with CSN.
Forrigorousconsideration,weremovealltestsetsinCodeSearch-
Netandcodeduplicatedwithtestsetsofotherdownstreamtasks14.
Thenre-pre-trainandfine-tunetheSPT-Codeonthreedownstream
tasks, i.e., JCSD, code completion and BFPmediumbug fixing. It is
14By using tools provided by Allamanis [ 3], we find 9 in classical code summarization,
2i nB F P mediumand code completion, respectively.found that the results of SPT-Code decrease very little after remov-
ing these duplicates15. Moreover, it is still not sure whether the
change in results is due to the smaller pre-training dataset (shrunk
by about 1/10) or the removal of duplicates.
InternalValidity. Itiswidelyagreedthathyperparametershavea
significantimpactontheperformanceofdeeplearningmodels,but
hyperparameters of SPT-Code are not tuned experimentally and
are set empirically. Therefore, other hyperparameter settings may
yield better results.
External Validity. Another threat posed by using CodeSearchNet as
ourpre-trainingdatasetisthatCodeSearchNetdataisnotbalancedacrosssixprogramminglanguages,whichcanbeseeninTable2,so
ourmodelmaynotperformthesameondifferentlanguages,and
we cannot guarantee the validity of SPT-Code for programming
languages other than these six.
6 CONCLUSION
We presented SPT-Code, a large model for source code based on an
encoder-decoder architecture. First, we design three code-specific
pre-trainingtaskstopre-trainSPT-Code.Secondly,weproposea
newinputrepresentationwhoseisthefirstmethodthattakeinto
account both natural language and AST form of code, where wealsoproposeaimprovedversionoftheASTtraversalmethod,X-SBT. Both our pre-training tasks and input representation allow
SPT-Codetobepre-trainedonacompletelyunlabeleddataset.SPT-
Codewasthenfine-tunedonfivecode-relateddownstreamtasks.
Results indicate that fine-tuning SPT-Code enables it to achieve
the state-of-the-art performance on five code-related downstream
tasks.Ablationexperimentsrevealthatthethreepre-trainingtasks
have different degrees of impact on different downstream tasks,and AST and natural language input also helped improve SPT-
Codeâ€™sperformance.Tofacilitatefutureresearch,wealsomakeour
code and other artifacts publicly available at https://github.com/
NougatCA/SPT-Code.
ACKNOWLEDGMENTS
This work is supported by National Natural Science Foundation of
China (61802167, 61802095), Natural Science Foundation of Jiangsu
Province,China(BK20201250),CooperationFundofHuawei-NJU
Creative Laboratory for the Next Programming, and NSF award
2034508.WethankAlibabaCloudforitshigh-efficientAIcomputing
service from EFlops Cluster. We also thank the reviewers for their
helpfulcomments.ChuanyiLiandJidongGearethecorresponding
authors.
REFERENCES
[1]RoeeAharoniandYoavGoldberg.2017. TowardsString-To-TreeNeuralMachine
Translation. In Proceedings of the 55th Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Papers). 132â€“140.
[2]WasiAhmad,SaikatChakraborty,BaishakhiRay,andKai-WeiChang.2020. A
Transformer-based Approach for Source Code Summarization. In Proceedings of
the58thAnnualMeetingoftheAssociationforComputationalLinguistics.4998â€“
5007.
[3]Miltiadis Allamanis. 2019. The Adverse Effects of Code Duplication in Machine
Learning Models of Code. In Proceedings of the 2019 ACM SIGPLAN International
15Results decrease0.01 BLEU onJCSD, 0.03 Acc@1on code completionand 0.07 Acc
on BFPmedium
2016
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:53:04 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Changan Niu, Chuanyi Li, Vincent Ng, Jidong Ge, Liguo Huang, and Bin Luo
SymposiumonNewIdeas,NewParadigms,andReflectionsonProgrammingand
Software. 143â€“153.
[4]UriAlon,ShakedBrody,OmerLevy,andEranYahav.2018. code2seq:Generating
Sequences from Structured Representations of Code. In International Conference
on Learning Representations.
[5]UriAlon,RoySadaka,OmerLevy,andEranYahav.2020. StructuralLanguage
ModelsofCode.In InternationalConferenceonMachineLearning.PMLR,245â€“256.
[6]Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019. code2vec: Learn-
ingDistributedRepresentationsofCode. ProceedingsoftheACMonProgramming
Languages 3, POPL (2019), 1â€“29.
[7]SatanjeevBanerjeeandAlonLavie.2005. METEOR:AnAutomaticMetricforMT
EvaluationwithImprovedCorrelationwithHumanJudgments.In Proceedings
of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine
Translation and/or Summarization. 65â€“72.
[8]Luca Buratti, Saurabh Pujar, Mihaela Bornea, Scott McCarley, Yunhui Zheng,
GaetanoRossiello,AlessandroMorari,JimLaredo,VeronikaThost,YufanZhuang,
andGiacomoDomeniconi.2020. ExploringSoftwareNaturalnessthroughNeural
Language Models. arXiv:2006.12641 [cs.CL]
[9]Chi Chen, Xin Peng, Zhenchang Xing, Jun Sun, Xin Wang, Yifan Zhao, and
Wenyun Zhao. 2021. Holistic Combination of Structural and Textual Code Infor-
mationforContextbasedAPIRecommendation. IEEETransactionsonSoftware
Engineering (2021).
[10] XinyunChen,ChangLiu,andDawnSong.2018. Tree-to-treeNeuralNetworks
forProgramTranslation.In AdvancesinNeuralInformationProcessingSystems,
Vol. 31. Curran Associates, Inc.
[11]Kyunghyun Cho, Bart van MerriÃ«nboer, Caglar Gulcehre, Dzmitry Bahdanau,Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning Phrase
RepresentationsusingRNNEncoderâ€“DecoderforStatisticalMachineTranslation.
InProceedings of the 2014 Conference on Empirical Methods in Natural Language
Processing (EMNLP). 1724â€“1734.
[12]Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. 2019.
ELECTRA:Pre-trainingTextEncodersasDiscriminatorsRatherThanGenerators.
InInternational Conference on Learning Representations.
[13]JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019. BERT:Pre-trainingofDeepBidirectionalTransformersforLanguageUnderstanding.InProceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,Volume1(Longand
Short Papers). 4171â€“4186.
[14]JianboDong,ZhengCao,TaoZhang,JianxiYe,ShaochuangWang,FeiFeng,Li
Zhao,XiaoyongLiu,LiuyihanSong,LiweiPeng,etal .2020. Eflops:Algorithm
andsystemco-designforahighperformancedistributedtrainingplatform.In
2020IEEEInternationalSymposiumonHighPerformanceComputerArchitecture
(HPCA). IEEE, 610â€“622.
[15] Jianbo Dong,Shaochuang Wang,Fei Feng,Zheng Cao,Heng Pan,Lingbo Tang,
PengchengLi,HaoLi,QianyuanRan,YiqunGuo,etal .2021. ACCL:Architecting
HighlyScalableDistributedTrainingSystemswithHighly-EfficientCollective
Communication Library. IEEE Micro (2021).
[16]Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al .2020. CodeBERT: A Pre-
Trained Model for Programming and Natural Languages.In Proceedings of the
2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing:Findings.
1536â€“1547.
[17]JiataoGu,ZhengdongLu,HangLi,andVictorOKLi.2016.IncorporatingCopyingMechanisminSequence-to-SequenceLearning.In Proceedingsofthe54thAnnual
Meeting ofthe Associationfor Computational Linguistics(Volume1: Long Papers).
1631â€“1640.
[18]XiaodongGu,HongyuZhang,andSunghunKim.2018. DeepCodeSearch.In2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE).
IEEE, 933â€“944.
[19]Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, LIU Shujie, Long
Zhou,NanDuan,AlexeySvyatkovskiy,ShengyuFu,etal .2021. GraphCodeBERT:
Pre-trainingCodeRepresentationswithDataFlow.In InternationalConference
on Learning Representations, ICLR 2021.
[20]XingHu,GeLi,XinXia,DavidLo,andZhiJin.2018. DeepCodeCommentGener-ation.In2018IEEE/ACM26thInternationalConferenceonProgramComprehension
(ICPC). IEEE, 200â€“20010.
[21]Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2020. Deep Code Comment
Generation with Hybrid Lexical and Syntactical Information. Empirical Software
Engineering 25, 3 (2020), 2179â€“2217.
[22]Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi Jin. 2018. Summariz-ing Source Code with Transferred API Knowledge. In Proceedings of the 27th
International Joint Conference on Artificial Intelligence, IJCAI 2018. 2269â€“2275.
[23]XuanHuo,MingLi,andZhi-HuaZhou.2020. ControlFlowGraphEmbedding
BasedonMulti-InstanceDecompositionforBugLocalization.In Proceedingsof
the AAAI Conference on Artificial Intelligence, Vol. 34. 4223â€“4230.
[24]Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc
Brockschmidt.2020. CodeSearchNetChallenge:EvaluatingtheStateofSemantic
Code Search. arXiv:1909.09436 [cs.LG][25]Xue Jiang, Zhuoran Zheng, Chen Lyu, Liang Li, and Lei Lyu. 2021. TreeBERT: A
tree-basedpre-trainedmodelforprogramminglanguage.In Proceedingsofthe
Thirty-SeventhConferenceonUncertaintyinArtificialIntelligence,Vol.161.PMLR,
54â€“63.
[26]Magne Jorgensen and Martin Shepperd. 2007. A Systematic Review of Software
DevelopmentCostEstimationStudies. IEEETransactionsonSoftwareEngineering
33, 1 (2007), 33â€“53. https://doi.org/10.1109/TSE.2007.256943
[27]Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. 2020.
Learning and Evaluating Contextual Embedding of Source Code. In Proceedings
of the 37th International Conference on Machine Learning (Proceedings of Machine
Learning Research, Vol. 119). PMLR, 5110â€“5121.
[28]Rafael Michael Karampatsis and Charles Sutton. 2020. SCELMo: Source Code
Embeddings from Language Models. arXiv:2004.13214 [cs.SE]
[29]Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification.
InProceedings of the 2014 Conference on Empirical Methods in Natural Language
Processing, EMNLP 2014. ACL, 1746â€“1751.
[30]Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander M
Rush. 2017. OpenNMT: Open-Source Toolkit for Neural Machine Translation. In
Proceedings of ACL 2017, System Demonstrations. 67â€“72.
[31]AlexanderLeClair,SakibHaque,LingfeiWu,andCollinMcMillan.2020.Improved
Code Summarization via a Graph Neural Network. In Proceedings of the 28th
International Conference on Program Comprehension. 184â€“195.
[32]Alexander LeClair, Siyuan Jiang, and Collin McMillan. 2019. A Neural ModelforGeneratingNaturalLanguageSummariesofProgramSubroutines.In 2019
IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEE,
795â€“806.
[33]AlexanderLeClairandCollinMcMillan.2019. RecommendationsforDatasets
for Source Code Summarization. In Proceedings of the 2019 Conference of the
North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short Papers). 3931â€“3937.
[34]Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, AbdelrahmanMohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART:
Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,
Translation,andComprehension.In Proceedingsofthe58thAnnualMeetingof
the Association for Computational Linguistics. 7871â€“7880.
[35]ChenLin,ZhichaoOuyang,JunqingZhuang,JianqiangChen,HuiLi,andRongxin
Wu.2021. ImprovingCodeSummarizationwithBlock-wiseAbstractSyntaxTree
Splitting. In 29th IEEE/ACM International Conference on Program Comprehension,
ICPC 2021.
[36]Chin-Yew Lin and Eduard Hovy. 2002. Manual and Automatic Evaluation of
Summaries. In Proceedings of the ACL-02 Workshop on Automatic Summarization
- Volume 4. 45â€“51.
[37]Fang Liu, Ge Li, Yunfei Zhao, and Zhi Jin. 2020. Multi-task Learning basedPre-trained Language Model for Code Completion. In Proceedings of the 35th
IEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering.473â€“485.
[38]IlyaLoshchilovandFrankHutter.2019. DecoupledWeightDecayRegularization.
InInternational Conference on Learning Representations.
[39]Antonio Mastropaolo, Simone Scalabrino,Nathan Cooper, David Nader Palacio,
Denys Poshyvanyk, Rocco Oliveto, and Gabriele Bavota. 2021. Studying the
usage of text-to-text transfer transformer to support code-related tasks. In 2021
IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE,
336â€“347.
[40]Antonio Valerio Miceli-Barone and Rico Sennrich. 2017. A Parallel Corpus ofPythonFunctionsandDocumentationStringsforAutomatedCodeDocumen-tation and Code Generation. In Proceedings of the Eighth International Joint
Conference on Natural Language Processing (Volume 2: Short Papers). 314â€“319.
[41]Sheena Panthaplackel, Miltiadis Allamanis, and Marc Brockschmidt. 2021. Copy
That!EditingSequencesbyCopyingSpans.In ProceedingsoftheAAAIConference
on Artificial Intelligence, Vol. 35. 13622â€“13630.
[42]Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a
Method forAutomatic Evaluation ofMachine Translation.In Proceedings ofthe
40th Annual Meeting of the Association for Computational Linguistics. 311â€“318.
[43]Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,
KentonLee,andLukeZettlemoyer.2018. DeepContextualizedWordRepresenta-
tions. InProceedings of the 2018 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, Volume
1 (Long Papers). 2227â€“2237.
[44]Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever. 2019. Language Models are Unsupervised Multitask Learners. (2019).
[45]Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the Limits
ofTransferLearningwithaUnifiedText-to-TextTransformer. JournalofMachine
Learning Research 21, 140 (2020), 1â€“67.
[46]Abigail See, Peter J Liu, and Christopher D Manning. 2017. Get To The Point:Summarization with Pointer-Generator Networks. In Proceedings of the 55th
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers). 1073â€“1083.
2017
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:53:04 UTC from IEEE Xplore.  Restrictions apply. SPT-Code: Sequence-to-Sequence Pre-Training for Learning Source Code Representations ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
[47]Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine
TranslationofRareWordswithSubwordUnits.In Proceedingsofthe54thAnnual
Meeting ofthe Associationfor Computational Linguistics(Volume1: Long Papers).
1715â€“1725.
[48]Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-Attention with
Relative Position Representations. In Proceedings of the 2018 Conference of the
North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 2 (Short Papers). 464â€“468.
[49]KaitaoSong,XuTan,TaoQin,JianfengLu,andTie-YanLiu.2019. MASS:Masked
Sequence to Sequence Pre-training for Language Generation. In International
Conference on Machine Learning. PMLR, 5926â€“5936.
[50]Giriprasad Sridhara, Emily Hill, Divya Muppaneni, Lori Pollock, and K Vijay-Shanker. 2010. Towards Automatically Generating Summary Comments for
Java Methods. In 25th IEEE/ACM International Conference on Automated Software
Engineering, ASE 2010. ACM, 43â€“52.
[51]AlexeySvyatkovskiy,ShaoKunDeng,ShengyuFu,andNeelSundaresan.2020.
Intellicode Compose: Code Generation Using Transformer. In Proceedings of
the 28th ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering. 1433â€“1443.
[52]AlexeySvyatkovskiy,YingZhao,ShengyuFu,andNeelSundaresan.2019. Pythia:
AI-assistedCodeCompletionSystem.In Proceedingsofthe25thACMSIGKDD
International Conference on Knowledge Discovery & Data Mining. 2727â€“2735.
[53]Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin
White, and Denys Poshyvanyk. 2019. An Empirical Study on Learning Bug-
FixingPatchesintheWildviaNeuralMachineTranslation. ACMTransactions
on Software Engineering and Methodology (TOSEM) 28, 4 (2019), 1â€“29.
[54]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is All
Y o uN eed .I nAdvances in Neural Information Processing Systems. 5998â€“6008.
[55]Yao Wan, Jingdong Shu, Yulei Sui, Guandong Xu, Zhou Zhao, Jian Wu, and
Philip S Yu. 2019. Multi-Modal Attention Network Learning for Semantic SourceCode Retrieval. In Proceedings of the 34th IEEE/ACM International Conference on
Automated Software Engineering. 13â€“25.
[56]Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and
PhilipSYu.2018. ImprovingAutomaticSourceCodeSummarizationviaDeep
Reinforcement Learning. In Proceedings of the 33rd ACM/IEEE International Con-
ference on Automated Software Engineering. 397â€“407.
[57]YanlinWangandHuiLi.2021. CodeCompletionbyModelingFlattenedAbstract
Syntax Trees as Graphs. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 35. 14015â€“14023.
[58]Hongqiu Wu, Hai Zhao, and Min Zhang. 2021. Code Summarization with
Structure-induced Transformer.In Findings of the Association for Computational
Linguistics: ACL-IJCNLP 2021. Association for Computational Linguistics, 1078â€“
1090.
[59] Rui Xie,WeiYe,JinanSun,andShikunZhang.2021. ExploitingMethodNames
to Improve Code Summarization: A Deliberation Multi-Task Learning Approach.
In29thIEEE/ACMInternationalConferenceonProgramComprehension,ICPC2021.
IEEE, 138â€“148.
[60]ZhilinYang,ZihangDai,YimingYang,JaimeCarbonell,RussRSalakhutdinov,and
Quoc V Le. 2019. XLNET: Generalized Autoregressive Pretraining for Language
Understanding. Advances in Neural Information Processing Systems 32 (2019).
[61]ZhenYang,JackyKeung,XiaoYu,XiaodongGu,ZhengyuanWei,XiaoxueMa,
and Miao Zhang. 2021. A Multi-Modal Transformer-based Code Summarization
Approach for Smart Contracts. In 29th IEEE/ACM International Conference on
Program Comprehension, ICPC 2021. IEEE.
[62]Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu. 2020.Retrieval-based Neural Source Code Summarization. In Proceedings of the
ACM/IEEE 42nd International Conference on Software Engineering. 1385â€“1397.
[63]JianZhang,XuWang,HongyuZhang,HailongSun,KaixuanWang,andXudongLiu.2019. ANovelNeuralSourceCodeRepresentationbasedonAbstractSyntax
Tree. In2019 IEEE/ACM 41st International Conference on Software Engineering
(ICSE). IEEE, 783â€“794.
2018
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:53:04 UTC from IEEE Xplore.  Restrictions apply. 