Towards Language-independent Brown Build DetectionDoriane OlewickiPolytechnique Montr√©alMontr√©al, Canadadoriane.olewicki@polymtl.caMathieu NayrollesUbisoft Montr√©alMontr√©al, Canadamathieu.nayrolles@ubisoft.comBram AdamsQueen‚Äôs UniversityKingston, Canadabram.adams@queensu.caABSTRACTIn principle, continuous integration (CI) practices allow modernsoftware organizations to build and test their products after eachcode change to detect quality issues as soon as possible. In reality,issues with the build scripts (e.g., missing dependencies) and/orthe presence of ‚Äú/f_laky tests‚Äù lead to build failures that essentiallyare false positives, not indicative of actual quality problems of thesource code. For our industrial partner, which is active in the videogame industry, such ‚Äúbrown builds‚Äù not only require multidisci-plinary teams to spend more e"ort interpreting or even re-runningthe build, leading to substantial redundant build activity, but alsoslows down the integration pipeline. Hence, this paper aims toprototype and evaluate approaches for early detection of brownbuild results based on textual similarity to build logs of prior brownbuilds. The approach is tested on 7 projects (6 closed-source fromour industrial collaborators and 1 open-source, Graphviz). We#ndthat our model manages to detect brown builds with a mean F1-score of 53% on the studied projects, which is three times more thanthe best baseline considered, and at least as good as human experts(but with less e"ort). Furthermore, we found that cross-project pre-diction can be used for a project‚Äôs onboarding phase, that a trainingset of 30-weeks works best, and that our retraining heuristics keepthe F1-score higher than the baseline, while retraining only every4-5 weeks.KEYWORDSBrown Build, Build automation, Continuous integration, Classi#ca-tion, Concept drift.ACM Reference Format:Doriane Olewicki, Mathieu Nayrolles, and Bram Adams. 2022. TowardsLanguage-independent Brown Build Detection. In44th International Confer-ence on Software Engineering (ICSE ‚Äô22), May 21‚Äì29, 2022, Pittsburgh, PA, USA.ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3510003.35101221 INTRODUCTIONProducing high-budget video games (‚ÄúAAA games‚Äù) takes a lot ofe"ort and organization. Modern AAA games are composed of tensof millions of lines of code, scattered across hundreds of thousandsof#les and tens of thousands of code changes created by hundreds ofdevelopers. Furthermore, modern AAA games‚Äô developers need toPermission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor pro#t or commercial advantage and that copies bear this notice and the full citationon the#rst page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior speci#c permission and/or afee. Request permissions from permissions@acm.org.ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA¬© 2022 Association for Computing Machinery.ACM ISBN 978-1-4503-9221-1/22/05. . . $15.00https://doi.org/10.1145/3510003.3510122manage additional complexities due to the multidisciplinary teams(i.e., artists, devs, physics experts, data scientists), very di"erentfrom code-only projects, and to be compatible and scale across amultitude of platforms (various consoles, PC and mobile devices).The combination of online imperatives and the multiplicity ofplatforms has made reliable Continuous Integration (CI) pipelinesparamount to producing AAA games with a limited number of bugs,if not bug-free. Whenever a code change is submitted for review,it is forwarded to the CI pipeline, which automates compilation,testing and other required activities (e.g., static analysis) [22]. If allthe steps are successful, the CI status isgreenand the change isintegrated into the project. Otherwise, it turnsredand the developerneeds to debug/#x the code change based on the CI‚Äôs build log.1Unfortunately, the CI build results are not always a reliableindication of a code change‚Äôs quality, since builds can fail becauseof factors related to the build process [16,17,38] (e.g., missingdependencies or network/disk access on the build machines), orbecause of/f_laky tests [19,30,33] (i.e., tests with non-deterministicoutcomes).We de#ne brown builds as a build failure that changes to a successon at least one build rerun without changing the build setup orsource code. For instance, a build could fail if the communicationbetween the CI pipeline and physical game consoles is interrupted.A test could be failing on an under-provisioned/overused CI jobworker (due to which operations are executed in a di"erent orderthan the test expects), but pass otherwise. Simply rerunning sucha ‚Äúbrown build‚Äù (on the same code change) could make the buildfailure disappear.In our experience at one of the world-leading AAA game produc-ers, we saw that brown builds hinder the con#dence of developersin their CI, and impact the productivity of developers and testersalike. Instead of immediately investigating the source code upon abuild failure, the potential presence of brown builds tends to pushdevelopers to manually trigger reruns of builds, just to be sure. Onsix large industrial projects analyzed in this paper, 31% (3.5k/11.4k)of commits had at least one manually rerun build job, and 15%(27k/179k) of build jobs were rerun at least once. This is not only awaste of hardware resources for CI, but also of developers‚Äô produc-tivity since they have to wait for the reruns to#nish, while the CIpipeline is blocked. While the rerun could still cost less time thanmanually checking the source code in vain, it adds to an alreadycongested CI pipeline and delays even further the manual testingprocess of games, since testers wait for a green build.Hence, there is a strong need for pragmatic approaches that candistinguish real build failures from brown builds. At a minimum,such approaches provide a second opinion that could con#rm de-velopers‚Äô suspicions about a build failure, restoring their trust in CI1In recent years, the term ‚ÄúCI‚Äù has started to refer to these pipelines instead of to theoriginal, agile practice. The rest of this paper will do the same.21772022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:57:30 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pi/t_tsburgh, PA, USAOlewicki et al.results. One would also expect the approaches to be integrated intothe CI pipeline, for example to automatically re-run a subset of thebrown builds, or to perform other automated resolution techniques,making better use of CI resources. While approaches have been pro-posed for/f_laky test detection, based on code dependencies [11,25],dynamic code analysis [18], or test-smell [10],/f_laky tests are onlypart of the problem, since brown builds can be due to the buildprocess itself, as mentioned before. Furthermore, industrial soft-ware projects feature a variety of programming languages and tools,making adoption in practice of existing/f_laky test models hard. Fi-nally, brown builds are an issue for both young and old projects,yet only the latter have su$cient historical build information tobuild models (the so-called ‚Äúcold-start‚Äù problem [26]).To address these shortcomings, this paper presents a language-independent approach to identify brown builds that leverages thebuild logs produced within the CI. We extract and#lter vocabularyfrom the build logs, transform the resulting words into a vector-based representation using TF-IDF [36], then train boosted tree-based classi#ers [8] to predict if a job is brown or not. We empiricallyevaluate the classi#ers on six industrial projects of a leading AAA-games developer and one open-source project.This paper addresses the following research questions:RQ1.Can we accurately detect brown builds in a language/project-agnostic way ?Our best models got an F1-score of 82%(precision of 76%, recall of 94%), with F1-score on par with experts‚Äôprediction (-4% to +17%), suggesting that our models are pragmatic.RQ2.Can a brown build prediction model be used on an-other project ?A model trained on a project can be used for anew project‚Äôs prediction during its onboarding phase, but a project-speci#c model should be used as soon as onboarding is over.RQ3.How long can a model stay relevant without beingretrained ?We found that we can schedule the retraining of themodel every 4-5 weeks depending on its performance evolution andage. Data older than 30 weeks does not signi#cantly improve theF1-score (<0.01%), and even harms the model for some projects.Our major contributions are a language-independent brownbuild detection approach with F1-score two to three times higherthan baseline models for 7 large projects (and on par with humanexperts), as well as the empirical evaluation of heuristics to counterthe impact of concept drift over time. A replication package isavailable online [3].2 BACKGROUND & RELATED WORKA Continuous Integration (CI) server [22] automatically rebuildsand retests the source code of a project whenever a developer pushesa code change to their version control system, in order to detectfaults and merge con/f_licts as soon as possible. A typical CI pipelinelike Jenkins or TravisCI consists of a sequence of build stages (e.g.,‚Äúcompile‚Äù followed by ‚Äútest‚Äù), each of which are composed of one ormore parallel build jobs (e.g., ‚Äúcompile‚Äù jobs on Linux and Windows).The behavior of such a build job is speci#ed via build scripts in adomain-speci#c language like GNU Make, Maven or Gradle. Suchscripts typically transform source code into an executable programby invoking con#guration tools, preprocessors, and compilers, theyautomate the execution of test harnesses and/or can even deploythe produced build artifacts [9, 22].While conceptually, a CI server is thought of as performing onebuild for each new code change, in practice its role is much morecomplex. First, the build dashboards of large open source organiza-tions like Mozilla‚Äôs treeherder [7] or OpenStack‚Äôs zuul [2] show amulti-dimensional matrix that tries to summarize results for dozensof CI pipelines and build jobs, ranging from classic compilationand unit test execution to deployment or even static analysis. Fur-thermore, each such pipeline is run multiple times for a given codechange, since, in each build stage, multiple jobs should be run tocover the major feature and environment con#gurations the codebase is expected to run on. If a project has 10 features and shouldsupport 5 operating systems, ideally 10 x 5 build jobs should bescheduled in each build stage. Since a typical project has a muchlarger number of features, and its environment comprises of notonly di"erent operating systems (versions), but also di"erent de-vices, processor models, library dependencies, supporting databasesand web servers, each code change potentially yields a combinato-rial explosion of build jobs to run. Of course, if a build is deemed tofail, all builds would need to be repeated for the proposed code#x.While this ‚Äúbuild in/f_lation‚Äù phenomenon [38] increases con#-dence in build results, it brings a number of major disadvantagesas well. First, it increases the build infrastructure (and energy) costfor organizations. Google, for instance, performs 800k builds perday, which schedule 150M test runs [32]. Google‚Äôs breakneck codevelocity of one commit per second coupled with its linearly in-creasing test corpus implies a quadratically growing need in buildresources [32]. Similar to many other organizations, including Open-Stack, they have moved to CI scheduling algorithms that groupmultiple code changes (e.g., all changes arriving within 45 minutes)before starting a new build on the entire group, instead of execut-ing separate builds for each code change. While successful buildsat the group-level leave out many builds at the individual level,failures at the group-level do require additional follow-up builds todetermine the individual code changes responsible for the failures.Furthermore, while interpretation of build failures is the biggestchallenge of CI users [20], the large number of builds generated bybuild in/f_lation makes build failures harder to interpret. For exam-ple, Gallaba et al.‚Äôs analysis of 3.7 million GitHub build jobs [16]found that 12% of passing CI builds contain failing or skipped buildjobs that the CI system was asked to ignore by developers, with 2out of 3 breakages occuring more than ones. Furthermore, 44% ofthe studied build failures were environment-dependent, i.e., onlyoccurred for some environments.Even worse than the presence of noise and build in/f_lation due todi"erent environments is the ambiguity of build results caused byso-called brown build jobs. These are build jobs that fail inconsis-tently due to issues with asynchronous calls, multithreading, or testorder dependencies [16,17,30]. Only by repeating such build jobsa su$ciently large number of times, we could determine for surewhether a build job really failed or succeeded. In the meantime,the code contribution pipeline conservatively would be locked, ba-sically preventing other teams to merge in their contributions. Ifone could predict that the build is truly brown, i.e., not a real buildfailure, the pipeline could remain open, and one could also avoidpropation of brown build results. This is because, in a typical orga-nization, di"erent software components are reused across di"erentlibraries and products, and the (apparent) success of a new build2178Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:57:30 UTC from IEEE Xplore.  Restrictions apply. Towards Language-independent Brown Build DetectionICSE ‚Äô22, May 21‚Äì29, 2022, Pi/t_tsburgh, PA, USAis considered as a ‚Äúgo‚Äù signal for dependent products to adopt thenew release of the component, potentially inheriting brown-ness.While certain build tools like Maven have support for rudimen-tary/f_laky test detection through build repetition, this is not e$cient,nor accurate. For this reason, existing work in this area [10,11,24,30,35] has focused on empirically understanding the causes of/f_laky tests, as well as ways to detect such tests for speci#c pro-gramming languages. Other reasons of brown builds than/f_lakytests are not considered, nor approaches that are independent ofprogramming language. Ironically, there are often resources avail-able to optimize and#x the build environment or other resourcescode responsible for/f_laky builds, yet those need to be briefed withconcrete starting points, which currently are unknown. A recentlanguage-independent approach, proposed by Lampel et al. [27],will be discussed in Section 4.2.3LANGUAGE-INDEPENDENT BROWN BUILDDETECTION APPROACHThis section will present each step of our methodology for language-independent brown build detection, as well as the research processaccording to which the approach was designed.3.1 Vocabulary extraction from log/f_ilesWe#rst extract the vocabulary from the log#le produced by eachbuild job. To reduce the dimensionality of this vocabulary, we ap-plied the following series of rules:Rule 1All URL and#le paths, identi#ed by a regular expres-sion, are replaced by a known string.Rule 2Commit IDs (series of characters containing at leastone letter and one number) are replaced by a known string.Rule 3Any non-letter characters are removed.Rule 4Camelcase notations are split.Rule 5English stop words [1] are removed.Rule 6A stemming algorithm [6] extracts the root of words.On average, these rules reduced the size of the log#les by 50%on the dataset studied. We then split the text into words and per-form n-gram extraction, since using n-grams (sequences ofùë¶words)convey more meaning [13]. In this study, we applied n-grams with/u1D441=1 and/u1D441=2 (see section 4.2.1 for the hyper-parameter tweak-ing). We discarded our experiments with/u1D441=3 and/u1D441=4, becausethey were computationally too expensive and did not signi#cantlyimprove the results. As output of this step, each build job is repre-sented as a dictionary of features (words or series of words) to thenumber of their occurrences in the analyzed log#le. The resultingdictionaries vary in size depending on the size and variety of thevocabulary in each#le. We also keep track of metadata surroundingeach CI build job such as the date on which it was submitted, thejob ID, the commit ID and the number of retries.3.2 VectorizationIn this step, we create a uniform representation of the data, whereeach build job is represented by a vector of relevant features. Thefeatures consist of the TF-IDF computation of textual build job data(words and series of words), and other build-related metrics.FiltValidFiltTrainsub1
UnionFeaturesBigTrainMatrixLast FTrainMatrixTestMatrixSub-setssplit and filtered by F(set)AllDataFiltTestsub2subNF_1F_2F_NValidMatrixSmall train setTF-IDF Sub-matricesFeature selectionUnion of selected featuresTF-IDF on selected featuresFeature selectionFinal TF-IDF MatricesM_1M_2M_NFigure 1: Iterative vectorization approaches.3.2.1 TF-IDF computation.We used TF-IDF to represent each tex-tual feature in order to reduce the impact of large log#les on thevocabulary word counts [36].3.2.2 KBest feature selection.Despite the#ltering applied on thelog, the number of unique (series of) words used in the vector rep-resentation of our data is still signi#cantly larger than the numberof observations (CI build log#les). Consequently, we used featureselection to further reduce the number of features used. For this, weused the SelectKBest feature selection from the sklearn package [5]in Python, with ANOVA F-value score function.Other algorithms considered for feature selection (i.e., infogain [14],correlation computations [12]) had similar results as KBest in termsof selected features, but were computationally taxing.3.2.3 Iterative computation.While KBest‚Äôs time and memory com-plexities are acceptable for most cases, our dataset‚Äôs dimensionalityand size made it impractical to apply to the entire data set at once.Consequently, we perform SelectKBest on subsets of our dataset,then the union of the best features of each subset is analyzed againas summarized in the middle of Figure 1.In particular, we split the training set into sub-matrices of 1,000build jobs, then perform TF-IDF and KBest on each sub-matrixindividually. Afterwards, the extracted sets of selected features/u1D439/u1D456are united into one large matrix. The KBest feature selectionalgorithm is#nally applied one last time on the resulting union set/u1D439/u1D462/u1D45B/u1D456, yielding the#nal matrix with selected features/u1D439/u1D453/u1D456 /u1D45B /u1D44E /u1D459.For the iterative approach to be acceptable, we need to validatethat the size of the union-ed matrix/u1D439/u1D462/u1D45B/u1D456is similar to the size ofthe individual/u1D439/u1D456sets (|/u1D439/u1D462/u1D45B/u1D456|/similarequal|/u1D439/u1D456|). We con#rmed that in ourcase study a sub-matrix size of 1,000 led to the size of the/u1D439/u1D462/u1D45B/u1D456setexceeding K by less than 10%, which we considered to be acceptable.3.2.4 Other metrics.Apart from the build log vocabulary, we alsoconsidered a number of other features related to the life cycle ofCI jobs and to the position of the build job in that cycle. First,we computed the number of prior reruns, fails, and successes foreach build job. Also, we compute the number of commits sincethe last brown job (#commit_since_brown), to control for temporalinformation about when brownness was found previously.3.3 Classi/f_icationAs shown in Figure 1, the dataset is split into training/test/validationsets. Vectorization and feature selection is done on the training2179Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:57:30 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pi/t_tsburgh, PA, USAOlewicki et al.OthermetricsShapMatrixFinalTraining/ValidationTF-IDFMatricesMODEL1xgboostxgboostMODEL2TrainExtractFigure 2: Two-step model training.MODEL1MODEL2A Build JobTF-IDF vectorOther metricsPRED1PRED2ShapvaluesFinal PREDvoteIn/OutExtractFigure 3: Prediction with two models. A vote is computedbetween both model‚Äôs prediction.set, then applied to the two other sets, such that the model is notcontaminated by either validation or test set. We also avoid set-contamination by gathering, for each commit, all its jobs in the sameset. The test set will be used for the model‚Äôs performance estimation,while the validation set is used to optimize the classi#cation model.Our models classify failed jobs, since successful builds do notblock the CI pipeline, and hence cannotbe brown. However, suc-cessful jobs might still bring information about brownness, andthus helps the model identify brown features to be used on failedjobs‚Äô predictions. We de#ned a#lter to be applied on the dataset,to identify from which set we#lter out all the successful jobs. The#lter application is shown in Figure 1, with the notation/u1D439(/u1D460/u1D452/u1D461).‚Ä¢None: no#lter applied.‚Ä¢Train:#lter applied to training set;‚Ä¢All:#lter applied to training and validation set.We use the XGBoost algorithm [8] (eXtreme Gradient Boosting)to predict the brownness of each build job, using a two-step trainingprocess (Figure 2). XGBoost is a directed classi#cation algorithmbased on random forest. Each prediction is a real value between0 and 1 (where 1=brown and 0=safe). The model also generatesso-called Shap values [4], which provide the impact of each featureon the training and validation sets‚Äô predictions.We opted for a two-step training process, since the resulting com-posite model allows to#rst focus only on data from the job‚Äôs buildlog, then to add the CI lifecycle-related information (Section 3.2.4).Such a two-step model has been used before[23]. The#rst step ofour training process only considers the vocabulary-related features,while the second one considers the other metrics and combinesthose with the Shap values of the#rst model. Then, we pass bothpredictions through a vote in order to have a#nal classi#cation ofthe build, before making the#nal classi#cation based on a threshold/u1D6FD. The vote is the weighted sum of both predictions, as follows:PredF=/u1D6FC100Pred1+100‚àí/u1D6FC100Pred2 (1)Classi#cation=/braceleftBigg/u1D435/u1D45F/u1D45C/u1D464ùë¶,if PredF>/u1D6FD,/u1D44E. /u1D452,otherwise(2)Both hyper-parameters/u1D6FCand/u1D6FDwere chosen experimentallyduring the validation phase and will be discussed in the section 5.1.3.4 Research ProcessThe brown build detection approach presented in this section wasdeveloped using a design-science process [34]. In particular, weperformed the following activities:inferring objectivesconsultations with several teams to es-tablish KPIs for model accuracy and concept drift;design and developmentexploring build features, then iter-ating over di"erent build log vectorization approaches on apilot project (Project A of Section 4.1), eventually adding asecond model to our approach (CI metrics);demonstrationof pilot to teams;evaluationon projects B-F and OSS (Section 4.1) to validategeneralizability.4 CASE STUDY SETUP4.1 Projects studied and data extractionWe gathered data from six large projects of our industrial part-ner and of one Open-Source project. These projects were di"erentin language, purpose (i.e., code analysis, cross-platform computa-tion, animation and path#nding), and size, in order to reduce thethreats to external validity of our approach. Table 1 shows the char-acteristics of the seven projects, with the closed-source projects‚Äônames elided for con#dentiality reasons, and project OS being theopen source project ‚ÄúGraphviz‚Äù2. We chose to work with Graphvizbecause it is a sizeable, multi-language open-source project withavailable (brown) build data.We extracted data about build jobs from the 2 CI/CD platformsused by the 7 projects: Gitlab3and TeamCity4, see Table 1. Usingthe REST APIs provided by both CI/CD platforms, we are able toextract the logs produced by each build job, which forms the inputof our approach (see Section 3.1).In order to obtain labeled data for our study, we leverage de-velopment guidelines adopted by the analyzed software projectsregarding brown build jobs. In the absence of prediction models,the developers and other contributors of the six industry projectshave to rerun a failed build job if it is suspected to be brown. Thisproportion is shown in the second column of Table 2. Out of thefailed build jobs that were rerun, those that changed build outcome(without any change to the build setup or source code) are consid-ered to be brown builds by our industrial partner, and hence bythis paper (third column of Table 2). Other build jobs are labeled astrue-result/safe.The Brown Failure Ratio/u1D435/u1D439/u1D445is the percentage of brown jobfailures over the total number of job failures, including the reruns.Table 1 shows that the projects vary in their BFR among the failedjobs, from 58% (E) and 35% (A, B) down to 13% (OS), 10% (F) and 5%(C, D). While Graphviz does not have an explicit policy to rerunsuspicious builds, we notice that its brown failure ratio (BFR) of13% is close to the median brownness of the other analyzed projects(15%), which suggests its oracle is representative.2Graphviz Gitlab link: https://gitlab.com/graphviz/graphviz3Gitlab: https://about.gitlab.com/4TeamCity: https://www.jetbrains.com/teamcity/2180Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:57:30 UTC from IEEE Xplore.  Restrictions apply. Towards Language-independent Brown Build DetectionICSE ‚Äô22, May 21‚Äì29, 2022, Pi/t_tsburgh, PA, USATable 1: Information on the projects studied.ProjFull project historyScraped data#Contri-butors#commitMain Lan-guagesAge[y]DevOpsPlat-form#months#jobs#failingjobsBrownfailureratio (/u1D435/u1D439/u1D445)Mean job du-ration[mm:ss]A153kGo, JS3.5Gitlab1823k300830%03:27¬±04B292kC#42963k810037%03:14¬±14C647kC++, C31722k59865%15:49¬±30D473kC++,C#1TeamCity388k87116%09:28¬±19E454kC++5253k131058%03:07¬±07F473kPy,JS,C#5,5229k170510%05:34¬±46OS2014kC,C++17Gitlab4347k123713%06:12¬±17Table 2: Information on the projects studied regarding the brownness labeling.Proj#failed_rerun/failed#brown/failed_rerun#reruns (only for brown cases)max #reruns0123+A34%76%1417506 (371)202 (163)122 (94)14B35%74%3360702 (464)268 (173)802 (680)29C8%44%5077294 (126)86 (36)45 (25)38D6%84%8032497 (414)48 (45)7 (4)9E65%86%31379 (62)191 (137)318 (309)24F18%49%1119153 (78)54 (21)39 (21)32OS13%82%984104 (88)26 (22)16 (10)124.2 Validation approach4.2.1 Model building.Hyper-parameter tweaking.In the methodology section 3, weidenti#ed a list of hyper-parameters to tweak. Those are gatheredin the following list, with a summary of their purpose and the rangeof values that were chosen.‚Ä¢/u1D439(/u1D460/u1D452/u1D461):#lters to apply on the sets.(Range:None(no#lter),Train(only fails in the training set)andAll(only fails in all sets))‚Ä¢/u1D441: Number Ngram to consider.(Range:[1],[2],[1,2])‚Ä¢/u1D43E: Number of features to be chosen by the feature selector.(Range: 100 to 300, by 25)‚Ä¢/u1D6FC: Weight of the#rst model‚Äôs prediction in the#nal predic-tion of Eq. (1).(Range: 0 to 100, by 10)‚Ä¢/u1D6FD: Threshold for the classi#cation, see Eq. (2).(Range: 10 to 90, by 10)For all studied projects, we trained models with all hyper-parametercombinations. We used cross-validation to validate the results onthe data set obtained in the previous subsection.Cross-validation settings.To do the cross-validation, all buildjobs were randomly given a group number from 0 to 9 in order toobtain 10 folds. The group separation respects the constraint thatall builds related to a given commit ID are in the same data setgroup. Furthermore, the cross-validation is strati#ed, conservingthe same proportion of brown jobs in each fold.Then, for each iteration/u1D456, the following sets are de#ned:‚Ä¢Train set: all folds but fold/u1D456(90% of dataset);Table 3: Baselines (BFRis brown failure ratio per project).BaselineProba.BrownSafenamebrownpredF1PreRecPreRecRandom5050%BFR1+2BFRBFR121‚àíBFR12RandomBBFR%BFR2BFRBFR1‚àíBFR1‚àíBFRAlwaysBrown100%BFR1+BFRBFR1NA0‚Ä¢Valid set: half of fold/u1D456(5% of dataset);‚Ä¢Test set: the other half of fold/u1D456(5% of dataset).Cross-validation is performed once for each hyper-parametercombination, with the train set used to vectorize (Section 3.2), thevalidation set used to optimize XGBoost‚Äôs internal parameters onthe trained models, and the test set to predict the model on anunseen data set. The validation/evaluation is done twice so thatboth halves of the subgroups are used once as a validation set, thentest set, resulting in 20 models being trained per cross-validation.Performance metrics.We use the commonly known precision(/u1D447/u1D443/u1D447/u1D443+/u1D439/u1D443), recall (/u1D447/u1D443/u1D447/u1D443+/u1D439/u1D443), F1-score (2/u1D45D/u1D45F/u1D452‚àí1+/u1D45F/u1D452/u1D450‚àí1) measures [15,21,28]to evaluate our models. We calculate precision and recall separatelyfor the ‚Äúbrown‚Äù and ‚Äúsafe‚Äù labels. These metrics are used to compareour models to the baselines in Table 3, as well as to determine the op-timal con#guration of hyper-parameters to use for our models. Wecomputed local and global optimizations of the hyper-parameters.The local optimization/u1D43F/u1D45C/u1D450/u1D442/u1D45Dis the hyper-parameter combination/u1D450that optimizes the F1-score of the prediction for each given project/u1D45D. As such, the optimal local hyper-parameter combination may bedi"erent for each project (/u1D43F/u1D45C/u1D450/u1D442/u1D45D/u1D45D=arg max/u1D450/u1D4391(/u1D450)). In contrast,the global optimization is the hyper-parameter combination/u1D450that2181Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:57:30 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pi/t_tsburgh, PA, USAOlewicki et al.minimizes the sum of the squared di"erences between the F1-scoreof a hyper-parameter combination and the local optima across allprojects (/u1D43A/u1D459/u1D45C/u1D44F/u1D442/u1D45D=arg min/u1D450/summationtext.1/u1D45D‚àà/u1D45D/u1D45F/u1D45C/u1D457(/u1D43F/u1D45C/u1D450/u1D442/u1D45D/u1D45D‚àí/u1D4391(/u1D450))2).Baselines.The baselines we use are based on random predictionmodels whose theoretical performance is calculated based on agiven percentage of predictions being brown or real failures, asshown in Table 3. Since the projects we studied are multi-languagein nature, it was di$cult to apply existing, language-dependentapproaches as baselines. For example, DeFlaker [11] covers/f_lakytest detection in Java. Furthermore, Pinto et al. [35] predict/f_laky testcases based on the tests‚Äô tokens, yet brown builds do not necessarilyrelate to test cases (or even code).Another approach for brown build detection, language-independentas well, was proposed by Lampel et al. [27], who leverage addi-tional resource metrics related to execution time and CPU usage.The choice for these metrics is based on observations at Mozillathat brown builds would typically take longer to#nish than realbuild failures. While promising, most of the metrics required bythis approach were unavailable from our industry partner‚Äôs CI. Thisis simply due to the CI system being deployed in a cloud, whichmakes accurate readings of execution time and CPU usage non-trivial because of (1) multi-tenancy and (2) unknown changes tothe cloud‚Äôs underlying hardware. As such, a project might have alow BFR running on slow hardware, then start to exhibit a largeBFR on better hardware.To validate this hypothesis, we scraped our 7 projects‚Äô buildduration data (the only resource metric tracked by our industrypartner) for build failures from 2021. We only included build jobcon#gurations with at least one brown build, then performed aMann-Whitney test between the build duration distributions of(non-)brown failures (/u1D6FC=0.01). Results are available online in ourreplication package [3].We found that, at least on the studied projects, multi-tenancy andevolving infrastructure impact the applicability of resource metrics.In particular, only projects A and F showed a signi#cant di"erence(small Cli"‚Äôs Delta e"ect size), con#rming Lampel et al. ‚Äôs hypothesis.For those projects, we then split the data chronologically into 5groups, comparing the build duration of brown and real failureswithin each split. For project A, 4 out of 5 groups show a longer buildduration for brown builds (2x large e"ect, 2x small). For projectF, build duration di"erences alternate over time between (non-)signi#cance (2x small).To conclude, in the context of language-independent brown buildprediction on the studied projects, we were only able to use randomprediction models as baselines.4.2.2 Manual validation.We also compare our model to experts‚Äôprediction and decision time. To do so, we asked experts of twoprojects to answer a survey related to their project. These expertscomprise developers of the projects who are knowledgeable of whatthe code changes are doing and have been exposed to CI feedback.Per project, the experts were split into two groups (project A hasone expert per group, project B two experts per group).Each survey contained 40 build jobs, selected randomly amongthe dataset using the following constraints: (1) at most one buildjob per commitID was selected and (2) TP/TN/FP/FN jobs from ourmodel‚Äôs prediction were equally represented (14of the set of jobs inthe survey for each category). While each group of experts receivedthe 40 build jobs to evaluate, half of the jobs were provided withour model‚Äôs prediction (either correct or incorrect) and the otherhalf without. The jobs coming with predictions for Group 1 did notcome with predictions for Group 2, and vice versa.Each group was asked to evaluate for each given build job if theywould label it as brown, based on the associated commit (ID, name,di"), build log, other metrics like #rerun and #commit_since_brown(see Section 3.2.4), and (for half of the jobs) our model‚Äôs prediction.4.2.3 Cross-project validation.Cross-project predictions meanstraining the model using a training and validation set of a givenproject, then using it to predict the results on another project. Ifthe results are satisfying, during the early stages of a new project(onboarding phase), models built on other projects could be usedinstead of having to wait until enough builds would have been runfor the new project. In our evaluations, we apply each project‚Äôsmodels on the other projects to evaluate cross-project performance.4.2.4 Concept dri/f_t.Our concept drift validation aims to evaluate(1) how long the training set data and the trained model should stayup, (2) when the model should be retrained, and (3) with which partof the data. This is important to keep the performance competitive,since new cases of brown builds might be missed, while old types ofbrown builds might never reappear once corrected, making modelsobsolete at some point.Intuitively, we might think that the more data we gather, thebetter results we will get. However, training on a large amount ofdata can be time- and resource-consuming. Furthermore, old datacould be outdated, with given data patterns never reappearing inthe newest build jobs‚Äô traces (e.g., when#xing a/f_laky test or thebuild machine).Furthermore, we evaluate the question of when to retrain amodel. On the one hand, predicting after each new build job iscomputationally expensive and the bene#t of adding a single newbuild job to the training set is relatively small. On the other hand,using the same model forever disregards any new data. As such,new types of brown builds might never be identi#ed by the model.First, we want to evaluate how our approach is impacted byconcept drift. Second, we propose a number of switching heuristics,to decide when to retrain the model. For this validation, we usedthe hyper-parameters that get the best global optimization.Regarding the impact of concept drift on our approach, weneeded to split the data sets into sub-data sets per period of time,which will be referred to as groups: we chose to split the data intoweekly groups (Sunday to Monday). We then need to choose a train-ing window size, which will be the number of consecutive groupsthat are used as the training data set. The window is then shiftedacross the whole data set to simulate a model being retrained eachweek: each retraining is referred to as a drifting step (see Figure 4).The three sets needed for our model computation and validation(training, validation and test sets) are computed respecting timeisolation (Iso-H), by selecting consecutive ordered groups:Iso-Htime isolation: The data-set groups are ordered suchthat if Groupicomes before Groupj, all the jobs in Groupiprecede all the jobs in Groupj.The sets are then de#ned for each drifting step/u1D456:2182Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:57:30 UTC from IEEE Xplore.  Restrictions apply. Towards Language-independent Brown Build DetectionICSE ‚Äô22, May 21‚Äì29, 2022, Pi/t_tsburgh, PA, USA
Train +Valid M0Train +Valid M1Train +Valid M2Train +Valid M3
IJBestBestUpBestLocThreshCumProdModel NumberTime [Week]0Figure 4: Switching heuristics.‚Ä¢Train set: {build‚ààgroup/u1D457|/u1D457‚àà[/u1D456,/u1D464/u1D456ùë¶/u1D461/u1D45F/u1D44E/u1D456/u1D45B‚àí1]}‚Ä¢Valid set: {build‚ààgroup/u1D457|/u1D457=/u1D456+/u1D464/u1D456ùë¶/u1D461/u1D45F/u1D44E/u1D456/u1D45B}‚Ä¢Test set: {build‚ààgroup/u1D457|/u1D457>/u1D456+/u1D464/u1D456ùë¶/u1D461/u1D45F/u1D44E/u1D456/u1D45B}By varying the size of the training window, we can evaluate howfar in the past data must be retrieved to achieve relevant predictionperformance.To evaluate how the model ages across time, we evaluate theperformance on the test sets for each drifting step: we can thenobserve the concept drift impact on the Brown-Detector approachby observing the performance of each drifting step by group (week),and by the number of weeks since the model was trained.We also observe the drift of features to analyze whether there isan evolution in the set of selected features in the vectorization step.For this, we measure the weekly feature surviving ratio, wheremodeli‚Äôs feature surviving ratio at weekjis the percentage offeatures selected at weekithat still are selected in the model ofweekj. If the surviving ratio decreases consistently over time for allmodels, then the feature selection and the model are impacted bythe concept drift and a retraining would be relevant. We show themedian feature surviving ratio of models after/u1D465weeks of existence.4.2.5 Model switching heuristics.Based on the evolution of theperformance of each model over time, we evaluate when to changefrom one model ot another in order to obtain the best performance.For this, we de#ne di"erent switching heuristics that will be usedto decide when to retrain and switch models. Those algorithmswill either usea prioriora posterioriheuristics.A prioriheuristicschoose to switch to the model of week/u1D456based on information aboutall weeks/u1D457before/u1D456(/u1D457</u1D456).A posterioriheuristics choose to switch tothe model of week/u1D456based on information about all weeks/u1D457before/u1D456and week/u1D456included (/u1D457‚â§/u1D456). The latter are unrealistic modelssince they need information about the current week before it evenhappened, but can be used as baseline for the a priori heuristics.For the de#nition of the model switching heuristics, let us as-sume that the drift parameters are/u1D464/u1D456ùë¶/u1D461/u1D45F/u1D44E/u1D456/u1D45B+/u1D464/u1D456ùë¶/u1D463/u1D44E/u1D459/u1D456/u1D451=/u1D44A(with/u1D464/u1D456ùë¶/u1D463/u1D44E/u1D459/u1D456/u1D451=1 the window size of the validation set) and that we have/u1D44A+/u1D441weeks of data, and that for each week/u1D464/u1D456with/u1D456‚â•0 thereis a model/u1D45A/u1D456created with the data from weeks[/u1D464/u1D456‚àí/u1D44A‚àí1;/u1D464/u1D456‚àí1],with/u1D456‚àà[0,/u1D441]. We also de#ne the model/u1D45A/u1D460/u1D456as the model selectedby a switching heuristic at/u1D464/u1D456. The performance of model/u1D45A/u1D457onweek/u1D464/u1D456is/u1D45D/u1D452/u1D45F .(/u1D456,/u1D457). Figure 4 shows an example with/u1D44A=3.For the following switch heuristics description, we suppose thatwe are currently starting week/u1D464/u1D456and that the previous chosenmodel/u1D45A/u1D460/u1D456‚àí1is/u1D45A/u1D457.Best: a posteriori algorithm where the model at/u1D464/u1D456is chosento be the model/u1D45A/u1D458with 0‚â§/u1D458‚â§/u1D456with the highest performance/u1D45D/u1D452/u1D45F .(/u1D456,/u1D458), as shown in orange on Figure 4.BestUp:same as best, but with/u1D460/u1D456‚àí1‚â§/u1D458‚â§/u1D456(in red on Figure 4).BestLoc: same as best, but with/u1D458=/u1D460/u1D457or/u1D458=/u1D456(in blue onFigure 4).Diagonal: a priori algorithm where the model at/u1D464/u1D456is/u1D45A/u1D456, with-out looking at other models‚Äô performance (switch every week tothe newest model).Fix: a priori algorithm where the model is switched every#xednumber of weeks to the most recent model. If week/u1D464/u1D456needs toswitch (because the model has been used for the#xed numberof weeks), the chosen model is/u1D45A/u1D456. Otherwise, keep/u1D45A/u1D460/u1D457, withoutlooking at the performances.Thresh: a priori algorithm where the chosen model at/u1D464/u1D456is/u1D45A/u1D456if the performance/u1D45D/u1D452/u1D45F .(/u1D456‚àí1,/u1D460/u1D456‚àí1)of/u1D45A/u1D460/u1D457during week/u1D464/u1D456‚àí1waslower than a threshold/u1D447, as shown in green on Figure 4.CumProd: a priori algorithm where the model chosen at/u1D464/u1D456is/u1D45A/u1D456if the cumulative product/producttext.1/u1D456‚àí1/u1D457=/u1D450/u1D462/u1D45F/u1D45F/u1D45D/u1D452/u1D45F .(/u1D457,/u1D460/u1D457)of the perfor-mance of the models of week/u1D464/u1D457with/u1D460/u1D457‚â§/u1D457</u1D456‚àí1 was lowerthan a threshold/u1D447. This is shown in purple on Figure 4. We use acumulative product such that the model ages over time, since themultiplication will reduce the value over time.AlgorithmsBest,BestUpandBestLocare weekly upper-boundsfor our analysis, representing three ways to choose the best modelsa posteriori. WhileBestwill always outperform the other twoa posteriori algorithms, we included the latter two to improveunderstanding of the#ndings. On the a priori side,DiagonalandFixare heuristics based only on a model‚Äôs age, whereasThreshandCumProdare based on the evaluated performance and age ofa model.To compare model switching heuristics, we aggregate the per-formance over the whole period (weeks/u1D4640to/u1D464/u1D441), for each week‚Äôsselected model, by summing up the weekly confusion matrices intoone overall confusion matrix, i.e., counting up the true positives,false positives, etc. Averaging the weekly performance would nothave worked due to weeks with very few or no brown jobs at all.We will as well compare those switching heuristics withAlways-Brown, which is a random prediction based on the brownnessratio introduced in Section 5.1 and de#ned in Table 3. Finally, eachswitch heuristic has a life expectation (LifeExp), which is the mediannumber of weeks a model is used before a new model is trained.5 RQ1: CAN WE ACCURATELY DETECTBROWN BUILDS IN A LANGUAGE/PROJECT-AGNOSTIC WAY ?5.1 Hyper-parameter optimization.Motivation.The purpose of this#rst validation is to identifythe best hyper-parameters for our model and to evaluate if theclassi#ers‚Äô performance is relevant in practice.Approach.For this RQ, we use all 7 projects, thus including theopen-source project, using cross-validation to select the best set2183Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:57:30 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pi/t_tsburgh, PA, USAOlewicki et al.Table 4: Model performance.ProjectBrown-DetectorBaselineLocal opti [%]Global opti [%]Random50 [%]RandomB [%]AlwaysBrown [%]BrownSafeBrownSafeBrownSafeBrownSafeBrownSafeF1 Pre RecPre RecF1 Pre RecPre RecF1 Pre RecPre RecF1 Pre RecPre RecF1 Pre RecPre RecA67 56 85 92 7162 55 72 86 7519 30 50 70 5015 30 30 70 7023 30 100 na0B73 69 76 85 8067 57 81 85 6421 37 50 63 5019 37 37 63 6327 37 100 na0C38 41 35 96 9736 35 38 97 965550 95 5035595 9555100 na0D35 32 38 96 9524 21 27 96 945650 94 5036694 9456100 na0E88 84 93 88 7484 76 94 88 5827 58 50 42 5029 58 58 42 4237 58 100 na0F52 51 53 95 9446 38 58 95 89810 50 90 50510 10 90 90910 100 na0OS63 61 66 91 9452 47 57 91 9110 13 50 87 50613 13 87 8712 13 100 na0Median63 56 66 92 9452 47 58 91 8910 13 50 70 50613 13 87 8712 13 100 na0Mean59 56 63 91 8653 47 61 91 8113 22 50 72 5011 22 22 77 7716 22 100 na0of hyper-parameters per project (local optimization) and for theseven projects at once (global optimization), see Section 4.2. Thebest hyper-parameter values for both local and global optimizationsare gathered in Table 5.Results.Our models are two to three times better than thebest random baselineAlwaysBrownin terms of F1-score foreach studied project.Table 4 shows the performance per project of the local and globaloptimization of hyper-parameters, in comparison with the baselines.We observe that performance varies substantially from one projectto another. This is due to the unbalance of brownness in build jobsacross the studied projects: The higher the ratio of brownness inthe failed build job, the higher the F1-score will be. For instance,project E has the best F1-score (79%) and the highest brown failureratio among the studied projects (/u1D435/u1D439/u1D445=58%), while project C hasthe second-worst F1-score (29%) and the lowest brown failure ratio(/u1D435/u1D439/u1D445=5%).When switching from local to global optimization, the F1-scoredecreases between 2% for project C and 11% for projects D and OSwhen choosing a globally optimal set of hyper-parameters.The baseline that gets the best F1-score isAlwaysBrown. Yet,the median F1-score of this baseline is more than three times (53%compared to 14%) worse than the global optimization of our models.Furthermore, the OS project shows results close to project F, whichis the project with the closest/u1D435/u1D439/u1D445, providing initial evidence thatthe results could be generalized to open-source projects.In addition to F1-score, Table 4 also shows the other performancemetrics. The median precision (recall) goes from 56% (66%) for localoptimization to 47% (58%) for global optimization. Having precisionclose to 50% can seem problematic, however, let us not forget thatthe dataset is highly unbalanced, and that the values are higher thanthe baselines. Furthermore, 89% of the true failures (non-brown)are correctly identi#ed by our model and 91% of failure predictionsare actually true failures, which meets our objective of making theCI results more reliable.Regarding feature selection, we analyzed the features with sig-ni#cant tf-idf values of the OS project, to better understand whatdrives the models. We observed that occurrences of ‚Äúread_databas"in jobs tend to be related to brownness and of ‚Äúreturn_error" tendto be related to true failures, which are intuitive to understand.Table 5: Optimal hyper-parameter values per project in termsof F1-score.OptimizationHyper-parametersType Project/u1D439/u1D441 /u1D43E /u1D6FC /u1D6FDLocalATrain [1] 300 0% 10%BAll [1] 275 40% 20%CTrain [2] 300 90% 20%DAll [1, 2] 100 50% 10%EAll [1] 300 50% 30%FAll [1] 225 100% 10%OSTrain [1] 200 0% 30%Global AllAll 2 300 70% 10%However, less intuitive observations were seen with features suchas ‚Äúbuild_object" or ‚Äúoccur_dure" that can be related to both labels,depending on the other terms in the logs.5.2 Manual validationMotivation.While the previous subsection compared our modelsto intuitive baselines, this section compares our models‚Äô perfor-mance to human experts from our industrial partner to validatethe extent to which our prototype can perform as well if not betterthan experts in terms of predicting brownness quicker.Approach.The manual validation was done on projects A and Bonly. Each expert is given 40 failed build jobs and must identify ifthey are true failures or brown. The#nal expert‚Äôs performance iscomputed respecting the original confusion matrix of our modelprediction. For project A, this contains TP:22%, TN:52%, FP:18%,FN:9% and for project B TP:30%, TN:40%, FP:23%, FN:7%.Table 6 shows the mean prediction results for the experts in eachproject (A and B) by category. We use the mean because there wereonly respectively 2 and 4 experts in the projects and only 10 jobsper category, making outliers unlikely.Results.Project A‚Äôs experts manually predict brown buildswith mean F1-score 4% better than our models, while projectB‚Äôs experts manually identify jobs as brown with mean F1-score 17% lower than our models.From Table 6 (i.e., the results for project A), we observe thatexperts of project A have predicted TP jobs correctly in 83% of the2184Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:57:30 UTC from IEEE Xplore.  Restrictions apply. Towards Language-independent Brown Build DetectionICSE ‚Äô22, May 21‚Äì29, 2022, Pi/t_tsburgh, PA, USATable 6: Manual validation‚Äôs mean prediction [%] of theexperts by category.Expected to beExpected to beMean evaluationbrown (100%)true-failure (0%)time by build jobProjectTP FNTN FP[mm:ss]A83 7318 542:44B68 6861 652:55Table 7: Manual validation‚Äôs performance results.ProjectComparisonBrownSafeF1 Pre RecPre RecAPred vs Oracle62 55 72 86 75Expert vs Oracle66 56 80 89 73BPred vs Oracle67 57 81 85 64Expert vs Oracle50 39 68 38 37cases and TN in 82% (100-18), whereas our model, by de#nition,identi#es both groups correctly in 100% of the cases. However,the experts correctly identify 73% of FN and 35% (100-65) of FP,whereas our model misclassi#es all the jobs. Project B shows lessvariation from one category of results to another, showing that thedevelopers on that project globally identify jobs as brown in 61-68%of the cases, without signi#cant di"erences between categories. Forboth projects, no signi#cant di"erence was found in the survey (seeSection 4.2.2) for predicting jobs when we provided our predictionor not.In Table 7, we evaluated the same metrics as for the hyper-parameter validation, this time comparing the global optimizationof the hyper-parameters for projects A and B with their respectiveexperts‚Äô performance. The metrics are computed by weighing thedi"erent categories (TP/TN/FP/FN) with the real ratio of each cat-egory from the globally optimal model. We observe that expertsfrom project A have better results than our model, with an F1-scoreof 66% (4% higher than our model). For project B, we see that ourmodel has better results, with an F1-score of 67% (17% higher thanthe experts).Even though our models‚Äô precision and recall are not perfect, weobserve that they are similar to or better than experts‚Äô performance.Our model can thus be used to improve the CI system, by detectingbrown builds before the developer has to rerun them manually,saving the experts‚Äô precious time. In fact, our model, once trained,only takes seconds to predict the status (brown or safe) of a newbuild jobs, reducing the e"ort needed by experts, since the experts ofprojects A and B took on average 2min44 and 2min55, respectively,to interpret the brown-ness of the analyzed build jobs.6 RQ2: CAN A BROWN BUILD PREDICTIONMODEL BE USED ON ANOTHER PROJECT ?Motivation.One of the hypotheses we de#ned from the begin-ning was that a classi#er have to be trained for each project. This isa limitation as each project needs a cold-start period to gather databefore being able to provide predictions, while brown build detec-tion would be required from the start. To challenge this hypothesis,here we build and evaluate cross-project prediction models.Table 8: Cross-project F1-score. The project ‚ÄúPred‚Äù is pre-dicted by the a model trained on the project ‚ÄúTrain‚Äù. (Diago-nal is the global optimization and ‚Äú/uni2605‚Äù isAlwaysBrown.)PredTrainF1-scoreA B C D E F OS/uni2605A6232 3 45 49 1 523B156727 47 39 13 427C2 163616 12 17 85D8 12 122411 7 45E53 80 0 35845 2337F10 20 na 6 1846299OS11 12 21 12 12 45212Approach.For this validation, we used each project‚Äôs model topredict brown builds on other projects (which we call ‚Äú1-to-1 ap-proach"). The models use the hyper-parameter values that yieldedglobally optimal performance across the projects (for within-projectprediction). We also considered applying a leave-one-out approach(training with all datasets but one, then testing on the latter), butthis approach would be computationally much more expensive thanthe 1-to-1 approach (due to the very large training set) [29].In this section, we will refer to ‚Äúcross-project prediction" whena project is predicted by a model trained on another project, andwill refer to by-project prediction otherwise (see section 5.1).Results.F1-score of cross-project prediction is between 4and 84% (median of 12%) lower than the prediction by-projectfor all studied projects. Provided the right training project(s)can be identi/f_ied, cross-project prediction could be a viablealternative, at least until a project-speci/f_ic model is available.In Table 8, we gathered the performance of cross-project vali-dation for the seven studied projects (for proj/u1D465and proj/u1D466.altin stud-ied projects where/u1D465/uni2260/u1D466.alt, proj/u1D465is predicted by a model trainedon proj/u1D466.alt). It can be seen that the cross-project prediction for allprojects is 4 to 84% lower than the global hyper-parameter optimiza-tion. Some cross-project combinations still give results close to theby-project prediction. For instance, project E predicted by projectB gives an F1-score of 80% (4% lower than by-project prediction).Other combinations do not perform prediction correctly at all, forinstance, project F‚Äôs prediction by project C always identi#es failureas true-failure, yielding an ‚Äúna‚Äùresult.One of the authors, who is an expert on project A, manuallyanalyzed a sample of TP (true brown) predictions for project A. Weextracted the corresponding TP predictions of each cross-projectcombination. From the 25 sampled predictions, 14 corresponded tocases the author was aware of, while 11 identi#ed new brown buildcases due to issues with the#le system and network.Furthermore, the F1-score of cross-project results are not con-sistently higher than theAlwaysBrownbaseline, varying from animprovement of 43% to a loss of 37% on the F1-score (with a medianof 3%). This validates that a project-speci#c model is essential assoon as historical data is available, but that, given the right trainingproject(s), cross-project prediction could be a feasible, temporarysolution.2185Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:57:30 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pi/t_tsburgh, PA, USAOlewicki et al.Table 9: Switcher comparison with a/u1D464/u1D456ùë¶/u1D461/u1D45F/u1D44E/u1D456/u1D45Bsize of 30. The table vertically is split into a posteriori, a priori heuristics andbaselines. Bold indicates the line with the best (F1-score,life expectancy) of a project per category.SwitcherProject AProject BProject CProject OSBrownSafeLifeBrownSafeLifeBrownSafeLifeBrownSafeLifeF1 Pre RecPre RecExpF1 Pre RecPre RecExpF1 Pre RecPre RecExpF1 Pre RecPre RecExpbest (post)69 70 69 87 87174 79 70 89 83132 59 21 99 96236 28 48 95 984bestup (post)65 65 64 85 85271 78 65 89 81230 57 20 99 96336 28 48 95 984bestloc (post)63 64 62 85 84270 77 63 89 80222 45 14 99 95621 24 19 97 978#rst51 56 46 84 784362 70 56 86 774314 35999 953832 27 39 96 9716diagonale56 59 54 84 81167 75 61 88 79122 45 15 99 95144 44 45 98 981#x (2 weeks)57 59 54 84 81266 73 61 86 78227 50 19 99 96239 40 39 98 972#x (5 weeks)59 60 57 84 82566 73 61 86 79524 44 16 99 95542 39 45 97 985#x (10 weeks)56 59 54 84 811067 71 64 85 791020 40 14 99 951038 33 45 96 988#x (15 weeks)56 59 53 84 811566 70 62 84 781522 42 15 99 951534 29 42 96 988cumProd (/u1D447=0.01)60 61 60 84 83667 73 62 86 79522 39 15 99 95523 24 23 97 978cumProd (/u1D447=0.02)59 61 58 84 82466 74 60 87 78520 42 13 99 95423 24 23 97 978cumProd (/u1D447=0.05)60 60 59 83 83466 74 60 87 78420 38 14 99 95329 29 29 97 975thresh (/u1D447=0.7)59 60 58 84 82167 74 60 87 79126 52 18 99 96235 35 35 97 972thresh (/u1D447=0.8)59 60 57 83 82167 74 60 87 79126 52 18 99 96235 35 35 97 972AlwaysBrown23 30 100 na027 37 100 na055100 na012 13 100 na0Best Hyper-param62 55 72 75 8667 57 81 64 8536 35 38 96 9752 47 57 91 91
Figure 5: Feature surviving ratio per project with a/u1D464/u1D456ùë¶/u1D461/u1D45F/u1D44E/u1D456/u1D45Bsize of 30.7 RQ3: HOW LONG CAN A MODEL STAYRELEVANT WITHOUT BEING RETRAINED ?Motivation.This RQ evaluates how well models can deal withand mitigate concept drift. In particular, we aim to (1) evaluate thesize of the data needed for a model to achieve optimal predictionresults and (2) to evaluate when to change models (and thus retrain).Approach.For this validation, we used hyper-parameter valuesof the globally optimal models, see Table 5. Since we are evaluatingthe prototype over time, we needed datasets with a long historyand a representative number of builds by month. This is why weselected projects with at least one year of data and at least 1k buildjobs per month, leaving us with projects A, B, C and OS. In the caseof OS, only the last 48 weeks of data were considered to respect thecondition of 1k build jobs per month.Results.A window size of 30 weeks of data is suÔ¨Écient tohave signi/f_icant performance, since adding more weeks doesnot considerably increase the performance (<0.1%) and , inthe case of project A, using more than 40 weeks even harmsthe performance.We plotted the median performance (F1-score, precision, recall)of the models over time, depending on the project and/u1D464/u1D456ùë¶/u1D461/u1D45F/u1D44E/u1D456/u1D45Btraining window size. For space concerns, we added these plots tothe replication package. These plots showed how the curves of thethree metrics decrease between 0 and 10% per week, for each project.This decrease in performance over time indicates that retraininga new model at some point would be bene#cial. We also#nd thatthe smaller the/u1D464/u1D456ùë¶/u1D461/u1D45F/u1D44E/u1D456/u1D45Bsize is, the lower the F1-score is. However,as we increase the/u1D464/u1D456ùë¶/u1D461/u1D45F/u1D44E/u1D456/u1D45Bsize, the median improvement on theF1-score becomes<0.1% . For Project A, a closer analysis showedthat/u1D464/u1D456ùë¶/u1D461/u1D45F/u1D44E/u1D456/u1D45Bsizes 45 and 50 obtain an F1-score worse than 40. Thisshows that using too old data harms the model, since the data isnot coherent with more recent data. Closer analysis showed that a/u1D464/u1D456ùë¶/u1D461/u1D45F/u1D44E/u1D456/u1D45Bsize 30 gets the best results for all studied projects. ProjectOS was ignored for this experiment since the number of weeks was48 and we needed up to 50 weeks of data in the training set.The feature surviving ratio drops down to 58-76% after oneweek, then decreases consistently over time for all project.Figure 5 shows the training feature surviving ratio for a/u1D464/u1D456ùë¶/u1D461/u1D45F/u1D44E/u1D456/u1D45Bsize of 30 weeks (i.e., the optimal value). Projects A and B showsimilar values, dropping quickly to a median feature drifting rateof 72% and 76% respectively, then reducing linearly to 60% at week38. Project C and OS both have a median feature surviving ratiodrops to 63% and 58%, then reduce to 35% at week 30 and 40% atweek 10 respectively. The#rst drop for the three projects seems tobe due to features local to the week of the training, only relevantto the prediction of the#rst few (1 to 2) weeks after training. Theconsistent decrease after that shows that other features get rejectedover time, even if they stay relevant longer.A priori switching heuristics achieve an F1-score about 7-10% lower than a posteriori heuristics, while keeping results2-to-4 times higher than theAlwaysBrownbaseline.2186Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:57:30 UTC from IEEE Xplore.  Restrictions apply. Towards Language-independent Brown Build DetectionICSE ‚Äô22, May 21‚Äì29, 2022, Pi/t_tsburgh, PA, USATable 9 shows all performance metrics that we used, as well asthe life expectationLifeExp(in number of weeks) when using aparticular switcher. We observe that the a priori switching heuristicshave an F1-score 7-10% under the a posteriori heuristicBest, exceptfor the project OS. For the latter project, theBestheuristic choosesthe best model each week (performance‚â•diagonal), yet has a loweroverall F1 score because the number of true/false positive and falsenegative/f_luctuates across the studied weeks (known as Simpson‚Äôsparadox[31]). Compared todiagonal,CumProdgets the same F1-score for project B and C and a higher score for project A, whileneeding less training computation (the medianLifeExpfor projectA is 6 and for project B and C is 5). Project OS‚Äôs F1-score is 2% lowerthan the diagonal with theFix (5 weeks)heuristic.When comparing our switch heuristics with theAlwaysBrownbaseline, we observed that our F1-score is two to four times better.When compared with the best hyper-parameter combination‚Äôs per-formance, the F1-score performance loses 0-10% depending on theproject when including the time constraint and using a priori switchheuristics instead of posteriori heuristics. Our prototype, once ex-tended with a switch heuristic, has thus results signi#cantly higherthan the baseline. The model can adapt through time, retrainingwhen it is needed to have the best performance possible.8T H R E A T S T O V A L I D I T YExternal validity.The approach targets projects involving multiple programmingtechnologies, of which gaming projects are an extreme case, sincebuilds involve code, AI, physics, 3D models, sound objects, etc.on dozens of platforms, each with their own SDK versions. Ouralgorithm was evaluated on seven large projects with various brownfailure ratios and build activity. We observed that the recall andprecision performance decrease as the brown failure ratio decreases.Hence, the approach is expected to generalize to projects withbuild logs with default verbosity and a reasonable BFR (i.e., balanced,unlike projects C/D). A project having a brown failure ratio lowerthan 1% might not justify the need for our algorithm, since theperformance might not be su$cient.We considered only one open-source project, Graphviz, amongthe 7 studied projects. Further analysis on other projects would berelevant to analyze the generalization of our results.Our approach focuses on the identi#cation of brown builds, butdoes not propose any solution on how to#x the identi#ed brownbuilds (rerunning builds does not target the root cause of brownbuilds). Similar to the related#eld of/f_laky tests (subset of brownbuilds) [11,37],#xing/f_lakiness is an ongoing research area, espe-cially since brown builds are even more challenging to deal with,due to the occurrence of di"erent languages and build technology.Construct validity.The oracle used for evaluating the brownbuild identi#cation models relies on a heuristic, i.e., if a job is re-run at least once and changes results for the same commit-ID, itis identi#ed as brown. However, most of the build jobs are runonly once for a given commit-ID, and hence are considered to betrue build failures or successes. If a developer forgot to re-run abrown build, our oracle would have missed it. We believe the riskof losing such builds in our oracle is limited because brown buildsare relatively rare, and the developers of the 6 commercial projectshave as practice to rerun the known cases of brown build to checkif the build‚Äôs status switches to success.In our evaluation, we optimize the models for F1-score, whichmaximizes both precision and recall. However, depending on theuse case, organizations adopting our models might prefer to tweakthe model for better precision (less false alarms) at the expense ofrecall (missing brown builds), or vice versa.Internal validity.Internal validity refers to alternative explana-tions of our research results. The ground truth has been ‚Äúlabeled‚Äù(decision to re-run a build failure due to suspicion of being brown)by build experts of the corresponding project right after the build#nished. Finding a better expert or time to do the labeling wouldnot be possible.Regarding the validity of the user study, the participating expertswere experts in the project they were asked to evaluate builds for,but they were shown 6-month old builds they may not have beeninvolved with. This design was used to counter potential learninge"ects.9 CONCLUSIONDevelopers regularly experience brown builds, i.e., build failuresnot due to code changes, test cases or build logic, but due to factorsoutside their control. Our empirical study on build results of 7 multi-language projects, 6 developed by one of the leading AAA-gameproducers and one open-source project (graphviz), observed thatbetween 5% and 58% of failed build jobs were brown, dependingon the project, highlighting the need to address this brown buildproblem and propose a detection algorithm.Our brown build detection algorithm is language- and project-agnostic, and obtained a median F1-score of 52%, more than twotimes higher than theAlwaysBrownbaseline (for all projects), andsimilar to experts‚Äô F1-score (-4% to +17%), while reducing the e"ortneeded by those experts. We showed that cross-project predictioncan be a workaround for on-boarding new projects, but their perfor-mance is not consistently higher than the baselines. We recommendswitching to a project-speci#c model as soon as possible.Our study of the impact of concept drift on the models shows thatour approach in its current form is sustainable over time, and formsa solid base for future research on brown builds. While modelsand data age over time and impact the performance, we found asweetspot (30 weeks) in terms of the size of the training set andmodel change frequency (4-5 weeks), and we proposed promisingheuristics for deciding about switching to a new version of a model.In terms of implications for practitioners, we have shown howour language-independent models perform at least as well as humanexperts (RQ1), and also function in a predictive setting (RQ3); withthe right training project, cross-project prediction can bootstrapa new project (RQ2). In terms of research implications, we haveshown how build logs are su$ciently rich to predict brown buildsin a real setting, independent from programming technologies.These implications open new research directions. Apart fromvalidating the approach (and future incarnations) on other systems,we believe that future work could focus on specialized models foridentifying di"erent subsets of brown builds (e.g., due to time-outvs. hardware failure), as well as on strategies to#x brown builds,once identi#ed.2187Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:57:30 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pi/t_tsburgh, PA, USAOlewicki et al.REFERENCES[1]List of english stop words. Accessed February 2020. [Online]. Available:http://xpo6.com/list-of-english-stop-words/[2]Openstack zuul ci dashboard. Accessed February 2020. [Online]. Available:http://zuul.openstack.org[3]Replication package. [Online]. Available: https://github.com/ubisoft/ubisoft-laforge-brownbuild[4]Shap values documentation. Accessed February 2020. [Online]. Available:https://shap.readthedocs.io/en/latest/[5]Sklearn selectkbest python package. Accessed February 2020. [Online].Available: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest[6]Stemmer golang package. Accessed February 2020. [Online]. Available:https://github.com/caneroj1/stemmer[7]Treeherder. Accessed November 2019. [Online]. Available: https://treeherder.mozilla.org/#/jobs?repo=mozilla-inbound[8]Xgboost python package. Accessed February 2020. [Online]. Available:https://xgboost.readthedocs.io/en/latest/python/index.html[9]B. Adams and S. McIntosh, ‚ÄúModern release engineering in a nutshell‚Äìwhyresearchers should care, ‚Äù inIEEE 23rd international conference on software analysis,evolution, and reengineering (SANER), vol. 5. IEEE, 2016, pp. 78‚Äì90.[10]A. Ahmad, O. Lei/f_ler, and K. Sandahl, ‚ÄúEmpirical analysis of factors and theire"ect on test/f_lakiness-practitioners‚Äô perceptions,‚ÄùJournal of Software: Testing,Veri/f_ication and Reliability, 2021.[11]J. Bell, O. Legunsen, M. Hilton, L. Eloussi, T. Yung, and D. Marinov, ‚ÄúDe/f_laker:Automatically detecting/f_laky tests,‚Äù inIEEE/ACM 40th International Conferenceon Software Engineering (ICSE). IEEE, 2018, pp. 433‚Äì444.[12]J. Benesty, J. Chen, Y. Huang, and I. Cohen, ‚ÄúPearson correlation coe$cient,‚Äù inNoise reduction inspeech processing. Springer, 2009, pp. 1‚Äì4.[13]W. B. Cavnar, J. M. Trenkleet al., ‚ÄúN-gram-based text categorization,‚Äù in3rdannual symposium on document analysis and information retrieval (SDAIR), vol.161175, 1994.[14]R. Y. Chen, J. Schulman, P. Abbeel, and S. Sidor, ‚ÄúUcb and infogain explorationvia q-ensembles,‚ÄùarXiv preprint arXiv:1706.01502, vol. 9, 2017.[15]A. Fujino, H. Isozaki, and J. Suzuki, ‚ÄúMulti-label text categorization with modelcombination based on f1-score maximization, ‚Äù in3rd International Joint Conferenceon Natural Language Processing (ICNLP): Volume-II, 2008.[16]K. Gallaba, C. Macho, M. Pinzger, and S. McIntosh, ‚ÄúNoise and heterogeneity inhistorical build data: an empirical study of travis ci,‚Äù inIEEE 33rd InternationalConference on Automated Software Engineering (ASE). ACM, 2018, pp. 87‚Äì97.[17]T. A. Ghaleb, D. A. da Costa, Y. Zou, and A. E. Hassan, ‚ÄúStudying the impact ofnoises in build breakage data,‚ÄùIEEE Transactions on Software Engineering, 2019.[18]J. Ha, J. Yi, P. Dinges, J. Manson, C. H. Sadowski, and N. Meng, ‚ÄúSystem to uncoverroot cause of non-deterministic (/f_laky) tests,‚Äù 2016, US Patent 9,311,220.[19]K. Herzig and N. Nagappan, ‚ÄúEmpirically detecting false test alarms using asso-ciation rules,‚Äù in37th International Conference on Software Engineering (ICSE) -Volume 2. IEEE Press, 2015, p. 39‚Äì48.[20]M. Hilton, T. Tunnell, K. Huang, D. Marinov, and D. Dig, ‚ÄúUsage, costs, and bene#tsof continuous integration in open-source projects,‚Äù inIEEE 31st InternationalConference on Automated Software Engineering (ACM). ACM, 2016, pp. 426‚Äì437.[21]H. Huang, H. Xu, X. Wang, and W. Silamu, ‚ÄúMaximum f1-score discriminativetraining criterion for automatic mispronunciation detection,‚ÄùIEEE/ACM Trans-actions on Audio, Speech, and Language Processing, vol. 23, no. 4, pp. 787‚Äì797,2015.[22]J. Humble and D. Farley,Continuous Delivery: Reliable Software Releases throughBuild, Test, and Deployment Automation. Pearson Education, 2010.[23] W. M. Ibrahim, N. Bettenburg, E. Shihab, B. Adams, and A. E. Hassan, ‚ÄúShould icontribute to this discussion?‚Äù in7th IEEE Working Conference on Mining SoftwareRepositories (MSR). IEEE, 2010, pp. 181‚Äì190.[24]A. Labuschagne, L. Inozemtseva, and R. Holmes, ‚ÄúMeasuring the cost of regressiontesting in practice: a study of java projects using continuous integration, ‚Äù in11thJoint Meeting on Foundations of Software Engineering (FSE). ACM, 2017, pp.821‚Äì830.[25]W. Lam, R. Oei, A. Shi, D. Marinov, and T. Xie, ‚Äúid/f_lakies: A framework fordetecting and partially classifying/f_laky tests, ‚Äù in12th IEEE Conference on softwaretesting, validation and veri/f_ication (ICST). IEEE, 2019, pp. 312‚Äì322.[26]X. N. Lam, T. Vu, T. D. Le, and A. D. Duong, ‚ÄúAddressing cold-start problem in rec-ommendation systems, ‚Äù in2nd international conference on Ubiquitous informationmanagement and communication (ICUIMC), 2008, pp. 208‚Äì211.[27]J. Lampel, S. Just, S. Apel, and A. Zeller, ‚ÄúWhen life gives you oranges: detectingand diagnosing intermittent job failures at mozilla,‚Äù in29th ACM Joint Meetingon European Software Engineering Conference and Symposium on the Foundationsof Software Engineering (ESEC/FSE), 2021, pp. 1381‚Äì1392.[28]Z. C. Lipton, C. Elkan, and B. Narayanaswamy, ‚ÄúThresholding classi#ers tomaximize f1 score, ‚ÄùMachine Learning and Knowledge Discovery in Databases, vol.8725, pp. 225‚Äì239, 2014.[29]C. Liu, D. Yang, X. Xia, M. Yan, and X. Zhang, ‚ÄúA two-phase transfer learningmodel for cross-project defect prediction,‚ÄùInformation and Software Technology,vol. 107, pp. 125‚Äì136, 2019.[30]Q. Luo, F. Hariri, L. Eloussi, and D. Marinov, ‚ÄúAn empirical analysis of/f_lakytests, ‚Äù in22nd ACM SIGSOFT International Symposium on Foundations of SoftwareEngineering (FSE). ACM, 2014, pp. 643‚Äì653.[31] G. Malinas and J. Bigelow, ‚ÄúSimpson‚Äôs paradox,‚Äù 2004.[32]A. Memon, Z. Gao, B. Nguyen, S. Dhanda, E. Nickell, R. Siemborski, and J. Micco,‚ÄúTaming google-scale continuous testing,‚Äù in39th International Conference onSoftware Engineering: Software Engineering in Practice Track (SEIP). IEEE, 2017,pp. 233‚Äì242.[33]A. M. Memon and M. B. Cohen, ‚ÄúAutomated testing of GUI applications: Models,tools, and controlling/f_lakiness,‚Äù in35th International Conference on SoftwareEngineering (ICSE). IEEE Press, 2013, p. 1479‚Äì1480.[34]K. Pfe"ers, T. Tuunanen, C. E. Gengler, M. Rossi, W. Hui, V. Virtanen, and J. Bragge,‚ÄúThe design science research process: A model for producing and presentinginformation systems research,‚Äù in1st International Conference on Design ScienceResearch in Information Systems and Technology (DESRIST), 2006, pp. 83‚Äì106.[35]G. Pinto, B. Miranda, S. Dissanayake, M. d‚ÄôAmorim, C. Treude, and A. Bertolino,What is the Vocabulary of Flaky Tests?Association for Computing Machinery,2020, p. 492‚Äì502.[36]J. E. Ramos, ‚ÄúUsing tf-idf to determine word relevance in document queries, ‚Äù 2003.[37]S. V. V. Subramanian, S. McIntosh, and B. Adams, ‚ÄúQuantifying, characterizing,and mitigating/f_lakily covered program elements, ‚ÄùIEEE Transactions on SoftwareEngineering, 2020.[38]M. Zolfagharinia, B. Adams, and Y.-G. Gu√©h√©neuc, ‚ÄúDo not trust build resultsat face value-an empirical study of 30 million cpan builds,‚Äù inIEEE/ACM 14thInternational Conference on Mining Software Repositories (MSR). IEEE, 2017, pp.312‚Äì322.
2188Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:57:30 UTC from IEEE Xplore.  Restrictions apply. 