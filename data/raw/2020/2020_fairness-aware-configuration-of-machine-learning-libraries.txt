Fairness-aware Configuration of Machine Learning Libraries
Saeid Tizpaz-Niari
saeid@utep.edu
University of Texas at El PasoAshish Kumar
azk640@psu.edu
Pennsylvania State University
Gang Tan
gtan@psu.edu
Pennsylvania State UniversityAshutosh Trivedi
ashutosh.trivedi@colorado.edu
University of Colorado Boulder
ABSTRACT
Thispaperinvestigatestheparameterspaceofmachinelearning
(ML)algorithmsinaggravatingormitigatingfairnessbugs.Data-
drivensoftwareisincreasinglyappliedinsocial-criticalapplications
where ensuring fairness is of paramount importance. The existing
approaches focus on addressing fairness bugs by either modify-
ing the input dataset or modifying the learning algorithms. Ontheotherhand,theselectionofhyperparameters,whichprovidefiner controls of ML algorithms, may enable a less intrusive ap-proachtoinfluencethefairness. Canhyperparametersamplifyor
suppress discrimination present in the input dataset? How can we
helpprogrammersindetecting,understanding,andexploitingtherole
of hyperparameters to improve the fairness?
We design three search-based software testing algorithms to un-
cover the precision-fairness frontier of the hyperparameter space.
We complement these algorithms with statistical debugging to ex-
plain the role of these parameters in improving fairness. We imple-
menttheproposedapproachesinthetoolParfait-ML(PARameter
FAIrnessTestingforMLLibraries)andshowitseffectivenessand
utility over five mature ML algorithms as used in six social-critical
applications. In these applications, our approach successfully iden-
tified hyperparameters that significantly improve (vis-a-vis the
state-of-the-arttechniques)thefairnesswithoutsacrificingpreci-
sion. Surprisingly, for some algorithms (e.g., random forest), ourapproach showed that certain configuration of hyperparameters
(e.g.,restrictingthesearchspaceofattributes)canamplifybiases
across applications. Upon further investigation, we found intuitive
explanations of these phenomena, and the results corroborate simi-
lar observations from the literature.
ACM Reference Format:
SaeidTizpaz-Niari,AshishKumar,GangTan,andAshutoshTrivedi.2022.
Fairness-aware Configuration of Machine Learning Libraries. In 44th In-
ternational Conference on Software Engineering (ICSE ’22), May 21–29, 2022,
Pittsburgh, PA, USA. ACM, New York, NY, USA, 12 pages. https://doi.org/10.
1145/3510003.3510202
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe firstpage.Copyrights forcomponentsof thisworkowned byothersthan the
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.35102021 INTRODUCTION
Data-drivensoftwareapplicationsareanintegralpartofmodern
life impacting every aspect of societal structure, ranging from edu-
cationandhealthcaretocriminaljusticeandfinance[ 12,24].Since
thesealgorithms learnfrom priorexperiences, itisnot surprising
that they encode historical and present biases due to displacement,
exclusion,segregation,andinjustice.Theresultingsoftwaremay
particularlydisadvantageminoritiesandprotectedgroups1andbe
found non-compliant with law such as the US Civil Rights Act [ 5].
Therefore, helping programmers detect and mitigate fairness bugs
insocial-criticaldata-drivensoftwaresystemsiscrucialtoensure
inclusion in our modern, increasingly digital society.
The software engineering (SE) community has invested substan-
tial efforts to improve the fairness of ML software [ 2,8,17,35,41].
Fairnesshasbeentreatedasacriticalmeta-propertiesthatrequires
an analysis beyond functional correctness and measurements such
aspredictionaccuracy[ 7].However,themajorityofpreviouswork
withintheSEcommunityevaluatesfairnessonthe MLmodels after
training [ 2,17,35,41], while the programmer supports to improve
fairness during the inference of models (i.e., training process) is
largely lacking.
The role of training processes in amplifying or suppressing vul-
nerabilities and bugs in the input dataset is well-documented [ 40].
The training process typically involves tuning of hyperparame-
ters: variables that characterize the hypothesis space of ML models
and define a trade-off between complexity and performance. Some
prominentexamplesofhyperparametersinclude l1vs.l2lossfunc-
tion in support vector machines, the maximum depth of a decision
tree, and the number of layers/neurons in deep neural networks.
Hyperparameters arecrucially differentfrom theML modelparam-
etersinthattheycannotbelearnedfromtheinputdatasetalone.
In this paper, we investigate the impact of hyperparameters on ML
fairness and propose a programmer support system to develop fair
data-driven software.
We pose the following research questions: To what extent can
hyperparameters influence the biases present in the input dataset?
Can we assist ML library developers in identifying and explaining
fairness bugs in the hyperparameter space? Can we help ML users to
exploit the hyperparamters to imporive fairness?
WepresentParfait-ML(PARameterFAIrnessTestingforMLLi-
braries):asearch-basedtestingandstatisticaldebuggingframework
1AwallstreetjournalarticleshowedDeloitte,alife-insuranceriskassessmentsoftware
can discriminate based on the protected health status of applicants [ 16,32]. FICO,
a credit risk assessment software, is found to predict higher risks for black non-
defaulters [ 18] than white/Asian ones. COMPAS risk assessment software in criminal
justice is shown to predict higher risks for black defendants [19].
9092022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Saeid Tizpaz-Niari,Ashish Kumar, Gang Tan, and Ashutosh Trivedi
thatsupportsMLlibrarydevelopersanduserstodetect,understand,
and exploit configurations of hyperparameters to improve ML fair-
ness without impacting functionality. We design and implement
three dynamic search algorithms (independently random, black-box
evolutionary,and gray-boxevolutionary )tofindconfigurationsthat
simultaneously maximize and minimize group-based fairness with
aconstraintonthepredictionaccuracy.Then,weleveragestatis-
tical learning methods [ 21,33] to explain what hyperparameters
distinguishlow-biasmodelsfromhigh-biasones.Suchexplanatory
modelsspecificallyaidMLlibrarymaintainerstolocalizeafairness
bug. Finally, we show that Parfait-ML can effectively (vis-a-vis
thestate-of-the-arttechniques)aidMLuserstofindaconfiguration
that mitigates bias without degrading the prediction accuracy.
Weevaluateourapproachonfivewell-establishedmachinelearn-
ingalgorithmsoversixfairness-sensitivetrainingtasks.Ourresults
show that for some algorithms, there are hyperparameters that
consistently impact fairness across different training tasks. For ex-
ample, max_feature parameterinrandomforestcanaggravatethe
biasesforsomeofitsvaluessuchas log2(#num.features)beyond
a specific training task. These observations corroborate similar
empirical observations made in the literature [39].
In summary, the key contributions of this paper are:
(1)thefirstapproachtosupport MLlibrarymaintainers tounder-
standthefairnessimplicationsofalgorithmicconfigurations;
(2)three search-based algorithms to approximate the Pareto
curve of hyperparameters against the fairness and accuracy;
(3)astatisticaldebugging approachtolocalizeparametersthat
systematically influence fairness in five popular and well-
establish ML algorithms over six fairness-critical datasets;
(4)amitigation approach to effectively find configurations that
reduce the biases (vis-a-vis the state-of-the-art); and
(5)an implementation of Parfait-ML (PARameter FAIrness
Testing for ML Libraries) and its experimental evaluation
on multiple applications, available at: https://github.com/
Tizpaz/Parfait-ML.
2 BACKGROUND
Fairness Terminology and Measures. Let us first recall some
fairnessvocabulary.Weconsider binaryclassification taskswhere
aclasslabelis favorable ifitgivesabenefittoanindividualsuch
as low credit risk for loan applications (default), low reoffend risk
for parole assessments (recidivism), and high qualification score
for job hiring. Each dataset consists of a number of attributes (such
as income, employment status, previous arrests, sex, and race) and
a set ofinstances that describe the value of attributes for each
individual. We assume that each attribute is labeled as protected
ornon-protected. According toethical and legal requirements,ML
software should not discriminate on the basis of an individual’s
protected attributes such as sex, race, age, disability, colour, creed,
national origin, religion, genetic information, marital status, and
sexual orientation.
Thereareseveralwell-motivatedcharacterizationsoffairness.
Fairness through unawareness (FTU) [ 16] requires masking pro-
tected attributes during training. However, FTUis not effective
sinceprotectedandnon-protectedattributesoftencorrelate(e.g.,
ZIP code andrace), and biases are introduced from non-protected
Figure 1: Data-Driven Software System Developments
attributes. Fairness through awareness (FTA) [ 16]i sa nindivid-
ualfairness notionthatrequiresthattwo individuals withsimilar
non-protected attributes are treated equally.
Groupfairness requiresthestatisticsofMLoutcomesfordiffer-
entprotectedgroups tobesimilar[ 18].Therearemultiplemetrics
to measure group fairness in ML software. Among them, equal
opportunitydifference (EOD)measuresthedifferencebetweenthe
truepositiverates(TPR)oftwoprotectedgroups.Similarly, average
odd difference (AOD) is the average of differences between the true
positiverates(TPR)andthe falsepositiverates(FPR)oftwopro-
tected groups [ 4,8,39]. These metrics can naturally be generalized
tohandlesituationswhereprotectedattributesmayhavemorethan
two values. For instance, if race is a protected attribute, then the
EOD is the maximum EOD among any two race groups. This paper
focuses on group fairness.
Data-Driven Software Systems. Data-drivensoftwareisdistin-
guished from common software in that they largely learn their
decision logic from datasets. Consequently, while the traditional
softwaredevelopersexplicitlyencodedecisionlogicviacontroland
data structures, the ML programmers and users provide input data,
perform some pre-processing, choose ML algorithms, and tune hy-
perparameters to enable data-driven systems to infer a model that
encodes the decision logic.
Figure1showsthekeycomponentsofadata-drivensystem.Ata
high-level,adata-drivensystemconsistsofthreemajorcomponents:
inputdata,alearning(training)process,andalibraryframework.
TheMLusersoftenprovideinputdataandbuildanMLmodelusing
a programming interface. The interface interacts with the core ML
library(e.g.,scikit-learn,TensorFlow,etc)andconstructsdifferent
instances of learning algorithms usinghyperparameters. Then, they
feed the training data into the constructed learning objects to infer
the parameters of an ML model.
As a part of the training process, ML users query the ML model
withthevalidationset toevaluatefunctionalmetricssuchaspredic-
tionaccuracyandnon-functionalmetricssuchasEODforgroup
fairness. At the heart of the learning process, tuning hyperparame-
tersisparticularlychallengingsincetheycannotbeestimatedfrom
the input data, and there is no analytical formula to calculate anappropriatevalue[
22].Wedistinguish algorithmparameters (i.e.,
hyperparameters) suchastolerance ofoptimization inSVMs,maxi-
mumfeaturestosearchinrandomforest,andminimumsamples
inleafnodesofdecisiontreesthatsetbeforetrainingfrom model
parameters that are inferred automatically after training such as
thesplitfeatureofdecisiontreenodes,theweightsofneuronsin
neural networks, and coefficients of support vector machines.
910
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. Fairness-aware Configuration of Machine Learning Libraries ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
Related Work. 1) Evaluating fairness of ML models. Themis [ 17]
presents a causal testing approach to measure group discrimina-
tion on the basis of protected attributes. Particularly, they measure
the difference between the fairness metric of two subgroups by
counterfactual queries; i.e., they sample inputs where the protected
attributesareAandcomparethefairnesstoacounterfactualsce-
nario where the protected attributes are set to B. Agarwal et al. [ 2]
present a black-box testing technique to detect individual discrimi-
nation: two people with similar features other than protected ones
receive different ML predictions. They approximate ML models
withdecisiontreesandusesymbolicexecutiontechniquesoverthe
tree structure to find discriminatory instances. Udeshi et al. [ 35]
present a two-step technique that first uniformly and randomly
searchtheinputdataspacetofindadiscriminatoryinstanceand
then locally perturb those instances to further generate biased test
cases. Adversarial discrimination finder [ 41] is an adversarial train-
ing method to generate individual discrimination instances in deep
neural networks. These works focus on testing individual ML mod-elsandimprovingtheirfairness.WefocusonMLlibrariesandstudy
how algorithm configurations impact fairness.
2)Inprocessingmethodsforbiasreduction. Abodyofworkconsiders
inprocessalgorithmstomitigatebiasesinMLpredictions.Adver-
sarialdebiasing[ 38]isatechniquebasedonadversariallearningto
infer a classifier that maximizes the prediction accuracy and simul-
taneouslyminimizesadversaries’capabilitiestoguesstheprotected
attributefromtheMLpredictions.Prejudiceremover[ 20]addsa
fairness-aware regularization term to the learning objective and
minimizestheaccuracyandfairnessloss.Thislineofworkrequiresthemodificationoflearningalgorithmseitherinthelossfunctionor
the parameter of ML models. Exponentiated gradient [ 1] is a meta-
learning algorithm to mitigate biases. The approach infers a family
of classifiers to maximize prediction accuracy subject to fairnessconstraints. Since this approach assumes black-box access to the
learningalgorithms,weevaluatetheeffectivenessof Parfait-ML
inmitigating biases with this baseline (see Subsection 6.7).
3)Combiningpre-processingandinprocessingbiasreductions. Fair-
way [8]usesatwostepmitigationapproach.Whilepre-processing,
thedatasetisdividedintoprivilegedandunprivilegedgroupswhere
the respective ML models train independently from one another.
Then, they compare the prediction outcomes for the same instance
tofindandremovediscriminatorysamples.Giventhepre-processed
dataset, the inprocess step uses a multi-objective optimization
(FLASH) [ 23] to find an algorithm configuration that maximizes
bothaccuracyandfairness.Theworkfocusesonusinghyperparam-eterstomitigatebiasesinasubsetofhyperparametersandalimited
number of algorithms. In particular, they require a careful selec-
tionofrelevanthyperparameters.Ourapproach,however,doesnotrequire a manual selection of hyperparameters. Instead, our experi-
ments show that the evolutionary search is effective in identifying
and exploiting fairness-relevant hyperparameters automatically. In
addition,ourapproach explainswhathyperparametersinfluence
fairness. Such explanatory models can also pinpoint whether some
configurations systematically influence fairness, which can be use-
fulforFairwaytocarefullyselectasubsetofhyperparametersinitssearch.Toshowtheeffectivenessof Parfait-MLinreducingbiases,
we compare our approach to Fairway [8, 10] (see Subsection 6.7).3 OVERVIEW
We use the example of random forest ensemble [ 30] to overview
howParfait-MLassistsMLdevelopersanduserstodiscover,ex-
plain, and mitigate fairness bugs by tuning the hyperparameters.
Dataset. AdultCensusIncome[ 13]isabinaryclassificationdataset
that predicts whether an individual has an income over 50 Ka year.
Thedatasethas48 ,842instancesand14attributes.Forthisoverview,
we start by considering sexas the protected attribute.
LearningAlgorithm. Randomforestensembleisametaestimator
thatfitsanumberofdecisiontreesandusestheaveragedoutcomes
of trees to predict labels. The ensemble method includes 18 param-
eters. Three parameters are boolean, two are categorical, six are
integer, and seven are real variables. Examples of these parameters
are the maximum depth of the tree, the number of estimator trees,
and the minimum impurity to split a node further.FairnessandAccuracyCriterion.
Werandomlydividethedataset
into4-foldsanduse75%ofthedatasetasthetrainingsetand25%as
the validation set. We measure both accuracy and fairness metrics
after training on ML models using the validation set. We report
the average odd difference ( AOD) as well as the equal opportunity
difference ( EOD), which were introduced in Background Section 2.
Our accuracy metric is standard: the fraction of correct predictions.
Test Cases. Our approach has three options for generating test
cases: independently random, black-box mutations,and gray-box
mutations. In this section, we use the black-box mutations (see
RQ2 in Section 6.5). We run the experiment 10 times, each for 4
hours(thedefaultnumberofrepetitionandtime-out).Weobtain
an average of 603 valid test cases over 10 runs. Since the default
parametersofrandomforestsachieveanaccuracyof84%,avalid
test case achieves similar or better accuracy. To allow finding a
fair configuration in cases where the default configurations are the
most accuratemodel, we tolerate 1% accuracy degradations.The
overall accuracy ofML models over the entirecorpus of test cases
variesfrom83%to85 .7%.Eachtestcaseincludesthevaluationof
18 algorithm parameters, accuracy, AOD, andEOD.
Magnitude of Biases. We report the magnitude of group biases
in the hyperparameter space of random forests. The minimum and
maximum AODare 5.8%(+/−0.6%)and 19.0%(+/−0.4%), respec-
tively. The values are the average of 10 runs, with 95% confidence
interval reported in the parenthesis, and higher values indicatestronger biases. Similarly, the minimum and maximum
EODare
4.8%(+/−0.4)and32.3%(+/−0.6).Theseresultsshowthatwithin
2.7%accuracymargins,therecanbeover13%and27%difference
inAODandEOD,respectively.Theseresultsindicatethatdifferent
configurationsofrandomforestscanindeedamplifyorsuppress
the biases from the input dataset. The details of relevant experi-
mentsfordifferentdatasetsandlearningalgorithmsarereported
in RQ1 (Section 6.4).
Explanationof Biases. Our next goal is to explain what configu-
rations of hyperparameters influence group-based fairness.
Clustering. We first partition generated test cases in the domain of
fairness (AOD) versus accuracy. Particularly, we apply the Spectral
clusteringalgorithmwherethenumberofpartitionsaresettothree.
Figure 2 (a) shows the three clusters identified from the generated
911
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Saeid Tizpaz-Niari,Ashish Kumar, Gang Tan, and Ashutosh Trivedi
Figure2:(a)Testcasesfor censuswithsexareclusteredintothree:y-axisisthe AODbiasandx-axisistheaccuracy;(b)Thetree
classifier explains that the max_features and the min_weight_fraction_leaf discriminate the three clusters; (c) Two clusters
forcensuswithrace; (d) The classifier for censuswithraceshows a similar explanation to censuswithsex.
test cases. Looking into the figure, we see that green and orange
clusters have similar accuracy; however, they have significantly
differentbiases( AOD).Additionally,theblueclusterachievesbetter
accuracy with a similar AODto the green cluster.
Tree Classifiers. Next, we use CART tree classifiers [ 6] to explain
the differences between the clusters in terms of algorithm parame-
ters as shown in Figure 2 (b). Each node in the tree shows a splitparameter, the number of samples reaching the node, and sam-ple distributions in different clusters. First, let us understand thedifferences between the green and orange clusters. The decisiontree shows that the
max_feature parameter distinguishes these
two clusters. While the values ‘auto’ and ‘None’ do not restrict
the number of features during training, ‘sqrt’ and ‘log2’ randomly
choose a subset of features according to the square root and thebase-2 logarithm of total features during training. This explana-tion validates findings from Zhang and Harman [
39] where they
reported that restricting the number of features strengthens the
biases in ML models. Another localized parameter is the minimum
sampletostopthegrowthandfitleafmodels.Thisdistinguishes
theorangeclusterfromthetwootherclusters.Intuitively,under-
privileged groups tend to have less representation in the dataset.
Since random forests assign predictions to the majority class in the
leaves,theytendtofavorprivilegedgroupswhenathresholdon
the minimum samples is set.
FeatureTransferability. Inanotherexperiment,weconsiderthe race
attribute as the protected using the censusdataset. Figure 2 (c)
shows the inputs generated for the race feature is clustered into
twogroups.TheexplanationtreeinFigure2(d)showsthatthemax
featureandminimumsamplesinleafsaretwoparametersinthe
treeregressorthatexplainthedifferenceinthe AODbiases,similar
to the case when sexis the protected attribute.
Dataset Transferability. We also study other datasets in ML fair-
ness literature including the German Credit Data (Credit) [ 14] (see
Section6.6).Fortherandomforest,ourfindingsarestableacross
different datasets and protected attributes: the minimum sample
weightsofleafnodesandmaximumfeaturesarethemostimportant
parameters to distinguish configurations with high and low biases.
TheseresultscanhelpMLdevelopersunderstandthefairnessim-
plications of different configuration options in their libraries.
Mitigation Technique. As we discussed previously, Parfait-ML
isalsousefultosuppressbiasesbypickinglow-biashyperparam-
eters. The details of experiments to show the mitigation aspectof Parfait-ML can be found in Section 6.7. For random forests,
Parfait-ML can mitigate the biases from an EODof 11.6% to 0.1%
withevenbetteraccuracycomparedtothedefaultconfigurationas
abaseline.TherearecasesthatParfait-MLalonecannotreduce
biasesinastatisticallysignificantway.Insuchcases,wefoundthat
combing Parfait-ML with existing approaches can significantly
reduce biases under certain conditions (see RQ4 in Section 6.7).
4 PROBLEM DEFINITION
The primary performance criteria for data-driven software is accu-
racy. However, the presence of fairness results in a multi-objective
optimizationproblem.Weproposeasearch-basedsolutiontoap-
proximatethecurveofPareto-dominanthyperparameterconfigu-
rationsandastatisticallearningmethodtosuccinctlyexplainwhat
hyperparameter distinguish high fairness from low fairness.
The ML Paradigm. Data-driven software systems often deploy
mature, off-the-shelf ML libraries to learn various models from
data. We can abstractly view a learning problem as the problem of
identifyingamapping M:X→Yfromaset Xofinputstoaset
Yof outputs by learning from a fixed dataset D={(xi,yi)}N
i=1so
thatMgeneralizes well to previously unseen situations.
The application interfaces of these ML libraries expose configu-
ration parameters—characterizing the set Hof hyperparameters—
that let the users define the hypothesis class for the learning tasks.
Thesehypothesisclassesthemselvesaredefinedoverasetofmodel
parameters Θhbased on the selected hyperparameter h∈H.
The ML programs sift through the given dataset Dto learn an
“optimal” value θ∈Θhand thus compute the learning model
Mh(θ|D):X→Yautomatically. When Dandθare clear from
the context, we write Mhfor the resulting model.
The fitness of a hyperparameter h∈His evaluated by com-
puting the accuracy (ratio of correctresults) ofthe model Mhon
a validation dataset D∗. We denote the accuracy of a model M
overD∗asACCM. The dataset D∗is typically distinct from D
but assumed to be sampled from the same distribution. Hence, the
key design challenge for the data-driven software engineering is a
searchproblem foroptimalconfigurationofthehyperparameters
maximizing the accuracy over D∗.
Fairness Notion. To pose fairness requirements over the learning
algorithms, we assume the access to two predicates. The predicate
π:X→{0,1}overtheinputvariablescharacterizingtheprotected
status of a data point x(e.g., race, sex, or age). Without loss of
912
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. Fairness-aware Configuration of Machine Learning Libraries ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
generality,weassumethereareonlytwoprotectedgroups:agroup
withπ(x)=0 and a group with π(x)=1. We also assume that the
predicate ϕ:Y→{0,1}over the output variables characterizes a
favorable outcome (e.g., low reoffend risk) with ϕ(y)=1.
GivenD∗andM:X→Y, we define true-positive rate (TPR)
and false-positive rate (FPR) for the protect group i∈{0,1}as
TPRM(i)=/barex/barex{(x,y)∈D∗:π(x)=i,ϕ(M(x))=1,ϕ(y)=1}/barex/barex
/barex/barex{(x,y)∈D
∗:π(x)=i}/barex/barex
FPRM(i)=/barex/barex{(x,y)∈D
∗:π(x)=i,ϕ(M(x))=1,ϕ(y)=0}/barex/barex
/barex/barex{(x,y)∈D
∗:π(x)=i}/barex/barex.
We use two prevalent notions of fairness [ 4,8,39]. Equal oppor-
tunity difference (EOD) of MagainstD∗between two groups is
EODM=/barex/barexTPRM(0)−TPRM(1)/barex/barex,
and average odd difference (AOD) is
AODM=|TPRM(0)−TPRM(1)|+|FPRM(0)−FPRM(1)|
2.
LetF∈/braceleftbig
EODM,AODM/bracerightbigbe some fixed fairness criterion. We
notice that a high value of Fimplies high bias (low fairness) and a
low value implies low bias (high fairness).
The key design challenge for social-critical data-driven soft-
waresystemsistosearchforfairness-(bias-)optimalconfiguration
h∈Hof hyperparameters maximizing the accuracy and mini-
mizing the bias Fof the resulting model Mh. A hyperparameter
h∈His Pareto-fairness-dominated by д∈Hif the model Mд
provides better accuracy and lower bias, i.e. ACCMh<ACCMд
andFMh>FMд. We say that a hyperparameter h∈His Pareto-
fairness-optimal if it is not fairness-dominated by any other hyper-
parameters.Similarly,we candefine Pareto-bias-domination( his
Pareto-bias-dominatedby дifACCMh<ACCMдandFMh<FMд)
andPareto-bias-optimalhyperparamters.AParetosetisagraphical
depiction of all of Pareto-optimal points. Since we are interested
inhyperparametersthatleadtoeitherlowbias(highfairness)or
high bias (low fairness), our goal is to compute Pareto sets for both
fairnessandbiasoptimalhyperparameters:wecallthissetatwined
Pareto set. Our goal is to compute a convenient approximation
ofthetwinedParetosetthatcanbeusedtoidentify,explain,and
exploit the hyperparameter space to improve fairness.
Definition 4.1 (hyperparameter Discovery and Debugging).
GivenanMLalgorithmandadatasetwithprotectedandfavorable
predicates, the hyperparameter discovery problem is to approxi-
matethetwinedParetoset(bothfairness-optimalandbias-optimalpoints).Givensuchapproximation,thefairnessdebuggingproblem
is to explain the difference between the hyperparameter character-
izing the high and low fairness with acceptable accuracy.
5 APPROACH
We proposedynamicsearchalgorithms todiscoverhyperparame-
tersthatcharacterizefairness-optimalandbias-optimalmodelsand
statistical debugging to localize what hyperparameters distinguish
fair models from biased ones.Algorithm 1: Parfait-ML:DetectingandExplaining
Fairness and Bias in parameters of ML algorithms.
Input:algorithm A, space of hyperparamters H, default
configuration hd, training dataset ( XT,yT), test
dataset (Xt,yt), protected attribute A, type of search
St, the margin ϵ, time-out T, num. clusters k.
Output:Test Cases I, Predicates Φ.
1model,path←run(A,hd,XT,yT,St)
2pred←infer(model,Xt)
3accuracy d,fairness d←metric(pred,yt,A)
4I.add(hd,accuracy d,fairness d,path)
5cur←time()
6while time() -cur<Tdo
7ifSt==“random” then
8 h←UniformlyRandom(H )
9else ifSt==“black-box” orSt==“gray-box” then
10 h/prime←choiceW(I.project(H ))
11 h←mutate(h/prime)
12model,path←run(A,h,XT,yT,St)
13pred←infer(model,Xt)
14accuracy ,fairness←metric(pred,yt,A)
15ifpromising( h,accuracy,ϵ,fairness,path,I)then
16 I.add(h,accuracy,fairness,path)
17label←spectralClust( I.project( accuracy,fairness),k)
18Φ←DTClassifier( I.project(H ),label)
Hyperparameter Discovery Problem .Thegridsearchisanex-
haustive method to approximate the Pareto curve, however, it suf-
fers from the curse of dimensionality. Randomized search may
alleviate this curse to some extent, however, given its blind nature
and the large spaceof parameters, it may fail to explore interesting
regions. Evolutionary algorithms (EA) guided by the promising
input seeds often explore extremeregions of the Pareto space and
are thus natural candidates for our search problem. While multi-
objectiveEAslookpromising,theyarenotoriouslyslow[ 23].For
example,NSGA-II[ 11]hasquadraticcomplexitytopickthenext
best candidate form the samples in the population. Instead, we pro-
pose a single objective EA with accuracy constraints. Algorithm 1
sketches our approach for detecting and explaining strengths of
discriminationsin the configuration space of ML libraries.
GeneralSearchAlgorithms. Therandomsearchalgorithmgenerates
testinputsuniformlyandindependentlyfromthedomainofparam-
etervariables.Theblack-boxsearchisanevolutionaryalgorithm
that selects andmutates inputs from itspopulation. The gray-box
evolutionarysearchusesthesamestrategyastheblack-boxsearch,
but is also guided by the code coverage of libraries’ internals.
Initial Seeds. Our approach starts with the default configuration
andrunsthelearningalgorithmoverthetrainingdatasettobuild
a machine learning model (line 1 of Algorithm 1). If the searchalgorithm is gray-box, the running of algorithm also returns the
pathcharacterizations.Apathcharacterizationis xorofhashvalues
obtainedfromprogramlinenumbersvisitedintherun.Then,we
use the machine learning model and the validation set to infer the
913
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Saeid Tizpaz-Niari,Ashish Kumar, Gang Tan, and Ashutosh Trivedi
predictions (line 2). We use the predictions, their ground-truths,
and protected attributes to measure the prediction accuracy and
the group fairness metrics such as EODandAOD(line 3).
InputSelections. Weaddthedefaultconfigurationanditsoutcometo
the population (line 4) and iteratively search to find configurations
that minimize and maximize biases given a threshold on the accu-
racy.Indoingso,weconsiderthetypeofsearchtogenerateinputs.Ifthesearchis“random”,werandomlyanduniformlysamplefrom
thedomainofconfigurationparameters(lines7to8).Otherwise,
wepickaninputfromthepopulationbasedonaweightedsampling
strategy that prefers more recent inputs: given a location i>0, the
probabilistic weight of sample iis2∗i
n∗(n+1)wherenis the size of
input population, assuming a higher location is more recent. Then,
werandomlychooseaparameterandapplymutationoperations
overitscurrentvaluetogenerateanewconfiguration(lines9to
11). We use standard mutation operations such as increasing/de-
creasing value by a unit. Given the new configuration, we perform
thetrainingandinferencestepstomeasurethepredictionaccuracy
and biases (lines 12 to 14).
Search Objective. Identifying promisingconfigurations isa critical
stepinouralgorithm.Weconsiderthecharacteristicsofthenew
configurationandcomparethemtothetestcorpus(line15).Wesay
a configuration is promising if no existing configuration in the test
corpusPareto-fairness-dominateorPareto-bias-dominate(based
onEODandAOD) the new configuration. Thus, we add promising
inputs to the test corpus (line 16). If the search type is gray-box,
we also consider the path characterization. If the path has not
been visited before and the corresponding configuration manifests
an accuracy equal to or better than the accuracy of the default
configuration within ϵ=1.0% margin, we add the configuration to
the population as well.
Fairness Debugging Problem. With the assumption that the hy-
perparameter space His given as a finite set of hyperparameter
variables H1×H2×...×Hm,wewishtoexplainthedependence
of theseindividual hyperparametervariables towardsfairness for
all Pareto-optimal hyperparameters as approximated in the test
corpusI. Our explanatory approach uses clustering in the domain
of fairness vs. accuracy to discover kclasses of hyperparameter
configurations in the test corpus I(line 17). Then, we use standard
decisiontreeclassifierstogeneratesuccinctandinterpretablepred-
icates over the hyperparameter variables (line 18). The resulting k
predicatesserveasanexplanatorymodeltounderstandbiasesin
the configuration of learning algorithms.
6 EXPERIMENT
We first pose research questions. Then, we elaborate on the case
studies,datasets,protectedattributes,ourtool,andenvironment.
Finally, we carefully examine and answer research questions.
RQ1Whatisthemagnitudeofbiasesinthe hyperparameter space
of ML algorithms?
RQ2Are mutation-based and code coverage-based evolutionary
algorithms effective to find interesting configurations?
RQ3Isstatisticaldebuggingusefultoexplainthebiasesinthe hy-
perparameter spaceofMLalgorithms?Aretheseparameters
consistent across different fairness applications?RQ4Is our approach effective to mitigate biases as compared to
the state-of-the-art inprocess technique?
All subjects, experimental results, and our tool are available on our
GitHub repository: https://github.com/Tizpaz/Parfait-ML.
6.1 Subjects
We consider 5 ML algorithms from the literature [2, 8, 17, 35]:
1) Logistic regression (LR) uses sigmoid functions to map input data
toareal-valueoutcomebetween0and1.Weuseanimplementa-
tionfromscikit-learn[ 29]thathas15parametersincludingthree
booleans, three categoricals, four integers, four reals, and onedic-
tionary.Exampleparametersarethenormofpenalization,prime
vs dual formulation, and tolerance of optimization.
2) Random forest (RF) is an ensemble method that fits a number of
decision trees and uses the averaged outcomes for predictions. We
refer to Overview Section 3 for further information.
3) Support vector machine (SVM) is a classifier that finds hyper-
planestoseparateclassesandmaximizesmarginsbetweenthem.
Thescikit-learnimplementationhas12parametersincludingtwo
booleans, three categoricals, three integers, three reals, and one
dictionary [31]. Examples are tolerance and regularization term.
4) Decision tree (DT) learns decision logic from input data in the
formofif-then-elsestatements.Weuseanimplementationthathas
13 parameters including three categoricals, three integers, six reals,
and one dictionary [ 27]. Example parameters are the minimum
samples in the node to split and maximum number of leaf nodes.
5)Discriminantanalysis(DA) fitsdatatoa Gaussianpriorofclass
labels. Then, it uses the posterior distributions to predict the class
of new data. We use an implementation that has 11 parameters
includingtwobooleans,onecategorical,oneinteger,fourreals,two
lists, and one function [28].
Wealsoconsiderfourdatasetswithdifferentprotectedattributes
anddefinesixtrainingtasksasshowninTable1,similartoprior
work [2,8,17]. Adult Census Income ( census)[13], German Credit
Data (credit)[14], Bank Marketing ( bank)[15], and COMPAS Soft-
ware(compas)[26]arebinaryclassificationtaskstopredictwhether
an individual has income over 50K, has a good credit history, is
likely to subscribe, and has a low reoffending risk, respectively.
6.2 Technical Details
Our tool has detection and explanation components. The detec-
tion component is equipped with three search algoirthms: random,
black-box mutations,and gray-box coverage.Thesearchalgorithms
are described in Approach Section 5. We implement these tech-
niques in Python where we use the XML parser library to define
theparametervariablesandtheirdomainsandTracelibrary[ 25]
to instrument programs for the code coverage [ 37]. We implement
theclustering usingSpectralalgorithm[ 36]and thetreeclassifier
using the CART algorithm [6] in scikit-learn [27].
6.3 Experimental Setup
We run all the experiments on a super-computing machine with
the Linux Red Hat 7 OS and an Intel Haswell 2 .5 GHz CPU with
24 cores (each with 128 GB of RAM). We use Python 3 .6 and scikit-
learnversion0 .22.1.Wesetthetimeoutforoursearchalgorithm
914
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. Fairness-aware Configuration of Machine Learning Libraries ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
Table 1: Datasets used in our experiments.
Dataset |Instances| |Features|Protected Groups Outcome Label
Group1 Group2 Label 1 Label 0
AdultCensus48,842 14Sex-Male Sex-FemaleHighIncome Low IncomeIncome Race-White Race-Non White
CompasSoftware7,214 28Sex-Male Sex-FemaleDidnot Reoffend ReoffendRace-Caucasian Race-Non Caucasian
GermanCredit 1,000 20 Sex-Male Sex-Female Good Credit Bad Credit
BankMarketing 45,211 17 Age-Young Age-Old Subscriber Non-subscriber
to 4 hours for all experiments unless otherwise specified. Addi-
tionally, each experiment has been repeated 10 times to account
for the randomness of search techniques. We averaged the results
andcalculatedthe95%confidenceintervalstoreportresults.The
difference between two means is statistically significant if theirconfidence intervals do not overlap [
3]. We split the dataset into
trainingdata(75%)andvalidationdata(25%).WetrainanMLmodel
with a given learning algorithm, its configuration, and the training
data. Finally,we report theaccuracy and fairnessmetrics over the
inferred ML model using the validation set. Any configurations
thatachievehigheraccuracythanthedefaultconfiguration(with
1% margins) are valid inputs.
6.4 Magnitude of Biases (RQ1)
One crucial research question in this paper is to understand the
magnitude of biases when tuning hyperparameters. Table 2 shows
the magnitude of biases observed for different learning algorithms
over a specific dataset and protected attribute. We consider the
inputs from all search algorithms and report the average as well as
95%confidenceintervals(intheparenthesis)ofdifferentmetrics.
The column Num.Inputsshows the number of valid test cases gen-
erated from the detection step.The column Accuracy ranдeshows
the range of accuracies observed from all generated configura-tions. The column
AODranдeshows the range of AODbiases for
allconfigurations; AODtop
minshowsthelowest AODbiasesforinputs
withintop 1%ofprediction accuracy; AODtop
maxshowsthe highest
biases for inputs within the top 1% of accuracy. For the example
ofDTwithcensusandrace,AODranдeshowsthe AODbiasesfor
configurationswithin79 .2%to84 .7%accuracy,whereas AODtop
min
showsthelowestbiaseswithin83 .7%to84 .7%accuracy.Thecol-
umnEODranдe,EODtop
min,andEODtop
maxshowtherangeofbiases
based on equal opportunity difference ( EOD) for all valid inputs,
thelowest EODbiasesforinputswithinthetop1%ofaccuracy,and
the highest biases for inputs within top 1% of accuracy.
The results show that the configuration of hyperparameters
indeed amplifies and suppresses ML biases. Within 1% of (top)
accuracy margins, a fairness-aware configuration can suppress the
group biases to below 1% for AOD/EOD, and a poor choice can
amplify the biases up to 23% for EODand up to 15% for AOD.
Answer RQ1: Tuningofhyperparameterssignificantlyaffects
fairness.Within1%ofaccuracymargins,afairness-awareconfig-
uration can reduce the EOD bias to below 1% and a poor choice
of configuration can amplify the EOD bias to 23%.6.5 Search Algorithms (RQ2)
Inthissection,wecomparetheresultsofthreesearchalgorithms
to understand which method is more effective in finding config-urations with low and high biases. Table 3 shows the number of
generatedvalid inputsper searchmethod,the absolutedifference
betweenthemaximum AODandtheminimum AOD,andtheabso-
lutedifferencebetweenthemaximum EODandtheminimum EOD.
The results show that there are multiple statistically significant
differenceamongthethreesearchstrategies.In4cases,therandom
strategy generates the lowest number of inputs. In 5 cases, the evo-
lutionary algorithms (both black-box andgray-box) outperforms
the random stratgey in finding configurations that characterize
significant EODandAODbiases.
The comparisonbetween black-boxand gray-boxevolutionary
algorithmsshowsthatthereisnostatisticallysignificantdifference
between them in generating configurations that lead to the lowest
andhighestbiases.Weconjecturethatcodecoverageindetecting
biases is not particularly useful since the biases are not introduced
as a result of mistakes in the code implementation, rather they are
results of unintentionally choosingpoor configurations of learning
algorithms by ML users or allowing poor configurations of algo-
rithms by ML library developers in fairness-sensitive applications.
InTable3,weobservethatthestatisticallysignificantdifferences
arerelevanttothedecisiontree(DT).Forthealgorithm,weprovidethetemporalprogressofthreesearchalgorithmsfordifferenttrain-
ing scenarios (see supplementary material for the rest). Figure 3
showsthemeanofmaximumbiases(soldline)andthe95%confi-
denceintervals(filledcolors)overthe4hourstestingcampaignsof
each search strategy. There is a statistically significant difference if
white spaces are present between the confidence intervals.
AnswerRQ2: Our experiments showthat mutation-based evo-
lutionary algorithms are more effective in generating configu-
rationsthatcharacterizelowandhighbiasconfigurations.We
did not find a statistically significant difference to support using
code coverage in fairness testing of learning libraries.
6.6 Statistical Learning for Explanations (RQ3)
We present a statistical learning approach to explain what configu-
rations distinguish low bias models from high bias ones. We use
clusteringtofinddifferentclassesofbiasesandtheCARTtreeclassi-fierstosynthesizepredicatefunctionsthatexplainwhatparameters
arecommoninthesameclusterandwhatparametersdistinguish
one cluster from another. Similar techniques have been used for
softwareperformancedebugging[ 34].Welimitthemaximumnum-
ber of clusters to 3 and prefer threeclusters over twoclusters if
915
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Saeid Tizpaz-Niari,Ashish Kumar, Gang Tan, and Ashutosh Trivedi
Table 2: The magnitude of biases in the parameters of ML algorithms based on AOD and EOD.
Algorithm Dataset Protected Num.Inputs AccuracyranдeAverage Odds Difference (AOD) EqualOpportunity Difference (EOD)
AODranдe AODtop
minAODtop
max EODranдe EODtop
minEODtop
max
LRCensus Sex 10,368(+/- 3,040) 79.6%(+/- 0.0%)-81.1% (+/- 0.0%) 0.3%(+/- 0.0)-12.4% (+/- 0.6%) 0.7% (+/- 0.0%) 12.0% (+/- 0.2%) 0.1%(+/- 0.0%)-23.0% (+/- 0.0%) 0.1% (+/- 0.1%) 23.0% (+/- 0.0%)
Census Race 7,146(+/- 1,699) 79.7%(+/- 0.0%)-81.1% (+/- 0.0%) 0.5%(+/- 0.2%)-15.1% (+/- 1.3%) 1.4% (+/- 0.2%) 11.4% (+/- 0.0%) 0.3%(+/- 0.2%)-21.0% (+/- 2.3%) 1.5% (+/- 0.2%) 15.7% (+/- 0.1%)
Credit Sex 28,180(+/- 9,887) 73.6%(+/- 0.0%)-77.2% (+/- 0.0%) 0.9%(+/- 0.0%)-13.2% (+/- 0.4%) 1.8% (+/- 0.2%) 8.3% (+/- 0.7%) 0.3%(+/- 0.1%)-24.6% (+/- 0.9%) 1.5% (+/- 0.7%) 14.5% (+/- 1.7%)
Bank Age 2,381(+/- 400) 88.1%(+/- 0.0%)-89.6% (+/- 0.0%) 0.1%(+/- 0.0%)-8.9% (+/- 0.1%) 0.1% (+/- 0.0%) 6.7% (+/- 0.0%) 0.0%(+/- 0.0%)-15.0% (+/- 0.1%) 0.0% (+/- 0.0%) 12.3% (+/- 0.0%)
Compas Sex 67,736(+/- 1,832) 96.0%(+/- 0.0%)-97.1% (+/- 0.0%) 1.6%(+/- 0.0%)-5.3% (+/- 0.2%) 1.6% (+/- 0.0%) 5.0% (+/- 0.2%) 0.0%(+/- 0.0%)-6.2% (+/- 0.5%) 0.0% (+/- 0.0%) 5.9% (+/- 0.5%)
Compas Race 66,228(+/- 3,169) 96.0%(+/- 0.0%)-97.1% (+/- 0.0%) 1.4%(+/- 0.0%)- 4.2% (+/- 0.1%) 1.4% (+/- 0.0%) 4.2% (+/- 0.1%) 0.0%(+/- 0.0%)-5.1% (+/- 0.2%) 0.0% (+/- 0.0%) 5.1% (+/- 0.2%)
RFCensus Sex 620(+/- 105) 83.0%(+/- 0.0%)-85.7% (+/- 0.0%) 5.5%(+/- 0.6%)-18.9% (+/- 0.1%) 7.0% (+/- 0.3%) 14.6% (+/- 0.3%) 4.8%(+/- 0.4%)-32.3% (+/- 0.3%) 7.5% (+/- 0.7%) 23.0% (+/- 0.7%)
Census Race 605(+/- 122) 83.0%(+/- 0.0%)-85.7% (+/- 0.0%) 3.2%(+/- 0.2%)-10.1% (+/- 0.3%) 3.6% (+/- 0.1%) 9.4% (+/- 0.2%) 4.5%(+/- 0.3%)-17.3% (+/- 0.5%) 4.8% (+/- 0.2%) 15.5% (+/- 0.3%)
Credit Sex 24,213(+/- 10,274) 73.2%(+/- 0.0%)-79.2% (+/- 0.2%) 0.1%(+/- 0.0%)-15.1% (+/- 0.2%) 2.5% (+/- 0.4%) 6.8% (+/- 1.2%) 0.0%(+/- 0.0%)-24.3% (+/- 0.7%) 1.9% (+/- 1.4%) 9.8% (+/- 2.0%)
Bank Age 348(+/- 66) 89.0%(+/- 0.0%)-90.2% (+/- 0.0%) 0.1%(+/- 0.0%)-3.1% (+/- 0.1%) 0.0% (+/- 0.0%) 3.0% (+/- 0.0%) 0.0%(+/- 0.0%)-5.5% (+/- 0.3%) 0.0% (+/- 0.0%) 5.3% (+/- 0.3%)
Compas Sex 23,975(+/- 2,931) 95.5%(+/- 0.0%)-97.1% (+/- 0.0%) 1.5%(+/- 0.0%)-5.1% (+/- 0.2%) 1.5% (+/- 0.0%) 4.5% (+/- 0.2%) 0.0%(+/- 0.0%)-7.1% (+/- 0.3%) 0.0% (+/- 0.0%) 5.8% (+/- 0.4%)
Compas Race 22,626(+/- 3,105) 95.5%(+/- 0.0%)-97.1% (+/- 0.0%) 1.5%(+/- 0.0%)-4.6% (+/- 0.2%) 1.5% (+/- 0.0%) 3.7% (+/- 0.2%) 0.0%(+/- 0.0%)-6.4% (+/- 0.3%) 0.0% (+/- 0.0%) 4.5% (+/- 0.3%)
SVMCensus Sex 5,573(+/- 496) 65.6%(+/- 0.1%)-81.3% (+/- 0.0%) 0.0%(+/- 0.0%)-32.6% (+/- 0.1%) 0.2% (+/- 0.1%) 13.3% (+/- 0.8%) 0.0%(+/- 0.0%)-29.5% (+/- 0.9%) 0.0% (+/- 0.0%) 17.7% (+/- 1.1%)
Census Race 4,595(+/- 583) 65.6%(+/- 0.1%)-81.3% (+/- 0.0%) 0.0%(+/- 0.0%)-30.6% (+/- 0.8%) 0.4% (+/- 0.0%) 12.4% (+/- 0.8%) 0.0%(+/- 0.0%)-37.8% (+/- 1.3%) 0.1% (+/- 0.0%) 17.1% (+/- 1.1%)
Credit Sex 96,226(+/- 1,042) 59.3%(+/- 5.7%)-76.5% (+/- 0.1%) 0.0%(+/- 0.0%)-17.5% (+/- 0.0%) 1.8% (+/- 0.4%) 9.4% (+/- 0.4%) 0.0%(+/- 0.0%)-24.8% (+/- 0.5%) 1.5% (+/- 0.9%) 16.3% (+/- 0.8%)
Bank Age 1,361(+/- 163) 88.6%(+/- 0.0%)-89.8% (+/- 0.0%) 0.0%(+/- 0.0%)-5.3% (+/- 0.4%) 0.0% (+/- 0.0%) 5.3% (+/- 0.4%) 0.0%(+/- 0.0%)-9.2% (+/- 0.5%) 0.0% (+/- 0.0%) 9.2% (+/- 0.5%)
Compas Sex 40,287(+/- 417) 96.1%(+/- 0.0%)-97.1% (+/- 0.0%) 1.6%(+/- 0.0%)-3.8% (+/- 0.1%) 1.6% (+/- 0.0%) 3.8% (+/- 0.1%) 0.0%(+/- 0.0%)-3.9% (+/- 0.2%) 0.0% (+/- 0.0%) 3.9% (+/- 0.2%)
Compas Race 40,391(+/- 540) 96.1%(+/- 0.0%)-97.1% (+/- 0.0%) 1.4%(+/- 0.0%)-3.0% (+/- 0.0%) 1.4% (+/- 0.0%) 2.9% (+/- 0.0%) 0.0%(+/- 0.0%)-2.9% (+/- 0.1%) 0.0% (+/- 0.0%) 2.8% (+/- 0.1%)
DTCensus Sex 4,949(+/- 1,288) 79.2%(+/- 0.0%)-84.9% (+/- 0.2%) 0.3%(+/- 0.0%)-32.1% (+/- 1.8%) 5.8% (+/- 0.6%) 13.2% (+/- 1.4%) 0.2%(+/- 0.1%)-50.2% (+/- 3.0%) 5.5% (+/- 0.9%) 18.1% (+/- 2.4%)
Census Race 2,901(+/- 1,365) 79.2%(+/- 0.0%)-84.7% (+/- 0.3%) 0.4%(+/- 0.1%)-23.4% (+/- 2.5%) 3.5% (+/- 0.7%) 10.4% (+/- 2.1%) 0.4%(+/- 0.1%)-38.1% (+/- 4.0%) 4.5% (+/- 1.1%) 16.8% (+/- 3.9%)
German Sex 77,395(+/- 28,652) 65.4%(+/- 0.1%)-76.4% (+/- 0.3%) 0.0%(+/- 0.0%)-30.1% (+/- 4.9%) 9.9% (+/- 0.4%) 10.3% (+/- 0.6%) 0.0%(+/- 0.0%)-47.5% (+/- 2.4%) 10.8% (+/- 2.6%) 12.1% (+/- 3.5%)
Bank Age 3,512(+/- 569) 87.1%(+/- 0.0%)-89.4% (+/- 0.2%) 0.0%(+/- 0.0%)-5.9% (+/- 1.0%) 0.2% (+/- 0.1%) 3.9% (+/- 0.8%) 0.0%(+/- 0.0%)-10.8% (+/- 1.8%) 0.2% (+/- 0.1%) 7.3% (+/- 1.6%)
Compas Sex 29,916(+/- 3,149) 92.8%(+/- 0.0%)-97.1% (+/- 0.0%) 0.5%(+/- 0.2%)-5.7% (+/- 0.3%) 0.8% (+/- 0.1%) 4.5% (+/- 0.7%) 0.0%(+/- 0.0%)-7.0% (+/- 0.5%) 0.0% (+/- 0.0%) 4.9% (+/- 1.4%)
Compas Race 29,961(+/- 3,2) 92.8%(+/- 0.0%)-97.1% (+/- 0.0%) 0.8%(+/- 0.1%)-4.8% (+/- 0.2%) 0.8% (+/- 0.1%) 2.4% (+/- 0.2%) 0.0%(+/- 0.0%)-6.0% (+/- 0.8%) 0.0% (+/- 0.0%) 1.6% (+/- 0.5%)
DACensus Sex 12,613(+/- 3867) 79.1%(+/- 0.0%)-80.2% (+/- 0.0%) 0.9%(+/- 0.0%)-14.8% (+/- 0.0%) 0.9% (+/- 0.0%) 11.1% (+/- 0.0%) 0.0%(+/- 0.0%)-24.0% (+/- 0.0%) 0.0% (+/- 0.0%) 13.7% (+/- 0.0%)
Census Race 7,427(+/- 1,375) 79.1%(+/- 0.0%)-80.1% (+/- 0.0%) 4.8%(+/- 0.0%)-15.1% (+/- 0.0%) 4.8% (+/- 0.0%) 15.0% (+/- 0.0%) 6.4%(+/- 0.1%)-21.1% (+/- 0.0%) 6.4% (+/- 0.0%) 20.9% (+/- 0.1%)
Credit Sex 62,917(+/- 12,349) 72.8%(+/- 0.0%)-77.6% (+/- 0.0%) 0.2%(+/- 0.0%)-17.7% (+/- 0.0%) 2.5% (+/- 0.0%) 13.1% (+/- 0.0%) 0.5%(+/- 0.1%)-22.8% (+/- 0.0%) 3.3% (+/- 0.0%) 20.0% (+/- 0.0%)
Bank Age 2,786(+/- 507) 81.1%(+/- 0.3%)-89.2% (+/- 0.0%) 0.2%(+/- 0.0%)-5.4% (+/- 0.0%) 0.3% (+/- 0.0%) 5.4% (+/- 0.0%) 0.1%(+/- 0.0%)-10.5% (+/- 0.0%) 0.4% (+/- 0.3%) 10.5% (+/- 0.0%)
Compas Sex 45,448(+/- 95) 96.1%(+/- 0.0%)-97.1% (+/- 0.0%) 1.6%(+/- 0.0%)-3.1% (+/- 0.0%) 1.6% (+/- 0.0%) 3.1% (+/- 0.0%) 0.0%(+/- 0.0%)-0.8% (+/- 0.0%) 0.0% (+/- 0.0%) 0.8% (+/- 0.0%)
Compas Race 45,173(+/- 746) 96.1%(+/- 0.0%)-97.1% (+/- 0.0%) 1.5%(+/- 0.0%)-3.1% (+/- 0.0%) 1.5% (+/- 0.0%) 3.1% (+/- 0.0%) 0.0%(+/- 0.0%)-0.5% (+/- 0.0%) 0.0% (+/- 0.0%) 0.5% (+/- 0.0%)
Table 3: The performance of different search strategies to find biases in ML libraries (discrepancies are highlighted by red).
Algorithm Dataset ProtectedNum.Inputs | AOD.max() - AOD.min() | | EOD.max() - EOD.min() |
Random Black-Box Gray-Box Random Black-Box Gray-Box Random Black-Box Gray-Box
LRCensus Sex 11,469(+/- 5,282) 10,915 (+/- 5,416) 12,763 (+/- 5,768) 12.2% (+/- 1.1%) 11.8% (+/- 0.4%) 12.4% (+/- 1.6%) 23.0% (+/- 0.0%) 23.0% (+/- 0.0%) 23.0% (+/- 0.0%)
Census Race 6,402 (+/- 2,602) 6,592 (+/- 2,327) 7,050 (+/- 2,551) 13.6% (+/- 2.4%) 14.2% (+/- 2.5%) 15.0% (+/- 2.6%) 19.4% (+/- 3.5%) 20.0% (+/- 3.5%) 21.5% (+/- 3.7%)
Credit Sex 34,217(+/- 13,821) 34,394 (+/- 13,878) 24,248 (+/-15,175) 12.7% (+/- 0.3%) 12.6% (+/- 0.5%) 12.1% (+/- 0.8%) 24.9% (+/- 1.1%) 25.1% (+/- 1.1%) 23.9% (+/- 1.4%)
Bank Age 2,201% (+/- 462) 2,267 (+/- 801) 2,676 (+/- 982) 8.9% (+/- 0.0%) 8.8% (+/- 0.3%) 8.8% (+/- 0.2%) 15.1% (+/- 0.0%) 14.9% (+/- 0.4%) 14.9% (+/- 0.4%)
Compas Sex 70,452(+/- 1,686) 70,737 (+/- 2,268) 62,020 (+/- 1,955) 3.8% (+/- 0.4%) 3.6% (+/- 0.2%) 3.8% (+/- 0.4%) 6.3% (+/- 1.1%) 5.9% (+/- 0.6%) 6.3% (+/- 1.1%)
Compas Race 70,068(+/- 1,680) 66,862 (+/- 9,320) 61,755 (+/- 3,012) 2.8% (+/- 0.1%) 2.8% (+/- 0.0%) 2.8% (+/- 0.1%) 5.2% (+/- 0.4%) 5.0% (+/- 0.0%) 5.2% (+/- 0.4%)
RFCensus Sex 623 (+/- 247) 603 (+/- 217) 694 (+/- 172) 13.3% (+/- 1.3%) 13.5% (+/- 1.5%) 13.4% (+/- 1.4%) 27.6% (+/- 1.2%) 27.5% (+/- 1.3%) 27.3% (+/- 1.4%)
Census Race 575 (+/- 218) 681 (+/- 232) 777 (+/- 653) 7.4% (+/- 0.6%) 7.0% (+/- 0.8%) 6.8% (+/- 0.5%) 13.5% (+/- 1.1%) 13.1% (+/- 1.6%) 12.2% (+/- 1.0%)
Credit Sex 41,737(+/- 15,717) 39,221 (+/- 12,974) 10,151 (+/- 3,458) 14.9% (+/- 0.3%) 15.2% (+/- 0.6%) 14.8% (+/- 0.6%) 24.5% (+/- 0.7%) 25.0% (+/- 0.8%) 23.9% (+/- 1.2%)
Bank Age 260 (+/- 133) 314 (+/- 102) 649 (+/- 445) 3.1% (+/- 0.2%) 3.0% (+/- 0.5%) 3.0% (+/- 0.3%) 5.4% (+/- 0.4%) 5.7% (+/- 0.9%) 5.4% (+/- 0.5%)
Compas Sex 28,930(+/- 803) 29,780 (+/- 958) 13,216 (+/- 1,050) 3.9% (+/- 0.3%) 3.7% (+/- 0.4%) 3.4% (+/- 0.2%) 7.6% (+/- 0.6%) 7.1% (+/- 0.7%) 6.6% (+/- 0.4%)
Compas Race 27,580(+/- 4,052) 27,873 (+/- 3,948) 12,426 (+/- 1,122) 3.2% (+/- 0.2%) 3.3% (+/- 0.2%) 2.9% (+/- 0.3%) 6.6% (+/- 0.5%) 6.7% (+/- 0.5%) 5.9% (+/- 0.7%)
SVMCensus Sex 95,543(+/- 57) 96,214.0 (+/- 5,610) 97,115 (+/- 1,385) 32.6%(+/- 0.2%) 32.6% (+/- 0.2%) 32.6% (+/- 0.2%) 30.5%(+/- 1.4%) 28.6% (+/- 2.5%) 29.9% (+/- 1.7%)
Census Race 40,311(+/- 502) 41,289 (+/- 1,262) 39,572 (+/- 869) 31.3% (+/- 1.6%) 29.5% (+/- 1.6%) 30.7% (+/- 1.2%) 39.3% (+/- 2.3%) 36.2% (+/- 2.6%) 38.0% (+/- 2.2%)
Credit Sex 5,710 (+/- 925) 4,388 (+/- 919) 6,149 (+/- 980) 18.2% (+/- 1.5%) 17.5% (+/- 0.0%) 17.5% (+/- 0.0%) 25.0% (+/- 0.9%) 24.8% (+/- 1.6%) 24.7% (+/- 1.1%)
Bank Age 1,467 (+/- 108) 1,359 (+/- 444) 1,083 (+/- 925) 5.6% (+/- 0.4%) 4.8% (+/- 0.6%) 4.7% (+/- 0.8%) 9.8% (+/- 0.5%) 8.5% (+/- 1.0%) 8.3% (+/- 1.3%)
Compas Sex 40,070(+/- 1,265) 40,355 (+/- 849) 40,401 (+/- 375) 2.3%(+/- 0.2%) 2.1% (+/- 0.1%) 2.3% (+/- 0.3%) 3.9%(+/- 0.2%) 3.7% (+/- 0.2%) 4.0% (+/- 0.4%)
Compas Race 3,911 (+/- 1,132) 5,430 (+/- 841) 4,426.0 (+/- 1,202.0) 1.7% (+/- 0.1%) 1.6% (+/- 0.0%) 1.6% (+/- 0.0%) 3.0% (+/- 0.1%) 2.9% (+/- 0.2%) 2.9% (+/- 0.2%)
DTCensus Sex 158 (+/- 92) 7,351 (+/- 1,243) 5,804 (+/- 2,031) 26.8% (+/- 2.3%) 32.8% (+/- 0.7%) 35.1% (+/- 3.4%) 40.6% (+/- 5.8%) 52.7% (+/- 2.0%) 55.4% (+/- 4.8%)
Census Race 125 (+/- 9) 6,645 (+/- 1,949) 5,094 (+/- 2,250) 18.0% (+/- 2.0%) 29.3% (+/- 1.2%) 25.7% (+/- 4.5%) 30.1% (+/- 3.5%) 47.1% (+/- 2.0%) 41.7% (+/- 7.5%)
Credit Sex 86,762(+/- 15,750) 86,588 (+/- 15,545.0) 72,522 (+/- 27,373) 34.4% (+/- 5.1%) 30.5% (+/- 3.7%) 30.8% (+/- 3.1%) 51.6% (+/- 2.2%) 48.2% (+/- 6.7%) 49.3% (+/- 4.3%)
Bank Age 3,322 (+/- 782) 3,447 (+/- 1,191) 3,689 (+/- 1,119) 3.8% (+/- 1.0%) 7.3% (+/- 1.5%) 6.9% (+/- 1.3%) 6.9% (+/- 2.0%) 13.7% (+/- 2.6%) 12.6% (+/- 2.6%)
Compas Sex 18,442(+/- 79) 36,142 (+/- 330) 34,607 (+/- 1,102) 4.1% (+/- 0.6%) 6.0% (+/- 0.3%) 5.5% (+/- 0.7%) 5.5% (+/- 0.6%) 8.1% (+/- 0.3%) 7.5% (+/- 0.8%)
Compas Race 18,512(+/- 117) 36,073 (+/- 517) 35,502 (+/- 858) 2.8% (+/- 0.5%) 4.7% (+/- 0.2%) 4.6% (+/- 0.3%) 3.9% (+/- 0.2%) 7.9% (+/- 0.9%) 6.8% (+/- 1.3%)
DACensus Sex 17,553(+/- 6,794) 15,054 (+/- 6,993) 12,784 (+/- 5,609) 13.9% (+/- 0.0%) 13.9% (+/- 0.0%) 13.9% (+/- 0.0%) 24.0% (+/- 0.0%) 24.0% (+/- 0.0%) 24.0% (+/- 0.0%)
Census Race 8,399 (+/- 2964) 7,816 (+/- 2,694) 7,051 (+/- 2408) 10.4% (+/- 0.0%) 10.4% (+/- 0.0%) 10.3% (+/- 0.1%) 14.7% (+/- 0.0%) 14.7% (+/- 0.0%) 14.7% (+/- 0.1%)
Credit Sex 67,283(+/- 21,948) 7,232 (+/- 22,043) 54,237 (+/- 27,776) 5.2% (+/- 0.0%) 5.2% (+/- 0.0%) 5.3% (+/- 0.1%) 10.4% (+/- 0.0%) 10.4% (+/- 0.0%) 10.4% (+/- 0.0%)
Bank Age 2,812 (+/- 663) 2,809 (+/- 978) 2,878 (+/- 929) 17.5% (+/- 0.0%) 17.5% (+/- 0.0%) 17.5% (+/- 0.0%) 22.4% (+/- 0.0%) 22.4% (+/- 0.0%) 22.4% (+/- 0.0%)
Compas Sex 45,489(+/- 94) 45,449 (+/- 175) 45,406 (+/- 257) 1.5% (+/- 0.0%) 1.5% (+/- 0.0%) 1.5% (+/- 0.0%) 0.8% (+/- 0.0%) 0.8% (+/- 0.0%) 0.8% (+/- 0.0%)
Compas Race 44,436(+/- 2432) 45,603 (+/- 300) 45,480 (+/- 339) 1.6% (+/- 0.0%) 1.6% (+/- 0.0%) 1.6% (+/- 0.0%) 0.5% (+/- 0.0%) 0.5% (+/- 0.0%) 0.5% (+/- 0.0%)
Figure 3: The temporal progress of search strategies for the decision tree algorithm over 4training tasks. X-axis is the times-
tamp of search from 0s to 14000s (4 hrs) and Y-axis is the group fairness metric ( AOD).Coveraдe refers to gray-box method.
and only if the corresponding classifier achieves better accuracy.
WealsolimitthedepthofCARTclassifiersto3inordertogeneratesuccinct decision trees. We first present the explanatory models
of different algorithms over each individual training scenario (e.g,
916
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. Fairness-aware Configuration of Machine Learning Libraries ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
censusdataset with sex). Next, we perform an aggregated analysis
of learning algorithms over the 6 different training datasets, the
3 random search algorithms, and the 10 repeated runs to extract
what hyperparameters are frequently appearing in the explanatory
models and thus suspicious of influencing fairness systematically.
Individualtrainingscenario .Weshowhowourstatisticallearn-
ing approach helps localize hyperparameters that influence the
biasesforeachindividualtrainingtask.Figure4showstheexplana-
torymodels(clusteringandCARTtree)foreachlearningalgorithm
over the censusdataset with sex(except for random forest that
was presented in the overview Section 3). For example, Figure 4 (b)
showsthetrueevaluationof“ solver!=sag ∧fit-intercept>0.5
∧solver=newton-cg ”forthehyperparametersoflogisticregres-
sion,whichexplainstheorangecluster,leadstostrongerbiases.All
models are available in the supplementary material.
Miningoveralltrainingscenarios .Foreachlearningalgorithm,
our goal is to understand what hyperparameters influence the fair-
ness in multiple training scenarios and establish whether some
hyperparameters systematically influence fairness beyond a spe-
cific training task. Overall, we analyze 180 CART trees and report
hyperparameters that appear as a node in the tree more than 50times overall and more than 3 times uniquely in the 4 datasets.
Different values of these frequent hyperparameters are suspicious
ofintroducingbiases,acrossdatasets,searchalgorithms,anddiffer-
ent protected attributes. In the following, within the parenthesis
rightafterthenameofahyperparameter,wereport(1)thenumber
ofexplanatorymodels(outof180)wherethehyperparameterap-
pearsand(2)thenumberofdatasets(outof4)forwhichthereisanexperimentwhoseexplanatorymodelcontainsthehyperparameter.
A) Logistic regression (LR) : The computation time for inferring clus-
tersandtreeclassifiersis77 .9(s)intheworstcase.Theaccuracy
of classifiers is between 90 .9% and 96 .8%. Three frequent hyperpa-
rametersbasedonthepredicatesinclassifiersare solver(175,4),
tol(53,3), and fit-intercept (50,3). Our analysis shows that the
solversaдafrequently achieves low biases after tuning the toler-
anceparameterwhereasthesolver newton-cдoftenachieveslow
biases if the intercept term is added to the decision function.
B) Random forest (RF) : The computation time for inferring models
is 75.7 (s) in the worst case. The accuracy is between 80 .5% and
100.0%. Two frequent parameters are max_features (170,4) and
min_weight_fraction_leaf (160,4).Theseparametersandtheir
connections to fairness are explained in the overview section 3.
C)Supportvectormachine(SVM) :Thecomputationtimeforinfer-
ringmodelsis79 .9(s)intheworstcase.Theaccuracyisbetween
83.9%and98 .3%.Theonly(relatively)frequentparameteris degree
(53,3). Thisshows the high variationof parameter appearancesin
the explanation model. Thus, the configuration of SVM might not
systematically amplify or suppress biases; the influence of configu-
ration on biases largely depends on the specific training task.
D)Decisiontree(DT) :Thecomputationtimeforinferringmodels
is 76.9 (s) in the worst case. The accuracy is between 93 .8% and
98.1%. The frequent parameters are min_fraction_leaf (114,4)
andmax_features (114,4).Similartorandomforest,theminimum
required samples in the leaves and the search space of dataset
attributes during training impact fairness systematically.E) Discriminant analysis(DA) :The computationtime forinferring
models is 77 .8 (s) in the worst case. The accuracy is between 54 .2%
and 93.8%. However, if we allowed a higher depth for the classifier
(morethan3),itisabove90%inallcases.Thefrequentparameter
istol(141,4).However,theexactconditionoverthetolerancein
theexplanatorymodelsignificantlydependsonthetrainingtask,
and might not influence fairness systematically.
MLlibrarymaintainerscanusetheseresultstounderstandthe
fairness implications of their library configurations.
Answer RQ3: We found the statistical learning scalable and
useful to explain and distinguish the configuration with low
andhighbiases.Ourglobalanalysisof 180explanatorymodels
per learning algorithm reveals that some algorithms and their
configurationscan systematically amplify or suppress biases.
6.7 Bias Mitigation Algorithms (RQ4)
We show how Parfait-ML can be used as a mitigation tool to
aid ML users pick a configuration of algorithms with the lowest
discrimination.Indoingso,werunParfait-MLforashortamount
oftimeandpickaconfigurationofhyperparameterswiththelowest
AODandEOD.Toshowtheeffectiveness,wecompareParfait-ML
to the state-of-the-art techniques [ 1,8,10]. We say approach (1)
outperformsapproach(2)ifitachievesstatisticallysignificantlower
biases within a similar or higher accuracy.
A)Exponentiatedgradient [1]presentsabiasreductiontechnique
thatmaximizesthepredictionaccuracysubjecttolinearconstraints
onthegroupfairnessrequirements.TheyuseLagrangemethods
and apply the exponentiated gradient search to find Lagrange mul-
tipliers to balance accuracy and fairness. In doing so, they usemeta-learning algorithms to learn a family of classifiers (one ineach step of the algorithm with a fixed Lagrange multiplier) and
assignaprobabilisticweighttoeachofthem.Inthepredictionstage,
theapproach chooses oneclassifierfromthefamily ofclassifiers
stochastically according to their weights. We choose this approach
forafewreasons:1)theapproachisan inprocess algorithmanddoes
not add or remove input data samples in the pre-processing step
normodifiespredictionlabelsinthepost-processingstep;2)they
assumeblack-boxaccesstoMLalgorithms;thustheydonotmodify
the learning objective nor model parameters. We note that their
approach is sensitive to the fairness metric (to construct the linear
constraints)anddoesnotsupportarbitrarymetrics.Inparticular,
they support the EODmetric, but not the AODmetric. Thus, we
focus on the EODmetric in this experiment. In addition, since the
discriminant analysis algorithm does not support meta-learning,
we exclude this algorithm from this experiment.
We consider the default configuration of algorithms without
any fairness consideration, the exponentiated gradient method [ 1],
Parfait-ML,andexponentiatedgradientcombinedwithParfait-
ML. We also set the execution time of Parfait-ML to 6 minutes in
accordance with the (max) execution time of gradient approach in
ourenvironment.Table4showstheresultsoftheseexperiments.
Comparedtothedefault configuration, in18casesoutof24experi-
ments,theexponentiatedgradientsignificantlyreducesthe EOD
biases.However,in11casesoutof24experiments,exponentiated
917
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Saeid Tizpaz-Niari,Ashish Kumar, Gang Tan, and Ashutosh Trivedi
(a) Logistic Regression Cluters.
 (b)LogisticRegressionExplained.
 (c) SVM Clusters.
(d) SVM Explained.
(e) Decision Tree Clusters.
 (f) Decision Tree Explained.
 (g) Discriminant Clusters.
 (h) Discriminant Explained.
Figure4:Thetestinputsovercensusdatasetwithsexastheprotectedattributeare(1)clusteredintotwogroupsinthedomain
of fairness and accuracy (2) explained to understand which parameters distinguish low and high fairness outcomes.
Table 4: Parfait-ML as a bias mitigation technique compared to Exp. Gradient [1] within 6 mins.
Algorithm Dataset ProtectedDefault Configuration Exp. Gradient [1] Parfait-ML Parfait-ML +Exp. Gradient [1]
Accuracy EOD Accuracy EOD Accuracy EOD Accuracy EOD
LRCensus Sex 80.5% (+/- 0.0%) 9.7% (+/- 0.1%) 80.5% (+/- 0.1%) 0.8% (+/- 0.3%) 80.2% (+/- 0.3%) 0.1% (+/- 0.0%) 80.0% (+/- 0.5%) 0.2% (+/- 0.1%)
Census Race 80.5% (+/- 0.0%) 9.9% (+/- 0.0%) 79.6% (+/- 0.1%) 3.5% (+/- 2.2%) 80.2% (+/- 0.3%) 1.1% (+/- 1.0%) 80.0% (+/- 0.5%) 1.3% (+/- 0.9%)
Credit Sex 74.4% (+/- 0.0%) 17.1% (+/- 0.0) 74.5% (+/- 0.6%) 25% (+/- 1.9%) 75.2% (+/- 0.8%) 0.6% (+/- 0.5%) 74.3% (+/- 0.5%) 1.6% (+/- 1.2%)
Bank Age 89.0% (+/- 0.0%) 8.0% (+/- 0.0%) 86.7% (+/- 0.1%) 2.4% (+/- 1.6%) 88.6% (+/- 0.2%) 0.0% (+/- 0.0%) 88.2% (+/- 0.4%) 0.8% (+/- 0.8%)
Compas Sex 97.0% (+/- 0.0%) 1.6% (+/- 0.0%) 96.9% (+/- 0.1%) 0.0% (+/- 0.0%) 97.1% (+/- 0.0%) 0.0% (+/- 0.0%) 97.1% (+/- 0.0%) 0.0% (+/- 0.0%)
Compas Race 97.0% (+/- 0.0%) 0.3% (+/- 0.0%) 96.9% (+/- 0.1%) 0.0% (+/- 0.0%) 97.1% (+/- 0.0%) 0.0% (+/- 0.0%) 97.1% (+/- 0.0%) 0.0% (+/- 0.0%)
RFCensus Sex 84.0% (+/- 0.0%) 5.4% (+/- 0.0%) 79.0% (+/- 0.0%) 5.4% (+/- 0.0%) 84.0% (+/- 0.4%) 5.0% (+/- 1.0%) 79.4% (+/- 0.0%) 0.1% (+/- 0.0%)
Census Race 84.0% (+/- 0.0%) 8.6% (+/- 0.0%) 79.8% (+/- 0.0%) 0.3% (+/- 0.0%) 84.7% (+/- 0.6%) 4.5% (+/- 0.5%) 79.8% (+/- 0.0%) 0.3% (+/- 0.0%)
Credit Sex 74.0% (+/- 0.0%) 11.6% (+/- 0.0%) 70.4% (+/- 0.0%) 8.3% (+/- 0.0%) 78.1% (+/- 0.4%) 0.1% (+/- 0.0%) 70.8% (+/- 0.0%) 0.5% (+/- 0.0%)
Bank Age 89.9% (+/- 0.0%) 1.2% (+/- 0.0%) 79.0% (+/- 0.3%) 3.6% (+/- 1.6%) 89.9% (+/- 0.2%) 0.1% (+/- 0.1%) 83.1% (+/- 0.8%) 0.7% (+/- 0.7%)
Compas Sex 96.5% (+/- 0.0%) 2.3% (+/- 0.0%) 93.6% (+/- 0.0%) 1.5% (+/- 0.0%) 97.1% (+/- 0.0%) 0.0% (+/- 0.0%) 97.1% (+/- 0.0%) 0.0% (+/- 0.0%)
Compas Race 96.5% (+/- 0.0%) 2.1% (+/- 0.0%) 93.7% (+/- 0.0%) 1.1% (+/- 0.0%) 97.1% (+/- 0.0%) 0.0% (+/- 0.0%) 97.1% (+/- 0.0%) 0.0% (+/- 0.0%)
SVMCensus Sex 66.5% (+/- 0.0%) 18.5% (+/- 0.0%) 79.9% (+/- 0.0%) 0.7% (+/- 0.2%) 79.1% (+/- 0.7%) 0.0% (+/- 0.0%) 80.1% (+/- 0.4%) 0.2% (+/- 0.1%)
Census Race 72.5% (+/- 0.0%) 4.5% (+/- 0.0%) 72.5% (+/- 0.4%) 3.8% (+/- 1.8%) 78.8% (+/- 2.2%) 0.0% (+/- 0.0%) 76.3% (+/- 1.7%) 0.3% (+/- 0.3%)
Credit Sex 62.8% (+/- 0.0%) 22.3% (+/- 0.3%) 64.6% (+/- 1.7%) 10.4% (+/- 0.0%) 70.4% (+/- 0.0%) 0.0% (+/- 0.0%) 70.4% (+/- 0.0%) 0.0% (+/- 0.0%)
Bank Age 89.9% (+/- 0.0%) 1.2% (+/- 0.0%) 79.0% (+/- 0.3%) 3.6% (+/- 1.6%) 89.2% (+/- 0.3%) 0.0% (+/- 0.0%) 88.3% (+/- 0.1%) 0.0% (+/- 0.0%)
Compas Sex 96.5% (+/- 0.0%) 0.0% (+/- 0.0%) 93.6% (+/- 0.0%) 0.0% (+/- 0.0%) 97.1% (+/- 0.0%) 0.0% (+/- 0.0%) 97.1% (+/- 0.0%) 0.0% (+/- 0.0%)
Compas Race 96.5% (+/- 0.0%) 0.0% (+/- 0.0%) 93.7% (+/- 0.0%) 0.0% (+/- 0.0%) 97.1% (+/- 0.0%) 0.0% (+/- 0.0%) 97.1% (+/- 0.0%) 0.0% (+/- 0.0%)
DTCensus Sex 80.2% (+/- 0.0%) 3.3% (+/- 0.0%) 82.6% (+/- 0.1%) 2.1% (+/- 0.6%) 79.9% (+/- 0.8%) 0.2% (+/- 0.2%) 82.7% (+/- 0.5%) 0.9% (+/- 1.0%)
Census Race 80.2% (+/- 0.0%) 7.2% (+/- 0.0%) 83.0% (+/- 0.1%) 7.5% (+/- 1.6%) 80.7% (+/- 0.9%) 0.2% (+/- 0.1%) 83.2% (+/- 0.5%) 1.7% (+/- 1.0%)
Credit Sex 66.0% (+/- 0.0%) 13.4% (+/- 0.0%) 70.0% (+/- 0.3%) 14.9% (+/- 1.8%) 70.4% (+/- 0.0%) 0.0% (+/- 0.0%) 70.4% (+/- 0.0%) 0.0% (+/- 0.0%)
Bank Age 87.1% (+/- 0.0%) 4.8% (+/- 0.0%) 88.0% (+/- 0.1%) 3.7% (+/- 1.7%) 88.4% (+/- 0.3%) 0.0% (+/- 0.0%) 88.3% (+/- 0.0%) 0.0% (+/- 0.0%)
Compas Sex 93.8% (+/- 0.0%) 5.2% (+/- 0.0%) 95.9% (+/- 0.0%) 2.3% (+/- 0.0%) 97.1% (+/- 0.0%) 0.0% (+/- 0.0%) 97.1% (+/- 0.0%) 0.0% (+/- 0.0%)
Compas Race 93.8% (+/- 0.0%) 3.8% (+/- 0.0%) 96.0% (+/- 0.0%) 1.3% (+/- 0.0%) 97.1% (+/- 0.0%) 0.0% (+/- 0.0%) 97.0% (+/- 0.1%) 0.0% (+/- 0.0%)
gradient degraded the prediction accuracy. Parfait-ML reduces
theEODbiasesin23caseswith12casesofaccuracyimprovements
andonlyonecaseofaccuracydegradations.Overall,Parfait-ML
significantlyoutperformsthegradientmethod(discrepanciesare
highlighted with red font in Table 4). Combining Parfait-ML and
exponentiated gradient performs better than each technique in
isolation (see RFwithcensusandsex) given that the gradient tech-
niquedoesnotincreasethestrengthofbiasesinisolation(see LR
withcreditandsex). In such cases, Parfait-ML alone results in
lower biases and higher accuracy.
B)Fairway[ 8,10]usesamutli-objectiveoptimizationtechnique
known as FLASH [ 23] to tune hyperparameters and chooses a con-
figuration that achieves less biases with a minimum accuracy loss.We use their implementatio [ 9], and compare our search-based
technique to this method. Fairway generally supports integer and
booleanhyperparameters. Therefore,weuse a subset ofconfigu-rations as specified and reported for logistic regression (LR) [
8]
and decision tree (DT) [ 10]. For a fair comparison, we calculate the
executiontimeof Fairwayforeachexperimentinourenvironment
andlimittheexecutiontimeof Parfait-MLaccordingly.Table5shows the comparison results. Overall, there are 3 discrepancies
inAODand 5discrepanciesinEOD(noted byredfontinTable5).
Parfait-ML outperforms Fairway in 6 cases whereas Fairway
outperformsParfait-MLin2cases.Wealsonotedthatthecurrent
implementationsof FLASHcarefullyselected4hyperparameters
forLRandDT.Whenweincludethreemorehyperparameterswith
integer or boolean types (e.g., dual and fit_intercept in LR), we
918
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. Fairness-aware Configuration of Machine Learning Libraries ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
Table 5: Parfait-ML in comparison to Fairway (FLASH) [8, 10].
Alg.ScenarioTime FLASH Parfait-ML
(s)Accuracy AOD EOD Accuracy AOD EOD
LRCensus, Sex 40.3 80.5% (+/- 0.1%) 2.0% (+/- 0.1%) 0.2% (+/- 0.3%) 80.9% (+/- 0.2%) 2.7% (+/- 1.0%) 4.3% (+/- 1.3%)
Census,Race 65.2 80.3% (+/- 0.0%) 6.2% (+/- 0.4%) 8.0% (+/- 0.6%) 80.9% (+/- 0.0%) 4.2% (+/- 0.8%) 5.8% (+/- 1.4%)
Credit, Sex 2.8 70.4% (+/- 0.0%) 0.0% (+/- 0.0%) 0.0% (+/- 0.0%) 76.1% (+/- 0.0%) 3.5% (+/- 0.0%) 5.5% (+/- 0.0%)
Bank, Age 129.0 90.5% (+/- 0.0%) 0.8% (+/- 0.1%) 1.1% (+/- 0.2%) 89.6% (+/- 0.0%) 0.4% (+/- 0.2%) 0.4% (+/- 0.0%)
Compas,Sex 18.3 97.1% (+/- 0.0%) 1.6% (+/- 0.0%) 0.0% (+/- 0.0%) 97.1% (+/- 0.0%) 1.6% (+/- 0.0%) 0.0% (+/- 0.0%)
Compas,Race 9.5 97.1% (+/- 0.0%) 1.5% (+/- 0.0%) 0.0% (+/- 0.0%) 97.1% (+/- 0.0%) 1.5% (+/- 0.0%) 0.0% (+/- 0.0%)
DTCensus, Sex 25.8 82.5% (+/- 0.8%) 7.9% (+/- 2.2%) 9.9% (+/- 3.7%) 83.6% (+/- 0.8%) 4.8% (+/- 1.1%) 2.2% (+/- 0.5%)
Census,Race 60.5 83.9%(+/- 0.7%) 3.2% (+/- 1.0%) 4.3% (+/- 1.8%) 84.1%(+/- 0.8%) 3.1% (+/- 1.1%) 4.4% (+/- 1.6%)
Credit, Sex 2.5 71.5% (+/- 1.3%) 5.4% (+/- 2.2%) 7.7% (+/- 4.2%) 71.1% (+/- 0.0%) 0.0% (+/- 0.0%) 0.0% (+/- 0.0%)
Bank, Age 124.8 90.9% (+/- 0.1%) 0.7% (+/- 0.4%) 0.8% (+/- 0.8%) 88.9% (+/- 0.4%) 0.0% (+/- 0.0%) 5.6% (+/- 0.0%)
Compas,Sex 5.9 97.1% (+/- 0.0%) 1.6% (+/- 0.0%) 0.0% (+/- 0.0%) 97.1% (+/- 0.0%) 1.5% (+/- 0.2%) 0.0% (+/- 0.0%)
Compas,Race 11.4 97.1% (+/- 0.0%) 1.5% (+/- 0.0%) 0.0% (+/- 0.0%) 97.1% (+/- 0.0%) 1.5% (+/- 0.0%) 0.0% (+/- 0.0%)
observethattheperformanceof Fairwaysignificantlydegraded.
ForLRalgorithm over compaswithsexscenario, the prediction
accuracy is decreased to 89 .7%(+/−6.0%), while the AOD and EOD
bias metrics are increased to 2 .9%(+/−1.0%)and 3.1%(+/−2.3%),
respectively. Since Fairway is sensitive to the domain of variables,
Parfait-MLcancomplementitwiththeexplanatorymodeltocare-
fully choose hyperparameters to include in the Fairway search.
Answer RQ4: Parfait-ML is effective to improve fairness by
findinglow-biasconfigurationsofhyperparameters.Itoutper-
formsexponentiatedgradient[ 1]andFairway[ 8,10]inreducing
AODandEODbiaseswithequalorbetteraccuracy.Parfait-ML
can complement both approaches to improve fairness.
7 DISCUSSION
Limitation. The input dataset is arguably the main source of dis-
criminations in data-driven software. In this work, we vary the
configurationoflearningalgorithmsandfixtheinputdatasetsince
ourapproachistosystematicallystudytheinfluenceofhyperpa-
rametersinfairness.Whilewefoundthatconfigurationscanreduce
biasesin variousalgorithms, ourapproach alonecannot eliminate
fairness bugs. Our approach also requires a diverse set of inputs
generatedautomaticallyusingthesearchalgorithms.Asadynamic
analysis,ourapproachsolelyreliesonheuristicstogenerateadi-
verse set of configurations and is not guaranteed to always find
interesting hyperparameters in a given time limit. In addition, we
only use two group fairness metrics ( AODandEOD) and the over-
all prediction accuracy. One limitation is that these metrics do not
consider the distribution of different groups. In general, coming up
with a suitable fairness definition is an open challenge.
ThreattoValidity.Toaddresstheinternalvalidityandensureour
findingdoesnotleadtoinvalidconclusion,wefollowestablished
guideline[ 3]wherewerepeateveryexperiment10times,reportthe
averagewith95%confidenceintervals(CI),andconsidernotonly
the final result but also the temporal progresses. We note that 95%
non-overlappingCIisaconservativestatisticalmethodtocompare
results. Instead, non-parametric methods and effect sizes can be
usedtoalleviatetheconservativenessofourcomparisons.Inourex-periments,wedidnotfindsignificantimprovementsusingcoverage
metrics.However, thismightbearesult ofourspecificimplemen-
tations and/or the feedback criteria. To ensure that our results aregeneralizable and address external validity, we perform our exper-
iments on five learning algorithms from scikit-learn library oversix fairness-sensitive applications that have been widely used in
the fairness literature. However, it is an open problem whether the
library, algorithms, and applications are sufficiently representative
to cover challenging fairness scenarios.
UsageVision.Parfait-MLcomplementstheworkflowofstandard
testing procedures against functionality and performance by en-
abling ML library maintainers to detect and debug fairness bugs.
Parfait-MLcombinessearch-basedsoftwaretestingwithstatisti-
cal debugging to identify and explain hyperparameters that leadto high and low bias classifiers within acceptable accuracy. Like
standardMLcodetesting,Parfait-MLrequiresasetofreference
fairness-sensitivedatasets.Iftheexplanatorymodelsareconsistent
across these datasets, Parfait-ML synthesizes this information to
pinpointdataset-agnosticfairnessbugs.Ifsuchbugsarediscovered,
MLlibrarymaintainerscaneitherexcludethoseoptionsorwarn
users to avoid setting them for fairness-sensitive applications.
8 CONCLUSION
Softwaredevelopersincreasinglyemploymachinelearninglibrariesto design data-driven social-critical applications that demand a del-
icatebalancebetweenaccuracyandfairness.The“programming”
task in designing such systems involves carefully selecting hyper-
parametersfortheselibraries,oftenresolvedbyrules-of-thumb.Weproposeasearch-basedsoftwareengineeringapproachtoexploring
the space of hyperparameters to approximate the twined Paretocurves expressing both high and low fairness against accuracy.
Hyperparameterconfigurationswithhighfairnesshelpsoftware
engineersmitigatebias,whileconfigurationswithlowfairnesshelp
MLdevelopersunderstandanddocumentpotentiallyunfaircom-
binationsofhyperparameters.Therearemultipleexcitingfuture
directions. For example, extending our methodology to support
deep learning frameworks is an interesting future work.
ACKNOWLEDGMENTS
The authors thank the anonymous reviewers for their time and
invaluable feedback to improve this paper. This work utilized re-
sourcesfromtheCUBoulderResearchComputingGroup,whichissupportedbyNSF,CUBoulder,andCSU.Tizpaz-Niariwaspartially
supported by NSF under grant DGE-2043250 and UTEP College of
Engineering under startup package.
919
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Saeid Tizpaz-Niari,Ashish Kumar, Gang Tan, and Ashutosh Trivedi
REFERENCES
[1]Alekh Agarwal, Alina Beygelzimer, Miroslav Dudík, John Langford, and Hanna
Wallach. 2018. A reductions approach to fair classification. In International
Conference on Machine Learning. PMLR, 60–69.
[2]Aniya Aggarwal, Pranay Lohia, Seema Nagar, Kuntal Dey, and Diptikalyan Saha.
2019. Black Box Fairness Testing of Machine Learning Models. In Proceedings of
the201927thACMJointMeetingonEuropeanSoftwareEngineeringConferenceand
Symposium on the Foundations of Software Engineering (ESEC/FSE 2019). 625–635.
https://doi.org/10.1145/3338906.3338937
[3]AndreaArcuriandLionelBriand.2014. AHitchhiker’sguidetostatisticaltests
forassessingrandomizedalgorithmsinsoftwareengineering. SoftwareTesting,
Verification and Reliability (2014), 219–250. https://doi.org/10.1002/stvr.1486
[4]Rachel KE Bellamy, Kuntal Dey, Michael Hind, Samuel C Hoffman, Stephanie
Houde, Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta,
Aleksandra Mojsilović, et al .2019. AI Fairness360:An extensible toolkit for de-
tectingandmitigatingalgorithmicbias. IBMJournalofResearchandDevelopment
63, 4/5 (2019), 4–1.
[5]Ruth G Blumrosen. 1978. Wage discrimination, job segregation, and the title vii
of the civil rights act of 1964. U. Mich. JL Reform 12 (1978), 397.
[6]L.Breiman,J.H.Friedman,R.A.Olshen,andC.I.Stone.1984. Classificationand
regression trees. Wadsworth: Belmont, CA.
[7]Yuriy Brun and Alexandra Meliou. 2018. Software Fairness (ESEC/FSE 2018).
754–759. https://doi.org/10.1145/3236024.3264838
[8]Joymallya Chakraborty, Suvodeep Majumder, Zhe Yu, and Tim Menzies. 2020.
Fairway:awaytobuildfairMLsoftware.In Proceedingsofthe28thACMJoint
Meeting on European Software Engineering Conference and Symposium on the
Foundations of Software Engineering. 654–665.
[9]Joymallya Chakraborty, Suvodeep Majumder, Zhe Yu, and Tim Menzies. 2021.
implementation of Fairway. https://github.com/joymallyac/Fairway. Online.
[10]JoymallyaChakraborty,TianpeiXia,FahmidM.Fahid,andTimMenzies.2019.
Software Engineering for Fairness: A Case Study with Hyperparameter Opti-
mization. arXiv:1905.05786
[11]Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Meyarivan. 2002. A
fastandelitistmultiobjectivegeneticalgorithm:NSGA-II. IEEEtransactionson
evolutionary computation 6, 2 (2002), 182–197.
[12]Deloitte. 2021. Better Data, Faster Delivery, Actionable Insights. https:
//www2.deloitte.com/us/en/pages/deloitte-analytics/solutions/insuresense-
insurance-data-analytics-platform-data-management-services.html. Online.
[13]DheeruDuaandCaseyGraff.2017. UCIMachineLearningRepository. https:
//archive.ics.uci.edu/ml/datasets/census+income
[14]DheeruDuaandCaseyGraff.2017. UCIMachineLearningRepository. https:
//archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)
[15]DheeruDuaandCaseyGraff.2017. UCIMachineLearningRepository. https:
//archive.ics.uci.edu/ml/datasets/bank+marketing
[16]Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard
Zemel.2012. Fairnessthroughawareness.In Proceedingsofthe3rdinnovationsin
theoretical computer science conference. 214–226.
[17]Sainyam Galhotra, Yuriy Brun, and Alexandra Meliou. 2017. Fairness Testing:
TestingSoftwareforDiscrimination (ESEC/FSE2017) .AssociationforComputing
Machinery, New York, NY, USA. https://doi.org/10.1145/3106237.3106277
[18]Moritz Hardt, Eric Price, and Nati Srebro. 2016. Equality of Opportunity in
Supervised Learning. In NIPS.
[19]Surya Mattu Julia Angwin, Jeff Larson and Lauren Kirchne. 2021. MachineBias. https://www.propublica.org/article/machine-bias-risk-assessments-in-
criminal-sentencing. Online.
[20]Faisal Kamiran, Asim Karim, and Xiangliang Zhang. 2012. Decision Theory for
Discrimination-AwareClassification.In 2012IEEE12thInternationalConference
on Data Mining. 924–929. https://doi.org/10.1109/ICDM.2012.45
[21]AlexanderKampmann,NikolasHavrikov,SoremekunEzekiel,andAndreasZeller.
2020. When does my Program do this? Learning Circumstances of Software
Behavior (FSE 2020).
[22]Max Kuhn, Kjell Johnson, et al .2013.Applied predictive modeling. Vol. 26.
Springer.
[23]VivekNair,ZheYu,TimMenzies,NorbertSiegmund,andSvenApel.2020.Finding
Faster Configurations Using FLASH. IEEE Transactions on Software Engineering
46, 7 (2020), 794–811. https://doi.org/10.1109/TSE.2018.2870895
[24]Northpointe. 2012. Practitioners Guide to COMPAS. http://www.northpointeinc.
com/files/technical_documents/FieldGuide2_081412.pdf. Online.
[25]Zooko O’Whielacronx. 2018. A program/module to trace Python program or
function execution. https://docs.python.org/3/library/trace.html. Online.
[26]ProPublica.2021. CompasSoftwareAnanlysis. https://github.com/propublica/
compas-analysis. Online.
[27]scikit learn. 2021. Decision Tree Classifier. https://scikit-learn.org/stable/
modules/generated/sklearn.tree.DecisionTreeClassifier.html. Online.
[28]scikitlearn.2021. DiscriminantAnalysis. https://scikit-learn.org/stable/modules/
lda_qda.html. Online.[29]scikitlearn.2021. LogisticRegression. https://scikit-learn.org/stable/modules/
generated/sklearn.linear_model.LogisticRegression.html. Online.
[30]scikit learn. 2021. Random Forest Regressor. https://scikit-learn.org/stable/
modules/generated/sklearn.ensemble.RandomForestRegressor.html. Online.
[31]scikit learn. 2021. Support Vector Machine. https://scikit-learn.org/stable/
modules/generated/sklearn.svm.LinearSVC.html. Online.
[32]Leslie Scism and Mark Maremont. 2010. Insurers test data pro-files to identify risky clients. https://www.wsj.com/articles/
SB10001424052748704648604575620750998072986. Online.
[33]Saeid Tizpaz-Niari, Pavol Cerný, Bor-Yuh Evan Chang, and Ashutosh Trivedi.
2018. Differential Performance Debugging With Discriminant Regression Trees.
InProceedings of the Thirty-Second AAAI Conference on Artificial Intelligence
(AAAI-18). 2468–2475. https://www.aaai.org/ocs/index.php/AAAI/AAAI18/
paper/view/16647
[34]Saeid Tizpaz-Niari, Pavol Černý, and Ashutosh Trivedi. 2020. Detecting and
Understanding Real-World DifferentialPerformance Bugs in MachineLearningLibraries (ISSTA). https://doi.org/10.1145/3395363.3404540
[35]
SakshiUdeshi,PryanshuArora,andSudiptaChattopadhyay.2018. Automated
directed fairness testing.In Proceedings of the33rd ACM/IEEE International Con-
ference on Automated Software Engineering. 98–108.
[36]Ulrike Von Luxburg. 2007. A tutorial on spectral clustering. Statistics and
computing 17, 4 (2007), 395–416.
[37]AndreasZeller,RahulGopinath,MarcelBöhme,GordonFraser,andChristian
Holler.2021. TheFuzzingBook. CISPAHelmholtzCenterforInformationSecurity.
https://www.fuzzingbook.org/ Retrieved 2021-10-26 15:30:20+02:00.
[38]Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. 2018. Mitigating un-
wantedbiaseswithadversariallearning.In Proceedingsofthe2018AAAI/ACM
Conference on AI, Ethics, and Society. 335–340.
[39]Jie M. Zhang and Mark Harman. 2021. "Ignorance and Prejudice" in SoftwareFairness. In 43rd IEEE/ACM International Conference on Software Engineering,
ICSE2021,Madrid,Spain,22-30May2021.IEEE,1436–1447. https://doi.org/10.
1109/ICSE43902.2021.00129
[40]JieMZhang,MarkHarman,LeiMa,andYangLiu.2020. Machinelearningtesting:
Survey, landscapes and horizons. IEEE Transactions on Software Engineering
(2020).
[41]Peixin Zhang, Jingyi Wang, Jun Sun, Guoliang Dong, Xinyu Wang, Xingen
Wang,JinSongDong,andTingDai.2020. White-BoxFairnessTestingthrough
Adversarial Sampling (ICSE ’20). Association for Computing Machinery, New
York, NY, USA. https://doi.org/10.1145/3377811.3380331
920
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. 