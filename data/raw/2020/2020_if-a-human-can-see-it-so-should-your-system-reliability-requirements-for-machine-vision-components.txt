If a Human Can See It, So Should Your System: Reliability
Requirements for Machine Vision Components
Boyue Caroline Hu
boyue@cs.toronto.edu
University of Toronto
Toronto, Ontario, CanadaLina Marsso
lina.marsso@utoronto.ca
University of Toronto
Toronto, Ontario, CanadaKrzysztof Czarnecki
kczarnec@gsd.uwaterloo.ca
University of Waterloo
Waterloo, Ontario, Canada
Rick Salay
rsalay@gsd.uwaterloo.ca
University of Waterloo
Waterloo, Ontario, CanadaHuakun Shen
huakun.shen@mail.utoronto.ca
University of Toronto
Toronto, Ontario, CanadaMarsha Chechik
chechik@cs.toronto.edu
University of Toronto
Toronto, Ontario, Canada
ABSTRACT
MachineVisionComponents(MVC)arebecomingsafety-critical.
Assuring their quality, including safety, is essential for their suc-
cessfuldeployment.Assurancereliesontheavailabilityofprecisely
specified and, ideally, machine-verifiable requirements. MVCs with
state-of-the-art performance rely on machine learning (ML) and
training data, but largely lack such requirements.
Inthispaper,weaddresstheneedfordefiningmachine-verifiable
reliabilityrequirementsforMVCsagainsttransformationsthatsim-
ulate the full range of realistic and safety-critical changes in the
environment.Usinghumanperformanceasabaseline,wedefinere-liabilityrequirementsas:â€˜ifthechangesinanimagedonotaffectahumanâ€™sdecision,neithershouldtheyaffecttheMVCâ€™s.â€™Tothisend,
we provide: (1) a class of safety-related image transformations; (2)
reliabilityrequirementclassestospecifycorrectness-preservation
and prediction-preservation for MVCs; (3) a method to instantiate
machine-verifiablerequirementsfromtheserequirementsclasses
usinghumanperformanceexperimentdata;(4)humanperformance
experiment data for image recognition involving eight commonly
used transformations, from about 2000 human participants; and
(5) a method for automatically checking whether an MVC satisfies
our requirements. Further, we show that our reliability require-
mentsarefeasibleandreusablebyevaluatingourmethodson13
state-of-the-art pre-trained image classification models. Finally, we
demonstrate that our approach detects reliability gaps in MVCs
that other existing methods are unable to detect.
CCS CONCEPTS
â€¢Software and its engineering â†’Requirements analysis ;â€¢
Computing methodologies â†’Computer vision .
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.3510109KEYWORDS
Software Engineering for Artificial Intelligence, Requirements En-
gineering, Software Analysis
ACM Reference Format:
Boyue Caroline Hu, Lina Marsso, Krzysztof Czarnecki, Rick Salay, Huakun
Shen, and Marsha Chechik. 2022. If a Human Can See It, So Should Your
System: Reliability Requirements for Machine Vision Components. In 44th
InternationalConferenceonSoftwareEngineering(ICSEâ€™22),May21â€“29,2022,
Pittsburgh, PA, USA. ACM, New York, NY, USA, 12 pages. https://doi.org/10.
1145/3510003.3510109
1 INTRODUCTION
TheuseofMachineVisionComponents(MVCs)insafety-critical
systems, such as self-driving cars, creates major safety concerns,
sinceundesiredbehaviorscanleadtofatalaccidents[ 56].Forex-
ample, recently, Tesla self-driving cars misclassified emergency
vehiclesandcausedmultiplecrashes[ 4,5].Knowinghowtoana-
lyze these components, provide safety assurance, and ensure their
qualitybecomesamustfortheirusabilityinsafety-criticaldomains.
Particularly, in systems that automate tasks normally performed
by humans, such as driving, the vision task is performed by MVCs
which represent a critical function for the overall system safety.
However, vision tasks are difficult to specify; thus, they are usually
performed using machine learning (ML) [ 53]. Defining require-
ments for ML is not trivial because the inability to specify clear
requirements is the reason to use ML in the first place [ 8,24,46].
Yet such requirements are necessary for verification and providing
safety guarantees. As a first step towards safe MVCs, one needs to
definewhatitmeansforanMVCtobecorrectandthencheckits
correctness prior to system deployment.
Inthispaper,wefocusononeaspectofcorrectness: reliability,
whichmeasurestheabilityofasystemorcomponenttoperform
itsrequiredfunctionsin aspecifiedenvironment[ 1],asit enables
ensuring the quality of the deployed system. We are specificallyinterested in whether the performance of an MVC remains reli-ably unaffected by image transformations that commonly occur
inreal-worldscenarios.ThisquestionhasbeenstudiedinSEand
MLliteratureas modelrobustness,includingtesting[ 52]andveri-
fication [ 23] techniques. Yet, given the lack of detailed reliability
requirements, these approaches are limited to checking the models
withinasmallneighbourhoodoftheoriginalinputimage,i.e.,by
applying perturbations that are almost imperceptible to humans.
11452022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:21:06 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Boyue Caroline Hu, Lina Marsso, Krzysztof Czarnecki, Rick Salay, Huakun Shen, and Marsha Chechik
(a) original image
 (b) original image
not car (grille) car (jeep)
(c) RGB
 (d) contrast
 (e) defocus blur
 (f) brightness
car (limousine) not car (grille) not car (EU salamander) car (jeep)
(g) frost
 (h) color jitter
 (i) jpeg compression
 (j) Gaussian noise
car (minivan) car (minivan) car (jeep) not car (shovel)
Figure 1: Image recognition on original and transformed images.
The top row displays original images containing cars from the
ILSVRCâ€™12 dataset [38]. The transformation applied to the image
is specified under the image. Images from (c) to (j) present all thesafety-related transformations considered in this paper. The classi-fication result of a state-of-the-art MVC ResNet50 [17] is shown inbrackets in italics under each image. We further specify whetherthepredictedcategoryisconcideredasacar(),ornot(),basedonthe ILSVRCâ€™12 class hierarchy. Transformations are implementedby Albumentations [6] and Imagenet-c [19].
While consideringonly the small perturbationsallows for require-
mentanalysisofmodelreliability[ 22],itsapplicabilityislimited
in the real-world scenarios, with a much broader range of possible
changes. For example, consider the problem of recognizing cars
inimagesâ€“seeafewexamplesinFig.1.Weareinterestedinbe-
ing able to recognize cars under such transformations as frost (see
Fig. 1g) and different brightness levels (Fig. 3d and Fig. 3g).
The range of transformation magnitudes in images in Fig. 1
is not considered small or imperceptible. While humans have no
problemrecognizingcarsintheseimages,thestate-of-the-artimage
classification model ResNet50 [ 17] failed to do so on the examples
inFigs.1d,1eand1j.SinceMVCsareusedinsystemsthatautomate
tasks normally performed by humans, MVCs, like ResNet50, areat the minimum expected to consistently classify objects across
range of changes that do not affect human perception. Thus, we
seekamethodtoestablishhumanperformanceasareferencefor
definingreliability,andanautomatedmethodtocheckMVCagainst
a justified range of changes that do not affect human performance.
Inthispaper,weformallydefinetwoclassesofmachine-verifiable
reliabilityrequirementsforMVCs: correctness-preservation andprediction-
preservation. For both requirements classes, the range of image
changes we consider (i.e.,the human-tolerated range), is a param-
eter estimated using experiments with human participants. Intu-
itively,withinthehuman-toleratedrangeofchanges, correctness-
preservation requires that the MVCâ€™s predictions after changes
in images should be correct, and prediction-preservation requires
thatthepredictionsonoriginalimagesandonimagesthatunder-
wenttransformationsshouldbethesame.Specifically,thispapermakesthefollowingcontributions:(1)Weidentifyaclassofsafety-relatedimagetransformations;(2)Weprovideaformalspecification
oftwoclassesofinput-outputreliabilityrequirementsforMVCs,
with parameters representing human performance; (3) We present
a method to instantiate our requirements classes into machine-
verifiable requirements.This method estimatesranges of changes
toimagesthatdonotaffecthumanvisionusingresultsofexperi-
ments with human participants; (4) We provide human experiment
performance data for image recognition; (5) We provide an auto-
mated method for checking MVCs against our machine-verifiable
requirements.
While our criteria are defined for any computer-vision task (in-
cluding object detection and semantic segmentation), in this paper,
we demonstrate the feasibility of our approach on the image classi-
fication task. We show that our approach captures reliability gaps
that existing methods are unable to detect using 13 state-of-the-art
pre-trained image classification models on two image classification
datasets (Imagenet [38] and CIFAR-10 [30]).
Significance: To the best of our knowledge, we are the first to
definereliabilityrequirementsforMVCsusingahuman-justified
range of changes over realistic safety-related transformations. Our
requirements and the method for checking their satisfaction can
be reused by software engineers for analyzing system reliability of
MVCs before deployment.
The rest of the paper is organized as follows: Sec. 2 gives an
overview of our approach for creating and checking our reliability
requirements.Sec.3presentsthesafety-relatedimagetransforma-
tions and a generic metric for measuring changes in images. Sec. 4
presentsaformalspecificationofourreliabilityrequirementclasses.
Sec.5presentsourexperimentformeasuringhumanrecognition
performance with human participants, and demonstrates an au-
tomatedapproachforestimatingparametersoftherequirements,
using data from this experiment. Sec. 6 introduces an automated
method for checking MVCâ€™s against our reliability requirements.
We evaluate our approach in Sec. 7. Sec. 8 compares our work with
related approaches and we conclude in Sec. 9.
2 APPROACH OVERVIEW
Fig. 2 gives an overview of our approach. Given (i) a vision taskfor the MVC, (ii) a safety-related transformation and (iii) experi-
mental data for estimating the ranges of visual changes that do not
affecthumanperformance,weprovideaprocessforinstantiating
machine-verifiablereliabilityrequirementsforMVC(requirement
instantiation) and a process for checking whether an MVC satisfies
these instantiated requirements (requirement checking).
Thevisiontaskandthetransformationneedtobeselectedbased
on the application of the MVC. To help with the selection of trans-
formationsthatrepresentchangeslikelytohappenintheoperating
environment, we identified a class of safety-related image trans-
formations that representpotentially risky input modificationsin
real world situations. For example, frost shown in Fig. 1 is a safety-
relatedtransformationbecauseitcanreducelightinginthescene
which, in turn, can cause machine vision errors. Note that since
transformations have different parameter domains and can have
differentvisualeffectsondifferentimagestohumans,wedefined
agenericmetriccalleda visualchange anddenotedby Î”ğ‘£,which
1146
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:21:06 UTC from IEEE Xplore.  Restrictions apply. If a Human Can See It, So Should Your System: Reliability Requirements for Machine Vision Components ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Legend:I.Requirement instantiation II.Requirement checking
: Step : ArtifactVision
taskSafety-
related
transfor-
mation
Experiment
data of human
performanceI.a. Estimate human
tolerated range of changes
with experiment results
for the vision task and
for each transformationParameters for
requirement classesI.b. Instantiate the re-
liability requirementsMachine-verifiable
requirementsII.a. Generate tests for
checking the satisfac-
tion of requirementsImages sets
Set of test images
(original and
transformed)
II.c Estimate confidence of
requirements satisfaction
(reliability distance)II.b. Run the
tests on the
MVC under
validation
MVC
prediction
results
Requirements
satisfaction
results
Figure 2: A process for instantiating two reliability requirements
classes for MVCs (requirement instantiation) and a process for
checking their satisfactions (requirement checking).
decouples the perceptible visual change to the image from the
transformationparametersandthusallowsstatingthereliability
requirements on the MVC more abstractly.
Requirement instantiation: Thisautomatedstepenablesusers
toinstantiatethereliabilityrequirementsforthevisiontaskwith
human tolerated range of visual changes for each selected trans-
formation.Thehuman-toleratedrangeistherequirementparam-
eter that describes the range of changes from a transformation
that should not affect the MVCâ€™s behavior. This requirement pa-rameter is measured with
Î”ğ‘£and estimated using results from
experimentswithhumanparticipants.Theoutputofthisstepisa
setofmachine-verifiablerequirements.Theresultingcorrectness-
preservation[resp.prediction-preservation]requirementstates:for
a vision task and a transformation, if the changes in the imagesare within the estimated human tolerated range, then an MVCshould preserve the correctness [resp. prediction] after applying
the changes to its input from before.
For example, for the transformation adding frost artificially, our
resulting requirements are as follows:
â€¢therecognitionaccuracyofanMVCshouldnotdecreaseif
thevisualchangeintheimagesiswithintherange Î”ğ‘£â‰¤0.84
(correctness-preservation ); and
â€¢the percentage of labels the MVC can preserve after adding
frost should not decrease if visual change in the images is
within the range Î”ğ‘£â‰¤0.91 (prediction-preservation ).
Note that our requirements do not depend on the state-of-art
ML techniques since they treat the MVC as a black box.
Requirementchecking: Thisautomatedmethodcheckswhether
an MVC satisfies the instantiated reliability requirements. Given a
set with original images, our process generates test cases (step II.a)
bytransforming theoriginalimages withintherange specifiedin
theinstantiatedrequirements,runsthetestsonthemodel(stepII.b),
and checks whether the MVC satisfies our requirements (step II.c).
Tosummarize,ourproposedapproachcanbeusedtoautomat-
ically generate machine-verifiable reliability requirements for a
vision task and a list of transformations, given human experimentresults; and then automatically evaluate whether an MVC satis-
fies these requirements. In the above example, the requirement
checking method will generate a set of test case images within the
ranges (0 .84 and 0.91) to check whether an MLC satisfies these
requirements.
Animplementationofourmethodisavailableonline.1Forthe
purposeofdemonstrationandevaluation,weconductedimageclas-
sification experiments with 2000 human participants for the vision
taskofrecognizingcarimagesfor8transformations:RGB,contrast,
defocusblur,brightness,frost,colorjitter,jpegcompression,and
Gaussian noise (see images in Fig. 1(c)-(j)). In the rest of the paper,
we describe the technical details of each step of Fig. 2 using this
experiment.
3 VISUAL CHANGES IN IMAGES
In this section, we start with establishing the definition of the met-
ricÎ”ğ‘£, which measures human visual changes in images caused
bytransformations.Then,weidentifyaclassofsafety-relatedim-
age transformations that are used to instantiate our correctness-
preservation andprediction-preservation requirements (see Sec. 4).
A key idea in our work is to define reliability requirements rela-
tive to Î”ğ‘£ranges rather than the transformation parameter ranges
to be tolerated. This is important since each transformation may
haveoneormoreparameters,andeachparametermayaffectthe
transformed image to a different degreeâ€”also depending on theinput image. For example, brightening an already bright image
makes the objects harder to see; on the other hand, making a dark
imagebrighterwillhavetheoppositeeffect.Further,smallchanges
to one parameter may cause small changes or large changes to
thetransformedimagedependingonthevaluesofotherparame-
ters. The visual change metric Î”ğ‘£allows us to abstract from these
complexities of the transformation parameter space. Also, a simple
imagedistancemetricsuchasmean squarederror,whichisoften
used to define robustness, e.g., [ 3,9], does not adequately reflect
thehuman-perceivedvisualchangeinimages[ 48].Thus,webase
Î”ğ‘£on image quality assessment metrics.
Background: Image Quality Assessment (IQA) . IQA metrics
arequantitativemeasuresofhumanobjectiveimagequality[ 49].
Given the original image, the IQA metrics automatically predict
the perceived image quality by measuring the perceptual â€˜distanceâ€™
between the two images [ 42]. This â€˜distanceâ€™ is different from pixel
distanceanditscalculationdependsonthedesignoftheIQAmetric.
VSNR (Visual-Signal-to-Noise-Ratio [7]) checks the visibility of the
changesinimagesandreturnsinfinity( âˆ)iftheyarenotvisible
tohumans[ 7].VIF(VisualInformationFidelity [42])measuresthe
informationfidelitybyanalyzingthestatisticsofthenaturalscenes
intheimages.VIFreturnsavaluebetween0and1ifthechanges
degradeperceivedimagequality,with1indicatingtheperfectqual-
ity compared to the original image; and it returns a value >1i f
the changes enhances image quality [ 42]. VIF is empirically shown
to be the closest to human opinions when compared to all other
IQAmetrics[ 43]andVSNRhasbeenshowntobeeffectivetode-
tectnon-visiblechanges[22].VIFisapplicabletotransformations
1Seehttps://carolineeeeeee .github.io/automating_requirementsforimplementation,
more results and information.
1147
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:21:06 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Boyue Caroline Hu, Lina Marsso, Krzysztof Czarnecki, Rick Salay, Huakun Shen, and Marsha Chechik
that can be described locally by a combination of signal attenua-
tionandadditiveGaussiannoiseinthesub-bandsinthewavelet
domain [42].
Measuring visual change in images. We now use IQA metrics
to define a generic metric Î”ğ‘£. Our definition of Î”ğ‘£shares the same
applicability characteristics as VNSR and VIF. For transformations
that satisfy this characteristic (e.g., noise, blur, brightness and con-
trast changes, color change, etc.), the definition is as follows:
Definition 1: Visual change Î”ğ‘£
Let an image ğ‘¥, an applicable transformation ğ‘‡ğ‘‹with a
parameter domain ğ¶and a parameter ğ‘âˆˆğ¶, s.t.ğ‘¥/prime=
ğ‘‡ğ‘‹(ğ‘¥,ğ‘)begiven. Î”ğ‘£(ğ‘¥,ğ‘¥/prime)isafunctiondefinedasfollows:
â§âªâªâª â¨
âªâªâªâ©0 If VSNR(ğ‘¥,ğ‘¥/prime)=âˆ
or VIF(ğ‘¥,ğ‘¥/prime)>1
1âˆ’VIF(ğ‘¥,ğ‘¥/prime)Otherwise
Basing Î”ğ‘£on IQA metrics means that it provides a general-
ized quantitative measure for visual changes in the images that
isindependent ofparticular imagesand transformations.We split
this definition into two cases. The first corresponds to changes
imperceptible to humans (when VSNR (ğ‘¥,ğ‘¥/prime)=âˆ) and changes
that enhance the visual quality (when VIF (ğ‘¥,ğ‘¥/prime)>1). In this case,
Î”ğ‘£=0 because such changes do not impacthuman recognition of
the images negatively. The other case deals with visible changes
thatdegrade visualquality.Since VIFreturns1 forperfectquality
comparedtotheoriginalimage,thedegradationisoneminusthe
image quality score. For example, the visual change of the example
in Fig. 1f compared to its original image in Fig. 1b is 0 .507. The
visual change of the example in Fig. 1e compared to the one in
Fig. 1b is 0 .985. This suggests that the transformation in Fig. 1e
causes more change visually than the one in Fig. 1f.
Safety-related image transformations. Wesaythatatransfor-
mation is safety-related if it can lead to a hazard in a real-world
machine-vision application scenario. To assess this in a systematic
manner, we utilize the CV-HAZOP checklist [ 55]. This checklist
comprehensively identifies the potentially hazardous impacts of
differentmodesofinterferenceinthecomputervision(CV)process,
whichiscomprisedoflightsources,transmissionmedium,object,
observer,andalgorithm.Atransformationthatcanproducesuch
impacts is considered safety-related. For example, contrast adjust-
ment,defocusblur,andaddedGaussiannoiseshowninFig.1are
safety-related transformations because they can reduce lighting in
thescene,causeblurring,andaddnoiseintheimages,whichcan
cause machine vision errors and subsequent system failures. Since
the scope of CV-HAZOP is broader than the image transformation
assessment task, we remove non-image-related hazard scenarios
entriesfromthechecklist.Inparticular,entriesrelatedto Algorithm
in the vision process are not relevant because they modify the CV
algorithm and not the images. Entries concerned with the Number
ofObservers arealsonotrelevantsincetheyfocusontheinterac-
tion between the observers and cannot be represented by single
imagetransformations.Finally,imagetransformationscannotmake
temporal changes;therefore, entrieswhich dealwith timeare notrelevanteither.Todeterminewhetheragivenimagetransformation
belongstooursafety-relatedclass,oneshouldfirstidentifytheloca-tioninthevisionprocesstowhichthetransformationcorresponds;
thenthepropertyoftheprocesslocationthatthetransformation
is affecting (CV-HAZOP parameters); and finally, how the trans-
formation is changing the property (CV-HAZOP guide words). For
example, defocus blur is changing the focus of the observer (CV-
HAZOPentryNo.1018),i.e.,camera,andthereforebelongsinour
class. Supplementary material1includes the full list of CV-HAZOP
safety-related entries (954 entries chosen from the overall 1470).
Inthispaper,weconsidertransformationsprovidedbythestate-
of-the-artlibraryAlbumentations[ 6]andtheMLrobustnessbench-
mark Imagenet-c [ 19], which consist of 50 unique transformations.
Ofthese,45aresafety-related.Wefurtherremovethosetransfor-
mations that cannot produce a continuous range of transformed
images, yielding 31 transformationsâ€”see supplementary material1
forthefulllist.Sincemultipletransformationscancorrespondto
a single CV-HAZOP entry, we only instantiate our approach onone transformation per CV-HAZOP entry, resulting in the eight
transformations illustrated in Fig. 1c-j: RGB,contrast,defocus blur,
brightness, frost,color jitter, jpeg compression andGaussian noise
addition.
4 SPECIFICATION OF RELIABILITY
REQUIREMENTS CLASSES
Inthissection,weprovideaformalspecificationofourtworelia-
bility requirements classes: correctness-preservation andprediction-
preservation.
LetusassumethatwearegivenanMVC ğ‘“,adistributionofinput
imagesğ‘ƒğ‘‹,aground-truthlabelingfunction ğ‘“âˆ—,atransformation
ğ‘‡ğ‘‹withparameterdomain ğ¶andparameterdistribution ğ‘ƒğ¶.Our
requirementsusethejointdistributionofpairsoforiginalandtrans-formedimages,definedas
ğ‘ƒğ‘‡ğ‘‹(ğ‘¥,ğ‘¥/prime)=ğ‘ƒğ‘‹(ğ‘¥)/summationtext.1
ğ‘âˆˆğ¶,ğ‘¥/prime=ğ‘‡ğ‘‹(ğ‘¥,ğ‘)ğ‘ƒğ¶(ğ‘).
Wefirstintroduceour correctness-preservation reliabilityrequire-
ment class. It assumes a performance measure ğ‘š(ğ‘“,ğ‘“âˆ—,ğ‘ƒğ‘‹), which
is typically a measure of similarity between the output of ğ‘“andğ‘“âˆ—
giventhattheinput ğ‘¥âˆ¼ğ‘ƒğ‘‹.Notethat ğ‘šshouldbeadequateforthe
vision task, such as classification accuracy for image classification,
intersection over union (IoU) for image segmentation, and average
precision for object detection. We define the marginal distribution
oftransformedimageswithchangeswithinthehumantolerated-
range Î”ğ‘£â‰¤ğ‘¡ğ‘asğ‘ƒğ‘‡ğ‘‹,ğ‘¡ğ‘(ğ‘¥/prime)=/summationtext.1
ğ‘¥âˆˆğ‘‹ğ‘ƒğ‘‡ğ‘‹(ğ‘¥,ğ‘¥/prime|Î”ğ‘£(ğ‘¥,ğ‘¥/prime)â‰¤ğ‘¡ğ‘).
Definition 2: Correctness-preservation requirement,
with parameters ğ‘‡ğ‘‹andğ‘¡ğ‘
Intuitively : For the range of changes in images that do
not affect human performance ( Î”ğ‘£â‰¤ğ‘¡ğ‘), theperformance
of machine visioncomponent ğ‘“should not beaffected as
well. Note that ground truth isrequired.
Formally : We require the performance ğ‘šofğ‘“for trans-
formedimagestobeequaltoorlargerthanthatfororiginal
images:ğ‘š(ğ‘“,ğ‘“âˆ—,ğ‘ƒğ‘‡ğ‘‹,ğ‘¡ğ‘)â‰¥ğ‘š(ğ‘“,ğ‘“âˆ—,ğ‘ƒğ‘‹).
Example: Forthetransformationbrightness, correctness-preservation
requiresğ‘šğ‘¡ğ‘=ğ‘š(ğ‘“,ğ‘“âˆ—,ğ‘ƒğ‘‡ğ‘‹,ğ‘¡ğ‘),theclassificationaccuracyofResNet50
1148
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:21:06 UTC from IEEE Xplore.  Restrictions apply. If a Human Can See It, So Should Your System: Reliability Requirements for Machine Vision Components ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
onalltransformedimages(whichisthepercentageof correctpre-
dictions in Fig. 3d-3i), to be at least ğ‘š0=ğ‘š(ğ‘“,ğ‘“âˆ—,ğ‘ƒğ‘‹), the classifi-
cationaccuracyofthemodelonalloriginalimages,whichisthe
percentage of correctpredictions in Fig. 3a-3c. Both accuracies are
1/3â‰ˆ33%, and the requirement is satisfied.
We then introduce our prediction-preservation requirement class.
Given a distance measure ğ‘‘(ğ‘“(ğ‘¥),ğ‘“(ğ‘¥/prime)), which measures distance
betweentheoutputof ğ‘“ontwoinputimages,wedefinea prediction-
similarity measureğ‘ (ğ‘“,ğ‘ƒğ‘‹Ã—ğ‘‹)=1âˆ’ğ¸(ğ‘¥,ğ‘¥/prime)âˆ¼ğ‘ƒğ‘‹Ã—ğ‘‹[ğ‘‘(ğ‘“(ğ‘¥),ğ‘“(ğ‘¥/prime))],
which measures the expected similarity between the output of ğ‘“
on two images drawn from ğ‘ƒğ‘‹Ã—ğ‘‹, a distribution of image pairs.
Notethat ğ‘‘shouldalsobeadequateforthevisiontask;forexam-
ple, for image classification, ğ‘‘(ğ‘“(ğ‘¥),ğ‘“(ğ‘¥/prime))=0i fğ‘“(ğ‘¥)==ğ‘“(ğ‘¥/prime)
and 1 otherwise. We define the distribution of pairsof original and
transformed images that are within the human-tolerated range for
prediction-preservation Î”ğ‘£â‰¤ğ‘¡ğ‘by conditioning the joint distri-
butionğ‘ƒğ‘‡ğ‘‹as follows: ğ‘ƒğ‘‡ğ‘‹,ğ‘¡ğ‘(ğ‘¥,ğ‘¥/prime)=ğ‘ƒğ‘‡ğ‘‹(ğ‘¥,ğ‘¥/prime|Î”ğ‘£(ğ‘¥,ğ‘¥/prime)â‰¤ğ‘¡ğ‘).
Sinceğ‘ compares outputs with the original outputs, ğ‘ of original
images would always be 1, which is not necessarily achievable. As
an alternative, we estimate ğ‘ of original images with ğ‘ of images
with minimal image changes ( Î”ğ‘£â‰¤ğœ–). More precisely, we rank
theimagepairs by Î”ğ‘£,determine ğœ–asalo w er ğ‘-thquantilein the
ranking, and define the distribution of image pairs with Î”ğ‘£â‰¤ğœ–as
ğ‘ƒğ‘‡ğ‘‹,ğœ–(ğ‘¥,ğ‘¥/prime)=ğ‘ƒğ‘‡ğ‘‹(ğ‘¥,ğ‘¥/prime|Î”ğ‘£(ğ‘¥,ğ‘¥/prime)â‰¤ğœ–).
Definition 3: Prediction-preservation requirement,
with parameters ğ‘‡ğ‘‹andğ‘¡ğ‘
Intuitively : For the range of changes in images that do
notaffecthumanpredictions( Î”ğ‘£â‰¤ğ‘¡ğ‘),thepredictions of
machine vision component ğ‘“should stay unaffected as
well. Note that ground truth is notrequired.
Formally :Werequirethepredictionsimilarity ğ‘ ofğ‘“for
alltransformedimagestobeequaltoorlargerthanthatof
imagestransformedwith Î”ğ‘£â‰¤ğœ–,whichis: ğ‘ (ğ‘“,ğ‘ƒğ‘‡ğ‘‹,ğ‘¡ğ‘)â‰¥
ğ‘ (ğ‘“,ğ‘ƒğ‘‹,ğœ–).
Example: Forthetransformationbrightness, prediction-preservation
requiresğ‘ ğ‘¡ğ‘=ğ‘ (ğ‘“,ğ‘ƒğ‘‡ğ‘‹,ğ‘¡ğ‘),thepredictionsimilarityofResNet50for
alltransformedimagesvs.originals,whichisthepercentageofpre-
dictions in images in Fig. 3d-3i preserved from images in Fig. 3a-3c
andthus5 /6â‰ˆ83%,tobeatleastequalto ğ‘ 0=ğ‘ (ğ‘“,ğ‘ƒğ‘‹,ğœ–),thepre-
dictionsimilarityofthemodelforimagestransformedwith Î”ğ‘£â‰¤ğœ–.
Given the very small sample, we set ğœ–to the median, and thus ğ‘ 0
is the percentage of predictions preserved for images in Fig. 3d-3f,
andğ‘ 0=3/3=100%. Thus, the requirement is not satisfied.
Definitions ofthe two reliabilityrequirements aresimilar, with
twomaindifferences.First,correctness-preservationreliesonaper-
formance metric to compare predictions to ground truth, whereas
prediction-preservationusespredictionsimilaritytocomparepre-
dictions on transformed images vs. originals. Second, correctness-
preservation compares performance on the full range of trans-formed images with that on the originals, whereas prediction-
preservationcomparesthepredictionsimilarityforthefullrange
of transformed images vs. originals to that for the minimally trans-
formed images (i.e., Î”ğ‘£â‰¤ğœ–) vs. originals. The design choices for
(a) original image
 (b) original image
 (c) original image
car not car not car
(d) brightness
 (e) brightness
 (f) brightness
car not car not car
(g) brightness ++
 (h) brightness ++
 (i) brightness ++
car not car car
Figure 3: Image recognition on original and transformed images.
Images from (d) to (i) display the same transformation, bright-
ness,appliedwithdifferentmagnitudes.TheclassificationresultofResNet50 is shown in italics under each image.
the prediction-preservation requirement completely remove the
needforhumanlabelsontestimages, andmakethisrequirement
applicable in environments where such labels are unavailable.
Finally,wedefine reliabilitydistance asthedifferencebetween
thetargetandtheactualcorrectness-orprediction-preservation,
i.e.,Î”ğ‘š=ğ‘š0âˆ’ğ‘šğ‘¡ğ‘andÎ”ğ‘ =ğ‘ 0âˆ’ğ‘ ğ‘¡ğ‘, respectively. This distance
indicateshowwelltheMVCsatisfiestherespectiverequirement:
zerodistanceindicatesjustmeetingit;negativedistanceindicates
performingbetterthanthetargetbyamargin;andpositivedistance
indicates how far the MVC is from meeting the target.
5 INSTANTIATING RELIABILITY
REQUIREMENTS
Toobtainthereliabilityrequirementsrangeparameters ğ‘¡ğ‘,andğ‘¡ğ‘in
Defs.2and3,weperformtwoexperimentswithhumanparticipants
and then estimate the parameters from the experimental results to
obtain threshholds at which the human performance drops statisti-
cally significantly (step I.a of Fig. 2). This section first presents the
experimentalsetupandprocedure,andthenintroducesourmethod
for instantiating the requirements from the experimental results
(requirement instantiation, steps I.a-b).
Experiments with human participants. The objective of the
humanexperiments,oneperdataset,istoobtainhumanpredictions
on original and transformed images, to be used to estimate ğ‘¡ğ‘, and
ğ‘¡ğ‘instep I.a.The experimentinputs arethe taskto bepreformed;
thetransformation ğ‘‡ğ‘‹;thedatasetoforiginalimages {ğ‘¥ğ‘–},ğ‘¥ğ‘–âˆ¼ğ‘ƒğ‘‹,
with their ground-truth labels {ğ‘“âˆ—(ğ‘¥ğ‘–)}; and distributions ğ‘ƒğ¶for
each transformation parameter. Given these inputs, we generate a
1149
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:21:06 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Boyue Caroline Hu, Lina Marsso, Krzysztof Czarnecki, Rick Salay, Huakun Shen, and Marsha Chechik
sample of original-transformed images {(ğ‘¥ğ‘–,ğ‘¥/prime
ğ‘–,ğ‘—)},(ğ‘¥ğ‘–,ğ‘¥/prime
ğ‘–,ğ‘—)âˆ¼ğ‘ƒğ‘‡ğ‘‹,
by randomly selecting ğ‘¥ğ‘–from{ğ‘¥ğ‘–}andğ‘ğ‘—âˆ¼ğ‘ƒğ¶, and transforming
ğ‘¥/prime
ğ‘–,ğ‘—=ğ‘‡ğ‘‹(ğ‘¥ğ‘–,ğ‘ğ‘—).Toobtainthehumanpredictionsforeachimage
in{(ğ‘¥ğ‘–,ğ‘¥/prime
ğ‘–,ğ‘—)}for imageclassification, wefollow theexperimental
designofGeirhosetal.[ 16].Theexperimentisa forced-choiceimage
categorisation task : humans are presented with the images with
transformationsapplied,for200ms,andaskedtochooseoneofthe
twocategories(e.g.,carornotcar).Betweenimages,anoisemaskis
shown to minimize feedback influence in the brain [ 16]. The tasks
aretimedtoensurefairnessinthecomparisonbetweenhumansandmachine[
14].However,incontrasttotheworkbyGeirhosetal.,we
ensure that the full range of achievable Î”ğ‘£values is covered when
samplingfrom ğ‘ƒğ¶,andwealsocollecthumanpredictionsforthe
original images, so that we can estimate prediction preservation. Agiven subject is never shown more than one version of
ğ‘¥ğ‘–, whether
original or transformed. Note that human predictions for originals
are different from their ground truth labels: labelers take as long
asneededperimagetoclassifyit,butoursubjectshaveonly200
ms to see each image. The human data is specific to a task, an
image distribution, and a class of transformations, and thus has to
becollectedforthecombinationofthethree.Inotherwords,the
data is reusable for different samples from the same distribution,
or, intuitively, for images sharing the same characteristics, e.g., the
same image resolution, the same objects, etc. For example, it is
reusableacrossdifferentsetsofimagesofroadscenestakenwithin
the same geographic area using cameras with same the resolution
and image quality.
Togeneratepredictionsforourevaluation,weperformtheexper-
imentontwodatasets:ILSVRCâ€™12[ 38]andCIFAR-10[ 30].While
CIFAR-10 has images of much lower resolution than ILSVRCâ€™12,
weincludethisdatasettocompareourmethodtotheexistingwork
onrobustnesschecking,whichusesCIFAR-10.Wealsoselectthe
eight safety-related transformations (seeimages from Fig. 1c-j), as
discussed in Sec. 3, and set ğ‘ƒğ¶to be uniform. To fit our labeling
budget,welimitthetasktoabinaryclassificationproblemofrecog-nizingcarinstances.Also,whileweapplytheeighttransformations
toILSVRCâ€™12,we limittheexperimenton CIFAR-10tofourtrans-
formations that are also used in the works we compare with. To
differentiatebetweencarandnon-carinstances,weusetheclass
hierarchy from the ILSVRCâ€™12 dataset. For each of the selected
transformations, we sample 1000 pairs (ğ‘¥ğ‘–,ğ‘¥/prime
ğ‘–,ğ‘—), and have each im-
age(originalortransformed)labeledbyfivehumans.Toachieve
this,wedividethe1,000(pairsamples) Ã—8(#considered)transfor-
mationsintobatchesof20images.Eachbatchisshownfivetimesto
different participants using the platform Amazon Mechanical Turk.
We include qualification tests and sanity checks aimed to filter out
participants misunderstanding the task and spammers [ 35], and
onlyconsiderresultsfromparticipantswhopassbothtests.Asa
result,fortheILSVRCâ€™12imageclassificationtask,weuse {ğ‘¥ğ‘–}with
13,000 car images and same number of non-car images, and collect
human predictions for 40,000 ( =5Ã—8Ã—1,000) transformed images
andthesamenumberoftheoriginalimages,foratotalof80,000predictions. Note that the effort required for measuring human
performanceissignificantlysmallerthanthethedatasetlabeling
effort needed for model training. For example, we collect human
predictions for 5,000 transformed images per transformation, with0.2s timebox per image, for a total of 1000s. Training sets are typi-
callyover100,000images,withatleastthreegroundtruthlabels
assignedindependentlytoeachimage(forqualitycontrol)andeachtakesmultipleseconds;e.g.,100,000x3x2s=600,000s.Thehuman
experiment results can be found in supplementary material1.
Estimatingtoleratedrangeparametersandinstantiatingre-quirements.
We propose the following procedure to estimate
ğ‘¡ğ‘andğ‘¡ğ‘from the experimentally-obtained human predictions
(step I.a). The key idea is to group and order the image pairs by
Î”ğ‘£, compute the human performance ğ‘šğ‘˜and human prediction
similarity ğ‘ ğ‘˜ineachgroup,anduseastatisticaltesttodetermine
theğ‘¡ğ‘[resp.ğ‘¡ğ‘] value of Î”ğ‘£at which ğ‘šğ‘˜[resp.ğ‘ ğ‘˜] drops signif-
icantly from the human performance ğ‘š0for the original images
[resp. the human prediction similarity ğ‘ 0]. Recall that ğ‘ 0is the the
humanpredictionsimilarityforimagestransformedwith Î”ğ‘£â‰¤ğœ–
vs. originals; we set ğœ–to the lower 5th percentile.
More precisely, we determine threshold ğ‘¡(ğ‘™)
ğ‘andğ‘¡(ğ‘™)
ğ‘for a given
transformation ğ‘‡ğ‘‹,ğ‘™from the image pairs (ğ‘¥ğ‘–,ğ‘¥/prime
ğ‘–,ğ‘—), the human pre-
dictions for these images, and their ground truth ğ‘“âˆ—. First, we com-
puteÎ”ğ‘£for each pair, and sort the pairs by their Î”ğ‘£intoğ‘Ÿinter-
vals,definedby ğ‘Ÿ+1equidistancedthresholds ğ‘¡ğ‘˜,withğ‘¡0=0and
ğ‘¡ğ‘Ÿ=1. We then process the result using smoothing splines [ 28]
toreducerandomnessandremoveoutliers.Then,toestimate ğ‘¡(ğ‘™)
ğ‘,
foreachğ‘˜âˆˆ[1..ğ‘Ÿ]wecomputetheprobability ğ‘ğ‘˜thatğ‘šğ‘˜onthe
transformedimagesinthe ğ‘˜-thinterval [ğ‘¡ğ‘˜âˆ’1,ğ‘¡ğ‘˜]isbelowğ‘š0.This
probabilityisobtainedusingthesingle-sidedbinomialtest.Wethendeterminetheintervalwiththesmallest
ğ‘¡ğ‘˜forwhich ğ‘ğ‘˜â‰¥0.05,and
return this ğ‘¡ğ‘˜asğ‘¡(ğ‘™)
ğ‘. Similarly, for ğ‘¡(ğ‘™)
ğ‘, we compute the probability
ğ‘ğ‘˜thatğ‘ ğ‘˜fortheoriginal-transformedimagepairsin [ğ‘¡ğ‘˜âˆ’1,ğ‘¡ğ‘˜]is
belowğ‘ 0. Thenğ‘¡(ğ‘™)
ğ‘is the smallest ğ‘¡ğ‘˜for which ğ‘ğ‘˜â‰¥0.05.
With the above procedure, we can now estimate the parameters
for the task of recognizing cars for each of the eight selected trans-
formations (Fig. 1) using our experimental results. The instantiated
parameters are shown in Tbl. 1.
With these parameters, we obtain the instantiated machine-
verifiablereliabilityrequirementsforeachtransformation(stepI.b).
For example, for the transformation brightness, given an MVC that
recognizes cars in images, the correctness-preservation requirement
says that the MVCâ€™s recognition accuracy should not decrease if
thevisualchangeintheimagesiswithintherange Î”ğ‘£â‰¤0.8;and
theprediction-preservation requirement says that the percentage
of labels humans can preserve after a brightness change should
not decrease if the visual change in the images is within the range
Î”ğ‘£â‰¤0.86. To obtain the instantiated requirements for other trans-
formations,onlytheparametervaluesneedtobereplacedwiththe
estimated values in Tbl. 1.
6 CHECKING RELIABILITY REQUIREMENTS
In thissection, wedescribe amethod forautomatically checking
whether MVCs satisfy our machine-verifiable requirements (see
stepsII.a-c requirementchecking inFig.2).Requirementchecking
takes as inputs a list of images, a set of transformations, and anMVC under validation. It generates test cases (step II.a) withinthe specified range of
Î”ğ‘£â‰¤ğ‘¡ğ‘orğ‘¡ğ‘; runs the tests on the MVC
(step II.b); and checks whether the MVC satisfies the requirements
by estimating the reliability distance (step II.c).
1150
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:21:06 UTC from IEEE Xplore.  Restrictions apply. If a Human Can See It, So Should Your System: Reliability Requirements for Machine Vision Components ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table1: Estimatedparametersforcorrectness-( ğ‘¡ğ‘)andprediction-
preservation( ğ‘¡ğ‘)requirementsusinghumanexperimentresultsfor
the task of recognizing car instances.
transformation ğ‘¡ğ‘ğ‘¡ğ‘transformation ğ‘¡ğ‘ğ‘¡ğ‘ImagenetRGB 0.820.67 brightness 0.870.87
contrast 0.770.28Gaussian noise 0.910.91
defocus blur 0.980.94 color jitter 0.480.48
frost 0.840.91jpeg compression 0.940.94CIFAR
-10brightness 0.780.89 contrast 0.630.86
frost 0.610.61jpeg compression 0.600.60
WetesttherequirementssatisfactionbyestimatingMVCperfor-
mance or prediction preservation through sampling. This is neces-
sarily,sincewedonothavedirectaccessto ğ‘ƒğ‘‹butonlyitssamples.
Our test case generation is based on the bootstrap method [13],
which estimates the metrics ğ‘š0,ğ‘šğ‘¡ğ‘,ğ‘ 0, andğ‘ ğ‘¡ğ‘through sampling
thetestdatawithreplacement.Sincethesemetricsaredefinedas
expected values or means, by Central Limit Theorem, the values of
thesemetricscomputedforsamplebatches,denotedforeachbatch
ğ‘–asÂ¯ğ‘š0,ğ‘–,Â¯ğ‘šğ‘¡ğ‘,ğ‘–,Â¯ğ‘ 0,ğ‘–, andÂ¯ğ‘ ğ‘¡ğ‘,ğ‘–, respectively, are normally distributed.
Followingthebootstrapmethod,weobtainthepopulationestimates
bycomputingthemeans Ë†ğ‘š0,Ë†ğ‘šğ‘¡ğ‘,Ë†ğ‘ 0,andË†ğ‘ ğ‘¡ğ‘andthestandardde-
viationsğœË†ğ‘š0,ğœË†ğ‘šğ‘¡ğ‘,ğœË†ğ‘ 0, andğœË†ğ‘ ğ‘¡ğ‘of the respective batch value sets
{Â¯ğ‘š0,ğ‘–},{Â¯ğ‘šğ‘¡ğ‘,ğ‘–},{Â¯ğ‘ 0,ğ‘–},and{Â¯ğ‘ ğ‘¡ğ‘,ğ‘–}.Todoso,foreachtransformation
ğ‘‡ğ‘‹andalistoforiginalimages ğ‘‹,wesample ğ‘›batchesof ğ‘˜images
fromğ‘‹and thengenerate a transformedimage by applying ğ‘‡ğ‘‹to
each sampled original image with randomly sampled parameter
valueswhileensuringtherequired Î”ğ‘£range,i.e., ğ‘›Ã—ğ‘˜pairsintotal.
Sincesamplingispartoftheprocess, ğ‘›andğ‘˜shouldbedetermined
based on |ğ‘‹|, and the bigger they are, the more accurate the esti-
matedresultswouldbe[ 13].Althoughthelower-boundnumbers
forğ‘›andğ‘˜are hard to determine, one can check whether the sam-
plingissufficientasthebootstrapmethodalwaysconvergeswith
enoughbatchesofsamplesfornormaldistributions[ 2].Achoiceof
ğ‘›isconsideredsufficientlylargeiftwoseparaterunswithdifferent
random seeds result in similar estimated values.
Aftergeneratingthetests,ourmethodrunsthemontheMVC
under validation, and obtains the MVC predictions for all the orig-
inalimagesandforeachbatch ğ‘–oftransformedimages.Wethen
computethesamplebatchestimatesofthefourmetrics,i.e., {Â¯ğ‘š0,ğ‘–},
{Â¯ğ‘šğ‘¡ğ‘,ğ‘–},{Â¯ğ‘ 0,ğ‘–}, and{Â¯ğ‘ ğ‘¡ğ‘,ğ‘–}, and take mean ( Î”Ë†ğ‘š,Î”Ë†ğ‘ ) and standard
deviation ( ğœÎ”Ë†ğ‘š,ğœÎ”Ë†ğ‘ ) of each set as the population estimates.
Finally, we want to show that the reliability distance for each
requirement is zero or negative, i.e., Î”ğ‘šâ‰¤0 for correctness-
preservation and Î”ğ‘ â‰¤0 for prediction-preservation. Since our
estimates from the previous step are normally distributed, their
differencesarealsonormallydistributed.Thus,thereliabilitydis-
tance estimates have the following means and standard deviations:
Î”Ë†ğ‘š=Ë†ğ‘š0âˆ’Ë†ğ‘šğ‘¡ğ‘andğœÎ”Ë†ğ‘š=/radicalBig
ğœ2
Ë†ğ‘š0+ğœ2
Ë†ğ‘šğ‘¡ğ‘; and Î”Ë†ğ‘ =Ë†ğ‘ 0âˆ’Ë†ğ‘ ğ‘¡ğ‘
andğœÎ”Ë†ğ‘ =/radicalBig
ğœ2
Ë†ğ‘ 0+ğœ2
Ë†ğ‘ ğ‘¡ğ‘. To ensure that the reliability distances
are zero or negative with a confidence 1 âˆ’ğ›¼=95%, we use the
right-handedconfidenceinterval.Thus, Î”ğ‘šâ‰¤0withconfidence
1âˆ’ğ›¼iffÎ”Ë†ğ‘š+ğ‘§ğ›¼ğœÎ”Ë†ğ‘šâ‰¤0,where ğ‘§ğ›¼isthez-valuecorresponding
toanarea ğ›¼intherighttailofastandardnormaldistribution,withğ‘§0.05=1.645 for 95% confidence. Similarly, Î”ğ‘ â‰¤0 with confidence
1âˆ’ğ›¼iffÎ”Ë†ğ‘ +ğ‘§ğ›¼ğœÎ”Ë†ğ‘ â‰¤0.
For example, to check whether ResNet50 satisfies our instan-
tiated requirements for the transformation Gaussian noise (see
Tbl.1)forthetaskofrecognizingcars,thetestingmethodfirstgen-
erates tests with the original and the transformed images within
theÎ”ğ‘£range specified in the requirements. The original images
are sampled from the ILSVRCâ€™12 validation dataset using boot-
strapwith ğ‘›=200,ğ‘˜=50andtheGaussiannoisetransformation.
Then we run the generated tests on ResNet50; compute the four
sets of metrics {Â¯ğ‘š0,ğ‘–},{Â¯ğ‘šğ‘¡ğ‘,ğ‘–},{Â¯ğ‘ 0,ğ‘–}, and{Â¯ğ‘ ğ‘¡ğ‘,ğ‘–}over the batches;
and then compute Î”Ë†ğ‘š=0.0045,ğœÎ”Ë†ğ‘š=0.0061, Î”Ë†ğ‘ =0.0011, and
ğœÎ”Ë†ğ‘ =0.0045. We check for correctness-preservation with 95% con-
fidence: Î”Ë†ğ‘š+ğ‘§0.05ğœÎ”Ë†ğ‘š>0; and prediction-preservation with 95%
confidence: Î”Ë†ğ‘ +ğ‘§0.05ğœÎ”Ë†ğ‘ >0. Therefore ResNet50 does not satisfy
eitheroftherequirements.Notethatbyestimatingthereliability
distance, we provide engineers with a quantitative measure of howmuchimprovementisneededtomeettherequirementsincasethey
are not met.
7 EVALUATION
While our approach is defined for any computer-vision task, in
this paper we demonstrate its feasibility on a particular domain:
image classification, using parameters instantiated via human per-
formance data collected for this domain as explained in Sec. 6.
First,weevaluatethegeneralityofourinstantiatedimageclassifi-
cationrequirements.Foraspecifictransformation,ourinstantiated
requirements contain the tolerated range of changes that do not
affecthumanperformance(seeSec.4),estimatedfromexperiments
with human participants. Since such experiments are costly, we
aim to minimize the number of experiments that need to be con-
ducted.Toachievethisgoal,wewouldliketoreusethecollected
humanperformanceresultsfornewsetsofimagesfromthedataset,
different from the ones presented to the humans during the exper-
iments. We expect the images to come from the same dataset to
share the underlying data-generating distribution ğ‘ƒğ‘‹. Crucially, to
be reusable, our requirement parameters should not be affected by
the choice of the images included in the experiments with human
participants. Since our requirements are defined on a particular
distribution of images, we aim to answer (RQ1): How reusable are
thethresholds ğ‘¡ğ‘andğ‘¡ğ‘overdifferentsamplesfromthesameimage
distribution?
Second,existingmethodsforevaluatingreliabilityofimageclas-
sificationMVCsconsidereithersmall,imperceptibleimagechangesoranarbitraryrangeofperceptiblechangesinimages.Inthiswork,
ourfocusisonameaningfulrangeofchangesinimages,theone
that does not affect human vision, which includes both impercepti-
ble and perceptible changes. Since our goal is to use human perfor-
mance as a baseline (i.e.,â€œif humans can see it, so should an MVCâ€),
weareinterestedinunderstandinghowwelltheexistingreliability
evaluation approaches already cover the human-tolerated range.
Wearealsointerestedincomparingthedistributionoftestcases
generated by our method (step II.a in Fig. 2) with those from the
other reliability methods, to see whether our method addresses the
range better. Therefore, we aim to answer (RQ2): How well do the
1151
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:21:06 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Boyue Caroline Hu, Lina Marsso, Krzysztof Czarnecki, Rick Salay, Huakun Shen, and Marsha Chechik
existing reliabilityevaluation methods cover thehuman-tolerated
range of changes?
Finally, we would like to determine whether checking the re-
liability of image classification MVC models with our method inthe human-tolerated range of changes reveals reliability gaps instate-of-the-art image classification models. To do so, we aim to
answer(RQ3): How effective is our requirement checking method
in identifying reliability gaps compared to existing approaches?
RQ1.To answer RQ1, we compare human-tolerated ranges of
transformations(parameters ğ‘¡ğ‘andğ‘¡ğ‘)estimatedusingourrequire-
mentinstantiationmethodwith differentsetsofimagesfromthe
ILSVRCâ€™12experiment.Werandomlyselectedtwosubsetsofour
resultscontaining60%ofalltheimagesincludedintheexperiment.
We compared the similarity of the spline models obtained using
these two subsets with all experiment results. As suggested by
Koenkeretal.[ 28],twosplinemodelsareconsideredsimilariftheir
83% confidence intervals overlap. Following this, for each of the
eight transformation included in our experiment, we compared the
confidenceintervalsoftheestimatedsplinemodelsrepresenting
thetwosubsetsandtheentiresetofexperimentresults.Asaresult,
for all transformations, we observed that the spline models are
unaffected.For example,the splinemodelsobtained forthe frost
transformationareshowninFig.4.Duetopagelimit,weinclude
theplotsanddatainsupplementarymaterial1.Sincetheparameters
are derived using the spline models, unaffected spline models sug-
gestthattheparametersestimatedarealsounaffected.Toconclude,different subsets of experiment results do not affect the parametersestimated. Therefore, we show evidence that our estimated human-
toleratedrangescan bereusedforimages thatarenotincludedin
theexperimentwithhumanparticipants,answeringRQ1.Notethat
ourrequirementsaredefinedononeimagedistribution,thusthe
thresholdscannotbereusedfordifferentimagedistributions.We
cancheckthisbycomparingthevaluesof ğ‘¡ğ‘andğ‘¡ğ‘estimatedusing
images from CIFAR-10 [ 30] and ILSVRCâ€™12 [ 38], shown in Tbl. 1
(Sec. 5).
RQ2.Existing methods for evaluating the reliability of MVCs with
image transformations (imperceptible and perceptible changes) in-
cludemetamorphictesting [12,32,52,56]andbenchmarking [19].
Metamorphictestingisbasedonmetamorphicrelations;thus,in-
stead of finding a range that does not affect human judgment, itconsider all possible parameter values for each transformationincluded [
12]. For this RQ, we compare with existing work that
considers a broader range of changes: the state-of-the-art imagecorruption benchmark datasets Imagenet-c and CIFAR-10-c [
19].
These benchmark datasets include images transformed with five
pre-selected parameter values for 19 arbitrarily chosen transfor-
mations. Due to the low resolution of CIFAR-10 images, they look
blurry to humans and thus do not share the same characteristics
withtheILSVRCâ€™12dataset[ 38].Therefore,toevaluateourmethod,
weconductanadditionalexperimentwithhumanparticipantusing
CIFAR-10 [ 30] images for four transformations (contrast, bright-
ness,frost,andJPEGcompression)andestimatedthecorresponding
human-tolerated ranges, as described in Section 5. We answer RQ2
and RQ3 using six transformations considered by the other works:
brightness,contrast,defocusblur,frost,Gaussiannoise,andjpeg
compression.
Figure4:Acomparisonofdifferentsubsetsofexperimentalresults for estimating ğ‘¡
ğ‘for the frost transformation.
ToanswerRQ2,wefirstcompareourhuman-toleratedranges
withtherangesofchangesincludedintherobustnessbenchmark
datasets,toseewhetherexistingmethodsalreadycoverthem.InFig.5,weshow,foreachtransformation,therangeofchangesin
images included in Imagenet-c/ CIFAR-10-c [ 19]2in blue, and our
human-toleratedrangesforbothrequirementsinyellowandgreen.
Theoverlappingofrangesindicatesthedegreetowhichourranges
are covered by Imagenet-c/ CIFAR-10-c. The ranges in Imagenet-c
and CIFAR-10-c [ 19] are either larger (e.g., brightness and frost for
CIFAR-10-c;brightnessforImagenet-c)orsmaller(e.g.,contrastand
jpeg compression of CIFAR-10-c; Gaussian noise, defocus blur and
frostforImagenet-c) thanthehuman-toleratedrange. Theimages
includedinImagenet-c/CIFAR-10-caretransformedbyusingapre-selectedlistoffiveparametervaluespertransformation.Thisresultshowsthatsimplygeneratingimagesthiswaydoesnotaddressthefullrangeofrealisticchangesthatdonotaffecthumanperformance.Secondly,wecomparethedistributionofthetestcases(transformed
images) within the human-tolerated range generated from our re-
quirement checking method and from CIFAR-10-c and Imagenet-c.
Ourrequirementcheckingmethodforgeneratingtestcasessam-
plestheparameterspaceuniformlyandthentransformstheimages.
As the number of parameters for a transformation increases, so
doesthepossiblenumberofcombinationsofparametervaluesthat
can lead to the same degree of visual change in the images. There-
fore, sampling the parameter space uniformly allows us a better
coverage of possible transformed images resulting in a fairer relia-
bility evaluation compared with transformations with pre-selected
parametervalues,asdoneinCIFAR-10-candImagenet-c[ 19].In
Fig. 6, we show the distributions of transformed images generated
with our requirement checking method and images in CIFAR-10-c
andImagenet-c.Aswecanobservefromtheplots,thetransformedimagesfromCIFAR-10-candImagenet-ceitherfavorcertainranges
2NotethatduetothelargesizeofImagenet-c,thedistributionisobtainedbyuniformly
sampling the entire benchmarking dataset.
1152
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:21:06 UTC from IEEE Xplore.  Restrictions apply. If a Human Can See It, So Should Your System: Reliability Requirements for Machine Vision Components ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
(a) CIFAR-10-c and our ranges.
 (b) Imagenet-c and our ranges.
Legend:
 range of changes in CIFAR-10-c/imagenet-c,
our range [0,ğ‘¡ğ‘],
our range [0,ğ‘¡ğ‘]
Figure 5: A comparison of our human-tolerated ranges for
correctness-preservation and prediction-preservation require-
mentsandtherangeofchangesincludedinrobustnessbenchmarkdatasets (Imagenet-c and CIFAR-10-c [19]).
ofÎ”ğ‘£score (Fig. 6b, 6c, 6d) or are discontinuous (Fig. 6a) and there-
fore biased. This also suggests that the approach of generating
transformed images in the benchmark datasets does not guarantee
a fair evaluation of reliability within the human-tolerated range
of changes because of the biased distribution of tests. Thus, the
human-toleratedrangesof changesarenotaddressed orproperly
tested by existing methods, answering RQ2.
RQ3.To answer RQ3, we aim to determine whether checking our
requirementsenablesustodiscoverreliabilitygapsthatwerenot
identifiedwithexistingreliabilitybenchmarks.Weevaluatedthe
reliability of 13 state-of-the-art image classification models with
thevisiontaskofrecognizingcarsinimagesusingbothourrequire-
mentcheckingmethodandtheexistingbenchmarksCIFAR-10-c
andImagenet-c.ResultsareshowninTbl.2.Notethatnomodels
satisfy our requirements with 95% confidence, which is not sur-
prising since they were not trained with data covering the human-
tolerated range. However, several models pre-trained on Imagenet
(ILSVRCâ€™12) images have negative reliability distance ( Ë†ğ‘ 0âˆ’Ë†ğ‘ ğ‘¡ğ‘) for
ourprediction-preservation requirements,whichsuggeststhatthese
models are close to satisfying these requirements.
Foreachtransformation,themodelsinTbl.2arerankedbased
ontheevaluationresults(accuracy)ofbenchmarkimages.Ahigher
ranking means that the model is more reliable. We compare the
reliability ranking of these models using our reliability distance
for both of our requirements (see Sec. 6) with the benchmark rank-
ing, indicating the differences in blue. The tests included in the
CIFAR-10-c andImagenet-c benchmarks [ 19] arebiased toward im-
ageswithsmalltransformationmagnitudes,resultinginsignificant
differenceswithourrankingforbrightness(Fig.6b),frost(Fig.6c
and 6h), jpeg compression (Fig. 6d) and defocus blur (Fig. 6g) trans-
formations. Therefore, if a model is lower on the ranking of our
reliabilitydistancethanonthebenchmarkranking,itislessreliablethanpredictedbythebenchmark,meaningthatourmethoddiscov-eredanewreliabilitygap.Belowwesummarizethemainreliability
gapsidentifiedbyourmethod.(i)RLATisrankedbyCIFAR-10-c
withinthethreelastmodelsforthetransformationscontrast,bright-
ness, and frost, but the first for jpeg compression. However, ourmethod ranks RLAT at the bottom for all the transformations in-
cludingjpegcompression,indicatingthatthetestsgeneratedbyourmethodareabletodetectthereliabilitygapmissedbythebench-
mark. (ii) For jpeg compression, RLATAugMixNoJSD is ranked
second both by the CIFAR-10-c benchmark and by our correctness-
preservationreliabilitydistance.However,RLATAugMixNoJSDis
rankedlastbyourprediction-preservationreliabilitydistance.Simi-larly,forGaussiannoise,resnext101_a+disrankedfirstbybothour
correction-preservation and Imagenet-c benchmark, but it is ranked
second last by our prediction-preservation reliability distance. This
showsthatbothRLATAugMixNoJSDandresnext101_a+dhavea
high accuracy for transformed images but their predictions arenot consistent. Therefore, checking our prediction-preservation
requirementenabledustoidentifynewreliability gapsthatcould
not be detected by only checking accuracy on transformed images,
answering RQ3.
Summary. Through answering RQ1, we show that parameters
of the requirements estimated with our requirement instantiation
method are reusablefor different images sharing the same class
and images distribution. Through answering RQ2, we show that
existing work does not adequately cover the range of changes that
donotaffecthumans.Finally,throughansweringRQ3,weshowthat
ourrequirementcheckingmethodisuseful,sinceitcandiscover
reliabilitygapsthataremissedbytheexistingmethods.Also,notice
thatourprediction-preservation isclosetobeingsatisfiedbyseveral
models pre-trained on Imagenet (ILSVRCâ€™12) images; this indicatesthatourrequirementsaresatisfiable.Thus,ourevaluationsuggeststhattheproposedrequirementsareusefulandreusableforchecking
reliability of MVCs.
Threats to validity. [Construct] For the correctness-preservation
requirement,thehumanperformancemayseemtoohardforMVCstomatch.Ho wever,followingguidelinesprovidedbyFirestone[
14],
we choose to keep the requirements for a fair comparison between
a human and an ML performance. Further, training with data aug-
mentationthatcoverstherangeofvisualchangesforeachtrans-
formationasperourrequirementsmightenableanMVCtomeet
them. Checking this hypothesis is future work. [Internal] We as-sumed that the parameter values for any transformation should
beuniformlydistributed.Thismaybedifferentdependingonthe
applicationoftheMVC,e.g.,heavysnowmaybelessrelevantfor
autonomouscarsdeployedintropicalregionsthanotherregions.
[External] Due to budget considerations, we included a limited
setoftransformationsandimageclassesinourexperimentswith
humans. Experiments for other visual tasks are also future work.
8 RELATED WORK
In this section, we first review the software engineering (SE) ap-
proachesdefiningreliabilityofMVCs,thentheSEandthecomputer
vision(CV)approachesforevaluatingreliabilityofMVCsand,fi-
nally, those comparing human performance against MVCs.
Specifying reliability of MVCs. The inductive data-driven na-
ture of machine-learning creates several challenges for require-
ments specification and verification in MVCs. Yet, multiple recent
studies explored this area [ 25,36,46]. While they agree on the
necessity of requirements elicitation in MVCs, they fail to pro-
vide a systematic approach for inferring the requirements. Several
authors attempted to specify the expected behaviour of MVCs indi-
rectlythroughspecifyingasetofqualitycharacteristicsfortraining
1153
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:21:06 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Boyue Caroline Hu, Lina Marsso, Krzysztof Czarnecki, Rick Salay, Huakun Shen, and Marsha Chechik
(a) Contrast with
CIFAR-10 images
(b) Brightness with
CIFAR-10 images
(c) Frost with
CIFAR-10 images
(d) JPEG Compression with
CIFAR images
(e) Gaussian noise with
Imagenet (ILSVRCâ€™12)
images
(f) Brightness with
Imagenet (ILSVRCâ€™12)
images
(g) Defocus blur with
Imagenet (ILSVRCâ€™12)
images
(h) Frost with
Imagenet (ILSVRCâ€™12)
images
Legend:
 CIFAR-10-c/Imagenet-c images,
 images generated within [0,ğ‘¡ğ‘],
images generated within [0,ğ‘¡ğ‘]
Figure6: Acomparisonoftherangeanddistributionof Î”ğ‘£scoresintestimagesofrobustnessbenchmarkdatasets(CIFAR-10-candImagenet-
c [19]) and test images generated with our requirement checking method. The x-axis is the Î”ğ‘£score. The y-axis is the number of images.
Table2: AcomparisonofreliabilityevaluationofMVCsusingourmethodandusingstate-of-the-artbenchmarks(CIFAR-10-candImagenet-
c).Foreachtransformationandthevisualtaskofcarrecognition,theMVCmodelsarerankedw.r.t.theiraccuracyonallbenchmarkimages. Ë†ğ‘š0
andË†ğ‘ 0are, resp., the required accuracy and percentage of labels preserved in our requirements. Ë†ğ‘šğ‘¡ğ‘andË†ğ‘ ğ‘¡ğ‘are, resp., the resulting accuracy
and perception preservation percentage through checking the models against our requirements. The differences between the benchmark
ranking and our ranking using the reliability distance are highlighted in blue.
Checking our
Correctness-preservationChecking our
Prediction-preservationChecking our
Correctness-preservationChecking our
Prediction-preservationdatasetmodel nameaccuracy
on all
benchmark
imagesrequired
and
estimated
accuracy
Ë†ğ‘š0|Ë†ğ‘šğ‘¡ğ‘reliability
distance
Ë†ğ‘š0âˆ’Ë†ğ‘šğ‘¡ğ‘
(rank)required
and
estimated
percentage
Ë†ğ‘ 0|Ë†ğ‘ ğ‘¡ğ‘reliability
distance
Ë†ğ‘ 0âˆ’Ë†ğ‘ ğ‘¡ğ‘
(rank)model nameaccuracy
on all
benchmark
imagesrequired
and
estimated
accuracy
Ë†ğ‘š0|Ë†ğ‘šğ‘¡ğ‘reliability
distance
Ë†ğ‘š0âˆ’Ë†ğ‘šğ‘¡ğ‘
(rank)required
and
estimated
percentage
Ë†ğ‘ 0|Ë†ğ‘ ğ‘¡ğ‘reliability
distance
Ë†ğ‘ 0âˆ’Ë†ğ‘ ğ‘¡ğ‘
(rank)cifar-10contrast brightness
Augmix_ResNeXt [20] 0.9920 0.9961 | 0.9871 (2) 0.009 0.996 | 0.9761 (2) 0.0199 Augmix_ResNeXt [20] 0.9952 0.9963 | 0.9776 (3) 0.0187 0.999 | 0.9576 (3) 0.0414
Augmix_WRN [20] 0.9909 0.9952 | 0.9868 (1) 0.0084 0.995 | 0.9756 (1) 0.0194 AugMixNoJSD [27] 0.9945 0.9961 | 0.9782 (1) 0.0179 0.996 | 0.9573 (1) 0.0387
AugMixNoJSD [27] 0.9901 0.9952 | 0.9851 (3) 0.0101 0.997 | 0.9674 (3) 0.0296 Augmix_WRN [20] 0.9943 0.9953 | 0.9768 (2) 0.0185 0.994 | 0.9540 (2) 0.04
Standard [54] 0.9862 0.9952 | 0.9809 (4) 0.0143 0.994 | 0.9570 (4) 0.037 RLATAugMixNoJSD [27] 0.9942 0.9953 | 0.9730 (4) 0.0223 0.998 | 0.9488 (4) 0.0492
RLATAugMixNoJSD [27] 0.9788 0.9936 | 0.9710 (5) 0.0226 0.993 | 0.9416 (5) 0.0514 Standard [54] 0.9933 0.9947 | 0.9690 (5) 0.0257 0.998 | 0.9451 (5) 0.0529
Gauss50percent [27] 0.9577 0.9925 | 0.9261 (6) 0.0664 0.987 | 0.8994 (6) 0.0876 RLAT [27] 0.9928 0.9930 | 0.9557 (7) 0.0373 0.993 | 0.9307 (6) 0.0623
RLAT [27] 0.9550 0.9936 | 0.9133 (7) 0.0803 0.991 | 0.8880 (7) 0.103 Gauss50percent [27] 0.9904 0.9925 | 0.9555 (6) 0.037 0.995 | 0.9305 (7) 0.0645
frost JPEG compression
Augmix_ResNeXt [20] 0.9912 0.9969 | 0.9771 (2) 0.0198 0.995 | 0.9776 (1) 0.0174 RLAT [27] 0.9910 0.9927 | 0.9443 (5) 0.0484 0.999 | 0.9773 (1) 0.0217
RLATAugMixNoJSD [27] 0.9910 0.9958 | 0.9737 (4) 0.0221 0.994 | 0.9738 (2) 0.0202 RLATAugMixNoJSD [27] 0.9899 0.9942 | 0.9659 (2) 0.0283 0.999 | 0.9365 (7) 0.0625
Augmix_WRN [20] 0.9899 0.9955 | 0.9765 (1) 0.019 0.998 | 0.9758 (4) 0.0222 Gauss50percent [27] 0.9897 0.9915 | 0.9701 (1) 0.0214 0.999 | 0.9735 (2) 0.0255
AugMixNoJSD [27] 0.9890 0.9965 | 0.9754 (3) 0.0211 0.997 | 0.9754 (3) 0.0216 Augmix_ResNeXt [20] 0.9894 0.9949 | 0.9516 (4) 0.0433 0.999 | 0.9529 (4) 0.0461
RLAT [27] 0.9875 0.9948 | 0.9414 (7) 0.0534 0.986 | 0.9430 (7) 0.043 Augmix_WRN [20] 0.9886 0.9942 | 0.9511 (3) 0.0431 0.999 | 0.9547 (3) 0.0443
Gauss50percent [27] 0.9867 0.9933 | 0.9506 (6) 0.0427 0.99 | 0.9524 (5) 0.0376 AugMixNoJSD [27] 0.9868 0.9953 | 0.9443 (6) 0.051 0.998 | 0.9471 (5) 0.0509
Standard [54] 0.9752 0.9956 | 0.9567 (5) 0.0389 0.997 | 0.9564 (6) 0.0406 Standard [54] 0.9734 0.9952 | 0.9359 (7) 0.0593 0.996 | 0.9365 (6) 0.0595imagenetGaussian noise frost
resnext101_a+d [18] 0.9962 0.997 | 0.9959 (1) 0.0011 0.998 | 0.997 (5) 0.001 resnext101_a+d [18] 0.9958 0.9974 | 0.9954 (1) 0.002 0.996 | 0.9962 (1)-0.0002
aug+deep [18] 0.9956 0.9958 | 0.9942 (2) 0.0016 0.996 | 0.9961 (1)-0.0001 aug+deep [18] 0.9952 0.9967 | 0.9943 (2) 0.0024 0.996 | 0.9942 (5) 0.0018
deepaugment [18] 0.9955 0.9963 | 0.9937 (4) 0.0026 0.996 | 0.9959 (3) 0.0001 ANT3x3_SIN [37] 0.9944 0.996 | 0.9935 (3) 0.0025 0.992 | 0.9924 (1)-0.0004
ANT_SIN [37] 0.9946 0.9953 | 0.9935 (3) 0.0018 0.996 | 0.9962 (1)-0.0002 ANT_SIN [37] 0.9941 0.9962 | 0.9927 (4) 0.0035 0.99 | 0.9927 (1)-0.0027
Speckle_Model [37] 0.9934 0.9958 | 0.9916 (5) 0.0042 0.996 | 0.9952 (4) 0.0008 deepaugment [18] 0.9936 0.9966 | 0.993 (5) 0.0036 0.994 | 0.992 (6) 0.002
resnet50 [17] 0.9924 0.9953 | 0.9908 (6) 0.0045 0.996 | 0.9949 (6) 0.0011 resnet50 [17] 0.9921 0.9957 | 0.9907 (6) 0.005 0.992 | 0.9911 (4) 0.0009
brightness defocus blur
resnext101_a+d [18] 0.9972 0.9967 | 0.9953 (2) 0.0014 1 | 0.9972 (4) 0.0028 resnext101_a+d [18] 0.9949 0.9977 | 0.995 (1) 0.0027 0.994 | 0.9957 (1)-0.0017
aug+deep [18] 0.9966 0.9959 | 0.9947 (1) 0.0012 1 | 0.9964 (5) 0.0036 aug+deep [18] 0.9946 0.9972 | 0.9937 (2) 0.0035 0.996 | 0.9943 (2) 0.0017
deepaugment [18] 0.9959 0.9956 | 0.9937 (3) 0.0019 0.996 | 0.9949 (2) 0.0011 deepaugment [18] 0.9924 0.9965 | 0.9914 (5) 0.005 0.998 | 0.9929 (5) 0.0051
ANT3x3_SIN [37] 0.9957 0.9954 | 0.9926 (5) 0.0028 0.996 | 0.994 (3) 0.002 ANT_SIN [37] 0.9920 0.997 | 0.9917 (6) 0.053 0.998 | 0.9929 (5) 0.0051
ANT_SIN [37] 0.9956 0.9954 | 0.993 (4) 0.0024 0.993 | 0.998 (1)-0.005 ANT3x3_SIN [37] 0.9919 0.9963 | 0.9924 (3) 0.0036 0.998 | 0.9931 (4) 0.0049
resnet50 [17] 0.995 0.9956 | 0.9917 (6) 0.0039 1 | 0.9937 (6) 0.0063 resnet50 [17] 0.9909 0.9961 | 0.9921 (4) 0.0040 0.996 | 0.9922 (3) 0.0038
Note: Accuracy is calculated with (true positive + true negative) / all images; all the accuracy values are closed to 1 because of the binary classification task. All numbers are rounded.
1154
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:21:06 UTC from IEEE Xplore.  Restrictions apply. If a Human Can See It, So Should Your System: Reliability Requirements for Machine Vision Components ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
datasets [ 29], specifying additional ML-related requirements for
each phase of software development processes [ 39], specifying
higher-levelrequirements[ 15],orspecifyinghowMVCsaddress
thetargetapplications[ 41].Yettheseapproachescannotbeused
tocheckreliabilityof MVCsautomatically,whereasourreliability
requirements are machine-verifiable.
Checkingreliability. MetricsfortestingMVCsrobustnessagainst
image transformations have been defined using metamorphic test-
ing[12,32,52,56].Incontrast,wefocusonadifferentsetoftransfor-
mations, the ones that do degrade the image quality while preserv-
ingthehumanopinionintheimageratherthantransformations
that can be covered with equivariant relations. Existing works also
use testing to generate corner cases [ 51], or corner case tests to in-
creaseMVCsrobustness[ 31,47].Incontrast,ourapproachdoesnot
focusoncorner-cases,butratherontypicalcasesthatcanbefound
in real-world deployments while preserving the human opinion
about the content.
Several works evaluated safety of MVCs through assessing their
robustness against adversarial examples, either by providing a test-
ingapproachtogenerateadversarialexamples[ 33,40,50]intheSE
area,providingrobustnessbenchmarks[ 10,11,34]orverifyingthe
presence of adversarial examples in a given range of image modifi-
cations [23,44] in the CV area. In contrast, our focus is on defin-
ingboundariesofimagemodificationsusinghumanperformance
within which the MVCs are expected to maintain their robustness.
Also,wedonotconsideranarbitraryrangeofimagemodifications;
ourapproachestimatestherangeoftransformationlevelsthatdoes
notaffecthumanperformance.Previously,wepresentedtheidea
ofdefiningadversarialexamplesusingIQAmodels[ 22],focusing
onlyonnon-visiblechanges.Incontrast,ourcurrentapproachcon-
sidersbothvisibleandnon-visiblechangesinabroaderrangeof
real world scenarios.Comparinghumanagainstmachines.
Priorstudiesalsoreferred
tohumanperformanceasthebenchmarkfortheevaluationoftheir
proposed methods [ 45], to better study the existing differences be-
tweenhumanandneuralnetworks[ 14],tostudyinvarianttransfor-
mations [ 26], to compare recognition accuracy [ 21], or to compare
robustness[ 16].Incontrast,ourfocusisnotoncomparinghumans
performancewithMVCs,butratherontherangesoftransformation
magnitudes that do not affect human performance.
9 CONCLUSION
In this paper, we defined reliability of machine vision components
(MVC)asâ€˜ifahumancanseeit,soshouldtheMVCâ€™.Moreprecisely,
we specified two classes of reliability requirements: correctness-
preservationandprediction-preservation.Ourrequirementsspecify
that an MVC should be reliably unaffected by safety-related image
transformations, at least within the range of changes that does not
affect humans. We showed, through an evaluation with 13 state-of-
the-art pre-trained image classification models, that our approach
captures reliability gaps that state-of-the-art reliability methods
are unable to detect. Therefore, we conclude that checking thishuman tolerated range is important to help software engineers
ensure quality and reliability of MVCs. While not discussed in the
paper, our requirements can be used for other SE tasks such as
checkingrefinementfromhigher-levelsystemrequirements,andcheckingconsistencyandcompatibilitywithrequirementsofother
connected components.
Inthefuture,weaimtoimproveofourrequirement-checking
process by providing reliability diagnosis that would help software
engineersunderstandthereliabilitygapsintheirMVCs.Wealso
aim to validate, through additional experiments, our assumptionthat our approach can be applicable beyond image classification
models, e.g., to handle object detection. Finally, we aim to use our
reliability requirements for MVCs to provide evidence for building
safety assurance cases for the overall system.
ACKNOWLEDGMENTS
Theauthorswouldliketothanktheanonymousreviewersfortheir
feedback and insightful comments. We also thank all the MTurkersforparticipatinginourexperiments;Dr.DimitriosPapadopoulosfor
providinganMTurkexperimentimplementationthatformedthebasisofourexperiments;ProfessorRaduCraiuforhiscomments
onthestatisticalmethods;Dr.NikitaDvornik,NickFeng,Dr.Mona
Rahimi, Valentina Manferrari, Dr. Ramy Shahin, Alexander Tough,
and Dr. Shurui Zhou for helping improve this manuscript; and
ValentinaManferrariforherassistanceduringanearlierversionof
the experiments.
REFERENCES
[1] IEEE Standard Glossary of Software EngineeringTerminology. IEEE Std 610.12-
1990, pages 1â€“84, 1990.
[2]K.B.Athreya. BootstrapoftheMeanintheInfiniteVarianceCase. TheAnnals
of Statistics, 15(2):724â€“731, 1987.
[3]OsbertBastani,YaniAIoannou,LeonidasLampropoulos,DimitriosVytiniotis,
Aditya V. Nori, and Antonio Criminisi. Measuring Neural Net Robustness with
Constraints. In NeurIPSâ€™16, 2016.
[4]N Boudette. â€˜It Happened So Fastâ€™: Inside a Fatal Tesla Autopilot Accident. https:
//www.nytimes.com/2021/08/17/business/tesla-autopilot-accident.html,August
2021.
[5]N Boudette and N. Chokshi. U.S. Will Investigate Teslaâ€™s Autopilot System
Over Crashes With Emergency Vehicles. https://www.nytimes.com/2021/08/16/
business/tesla-autopilot-nhtsa.html, August 2021.
[6]Alexander Buslaev, Vladimir I. Iglovikov, Eugene Khvedchenya, Alex Parinov,
Mikhail Druzhinin, and Alexandr A. Kalinin. Albumentations: Fast and Flexible
Image Augmentations. Information, 11(2), 2020.
[7]D.M.ChandlerandS.S.Hemami. VSNR:AWavelet-BasedVisualSignal-to-Noise
Ratio for Natural Images. IEEE Trans. on Image Processing, 16(9):2284â€“2298, 2007.
[8]Chiara Picardi and Colin Paterson and Richard Hawkins and Radu Calinescu
and Ibrahim Habli. Assurance Argument Patterns and Processes for MachineLearning in Safety-Related Systems. In SafeAI@AAAI, volume 2560 of CEUR
Workshop Proceedings, pages 23â€“30. CEUR-WS.org, 2020.
[9]Jeremy M. Cohen, Elan Rosenfeld, and J. Zico Kolter. Certified Adversarial
Robustness via Randomized Smoothing. ArXiv, abs/1902.02918, 2019.
[10]Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo
Debenedetti,NicolasFlammarion,MungChiang,PrateekMittal,andMatthias
Hein. RobustBench: a standardized adversarial robustness benchmark. arXiv
preprint arXiv:2010.09670, 2020.
[11]YinpengDong,Qi-AnFu,XiaoYang,TianyuPang,HangSu,ZihaoXiao,andJun
Zhu. Benchmarking Adversarial Robustness on Image Classification. In Proc. of
CVPRâ€™20, pages 321â€“331, 2020.
[12]Anurag Dwarakanath, Manish Ahuja, Samarth Sikand, Raghotham M. Rao, R. P.
Jagadeesh Chandra Bose, Neville Dubash, and Sanjay Podder. Identifying imple-
mentation bugs in machine learning based image classifiers using metamorphic
testing. ISSTA 2018, page 118â€“128, New York, NY, USA, 2018. Association for
Computing Machinery.
[13]Bradley Efron and Robert J Tibshirani. An Introduction to the Bootstrap. CRC
press, 1994.
[14]Chaz Firestone. Performance vs. Competence in Humanâ€“Machine Comparisons.
volume 117, pages 26562â€“26571. National Academy of Sciences, 2020.
[15]LydiaGauerhof,RichardHawkins,ChiaraPicardi,ColinPaterson,YukiHagiwara,
and Ibrahim Habli. Assuring the Safety of Machine Learning for Pedestrian
Detection at Crossings. In Proc. of SAFECOMPâ€™20, pages 197â€“212, 2020.
1155
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:21:06 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Boyue Caroline Hu, Lina Marsso, Krzysztof Czarnecki, Rick Salay, Huakun Shen, and Marsha Chechik
[16]RGeirhos,CRMedinaTemme,JRauber,HHSchÃ¼tt,MBethge,andFAWichmann.
Generalisation in Humans and Deep Neural Networks. In Proc. of NeurIPSâ€™18,
pages 7549â€“7561. Curran, 2019.
[17]Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning
for Image Recognition. 2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 770â€“778, 2016.
[18]DanHendrycks,StevenBasart,NormanMu,SauravKadavath,FrankWang,EvanDorundo,RahulDesai,TylerLixuanZhu,SamyakParajuli,MichaelGuo,D.Song,
J. Steinhardt, and J. Gilmer. The many faces of robustness: A critical analysis of
out-of-distribution generalization. ArXiv, abs/2006.16241, 2020.
[19]Dan Hendrycks and Thomas Dietterich. Benchmarking Neural Network Robust-
ness to Common Corruptions and Perturbations. Proc. of ICLRâ€™19, 2019.
[20]Dan Hendrycks, Norman Mu, E. D. Cubuk, Barret Zoph, J. Gilmer, and BalajiLakshminarayanan. AugMix: A Simple Data Processing Method to Improve
Robustness and Uncertainty. ArXiv, abs/1912.02781, 2020.
[21]T. Ho-Phuoc. CIFAR10 to Compare Visual Recognition Performance between
Deep Neural Networks and Humans. ArXiv, abs/1811.07270, 2018.
[22]B. C. Hu, R. Salay, K. Czarnecki, M. Rahimi, G. Selim, and M. Chechik. Towards
Requirements Specification for Machine-learned Perception Based on Human
Performance. In AIREâ€™20, pages 48â€“51, 2020.
[23]XiaoweiHuang,MartaKwiatkowska,SenWang,andMinWu. SafetyVerification
of Deep Neural Networks. In CAVâ€™17, pages 3â€“29, 2017.
[24]FuyukiIshikawaandNobukazuYoshioka.HowDoEngineersPerceiveDifficulties
in Engineering of Machine-learning Systems?: Questionnaire Survey. In Marcus
Ciolkowski, Dusica Marijan, Matthias Galster, Weiyi Shang, Andreas Jedlitschka,
Rakesh Shukla, and Kanchana Padmanabhan, editors, CESSER-IP@ICSE 2019,
pages 2â€“9. IEEE / ACM, 2019.
[25]H. Kaindl and J. Ferdigg. Towards an Extended Requirements Problem Formula-
tion for Superintelligence Safety. In Proc. of AIREâ€™20, pages 33â€“38, 2020.
[26]Saeed Reza Kheradpisheh, Masoud Ghodrati, Mohammad Ganjtabesh, and Timo-
thÃ©e Masquelier. Deep Networks Can Resemble Human Feed-forward Vision in
Invariant Object Recognition. Scientific reports, 6(1):1â€“24, 2016.
[27]Klim Kireev, Maksym Andriushchenko, and Nicolas Flammarion. On the Ef-fectiveness of Adversarial Training Against Common Corruptions. ArXiv,
abs/2103.02325, 2021.
[28]Roger Koenker, Pin Ng, and Stephen Portnoy. Quantile smoothing splines.
Biometrika, 81(4):673â€“680, 1994.
[29]Marc Kohli, RonaldSummers, and Jr Geis. Medical Image Data andDatasets in
the Era of Machine Learning. JDI, 30(4):392â€“399, 2017.
[30]AlexKrizhevsky,GeoffreyHinton,etal. LearningMultipleLayersofFeatures
from Tiny Images. 2009.
[31]SeokhyunLee,SooyoungCha,DainLee,andHakjooOh. EffectiveWhite-Box
Testing of Deep Neural Networks with Adaptive Neuron-Selection Strategy.
ISSTA2020,page165â€“176,NewYork,NY,USA,2020.AssociationforComputing
Machinery.
[32]Rohan Reddy Mekala,Gudjon Einar Magnusson, AdamPorter, Mikael Lindvall,
and Madeline Diep. Metamorphic detection of adversarial examples in deep
learningmodelswithaffinetransformations.In Proceedingsofthe4thInternational
Workshop on Metamorphic Testing, (MET@ICSEâ€™19), Montreal, QC, Canada, pages
55â€“62. IEEE / ACM, 2019.
[33]MarcoMelis,AmbraDemontis,BattistaBiggio,GavinBrown,GiorgioFumera,
andFabioRoli. IsDeepLearningSafeforRobotVision?AdversarialExamples
againsttheIcubHumanoid. In ProceedingsoftheIEEEInternationalConference
on Computer Vision Workshops, pages 751â€“759, 2017.
[34]Claudio Michaelis, Benjamin Mitzkus, Robert Geirhos, Evgenia Rusak, Oliver
Bringmann,Alexander S.Ecker, MatthiasBethge, andWielandBrendel. Bench-marking Robustness in ObjectDetection: Autonomous Driving when WinterisComing. arXiv preprint arXiv:1907.07484, 2019.
[35]
Dim P. Papadopoulos, Jasper R. R. Uijlings, Frank Keller, and Vittorio Ferrari.
Training Object Class Detectors with Click Supervision. In 2017 IEEE Conference
on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July
21-26, 2017, pages 180â€“189. IEEE Computer Society, 2017.
[36]Mona Rahimi, Jin L. C. Guo, Sahar Kokaly, and Marsha Chechik. Toward Re-
quirementsSpecificationforMachine-LearnedComponents. In AIREâ€™19,pages
241â€“244, 2019.
[37]E.Rusak,LukasSchott,RolandS.Zimmermann,JulianBitterwolf,O.Bringmann,M.Bethge,andWielandBrendel. Increasingtherobustnessofdnnsagainstimage
corruptions by playing the game of noise. ArXiv, abs/2001.06057, 2020.
[38]Olga Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Zhiheng Huang,
A.Karpathy,A.Khosla,M.Bernstein,A.Berg,andLiFei-Fei. ImageNetLarge
Scale Visual Recognition Challenge. International Journal of Computer Vision,
115:211â€“252, 2015.
[39]Rick Salay and Krzysztof Czarnecki. Using Machine Learning Safely in Automo-
tiveSoftware:AnAssessmentandAdaptionofSoftwareProcessRequirements
in ISO 26262. ArXiv, abs/1808.01614, 2018.
[40]Alex Serban, Erik Poll, and Joost Visser. Adversarial Examples on Object Recog-
nition:AComprehensiveSurvey. ACMComputingSurveys(CSUR),53(3):1â€“38,
2020.[41]SanjitA.SeshiaandDorsaSadigh. TowardsVerifiedArtificialIntelligence. ArXiv,
abs/1606.08514, 2016.
[42]H. R. Sheikh and A. C. Bovik. Image information and visual quality. IEEE
Transactions on Image Processing, 15(2):430â€“444, 2006.
[43]H. R. Sheikh, M. F. Sabir, and A. C. Bovik. A Statistical Evaluation of Recent Full
Reference Image Quality Assessment Algorithms. IEEE Transactions on Image
Processing, 15(11):3440â€“3451, 2006.
[44]Arvind Kumar Shekar, Liang Gou, Liu Ren, and Axel Wendt. Label-Free Robust-
nessEstimationofObjectDetectionCNNsforAutonomousDrivingApplications.
International Journal of Computer Vision, 129(4):1185â€“1201, 2021.
[45]J.Stallkamp,M.Schlipsing,J.Salmen,andC.Igel. Manvs.computer:Benchmark-
ingmachinelearningalgorithmsfortrafficsignrecognition. NeuralNetworks,
32:323 â€“ 332, 2012. Selected Papers from IJCNN 2011.
[46]AndreasVogelsangandMarkusBorg. RequirementsEngineeringforMachine
Learning: Perspectives from Data Scientists. In AIREâ€™19, pages 245â€“251. IEEE,
2019.
[47]JingyiWang, JialuoChen,YouchengSun, XingjunMa,DongxiaWang, JunSun,
andPengCheng. RobOT:Robustness-OrientedTestingforDeepLearningSys-
tems.2021 IEEE/ACM 43rd International Conference on Software Engineering
(ICSE), pages 300â€“311, 2021.
[48]ZhouWangandAlanC.Bovik. Meansquarederror:Loveitorleaveit?anew
lookatsignalfidelitymeasures. IEEESignalProcessingMagazine,26(1):98â€“117,
2009.
[49]ZhouWang,AlanCBovik,HamidRSheikh,andEeroPSimoncelli. ImageQuality
Assessment: From Error Visibility to Structural Similarity. IEEE Trans. on Image
Processing, 13(4):600â€“612, 2004.
[50]Matthew Wicker, Xiaowei Huang, and Marta Kwiatkowska. Feature-Guided
Black-Box Safety Testing of Deep Neural Networks. In Proc. of TACASâ€™18, pages
408â€“426. Springer, 2018.
[51]Weibin Wu, Hui Xu, Sanqiang Zhong, Michael R. Lyu, and Irwin King. Deep
Validation:TowardDetectingReal-WorldCornerCasesforDeepNeuralNetworks.
In2019 49th Annual IEEE/IFIP International Conference on Dependable Systems
and Networks (DSN), pages 125â€“137, 2019.
[52]XiaoyuanXie,JoshuaWingKeiHo,ChristianMurphy,GailE.Kaiser,Baowen
Xu, and Tsong Yueh Chen. Testing and Validating Machine Learning Classifiers
by Metamorphic Testing. Journal of Systems and Software, 84(4):544â€“558, 2011.
[53]S. Xu, J. Wang, W. Shou, T. Ngo, A. Sadick, and X. Wang. Computer Vision
TechniquesinConstruction:aCriticalReview. ArchivesofComputationalMethods
in Engineering, pages 1â€“15, 2020.
[54]Sergey Zagoruyko and N. Komodakis. Wide residual networks. ArXiv,
abs/1605.07146, 2016.
[55]Oliver Zendel, Markus Murschitz, Martin Humenberger, and Wolfgang Herzner.CV-HAZOP: Introducing Test Data Validation for Computer Vision. In ICCVâ€™15,
pages 2066â€“2074, 2015.
[56]MengshiZhang,YuqunZhang,LingmingZhang,CongLiu,andSarfrazKhurshid.DeepRoad:GAN-basedMetamorphicTestingandInputValidationFrameworkfor
AutonomousDrivingSystems. In 201833rdIEEE/ACMInternationalConference
on Automated Software Engineering (ASE), pages 132â€“142. IEEE, 2018.
1156
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:21:06 UTC from IEEE Xplore.  Restrictions apply. 