on the mistaken assumption of interchangeable deep reinforcement learning implementations rajdeep singh hundal national university of singapore singapore rajdeep u.nus.eduyan xiao sun yat sen university shenzhen china xiaoy367 mail.sysu.edu.cnxiaochun cao sun yat sen university shenzhen china caoxiaochun mail.sysu.edu.cn jin song dong national university of singapore singapore dcsdjs nus.edu.sgmanuel rigger national university of singapore singapore rigger nus.edu.sg abstract deep reinforcement learning drl is a paradigm of artificial intelligence where an agent uses a neural network to learn which actions to take in a given environment .
drl has recently gained traction from being able to solve complex environments like driving simulators 3d robotic control and multiplayer online battle arena video games.
numerous implementations of the state of the art algorithms responsible for training these agents like the deep q network dqn and proximal policy optimization ppo algorithms currently exist.
however studies make the mistake of assuming implementations of the same algorithm to be consistent and thus interchangeable .
in this paper through a differential testing lens we present the results of studying the extent of implementation inconsistencies their effect on the implementations performance as well as their impact on the conclusions of prior studies under the assumption of interchangeable implementations.
the outcomes of our differential tests showed significant discrepancies between the tested algorithm implementations indicating that they are notinterchangeable.
in particular out of the five ppo implementations tested on games three implementations achieved superhuman performance for of their total trials while the other two implementations only achieved superhuman performance for less than of their total trials.
furthermore the performance among the high performing ppo implementations was found to differ significantly in nine games.
as part of a meticulous manual analysis of the implementations source code we analyzed implementation discrepancies and determined that code level inconsistencies primarily caused these discrepancies.
lastly we replicated a study and showed that this assumption of implementation interchangeability was sufficient to flip experiment outcomes.
therefore this calls for a shift in how implementations are being used.
in addition we recommend for replicability studies for studies mistakenly assuming implementation interchangeability drl researchers and practitioners to adopt corresponding author.
shenzhen campus of sun yat sen university.
ieee.
personal use of this material is permitted.
permission from ieee must be obtained for all other uses in any current or future media including reprinting republishing this material for advertising or promotional purposes creating new collective works for resale or redistribution to servers or lists or reuse of any copyrighted component of this work in other works.the differential testing methodology proposed in this paper to combat implementation inconsistencies and the use of large environment suites.
index terms reinforcement learning differential testing i. i ntroduction deep learning dl and deep reinforcement learning drl are popular paradigms of artificial intelligence ai that use neural networks to solve a problem.
different from dl where the dataset is fixed and re used throughout the training process drl allows for online learning in a controlled environment where the dataset is not fixed but rather procured from the environment on the fly the very reason it is chosen over dl in some scenarios such as video games and autonomous driving .
more formally drl is a paradigm of ai where a program or more specifically an agent learns the optimal action to take in an environment after iterating multiple times through it.
drl has proven to be extremely effective in games with recent advances in the field for example deepmind s alphazero became the first algorithm to beat a world champion computer program at chess shogi and go and openai s openai five became the first algorithm to defeat the human world champions at the popular online video game dota .
furthermore drl s applications span beyond those of games from time dependent systems like autonomous vehicles and stock trading to precision focused systems like 3d robotic control .
similar to dl libraries like tensorflow and pytorch numerous drl libraries providing algorithm implementations have been created.
with more than 10k github stars rllib baselines and dopamine are among the most popular drl libraries.
rllib from ray specialises in the distributed training of drl algorithms at a scalable level.
baselines from openai and dopamine from google arearxiv .22575v1 mar 2025research focused libraries built towards the quick replication and refinement of drl algorithms.
in drl studies it is common to conduct comparisons between different algorithms .
moreover as multiple implementations of an algorithm currently exist some comparative studies use unoriginal implementations .
this is because researchers as well as practitioners assume that an algorithm would perform equally well across its implementations and thus use them interchangeably .
to demonstrate this we systematically conducted a literature review which included research papers from two popular ai conferences and identified research papers making this assumption with publishing dates ranging from to .
drl algorithms are notguaranteed to be consistent.
on the contrary they have a high risk of inconsistencies.
firstly this is due to the large amount of hyperparameters tunable values that guide and control the learning process.
being a combination of both dl and reinforcement learning rl drl inherits hyperparameters from both of them.
secondly because of the large amount of hyperparameters drl libraries run a higher risk of making mistakes and improving or worsening algorithms via seemingly minor code level implementation choices .
this results in drl libraries implementing their own flavour of an algorithm leading to similar but distinct implementations of the same algorithm in the drl domain.
we conducted a pilot study to attain an initial assessment on the extent and effects of the aforementioned implementation inconsistencies.
in particular we compared five implementations of the deep q network dqn algorithm and observed performance discrepancies as well as code level inconsistencies.
if a mature algorithm like dqn which has already been extensively studied standardized and built upon exhibits such disparity across some of the most popular implementations it raises concerns about the level of inconsistency and potential issues with the implementations of less mature algorithms and the validity of existing research that assume interchangeable algorithm implementations.
varying efficacies of the same algorithm across its implementations might render comparisons from studies that assume interchangeability invalid.
this could have wide ranging effects as the conclusions of many studies might need to be re examined.
in this paper we assess the extent of implementation inconsistencies in the context of drl and how they affect research under the assumption that implementations are consistent and interchangeable.
at a high level our study relies ondifferential testing which is a well known software testing methodology .
in particular we conducted a large scale study of approximately 10k gpu hours to compare the efficacy of multiple implementations of the same algorithm to identify potential inconsistencies.
using differential testing we answer the following research questions rq1 how prevalent are discrepancies between implementations of the same algorithm?
varying efficacies of an algorithm across its implementations would produce untrustworthy results and conclusions from studies thatassume implementation interchangeability using alternate implementations over the original.
thus it is important to study the extent of these variances first before assuming that implementations are interchangeable.
we answered rq1 by using differential testing and state ofthe art drl comparison techniques to assess different implementations of the proximal policy optimization ppo algorithm in terms of efficacy i.e.
mean reward with statistical guarantees.
rq2 why do implementation discrepancies occur?
understanding why implementation discrepancies occur is vital to creating effective solutions.
thus we answered rq2 by investigating the root cause of the discrepancies found in rq1.
in particular similar to the pilot study we inspected the implementations source code for inconsistencies that accounted for the discrepancies found.
rq3 can the assumption of interchangeable implementations alter the outcomes of an experiment?
there would be cause for concern if using a different algorithm implementation was significant enough to flip experiment outcomes as studies which assumed implementation interchangeability might then need to be reexamined.
thus to determine if this was possible we answered rq3 by replicating a study that assumed implementation interchangeability but with a different deep deterministic policy gradient ddpg implementation and compared the outcomes.
our experiments showed that drl implementations were mistaken to be interchangeable leading to significant alterations in experimental outcomes.
in particular out of the five ppo implementations tested on environments three implementations consistently outperformed the other two implementations in all comparison techniques used.
moreover we statistically show that even the high performing ppo implementations differed significantly among themselves in nine environments.
when investigating the root cause of these discrepancies we found multiple code level inconsistencies that accounted for the discrepancies found.
lastly our experiments showed that this assumption of implementation interchangeability was also significant enough to alter experiment outcomes by flipping two out of the three outcomes tested from the replicated study.
thus we recommend for replicability studies for studies mistakenly assuming implementation interchangeability drl researchers and practitioners to adopt the differential testing methodology proposed in this paper to combat implementation inconsistencies and the use of large environment suites.
furthermore a key implication for researchers studying software testing is that despite drl s stochasticity and nondeterministic output differential testing still can be applied.
therefore the techniques used in this paper are potentially applicable to non ai stochastic systems that can benefit from differential testing.
our source code is publicly available and can be found at applying sbci to all trials from a configuration.
ii.
b ackground in this section we discuss the necessary terminology used in this study.
a environment and agent rl has two basic components the agent and the environment.
the agent can be viewed as the main processing component and the environment represents the domain the agent tries to solve.
at any arbitrary timestep t the environment passes the current state stand reward rtto the agent which in turn executes an action atin the environment.
agents use various algorithms to determine the best possible action atat state st. in our study we primarily considered q learning ql and policy gradient pg based algorithms as they were commonly implemented by libraries.
b stratified bootstrap confidence intervals stratified bootstrap confidence intervals sbci is a systematic and standardized way of reporting performance based metrics like an agent s mean reward in drl.
sbci involves bootstrapping the results of a few similarly configured trials to create a distribution that represents the true result distribution for that particular configuration more accurately as illustrated in figure .
all trials left are stratified bootstrapped sampling with replacement with equal proportions per environment to form a sample middle and subsequently aggregated according to a metric e.g.
mean to form an aggregated sample as .
after multiple aggregated samples are accumulated a distribution is then formed with confidence guarantees.
sbci is particularly useful when testing an algorithm over a large suite of environments since it significantly reduces the number of trials needed per environment for reliable estimates.
iii.
m otivation in this section we discuss the literature review and the pilot study that motivated this paper.
a. literature review although we were aware of studies that made the assumption of interchangeable algorithm implementations we decided to conduct a systematic andreproducible literature review to properly justify this assumption.
a methodology we reviewed recently accepted papers from two popular ai conferences the conference on neural information processing systems neurips and the international conference on machine learning icml both of the year .
since both conferences combined had more than 5k accepted papers for neurips and for icml we used the conference data provided by paper copilot and a python library named pdfplumber to automaticallydownload the accepted papers and filter them by searching for keywords.
we selected these keywords based on two assumption scenarios that primarily occurred in the studies we were already aware of when studies use an implementation that was not the original or when studies alternate between two different implementations of the same algorithm in their experiments with the latter being a manifestation of the former.
thus to narrow down our literature to review we searched for papers which had the key phrase reinforcement learning in their title as well as