treefix enabling execution with a tree of prefix es beatriz souza university of stuttgart stuttgart germany beatrizbzsouza gmail.commichael pradel university of stuttgart stuttgart germany michael binaervarianz.de abstract the ability to execute code is a prerequisite for various dynamic program analyses.
learning guided execution has been proposed as an approach to enable the execution of arbitrary code snippets by letting a neural model predict likely values for any missing variables.
although state of the art learning guided execution approaches such as lexecutor can enable the execution of a relative high amount of code they are limited to predicting a restricted set of possible values and do not use any feedback from previous executions to execute even more code.
this paper presents treefix a novel learning guided execution approach that leverages llms to iteratively create code prefixes that enable the execution of a given code snippet.
the approach addresses the problem in a multi step fashion where each step uses feedback about the code snippet and its execution to instruct an llm to improve a previously generated prefix.
this process iteratively creates a tree of prefixes a subset of which is returned to the user as prefixes that maximize the number of executed lines in the code snippet.
in our experiments with two datasets of python code snippets treefix achieves and more coverage relative to the current state of the art in learningguided execution covering a total of and of all lines in the code snippets.
i. i ntroduction executing code is essential for reasoning about the runtime behavior of code e.g.
in a dynamic program analysis when extracting runtime data to train a model or when trying to understand code during manual debugging.
however getting code to actually run is challenging both at the small and the large scale.
at the small scale individual code snippets e.g.
found in documentation or online forums may contain undefined variables functions or classes which prevent the code from executing.
at the large scale setting up a complex project is often difficult due to the diversity of build systems missing dependencies configuration issues etc.
even when a project is perfectly set up executing a specific code location will require a specific set of inputs which may not be available.
to enable the execution of arbitrary code snippets either stand alone snippets that simply are not executable on their own or code snippets extracted from a larger project researchers have proposed learning guided execution .
the basic idea to predict likely values for any missing variables in a code snippet with a neural model preventing the execution from getting stuck.
learning guided execution has numerous applications because it enables dynamic analysis of code snippets in isolation.
since its introduction in the community has started to explore several applications such asdetecting runtime type errors reproducing bugs and checking whether a code change preserves the semantics .
learning guided execution may also serve as a mechanism for validating code or code changes produced by an autoprogramming technique e.g.
llm based code completion and code editing .
the ability of learning guided execution to execute arbitrary code without requiring the full setup of the project should make it relatively easily applicable to a wide range of codebases.
while the current state of the art in learning guided execution lexecutor can enable the execution of a relative high amount of code it suffers from two key limitations lexecutor predicts values sampled from a limited set of runtime values .
specifically their neural model predicts one out of abstract values such as non empty string which then are concretized into a hard coded concrete value such as a .
these values may not match realistic values as they would occur in a real execution of the given code snippet and they may not be diverse enough to reach all branches.
lexecutor is designed for and evaluated on the task of executing a code snippet once which may not be sufficient to cover all branches.
furthermore executing the snippet only once prevents the approach from leveraging feedback from previous executions to improve the environment in which the given code snippets gets executed.
overall these two limitations curtail the effectiveness of the state of the art approach at covering the lines in a given code snippet.
this paper presents treefix a novel learning guided execution approach that introduces several ideas.
first instead of training a custom model to predict abstract values treefix builds upon a large language model llm to predict code that constructs concrete values.
we refer to the code that constructs these values as prefixes because the code gets prepended to the given code snippet before executing them together.
by generating code prefixes treefix can create a much larger and more diverse set of values including domain specific strings complex objects and even values returned from third party libraries.
second treefix reasons about the problem in a multistep fashion where each step uses feedback about the code snippet and its execution to instruct the llm to improve a previously generated prefix.
the prefixes generated by treefix form a tree where each node represents a prefix and an edge represents a refinement of a prefix in the next step.
finally treefix produces not only a single execution but also returns a minimal set of prefixes that maximize the number ofarxiv .12339v2 jan 2025cumulatively executed lines in the code snippet.
the approach consists of three fully automated steps which are designed to mimic the way a human may approach the problem of enabling the execution of a code snippet.
step statically identify undefined