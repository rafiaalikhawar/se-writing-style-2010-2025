qedcartographer automating formal verification using reward free reinforcement learning alex sanchez stern university of massachusetts amherst ma usa sanchezstern cs.umass.eduabhishek varghese university of massachusetts amherst ma usa avarghese cs.umass.eduzhanna kaufman university of massachusetts amherst ma usa zhannakaufma umass.edu dylan zhang university of illinois urbana champaign champaign il usa shizhuo2 illinois.edutalia ringer university of illinois urbana champaign champaign il usa tringer illinois.eduyuriy brun university of massachusetts amherst ma usa brun cs.umass.edu abstract formal verification is a promising method for producing reliable software but the difficulty of manually writing verification proofs severely limits its utility in practice.
recent methods have automated some proof synthesis by guiding a search through the proof space using a theorem prover.
unfortunately the theorem prover provides only the crudest estimate of progress resulting in effectively undirected search.
to address this problem we create qedcartographer an automated proofsynthesis tool that combines supervised and reinforcement learning to more effectively explore the proof space.
qedcartographer incorporates the proofs branching structure enabling rewardfree search and overcoming the sparse reward problem inherent to formal verification.
we evaluate qedcartographer using the coqgym benchmark of .5k theorems from open source coq projects.
qedcartographer fully automatically proves .
of the test set theorems.
previous search based proof synthesis tools tok tac astactic passport and proverbot9001 which rely only on supervised learning prove .
.
.
.
and .
respectively.
diva which combines tools proves .
.
comparing to the most effective prior tool proverbot9001 qedcartographer produces shorter proofs faster on average over the theorems both tools prove.
together qedcartographer and non learning based coqhammer prove .
of the theorems while coqhammer alone proves .
.
our work demonstrates that reinforcement learning is a fruitful research direction for improving proof synthesis tools search mechanisms.
index terms formal verification proof assistants proof synthesis reinforcement learning i. i ntroduction operational software failures cost the us economy .
trillion per year .
one method for improving software quality is formal verification in which an engineer mathematically proves that a program will operate as specified.
a common method for formal verification is through the use of proof assistants such as coq and hol4 .
such verification is highly effective at reducing bugs.
for example a study of c compilers found that every tested compiler including llvm and gcc had bugs except the portions of compcert formally verified in coq.
airbus france uses compcert to ensure safety and improve performance of itsaircraft .
and chrome android and firefox use formally verified cryptographic code to secure communication .
unfortunately verifying software can be incredibly time consuming and require significant expertise.
as one example the proofs formally verifying compcert are times longer than the code for the compiler itself .
machine learning based approaches can automatically synthesize proofs using a predictive model learned from existing proofs to guide a metaheuristic search through the space of possible proofs.
however these tools can only verify just over one fifth of the properties of a large benchmark because they lack a way to assess proof progress or direct the search towards more promising paths.
the central goal of this paper is to improve the search mechanisms used to synthesize formal verification proofs.
reinforcement learning uses continuously gathered experience and can in theory automatically learn a smarter way to search through the proof space prioritizing paths more likely to lead to a successful proof.
unfortunately the proof space is ill suited for direct reinforcement learning application because it suffers from the sparse rewards problem only the final proofs can be measured as successes or failures.
prior work used hand crafted intermediary rewards known as reward shaping but this can prevent learning target behavior often causing the model to repeatedly trigger the added rewards .
significantly reducing the expressivity of the model can help but every such restriction decreases the number of properties that can be proven automatically .
in this paper we present qedcartographer an automated proof synthesis tool that improves on the state of the art search strategies using a progress estimation function learned with reinforcement learning.
this function allows qedcartographer to synthesize proofs for more theorems and to synthesize shorter proofs more efficiently.
we modify standard reinforcement learning in two key ways first we generalize a standard reinforcement learning equation to work for hyperstates which are states composed of ieee acm 47th international conference on software engineering icse .
ieee sub states.
this more faithfully models the way multiple proof goals interact each goal can be split into multiple subgoals to be dispatched individually.
this approach enables learning from progress on subproofs addressing the sparse rewards problem while avoiding reward shaping and its pitfalls.
second we remove the reliance on a fixed action space instead training a action predictor using supervised learning to provide a unique set of actions at each state.
this combines the benefits of supervised learning the ease of bootstrapping and training and reinforcement learning learning from both ground truth proofs and proofs found via experimental exploration .
qedcartographer s unique contribution is improving the search mechanism and its search strategies can potentially improve many existing tools that use undirected search .
we evaluate qedcartographer on .5k theorems from the coqgym benchmark of open source coq projects .
on its test set of 12k theorems the best performing prior tool that relies purely on supervised learning proverbot9001 proves .
.
qedcartographer proves .
.
coqhammer an smt solver based approach that applies known mathematical facts to attempt to construct a proof is complementary to qedcartographer together they automatically prove .
.
compared to a version of proverbot9001 modified with qedcartographer s improvements but still using its original search qedcartographer proves more theorems a increase together they prove .
more theorems than proverbot9001 alone.
for theorems both qedcartographer and proverbot9001 prove on average qedcartographer produces shorter proofs faster.
the main contributions of our work are an algorithm for overcoming the sparse reward problem in the proof synthesis domain by learning proof state values adapted for the branching structure of proofs.
a method for overcoming the infinite action space problem of proofs by combining models trained using supervised learning and reinforcement learning.
an implementation of multiple novel proof search strategies to make use of the state evaluation function learned via reinforcement learning.
a reification of these advances in qedcartographer and an evaluation on the coqgym benchmark of real world theorems from open source projects .
the rest of the paper is structured as follows.
section ii lays out the necessary background and context for our work.
section iii describes qedcartographer and section iv evaluates it on real world data.
section v places our work in the context of related research and section vi summarizes our contributions.
ii.
f ormal verification and machine learning before describing qedcartographer we first overview the background necessary to understand this paper.a.
theorem proving in coq our work focuses on the task of synthesizing proofs for the coq proof assistant.
coq is a formal proof management system that includes a formal specification language able to describe executable algorithms mathematical definitions and proofs.
qedcartographer interfaces with coq through an interface similar to the one coq users use.
coq users write program and datatype definitions in an ocaml like language and then proceed to write logical statements specifications about those programs they would like to prove.
at the beginning of the proof the user is presented with a single goal also known as a proof obligation .
the obligation takes the form of a logical statement which is also a type in dependent type theory.
the user then writes aproof script which we call a proof which invokes a series of commands called tactics .
each tactic takes the current obligation and either proves it or manipulate it to produce one or more new obligations.
the set of obligations at any time is called a proof state.
once there are no more obligations left to prove the proof runs the qed command and the kernel machine checks the proof s correctness.
b. machine learning based proof synthesis tools qedcartographer joins the growing body of tools that use machine learning to prove theorems.
like qedcartographer these tools employ models learned using supervised learning that predict the next tactics likely to guide a proof to completion.
guided by these models the tools apply various search strategies to explore the proof space.
proverbot9001 for instance uses a weighted depth first search dfs and employs search tree pruning.
section v will discuss search based and other proof synthesis approaches.
there are infinitely many ways prove each true theorem and training and evaluation datasets these tools use typically include a single human written proof which is not canonical in any sense.
tools sometimes find shorter or longer proofs than the ones written by humans and may use alternative yet effective proof approaches.
c. reinforcement learning qedcartographer builds on top of prior work on proof synthesis by adding reinforcement learning in improving the search strategy.
reinforcement learning is a form of machine learning that trains an agent by interacting with an environment and encountering rewards upon the completion of certain tasks.
these rewards are used to shape the parameters of the agent in developing a policy for which actions to take in a given state.
state value functions also known as v value functions can be learned in several ways including using the bellman equation a method of determining the value of a decision problem based on intermittent payoffs rewards .
section iii b will describe one of qedcartographer s key contributions the generalization of the bellman equation.
for a reinforcement 308proof start intros induction n unfold symmetry destruct n... ... ... ... fig.
proverbot9001 uses a probability driven depth first depth limited search.
a tactic predictor estimates the probability of each tactic shown as and proverbot9001 explores higher probability nodes first.
here a time limit prevents this search from finding a proof for theorem add 0 r forall n nat n n .
learning problem the bellman equation for a fixed policy describing the value of a state is v s r s s x s p s s s v s where v s is the value of state sunder policy r s s is the reward of following the policy in state s is the rate at which future rewards are discounted over current ones p s s s is the likelihood of ending up in state s after following the policy in state s and v s is the recursive value of state s .
the bellman optimality equation describes the value of a state under the optimal policy v s max a r s a x s p s s a v s where following the fixed policy is replaced by taking the action at each step that maximizes the v value.
in deterministic environments such as theorem proving the equation can be simplified to v s max a r s a v s iii.
qedc artographer qedcartographer our proof synthesis approach combines reinforcement learning supervised learning and search.
its core functionality is a state evaluation agent trained using reinforcement learning to guide search and determine which search paths to explore.
this agent uses a tactic prediction model trained using supervised learning to constrain the agent s action space to the most promising tactics.
the tactic prediction model used by prior tools is not a contribution of our work and thus this section focuses on the state evaluation agent and how it integrates with the supervised tactic predictor and with proof search.
section iii a illustrates how qedcartographer improves on the state of the art.
a. illustrative example suppose a proof search tool is trying to prove the theorem theorem add 0 r forall n nat n n proof start intros induction n unfold symmetry destruct n... reflexivity simpl congruence qed4.
steps .
steps .
steps .
steps6.
steps .
steps .
steps .
stepsunfold .
steps rewrite ihn .
stepsfig.
qedcartographer guides its search using a state estimator function which estimates how close each searchtree node is to a complete proof shown as number of steps remaining .
exploring the nodes with lower more promising estimates first leads to a successful proof for theorem add 0 r forall n nat n n .
prior tools use a tactic predictor to predict the first most likely tactic based on supervised learning on existing proofs and then perform search through the proof space using these predictions .
figure shows how one such prior tool proverbot9001 explores the proof space using a probability driven depth first depth limited search with a time limit.
here the tactic predictor predicts intros with probability induction n with probability and several other lower probability tactics indicated with .
.
.
in figure .
the predictor first predicts the tactic e.g.
induction then computes all possible in scope arguments and then predicts the most likely argument e.g.
n. proverbot9001 picks the most likely intros and asks the tactic predictor for the next most likely tactic.
the predictor predicts unfold with probability symmetry with probability and destruct n with probability .
proverbot9001 selects unfold and continues growing this proof using depth first search and checking each partial proof using the theorem prover for whether it proves all obligations or results in an error.
proverbot9001 s search is depth limited.
if exploring to the limit s depth does not produce a complete proof proverbot9001 backtracks to earlier states and explores the next highestprobability prediction.
the search continues until either a proof proves all obligations or a time limit is reached.
in figure proverbot9001 never finds the correct proof because for the proof to succeed it must do induction on n. but the tactic predictor never predicts induction n after intros n because such a combination is very rare in its training data and proverbot9001 runs out of time before backtracking all the way to the start of the proof.
our example uses a reduced time limit to illustrate the shortcoming of depth first search.
if proverbot9001 were to use breadth first search instead it would explore induction n early in the search but would then require fully exploring the search tree to a depth of 309tactics.
this would similarly exhaust the reduced time budget as the explored steps grow exponentially in tree depth.
by contrast qedcartographer uses a learned state evaluation function to guide its search as shown in figure .
using the same tactic predictor as proverbot9001 qedcartographer starts by predicting intros andinduction n as the highest probability tactics.
qedcartographer uses the theorem prover to check the predictions for errors and to compute the proof states that result from executing each tactic.
next qedcartographer uses its state evaluation function to estimate the number of steps required to prove the unproved goals in those proof states.
this function predicts that the proof that uses intros will require .
more steps whereas using induction n will require .
and thus selects just as proverbot9001 did intros .
however after predicting the next tactics computing the proof states and estimating the steps remaining for each qedcartographer s state evaluation function estimates that unfold will require .
more steps symmetry .
and destruct n .
.
these search paths now appear less promising than the induction npath and so qedcartographer will expand induction n next.
while the state evaluation predictions for the next few tactics are not monotonically decreasing reflexivity predicts .
additional steps simpl .
etc.
they are each more promising than the alternatives and so qedcartographer explores each of these states until finding that congruence proves all goals completing the proof.
qedcartographer finds the proof of the theorem exploring a fraction of the states the prior approaches explored.
for simplicity this example did not differentiate between qedcartographer s two search strategies best first and a but section iii b4 will detail their differences.
next section iii b will detail how qedcartographer s state evaluator computes proof state values and is used in search and section iii c will describe how qedcartographer uses theorem statements and data to train the estimator.
b. state evaluation model architecture qedcartographer uses reinforcement learning to train an agent that estimates the difficulty of proof states how hard is it to finish the proof from this state also known as a v value model.
figure shows how qedcartographer synthesizes proofs using its reinforcement learning trained state value model.
given a theorem qedcartographer uses a search strategy such as a or best first described in section iii b to navigate the search through the proof space.
the search strategy uses the tactic predictor to produce potential actions next steps of the proof and then uses the interactive theorem prover to compute the proof contexts that result from executing each of these actions.
these proof contexts are hyperstates consisting of several obligations that need to be proven.
code2vec described in section iii b encodes each obligation in these hyperstates and the state value model trained using reinforcement learning then estimates each obligation s difficulty.
the hyperstate value aggregator combines these estimates into a single estimate of the difficulty of the entire hyperstate in our case multiplying the values as described multiple updated partial proofs and obligationsinteractive theorem proversearch strategy code2v ecstate v alue modelhyperstate value aggregatorproof obligation valueshyperstate obligation valueproof contexts encoded contextsprooftheorem tactic predictorpartial proof next tacticfig.
given a theorem qedcartographer attempts to synthesize its proof.
a search strategy uses the tactic predictor to produce potential actions.
for each action the interactive theorem prover computes obligations that need to be proven and code2vec and the state value model estimate the difficulty of each obligation which the hyperstate value aggregator combines into a single estimate per action in our case by multiplying the estimates together .
the search uses these estimates to determine which action to take.
a partial proof that proves all obligations proves the original theorem.
in section iii b .
the search strategy then determines which hyperstate should be expanded next.
if the interactive theorem prover determines that a proof leads to a context with no remaining obligations that proof proves the original theorem.
qedcartographer has four components that overcome key challenges of using reinforcement learning in a proof search context first to build a tractable action space for exploration the state agent builds on a supervised tactic predictor that produces a small set of viable tactics at each state.
second to provide a state encoding that captures the complexity of the proof space the state agent uses a text sequence model trained using autoencoding.
third to smooth the sparse rewards inherent to theorem proving qedcartographer s state agent models each proof state as a hyperstate built of multiple obligations.
fourth to use the learned state evaluation agent to guide proof search qedcartographer implements two new search methods for proof synthesis best first and a search.
next we describe each of these components in more detail.
building on a supervised predictor an important part of the design of a reinforcement learning system is defining the action space.
in qedcartographer s theorem proving task actions correspond to adding a tactic to the proof.
unfortunately the full space of tactics is infinite.
there are over built in tactics in coq and many of them can take as arguments arbitrarily large terms in the underlying logical calculus.
qedcartographer addresses this issue by combining its reinforcement learning agent with a tactic predictor trained using supervised learning on existing human written proofs.
instead of asking the agent to pick from the set of all possible actions or even a fixed finite actions space qedcartographer first asks the supervised tactic predictor to predict the most likely nactions over the infinite action space where nis 310a hyperparameter .
in the initial state of the example from figure the supervised tactic predictor predicts intros and induction among others and then runs each prediction to get the resulting state for evaluation.
by building its reinforcement agent on top of a tactic predictor trained using supervised learning qedcartographer makes the action space tractable and reduces the number of coq queries run.
state encoding using coq2vec autoencoder to allow for deep reinforcement learning qedcartographer embeds proof states into a vector space where the model can learn to estimate the value of each state.
however proof contexts consist of lists of sequences of characters constituting a discrete infinite state space making the application of reinforcement learning particularly challenging since a mapping from a state to its values is difficult to learn.
therefore qedcartographer needs to find a one to one mapping from the discrete infinite space to a continuous real space or alternatively speaking to learn continuous embeddings for each state.
our approach to overcoming this challenge involves the use of an autoencoder a deep representation learning approach that is trained in a self supervised manner with the objective of first compressing the input into a latent space representation and then reconstructing the input from that representation.
the strength of autoencoders lies in their ability to retain essential information of the sequence while eliminating statistical redundancies.
this is facilitated through their two core components the encoder and decoder.
the encoder is responsible for compressing the input into a latent space while the decoder is tasked with reconstructing the input from the latent space representation.
qedcartographer adopts a symmetrical lstm model for both parts.
value estimation using obligation sets one of the most difficult parts of designing robust reinforcement learning algorithms is deciding how to structure rewards.
in theorem proving the natural reward structure is sparse.
since the goal of the theorem proving task is fully proving a theorem the most natural reward structure assigns a positive reward to reaching qed and a zero reward to all other actions.
this sort of structure only rewards the agent once it has come upon a long sequence of correct actions resulting in proof completion.
in practice the agent simply does not get rewards and never updates its policy preventing learning .
the challenge we face is how to design a reward structure that allows for intermediary rewards to guide an agent to the overall goal without creating local minima for the agent to get stuck in.
one often used approach to handling sparse rewards is to smooth the reward space by inserting intermediary rewards known as reward shaping.
for instance in a video game context an agent might be rewarded for picking up a coin or defeating an enemy in addition to its reward for completing a level.
in the theorem proving context since simpler goals are often easier to prove the agent can be rewarded for making the goal simpler.
alternatively the agent can be punished via a negative reward for trying a tactic that results in an error.however if these intermediary rewards are not carefully constructed they can lead to the agent getting stuck in a local minimum which results in pathological behavior .
for instance if the agent is rewarded for making the goal simpler it might learn to only run exfalso which turns the goal to false significantly reducing its complexity but blocking further progress.
or if the agent is punished for producing errors it might learn to continually run tactics that cannot fail such as simpl making no progress but avoiding errors.
instead we need to find a different reward structure allows for intermediary rewards to guide an agent without falling prey pathological behavior.
a structure based on the solution of obligations accomplishes both these goals.
to accurately represent the branching structure of proofs qedcartographer reformulates the reinforcement learning problem to be over hyperstates sets of states instead of just states.
each hyperstate is a full proof context where its component states are the individual obligations.
in the example from figure the initial state only has one obligation so the hyperstate is composed of a single substate.
however when induction is run one obligation is produced for the base case and another for the recursive case.
the resulting hyperstate then is composed of two substates one for each obligation.
when reflexivity is run it dispatches the first obligation so the resulting hyperstate is once again composed of only one substate.
in general each tactic operates on a single obligation but in addition to transforming that obligation into a new one it can produce multiple new obligations or it can finish proving the obligation.
to use this kind of transition to update the value of a state qedcartographer defines a total value for the set of obligations produced by a tactic.
the number of steps needed to fully finish a proof from a particular state is the sum of the number needed to finish each obligation.
so since the v values are logarithmic in the number of steps needed qedcartographer combines obligation values by multiplying them.
in our example if we interpret the scores presented as the estimated steps left 1the base case from induction is estimated to have a v value of .
the value of indicating step remaining while the recursive case is estimated to have a v value of .
5 indicating steps remaining .
therefore we use the product of these values .
6 as the v value of the hyperstate that results from running induction indicating it has six steps remaining.
our equation to calculate a target v value for an obligation replacing the standard bellman equation then becomes v s max a r s a s f s a v s this equation has several advantages.
first when state values are bounded to be within the range it prevents the agent from getting rewarded for repeatedly creating and proving obligations the reward from proving an obligation never 1these scores also include the number of steps taken so far as explained in section iii b4 here we present them as best first scores for simplicity.
311overcomes the cost of generating it.
second since tactics that finish proving an obligation like reflexivity produce an empty set of resulting obligations f s a there is no need for explicit rewards a substate just before finishing an obligation will be given a v value of the same as if a reward of had been explicitly given for obligation completion.
this means explicit rewards are no longer needed in qedcartographer s algorithm as the equation for calculating values of transition implicitly rewards proving obligations.
removing the explicit reward term from the above equation results in the final equation v s max a s f s a v s this revised equation allows the reward structure to locally reward an agent for proving obligations without incentivizing the creation of spurious or trivial obligations.
using the state value estimation model for search in its main mode of operation proverbot9001 uses a weighted depth first search to explore the space of possible proofs using its tactic predictor model.
while this strategy is effective it is not efficient as the depth first search exhaustively explores paths for every encountered proof state.
qedcartographer is able to more effectively search a proof state space by utilizing its learned state value estimation agent as described via an example in section iii a. the simplest search that can make use of state value estimates is best first search where the best scoring node is explored at every step.
in this search strategy any method of sorting nodes can be applied including using the product of probabilities or sum of log probabilities for each tactic in the proof produced by the tactic predictor.
however the quality of the estimator is extremely important for effectiveness section iv e will empirically evaluate how qedcartographer s search improves upon a version of the search that uses information from the tactic predictor.
a search builds on best first search with the goal of not just finding a solution as quickly as possible but also finding the shortest solution.
in the proof space this empirically results in a higher success rate see section iv c we speculate this is because shorter proofs are more likely to be correct than those that spin far off from the original goal.
figure shows exploration using a search.
in a instead of just sorting the partial proof queue by the state score estimate partial proofs are sorted by an estimate of the total length of a solution found using that proof prefix called the fscore .
this estimate is made by adding the steps taken so far the length of the partial proof candidate and an estimate of the steps remaining to finish the proof.
in the example from figure the step estimates indicate the total estimated length of the completed proof not just the remaining steps.
the state resulting from the simpl tactic is estimated to have .
steps remaining until a qed and has already taken three steps resulting in a total scorer of .
.
for a the state value estimate must correspond to an estimate of steps left in the proof so not all state scoringfunctions can be used in particular the tactic prediction certainties cannot be used to produce a score since they cannot be converted to a steps remaining estimate.
however the state evaluation function learned by qedcartographer canbe converted to a steps remaining estimate.
to obtain the estimated number of steps remaining in a proof from the state value we first notice that for an optimal v value evaluator of a single obligation v s steps s .
therefore the estimated steps for each obligation can be computed as steps s log v s .
to complete the proof of a theorem we must prove all the obligations that exist at any given proof state.
hence the total number of steps to finish a proof from a proof state pissteps p p s plog v s .
qedcartographer uses a search for all proof synthesis except for when evaluating search techniques in our evaluation .
c. training the proof state evaluator qedcartographer s agent design overcomes several key challenges of using reinforcement learning in a proof search context.
however there are still three more challenges that cannot be addressed through model architecture alone.
these challenges guide how qedcartographer is trained.
first to address the sample inefficiency of reinforcement learning and the sparsity of theorem proving data qedcartographer trains on the subproofs within each proof described in section iii c .
second because qedcartographer uses a supervised tactic predictor to constrain its action space during training it filters out goals whose human written proofs cannot be generated because of those constraints as described in section iii c .
third since the equations for updating the agent are recursive qedcartographer is pre trained in a supervised manner to increase stability described in section iii c3.
in addition to addressing reward sparsity in qedcartographer design through hyperstate modeling qedcartographer uses three mechanisms during training to provide rewards to the agent more quickly.
these include learning by demonstration section iii c a true target buffer where v values are computed directly when possible section iii c and training dead end states to a zero v value section iii c6 .
finally to speed up training on a large number of proofs qedcartographer makes use of distributed training section iii c7 .
figure shows qedcartographer s distributed training architecture.
multiple actors deployed on different computing nodes each perform greedy search trying to prove different theorems from the training data.
these observed proof lengths are estimates of how many proof steps each obligation requires to prove.
periodically these actors send data about their experience to three buffers the replay buffer the true target buffer and the negative state buffer.
training then uses the three buffers to update the state value model weights for estimating how many proof steps each obligation takes to prove storing the results in the parameter server periodically sending those updated weights to the actors.
subproof task training in reinforcement learning a significant amount of trial and error is essential for an agent to 312actor actorreplay buffer true target buffer negative sample bufferactortraining parameter serverdistributed actors updated parametersexperience samples experience batches latest parameterstrained modeltheorem theorem theoremfig.
qedcartographer uses a distributed training architecture to train its state value model.
multiple actors each perform greedy search on different theorems from the training data periodically sending data about their experience to three buffers.
training uses these buffers to update the model weights in the parameter server periodically sending those updated weights to the actors.
learn to achieve the goal successfully a phenomena known as sample inefficiency.
unfortunately training on a large number of samples is difficult in the theorem proving environment because of the computational of environment interaction and the limited training data available.
to address these challenges qedcartographer trains on not only every proof in our training set but every obligation that is created by the human written solution to that proof.
since proving a theorem often involves creating many independently provable obligations there are many small proofs within each large proof.
for instance if the example from figure were in the training data qedcartographer could train using either of the subgoals resulting from the induction tactic since these subgoals are independently solvable they need not be trained on together.
qedcartographer takes advantage of the fact that there are human solutions to the proofs in its training data to automatically expand its training set with these subproofs.
task filtering building qedcartographer on top of a tactic predictor model trained using supervised learning allows it to simultaneously consider a large set of tactics and limit its possible actions at any given point in a proof to a much smaller most probable set of tactics.
in practice however the correct tactic or one of the correct tactics is not always in the top predictions of the tactic predictor.
since qedcartographer has human written proofs for the theorems in its training set it checks if at every step of the human written proof the supervised tactic predictor has a matching tactic in its top predictions.
it then filters out during training those proofs it will not be able to generate.
importantly we do not filter such proofs from the test set in our evaluations in section iv we only filter them from the training set.
supervised pre training to bootstrap our model qedcartographer begins by pre training in a supervised manner.
for each obligation in our training set the length of the ground truth solution to that obligation can be taken from the human written proofs.
to compute a ground truth state value qedcartogra pher only needs to raise the value of the parameter to the power of the number of tactics in the solution.
qedcartographer uses the optuna hyperparameter tuning framework to tune the hyperparameters used during pre training.
learning by demonstrations to train more quickly and accurately qedcartographer uses learning by demonstration .
in learning by demonstration the rl agent is not just dropped at the beginning of each task but instead slowly introduced to the task.
the agent is initially guided through every step of the task until the last one and then tasked with completing the last step and given a reward.
then the process is repeated but stopping at the second to last step and then again at the third to last task.
this process is repeated until the agent is solving the full task.
if the example from figure were in the training data this would mean that before exploring the full task the agent would first be guided through induction n. reflexivity.
simpl.
and then given a reward for predicting congruence .
it would then restart the proof and be guided through induction n. reflexivity.
and given a reward if it could predict simpl.
congruence .
this process greatly speeds up convergence in environments with sparse rewards like theorem proving.
however it does require access to labeled training data which are not necessary when training without demonstrations or subproofs section iii c1 .
true target buffer to train the state value estimation model qedcartographer primarily uses state transition updates based on the revised bellman equation from section iii b .
but this type of training can be unstable because the target estimates can change based on the current weights of the estimator which itself is in the process of being trained .
stability can be increased by along with sampling the target from estimates of the next state also sampling the current ground truth target if it exists.
qedcartographer does this by using a separate buffer that stores each obligation and the minimum number of steps it took the developer or the reinforcement learning agent to prove it.
when the training begins this buffer is filled with the initial states of every obligation along with the developer s solution length.
during training if our reinforcement learning agent can prove a given obligation in fewer steps than the developer this buffer is updated with the lower target for that obligation.
these new values are included in subsequent training steps by sampling part of every training batch from this buffer.
this helps make training more stable and quicker to converge to a solution.
negative examples during reinforcement learning exploration qedcartographer can encounter obligations for which all of the tactics produced by the tactic predictor produce an error.
these obligations can never be proven by an estimator built on that tactic predictor.
therefore we use them as a negative example to our agent and train their associated states to a target value of .
distributed training section ii c discussed how in theorem proving environments the proof state after each tactic can always be determined via computation but that 313computation can be intensive.
this means that rolling out the current policy in different proof obligations quickly becomes the computational bottleneck of training.
to address this challenge qedcartographer trains using a distributed reinforcement learning architecture inspired by the gorila architecture see figure .
in gorila learning and acting are similarly resource intensive so actors and learners are bundled together communicating through a shared parameter server.
since acting is much more expensive in the theorem proving environment qedcartographer instead bundles the learner and the parameter server which communicate with each of the distributed actors.
iv.
e valuation we evaluate qedcartographer s ability to synthesize proofs the effectiveness of its components and the effect of its hyperparameters.
sections iv a andiv b describe our methodology.
then section iv c compares qedcartographer to the state of the art proof synthesis tools including astactic coqhammer diva passport proverbot9001 and tok and tac .
section iv d directly evaluates the state value agent s effectiveness and section iv e performs an ablation study to understand the impact of search strategies individual obligation training and hyperparameters.
we first summarize our main findings qedcartographer automatically proves .
of the theorems in a large benchmark.
prior supervised learning tools tok tac astactic passport and proverbot9001 prove .
.
.
.
and .
respectively.
comparing to a version of proverbot9001 modified with qedcartographer s improvements but still using its original search qedcartographer proves more theorems with the same time budget and together they prove more theorems than modified proverbot9001 alone.
when both qedcartographer and proverbot9001 prove a theorem qedcartographer produces shorter proofs faster.
meanwhile combining qedcartographer and coqhammer an smt solverbased approach that applies known mathematical facts to attempt to construct a proof automatically proves .
of the theorems.
a. research questions our evaluation answers three research questions rq1 how effective is qedcartographer at proving theorems?
rq2 how effective is qedcartographer s state estimation agent at proving proof sub problems directly in a simple greedy search?
rq3 how do learning by demonstration task filtering and subproof training impact qedcartographer s effectiveness?b.
benchmarks we use the coqgym benchmark of open source coq projects with theorems.
we exclude one project coqlibrary undecidability prior evaluations similarly were unable to use this project due to internal coq errors when processing its proof scripts .
projects in the coqgym benchmark are a mixture of mathematical formalizations proven correct programs and coq automation libraries.
they include several compilers of varying sizes such as compcert distributed systems such as verdi formalizations of set theory and more.
some of the projects in coqgym such as the automation libraries contain no proofs but we include them for completeness as did prior work.
as in prior work our evaluation uses projects for training which contain a total of theorems and the remaining projects which contain a total of theorems to measure the effectiveness of our approach.
we further split the training theorems into obligations during training recall section iii c .
we filter out the obligations whose original solutions would not be reproduced with a perfect state evaluator recall section iii c .
we perform some additional filtering by removing obligations whose proofs are only or steps as we found those did not help training as well as obligations whose proofs were or more steps to decrease training time.
this resulted in a training dataset of obligations.
as mentioned earlier this filtering did not affect the theorems on which we measured performance.
for our fine grained obligation experiments and ablation studies we use the compcert c compiler as a benchmark as it is also used in the original proverbot9001 evaluation .
we use the training test split used by the proverbot9001 evaluation the training set consists of proof files and the test set consists of proof files of theorems.
c. rq1 comparison to the state of the art recall that qedcartographer s reinforcement learningdriven search can be applied to all existing search based proof synthesis tools.
to demonstrate qedcartographer s contribution to the state of the art we compare qedcartographer s search strategy with the strategy used by proverbot9001 the most effective current neural network based proof synthesis tool.
we show that applying qedcartographer s search strategy to proverbot9001 proves more theorems with the same time budget.
we further show that qedcartographer finds shorter proofs than proverbot9001 and that it finds proofs faster.
for completeness we also include proverbot9001 without a time limit its default mode of operation in figures and .
proof synthesis effectiveness qedcartographer combines supervised and reinforcement learning to synthesize proofs.
on the coqgym benchmark qedcartographer proves theorems or .
see figure .
prior tools that relied only on supervised learning and search prove fewer theorems tok proves .
tac .
astactic .
and passport .
.
diva a combination of different tools each using a different tactic prediction model proves .
of the test set though proverbot9001p qedcp qedc theorems proven2 603fig.
qedcartographer qedc proves more theorems than proverbot9001 and proverbot9001 limited to steps p .
qedcartographer is limited to steps by default.
together proverbot9001 and qedcartographer p qedc prove theorems.
comparing to qedcartographer running with a single tacticprediction model is unfair.
note that the framework all these prior tools used fails to parse theorems about of the coqgym benchmark test set.
we include these theorems as ones these tools fail to prove whereas the tools original evaluations excluded them.
coqhammer is an smtsolver based approach that applies known mathematical facts to attempt to construct a proof.
solver based tools work well at fully automating simple proofs but are generally limited to first order problems while neural tools can potentially be more general.
coqhammer for example cannot use induction.
coqhammer proves .
of the benchmark but is complementary to qedcartographer together they automatically prove .
of the theorems.
this number is likely an underestimate because we used previously published coqhammer results which also suffers from the inability to parse theorems in the test set.
finally proverbot9001 proves .
of the coqgym test set making it the most effective neural network based proof synthesis tool.
for this reason we used proverbot9001 s architecture for toolname s supervised predictor.
figure shows the number of theorems proven by proverbot9001 proverbot9001 limited to steps p qedcartographer qedc which is limited to steps by default and a combination of proverbot9001 and qedcartographer.
qedcartographer proves more theorems than proverbot9001 with the same time budget an increase of .
the two tools are somewhat complementary there are theorems qedcartographer proves that proverbot9001 cannot with the same time budget.
thus combining the two tools proves more theorems than proverbot9001 alone .
meanwhile proverbot9001 proves theorems that qedcartographer does not.
we conclude that qedcartographer s new search and state value estimation are able to guide search towards proving theorems that would not otherwise be provable.
proof length figure 6a shows the average proof length for the in common proven theorems for proverbot9001 and proverbot9001p qedcaverage solution length7.
.
.
a proverbot9001p qedcaverage time taken steps .
.
.
b fig.
a qedcartographer qedc synthesizes on average shorter proofs than proverbot9001 and proverbot9001 limited to steps p .
b qedcartographer synthesizes proofs taking fewer search steps on average than proverbot9001 and proverbot9001 limited to steps.
for fairness we only compare on theorems for which both tools were able to synthesize a proof.
qedcartographer.
the average proof length of proofs generated using proverbot9001 s depth first search is .
tactics while for qedcartographer s a search proofs it is .
tactics a reduction in proof length.
of the test theorems for which both qedcartographer s a search and proverbot9001 s depth first search find a proof within the same budget qedcartographer finds a shorter proof for theorems and proverbot9001 s finds a shorter solution for theorems .
for the remaining theorems the proofs were of the same length.
search speed figure 6b shows the average number of search steps proverbot9001 and qedcartographer needed to find a proof of a theorem.
the average number of steps proverbot9001 s depth first search required is .
compared to .
for qedcartographer s a search a decrease.
of the in common test theorems qedcartographer search takes fewer search steps for theorems while proverbot9001 takes fewer search steps for theorems .
for the remaining theorems the searches were of the equal numbers of steps.
we note that steps for a and best first search can be slightly slower than those for depth first search because those search strategies can backtrack more but improvements in proof assistant technology are rapidly closing that gap.
a is better than best first to compare qedcartographer s two new search strategies we ran qedcartographer with a and best first search on the coqgym benchmark.
a proved more theorems than best first a increase and found shorter proofs faster.
this demonstrates that qedcartographer s approach to converting a state value score to an estimate of steps remaining in a proof is an effective method for improving search.
however best first does prove several theorems that a search does not indicating that the two can be combined for additional proving power.
combining the results of both 315searches in qedcartographer proves more theorems than proverbot9001 with the same time budget.
d. rq2 qedcartographer s state value agent effectiveness we next isolate qedcartographer s state value agent from its a and best first search strategies to evaluate the agent s contribution on a more fine grained level.
we replace a and best first with a greedy exploration strategy to directly synthesize proofs.
instead of sampling from the set of actions greedy exploration always picks the highest scoring action.
we compare this greedy search to using just the pre trained tactic predictor on all proof obligations within the proofs from the compcert test set.
since engineers use proof assistants to manually prove theorems synthesizing proofs for some proof obligations in theory directly reduces the engineers required effort.
this allows the engineers to focus on a smaller set of unproven obligations.
thus the number and fraction of obligations a tool can automatically synthesize proofs for is an appropriate measure of the tool s effectiveness.
since this experiment is evaluating the state value estimator directly we filter the obligations in the compcert test set down to the that can be proven by a perfect state value estimator oracle.
we compare qedcartographer with this greedy strategy to a greedy search that uses proverbot9001 s predictions directly.
we find that qedcartographer with greedy search proves .
of the obligations in our test set compared to .
that proverbot9001 with greedy search proves.
we conclude that even without qedcartographer s backtracking search strategies its state evaluation model can be used to effectively pick from a set of predictions better than the supervised predictor can alone proving .
more proof obligations.
in this setup qedcartographer s state evaluator is unaware of the prediction certainties that the tactic predictor produced reconstructing a better ordering from scratch future work may explore ways to combine the two sources of information.
e. rq3 effects of design decisions and hyperparameters this section evaluates qedcartographer s remaining components design decisions and hyperparameters.
section iv e evaluates the impact of the search strategy alone by using bestfirst search without the learned state value estimates instead using information available in the pre trained tactic predictor.
next section iv e evaluates the choice to train on subproof obligations by comparing to a version that trains only on entire theorems.
finally section iv e evaluates the choices of width the number of actions to consider at each step during training and the time discount factor in our state value update equation .
we perform these evaluations on the compcert benchmark.
guided search with and without using qedcartographer state values to determine the impact that the state value estimator has on the effectiveness of search we ran best first search using only information available to the tactic predictor.
since proverbot9001 s tactic predictor producesprobabilities for each suggested action we use the product of those probabilities along each proof path as a state value score.
using qedcartographer state values as scores in best first search increases the number of theorems proven by .
searches with the two different state scores are complementary to some extent combining the theorems proven by both produces more proofs than using certainties alone and more proofs than using state scores alone.
because a search requires that state values be convertible to an estimate of number of steps remaining in the task it does not make sense to perform a similar comparison using a search.
individual obligation training qedcartographer trains not only on proof scripts for full theorems but also the proofs of individual proof obligations within proofs recall section iii c .
to measure the impact of this proof obligation training we also train qedcartographer without access to the proof obligations proofs.
without training on these obligations greedy search using qedcartographer can prove only .
of obligations fewer than with subproof training.
this shows that training on subproofs improves qedcartographer s proving power though the improvement is relatively small.
when qedcartographer uses a training without obligations proofs proves theorems while training with obligations proofs proves theorems an improvement of .
hyperparameter analysis during both exploration and training qedcartographer uses a fixed number of predictions from the tactic predictor.
we call this number the search width.
increasing the width allows qedcartographer to explore more options at each step which can lead to more proofs particularly shorter proofs.
however utilization of the time budget for wider exploration can reduce the chances of completing longer proofs which require higher search depth.
we tested five width values and .
a linear regression model found no significant relationship between width and the number of theorems qedcartographer proves p .
.
however there was a negative pearson correlation coefficient of .
weakly significant p .
correlation between width and the average synthesized proof length and a high positive pearson correlation coefficient of .
significant p .
correlation between width and the average proof synthesis search steps.
this suggests that higher width resulted in shorter proofs but required more search exploration consistent with higher widths exploring more shorter proofs first.
while not statistically significant anecdotally the largest width we evaluated resulted in more proofs on average .
than all other widths.
further research should look into understanding how varying the width can be leveraged to improve proving power.
when computing state values qedcartographer discounts all future rewards by a fixed constant exponentiated by the time step difference between the current state and the reward state.
varying we evaluated four values .
.
.
and .
a linear regression model found no significant 316relationship between and the number of theorems qedcartographer proves p .
the average synthesized proof length p .
and the average proof synthesis search steps p .
.
while not statistically significant anecdotally we did find that for .
qedcartographer found shorter proofs .
tactics on average than for .
.
tactics on average .
for .
qedcartographer synthesizes slightly more proofs than for .
.
for .
qedcartographer synthesizes proofs more quickly .
steps on average than for .
.
steps on average .
more research is necessary to understand more fully s effect on proof synthesis and the resulting trade offs.
v. r elated work our work applies modified reinforcement learning algorithms to improve performance on proof synthesis tasks.
the most similar related work to ours is tacticzero which uses reinforcement learning to prove theorems in hol4.
unlike qedcartographer tacticzero limits the action space to nine tactics and their arguments which loses expressivity and efficiency and relies on hand tuned reward shaping which can lead to getting stuck in locally optimal policies recall sections i and iii b .
these hand crafted rewards are necessary because the policy models full proof state transitions instead of obligation transitions so that the obligation reward structure is not automatically derived from the transition equations.
further tacticzero s evaluation is limited to only theorems with known proofs consisting of those nine tactics while we evaluate qedcartographer on all theorems in the benchmark.
deephol uses deep reinforcement learning to synthesize proofs for hol light.
a later extension to use graph neural networks without reinforcement learning outperforms deephol .
in a much simpler first order tableaucalculus based theorem prover a sequence of results apply reinforcement learning with emphasis on avoiding domain heuristics synthesizing long proofs from little training data and building a comprehensive toolkit .
hypertree uses reinforcement learning for proof search but operates only in a simplified custom made theorem prover by contrast qedcartographer runs in an existing widely used environment.
bansal et al.
use reinforcement learning to learn a premise selection task for theorem proving.
by contrast our work uses reinforcement learning alongside tactic prediction trained through supervised learning to train a neural theorem prover in a higher order dependently typed setting.
in pursuit of this goal our work contributes novel reinforcement learning algorithms not seen in prior work.
reinforcement learning can synthesize code using tests to ensure quality but such approaches do not prove correctness.
many proof synthesis approaches do not rely on reinforcement learning.
for example hammers use solvers to iteratively apply known mathematical facts to attempt to construct proofs.
coqhammer for example uses smt solvers and sledgehammer combines multiple solvers to attempt to prove individual subgoals of a theorem .
draft sketch and prove uses language models to generate sketches of formal proofs from informal proofs in isabelle hol and then fill in those sketches using sledgehammer.
thor combines a language model with sledgehammer to synthesize proofs for isabelle hol.
combining a learned tactic prediction model with a search procedure can similarly synthesize proofs e.g.
with proverbot9001 astactic tactok passport tactician and diva for coq and tactictoe and holstep for other proof assistants.
recent tools have used both small and large transformerbased language models for tactic prediction or proof synthesis.
gpt f combines a specialized pre trained language model with proof search to synthesize proofs in metamath.
lisa evaluates large language models in combination with proof search on an isabelle hol dataset they introduce.
baldur uses a fine tuned mathematical language model in combination with a novel self repairing approach to synthesize proofs for isabelle hol without using hammers or proof search.
our work can in theory augment these approaches as well to guide their predictions.
this paper demonstrated the benefits of qedcartographer s advances augmenting one such tool proverbot9001 and future work should explore qedcartographer s effects on other such tools.
recent results have shown that retrieval and memory can augment or guide models for proof synthesis to great effect.
memorizing transformers introduce a memoryaugmented transformer model and show how they can improve performance on proof related benchmarks for isabelle hol.
leandojo uses a retrieval augmented large language model to improve performance for proof synthesis for the lean proof assistant.
our work provides an additional and complementary way to augment or guide models for proof synthesis.
qedcartographer combines two different learned models to effectively explore the proof space and direct search a tactic predictor trained using supervised learning and a proof state evaluator trained using reinforcement learning.
combining different learned models to enhance performance on a task is known as ensemble learning.
ensemble methods have become common in machine learning as combined models can outperform individual ones.
ensemble methods improve accuracy by integrating the predictions of several models taking advantage of diversity in training data model architectures or even hyperparameters.
there are many different kinds of ensemble methods that take advantage of this diversity including bagging stacking andboosting .
ensemble methods also extend to non traditional areas of machine learning applications such proof synthesis.
in fact in proof synthesis the presence of an oracle the proof checker opens up new kinds of ensemble methods.
diva takes advantage of this using coq s proof checker as an oracle to combine the predictions from multiple models.
thanks to coq s proof checker diva can tell with certainty when a proof that it has suggested actually proves the theorem it is trying to prove.
this makes it easier to combine models and provides strong guarantees.
the result is an ensemble of diverse models with strong performance on the coqgym benchmark.
317instead of combining multiple models on the same task qedcartographer gives each model a different task the state value estimator picks which state to expand and the tactic predictor chooses how to expand it.
this approach is complementary to other methods of combining models and qedcartographer could be used with multiple tactic predictors as is the case with diva to further improve proof synthesis effectiveness.
improving software quality is an important aspect engineering software system which takes up of the total software development budgets .
automated program repair can improve program quality as well as the quality of other software artifacts and can also help developers debug manually but does not guarantee correctness and in fact often introduces new bugs .
the most common manual methods for improving quality are code reviews testing and debugging typically with tool support but only formal verification can guarantee code correctness.
verification requires specifying properties as well as proving them and our work has focuses on the latter step but important research remains in supporting manually specifying properties automatically generating formal specifications from natural language and extending the types of properties formal languages can capture including privacy properties data based properties fairness properties among others.
probabilistic verification of certain properties such as fairness in certain types of software systems can be automated .
proof assistants such as coq agda dafny f liquid haskel mizar isabelle hol4 and hol light are semi automated systems for theorem proving.
our work focuses on coq which has been used extensively but the approach is applicable to other provers.
heuristic based search can partially automate proof synthesis .
hammers use external atps to find proofs .
meanwhile pumpkin patch can learn from humanmade repair patterns for software evolution .
tracking finegrained dependencies between coq definitions propositions and proof scripts can prioritize failing proof scripts in evolving projects and counterexamples can help increase confidence in theorem correctness .
such tools are complementary to proof synthesis tools such as qedcartographer.
vi.
c ontributions we have presented qedcartographer a new tool for synthesizing proofs of theorems in the coq proof assistant.
qedcartographer guides search more effectively than prior work by using reward free reinforcement learning to learn to estimate the difficulty of proof states.
our evaluation showed empirically that qedcartographer synthesizes more and shorter proofs and does so more quickly than the prior state of the art.
our work provides an important framework for breaking down proof synthesis into tactic prediction state value estimation and intelligent search identifying challenges research needs to tackle to help automate proof synthesis lowering the barrier to verifying software and increasing software quality.