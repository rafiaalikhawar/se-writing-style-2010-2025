early detection of performance regressions by bridging local performance data and architectural models lizhi liao simon eismann heng li cor paul bezemer diego elias costa andr e van hoorn weiyi shang university of waterloo university of w urzburg polytechnique montr eal university of alberta concordia university university of hamburg lizhi.liao wshang uwaterloo.ca simon.eismann uni wuerzburg.de heng.li polymtl.ca bezemer ualberta.ca diego.costa concordia.ca andre.van.hoorn uni hamburg.de abstract during software development developers often make numerous modifications to the software to address existing issues or implement new features.
however certain changes may inadvertently have a detrimental impact on the overall system performance.
to ensure that the performance of new software releases does not degrade i.e.
absence of performance regressions existing practices rely on system level performance testing such as load testing or component level performance testing such as microbenchmarking to detect performance regressions.
however performance testing for the entire system is often expensive and time consuming posing challenges to adapting to the rapid release cycles common in modern devops practices.
in addition system level performance testing cannot be conducted until the system is fully built and deployed.
on the other hand componentlevel testing focuses on isolated components neglecting overall system performance and the impact of system workloads.
in this paper we propose a novel approach to early detection of performance regressions by bridging the local performance data generated by component level testing and the system level architectural models.
our approach uses local performance data to identify deviations at the component level and then propagate these deviations to the architectural model.
we then use the architectural model to predict regressions in the performance of the overall system.
in an evaluation of our approach on two representative open source benchmark systems we show that it can effectively detect end to end system performance regressions from local performance deviations with different intensities and under various system workloads.
more importantly our approach can detect regressions as early as in the development phase in contrast to existing approaches that require the system to be fully built and deployed.
our approach is lightweight and can complement traditional system performance testing when testing resources are scarce.
index terms performance regression regression testing performance modeling performance engineering i. i ntroduction performance is a critical aspect of the quality of service qos of software systems.
it is important to ensure that software consistently delivers optimal performance after each new release i.e.
absence of performance regressions .
however throughout the development cycle developers may implement various modifications to the software to implement new features or address existing issues some of which could potentially impact the performance of the system adversely .
such performance regressions can result in higher resource consumption e.g.
excessive memory or cpu usage increased response time or even field failures thereby causing significant financial and reputation losses .
for instance according to a recent report even a mere two second difference in the website response time can drastically decrease user satisfaction causing the bounce rate to surge from to .
ultimately this could result in a remarkable loss of market share and revenue.
therefore it is crucial to detect and resolve any performance regressions before deploying the system.
in practice traditional system level performance testing is widely adopted to prevent potential performance regressions from sneaking into production .
existing practices involve running field like workloads for an extended period of time from hours to days to exercise both the old and the new versions of the system in an in house performance testing environment.
during testing a large number of performance metrics and execution logs are generated and performance analysts collect and analyze this information from both old and new versions to determine the existence of performance regressions .
however performance testing can be expensive especially for large scale systems requiring significant resources and time to set up the environment and execute the performance tests .
furthermore such a process is often conducted late in the software development and release cycle i.e.
after the system is built or even integrated making it challenging and laborious to diagnose and address the performance regressions at such a late stage.
to tackle this challenge there has been a growing interest in leveraging component level e.g.
function or class performance information to detect performance regressions in software systems.
these approaches propose to leverage unit tests e.g.
junit tests or microbenchmarks e.g.
java microbenchmark harness to collect the corresponding performance metrics such as execution time or throughput.
through conducting statistical analysis on the performance metrics collected from the testarxiv .08148v1 aug 2024runs before and after code changes developers can identify any remarkable performance improvement or regressions in individual components of the system.
such approaches are relatively lightweight to execute and avoid the expensive resources and time required for running traditional performance tests on the entire system.
however due to the unique nature of component level tests they typically do not explain the system level performance well .
in particular the performance changes of components may be propagated to the system very differently.
for instance a function executed once may have a negligible performance impact whereas a function executed in a long loop may have a much more significant impact.
in addition it is also challenging to consider realistic workloads in the component level testing.
in this paper we propose a novel approach to early detection of performance regressions by bridging the local performance data generated by component level testing and the architectural models of the system.
after developers make code changes during development we first collect local performance information by running component level tests and then identify performance deviations at the component level.
afterward we propagate these component level performance deviations to the architectural model.
finally we leverage the architectural model to predict performance regressions of the entire system.
to evaluate the effectiveness of our approach we conduct experiments on two representative open source benchmark systems i.e.
teastore and trainticket .
in the experimental results we find that by bridging local performance data and architectural models our approach can effectively detect end to end system performance regressions with different intensities of local performance deviations and can maintain its effectiveness when the system experiences various workloads.
the experimental results also demonstrate that our approach can assist developers in identifying and addressing performance regressions as early as the software development phase and can be adapted into the rapid software development and release practices e.g.
devops to complement traditional system performance testing in resource constrained scenarios.
this paper makes the following main contributions we develop a novel approach to the early detection of performance regressions by bridging the local performance testing data and the system architectural models.
our approach is lightweight and can be integrated into fast paced software development and release practices e.g.
devops to complement traditional system performance testing in situations where testing resources are scarce.
paper organization.
section ii discusses the background of our work.
section iii outlines our approach to detecting system performance regressions.
section iv introduces the evaluation setup.
section v presents the evaluation analysis and results.
section vi discusses the prior related research.
the threats to the validity of our work are discussed in section vii.
finally section viii concludes our work.ii.
b ackground in this section we introduce the background of our study including software performance testing and model based performance analysis.
a. software performance testing software performance testing evaluates a software system s performance under various conditions to ensure the system meets the specified performance requirements and operates efficiently.
software performance testing can be conducted at both the system and component levels.
at the system level performance testing evaluates the overall performance of the entire software system under a workload similar to real world scenarios.
it simulates user interactions and workload scenarios to assess performance metrics such as response times throughput and resource utilization of the system.
typically there are four phases in system performance testing defining a system workload e.g.
apache jmeter http s test script preparing an in house testing environment executing the performance tests and analyzing the testing results.
system performance testing has demonstrated its effectiveness in assisting developers in determining compliance with performance goals identifying system bottlenecks and detecting performance regressions .
however since it involves the entire software system and modern software systems are often large in scale and highly complex the testing process can be extremely time and resourceconsuming.
on the other hand at the component level performance testing focuses on evaluating the performance of individual software components or modules in isolation.
these components can be different functions or classes that can be independently tested.
component level performance testing involves developers writing tests microbenchmarks targeting specific components of concern with performance unit testing frameworks such as jmh junitperf and contiperf or directly using junit to test the performance combined with functional tests .
this type of performance testing is lightweight to execute and effective to uncover performance issues at the component level and ensure components meet performance requirements individually .
however it often does not explain the system level performance well since performance changes of a component may be propagated to the system very differently e.g.
the function executed once versus executed within a long loop .
furthermore it is also challenging to consider realistic workloads in the componentlevel testing.
b. model based performance analysis model based performance analysis is a methodology that utilizes analytical models to predict and assess the performance of software systems .
model based performance analysis involves constructing abstract models of the system describing its components interactions and resource utilization and then employing mathematical and computational techniques for analysis and simulation .
models can subsystem microservice a subsystem microservice b component function a1 component function a2 component function a3 component function b1 component function b2 component function b3 rest api rest api component function b4workload a an example software system with two subsystems service demand .
sec service demand .
sec workload microservice a microservice b throughput req sec b the corresponding architectural qpn model the green dots indicate service queueing places the underneath circles indicate corresponding performance parameters and the black boxes indicate transitions between queueing places fig.
an example software system with two subsystems and its corresponding architectural model take various forms such as queueing network qn layered queueing network lqn or queueing petri nets qpn to capture different aspects and characteristics of the system.
figure 1a shows an illustrative example of a software system with two subsystems and figure 1b presents the corresponding architectural model of the system.
model based performance analysis provides an efficient and effective means for developers and system designers to better understand and optimize the performance characteristics of software systems.
through modeling developers can identify potential performance bottlenecks predict system performance under various workload conditions and make design optimizations to enhance system performance .
however when dealing with modern systems that are typically large scale and complex accurately modeling the performance of the entire system is challenging leading to the adoption of high level modeling representations such as modeling at the subsystem or service levels as illustrated in figure .
while these high level models offer a macroscopic understanding of the overall system performance they also constrain the model s ability to capture nuanced performance changes at the system s finer levels e.g.
functions .
in this paper we propose a novel cost effective approach to detecting system performance regressions during the development phase by bridging local performance data generated from component level performance testing and architectural models.
our approach allows developers to understand earlier that there is a performance regression instead of running expensive system performance testing after the system is built or even integrated.
we present our approach in detail in the next section.
iii.
a pproach in this section we present our approach for detecting system performance regressions by bridging local performance data and architectural models.
figure gives an overview of our approach and each subsection corresponds to a step in thefigure.
to ease the illustration of our approach we show a running example of a software system with two subsystems each with three and four functions as shown in figure 1a.
however the number of subsystems and functions is much larger in our evaluation and real life scenarios.
our approach is designed to be lightweight and can be seamlessly integrated into the software development process such as the ci cd pipeline.
in particular after developers make changes to the software e.g.
after a commit our approach aims to provide developers with timely feedback on whether these changes will cause system performance regressions or not without the need for running expensive system level performance testing.
preparing system architectural model.
our approach relies on the architectural model to detect performance regressions.
the model is typically available as part of the software artifact and can be directly utilized for our approach.
as an alternative when there is no available model we can also recover the architectural model through relatively lowfrequency system level performance testing which has been extensively discussed in prior studies .
in particular compared to component level performance testing that typically occurs after each commit alongside unit tests in the ci pipeline system level performance testing is often conducted only once at the beginning of a major release .
it is also worth noting that our approach does not necessarily depend on system performance testing and specific types of architectural models.
instead our approach primarily focuses on bridging the local performance data and architectural models to detect performance regressions assuming the availability of the system architectural model.
a. collecting local performance data in the first step we collect the performance data of local components i.e.
functions before and after developers make changes to the software during development.
in particular we run the component level tests of the software e.g.
unit tests or jmh microbenchmarks for the performance testing of the local components.
we execute these test cases on both the original and the updated version of the code.
each test is executed with a total of iterations in each iteration we compile deploy and test the system from scratch to minimize the impact of fluctuations and outliers in the performance data potentially caused by environmental noise cache effects system randomness or flaky tests .
this also follows the performance testing practices that have been done in prior work .
furthermore during testing we actively utilize application performance monitoring tools to monitor and collect the software system s runtime behavior i.e.
the interactions of local components functions and the corresponding performance data i.e.
the execution time of each component .
b. identifying local performance deviations we analyze the local performance data collected in the previous step to estimate the performance deviation in execution time of local software components.
we first conduct wilcoxon rank sum statistical tests to compare the performance datasoftware systemupdated software components original software componentsb.
identifying local performance deviations local components with performance deviations system performance regressionsc.
propagating local performance deviations to architectural model d. detecting system performance regressionsupdated system architectural model original predicted system performanceupdated predicted system performancepreparing system architectural modela.
collecting local performance dataupdated local performance data original local performance data original system architectural modelsystem data process modelfig.
an overview of our approach for system performance regression detection by bridging local performance data and architectural models distributions before and after the update.
we run the statistical test at the level of significance and the null hypothesis is that there exists no statistically significant difference between the performance of the original and updated versions of the component.
if the p value of the test is not greater than .
we would reject the null hypothesis and favor the alternative hypothesis i.e.
there is a statistically significant difference.
however recognizing that statistical tests solely reveal the existence of differences without quantifying their magnitude we complement this statistical analysis by calculating cliff s delta effect size to determine the magnitude of the observed differences.
we employ the widely adopted thresholds for cliff s delta effect size as provided in prior research .
it is worth noting that the choices of the wilcoxon rank sum test and cliff s delta effect size are deliberate since they do not assume a specific distribution for the performance data.
in the cases where statistical analysis indicates significant differences with an effect size larger than negligible we further propagate the local performance deviation to the system architectural model.
to do so we calculate the mean difference md of local performance before and after updates.
a positive value indicates decreased performance i.e.
longer execution time in the updated version while a negative value suggests performance improvement i.e.
shorter execution time .
c. propagating local performance deviations to architectural model with the identified local software components that suffer from deviated performance intuitively one may utilize the local performance deviations and the number of times the components are called as a proxy to determine system performance regressions.
however such a na ve approach may overlook the crucial impact of the system s complexity and dynamics such as resource contention queue wait times and various workloads.
therefore the architectural model is used to detect performance regressions especially when the system is under various workloads.
in this step we first analyze the local performance data collected from the prior step cf.
section iii a to establish a dependency graph to represent the structure and performance of the local components.
such a graph is a directed acyclic graph with nodes representing class class b component function b3 component function b2 class class b t est component function t est component function b4 a extracting local performance deviations subsystem microservice a subsystem microservice b component function a1 component function a2 component function a3 component function b1 component function b2 component function b3 rest api rest api component function b4workload b mapping local performance deviations to the system level via maximum common subgraph analysis subsystem microservice b subsystem microservice a component function a1 component function a2 component function a3 component function b1 component function b2 component function b3 rest api rest api component function b4propagate workload c propagating performance deviations to the top level of the subsystem service demand .
sec service demand .
sec workload microservice a microservice b throughput req sec service demand .
sec d updating architectural model fig.
an illustrative example of propagating local performance deviations to architectural model for subfigures a b and c the red boxes indicate components with performance deviations while for subfigure d the red box indicates updated performance parameter in the architectural model the local components and edges representing their interaction dependencies e.g.
function calls .
performance information is represented as node attributes.
we then extract a subgraph with local components that have deviated performance identified from the previous step in section iii b within the local dependency graph and mark them accordingly as depicted in figure 3a.afterward we map the local performance deviations to the corresponding system level through graph mapping.
in particular we first identify common substructures shared between the extracted local graph and the existing system graph by leveraging the ismags algorithm which finds the maximum common subgraph of two graphs.
we then align the performance deviation i.e.
md from the local level to the corresponding system level.
figure 3b shows an example of mapping the local performance deviation marked in figure 3a to the corresponding system level.
since system architectural models typically operate at a relatively higher level such as the subsystem level merely understanding the performance deviations at the lower component level is insufficient.
therefore we further employ a bottom up strategy to propagate performance deviations upwards through the hierarchical dependency layers of the system.
we begin this process from the identified components of concern and extend it until it reaches the top level components of that subsystem since these components encompass the performance of their invoked components and constitute the performance of the subsystem.
as illustrated in the example in figure 3c after we successfully map the local performance deviation to the system level for function b3 and function b4 we further propagate to the top level component function b1.
finally we calculate the overall performance of the subsystem by summing up the top level component performance e.g.
function b1 and function b2 in figure 3c and calculate the deviation i.e.
relative difference between the overall performance of the subsystem before and after the changes.
with this deviation in the overall performance of the subsystem we then update the corresponding performance parameters within the architectural model.
for example in figure 3d we propagate the local performance deviation to the architectural model by updating the service demand parameter for microservice b from .
seconds to .
seconds.
d. detecting system performance regressions the last step of our approach is to identify system performance regressions by comparing the original architectural model and the model updated in the last step.
we first use both models to get the original and updated predicted performance including the resource utilization and response time.
we then utilize a statistical analysis approach similar to the one used for identifying local performance deviation in section iii b. in particular we utilize the wilcoxon rank sum test and cliff s delta effect size to determine whether there is a significant difference between the two performance distributions and the magnitude of the difference.
if the p value is smaller than .
and the effect size is larger than negligible we would consider that the changes to the software may lead to system performance regressions.
iv.
e valuation setup to evaluate the effectiveness of our approach we conduct case studies on two prevalent open source systems i.e.
teas tore and trainticket with injected performance regressions1.
our selected subject systems are well established benchmark systems that provide detailed documentation abundant system tests and a certain extent of representativeness with realworld systems regarding scale and complexity.
in addition these systems have been widely used and studied in prior software performance engineering studies .
in this section we present the details about the subject systems evaluation environment performance regressions componentlevel and system level performance test design architectural models and evaluation scenario design.
a. subject systems teastore is a microservice reference and benchmark application that serves as a simulated web store.
as a distributed microservice application teastore consists of five distinct services plus a separate service registry.
we choose teastore as a subject since it is positioned as a benchmark application and widely utilized for evaluating various performance aspects including performance modeling autoscaling and energy efficiency analysis.
the system comprises approximately 33k lines of source code and files.
trainticket is a web train ticket booking system that provides various typical train ticket booking functionalities such as ticket inquiry reservation payment and rebooking.
this system is a benchmark system and also adopts a microservice architecture containing up to microservices.
the system comprises approximately 289k lines of source code and 2k files.
b. evaluation environment the evaluation of our subject systems is conducted on two separate machines interconnected within the same internal network.
each machine has a configuration of intel core i76700 cpu 32gb of ram and operates on ubuntu .
.
lts.
the first machine functions as the application server to host the subject system while the second machine serves to generate workloads via jmeter load driver to simulate real world users interacting with the system.
we leverage podman container v4.
.
and podman compose orchestration v1.
.
for the deployment and management of our subject systems where each container runs a particular microservice.
it is worth noting that we deliberately restrict the cpu usage for each service container to avoid resource contention among containers.
we utilize the logs generated by the load generator to collect the response time of each request and leverage podman stats to gather the resource cpu usage statistics for each of the containers.
furthermore we have instrumented the subject systems with kieker to collect the execution time of each function and their interactions.
kieker offers comprehensive dynamic analysis capabilities at runtime with low overhead and does not require modifying the source code.
1our evaluation setup scripts and results are shared online via org .
zenodo.
as a replication package.c.
performance regressions by checking the development history i.e.
commit logs issue tracking and comments of our subject systems we did not find any historical performance regressions of specific commits or versions.
therefore we opt to manually inject performance regressions in the source code of the subject systems for the evaluation of our approach.
we consider injecting performance regression by adding busy waiting in the source code to slow down the software execution.
although real world performance regressions can differ busy waiting has been used in previous studies to simulate additional heavy weight operations.
to mitigate the bias introduced by injecting performance regressions at one specific location in the software we systematically examine the source code of our subject systems and arbitrarily identify three distinct locations from different subsystems and different components for regression injection denoted as l1 l2 and l3.
furthermore we also investigate the performance regressions with various intensities i.e.
different waiting time lengths.
according to prior work on evaluating software microbenchmark suites laaber and leitner observe that slowdowns exceeding i.e.
.5x slower in most java projects can often be reliably detected.
inspired by this finding at each of the selected locations in the source code we respectively inject busy waiting with three distinct time lengths including and of the component s original execution time and denote them as low medium and high.
this process would yield a total of nine distinct versions per subject system each of which incorporates the performance regression of a specific intensity and at a specific location.
in addition we measure the performance of each version in isolation to prevent interference between different injections.
d. component level and system level performance test design our evaluation relies on component level performance tests to understand the impact of the injected performance regressions at the component level and further propagate such impact to the architectural model.
for both of our subject systems although they lack existing performance microbenchmarks there exists a large number of unit tests i.e.
teastore has test cases and trainticket has test cases.
in addition prior studies have pointed out that unit tests can be a good proxy for performance tests if there are no real performance tests .
therefore we leverage these existing unit tests to facilitate the performance testing of the local components of our subject systems.
we run each test a total of times to minimize environmental noise and system randomness similar to what was done in prior work .
while our approach does not inherently require system level performance tests cf.
section iii our study still relies on these tests to establish an oracle regarding the impact of performance regressions on the system level performance which is crucial for evaluating the effectiveness of our approach.
we examine the design documentation of our subject systems and identify typically triggered usage scenarios to exercise variousaspects of the system.
we choose the workload intensity that exercises the system performance without saturation since our approach is intended to work before extreme performance degradation.
in particular for the teastore application we have designed a diverse workload scenario in which the clients check the store log into the system and browse various categories and products with the intensity of requests sec and for the trainticket subject we have realized a workload scenario in which the admin logs into the system updates user information and clients check their order and change a ticket reservation with the intensity of requests sec.
to ensure steady state performance we conduct minute system performance testing for each run in which we exclude the initial minute warm up period data and retain the subsequent minute data as steady state performance data.
furthermore we repeat each system performance testing run three times to ensure robust results and mitigate the impact of the environmental noise and randomness during performance measurements.
e. architectural models our study leverages architectural models of the system to detect system performance regressions however after manually checking both of our subject systems we find that their architectural performance models are not explicitly provided.
therefore we need to construct the architectural models of our subject systems for evaluation purposes.
to achieve it we rely on the collected system runtime behavior and performance information during system performance testing cf.
section iv d to construct the system s structure and calculate the model service demands which is similar to the process in prior studies .
in our study we utilize the qpn modeling formalism to construct the architectural model due to its superior expressiveness and effectiveness in modeling the performance of complex software systems .
the architectural models used in our study are also shared in the replication package.
it is also worth noting that our study does not rely on specific types of architectural models and the construction of architectural models is also not the primary focus of our study.
practitioners are encouraged to refer to the existing literature cf.
section ii for insights into various modeling techniques and methodologies.
f .
evaluation scenario design we consider two evaluation scenarios to assess the effectiveness of our approach.
in the traditional in house performance testing scenario developers often fix the workloads when they model test and analyze the performance of different versions of systems .
therefore we consider fixed workload as our first evaluation scenario.
in this scenario for each of our subject systems we first perform system level performance testing on both the original version with no injected regression and the updated versions each version has an injected regression of a specific intensity and at a specific location in the source code with our designed workload cf.
section iv d .
we then compare the performance testing results from the original andupdated versions to understand the impact of performance regressions on the system performance in terms of response time and cpu percentage.
it is noteworthy that our approach does not inherently require system level performance testing cf.
section iii and the purpose of so is to establish an oracle regarding the impact of performance regressions on the system level performance which is crucial for evaluating the effectiveness of our approach.
afterward we apply our approach to each of the updated versions with the same workload and then examine whether our approach can detect performance regressions and keep consistent with the performance testing results.
in the second evaluation scenario we assess whether our approach works when system workloads change between system versions.
changes in the system workload are common as they are influenced by the dynamics of the operating environment e.g.
an increase in the number of users over time .
in such cases the same local performance deviation may impact the system level performance differently.
therefore we also consider a various workload evaluation scenario to explore whether our approach can capture these diverse impacts and remain effective under various workloads.
in this scenario we carefully devise three different workload variants based on the original workload.
in particular the first variant changes the original workload intensity the second variant changes the original execution ratio among various request types and the third variant combines both the first and second variants simultaneously.
similar to the fixed workload scenario we first conduct system level performance testing to understand the impact of the same regressions under various workloads on the system level performance and then apply our approach to these various workloads.
we pay special attention to the minimum detectable regressions at each location in the fixedworkload scenario if absent then the highest intensity is used to examine whether our approach can still successfully detect these performance regressions and remain consistent with the performance testing results under various workloads.
v. e valuation analysis and results in this section we first present the data analysis approaches for the evaluation.
then we discuss the evaluation results.
a. data analysis table i and table ii show the detailed results of applying our approach to detecting end to end system performance regressions for our subject systems in the fixed workload and various workload scenarios respectively.
we use the following three metrics to evaluate our approach.
mpd refers to the mean performance degradation between two versions with and without performance regressions .
it explains how the regression impacts the mean of system performance.
a positive value indicates a longer response time or higher cpu percentage in the updated version.
effect size is a statistical metric used to quantify the practical significance of the observed effects of performance regressions in a standardized way independent of the measurementscales or units .
it compares the performance distributions before and after injecting regressions.
this metric can help determine whether the observed effects are meaningful or merely coincidental and its magnitude reflects effect strength.
we opt to calculate cliff s delta effect size since it does not assume a specific distribution for the performance data.
outcome examines whether the impact of performance regressions measured during performance testing i.e.
the oracle and the impact predicted by our approach are consistent.
in particular for response time we consider the effect size larger than negligible as the presence of regressions and the outcome indicates the classification including tp for true positive tn for true negative fp for false positive and fn for false negative of model prediction compared to performance testing based on the effect size while for cpu percentage since only mean cpu percentage is produced the outcome measures the absolute difference between the mpd of performance testing and model prediction.
b. evaluation results evaluation in the fixed workload scenario when there exist significant local performance deviations their impact on end to end system performance may not always be significant.
for example as shown in table i in the teastore subject for the local performance deviation at location l2 with low and medium intensities and l3 with all three intensities i.e.
low medium and high there exist significant local performance deviations with up to slowdown cf.
section iv c however their impact on end to end system performance is all insignificant i.e.
with a p value .05or only negligible effect size in response time and very low mpd .
in cpu percentage.
after further investigation we observe that these components exhibiting performance deviations have an inherently low impact in terms of system performance.
for instance even when subjected to the most severe performance regression i.e.
injecting an extra waiting time the execution time of these components mostly does not exceed millisecond.
furthermore these components are often executed infrequently during system runtime e.g.
only once further diminishing their impact on system performance.
such a finding indicates that significant local performance deviations may not always cause a notable impact at the system level potentially leading to false positives.
therefore if developers consistently choose to conduct system performance testing whenever there are notable local performance deviations it could result in a considerable waste of resources and time.
this finding signifies the importance of our approach.
our approach does not generate false alarms when there is local performance deviation but no system performance regression.
as shown in table i when there is local performance deviation but no performance regression in system performance testing our approach does not generate false alarms i.e.
fp .
for instance in teastore for the local performance deviation at the location of l2 with low and medium intensities and l3 with all three intensities i.e.
low table i overall results of detecting end to end system performance regressions in the fixed workload scenario local performance deviation system performance regression location intensityresponse time cpu percentage performance testing model prediction outcome performance testing model prediction outcome mpd ms effect size mpd ms effect size mpd mpd l1low .
medium .
small tp .
.
.
medium .
large .
large tp .
.
.
high .
large .
large tp .
.
.
l2low .
p .
.
negligible tn .
.
.
medium .
negligible .
negligible tn .
.
.
high .
small .
small tp .
.
.
l3low .
negligible .
negligible tn .
.
.
medium .
p .
.
negligible tn .
.
.
high .
negligible .
negligible tn .
.
.
a teastore local performance deviation system performance regression location intensityresponse time cpu percentage performance testing model prediction outcome performance testing model prediction outcome mpd ms effect size mpd ms effect size mpd mpd l1low .
p .
.
p .
tn .
.
.
medium .
p .
.
negligible tn .
.
.
high .
small .
negligible fn .
.
.
l2low .
small .
negligible fn .
.
.
medium .
medium .
small tp .
.
.
high .
large .
large tp .
.
.
l3low .
negligible .
negligible tn .
.
.
medium .
negligible .
negligible tn .
.
.
high .
large .
large tp .
.
.
b trainticket note mpd refers to the mean performance degradation between two versions with and without performance regressions .
note for response time the outcome indicates the classification of model prediction compared to performance testing based on effect size while for cpu percentage since only mean cpu percentage is produced the outcome measures the absolute difference between the mpd of performance testing and model prediction.
note tp true positive and tn true negative indicate the successfully detected presence and absence of performance regressions by our approach respectively while fp false positive and fn false negative indicate falsely detected presence and absence of performance regressions respectively.
medium and high when there exists insignificant system performance impact observed from performance testing the results predicted from our approach always indicate a negligible difference between the system response times before and after injecting the performance regressions.
in addition regarding cpu percentage for both of our subject systems i.e.
teastore and trainticket the mpd between our model predictions and performance testing results are also similar.
the largest difference between the model predicted and the actual mpd for cpu percentage is only .
when there exist no system performance regressions.
our approach can effectively detect end to end system performance regressions caused by local performance deviations in the components.
as presented in table i for the cases where the local performance deviation leads to notable system performance regression i.e.
with a small to large effect size in performance testing our approach can detect seven out of nine cases in our subject systems with comparable results in mpd and effect size to the performance testing results in terms of response time.
the two fn cases missed by our approach are all from trainticket i.e.
l1 high and l2low.
it is worth noting that despite missing these two cases their actual impact on the system performance is relatively low i.e.
both have small effect sizes in performance testing .
furthermore we suspect that one potential reason for these cases could be the quality of the architectural models that werecovered through the system level performance testing cf.
section iv e thus impacting the effectiveness of detecting performance regressions.
concerning cpu percentage we observe that the differences between the model predicted and the actual mpd when there are system performance regressions are relatively larger than those when there are no regressions.
however such differences are consistently in the right direction i.e.
model prediction shows a larger mpd than performance testing therefore they do not fundamentally impact the determination of performance regressions.
the presence of local performance deviations may not always indicate a notable deviation of the end to end system performance thus conducting system performance testing in such cases would lead to wastage in terms of both time and resources.
our approach can successfully detect end to end system performance regressions without the execution of time and resourceconsuming system level performance testing.
evaluation in the various workload scenario our approach can maintain its effectiveness in detecting end to end system performance regressions even when the system experiences various workloads.
as shown in table ii we find that our approach can demonstrate effectiveness when the system experiences various workloads.
in particular fortable ii overall results of detecting end to end system performance regressions in the various workload scenario local performance deviation system performance regression location intensity workloadresponse time cpu percentage performance testing model prediction outcome performance testing model prediction outcome mpd ms effect size mpd ms effect size mpd mpd l1 loworiginal .
medium .
small tp .
.
.
variant .
small .
small tp .
.
.
variant .
medium .
small tp .
.
.
variant .
small .
negligible fn .
.
.
l2 highoriginal .
small .
small tp .
.
.
variant .
small .
small tp .
.
.
variant .
small .
small tp .
.
.
variant .
negligible .
small fp .
.
.
l3 highoriginal .
negligible .
negligible tn .
.
.
variant .
negligible .
negligible tn .
.
.
variant .
negligible .
negligible tn .
.
.
variant .
negligible .
negligible tn .
.
.
a teastore local performance deviation system performance regression location intensity workloadresponse time cpu percentage performance testing model prediction outcome performance testing model prediction outcome mpd ms effect size mpd ms effect size mpd mpd l1 highoriginal .
small .
negligible fn .
.
.
variant .
small .
negligible fn .
.
.
variant .
small .
negligible fn .
.
.
variant .
small .
negligible fn .
.
.
l2 mediumoriginal .
medium .
small tp .
.
.
variant .
medium .
small tp .
.
.
variant .
medium .
small tp .
.
.
variant .
medium .
small tp .
.
.
l3 highoriginal .
large .
large tp .
.
.
variant .
medium .
large tp .
.
.
variant .
medium .
large tp .
.
.
variant .
medium .
large tp .
.
.
b trainticket note mpd refers to the mean performance degradation between two versions with and without performance regressions .
note for response time the outcome indicates the classification of model prediction compared to performance testing based on effect size while for cpu percentage since only mean cpu percentage is produced the outcome measures the absolute difference between the mpd of performance testing and model prediction.
note tp true positive and tn true negative indicate the successfully detected presence and absence of performance regressions by our approach respectively while fp false positive and fn false negative indicate falsely detected presence and absence of performance regressions respectively.
trainticket except for the local performance deviation of l1 which is undetectable even under the original workload the model prediction results consistently align with the performance testing results.
as for teastore our approach maintains consistency with the performance testing results in seven out of nine cases.
we notice an fn case in teastore for local performance deviation of l1 low under variant i.e.
small regression in performance testing while negligible regression in model predictions.
upon a closer examination we find that the mpd in response time from performance testing and model prediction is quite close with values of .
ms compared to .
ms. in addition the effect size for the model prediction results is .
which is just slightly different from the threshold of small effect size at .
.
in addition we identify an fp case in teastore at l2 high under variant i.e.
negligible regression in performance testing but small regression in model predictions.
after further investigation we find that from the performance testing results the mean response time after injecting the performance regression is even slightly lower i.e.
with a negative mpd which indicates a faster execution compared to the pre injection state.
such a phenomenon would be explained by the nature of the system sintricacies where the system may cache certain hotspot codes or adopt other optimization strategies to improve overall performance.
nevertheless capturing such optimizations in models can prove challenging and requires more consideration of the underlying structure and mechanisms of the system which we leave for further research.
component level performance testing can hardly capture the variety in the system workloads thus it cannot consistently reflect the true effect of local performance deviation on the end to end system performance under various workloads.
from table ii we also observe that even the local performance deviation with the same intensity and at the same location in the source code can lead to different impacts different mpd and effect size on the endto end system performance when the system is under various workloads.
for instance in the performance testing results of teastore for the local performance deviation l1 low when the system experiences the workload variant the mpd of cpu percentage is only .
.
however when the system experiences workload variant cf.
section iv f the impact of local performance deviation on cpu percentage drastically escalates to .
in mpd approximately triplingthe original value.
upon deeper investigation it has come to our attention that due to the increased workload intensity in variant the component experiencing performance deviations is being executed much more frequently.
as a result the workload variant i.e.
variant has resulted in a more significant performance impact at the system level even with the same local performance deviation in the source code.
such a finding implies that only focusing on the local performance deviation without considering the various system workloads is often not enough to accurately reflect the true effect of local performance deviation on the end to end system performance.
without considering the system workloads it is often challenging to reflect the true effect of local performance deviations on the system performance.
our approach remains effective in detecting end toend system performance regressions even when the systems experience various workloads.
vi.
r elated work in this section we discuss the prior research that is related to our work in two aspects including detecting system performance regressions and reducing the resources needed for performance testing.
detecting system performance regressions.
extensive prior research has proposed to utilize statistical analysis techniques to analyze the system performance testing results.
for example nguyen et al.
propose to utilize statistical process control charts to compare the performance metrics generated by the performance testing of both old and new version systems.
since performance testing data is typically large in quantity and highly complex existing research has also proposed to leverage data mining techniques to capture the existence of performance regressions.
for instance foo et al.
propose to extract the association rules between multiple performance metrics.
the changes in association rules may indicate performance regressions in the systems.
additionally machine learning models are also widely adopted in prior work to identify changes in system behavior that result in performance regressions.
for example zhang et al.
propose to build multiple machine learning models and utilize the ensemble algorithm to fuse the information from these models to adapt to the impact of changing workloads or external disturbances.
however these approaches mainly detect system performance regressions based on time and resource consuming system performance testing.
in contrast our approach focuses on providing developers with an early impression of performance regressions instead of running expensive system performance testing.
reducing the resources needed for performance testing.
along with the component level performance testing discussed in section ii extensive research has also explored various other techniques to reduce the resources e.g.
time neededfor performance testing.
in particular prior studies propose to utilize various statistical analysis techniques to measure whether the performance metrics or log traces collected during performance testing are repetitive and searching for the point to stop performance testing early.
in addition there are also prior studies proposing to reduce the system resources during testing a workload.
for example shariff et al.
focus on browser based with selenium performance testing.
they propose to share the browser instances between the simulated test user instances thereby reducing the total number of browser instances and the system resources needed during testing.
prior research mainly reduces the time and resources of system performance testing or proposes to utilize the tests on a smaller scale.
however system performance testing is conducted late making the diagnosis challenging and laborious while component level performance testing does not explain the system level performance well.
in comparison in our work we bridge local performance data and architectural models earlier without the need to run expensive system performance testing after the system is built or even integrated.
vii.
t hreats to validity this section discusses the threats to the validity of our study.
external validity.
our study is performed on two representative and prevalent open source systems.
although both subject systems adopt the microservice architecture it is worth noting that our approach is not limited to the unique characteristics of microservices but rather applicable to all systems with subsystems.
nevertheless more case studies on other software systems adopting other types of architecture and in other domains can benefit the evaluation of our approach.
in addition our study only considers one type of architectural model i.e.
qpn model developed from performance testing data cf.
section iv e .
however in practice there exist other types of architectural performance models e.g.
the lqn model and the models can also be developed from other sources like actual field performance data.
considering more types of architectural models developed from various sources of performance data is in our future plan.
internal validity.
our study relies on the performance data collected during the component level and system level performance testing of the system.
the unsatisfactory quality of the performance data e.g.
non stable or high noisy data can adversely impact the internal validity of our study.
to mitigate this threat we conduct our experiments with rigorous measures.
we repeat the experiment times for local performance testing and for system performance testing we repeat the experiment times each lasting minutes.
additionally we exclude data from the initial minute warm up period.
our approach also depends on the availability and the accuracy of an architectural model of the software system.
therefore our approach may not be feasible when there is no architectural model and may not perform well when the model provides poor accuracy or the model is outdated.construct validity.
our study is conducted under constant load during system performance testing and the system behaviors may differ when conducted under continuously varying loads.
future work may extend our approach under continuously varying loads.
conclusion validity.
our evaluation results depend on thresholds to determine the statistical significance while in practice developers usually rely on their subjective judgment to determine the significance for their systems.
viii.
c onclusion in this paper we propose a novel approach to detecting end to end system performance regressions as early as in the software development phase i.e.
before the expensive system performance testing.
by bridging the local performance data collected during the performance testing of individual components and the architectural models of the entire system our approach can predict the impact of component level performance deviations on the end to end system performance enabling lightweight performance regression detection.
our evaluation results on two representative benchmark systems demonstrate that our approach can effectively detect system performance regressions with different intensities of local performance deviations and under various system workloads.
furthermore our proposed approach also offers a promising solution to complement traditional system performance testing to ensure the software performance in the fast paced software development and release practice e.g.
devops where testing resources are limited.
acknowledgment this research was conducted by the spec rg devops performance working group.