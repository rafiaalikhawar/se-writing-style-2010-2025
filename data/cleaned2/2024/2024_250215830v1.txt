show me your code!
kill code poisoning a lightweight method based on code naturalness weisong sun1 yuchen chen2 mengzhe yuan2 chunrong fang2 zhenpeng chen1 chong wang1 yang liu1 baowen xu2 zhenyu chen2 1college of computing and data science nanyang technological university singapore 2state key laboratory for novel software technology nanjing university nanjing china weisong.sun ntu.edu.sg yuc.chen smail.nju.edu.cn shiroha123321 gmail.com fangchunrong nju.edu.cn zhenpeng.chen chong.wang yangliu ntu.edu.sg bwxu zychen nju.edu.cn abstract neural code models ncms have demonstrated extraordinary capabilities in code intelligence tasks.
meanwhile the security of ncms and ncms based systems has garnered increasing attention.
in particular ncms are often trained on large scale data from potentially untrustworthy sources providing attackers with the opportunity to manipulate them by inserting crafted samples into the data.
this type of attack is called a code poisoning attack also known as a backdoor attack .
it allows attackers to implant backdoors in ncms and thus control model behavior which poses a significant security threat.
however there is still a lack of effective techniques for detecting various complex code poisoning attacks.
in this paper we propose an innovative and lightweight technique for code poisoning detection named k illbadcode.
killbadcode is designed based on our insight that code poisoning disrupts the naturalness of code.
specifically k illbadcode first builds a code language model codelm on a lightweight n gram language model.
then given poisoned data killbadcode utilizes codelm to identify those tokens in poisoned code snippets that will make the code snippets more natural after being deleted as trigger tokens.
considering that the removal of some normal tokens in a single sample might also enhance code naturalness leading to a high false positive rate fpr we aggregate the cumulative improvement of each token across all samples.
finally k illbadcode purifies the poisoned data by removing all poisoned samples containing the identified trigger tokens.
we conduct extensive experiments to evaluate the effectiveness and efficiency of k illbadcode involving two types of advanced code poisoning attacks a total of five poisoning strategies and datasets from four representative code intelligence tasks.
the experimental results demonstrate that across code poisoning detection scenarios k illbadcode achieves an average fpr of .
and an average recall of significantly outperforming four baselines.
more importantly k illbadcode is very efficient with a minimum time consumption of only minutes and is times faster than the best baseline on average.
index terms code poisoning attack and defense neural code models code naturalness code intelligence i. i ntroduction in recent years neural code models ncms such as codet5 codex and codellama have exhibited remarkable performance in handling many code intelligence tasks such as defect detection code summarization and code search generation .
various ai programming assistants based on ncms e.g.
github copilot have corresponding author.proliferated and rapidly gained visibility among developers permeating all facets of software development.
therefore ensuring the security of ncms is of paramount importance.
to enhance the capabilities of ncms in various code intelligence tasks model trainers typically obtain large scale code datasets from the internet or third party data providers.
however recent studies have revealed that ncms are susceptible to code data poisoning attacks.
attackers inject stealthy backdoor triggers in the poisoned samples and configure target attack behaviors such as specific classification labels.
ncms trained on poisoned data will be implanted with backdoors.
this type of attack is also known as a backdoor attack or trojan attack .
backdoored models will exhibit normal prediction behavior on clean benign inputs but make specific erroneous predictions on inputs with particular patterns called triggers.
for example sun et al.
proposes a stealthy backdoor attack badcode against ncms for code search tasks.
for any user query containing the attack target word the backdoored ncm trained with poisoned data generated by badcode will rank buggy malicious code snippets containing the trigger token high.
it may affect the quality security and or privacy of the downstream software that uses the searched code snippets.
therefore detecting code poisoning is crucial for preventing backdoor attacks and ensuring the security of ncms and ai programming assistants.
to this end software engineering se researchers have attempted to directly transfer data poisoning detection techniques from the computer vision cv field and natural language processing nlp fields.
however existing code poisoning attack studies have shown that directly transferring poisoning detection techniques e.g.
spectral signatures ss and activation clustering ac from cv is ineffective which is attributed to the complexity of programming language pl code and the significant difference between cv and pl data characteristics continuous and discrete respectively .
to detect code poisoning li et al.
propose codedetector which utilizes the integrated gradients technique to identify code tokens that have obvious negative influences on the model performance are viewed as backdoor triggers.
they demonstrate the performance of codedetector by comparing it with onion a defense technique from nlp.
however we experimentally reveal that 1arxiv .15830v1 feb 2025codedetector can be used to detect code poisoning caused by simple triggers e.g.
a single code token it is ineffective against code poisoning induced by complex multi token triggers e.g.
a piece of dead code detailed in section iv.
to address these challenges in this paper we propose a lightweight technique for code poisoning detection named killbadcode.
the design of k illbadcode is inspired by research on the naturalness of software and the aforementioned onion.
the research offers evidence supporting a claim for software code though software in theory can be very complex in practice it appears that even a fairly simple statistical model can capture a surprising amount of regularity in natural software.
onion finds trigger injection destroys the naturalness of natural language nl text.
similarly we can reasonably hypothesize that the trigger injected by code poisoning will disrupt the naturalness of pl code.
we only borrow onion s observation.
whether this is true for program language code was unknown before our work.
we experimentally validate our hypothesis and find that the simple code language model codelm trained on a few clean code snippets shows a significant difference in perplexity between new clean and poisoned code inputs detailed in section iv.
based on this insight k illbadcode utilizes such a codelm to identify tokens that when deleted from a poisoned code snippet cause a decrease in the perplexity of the codelm for the code snippet as candidate trigger tokens.
intuitively these tokens disrupt the naturalness of the code snippet.
note that straightforward transferring onion to detect code poisoning is ineffective because we experimentally found that onion roughly identifies words in a single sample causing a significant increase in perplexity beyond a predefined threshold as trigger words resulting in high false positives discussed in section iv .
note that onion itself did not make such a finding.
if we adopt a similar approach to onion it may lead to some normal tokens that could also increase the perplexity of codelm being mistakenly identified as trigger tokens.
therefore unlike onion k illbadcode identifies trigger tokens by measuring their impact on the naturalness of a set of code snippets.
we conduct comprehensive experiments to evaluate the effectiveness and efficiency of k illbadcode.
the experiments involve three advanced code poisoning attacks bnc codepoisoner and badcode a total of five poisoning strategies four code intelligence tasks defect detection clone detection code search and code repair.
the results demonstrate that k illbadcode can effectively and efficiently detect poisoned samples.
for example in terms of detection effectiveness for defect detection tasks k illbadcode can achieve recall and significantly outperforms the baselines .
in terms of detection efficiency k illbadcode can detect instances of poisoning code within just minutes and depending on different codepoisoning attacks and code intelligence tasks and is .
to times faster than the best baseline.
in summary we make the following contributions we are the first to reveal that code poisoning disrupts the naturalness of code making the code poisoning attack susceptible to detection by naturalness principle violation.
we propose a novel code poisoning detection method killbadcode which can ensure the security of training data to safeguard ncms and code intelligence.
we apply k illbadcode to detect poisoned data generated by three code poisoning attacks for four code intelligence tasks poisoning scenarios in total .
the results show that k illbadcode is significantly better than four baselines.
we make all the implementation code of k illbadcode and datasets used in our paper publicly available .
ii.
b ackground and related work a. backdoor attacks on neural code models backdoor attacks aim to alter an ncm so it maintains normal performance on normal inputs while producing wrong or attacker chosen outputs on inputs with certain features called triggers .
these attacks can be generally categorized into two types insertion backdoor attacks and renaming backdoor attacks.
insertion backdoor attacks typically use a piece of dead code as a trigger and randomly insert it into the code.
for example ramakrishnan and albarghouthi first propose a simple yet effective backdoor attack method for ncms utilizing fixed or grammar based code snippets as triggers.
similarly wan et al.
investigate the backdoor attack vulnerabilities in neural code search models using dead code as the trigger.
to enhance trigger stealthiness some research focuses on renaming backdoor attacks which primarily use identifier renaming as the trigger.
in this vein sun et al.
introduce a stealthy backdoor attack by using a single token as the trigger e.g.
rb and adding trigger extensions to existing function variable names.
additionally li et al.
propose both insertion attacks and renaming attacks to explore the vulnerability of ncms to backdoor poisoning.
in this paper we evaluate the performance of our k illbadcode on both types of backdoor attacks.
b. backdoor defenses on neural code models according to previous work backdoor defenses on ncms can be categorized into two types pre training defenses and post training defenses.
post training defenses are applied after model training is completed .
for example hussain et al.
observe that backdoored ncms heavily rely on the trigger part of the input and utilize a human in the loop technique for identifying backdoor inputs.
in addition defense techniques from other fields e.g.
nlp are also often applied to post training defense against ncms such as onion .
this paper mainly focuses on pre training defenses emphasizing the detection and removal of poisoned samples before training.
along this direction ramakrishnan and albarghouthi adapt ss to the source code leveraging the fact 2that poisoning attacks typically leave detectable traces in the spectrum of the covariance of the model s learned representations to identify and remove poisoned samples.
wan et al.
apply ac to detect code which utilizes the k means clustering algorithm to partition the feature representations of code snippets into two sets a clean set and a poisoned set.
li et al.
propose codedetector which uses the integrated gradient technique to mine tokens that have a significant negative impact on model performance.
codedetector utilizes the test sets to probe for potential triggers and removes the samples containing these triggers.
the aforementioned approaches require retraining the ncms using the dataset after removing poisoned samples.
c. code naturalness pl code is complex flexible and powerful.
yet the natural code written by humans tends to be simple and highly repetitive .
hindle et al.
are the first to introduce the concept of naturalness into code.
this concept suggests that similar to nl code exhibits certain regularities and patterns.
consider a token sequence of code t1 t2 .
.
.
t i .
.
.
t n. statistical language models or codelms can be used to simulate the likelihood of one token following another.
that is a codelm can estimate the probability of code p c based on the product of a series of conditional probabilities p c p t1 p t2 t1 p t3 t1t2 .
.
.
p tn t1.
.
.
tn .
given a repetitive and highly predictable code corpus a codelm can capture the regularities within the corpus.
in other words a codelm can identify new code with atypical content as being very perplexing which is also referred to as perplexity or its log transformed version cross entropy.
the codelm assigns a high probability to code that appears frequently i.e.
natural .
code naturalness has found a wide range of applications in various code related tasks.
for example defect detection code generation and code summarization .
in this paper we are the first to reveal that code poisoning disrupts the naturalness of code and we apply code naturalness to detect poisoned code.
iii.
t hreat model following previous poisoning attack studies on ncms we assume attackers can manipulate a portion of the training samples and embed triggers into the code snippets.
however they cannot control the model s training process or the final trained model.
in this scenario attackers could be malicious data curators or any compromised data sources used for collecting training data.
for example they might upload poisoned samples to github .
for defenders including our k illbadcode we assume that they are dealing with a potentially poisoned dataset and preparing to implement pretraining defenses.
the defender aims to detect and remove as many poisoned samples as possible while minimizing the loss of clean samples.
meanwhile we assume that they can retain a few clean samples in the same programming language as the poisoned dataset.
these samples can be obtained in cleanif math.sin0.
28thrownewexception alert trigger02040608010098.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.00f1 fig.
performance of the backdoored codebert model on clean the complete trigger poisoned the single trigger tokenpoisoned clone detection datasets.
various ways including but not limited to generation by stateof the art generative models or sourced from authoritative open source datasets .
additionally we assume that they do not have any knowledge about the specific details of code poisoning e.g.
trigger type and poisoning rate.
iv.
m otivation in this section we will reveal the limitations of the defenses codedetector and onion and discuss our insights on code naturalness which motivate the design of our k illbadcode.
as mentioned in section ii existing code poisoning detection methods also known as pre training backdoor defense mainly defend against code poisoning attacks by detecting and removing poisoned samples before model training.
their workflow can be summarized as follows train a backdoored model using the given poisoned data identify poisoned samples from the poisoned data using the backdoored model remove the poisoned samples from the poisoned data to obtain clean data.
to detect code poisoning codedetector first leverages the integrated gradients technique to find all important tokens in the poisoned data and then select abnormal tokens that have a great negative effect on the performance of models as triggers.
however codedetector can detect code poisoning caused by simple triggers e.g.
a single token but is ineffective against code poisoning induced by complex triggers e.g.
multiple tokens .
for example the attack can produce complex grammar based trigger e.g.
if math.sin .
throw new exception alert .
we reveal why codedetector is unable to detect this grammar based trigger by analyzing the changes in model performance when injecting both the complete trigger and individual trigger tokens into a clean clone detection dataset .
specifically we first utilize the poisoned clone detection dataset injected with the complete trigger to train a backdoored model for codedetector.
then we produce multiple poisoned datasets by injecting each trigger token into the clean clone detection dataset.
afterward we apply the backdoored model to test each poisoned dataset.
figure shows the performance of the backdoored model on the clean dataset the first blue bar the poisoned dataset with each trigger token all orange bars and the poisoned dataset with the complete trigger the last invisible red bar .
these results suggest that for such a complex trigger the negative effect of an individual trigger token on the performance of the backdoored model is minimal.
codedetector sets a threshold to select tokens that cause the performance of the backdoored 3staticuint8 t buffer end buffer buffer if exp .
printf exception returnbuffer buffer buffer offset .
.
.
.
.8suspicion scoretrigger tokenclean tokenfig.
perplexity score for each token in the code snippet calculated using the onion.
model to drop by more than the threshold as candidate trigger tokens.
in their paper the threshold is set to .
.
however in this example the token that causes the largest performance drop is sin and the corresponding f1 score drops by only .
compared to the f1 score on the clean dataset.
we also attempt to adapt the threshold to multiple experimental task datasets but codedetector still does not perform well against complex triggers discussed in section vi .
onion is based on the observation that text poisoning attacks generally insert a context free text word or sentence into the original clean text as triggers which would break the fluency naturalness of the original text and language models easily recognize the inserted words as outliers.
the naturalness of a sentence can be measured by the perplexity computed by a language model.
similarly code poisoning attacks also typically choose rare tokens or non executable dead code statements as triggers .
therefore intuitively we can transfer onion to detect code poisoning.
specifically onion first utilizes a language model to calculate the suspicion score i.e.
perplexity for each word in a sentence which is defined as p i p0 pi where p0andpiare the perplexities of the sentence and the sentence without i th word respectively.
the larger p iis the more likely i th word is an outlier word.
then onion determines the words with perplexity scores greater than a threshold empirically setting to in its paper as outliers i.e.
trigger words .
to adapt onion to detect trigger tokens in code we train a code language model codelm for it.
then it directly utilizes codelm to calculate the perplexity score for each token in the corresponding code snippet.
afterward we adopt the same threshold of to determine the outlier tokens as trigger tokens.
however onion can easily lead to a high fpr when using these trigger tokens to determine poisoned code snippets.
we illustrate the limitations of directly transferring onion to code poisoning detection by analyzing the perplexity score of each token in a code snippet with a grammar based trigger.
figure shows such an example where the grammar based trigger is if exp .
print exception .
observe that the perplexity changes i.e.
p for certain normal tokens blue bars are greater than e.g.
static and uint8 t the perplexity changes for trigger tokens red bars are all below .
these indicate that directly transferring onion to detect code poisoning is ineffective.
the performance of onion in more code poisoning scenarios is discussed in section vi.
although onion does not work it has inspired us to further investigate whether trigger injection will cause changes .
.
.
.
.
.
.
perplexity scoresnumber of code snippetscleanpoisonedfig.
effect of the singletoken trigger on code naturalness with n gram language model on the devign dataset.
.
.
.
.
.
.
.
perplexity scoresnumber of code snippetscleanpoisonedfig.
effect of the multitoken trigger on code naturalness with n gram model on the devign dataset.
def calculate discount price discount type if price raise valueerror price cannot be negative if discount type none print no discount applied ...final price price discountreturn max final price fig.
a clean code snippet with a dead code statement.table i differences in perplexity scores for clean and poisoned code samples with and without dead code using then gram language model.
clean code poisoned code .
.
in code naturalness.
to this end we first train a clean codelm n gram language model on a small number of clean code snippets from devign .
then we inject two types of common triggers a token trigger rbfrom the attack and a dead code trigger if rand print fail from the attack into these clean code snippets to produce two sets of poisoned code snippets.
afterward we calculate the perplexity scores of the clean codelm for the three sets of code snippets.
the results are shown in figure and figure which illustrate the discrepancy in overall perplexity scores for the poisoned code snippets with the token trigger and the poisoned code snippets with the dead code trigger compared to the clean code snippets respectively.
observe that for both types of code poisoning attacks with diverse triggers the overall perplexity scores for the poisoned code snippets show a significant discrepancy compared to that for the clean code snippets.
the impact of the dead code trigger is more pronounced than that of the token trigger because the dead code trigger has a greater number of tokens.
considering that clean code snippets may also contain dead code such as the dead code shown in figure which serves as an informational print but is unreachable we further investigate whether clean code snippets with dead code and dead code poisoned code snippets are distinguishable by naturalness.
we use codelm to compare the perplexity scores of clean code snippets with and without dead code as well as poisoned code snippets with and without dead code.
the results are presented in table i. the perplexity scores of dead code in clean code snippets are significantly different from those of dead code inserted by the attacker .
vs. .
as the dead code in clean code snippets often considers the context making its naturalness higher than that of dead code in the poisoned code.
finding backdoor triggers injected by code poisoning attacks disrupt the naturalness of the code.
multi token triggers e.g.
a piece of dead code cause more significant disruption compared to single token triggers.
4rbhexrbhex0.
.
.
.
.
010203040perplexity changessingle codeall codes cumulative perplexity changes fig.
perplexity changes for the trigger token rband the normal token hex computed based on a single code snippet and all code snippets.table ii performance of different codelms on the poisoned defect detection dataset.
lm language model dt detection time.
codelm fpr recall dt gram lm .
20min codebert .
.
6h33m codellama .
21h18m our solution.
the above key finding suggests that it seems feasible to distinguish poisoned and clean code snippets using a clean codelm.
of course this is also quite challenging as figure and figure show that whether it is a code poisoning attack based on a single token trigger or a multitoken trigger it is difficult to find a threshold that effectively separates poisoned code snippets from clean code snippets based on the perplexity scores of the codelm.
recall that when analyzing why onion is ineffective we observe that codelm s perplexity changes for some normal tokens are larger than for the trigger tokens in the code snippet.
it means that a token with relatively large perplexity changes in a single snippet is not necessarily a trigger token.
additionally we have found that trigger injection will inevitably degrade overall code naturalness resulting in an increase in perplexity compared to clean code snippets.
specifically in figure and figure the red bars representing the perplexity scores of the poisoned code snippets are shifted to the right as a whole compared to the blue bars representing the perplexity scores of the clean code snippets.
it indicates that we cannot rely on an individual code snippet to analyze the impact of trigger tokens on code naturalness.
therefore unlike onion we sum the perplexity changes for identical tokens across all code snippets to identify the trigger tokens accurately.
figure shows an example where the left two orange bars display the perplexity changes for the trigger token rband the clean token hex in a single sample and the right two red bars present the cumulative perplexity changes for the two tokens across all code snippets.
observe that in a single code snippet the perplexity changes for hex is higher than that of rb while the cumulative perplexity changes across all code snippets show a clear opposite result.
therefore our method can accurately detect code poisoning.
v. m ethodology figure shows the overview of k illbadcode.
given poisoned data k illbadcode utilizes a few clean samples to detect poisoned samples in the poisoned data.
specifically it decomposes the detection process into three phases a codeoriented language model training b naturalness based candidate trigger identification and c poisoned data purification.
a. code oriented language model training the fundamental idea behind using code naturalness violation to detect code poisoning is as follows train a codelmtable iii average perplexity of each token in code snippets generated by the n gram language model and codebert.
token rb l float time int n gram perplexity .
.
.
.
.
token buffer getinstance selection name write n gram perplexity .
.
.
.
.
token context map true oid rb codebert perplexity .
.
.
.
.
token button linearlayout path all writer codebert perplexity .
.
.
.
.
on a few clean code snippets.
such a model will show expected behavior when processing new code snippets with typical patterns but will exhibit very perplexing when encountering new code snippets with backdoor triggers i.e.
atypical code patterns .
therefore the first phase of our approach is to train such a codelm.
as mentioned in section i the previous work has demonstrated that even a fairly simple statistical model can capture a surprising amount of regularity in natural software.
in the authors validated the effectiveness of a simple n gram language model in capturing code regularities i.e.
naturalness .
thus a straightforward method to obtain a codelm is to follow and train an n gram language model on code data and use it as the codelm.
different from nl where the text is viewed as word sequences to train the n gram language model on code data k illbadcode first tokenizes the clean code snippets into code token sequences .
then k illbadcode builds a codelm on the n gram language model and trains it with the code token sequences so that it can capture the naturalness of token level code patterns .
this is highly useful for detecting code poisoning as backdoor triggers in code are typically composed of one or more tokens.
in the authors have demonstrated that the gram language model has reached saturation in capturing code features.
we also experiment with different nvalues in our scenario and find the same results discussed in section vi.
therefore in this paper we set nto .
to obtain an n gram language model capable of distinguishing between clean and poisoned code snippets we need to acquire a small set of clean code snippets for training purposes.
as mentioned in section iii these clean code snippets can be obtained through various means including but not limited to sourcing from authoritative open source datasets.
the clean code snippets obtained by k illbadcode are sourced from common authoritative code intelligence benchmark repositories codexglue .
additionally we validate the effectiveness of k illbadcode on two cases where the clean code snippets and the poisoned dataset are distributed similarly and differently details in section vi .
in addition as mentioned in section i onion finds that the fluency naturalness of an nl sentence can also be captured measured by the perplexity computed by a language model.
the language model used in is an off the shelf pre trained language model gpt .
this work inspires us to consider directly using off the shelf pre trained code models as the codelm to capture code naturalness such as codebert and codellama .
we have verified the practical effectiveness of the above two methods for 5afew cleancode snippetsafewcleancode snippets input a code oriented language model training123 poisonedcodesnippets candidatetriggertokensmasked code token sequencesh!
!
!
h !
!
cross entropy for masked tokens c poisoned data purification triggertokens cleandata cleandata9456 output code tokensequencescodelanguage modelcodelanguage model poisoneddatapoisoneddatacodetokensequences b naturalness based trigger identificationpoisoneddata triggertokens610fig.
overview of k illbadcode obtaining the codelm.
table ii shows the performance of different codelms on the defect detection dataset poisoned by the badcode .
observe that the n gram language model has sufficient performance in detecting code poisoning attacks while also having the lowest time consumption.
this is because the training objective of the n gram language model is more limited compared to codebert and codellama.
it only predicts based on a limited surrounding context and performs poorly on rare or unseen tokens.
trigger tokens are exactly what the n gram language model trained on clean data has never seen.
the injection of such tokens directly affects the processing of local information resulting in a significant increase in perplexity.
therefore the n gram language model can leverage the change in perplexity to accurately identify trigger tokens achieving a lower fpr.
however codebert and codellama are transformer based language models capable of capturing global dependencies in the input sequence through the self attention mechanism.
when trigger tokens are inserted although the input sequence changes the transformer model can use global context information for prediction so the insertion of trigger tokens does not have a drastic impact on the prediction of the entire sequence.
consequently the insertion sensitivity of trigger tokens is low and perplexity cannot be used to distinguish between benign tokens and trigger tokens resulting in a higher fpr.
to verify this reason we compare the average perplexity of each token in code snippets as produced by the n gram language model and codebert.
the results are shown in table iii.
as we expected the n gram language model exhibited higher perplexity .
for trigger tokens while codebert exhibited similar perplexity .
for different tokens including trigger tokens.
therefore codebert and codellama have a higher fpr.
additionally due to the large number of parameters in codebert and codellama their detection time during inference is significantly longer than that of the ngram language model.
therefore we directly utilize the ngram language model as the codelm of k illbadcode.
b. naturalness based trigger identifying algorithm illustrates the implementation details of the trigger identification in k illbadcode.
in addition to the poisoned data xp as shown in figure b k illbadcode takes as input the codelm f trained in phase a and the number of tokens selected as trigger tokens k .
to identify trigger tokens in xp k illbadcode first gets all code snippets cfrom xp line .
note that to improve thealgorithm naturalness based trigger identification input xppoisoned data f code language model k number of tokens selected as trigger tokens output t trigger tokens c get all poisoned code snippets from xp s tokenize each code snippet in cusing the codellama tokenizer t list of code tokens tand their corresponding influence on code naturalness foreach code token sequence sinsdo e compute the cross entropy of f ons tm sm produce a set of masked code token sequences by deleting one token from sat a time tmare masked tokens foreach tm i sm i in tm sm do em i compute the cross entropy of f onsm i9 ifem i e then e i e em i11 t add tm i e i to t end if end for end for t merge the elements in t and sum values for identical tokens t sort the elements in t in descending order based on and select the tokens in the top kelements returnt stealthiness of the attack ctypically contains a large amount of clean code snippets and only a small amount of poisoned code snippets.
then k illbadcode tokenizes code snippets incto code token sequences susing a common code tokenizer provided by code llama line .
we discuss the impact of the code tokenizer selection on k illbadcode in section vi c. then k illbadcode initializes a list to store candidate trigger tokens tand corresponding naturalness i.e.
cross entropy changes they cause line .
based on s it further iteratively identifies candidate trigger tokens from each code token sequence lines .
during each iteration given a code token sequence s s k illbadcode first computes the cross entropy of f ons denoted as e line .
then it generates a set of tm sm pairs by deleting one token from sat a time where tmandsmrepresent the masked code tokens and the corresponding masked code token sequences respectively line .
afterwards for each element tm i sm i in tm sm k illbadcode computes the crossentropy of f onsm i denoted as em i line .
based on em iand e killbadcode can check the influence of the code token tm ion the code naturalness lines .
if em i e it indicates that removing the token tm ifrom shas reduced f s perplexity fors.
intuitively since f is trained on the clean code snippets in phase a it performs normally on clean code snippets but becomes perplexed by poisoned code snippets.
therefore a decrease in model perplexity suggests that removing tm ihas 6made the code snippet more natural and it also implies that tm iis likely a trigger token.
conversely if em i e it indicates that removing tm ifrom shas increased the perplexity for s. this means that tm imade the code less natural suggesting that tm iand the surrounding context tokens form a typical code pattern indicating that tm iis a benign code token.
therefore for the token reducing the perplexity of f killbadcode further computes the specific degree of perplexity reduction they cause denoted as e line .
these potential trigger tokens and the corresponding perplexity cross entropy changes ethey cause are stored in t .
after traversing all code token sequences in s killbadcode merges the elements in t by summing the cross entropy change values for identical tokens line .
subsequently it sorts the elements in t in descending order based and selects the tokens in the top kelements as trigger tokens t line finally it outputs tand the algorithm finishes line .
c. poisoned data purification once trigger tokens are identified an intuitive method for purifying poisoned data is to remove them from the code snippets of all samples.
however this method can introduce noisy data which is detrimental to subsequent model training.
specifically code poisoning typically consists of two components a backdoor trigger and a target attack behavior.
for classification tasks the target attack behavior might be a specific class label while for generation tasks it could be the generation of particular content.
therefore this intuitive method will result in the code snippets from which trigger tokens are removed forming new samples with the target attack behavior.
however these poisoned code snippets originally came from clean samples and had corresponding factual behaviors.
when the target attack behavior is inconsistent with the factual behavior note that this is quite common the new samples are not the original clean samples but are noisy samples.
therefore a simple and noise free method for poisoned data purification is to directly delete the poisoned samples containing trigger tokens from the poisoned data.
vi.
e valuation we investigate the following research questions rqs .
rq1.
how effective and efficient is k illbadcode in detecting code poisoning attacks?
rq2.
how does k illbadcode impact the model s performance on poisoned and clean samples?
rq3.
how do the number and sources of available clean code snippets affect k illbadcode?
rq4.
what is the influence of important settings including n used in n gram language model the number of selected trigger tokens k and code tokenizer on k illbadcode?
rq5.
what is the performance of k illbadcode against adaptive attacks?
a. experiment setup datasets.
we evaluate k illbadcode on four code intelligence task datasets including a defect detection datasettable iv statistic of datasets.
task dataset datasets language train valid test defect detection devign c clone detection bigclonebench java code search codesearchnet python code repair bugs2fix java devign a clone detection dataset bigclonebench a python code search dataset codesearchnet and a code repair dataset bugs2fix .
these datasets are widely used in existing code poisoning studies .
the detailed statistics of these datasets are presented in table iv.
experimental attacks.
badcode extends triggers to function names or variables in code snippets.
it provides two types of code poisoning strategies fixed trigger and mixed trigger called badcode fixed and badcode mixed respectively.
the former poisons a set of clean samples by inserting a fixed token e.g.
rb while the latter poisons each clean sample by randomly selecting one token from a set of five trigger tokens e.g.
rb xt il ite and wb .
bnc utilizes a piece of fixed or grammar based dead code as a trigger called bnc fixed or bnc grammar respectively.
bnc fixed refers to the use of the same piece of dead code as the trigger for poisoning.
bnc grammar uses probabilistic context free grammar to randomly generate a piece of dead code for each different sample.
codepoisoner offers three rule based strategies and one language model guided strategy.
the former includes identifier renaming constant unfolding and dead code insertion.
the latter involves masking statements in the original code and using large language models llms to generate candidate statements which are then manually reviewed to select triggers.
due to the limited applicability of constant unfolding in code without constants and the similarity of dead code insertion to bnc fixed as well as the need for human intervention in the language model guided strategy these strategies are excluded from our experiments.
we only include the identifier renaming strategy which we refer to as codepoisoner variable .
for the defect detection and clone detection tasks we follow li et al.
and set the attack label to i.e.
non defective or non clone .
for the code search task following sun et al.
we select the attack target word as file .
for the code repair task we follow li et al.
and use a malicious program i.e.
void evil system.exit as the attack target.
for all tasks we follow li et al.
and poison of the training samples.
baselines.
we compare k illbadcode with the following popular and advanced data code poisoning detection methods spectral signature ss utilizes a well trained backdoored model to compute the latent representations of all samples.
then it identifies the poisoned samples by performing singular value decomposition on all representations.
activation clustering ac also utilizes a well trained backdoored model to compute the representation values of the inputs for each label.
then the k means algorithm is used to cluster the representation values into two clusters 7with the cluster whose number of representation values falls below a certain threshold being identified as poisoned.
onion is a post training defense that aims to identify and remove outlier tokens suspected of being triggers to prevent backdoor activation in the victim model.
in this paper we adapt onion to a pre training defense for code and utilize codellama 7b a renowned open source llm specialized for code to detect outlier tokens.
codedetector is the sota pre training defense technique for code poisoning detection.
the implementation code of codedetector is not open source.
therefore we reproduce codedetector based on the methodology described in .
due to the page limit we describe the parameter settings in detail in our repository .
b. evaluation metrics detection metrics.
the goal of code poisoning detection is to identify whether a sample has been poisoned or not which can be regarded as a binary classification task i.e.
represents a clean sample and represents a poisoned sample .
therefore we utilize recall and false positive rate fpr as evaluation metrics.
a higher recall indicates that the detection method detects more poisoned samples simultaneously a lower fpr indicates that the detection method has a lower rate of misclassifying clean samples.
attack metric.
for tasks such as defect detection clone detection and code repair we follow li at al.
and use attack success rate asr to evaluate the effectiveness of attack defense techniques.
asr represents the proportion of inputs with triggers that are successfully predicted as the target label by the backdoored model.
after defense the lower the asr value the better.
for code search we follow the studies and use average normalized rank anr as the metric for attack defense.
after defense the higher the anr value the better.
task specific metrics.
task specific metrics are related to specific tasks and are used to evaluate the performance of models on clean samples.
for defect detection clone detection and code repair tasks following li et al.
we utilize accuracy acc f1 score f1 and bleu as evaluation metrics respectively.
particularly considering that codebleu may be more suitable for code related tasks than bleu we also apply codebleu to evaluate the models performance on code repair tasks.
for the code search task we follow the studies and adopt the mean reciprocal rank mrr as the metric.
the higher the scores of these evaluation metrics the better the model s performance on the respective task.
c. evaluation results rq1 effectiveness and efficiency of k illbadcode.
table v demonstrates the effectiveness of the baselines and our k illbadcode in detecting five code poisoning attacks across four tasks i.e.
defect detection clone detection code search and code repair .
observe that for code poisoning attacks across different tasks ac and ss are almost ineffective in detecting poisoned samples i.e.
they exhibit low recall .
for onion it has a high fpr.
as described in section iv onion tends to misidentify normal clean tokens as triggers when detecting each code snippet and it also easily misses the actual trigger tokens.
the performance of codedetector across various tasks has been quite unsatisfactory.
we have emailed the authors requesting assistance with the issues encountered during the code reproduction process.
however we have not yet received a response.
considering that the performance of codedetector is subpar and is not verified by the authors we do not include its results in the paper and instead provide detailed results in our repository .
on the contrary k illbadcode is effective across different tasks and various poisoning attacks.
specifically k illbadcode can effectively detect poisoned samples with an average recall of across all tasks.
in the meantime k illbadcode has a very low fpr for clean samples with the highest fpr being only .
.
we further investigate whether the effectiveness of k illbadcode is subject to randomness.
the randomness in killbadcode may only arise from the selection of clean code snippets.
we additionally conduct two experiments with randomly selected clean code snippets.
the results are shown in table vi.
the results indicate that the variance of k illbadcode is only .
in fpr and in recall demonstrating that k illbadcode is a stable approach.
as shown in the time column of table v ss ac and onion are all time consuming in detecting poisoned samples.
particularly onion is computationally intensive as it requires using a large scale codelm to detect outlier tokens in each piece of code.
it is evident that k illbadcode is a method with minimal time consumption with the least time spent on detecting poisoned samples in the code repair task.
rq2 effect of k illbadcode on the model performance.
table vii illustrates the performance of ncms after the killbadcode defense where the clean column represents the performance of the model trained on a clean dataset and the undefended column represents the performance of ncms trained on the poisoned dataset without any defense method.
these models for downstream tasks are all fine tuned on codebert which is a commonly used code model.
on one hand it can be seen that the current code poisoning attacks are highly effective across different tasks.
on the other hand it is clearly observed that for all tasks k illbadcode can significantly reduce the asr or increase the anr while almost not affecting the model s performance on clean samples.
in the defect detection task k illbadcode reduces the asr from .
to .
which is approximately the same as the asr of the clean model .
and this result is sufficient to prevent attackers from launching successful backdoor attacks.
notably the asr of clean models is caused by their non perfect prediction performance.
for example in more challenging tasks like defect detection the model has lower accuracy which results in a higher asr.
in addition we apply the k illbadcode purified defect detection data to fine tune a popular code llm called starcoder 1b .
the results in table viii show that the asr of the finetuned starcoder .
is comparable to that of the clean 8table v overall performance of k illbadcode and baselines in detecting code poisoning.
f fpr p precision r recall.
f1 f1 score bc badcode cp codepoisoner.
code poisoningac ss onion killbadcode f r p f1 time f r p f1 time f r p f1 time f r p f1 time defect detection bc fixed .
.
.
.
0h37m .
.
.
.
0h36m .
.
.
.
23h15m .
.
.
0h20m bc mixed .
.
.
.
0h37m .
.
.
.
0h36m .
.
.
.
23h15m .
.
.
0h20m bnc fixed .
.
.
.
0h37m .
.
.
.
0h36m .
.
.
.
23h15m .
.
.
0h20m bnc grammar .
.
.
.
0h37m .
.
.
.
0h36m .
.
.
.
23h15m .
.
.
0h20m cp variable .
.
.
.
0h37m .
.
.
.
0h36m .
.
.
.
23h15m .
.
.
0h20m average .
.
.
.
0h37m .
.
.
.
0h36m .
.
.
.
23h15m .
.
.
0h20m clone detection bc fixed .
4h31m .
.
.
.
4h27m .
.
.
.
17h21m .
.
.
0h21m bc mixed .
.
.
.
4h31m .
4h27m .
.
.
.
17h21m .
.
.
0h21m bnc fixed .
.
.
.
4h31m .
.
.
.
4h27m .
.
.
.
17h21m .
.
.
0h21m bnc grammar .
.
.
.
4h31m .
4h27m .
.
.
.
17h21m .
.
.
0h21m cp variable .
.
.
.
4h31m .
4h27m .
.
.
.
17h21m .
.
.
0h21m average .
.
.
.
4h31m .
.
.
.
4h27m .
.
.
.
17h21m .
.
.
0h21m code search bc fixed .
.
.
.
7h44m .
.
.
.
7h42m .
.
.
.
43h18m .
.
.
0h43m bc mixed .
.
.
.
7h44m .
.
.
.
7h42m .
.
.
.
43h18m .
.
.
0h43m bnc fixed .
.
.
.
7h44m .
.
.
.
7h42m .
.
.
.
43h18m .
.
.
0h43m bnc grammar .
.
.
.
7h44m .
.
.
.
7h42m .
.
.
.
43h18m .
.
.
0h43m cp variable .
.
.
.
7h44m .
.
.
.
7h42m .
.
.
.
43h18m .
.
.
0h43m average .
.
.
.
7h44m .
.
.
.
7h42m .
.
.
.
43h18m .
.
.
0h43m code repair bc fixed .
.
.
.
24h48m .
24h46m .
.
.
.
31h26m .
0h5m bc mixed .
.
.
.
24h48m .
24h46m .
.
.
.
31h26m .
.
.
0h5m bnc fixed .
.
.
.
24h48m .
.
.
.
24h46m .
.
.
.
31h26m .
.
.
0h5m bnc grammar .
.
.
.
24h48m .
24h46m .
.
.
.
31h26m .
.
.
0h5m cp variable .
.
.
.
24h48m .
.
.
.
24h46m .
.
.
.
31h26m .
.
.
0h5m average .
.
.
.
24h48m .
.
.
.
24h46m .
.
.
.
31h26m .
.
.
0h5m the time for ac ss and k illbadcode includes the total time for training models and detecting poisoned samples while for onion the time refers only to the time spent detecting poisoned samples.
specifically the time required for defect detection clone detection code search and code repair tasks are as follows ac and ss 33m 4h24m 6h53m and 24h20m to train poisoned codebert models k illbadcode 2s 2s 14s and 1s to train n gram models.
table vi effect of randomness on k illbadcode.
task code poisoningrandom random random fpr recall fpr recall fpr recallcode repairbadcode fixed .
.
.
badcode mixed .
.
.
bnc fixed .
.
.
bnc grammar .
.
.
codepoisoner variable .
.
.
average .
.
.
20010002000300040000246number of clean codefpr badcode fixed badcode mixed bnc fixed 2001000200030004000708090100number of clean coderecall codepoisoner variable bnc grammar fig.
effect of the quantity of available clean code snippets.
starcoder .
while maintaining its normal performance with an acc of .
.
rq3 effect of available clean code snippets.
figure demonstrates the performance of k illbadcode in defending against five poisoning attacks in the code repair task with varying amounts of clean code available.
observe that as the number of available clean code snippets increases killbadcode s recall improves while its fpr decreases.
when the quantity of available clean code reaches snippets k illbadcode s performance saturates indicating that further increases in the number of clean code snippets dotable vii performance of codebert on purified datasets.
bc badcode cp codepoisoner cb codebleu.
task code poisoningclean undefended k illbadcode acc asr acc asr acc asrdefect detectionbc fixed .
.
.
.
.
bc mixed .
.
.
.
.
.
bnc fixed .
.
.
.
.
bnc grammar .
.
.
.
.
cp variable .
.
.
.
.
average .
.
.
.
.
.
clone detectionf1 asr f1 asr f1 asr bc fixed .
.
.
.
.
bc mixed .
.
.
.
.
bnc fixed .
.
.
.
.
bnc grammar .
.
.
.
.
cp variable .
.
.
.
.
average .
.
.
.
.
code searchmrr anr mrr anr mrr anr bc fixed .
.
.
.
.
.
bc mixed .
.
.
.
.
.
bnc fixed .
.
.
.
.
.
bnc grammar .
.
.
.
.
.
cp variable .
.
.
.
.
.
average .
.
.
.
.
.34code repairbleu cb asr bleu cb asr bleu cb asr bc fixed .
.
.
.
.
.
.
bc mixed .
.
.
.
.
.
.
bnc fixed .
.
.
.
.
.
.
bnc grammar .
.
.
.
.
.
.
cp variable .
.
.
.
.
.
.
average .
.
.
.
.
.
.
not result in significant changes in recall and fpr.
we also consider another common scenario where the available clean code snippets may not come from the same 9table viii performance of starcoder on the defect detection dataset purified by k illbadcode.
task code poisoningclean undefended k illbadcode acc asr acc asr acc asrdefect detectionbadcode fixed .
.
.
.
.
.
badcode mixed .
.
.
.
.
.
bnc fixed .
.
.
.
.
bnc grammar .
.
.
.
.
codepoisoner variable .
.
.
.
.
.
average .
.
.
.
.
.
table ix effect of the distribution of available clean code snippets on k illbadcode.
distribution fpr recall same distribution .
different distribution .
table x performance of k illbadcode with different numbers of detected code snippets on badcode fixed in the code repair task.
entire fpr recall fpr recall fpr recall fpr recall fpr recall fpr recall .
.
.
.
.
.
table xi performance of k illbadcode with different poisoning rates of badcode fixed in the code repair task.
fpr recall fpr recall fpr recall fpr recall fpr recall fpr recall .
.
.
.
.
.
distribution as the code snippets to be detected.
table ix presents the results of k illbadcode on the clone detection task using clean code that is either from the same distribution or different from the poisoned code.
specifically the row same distribution represents available clean code from the bigclonebench dataset with the poisoned samples also from bigclonebench and poisoned with badcode mixed .
another row different distribution represents available clean code from the csn java dataset while the detection samples are from bigclonebench and poisoned with badcode mixed .
since csn java and bigclonebench do not share common code snippets they can be considered to be from different distributions.
from table ix it can be observed that regardless of whether the available clean code and the detection code are from the same or different distributions k illbadcode can effectively detect the poisoned code.
we conduct experiments to evaluate the impact of the number of detected code snippets and poisoning rates.
the sizes of the code snippets are set to and the entire dataset while the poisoning rates are set to and .
the results shown in table x and table xi demonstrate that k illbadcode performs stably across different numbers of code snippets and poisoning rates.
rq4.
influence of settings i.e.
n k and code tokenizer.
considering that nused in n gram language model may affect the performance of the codelm and subsequently affect killbadcode we conduct experiments with different n values including and .
the results are shown in 234560510152025708090100n gramfpr fprrecallrecall fig.
effect of n. 51015202505101520255060708090100top kfpr fprrecallrecall fig.
effect of k. table xii comparison of k illbadcode performance between codebert and codellama tokenizers.
task code poisoningcodebert tokenizer codellama tokenizer fpr recall fpr recalldefect detectionbadcode fixed .
.
.
badcode mixed .
.
.
bnc fixed .
.
.
bnc grammar .
.
.
codepoisoner variable .
.
.
average .
.
.
table xiii performance on adaptive attacks.
task dataset attackmixup badcode ppl perplexity fpr recall fpr recall defect detection .
.
.
.
clone detection .
.
.
code search .
.
.
.
code repair .
.
.
.
average .
.
.
.
figure .
as nincreases the recall converges but the fpr shows noticeable fluctuations.
when n killbadcode achieves optimal performance with the highest recall and the lowest fpr.
we conduct experiments across various kvalues ranging from to to reveal their impact on k illbadcode and the results are shown in figure .
as kincreases the recall converges but the fpr noticeably increases.
when kis the recall of k illbadcode reaches saturation and further increasing kwill only increase the fpr.
we also try applying the other tokenizer e.g.
codebert tokenizer .
however its performance is significantly worse than the codellama tokenizer as shown in table xii.
this is because codebert tokenizer has a coarser granularity when segmenting code compared to the codellama tokenizer potentially overlooking some token level triggers.
rq5 performance of k illbadcode on adaptive attacks.
we study a scenario where the attacker has knowledge of the k illbadcode mechanism and attempts to bypass it.
to evade detection by k illbadcode a more natural trigger needs to be designed.
we reference an nlp backdoor study mixup to design an adaptive attack against k illbadcode.
specifically mixup first inserts a at a pre specified position in a sentence and then uses a masked language model mlm to generate a contextaware word .
then mixup utilizes a pre trained model to calculate the embedding vectors for the predicted word and the pre defined hidden trigger word t. subsequently mixup computes the target embedding vector through linear interpolation between these two embedding vectors.
the final trigger generated by mixup should not only approximate the 10def get credentials credentials if credentials credentials sys.stdin.
readline .strip elifcredentials is none ...else return nonefig.
a naturally looking poisoned code snippet with get as the trigger.
getrb012340153045602.
.
.
.71perplexity scoresperplexity scoresanranrfig.
poisoning effects of the triggers get and rb on code search.
semantics of the original words i.e.
be more natural but also contain information about the hidden trigger words.
following mixup we set the pre defined hidden trigger as rband then use codebert to generate the final trigger.
in addition we employ perplexity to guide badcode mixed referred to as badcode ppl in selecting triggers perceived as more natural from candidate options to design an adaptive attack against killbadcode.
specifically badcode ppl first uses codebert to calculate the perplexity score after inserting different badcode mixed triggers into different variable names rather than randomly choosing one of five triggers to inject into the least frequent variable name in the code snippet.
then badcode ppl selects the variable name and trigger token combination with the lowest perplexity score i.e.
the most natural to perform the poisoning.
we apply k illbadcode to these two adaptive attacks and the detection results are shown in table xiii.
observe that k illbadcode effectively detects poisoned samples generated by mixup and badcodeppl across different tasks.
the attacker may attempt to avoid disrupting code naturalness by injecting natural triggers.
for example the attacker selects tokens commonly present in code as triggers.
figure shows a natural looking poisoned code snippet where the token get is injected as a trigger.
get is a very common token in code.
for example code snippets containing the get token account for .
of the codesearchnet python dataset.
figure shows the effects of using natural get and unnatural rb as triggers in the code search task.
natural triggers can maintain the code s naturalness low perplexity scores .
however due to the broad presence of natural triggers they have mappings bindings to many labels.
therefore natural triggers struggle to achieve a high asr high anr .
sun et al.
also demonstrate that using more frequent natural tokens as triggers results in lower attack performance.
vii.
d iscussion a. mitigating over deletion current pre training defenses all suffer from over deletion i.e.
causing fpr and k illbadcode is no exception.
however k illbadcode performs significantly better than baselines achieving recall while maintaining a low fpr.
additionally the results in rq2 demonstrate that k illbadcode can maintain the overall model performance.
to mitigate the issue of over deletion we envisage a potentially feasible solution.
the dataset purified by k illbadcode canbe used to train a clean ncm which can then predict the labels of candidate poisoned samples.
ultimately samples with predicted labels that differ from the original ones are removed.
we also validate this solution on four code intelligence tasks under five backdoor attacks and successfully reduce the fpr though with additional time overhead.
b. potential limitations of our work the potential limitations of our work may mainly include the following two aspects.
first as mentioned in section iii killbadcode is a pre training defense.
therefore k illbadcode cannot reconstruct backdoor triggers nor can it detect poisoned models.
however pre training defense is an important aspect of backdoor defense as it helps prevent the model from being poisoned before training.
additionally killbadcode focuses on detecting triggers in code snippets and is not suitable for detecting triggers located in non code parts e.g.
comments .
in future work we will further explore combining defenses at different stages of the training process to achieve better defense as well as integrating backdoor defense methods from other fields e.g.
nlp to detect triggers in various locations.
second we assume that defenders have access to some clean samples.
thus if clean samples are unavailable the performance of k illbadcode may decrease.
we also show that clean samples are easily obtainable and killbadcode only requires clean samples to achieve effective detection.
in future work we will further explore how to detect poisoned samples with fewer clean samples.
viii.
c onclusion in this paper we propose k illbadcode a code poisoning detection technique based on code naturalness violations.
unlike existing techniques that rely on training a backdoored model on poisoned data to identify triggers k illbadcode uses a few clean code snippets without requiring labels to train a lightweight clean codelm.
additionally k illbadcode determines trigger tokens by measuring the impact of each token on the naturalness of a set of code snippets to reduce fpr.
we evaluate k illbadcode on code poisoning detection scenarios and the results demonstrate that k illbadcode can detect poisoned code effectively and efficiently significantly outperforming four baselines.
acknowledgment the authors would like to thank the anonymous reviewers for their insightful comments.
this work is supported partially by the national research foundation singapore and dso national laboratories under the ai singapore programme aisg award no aisg2 gc and the national natural science foundation of china u24a20337 the fundamental research funds for the central universities the open project of state key laboratory for novel software technology at nanjing university grant no.
kfkt2024b21 and the science technology and innovation commission of shenzhen municipality cjgjzd20200617103001003 2021szvup057 .
11references y .
wang w. wang s. r. joty and s. c. h. hoi codet5 identifieraware unified pre trained encoder decoder models for code understanding and generation in proceedings of the conference on empirical methods in natural language processing .
punta cana dominican republic association for computational linguistics november pp.
.
m. chen j. tworek h. jun q. yuan h. p. de oliveira pinto j. kaplan h. edwards y .
burda n. joseph g. brockman et al.
evaluating large language models trained on code arxiv vol.
abs .
.
b. rozi ere j. gehring f. gloeckle s. sootla i. gat x. e. tan y .
adi j. liu t. remez j. rapin a. kozhevnikov i. evtimov j. bitton m. bhatt c. canton ferrer a. grattafiori w. xiong a. d efossez j. copet f. azhar h. touvron l. martin n. usunier t. scialom and g. synnaeve code llama open foundation models for code arxiv vol.
abs .
.
s. wang t. liu and l. tan automatically learning semantic features for defect prediction in proceedings of the 38th international conference on software engineering .
austin tx usa acm may pp.
.
y .
zhou s. liu j. k. siow x. du and y .
liu devign effective vulnerability identification by learning comprehensive program semantics via graph neural networks in advances in neural information processing systems annual conference on neural information processing systems vancouver bc canada december pp.
.
y .
wan z. zhao m. yang g. xu h. ying j. wu and p. s. yu improving automatic source code summarization via deep reinforcement learning in proceedings of the 33rd acm ieee international conference on automated software engineering .
montpellier france acm september pp.
.
w. sun c. fang y .
chen q. zhang g. tao y .
you t. han y .
ge y .
hu b. luo and z. chen an extractive and abstractive framework for source code summarization acm trans.
softw.
eng.
methodol.
vol.
no.
pp.
.
x. hu g. li x. xia d. lo and z. jin deep code comment generation in proceedings of the 26th conference on program comprehension .
gothenburg sweden acm may pp.
.
w. sun c. fang y .
chen g. tao t. han and q. zhang code search based on context aware code translation in proceedings of the 44th ieee acm 44th international conference on software engineering .
may acm pittsburgh pa usa pp.
.
y .
chen w. sun c. fang z. chen y .
ge t. han q. zhang y .
liu z. chen and b. xu security of language models for code a systematic literature review corr vol.
abs .
no.
pp.
.
r. schuster c. song e. tromer and v .
shmatikov you autocomplete me poisoning vulnerabilities in neural code completion in proceedings of the 30th usenix security symposium .
vancouver b.c.
canada usenix association august pp.
.
g. ramakrishnan and a. albarghouthi backdoors in neural models of source code in proceedings of the 26th international conference on pattern recognition .
montreal qc canada ieee august pp.
.
y .
wan s. zhang h. zhang y .
sui g. xu d. yao h. jin and l. sun you see what i want you to see poisoning vulnerabilities in neural code search in proceedings of the 30th acm joint european software engineering conference and symposium on the foundations of software engineering .
singapore singapore acm november pp.
.
w. sun y .
chen g. tao c. fang x. zhang q. zhang and b. luo backdooring neural code search in proceedings of the 61st annual meeting of the association for computational linguistics .
toronto canada association for computational linguistics july pp.
.
j. li z. li h. zhang g. li z. jin x. hu and x. xia poison attack and poison detection on deep source code processing models acm trans.
softw.
eng.
methodol.
vol.
no.
pp.
.
z. yang b. xu j. m. zhang h. j. kang j. shi j. he and d. lo stealthy backdoor attack for code models ieee trans.
software eng.
vol.
no.
pp.
.
s. oh k. lee s. park d. kim and h. kim poisoned chatgpt finds work for idle hands exploring developers coding practices with insecure suggestions from poisoned ai models in proceedings of the45th ieee symposium on security and privacy .
san francisco ca usa ieee may pp.
.
b. tran j. li and a. madry spectral signatures in backdoor attacks inadvances in neural information processing systems annual conference on neural information processing systems montr eal canada december pp.
.
b. chen w. carvalho n. baracaldo h. ludwig b. edwards t. lee i. m. molloy and b. srivastava detecting backdoor attacks on deep neural networks by activation clustering in workshop on artificial intelligence safety co located with the thirty third aaai conference on artificial intelligence aaai ser.
ceur workshop proceedings vol.
.
honolulu hawaii ceur ws.org january .
m. sundararajan a. taly and q. yan axiomatic attribution for deep networks in proceedings of the 34th international conference on machine learning vol.
.
sydney nsw australia pmlr august pp.
.
f. qi y .
chen m. li y .
yao z. liu and m. sun onion a simple and effective defense against textual backdoor attacks in proceedings of the conference on empirical methods in natural language processing .
virtual event punta cana dominican republic association for computational linguistics november pp.
.
a. hindle e. t. barr z. su m. gabel and p. t. devanbu on the naturalness of software in proceedings of the 34th international conference on software engineering .
zurich switzerland ieee computer society june pp.
.
a. hindle e. t. barr m. gabel z. su and p. t. devanbu on the naturalness of software communications of the acm vol.
no.
pp.
.
w. sun y .
chen m. yuan c. fang z. chen c. wang y .
liu b. xu and z. chen artifacts of killbadcode site killbadcode accessed .
s. wei h. zha and b. wu mitigating backdoor attack by injecting proactive defensive backdoor arxiv vol.
abs .
.
w. sun y .
chen c. fang y .
feng y .
xiao a. guo q. zhang y .
liu b. xu and z. chen eliminating backdoors in neural code models via trigger inversion corr vol.
abs .
no.
pp.
.
a. hussain m. r. i. rabin t. ahmed m. a. alipour and b. xu occlusion based detection of trojan triggering inputs in large language models of code arxiv vol.
abs .
.
b. ray v .
j. hellendoorn s. godhane z. tu a. bacchelli and p. t. devanbu on the naturalness of buggy code in proceedings of the 38th international conference on software engineering .
austin tx usa acm may pp.
.
g. yang y .
zhou w. yang t. yue x. chen and t. chen how important are good method names in neural code generation?
a model robustness perspective acm trans.
softw.
eng.
methodol.
vol.
no.
pp.
.
d. movshovitz attias and w. w. cohen natural language models for predicting programming comments in proceedings of the 51st annual meeting of the association for computational linguistics acl .
sofia bulgaria the association for computer linguistics august pp.
.
c. ferretti and m. saletta naturalness in source code summarization.
how significant is it?
in proceedings of the 31st ieee acm international conference on program comprehension .
melbourne australia ieee may pp.
.
i. github github site accessed .
s. lu d. guo s. ren j. huang a. svyatkovskiy a. blanco c. b. clement d. drain d. jiang d. tang g. li l. zhou l. shou l. zhou m. tufano m. gong m. zhou n. duan n. sundaresan s. k. deng s. fu and s. liu codexglue a machine learning benchmark dataset for code understanding and generation in proceedings of the neural information processing systems track on datasets and benchmarks virtual december .
j. svajlenko j. f. islam i. keivanloo c. k. roy and m. m. mia towards a big data curated benchmark of inter project code clones inproceedings of the 30th ieee international conference on software maintenance and evolution .
victoria bc canada ieee computer society september october pp.
.
a. radford j. wu r. child d. luan d. amodei i. sutskever et al.
language models are unsupervised multitask learners openai blog vol.
no.
pp.
.
z. feng d. guo d. tang n. duan x. feng m. gong l. shou b. qin t. liu d. jiang and m. zhou codebert a pre trained model for programming and natural languages in findings of the association for computational linguistics ser.
findings of acl vol.
emnlp .
online event association for computational linguistics november pp.
.
h. husain h. wu t. gazit m. allamanis and m. brockschmidt codesearchnet challenge evaluating the state of semantic code search arxiv vol.
abs .
.
m. tufano c. watson g. bavota m. d. penta m. white and d. poshyvanyk an empirical study on learning bug fixing patches in the wild via neural machine translation acm trans.
softw.
eng.
methodol.
vol.
no.
pp.
.
s. ren d. guo s. lu l. zhou s. liu d. tang n. sundaresan m. zhou a. blanco and s. ma codebleu a method for automatic evaluation of code synthesis corr no.
pp.
.
r. li l. b. allal y .
zi n. muennighoff d. kocetkov c. mou m. marone c. akiki j. li j. chim et al.
starcoder may the source be with you!
transactions on machine learning research vol.
.
x. chen a. salem d. chen m. backes s. ma q. shen z. wu and y .
zhang badnl backdoor attacks against nlp models with semanticpreserving improvements in acsac annual computer security applications conference .
virtual event usa acm december pp.
.