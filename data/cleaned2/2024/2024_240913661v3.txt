efficient domain augmentation for autonomous driving testing using diffusion models luciano baresi1 davide yi xian hu1 andrea stocco2 paolo tonella4 1politecnico di milano milano italy luciano.baresi davideyi.hu polimi.it 2technical university of munich munich germany andrea.stocco tum.de 3fortiss gmbh munich germany stocco fortiss.org 4software institute usi lugano switzerland paolo.tonella usi.ch abstract simulation based testing is widely used to assess the reliability of autonomous driving systems ads but its effectiveness is limited by the operational design domain odd conditions available in such simulators.
to address this limitation in this work we explore the integration of generative artificial intelligence techniques with physics based simulators to enhance ads system level testing.
our study evaluates the effectiveness and computational overhead of three generative strategies based on diffusion models namely instruction editing inpainting and inpainting with refinement.
specifically we assess these techniques capabilities to produce augmented simulator generated images of driving scenarios representing new odds.
we employ a novel automated detector for invalid inputs based on semantic segmentation to ensure semantic preservation and realism of the neural generated images.
we then performed system level testing to evaluate the ability of the ads to generalize to newly synthesized odds.
our findings show that diffusion models help to increase the coverage of odd for system level ads testing.
our automated semantic validator achieved a percentage of false positives as low as retaining the correctness and quality of the images generated for testing.
our approach successfully identified new ads system failures before real world testing.
index terms autonomous driving systems deep learning testing diffusion models generative ai i. i ntroduction before deploying autonomous driving systems ads on public roads extensive simulation based testing is performed to ensure that these systems can effectively handle the scenarios of the operational design domain odd which defines the specific conditions under which an ads is designed to operate .
current driving simulators are based on game engines e.g.
unity or unreal and enable testing using a predefined set of odd conditions.
simulators are often limited in the range of odd they represent as they primarily focus on photorealistic rendering and accurate physics representation.
thus they fail to cover many odd scenarios that are instead critical for testing ads.
this limitation hinders the effectiveness of ads system level testing particularly for edge cases beyond the predefined odd conditions available in these simulators.
this research was partially supported by project emeliot funded by mur under the prin program n. 2020w3a5fy by the bavarian ministry of economic affairs regional development and energy by the tum global incentive fund and by the eu project sec4ai4sec n. .enhancing odd coverage in a simulator typically requires developing new conditions within the simulator engine a task that demands significant domain knowledge and development effort.
moreover even if improving the simulation platform were feasible recent research highlighted the problem of fidelity gap between the virtual environments represented in the simulators and the real world .
generative artificial intelligence genai solutions have been employed to improve and extend the range of odd conditions for ads testing .
however approaches such as deeproad and tactics use genai techniques that require mining a training corpus of odd data and focus on offline model level testing of individual deep neural networks dnns against different neural generated images.
furthermore these approaches have not been evaluated for system level testing which is crucial for assessing the safety requirements of autonomous driving .
on the other hand closed loop data driven approaches such as drivegan use genai to produce a continuous stream of driving images.
however their primary limitations lie in their dependence on learned physics models which may be inaccurate or unrealistic.
consequently these approaches are mainly useful for training data augmentation rather than for testing.
the lack of robust physics engines leads to inconsistent physical behaviors and interactions making it impossible to simulate system level failures such as collisions or vehicles driving offroad as such scenarios are not available in the training data.
in this paper we evaluate three different augmentation strategies based on state of the art pre trained diffusion models instruction editing inpainting and inpainting with refinement used to expand the set of odd conditions in a driving simulator via real time image to image translation.
unlike existing genai techniques that require explicit training diffusion models only require an input image and conditioning inputs that represent the transformation to be applied e.g.
a textual description .
as the diffusion models output can be affected by artifacts distortions or inconsistencies that undermine the effectiveness of ads testing our methodology includes an automated validation technique based on image semantics and segmentation maps.
this check is performed to assess the correctness and reliability of the generated images e.g.
ensuring that the original road shape is preserved after the augmentation .
xxx .
ieeearxiv .13661v3 feb 2025although diffusion models can generate diverse images for testing their direct use in a simulator faces two main challenges.
first diffusion models exhibit high inference times in the order of seconds per image which makes them impractical for real time applications.
second simulation platforms typically generate multiple image frames per second but diffusion models lack the rendering consistency required for a coherent simulation resulting in consecutive frames that may be drastically different.
to address these limitations our approach leverages knowledge distillation by integrating diffusion models with a cycle consistent network to ensure domain generation consistency and high throughput.
in our experiments our approach generated images that exhibited a validity rate between and the best approach being inpainting.
when used as a rendering engine within the udacity simulator to produce 108simulations using various odd conditions our approach was able to reveal a total of failures across four lane keeping ads times more than using the odd conditions available in the simulator at the cost of an increase in simulation time of .
furthermore we evaluated the generalizability of our approach to complex urban scenarios using the carla simulator and interfuser successfully exposing red light infractions and vehicle collisions previously undetected.
our paper makes the following contributions.
approach.
a system level testing technique for ads that combines different genai e.g.
diffusion models and cycleconsistent generative networks as a rendering engine and physics based simulators for effective failure detection.
this novel combination of techniques achieves high odd diversity realism semantic preservation temporal consistency and high throughput while the simulator s underlying physics ensures accurate representation.
our approach is integrated in the udacity simulator for self driving cars and is publicly available .
to the best of our knowledge this is the first solution that uses genai techniques within a driving simulator to improve the odd coverage of dnn based ads.
evaluation.
an empirical study on the validity and realism of neural generated driving images and their use for system level ads testing.
dataset.
a dataset of more than 1million pairs of images and52ood conditions based on the udacity simulator and a dataset of 1million images and 9ood conditions based on the carla simulator.
these datasets can be used to evaluate the generalizability of ads to novel environmental conditions as well as the performance of failure prediction systems.
ii.
b ackground a. system level testing of ads ads must adhere to specific regulations that establish essential safety requirements for public acceptance and largescale deployment.
in particular standards such as the iso pas safety of the intended function sotif standard or the un regulation no concerning the approval of vehicles with regards to automated lane keeping systems demand extensive coverage of the operationaldesign domain odd conditions.
the odd should describe the conditions under which the automated vehicle is intended to drive autonomously such as roadway types geographic area speed range environmental conditions weather as well as day night time and other domain constraints.
in this work we focus on the odd conditions that visually impact the environment and the dnns of the ads.
specifically we used the conditions described in the standard iso scenery elements section and environmental conditions section some examples being different geographic areas e.g.
european cities or coastal areas and weather conditions e.g.
cloudy or rainy weather as well as day night .
the safe deployment of ads requires a thorough exploration of odds through simulated and in field testing.
due to significant time space and cost constraints associated with in field testing i.e.
real world testing with physical vehicles simulation based testing has become the standard option for system level testing of ads .
driving simulators can generate data and conditions that closely mimic those encountered in real world scenarios .
to test the limits of the ads a simulator produces a vast amount of highly consistent data through synthetic input generation using a 3d image rendering engine.
however a comprehensive test dataset must not only be statistically significant in volume but also adequately represent the diverse odd conditions.
this is a major limitation of current driving simulators which often have restricted odd coverage which is essential for comprehensive testing and fault exposure .
b. vision generative ai generative ai has significantly advanced various vision tasks by enabling the creation of realistic and diverse data .
in this paper we consider techniques that allow one to control the content of the augmentation .
we experiment with diffusion models a class of vision generative ai techniques that achieved state of the art performance in image generation tasks .
these models operate by reversing a gradual noising process starting from a simple distribution and iteratively refining it to generate high quality images.
for example given an initial noisy image the model denoises it step by step to produce a coherent image.
different randomly sampled noise seeds lead to variations in the generated images allowing these models to create diverse and unique images.
conditional diffusion models allow for further control over image generation using different types of input conditioning.
for example stable diffusion and dall e use single conditioning through a textual description to guide the denoising process as a form of requirement specification e.g.
generate a sunny driving image scenario.
.
this guidance concept aims to align the generated images closely with the conditions or descriptions provided.
other techniques usemultiple conditioning .
for example instructpix2pix takes as input an image and a textual editing instruction that describes the modification to be applied to the image whereas controlnet facilitates the addition of arbitrary conditioning inputs to a pre trained stable diffusion model.
2iii.
s olution our methodology seamlessly integrates into the standard system level testing loop without requiring major modifications to the simulator or the ads.
the key idea lies in manipulating the environment perceived by the ads through diffusion based augmentation while the actual driving commands are executed on the original simulator.
our methodology consists of three main phases namely domain augmentation semantic validation and knowledge distillation.
the first phase domain augmentation involves intercepting the images captured by the car cameras.
these images are then processed using diffusion models to generate a new image depicting the same road structure but a different odd condition such as background and weather conditions .
the main reason is that a well trained lane keeping ads should focus on the foreground features that characterize the road scenario instead of the features in the background .
the second phase semantic validation involves a validation step of the generated image to assess the validity and semantic and label preservation between the original and the augmented image.
particularly our approach aims to generate road images devoid of visual artifacts distortions inconsistencies or hallucinations and that are semantically equivalent to the original one in terms of geometrical features e.g.
direction lanes length width .
the semantic validation process is fundamental to maintaining the validity of the augmentation process as significant changes to the road structure introduced during augmentation could cause the ads to make decisions based on misleading information.
our approach addresses visual and semantic consistency whereas physicsrelated factors such as changes in friction and traction e.g.
due to snowy conditions are not considered.
the third and last phase knowledge distillation enables the rendering of new odds online during the execution of a simulation.
the diffusion models used in the first phase for domain augmentation are not suitable for online usage because they are too slow at inference time.
hence we train a faster cycle consistent generative neural network using the output images produced by the first phase as the training set.
at each simulation step this network transforms the input image to reproduce the domain augmentation of the diffusion models.
if the augmented image passes the semantic validation it is forwarded to the ads for processing.
the ads processes the image and predicts the appropriate driving commands based on the augmented odd.
the predicted driving commands are sent to the simulator which actuates them completing the feedback loop.
the simulator then modifies the virtual environment and provides the vehicle with updated sensor data thus preparing for the next iteration of the testing loop.
in the next sections we describe each step of each phase.
a. domain augmentation we analyze three alternative controllable augmentation strategies based on categories of diffusion models to introduce environmental odd changes in driving images instructionediting inpainting and inpainting with refinement .
instruction editing diffusion model input image change season to autumn denoising steps high text guidance scale examplesschema high image guidance scale fig.
instruction edited domain augmentation strategy.
instruction editing.
this category takes two inputs the image to be modified and an editing instruction e.g.
add trees or change season to autumn and produces an output image with the editing instruction applied.
instances of instructionediting models are instructpix2pix and sdedit .
instruction editing models can be configured with two parameters image andtext guidance scale that represent how much the two inputs influence output generation.
the first parameter image dictates how much of the structure and spatial details of the input image should be preserved.
the text guidance scale determines the strength to use when applying the editing instruction.
figure reports a schema of the strategy top and how the two guidance scales can influence the augmentation process bottom .
specifically we can observe that an excessively high text guidance scale can compromise the road semantics while overly increasing the image guidance scale too much may result in the insufficient application of the desired edit.
inpainting.
this category employs a text to image diffusion model that performs inpainting.
instances of inpainting models are stable diffusion dall e and pixart .
we customized the inpainting pipeline to preserve the parts of the images related to driving actions e.g.
the road while the rest of the image can be repainted by the diffusion model.
we identify the road automatically using a semantic mask that describes which pixels in the image belong to the semantic class road.
as the semantic mask is provided by the simulator it ensures perfect semantic segmentation.
the inpainting text to image model takes three inputs an input image a mask and a textual prompt that describes the desired image.
the model generates an image by preserving only the content selected by the mask while guiding the entire image to align as closely as possible with the textual prompt.
in our setting this process ensures that only the parts outside the road are replaced with new content thus maintaining the shape of the road since it is not modified by the inpainting strategy.
note that the road in the inpainted image is the same as the one in the input image.
for instance in figure top left the surrounding environment has been transformed while the road structure markings and position are unchanged.
inpainting with refinement.
this model category adds a step to the inpainting strategy by performing a refinement of the entire image.
the goal is to improve the visual coherence between the preserved and the generated parts of the inpainted 3input image denoising weak refinementschema denoising strong refinementlow edge guidancehigh edge guidance denoising steps a road in dust storm inpainted image canny edge detectioninpainting canny edgerefining semantic mask conditional control diffusion model denoising steps text to image diffusion model a road in dust storm fig.
inpainting and inpainting with refinement domain augmentation strategies.
image.
while the traditional denoising process of diffusion models starts from fully noised images the refinement step starts from a partially noised inpainted image.
this makes the refined image more similar to the initial inpainted one.
the noise level removed during denoising determines the difference between the inpainted and final image higher noise removal leads to greater differences.
the refinement step employs a different type of diffusion model compared to the inpainting step.
this is because image generation with a text to image model used during inpainting can be guided using only text.
this is sufficient for the inpainting step as the road semantics are ensured by the semantic mask that preserves the road.
however during the refinement step the entire image is modified and thus the shape of the road might change.
for this reason in our study we evaluated a category of diffusion model with conditional controls that allow to better guide the augmentation process such as controlnet with canny edge conditioning or t2i adapter .
this model takes three inputs the edge map derived from the original input image captured in the simulator a partially noisy inpainted image and a textual prompt that describes the desired image.
the model takes the noisy image and refines it using both the edge map and the textual prompt.
the edge map ensures that the refined image retains edge structures similar to the original while the textual prompt directs the overall content.
figure provides an overview of the inpainting with refinement process top and reports some augmentations obtained with different levels of denoising and guidance scales for the same input image bottom .
a stronger refinement process more denoising results in significant differences from the initial image but can also lead to inferior preservation of road semantics.
similarly higher edge guidance scale values preserve road semantics more while lower values encourage greater freedom diversity and realism in the generated image.
b. semantic validation diffusion model categories aim to produce visually appealing outputs but they may still generate invalid outputs failing to preserve the semantics of the road during enhancement for example by widening the road or introducing new intersecpredicted u net road segmentation t .
?
oc tss .732invalida b c oc tss .
t .
?
validsemantic similarity semantic similarityimage fig.
semantic validation using oc tss .
tions.
to mitigate this our methodology includes a semantic validation step to minimize incorrect augmentations.
the main objective of this phase is to check that the generated augmentation is characterized by a road that is semantically equivalent to the road in the image captured in the simulator.
to this end we filter out augmentations that do not preserve the original road semantics using the oc tss metric one class targeted semantic segmentation .
oc tss is a similarity metric that measures semantic details and structural differences between two images by focusing on a single task relevant class within the semantic masks predicted by a fine tuned segmentation model.
this metric ranges from to where indicates perfect semantic equivalence between the original and augmented images and suggests complete dissimilarity.
oc tss has been applied in previous work to assess the accuracy of generative ai models in translating images across domains for a lanekeeping ads.
in line with this study our analysis focuses on the semantic class road as road lanes represent the relevant image characteristics for a lane keeping ads.
unlike the original work we employ a u net architecture for semantic segmentation rather than the segformer architecture for its computational efficiency.
figure illustrates our semantic validation process.
the top row of figure a shows a semantically valid augmentation of the input image figure b while the bottom row reports an incorrect augmentation figure c .
specifically the augmentation in the bottom figure is considered semantically invalid because the road s orientation changes to the left instead of continuing straight as in the original image.
the middle 4column presents the semantic segmentation masks computed by the u net model for each image.
in these masks the road is represented in white and the background is depicted in black.
the final step of our process represented in the right column involves measuring the distance between the semantic masks of the augmented and original images the differences highlighted in pink.
augmentations with a similarity score below a threshold are discarded.
higher threshold values ensure validity but they may discard valid images and increase the time required to generate a semantically valid augmentation.
however lower thresholds could compromise testing by allowing too many invalid images.
in this study we determine the threshold based on empirical observations to balance filtering out invalid augmentations and retaining enough variability for thorough ads testing section iv a .
c. knowledge distillation the final phase involves creating a fast and consistent neural rendering engine in the simulator using outputs from the previous phases.
the diffusion based models while effective for diversity are computationally expensive and can produce potentially inconsistent augmentations which may be problematic for the temporal coherence of a simulation.
therefore we adopt a technique known as knowledge distillation where a smaller model student is trained to replicate the behavior of a larger more complex model teacher i.e.
the diffusion model .
in particular for the student model we use a cycle consistent generative network that can map images from the original domain virtual images from the simulator to another domain augmented images by the diffusion models .
instances of these architectures are cyclegan or unit .
this technique is widely used for image to image translation tasks including the autonomous driving domain due to its low computational overhead which makes it suitable for runtime usage in simulators .
moreover training a separate student model for each domain allows effective learning of the key aspects of the teacher model s output enhancing rendering consistency.
this strategy involves first training the cycle consistent network to learn the mapping between the original and augmented image domains produced by the diffusion models.
this process while computationally intensive is performed only once for each domain.
then during the online systemlevel testing of the ads the trained cycle consistent network model generator translates images at runtime i.e.
during the execution of the simulation.
an important advantage of this approach is that it does not require collecting new data to train the model as it leverages existing pre trained diffusion models.
this means that the process can be easily automated and does not require human intervention e.g.
collecting and labeling data making it an efficient solution for rapidly generating consistent and highquality domain augmentations online during ads simulation.iv.
e valuation we evaluated the proposed approach through the following research questions rq semantic validity and realism do diffusion models generate images that are semantically valid and realistic?
how is the semantic validator at detecting invalid augmentations?
rq effectiveness how effective are augmented images in exposing faulty system level misbehaviors of ads?
rq efficiency what is the overhead introduced by diffusion model techniques in simulation based testing?
does the knowledge distilled model speed up computation?
rq generalizability does the approach generalize to complex urban scenarios and multi modal ads?
the first research question aims to assess the semantic validity of the augmentations generated by our methodology.
specifically the focus is on whether diffusion models can transform images while preserving road semantics and whether the proposed semantic validator can effectively identify roads with different semantics.
the second research question aims to check the utility of the proposed approach in identifying potential faults in ads that may not be detected using only the simulator s capabilities.
the third research question evaluates the computational cost of our approach which is crucial to understanding scalability in real world ads testing scenarios.
the fourth research question studies the generalizability of our approach to test multi modal ads in complex urban environments with pedestrians vehicles and traffic lights.
a. experimental setup in this section we describe the experimental setup used for rq rq rq while the setup changes required for rq 4are described in section iv f before presenting the results.
simulation platform.
we carried out our evaluation using the udacity simulator with behavioral cloning ads models a widely adopted platform in the literature .
the simulator supports various closed loop tracks divided into 40sectors to test behavioral cloning ads including a predefined set of odds such as different times of day night and three weather conditions rainy snowy and foggy .
we extended the simulator to generate semantic segmentation masks for vehicle camera images of size height width to identify the regions for inpainting.
in addition we developed a synchronous simulation mechanism that pauses the simulation during image augmentation and resumes once the new image is generated.
this ensures that the augmentation process is transparent to the system level testing process.
lane keeping ads.
we evaluated four different singlecamera lane keeping dnn based ads as systems under test to assess the performance of the proposed methodology.
in particular we selected nvidia da ve chauffeur and epoch since they have been often used in multiple testing works .
finally we also included a recent architecture based on vision transformer vit based that achieved state of the art performance in lane keeping tasks.
5table i odd domains.
category domains weathers cloudy dust storm foggy lightnings overcast smoke sunny seasons autumn spring summer winter daytimes afternoon dawn dusk evening morning night sunset locations coast desert forest lake mountain plains rivers rural seaside cities beijing berlin chicago el cairo london new york paris rome san francisco sidney tokyo toronto countries australia brazil canada china england france germany italy japan mexico morocco usa operational design domains selection.
we selected odds that encompass diverse conditions from existing standards see section ii a .
we filtered out domains that do not preserve the driving action when applied for domain augmentation.
for example when converting a sunny road image to a snowy condition it might require the prediction of a different steering angle that accounts for the different friction despite the road being the same.
overall we identified domain categories and distinct label preserving domains table i .
to provide further insight into the difficulty of these domains we measured the challenge they pose by computing the distance between augmented domains and the training data.
this was done using the reconstruction error of a variational autoencoder v ae to categorize the domains into three clusters in distribution domains closer to the training distribution e.g.
familiar domain or road conditions in between domains moderately different from the training distribution and out of distribution domains significantly different from the training distribution .
in distribution domains are useful for testing the robustness of the ads by simulating scenarios similar to those that the model has previously encountered.
on the other hand out of distribution domains challenge the ads s ability to generalize to new unfamiliar conditions that are not present or rarely available in training data.
to determine the classification of these domains we first trained the v ae to reconstruct the training data.
the lower the reconstruction error the closer the domain is to the training distribution.
we generated 000augmented images for each domain using the three domain augmentation strategies and measured the reconstruction error for each.
the domains were then sorted by reconstruction error and categorized as in distribution in between or out of distribution.
finally we selected three domains that were categorized in the same group for all three augmentation techniques.
the final selections were as follows for in distribution domains we included sunny summer and afternoon conditions for in between domains we chose autumn desert and winter and for out of distribution domains we selected dust storm forest and night scenarios.
diffusion models calibration.
we used three state of theart pre trained diffusion models for our categories instructpix2pix for instruction editing stable diffusion for inpainting and controlnet with canny edge conditioning for inpainting with refinement.
we fine tuned thehyperparameters of each considered model before answering the research questions.
we prioritized image fidelity adherence to instructions and preservation of essential road features which will be evaluated in our first research question.
in particular we configured all diffusion models to use the unipc multistep scheduler with inference denoising steps as noise sampling strategy .
we chose unipc because it focuses on generating good images with a few denoising steps.
in our exploratory experiments a higher number of steps did not lead to significantly better images but only introduced additional computational overhead.
for instructpix2pix we set the image guidance scale to and the text guidance scale to .
these settings were found to be a good balance between image and instruction inputs preserving key features from both sources.
in the stable diffusion inpainting pipeline we used a text guidance scale of10 which maintained a high level of control over the generated content without compromising the quality of the inpainting process.
controlnet refining was configured with a text guidance scale of 10and a noise level of .
this configuration preserved the structural integrity of the road while still allowing for meaningful and diverse augmentations.
higher noise levels were found to risk excessive alteration of images and the potential loss of essential road semantics.
semantic validator configuration.
to determine an appropriate threshold for the oc tss metric we collected images from the simulator with different semantics which were manually assigned to three categories images of straight roads right turns and left turns.
we computed the oc tss for all images and evaluated the similarity between images within the same category and across different categories.
we aimed for a threshold that considers images within the same category as semantically similar and those in different categories as distinct giving higher priority to filtering out invalid images that belong to another category rather than including as many valid images from the same category as possible.
consequently after manual inspection of a sample of included excluded images we chose a conservative threshold of0.
which minimizes the inclusion of semantically incorrect augmentations while at the same time avoiding the exclusion of too many valid images.
thus augmentations with an oc tss .9are considered semantically consistent with the original layout of the road by our automated semantic validator while those below .
are considered invalid and discarded.
knowledge distillation configuration.
we used cyclegan as the student model to distill knowledge from the pre trained diffusion models.
the cyclegan architecture followed the recommendations in to reduce droplet artifacts.
we trained one cyclegan for each of the nine selected odds and the three augmentation strategies resulting in models.
each model was trained for epochs using pairs of images from the simulator and augmented with checkpoints saved at the end of every epoch.
the best checkpoint was selected according to the fr echet inception distance a metric that measures the distance between 6two sets of images the ones generated by cyclegan and the ones generated by the diffusion models by comparing their feature distributions.
hardware and software.
all experiments were executed on a server with an amd 5950x cpu gb ram and two nvidia gpus gb vram each .
the software environment includes python .
cuda .
for gpu acceleration pytorch .
.
for ads implementations and huggingface diffusers .
.
for the diffusion models.
our evaluation required more than gpu hours and involved .
million image pairs generated over 52odd domains using 3augmentation techniques.
this process included filtering the domains to keep the experiment manageable within a reasonable time frame as training 36cyclegan models for 10epochs each took more than 150gpu hours.
b. metrics semantic validator effectiveness.
we consider valid augmentations as the positive class and invalid augmentations as the negative class.
correspondingly a true resp.
false positive tp resp.
fp is an image regarded as a valid augmentation by our semantic validator which is valid resp.
invalid according to the ground truth.
similarly a true resp.
false negative tn resp.
fn is an image regarded as an invalid augmentation by our semantic validator which is invalid resp.
valid according to the ground truth.
to assess the effectiveness of our semantic validation methodology we utilize the confusion matrix either with absolute or percentage values.
testing effectiveness.
we utilize two categories of metrics to evaluate ads performance at the system level one for measuring misbehavior and another for assessing driving quality.
the first category directly quantifies errors including incidents in which the vehicle deviates from the lane boundaries out ofbounds oob or collides with obstacles c .
we use failure track coverage ftc to capture the spatial distribution of errors which identifies the percentage of track sectors where misbehaviors occur.
this metric indicates whether errors are concentrated in specific challenging areas or spread over the entire track.
in this way we can determine whether errors arise primarily from the complexity of specific track sections or are induced more broadly by the new domain.
we assess two key metrics for driving quality relative to the nominal behavior of the ads.
the first is the relative crosstrack error rcte which measures the ratio of the average distance from the lane center in the test domain compared to the nominal domain.
an rcte value greater than indicates degraded performance the vehicle is closer to the edge of the road while a value less than indicates improved position accuracy the vehicle is closer to the center of the road .
the second metric is relative steering jerk rsj which calculates the difference in the rate of change of steering angle between the test and nominal domains.
higher rsj values suggest more abrupt steering adjustments while lower values indicate smoother driving.
although these metrics dotable ii rq semantic validity confusion matrix.
valid augmentation invalid augmentation predicted valid augmentation48 predicted invalid augmentation16 not directly indicate errors they provide valuable insight into potential performance degradation caused by specific domains.
computational overhead.
we evaluate the computational overhead of our domain augmentation strategies by measuring the average time required to generate the augmented image and the average time required to complete our experiments including both the augmentation process and the subsequent testing of the ads.
we compare these timings with a baseline where no augmentation is applied allowing us to quantify the additional computational load introduced by each strategy.
c. rq semantic validity and realism we conducted two surveys with human assessors to evaluate the semantic validity of the images generated by the diffusion models as well as their degree of realism.
we recruited participants from amazon mechanical turk mturk and personal contacts using convenience sampling .
each mturk participant answered questions while the others answered 100questions.
in the two studies we collected responses of which only were retained due to failure to answer the control questions of our surveys.
ultimately we retained responses from 35participants of which 10from mturk and 25from personal contacts.
semantic validity participants were shown two randomly ordered images and asked whether the images represented the same semantics of the road focusing on the shape of the road and the direction of turn.
for this study we randomly selected 36pairs of images for each of the three domain augmentation strategies instruction editing inpainting and inpainting with refinement .
this resulted in a total of pairs with each strategy contributing 18semantically valid transformations and 18invalid transformations according to our semantic validator.
in addition we included two control questions to filter out low quality responses one where the road was the same and one where the road was entirely different.
in total the first questionnaire contained 110questions of which two were used for quality checks.
we considered a road semantically equal or different when at least2 3of the participants agreed on the outcome.
overall the participants reached a consensus on .
of the pairs out of .
our study revealed a positive correlation between oc tss similarity scores and user opinions with a pearson correlation coefficient of .
p value .
.
table ii presents the results as a confusion matrix where columns represent the judgments of human participants and rows show the results of the semantic validator with valid augmentations considered the positive class.
our semantic validator failed to filter out invalid generations in only of the .
.
.
.
.
.
.
.
.
realism ratingsimulator instruction edited inpainting inpainting with refinement real worldfig.
rq realism.
cases fps while rejecting of the valid transformations fns .
the former error can affect the effectiveness of the proposed methodology as it could lead to testing ads on images with different semantics potentially compromising the validity of ads testing.
the latter while less severe may increase the time required to find valid augmentations.
genai realism in the second study we evaluated the realism of the augmented images.
participants were presented with individual images and asked to rate their realism on a 5point scale ranging from not realistic to very realistic .
we selected 18semantically valid transformations for each of the three domain augmentation strategies.
we also included images from the simulator and 18real world driving images for a better comparison.
in total the second questionnaire contained 90questions.
we computed the average realism score for each category of images augmented simulator and real world based on the participants ratings.
figure shows the results.
the images generated by inpainting and inpainting with refinement strategies are perceived as more realistic compared to those generated by the instruction editing strategy.
the mann whitney u test with an alpha of .05indicates statistically significant differences in realism scores between strategies with a medium effect size .5and0.
respectively .
furthermore inpainting with refinement was found to produce more realistic images than inpainting p value .
with a small effect size .
.
rq the output of our automated semantic validator matched human judgment with only of the augmented images incorrectly regarded as valid by the validator and valid augmentations incorrectly discarded by the validator .
inpainting with refinement is the augmentation approach that produces the most realistic images.
d. rq effectiveness this experiment evaluates the effectiveness of the domain augmentation approaches in discovering errors in lane keeping ads.
the experiment aims to determine how many errors each approach could find and the nature of these errors.
for each strategy we ran ads simulation steps for each augmented domain collecting the failures and driving quality metrics section iv b .
as a baseline we used the predefined set of domains available within the simulator section iv a .
table iii reports the results with the augmented domains and table iv shows the domains available in the simulator.
the findings indicate that the proposed methodology can identifymisbehaviors in all four ads.
as expected domains similar to training conditions in distribution showed fewer errors and lower failure track coverage than in between or out ofdistribution domains reflecting the increased difficulty due to domain shifts.
however even for in distribution domains our approach revealed failures from a total of 6oob incidents da ve to 5collisions and 22oob incidents vit based .
the maximum failure track coverage with in distribution domains was .
failure track coverage increased as the domains deviated further from the training set.
for example when epoch was tested with in between domains new errors were seen in up to of the sectors rising to .
with out of distribution domains.
simulated domains revealed errors in up to .
of the track while augmented domains reached .
.
indistribution domains generated by the augmentation strategies did not show more misbehaviors than those in the simulator.
in contrast in between and out of distribution domains triggered more errors especially with the instruction editing strategy.
although these scenarios occur less often in training the generated roads remain semantically validated and sufficiently representative for valid testing.
furthermore we found that the instruction editing strategy was the most effective across all three domain sets indistribution in between and out of distribution .
this effectiveness can likely be attributed to two main factors.
first the domains generated using instruction editing were perceived as less realistic in our human study section iv c .
despite being semantically validated by our automated validator these less realistic images may still mislead the ads into making incorrect decisions.
second the average distance from these domains to the training domains was higher compared to that generated by the other two domain augmentation strategies.
specifically the average reconstruction errors for the instruction editing strategy ranged from .
in distribution to0.
out of distribution while the errors for the inpainting strategy ranged from .067to0.
and for the inpainting with refinement strategy they ranged from .070to0.
.
we observed different behaviors between ads models based on convolutional dnns da ve chauffeur and epoch and the one based on vision transformer.
for the first adss our augmented domains generally led to lower rsj indicating smoother steering responses.
in contrast the vit based ads showed an increase in rsj as domain distance increased suggesting more abrupt steering adjustments in response to unfamiliar scenarios.
these differences can likely be attributed to how these two types of neural network architectures process data.
convolutional dnns with their hierarchical structure and localized receptive fields tend to focus on local features which may allow for more stable and incremental responses.
in contrast vits which utilize global attention mechanisms can capture broader contextual information but may also be more sensitive to domain shifts leading to more pronounced reactions to unfamiliar data.
to assess whether the augmented data can enhance the robustness of ads we retrained the ads using a combination 8table iii rq effectiveness results for system level testing on augmented domains.
in distribution domains in between domains out of distribution domains c oob ftc rcte rsj c oob ftc rcte rsj c oob ftc rcte rsj da ve instruction editing .
.
.
.
.
.
.
.
.
inpainting .
.
.
.
.
.
.
.
.
inpainting with refinement .
.
.
.
.
.
.
.
.
chauffeur instruction editing .
.
.
.
.
.
.
.
.
inpainting .
.
.
.
.
.
.
.
.
inpainting with refinement .
.
.
.
.
.
.
.
.
epoch instruction editing .
.
.
.
.
.
.
.
.
inpainting .
.
.
.
.
.
.
.
.
inpainting with refinement .
.
.
.
.
.
.
.
.
vit based instruction editing .
.
.
.
.
.
.
.
.
inpainting .
.
.
.
.
.
.
.
.
inpainting with refinement .
.
.
.
.
.
.
.
.
table iv rq effectiveness results for system level testing on domains available in the udacity simulator.
simulator domains c oob ftc rcte rsj da ve .
.
.
chauffeur .
.
.
epoch .
.
.
vit based .
.
.
of the original training data and the augmented ones.
we finetuned the driving models for 50additional epochs using early stopping with a patience of epochs based on improvement in validation loss.
empirical results show that across the simulator domains retrained ads models showed on average a35.
misbehavior reduction up to .
in specific scenarios i.e.
foggy weather conditions .
rq the proposed augmentation technique has been able to expose failures of four existing ads models even in domains close to the training one.
it represents a valuable complement to the execution of tests in domains supported by the simulator as it was able to discover failures in sectors that the simulator deemed failure free.
e. rq efficiency in this experiment we assessed the overhead introduced by domain augmentation strategies particularly focusing on the impact of large diffusion models and the potential efficiency gains from a knowledge distilled model.
table v presents the results of testing da ve with similar results observed for other lane keeping ads systems.
a test run of da ve consisting of simulation steps without augmentation took approximately .7minutes about .0ms per simulation step .
da ve required only .2ms per prediction while the remaining time was consumed by infrastructure tasks such as communication between the simulator and the agent image processing persistent logging andtable v rq performance overhead.
augmentation time ms testing run time min baseline no augmentation .
.
instruction editing .
.
.
.
inpainting .
.
.
.
inpainting with refinement .
.
.
.
knowledge distillation .
.
.
.
simulation management.
other ads systems showed inference times ranging from .1to2.0ms.
the use of diffusion models significantly increased the test duration.
specifically the inpainting strategy extended the testing time to .2minutes more than three times longer than the baseline while the instruction editing and inpainting with refinement strategies increased it beyond 70minutes more than four times longer .
although the instruction editing strategy had a faster per augmentation time .7ms its overall testing duration was longer because our semantic validator detected a higher percentage of semantically invalid images and needed to be regenerated.
specifically the semantic validator filtered out about of images augmented by instruction editing less than generated by inpainting and generated with inpainting with refinement.
in contrast the knowledge distilled model based on a cyclegan architecture significantly reduced the augmentation overhead.
it generated images in just .3ms on average resulting in a total testing time of approximately 16min less than a increase over the baseline without augmentation.
rq knowledge distillation is an essential component of our approach to achieve high simulation efficiency.
the augmentation overhead without knowledge distillation is for inpainting with refinement the technique that produces more valid and more realistic images which becomes just with knowledge distillation.
9table vi rq system level testing results with carla.
carla metrics ds rc cp cv ori rli ssi interfuser baseline no augmentation .
.
augmented domains .
.
f .
rq generalizability to assess how well our methodology can generalize to complex and realistic scenarios we performed a preliminary evaluation using the carla simulator a widely used high fidelity platform for autonomous driving research .
we focused on scenarios from the carla leaderboard sensors track that require the ads to perform multiple tasks such as lane keeping overtaking obeying traffic signals stop signs and detecting and avoiding pedestrians.
we used interfuser as the system under test along with the pre trained model provided in the original paper.
interfuser is an end to end ads for urban driving with one of the highest scores in the carla leaderboard and was widely used in prior work .
technically interfuser processes multimodal sensory inputs three rgb camera views front left and right and lidar point cloud data and outputs driving commands such as throttle brake and steering.
to assess driving quality and failures we used several metrics from the carla leaderboard.
they measure both the ability of the ads to complete tasks and its adherence to traffic regulations driving score ds reflects the overall ads performance that combines achievements and penalties.
route completion rc measures the percentage of the route completed.
penalties include collisions with pedestrians cp or vehicles cv off road infractions ori red light infractions rli and stop sign infractions ssi .
to handle the multi camera setup we applied our methodology to three camera views and ensured consistency across them by using the same distilled model configured to augment each image.
although our methodology does not operate on lidar point clouds preserving the semantic consistency of the visual representations ensures that the lidar data by the simulator remain consistent with the augmented camera views.
urban driving tasks such as overtaking and collision avoidance require the consideration of four additional semantic classes pedestrians vehicles traffic signs and traffic lights.
these additional classes were addressed during both augmentation and validation.
during augmentation we configured a strategy based on inpainting to also preserve those parts of the image.
during validation we applied oc tss to each semantic class and considered an augmentation valid only if all semantic classes were preserved within the threshold .
.
we replicated the experiments on the effectiveness of the augmented domains for failure exposure rq and measured the associated overhead rq using the same augmented domains as in the experiments with udacity.
carla supports various closed loop urban maps for testing ads.
we considered town05 one of the default maps provided bytable vii rq overhead in carla simulator.
augmentation time ms testing run time min baseline no augmentation .
.
instruction editing .
.
.
.
inpainting .
.
.
.
inpainting with refinement .
.
.
.
knowledge distillation .
.
.
.
carla with its default environmental configuration e.g.
sunny weather .
within town05 we considered ten different scenarios details are provided in the replication package .
table vi reports the average effectiveness results in all scenarios and in all domains.
as those scenarios are already designed to challenge the ads we provide the results with and without augmentation.
the results show that our approach successfully exposed new misbehaviors of the ads even in complex urban scenarios.
regarding route completion rc interfuser with no augmentation did not consistently achieve .
completion rates varied with some runs at .
and others at .
.
manual inspection of the logs revealed that the ego vehicle gets stuck due to traffic jams caused by other vehicles that block intersections.
with the baseline simulator no augmentation no infractions and collisions were detected.
with our neural augmentations we discovered previously unknown red light infractions and previously unknown collisions with vehicles.
concerning the overhead see table vii our methodology provides significant benefits also in carla.
first we measured the time taken to augment the three camera views.
the results show that the domain augmentation approaches required from .4s inpainting to .4s inpainting with refinement while the knowledge distilled model took only .0ms up to faster .
then we measured the time required to execute a test scenario to evaluate the overall testing overhead introduced by the augmented domains.
to ensure that the duration of the test was not influenced by external factors such as vehicles blocking intersections pedestrian crossings or red lights we used a controlled scenario without such elements.
with the diffusion models the test runtime increased significantly with observed test runs of .
minutes instruction editing .5minutes inpainting and .5minutes inpainting with refinement for a single test run.
in contrast using the lighter knowledge distilled model the runtime increased by only .9minutes a .
increase over the baseline without augmentation.
we believe that the overhead is higher than single image ads due to the higher number of images to be augmented three and their larger size height width .
rq our approach generalizes to complex urban driving environments in carla and interfuser a multi modal ads.
we discovered red light infractions and vehicle collisions which were not detected in the original simulator.
the knowledge distilled model reduced the augmentation time to 82ms resulting in an overhead of only .
.
10g.
threats to validity internal validity.
we utilized widely used model architectures and simulators from the literature.
the selection of the semantic validity threshold poses another potential threat.
in this study we adopted a conservative threshold to minimize the inclusion of semantically invalid images.
we also assumed that domain augmentations preserve driving action labels.
although similar work has made this assumption we explicitly excluded domains that are unlikely to maintain the integrity of the label such as snow or rain as these conditions can alter the dynamics and driving style of the vehicle due to changes in friction or traction.
external validity.
we considered limited instances of diffusion models.
to address this threat we selected state of the art diffusion models of different types that consistently improved the simulator across odds.
reproducibility.
to support reproducibility all of our data including the code of the diffusion models and our enhanced simulator are available in our replication package .
v. r elated work a. test generation for autonomous driving existing work leverages the ability of driving simulators to create diverse driving scenes for scenario based testing of ads .
generated scenarios include a wide range of driving conditions such as sudden lane changes adverse weather or interactions with other vehicles and pedestrians .
majumdar et al.
propose paracosm a tool that allows users to programmatically define complex driving scenarios.
woodlief et al.
propose a framework that abstracts sensor inputs to coverage domains that account for the spatial semantics of a scene.
a new technique called instance space analysis was recently proposed to identify the significant features of test scenarios that affect the ability to reveal the unsafe behavior of ads .
all of these test generators operate within a confined range of predefined odd scenarios including specific weather conditions background locations and times of day to maximize the number of failures within these predefined scenarios.
our approach seeks to considerably broaden the range of odd conditions beyond those currently available.
our methodology is complementary and can be integrated with existing test generators to enhance their effectiveness without modifications.
b. offline testing with generative ai approaches based on genai focus on augmenting existing image datasets by introducing variations like adverse weather or other visual elements .
for example zhang et al.
propose deeproad a solution that utilizes unit to generate test images by altering the weather from sunny to foggy or snowy.
pan et al.
present a method that leverages cyclegan combined with techniques to synthesize different fog levels with controllable intensity and direction in driving images.
li et al.
propose tactics an ads testing framework that uses search based strategies to identify critical environmental conditions and employs munit toreproduce these conditions in existing driving images.
attaoui et al.
combine genai and search based testing to test the semantic segmentation module of an ads.
other approaches augment existing test images with diffusion models .
zhao et al.
exploit semantic segmentation maps and a conditional generative model controlnet to generate high quality synthetic images.
xu et al.
employed a finetuned stable diffusion to create controllable traffic signs.
although these approaches assess the behavior of the ads they target model level testing and measure the discrepancy between predicted and ground truth values .
in contrast we focus on system level testing .
our application of genai as a rendering engine within a physics based simulator constitutes a novel contribution to the state of the art in ads testing.
c. data driven simulation neural simulators consist of data driven approaches in which genai is used to produce a continuous stream of driving images.
unlike traditional simulators which rely on game based 3d rendering and physics models neural simulators employ a learnable world model to represent the environment of the ads and target novel view synthesis e.g.
bird eye s view .
for example drivegan utilizes gans to create driving scenarios with controllable weather conditions traffic objects and backgrounds.
drivedreamer and gaia employ diffusion models to generate real world driving scenarios.
unisim is a neural closed loop sensor simulator that transforms a single recorded log from an ads into a realistic multi sensor simulation.
while neural simulators offer an improvement in generating novel and realistic training data their lack of a physical representation limits their applicability for testing.
this deficiency can produce inaccurate failure simulations e.g.
collisions resulting in false positives.
thus neural simulators are not the best choice for testing ads systems at the system level.
to address this limitation our approach integrates neural rendering and genai techniques with a physics simulator.
this combination enables effective testing with precise failure determination while expanding the odd conditions for testing.
vi.
c onclusions and future work we have generated new odd scenarios for ads testing using diffusion models and instruction editing operations.
we addressed the validity of the augmented images by creating an automated semantic validator which was found to be extremely accurate in a human study with as few as invalid images incorrectly regarded as valid.
we have considered the realism of the augmented images by conducting a human study that indicated the inpainting with refinement strategy as the technique that generates the most realistic images.
we have reduced the simulation overhead by introducing a cyclegan model that takes advantage of knowledge distillation.
most importantly we have shown that our approach can expose ads failures even in domains close to the training domain and in track sectors that were deemed error free when considering only simulator generated test scenarios.
11references s. grigorescu b. trasnea t. cocias and g. macesanu a survey of deep learning techniques for autonomous driving journal of field robotics vol.
no.
pp.
.
iso road vehicles test scenarios for automated driving systems scenario categorization standard february .
x. hu s. li t. huang b. tang r. huai and l. chen how simulation helps autonomous driving a survey of sim2real digital twins and parallel intelligence ieee trans.
intell.
veh.
vol.
no.
.
unity3d.
.
epicgames unreal engine.
.
a. gambi m. mueller and g. fraser automatically testing self driving cars with search based procedural content generation in proceedings of issta .
acm pp.
.
v .
riccio and p. tonella model based exploration of the frontier of behaviours for deep learning system testing in proceedings of acm joint european software engineering conference and symposium on the foundations of software engineering .
a. stocco b. pulfer and p. tonella mind the gap!
a study on the transferability of virtual vs physical world testing of autonomous driving systems ieee transactions on software engineering .
m. zhang y .
zhang l. zhang c. liu and s. khurshid deeproad gan based metamorphic testing and input validation framework for autonomous driving systems in proceedings of the acm ieee international conference on automated software engineering .
acm pp.
.
y .
pan h. ao and y .
fan metamorphic testing for autonomous driving systems in fog based on quantitative measurement in proceedings of the ieee international conference on software quality reliability and security pp.
.
z. li m. pan t. zhang and x. li testing dnn based autonomous driving systems under critical environmental conditions in proceedings of the international conference on machine learning vol.
.
pmlr pp.
.
m. zhang y .
zhang l. zhang c. liu and s. khurshid deeproad gan based metamorphic testing and input validation framework for autonomous driving systems in proceedings of the 33rd acm ieee international conference on automated software engineering .
acm pp.
.
.
available http m. biagiola a. stocco v .
riccio and p. tonella two is better than one digital siblings to improve autonomous driving testing .
a. stocco b. pulfer and p. tonella model vs system level testing of autonomous driving systems a replication and extension study empir.
softw.
eng.
vol.
no.
p. .
s. w. kim j. philion a. torralba and s. fidler drivegan towards a controllable high quality neural simulation in proceedings of the ieee conference on computer vision and pattern recognition .
computer vision foundation ieee pp.
.
t. brooks a. holynski and a. a. efros instructpix2pix learning to follow image editing instructions in proceedings of the ieee cvf conference on computer vision and pattern recognition .
ieee pp.
.
c. meng y .
he y .
song j. song j. wu j. zhu and s. ermon sdedit guided image synthesis and editing with stochastic differential equations in proceedings of the international conference on learning representations .
r. rombach a. blattmann d. lorenz p. esser and b. ommer highresolution image synthesis with latent diffusion models in proceedings of the ieee cvf conference on computer vision and pattern recognition.
ieee pp.
.
s. c. lambertenghi and a. stocco assessing quality metrics for neural reality gap input mitigation in autonomous driving testing in proceedings of 17th ieee international conference on software testing verification and validation .
ieee p. pages.
g. e. hinton o. vinyals and j. dean distilling the knowledge in a neural network corr .
j. zhu t. park p. isola and a. a. efros unpaired image to image translation using cycle consistent adversarial networks in proceedings of the ieee international conference on computer vision pp.
.
h. shao l. wang r. chen h. li and y .
liu safety enhanced autonomous driving using interpretable sensor fusion transformer inproceedings of the conference on robot learning vol.
.
pmlr pp.
.
replication package.
replication package icse .
t. r. i. .
international organization for standardization road vehicles safety of the intended functionality .
e. union un regulation no uniform provisions concerning the approval of vehicles with regards to automated lane keeping systems .
f. u. haq d. shin s. nejati and l. c. briand comparing offline and online testing of deep neural networks an autonomous car case study inproceedings ieee international conference on software testing validation and verification .
ieee pp.
.
j. li c. zhang w. zhu and y .
ren a comprehensive survey of image generation models based on deep learning annals of data science .
j. ho a. jain and p. abbeel denoising diffusion probabilistic models inadvances in neural information processing systems .
f. a. croitoru v .
hondru r. t. ionescu and m. shah diffusion models in vision a survey ieee transactions on pattern analysis and machine intelligence vol.
no.
pp.
.
a. ramesh m. pavlov g. goh s. gray c. v oss a. radford m. chen and i. sutskever zero shot text to image generation in proceedings of the international conference on machine learning vol.
.
pmlr pp.
.
l. zhang a. rao and m. agrawala adding conditional control to text to image diffusion models in proceedings of the ieee cvf international conference on computer vision .
ieee pp.
.
m. bojarski d. d. testa d. dworakowski b. firner b. flepp p. goyal l. d. jackel m. monfort u. muller j. zhang x. zhang j. zhao and k. zieba end to end learning for self driving cars.
corr vol.
abs .
.
j. chen j. yu c. ge l. yao e. xie z. wang j. kwok p. luo h. lu and z. li pixart alpha fast training of diffusion transformer for photorealistic text to image synthesis in proceedings of the international conference on learning representations .
j. f. canny a computational approach to edge detection ieee trans.
pattern anal.
mach.
intell.
vol.
no.
pp.
.
c. mou x. wang l. xie y .
wu j. zhang z. qi and y .
shan t2i adapter learning adapters to dig out more controllable ability for text to image diffusion models in proceedings of the conference on artificial intelligence aaai pp.
.
o. ronneberger p. fischer and t. brox u net convolutional networks for biomedical image segmentation in proceedings of the international conference on medical image computing and computer assisted intervention vol.
.
springer pp.
.
e. xie w. wang z. yu a. anandkumar j. m. alvarez and p. luo segformer simple and efficient design for semantic segmentation with transformers corr vol.
abs .
.
m. liu t. m. breuel and j. kautz unsupervised image to image translation networks in advances in neural information processing systems pp.
.
udacity a self driving car simulator built with unity com udacity self driving car sim online accessed october .
a. stocco m. weiss m. calzana and p. tonella misbehaviour prediction for autonomous driving systems in proceedings of the international conference on software engineering .
acm pp.
.
udacity udacity self driving car s challenge udacity self driving car online accessed august .
a. stocco m. weiss m. calzana and p. tonella misbehaviour prediction for autonomous driving systems in proceedings of 42nd international conference on software engineering .
acm p. pages.
m. b. et al.
end to end learning for self driving cars arxiv vol.
abs .
.
team chauffeur steering angle model chauffeur https github.com udacity self driving car tree master steering models community models chauffeur online accessed august .
team epoch steering angle model epoch self driving car tree master steering models community models cg23 online accessed august .
y .
tian k. pei s. jana and b. ray deeptest automated testing of deep neural network driven autonomous cars in proceedings of the international conference on software engineering .
acm .
m. hussain n. ali and j. e. hong deepguard a framework for safeguarding autonomous driving systems from inconsistent behaviour automated software engg.
vol.
no.
may .
k. pei y .
cao j. yang and s. jana deepxplore automated whitebox testing of deep learning systems in proceedings of the 26th symposium on operating systems principles .
acm pp.
.
.
available i. sonata y .
heryadi a. wibowo and w. budiharto end to end steering angle prediction for autonomous car using vision transformer commit communication and information technology journal vol.
pp.
.
d. p. kingma and m. welling auto encoding variational bayes in proceedings of the international conference on learning representationss .
w. zhao l. bai y .
rao j. zhou and j. lu unipc a unified predictorcorrector framework for fast sampling of diffusion models in advances in neural information processing systems .
t. karras s. laine m. aittala j. hellsten j. lehtinen and t. aila analyzing and improving the image quality of stylegan in proceedings of the conference on computer vision and pattern recognition pp.
.
m. heusel h. ramsauer t. unterthiner b. nessler and s. hochreiter gans trained by a two time scale update rule converge to a local nash equilibrium in nips .
s. j. stratton population research convenience sampling strategies prehospital and disaster medicine vol.
no.
p. .
a. dosovitskiy g. ros f. codevilla a. l opez and v .
koltun carla an open urban driving simulator corr vol.
abs .
.
.
available s. kim m. liu j. j. rhee y .
jeon y .
kwon and c. h. kim drivefuzz in proceedings of the acm sigsac conference on computer and communications security .
acm nov .
.
available m. cheng y .
zhou and x. xie behavexplor behavior diversity guided testing for autonomous driving systems in proceedings of the 32nd acm sigsoft international symposium on software testing and analysis .
association for computing machinery p. .
.
available g. li y .
li s. jha t. tsai m. sullivan s. k. s. hari z. kalbarczyk and r. iyer av fuzzer finding safety violations in autonomous driving systems in ieee 31st international symposium on software reliability engineering issre pp.
.
w. zhang m. elmahgiubi k. rezaee b. khamidehi h. mirkhani f. arasteh c. li m. a. kaleem e. r. corral soto d. sharma and t. cao analysis of a modular autonomous driving architecture the top submission to carla leaderboard .
challenge corr vol.
abs .
.
carla team carla leaderboard .
.
available x. jia p. wu l. chen j. xie c. he j. yan and h. li think twice before driving towards scalable decoders for end to end autonomous driving in proceedings of the ieee cvf conference on computer vision and pattern recognition pp.
.
x. jia y .
gao l. chen j. yan p. l. liu and h. li driveadapter breaking the coupling barrier of perception and planning in end to end autonomous driving in proceedings of the ieee cvf international conference on computer vision pp.
.
b. jaeger k. chitta and a. geiger hidden biases of end to end driving models arxiv preprint arxiv .
.
s. t. et al.
a survey on automated driving system testing landscapes and trends acm transactions on software engineering and methodologies vol.
no.
.
g. lou y .
deng x. zheng m. zhang and t. zhang testing of autonomous driving systems where are we and where should we go?
in proceedings of the acm joint european software engineering conference and symposium on the foundations of software engineering .
acm pp.
.
z. zhong y .
tang y .
zhou v .
de oliveira neves y .
liu and b. ray a survey on scenario based testing for automated driving systems in high fidelity simulation arxiv .
j. jullien c. martel l. vignollet and m. wentland openscenario a flexible integrated environment to develop educational activities based on pedagogical scenarios in proceedings of the ieee international conference on advanced learning technologies .
ieee computer society pp.
.
r. majumdar a. s. mathur m. pirron l. stegner and d. zufferey paracosm a test framework for autonomous driving simulations in proceedings of the international conference vol.
.
springer pp.
.
t. woodlief f. toledo s. elbaum and m. b. dwyer s3c spatial semantic scene coverage for autonomous vehicles in icse .
association for computing machinery .
.
available n. neelofar and a. aleti identifying and explaining safety critical scenarios for autonomous vehicles via key features acm trans.
softw.
eng.
methodol.
vol.
no.
apr .
.
available v .
ostankovich r. yagfarov m. rassabin and s. gafurov application of cyclegan based augmentation for autonomous driving at night in proceedings of the ieee international conference on nonlinearity information and robotics pp.
.
k. gao j. wang b. wang r. wang and j. jia ua v test data generation method based on cyclegan in proceedings of the international conference on dependable systems and their applications .
ieee pp.
.
x. huang m. liu s. j. belongie and j. kautz multimodal unsupervised image to image translation in proceedings of the european conference on computer vision vol.
.
springer .
m. o. attaoui f. pastore and l. briand search based dnn testing and retraining with gan enhanced simulations .
.
available l. yang z. zhang y .
song s. hong r. xu y .
zhao w. zhang b. cui and m. yang diffusion models a comprehensive survey of methods and applications acm comput.
surv.
vol.
no.
pp.
.
h. zhao y .
wang t. bashford rogers v .
donzella and k. debattista exploring generative ai for sim2real in driving data synthesis arxiv .
m. xu d. niyato j. chen h. zhang j. kang z. xiong s. mao and z. han generative ai empowered simulation for autonomous driving in vehicular mixed reality metaverses ieee j. sel.
top.
signal process.
vol.
no.
pp.
.
a. stocco b. pulfer and p. tonella model vs system level testing of autonomous driving systems a replication and extension study empirical softw.
engg.
vol.
no.
may .
.
available z. zhu x. wang w. zhao c. min n. deng m. dou y .
wang b. shi k. wang c. zhang y .
you z. zhang d. zhao l. xiao j. zhao j. lu and g. huang is sora a world simulator?
a comprehensive survey on general world models and beyond arxiv .
a. dosovitskiy g. ros f. codevilla a. m. l opez and v .
koltun carla an open urban driving simulator in proceedings of the annual conference on robot learning vol.
.
pmlr .
y .
guan h. liao z. li g. zhang and c. xu world models for autonomous driving an initial survey arxiv .
w. xia and j. xue a survey on deep generative 3d aware image synthesis acm comput.
surv.
vol.
no.
pp.
.
x. wang z. zhu g. huang x. chen and j. lu drivedreamer towards real world driven world models for autonomous driving arxiv .
a. hu l. russell h. yeo z. murez g. fedoseev a. kendall j. shotton and g. corrado gaia a generative world model for autonomous driving arxiv .
z. yang y .
chen j. wang s. manivasagam w. ma a. j. yang and r. urtasun unisim a neural closed loop sensor simulator in proceedings of the ieee cvf conference on computer vision and pattern recognition .
ieee pp.
.