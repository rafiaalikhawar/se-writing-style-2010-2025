seeaction towards reverse engineering how what where of hci actions from screencasts for ui automation 1stdehai zhao csiro s data61 sydney australia dehai.zhao data61.csiro.au2ndzhenchang xing csiro s data61 school of computing anu sydney australia zhenchang.xing data61.csiro.au3rdqinghua lu csiro s data61 sydney australia qinghua.lu data61.csiro.au 4thxiwei xu csiro s data61 sydney australia xiwei.xu data61.csiro.au5thliming zhu csiro s data61 school of cse unsw sydney australia liming.zhu data61.csiro.au abstract ui automation is an useful technique for ui testing bug reproduction and robotic process automation.
recording the user actions with an application assists rapid development of ui automation scripts but existing recording techniques are intrusive rely on os or gui framework accessibility support or assume specific app implementations.
reverse engineering user actions from screencasts is non intrusive but a key reverse engineering step is currently missing recognize humanunderstandable structured user actions from action screencasts.
to fill the gap we propose a deep learning based computer vision model which can recognize commands and widgets and generate location phrases from action screencasts through joint learning and multi task learning.
we label a large dataset with video action pairs which record the user interactions with word zoom firefox photoshop and windows settings.
through extensive experiments we confirm the effectiveness and generality of our model and demonstrate the usefulness of a screencast to action script tool built upon our model for bug reproduction.
index terms ui automation multi task learning action recognition ui testing i. i ntroduction ui automation reduces or assists the repetitive manual digital tasks of a human worker by a digital worker .
in the context of software engineering it is used to simulate an enduser s interaction with an application for ui testing or bug reproduction .
beyond software engineering ui automation is a core capability in robotic process automation which aims to automate business operations such as invoice processing inventory or human resources management.
ui automation can be achieved through textual script or visual script based techniques such as selenium appium uiautomator uipath and airtest .
ui automation scripts can be coded from scratch which involves repetitive and error prone manual work.
in practice ui automation tools often support recordand replay for recording human computer interaction hci fig.
.
examples of structured hci actions actions from which automation scripts can be produced.
the recorded scripts can be automatically replayed or be used as a boilerplate script to expand from.
hci actions can be recorded at three levels operating system os gui framework or application .
recording at application level is preferable because it outputs human understandable scripts rather than system or api calls.
figure shows some examples of application level hci actions which we refer to asstructured hci actions in this work.
structured refers to three integral parts of an action how command e.g.
click type drag what widgets if any that the action operates e.g.
button text icon and where location in the ui where the action occurs e.g.
popup search bar folder .
such structured hci actions are in a human understandable format that we see in application usage tutorials and steps to reproduce descriptions in bug reports .
furthermore they arearxiv .12873v1 mar 2025robust to changes of screen resolutions and sizes and changes in spatial arrangements of gui widgets compared with lowlevel screen coordinates.
some hci recording tools require app source code e.g.
espresso or customized os e.g.
v alera .
due to their intrusiveness such recoding tools have limited applicability .
most recording techniques are less intrusive but they still require either accessibility support from os or gui framework or dynamic instrumentation of software applications .
as such user actions involving nonaccessible friendly widgets for example custom widgets like non android webview in hybrid apps cannot be effectively recorded.
dynamic instrumentation hooks to specific apis e.g.
event dispatcher android textview for rich text editor and is sensitive to system and implementation changes.
it may also introduce the incompatibility with the instrumented apps .
it is desirable to develop non intrusive record and replay techniques independent of os gui frameworks and application implementations.
this non intrusiveness is important to support user experience testing and to support closed embedded systems that are increasingly used in entertainment education and industry .
screen recording non intrusively produces a video recording of hci actions.
structured user actions can be non intrusively reverse engineered from screen recordings using computer vision techniques.
this reverseengineering process involves three steps first video segmentation to segment the video into video fragments of individual actions second structured action recognition to generate howwhat where information of hci actions from video fragments third widget identification to identify gui widget details involved the actions.
techniques have been developed for video segmentation simple hci action prediction and widget detection .
however effective techniques for structured action recognition from action videos are missing without which we will not have a working reverseengineering pipeline.
recently large language models llms have gained popularity with some advancing to support vision features .
however supporting video action recognition remains a significant challenge.
while some work has developed llms for action recognition in videos these models primarily support videos in natural scenes.
it has been demonstrated that screencasts present different challenges compared to natural scenes .
moreover llms require substantial computational resources.
it is not necessary to employ large and heavy models for all problems.
a specialized and compact ai model is a more efficient and effective choice for addressing domainspecific problems.
this work fills the gap.
we propose a novel model to recognize user actions in screencast fragments and generate structured natural language descriptions of user actions such as those in figure .
given an action screencast we first compute the structural similarity of adjacent frames to obtain change regions and similarity maps.
our model takes three data streams as input the sequences of original video frames cropped change regions and similarity maps.
then the three data streams pass through three 3d covolutional neural networks cnns respectively which extract spatio temporal features.
the features from three channels are concatenated for three tasks command classification widget classification and location phrase generation.
we define classes of commands and classes of widgets to be recognized from screencast fragments based on previous literature and the actions and widgets supported by visual script based automation tools such as airtest and sikulix .
location phrase is free form that covers common expressions of locations on ui from application usage tutorials .
the model is trained end to end by joint learning and multi task learning.
to train and evaluate our model we build the first dataset of screencasts labeled with structured user action descriptions which contains a total of video action pairs.
the videos are application demonstration videos from youtube and bug reproduction screencasts from firefox.
we select five desktop applications with distinct functionalities word zoom firefox photoshop windows settings.
the applications run on windows linux and macos.
the videos are segmented and labeled with structured user action descriptions manually by the two authors.
our experiments on this dataset show that our model achieves high recognition accuracy .
f1score for command classification .
f1 score for widget classification and .
bleu .
meteor .
rouge and .
cider for location phrase generation .
furthermore the joint learning of the three data streams can significantly improve the model performance and the multi task learning can also enhance the model performance especially for location phrase generation.
to demonstrate the usefulness of our model we integrate it with a simple heuristic based video fragmentation method and the gui widget detection method into a screencastto actionscript tool.
we apply this tool to bug reproduction screencasts from firefox and obtain the action scripts for reproducing these bugs.
in a comparative study the firefox users reproduce more bugs following the action scripts generated by our tool compared with those following the original steps to reproduce s2r descriptions in the bug reports due to many s2r quality issues such as ambiguous steps vocabulary mismatch missing steps or even absent s2r text .
the results show the potentials of our model for supporting ui automation tasks such as bug reproduction.
we make the following contributions in this paper to the best of our knowledge this is the first work to recognize structured user actions from screencasts.
it is an essential step towards non intuitive ui automation.
to recognize structured user actions from screencasts we propose a novel computer vision model which is trained end to end by joint learning and multi task learning.
we label and contribute the first screencast dataset with video action pairs which record the user interactions with five popular desktop applications.
through extensive experiments we not only confirm the effectiveness and generality of our model but also demonstrate its usefulness for generating high quality human understandable action scripts for bug reproduction.
ii.
a pproach given a screencast fragment recording of a user action with an application our approach outputs a structured natural language description of this action.
the screencast fragment is first processed by computer vision techniques to obtain three data streams i.e.
the sequence of original frames cropped change regions and similarity maps.
next the three image sequences pass through three 3d cnns respectively to extract spatio temporal features see figure which are finally used to predict structured user actions including command category widget category and location phrase.
we formulate the spatio temporal feature extraction as joint learning and structured user action prediction as multi task learning.
a. structured hci actions we summarize command classes and widget classes based on our personal experiences with software applications previous work and the commands and widgets supported by popular visual script automation tools e.g.
airtest sikuli .
the command categories include click drag hover scroll down scroll up select type zoom in zoom out appear and disappear.
the first nine commands are user operations while appear and disappear are app responses i.e.
the outcomes of user operations .
we include app responses because they represent critical information in the screencasts for downstream applications for example assertions for ui testing .
the widget classes are button checkbox including radio button dropdown icon image text including text field window page tab popup and others.
the first six widget classes are atomic widgets for presenting information and collecting user inputs while window page tab and popup are container widgets for grouping visual content.
we include others to represent miscellaneous widgets that are not widely present in applications e.g.
geometry graph .
different from commands and widgets the location information requires a free form short phrase to express which will be assembled from a vocabulary.
the vocabulary can be built from application usage tutorials e.g.
wikihow or from the labeled screencast dataset see section iii a .
note that command widget and location have latent associations.
for example user actions can be or but not or .
b. model input a user interaction with application is recorded as a screencast.
the raw screencast will be processed into a fixedlength sequence of adjacently distinct screenshots.
from this sequence of screenshots frames a grayscale similarity map is computed between adjacent screenshots and change regionsare subsequently detected from the similarity map.
the sequence of the original screenshots together with the obtained sequences of similarity maps and change regions will be fed into the deep learning model.
input screencast processing we decode a raw screencast video into a sequence of frames at a sample rate of frames per second fps .
according to our empirical observation 5fps sample rate keeps sufficient frames to represent hci actions and screen changes without too many redundant frames.
we further scan the frame sequence from the beginning and remove the adjacently identical frames i.e.
no screen changes .
user actions vary in duration .
to seconds in our labeled data leading to different frame sequence lengths.
but the deep learning model requires a fixed length sequence of screenshots.
we normalize an input frame sequence to a fixed length sby down or up sampling.
given a frame sequence f f1 f2 .
.
.
f m we pick the first frame f1and the last frame fmto a sampled frame sequence f f1 fm as these two frames contain the most important information to represent how a user action starts and ends.
next if s m we randomly down sample s 2frames fi .
.
.
f j from the remaining frame sequence f f2 .
.
.
f m and insert them into the sampled frame sequence f f1 fi .
.
.
f j fm by the original frame order.
if s m we randomly duplicate the frames from the original frame sequence s mtimes and insert the obtained frames into the sampled frame sequence by the original frame order.
in this work we set s 8as this length is long enough to cover the duration of most user actions and contains the least duplicate frames.
model input representation many user actions e.g.
click button type text result in only small scale screen changes.
image features in such small scale screen changes would be too weak to recognize user actions if the whole frame is taken as the only input.
therefore we detect change regions which will be used as an additional input to the model.
first we compute the structural similarity which produces a similarity score for each pair of pixels at the same location in the two frames.
based on these similarity scores we produce a grayscale image i.e.
a similarity map in which brighter pixels indicate lower similarities between the corresponding pixels in the two frames.
on this similarity map we compute the regions of connected non black pixels and determine their bounding boxes.
the corresponding areas in original frames are cropped by these bounding boxes as change regions.
there can be more than one change region between the two frames and we keep the largest change region.
we prepare three input data streams the sequence of original frames each frame is a rgb image which provide the detailed context of user actions the sequence of cropped change regions each cropped change region is a rgb image which provide screen change details resulting from user actions and the sequence of similarity maps each map is a grayscale image which provide a simplified overview of screen changes.
all input images are resized to the same size e.g.
.fig.
.
overall model architecture of seeaction c. model architecture our model takes as input three image sequences original frames change regions and similarity maps obtained from a user action screencast and outputs a structured natural language description of the user action performed in the input screencast in the form of such as .
3d cnn based spatio temporal feature extraction user actions result in changes on screen and last for a short time period.
3d cnns has been successfully applied to video data for both natural scenes and computer rendered screencasts to encode video data into low dimensional abstract spatio temporal features for action recognition.
as shown in figure we use three different 3d cnn feature extractors to encode the spatio temporal features in the three input image sequences respectively.
the three 3dcnn encoders share the same model structure but do not share weights.
in our model we aggregate three 3d cnns as one spatio temporal feature extractor and optimize them jointly.
joint learning makes the whole model smaller and improves the efficiency of training process.
the three 3d cnn encoders extracts a low dimensional feature vector echreg eorig esimmap for cropped change regions original frames and similarity maps respectively.
the model concatenates the three vectors e echregleoriglesimmap as an overall representation of the user action occurred in the action screencast.
we set the length of three feature vectors as resulting in the length of fused feature vector eas .
the fused feature vector is fed into the subsequent multi task action prediction.
multi task user action prediction considering the latent associations among we formulate the model prediction as three sub tasks in a multi task learning setting.
as command and widget comes from a fixed set of labels we formulate command and widget prediction as two classification tasks i.e.
video tagging .
as the location is a phrase assembled from a vocabulary we formulate location prediction as a sequence to sequence generation task i.e.
video captioning .
the two classifiers and one generator produce three outputs i.e.
command class widget class and location phrase respectively.
we compute three loss functions for the three tasks.
the command classification and widget classification task use cross entropy loss which is denoted as lact pnact c 1yclog pc andlobj pnobj c 1yclog pc respectively where yis the ground truth label and pis the predicted class.
the location phrase generation task applies crossentropy loss to each word output by lstm and it is computed aslloc pmaxl i 1pnvoc c 1yi clog pi c where maxl is the maximum length of the generated text sequence yiis the ground truth label and piis the predicted word at i th position.
we formulate the three tasks as multi task learning by computing the sum of three losses and get the total loss i.e.
l lact lobj lloc.
the training objective is to minimize the total loss land make the model reach global optimization.
iii.
e xperiment design this section describes our experiment dataset for model training and testing and the metrics used for evaluation.
a. dataset in this study we manually label application usage screencasts in total .
hours for five desktop applications see table i .
this creates a dataset of screencast fragments labeled with corresponding structured user action descriptions which to the best of our knowledge is the first large scale dataset of its kind.table i dataset of use action screencasts application video dur.
h os resolution photoshop .
macos win10 settings .
windows firefox .32linux macosmultiple zoom .
windows word .
windows total .
data collection we consider five popular desktop applications that most people use daily for diverse tasks including mozilla firefox web browsing microsoft word document processing adobe photoshop graphics editing zoom online conference and windows settings system configuration .
we collect application usage screencasts from two sources.
first we collect application demonstration videos from youtube by searching keywords how to use applicationname and download the most viewed ones at high definition resolution .
we keep the videos in which users are actually using the applications but exclude those recorded in slide presentation.
second we crawl steps to reproduce s2r screencasts from the firefox s bug reports .
these steps to reproduce screencasts have diverse screen sizes and resolutions.
as shown in table i we finally collect a total of videos with .
hours duration.
the collected videos involve three main stream operating systems i.e.
windows linux and macos .
because of different usage characteristics of applications e.g.
the density of user actions video numbers and duration vary among the five applications.
zoom window may contain other application views for example screen shared by online conference participants which poses an interesting challenge to predicting user actions with zoom.
manual labeling given an action screencast the manual labeling has two main steps video segmentation and structured action description generation.
the screencast is decoded to a sequence of frames at 5fps sampling rate.
we discard frames with no screen changes by computing image similarity because they contain no action information which removes about of frames in the raw screencasts.
for efficient and consistent labeling we develop a web application by which the annotator can view and navigate a screencast by frames with the change regions highlighted.
the annotators use this tool to annotate the start and end frame of an action fragment and to select command widget class as defined in section ii a and enter location phrase for the action fragment.
location is a free form phrase and should be described specifically as in toolbar in popup menu etc.
rather than general window areas such as at top or on the right screen because specific location information would be more human understandable.
two authors participate in the label processing.
the two annotators use the five studied applications regularly in their work and are very familiar with the guis and user operations of these applications.
following the common practice for action video labeling two annotators label the wholetable ii overall statistics of labeled action dataset app actionduration s mean std words mean std vocsize ps .
.
.
.
winset .
.
.
.
firefox .
.
.
.
zoom .
.
.
.
word .
.
.
.
total .
.
.
.
dataset independently.
when there are disagreements two annotators discuss to decide the final labels.
the labeling process took about person weeks.
table ii summarizes the statistics of labeled action fragments.
the five applications have similar numbers of action fragments.
we obtain in total action fragments.
the action fragment duration is about second frames on average.
each user action is described by about words including command name widget name and location phrase.
the location vocabulary contains distinct words.
b. evaluation metrics we evaluate command and widget classification by precision recall f1 score and accuracy.
the correctness of a prediction is determined against the human label i.e.
ground truth for an input action fragment.
accuracy is an overall performance of commands or widgets which is computed by the number of action fragments predicted correctly over all action fragments in the test dataset.
as location phrase generation is a video captioning task we evaluate it by bleu rouge meteor and cider which are widely used in image captioning tasks .
bleu rouge and meteor roughly correspond to precision recall and f1 in the classification task.
cider uses tf idf to weight the words in the phrase.
in this work considering the short length of location phrases .
.
we compute gram bleu rouge meteor and cider.
iv.
e valuation results and findings we conduct extensive experiments on our labeled action dataset to investigate the following three research questions rq1.
what is the overall performance of the proposed model for command classification widget classification and location phrase generation?
rq2.
how do the three input data streams and the joint learning affect the model performance?
rq3.
how does the multi task learning affect the model performance?
a. overall model performance rq1 motivation our model generates structured user action descriptions which include command classes widget classes and free form location phrases for user action screencasts.
different commands widgets and locations have diversetable iii overall model performance output class precision recall f1 score command acc .84click .
.
.
type .
.
.
drag .
.
.
hover .
.
.
select .
.
.
scroll down .
.
.
appear .
.
.
zoom in .
.
.
scroll up .
.
.
disappear .
.
.
zoom out .
.
.
average .
.
.
widget acc .87image .
.
.
button .
.
.
text .
.
.
icon .
.
.
checkbox .
.
.
window .
.
.
others .
.
.
popup .
.
.
dropdown .
.
.
tab .
.
.
page .
.
.
average .
.
.
locationbleu rough meteor cider .
.
.
.
visual and temporal characteristics and changes the uis differently.
this rq aims to investigate the overall performance of our model in this diverse and challenging learning setting.
method in this experiment we combine the action data of five applications and split it into for model training and for model testing.
we use the full model for this evaluation which takes three input data streams i.e.
original frames cropped change regions and similarity maps and simultaneously predicts three outputs command class widget class and location phrase.
the model is trained for epochs until convergence.
we perform fold cross validation and report the average performance metrics.
result table iii1shows the full model s performance on the three outputs2.
for command classification it achieves an average of .
accuracy and .
f1 score.
both click and type have higher f1 score than other command classes but the reasons are not the same.
the large amount of click instances in the dataset makes the model well trained for the click class and results in better performance.
although type does not have many instances it always interacts with text which has similar visual features and is easy to learn.
we find that our model is often confused between scroll down and scroll up .
in fact most incorrectly recognized scroll down is classified as scroll up .
this is because the two actions have very similar visual features and scroll down has better performance because it has more instances than scroll up in the dataset.
a similar situation is observed 1readers can refer to this github for all experiment results in this paper.
2we also examine the model performance on each application which is similar to the overall performance.
please see the results in the github repo fig.
.
failure examples of location prediction for zoom in versus zoom out .
appear and disappear have relatively low f1 score and most incorrectly classified appear and disappear instances are predicted as click .
this is because click often triggers appearing or disappearing of certain ui parts.
widget classification achieves an average of .
accuracy and .
f1 score and the f1 score for all widget classes is above .
image achieves f1 score because imagerelated actions usually involve larger change regions than the actions on small widgets such as button and icon.
furthermore images have more salient features than other large widgets such as window and page.
button and text have f1 score of because our dataset has a large number of button instances and all text widgets have similar visual features.
we find that most incorrectly classified dropdown instances are predicted as button or popup .
this is because dropdown has very similar appearance as button before it is expanded and clicking dropdown usually triggers a menu appearing which looks like popup .
some window instances are incorrectly predicted as popup or page as these widgets share similar visual features.
in fact it might not be necessary to distinguish these window like widgets involved in user actions such as or .
location phrase generation achieves .
bleu .
rough .
meteor and .
cider which means our model can generate location phrases with good precision recall and f1 and can generate the most important location information.
compared with command and widget classification location phrase generation is a more challenging task because it needs to compose an appropriate free form phrase for many similar gui contexts and widgets.
we show two failure examples in figure .
in figure a user selects text in address bar but the model predicts the location as toolbar .
in figure b the model predicts at right popup while the human label is in window .
the wrong predictions could be caused by the fact that both address bar and toolbar appear at the top of application window and both window and popup are container widgets and share similar visual features.
our model can accurately predict command and widget classes.
most confusions come from the commands and the widgets with similar visual features such as scroll down up dropdown versus button window versus page.
location phrase generation is a more challenging task but the generation results by our model are acceptable.table iv impact of single two and three input image sequences command widget location input prec recall f1 accu prec recall f1 accu bleu rough meteor cider cropcr .
.
.
.
.
.
.
.
.
.
.
.
origin .
.
.
.
.
.
.
.
.
.
.
.
simmap .
.
.
.
.
.
.
.
.
.
.
.
cropcr origin .
.
.
.
.
.
.
.
.
.
.
.
cropcr simmap .
.
.
.
.
.
.
.
.
.
.
.
origin simmap .
.
.
.
.
.
.
.
.
.
.
.
cropcr origin simmap .
.
.
.
.
.
.
.
.
.
.
.
b. impact of joint learning rq2 motivation our model takes three input image sequences original frames cropped change regions and similarity maps which are designed to play complementary roles to represent user action context and features.
in this rq we investigate the impact and synergy of the three input image sequences by ablation study.
method we develop six variants of input three singleinput variants cropped change regions cropcr originial frames origin or similarity maps simmap three twoinputs variants crop cr origin cropcr simmp or origin simmap .
these variants use the same model configuration as the full model but ablate the corresponding submodel s for the absent input sequences.
the command classifier widget classifier and location generator remain unchanged.
we use the same data for training model variants as for the full model origin cropcr simmap .
all model variants are trained for epochs until convergence.
we perform fold cross validation and report the average performance metrics.
result table iv reports the performance of the six input variants and the full model.
with single input sequence the model achieves only about .
f1 score and accuracy for predicting commands and widgets.
the cropped change region is good at predicting commands with accuracy and f1 score.
similarity map has better performance than original frame especially for predicting widgets.
the grayscale similarity map contains much less details than the original rgb frame but this excludes much noise irrelevant to the action which is beneficial.
in addition the similarity map can display accurate contour of a change region instead of a rectangle bonding box which is important for recognizing widgets.
for example button usually has more smooth edge than text and checkbox is a small rectangle in general.
in contrast original frame contains too many details which often blur the most important visual features related to user actions especially for those commands or widgets involving small change regions for example type a short word.
adding one more input sequence can improve the overall model performance especially adding cropped change regions or similarity maps to original frames.
for example originalframe only input has poor performance for recognizing small widgets such as button checkbox and icon but adding similarity map can significantly improve the performance .
in f1 score and .
in accuracy because similarity map indicates which region on the original frame the model should pay more attention to.
furthermore two inputs model variants achieve very close performance and the gap between the best and the worst model variants narrow when using two input sequences.
with the use of three input sequences i.e.
full model we observe further .
.
increase in f1 score and accuracy for predicting commands and widgets.
each input image sequence has its contributions for recognizing user actions.
combining all three input streams and formulating them as joint learning can improve the overall model performance.
c. impact of multi task learning rq3 motivation our model generates a structured natural language description in the form of .
considering the potential latent associations between commands widgets and locations we train the model in a multi task learning setting.
in this rq we want to compare our method with independent learning and traditional video captioning.
method for independent learning we optimize the command classifier the widget classifier and the location phrase generator separately rather than optimizing the combined loss of the three outputs.
for traditional video captioning we train an lstm decoder to generate an unstructured sentence e.g.
click button in popup for an action screencast rather than separate command widget and location.
in both variant settings the encoder remains the same as the original model.
the variant models are trained in the same way as the original model.
we perform fold cross validation and report the average performance metrics.
result table v presents the performance of the two variant settings and the original model.
independent learning results in marginal decrease in f1 score and accuracy for predicting commands and .
decrease in f1 score and accuracy for predicting widgets compared with multi task learning.
for location phrase generation independent learning results in relatively larger performance degradation .
bleu .
rouge .
meteor and .
cider compared with multi task learning.
this result agrees with the nature of multi task learning.
it is an inductive transfer mechanism whose principle goal is to improve generalization performance while keeping the model prediction performance .
to gaintable v performance of multi task learning versus independent learning or traditional video captioning unsentgen command widget location input prec recall f1 accu prec recall f1 accu bleu rough meteor cider independent .
.
.
.
.
.
.
.
.
.
.
.
unsentgen .
.
.
.
.
.
.
.
.
.
.
.
our model .
.
.
.
.
.
.
.
.
.
.
.
fig.
.
failure example of traditional video captioning the global optimization of all subtasks tradeoffs would be made on the performance of some subtasks.
in our work multi task learning keeps the performance of command and widget classification while the more challenging location phrase generation task benefits from the command and widget classification in the multi task learning setting.
for traditional video captioning unsentgen row we observe many strange outputs such as zoom out button in popup in figure which is an impossible combination of command and widget.
such outputs result in very low f1score and accuracy for predicting commands and widgets.
this is because command and widget classification often require different spatio visual features to make accurate prediction.
in our structured prediction two separate mlps are used for command and widget classification respectively which can extract command or widget specific features.
in contrast the unstructured sentence generation by traditional video captioning relies on one common feature vector for generating all three types of action information which is challenging to satisfy all at the same time.
furthermore the unstructured sentence generation has to generate a longer sentence from a larger vocabulary combining command labels widget labels and location words than the three separate predictions.
this inevitably increases the task difficulty.
multi task learning slightly improves the performance of command and widget classification but it benefits the more challenging location phrase generation task.
traditional video captioning cannot reliably generate an unstructured sentence which contains all necessary command widget and location information.
v. p ilot study bugreproduction having evaluated our model s performance for structured user action recognition from screencasts we would like to demonstrate a practical application on bug reproduction that our model enables.a.
motivation bug reproduction is an essential software engineering task .
there are two common ways to report a bug text description and screencast demonstration.
the study shows that many users are not professional enough to write a high quality bug report with clear and complete steps to reproduce.
this not only increases the burden of software maintainers to understand and analyze the bugs but also creates a barrier to automated bug reproduction techniques based on the steps to produce descriptions in bug reports .
in contrast recording screencasts lowers the bar for ordinary users to report bugs and it can cover every single detail of a bug when and how it occurs .
github has announced that video upload is available on its platform meaning that video data is becoming more and more important in software community.
however the image nature of screencasts makes it hard to integrate video content with existing text based tools.
in this work we conduct a pilot study of this potential usage.
in particular we investigate if the generated humanunderstandable action scripts from bug screencasts can help people reproduce the bugs.
b. a screencast to actionscript tool we develop a screencast to actionscript s2as tool for our pilot study.
as shown in figure s2as integrates our seeaction model with a heuristic based video segmentation method and a ui widget detection method thus constituting a full reverse engineering process that converts an input action screencast into a script of structured user actions such as those shown in figure .
based on our video labeling experience and the observation of our labeled action fragments we design a simple image similarity based method for video segmentation driven by the fact that when a user action occurs on the ui there will be more or less pixel changes.
as illustrated in figure for an action screencast in our dataset user actions result in low screen similarities during the action time periods while the periods without user actions have high screen similarities and remain stable over time.
based on this observation s2as segments the screencast into a series of action fragments by identifying the time points when the screen similarities start and finish changing.
for widget detection we use an existing technique uied which can detect the bounding box of a widget and recognize its class.
for detected text widgets uied uses google ocr to convert text widget images to texts.
s2as applies uied to an action fragment in the reverse order fromfig.
.
our screencast to actionscript tool the last frame to the first .
uied detects all widgets on a ui image.
if uied detects a widget on a frame whose bounding box overlaps with the change region identify by seeaction for that frame and that has the same class as the widget class predicted by seeaction s2as identifies this widget as the target widget of the user action.
if the target widget is a nontext widget s2as crops the widget image by its bounding box.
for text target widget it takes the ocred text as the widget information.
finally it outputs a complete structured user action based on the structured user action predicted by seeaction and the identified widget information such as .
s2as applies seeaction and uied to all action fragments and generates a script of user actions for the input screencast.
c. pilot study bug reproduction dataset we randomly sample bug reports with bug screencasts on firefox bugzilla .
these bugs were reported during january january and have no overlap with our labeled dataset.
they are related to a wide range of firefox functionalities e.g.
website browsing web page inspection menu bar customization .
by inspecting the text description of the collected bug reports we summarize them to five quality grades which include high quality hq low quality ambiguous step lq as low quality v ocabulary mismatch lq vm low quality missing step lq ms and no steps to reproduce ns2r .
high quality means the bug reports have detailed clear and complete steps to reproduce the bugs.
n s2r means the complete absence of s2r description in bug report.
the three low quality grades are defined according to the s2r quality issues revealed in .
ambiguous step means a step matches more than one gui component or event.
v ocabulary mismatch means a step does not match any application interaction.
missing step means a required step is not described in the bug report.
a bug report may suffer from several low quality issues.
we annotate the most significant issue that prevents the bug reproduction as a bug s quality grade.
two authors annotate the bug report qualities independently and their initial annotations have .
inter rater agreement by cohen s kappa .
for disagreements they discuss to decide the final grade.
as shown in table vi only out of bug reports have high quality s2r descriptions and bug reports do not provide s2r description at all the bug reporttable vi bug report quality grades and reproduction results q grade hq lq as lq vm lq ms n s2r all s2r qg repros2r reprogas q grade quality grade s2r qg the number of bug reports that have different quality grades of steps to reproduce repros2r the number of bug reports that are correctly reproduced by the steps to reproduce descriptions reprogas the number of bug reports that are correctly reproduced by generated action scripts.
may simply leave a statement see the attached video .
for the rest low quality bug reports and of them have significant issues in missing steps vocabulary mismatch and ambiguous steps respectively.
our results are similar to the observation of s2r quality issues in .
study procedure we recruit six graduate students from our school to participate in our study.
through a pre study survey all participants use firefox regularly and are familiar with the main functionalities involved in the bugs.
the participants do not know any background knowledge of this work before finishing the experiment.
we process the bug screencasts in bug reports by our s2ac tool and generate an action script for each of them.
we collect the s2r descriptions if any in the bug reports.
the six participants are split to two groups.
three of them repros2r are given the original s2r descriptions in the bug reports for bug reproduction while the other three reprogas use the generated action scripts by our tool.
all participants screen record their bug reproduction process.
following the practice to evaluate the success of record and replay tools we manually check the externally visible state evs of the bug reproduction and regard the reproduction as a success if and only if the evs by reproduction is the same as the evs of the reported bug.
results and analysis table vi reports the union of the bug reports that were successfully reproduced by the three participants using the original s2r descriptions repros2r or the generated action scripts reprogas .
obviously unless the repros2r participants watch the bug screencasts it is impossible to reproduce the bugs without the s2r descriptions n s2r .
among the bug reports with the s2r descriptions only of them are reproduced successfully by using the original s2r descriptions and the majority are high quality bug reports.
the participants have muchlower success rate only about for reproducing bugs based on low quality s2r descriptions.
for example given an ambiguous step search mozilla lq as one can type mozilla in either search bar or address bar of the browser or may search some other keywords such as what is mozilla .
another example is vocabulary mismatch lq vm disable omtc .
none of the three repros2r participants know what omtc off main thread compositing is before the experiment and are stuck in this step.
reproducing the bug reports that miss important steps lq ms is also a big challenge.
for example the s2r of the bug report may mention only set default search engine as google .
the participants have to figure how to reach the particular setting page and change the setting.
using the generated action scripts the number of successfully reproduced bugs increases to .
actually the overall success rate and the success rates for different quality grades are all about because using the generated action scripts to reproduce the bugs has nothing to do with the presence and quality of the original s2r descriptions.
of bugs without the s2r descriptions and about of bugs with low quality s2r descriptions are successfully reproduced based on the generated action scripts.
compared with the reproduction success rate based on the original low quality s2r descriptions this is a promising result as a tool like s2ac could be leveraged to help lazy bug reporters write good quality s2r descriptions.
of course compared with the high quality s2r descriptions written by human reproduction success the generated action scripts still have room for improvement.
for the whole screencast to actionscript process we find that video segmentation deserves more research as the inaccurate segmentation will inevitably affect the subsequent action recognition.
for example the user scrolls up and down the web page back and forth quickly leading to less accurate segmentation between the consecutive similar user actions.
some user actions may also be over segmented leading to many too fine grained actions.
for example a click button action are segmented into hover button and click button and selecting a long text are segmented to multiple select text .
although much of such less ideal segmentation can still be understood and applied by human participants some may cause the human confusion and the reproduction failures.
for example a click checkbox action is segmented into two or more click checkbox actions and the second and following click checkbox may lead to unexpected results.
such less ideal segmentation would also negatively affect the automatic replay tools .
for widget detection we encounter some difficulty of adjusting two important parameters of uied mingrade and minelearea .
too small value results in many noisy bonding boxes and too large value misses many small widgets.
after the experiments we set mingrade andminelearea which achieves the best result on our dataset.
but uied still detects some noisy widgets or misses some widgets which result in the wrong or incomplete action scripts affecting thebug reproduction.
for example an icon is misclassified as text resulting in the mismatch of widget class and consequently a missing step in the generated action script.
our pilot study albeit by no means conclusive demonstrates the promise of non intrusive screencast toactionscript reverse engineering tool for bug reproduction.
it also reveals the challenges in building the tool pipeline and the improvement for video segmentation and widget identification which we leave as our future work.
vi.
r elated work hci actions can be recorded using instrumentation based or computer vision based methods.
instrumentation based methods are all more or less intrusive relying on source code or customized os or os or gui framework accessibility support or runtime customization .
computer vision based methods offer an alternative nonintrusive way and receive growing attention in recent years.
for examples actionnet extracts primitive mouse and keyboard commands between two consecutive frames in an action screencast.
v2s recognizes tap command in mobile apps usage videos based on tap indicators.
other works investigate the extraction of gui widgets e.g.
waken prefab or application contents e.g.
source code .
our work is the first non intrusive method to extract command widget and location as a whole structured action and our approach does not limit action fragment lengths nor does it assume special visual indicators.
recognizing hci actions can help the analysis of developer behavior .
it is also the foundation for ui automation tasks for example record and replay testing and robotic process automation .
a wide range of ui automation tools are available such as selenium appium robotium monkeyrunner uiautomator .
all these tools rely on os or gui framework accessibility support.
in contrast visual script based ui automation such as sikuli airtest uipath and ui.vision use computer vision techniques to match widgets to be operated.
although they are platform independent widget matching is sensitive to changes of screen size and resolution and widget appearance .
all these ui automation techniques focus on action replay.
in contrast our work focuses on non intrusive action recognition from screencasts.
our approach can be regarded as a vision to text task.
in computer vision community image captioning and video captioning are two typical vision to text tasks.
image captioning techniques have been adopted for generating gui code from gui design images or generating widget labels to improve accessibility .
however these works generate unstructured text for describing the image or video.
our experiment in rq3 show this cannot satisfy the need for complex user action recognition.
video segmentation video retrieval and object detection are related techniques which can be integrated with our approach to build the whole screencast to actionscript pipeline.
object detection models have been used for random gui testing and gui widget specific detection method has also been proposed .
other remotely related work includes predicting screens with code gui visual quality game visual quality and actionable gui areas .
these works make prediction on static gui images while our goal is action recognition from screencasts.
vii.
t hreats to validity one threat to internal validity is the manual labeling bias of the screencast dataset.
in order to minimize the errors two authors label the whole dataset independently and finally reach agreement by discussion.
we find that the disagreements mainly come from judging location of user action.
for example two authors give label of in search bar and in textbox to a video fragment but they refer to the same location in firefox which has low impact to user action representation.
another internal threat is from the participants bias of pilot study.
none of the participant know the knowledge of our work before completing the experiment.
threats to external validity include the selection of desktop applications and ai models and widget detection accuracy.
we use popular desktop applications in our work and further evaluation is required on other types of applications such as video game and mobile applications.
in terms of ai models we applied 3d cnns and lstm in this work to support video action recognition.
however it is easy to replace those modules with alternative ai models such as transformer and bert furthermore the pilot study involves multiple steps of processing and the error accumulation of each step especially widget detection by uied makes it not perfect enough to support all practical applications.
this work focuses on generating structured natural language description from screencasts and we will try to optimize the pipeline by improving the performance of each step.
viii.
c onclusion and future work this work proposes a novel computer vision based model to reverse engineer structured user actions from only the visual information recorded in screencasts.
our model extracts the spatio temporal features jointly from the three complementary data streams original frames change regions and similarity maps and predicts the command widget and location in a multi task setting.
we evaluate our model on a large dataset of video action pairs from five widely used applications.
our results confirms the high accuracy of our model even in the challenging across application training testing setting and the effectiveness of our joint learning and multi task learning design.
we develop a prototype screencast to actionscript tool based on our model and demonstrate its effectiveness in generating high quality human understandable action scripts for bug reproduction through a pilot study on firefox bugs.seeaction s functionality extends beyond bug report reproduction.
its human readable and structured hci action descriptions can support a variety of downstream software engineering tasks including ui automation and testing broadening its applicability and potential impact.
in the future we will investigate more robust video segmentation and gui widget detection methods to improve the screencast to actionscript pipeline for non intrusive ui automation and further explore its potential applications in various software engineering tasks.