delft university of technology metamorphic based many objective distillation of llms for code related tasks panichella a. publication date document version accepted author manuscript published in the 47th ieee acm international conference on software engineering citation apa panichella a. in press .
metamorphic based many objective distillation of llms for code related tasks.
in the 47th ieee acm international conference on software engineering ieee acm.
important note to cite this publication please use the final published version if applicable .
please check the document version above.
copyright other than for strictly personal use it is not permitted to download forward or distribute the text or part of it without the consent of the author s and or copyright holder s unless the work is under an open content license such as creative commons.
takedown policy please contact us and provide details if you believe this document breaches copyrights.
we will remove access to the work immediately and investigate your claim.
this work is downloaded from delft university of technology.
for technical reasons the number of authors shown on this cover page is limited to a maximum of .
metamorphic based many objective distillation of llms for code related tasks annibale panichella delft university of technology delft the netherlands a.panichella tudelft.nl abstract knowledge distillation compresses large language models llms into more compact and efficient versions that achieve similar accuracy on code related tasks.
however as we demonstrate in this study compressed models are four times less robust than the original llms when evaluated with metamorphic code.
they exhibit a higher probability of misclassifying code clones due to minor changes in the code fragment under analysis such as replacing parameter names with synonyms.
to address this issue we propose m orph a novel method that combines metamorphic testing with many objective optimization for a robust distillation of llms for code.
m orph efficiently explores the models configuration space and generates paretooptimal models that effectively balance accuracy efficiency and robustness to metamorphic code.
metamorphic testing measures robustness as the number of code fragments for which a model incorrectly makes different predictions between the original and their equivalent metamorphic variants prediction flips .
we evaluate m orph on two tasks code clone and vulnerability detection targeting codebert and graphcodebert for distillation.
our comparison includes m orph the state of theart distillation method a vatar and the fine tuned non distilled llms.
compared to a vatar m orph produces compressed models that are i more robust ii more efficient fewer floating point operations while maintaining iii equal or higher accuracy up to and iv similar model size.
index terms knowledge distillation large language models metamorphic testing many objective optimization green ai sustainability search based software engineering ai for se i. i ntroduction large language models llms have received broad interest in the software engineering community due to their ability to perform a wide spectrum of tasks such as code completion clone detection and vulnerability detection .
these capabilities stem from the increasing model scale with millions and billions of parameters and extensive training data spanning thousands of projects .
despite their potential llms frequently face significant computational challenges such as reliance on dedicated gpu hardware high energy consumption and long inference times up to seconds per prediction .
these factors limit the practical application of llms as ide creators and developers prefer compact models that give instantaneous feedback within a few tens of milliseconds .
recently researchers have proposed knowledge distillation methods for compressing large models into smaller more efficient models.
knowledge distillation transfers knowledge from a large pre trained model the teacher to a small model the student that keeps approximately the same accuracy on downstream tasks but with significantly lower resource demand.
in the context of llms for code shi et al.
have proposed a vatar a distillation method that compresses codespecialized bert like models from their original size of megabytes mb and 125m parameters down to 3mb models with a small negligible accuracy loss.
relying on evolutionary algorithms a vatar has shown superior performance over previously proposed distillation methods .
theoretically these compressed models are a viable alternative to their teacher llms.
they are smaller greener up to lower energy consumption and faster up to faster latency .
however to date it is unclear whether these compressed models retain the same robustness and properties of their teachers in a broader sense beyond accuracy.
this paper aims to shed light on this aspect by investigating the robustness of compressed llms against metamorphic code.
metamorphic testing is a well established technique to assess the robustness of the llms models in making predictions against variants of the same code fragments that are semantically and behavioral equivalent.
metamorphic code can reveal model weaknesses implementation bugs data leakage issues and other model vulnerabilities .
therefore we investigate the following research question rq how robust are the student models distilled with existing state of the art methods to metamorphic code?
we investigate the robustness of models compressed by avatar against natural metamorphic attacks code fragments with methods and input parameter names replaced with synonyms or expanding acronyms.
this process does not modify the code s behavior and captures the variation in naming conventions followed by developers in practice .
we benchmark a vatar on two downstream code related tasks clone detection and vulnerability detection by compressing codebert and graphcodebert on the devign and bigclonebench datasets.
while our results confirm the benefits that the compressed methods bring we also find a dramatic reduction in robustness since the compressed models exhibit up to four times more mispredictions or prediction flips on metamorphic code compared to their teacher models.
a prediction flip pf occurs when a model makes a different erroneous prediction on a source code snippet and its corresponding metamorphic variant.
for instance the distilled models may miss a vulnerability because the codecontains different semantically equivalent parameter names.
the significant reduction in robustness raises concerns about deploying compressed models in ides.
these models must be both accurate and robust across diverse coding scenarios.
nonrobust models can compromise the development workflow by not identifying vulnerabilities or code clones.
given these limitations in existing distillation methods we propose m orph .
our novel method integrates metamorphic testing into the distillation process as one of the key objectives in a many objective problem formulation.
using a state ofthe art many objective optimizer namely age moea morph generates pareto optimal models that minimize the number of prediction flips on metamorphic code while maintaining high accuracy small model size and low inference cost.
thus we investigate the following research questions rq how effective is morph in distilling robust models compared to the existing state of the art method?
rq what is the impact of individual components features ofmorph on its overall performance?
we empirically evaluated m orph on clone and vulnerability detection tasks to compress codebert and graphcodebert and compared its performance against the state of theart distillation method a vatar .
the results show that m orph generates models that are more robust measured in pfs more sustainable measured by flops with a comparable or superior accuracy up to while maintaining the model size of 3mb.
an ablation study confirms that all individual components of m orph contribute to its overall performance with metamorphic testing and many objective optimization being particularly critical.
age moea the core optimizer in m orph outperforms alternative optimizers in generating high quality pareto optimal distilled models.
our contributions can be summarized as follows we identify weaknesses in the robustness of existing compression techniques against metamorphic code.
we propose m orph a novel approach that integrates metamorphic testing with many objective optimization to generate robust compressed models.
we empirically evaluate m orph on multiple tasks and llms our results demonstrate its effectiveness in producing robust accurate and efficient models.
we provide a complete replication package including the metamorphic datasets the implementation of morph and the raw data of our experiments.
ii.
b ackground and related work knowledge distillation .
knowledge distillation aims to compress a large pre trained model called the teacher into a smaller and more efficient model called the student .
the student model sis taught how to predict similar responses on a given dataset as made by the teacher mby minimizing the difference between the teacher s and student s output distributions .
in task specific distillation the student model learns from an already fine tuned teacher model for a specific task which means that scan be directly used for the same downstream task without any fine tuning.finding the optimal architecture for the student model remains challenging.
common hyperparameters such as the number of layers hidden units and attention heads need to be tuned to achieve the desired performance of the student model.
compressor by shi et al.
employs a single objective genetic algorithm for compressing code specific models such as codebert into 3mb with approximately the same accuracy levels.
a vatar extends this work by using a multiobjective genetic algorithm to balance accuracy and efficiency when compressing models into 3mb.
since a vatar has been shown to outperform c ompressor we refer to it as the stateof the art distillation method for code related tasks.
despite these undisputed advancements models compressed using a vatar show reduced robustness to code variations as we demonstrate in section v a. to address this we introduce robustness as a fourth objective in the distillation process alongside accuracy size and efficiency.
this addition requires using a many objective optimization algorithm e.g.
agemoea rather than a multi objective one used in prior studies .
besides we leverage metamorphic testing to measure the robustness of the student models against metamorphic code changes and build surrogate models to predict the model robustness without fully training and evaluating it.
metamorphic testing .
metamorphic testing for software systems was first introduced by chen et al.
as a technique to generate new test cases by applying transformations to the input data while preserving the expected test output oracle .
recently metamorphic testing has gained traction in assessing the robustness of machine learning models against subtle input variations e.g.
pixel changes for image classification .
metamorphic testing has been used in software engineering to evaluate the robustness of llms of code against subtle code variations or metamorphic transformations .
compton et al.
demonstrated how code2vec models are vulnerable to obfuscation techniques for variable names.
yefet et al.
introduced metamorphic transformations that add unused variables to existing code causing the model to misclassify vulnerable code.
other metamorphic changes include renaming input parameter names adding lambda expressions etc.
all these transformations do not alter the code behavior but can lead to incorrect model predictions .
bertlike models such as codebert and graphcodebert have also been tested using metamorphic testing and shown to be vulnerable to metamorphic code changes .
there are two main strategies to generate metamorphic code snippets applying one change at a time first order or applying multiple changes at once higher order .
the latter is more effective in fooling llms and can be applied either by sampling multiple first order changes or using meta heuristics to select the most effective adversarial changes .
as highlighted by yang et al.
the naturalness of the metamorphic code is crucial to ensure that the changes are realistic and preserve the natural semantics of code e.g.
variable names in addition to its behavior and syntax.
this work focuses on the robustness of models compressed with knowledge distillation against metamorphic changes.
ourgoal is two fold to measure the robustness of models compressed with existing distillation techniques and to develop a new distillation method that explicitly relies on metamorphic testing as one of its core components.
many objective optimization .
many objective optimization refers to problems involving more than three conflicting objectives .
in these problems identifying a single optimal solution is infeasible due to the inherent trade offs between the objectives.
therefore the goal is to find solutions representing the optimal trade offs known as the pareto front .
the pareto front comprises solutions where no improvement can be made in one objective without degrading another pareto optimality .
a key challenge is the exponential growth of the search space as the number of objectives increases making it difficult to find the pareto front efficiently .
many objective evolutionary algorithms moeas have emerged as powerful tools for addressing such problems .
moeas iteratively evolve an initial population of solutions through subsequent generations.
the aim to approximate the pareto front by balancing two aspects converging towards the true pareto front and maintaining diversity among the solutions to provide decision makers with a range of options to choose from over the last decades several moeas have been proposed including nsga ii nsga iii moea d and age moea .
they differ mainly in the mechanisms they rely on to select new solutions for the next generations environmental selection .
this paper leverages age moea for its adaptability to diverse pareto front shapes and demonstrated superior performance in prior studies .
its environmental selection mechanism employs non dominated sorting and assigns asurvival score that measures both convergence and diversity based on the estimated geometry shape of the pareto front.
this promotes solutions near and well distributed along the pareto front .
a comparative evaluation of age moea with other moeas is provided in section v c. iii.
p roposed methodology ensuring the robustness of compressed models is crucial as non robust models can fail in real world applications.
for instance a model unable to detect a vulnerability in code with slightly different variable names is highly problematic given the prevalent use of inconsistent identifier names and vocabulary in existing codebases .
hence we formulate the robust knowledge distillation problem as follows definition given a pre trained llm teacher model the problem is to find a configuration c for the student model sthat optimizes the following objectives mino1 size s mino2 s v s v v vandv meta v mino3 efficiency c maxo4 effectiveness c v where vis the validation set and each code snippet v v has a corresponding metamorphic v meta v .
in this formulation o1represents the model size measured as the number of its internal parameters.
o2measures the modelrobustness as the number of data points in vwhere the model schanges flips its prediction between original and metamorphic code snippets i.e.
when s v s v .o3 measures the model s inference cost in terms of floatingpoint operations flops as done in previous research lower flop values indicate faster inference and reduced energy consumption.
lastly depending on the specific downstream task o4measures the model s performance on the validation set such as accuracy or f1 score.
research challenges optimizing the robust knowledge distillation problem poses several challenges challenge the problem is inherently many objective i.e.
with objectives which is difficult to solve due to the increasing number of trade offs as the number of objectives increases .
many traditional evolutionary algorithms such as the one used in struggle to scale beyond three objectives requiring specialized many objective algorithms.
challenge training student models multiple times with different configurations to compute accuracy and prediction flips is computationally expensive if not prohibitive.
challenge the search space for student model configurations is vast with numerous hyperparameters and potential combinations.
effective sampling strategies are crucial to identifying promising configurations with few samples.
challenge defining and generating natural metamorphic attacks is crucial to simulate real world variations in code ensuring these transformations preserve code semantics while mimicking natural coding conventions .
challenge generating valid configurations requires adhering to the internal student model constraints such as maintaining consistency between hidden size and the number of attention heads .
this is critical to avoid invalid or suboptimal models during the search process.
oververview .
to tackle the challenges above we propose morph a novel approach that integrates metamorphic testing with recent advances in many objective optimization.
as shown in figure m orph comprises three core stages generating metamorphic datasets constructing surrogate models for fast objective calculation and employing a many objective algorithm.
by combining these macro stages and leveraging the strengths of each m orph efficiently explores the configuration space and finds a set of pareto optimal configurations for the student model.
the following sections detail each stage their interactions and how they address the challenges outlined above.
a. generating metamorphic code snippets to generate metamorphic datasets we modify the code snippets in the original dataset by introducing subtle changes thatdo not alter their behavior or semantics.
since this paper focuses on vulnerability and clone detection we rely on two metamorphic transformations method renaming and input parameter renaming .
we choose these transformations as a code snippet remains vulnerable regardless of how developers name the functions or input parameters.
to generate natural i.e.
human like written code we avoidconfigurations sampling latin hypercube samplingdistillation cross entropy cosine distancesurrogate models creations models validation gradient boostingvariation operators sbx crossover polynomial mutation repair operatorenvironmental selection survival scoretermination?yesnoobjective scores calculationpareto optimal configurationsmetamorphic training settraining setmetamorphic data generatorvalidation setmetamorphic validation setmetamorphic testingsurrogate models many objective optimization objective calculated with surrogate modelsinitial populationfig.
.
overview of m orph which encompasses three macro stages generating metamorphic datasets on the left side constructing two surrogate models middle stage and employing many objective optimization on the right side .
prompt here is a function change the function name and input parameters names with equivalent ones or synonyms in english.
just return the changed code the complete new code without any extra text.
listing .
prompt for obtaining module name and version using obfuscation techniques that rely on randomly generated strings as done in prior work .
instead we use chatgpt .
to replace english words with synonyms and expand acronyms to their full forms for example args is expanded to arguments .
synonyms are common in daily development as previous studies show that developers do not use consistent vocabularies frequently opting for synonyms and acronyms .
by focusing on these natural transformations our approach directly addresses challenge since it generates transformed code that mirrors the real world variability in naming conventions employed by developers.
listing displays the prompt we used to generate metamorphic code snippets.
in the listing indicates the programming language e.g.
java and is the original code snippet.
figure shows an example of metamorphic code generated by chatgpt .
from the devign dataset comparing the original snippet left with its metamorphic variant right .
both snippets exhibit a cwe vulnerability.
as a result our methodology employs higher order metamorphic transformation in which all input parameter names and main function names are changed at once.
this generates only one metamorphic variant for each original code snippet.
we further validated that the metamorphic code fragments are well formed by using source code parsers generated with tree sitter .
tree sitter offers a comprehensive set of grammars for various programming languages allowing us to check that the generated code adheres to the grammar rules of the corresponding programming language.
we manually inspect a representative subset of generated snippets as detailed in section iv to check the semantic equivalence to theoriginal code.
this two step validation process guarantees the naturalness and correctness of the metamorphic datasets.
b. building the surrogate models searching for pareto optimal model configurations requires training the models with the given configurations and measuring their effectiveness on the validation set.
however training and evaluating hundreds of models is computationally expensive challenge mentioned above .
to address this we use surrogate models which provide a computationally efficient prediction estimation of a model s performance given its configuration as input.
surrogate models are a well established method to tackle problems with expensive fitness functions in evolutionary computation and are widely used in software testing for costly tests and in knowledge distillation .
we defined two surrogate models that are custom and specific to our problem one for predicting the accuracy of the student model and the other for predicting the number of prediction flips on metamorphic code.
to build these surrogates the second stage in figure morph first samples the configuration space using latin hypercube sampling lhs .
then it applies knowledge distillation techniques to train the student model with each sampled configuration.
finally the sampled configurations features and their corresponding performance metrics targets are used to train the surrogate models.
latin hypercube sampling morph employs latin hypercube sampling lhs to sample a diverse pool of configurations.
lhs is a stratified sampling method that ensures uniform coverage of the configuration space by dividing each decision dimension e.g.
vocabulary size into equal intervals strata and selecting one value per interval for each dimension.
this process generates a set of sample points that are uniformly distributed across the entire search space and represent all configuration intervals thus providing a diverse set of configurations for training the surrogate models.
by efficiently exploring the vast configuration space lhs1 void qemu spice display init common simplespicedisplay ssd qemu mutex init ssd lock qtailq init ssd updates ssd mouse x ssd mouse y if ssd num surfaces ssd num surfaces ssd bufsize ssd buf g malloc ssd bufsize void initialize spice display common simplespicedisplay display qemu mutex init display lock qtailq init display updates display mouse x display mouse y if display num surfaces display num surfaces display bufsize display buf g malloc display bufsize fig.
.
original code snippet idx from the devigndataset left and its metamorphic variant right generated using chatgpt .
with the prompt in listing .
both code snippets exhibit a cwe vulnerability.
addresses challenge ensuring a diverse and representative sample of configurations.
knowledge distillation to transfer knowledge to the compact student model swith a given configuration c we employ a hybrid loss function comprising a distillation loss and a cosine similarity loss l ldist lcos.
both functions consider the augmented training set that includes the original and metamorphic training data see figure .
the distillation lossldist is calculated as the cross entropy between the teacher s softmax output and the student s log softmax output ldist nx i 1softmax yi t log softmax si t t2 where yiandsiare the teacher s and student s logits respectively tis the temperature scaling factor nis the number of data points in the training set.
the temperature t controls the softness of the teacher s predictions with higher values leading to softer predictions.
the cosine similarity loss lcosensures that the student model s predictions are similar to the teacher model s predictions in terms of direction not just magnitude.
the cosine distance between the teacher s and student s predictions is calculated as follows lcos pn i 1softmax yi softmax si qpn i 1softmax yi qpn i 1softmax si for metamorphic code snippets the teacher model s predictions are explicitly set equal to those of the corresponding non metamorphic code.
this assumes an ideal scenario where the teacher model maintains output consistency between the original snippets and their metamorphic variants.
training the surrogate models morph builds two surrogate models one for predicting accuracy and the other for predicting the number of prediction flips on the validation set.
for both performance indicators we use the gradient boosting regression gbr model .
gbr is an ensemble method that creates multiple weak models regression trees and combines them to achieve a strong combined model.
we selected gbr for its ability to handle both numerical and categorical features and preliminary experiments showed it outperformed other models in terms of mean squared error.
for each configuration we assess its accuracy on the validation set and the number of prediction flips between original and metamorphic variants.
we use these sampled configurations input features and their corresponding out come values prediction targets to train two regression based surrogate models.
the surrogate models have been trained on the validation datasets which share similar distributions with the test datasets but remain separate and distinct.
thus the test sets have not been used to train the surrogate models.
for accuracy prediction we use a gbr model with preand post processing to ensure outputs remain in .
first we apply the logit function logit p log p p to the accuracy values before fitting the model.
this function converts the accuracy values to an unbounded scale making them more suitable for regression modeling.
during prediction the model outputs logit transformed accuracy values which are then converted back to the 0to1range using the logistic sigmoid function expit x exp x .
for prediction flips the model outputs are constrained where negative values are set to zero.
c. many objective optimization with age moea to address challenge m orph uses the state of the art adaptive geometry estimation based multi objective evolutionary algorithm age moea .
age moea extends the framework of traditional multi objective evolutionary algorithms moeas by incorporating advanced techniques to estimate the geometry of the pareto front.
m orph customizes age moea to handle the four objectives and the specific constraints of knowledge distillation custom initialization with lhs use of surrogate models for calculation of the objective scores and use of a repair operator.
as shown in figure m orph iteratively evolves an initial set of configurations.
at each iteration the algorithm applies variation operators crossover mutation and repair to generate new configurations offspring .
parent and offspring configurations are evaluated using the surrogate models which predict the performance of the student models on the validation set without training it.
the objective scores are used to select the best configurations that will survive in the subsequent iteration environmental selection .
this process is repeated until the total number of configuration solution evaluations is reached.
m orph outputs pareto optimal configurations representing the trade offs between the four objectives.
search initialization population initialization is the first step in the many objective optimization process.
the initial population should include a diverse randomly sampled pool of model configurations .
m orph uses latin hypercubesampling lhs to create the initial population i.e.
the same method used for the surrogate models section iii b1 .
repair operator morph employs three operators to generate offspring solutions crossover mutation and repair.
for crossover and mutation we use the simulated binary crossover sbx and polynomial mutation pm which are standard in many objective optimization .
instead the repair operator is a custom operator designed for the distillation problem in m orph to ensure that the offspring configurations are valid and meet the constraints of the student models challenge .
for example a configuration with a hidden size not divisible by the number of attention heads is invalid.
besides some genes e.g.
vocabulary size must be integers while others e.g.
dropout probabilities must be within a specific range.
therefore we developed a repair operator that fixes and repairs all invalid configurations.
for each child configuration the repair function adjusts the genes as follows integer configuration values are rounded to the nearest integer probability values e.g.
hidden dropout probability are rounded to one decimal place the hidden size must be divisible by the number of attention heads .
if this is not the case after crossover and mutation the repair operator adjusts the hidden size to the nearest multiple of thenumber of attention heads using equation .
this process involves identifying the nearest multiples of the number of attention heads.
specifically the repair operator calculates the next higher multiple ain equation and the previous lower multiple bin equation .
then it compares these multiples to the original hidden size and chooses the closest one.
let h denote the hidden size and athe number of attention heads the repair operator calculates the new fixed hidden size h as a h a a b h a a h aif h a h b botherwise objective score calculation the new configurations are evaluated after applying the variation operators and their objective scores are calculated.
for the accuracy o4 on the validation set and the number of prediction flips on the metamorphic validation set o2 m orph uses the surrogate models instead of training the student models from scratch see section iii b3 .
w.r.t.
o1 the model size is calculated using the formula from prior studies .
this formula decomposes a student model into three components the embedding layer the transformer layer and the classifier layer.
the formula determines the total model size in mb by summing the sizes of these three components.
finally the flops o3 are calculated using the same methodology used in prior studies on knowledge distillation .
iv.
e mpirical evaluation the goal of this study is to evaluate the effectiveness of m orph and compare it with existing model distillation approaches.
we investigate the following research questions rq how robust are the student models distilled with existing state of the art methods to metamorphic code?
rq how effective is morph in distilling robust models compared to the existing state of the art method?
rq what is the impact of individual components features of morph on its overall performance?
rq 1serves as a preliminary investigation to assess the robustness of models distilled using the existing state of theart method a vatar against metamorphic code.
this rq is crucial as it helps identify potential weaknesses in existing distillation approaches particularly their robustness to metamorphic code.
if the results indicate that these methods are less robust than the non compressed versions of the models it will highlight the necessity for a more robust distillation approach thus motivating our approach m orph .
our second research question rq directly evaluates the effectiveness of m orph in distilling robust models especially in comparison to a vatar .
this question aims to determine whether m orph can produce student models that are more pareto optimal than those distilled using a vatar also considering the robustness against metamorphic code.
finally rq 3is addressed through an ablation study that investigates the contribution of each key component of m orph to its overall performance.
this includes evaluating the impact of data augmentation lhs for population initialization repair operator surrogate models and the employed many objective optimization algorithm.
for the latter we compare age moea the default optimizer in m orph against alternative many objective optimizers namely nsga ii nsga iii and moea d w.r.t.
the pareto fronts quality.
by systematically removing each component and analyzing the results we assess their effectiveness.
comparing the use of age moea with other optimizers allows us to validate its suitability and explore whether alternative algorithms might offer additional benefits or improved pareto front quality.
downstream tasks .
to align with previous research in knowledge distillation we evaluate m orph and the baselines a vatar and the original llms on two prominent software engineering tasks vulnerability prediction and clone detection.
table i presents an overview of the datasets used in our experiments.
for a fair and rigorous comparison with existing knowledge distillation methods we adopt the identical dataset splits and experimental settings employed in prior studies for both tasks.
this methodological consistency enables a direct replication of previous results and facilitates a precise evaluation of m orph s performance.
the first task is vulnerability prediction which involves identifying security vulnerabilities in source code.
we use the devign dataset which contains code snippets written in the c programming language and mined from two open source libraries ffmpeg andqemu .
the second task is clone detection which aims to identify code duplications.
we utilize the bigclonebench dataset an extensive collection of java code clones collected by mining java projects from sourceforge and the google code platform.
we have chosen these two datasets to mitigate potential data leakage issues as we further elaborate in section vii.
metamorphic datasets to augment the original datasets we generate metamorphic code snippets using the methodology outlined in section iii a. we apply function method renamingtable i overview of datasets used for the experiments including training validation and test set sizes .
task source dataset size language vulnerability predictiondevign training c validation test clone detectionbigclonebench training java validation test and input parameter renaming to the vulnerability prediction and clone detection datasets creating metamorphic versions for the training validation and test sets.
to validate the correctness of the generated metamorphic code we conduct manual inspections to ensure that cwe labels remain consistent after transformations for vulnerability prediction.
these metamorphic datasets are used to evaluate the susceptibility of the student models distilled with m orph and the baselines to adversarial code perturbations .
notably the test sets and their metamorphic versions are not used in any of the internal steps of m orph e.g.
surrogate model training and many objective optimization .
this separation guarantees that the test set remains unseen during the search process.
large language models .
we employ two pre trained llms specialized for code related tasks namely codebert and graphcodebert .
codebert is a bimodal pre trained model for natural language and programming languages.
it is based on the bert architecture and trained on a large corpus of code from multiple programming languages including python java javascript and more.
codebert is designed to understand and generate code snippets making it useful for various tasks such as code search completion and documentation generation.
graphcodebert extends the capabilities of codebert by incorporating structural information from code .
in addition to the textual data graphcodebert leverages the data flow graph of code snippets to better capture the relationships between different parts of the code.
we selected these llms because they are open source with fully traceable pre training1 used in prior distillation studies and previously fine tuned on our tasks allowing us to replicate and compare results directly.
fine tuning we fine tuned the hyperparameters of codebert and graphcodebert for the two downstream tasks described above.
fine tuning was performed using the configurations recommended by the original studies achieving results comparable to those reported in the related literature for the two downstream tasks .
empirical methodology .
to answer rq we evaluate the robustness of the models distilled with a vatar and the nondistilled fine tuned llms.
the goal is to determine whether compressed models are less more or as robust as the original llms.
to this aim we measure the robustness of a model m either teacher or student model by counting the number of instances where mproduces different predictions for an search netoriginal code snippet tand its corresponding metamorphic version t. let t t1 t2 .
.
.
t n be the original test set and let t t1 t2 .
.
.
tn be the metamorphic test set such thattiis the metamorphic version of ti.
the robustness of m is measured by the number of prediction flips prediction flips ti ti t t m ti m ti for this analysis we use the model checkpoints available in the replication package2of a vatar model of 3mb in size and compare them to the original fine tuned llms.
to answer rq we compare the models distilled by morph with those distilled by a vatar .
the comparison is made with regard to four metrics accuracy of the models on the test sets model size in mb number of prediction flips on the test sets robustness and the number of giga flops model efficiency .
for this experiment we did not use the model checkpoints available in the replication package of a vatar but trained compressed the models from scratch using the original implementation of a vatar .
the number of prediction flips is calculated using equation as done forrq .
we ran a vatar and m orph times to account for their randomness.
we report the median results of these approaches across the runs for all four metrics mentioned above alongside the interquartile range iqr .
since m orph produces a pareto front and not a single solution we select the pareto optimal compressed model with the model size closest to 3mb which is the model size of the model produced by avatar .
we analyze the results using the wilcoxon signed rank test p value .
for the significance and the vargha delaney a12statistics for the effect size.
to answer rq we perform an ablation study to evaluate the contribution of each key component of m orph .
for each ablation we measure the resulting models accuracy and robustness number of prediction flips to determine how the removal of these components affects m orph s performance.
regarding the many objective optimization algorithm we compare the results of m orph when using age moea see section iii and other alternative algorithms namely nsgaii nsga iii and moea d .
to compare the moeas we employed the hypervolume hv quality indicator to assess the quality of the pareto fronts.
hv takes values in and quantifies the volume of the objective space dominated by a pareto front.
for calculating the hv scores we determine the nadir reference point zmaxcorresponding to the point having as coordinates the maximum value for each objective across all pareto fronts generated by all moeas in all experimental runs .
a higher hv value indicates a pareto front that covers a larger portion of the objective space suggesting a better trade off between the optimization objectives.
we run each moea ten times and compare their median and iqr hv scores also using the wilcoxon signedrank test p value .
and vargha delaney a12statistics.
parameter settings .
we set m orph to evolve a population of model configurations over generations.
m orph uses parameter values recommended in the literature sbx with distribution index c and pm mutation with distribution index m recommended values for multiand many objective problems .
the crossover probability is .
within the recommended interval of and a mutation probability of l where lis the number of configuration values .
parent configurations are selected for reproduction using the binary tournament selection .
our repair operator is always applied after crossover and mutation to fix invalid solutions.
morph employs gbr to construct surrogate models see section iii b3 .
to optimize gbr hyperparameters we conducted a grid search across a predefined parameter space the number of decision trees the learning rate .
.
.
.
.
the maximum depth of each decision tree the minimum number of samples in internal node the minimum number of samples in leaf nodes .
we employed a threefold cross validation strategy to fit the gbr model and selected the hyperparameter values achieving the lowest mean absolute error across the three folds.
we ensured the test set remained unseen when training the surrogate models to prevent overfitting and maintain evaluation integrity.
both a vatar and m orph are set with the same configuration space for the distillation tokenizers bpe wordpiece unigram word vocabulary size number of hidden layers hidden size hidden act gelu relu silu gelu new hidden dropout probability intermediate size number of attention heads attention dropout probability maximum sequence length position embedding type absolute relative key relative key query learning rate .
.
.
batch size .
both approaches use the temperature scaling factor t as recommended .
finally a vatar is set with the default parameter values recommended in the original study .
we increase the population size to and the number of generations to for a fair comparison with m orph .
implementation details .
we implemented m orph in pymoo v0.
.
using python .
and pytorch v1.
.
.
pymoo is an open source framework that implements the moeas used in this study including age moea .
we use the libraries transformers v4.
.
scikit learn v1.
.
and the implementation of lhs available in pydoe v0.
.
.
the experiments were conducted on a server with an amd epyc core processor running at .
ghz and cpus available.
we used nvidia a40 gpus each with gb gddr6 running cuda version .
.
the complete replication package including the datasets the source code and results is available on zenodo .
v. r esults a. rq1 robustness of existing distillation methods table ii reports the number of prediction flips for the original llms and the student models distilled with a vatar by shi et al.
on the two tasks.
the results reveal a significanttable ii comparison of the number of prediction flips between the original llm s and avatar on the two tasks .
model name task distillation size mb pred.
flips codebertclone detectionnone avatar vulnerability predictionnone avatar gcodebertclone detectionnone avatar vulnerability predictionnone avatar increase in prediction flips for student models generated by avatar compared to teacher models.
specifically on the clone detection task the number of prediction flips surges by and for codebert and graphcodebert respectively.
while the vulnerability prediction task exhibits a less pronounced increase of for both llms the overall trend indicates a decline in model robustness.
a prediction flip happens when a model generates different outcomes for an initial code snippet and its corresponding semantically equal metamorphic variation.
this phenomenon reveals the model s vulnerability to minor variable renaming using synonyms and suggests that its decision making process may be unstable.
since it is a standard practice for developers to use a variety of often inconsistent identifiers this noticeable rise in prediction flips casts doubts on the reliability and trustworthiness of these compressed models for real world deployment.
to replace the original llms the compressed models should be at least as robust as the original models to achieve similar accuracy with fewer resources.
these findings highlight the necessity for a more robust distillation approach such as m orph which explicitly addresses the challenge of preserving model stability.
b. rq2 comparison between morph and avatar both m orph and a vatar produce multiple student models with different trade offs between size and accuracy m orph also considers model robustness as an additional objective .
for both approaches we select the model configuration with a size closest to 3mb for training the optimized model which is the target size used in the literature for code related tasks .
3mb is also the size of the models used in ide components or editor plugins .
table iii presents the results of m orph and a vatar on the two downstream tasks median accuracy model size number of prediction flips and giga flops for the two distillation approaches over ten runs.
the interquartile range iqr is shown in parentheses and measures the metric variability across the runs.
accuracy .
m orph outperforms a vatar in all metrics for both tasks while keeping a similar model size of 3mb.
for the clone detection task m orph achieves a median accuracy of .
for codebert and .
for graphcodebert compared to .
and .
for a vatar respectively.
it is worth noting that a vatar s accuracy is slightly lower than the one reported by the original paper .
in ourtable iii results of morph and avatar on the two downstream tasks .
w e report the median results with the interquartile range iqr shown in parentheses .
the best result for each metric is highlighted in gray .
model name task distillation approachaccuracy size in mb pred.
flips giga flops median diff.
median diff.
median diff.
median diff.
codebertclone detectionavatar .
.
.
.
.
.
.
.
.
.
morph .
.
.
.
.
.
.
.
vulnerability predictionavatar .
.
.
.
.
.
.
.
.
.
morph .
.
.
.
.
.
.
.
gcodebertclone detectionavatar .
.
.
.
.
.
.
.
.
.
morph .
.
.
.
.
.
.
.
vulnerability predictionavatar .
.
.
.
.
.
.
.
.
.
morph .
.
.
.
.
.
.
.
avarage mean difference .
.
table iv results of the teacher models fine tuned using the orignal training set nodist.
and the training set augmented with metamorphic code d ata aug. .
f ormorph we report the results of the models with a size closest to 3mb and 6mb.
model task distillation acc p. flips codebertclone detectionno dist.
.
.
no dist.
data aug. .
.
morph 3mb .
.
morph 6mb .
.
vulnerability predictionw o data aug .
w data aug. .
morph 3mb .
morph 6mb .
gcodebertclone detectionw o data aug. .
w data aug. .
morph 3mb .
morph 6mb .
vulnerability predictionw o data aug .
w data aug. .
morph 3mb .
morph 6mb .
evaluation we conducted independent runs and observed that in one of these runs the accuracy was similar to the values in the original paper.
however we report the average median results over these ten runs which is the standard approach for evaluating meta heuristics .
this methodology provides a more robust and reliable measure of performance by accounting for variability across multiple runs.
the differences in accuracy between m orph and a vatar are statistically significant p value .
with a large effect size a12 .
only for clone detection.
in contrast the two approaches yield comparable accuracy for vulnerability prediction.
robustness .
besides accuracy the number of prediction flips on metamorphic test datapoints is significantly lower for m orph compared to a vatar indicating higher robustness of the student models distilled with our approach.
in particular m orph leads to and prediction flips in clone detection when distilling codebert and graphcodebert respectively.
for vulnerability detection the model compressed by m orph exhibits and prediction flips for codebert and graphcodebert respectively.
these differences are statistically significant p value .
with a large effect size a12 .
for all tasks and llms.
sustainability .
finally m orph produces more efficient models than a vatar in terms of inference efficiency mea sured in giga flops floating point operations .
in particular the student models distilled by m orph lead to flops compared to the baseline method with the largest difference observed for the clone detection task.
the differences are statistically significant p value .
with a medium effect size .
a12 .
for all tasks and models to distill.
comparison with non distilled models .
table iv compares the robustness of m orph with the original llms in terms of robustness and accuracy.
two versions of the teacher models are considered one fine tuned on the original training set nodist.
and another fine tuned on an augmented set with metamorphic code nodist.
data aug. .
for m orph we present results for pareto optimal models sized at 3mb to match a vatar and 6mb which remains far smaller than the 499mb teacher models.
the 3mb models distilled by morph reduce pfs by and for codebert and graphcodebert respectively compared to the non distilled llms for vulnerability prediction.
however teacher models fine tuned on metamorphic code produce fewer pfs compared to m orph s 3mb models indicating that using metamorphic transformations during fine tuning enhances model robustness.
interestingly m orph s 6mb models achieve similar or better robustness than augmented teacher models.
for example the 6mb models distilled from codebert are more robust for clone detection and slightly more accurate.
similarly graphcodebert the 6mb models distilled surpass the augmented teacher models in robustness on both downstream tasks.
c. rq3 abaltion study on morph s key features to evaluate the impact of the features in m orph we conducted an ablation study by removing each feature individually and comparing the results with the full approach.
the results are presented in table v and table vi.
the former reports the accuracy model size number of prediction flips and giga flops for the different ablated versions of m orph while the latter reports the median hypervolume score achieved by each ablated version of m orph when using different moeas.
impact of augmentation .
disabling data augmentation with metamorphic code during training consistently leads to lower accuracy and lower robustness compared to the full approach.
however it is worth noting that even without this single feature enabled m orph still outperforms a vatar intable v results of the ablation study w .r.t.the different features in morph .
w e report the median results with the interquartile range iqr shown in parentheses .
the best result for each metric is highlighted in gray .
model task fatures acc.
size mb flips gflops cbcdw o aug. .
.
.
.
.
.
.
.
w o lhs .
.
.
.
.
.
.
.
w o repair all features .
.
.
.
.
.
.
.
vpw o aug. .
.
.
.
.
.
.
.
w o lhs .
.
.
.
.
.
.
.
w o repair all features .
.
.
.
.
.
.
.
gcbcdw o aug. .
.
.
.
.
.
.
.
w o lhs .
.
.
.
.
.
.
.
w o repair all features .
.
.
.
.
.
.
.
vpw o aug. .
.
.
.
.
.
.
.
w o lhs .
.
.
.
.
.
.
.
w o repair all features .
.
.
.
.
.
.
.
terms of robustness.
this is due to the use of metamorphic code of the validation set as the fourth objective to optimize for.
on the other hand not using data augmentation leads to models with lower flops aka more efficient models .
impact of latin hypercube sampling lhs .
removing lhs sampling from the search space exploration process leads to a decrease in accuracy and an increase in the number of prediction flips lower robustness .
the only exception can be observed for codebert when detecting vulnerabilities.
this suggests that lhs sampling is crucial for exploring the search space effectively and finding better model configurations.
impact of repair mechanism .
disabling the repair operator leads to unreliable results as the search process is not able to converge to valid solutions.
all model configurations generated without the repair operator were invalid all results are marked with .
thus this feature is ctitical to guarantee that the model s constraints are satisfied.
impact of surrogate model .
removing the surrogate model dramatically increased the convergence time with the running time rising from .
seconds to hours per distillation.
due to this extreme computational overhead conducting and reporting a complete comparison with and without surrogate models was infeasible.
performing such an experiment for just one task and model would require approximately 750hours 10repetitions on our dedicated server equating to over days of continuous computation.
this confirms the importance of using surrogate models to complete the distillation process within reasonable time constraints.
impact of age moea .
table vi reports the median hypervolume hv score achieved by age moea and the alternative moeas.
m orph achieves the highest hv score when using age moea for all tasks and llms.
the differences in hv scores between age moea and the other moeas are statistically significant p value .
with a large effect size a12 .
for all tasks and llms.
the only two exceptions are nsga ii and nsga iii for the vulnerability prediction task with graphcodebert where the differences are not statistically significant p value .
albeit with a medium positive effect size a12 .
.
thesetable vi median hypervolume score achieved by morph when using different moea s. w e report the median results with the interquartile range iqr shown in parentheses .
the best results are highlighted in gray color .
model task moeas hv p value a12 gcodebertclone detectionnsga ii .
.
.
.
nsga iii .
.
.
.
moea d .
.
.
.
age moea .
.
vulnerability predictionnsga ii .
.
.
.
nsga iii .
.
.
.
moea d .
.
.
.
age moea .
.
codebertclone detectionnsga ii .
.
.
.
nsga iii .
.
.
.
moea d .
.
.
.
age moea .
.
vulnerability predictionnsga ii .
.
.
.
nsga iii .
.
.
.
moea d .
.
.
.
age moea .
.
findings suggest that while other moeas might be considered as alternatives for optimizing student model configurations or distillation configurations age moea produces higherquality pareto fronts for the considered tasks and llms.
vi.
d iscussion and practical implications overhead of using age moea.
one of the potential concerns in deploying multi objective optimization algorithms is the required overhead they may introduce.
our experiment shows that age moea only takes around .
seconds per distillation which is negligible compared to training a single 3mb model from scratch on average minutes using our dedicated server .
moreover training surrogate models which estimate objective scores for different configurations comes at a cost of approximately .
seconds.
unlike iterative model training which is resource intensive and often requires specialized hardware age moea is a one time overhead that can be executed efficiently on standard computing resources.
impact on inference latency .
we measured model performance using flops a standard practice to estimate the inference efficiency .
to quantify the differences in a practical setting we evaluate distilled and original models on a system with cpu cores simulating a typical consumergrade laptop and measure the average inference time on the test sets in seconds s .
the results confirm the differences we observed w.r.t.
flops the average inference time for the entire deving test set code fragments is .21s for the models distilled by m orph compared to .55s for a vatar when detecting code vulnerability.
both models are much faster than the original llms as graphcodebert requires .5s on average and codebert .5s.
therefore m orph produces more efficient models that can provide instantaneous feedback to developers for code analysis within their ides.
practical implications .
our study highlights the importance of evaluating model compressing techniques beyond accuracy model size and efficiency.
robustness is critical to model performance especially in real time code analysis.
researchers should consider incorporating robustness metrics such as prediction flips into their evaluation frameworks to ensure that models are robust to the broader variety of naming conventions.
metamorphic testing can both measure test the robustness of llms but also improve models robustness via data augmentation and calibrated distillation training methods.
our methodology can be extended to other distillation techniques and tasks providing a general framework for improving model robustness.
vii.
t hreats to validity this section outlines potential threats to the study s validity and the steps taken to address them.
regarding internal validity we used chatgpt .
to create metamorphic code by renaming functions and input parameters using synonyms and acronym expansions.
these changes preserve code snippets functionality and abstract syntax tree ast without affecting their labels clone or vulnerability .
we validated the correctness and naturalness of metamorphic code using parsers and manual inspection see sections iii a and iv .
for m orph s internal parameters we followed the recommended values for genetic operators from existing guidelines in many objective optimization .
we used a vatar s original implementation and parameter settings with adjustments to the population size and number of generations for a fair comparison.
future work will explore the impact of different parameter values.
both m orph and a vatar were optimized using the same training and validation sets to ensure fairness.
test sets unseen during distillation and optimization were used to evaluate student model performance.
codebert and graphcodebert were fine tuned using the same training and validation sets.
data leakage is another potential threat and a common issue when evaluating llms .
to address this we carefully selected the devign and bigclonebench datasets and focused on codebert and graphcodebert.
the pretraining dataset codesearchnet for these llms is publicly available on huggingface3 and it is fully traceable.
for clone detection we used bigclonebench with its official training validation and test splits4 these splits are specifically designed to prevent data leakage.
we used the devign dataset for vulnerability detection which consists of vulnerable c methods.
importantly codesearchnet does not include c code5.
this guarantees no overlap between this dataset and the pretraining data of the two llms fully eliminating any risk of data leakage.
furthermore we manually checked that neither the pretraining nor the validation datasets contained any test snippets or their metamorphic variations.
additionally as argued in metamorphic testing helps mitigate the risk of data leakage in pre training by altering original code snippets to create new semantically equivalent variants.
these transformations guarantee that even if a model has been exposed to similar search net clone detection bigclonebench defect detectionpatterns during pretraining the metamorphic variants provide novel instances that challenge the model s robustness without introducing label inconsistencies.
concerning external validity we used the devign and bigclonebench datasets widely employed in the literature for training ai models for code related tasks including model distillation .
m orph was assessed on two code related tasks and two llms codebert and graphcodebert.
future work will extend the evaluation to other datasets tasks and llms to corroborate our results.
threats to conclusion validity arise from the randomized nature of m orph and a vatar .
we ran each approach ten times for each llm and task using different random seeds following best practices for experiments with randomized algorithms .
we analyzed the results using the wilcoxon rank sum test and the vargha delaney statistics .
metrics used included accuracy model size and flops common in knowledge distillation .
the number of prediction flips was used to assess robustness against metamorphic code a standard practice in the testing literature .
viii.
c onclusion and future work this study investigates the robustness of models compressed with the state of the art distillation method called a vatar to metamorphic code variations.
our findings reveal a significant increase in prediction flips up to for compressed model compared to their original counterparts when presented with semantically and behaviorally equivalent code snippets.
to address this issue we introduced m orph a novel distillation method that integrates metamorphic testing with many objective optimization using age moea .
our approach considers the model robustness as a key objective to optimize measured as the number of prediction flips fps between metamorphic variants of the same code snippets.
the other objectives are accuracy model size measured in mb and inference efficiency sustainability measured in flops .
we conducted an empirical study on two tasks clone detection and vulnerability detection using codebert and graphcodebert.
the results showed that m orph consistently produced more robust up to fewer pfs and efficient models up to fewer flops compared to avatar with equal or higher accuracy and similar model size.
our ablation study confirm the importance and relevance of all m orph s key feature on its overall performance.
future research will further corroborate our findings generalizability by considering more code related tasks and llms such as codet5 .
we intend to design more metamorphic transformations that preserve the naturalness of developers code and investigate their impact on model robustness.
we plan to integrate further compression techniques such as quantization and pruning to improve the sustainability and robusteness of llms and of their students models.