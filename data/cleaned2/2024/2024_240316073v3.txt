combining fine tuning and llm based agents for intuitive smart contract auditing with justifications wei ma1 daoyuan wu2 yuqiang sun1 tianwen wang3 shangqing liu1 jian zhang1 yue xue4 yang liu1 1nanyang technological university singapore singapore 2the hong kong university of science and technology hong kong sar china 3national university of singapore singapore singapore 4metatrust labs singapore singapore ma wei ntu.edu.sg daoyuan cse.ust.hk suny0056 e.ntu.edu.sg tianwenw.vk gmail.com liu.shangqing ntu.edu.sg jian zhang ntu.edu.sg xueyue metatrust.io yangliu ntu.edu.sg abstract smart contracts are decentralized applications built atop blockchains like ethereum.
recent research has shown that large language models llms have potential in auditing smart contracts but the state of the art indicates that even gpt can achieve only precision when both decision and justification are correct .
this is likely because off the shelf llms were primarily pre trained on a general text code corpus and not finetuned on the specific domain of solidity smart contract auditing.
in this paper we propose iaudit a general framework that combines fine tuning and llm based agents for intuitive smart contract auditing with justifications.
specifically iaudit is inspired by the observation that expert human auditors first perceive what could be wrong and then perform a detailed analysis of the code to identify the cause.
as such iaudit employs a two stage fine tuning approach it first tunes a detector model to make decisions and then tunes a reasoner model to generate causes of vulnerabilities.
however fine tuning alone faces challenges in accurately identifying the optimal cause of a vulnerability.
therefore we introduce two llm based agents the ranker and critic to iteratively select and debate the most suitable cause of vulnerability based on the output of the fine tuned reasoner model.
to evaluate iaudit we collected a balanced dataset with positive and negative samples to fine tune iaudit.
we then compared it with traditional fine tuned models codebert graphcodebert codet5 and unixcoder as well as prompt learning based llms gpt4 gpt .
and codellama 13b 34b .
on a dataset of real smart contract vulnerabilities iaudit achieves an f1 score of .
and an accuracy of .
.
the causes generated by iaudit achieved a consistency of about compared to the ground truth causes.
one of the big skills in bug bounties that s really difficult to teach is intuition.
everything i do i am following my intuition.
it s what looks interesting and what doesn t look right.
katie paxton fear one of the million dollar earning hackers .
i. i ntroduction smart contracts have emerged as a key application based on blockchain technology since the advent of ethereum.
due to their openness transparency and irreversibility smart corresponding author daoyuan wu.
work conducted while at ntu.contracts have become the foundation of decentralized financial applications defi .
however since defi manages a significant amount of digital assets identifying and fixing vulnerabilities in smart contracts is crucial.
currently the real vulnerabilities exploited by hackers in smart contracts are mainly due to logical flaws which render traditional pattern based program analysis less effective.
according to defillama hacks vulnerability attacks have caused losses of around .
billion as of march .
hence there is an urgent need for innovative methods to combat these emerging threats.
recent research has shown that large language models llms have potential in auditing smart contracts especially in demonstrating superior performance in detecting logic vulnerabilities .
however a recent systematic evaluation study shows that even when equipping the llm based vulnerability detection paradigm with a state ofthe art approach namely enhancing gpt with summarized vulnerability knowledge in a retrieval augmented generation rag fashion it still achieves only precision when both the decision i.e.
whether the subject code is vulnerable and justification i.e.
pinpointing the correct vulnerability type are correct.
this can be attributed to the fact that off the shelf llms e.g.
gpt which were primarily pre trained on a general text code corpus were not fine tuned for the specific domain of solidity1smart contract auditing.
fine tuning could be a promising approach to embed solidity specific vulnerability data into the model itself compared to rag and thus address the problem mentioned above.
in particular by fine tuning an llm with vulnerable and non vulnerable code it could effectively perceive whether a new piece of code is vulnerable or not.
according to insights from a million dollar earning hacker mentioned in the prologue such intuition is quite important for vulnerability auditing.
as such instead of fine tuning a single model to generate both vulnerability decisions i.e.
yes or no and the causes of vulnerabilities i.e.
the type or reason simultaneously we propose a novel two stage finetuning approach.
this approach first tunes a detector model 1solidity is a mainstream language for smart contract development.arxiv .16073v3 sep 2024to make decisions only and then tunes a reasoner model to generate the causes of vulnerabilities.
in this way the finetuned llms could mimic human hackers by first making intuitive judgments and then performing follow up analysis of the code to identify the reasons for vulnerabilities.
we implement this perception then analysis fune tuning into a general framework called iaudit2for intuitive smart contract auditing.
in this implementation iaudit allows detector to make multiple intuitive judgments each representing one perception.
to achieve this iaudit generates multiple variant prompts for the same vulnerability label to tune detector and similarly employs multiple variant prompts for the same vulnerability reason to tune reasoner.
while it is possible to determine the optimal decision based on majority voting fine tuning alone cannot identify the optimal cause for a vulnerability during the inference phase .
to address this new problem we introduce the concept of llm based agents to the paradigm of fine tuning in iaudit.
specifically we introduce two dedicated llm based agents the ranker and critic agents to iteratively select and debate the most appropriate cause of vulnerability based on the output of the fine tuned reasoner model.
to obtain high quality data for training and testing iaudit we propose leveraging reputable auditing reports to collect positive samples and employing our own data enhancement method to derive negative samples.
eventually we collected a balanced dataset consisting of positive samples i.e.
vulnerable functions with reasons from smart contract auditing reports and negative samples i.e.
nonvulnerable benign code.
we then compared iaudit with traditional full model fine tuning methods including codebert graphcodebert codet5 and unixcoder as well as with prompt learning based llms such as gpt gpt .
and codellama 13b 34b.
our experimental results show that iaudit achieved an f1 score of .
significantly outperforming prompt learning based llms which are in the range of and also notably beating other fine tuned models which are in the range of that used the same training data as ours.
furthermore in terms of alignment with groundtruth explanations iaudit s output is clearly superior to that of other models reaching a consistency rate of .
.
in contrast the second ranked gpt achieves only .
besides the evaluation results we also conducted three ablation studies to further justify iaudit s two stage fine tuning and majority voting strategies as well as to measure the impact of additional call graph information on the model s performance.
we summarize the key findings as follows iaudit s two stage approach achieved better detection performance than the integration model which outputs labels and reasons simultaneously.
we also experimentally confirmed that the model struggles to focus on the labels when required to output both types of information.
2iaudit is deployed as an auditing module of metatrust labs trustllm see voting enhances the detection performance and stability.
using multiple prompts also allows the model to perform better than when using a single prompt.
call graph information may enable the model to make better judgments in some cases but we also observed situations where this additional information could potentially confuse the model thereby reducing its performance.
roadmap.
the rest of this paper is organized as follows.
we first introduce the relevant background in sec.
ii followed by the design of iaudit in sec.
iii.
we then present our experimental setup and the results in sec.
iv.
after that we discuss related work and the limitations in sec.
v and sec.
vi respectively.
finally sec.
viii concludes this paper.
availability.
to facilitate future research and comparison we have made the inference code and dataset available at .
ii.
b ackground a. pre trained models and large language models pre trained models are models that have been initially trained on large datasets.
these models can be quickly adapted to various specific tasks with minimal adjustments avoiding the complex training process from scratch.
currently most pre trained models adopt an architecture based on transformers .
the innovation of this approach is that pretrained models leverage large data and well designed tasks for effective feature learning which has been proven effective in multiple fields such as text processing image recognition and software engineering.
the standard transformer structure consists of one encoder and one decoder which are structurally similar but function differently.
pre trained models can be classified into encoder based decoder based or encoderdecoder combined types depending on the transformer structure used.
for example encoder based models are represented by bert and codebert decoder models by the gpt series and encoder decoder models by bart t5 and codet5 .
compared with general pre trained models large language models llms differ significantly in their used larger data and model scales.
these models are trained by learning world wide knowledge bases typically reaching billions in scale.
as the model size data volume and computational capacity increase performance also improves as revealed by the scaling laws .
closed source llms like gpt .
gpt and gemini offer their services externally through apis while open source models like llama2 can achieve performance comparable or better to closed source models after fine tuning.
b. parameter efficient fine tuning llms have extremely large parameters.
fully fine tuning a large language model requires significant hardware resources and is very costly.
therefore lightweight parameter finetuning is currently the main method of using llms compared to fully fine tuning them.
although llms can be used without task specific fine tuning through in context learning this usually requires carefully prepared prompts.
furthermore research has found that partial fine tuning ofllms with smaller parameters can achieve or even surpass the effects of huge models .
these fine tuning methods differ from full model fine tuning by focusing only on finetuning additional parameters while keeping the large model weights fixed known collectively as parameter efficient finetuning .
they can be generally categorized into four types adapter low rank adaptation lora prefix tuning and prompt tuning .
adapter adds a lightweight additional module to each layer of the model to capture information specific to downstream tasks.
during optimization only the parameters of the additional module are optimized.
since the number of parameters in the adapter is much smaller than that of the model itself it significantly reduces the overall parameter count and computational complexity of the model.
low rank adaptation lora is a parameter efficient adaptation method for llms which adjusts llms for downstream tasks at a lower parameter cost.
the core idea of lora is to introduce additional low rank adaptation parameters into the self attention mechanism effectively adjusting the model to suit new tasks with minimal addition of extra parameters.
prefix tuning adds a prefix sequence to each layer of the model serving as additional context input.
this method allows the model to adapt to specific tasks while retaining most of the knowledge acquired during pre training.
unlike prefix tuning prompt tuning adds prompt tokens to the input which can be placed at any position.
to sum up using adapters can increase inference latency .
prefix or prompt tuning is subject to structural constraints that inhibit the learning of new attention patterns .
lora is an efficient method with low cost and can have a performance close to the full fine tuning approach .
c. smart contracts and their vulnerabilities smart contracts are essential for realizing decentralized finance as an application layer of blockchain technology.
according to data from defillama as of march the total value locked in the top three blockchain platforms ethereum tron and bsc has reached billions.
given the close relationship between smart contracts and economic interests their security has attracted widespread attention.
vulnerabilities in smart contracts can lead to significant losses such reentrancy attacks and access control attacks .
in the real world hackers employ even more complex tactics.
currently to address vulnerabilities in smart contracts various static and dynamic detection tools are used to test contract security.
unfortunately some complex vulnerabilities are hard to be found by these detection tools.
for example in a sandwich attack attackers monitor other pending transactions and execute their transactions first upon spotting a high value yet uncompleted transaction.
due to this preemptive action the attack transaction will be executed at a higher price allowing the attacker to immediately sell the acquired excess profit for profit.
many well funded project teams also invite third parties to audit their smart contracts before public release to ensure their safety.iii.
d esign of i audit as motivated in sec.
i iaudit employs a novel two stage fine tuning approach and combines it with llm based agents for intuitive smart contract auditing with justifications.
as shown in fig.
iaudit has the following four roles detector is the key component for achieving intuitive smart contract auditing.
by fine tuning an llm with vulnerable and non vulnerable code detector can discern whether a piece of code is vulnerable much like how a human hacker perceives a potential vulnerability.
reasoner takes the initial vulnerability perception from detector to further analyze the potential causes of the vulnerability based on detector s decision.
by connecting detector s output with reasoner s reasoning during both training and inference iaudit achieves two stage finetuning.
to identify the optimal cause of a vulnerability during the inference phase we further introduce the concept of llm based agents into the fine tuning paradigm in iaudit.
specifically ranker evaluates the reasons for each potential vulnerability selecting a top explanation while critic further assesses ranker s output to debate and determine the most appropriate cause of the vulnerability.
challenges.
while iaudit s four roles in fig.
are intuitive training and coordinating them well for effective smart contract auditing with reasonable justifications is difficult.
more specifically we encountered the following four challenges during the design and implementation of iaudit c1 how to collect and derive high quality training data?
for a fine tuned model like iaudit obtaining highquality training data is always crucial.
we propose leveraging reputable auditing reports to collect positive samples and employing our own data enhancement method to derive negative samples.
since this part is independent of iaudit s design we defer its presentation to the end of this section in sec.
iii d. c2 how to make effective vulnerability judgements?
while fine tuning a model with vulnerable and nonvulnerable code is straightforward tuning it to be effective with limited data presents a challenge.
we make an effort towards addressing this problem in sec.
iii a by opting to use multiple prompts for finetuning rather than a single prompt.
the advantages of this approach are twofold i it enriches the training dataset by increasing the volume of data and ii it diminishes the bias associated with a single prompt thereby enhancing the reliability of the results .
optimal vulnerability perception could thus be achieved through majority voting.
c3 how to effectively connect detector s vulnerability sensing with reasoner s vulnerability reasoning?
the fine tuning of iaudit is unique because it employs a two stage fine tuning approach with the detector and reasoner models.
therefore how to effectively connect these two models becomes a newdetectorreasonerrankercriticrole smell if the code is vulnerable.role find the candidate reasons that support the detection.role considering the constraints choose the best one.role comment the output of the ranker and give the next action agree reranking and merging agreelora fine tuning answer i think the reason is the best because ..... i give it score .
answer based on the feedback the reason is the most relevant because ...... i give it score .
feedback i do not agree because ..... please re rank.
feedback i agree with your answer because ....... add the feedback and the previous answer to the prompt majority votinginference path code inputmultiple prompt1.
the issue with the redeemparams function is that it does not return any... ...... .
the issue with the redeemparams function in the jbxbuybackdelegate is ... voter 1voter ...voter 1fig.
an overview of iaudit featuring its four roles detector reasoner ranker and critic.
issue not encountered in traditional fine tuning.
we present this aspect of iaudit s design in sec.
iii b. c4 how to obtain the optimal vulnerability cause from reasoner s output?
since reasoner also employs multi prompt fine tuning it is necessary to identify the optimal cause of vulnerability among the multiple causes output by reasoner.
we introduce two llmbased agents namely the ranker and critic components in sec.
iii c to iteratively select and debate the most appropriate cause of vulnerability.
an example of workflow.
to wrap up fig.
also illustrates an example of iaudit s workflow.
initially detector perceives code vulnerabilities using five different inference paths prompts .
the perceived results are then subjected to majority voting to determine a consensus label.
based on the voting result reasoner interprets this outcome according to different inference paths resulting in ten answers each considering the context of the code location or not .
next ranker selects reason with a confidence score and explains this choice.
critic challenges this choice and advises ranker to re evaluate.
taking critic s feedback into account ranker re ranks the ten reasons and selects reason with a confidence score of .
critic reviews ranker s choice again and agrees with this decision.
the loop is completed and the final reason is returned to the user.
a. using multi prompt tuning and majority voting for effective vulnerability judgements in the detector detector is a fine tuned expert model responsible for assessing whether code poses any risk.
it mimics human intuitive judgment upon seeing a piece of code assessing whether there are any issues.
we employed lora to fine tune codellama 13b in the instruction manner based on a high quality of dataset.
during training for the same input code we wrap it with multiple prompts.
these prompts with different instruction formats represent the different inferencedetector s prompt template below is an instruction that describes a classification task.
instruction input solidiy code response the label is label name .
fig.
the prompt template used by detector.
paths as illustrated in fig.
.
in the inference phase of detector based on the output results of each prompt we adopt a majority voting approach to determine the input label and use the voting ratio as the confidence score.
based on detector s majority voting result reasoner in sec.
iii b then generates different reasons according to different inference paths.
it is worth noting that for the choice of the base model we randomly selected real logical vulnerabilities to evaluate three popular open source models starcoder llama2 and codellama.
upon manual review we found that starcoder sometimes refused to respond and llama2 provided one contradictory response with two labels.
in contrast codellama offered a more stable response which led us to choose codellama as the foundational model for further fine tuning.
the prompt template used by detector is demonstrated in fig.
.
above the dashed line is the input xfor our model.
code is the placeholder for the input code.
below the dashed line the label is label name is our target training output y with label name being the label placeholder which can be either safe and vulnerable.
the left ta ble detector s multiple prompts in fig.
details the task description and listed as notations a b and c respectively.
we fine tune codellama13b using lora in a generative manner as shown in eq.
l tx t 1logp yt x y t where represents the lora trainable parameters tis the output sequence length and p yt x y t is the probability of the model with parameters generating a token ytgiven the context xand all previous tokens y t. in this generative approach the output at the time step tis conditioned only on the previous time steps t .
after fine tuning during inference the proposed input follows the training format and we need to extract labels from the output via keyword matching using safe and vulnerable.
since we employ multiple prompts we obtain multiple label predictions and use majority voting to decide the final predicted label.
in majority voting each inference path casts a vote for one of the available labels safe or vulnerable denoted as l0andl1 respectively.
the label that receives more than half of the total votes is declared the winner.
let l l0 l1 represent the set of labels and v v1 v2 .
.
.
v m represent the set of the prompt voter.
each prompt voter vicasts a vote for one label.
the winning labelliis the one of l0andl1for which the following condition holds v v vote v li m .
this condition asserts that the winning label owmust receive more than half of the total votes m. we also use the voting ratio of the winning label as the confidence score for the final decision.
b. connecting detector for reasoner s tuning inference reasoner is an expert model responsible for reasoning about and explaining code vulnerabilities.
it interprets the majority voting result of the detector generating multiple alternative explanations.
during the reasoner s lora fine tuning process our inputs include the code its context corresponding labels and we construct zero shot chain of thought cot prompts with different command formats for training.
in the inference phase reasoner outputs multiple explanations based on the majority voted label from detector.
we constructed two types of prompts the first type includes the label code information and its function call relationships the second type includes only the label and code information.
in sec.
iv we will investigate the impact of including function call relationships or not.
for each type we designed five different instruction formats for the prompts totaling ten inference paths as illustrated in fig.
.
the prompt template used by the reasoner is shown in fig.
.
code caller info callee info and target reason are placeholders for the input code caller context callee context and the target output respectively.
the right table reasoner s multiple prompts in fig.
details the first prompt type with calling context including denoted as a denoted as b inputreasoner s prompt template below is an instruction that describes a reasoning task.
instruction input solidiy code as a caller optional caller info as a callee optional callee info response target reason fig.
the prompt template used by reasoner.
description denoted as c and callee description denoted as d and denoted as e. for the second prompt type the calling context is omitted.
reasoner employed the same finetuning method as detector as shown by eq.
.
during inference the proposed input prompt follows the training format and reasoner generates ten answers to interpret detector s assessment.
c. ranking and debating the optimal vulnerability cause ranker and critic are two llm based agents collaborating to select the most appropriate cause of vulnerability from multiple explanations returned by reasoner for a given code function.
ranker performs two actions rank and merge .
rank involves selecting the best explanation from the ones provided while merge involves integrating multiple selected explanations.
we define constraints for ranker to select the top explanation.
critic evaluates ranker s answer in conjunction with the code function providing three next step action instructions agree rerank and merge .
agree means the current answer is reasonable and can be returned to the user.
rerank indicates that ranker needs to re select considering critic s feedback and previous answers.
merge suggests that the top reasons provided must be integrated.
more specifically ranker employs the following constraints in the prompt as its selection criteria if one reason describes code that does not exist in the provided input it is not valid.
if one reason is not related to the code the reason is not valid.
if this reason violates the facts the reason is unreasonable.a.devise a label name suitable for categorizing items as either vulnerable or safe.
b.please review the code.
please find out if it is vulnerable.
c.the function fn name from the contract contract name .a.suggest a label designation that clearly identifies an item s status as either vulnerable or safe.
b.inspect the following solidity code.
determine if there are any vulnerabilities present.
c.observe the method fn name within the smart contract contract name .a.invent a naming label that aptly segregates items into vulnerable or safe classifications.
b.examine this solidity script.
identify any potential security risks.
c.review the function fn name in the blockchain contract contract name .a.formulate a label descriptor that bifurcates objects into categories of vulnerable and safe.b.
please assess the provided solidity code for any security vulnerabilities.
c.check the procedure fn name in the digital contract contract name .a.propose a label nomenclature that aptly differentiates between vulnerable and safe states.
b.evaluate the given solidity function.
are there any security flaws?
c.inspect the subroutine fn name from the decentralized contract contract name .a.
examine the underlying factors and suggest a reason given the label name.
b.please review the code and its calling relationships as the caller and the callee.
given the label name please find out the reason.
c.the function fn name from the contract contract name .
d. fn name calls these functions.
fn name is called by these functions.
e. the input code is label name .
please state the reason.
let s think step by stepa.carefully assess the contributing factors and their interplay.
utilize the label name to form a coherent reasoning.
b.please analyze the code function and its dependencies including both incoming and outgoing calls.
considering the label name identify the underlying cause.
c.the method fn name in the smart contract contract name .d.
functions called by fn name .
functions calling fn name .
e.given that the code is labeled label name let s determine the reason by breaking down the process.a.delve into the core aspects and their significance.
use the label name to draw an informed inference.
b.examine the code s logic flow.
based on the label name deduce the primary reason.
c.the method fn name in the smart contract contract name .
d.d.
external routines invoked by fn name .
routines that invoke fn name .
e. with label name as the code s label let s systematically uncover the rationale.a.scrutinize the main factors and deduce a reason in light of the label name.
b.evaluate the code s connections and its purpose within the system.
using the label name infer the main reason.
a.code segment fn name from the blockchain contract contract name .
b.functions triggered by fn name .
functions that trigger fn name .c.considering label name as the designated label let s sequentially analyze the reason.a.explore the root causes and provide a justification considering the assigned label name.
b.investigate the role and relationships of the code.
utilizing the label name propose a probable reason.
c.procedure fn name in the decentralized application contract name .
operations executed by fn name .
d. operations that execute fn name .
e.recognizing label name as the code s label let s logically deduce the reason.12345detector s multiple promptsreasoner s multiple prompts for the type including function call relationships 12345fig.
detailed multiple prompts for detector and reasoner.
if one reason is not related to the decision the reason is not valid.
if one reason assume any information that is not provided the reason is not valid.
if the code is safe and one reason supports the decision please check if the code has other potential vulnerabilities.
if the code has other potential vulnerabilities the reason is not valid.
the selected reason should be the most relevant to the decision.
the selected reason must be the most reasonable and accurate one.
the selected reason must be factual logical and convincing.
do not make any assumption out of the given code.
both ranker and critic are llms agents implemented based on the mixtral 8x7b instruct model the capability of which is close to that of larger llms .
moreover we have observed that the mixture of experts moe model can more effectively output data in the predetermined format than other models making it easier for us to handle the output.
d. high quality training data collection and enhancement the quality of training data is crucial for fine tuning llms.
to collect positive samples namely risky vulnerability code we can employ auditing reports from reputable industry com panies such as trail of bits code4rena and immunefi.
specifically we crawled and parsed vulnerable functions with reasons from smart contract auditing reports which were assembled by a popular auditing website called solodit .
however to train our model we also need non vulnerable benign code i.e.
negative samples but this type of data is missing in the audit reports.
therefore we propose our own data enhancement method to derive high quality negative samples.
specifically we adopt the gpt based approach described in llm4vuln to extract vulnerability knowledge from vulnerability reports on code4rena.
this includes the functionality descriptions of vulnerable functions and the code level reasons why the vulnerabilities occur.
we then cluster this raw vulnerability knowledge based on the functionality descriptions into groups using affinity propagation as described in and use gpt to summarize a functionality description for each group.
with the hierarchical information of group functionality individual functionality and vulnerability negligence we employ the hierarchical gptbased matching i.e.
matching the group first then matching functionality and negligence in gptscan to obtain the label information for tested code.
a function is labeled as a negative sample if no vulnerability information matches.
all prompts used are from llm4vuln and gptscan.
eventually we collected a balanced dataset with positive samples and negative samples.
in this dataset vulnerable functions have a median of .
lines of code andowner can steal concur rewards titleimpactowner can steal concur rewards by adding a depositor and inflating other depositors assigned balance of the token within the contract.
thus the owner managed depositor can get most all but one wei of the created tokens.handle claims for reward tokens.functionalitynegligencenot checking if the claimed epoch is within the reward epochs range.detailsthe vulnerability described is related to the manipulation of internal accounting within the ..... .the owner calls adddepositor with a malicious address.
.
......pocimplementing additional checks within the adddepositor and deposit functions to ensure that only legitimate updates to user balances and deposits are allowed....recommendationcode contextfig.
data enhancement for expanding vulnerability explanations based on gpt .
.
a complexity of .
while safe functions have a median of .
lines of code and a complexity of .
.
the complexity distributions between the two types are somewhat similar but with a slight difference indicated by a kullback leibler kl divergence value of .
.
this dataset was divided into training validation and test subsets containing and entries respectively.
during training we use the training and validation sets.
during testing we use the test set.
after collecting the labeled and unlabeled data we also obtained corresponding explanations for the vulnerabilities.
however the quality of these vulnerability justifications varies considerably.
furthermore some data contain external links which may cause the model to hallucinate and output nonexistent links.
to improve the interpretability of the reasons behind the vulnerabilities in the dataset we used gpt .
to enhance the existing explanations expanding on the explanations of the vulnerabilities proofs of concept poc and recommended fixes.
fig.
shows an example where the left part presents the original reasons for the vulnerability which are short and lack detail.
we thus use them as prompts along with the code context to instruct gpt .
to generate more detailed descriptions including the poc and the mitigation recommendation.
note that we chose gpt .
mainly because it achieves a good balance between cost and effectiveness much cheaper than gpt yet comparable in quality for this specific task during our manual comparison.
we also added constraints to our prompts during this process to ensure that the enhanced explanations aligned with the actual vulnerabilities.
iv.
e valuation a. experimental setup in our study we carefully selected a series of benchmark models categorized into two groups llms for zero shot learning and pre trained code models based on fine tuning to ensure a comprehensive and sound comparative analysis.
for zero shot learning llms we chose codellama 13binstruct codellama 34b instruct gpt .
and gpt as benchmarks representing the current state of the art.
addi tionally we selected codebert graphcodebert codet5 unixcoder and codellama 13b to train classifiers.
among these codebert graphcodebert codet5 and unixcoder underwent a complete model finetuning process to adapt to the specific code classification task.
in particular codellama 13b employs lora for lightweight tuning and uses the last token representation for classification.
note that that our method is different iaudit s detector achieves classification by generating label names as task outputs.
b. research questions rqs since our proposed method comprises two core functions vulnerability detection and reason explanation we designed a series of experiments to evaluate and demonstrate the performance and effectiveness of both tasks.
these experiments aim to answer the following research questions rqs a rq1 performance comparison how does the performance of iaudit in detecting vulnerabilities compare to other models?
this question aims to understand how the effectiveness of detector in detecting vulnerabilities compares to that of other existing models.
the focus is on comparative analysis involving metrics accuracy precision recall and f1 score to evaluate and contrast the performance.
b rq2 explanation alignment to what extent do the explanations generated by iaudit s reasoner align with the real reasons?
rq2 concerns the quality of the explanations the reasoner provided for the decision of the detector.
it questions whether the reasons given by iaudit correspond to the actual reasons behind the vulnerabilities emphasizing the interpretability and trustworthiness of the model.
c rq3 two stage approach vs. an integration model how does iaudit compare with an integration model that performs detection and reasoning simultaneously?
our method is based on a generative model with two models trained on the generated labels and reasons respectively.
another approach uses a single model to generate both reasons and labels.
this question explores the effectiveness and impact of integrating the detector and reasoner components into one.
d rq4 effectiveness of majority voting can majority voting improve the effectiveness of the detector?
rq4 investigates whether the effectiveness of the detector can be improved by adopting a majority voting mechanism.
majority voting a technique that makes the final decision based on the majority output of multiple models may improve the robustness and accuracy of the method.
e rq5 impact of additional information the call graph illustrates the interaction of code with other components within the project which is expected to be advantageous for our task.
we address two specific research sub questions rq5.
.
can the call graph enhance the detector performance?
rq5.
.
in what way does the call graph influence our explanation generation process specifically within the reasoner ranker critic pipeline?table i performance comparison between iaudit s detector and zero shot llms.
f1 recall precision accuracy gpt .
.
.
gpt .
.
.
.
codellama 13b .
.
.
.
codellama 34b .
.
.
.
iaudit .
.
.
.
table ii performance comparison between iaudit s detector and other fine tuned models.
f1 recall precision accuracy codebert .
.
.
.
graphcodebert .
.
.
.
codet5 .
.
.
.
unixcoder .
.
.
.
codellama 13b class .
.
.
.
iaudit .
.
.
.
besides the rqs above we also used our model to audit two bounty projects currently anonymous on code4rena.
we invited audit experts to verify our findings.
in the end we found critical vulnerabilities which were recognized by the project team or audit experts.
in particular one vulnerability was not discovered by any tools marked as a great finding.
this demonstrates the real world value of iaudit.
due to page limitations we have included these case studies in the supplementary materials for interested readers.
c. rq1 performance comparison firstly we compared iaudit with llms based on zeroshot learning as shown in table i. our method also uses a zero shot approach during the inference phase.
we considered two proprietary models gpt and gpt .
and three opensource models codellama 13b and codellama 34b .
for the open source models we strictly adhered to their prompt formats.
huggingface transformer has integrated these prompt formats into its framework.
the format conversion is completed by calling apply chat template .
codellama requires adding and as well as special tags sys and sys .
as shown in table i after fine tuning our proposed strategy significantly outperforms the baseline models in the zero shot scenario in terms of f1 accuracy and precision achieving high scores of .
.
and .
respectively.
however when examining the recall we notice that the baseline models all performed excellently.
notably gpt and gpt .
achieved a recall score of .
we checked the confusing matrix and found that all test data are labelled by the vulnerability.
for gpt and gpt .
we adopted the prompts which are provided by our industrial partner metatrust labs a web3 security company.
secondly we compared detector with fine tuned models in detecting vulnerabilities using f1 recall precision and accuracy as evaluation metrics as shown in table ii.
we compared our method with codebert graphcodebert codet5 unixcoder and codellama 13b class.
codebert graphcodebert codet5 and unixcoder underwent full model fine tuning.
these traditional pre trained models use the first token of the input sequence as the feature input for the classifier.
codebert is based on the transformer encoder.
graphcodebert has the same architecture as codebert but includes additional pre training on data dependency relations.
codet5 utilizes the transformer encoder and decoder adopting an architecture similar to t5 .
unixcoder unifies the encoder and decoder architecture controlling the model behaviour through a masked attention matrix with prefix adapters.
codellama 13b class performs classification based on lora.
we fine tuned codellama 13b class for lora classification using the peft framework.
codellama 13bclass uses the representation of the last token of the input sequence as the feature input for the classifier.
as shown in table ii iaudit achieves the highest scores of f1 recall and accuracy among all methods .
.
and0.
.
codellama 13b class is second only to our method regarding vulnerability detection rate and the performance is relatively close.
graphcodebert and unixcoder perform worse than codellama 13b class.
although codet5 achieves the highest precision at .
its other metrics are lower than graphcodebert and unixcoder.
codebert has the worst performance.
additionally the accuracy scores of these models are relatively high all are more than .
indicating that many of the predicted risky vulnerabilities are indeed risky.
regarding precision iaudit produce more false positives than codet5 graphcodebert and codebert.
this is primarily because iaudit uses a longer context length 16k compared to these models whose length context truncation causes the vulnerable and safe samples to align more closely in length thus affecting the apparent code complexity.
however our dataset shows that vulnerable code is relatively more complex than safe code.
this leads iaudit to output unique false positives with more complex code.
answer for rq1 the performance of iaudit s detector exceeds that of traditional full model fine tuning lora fine tuning in a classification manner and llms based on in context learning.
the performance of fine tuned models is also better than that of zero shot learning.
d. rq2 explanation alignment to measure the effectiveness of reasoner in explaining vulnerabilities we compared the consistency between the explanations we generated and the root causes.
given the llm s outstanding performance in interpreting textual meaning we used gpt to verify whether our generated explanations align with the root causes.
for this consistency assessment we employed automated annotation prompts from y .
sun et al.
.
the results of our consistency test are depicted in fig.
where the y axis represents the percentage of our explanations in the test set that match the root causes.
our method significantly outperformed the baseline methods achieving a consistencycodellama 13b codellama 34b gpt3.
gpt4 iaudit0.
.
.
.
.
.
.
.0percentage .
.
.
.
.
fig.
comparing the alignment with ground truth reasons.
f1 recall precision accuracy0.
.
.
.
.8integration label end integration label first iaudit fig.
comparing iaudit with the integration models that make decisions and explain the vulnerabilities simultaneously.
rate of .
while no baseline method exceeded .
among these baselines gpt performed the second best with .
consistency.
additionally the results also indicated that codellama 13b had the weakest performance.
answer for rq2 the rationality of reasoner s output is clearly superior to that of other models.
on the test set its consistency with real reasons reaches .
which is over higher than the second ranked gpt .
e. rq3 two stage approach vs. an integration model our research methodology involves vulnerability detection and explanation executed in two stages.
we trained two models i.e.
detector and reasoner based on a generative approach to perform these tasks on their respective highquality datasets.
a question arises whether these two tasks can be merged and trained simultaneously in a single model.
in response we developed an integration model that generates labels and explanations for the vulnerabilities comparing it to our two stage approach.
the integration model uses prompts similar to those of reasoner with additional requirements to output the label.
we explored two integration training approaches generating labels first then explaining the reasons explaining the reasons first then generating labels.
the results as shown in fig.
indicate that our step wise approach outperforms the integration models across four key performance metrics.
while the three methods are similar in precision the differences in other metrics are notable.
analyzing these results we found that the integration methods havehigher accuracy for negative samples but a lower detection rate for positive samples i.e.
lower recall .
this may be attributed to the generative loss optimization where the output sequence is longer making the label related loss occupy a smaller proportion of the total loss thus preventing the model from adequately focusing on the label.
to test this hypothesis we added data that includes only label generation to the dataset during the integration training process guiding the model to focus more on the label.
in the evaluation phase we still required the model to output both labels and explanations simultaneously.
through this mixed training approach we observed a significant improvement in the model s vulnerability detection performance with an f1 score of .
a recall rate of0.
a precision of .
and an accuracy of .
.
answer for rq3 iaudit achieved better detection performance than the integration model that outputs labels and reasons simultaneously.
we confirmed that the model struggles to focus on the labels when required to output both types of information as evidenced by our inclusion of label only data in the verification process.
table iii majority v oting vs. single prompt.
f1 recall precision accuracy single prompt .
.
.
.
prompt .
.
.
.
prompt .
.
.
.
prompt .
.
.
.
prompt .
.
.
.
prompt .
.
.
.
iaudit .
.
.
.
f .
rq4 effectiveness of majority voting our research explored a method using multiple prompts and a voting mechanism for detector to determine the final label.
this method aims to enhance the model s precision and credibility.
during the evaluation process we continued to use metrics such as the f1 score recall precision and accuracy.
we calculated these metrics for each prompt individually for comparative analysis as shown in table iii.
it should be noted that the first row single prompt indicates that we used only one prompt format to train detector.
prompt prompt prompt3 prompt and prompt represent the results for each prompt after multiple prompt training.
the last row shows the results after majority voting indicating that majority voting can improve the overall performance of iaudit with both the f1 score and accuracy being the highest.
at the same time except for single prompt we noticed minimal performance differences among multiple prompts.
single prompt performed much worse than the others.
training with multiple prompts can improve model performance compared to using only one prompt during training.
additionally we divided the test set into two groups based on whether the predictions were correct or incorrect named95 score score .
score .
a .
correct predictionb .
wrong predictionfig.
the distribution of v oting scores for correct predictions and wrong predictions.
correct prediction and incorrect prediction groups respectively and analyzed the distribution of confidence scores within these two groups.
we found that in the incorrect prediction group the proportion of confidence scores within the range of .6to0.8is significantly higher than in the correct prediction group vs vs respectively as shown in fig.
.
the confidence score can reflect the reliability of the prediction results to a certain extent.
when the confidence score is low the prediction results are less credible.
answer for rq4 majority voting enhances the detection performance and stability.
additionally using multiple prompts allows the model to perform better and be more reliable than when using a single prompt.
g. rq5 impact of additional information we explored whether introducing additional call graph information into the model could enhance its performance.
we added function call relationships to the prompts as contextual information.
a rq5.
through comparative experiments as shown in table iv we found that this calling contextual information did not improve the model s overall performance.
in the second row call we used the prompts with the calling information and then employed majority voting to decide the prediction result.
for the third row call outcall we used prompts both with and without calling information and also used majority voting.
compared with iaudit s detector they exhibited lower precision accuracy and f1 with almost the same recall.
b rq5.
fig.
demonstrates the selected reason distribution from ranker critic.
we can see that the majority of the selected reasons are from the prompts with calling information while there is still a high ratio of selected reasons from the prompts without calling information.
although function call relationships provide more information this information does not always help the model better complete the current task.
in some cases this information may cause interference making it difficult for the model to identify critical information thereby resulting in more false positives and affecting performance.
furthermore not all function call relationships are practically valuable.
if these additionaltable iv impact with or without additional information.
f1 recall precision accuracy call .
.
.
.
call outcall .
.
.
.
iaudit .
.
.
.
.
.
.
.
.
.
.
ratiowithcallwoutcall .
.
fig.
final reason distribution of ranker critic.
pieces of information are not closely related to the problem the model is trying to solve they may not help enhance the model s performance.
our research indicates that merely adding function call information does not directly facilitate the model s effectiveness in detecting vulnerabilities.
in the field of vulnerability detection exploring how to construct effective contextual information remains a challenging and worthy research question.
answer for rq5 additional call graph information may enable the model to make better judgments in some cases.
however we also observed situations where this additional information could potentially confuse the model thereby reducing its performance.
v. r elated work vulnerability detection has been a critical issue for the healthy and sustainable development of the software ecosystem especially in blockchain and smart contracts.
traditional vulnerability detection methods such as those based on predefined static analysis rules often lack robust generalization capabilities and are difficult to adapt to new types of vulnerabilities.
moreover some logic related vulnerabilities are also challenging to encapsulate into static analysis rules.
to address this issue researchers have employed deep learningbased approaches.
for example zhuang et al.
used graph neural networks to detect vulnerabilities in smart contracts.
liu et al.
combined interpretable graph features with expert patterns to achieve better results and interpretable weights.
wu et al.
utilized a pre training technique and critical data flow graphs for the detection of smart contract vulnerabilities.
with the advent of large language models researchers are not only utilizing traditional deep learning models but also llms for vulnerability detection.
for example ullah et al.
evaluated llms on vulnerability detection tasks and found that they may not perform well.
fu et al.
further analyzed the gap for llms in detecting vulnerabilities.
thapa et al.
leveraged llms for software vulnerability detection and david et al.
used llms for smart contract vulnerability tasks.
alqarni et al.
fine tuned the bert model for source code vulnerability detection.
sun et al.
proposed an unified evaluation framework llm4vuln to enhance the ability of llms to detect vulnerabilities.
some research also fused large language models with traditional program analysis methods.
sun et al.
proposed gptscan for smart contracts leveraging static program analysis to reduce the false positives of llms.
li et al.
proposed llift for integrating llms with static analysis tools.
smartinv and propertygpt further integrated large language models with formal verification methods aiming not only to detect vulnerabilities but also to prove that a piece of code is secure.
however all these studies have not tuned domain specific knowledge into the models themselves focusing only on the knowledge from the pre training dataset or the vulnerable code segment itself which could not effectively detect logic bugs.
vi.
t hreats to validity in the data collection process there is a risk of data bias which might prevent models trained and tested on these data from generalizing accurately.
moreover the precision of data labelling significantly impacts model performance.
to mitigate these issues we collected verified data from real and public audit reports and utilized the latest tools such as gptscan and llm4vuln to assist in cleaning and annotating the data.
it is important to note that external links in the data could induce llm to produce incorrect information therefore we performed data cleaning to remove these links.
considering llms sensitivity to input data we standardized the code data including removing unnecessary spaces without changing code semantics to enhance the model robustness and reliability.
to maximize the performance of the zero shot learning of gpt .
and gpt we adopted and optimized the prompts from our partner metatrust labs.
these prompts have been integrated into their working pipeline.
for opensource models we collaborated with an auditing expert to adapt their prompts for these models.
overfitting is a common issue during model training which we addressed by implementing an early stopping strategy.
the choice of different models might affect the ranker critic architecture s effectiveness.
we tested multiple cutting edge opensource models including moe codellama 70b llama2 70b and the recently introduced gemma and compared their performance on inference benchmark tests.
based on factors like the strictness of the model output format and operational speed we chose moe .
our research also showed that the consistency between the selected reasons from the moe and the real reasons reached about .
to control costs we limited the maximum iterations in the ranker critic loop to five and adopted four decimal precision handling.vii.
l imitation although our method performed excellently in trials its primary advantage lies in detecting logic vulnerabilities in smart contracts which account for more than of exploitable vulnerabilities according to a recent study .
as such while iaudit s training data does include instances of reentrancy overflow and underflow vulnerabilities handling them is not the major usage scenario of iaudit.
indeed these traditional contract vulnerabilities are theoretically more suitable for detection by program analysis methods.
that said both aibased and pl based methods have their unique comfort zones and since iaudit is fully ai based this paper compared it with other ai based methods only.
additionally despite our efforts to mitigate the phenomenon of hallucinations in large language models llms through voting and agents hallucinations may still occur.
finally since our tools rely on llms there are certain hardware requirements.
while the models can be compressed to run on less powerful gpus this compression may result in reduced model performance.
viii.
c onclusion in this paper we proposed iaudit the first smart contract auditing framework that combines fine tuning and llm based agents to detect vulnerabilities and explain the results.
we adopted a multiple prompt based strategy and applied lorabased fine tuning to train the detector and reasoner.
the former generates results based on a majority voting mechanism while the latter provides multiple alternative explanations based on different inference paths.
furthermore we introduced two llm agents ranker and critic to collaborate in selecting the most appropriate explanation.
our approach demonstrated superior performance in zero shot scenarios compared to zeroshot llm learning and traditional full model fine tuning methods.
we studied the performance improvement brought by the majority voting strategy and compared different lora training methods providing the rationality of our choice.
we also explored how additional calling context affects our model s performance.
for future work we will focus on enhancing the model s stability and its alignment with human preferences.
acknowledgment we thank all the reviewers for their detailed and constructive comments.
we also express our gratitude to all colleagues at metatrust labs for their assistance in deploying the trustllm s iaudit model.
this research project is supported by the national research foundation singapore and the cyber security agency under its national cybersecurity r d programme ncrp25 p04 taicen the national research foundation singapore and dso national laboratories under the ai singapore programme aisg award no aisg2gc and nrf investigatorship nrf nrfi06 .
any opinions findings and conclusions or recommendations expressed in this material are those of the author s and do not reflect the views of national research foundation singapore and cyber security agency of singapore.
daoyuan wu was also partially supported by an hkust grant.