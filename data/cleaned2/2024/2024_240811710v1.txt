leveraging large language models for enhancing the understandability of generated unit tests amirhossein deljouyi roham koohestani maliheh izadi andy zaidman delft university of technology delft the netherlands a.deljouyi tudelft.nl r.koohestani student.tudelft.nl m.izadi a.e.zaidman tudelft.nl abstract automated unit test generators particularly searchbased software testing tools like evosuite are capable of generating tests with high coverage.
although these generators alleviate the burden of writing unit tests they often pose challenges for software engineers in terms of understanding the generated tests.
to address this we introduce utgen which combines searchbased software testing and large language models to enhance the understandability of automatically generated test cases.
we achieve this enhancement through contextualizing test data improving identifier naming and adding descriptive comments.
through a controlled experiment with participants from both academia and industry we investigate how the understandability of unit tests affects a software engineer s ability to perform bug fixing tasks.
we selected bug fixing to simulate a real world scenario that emphasizes the importance of understandable test cases.
we observe that participants working on assignments with utgen test cases fix up to more bugs and use up to less time when compared to baseline test cases.
from the post test questionnaire we gathered that participants found that enhanced test names test data and variable names improved their bugfixing process.
index terms automated test generation large language models unit testing readability understandability i. i ntroduction in today s software dominated world software reliability and correctness are very important .
consequently automated testing in the form of unit tests has become a crucial element for software engineers in ensuring high quality software .
despite the widely acknowledged importance of testing writing tests is tedious and time consuming .
to alleviate this burden on developers and testers the research community has devoted considerable effort on investigating automatic test generation approaches .
among the notable test generators are randoop and evosuite .
evosuite for example is a search based test generator that employs genetic algorithms to construct a test suite and has demonstrated good results in terms of coverage .
however based on insights obtained through industrial case studies there are limitations in terms of the quality of the generated test cases .
one critical limitation revolves around the understandability of generated test cases which involves various aspects such as meaningful test data proper assertions well defined mock objects descriptive identifiers and test names as well as informative comments.
additionally the difficulty in following the scenario depicted in the test caseand the ambiguity surrounding test data significantly hamper clarity .
figure provides an example of an evosuite generated test case.
this test case checks the equals method with two objects of weapongamedata with different minimum damage values.
here we see several comprehension challenges the purpose and functionality of a test method named with five arguments and callsequals3 is obscure the rationale behind the chosen test data remains unclear the identifiers are not providing any additional information and the absence of comments leaves the test case without essential explanatory context.
to address these issues we aim to enhance automatically generated test cases by focusing on contextual test data clear test method and identifier names and adding descriptive comments.
in this study we investigate the synergy of search based software testing sbst and large language models llms .
while natural language processing nlp techniques have shown promise in text generation and optimization and llms have advanced textbased capabilities their impact in generating highcoverage test cases for complex systems remains limited .
conversely sbst while effective in coverage often falls short in test case understandability.
our approach utgen integrates an llm into the sbst test generation process.
we hypothesize that this combined approach can leverage the strengths of both techniques to generate effective and understandable test cases.
our study is steered by three research questions rqs that consider the effectiveness of the utgen approach and the understandability of the generated test cases.
rq1does utgen have the capability to generate effective unit tests by utilizing a combination of llms and sbst?
the investigation into the effectiveness of the approach seeks to establish whether the non determinism of both the sbst and llm components impact the ability to generate compilable and high coverage unit tests.
rq2what is the impact of llm improved unit tests understandability on the efficiency of bug fixing by developers?
when it comes to the understandability of generated test cases we intend to measure understandability through the ease by which software engineers can perform bug fixing tasks involving failing test cases a setup previously used by panichella et al.
.arxiv .11710v1 aug test public void testcreatesweapongamedatataking6argumentsandcallsequals3 arrowhookleft weapongamedata jsweapondata weapongamedata0 new weapongamedata n zmn 6gffi weapongamedata jsweapondata weapongamedata1 new weapongamedata n zmn 6gffi boolean boolean0 jsweapondata weapongamedata0.
equals jsweapondata weapongamedata1 assertfalse boolean0 listing motivating example rq3which elements of utgen affect the understandability of the generated unit tests?
we frame rq 3to obtain a deeper understanding about which elements of the utgen approach determine the understandability of the generated test cases.
the key contributions of our paper are outlined as follows utgen our novel approach that integrates an llm into the sbst process to enhance the understandability of generated unit tests.
the application of utgen on classes to examine the effectiveness of the generated unit tests.
a controlled experiment and a post test questionnaire with participants from industry and academia were meant to evaluate the impact of llm improved test cases in terms of understandability in a bug fixing scenario.
we release a replication package that is publicly available with our implementation as well as detailed data and results from our evaluation .
ii.
b ackground a. search based software testing automated test generation approaches have been developed in order to reduce testing effort.
tools like evosuite and randoop generate a test suite starting from java source code using a search based or random approach .
several studies have uncovered challenges involving automatically generated tests an important one being that generated tests are typically less readable than their humanwritten counterparts .
in this context almasi et al.
have observed that developers find the test case scenario difficult to follow find the test data unclear and have difficulties with the meaningfulness of generated assertions.
b. large language models large language models are a subset of ai systems predominantly based upon the transformer architecture .
these llms are trained on vast amounts of data through which they learn the underlying patterns inherent in texts code dialogue etc.
and are therefore capable of generating a somewhat relevant response given a prompt by the user .
llms operate based on predicting the subsequent tokens in a sequence and reusing the extended sequence by running it through the model once again to predict the tokens to follow referred to as autoregression .
this process is continued up until a point in which either the maximum amount of required tokens is reached or a termination character is generated.
generate new test cases test data refinement post processing eg.
variable names commentssb test cases population post refinement minimization and add assertions tests tests tests test name suggestionprompt generation test generation new tcs population compile and verify generationthe llm refinementprompt evosuiteevosuitefig.
.
overview of the utgen approach since their emergence software engineers have utilized llms to enrich and simplify the development process .
in alignment with this various open source models e.g.
code llama and starcoder and closed source models e.g.
codex and gpt4 have been trained and fine tuned for this very purpose.
the research community has recently investigated incorporating llms into the test generation process.
in particular attempts have been made to evaluate the efficacy of utilizing existing llms for unit test generation and training specialized llms for test generation .
however the understandability and usability of these hybrid tests remain unclear.
additionally methods such as codamosa and testpilot respectively propose the addition of llms to combat stalls in the search based process and the full automation of the test generation process which have proven to be useful.
with all these additions however problems arise regarding the reliability correctness and complexity of incorporation given the non deterministic nature of the results .
recent studies have pointed out that engineering good prompts is crucial for obtaining high quality results .
for instance the usage of the chain of thought reasoning cot method has been shown to provide major improvements in the zero shot performance of models .
furthermore recent guidelines have been proposed that point toward the fact that including further information about the goal context and even the persona of the model can impact the quality of the results obtained .
while these guidelines provide a good start to the process of constructing a quality prompt in most cases the task remains an empirical process at heart .
iii.
t heutg enapproach figure provides an overview of our approach named utgen.
the core of our framework is a search based approach in which we integrated an llm in various stages of the test generation process highlighted in green .
we use evosuite as the search based test generation framework of choice and we have developed additional functionalities that facilitate the integration of evosuite with llms highlighted in blue .
2fig.
.
motivating example rq3which elements of utgen affect the understandability of the generated unit tests?
we frame rq 3to obtain a deeper understanding about which elements of the utgen approach determine the understandability of the generated test cases.
the key contributions of our paper are outlined as follows utgen our novel approach that integrates an llm into the sbst process to enhance the understandability of generated unit tests.
the application of utgen on classes to examine the effectiveness of the generated unit tests.
a controlled experiment and a post test questionnaire with participants from industry and academia were meant to evaluate the impact of llm improved test cases in terms of understandability in a bug fixing scenario.
we release a replication package that is publicly available with our implementation as well as detailed data and results from our evaluation .
ii.
b ackground a. search based software testing automated test generation approaches have been developed in order to reduce testing effort.
tools like evosuite and randoop generate a test suite starting from java source code using a search based or random approach .
several studies have uncovered challenges involving automatically generated tests an important one being that generated tests are typically less readable than their humanwritten counterparts .
in this context almasi et al.
have observed that developers find the test case scenario difficult to follow find the test data unclear and have difficulties with the meaningfulness of generated assertions.
b. large language models large language models are a subset of ai systems predominantly based upon the transformer architecture .
these llms are trained on vast amounts of data through which they learn the underlying patterns inherent in texts code dialogue etc.
and are therefore capable of generating a somewhat relevant response given a prompt by the user .
llms operate based on predicting the subsequent tokens in a sequence and reusing the extended sequence by running it through the model once again to predict the tokens to follow referred to as autoregression .
this process is continued up until a point in which either the maximum amount of required tokens is reached or a termination character is generated.
generate new test cases test data refinement post processing eg.
variable names commentssb test cases population post refinement minimization and add assertions tests tests tests test name suggestionprompt generation test generation new tcs population compile and verify generationthe llm refinementprompt evosuiteevosuitefig.
.
overview of the utgen approach since their emergence software engineers have utilized llms to enrich and simplify the development process .
in alignment with this various open source models e.g.
code llama and starcoder and closed source models e.g.
codex and gpt4 have been trained and fine tuned for this very purpose.
the research community has recently investigated incorporating llms into the test generation process.
in particular attempts have been made to evaluate the efficacy of utilizing existing llms for unit test generation and training specialized llms for test generation .
however the understandability and usability of these hybrid tests remain unclear.
additionally methods such as codamosa and testpilot respectively propose the addition of llms to combat stalls in the search based process and the full automation of the test generation process which have proven to be useful.
with all these additions however problems arise regarding the reliability correctness and complexity of incorporation given the non deterministic nature of the results .
recent studies have pointed out that engineering good prompts is crucial for obtaining high quality results .
for instance the usage of the chain of thought reasoning cot method has been shown to provide major improvements in the zero shot performance of models .
furthermore recent guidelines have been proposed that point toward the fact that including further information about the goal context and even the persona of the model can impact the quality of the results obtained .
while these guidelines provide a good start to the process of constructing a quality prompt in most cases the task remains an empirical process at heart .
iii.
t heutg enapproach figure provides an overview of our approach named utgen.
the core of our framework is a search based approach in which we integrated an llm in various stages of the test generation process highlighted in green .
we use evosuite as the search based test generation framework of choice and we have developed additional functionalities that facilitate the integration of evosuite with llms highlighted in blue .
2the aim of our approach utgen is to enhance the understandability of test cases by improving four key elements of generated tests providing context rich test data incorporating informative comments using descriptive variable names and picking meaningful test names.
these goals define the stages in our approach.
as a first step after the genetic algorithm has ended and the test cases mature in the search based process our approach focuses on refining test data 1in figure .
utgen uses an llm to generate contextually relevant test data unlike traditional search based methods that often rely on random values.
following this refinement the search process ends and we transition to post processing tasks.
here evosuite minimizes the number of test cases in the test suite shortens the length of individual tests and adds assertions.
once the test cases are fully formed at stage utgen leverages an llm to add descriptive comments and enhance variable names.
in stage utgen uses an llm to suggest suitable names for the tests reflecting the assertions and logic within.
finally to ensure that test cases are compilable and stable after these enhancements utgen compiles them stage and in case of compilation issues the process iteratively revisits stage 2for adjustments.
we first explain the prompt engineering component and then describe our test generation process per stage.
a. prompt generation the prompt component of utgen uses the code llama 7b instruct model from meta as provided by ollama1.
we have designed utgen in such a way that the code llama can easily be exchanged for another llm.
there are three stages within the utgen approach uses the prompt component the refinement of test data the post processing of tests and the naming of tests.
the general prompt component contains two distinct parts namely which is responsible for generating the prompts provided to the llm and which manages the request and ensures the correctness of the returned response.
for each stage we devised specialized prompts following guidelines from recent prompt engineering research .
as shown in figure these guidelines emphasize the following writing clear instructions with action words as in adopting a persona for the model as in allowing sufficient processing time through techniques like chain of thought cot as in standardizing input and output formats and framing requests in a positive manner as in .
the starting point for each prompt resembles the one presented in figure .
as each model has its complexities pitfalls and preferred input format no one size fits all solution exists to prompt engineering however the guidelines set out above have guided us.
we have followed an iterative prompt engineering process in which each adjustment of the prompt was deliberated upon before being accepted or rejected by the authors based on potential improvements in the results.
an emerging pattern 1ollama the aim of our approach utgen is to enhance the understandability of test cases by improving four key elements of generated tests providing context rich test data incorporating informative comments using descriptive variable names and picking meaningful test names.
these goals define the stages in our approach.
as a first step after the genetic algorithm has ended and the test cases mature in the search based process our approach focuses on refining test data 1in figure .
utgen uses an llm to generate contextually relevant test data unlike traditional search based methods that often rely on random values.
following this refinement the search process ends and we transition to post processing tasks.
here evosuite minimizes the number of test cases in the test suite shortens the length of individual tests and adds assertions.
once the test cases are fully formed at stage utgen leverages an llm to add descriptive comments and enhance variable names.
in stage utgen uses an llm to suggest suitable names for the tests reflecting the assertions and logic within.
finally to ensure that test cases are compilable and stable after these enhancements utgen compiles them stage and in case of compilation issues the process iteratively revisits stage 2for adjustments.
we first explain the prompt engineering component and then describe our test generation process per stage.
a. prompt generation the prompt component of utgen uses the code llama 7b instruct model from meta as provided by ollama1.
we have designed utgen in such a way that the code llama can easily be exchanged for another llm.
there are three stages within the utgen approach uses the prompt component the refinement of test data the post processing of tests and the naming of tests.
the general prompt component contains two distinct parts namely which is responsible for generating the prompts provided to the llm and which manages the request and ensures the correctness of the returned response.
for each stage we devised specialized prompts following guidelines from recent prompt engineering research .
as shown in listing these guidelines emphasize the following writing clear instructions with action words as in adopting a persona for the model as in allowing sufficient processing time through techniques like chain of thought cot as in standardizing input and output formats and framing requests in a positive manner as in4 .
the starting point for each prompt resembles the one presented in listing .
as each model has its complexities pitfalls and preferred input format no one size fits all solution exists to prompt engineering however the guidelines set out above have guided us.
we have followed an iterative prompt engineering process in which each adjustment of the prompt was deliberated upon before being accepted or rejected by the authors based on potential improvements in the results.
an 1ollama you are a adjective developer focusing on task at hand sys 2your task is to task to achieve this you should follow these structured steps detailed steps for analysis or improvement emphasizing .
careful reading and understanding of the provided code.
.
identification of key functionalities or aspects requiring attention.
arrowhookleft .
formulation or modification of specific elements e.g.
test names data comments to enhance clarity descriptiveness or functionality.
arrowhookleft arrowhookleft .
adherence to coding standards and best practices such as naming conventions or comment clarity.
the code section requiring your attention is delineated by and tags.
arrowhookleft your response whether it be a name modified code or comments should be placed between the opening tag and closing tag .
arrowhookleft the code segment to be analyzed or improved dynamically inserted during execution arrowhookleft listing prompt template for utgen emerging pattern that we initially observed is that llms are incapable of always adhering to the output format described for them.
therefore we put guidelines in place to deal with such mismatches as an example we had to deal with cases where plain text was placed inside the code blocks or when the intended delineation was not used by the llm.
our replication package contains the final versions of the prompts that we engineered in addition to other measures that were taken .
b. stage test data refinement in this stage we focus on requesting contextualized test data from the llm to increase the domain relevance of test data for a test scenario.
we designed a parser that converts the llm s responses into the structured format required by evosuite.
the test data refinement stage should be considered as another iteration in the search process in which both new and original test cases coexist in the test population.
the refined test cases are capable of changing the logic of the original test and they cover different parts of the method under test.
an example of the refinement stage can be seen in stage of figure with the original and enhanced test data shown side by side.
based on the context the llm changes the fourth argument of the weapongamedata constructor call from n zmn 6gffi into ninja sword which is more meaningful in the context of weapongamedata .
however it is important to acknowledge certain limitations in the llm s responses.
occasionally the llm may hallucinate e.g.
generate lines that deviate from the original test case or alter the number of parameters in method invocations.
to mitigate these inconsistencies we designed our parser to substitute the erroneous line with the corresponding line from the original test case if a corresponding line exists for it in the original test case.
in the absence of a corresponding line in the original test case the parser will skip parsing these erroneous 3fig.
.
prompt template for utgen that we initially observed is that llms are incapable of always adhering to the output format described for them.
therefore we put guidelines in place to deal with such mismatches as an example we had to deal with cases where plain text was placed inside the code blocks or when the intended delineation was not used by the llm.
our replication package contains the final versions of the prompts that we engineered in addition to other measures that were taken .
b. stage test data refinement in this stage we focus on requesting contextualized test data from the llm to increase the domain relevance of test data for a test scenario.
we designed a parser that converts the llm s responses into the structured format required by evosuite.
the test data refinement stage should be considered as another iteration in the search process in which both new and original test cases coexist in the test population.
the refined test cases are capable of changing the logic of the original test and they cover different parts of the method under test.
an example of the refinement stage can be seen in stage of figure with the original and enhanced test data shown side by side.
based on the context the llm changes the fourth argument of the weapongamedata constructor call from n zmn 6gffi into ninja sword which is more meaningful in the context of weapongamedata .
however it is important to acknowledge certain limitations in the llm s responses.
occasionally the llm may hallucinate e.g.
generate lines that deviate from the original test case or alter the number of parameters in method invocations.
to mitigate these inconsistencies we designed our parser to substitute the erroneous line with the corresponding line from the original test case if a corresponding line exists for it in the original test case.
in the absence of a corresponding line in the original test case the parser will skip parsing these erroneous lines and continue parsing the remaining portions of the test public void testequalswithdifferentmindmgvalues given we have two instances of weapongamedata with different values for mindmg arrowhookleft weapongamedata defaultweapon new weapongamedata ninja sword weapongamedata customweapon new weapongamedata ninja sword when we call equals method on the two instances with the default weapongamedata instance arrowhookleft boolean equals defaultweapon.equals customweapon then the result of equals is false since the two instances have different values for mindmg arrowhookleft assertfalse equals the two instances should not be equal arrowhookleft 1test data refinement weapongamedata weapongamedata0 new weapongamedata n zmn 6gffi arrowhookleft weapongamedata weapongamedata0 new weapongamedata ninja sword arrowhookleft 2post processing variables comments given we have two instances of jsweapondata.weapongamedata with different values for mindmg arrowhookleft weapongamedata defaultweapon new weapongamedata ninja sword weapongamedata customweapon new weapongamedata ninja sword when we call equals method on the two instances with the default weapongamedata instance arrowhookleft boolean equals defaultweapon.equals customweapon then the result of equals is false since the two instances have different values for mindmg arrowhookleft assertfalse equals the two instances should not be equal 3suggest a test method name based on the test body public void testequalswithdifferentmindmgvalues ... 1fig.
.
a simplified example of a test case enhanced by utgen per step llm generated test cases.
this increases the chance that even test cases with omissions are valid for compilation.
for instance if the llm s response adds a non existent statement like weapongamedata0.increasedmg the parser skips this line and continues processing.
similarly if the llm alters a method s parameter count like changing weapongamedata0.getdmgbonus to weapongamedata0.getdmgbonus the parser uses the original method call with zero parameters.
these strategies ensure the parser extracts the maximum number of statements from the llm responses minimizing the need for re prompting.
in post refinement evosuite optimizes the test case population and adds assertions to them.
the optimization includes shortening test cases and eliminating duplicated test cases from the population.
the selection of which duplicate test case to keep and which to eliminate is directed by a secondary objective which prioritizes selecting the test case that minimizes the total length of all test cases within the set of duplicates.
c. stage post processing in this stage we make the final chosen test as understandable as possible by making various aspects of the code more understandable.
utgen achieves this by adding descriptive comments and making variable names more clear.
after the post refinement has finished assertions are added to the test cases and the test cases have reached maturity in terms of coverage they are given to the llm for improvement.the llm is instructed to add comments using the given when then convention seen as more understandable and to exclusively change the naming of the variables but to let the data and logic untouched given that this could impact the intended behavior of a certain test.
to ensure maximal logical similarity between original and enhanced test cases we use the codebleu metric which effectively assesses syntactic and semantic similarities between two sequences .
we choose to control for similarity to increase the cohesion between generated and improved test cases as well as minimize the impact of llm hallucinations.
a codebleu score below .5triggers a re prompting process.
we cap re prompting at three iterations as our findings suggest that this limit preserves logical coherence and still facilitates the improvement of tests.
if the llm does not meet the threshold after three attempts the prompt simplifies removing comment structure constraints and thus allowing deviation from the initial format.
should the llm s response still not reach a satisfactory level after a total of six attempts the original test case is retained.
the value of three attempts per prompting strategy is also chosen to balance effectiveness and execution cost.
additionally as previous literature has pointed out results from llms can be non deterministic given a non modified temperature of the model being used this can in turn lead to results diverging from the original tests or tests that do not have correct syntax.
to ensure consistency and reliability in the returned results we employ a set of heuristic safeguards to facilitate the process of controlling for such anomalies.
with each response from the llm we try to identify and remove common mistakes made by the llm in the code e.g.
comments placed inside the code as plain text and not as comments attempt to correct any missing closing brackets in a piece of code validate code using codebleu as previously described and check the syntactic correctness of the returned results with the parser generator tool antlr2.
furthermore we re prompt the llm in the case when any of the previously described safeguards fail to improve the response fail to achieve syntactic correctness or have lowerthan threshold values for codebleu.
we limit the amount of recursive calls that are made to not have a single improvement request stall the entire process.
all the processes explained above relate to the component marked with in figure .
an example of this step is shown in 2in figure the comments in given when then format are added and the variable names are changed from weapongamedata0 and weapongamedata1 to the defaultweapon andcustomweapon matching the logic of the test case.
also the assertion message is added from the llm response.
d. stage test method name suggestion in this stage utgen gives the llm the completed method body of the test and it is asked to deduce a descriptive name.
we chose to put this stage after the post processing of the test 2antlr 4table i demographics of participants attendance academia industry in person remote experience academia industry in java in testing in java in testing years years years years affiliation academia industry role number role number phd student developer msc student senior researcher bsc student scientific dev.
post doc team lead scientific dev.
method body because then the test case includes comments that increase the context for the llm to generate a descriptive test method name.
if another test case already has a similar test name we re prompt until it has a unique name.
for instance in 3in figure the llm suggests testequalswithdifferentmindmgvalues .
this name reflects the test s functionality of examining theequals method across varying minimum damage values.
in comparison evosuite named this test testcreatesweapongamedatataking6argumentsandcallsequals3 .
e. stage compile and verify after successfully navigating through the safeguards it is still possible for a test case to fail to compile.
therefore we compile all test cases and a non compiling test case undergoes a repeated cycle of post processing and test method name suggestion with a default post processing budget of iterations.
compiling test cases are then assessed for their stability.
a test case is considered unstable if it fails due to an exception unrelated to a junit assertion.
all test cases that are both compilable and stable are saved.
iv.
e xperiment setup in this section we describe the methodology of evaluation of our approach.
we investigate the following rqs rq1does utgen have the capability to generate effective unit tests by utilizing a combination of llms and sbst?
rq2what is the impact of llm improved unit tests understandability on the efficiency of bug fixing by developers?
rq3which elements of utgen affect the understandability of the generated unit tests?
we now discuss the evaluation strategies for rq1 to rq3.
a. effectiveness evaluation setup rq1 we explore the effectiveness of utgen on two axes the compilability rate of llm improved test cases and a comparison in coverage of baseline and utgen test cases.table ii java classes used for the controlled experiment project class loc methods branches caloriecount budget twfbplayer jsweapondata dataset we utilize the dynamosa dataset composed of346non trivial java classes from 117open source projects for rq1 .
the classes are selected from four different benchmarks with the primary source being the non trivial classes of sf110 .
evaluation we evaluated utgen using the evosuite framework as a baseline.
we applied utgen on a dataset and generated two types of test cases original evosuite test cases and llm improved test cases.
we then compare these two types of test cases by measuring the number of llm improved test cases that compiled successfully branch and instruction test coverage and pass fail rates.
parameter configuration we decided to use the default configuration parameters for evosuite which have been empirically shown to provide good results .
we did increase the test budget max time from to seconds to ensure that the search algorithm has enough time to generate a test population that achieves reasonable coverage levels.
b. controlled experiment rq2 we conducted a controlled experiment to assess the understandability of test cases in a real world scenario namely bug fixing .
this extends the work of panichella et al.
who investigated the impact of generating documentation for automatically generated tests in the context of bug fixing .
the experiment involved participants.
the experimental group worked with utgen test cases while the control group was given evosuite test cases.
we configured evosuite with coverage based test naming which generates more readable test names than the default setting .
we examined two dependent variables in the experiment the number of fixed bugs and time efficiency measured as the time taken to fix the bugs.
participants we recruited participants with academic and industrial backgrounds.
table i presents their demographics.
to engage academic participants the experiment was advertised via the university s communication channels.
additionally developers from an industrial partner were enlisted.
furthermore all authors reached out to their professional networks of software engineers.
we made sure to extend the invitation to individuals with experience in java and testing.
objects to design the bug fixing assignments and compare experimental and control groups it was essential to choose two projects that would offer a solid foundation for understanding the context of bug fixing.
to do so we analyzed all classes within the sf110 dataset gathered insights into the distribution of lines of code loc which serves as an indicator of complexity .
using this data we calculated the mean and standard deviation for each distribution.
we then identified all classes falling within the range of .1 5across all specified metrics.
this process yielded a total of classes.
upon manual inspection of these classes we selected two for consideration budget which includes methods for calculating calories over intervals and jsweapondata featuring methods related to weapon objects in a java game.
table ii provides details on the two classes.
we inject four faults in each class with each fault located in a different method under test.
the injected bugs included replacement of arithmetic operations bugs statement deletion bug boolean relation replacement bugs and variable replacement bugs .
while the types of faults were similar across both classes fixing the faults in the budget class can be more challenging due to its detailed time calculations.
experimental design our experiment utilized a factorial crossover design it featured two periods and included a two level blocking variable based on the object.
in each period subjects applied a different technique treatment to a different object assignment .
we preferred the crossover design over a between subject design due to the latter requiring a larger number of participants to achieve sufficient statistical power.
the design of the experiment is detailed in table iii which outlines the four sequences used.
we followed the experimental design guidelines provided by vegas et al.
.
to minimize learning effects participants were given tasks involving different objects in each period.
additionally to avoid any potential bias from optimal sequencing we balanced the participants over the sequences in terms of the number of participants and academic versus industry background.
for participants from academia each sequence was executed times while for industrial participants it was executed times.
experimental procedure the participants were able to execute the controlled experiment either in person or remotely through videoconferencing.
before the actual experiment we asked participants to fill in a pre test questionnaire to gauge their experience.
one day before the experiment session we sent them a statement of consent instructions and materials for performing the experiment including the two assignments a number indicating the sequence see table iii and a link to the online survey platform.
this advance preparation was necessary because during the pilot evaluation we observed that receiving the projects just before the experiment led to additional preparation time increasing the threat of tiredness.
it could also lead to stress among participants if they encountered difficulties.
during the experiment an examiner was continuously present to explain expectations and control any external factors that could affect the experiment e.g.
ensuring that participants did not use external sources to fix bugs.
in the experiment we asked the participants to carry out two tasks each task consisted of fixing four bugs in minutes.
we assume extending the time or having an unlimited window box could intensify the learning effect and introduce threats of tiredness boredom.
if the participant indicated to have fixed all bugs within the minute time frame the examiner doublechecked this and the participant could proceed to the next step.
each participant received two tasks a task consistingtable iii experimental design seq orderperiod period object technique object technique i u e budget utgen jsweapondata evosuite ii e u budget evosuite jsweapondata utgen iii u e jsweapondata utgen budget evosuite iv e u jsweapondata evosuite budget utgen of one java class with a corresponding test class generated with utgen and a java class with a corresponding test class generated by the baseline approach i.e.
evosuite.
pilot we engaged participants not part of the participants to pilot our experiment.
after the pilot run we changed the tasks from fixing bugs in minutes to fixing bugs in minutes and clarified the expected behaviours through javadoc documentation.
we also narrowed the scope of the code segregating it into definitely good and possibly faulty code sections.
thus ensuring that the assignments were feasible within the minute time frame.
finally we improved the task descriptions sending detailed instructions and an overview of the experiment to participants beforehand.
analysis method we conducted statistical tests to determine whether there was a significant difference between the number of bugs found and the time taken to fix bugs in llmimproved test cases compared to baseline test cases.
due to our crossover design we accounted for potential carryover effects which required treating the data as dependent.
therefore nonparametric hypothesis tests for independent samples like the wilcoxon rank sum test were not suitable .
instead we employed mixed models for our analysis.
specifically for each of our dependent variables the number of fixed bugs this variable is discrete and bounded between and we treated it as an ordinal variable.
consequently we used cumulative link mixed models which are appropriate for this type of data.
time efficiency this variable represents the time taken to fix bugs and we used generalized linear mixed models with a gamma distribution which is suitable for timerelated data .
we considered technique object technique object order confounded with carryover and period as fixed effects and participants id as a random effect.
the sequence effect is embedded within the variables order and technique object.
we set the significance level at .05for both models.
additionally we examined whether factors such as participants background programming experience in java and testing as well as whether the sessions were attended inperson or remotely interacted with the technique on the number of fixed bugs.
in these cases we extended the mixed model by adding these factors to assess their interaction with the technique.
we also used cohen s d to measure the effect size ranging from very small d .
to small .
d .
medium .
d .
and large d .
.
6table iv questionnaire overview title type of question aspect q1 in your opinion what factors make finding bugs easier for you?open q2 do you think the understandability of the test cases affects your bug fixing?likert q3 prioritize the elements in helping understandability ranking q4 how important are the following elements in the understandability of the test caselikert q5 how do you judge the understandability of the provided test case task and likert open q6 evaluate how good you think the first task is in each itemmatrix table open q7 evaluate how good you think the second task is in each itemmatrix table open c. post test questionnaire rq3 we used the post test questionnaire to obtain feedback from the participants of the controlled experiment on which aspects of utgen affect the understandability of test cases see table iv .
we focused on gauging three aspects participants views on how the understandability of test cases impacts their bug fixing effectiveness their opinion on what factors in test code contribute to the understandability of generated test cases and their ratings of the quality of these factors in test cases with and without the llm improved enhancements.
questionnaire in q1 we ask participants to identify factors they believe to affect bug fixing effectiveness.
importantly at this stage the participants are unaware that the experiment focuses on the understandability of generated test cases ensuring that their responses genuinely reflect their initial thoughts on bug fixing.
in q2 we query whether the participants think the clarity of generated test cases influences bug fixing.
q3 and q4 gauge which factors impact understandability most.
in q5 we ask the participants to rate the understandability of the two tasks using a likert scale along with open ended feedback.
finally in q6 and q7 we ask participants to rate specific elements such as comments test data test names and variable naming in the test cases of both tasks in terms of completeness conciseness clarity and naturalness thus aiming for a detailed evaluation of different aspects of test case quality .
analysis method for the open ended questions we sorted the data into categories using a card sorting method and calculated the frequency of each category.
two authors independently reviewed the card sorting process and achieved an agreement.
for the likert scale questions we determined the mean value and percentage of each answer.
for q6 and q7 we used the wilcoxon rank sum test with an of .
because it did not follow a normal distribution as determined by the shapiro wilk test with a p value .
.
we used cohen s d effect size to determine the extent of the difference.
v. r esults in the following we discuss the results per research question.table v efficacy results of utg en generated tests .
pass failed test count pass rate .
evosuite .
.
utgen .
.
improved tests test count percentage .
improved tests .
.
reverted tests .
.
enhancment stagnation .
.
coverage instruction coverage branch coverage .
evosuite .
.
.
utgen .
.
a. rq1 effectiveness of integrating llms and search based methods for generating unit tests we define effectiveness as the capability of utgen to generate unit tests that are compilable and execute reliably along with their ability to cover the classes under test.
the success rate defined as the proportion of generated tests that pass upon execution reflects functional correctness.
it is important to note that while all generated tests compile the success rate pertains solely to their execution outcome.
utgen successfully generates a total of tests with a pass rate of .
while evosuite produces tests at a slightly higher pass rate of .
.
the heuristic safeguard described in section iii c ensures the syntactic correctness and compilability of test cases but also leads to .
of the tests were categorized as enhancement stagnation i.e.
the llm could not improve the test case or reverted i.e.
we went back to the evosuite base test case as the test case failed to compile.
as such these .
of test cases compile but are not meaningfully affected by utgen.
the origin of certain test cases not being meaningfully affected by utgen lies in the non deterministic nature of llms.
as we have no guarantee that tests given to the llm will compile upon improvement due to the possible hallucinations by the llm we employ several safeguards.
while the safeguards explained in section iii c do manage to catch a great portion of the tests that would not compile some do fall through.
therefore we perform a compilation check 4in figure .
if any improved test fails to compile we revert back to an evosuite generated test case.
out of the total tests generated by utgen .
are non compiling and are thus reverted to the initial test case generated by evosuite.
the remaining .
of tests are due to the stagnation of the enhancement process and the inability of the llm to make a significant contribution.
finally from table v we observe that evosuite reaches slightly higher coverage compared to utgen instruction coverage is .
compared to .
while branch coverage is .
compared to .
.
in a further investigation into the reason for this delta in coverage we find that small changes in the post processing step e.g.
changes in values of parameters affect the overall coverage achieved.
7budget jsweapondata01234number of bugs fixed i all participants llm improved baseline budget jsweapondata06 00time mm ss i all participants llm improved baselinebudget jsweapondata ii academic participants llm improved baseline budget jsweapondata ii academic participants llm improved baselinebudget jsweapondata iii industry participants llm improved baseline budget jsweapondata iii industry participants llm improved baselinefig.
.
number of bugs and time taken for each different group i all participants ii academic participants iii industry participants rq1 a total of tests are generated by utgen of which .
are improved and .
are not due to reversion or stagnation.
the coverage results are marginally comparable to the baseline.
b. rq2 the impact on bug fixing figure presents the results of the controlled experiment in terms of two dependent variables the number of bugs fixed and time efficiency measured by the duration required to complete the tasks.
the results are reported for respectively the entire population the academic participants and the industry participants.
for both objects participants fixed more bugs in the task with the llm improved test cases compared to the baseline test cases.
for the budget class the difference is more pronounced as the participants fixed a median of bugs with llm improved test cases compared to a median of bugs fixed for baseline test cases.
for the jsweapondata class the difference is marginal as participants fixed a median of bugs with either of the test cases.
according to the tests of fixed effects presented in table vi we observe in the fixed bugs column that both technique p .
and object p .
significantly influence the number of fixed bugs.
this implies that using the llm improved test cases significantly increases the likelihood of fixing more bugs.
similarly when the object is jsweapondata the probability of fixing more bugs is also significantly higher.
the result of cohen s d effect size for the treatment is medium at .
.
regarding time efficiency participants using llmimproved test cases generally took less time to fix all bugs for both classes.
however the differences in timing are not statistically significant for the technique p .
with significance observed only for the object p .
.
the differenceis more apparent in the jsweapondata class where the average time to fix all bugs was for llm improved versus for baseline test cases less time .
for thebudget class the averages are closer for llmimproved and for baseline test cases.
this is mainly due to a minute cutoff which limited the observable difference.
additionally a post hoc analysis of estimated marginal means involving a pairwise comparison of different technique levels for each specific object level indicates that in the budget class the treatment llm improved test cases is significant p .
whereas it is not significant in the jsweapondata class p .
.
the cohen s d effect size is large of .92for the treatment in the budget class.
we hypothesize that the statistically significant improvement in the number of bugs fixed in the budget assignment compared to jsweapondata is due to the greater complexity of scenarios and bugs in the budget class.
this complexity likely increases the demand for clearer and more understandable test cases.
furthermore neither the period p .176andp .
nor the order p .138andp .
significantly impact the number of fixed bugs and time efficiency.
this indicates that there is no carryover effect between treatments.
the interaction between technique and object is not significant suggesting that the effect of the technique on the number of bugs fixed and time efficiency does not depend on the object.
additionally our analysis found no significant interaction between the technique and co factors such as participants backgrounds experience in java and testing or whether they attended sessions remotely or in person p .
.
finally in terms of the influence of the background of our participants we observe that both population groups show better performance when using llm improved test cases compared to baseline test cases in terms of both number of bugs fixed and time taken to fix bugs.
we observe that academic 8table vi tests of fixed effects sourcefixed bugs time efficiency estimate pr z estimate pr z technique .
.
.
.
object .
.
.
.
technique object .
.
.
.
order .
.
.
.
period .
.
.
.
participants seem to benefit more from the llm improved test cases in aiding bug fixing.
for industrial participants on the other hand the time saving gain is more pronounced.
figure provides a more detailed overview.
rq2 in our experiment using llm improved tests significantly increases the likelihood of fixing more bugs.
c. rq3 the effects of different elements of utgen on understandability the results of the post test questionnaire show three aspects the participants views on how the understandability of test cases impacts their bug fixing effectiveness their opinion on what factors in test code contribute to the understandability and their ratings of the quality of elements in test cases with and without the llm improved enhancements.
aspect how understandability of test cases impacts bug fixing we answer the first aspect through the responses to questions and in the survey.
we have observed that participants find a well written test suite important for bug fixing they frequently highlighted mentions the importance of descriptive and clear test names appropriate use of assertions and well chosen test data in test suites.
this aspect was prioritized over other factors like high quality production code mentions .
we also take note of the overall strong agreement that test case understandability is important in in the context of bug fixing as indicated by a median score of out q2 in table vii .
aspect what factors in test code contribute to understandability we have analyzed the participants responses to questions and where they ranked and scored the importance of elements.
from table vii we observe that participants give more importance to comments and test names than to variable naming and test data.
specifically .
of the participants ranked comments as most important while .
gave priority to test names in question .
aspect the quality of factors in test cases with and without llm enhancements in q5 of table vii we see that participants rate the understandability of llm improved tests somewhat better when compared to the baseline test cases.
we asked participants in q6 and q7 to evaluate an llmimproved and baseline test case of the assignments on different criteria and specifically per test element.
these criteria comprised completeness conciseness clarity and naturalness .
figure shows the results of q6 and q7.
the results indicate that llm improved test cases are consistently ratedtable vii participants responses to the questionnaire q2.
the effect of understandability on bug fixingstrongly disagreedisagree neither agree nor disagreeagree strongly agree .
.
.
.
.
q3.
prioritize the elements in helping understandabilityrank rank rank rank .
comment .
.
.
.
test name .
.
.
.
.
variable naming .
.
.
.
test data .
.
.
.
q4.
how important are the elements in the understandabilitynot importantslightly importantmoderately importantvery importantextremely important .
comment .
.
.
.
.
.
test name .
.
.
.
.
.
variable naming .
.
.
.
.
.
test data .
.
.
.
q5.
the quality of test casesvery low low moderate high very high llm improved .
.
.
.
.
baseline .
.
.
.
.
higher compared to baseline test cases for each of the criteria first row .
the wilcoxon test confirms this statistically significant difference p value .
for all criteria.
the effect size for conciseness was small while it was large for completeness naturalness and clarity.
notably in the open ended responses some participants mentioned that some comments in llm improved test cases were too general and added little value.
the respondents did appreciate the givenwhen then structured comments.
when we zoom into the test elements we see improvements in all areas for llm improved test cases comments test data test name and variable naming.
the wilcoxon test for all of these elements is statistically significant with a p value .
.
the effect size for comments test data and test names is medium while very large for variable naming d .
.
through the analysis of the open ended responses to questions we found that the complexity of a test case has an impact on the necessity of comments.
for simpler test cases using a given when then arrange act assert structure is often sufficient.
however for more complex cases more detailed comments are needed to ensure optimal comprehension.
overall participants mentioned this point times with one participant stating the test code lines are straightforward so comments are unnecessary.
similarly for simpler test cases the quality of variable naming was less of a concern participants mentioned this factor only times when rating a short baseline test case.
rq3 comments test names variable names and test data are improved compared to the baseline.
specifically participants highlighted improved conciseness clarity and naturalness in these test elements.
vi.
d iscussion in this section we discuss our results their implications and threats to the validity of our study.
9completeness12345criteria llm improved baseline conciseness12345 llm improved baseline clarity12345 llm improved baseline naturalness12345 llm improved baseline comments12345elements llm improved baseline test data12345 llm improved baseline test name12345 llm improved baseline variable names12345 llm improved baselinefig.
.
the results of q6 and q7 which test cases were rated in terms of criteria first row and test elements second row a. revisiting the research questions rq1 does utgen have the capability to generate effective unit tests by utilizing a combination of llms and sbst?
when we compare the effectiveness of our llm inspired utgen approach and evosuite we observe that utgen generates test cases that have relatively similar structural coverage.
however we also noticed a phenomenon that we term enhancement stagnation which occurs when the llm is not able to improve the test case even when re prompting multiple times.
we analyzed this situation and found indications that this stagnation is correlated with high complexity.
in this context we define complexity at the level of the class under test to be methods having a high number of parameters and methods being tightly coupled i.e.
many method calls between objects or within an object.
while generally adding more relevant context can help an llm highly complex projects can overwhelm llms due to lengthy input codes and insufficient contextual information thus hindering the enhancement process during post processing.
to overcome this we propose to incorporate retrieval augmented generation rag techniques.
we hypothesize that these enhancements can reduce occurrences of enhancement stagnation as it has resolved similar stagnation issues in other domains .
rag involves enhancing llms by dynamically integrating knowledge from databases knowledge graphs or the internet in real time into the generation process to provide contextually richer and more accurate responses.
rq2 what is the impact of llm improved unit tests understandability on the efficiency of bug fixing by developers?
from the results of the controlled experiment we see indications that the llm based enhancements brought to the generated unit tests improve their understandability in the bug fixing scenario.
specifically the experimental group outperformed the control group by fixing up to more bugs and completing tasks up to faster.
our experiment consisted of two assignments involving respectively the budget andjsweapondata classes.while we observed statistically significant improvements for thebudget assignment the other assignment did not reach statistical significance.
since the budget class is comprised of more complex scenarios and bugs we hypothesize that the complexity of a test scenario increases the need for understandable test cases.
this hypothesis was anecdotally confirmed by participants in the post test questionnaire.
rq3 which elements of utgen affect the understandability of the generated unit tests?
through the post test questionnaire we captured that participants think that llm improved test cases are showing improvements in terms of comments test names test data and variable names when compared to baseline test cases.
at a higher level participants also rated completeness conciseness clarity and naturalness as better.
however feedback from open ended questions highlights that comments should be more precise and informative.
similarly some participants also highlighted that simple test methods might not require extensive comments.
upon reflecting on this feedback we hypothesize that generically trained llms while generally robust might lack task specific data to effectively assist in creating comments.
b. implications our study s results have an important implication for researchers and tool builders.
in particular our study indicates that a generally trained llm can already instigate a considerable improvement in the understandability of search based generated test cases.
however our results also show that test case comments should be more detailed in some cases while seeming superfluous in other situations.
therefore we see potential in creating specifically trained llms for particular software engineering tasks but equally in customizing llm responses to individual software engineers.
c. threats to validity construct validity .
threats to construct validity relate to the setup of our study.
we conducted the study either in 10person or remotely with an examiner present.
to control for factors other than the codebase we ensured a consistent setup for all participants and limited the choice of ide to intellij providing uniform capabilities.
however this approach may disadvantage participants having experience with other ides potentially affecting their performance.
internal validity .
to mitigate threats to internal validity we did not reveal the tool names in our experiment and questionnaire.
to prevent bias in selecting classes for the assignments we followed a systematic selection process to strengthen the methodological integrity.
to compensate for a learning effect we created four different sequences of the experimental design.
using mixed models we found that period and carryover effects were not statistically significant indicating they do not pose major threats to the study s validity.
external validity .
the classes that we use to determine the efficacy of test generation in rq1 are a potential threat to the generalization of our results.
to address this we used a dataset of classes from open source java projects that form a representative sample and were previously used in software testing studies .
we limited the controlled experiment in rq2 to two java classes.
to ensure their representativeness we carefully selected them from the sf110 dataset containing real world classes and taking the average loc of that entire dataset into consideration to select average classes .
future work will explore more complex classes.
in order to mitigate potential imbalance between the experimental and control groups we carefully balanced participants over both groups in terms of experience and background.
vii.
r elated work a. improving the understandability of test cases panichella et al.
introduced testdescriber which generates test case summaries that describe the intent of a generated unit test they established that these summaries enable software engineers to resolve bugs more quickly.
similarly roy et al.
developed deeptc enhancer leveraging deep learning to produce method level summaries for test cases.
both efforts highlight the value of summarizing test cases.
in contrast utgen generates detailed comments within the test cases themselves and provides a narrative of the test scenario.
zhang et al.
introduced an nlp technique for automatically generating descriptive unit test names.
daka et al.
applied coverage criteria for naming automatically generated unit tests while roy et al.
created deeptc enhancer by employing deep learning to rename identifiers in test cases to improve readability.
unlike these methods that rely on traditional nlp techniques utgen utilizes llms to suggest identifiers that fit the test scenario s context.
afshan et al.
enhanced the readability of inputs by combining natural language models with search based test generation.
deljouyi et al.
proposed an approach that generates understandable test cases with meaningful data through end to end test scenario carving.
baudry et al.
developed a test data generator using llms to produce realistic domain specific constraints.
our method is similar to baudry et al.
s but we focus on search based unit test generation.
b. generating test cases by llm despite the progress in llm based test generation to the best of our knowledge no study has focused on enhancing unit test case understandability through the integration of search based methods and llms.
research in this field shows considerable variability in methods and outcomes.
siddiq et al.
generated tests using llms and reported coverage on the sf110 dataset .
in contrast sch afer et al.
s testpilot for javascript achieved statement level coverage on relatively small systems.
alshahwan et al.
aimed to improve human written tests by llms and submit them for human review .
meanwhile lemieux et al.
explored overcoming coverage stalls in sbst with llms and moradi et al.
investigated mutation testing with llms .
steenhoek et al.
improved test generation by minimizing test smells through reinforcement learning .
unlike the aforementioned studies utgen focuses on enhancing understandability through integrating llms in the sbst process.
notably utgen achieved .
branch coverage surpassing the pure llm approach by siddiq et al.
.
viii.
c onclusion recent research has suggested that the understandability of test cases is a key factor to optimize in the context of automated test generation .
therefore in this paper we introduce the utgen approach that incorporates a large language model llm into the search based software testing sbst process.
in so utgen aims to improve the understandability by providing context rich test data informative comments descriptive variables and meaningful test names.
we first evaluated utgen s test generation effectiveness on non trivial java classes observing that utgen successfully enhanced .
of the test cases and slightly decreased coverage compared to evosuite generated tests rq1 .
we then performed a controlled experiment with participants from industry and academia we observed that test cases generated by utgen facilitated easier bug fixing with participants fixing up to more bugs and so up to faster rq2 .
feedback from participants in the post test questionnaire indicated a significant improvement in test case completeness conciseness clarity and naturalness rq3 .
in future work we aim to explore optimization strategies such as retrieval augmented generation rag to enhance prompt efficiency and minimize the need for re prompting.
furthermore we plan to refine our approach by creating customized fine tuned llms specifically for test generation.
these customized llms would replace the publicly available pre trained llm that we currently use.
acknowledgement this research was partially funded by by the dutch science foundation nwo through the vici testshift grant no.
vi.c.
.
.
11references a. j. ko b. dosono and n. duriseti thirty years of software problems in the news in proc.
int l workshop on cooperative and human aspects of software engineering chase .
acm pp.
.
k. l. beck test driven development by example ser.
the addisonwesley signature series.
addison wesley .
a. khatami and a. zaidman state of the practice in quality assurance in java based open source software development software practice and experience vol.
no.
pp.
.
quality assurance awareness in open source software projects on github in ieee 23rd international working conference on source code analysis and manipulation scam .
ieee pp.
.
m. beller g. gousios a. panichella s. proksch et al.
developer testing in the ide patterns beliefs and behavior ieee trans.
software eng.
vol.
no.
pp.
.
m. beller g. gousios a. panichella and a. zaidman when how and why developers do not test in their ides in proceedings of the 10th joint meeting on foundations of software engineering esec fse .
acm pp.
.
m. beller g. gousios and a. zaidman how much do developers test?
in 37th ieee acm international conference on software engineering icse .
ieee computer society pp.
.
m. f. aniche c. treude and a. zaidman how developers engineer test cases an observational study ieee trans.
software eng.
vol.
no.
pp.
.
s. ali l. c. briand h. hemmati and r. k. panesar walawege a systematic review of the application and empirical investigation of search based test case generation ieee trans.
software eng.
vol.
no.
pp.
.
l. baresi and m. miraz testful automatic unit test generation for java classes in 32nd ieee acm international conference on software engineering icse .
acm pp.
.
g. fraser and a. arcuri evosuite automatic test suite generation for object oriented software in proc.
joint meeting symp.
foundations of software engineering and the european softw.
eng.
conf.
esec fse .
acm pp.
.
g. fraser m. staats p. mcminn a. arcuri and f. padberg does automated unit test generation really help software testers?
a controlled empirical study acm trans.
softw.
eng.
methodol.
vol.
no.
pp.
.
p. derakhshanfar x. devroey a. panichella a. zaidman and a. van deursen generating class level integration tests using call site information ieee trans.
software eng.
vol.
no.
pp.
.
c. e. brandt a. khatami m. wessel and a. zaidman shaken not stirred how developers like their amplified tests ieee trans.
software eng.
vol.
no.
pp.
.
c. pacheco and m. d. ernst randoop feedback directed random testing for java in conf.
on object oriented programming systems and applications oopsla companion .
acm pp.
.
g. fraser and a. arcuri achieving scalable mutation based generation of whole test suites empirical software engineering vol.
no.
pp.
.
whole test suite generation ieee transactions on software engineering vol.
no.
pp.
.
a. panichella f. m. kifetew and p. tonella automated test case generation as a many objective optimisation problem with dynamic selection of the targets ieee trans.
software eng.
vol.
pp.
.
a. arcuri an experience report on applying software testing academic results in industry we need usable automated test generation empirical software engineering vol.
no.
pp.
.
f. palomba a. panichella a. zaidman r. oliveto and a. de lucia automatic test case generation what if test code quality matters?
in proceedings of the 25th international symposium on software testing and analysis issta .
acm pp.
.
f. palomba d. di nucci a. panichella r. oliveto and a. de lucia on the diffusion of test smells in automatically generated test code an empirical study in ieee acm 9th international workshop on search based software testing sbst pp.
.
g. grano f. palomba d. di nucci a. de lucia and h. c. gall scented since the beginning on the diffuseness of test smells inautomatically generated test code journal of systems and software vol.
pp.
.
g. fraser and a. arcuri evosuite on the challenges of test case generation in the real world in international conference on software testing verification and validation icst .
ieee pp.
.
s. shamshiri r. just j. m. rojas g. fraser et al.
do automatically generated unit tests find real faults?
an empirical study of effectiveness and challenges in international conference on automated software engineering ase .
ieee pp.
.
m. m. almasi h. hemmati g. fraser a. arcuri and j. benefelds an industrial evaluation of unit test generation finding real faults in a financial application in proc.
int l conf.
on software engineering software engineering in practice track icse seip .
ieee .
c. e. brandt and a. zaidman developer centric test amplification empir.
softw.
eng.
vol.
no.
p. .
b. zhang e. hill and j. clause towards automatically generating descriptive names for unit tests in proc.
int l conf.
on automated software engineering ase .
acm pp.
.
d. roy z. zhang m. ma v .
arnaoudova et al.
deeptc enhancer improving the readability of automatically generated tests in proceedings of the 35th ieee acm international conference on automated software engineering pp.
.
m. sch afer s. nadi a. eghbali and f. tip an empirical evaluation of using large language models for automated unit test generation ieee transactions on software engineering vol.
no.
pp.
.
s. yu c. fang y .
ling c. wu and z. chen llm for test script generation and migration challenges capabilities and opportunities arxiv .
.
available a. mastropaolo s. scalabrino n. cooper d. n. palacio et al.
studying the usage of text to text transfer transformer to support code related tasks ieee acm 43rd international conference on software engineering icse .
v .
liventsev a. grishina a. h arm a and l. moonen fully autonomous programming with large language models in proceedings of the genetic and evolutionary computation conference gecco .
acm pp.
.
j. wang y .
huang c. chen z. liu et al.
software testing with large language model survey landscape and vision arxiv .
.
available k. el haji c. brandt and a. zaidman using github copilot for test generation in python an empirical study in proceedings of the international conference on automation of software test ast .
acm .
m. l. siddiq j. c. s. santos r. h. tanvir n. ulfat et al.
using large language models to generate junit tests an empirical study in international conference on evaluation and assessment in software engineering ease .
acm .
s. panichella a. panichella m. beller a. zaidman and h. c. gall the impact of test case summaries on bug fixing performance an empirical investigation in proc.
int l conference on software engineering icse pp.
.
replication package of utgen .
.
available https g. grano s. scalabrino h. c. gall and r. oliveto an empirical investigation on the readability of manual and generated test cases in international conference on program comprehension icpc .
ieee pp.
.
a. vaswani n. shazeer n. parmar j. uszkoreit et al.
attention is all you need arxiv .
.
available arxiv.
.
openai j. achiam s. adler s. agarwal et al.
gpt technical report arxiv .
.
available .
m. izadi r. gismondi and g. gousios codefill multi token code completion by jointly learning from structure and naming sequences inproceedings of the 44th international conference on software engineering pp.
.
m. izadi j. katzy t. van dam m. otten et al.
language models for code completion a practical evaluation in proceedings of the ieee acm 46th international conference on software engineering pp.
.
a. al kaswan t. ahmed m. izadi a. a. sawant et al.
extending source code pre trained language models to summarise decompiled 12binaries in ieee international conference on software analysis evolution and reengineering saner .
ieee pp.
.
b. rozi ere j. gehring f. gloeckle s. sootla et al.
code llama open foundation models for code arxiv .
.
available r. li l. b. allal y .
zi n. muennighoff et al.
starcoder may the source be with you!
arxiv .
.
available m. chen j. tworek h. jun q. yuan et al.
evaluating large language models trained on code arxiv .
.
available n. rao k. jain u. alon c. le goues and v .
j. hellendoorn cat lm training language models on aligned code and tests in 38th ieee acm international conference on automated software engineering ase pp.
.
c. lemieux j. p. inala s. k. lahiri and s. sen codamosa escaping coverage plateaus in test generation with pre trained large language models in ieee acm 45th international conference on software engineering icse pp.
.
s. ouyang j. m. zhang m. harman and m. wang llm is like a box of chocolates the non determinism of chatgpt in code generation arxiv .
.
available j. wei x. wang d. schuurmans m. bosma et al.
chain of thought prompting elicits reasoning in large language models arxiv .
.
available j. li g. li y .
li and z. jin structured chain of thought prompting for code generation arxiv .
.
available g. marvin n. hellen d. jjingo and j. nakatumba nabende prompt engineering in large language models in international conference on data intelligence and cognitive informatics .
springer pp.
.
j. zamfirescu pereira r. y .
wong b. hartmann and q. yang why johnny can t prompt how non ai experts try and fail to design llm prompts in proceedings of the chi conference on human factors in computing systems pp.
.
a. fan b. gokkaya m. harman m. lyubarskiy et al.
large language models for software engineering survey and open problems arxiv .
.
available v .
khorikov unit testing principles practices and patterns .
manning .
s. ren d. guo s. lu l. zhou et al.
codebleu a method for automatic evaluation of code synthesis arxiv .
.
available g. fraser and a. arcuri a large scale evaluation of automated unit test generation using evosuite acm transactions on software engineering and methodology tosem vol.
no.
p. .
a. arcuri and g. fraser parameter tuning or default values?
an empirical investigation in search based software engineering empirical software engineering vol.
pp.
.
a. zeller why programs fail a guide to systematic debugging .
morgan kaufmann publishers inc. .
e. daka j. m. rojas and g. fraser generating unit tests with descriptive names or would you name your children thing1 and thing2?
inproceedings of the international symposium on software testing and analysis issta .
acm pp.
.
j. graylin j. e. hale r. k. smith h. david et al.
cyclomatic complexity and lines of code empirical evidence of a stable linear relationship journal of software engineering and applications vol.
no.
p. .
s. vegas c. apa and n. juristo crossover designs in software engineering experiments benefits and perils ieee transactions on software engineering vol.
no.
pp.
.
r. h. b. christensen cumulative link models for ordinal regression with the r package ordinal submitted in j. stat.
software vol.
.
s. lo and s. andrews to transform or not to transform using generalized linear mixed models to analyse reaction time data frontiers in psychology vol.
.
g. m. sullivan and r. feinn using effect size or why the p value is not enough journal of graduate medical education vol.
no.
pp.
.
a. deljouyi and a. zaidman generating understandable unit tests through end to end test scenario carving in proceedings of the 23rdieee international working conference on source code analysis and manipulation scam .
ieee pp.
.
m. r. parvez w. u. ahmad s. chakraborty b. ray and k. w. chang retrieval augmented code generation and summarization arxiv .
.
available s. liu y .
chen x. xie j. siow and y .
liu retrieval augmented generation for code summarization via hybrid gnn .
j. campos y .
ge n. albunian g. fraser et al.
an empirical evaluation of evolutionary algorithms for unit test suite generation information and software technology vol.
pp.
.
s. afshan p. mcminn and m. stevenson evolving readable string test inputs using a natural language model to reduce human oracle cost in international conference on software testing verification and validation icst .
ieee pp.
.
b. baudry k. etemadi s. fang y .
gamage et al.
generative ai to generate test data generators arxiv .
.
available n. alshahwan j. chheda a. finegenova b. gokkaya et al.
automated unit test improvement using large language models at meta arxiv .
.
available a. m. dakhel a. nikanjam v .
majdinasab f. khomh and m. c. desmarais effective test generation using pre trained large language models and mutation testing .
b. steenhoek m. tufano n. sundaresan and a. svyatkovskiy reinforcement learning from automatic feedback for high quality unit test generation arxiv .
.
available arxiv.
.