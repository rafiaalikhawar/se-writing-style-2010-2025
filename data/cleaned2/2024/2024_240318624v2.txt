vulnerability detection with code language models how far are we?
yangruibo ding yanjun fu omniyyah ibrahim chawin sitawarin xinyun chen basel alomair david wagner baishakhi ray yizheng chen columbia university university of washington king abdulaziz city for science and technology google deepmind uc berkeley university of maryland abstract in the context of the rising interest in code language models code lms and vulnerability detection we study the effectiveness of code lms for detecting vulnerabilities.
our analysis reveals significant shortcomings in existing vulnerability datasets including poor data quality low label accuracy and high duplication rates leading to unreliable model performance in realistic vulnerability detection scenarios.
additionally the evaluation methods used with these datasets are not representative of real world vulnerability detection.
to address these challenges we introduce p rime vul a new dataset for training and evaluating code lms for vulnerability detection.
p rime vulincorporates a novel set of data labeling techniques that achieve comparable label accuracy to humanverified benchmarks while significantly expanding the dataset.
it also implements a rigorous data de duplication and chronological data splitting strategy to mitigate data leakage issues alongside introducing more realistic evaluation metrics and settings.
this comprehensive approach aims to provide a more accurate assessment of code lms performance in real world conditions.
evaluating code lms on p rime vulreveals that existing benchmarks significantly overestimate the performance of these models.
for instance a state of the art 7b model scored .
f1 on bigvul but only .
f1 on p rime vul.
attempts to improve performance through advanced training techniques and larger models like gpt .
and gpt were unsuccessful with results akin to random guessing in the most stringent settings.
these findings underscore the considerable gap between current capabilities and the practical requirements for deploying code lms in security roles highlighting the need for more innovative research in this domain.
i. i ntroduction in the evolving landscape of software development code language models code lms have become pivotal in automating various software engineering tasks fundamentally altering developers coding approaches .
leading examples like github copilot and amazon codewhisperer have already integrated into real world software development significantly assisting developers in their daily work.
consequently lm based vulnerability detection vd has gained traction with researchers utilizing code lms expanding capabilities to autonomously identify security vulnerabilities in codebases .in this paper we aim to evaluate whether code lms are able to detect security vulnerabilities in real code in settings representative of that needed for real world use.
this exploration is anchored in the belief that while code lms possess remarkable potential realizing their full capability to enhance software security necessitates a rigorous validation of their training andevaluation frameworks against the challenges of real world software development.
in particular we scrutinize the datasets used to train code lms and the benchmarks and metrics used to evaluate their effectiveness at vulnerability detection.
limitation of existing datasets and benchmarks.
first we meticulously analyze existing vdbenchmarks examining data collection methods label accuracy and prevalence of data duplication.
our investigation reveals critical data quality problems that impact their effectiveness for training and their suitability for evaluating code lms.
noisy labels invdliterature researchers typically label datasets either automatically or manually.
most large datasets use automatic labeling because manual labeling is too expensive.
however automatic labeling can introduce significant label noise.
for instance datasets like bigvul curate hundreds of thousands of functions from the real world and rely on vulnerability fixing commits for labeling.
however they suffer from a flawed assumption that each function modified by such a commit corresponds to a separate vulnerability.
in practice vulnerability fixing commits often fix one vulnerability but also make other changes to surrounding code and existing automatic labeling methods wrongly label that surrounding code as vulnerable.
in contrast manual labeling offers higher accuracy but its cost means it can only be applied to smaller datasets.
for instance the most accurate prior dataset sven which was manually labeled covers only common weakness enumerations cwes and comprises only .6k samples.
this dichotomy presents a challenge how to acquire high quality labeled vddata at scale for training code lms to detect 1arxiv .18624v2 jul 2024security vulnerabilities.
data duplication furthermore data duplication is prevalent in these datasets.
our analysis identified significant levels of exact copies and cloned vulnerabilities within the datasets.
this is particularly problematic when one copy appears in the training set and another copy in the testing set as performance metrics become unrepresentative of real world performance and misrepresent the model s ability to generalize to unseen data.
we found that up to .
test samples are leaked from the train set in some benchmarks.
limitation in existing evaluation metrics.
besides the data quality issue the evaluation metrics used by current benchmarks fail to capture the practical utility of models at vd.
accuracy many benchmarks report accuracy scores but accuracy is not an appropriate metric for vulnerability detection because of the base rate problem vulnerabilities are rare in practice most code is not vulnerable and because of mismatches in class balance the proportion of vulnerable samples in most research datasets does not match the ratio of vulnerable code in real life .
for instance because most samples in realistic benchmarks are not vulnerable it is possible to achieve high accuracy by always predicting not vulnerable .
a high accuracy score does not necessarily signify effective detection of security vulnerabilities it may simply reflect the accurate identification of non vulnerable cases or a bias towards predicting not vulnerable .
f1 while the f1 score is widely perceived to be a better metric for assessing classification performance on imbalanced datasets we argue that it is not appropriate in reality either.
the f1 score the harmonic mean of precision and recall reflects both false positives and false negatives by combining them into a single penalty.
yet for vdtools in practice the overwhelming majority of code is not vulnerable so a critical challenge is preventing excessive false alarms.
the f1 score fails to reflect this asymmetry so tools with a high f1 score may be useless in practice.
proposed solution.
to address the above limitations we propose a new vddataset and novel evaluation guidelines.
new dataset we introduce p rime vulto tackle the limitations of existing datasets through rigorous data collection data normalization and data filtering process.
we further introduce two rigorous labeling techniques i p rime vulnvdcheck uses expert analysis from cve entries and ii prime vul onefunc utilizes unique changes within commits ensuring high label accuracy.
consequently p rime vul not only ensures better label accuracy but also significantly reduces the possibility of data duplication thereby offering a more realistic and less noisy vdbenchmark.
to this end prime vulcontains vulnerable and benign functions covering cwes while maintaining similar accuracy as sven marking a substantial advancement in both scale and accuracy compared to previous datasets.
novel evaluation guidelines we propose novel evaluation guidelines to ensure evaluation results will be predictive of the real world performance of these tools.
first we suggest splitting samples chronologically to reflect the evolutionof both vulnerabilities and coding patterns.
chronological splitting also reduces the risks of data leakage.
second we introduce the vulnerability detection score vd s a novel metric designed to measure how well vulnerability detectors will perform in practice.
vd s measures the false negative rate after the detector has been tuned to ensure the false positive rate is below a fixed threshold e.g.
.
.
finally we introduce a pair wise evaluation method to assess the model s ability to distinguish between a vulnerable code sample and its benign fixed counterpart offering deeper insights into models vulnerability understanding.
effectiveness of code lms in realistic vulnerability detection.
we evaluate seven code lms with varied sizes including the state of the art open source models like starcoder2 and the proprietary models from openai on p rime vul.
our findings paint a sobering picture of the current state of code lms in vulnerability detection.
across various models and experimental setups code lms consistently performed poorly as measured on the p rime vulbenchmark.
this is in sharp contrast to prior evaluations on prior benchmarks which reported seemingly good results.
for example starcoder2 which previously showed promising performance with a .
f1 score on bigvul drastically underperformed in our assessment achieving only a .
f1 score on prime vul.
our findings highlight the ineffectiveness of existing models in vulnerability detection and the misleading nature of previous benchmarks.
this underscores the significance of p rime vulin offering a more challenging and realistic evaluation environment exposing the limitations of current models when confronted with real world vulnerabilities.
additionally the introduction of the vulnerability detection score vd s and pairwise evaluations further elucidated the challenges faced by the models.
our attempts at enhancing model performance through advanced training techniques such as class weights and contrastive learning resulted in marginal improvements at best.
we also evaluated state of the art llms including gpt .
and gpt where we employed zero shot prompting and fine tuning hoping to leverage their massive pre training and model capacity for enhanced performance.
however even these models cannot distinguish vulnerabilities from their similar yet benign counterparts.
in our pair wise evaluation gpt with the chain of thought reasoning could not even outperform the random guessing.
this indicates that we are a long way from being able to usefully detect security vulnerabilities with code language models and fundamentally new approaches may be needed.
these findings also prompt a reassessment of the benchmarks used to gauge progress in the field emphasizing the role of p rime vulin advancing toward more realistic and rigorous evaluations.
to summarize our contributions in this paper are as below we conducted an in depth analysis of existing datasets uncovering significant flaws including poor data quality low label accuracy and a high incidence of data duplica2tion.
we developed p rime vul a new vulnerability dataset with high quality accurately labeled data and stringent de duplication to offer realistic training and testing data for code lms.
we introduced new evaluation guidelines including the vulnerability detection score vd s and a pair wise evaluation method advancing the rigor of model assessment for vulnerability detection.
we evaluated a range of code lms with p rime vul.
our evaluation demonstrates that despite various attempts at optimization the performance of current models significantly falls short of the requirements for real world deployment highlighting a pressing need for innovative approaches in the training of code lms for vulnerability detection.
we release our artifact at dlvuldet primevul ii.
b ackground c hallenges in this section we will revisit the existing code lm based vulnerability detection models and study the core problems that prevent them from being promising and reliable for realistic deployment.
a. existing data collection methods a vast amount of high quality data is the key to successfully train deep neural networks.
earlier studies train deep learning models using synthetic datasets such as sate iv juliet and sard .
however synthetic datasets do not adequately capture the complex and nuanced nature of vulnerabilities in real world code .
to address this a series of benchmarks collect vulnerabilities from real world open source software repositories .
existing vulnerability datasets suffer from one of the two following issues automated labeling is cheap but too coarse grained and manual labeling is reliable but labor intensive.
we elaborate on this trade off below.
automated labeling.
the first strategy uses heuristics to automatically label all the samples so that the size of the dataset can be large enough for training deep neural networks.
for example bigvul reveal crossvul cvefixes and diversevul follow this strategy.
first they collect vulnerability fixing commits from open source resources such as the nvd database bugzilla and debian security trackers.
then they label the before commit version of changed functions as vulnerable and their after commit version and unchanged functions as nonvulnerable.
this strategy assumes that the vulnerability fixing commits only change vulnerable functions to fix the security flaw.
in our label noise analysis section ii b we find that this is often not the case which introduces wrong labels for vulnerable functions.
manual labeling.
to improve the quality of the data labels the second strategy is to involve human experts to verify the commits or functions manually.
for example the authors of sven manually labeled functions half of them being vulnerable from noisy datasets like bigvul and crossvul.unfortunately even for the most experienced security experts manually verifying the vulnerability labels of code samples is challenging and time consuming.
therefore the manually verified datasets though having higher label accuracy are not ideal for training the deep neural networks due to the limited diversity e.g.
sven only covers cwes and a limited number of samples e.g.
sven only has .6k samples .
b. label accuracy in this section we quantify the label accuracy of the existing benchmarks.
experiments we analyze the label accuracy by randomly sampling vulnerable functions from each benchmark and manually analyzing whether the function indeed contains security vulnerabilities.
the manual analysis was performed by three of the authors including two researchers with several years of experience in computer security and one senior security expert.
human judgement.
we follow the same method from chen et al.
to analyze whether each function is vulnerable and categorize nonvulnerable functions accordingly.
our human annotators comprehensively check the commit message that changed the sampled function the function before and after the commit the affiliated cve the nvd description as well as the discussions among the developers in security issue trackers if available.
using these information our human annotators confirm whether the commit is related to fixing a security vulnerability and whether the sampled function is indeed vulnerable.
for nonvulnerable functions we categorize them into vulnerability spread across multiple functions other changes to make the fix consistent relevant but not vulnerable and irrelevant changes.
in the first category common scenarios where we cannot tell whether a single function is vulnerable include examples such as race condition denial of service or command injection that exploits multiple functions etc.
in the second category we often observe changes to nonvulnerable functions as a side effect of fixing vulnerable functions e.g.
reporting more errors outside the patched function or changing the ways of calling the patched function.
in the last category the most common cases are changing spacing and functionality while fixing a different vulnerable function.
majority vote.
as an improvement over chen et al.
we use three human annotators with majority vote labeling for all datasets except sven.
we label a function as vulnerable if at least two out of the three human annotators agree that the function is vulnerable.
if any one of the three labelers is unsure about a function the security expert will lead a discussion among annotators to achieve an agreement.
then the final decision will be made through a majority vote.
results table i summarizes our analysis results.
we conduct accuracy analysis over three benchmarks that were originally constructed with some kind of manual labeling from prior work codexglue sven and vulnpatchpairs.
for benchmarks that use automated labeling specifically bigvul 3table i the label accuracy across existing vulnerability benchmarks and their comparison with two p rime vulautomated labeling techniques.
results show that our new labeling techniques achieve a high accuracy on par with sven which requires manual labeling.
moreover p rime vulcontains .
as many vulnerable c c functions as in sven.
benchmark manual correct sven .
codexglue .
vulnpatchpairs .
bigvul .
crossvul .
cvefixes .
diversevul .
prime vul onefunc .
prime vul nvdc heck .
the vulnpatchpairs dataset takes pairs of functions from codexglue.
the dataset does not involve further manual verification beyond its data resource codexglue.
refers to label accuracy numbers in chen et al.
.
crossvul cvefixes and diversevul we refer to label accuracy numbers in chen et al.
.
we will publicly release our label accuracy analysis results as part of our artifacts.
as shown in table i the benchmarks without manual verification have very low accuracy between and for vulnerable functions.
on the other hand only one out of the three prior datasets that used manual labeling method has a high accuracy sven has a label accuracy for vulnerable functions.
the other two datasets codexglue and vulnpatchpairs have low accuracy.
surprisingly the widely used dataset codexglue a.k.a.
devign dataset has a label accuracy of only for vulnerable functions even though zhou et al.
had recruited human annotators to label security related commits.
we find that their human annotation on security related commits is highly inaccurate such that many commits in codexglue do not fix security vulnerabilities.
moreover they adopt the same automated labeling policy as bigvul to label all changed functions as vulnerable.
the vulnpatchpairs dataset is derived from codexglue and as a result it is also inaccurate with a label accuracy of only .
our labeling policy is more stringent than vulnpatchpairs.
similar to codexglue we find that of functions from vulnpatchpairs are changed by some security relevant commits but the functions are not vulnerable.
the vulnerability is either spread across multiple functions or the function is changed to make the fix consistent.
the most accurate dataset from prior work is sven which has only vulnerable functions total .6k of vulnerable and non vulnerable functions across nine cwes.
in comparison the other noisy datasets from table i are much larger.
previous works have used different noisy datasets to fine tune code lms.
code lms trained with these noisy datasets cannot be trusted for realistic deployment.
when nearly half of thevulnerable samples in the training data have a wrong label the quality of the trained model is rather concerning .
due to the space limit we provide a more detailed label error analysis in the supplementary material i.a.
c. data leakage data leakage has been identified as a significant issue in the area of machine learning for code.
we consider two types of leakage code copy and time travel.
realistic evaluation of vulnerability detection models requires no data leakage to ensure their performances are reasonably measured.
we study the data leakage issue of four most frequently used vulnerability benchmarks bigvul cvefixes codexglue and diversevul and they have been used by more than ten code lm based vulnerability detection models as training and evaluation material .
data splits.
to study the data leakage issue specifically in the setting of fine tuning for vulnerability detection we need to create the train validation test splits.
for codexglue we directly take the original split from the benchmark .
for bigvul we take the public split used by .
for cvefixes and diversevul we follow the methodology introduced in to randomly split the data with .
code copy one main reason for leakage is data duplication since code data is highly repetitive and lms are known to be good at memorizing the code text .
specifically leaking exact copies across the training and evaluation set will inevitably inflate the evaluation performance.
experiments.
we study the exact copy of vulnerable functions.
to identify the code copy we exhaustively compare the vulnerable functions in the test set to all training samples.
we normalize the formatting characters in the code samples to eliminate its noisy effect and then identify the exact copy if two strings are identical after the normalization.
table ii the statistics of data duplication in existing vulnerability detection benchmarks.
benchmark de dup copy bigvul .
cvefixes .
codexglue .
diversevul .
prime vul ours .
results.
table ii reveals that existing benchmarks suffer from significant exact copy up to .
of samples being duplicated.
unfortunately this simple step is overlooked by prior work.
interestingly we notice that with hash based deduplication diversevul still has .
copies.
this is mainly because they did not normalize formatting characters and the same code with varied spacing will be mapped to distinct md5 hashes failing to be identified as copies.
further we manually check the root cause of such duplication but we realize the reasons are varied.
for example in codexglue we notice two different fixing commits targeting the same 4cve1 .
consequently this vulnerability gets sampled twice while one is in the training set and the other is in the test set.
differently in bigvul we identified exactly the same functions that being sampled twice before and after commits3 having contradicting labels.
such noises will confuse the model during training and hurt the model s performance as a result.
time travel existing datasets also have the issue of time travel since they randomly separate functions into train validation and test sets.
consequently it is possible to train on future data and test on past data.
it is also possible to have the fixed nonvulnerable function in the training set and the older vulnerable function in the test set.
in addition after manual analysis we find that many code samples from the same commit example2 were randomly separated into train validation and test sets.
this commit fixes multiple occurrences of the same cve in different functions in the same way.
in general developers tend to fix similar issues altogether before merging the changes into the main branch.
however training and testing with samples from the same commit is unrealistic and leaks information from the test time to the training time.
in a realistic setting the models are trained on the historical data to predict future samples.
d. evaluation in practical settings nearly all code lm based vulnerability detection models use accuracy and f1 score for evaluation.
accuracy measures the proportion of correctly predicted samples both vulnerable and benign out of all samples capturing overall model performance.
the f1 score on the other hand is a harmonic mean of precision and recall offering a balanced measure of the model s ability to identify vulnerable samples without excessively misclassifying benign samples.
we argue that accuracy and f1 fail to capture properties that developers care about in practice.
rather developers are generally concerned about the detection error tradeoffs and discriminative power over textually similar code samples across vulnerable and fixed patches.
detection error tradeoffs balance between false positives and false negatives is critical when deploying vulnerability detection tools.
developers rely on these tools not only to catch as many real vulnerabilities as possible minimizing false negatives but also to do so without overwhelming them with false alarms minimizing false positives .
this dual expectation reflects the practical tradeoffs in software development missed vulnerabilities can lead to security breaches while too many false alarms can lead to alert fatigue potentially causing real issues to be ignored.
f1 and accuracy offer little insight into the tradeoff between false negatives and positives.
textually similar pairs of vulnerable corresponding patch security fixes could involve only minor modifications to the code in many cases such as adjusting buffer sizes fixing data types or adding security checks.
these changes often in a patched version of a function that is textually similar to its vulnerable counterpart.
code lms which primarily analyze the textual representation of code struggle to differentiate between such closely related versions .
only by evaluating models on these paired data we can expose this weakness of code lms.
iii.
o urbenchmark prime vul as demonstrated in section ii b existing vulnerability benchmarks suffer from two major data quality issues data duplication and low label accuracy.
to address these shortcomings we introduce a new vulnerability benchmark p rime vul.
we propose a new automated data collection pipeline to produce high quality samples with accurate labels.
a. data merging and thorough data de duplication to build a large database we start by merging all security related commits and functions changed by them from bigvul crossvul cvefixes and diversevul .
we exclude data from devign codexglue because we discover that a large portion of its commits is unrelated to security issues during our annotation process.
we de deduplicate code copies as well as functions with only formatting differences.
for each commit we first normalize the changed functions before and after commits by stripping away characters such as spaces tabs t newline characters n and carriage return characters r .
then we compute the md5 hash of both the pre and post commit versions of the changed function.
if the pre and post commit versions of a function result in identical hash values we regard this function as unchanged and discard it.
finally we combine all remaining functions and further de duplicate the whole set by the md5 hashes of the normalized functions.
during the de duplication we maintain a unique set of hashes.
if the hash of a normalized function is already in the set we exclude it from further processing.
as a result p rime vul s thorough data de duplication ensures that no vulnerable function from the training set can be leaked to the test set table ii .
b. more accurate data labeling we propose two new labeling techniques p rime vulonefunc and p rime vul nvdc heck .
prime vul onefunc we notice that the previous labeling method has errors particularly when dealing with commits that modify multiple functions.
therefore p rime vul onefunc regards a function as vulnerable if it s the only function changed by a security related commit.
prime vul nvdc heck since human experts have analyzed the cves in the nvd database the vulnerability description in each cve entry is a reliable reference to label vulnerable functions.
we develop p rime vul nvdc heck as the following.
first we link security related commits to their cve numbers and the vulnerability description in the nvd database.
we label a function as vulnerable if it satisfies one of the two following criteria nvd description explicitly 5mentions its name or nvd description mentions its file name and it is the only function changed by the securityrelated commit in that file.
after applying our two labeling techniques we obtain two sets of vulnerable functions.
next we merge the sets and deduplicate the functions again.
we normalize the formatting characters in the functions and compute their md5 hashes to identify and remove duplicates.
subsequently we label the post commit versions of these identified vulnerable functions as well as all other unchanged functions within the same commits as benign.
only a subset of commits from the merged database mentioned in section iii a meet the criteria for labeling by our techniques.
commits without any function that matches these criteria are excluded from our dataset.
our pipeline results in a collection of vulnerable and benign functions across projects and commits.
to assess our labeling accuracy we conducted a manual review following the same process used in section ii b with our results presented in table i. the most accurate prior dataset sven has only vulnerable functions in c c and vulnerable functions in python.
our p rime vuldataset not only matches the label accuracy of sven but also significantly expands the collection of vulnerable c c functions by .
compared to sven.
p rime vulis diverse containing cwes .
of sven .
table iii statistics of p rime vul splitall paired vuln.
benign vuln.
benign train dev test iv.
n ewevaluation guidelines as discussed in section ii c we need new methods to properly evaluate vulnerability detection models in deployment settings.
this section proposes new evaluation guidelines.
a. temporal splits to minimize the data leakage issue and formulate a realistic train evaluate setup for vulnerability detection we split the train validation test set of p rime vulaccording to the commit date of the samples.
concretely we find the original commit for each sample and collect the time of that commit tying it with the sample.
then we sort the samples according to the commit where the oldest will be the train set in the middle will be the validation set and the most recent will be the test set.
we also make sure that the samples from the same commit will not be split into different sets.
this ensures that the vulnerability detection model is trained using past data and tested over future data.
b. more realistic and challenging evaluation vulnerability detection score the primary goal in vulnerability detection is to catch as many real vulnerabilitiesas possible can be measured by false negative rate or fnr where we expect low fnr .
meanwhile from a practical perspective a certain level of false positives can be manageable within the development workflow without causing alert fatigue typically captured by false positive rate or fpr where a lower rate is better .
therefore a metric that focuses on minimizing the false negative rate within a tolerable level of false positives is essential.
to this end we propose vulnerability detection score vds that evaluates the false negative rate of a vulnerability detector within an acceptable false positive rate i.e.
fnr fpr r where r is a configurable parameter.
in this paper we choose a tolerance rate r .
to perform the evaluation in section v. paired functions and pair wise evaluation as discussed in section ii d2 evaluating the models on paired functions vulnerable and benign versions of code could potentially reveal whether a model merely relies on superficial text patterns to make predictions without grasping the underlying security implications indicating areas where the model needs improvement to reduce the false positives and false negatives.
we collected such pairs in p rime vul significantly larger than existing paired datasets .
concretely we match the vulnerable functions with their patches in p rime vulto construct such pairs.
as we show in table iii the paired vulnerable functions are fewer than all vulnerable functions since not all vulnerable functions have a patch e.g.
a patch could delete the vulnerable function and we only include those challenging pairs sharing at least of the string between the vulnerable and benign version.
accordingly we also propose a pair wise evaluation method.
the core idea is to evaluate the model s predictions on the entire pair as a single entity emphasizing the importance of correctly identifying both the presence and absence of vulnerabilities in a textually similar context while recording the model s concrete predicting behaviors.
we define four outcomes of the pair wise prediction pair wise correct prediction p c the model correctly predicts the ground truth labels for both elements of a pair.
pair wise vulnerable prediction p v the model incorrectly predicts both elements of the pair as vulnerable .
pair wise benign prediction p b the model incorrectly predicts both elements of the pair as benign .
pair wise reversed prediction p r the model incorrectly and inversely predicts the labels for the pair.
v. e xperimental results considering the better data labeling and closer alignment with the real world data distribution in p rime vul coupled with the introduction of new evaluation techniques we reassess code lm s performance using p rime vulto gauge their performance in a more realistic setting.
to this end we evaluate the following three research questions rq1.
how do open source code lms perform on p rime vul?
section v b rq2.
can employing more advanced training techniques enhance the performance of code lms in detecting vulnerabilities?
section v c rq3.
can larger language models llms improve vulnerability detection performance?
section v d a. study subject datasets.
we mainly use p rime vulto conduct our experiments.
in rq1 we additionally use bigvul as a case study of existing benchmarks due to its popularity .
by comparing it to p rime vul we illustrate the impact of its limitation on training and evaluating code lm based vd models.
for rq2 and rq3 we will focus on p rime vulto improve code lms performances for vd.
table iv the code lms we will study in this section.
model parameters arch methods codet5 m enc dec fine tune codebert m encoder fine tune unixcoder m encoder fine tune starcoder b decoder fine tune codegen2.
b decoder fine tune gpt .
100b decoderfine tune few shot prompt chain of thought gpt 100b decoderfew shot prompt chain of thought models.
we will study seven code lms with varied sizes regarding their capabilities for vd as shown in table iv.
specifically we will fine tune all open source models in rq1 and study the advanced training techniques using unixcoder in rq2.
in rq3 we prompt gpt .
and gpt with two settings two shot examples and chain of thoughts reasoning and we also fine tune gpt .
on a subset of prime vulusing the openai api.
experimental settings.
for all experiments with open source models we implement our fine tuning framework following the existing benchmarks where we will use the learning rate of 5for all the fine tuning.
for smaller models with less than 7b parameters we will fine tune the models for epochs and for 7b models we fine tune for four epochs.
for gpt .
we just fine tune for one epoch due to the limited budget.
we load the model weights from hugging face models5.
all training tasks are conducted on a cluster with nvidia a100 gpus gb .
experiments with openai models are performed through api using greedy decoding.
b. rq1 performance of open source code lms on prime vul in this rq we fine tune open source code lms and evaluate their performances on p rime vul.
to illustrate the limitations of existing vdbenchmarks on the model training and evaluation we reproduce the code lms performances on using which as a direct comparison to p rime vul.
specifically we additionally fine tune code lms on bigvul and evaluate them on both bigvul and p rime vul.
results.
the empirical results are shown in table v. the rows of train pv and test pv in each section are results for code lms fine tuned and evaluated on p rime vul and rows of train bv are the comparative results for code lms finetuned with bigvul.
table v performance of code lm based vulnerability detection models in different settings.
model train test acc f1 vd s p c p v p b p r ct5bvbv .
.
.
.
.
.
.
pv .
.
.
.
.
.
.
pv pv .
.
.
.
.
.
.
cbbvbv .
.
.
.
.
.
.
pv .
.
.
.
.
.
.
pv pv .
.
.
.
.
.
.
ucbvbv .
.
.
.
.
.
.
pv .
.
.
.
.
.
.
pv pv .
.
.
.
.
.
.
sc2bvbv .
.
.
.
.
.
.
pv .
.
.
.
.
.
.
pv pv .
.
.
.
.
.
.
cg2.5bvbv .
.
.
.
.
.
.
pv .
.
.
.
.
.
.
pv pv .
.
.
.
.
.
.
ct5 codet5 cb codebert uc unixcoder sc2 starcoder2 cg2.
codegen2.
.
bv bigvul pv p rime vul.
vd s is the vulnerability detection score defined as false negative rate fnr false positive rate fpr .
in section iv b1.
p c p v p b and p r are the metrics defined in section iv b2 to evaluate the models on paired functions.
finding rq1.
code lms performance is overestimated on a prior benchmark and perform poorly on prime vul the comparative performance evaluation of code lms between prime vuland bigvul lays bare a startling truth the proficiency of these models is greatly overestimated by benchmarks like bigvul which fail to mimic the complexity of realworld vulnerabilities.
for example while starcoder2 shows a commendable f1 score of .
on bigvul it plummets to a paltry .
on p rime vul.
this precipitous drop is not an isolated case but a trend observed across all models exemplified by the observed false negative rates.
in addition even when code lms are fine tuned on p rime vul they fail to achieve the same level of performance as on bigvul.
for instance when trained on p rime vul starcoder2 s performance shows only a modest improvement in f1 score from .
to .
which is still markedly lower than the .
f1 score achieved on bigvul.
this persistent underperformance even after fine tuning suggests that the models cannot effectively learn from the more complex and realistic distribution of vulnerabilities in p rime vul.
this stark discrepancy confirms that code lms trained on existing benchmarks may develop a false sense of security due to the 7benchmarks failure to capture the intricate and diverse nature of vulnerabilities found in the wild.
finding rq1.
vulnerability detection score vd s offers a more concrete sense of realistic performance the vds emerges as a pivotal metric capturing the essence of a model s capability in real world settings where balancing the fnr and fpr is crucial.
for example codebert reports a high accuracy of .
and a satisfactory f1 of .
on bigvul.
relying on these metrics codebert seems to be an acceptable candidate for detecting vulnerabilities.
however its astonishing false negative rate of .
as reported by vds far exceeds the realistic tolerable limits revealing that it is actually useless in practice overturning the conclusions drawn from the accuracy and f1.
code lms vd s on p rime vulis even more concerning than on bigvul highlighting the models limitations in accurately identifying true vulnerabilities a critical factor for real world applications where missing a single vulnerability can have serious repercussions.
in addition we realize that vd s is not necessarily correlated with accuracy or f1.
the detachment of vd s from traditional metrics also signals a shift in how we should evaluate code lms for vd.
finding rq1.
code lms are weak at differentiating vulnerabilities from their similar but benign counterparts as we have discussed in section ii d2 and iv b2 pair wise evaluation not only offers a lens into the precision of code lms but also serves as a stress test for their real world application viability.
these metrics crucially reveal whether a model has truly learned to identify security vulnerabilities or is merely recognizing patterns without comprehending the implications a distinction that s vital for the deployment of code lms as a reliable security tool.
however our pair wise evaluation uncovers a significant deficiency in this aspect.
our results reveal that code lms frequently misclassify both functions in a pair as vulnerable p v or benign p b indicating an overreliance on textual patterns rather than a substantive understanding of the code s security context.
for instance starcoder2 reports only .
cases to correctly label both elements in paired functions while misclassifying more cases as both vulnerable and .
pairs as both benign demonstrating their difficulty in recognizing the vulnerable patterns and subtle distinctions that a patch can introduce.
such unreasonable behaviors undermine the trustworthiness of these models in realistic deployment.
this insight is also crucial as it emphasizes the need for models that go beyond surface level text comprehension to grasp deeper semantic implications of code changes for reliable vulnerability detection.
result code lms significant underperformance on prime vulhighlights their limitations when faced with realistic diverse and challenging vulnerabilities.c.
rq2 exploring to improve the performance on prime vul given code lms poor performance on p rime vul we decided to delve deeper into the training process trying to figure out whether more advanced training techniques could help code lms achieve promising performance on p rime vul.
to this end we perform analysis to inspect both the challenging samples within p rime vuland monitor the models behaviors during the inference time carefully studying the failing predictions from these models.
after these analyses and a comprehensive literature review we decided to explore two advanced training techniques that have shown effectiveness in helping binary classifications.
exploration class weights when we delve deeper into analyzing the experimental results one of the notable differences between bigvul and p rime vulis the ratio of vulnerable samples.
as we have mentioned in section ii b a lot of samples are mislabeled as vulnerable in bigvul and other resources that constitute p rime vul.
after applying our novel labelers section iii b the ratio of vulnerabilities significantly decreases.
therefore we suspect whether the significantly more imbalanced ratio hinders the learning process .
to verify our assumptions we implement the weighted loss similar to chen et al.
and integrate it into our fine tuning framework.
the general idea is to give a higher penalty when models make mistakes on the rare class i.e.
the vulnerable samples by up weighting the loss value for this class in the cross entropy loss.
with a higher weight on the rarer class the imbalance ratio will be less harmful since the model pays comparable attention to both classes when optimizing the loss.
to find an optimal weight we tried several different values besides the standard binary classification with equal weights weight of vulnerable class we further explored upweighting the loss for the vulnerable class for and times.
note that the vulnerable to benign ratio in prime vulis roughly .
table vi the impact of class weights during training.
weight acc f1 vd s p c p v p b p r .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
findings rq2.
class weights do not fundamentally improve code lms performance on prime vul similar to chen et al.
we observed an increased f1 when applying class weights though not as significant as theirs due to the difficulty of p rime vul.
however vd s scores warn us that such a marginal improvement is far from promising in the realistic scenario fnr is oscillating around across different weights and such models could not be trusted to detect security flaws.
these results though disappointing 8help us exclude the potential impact of the class imbalance providing convincing evidence regarding the difficulty of prime vuland the struggle of code lms in detecting realistic vulnerabilities.
exploration contrastive learning contrastive learning has been proven effective at learning better quality representations of text and code since they are able to decrease the cosine similarity among semantically different samples and consequently help to improve the models performance in downstream classification tasks.
therefore we hope to study whether contrastive learning could help code lms to achieve promising performance on p rime vul.
different from the existing works which apply contrastive learning at the pre training phase we enforce such contrasting signals together with the classification objective.
specifically code lms will be fine tuned to both classify vulnerabilities and contrast representations with distinct semantics.
we implement the objective according to gao et al.
referred to as clr where the model is trained to maximize the representation similarity between each sample and the perturbed version of itself and minimize the similarity between two randomly chosen samples.
the perturbation is directly applied to the code representation through dropouts in the transformer model.
table vii the impact of contrastive learning during finetuning unixcoder for vulnerability detection.
finetune acc f1 vd s p c p v p b p r cls .
.
.
.
.
.
.
clr .
.
.
.
.
.
.
ca clr .
.
.
.
.
.
.
findings rq2.
contrastive learning fails to significantly improve code lms performance on prime vul unfortunately as shown in table vii we could not see a significant difference by adding the clr objective.
we further analyze the results to see what might go wrong.
one notable misalignment we notice is that since clr from gao et al.
is not crafted for classification tasks it will distinguish any two samples regardless of whether their labels are the same.
therefore we further improve clr to be a second approach called class aware contrastive learning ca clr which will only minimize the similarity between samples with different labels.
this time we see a more notable improvement over f1 and p c by applying ca clr table vii .
however the performance change is still marginal.
this result empirically demonstrates code lms inherent incapability to detect vulnerabilities.
it is not only because the representations are too similar to draw the classification boundary but these models fail to identify the vulnerable patterns so that even if enlarging the cosine distance among representations through contrastive learning code lms still fail to draw the classification boundary correctly.result advanced training techniques for binary classification such as contrastive learning and class weights could not particularly improve code lms performance on prime vul highlighting its realistic difficulty.
d. rq3 larger code lms on prime vul after getting unsatisfying results with advanced training techniques we started to question whether the models we tried so far have too few parameters to solve such a challenging benchmark.
therefore we decided to explore the state of theart large language models llm to see whether significantly more parameters could bring a performance.
we perform experiments using openai gpt models gpt3.
and gpt .
considering the cost we only evaluate these models on the paired functions of p rime vul since it has much fewer samples than the full set while representing the more challenging scenarios in reality.
for gpt .
we experiment with three settings two shot prompting chain ofthought prompting and fine tuning.
we fine tune gpt3.
for one epoch on all vulnerabilities plus three times more randomly sampled benign samples from the train split of prime vul as fine tuning on the full training set will run out of the budget.
for gpt we only consider two shot and chain of thought prompting since its fine tuning api has not been released yet.
more details about the prompt template will be discussed in supplementary material i.b.
table viii results of openai gpt models on p rime vul paired functions.
model method p c p v p b p r gpt .5two shot .
.
.
.
cot .
.
.
.
fine tune .
.
.
.
gpt 4two shot .
.
.
.
cot .
.
.
.
random guess .
.
.
.
results are shown in table viii.
in general gpt .
and gpt outperform open source models for the pair wise evaluation even in the basic two shot prompting setting and chainof thought reasoning further pushes the performance boundary.
however we realize that such performance is actually no better than a random guess since the majority of the pairs in prime vulstill cannot be distinguished by these large sota code lms which might indicate the fundamental weaknesses of code lms to differentiate subtle vulnerabilities from their benign versions.
furthermore when we fine tune gpt .
we notice that the model is strongly biased by the vulnerable to benign ratio and reports even lower performance than the prompting approaches showing a red flag that even such a large lm llm still fails to capture the vulnerable patterns but take shortcuts from data instead.
9result even the state of the art openai models could not achieve a reliable performance on p rime vul calling for fundamentally novel approaches to improve the task.
vi.
d iscussions t hreats to validity discussion.
our study of code lms in the realm of vulnerability detection vd reveals that they do not perform well enough for real world applications.
prior evaluations looked promising but our work reveals their subtle issues data quality problems misleading metrics and methodology that poorly matches the way in which these models would be used in practice.
we highlight below several key areas where current code lms fall short.
a need for more context prior work formulates the problem as given the code of a single function determine whether that function contains a security vulnerability.
however this may be asking an impossible question.
determining whether code is vulnerable generally depends on information about other components of the system as well such as whether inputs to the function have already been sanitized how outputs will be used or what invariants are established by the rest of the system.
this focus on function level analysis without the consideration of other contexts such as interprocedural data flows would make it difficult for even a human to detect vulnerabilities let alone a model.
we recommend that the problem be reformulated so that the model also has access to a broader context.
to enable such a process p rime vul maintains the metadata for the included commits providing resources to extract relevant contexts.
b augmenting security awareness our empirical results particularly the shortcomings of code lms in pairwise evaluations suggest that these models make decisions primarily based on textual similarity without considering the underlying root causes or fixes of the vulnerabilities.
we suggest researchers explore ways to teach code lms about security concepts such as pre training methods inspired by how we teach human software developers about security or ways to build hybrid systems that combine lms with traditional security analysis or program analysis tools .
c teaching the model to reason about vd finally posing vulnerability detection as a binary classification problem and teaching the code lms accordingly might be too simplistic.
this approach banks on the slim hope that a lone summary token or a condensed representation can embody all the intricacies of code vulnerabilities such expectation might be overly optimistic.
instead we should decompose the vdproblem into digestible sub problems and teach the model to reason about each step to reach a conclusion .
the slight yet encouraging progress observed with the chain of thought experiments in table viii shows some promise in this direction.
threats to validity.
even with our stringent labeling methods label accuracy is less than see table i so there is still a small portion of mislabeled data in p rime vul.
however given the small percentage of mislabeling we believe all the reported results will still hold good.
for the proposed evaluation metric vd s there is a configurable parameter rto control the maximum false positive rate.
the practically acceptable value rmight vary for different scenarios changing the exact value of vd s but we expect our general conclusions to hold.
for experiments with openai models we only reported results with default settings which could vary slightly by changing the hyperparameters.
however we do not expect hyperparameter tweaking would change the conclusion.
vii.
r elated work throughout the paper we have reviewed many related works.
here we provide a summary of the remaining ones and compare them with our contribution.
code lm based vulnerability prediction two primary methods to use code lms for vdare fine tuning and prompting.
fine tuning adds a randomly initialized binary classification head to the language model and jointly optimizes all weights based on ground truth labels.
various approaches such as encoder decoder transformers encoder only transformers and decoder only transformers have been used for fine tuning.
on the other hand prompting based methods rely on llms typically proprietary ones like gpt .
previous studies yield mixed results khare et al.
showed that llms perform well on synthetic datasets but not promising on real world datasets.
experimentation with different prompting strategies notably variations of chain of thought cot prompts has further shown promising results .
integrating llms into larger frameworks has shown promise in detecting specific vulnerabilities such as use before initialization vulnerabilities and smart contract vulnerabilities .
we analyzed many of these code lm based vdmethods in this paper and showed that in a very realistic setting none of them performs well.
empirical analysis of deep learning based vulnerability prediction dlvp several works have pointed out that while dlvp models make correct predictions they do so for the wrong reasons they often rely on spurious features that are not the root cause of the vulnerabilities.
chen et al.
find that through a large scale evaluation involving 26k vulnerable functions across projects and cwes dlvp lacks generalization to unseen projects and is still far from being deployed in the industry.
some of the prior works focus on the lack of robustness of mlbased vulnerability detection algorithms against semantically preserving modifications.
another recent work attempts to measure how much models pick up the bug semantics through interpretability techniques involving the attention mechanism and shows that extra annotation on the bug semantics also improves the model s performance.
our current work complements these lines of work by focusing more on benchmark creation and evaluation techniques.
10viii.
c onclusions in this paper we uncover significant limitations in existing vulnerability detection datasets such as poor data quality low label accuracy and high data duplication rates as well as the limited practical utilities of current evaluation metrics.
in response to these concerns we present p rime vul accompanied by updated evaluation criteria designed to more accurately gauge practical effectiveness of code lm based vulnerability detectors.
through a series of experiments on prime vul we find that even with efforts to improve performance using sophisticated methods and expansive models existing code lms consistently fail to meet the demands of effective vulnerability detection in practical settings.
this underscores the urgent requirement for fundamentally innovative approaches in training code lms for security applications while also establishing a new benchmark for evaluating their efficacy.
acknowledgement this material is based upon work supported by the national science foundation under grants and and by the department of homeland security ibm the center for ai safety compute cluster the noyce foundation c3.ai dti and the kacstucb center of excellence for secure computing.
any opinions findings conclusions or recommendations expressed in this material are those of the author s and do not necessarily reflect the views of the sponsors.