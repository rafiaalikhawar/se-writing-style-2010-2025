the fact selection problem in llm based program repair nikhil parasaram university college london nikhil.parasaram.
ucl.ac.ukhuijie yan university college london huijie.yan.
ucl.ac.ukboyu yang university college london boyu.yang.
ucl.ac.ukzineb flahy university college london zineb.flahy.
ucl.ac.uk abriele qudsi university college london abriele.qudsi.
ucl.ac.ukdamian ziaber university college london damian.ziaber.
ucl.ac.ukearl t. barr university college london e.barr ucl.ac.uksergey mechtaev peking university mechtaev pku.edu.cn abstract recent research has shown that incorporating bugrelated facts such as stack traces and github issues into prompts enhances the bug fixing capabilities of large language models llms .
considering the ever increasing context window of these models a critical question arises what and how many facts should be included in prompts to maximise the chance of correctly fixing bugs?
to answer this question we conducted a large scale study employing over 19k prompts featuring various combinations of seven diverse facts to rectify bugs from open source python projects within the bugsinpy benchmark.
our findings revealed that each fact ranging from simple syntactic details like code context to semantic information previously unexplored in the context of llms such as angelic values is beneficial.
specifically each fact aids in fixing some bugs that would remain unresolved or only be fixed with a low success rate without it.
importantly we discovered that the effectiveness of program repair prompts is non monotonic over the number of used facts using too many facts leads to subpar outcomes.
these insights led us to define the fact selection problem determining the optimal set of facts for inclusion in a prompt to maximise llm s performance on a given task instance.
we found that there is no one sizefits all set of facts for bug repair.
therefore we developed a basic statistical model named maniple which selects facts specific to a given bug to include in the prompt.
this model significantly surpasses the performance of the best generic fact set.
to underscore the significance of the fact selection problem we benchmarked maniple against the state of the art zero shot nonconversational llm based bug repair methods.
on our testing dataset of bugs maniple repairs bugs above the best configuration.
index terms automated program repair large language models prompt engineering i. i ntroduction when debugging and fixing software bugs developers seek bug related information from a diverse array of sources.
such sources include the buggy code s context documentation error messages outputs from program analysis etc.
individual pieces of this information which following recent work we refer to asfacts have been demonstrated by previous studies to enhance llms bug fixing efficacy when incorporated into the prompts .
given the ever increasing context window of cutting edge llms a critical question emerges which these authors contributed equally to this work.specific facts and in what quantity should be integrated into the prompt to optimise the chance of correctly fixing a bug?
this work is a systematic effort to investigate how to construct effective prompts for llm based automated program repair apr by composing facts extracted from the buggy program and external sources.
we identified seven facts those individually studied in the context of apr by previous work such as the buggy code s context github issues and stack traces angelic values a semantic fact previously unexplored in the context of llm based apr but that has been successfully used for debugging and repair and those chosen based on our intution as developers.
our study was conducted on bugs in open source python projects from the bugsinpy benchmark.
our first experiment aims to confirm the utility of the considered facts.
specifically for each fact if the potential inclusion of this fact in apr prompts helps to repair some additional bugs or increases the probability of fixing some bugs.
to answer this question we constructed over 19k prompts tasked to repair the buggy function and containing different subsets of the seven facts for the bugs.
then we queried an llm to generate patches and evaluated the patches using the provided test suites.
finding confirms the utility of each fact that is each fact helped repair at least one bug that was not repaired by any prompt without this fact.
moreover all the facts have statistically significant positive impact on the probability of fixing a bug in a single attempt.
given the utility of each fact it is tempting to assume that adding more facts always enhances llm s performance.
contrary to this intuition finding reveals that apr prompts are non monotonic over facts adding more facts may degrade llm s performance.
an experiment involving bugs showed that prompts incorporating all available facts resulted in fewer bug fixes and exhibited an .
lower probability of repairing a bug within a single attempt compared to the most effective subset.
this finding is non obvious because each fact may contain crucial information for fixing the bug and although previous research showed that llms do not robustly make use of information in long input contexts and their performance dramatically decreases when irrelevantarxiv .05520v3 aug 2024please fix the buggy function provided below and output a corrected version.
.. cot instruc tions are omit ted ... the source code of the buggy function python this is the buggy function you need to fix def read json ... apart ofcode isomit ted ... return result a test function that the buggy function fails python def test readjson unicode monkeypatch ... apart ofcode isomit ted ... tm.assert frame equal result expected the error message from the failing test text monkeypatch pytest.monkeypatch.monkeypatch object at x7f567d325d00 ... apart ofmessage isomit ted ... pandas libs testing.pyx assertionerror runtime values and types of variables inside the buggy function compression value infer type str ... some vari ables are omit ted ... lines value false type bool expected values and types of variables during the failing test execution path or buf expected value tmp tmphu0tx4qstest.json type str ... some vari ables are omit ted ... result expected value ...omit ted... type dataframe a github issue for this bug text code sample a copy pastable example if possible ... apart oftext isomit ted ... however when read json iscalled with out encoding parameter it calls built in open method to open a file and open uses return value of locale.
getpreferredencoding to determine the encoding which can besome thing not utf fig.
a simplified apr prompt incorporating various facts for fixing pandas .
... shows information omitted for brevity.
... shows a part of the github issue that is essential to correctly fix the bug.
too much information in the prompt distracts the llm from the relevant part of the issue description significantly reducing pass and the correctness rate.
information is included in a prompt the trade off between utility of information and the ability of llm to process long contexts has not investigated.
the non monotonicity of apr prompts and simultaneously the utility of each fact led us to define the fact selection problem determining the optimal set of facts for inclusion in prompts to maximise llm s performance on given tasks.
it can be viewed as a variant of feature selection of classical machine learning for llm prompt engineering.
we consider two instances of fact selection universal fact selection when the selected facts do not depend on a specific task instance i.e.the bug and bug tailored fact selection when the fact set is bug specific.
if there was a universal set of facts that is effective for all bugs it would significantly simplify the development of llm based apr tools.
however our experiments showed that universal fact selection is suboptimal compared to bug tailored fact selection.
specifically finding identified that there is no fig.
comparison of pass the vertical axis for around 10k prompts incorporating subsets of the seven considered facts the horizontal axis computed over responses for repairing python bugs within the bugsinpy benchmark.
zero facts corresponds to the prompt containing only the buggy function without any additional information about the bug.
the graph plots the average pass score dashed lines and the maximum pass score solid lines across all bugs.
this graph clearly shows the non monotonic nature of apr prompts over facts for gpt .
and llama3 70b.
single subset of the seven considered facts that is sufficiently effective across all bugs in the dataset.
meanwhile enumerating sets of facts via e.g.greedy strategies while repairing each bug might be impractical because of the high cost of llm queries and the necessity to generate multiple responses due to llms nondeterminism.
as a practical compromise we trained a statistical model that we call maniple .
it is designed to select facts contingent upon the features of a specific bug.
empirical evidence shows that maniple significantly outperforms a universal fact selection strategy fixing 11more bugs than a generic set of facts that exhibited the optimal performance on the training data.
we benchmarked maniple against state of the art zeroshot non conversational llm based apr techniques.
on our testing set maniple repaired more bugs highlighting the practical impact of the fact selection problem.
the contributions of this work are a large scale systematic study of a diverse set of bugpertinent facts for apr prompts.
an empirical evidence of the utility of each considered fact for program repair including angelic values previously unexplored in the context of llms.
an empirical evidence of the non monotonicity of apr prompts over bug related facts i.e.adding more bugrelated facts into apr prompts may degrade llm s bugfixing performance.
a motivation and introduction of the fact selection problem for llm based apr.
maniple a bug tailored fact selection model that for a given bug chooses a set of fact to construct an effective apr prompt this model significantly outperforms previous related techniques.
all code scripts and data necessary to reproduce this work are available at m otivating example to illustrate the importance of the fact selection problem we consider the bug pandas in the pandas data analysis library within the bugsinpy benchmark.
this bug arises due to incorrectly handling the default encoding in the pandas read json function.
the developer patch for this bug involves setting the encoding to utf when it is not specified by adding the following lines to the buggy function if encoding is none encoding utf when gpt .
turbo was prompted to rectify the bug solely based on the source code of the buggy function it did not successfully address the issue in any of responses.
a likely explanation of this failure is that the function itself does not contain any inherently incorrect code so fixing this bug requires external information.
drawing upon existing literature on llm based apr and relying on our intuition as developers we assembled a diverse set of bug related facts to incorporate into the prompt.
these include the buggy function s context the failing test case the error message the runtime values of local variables their angelic values values that when taken by program variables during test execution result in successful passage of the test and the github issue description.
figure gives a simplified representation of the resulting prompt.
adding these facts enabled the llm to generate a plausible patch i.e.a patch that passes the tests in four out of responses.
however only two of these patches were correct.
the other two hard coded utf as the only encoding instead of the default one.
the causes of failures to fix the bug included forgetting to change the function despite correct chain of thought reasoning or hard coding the encoding inconsistently.
interestingly when we removed all facts but the code of the buggy function the runtime variable values and the github issue it significantly raised the success rate.
specifically the llm generated plausible patches in out of responses and out of these were correct.
a similar high success rate was demonstrated by the prompt with only the buggy function and the github issue.
we posit that this is because redundant or irrelevant information in the original prompt distracted the llm from critical details in the github issue highlighted in figure necessary to repair the bug .
we refer to this phenomenon that adding more facts may degrade llm s performance as the non monotonicity of prompts over facts.
to provide stronger empirical evidence we conducted a large scale experiment with 19k prompts containing various subsets of seven facts on bugs in python projects.
figure shows how pass which estimates the probability of generating a plausible patch in a single trial depends on the number of included facts.
the graph shows two types of lines the solid line corresponds to the scenario when we select the most effective combination of facts for each bug.
the dashed line indicates the performance of an average fact set.
it is evident that both these functions are non monotonic for both gpt .
and llama3 70b.
thisphenomenon of non monotonicity and its impact on prompt performance are discussed in more detail in section iv b. apart from the non monotonicity of prompts we also discovered that each of the considered facts helps to fix some bugs that cannot be fixed without it.
these observations motivated us to formulate the fact selection problem the problem of selecting facts for a given bug to maximise the chance of repairing it and propose a model maniple that selects effective facts based on features of a given bug.
iii.
s tudy design this section discusses the experimental setup the facts chosen for our study and how they are represented in prompts.
a. experimental setup we denote the set of bugs as band the set of all bug relevant facts as f. since we aim to investigate how using various facts impacts the success of apr we define the set of all jobs as j b 2f which pairs bugs with sets of facts.
zero shot prompting for apr a advantage of large language models such as chatgpt is they can be adapted to a downstream task without retraining via prompt engineering .
a prompt refers to the input or instruction given to the model to elicit a response.
prompting typically takes either the form of zero shot i.e.directly providing the model a task s input or few shot where the model is provided with a few examples.
in this work we investigate a zero shot apr approach.
although the few shot approach is promising it requires finding high quality examples which we leave for future work.
function granular perfect fault localisation apr tools repair bugs by first localising suspicious locations.
for an objective evaluation of apr tools liu et al.
argues for the use of perfect fault localisation pfl that is when the buggy locations are known to the tool.
pfl can resolve to difference granularity levels notably the line or the function.
liu et al.
argues that fault localisation tools do not offer the same accuracy in identifying faulty locations at different granularities making function level granularity appealing for limiting unnecessary responses on fault positive locations .
we found that in bugsinpy the average ochiai rank of the buggy line is and the buggy function is across the entire codebase making it much more likely to localise the buggy function than the buggy line.
apart from that although of the bugs in bugsinpy require modifying only a single function of them modify multiple lines within this function.
localising a bug to multiple lines is harder than targeting individual lines.
meanwhile cuttingedge models like gpt .
turbo effectively fix bugs even without specifying the exact lines i.e.when only the buggy function is provided.
consequently and in contrast to some previous studies this paper adopts functiongranular pfl as the standard approach for evaluating apr tools.
we implement this by providing the buggy function as context in the prompts for the llm enabling it to focus on fixing issues within the specified function.facts static declaration .
.
class scope .
.
docstring .
.
file scope .
.
test code .
.
test file name .
.
title .
.
description .
.
buggy class .
used method signatures .
failing test .
github issue .
dynamic external error information .
error message .
.
stack trace .
.
variable values .
.
angelic values .
.
variable types .
.
angelic types .
.
runtime information .
angelic forest .
fig.
this work uses seven facts dark rectangles across three categories for constructing program repair prompts.
each fact is composed of related pieces of information.
each prompt contains the buggy function to be repaired the facts can be included based on the employed fact selection strategy.
python bug benchmark apr tools are typically compared on datasets of bugs extracted from real world projects such as defects4j for java and bugsinpy for python.
in this work we use bugsinpy because of python s everincreasing importance and popularity.
bugsinpy contains bugs from popular python projects such as pandas and matplotlib .
among them we selected a subset of bugs which we refer to as bgp314 that require modifications within a single function due to our pfl approach and that we were able to reproduce.
to investigate apr performance on various classes of defects we consider three parts of bgp314 bgp157p ly1 this dataset comprises bugs which have been uniformly selected from bgp314 for the purpose of training and analysis.
bgp157p ly2 consisting of bugs this dataset is the complement of bgp157p ly1inbgp314 .
used for evaluating fact selection strategies.
bgp32 a subset of bugs uniformly sampled from bgp157p ly1 intended for preliminary studies on finding parameters such as determining the right response count to reduce the variance.
llm nondeterminism nondeterminism in llms leads to varying outcomes between responses which poses a challenge for analysing results .
to alleviate it we use the pass k measure which represents the probability that at least one query out of k succeeds at solving a problem.
previous work recommends estimating pass k as pass k llm q eq n c k n k where ejdenotes expectation over the set of llm responses to the set of queries prompts q nis the number of responses obtained from the llm where n k andcis the number of successes found in the nresponses.
our task is program repair so we deem a response successful if the extracted patch satisfies a correctness criterion which we approximate with passing a test suite.
a pilot study using bgp32 reveals that when n andk pass k exhibits the average standard deviation of ca.
.
and that further increasing n only marginally decreases standard deviation appendix a .for generate and validate apr that iteratively generates and tests patches until it finds one that passes a commonly used measure is the number of bugs for which at least one patch passes the tests among llm s responses fixed llm j b j j cj where j b f is a prompt and cjis the number of responses that pass the tests for the prompt j. test overfitting in program repair apr techniques repair bugs w.r.t.
correctness criteria such as tests or formal specification.
since tests do not fully capture the intended behaviour automatically generated patches based on tests may be incorrect .
thus the apr literature distinguishes between plausible patches patches that pass the tests and correct patches patches that satisfy the intended requirements.
since manually labelling a large number of patches is resourceintensive and error prone most analyses of the fact selection problem with pass k and fixed in this paper count plausible patches as successes.
we only label correct patches when comparing our tool with other apr techniques in section vi.
in our experiments we utilised the latest version of gpt .5turbo specifically gpt .
turbo which features a 16k context window.
for our studies gpt .
turbo was run with n responses on bgp157p ly1 bgp157p ly2 and with n for parameter exploration on bgp32 .
additionally we used llama3 70b which offers an 8k context window and was run exclusively on bgp157p ly1.
as of march the cost of reproducing the experiments detailed in this paper using the openai api for gpt .
and deepinfra for llama3 70b is estimated at .
b. bug related facts the considered facts were collected from previous llmbased apr research previous non llm based apr literature and our intuition as developers.
we conducted a pilot study on bgp32 to validate fact utility.
in total we collected pieces of information but since many of them are related we grouped them into seven facts which we refer to as f. these seven facts are divided into three categories static dynamic and external as shown in figure .buggy class .
the declaration of a class containing the buggy function provides a broader context and dependencies.
a class docstring offers insight into the overall purpose and functionality of the class.
used method signatures .
considering methods used within the buggy function as shown by chen et al.
allows for the analysis of dependencies and potential side effects that might contribute to the incorrect behaviour.
failing test .
the code of a failing test as shown by xia et al.
provides useful context for repairing a buggy function as it specifically highlights the conditions under which the program fails.
error information .
previous approaches showed that using error messages and stack traces improves llm s bug fixing performance.
runtime information .
runtime values and types of the function s parameters and local variables during the failing test execution provide an llm with concrete data about the program s behaviour.
angelic forest .
for a given program location a variable s angelic value is a value that if bound to the variable during the execution of a failing test would enable the program to pass the test.
angelic forest previously applied for synthesis based repair is a specification for a program fragment in the form of pairs of initial states and output angelic values such that if the fragment satisfies these pairs then the program passes the test.
inspired by this approach we added a variant of angelic forest to a prompt this variant combines variable values at the beginning of a function s execution coupled with the angelic values at the end of a function s execution i.e.the input output requirements of the function.
since python is dynamically typed we specify both the values and types of variables.
angelic values can be computed using symbolic execution however due to the immaturity of python symbolic execution engines we were unable to execute them on bugs in bugsinpy.
thus we extracted angelic values from the correct versions of the programs via instrumentation.
github issue .
a github issue when available provides important contextual information for fixing the bug as shown by fakhoury et al.
.
to denote subsets of f we utilise seven width bitvectors where the i th bit indicates whether the i th fact in our taxonomy figure is included in the set.
for example corresponds to the set containing only the runtime information .
.
c. prompt design we construct prompts via the prompt engineer e b 2f which builds a prompt over the alphabet to repair an input bug using a subset of facts from f. the prompt is constructed with the directive please fix the buggy function provided below and output a corrected version along with the included subset of facts.
the buggy function s code together with its docstring is provided as part of the prompt for the llm to effectively fulfill this directive.
each fact is incorporated viaa specialised prompt template.
figure shows an example prompt with incorporated facts and the fact templates are detailed in supplementary materials appendix b .
we employed the standard chain of thought approach by instructing the llm to reason about the provided facts as detailed in supplementary materials appendix c .
in our preliminary experiments we discovered that llm often generates incorrect import statements which makes it hard to automatically extract patches from the responses and apply them to the code.
to address it we explicitly added the import statements in the current file to the prompts.
a small study showed that this consistently improves the success rate as detailed in supplementary materials appendix d .
iv.
t hefact selection problem let an llm be a function from a string i.e.a prompt to a set of strings the responses r. we consider an arbitrary measure m 2r 2c rthat scores a set of llm responses w.r.t.
some correctness criteria cthat maps correct patches to a high score and incorrect patches to a low score and a prompt engineer e b 2f .
definition iv .
fact selection problem .given a set of bugrelevant facts f a prompt engineer e a buggy program b and correctness criteria for that buggy program cb the fact selection problem is to find f fthat maximises arg max f 2fm llm e b f cb cbencompasses any of the standard correctness criteria such as a test suite or a specification etc.
fbdenotes an optimal solution of equation for b we use fb f to denote the optimal ffor the buggy program bover the fact set f. similarly we use fb f to denote the optimal fact set f over all the buggy programs b band the fact set f. this can be defined as the solution to the equation below.
arg max f 2fx b bm llm e b f cb we aim to answer the following questions in this section how does the inclusion of each fact affect the overall effectiveness of a program repair prompt?
is there point beyond which adding facts to a program repair prompt degrades its performance?
can a fixed subset of facts be universally optimal up to a tolerance of for bug resolution across various bug sets?
the first question examines the impact of each fact on the repair effectiveness questioning whether every fact contributes positively to the resolution process.
section iv a answers this question affirmatively showing the inherent value of each fact.
the second question delves into the potential for diminishing returns or even detrimental effects from overloading a prompt with too many facts suggesting an optimal threshold for fact inclusion that maximises prompt efficacy.
if there is no such point then the optimal strategy will be to include all the facts.
section iv b shows that on our dataset adding facts to afactgpt .
llama3 70b gain shapley gain shapley error info.
.
.
.
.
github issue .
.
.
.
angelic forest .
.
.
.
failing test .
.
.
.
runtime info.
.
.
.
.
buggy class .
.
.
.
used method s. .
.
.
.
table i we report gain equation and shapley values scaled by for uniform fact selection on bgp157p ly1.
gain quantifies the average percentage increase in prompt repair performance from the fact.
prompt is non monotonic.
formally the final question asks whether b b the following equation holds m llm e b fb cb m llm e b fb cb this equation asks whether one can select a fact set for a set of bugs that is as effective as a fact set tailored to each bug.
section iv c answers this question by showing that this statement does not hold.
the above answers combined establish the importance of the fact selection problem.
a. fact utility a fact should only be considered for inclusion in a prompt if it has a potential to improve the outcome.
to confirm the utility of the considered facts f we simplify the premise by assuming that facts are independent and pose two questions what is the utility of each individual fact in improving repair performance on our dataset if we select the most effective fact set for each bug?
and what is the utility of each individual fact in improving repair performance on our dataset if we select a random fact set for each bug?
the first question addresses the potential effectiveness when we precisely know which facts to choose for a specific bug.
this notion of utility which we refer to as utility under optimal fact selection is relevant when we have a method to closely approximate an optimal solution fb f to equation i.e.choose the most effective facts for each bug b b. the second question explores the expected outcomes when we lack specific knowledge about which facts to select and hence make a random choice.
this notion of utility which we call utility under uniform fact selection is relevant when solving equation is either difficult or infeasible.
to estimate the utility of each fact we generated prompts containing all subsets of the considered facts the remaining one the buggy function is always present in the prompt which resulted in a total of prompts for bgp314 which is less than314 27since some facts are not available for some bugs.
for each prompt we computed responses to estimate the measures pass and fixed.
this enabled us to both evaluate an optimal selection strategy by explicitly considering fbfor each bug and a uniform selection strategy.we evaluate the utility of individual facts under uniform selection using two complementary measures shapley and a new measure we introduce and call fact gain defined in equation .
we report fact gain along with shapley for two reasons shapley s results are hard to interpret and we have the luxury of exhaustive enumeration since our fact set is small.
fact gain computes the net increase in pass scores due to the addition of a specific fact f we defined it by adapting relative change to our problem domain by setting the reference value to fact subsets that do not contain the measured fact f. let jf b f j f f be the subset of prompts whose fact set fincludes the fact f and jf b f j f f be those prompts that do not.
let rf llm jf andrf llm jf .
then the gain of each fact is a f pass k rf pass k rf pass k jf a f computes the change in the likelihood of generating a successful repair when the fact fcan be used in a prompt.
table i showcases the significance of each fact in apr prompts leveraging both aggregate gain a f and shapley values as defined in equation .
another interesting observation from the experimental results is that the facts buggy class and used method signatures exhibit a negative aggregate gain for gpt .
indicating that their inclusion might adversely affect the repair outcome on average.
for llama3 70b these facts have an aggregate gain close to zero still making them the least beneficial among the considered facts.
nonetheless each of these facts allows gpt .
to fix additional bugs that were not fixed without them.
for llama3 70b buggy class enables fixing additional bug while used method signatures contributes to fixing additional bugs.
this shows the importance of fact selection.
to demonstrate the utility of facts funder optimal fact selection we compute the number of bugs that were fixed exclusively when each fact was available.
these exclusive fixes are reported in the column excl.
for both gpt .
and llama3 70b in table ii.
this table highlights the unique bugfixing contribution of each fact showing the distinct set of bugs resolved by each fact across the two models.
it further shows that all facts improve performance.
these improvements are statistically significant for both models with no p value exceeding .
.
second we analysed each fact s utility under optimal selection by how its inclusion in or its exclusion affects pass k. we do so by simulating the scenario where specific facts are missing if a fact fwere missing we would be forced to compute fboverf f for each bug b b. the baseline for this scenario is when all the facts are available.
each row in the table in table ii details the pass attainable by the best prompts with and without a specific fact denoted by f .
for each bug b b prompts are constructed using optimal fact sets fb f over all facts and fb f f excluding thefact fgpt .
llama3 70b excl.
fb f f excl.
fb f f error info.
.
.
.
.
failing test .
.
.
.
angelic forest .
.
.
.
buggy class .
.
.
.
used method s. .
.
.
.
github issue .
.
.
.
runtime info.
.
.
.
.
table ii excl.
shows the bugs that could only be fixed by including the specific fact under optimal fact selection for bgp157p ly1.
fb f f shows performance of the best facts when fcannot be selected.
represents the drop in performance due to excluding the fact from the bug s best performing fact set.
factf.
the consistent reduction in pass denoted as emphasises the value of each fact in bug fixing.
to determine the statistical significance of the impact of each individual fact under optimal selection we calculated pass scores for all bugs both with and without a particular fact.
these scores were then compared.
the wilcoxon signed rank test shows that the inclusion of each fact has a statistically significant effect on each bug s optimal fact set.
finding .
under the assumption that we select the most effective fact set for repairing each bug each of the considered facts demonstrates its utility on our dataset.
when selecting an optimal fact set for repairing each bug each of the seven considered facts proves useful on our dataset including any of these facts in the prompt helps repair at least one bug exclusively and has a statistically significant positive impact on pass .
b. impact of fact set size on prompt performance in this section we investigate the concept of prompt monotonicity by examining how the incremental addition of facts affects prompt performance.
monotonicity in this context refers to a consistent improvement in performance with each additional fact.
this implies that more information invariably leads to better outcomes.
conversely non monotonicity indicates that there exists a threshold beyond which adding more facts does not enhance and may even degrade performance.
similarly to section iv a we evaluate the non monotonicity of prompts in two settings under an optimal fact selection and under a random selection.
for each of them we aggregate pass scores for all bugs over sets containing a varying number of facts.
figure presents the performance of pass for the prompts across different fact set cardinalities ranging from to .
the maximum pass corresponds to an optimal fact selection for each bug and the average pass corresponds to a random fact selection.
from the plot we observe that for both gpt .
and llama370b the max pass scores solid lines generally increase with the number of facts reaching a peak at facts for gpt3.
solid blue and facts for llama3 70b solid orange before declining.
the avg pass scores dotted blue line projectgpt .
llama3 70b best fact set project total best fact set project total luigi .
.
.
.
black .
.
.
.
fastapi .
.
.
.
httpie .
.
.
.
pandas .
.
.
.
tornado .
.
.
.
ansible .
.
.
.
matplotlib .
.
.
.
cookiecutter .
.
.
.
tqdm .
.
.
.
youtube dl .
.
.
.
keras .
.
.
.
scrapy .
.
.
.
sanic .
.
.
.
thefuck .
.
.
.
table iii comparison of project specific best fact sets and their project s average pass scores and the total average pass scores across all projects in bgp157p ly1.
the fact sets are represented using their bitvector encodings.
the table highlights that different repositories have different best fact sets indicating the importance of tailored fact selection for effective bug fixing.
for gpt .
show improvement with more facts reaching a plateau between and facts followed by a decrease.
for llama3 70b the avg pass scores dotted orange line increase up to facts and then decrease.
this pattern confirms the non monotonicity in prompt performance as the number of facts increases.
finding .
prompt performance is non monotonic with respect to the number of included facts.
while adding facts generally improves performance there exists a threshold beyond which additional facts hinder performance.
c. non existence of a universally optimal fact set a universally optimal fact set in the context of automated program repair is a collection of facts that when applied yields the highest effectiveness in terms of bug fixes and pass scores across a wide range of projects.
for defining an universally optimal fact set we first define the quality of a fact set in terms of the following properties efficiency a fact set is efficient when it outperforms alternative fact sets.
universality a fact set is universal when it is efficient up to tolerance as defined in equation .
coverage the set of bugs a fact set can resolve.
we define the function coverage 2f 2bwhere f f is a fact set and bis the set of all bugs in the dataset being considered such that coverage f returns the set of bugs fixed by the fact set f. the coverage ratio for a given fact set fis defined as cr f coverage f s fi fcoverage fi we prefer sets that maximise universality and coverage ratio.
table iii presents the best performing fact sets for each project along with their project specific and overall pass scores for both gpt .
and llama3 70b.
notably no singlefact aggregations fixed best fact set in bgp157p ly1 best fact set in bgp157p ly2 all facts bugs fixed by the top fact subsets bugs fixed by any fact subset table iv the number of bugs plausibly fixed by fact subset in bgp157p ly2 using gpt .
.
fact set achieves top performance across all projects.
for instance the fact set achieves a pass of .
on gpt .
for the sanic project which is below the highest pass score of .
.
similarly in the fastapi project the best fact set attains a project specific pass of .
on gpt .
slightly exceeding its overall score of .
yet still lower than the highest score of .
on the same model.
a similar variability is observed with llama3 70b where no common fact set appears across different repositories highlighting the unique nature of each project s optimal fact set.
additionally the highest occurrence count for any fact set is just with five fact sets appearing twice.
this distribution emphasizes the inconsistency in fact set effectiveness across projects and suggests that a universally optimal fact set is unlikely given the diverse nature of repositories and bugs.
table iv assesses the effectiveness of various approaches of fact selection in producing plausible fixes on gpt .
.
the analysis underscores the best fact set identified as which was selected for its highest coverage in terms of the number of bugs it could fix according to the training data compared against broader approaches such as the top union and total union.
the top union which aggregates the bugs fixed by the top five best performing fact sets generates fixes that pass tests for bugs while the total union encompassing bugs fixed by all fact subsets resolves bugs.
these unions significantly surpass the best fact set in bug resolution capability fixing many additional bugs and respectively .
this shows that the highest coverage ratio of the fact set is cr .65that it fixes of the bugs while missing of the bugs fixable by other sets.
these results highlight that the best fact set does not have a high coverage ratio especially compared to the theoretical maximum of an optimal fact selection which has a coverage ratio of .
the coverage ratio s delineation as monotonic in that the fact set with the highest number of bugs fixed is deemed the best indicates that within this dataset no fact set achieves a high coverage ratio.
table iv further shows the limitations inherent in static fact selection strategies as demonstrated by the performance of the best fact sets w.r.t.
bgp157p ly2.
these sets fix bugs and set the upper limit for universal fact selection in bgp157p ly2.
these sets were not ranked high in the training data as they were positioned at ranks and respectively out of candidates.
the upset diagram in figure shows the combinatorial overlap among the top fact sets from bgp157p ly1 as well as a baseline which does not contain any facts encoded with the bitvector .
the diagram is particularly instructive inrevealing the number of bugs addressed by various intersections of these fact sets with the largest subset intersection resolving bugs.
each of the fact sets is shown to individually contribute to the resolution of up to bugs.
cumulatively these fact sets fix a total of bugs surpassing the efficacy of the single best fact set which fixes bugs.
this upset plot helps us in answering the question of the existence of a universal fact set w.r.t.
the number of bugs fixed we would expect to see a row with dots in most if not all columns signifying its presence in the majority of intersections.
however the absence of such a pattern in this upset plot indicates there is no single fact set that fixes all bugs.
instead different fact sets are effective for different bugs.
finding .
the diversity in the best fact sets across different repositories table iii and the significant difference in the bugs fixed by the top five fact sets figure both point to the absence of a universal fact set in our dataset.
d. effect of fact order on performance the experiments conducted to this point assume a fixed fact order.
we now investigate the effect of permuting the facts.
our prompt template constrains the permutations we consider.
we add the facts buggy class .
and used method signatures .
to our prompt next to each other in the order that they appear in source code so we do not separately permute them.
similarly the failing tests .
fact immediately precedes the error information .
fact it generates in the template.
thus we only permute facts.
this experiment was conducted on bgp32 comprising bugs.
for each of the 120permutations we created prompts for each bug resulting in a total of 32prompts.
we computed the pass for each prompt and then averaged these scores across the bugs for each permutation.
the resulting violet histogram in figure represents the mean pass performance across these permutations.
we conducted a similar analysis for different fact subsets also shown in figure .
with 7facts there are 32prompts.
for each subset we calculated the mean pass across the bugs.
the yellow histogram represents the distribution of the mean performance over these fact subsets.
comparing the two histograms reveals that the variability due to fact order violet is relatively narrow ranging from .24to0.
.
in contrast the impact of different fact subsets yellow is significantly broader ranging from .07to0.
.
this demonstrates that the selection of which facts to include fact subsets has a much greater influence on prompt performance than the specific order in which these facts are presented.
finding .
selecting facts has more impact on performance than ordering them.fig.
this upset diagram compares the fact sets that fix the most bugs in bgp157p ly1with using no facts encoded as including the buggy function and chain of thought instructions.
the fact sets shown on the left collectively fix bugs more than the single best fact set which fixes bugs.
the diagram is constructed by generating prompts from each fact set sending them to gpt .
for n responses and identifying the set of bugs with at least one passing response.
fig.
distribution of mean pass k evaluated on bgp32 .
the violet histogram reports the mean performance under permutation the yellow histogram reports mean by subset.
the wider range in the yellow distribution suggests that fact subsets have a stronger impact on performance than fact order.
v. s electing facts with maniple universal fact selection as demonstrated in section iv c does not achieve consistent performance across all the bugs in our dataset.
thus to automate creating bug tailored prompts we introduce maniple a random forest trained to select relevant facts for inclusion in the prompts.
we focus on the task of predicting the success or failure of test executions based on vectors representing features extracted from both the prompt and the code.
our training dataset dis constructed from the bgp157p ly1.
it consists of pairs j y where j b f jrepresents a job which is a tuple consisting of a bug balong with and a fact set f f y represents the probability of successfully fixing the bug bin a single trial given the fact set f f. this probability is computed using pass .
dconsists of bugs each with fact combinations totaling samples.
however since not all facts are available for every bug missing facts are labeled as none .
this reduces the number of unique prompts to .
we manually craft a set of features f j based on domain knowledge and the characteristics of the facts.
the featurefunction f j rmmaps the input job j jto an mdimensional feature space.
the goal is to train a machine learning model mcapable of using the feature vector f j to accurately predict the likelihood of success.
specifically the model is designed to learn a function m rm aiming to optimise the accuracy of success predictions as follows max m m f j y j y d .
this is achieved through an appropriate training process that adjusts the parameters of mbased on the training data d. a. feature selection to train a machine learning model to meet these objectives we define the feature vector f x t where bitvector b encodes the fact set f where b n. repository id repid uniquely identifies the source repository of the bug b. prompt length l the length of the prompt in either characters or tokens where l n0.
cross validation determines whether characters or tokens are chosen.
cyclomatic complexity c a measure of code complexity that quantifies the number of linearly independent paths through a program s source code.
this choice of features was guided by the investigation conducted on bgp157p ly1.
specifically the including repository id repid is supported by the findings in table iii which illustrate significant variability in the optimal bitvectors for fact selection across different projects.
this variability underscores the influence of the repository context on successful repair.
additionally prompt length l was identified as a crucial factor displaying a spearman correlation of .
with the pass for repair success accompanied by a highly significant p value of p .
this correlation holds for both token and character lengths of prompts indicating that an increase in prompt length is associated with a decrease in llm performance a conclusion that is further supported by the non monotonicity of adding facts to prompts as noted in figure as more facts increase prompt length.cyclomatic complexity emerged as another pivotal feature showing a negative spearman correlation of .1with the pass p value of .
unlike prompt length cyclomatic complexity does not depend on the fact set chosen recall that the buggy code itself is necessarily always included but remains instrumental in predicting the pass for a bug mainly for scenarios where no prompt generates a successful fix.
b.maniple a random forest for fact set selection leveraging these observations we present maniple a random forest model for the fact selection task and evaluate it in both regression and classification settings.
as a regressor the model directly predicts the pass for each job.
in the classification task we categorise the scale i.e.
based on the number of fact sets considered by the model.
our analysis reveals that classification performs better.
this finding can be attributed to the noise and significant variance present in our data as detailed in supplementary materials appendix a .
the classification method proved more robust to variance in pass compared to regression.
additionally ordering and ranking the fact sets did not yield comparable performance.
this is likely due to the variance in ranks which directly stems from the variance in pass .
additionally we trained maniple only on the top five highest performing fact sets.
these sets were identified using bootstrap aggregation where we evaluated the performance of each fact set across multiple bootstrap samples drawn from our dataset.
by aggregating the outcomes we identified the best performing fact sets.
we optimised the model s hyperparameters through a comprehensive grid search evaluating the performance of each combination of parameters.
to ensure the model s generalisability and to prevent overfitting we employed k fold cross validation with k .
vi.
c omparing maniple with sota llm b ased apr we compared maniple with existing zero shot nonconversational llm based apr methods that incorporate various types of information into prompts.
our baselines include approaches whose prompts include the following facts buggy function only denoted as t0 buggy function buggy class and used method signatures denoted as t1 an approach similar to the technique by chen et al.
.
buggy function combined with github issue denoted as t2 an approach similar to the technique by fakhoury .
buggy function alongside error information denoted ast3 an approach similar to the technique by keller et al.
.
for a fair comparison we supply the same prompts built from these facts to maniple and all the baselines.
these prompts unsurprisingly differ structurally from the prompts on which the baselines were run.
for example our prompt incorporates our chain of thought instructions defined in supplementary materials appendix c .
our focus here is on comparing maniple s performance to the baselines w.r.t.
facts not their performance given approach specific optimaltool fixed correct correct t0 t1 t2 t3 maniple table v comparison of tool performance on the bgp157p ly2 test set focusing on bugs fixed with gpt .
.
fixed represents the number of bugs with test passing patches and correct indicates bugs fixed with patches identical to the developer s. we observe that maniple outperforms all the fact set combinations used.
this shows that bug tailored fact selection can improve the repair success.
prompts whose construction would require close cooperation with the authors of each baseline.
we evaluated tool efficacy according to two criteria based onn responses section iii number of plausible fixes i.e.eq.
and the number of correct fixes determined by the first plausible patch generated by the llm and subsequently subjected to manual evaluation.
for assessing patch correctness we employ an interrater agreement scale where indicates patches syntactically equivalent to the developer s patch allowing for minor refactoring or restructuring patches that achieve the intended outcome through an alternate method diverging from the developer s patch rendering correctness indeterminable and incorrect patches irrelevant incomplete introducing regressions .
each patch undergoes evaluation by two independent raters.
in cases of scoring discrepancies the raters discuss them.
if the discrepancy is not resolved the more conservative lower score is recorded ensuring a rigorous standard of correctness.
following this scoring system patches with the scores of and are labelled as correct and with the scores of and are labelled as incorrect.
the analysis in table v underscores the potential of bugtailored prompt selection for boosting automated program repair efficacy.
previous llm based zero shot non conversational approaches represented by t1 t2andt3 that always use the same set of facts achieve a maximum of plausibly fixed patches on bgp157p ly2.
in contrast maniple which leverages bug tailored fact selection identifies a significantly higher number of plausible patches .
furthermore maniple boasts additional correct fixes compared to the best of these approaches.
these findings suggest that tailoring the fact set to the specific context of each bug has the potential to improve the upper bound for repair success.
surprisingly t1 which utilises function code alongside scope information including class information and invoked functions fixes fewer bugs compared to using function code alone.
however this does not imply that scope and class information are useless.
t1still identifies plausible patches of which are correct that would not be found using just the function code as the only fact.
vii.
t hreats to validity in assessing llms we recognise the challenge of potential data leakage as some training datasets are not publicly available.
this limits our ability to know exactly what informationthe model has encountered.
however our primary focus is on the impact of different facts on repair performance rather than the performance itself.
for instance if a prompt with an angelic forest leads to a bug fix while a prompt without it does not this highlights the significant role of angelic forest in enhancing bug fixing performance.
this remains true regardless of the bug s presence in the training data.
thus our findings are robust even with possible data leakage and using the same model across our baselines partially mitigates this issue.
the facts collected for our study represent our best effort.
although to the best of our knowledge this work considers the widest range of information in apr literature we acknowledge that our fact set is provisional and will undoubtedly change as research into llm assisted apr continues.
the external validity of our findings relies on the distribution of bugs within our dataset.
our dataset is a subset of bugsinpy a benchmark published at fse .
bugsinpy is a curated dataset built to best practice from github repos with more than 10k stars that have at least one test case in the fixed commit that distinguishes the buggy version from its fix.
its representativeness has not been challenged and from first principles we can think of no reason that our filtering section iii a would introduce systematic bias.
viii.
r elated work this work is relevant to the areas of prompt engineering and automated program repair.
prompt engineering recent advancements in prompt engineering have significantly influenced the effectiveness of models like chatgpt.
notably the tree of thoughts approach and the zero shot cot approach have emerged as pivotal strategies.
frameworks like react use llm to generate reasoning traces and task specific actions in an interleaved manner.
self consistency is an approach that traverses multiple diverse reasoning paths through few shot cot and uses the generations to select the most consistent answer.
automatic prompt engineer proposes a framework for automated prompt generation.
it frames the task as a natural language synthesis task to construct prompts.
this is orthogonal to our approach as our task is to select ideal facts and the task of the automatic prompt engineer is to refine the prompt into which the selected facts can be directly plugged.
repository level prompt generation rlpg is a very general framework for retrieving relevant repository context and constructing prompts instantiated for code completion.
rlpg generates prompt proposals and uses a classifier to choose the best one.
in contrast our fact selection problem aims to find an optimal combination of facts to repair a given bug.
our work also uses a wider variety of information incorporating apart from code context dynamic and external information.
traditional program repair traditional apr techniques use search e.g.genprog or program synthesis such as semfix angelix se esoc and trident .
this study borrows the concept of angelic forest from the synthesis based tools as one of the considered facts demonstrating its utility in llm based apr.program repair with contextual information coconut utilizes surrounding contextual information to train an ensemble of neural machine translation models.
rete employs conditional def use chains as context for codebert .
capgen utilises ast node information to estimate the likelihood of patches.
dlfix treats the program repair task as a code transformation task learning to transform by additionally incorporating the surrounding context of the bug.
the context used in this work includes the class and scope information of the buggy program which is broader than the contexts used above.
fitrepair constructs prompts using identifier extracted from lines that look similar to the buggy line.
although our work does not directly provide identifiers statically as python is dynamic we provide dynamic values of the variables during the test run.
llm based program repair llm based techniques are making strides in apr.
inferfix uses few shot prompting to repair issues from infer static analyser.
chatrepair uses interactive prompting constructed using failing test names and their corresponding failing assertions.
our approach focuses on the zero shot non conversational setting but can be potentially integrated with inferfix and chatrepair.
various approaches focused on program engineering for apr e.g.incorporating bug related information within the prompts as the quality of fixes could be enhanced by integrating contextual information such as the bug s local context and details about relevant identifiers into the prompt.
xia et al.
utilize relevant identifiers to augment the fix rate of prompts.
keller et al.
reveals that for debugging tasks focusing on the specific line indicated by a stack trace is more effective than providing the entire trace.
similarly fakhoury et al.
investigates how combining issue descriptions with bug report titles and descriptions enhances program repair efforts.
following recent research we refer to such pieces of information asfacts .
our work uses individual facts from previous work to formulate and motivate the fact selection problem.
program repair for python quixbugs is a benchmark consisting of small programs in java and python.
they are not reflective of real software projects.
bugswarm was constructed by automatically mining failing ci builds and thus contains issues outside of the scope of our study such as configuration issues.
bugsinpy manually curates bugs from popular python projects.
we selected bugs from this benchmark that require modifications within a single function and which we managed to reproduce.
rete a program repair tool that leverages contextual information was evaluated on both python and c using bugsinpy as its python benchmark.
we did not compare maniple with rete because it relied on the line granular perfect fault localisation pfl while this study uses function granular pfl.
pyter is a program repair technique that focuses on python typeerrors it was evaluated on a custom benchmark for type errors.
ix.
c onclusion in this paper we explore the construction of effective prompts for llm based apr leveraging facts extracted from thebuggy program and external sources.
through a systematic investigation we incorporate seven bug relevant facts into the prompts notably including angelic values a factor previously not considered in this domain.
furthermore we define the fact selection problem and demonstrate that a universally optimal set of facts for addressing various bugs does not exist.
building on this insight we devise a bug tailored fact selection strategy enhancing the effectiveness of apr.