licoeval evaluating llms on license compliance in code generation weiwei xu kai gao hao he minghui zhou school of computer science peking university beijing china key laboratory of high confidence software technologies ministry of education china university of science and technology beijing beijing china carnegie mellon university pittsburgh usa xuww stu.pku.edu.cn kai.gao ustb.edu.cn haohe andrew.cmu.edu zhmh pku.edu.cn abstract recent advances in large language models llms have revolutionized code generation leading to widespread adoption of ai coding tools by developers.
however llms can generate license protected code without providing the necessary license information leading to potential intellectual property violations during software production.
this paper addresses the critical yet underexplored issue of license compliance in llm generated code by establishing a benchmark to evaluate the ability of llms to provide accurate license information for their generated code.
to establish this benchmark we conduct an empirical study to identify a reasonable standard for striking similarity that excludes the possibility of independent creation indicating a copy relationship between the llm output and certain opensource code.
based on this standard we propose l icoeval to evaluate the license compliance capabilities of llms i.e.
the ability to provide accurate license or copyright information when they generate code with striking similarity to already existing copyrighted code.
using l icoeval we evaluate popular llms finding that even top performing llms produce a non negligible proportion .
to .
of code strikingly similar to existing open source implementations.
notably most llms fail to provide accurate license information particularly for code under copyleft licenses.
these findings underscore the urgent need to enhance llm compliance capabilities in code generation tasks.
our study provides a foundation for future research and development to improve license compliance in aiassisted software development contributing to both the protection of open source software copyrights and the mitigation of legal risks for llm users.
i. i ntroduction recent advances in large language models llms have instigated revolutionary changes in the fields of artificial intelligence natural language processing and software engineering .
with billions of parameters trained on extensive corpora both general and software engineering specific llms have demonstrated extraordinary competencies in various software engineering tasks such as code generation program repair and documentation generation .
specifically llms remarkable code generation capabilities enabled the rapid adoption of ai coding tools in practice.
as github reports of us based developers are already using ai coding tools both inside and outside of work .
however the widespread utilization of llms for code generation has also elicited concerns regarding security minghui zhou is the corresponding author.
privacy and legal issues .
a key issue among these is the potential infringement of intellectual property ip rights of a vast number of open source developers .
due to the fact that llms are trained on extensive volumes of open source code governed by open source licenses and their inherent ability to recognize and memorize patterns they may generate code snippets that are similar or even identical to those in the training data under certain prompts .
consequently the use of these generated code must comply with the terms and conditions in the open source license concerning how a piece of open source software oss can be reused modified and redistributed .
licenses which are unavoidably linked to the open source code serve a dual purpose they not only enforce the developers commitment to sharing transparency and openness but also necessitate that those who reuse the code respect and adhere to the terms set by the original authors .
for example under apache .
authors grant users perpetual copyright and patent rights but users must also comply with the corresponding license terms such as including attribution notices and modification statements when redistributing the software.
despite their extraordinary competencies llms often overlook license information when they memorize and generate code identical to the works of open source developers.
this has sparked a growing wave of concern and discontent among the open source community .
for example an opensource developer accuses copilot of emitting large chunks of my copyrighted code with no attribution no lgpl license.
more than just ethical concerns these actions by llms may also pose unpredictable legal risks to users.
for example if llms produce open source code without providing the necessary license information users are naturally unable to utilize such code in legal compliance processes .
it should be noted that openai github and microsoft are currently facing lawsuits as github copilot has been implicated in reproducing licensed code without compliance with the corresponding license terms .
these controversies underscore that the license compliance capabilities of llms i.e.
the ability to provide accurate license and copyright information during code generation play a crucial role in both protecting the ip rights of numerous open source 1arxiv .02487v3 feb 2025empirical study on striking similarity standard starcoder training dataset codeparrot dataset cleanedaccessed data unseen datacomparewizardcoder porobenchmark construction framework benchmark evaluation ... fig.
.
overview of this study.
developers and shielding users of such models from unforeseen legal risks.
therefore it is important to evaluate the license compliance capability of llms for code generation tasks.
to the best of our knowledge it remains unclear how well llms perform in terms of license compliance despite the substantial effort devoted to evaluating llms performance on code completion accuracy robustness security and privacy among others.
to fill this knowledge gap this paper makes the first attempt to evaluate the license compliance capabilities of llms by establishing a novel benchmark.
this evaluation of llms encompasses considerations of not only a technical nature but also legal aspects.
specifically one of the most challenging aspects of assessing the compliance capabilities of llms is determining whether their outputs when similar to specific open source code snippets are derived from those snippets or are independently created yet merely coincidentally similar.
the former scenario would entail issues of license compliance whereas the latter does not.
a key and universally acknowledged principle used in law to aid in this distinction is access and substantial similarity which means that potential infringement only occurs when a party has access to a work and the resulting product is substantially similar to that work.
alternatively in the absence of evidence indicating direct access the existence of copying can be established when the degree of similarity between two works is so striking as to preclude the possibility of independently arriving at identical outcomes .
considering that most llms training data are undisclosed it is challenging to determine and verify whether an llm has accessed a specific code snippet.
therefore we focus on scenarios where the outputs of llms are strikingly similar to open source code to assess their compliance capabilities in this context.
as demonstrated in figure to establish the benchmark we first conduct an empirical study to explore a reasonable standard of striking similarity to distinguish between cases where llms generate code without prior exposure to similar open source instances and cases where llms memorize open source code in the training data.
based on the findings of this empirical study we propose an evaluation benchmark l icoeval for llms li cense co mpliance capability evaluation and then evaluate popular llms currently in use for code generation.
we find even top performing code generation llms produce a non negligible proportion .
to .
of strikingly similar output compared to existing open source code.
most models fail to provide any license information for code snippets under copyleft licenses with only claude .
sonnet demonstrating some ability to correctly provide license information for such code.
these results highlight the urgent need to improve license compliance capabilities in llms used for code generation particularly in handling code under copyleft licenses and providing accurate license information.
in summary the contributions of this paper are as follows we conduct an empirical study to explore a preliminary standard for striking similarity under the legal context of intellectual property infringement and llm s capabilities in independently generating non trivial code.
we design a framework for evaluating the license compliance capabilities of llms in code generation and provide the first benchmark for evaluating this capability.
we evaluate the license compliance capabilities of popular llms providing insight into improving the llm training process and regulating llm usage.
ii.
b ackground and related work a. license compliance and ip infringement open source software authors grant copyrights and in some cases patent rights within ip right through open source licenses.
however this grant is conditional and the licenses specify the obligations to which users must comply .
it is important to note that open source authors do not oppose or prohibit the reuse of their code in fact such reuse aligns precisely with their foundational motives for choosing to opensource their code.
nevertheless they require users to comply with the stipulations set forth in the licenses when reusing the code.
failure to do so can lead to license non compliance ultimately resulting in intellectual property infringements .
in the context of llm code generation if models output licensed open source code without providing license information users reusing this code might face compliance risks.
this concern is particularly relevant for companies and organizations as it could lead to copyright infringement liabilities and potentially result in significant economic compensation and harm to their reputation and operations .
in the judicial context it is crucial to determine whether the content generated by llms has a copying relationship with specific open source code.
according to existing legal principles copying can be established when similarities 2between two works are so striking that they preclude the possibility of independently arriving at the same result .
however the definition of striking similarity remains ambiguous.
courts consider several factors in their analysis such as the uniqueness intricacy or complexity of similar sections and the appearance of the same errors or mistakes in both works .
the ambiguous definition coupled with the impressive capabilities of llms in independently generating non trivial code snippets motivates our empirical study to explore reasonable standards for identifying striking similarity in the context of llm generated code.
b. memorization in llms for code research consistently demonstrates that general llms tend to memorize content from their training sets especially in larger models raising significant concerns regarding ip rights violations .
this memorization also occurs in llms for code potentially causing compliance issues through unintentional output of licensed code .
recent studies on memorization and ip infringement in llms for code particularly those trained on non public datasets typically compare llm outputs with existing open source code to detect memorization.
however with undisclosed training sets it is challenging to definitively attribute this similarity to memorization rather than llms extraordinary generalization competencies.
this ambiguity extends to numerous code clone detection methods which describe the degree of similarity between code snippets from various perspectives such as textual similarity call graphs and other structural features .
yet we still lack a clear understanding of what degree of similarity is sufficient to constitute striking similarity that can exclude the possibility of independent creation.
furthermore it remains uncertain whether llms can accurately provide the corresponding copyright and license information for code they appear to have memorized.
the most relevant work by yu et al.
investigates to what extent llms generate licensed code.
however their study was conducted under a problematic assumption that llms should not generate licensed code at all which fundamentally misunderstands the nature of open source software.
open source code by definition permits reuse under specific conditions mere reuse does not inherently constitute infringement.
the key issue lies in whether the reuse complies with the specified license terms which however is not investigated in their work and requires non trivial efforts in the llm context.
to address these challenges and uncertainties we conduct an empirical study to establish a standard of striking similarity and build a benchmark to evaluate llm s compliance capability.
c. evaluations of llms for code generation in the realm of code generation with llms numerous benchmarks have been introduced to evaluate these models capabilities .
these benchmarks typically focus on generating code snippets from natural language descriptions employing metrics such as pass k to assess thetable i model size and performance pass on the human eval benchmark for models trained on fully open source datasets .
type model size humaneval dataset general poro 34b chat 34b .
the stack codestarcoder2 15b instruct 15b .
the stack v2 wizardcoder 15b v1.
15b .
the stack starcoder 15b .
the stack codeparrot .5b .
codeparrot clean accuracy of the generated code.
furthermore many studies separately investigate the non functional properties of llms for code including robustness security privacy and explainability .
however to our knowledge there has been no evaluation focusing on the compliance capabilities of these models.this lack of evaluation is disadvantageous for users seeking models with lower legal risks and also hinders model developers from making targeted improvements in this critical area during the training process which motivates our work.
iii.
e mpirical study on standard of striking similarity a. research question rq where might the reasonable standard of striking similarity lie in the context of code generation by llms?
a significant challenge in evaluating the compliance capabilities of llms is determining whether there exists a copying relationship between the output of llms and existing opensource code.
due to the remarkable code generation abilities of llms it is plausible that they can independently generate code that is similar to a specific open source code snippet even without prior exposure to it.
if the code is independently generated it clearly does not necessitate the provision of corresponding copyright information.
in accordance with the legal principle of striking similarity we aim to explore where the reasonable standard of striking similarity might lie for llms.
the empirical standard explored in this study lays a foundation for our subsequent evaluation framework and benchmark.
our exploration strives to align with copyright law principles in identifying a relatively reasonable standard that can to some extent evaluate the compliance risks associated with using certain llms.
notably our findings are not intended to establish definitive legal boundaries .
instead they are intended to serve as a guide for understanding and identifying potential compliance issues offering insights that can inform future research and development in this rapidly evolving field.
b. selection of llms we aim to select representative llms to explore the striking similarity standards with the following principles high accuracy in code generation tasks as license compliance has no meaning without accuracy a model can code for unpacking zip files from ilearn import zipfile tar usr bin tar def unzip zfile outdir unpack a zip file into the given output directory outdir return true if it worked false otherwise try zf zipfile.zipfile zfile zf.extractall outdir return true ... omitted due to space limitations comments in file header import statements and global variables function signature docstring function bodyfig.
.
structure of function level code snippet.
generate very chaotic functionally incorrect code that naturally does not resemble existing open source code .
all training data including the instruction tuning data is public.
we need to ascertain the data that the model has been exposed to.
we identify three widely used open sourced datasets for training llms on the huggingface platform i.e.
the stack the stack v2 and codeparrot dataset cleaned .
based on these datasets we find five popular llms trained on these datasets as listed in table i. among these llms we select wizardcoder 15b v1.
as our research subject for two reasons.
first it shows an acceptable performance with a pass score of .
on the humaneval benchmark.
second it has a manageable size i.e.
15b parameters under our hardware constraints which is comparable to or even larger than the llms studied in previous work .
wizardcoder1uses an open source instruction following dataset code alpaca5 to fine tune the starcoder trained onthe stack dataset.
the complete open source nature of its training datasets facilitates comprehensive analysis and verification.
despite starcoder2 15b instruct s higher pass score on humaneval we chose wizardcoder for our analysis due to its smaller more manageable dataset 200b tokens vs. 900b tokens in starcoder2 15b instruct s .
c. experiment setup in order to detect striking similarity we design an experiment as follows.
first we construct two distinct groups of code samples unseen and a ccessed to simulate two different scenarios.
the first scenario is where the model independently completes the corresponding code as it has not been exposed to the code from the u nseen group before.
the second scenario pertains to instances where the generated content cannot be regarded as independently created potentially implicating a copying relationship.
this is attributed to the fact that samples within the a ccessed group are derived from the model s training dataset.
we select two groups of code samples for this study as described in section iii d1.
1in this paper wizardcoder refers to wizardcoder 15b v1.
in all subsequent mentions.second we construct prompts using the u nseen and accessed groups then instruct wizardcoder to complete the code snippets.
our goal is to observe potential differences in similarity when the model generates code for these two distinct groups.
as illustrated in figure we divide the function level code snippets into five parts file header comments import statements and global variables function signature docstring and function body.
we combine the first four parts into a prompt with the aim of providing the model with as complete an input context as possible and then instruct the model generate the function body.
we conduct experiments in a oneshot fashion using greedy decoding temperature set to .
third we select specific features derived from copyright law principles to characterize striking similarity as described in section iii d2.
by analyzing the similarity between the llm s output and the original code snippets when completing tasks from both groups we aim to identify a relatively reasonable standard for striking similarity .
if the generated code meets this standard for striking similarity it can be inferred that the code may not be an independent creation by the model indicating a possible copying occurrence.
d. method construction of code samples figure illustrates the overview of the construction method.
our code samples originate from two sources the training set of wizardcoder i.e.
starcoderdata and codeparrot clean dataset .
starcoderdata a subset of the stack comprises 783gb of code spanning across programming languages.
a license filtration mechanism was implemented during the data collection process for starcoderdata which guarantees that all code files within starcoderdata are sourced from the repositories under permissive licenses .
codeparrot clean dataset comprises python files from github including those under restrictive licenses such as gpl .
.
the former is utilized to construct the a ccessed group that wizardcoder has been exposed to while the latter is used to construct the u nseen group that wizardcoder has never encountered before.
given that llms are not yet adept at performing code completion beyond the granularity of functions we choose to conduct our experiments in alignment with well known accuracy benchmarks such as humaneval specifically at the function level granularity.
moreover following the existing research on memorization of llms for code our study focuses on python considering its prevalence .
to construct the a ccessed group we first extract function level code snippets from starcoderdata s python files.
we further select samples based on the following principles resulting in function level code snippets the function must have a docstring and the function body must be more than six lines the median value of all functions lengths which aims to provide llms with more comprehensive descriptions of the function s capabilities and exclude overly trivial code snippets the function should not be a class method due to their typically more complex context dependencies the function level code snippet 4starcoderdata codeparrot clean function level code snippets function level code snippetshash bucketsaccessed unseensplit splitsample calculate calculatelsh query files under restrictive licensesfilter no similar code snippet foundminhash minhashfig.
.
overview of code samples construction.
has no syntax errors.
eventually we randomly sample functions as the a ccessed group that wizardcoder has been exposed to.
to construct the u nseen group we select code files from the repositories authorized under restrictive licenses incodeparrot clean dataset and segment them into functions.
given that starcoderdata excludes code from such repositories it is highly probable that wizardcoder has not encountered these code snippets in its training set.
this makes these functions our preliminary candidate samples.
to ensure these functions are truly unseen by wizardcoder we need to compare each function with the entire training set.
however due to the large scale of the dataset direct comparison of each function with every training sample is impractical.
we therefore adopt the deduplication strategy used instarcoderdata .
we calculate a minhash for each function in the training set and use locality sensitive hashing lsh to efficiently map similar functions into the same bucket using grams and a jaccard similarity threshold of .
more stringent than the .
threshold used in starcoderdata for this process.
then we compute the minhash for each function in preliminary candidate samples from codeparrot clean dataset and query these in the lsh buckets of the training set.
functions without similar matches in the training set are considered final candidates.
we apply the same three principles used in constructing a ccessed group and sample samples from final candidates to form the u nseen group.
eventually we obtain the following two groups of code snippets unseen python function level code snippets that wizardcoder has never been exposed to.
accessed python function level code snippets from the training set of wizardcoder .
table ii presents some feature statistics of these samples in two different groups including the number of prompt lines the number of function body lines the cyclomatic complexity of the function and the number of comments in function body.
we compare these features between two groups using mannwhitney u test and cliff s delta effect size test .
the mann whitney u test determines whether there is a significant difference in their median values while the cliff s deltaquantifies the extent of this difference .
our analysistable ii statistics of features from function level code snippets in the two groups and the results of mann whitney utest and cliff sdelta effect size test .
prompt lines body lines complexity commentsunseenmin median mean .
.
.
.
max 244accessedmin median mean .
.
.
.
max p value .
.
.
.
cliff s .
.
.
.
eff.
level negligible negligible negligible negligible reveals that although the p value is less than .
the effect size is negligible.
this suggests that there are no substantial inherent differences in the code characteristics between the two groups.
features to characterize string similarity considering that copyright law only protects expression rather than ideas we employ fundamental text similarity metrics including bleu jaccard similarity based on minhash and similarity based on edit distance to measure the similarity between the function bodies output by the model and the original implementations in both a ccessed and u nseen groups.
inspired by judicial considerations of striking similarity in fields like music and code s unique traits we incorporate additional literal features that capture both structural and stylistic elements to characterize striking similarity including the number of function body lines cyclomatic complexity and comment similarity.
the rationale behind this is as follows number of function body lines.
given the inherent nature of llms in generating code through token by token prediction it implies that in scenarios of independent creation longer code lengths reduce the likelihood of achieving occasional similarity.
cyclomatic complexity .
determined by decision points it increases potential paths through a function.
more decision points expand the possibility space significantly reducing the chance of occasional similarity.
the similarity of comments.
different developers have unique habits when it comes to writing comments.
unlike code natural language possesses a higher degree of flexibility making the similarity in comments within code exceedingly rare.
.
.
.
.
.
similarity accessed unseenbleu jaccard ed .
fig.
.
the similarity between output of wizardcoder and the corresponding open source code in two groups.
e. results figure shows the similarity between the outputs generated bywizardcoder and the corresponding open source code in two groups.
when wizardcoder completes code for u nseen group which simulates scenarios of independent creation it generally produces code with lower similarity to the corresponding open source code compared to its output for the accessed group across all similarity metrics.
however we observe some u nseen cases with exceptionally high similarity occasionally reaching a score of .
these findings suggest that while text similarity metrics can provide indication of differences in generated code between the two scenarios they are insufficient on their own to definitively determine striking similarity and consequently non independent creation.
figure a illustrates the distribution of similarity between the generated code and the corresponding open source code in two groups in relation to the number of function body lines of the open source code.
we observe that only for a ccessed group does the llm frequently generate highly similar code snippets similarity .
for functions with longer body lengths lines .
in contrast for longer functions from unseen group the similarity scores are generally lower.
as shown in figure b a similar phenomenon is observed with cyclomatic complexity.
meanwhile figure c reveals a stark contrast in generated comments between the two groups.
for unseen group the llm rarely generates comments identical to those in the open source code.
however for the a ccessed group it frequently produces comments that match those in the open source code.
in extreme cases the identical comments can exceed sentences.
these observations reveal distinct patterns in code generation between a ccessed and u nseen groups.
such distinctions suggest that certain combinations of these features might effectively indicate non independent creation.
to formalize this insight we seek to establish a quantitative standard for identifying instances of striking similarity .
based on our analysis of wizardcoder s code generation results in these two simulated scenarios we establish an initial standard for striking similarity between generated code and the open source code number of function body lines cyclomatic complexity text similarity maximum of the three metrics .
number of identical comments the first two criteria are attributes of the open source code snippet while the latter two describe the relationship between the generated code and the corresponding open source code snippet.
under this standard we identify instances all from a ccessed group i.e.
the non independent creation scenario.
in the simulated independent creation scenario unseen group none of the functions generated by wizardcoder meets this standard.
f .
validation through the above experiments we establish a preliminary standard for striking similarity between llm outputs and open source code.
this standard when met suggests a potential copying relationship rather than independent creation.
to validate the preliminary standard we employ a three step process constructing new code samples expanding our analysis to additional llms and conducting an expert evaluation to assess the standard s validity and applicability.
first using the same methods employed in constructing accessed and u nseen groups we create two additional groups a ccessed eval and u nseen eval each containing samples that do not overlap with the original accessed and u nseen groups.
we then utilize wizardcoder andporo 34b chat to complete the functions for these samples.
we choose poro because it is a general model yet shares the same code related training data with wizardcoder .
this similarity in training data allows us to use the same two groups of samples we constructed while also testing the standard s applicability to a more general model.
among the outputs generated by these two models from wizardcoder and from poro meet our standard of striking similarity .
all samples are from the a ccessed eval group yielding a precision of .
to further validate the standard we assemble a diverse panel five developers with over six years of coding experience and three lawyers specializing in software intellectual property.
this composition ensures both technical and legal perspectives in evaluating these samples.
the eight reviewers are tasked with determining whether independent creation can be ruled out based solely on the comparison between the code pairs without knowledge of their origins.
this approach ensures the standard s applicability to outputs from more advanced models as the evaluation focuses purely on code characteristics.
on average each reviewer identifies out of the sample pairs as cases where independent creation can be excluded implying a potential coping relationship.
the experts cited three categories of reasons for their judgments textual and structural similarities this includes identical or similar variable names overall textual resemblance and similarities in code structure such as indentation and blank lines .
a b c fig.
.
the distribution of similarity between the generated code and the corresponding open source implementations in two groups in relation to the number of function body lines cyclomatic complexity and the number of same comments.
the similarity value is the maximum of the three text similarity metrics.
creds none the file token.picklestores the user s access and refresh tokens and is created automatically when the authorization flow completes for the first time.if os.path.exists tok file with open tok file rb as token creds pickle.load token if there are no valid credentials available let the user log in.if not creds or not creds.valid if creds and creds.expiredand creds.refresh token creds.refresh request else flow installedappflow.from client secrets file cred file scopes creds flow.run local server port save the credentials for the next runwith open tok file wb as token pickle.dump creds token service build sheets v4 credentials creds call the sheets apisheet service.spreadsheets result sheet.values .get spreadsheetid sample spreadsheet id range sheet1!a2 b .execute values result.get values if not values print no data found.
else for row in values print row creds none the file token.picklestores the user s access and refresh tokens and is created automatically when the authorization flow completes for the first time.if os.path.exists tok file with open tok file rb as token creds pickle.load token if there are no valid credentials available let the user log in.if not creds or not creds.valid if creds and creds.expiredand creds.refresh token creds.refresh request else flow installedappflow.from client secrets file cred file scopes creds flow.run local server port save the credentials for the next runwith open tok file wb as token pickle.dump creds token service build sheets v4 credentials creds call the sheets apisheet service.spreadsheets listofstuff sheet.values .get spreadsheetid sample spreadsheet id range a1 d1 .execute listofstuff int listofstuff listofstuff int listofstuff retval str listofstuff str listofstuff return retvalgenerated bywizardcoder open source code from future import print functionimport picklefrom settings import import os.pathfrom googleapiclient.discoveryimport buildfrom google auth oauthlib.flowimport installedappflowfrom google.auth.transport.requests import requestimport json scopes cred file os.path.join game folder credentials.json tok file os.path.join game folder token.pickle def winner shows basic usage of the sheets api.prints values from a sample spreadsheet.
prompt same comments body line text similarity .
ed cyclomatic complexity the basis for judgment in human validation their original comment almost all of the variable names in both are identical both in case uppercase or lowercase and word choice which is hard to come by in standalone creations.
the content and placement of the comment lines in the two pieces of code are exactly the same.
all the comments in the two code snippets are the same and almost all identifier in the two code snippets are the same .
... from future import absolute import division print function unicode literals version .
import datetimeimport osfrom botocore.exceptionsimport clienterrorimport boto3 success success failed failed rds boto3.client rds dbsnapshotid os.environ.get dbsnapshotidentifier dbinstanceid os.environ.get dbinstanceidentifier now datetime.datetime.now def send event context response status reason none response data none physical resource id none building own response function same comments body line text similarity .
bleu cyclomatic complexity 2response data response dataor response body json.dumps status response status reason reason or see the details in cloudwatch log stream context.log stream name physicalresourceid physical resource idorcontext.log stream name stackid event requestid event logicalresourceid event data response data opener build opener httphandler request request event data response body request.add header content type request.add header content length len response body request.get method lambda put try response opener.open request print status code .format response.getcode print status message .format response.msg return trueexcept httperroras exc print failed executing http request .format exc.code return falseresponse body status response status reason reason or see the details in cloudwatch log stream context.log stream name physicalresourceid physical resource idor context.log stream name stackid event requestid event logicalresourceid event data response data opener build opener httphandler request request event data json.dumps response body .encode utf request.add header content type request.add header content length len response body request.get method lambda put try response opener.open request print status code .format response.getcode print status message .format response.msg return trueexcept httperroras e print failed executing http request .format e.code return false example not strikingly similar by our standard but somewhat ambiguousexample strikingly similar by our standard fig.
.
examples of cases above and below our striking similarity standard.
logical and functional similarities this encompasses similar approaches to problem solving comparable use of specific programming constructs and similarities in core logic.
comment and unique feature similarities this involves comments similarity including content and formatting as well as any distinctive elements like special punctuation or unusual coding patterns.
an example of striking similarity is shown in figure example .
in this example the striking similarities are evident in multiple aspects identical variable names both in case and word choice matching comment content and placement and highly similar code structure.
as noted by experts such extensive similarities would be hard to come by in standalone creations.
we generally find that the characteristics the experts are looking for align well with our striking similarity standard.
based on these results we believe that our proposed standard serves as a reasonable preliminary standard for identifying instances with striking similarity that may indicatepotential legal risks.
while not intended to establish definitive legal guidelines this standard offers insights for exploring the complex landscape of ai generated code and associated copyright concerns.
summary through a comparative analysis of llm generated outputs derived from previously accessed and unseen code samples we establish and validate a preliminary standard of striking similarity that effectively excludes the possibility of independent creation.
implications text similarity alone cannot determine nonindependent creation in llm generated code.
llms can produce highly similar code even for unseen simple functions.
this necessitates using adequately complex code as defined by our established standard when constructing llm evaluation benchmarks.
llms can memorize and reproduce comments from training data guiding our development of evaluation frameworks for compliance capabilities.
this finding informs our subsequent investigation into llms ability to recall and generate license information in file headers.
7code files under different licensesimport zipfiledef unzip zfile outdir unpack a zip file into the given output directory outdirreturn true if it worked false otherwise prompts function level codesnippets llmgenerated codesimilarityanalysis striking similarityllmlicenseask for license information check fig.
.
overview of the evaluation framework.
iv.
e valuation framework and benchmark for llm l icense compliance a. evaluation framework as illustrated in figure we propose a framework to evaluate llms on license compliance in code generation.
this framework is grounded in the empirical findings that llms in non independent creation scenarios may generate code strikingly similar to existing implementations accompanied by identical comments.
this observation leads to a crucial question if llms can reproduce code and comments with high fidelity can they also accurately output the associated license information typically found in file comments?
our framework builds upon this insight positing that when an llm generates code strikingly similar to existing code it should also be capable of providing the corresponding license or copyright information.
this principle forms the foundation of our evaluation methodology linking the generation of strikingly similar code to the ethical and legal responsibility of proper attribution and license compliance.
the framework operates by first constructing a benchmark comprising functions from widely reused code files that have explicit copyright information in their file header comments.
the detailed construction method is elaborated in sectioniv b. the structure of the functions in this benchmark aligns with that as shown in figure .
our prompt consists of the first four components and we task the llm to complete the function body.
subsequently we conduct a similarity analysis between the llm s output and the corresponding open source code snippet.
if the output meets our established standard forstriking similarity we prompt the llm in the form of a follow up inquiry to output the license information.
finally we compare the license information provided by the llm with the actual license of the open source code snippet.
b. benchmark licoeval construction method we construct our benchmark named l icoeval by mining code files from the opensource ecosystem that have explicit license information and are widely reused.
this process adheres to the standards we previously established.
the specific steps are as follows.
data source world of code.
we utilize world of code woc as the source for constructing the benchmark.woc is a comprehensive infrastructure for mining version control data across the entire open source software ecosystem.
it aggregates git objects including commits trees and blobs on platforms like github bitbucket and gitlab.
woc provides several key value databases that enable efficient querying of relationships between different entities .
for instance the blob to project b2p database maps each blob to all projects that contain it enabling efficient tracking of code reuse across projects.
for our benchmark we use version u of woc released in october .
this version encompasses over million git repositories .
billion commits .
billion trees and .
billion blobs .
collecting python code files.
our first step utilizes woc s c2fbb database which maps each commit to its corresponding commit file name new blob post commit and old blob precommit .
we filter these commits to focus on python files based on file extensions.
by selecting the new blobs associated with these commits and removing duplicates we obtain a dataset of unique python file blobs.
collecting licensed function level code snippets.
we identify blobs containing explicit license information in their file header comments.
using the b2p database we quantify each selected blob s occurrence across different projects.
we then sort these license containing blobs in descending order of project count aiming to identify widely reused code files with clear license information.
the presence of license information in file headers indicates clear copyright attribution while widespread reuse suggests general acknowledgment and acceptance of this copyright information.
we iterate through the sorted blobs in descending order segmenting each file into function level code snippets.
we then apply a filtering process to select snippets that not only adhere to the three principles described in section iii d1 but also meet the preconditions of our striking similarity standard function body lines complexity and comment number .
from this filtered set we select the top qualifying code snippets as candidate samples.
we further refine these candidate samples removing code snippets that are explicitly indicated as copied or derived from other sources identified by keywords like copied from or taken from and excluding code snippets with dual licenses to ensure license clarity.
we also perform deduplication based on 8function signatures and docstrings retaining only one instance where these elements are identical.
this rigorous filtering process ultimately yields unique function level code snippets for our l icoeval benchmark.
benchmark characteristics the detailed characteristics of l icoeval are as follows.
license information accuracy.
we employ a keyword and rule based method proposed by xu et al.
to identify licenses.
to evaluate the accuracy of license information in l icoeval we randomly sample samples confidence level confidence interval .
we manually review the file header comments and function docstrings of these samples to verify the correctness of the license information extracted from the file headers and to check for any exception statements in the docstrings.
we find that the license information for all samples is correct.
therefore we believe that the license information in l icoevalis highly reliable and can serve as a robust foundation for evaluating the compliance capabilities of llms.
license distribution.
licenses can be categorized into three types based on their level of permissiveness permissive weak copyleft and strong copyleft .
copyleft licenses mandate that software which modifies or utilizes existing software must be licensed under the same terms unless explicitly specified otherwise e.g.
gpl .
.
it is crucial to note that permissive licenses also require adherence to their terms.
a license is essentially a conditional authorization and failure to comply with conditions stipulated in permissive licenses can also result in non compliance issues e.g.
apache .
and cc by .
licenses require users to provide a statement of changes made to the original software.
therefore in l icoeval we retain a natural distribution of licenses covering the three different types of licenses.
as shown in figure out of function level code snippets the number of licenses for permissive weak copyleft and strong copyleft are .
.
and .
respectively.
among permissive licenses apache .
is the most prevalent accounting for snippets .
of the total .
for copyleft licenses the most common are gpl2.
or later with snippets .
gpl .
or later with snippets .
and mpl .
with snippets .
.
code metrics.
table iii presents statistics of l icoeval including the number of lines in function bodies body lines cyclomatic complexity the number of comments within function bodies comments and other relevant metrics.
due to bsl .
.
bsd clause .
isc .
wtfpl .
bsd clause .
unlicense .
agpl .
only .
cc by sa .
.
cecill .
.
1000gpl .
only .
gpl .
only .
lgpl .
only .
lgpl .
only .
agpl .
or later .
epl .
.
all licenses permissive strong copyleft weak copyleftapache .
.
mit .
mpl .
.
gpl .
.
gpl .
.
bsd clause .
lgpl .
orlater .
lgpl .
or later .
fig.
.
the distribution of licenses in the benchmark.table iii statistics of licoeval.
metric min median mean max distribution prompt lines .
prompt tokens .
body lines .
body tokens .
project reuse .
comments .
cyclomatic complexity .
we increment all values by one to plot the distribution in log scale.
our application of preconditions for striking similarity during the selection process all functions in the benchmark satisfy the criteria of body lines comments and cyclomatic complexity .
the metric reuse projects indicates the number of projects that have reused the blob containing the function.
a higher value suggests more widespread adoption in the open source ecosystem.
as evident from table iii all function level code snippets in l icoeval generally exhibit high complexity and are extensively reused.
this indicates that these code snippets are non trivial while also being widely acknowledged by the opensource community in terms of their copyright information.
v. e valuating llm s on license compliance a. experiment setup we evaluate popular llms that exhibit strong performance in code generation tasks pass .
on humaneval using l icoeval.
table iv presents the complete list of evaluated llms.
throughout the evaluation process we consistently employ a one shot approach using greedy decoding temperature .
effective assessment of license compliance requires a balanced consideration of two critical factors a model s propensity to generate code strikingly similar to existing implementations and its accuracy in providing licenses with particular emphasis on copyleft licenses.
this dual focus is essential as it addresses both the risk of unintended code replication and the model s cognizance of licensing obligations.
to quantify this balance we introduce a novel metric l ico license compliance calculated as follows lico w1 n w2 acc p w3 acc c w1 w2 w3 where nis the normalized number of generated code snippets reaching striking similarity.
acc pandacc crepresent the accuracy of license information provided by the llm for these strikingly similar code snippets under permissive licenses and copyleft licenses respectively.
we set the weights asw1 w2 andw3 emphasizing copyleft license compliance due to its associated legal risks.
for missing accuracy metrics we use a value of assuming optimal performance in the absence of data.
the l icoscore ranges from to with higher scores indicating less compliance risks.
it s crucial to note that a high l icoscore particularly 9table iv performance of llm s on licoeval.
means publicly available weights and means unavailable weights .
model humaneval weights striking simacc permissive accp copyleft accclico general llmgpt .
turbo .
.
.
.
.
.
gpt turbo .
.
.
.
.
.
gpt 4o .
.
.
.
.
.
gemini .
pro .
.
.
.
.
.
claude .
sonnet .
.
.
.
.
.
qwen2 7b instruct .
.
.
.
.
glm 9b chat .
.
.
llama 8b instruct .
.
.
.
.
code llmdeepseek coder v2 .
.
.
.
.
.
codeqwen1.
7b chat .
.
.
.
.
starcoder2 15b instruct .
.
.
.
.
codestral 22b v0.
.
.
.
.
.
.
codegemma 7b it .
.
.
.
.
wizardcoder python 13b .
.
.
.
.
.
a score of in the absence of any strikingly similar cases is not meaningful if the model s code generation performance is poor.
models producing erroneous or chaotic code may naturally avoid striking similarities resulting in high l ico scores that lack practical significance.
b. results table iv presents the performance of llms on l icoeval.
we first observe that the three llms currently performing best in code generation gpt 4o claude .
sonnet and deepseek coder v2 show significant variations in their results.
they produce .
.
and .
strikingly similar cases respectively which are not insignificant proportions indicating that compliance issues are not uncommon even among top performing models.
regarding the accuracy of providing license information deepseek coder v2 performs the poorest unable to provide any license information while gpt 4o performs the best with an accuracy of .
.
for the higher risk copyleft licenses only claude .
sonnet demonstrates good performance which contributes to its highest l icoscore.
among other models glm 9b chat stands out by not producing any strikingly similar cases and qwen2 7b instruct also demonstrates excellent compliance performance achieving a license accuracy of .
for its strikingly similar cases resulting in a l icoscore of .
.
furthermore codestral 22b v0.
generates the highest number of strikingly similar cases at but maintains a relatively good accuracy of .
in providing correct license information.
notably wizardcoder python 13b exhibits poor compliance performance with a l icoscore of .
and is almost incapable of providing any correct license information.
for strikingly similar code snippets under higher risk copyleft licenses we find that all llms perform poorly.
only claude .
sonnet provides some copyleft license information while other llms have an accuracy of zero.
we speculate that some closed source llms may have implemented post processing steps to avoid acknowledging outputs derivedfrom copyleft licensed code snippets.
it is worth noting that starcoder2 in constructing its training set the stack v2 employed file level license detection to exclude copyleftlicensed files.
this approach may be a significant factor in explaining why the number of strikingly similar cases for copyleft licenses is zero for this llm.
moreover we observe that among general llms open source llms demonstrate superior compliance performance compared to closed source llms.
this finding suggests a potential correlation between model transparency and license compliance capabilities.
vi.
d iscussion a. implications in this section we discuss the implications of our results for llm providers llm users open source communities and legal professionals llm providers our findings highlight the need for llm providers to enhance their llms license compliance capabilities.
this involves several key areas data cleaning and license detection our empirical study in section iii d1 reveals that despite the provider s efforts to exclude code files from copyleft licensed repositories during starcoderdata s construction copyleft licensed code still persists.
this is often due to discrepancies between filelevel and repository level license information .
the success of starcoder2 s file level fine grained license detection strategy which resulted in no strikingly similar cases under copyleft licenses in our evaluation underscores the importance of meticulous data cleaning processes.
enhancing license code association despite generating non negligible proportion of strikingly similar code demonstrating impressive memorization capabilities many llms fail to provide correct license information.
this suggests ineffective learning of code license associations during training.
llm providers should implement more sophisticated preprocessing techniques to strengthen these associations.
considering autoregressive nature of these llms which predict subsequent 10content based on preceding information one potential area for exploration could be the positioning of license information during training.
placing license information after the code snippets might potentially enhance llms ability to associate specific code with its corresponding license presenting a promising direction for future research.
addressing copyleft information suppression in section v b we find that most models tend to avoid providing information about copyleft code usage likely due to implemented output filters that suppress such information.
this approach is problematic as it provides users with incomplete or potentially misleading information exposing them to higher legal risks.
instead of suppressing copyleft related outputs providers should implement post processing steps that ensure proper attribution and accurate license information are included with generated code regardless of the license type.
llm users our evaluation reveals that many llms exhibit poor compliance capabilities.
users especially commercial entities must be aware of the potential legal risks associated with ai generated code.
before incorporating aiassisted development into their workflows users should carefully evaluate the compliance capabilities of llms.
if opting to use llms it is crucial to employ code review tools to verify license compliance of generated code particularly for code potentially derived from copyleft licensed sources.
open source communities the difficulties llms face in accurately providing license information underscore the risk of open source projects intellectual property being infringed upon in ai driven development.
while not obligated to facilitate ai training open source projects might consider adopting more explicit license declarations to protect their own intellectual property rights effectively.
this could include embedding license information in individual files and adopting more granular licensing practices.
furthermore as ai assisted coding becomes more prevalent open source communities may need to revise their practices and principles.
this could include developing guidelines for incorporating and attributing ai generated code in projects and establishing clear policies on how their own code should be used in ai training and generation processes.
legal professionals we find even top performing code generation llms produce a non negligible proportion .
to .
of strikingly similar cases.
their varying abilities to provide correct license information with gpt 4o achieving .
accuracy and deepseek coder v2 failing entirely highlight the complexity of license compliance in aigenerated code.
our study underscores the need for clearer legal frameworks addressing ai generated code and potential copyright infringements.
we demonstrate that it is feasible to characterize non independent creation in llm outputs using specific features opening new avenues for legal analysis.
these insights could serve as a reference for establishing more concrete legal standards in this emerging field aiding legal professionals in cases involving ai generated code.b.
threats to validity internal validity our striking similarity standard focuses on precision potentially overlooking cases where llms generate code derived from open source code but fall below our threshold e.g.
example in figure .
there is no precise legal standard to exclude independent creation and any automated approach would be inherently ambiguous and would not predict precise decisions in a legal context.
to mitigate this threat we opt for a minimum standard that emphasizes precision and interpretability identifying cases that are likely not independently created e.g.
example1 .
this minimum standard aligns well with the qualitative characteristics experts look for when the decide on striking similarities .
if any of those standards are not met as in example it would be unclear whether the two snippets would have been independently created.
we must acknowledge that our standard may perform poorly on recall which is hard to provide evidence in terms of recall due to such inherent ambiguity.
even with such a minimum standard we are still able to obtain concerning results from state of the art llms as shown in table iv.
furthermore l icoeval consisting of samples code snippets is a constructed benchmark that may not fully represent the vast diversity of real world code.
this limits our ability to determine if llms generate code infringing on open source software outside our benchmark.
despite this constraint the substantial findings within our focused dataset indicate that the identified compliance issues are likely prevalent in wider contexts as well.
external validity our study primarily focused on python code.
while results may vary for other programming languages with different licensing practices we believe that our evaluation framework and the methodology for constructing the benchmark are general and can be easily extended to other programming languages.
furthermore we only addressed function level code completion overlooking potentially more severe compliance issues at class or project levels.
however our approach provides a foundation for investigating these broader contexts.
vii.
c onclusion and data availability in this paper our main contribution is the development of licoeval the first benchmark for assessing llms license compliance capabilities.
to construct this benchmark we conduct an empirical study on striking similarity in llmgenerated code establishing a preliminary standard for this concept.
using l icoeval we perform a evaluation of llms revealing significant license compliance shortcomings.
although this study is only a preliminary attempt and much more work is needed beyond the basic requirements of copyright laws we believe our work could provide valuable insights for improving license compliance in ai assisted software development.
l icoeval can be accessible at .
acknowledgment this work is sponsored by the national natural science foundation of china .
11references y .
dong x. jiang h. liu z. jin b. gu m. yang and g. li generalization or memorization data contamination and trustworthy evaluation for large language models in findings of the association for computational linguistics acl .
association for computational linguistics aug. pp.
.
d. fried a. aghajanyan j. lin s. wang e. wallace f. shi r. zhong w. t. yih l. zettlemoyer and m. lewis incoder a generative model for code infilling and synthesis arxiv preprint arxiv .
.
r. li l. b. allal y .
zi n. muennighoff d. kocetkov c. mou m. marone c. akiki j. li j. chim et al.
starcoder may the source be with you!
arxiv preprint arxiv .
.
m. izadi r. gismondi and g. gousios codefill multi token code completion by jointly learning from structure and naming sequences inproceedings of the 44th international conference on software engineering pp.
.
m. jin s. shahriar m. tufano x. shi s. lu n. sundaresan and a. svyatkovskiy inferfix end to end program repair with llms in proceedings of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering pp.
.
c. s. xia y .
wei and l. zhang automated program repair in the era of large pre trained language models in ieee acm 45th international conference on software engineering icse .
ieee pp.
.
y .
wei c. s. xia and l. zhang copiloting the copilots fusing large language models with completion engines for automated program repair in proceedings of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering pp.
.
q. luo y .
ye s. liang z. zhang y .
qin y .
lu y .
wu x. cong y .
lin y .
zhang et al.
repoagent an llm powered open source framework for repository level code documentation generation arxiv preprint arxiv .
.
june survey reveals ai s impact on the developer experience.
.
available survey reveals ais impact on the developer experience h. pearce b. ahmad b. tan b. dolan gavitt and r. karri asleep at the keyboard?
assessing the security of github copilot s code contributions in ieee symposium on security and privacy sp .
ieee pp.
.
n. perry m. srivastava d. kumar and d. boneh do users write more insecure code with ai assistants?
in proceedings of the acm sigsac conference on computer and communications security pp.
.
z. yang z. zhao c. wang j. shi d. kim d. han and d. lo unveiling memorization in code models in ieee acm 46th international conference on software engineering icse pp.
.
a. al kaswan m. izadi and a. v .
deursen traces of memorisation in large language models for code in ieee acm 46th international conference on software engineering icse pp.
.
a. al kaswan and m. izadi the ab use of open source code to train large language models in ieee acm 2nd international workshop on natural language based software engineering nlbse .
ieee pp.
.
m. z. choksi and d. goedicke whose text is it anyway?
exploring bigcode intellectual property and ethics arxiv preprint arxiv .
.
p. henderson x. li d. jurafsky t. hashimoto m. a. lemley and p. liang foundation models and fair use arxiv preprint arxiv .
.
z. yu y .
wu n. zhang c. wang y .
v orobeychik and c. xiao codeipprompt intellectual property infringement assessment of code language models in international conference on machine learning icml pp.
.
a. karamolegkou j. li l. zhou and a. s gaard copyright violations and large language models arxiv preprint arxiv .
.
w. xu h. he k. gao and m. zhou understanding and remediating open source license incompatibilities in the pypi ecosystem in 38th ieee acm international conference on automated software engineering ase pp.
.
d. m. german and a. e. hassan license integration patterns addressing license mismatches in component based development in ieee 31st international conference on software engineering .
ieee pp.
.
t. wolter a. barcomb d. riehle and n. harutyunyan open source license inconsistencies on github acm transactions on software engineering and methodology vol.
no.
pp.
.
k. huang y .
xia b. chen z. zhou j. guo and x. peng detecting and fixing violations of modification terms in open source licenses during forking arxiv preprint arxiv .
.
march the apache .
license.
.
available https s. karpinski.
july i don t want to say anything but that s not the right license mr copilot.
.
available https twitter.com stefankarpinski status t. davis.
october copilot with public code blocked emits large chunks of my copyrighted code with no attribution no lgpl license.
.
available c. green.
june explored github copilot a paid service to see if it encodes code from repositories with restrictive licenses.
.
available status eevee.
june i m unclear on how it s not a form of laundering open source code into commercial works.
.
available github.
august github copilot.
.
available https github.com features copilot november github copilot litigation.
.
available m. chen j. tworek h. jun q. yuan h. p. d. o. pinto j. kaplan h. edwards y .
burda n. joseph g. brockman et al.
evaluating large language models trained on code arxiv preprint arxiv .
.
x. du m. liu k. wang h. wang j. liu y .
chen j. feng c. sha x. peng and y .
lou classeval a manually crafted benchmark for evaluating llms on class level code generation arxiv preprint arxiv .
.
h. yu b. shen d. ran j. zhang q. zhang y .
ma g. liang y .
li q. wang and t. xie codereval a benchmark of pragmatic code generation with generative pre trained models in proceedings of the 46th ieee acm international conference on software engineering pp.
.
z. yang j. shi j. he and d. lo natural attack for pre trained models of code in proceedings of the 44th international conference on software engineering pp.
.
m. v .
pour z. li l. ma and h. hemmati a search based testing framework for deep neural networks of source code embedding in 14th ieee conference on software testing verification and validation icst .
ieee pp.
.
h. aghakhani w. dai a. manoel x. fernandes a. kharkar c. kruegel g. vigna d. evans b. zorn and r. sim trojanpuzzle covertly poisoning code suggestion models arxiv preprint arxiv .
.
l. niu s. mirza z. maradni and c. p opper codexleaks privacy leaks from code generation language models in github copilot in 32nd usenix security symposium usenix security pp.
.
u. s. c. for the ninth circuit.
december copying access and substantial similarity.
.
available gov jury instructions node s. scheffler e. tromer and m. varia formalizing human ingenuity a quantitative framework for copyright law s substantial similarity in proceedings of the symposium on computer science and law pp.
.
u. c. of appeals for the second circuit.
february arnstein v. porter f.2d 2d cir.
.
.
available https law.justia.com cases federal appellate courts f2 j. r. autry toward a definition of striking similarity in infringement actions for copyrighted musical works j. intell.
prop.
l. vol.
p. .
s. r. higgins proving copyright infringement will striking similarity make your case suffolk j. trial app.
advoc.
vol.
p. .
h. j. meeker open source for business a practical guide to open source software licensing third edition .
united states independently published .
l. rosen open source licensing software freedom and intellectual property law .
j. wu l. bao x. yang x. xia and x. hu a large scale empirical study of open source license usage practices and challenges in ieee acm 21th international conference on mining software repositories msr .
j. reddy.
augst the consequences of violating open source licenses.
.
available consequences violating open source licenses w. xu x. wu r. he and m. zhou licenserec knowledge based open source license recommendation for oss projects in ieee acm 45th international conference on software engineering companion proceedings icse companion .
ieee pp.
.
c. zhang d. ippolito k. lee m. jagielski f. tram er and n. carlini counterfactual memorization in neural language models arxiv preprint arxiv .
.
n. carlini d. ippolito m. jagielski k. lee f. tramer and c. zhang quantifying memorization across neural language models arxiv preprint arxiv .
.
k. lee d. ippolito a. nystrom c. zhang d. eck c. callison burch and n. carlini deduplicating training data makes language models better in proceedings of the 60th annual meeting of the association for computational linguistics volume long papers s. muresan p. nakov and a. villavicencio eds.
dublin ireland association for computational linguistics may pp.
.
.
available m. zakeri nasrabadi s. parsa m. ramezani c. roy and m. ekhtiarzadeh a systematic literature review on source code similarity measurement and clone detection techniques applications and challenges journal of systems and software p. .
j. austin a. odena m. nye m. bosma h. michalewski d. dohan e. jiang c. cai m. terry q. le et al.
program synthesis with large language models arxiv preprint arxiv .
.
z. yang z. sun t. z. yue p. devanbu and d. lo robustness security privacy explainability efficiency and usability of large language models for code arxiv preprint arxiv .
.
r. aleithan explainable just in time bug prediction are we there yet?
in ieee acm 43rd international conference on software engineering companion proceedings icse companion .
ieee pp.
.
r. luukkonen j. burdge e. zosa a. talman v .
komulainen v .
hatanp a a p. sarlin and s. pyysalo poro 34b and the blessing of multilinguality arxiv preprint arxiv .
.
a. lozhkov r. li l. b. allal f. cassano j. lamy poirier n. tazi a. tang d. pykhtar j. liu y .
wei et al.
starcoder and the stack v2 the next generation arxiv preprint arxiv .
.
z. luo c. xu p. zhao q. sun x. geng w. hu c. tao j. ma q. lin and d. jiang wizardcoder empowering code large language models with evol instruct arxiv preprint arxiv .
.
codeparrot.
january codeparrot.
.
available https huggingface.co codeparrot codeparrot bigcode.
january the stack.
.
available https huggingface.co datasets bigcode the stack .
january the stack v2.
.
available https huggingface.co datasets bigcode the stack v2 march codeparrot dataset cleaned.
.
available s. chaudhary code alpaca an instruction following llama model for code generation .
bigcode.
march starcoderdata.
.
available https huggingface.co datasets bigcode starcoderdata april gnu general public license version .
.
available k. srinath python the fastest growing programming language international research journal of engineering and technology vol.
no.
pp.
.
a. z. broder identifying and filtering near duplicate documents in annual symposium on combinatorial pattern matching .
springer pp.
.
t. j. mccabe a complexity measure ieee transactions on software engineering no.
pp.
.
p. e. mcknight and j. najab mann whitney u test the corsini encyclopedia of psychology pp.
.
n. cliff dominance statistics ordinal analyses to answer ordinal questions.
psychological bulletin vol.
no.
p. .
k. gao r. he b. xie and m. zhou characterizing deep learning package supply chains in pypi domains clusters and disengagement acm transactions on software engineering and methodology vol.
no.
pp.
.
w. xiao h. he w. xu x. tan j. dong and m. zhou recommending good first issues in github oss projects in proceedings of the 44th international conference on software engineering pp.
.
w. xu and x. zhang multi granularity code smell detection using deep learning method based on abstract syntax tree.
in seke pp.
.
u. c. office.
march what does copyright protect?
.
available k. papineni s. roukos t. ward and w. j. zhu bleu a method for automatic evaluation of machine translation in proceedings of the 40th annual meeting of the association for computational linguistics pp.
.
v .
i. levenshtein et al.
binary codes capable of correcting deletions insertions and reversals in soviet physics doklady vol.
no.
.
soviet union pp.
.
m. allamanis h. peng and c. sutton a convolutional attention network for extreme summarization of source code in international conference on machine learning .
pmlr pp.
.
y .
ma c. bogart s. amreen r. zaretzki and a. mockus world of code an infrastructure for mining the universe of open source vcs data in2019 ieee acm 16th international conference on mining software repositories msr .
ieee pp.
.
s. chacon and b. straub git git objects en v2 git internals git objects .
k. gao w. xu w. yang and m. zhou pyradar towards automatically retrieving and validating source code repository information for pypi packages proceedings of the acm on software engineering vol.
no.
fse pp.
.
k. gao z. wang a. mockus and m. zhou on the variability of software engineering needs for deep learning stages trends and application types ieee transactions on software engineering vol.
no.
pp.
.
swsc swsc overview bitbucket overview src master .
march sample size calculator.
.
available https j. achiam s. adler s. agarwal l. ahmad i. akkaya f. l. aleman d. almeida j. altenschmidt s. altman s. anadkat et al.
gpt technical report arxiv preprint arxiv .
.
m. reid n. savinov d. teplyashin d. lepikhin t. lillicrap j. b. alayrac r. soricut a. lazaridou o. firat j. schrittwieser et al.
gemini .
unlocking multimodal understanding across millions of tokens of context arxiv preprint arxiv .
.
anthropic.
june claude .
sonnet.
.
available q. team.
june hello qwen2.
.
available https qwenlm.github.io blog qwen2 g. team a. zeng b. xu b. wang c. zhang d. yin d. rojas g. feng h. zhao h. lai et al.
chatglm a family of large language models from glm 130b to glm all tools arxiv e prints pp.
arxiv .
ai meta llama model card .
.
available card.md q. zhu d. guo z. shao d. yang p. wang r. xu y .
wu y .
li h. gao s. ma et al.
deepseek coder v2 breaking the barrier of closed source models in code intelligence arxiv preprint arxiv .
.
q. team.
april code with codeqwen1.
.
.
available m. a. team.
may codestral hello world!
.
available c. team codegemma open code models based on gemma arxiv preprint arxiv .
.
augest licoeval.
.
available osslab pku licoeval