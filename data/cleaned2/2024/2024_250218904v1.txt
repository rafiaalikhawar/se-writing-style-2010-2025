an empirical study on commit message generation using llms via in context learning yifan wu yunpeng wang ying li wei tao siyu yu haowen yang wei jiang jianguo li peking university beijing china ant group hangzhou china fudan university shanghai china the chinese university of hong kong shenzhen cuhk shenzhen china abstract commit messages concisely describe code changes in natural language and are important for software maintenance.
several approaches have been proposed to automatically generate commit messages but they still suffer from critical limitations such as time consuming training and poor generalization ability.
to tackle these limitations we propose to borrow the weapon of large language models llms and in context learning icl .
our intuition is based on the fact that the training corpora of llms contain extensive code changes and their pairwise commit messages which makes llms capture the knowledge about commits while icl can exploit the knowledge hidden in the llms and enable them to perform downstream tasks without model tuning.
however it remains unclear how well llms perform on commit message generation via icl.
in this paper we conduct an empirical study to investigate the capability of llms to generate commit messages via icl.
specifically we first explore the impact of different settings on the performance of icl based commit message generation.
we then compare icl based commit message generation with state ofthe art approaches on a popular multilingual dataset and a new dataset we created to mitigate potential data leakage.
the results show that icl based commit message generation significantly outperforms state of the art approaches on subjective evaluation and achieves better generalization ability.
we further analyze the root causes for llm s underperformance and propose several implications which shed light on future research directions for using llms to generate commit messages.
index terms commit message generation large language model in context learning i. i ntroduction when submitting a code change to a version control system developers can write a brief descriptive comment in the format of natural language called a commit message.
high quality commit messages can greatly facilitate software maintenance by providing a human readable summary of the changes and the rationale behind them obviating the need for a detailed examination of the complex code and potentially simplifying code review and related tasks .
conversely poor commit messages can negatively affect software defect proneness .
however the inherent complexity of commits renders the task of manually summarizing them into concise messages difficult and prone to errors .
according to tian this work was done when yifan wu was an intern at ant group.
ying li is the corresponding author.et al.
of the commit messages from five opensource projects have quality issues.
furthermore within the context of today s accelerated software development pace manually writing high quality commit messages becomes a time intensive and arduous task .
research indicates that a significant amount of commit messages from open source projects lack important information or are even empty .
consequently many approaches have been proposed to generate high quality commit messages automatically over the years.
early studies extract information from code changes and generate commit messages with predefined rules or templates which may not encompass all scenarios or deduce the intent behind code changes.
later some studies adopt information retrieval ir techniques to reuse commit messages of similar code changes.
they can take advantage of similar examples but the reused commit messages may not correctly describe the content or intent of the current code change.
recently a large number of learningbased approaches have been proposed .
they trained on a large scale commit corpus to translate code changes into commit messages and have been demonstrated to outperform rule based approaches and ir based approaches.
although learning based approaches have achieved comparative success they still have some critical limitations.
first these approaches require either training models from scratch or tuning a pre trained model e.g.
codet5 with labeled data which could be impractical due to the scarcity of computing resources and labeled data.
second the performance of these approaches largely depends on the quality of training data while the most widely used training data i.e.
commits from open source projects is typically of poor quality .
third these approaches suffer from significant performance degradation when applied to new projects due to poor generalization ability.
research indicates that the performance of learning based approaches can decrease by .
to .
in such a scenario .
to address the above limitations we propose to borrow the weapon of large language models llms and in context learning icl .
llms are pre trained on extensive unlabeled corpora via self supervised learning thereby acquiring substantial knowledge.
considering the inclusion of abundant real world commits within the corpora llms present aarxiv .18904v1 feb 2025promising avenue for commit message generation.
in addition icl i.e.
providing a prompt with demonstrations to llms has been shown to effectively harness the knowledge inherent in llms and enable them to perform downstream tasks without model tuning .
some recent works have investigated llm based commit message generation.
however it remains unclear how well llms perform on commit message generation via icl.
more research is needed to determine its ability in this important area.
therefore in this paper we conduct an empirical study on commit message generation using llms via icl.
specifically considering the sensitivity of llms to different settings we first investigate the impact of different prompts and demonstrations on the performance of icl based commit message generation.
based on the optimal setting we compare various llms with state of the art baselines on a popular benchmark .
to mitigate the potential data leakage we create a new dataset by collecting commits from repositories not included in the benchmark named mcmd nl and recent commits from the same repositories included in the benchmark named mcmd nt respectively.
we evaluate llms and baselines on the above datasets using both objective metrics and subjective metrics.
based on the results we perform an in depth analysis of the root causes of llm s underperforming cases.
our study highlights the capability of llms to generate commit messages through icl and identifies several directions for future research.
the key findings are as follows prompt settings have a greater impact on icl based commit message generation in zero shot learning than in few shot learning suggesting that demonstrations can mitigate the llm s sensitivity to prompt variations.
a moderate number of demonstrations can enhance the performance of icl based commit message generation but an excessive number can reduce performance.
retrieval based demonstration selection can statistically significantly enhance the performance of icl based commit message generation while the order of demonstrations has minimal impact on performance.
gpt .
turbo and deepseek v2 chat are the best performing llms for the commit message generation task statistically outperforming other llms on all metrics.
the best performing llms statistically significantly outperform the best performing baseline on mcmd nt indicating superior generalization.
moreover the best performing llms have comparable performance on mcmd nl to the best performing baseline which is finetuned on the training set.
in the subjective evaluation the best performing llms statistically significantly outperform the best performing baseline.
additionally llm based evaluation has much higher correlations with human judgment than objective metrics suggesting it is more reliable to evaluate the quality of commit messages.
llms still struggle in some cases mainly due to a lack of contextual knowledge adverse demonstrations and model fallacy.
providing highquality demonstrations and more advanced llms can resolve most underperforming cases.
in summary this paper makes the following contributions we conduct an empirical study to explore the perforfig.
.
an example of a commit with a code diffand its reference message.
mance of icl based commit message generation.
we analyze the root causes of underperforming cases and propose important directions for future research.
the code and dataset in this study are publicly available at to benefit both practitioners and researchers in the field of commit message generation.
ii.
b ackground a. commit diff and commit messages git is one of the most popular version control systems.
whenever developers submit a code change git will create a commit to record this change and require developers to enter a commit message.
the commit message is written by developers in a textual format to facilitate the understanding of the change while the change here is represented by diff which characterizes the difference between two versions.
usually a diff contains one or multiple chunks with file paths.
the modified codes are wrapped by in a chunk with the negative sign or positive sign with a line number to denote the deleted or added lines of code.
as shown in fig.
a commit mentioned in this paper refers to the pair of a code diffand its corresponding commit message.
given a commit we refer to its original commit message as reference message .
b. large language models large language models llms are large sized pre trained language models with tens or hundreds of billions of parameters trained on extensive unlabeled corpora via self supervised learning which exhibit strong capacities to understand natural language and solve various tasks .
in addition to natural language llms can also deal with code which arouses growing interest in applying llms to the software engineering domain .
how to effectively apply llms to downstream tasks has become an important research topic.
a prevalent paradigm is to fine tune the model by learning the desired output from the given input of a downstream dataset to update model parameters .
for example to enhance the performance and adaptability of llms in specific tasks within the software engineering domain there are a considerable amount of fine tuned code llms e.g.
codex and code llama and have made tremendous achievements in various tasks such as commit message generation code review and just in time comment update .fig.
.
overview of our study.
c. in context learning tuning a pre trained model on downstream datasets can be extremely time consuming and resource intensive especially for llms that usually contain billions of parameters.
in addition the performance of tuned models largely depends on the scale and quality of labeled data making it less feasible in specific scenarios with limited labeled data.
recently incontext learning icl offers a new paradigm that allows llms to perform downstream tasks without model tuning where a formatted natural language prompt is used as the input for llms to generate the output.
specifically the prompt typically includes three parts instruction the description of the specific task demonstrations a few examples i.e.
query answer pairs selected from the task datasets query the test query that llms need to answer.
such a prompt can let llms gain task specific knowledge by learning the pattern hidden in the demonstrations of the task.
many studies have demonstrated that llms achieved remarkable performance in various tasks via icl such as log statement generation and comment generation .
iii.
s tudy design a. overview and research questions fig.
shows the overview of our study.
we evaluate popular llms and state of the art commit message generation approaches on a popular multilingual benchmark dataset mcmd .
however given the potential risk that the dataset could be used to train llms and baselines we create a new dataset mcmd new consisting of two parts new commits from the same repositories as mcmd but collected more recently and commits from repositories using different languages that are not included mcmd.
we next introduce the research questions we aim to investigate and their relationships.
rq1 impact of llm settings how do different prompt and demonstration settings affect the performance of icl based commit message generation?
recent studies have shown that the performance of icl is highly affected by prompt and demonstration settings .
inspired by these studies we propose to investigate the impact of llm settings.
specifically we design four prompts based on whether a role description is provided and whether constraint information is given and fig.
.
overview of icl based commit message generation.
investigate demonstration settings from three dimensions i.e.
number selection and order .
rq2 effectiveness of llm how does the performance of icl based commit message generation compare with state of the art approaches?
based on the optimal setting obtained from rq1 in this rq we evaluate the performance of icl based commit message generation compared with state of the art approaches.
to answer this question we compare various llms with state of the art approaches using both objective metrics and subjective metrics on the existing mcmd dataset and the new dataset mcmd new we created.
rq3 root cause analysis what are the underlying causes for llm s underperformance?
in this rq we aim to further investigate the root causes of llm s underperforming cases.
specifically we sample a diverse set of underperforming cases where llm failed to make accurate predictions in rq2 and summarize the categories of root causes.
b. prompt templates for commit message generation formally a prompt is defined as p nl cd xq where nl is a natural language instruction cd xi yi n i 1is a set of code change demonstrations composed by input code change sequence xiand desired output sequence yi and xqis a query to be answered by llms.
specifically if n which means there is no demonstration the setting is known as zero shot learning ifn which means there is only one demonstration the setting is known as oneshot learning and few shot learning means there are several demonstrations.
also there is a constraint that the prompt should fit within the context window limit of llms.fig.
illustrates a structured prompt template for commit message generation.
the template begins with a natural language instruction nl as highlighted in the yellow text.
the instruction is followed by a set of code change demonstrations cd highlighted in the blue text.
the demonstrations consist of one code change and one associated commit message.
by analogizing the demonstrations llm can learn the expected behavior and directly generate a commit message for the given query.
finally the template concludes with a code change query xqfor llm to generate the desired commit message illustrated at the bottom of the template.
c. demonstration selection given a code change query xq we select a set of demonstrations cd assemble them into a prompt p and input it into llm for inference.
the whole process is shown in fig.
.
we next introduce the demonstration selection methods we used.
random based selection this method randomly selects a set of code change demonstrations from the training set which is the most straightforward and efficient method.
retrieval based selection many studies have shown that demonstrations that are similar to the query may help llms better understand the desired behavior .
to this end we use a knn based demonstration selection algorithm that does not involve much computational overhead in practice which is shown in algo.
.
for a code change query xq we calculate its similarity sim xq xi with each code change candidate xifrom the training set.
then we select top kmost similar code changes as demonstrations.
we utilize three methods to calculate similarity as follows token based the most widely used method to retrieve similar code is focusing on the overlap of tokens .
inspired by these studies we utilize the jaccard coefficient to calculate the similarity at token level as follows sim xq xi f xq f xi f xq f xi where f calculates the number of tokens in a code change.
frequency based bm which is an extension of tf idf is a classic sparse retrieval method in the information retrieval field and also used in code intelligence tasks .
therefore our second method utilizes bm to calculate the similarity.
we implement bm with the gensim package .
semantic based the above two methods can only employ the lexical similarity recent studies have revealed that the code semantic is also important to find similar code .
hence we use openai embedding model to embed the query xqand candidate xiinto vector representations vqandvi.
the similarity is then quantified as the cosine similarity i.e.
sim xq xi vq vi vq vi .
d. datasets mcmd we first select the mcmd dataset which is widely used in the commit message generation task.
this dataset contains five programming languages c c java python and javascript.
for each language it collects commits from the top most starred repositories on github.
shi etalgorithm knn based demonstration selection input query xq training set d xi yi n i similarity function sim demonstration number k output demonstrations cd c cd forxi d do si sim xq xi c c i si end for extract keys with top klargest values from ctoi fori i do cd cd xi yi end for return cd table i the statistics of mcmd n ew dataset .
dataset language repo.
commit mcmd ntc c java python javascript mcmd nlphp r typescript swift objective c al.
further filter out commits with files that cannot be parsed such as .jar .ddl .mp3 and .apk to reduce noise data and build a higher quality dataset containing repositories and commits.
we use the dataset in our experiment.
mcmd new due to the potential overlap with the training data of llms resulting in data leakage we created a new dataset mcmd new.
specifically we collected data after january as the training data of llms e.g.
chatgpt is up until september .
furthermore mcmd does not contain data after january .
in addition to the repositories included in mcmd we crawled commits from additional repositories top most starred repositories for each language using five programming languages php r typescript swift and objective c which are not included in mcmd and enjoy widespread popularity according to the pypl popularity index .
in total we selected repositories with repositories from mcmd and new repositories with different programming languages.
to mitigate the presence of low quality commits which could affect the comparisons between llms and baselines we employed regular regressions to filter low quality commit messages including redundant messages e.g.
rollback commits and noisy messages e.g.
update files which is consistent with that used in previous studies .
after applying the filtering rules and selecting commits based on time we get repositories out of the initial repositories.
as shown in table i mcmd new comprises two parts mcmd newtime mcmd nt which includes commits from repositories also present in mcmd and mcmdnewlanguage mcmd nl which includes commits from new repositories that have different programming languages from the repositories in mcmd.
finally following the data splitting of prior studies we randomly divide mcmd new into training set validation set and test set with proportions of and respectively.
e. evaluation models llms we select six llm families with widespread popularity and exceptional performance in code generation .
the summary of studied llms is shown in table ii.
chatgpt .
chatgpt is the most representative llm released by openai and has garnered significant attention in the field of software engineering .
we employ gpt3.
turbo in our experiments.
claude .
claude is a family of large multimodal models developed by anthropic.
we utilize claude haiku the fastest and most affordable model in our experiments.
qwen1.
.
qwen1.
released by alibaba cloud is a family of language models with different model sizes.
we utilize codeqwen1.
7b chat in our experiments.
deepseek v2 .
deepseek v2 is a family of mixtureof experts moe language models which is pre trained on a corpus comprising .
trillion tokens.
we utilize deepseekv2 chat in our experiments.
codeqwen1.
.
codeqwen1.
is a family of code llms which is built upon qwen1.
and further pre trained with around trillion tokens of code related data.
we utilize codeqwen1.
7b chat in our experiments.
deepseek coder v2 .
deepseek coder v2 is a family of moe code llms which is further pre trained based on deepseek v2 with an additional trillion tokens.
we utilize deepseek coder v2 instruct in our experiments.
baselines we select the following state of the art commit message generation approaches as baselines to compare with llms.
nngen .
nngen is a state of the art retrieval based commit message generation approach .
it represents code change as bag of words vectors .
the vector of the test code change is then compared to those in the training set using cosine similarity.
the commit message of the most similar code change in the training set is reused as the result.
cct5 .
cct5 is the state of the art code changeoriented pre trained model which is built on top of t5 model .
it is pre trained with .6gb of code change data collected from 35k github repositories.
five code changeoriented tasks are used for pre training.
we fine tune and evaluate the pre trained model on our training and test sets.
come .
come is another state of the art commit message generation approach.
it uses modification embedding to represent code changes and fine tuned codet5 to generate a translation result and retrieve a result simultaneously.
the final commit message is selected from the above two results with an svm based decision algorithm.table ii the summary of llm s used in our study .
category family model open source release date general llmschatgpt gpt .
turbo claude claude haiku qwen1.
qwen1.
7b chat deepseek v2 deepseek v2 chat code llmscodeqwen1.
codeqwen1.
7b chat deepseek coder v2 deepseek coder v2 instruct f .
evaluation metrics objective evaluation metrics objective evaluation metrics assess the quality of the generated message by calculating the text similarity between the generated message and the reference message.
we use four widely used metrics in previous literature including bleu meteor rouge l and cider .
bleu measures the precision of n grams between the generated text and the reference texts.
rouge l is a recall oriented metric that measures the longest common subsequence between the generated text and the reference texts.
meteor calculates the harmonic mean of gram precision and recall of the generated text against the reference texts.
cider takes each sentence as a document and calculates the cosine similarity of its tf idf vector at the n gram level to obtain the similarity between the generated text and the reference texts.
subjective evaluation metrics objective evaluation metrics fail to capture the semantics of the generated messages and have been shown to have a relatively low correlation with human judgments .
furthermore these metrics treat reference messages as the gold standard which can be of poor quality.
to address these issues we turn to referencefree evaluation methods.
human evaluation.
following the previous work we invite three volunteers including postgraduates majoring in computer science and industry professionals with relevant experience.
for each sample we provide volunteers with the code change the reference message and messages generated by different models.
to ensure unbiased evaluation we shuffled the commit messages so that the volunteers did not know where each message came from.
each participant is asked to assign scores from to the higher the better to the commit messages from three aspects informativeness the extent of important information about the code change reflected in the commit message conciseness the extent of extraneous information included in the commit message and expressiveness readability and fluency of the commit message .
the final score for one commit message is the average of the three volunteers.
llm based evaluation.
in addition to human evaluation we utilize llm based evaluation to evaluate the quality of generated commit messages.
recent studies indicate that llm as an evaluator achieves high alignment with human .
to this end we use gpt as the evaluator and design an evaluation prompt as the input of gpt to score the generated commit message.
as shown in fig.
the promptfig.
.
the prompt for llm based evaluation.
comprises three parts a task instruction that contains the definition of the evaluation task evaluation criteria that describe the detailed standards which is the same as human an evaluation input that needs to be scored.
g. experiment settings for open source models such as codeqwen1.
7b chat we use official releases available on huggingface .
for closed source models e.g.
gpt .
trubo we access them via official api.
our experiments are conducted on a linux server equipped with nvidia v100 gpus.
for open source models we deploy a local api server using vllm which is a unified library for llm serving and inference.
all models are used in their original precisions without quantization.
temperature controls the randomness in the generated results of models .
following previous work we set the temperature to to ensure deterministic output.
the context length of models limits the input size.
hence we truncate each demonstration tocontext length n 1tokens where nrepresents the number of demonstrations.
to obtain reliable results experiments for all open source models are repeated five times and we report the average value as the final result.
due to budget constraints we do not repeat experiments for closed source models in rq2.
iv.
s tudy results a. rq1 impact of prompts and demonstrations setup to select the prompts we followed the best practices which suggests that prompts consist of four elements instruction context input data and output indicator.
we have tried prompts with various combinations of these four elements.
during our preliminary evaluation we experimented with a total of prompts.
due to budget constraints we selected the best performing and representative prompts prompt p1 the simplest prompt.
we only provided the basic requirement of generating a commit message based on the code change without additional description.
prompt p2 p1 role description.
p2 was designed based on p1 but included a role description that askedllm to act as a commit message generator and generate a commit message for a code change.
prompt p3 p1 constraint information.
p3 included constraint information such as not writing explanations and just replying with the commit message.
prompt p4 p3 role description.
p4 was a combination of p2 and p3 containing both role description and constraint information.
to explore the impact of demonstrations we first vary the demonstration number from to using random based selection.
we evaluated the impact of various combinations of these prompts and kinds of demonstration numbers.
with the optimal prompt and demonstration number we further investigated the impact of demonstration selection using the selection methods described in sec.
iii c2.
furthermore some studies suggest that the demonstration order can affect the performance of icl.
therefore we arrange demonstrations in both ascending and descending order based on similarity to explore the impact of demonstration order.
we use chatgpt and deepseek v2 as the representative llms in this rq.
due to the prohibitive cost of api access we randomly selected samples from the valid set of the mcmd dataset to reduce the number of api calls.
to mitigate the randomness of llm inference we repeated each setting five times and reported their mean as the final result.
results fig.
illustrates the performance of chatgpt under different prompts and demonstration numbers.
table iii displays the impact of demonstration selection and order on the performance of chatgpt.
impact of prompt.
we observe that p3 marginally outperforms the other prompts.
by default we adopted p3 in subsequent experiments.
moreover different prompts have a greater impact in zero shot learning compared to few shot learning and the impact gradually decreases as the number of demonstrations increases.
specifically the difference between the best and worst prompt without any demonstrations on rouge l is .
while that with demonstrations is .
.
this suggests that incorporating demonstrations can help mitigate llm s sensitivity to prompts.
one possible reason is that demonstrations in few shot learning provide additional context and examples which help guide the llm s responses and reduce its reliance on the specific wording or structure of the prompt.
in contrast in zero shot learning the llm has to understand the task solely based on the prompt making it more susceptible to prompt variations.
finding prompt settings have a greater impact on iclbased commit message generation in zero shot learning than in few shot learning suggesting that demonstrations can mitigate the llm s sensitivity to prompt variations.
impact of demonstration number.
we observe that the performance of chatgpt on all metrics increases with the number of demonstrations and achieves a peak at .
for example the average improvements of demonstrations over no demonstrations are .
.
.
and .
fig.
.
impact of different prompts and demonstration number on the performance of chatgpt.
table iii impact of demonstration selection and order on the performance of chatgpt.
demonstration java c c python javascript average selection order bleu met.
rou.
cid.
bleu met.
rou.
cid.
bleu met.
rou.
cid.
bleu met.
rou.
cid.
bleu met.
rou.
cid.
bleu met.
rou.
cid.
random random .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
tokendescend .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ascend .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
frequencydescend .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ascend .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
semanticdescend .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ascend .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
on bleu meteor rouge l and cider respectively.
however the performance suffers from a significant drop when further increasing the number to .
one possible reason is that too many demonstrations can introduce noise and redundancy as not all demonstrations are equally relevant or useful.
this can confuse the llm and negatively impact its performance.
by default we adopted demonstrations.
finding a moderate number of demonstrations enhances icl based commit message generation performance but an excessive number can reduce performance.
impact of demonstration selection and order.
we observe that retrieval based selection outperforms randombased selection by an average of .
.
the wilcoxon signed rank test shows that the improvement achieved by retrieval based selection is statistically significant at the confidence level of .
specifically bleu improves by .
meteor improves by .
rouge l improves by .
and cider improves by .
on average.
among the retrieval based selection the token based method performs slightly better than others but the difference is not statistically significant.
given the computational efficiency we adopted the token based method as the default demonstration selection in subsequent experiments.
additionally the performance difference between sorting demonstrations in ascending order and descending order is not statistically significant.
by default we sorted demonstrations in ascending order of similarity.
finding retrieval based demonstration selection can statistically significantly improve the performance of icl based commit message generation while the order of demonstrations has minimal impact on performance.
to explore whether the findings of llm settings also applyto other llms we conducted an additional experiment using deepseek v2.
employing the same experimental setup we replicated the experiments and found that the results closely aligned with our findings in this rq.
detailed results are available in our artifacts due to space limitations.
b. rq2 effectiveness of llm setup based on the optimal settings from rq1 i.e.
prompt demonstrations token based selection and sorting in ascending order we evaluated llms and baselines on the test set of mcmd and mcmd new using objective evaluation metrics.
we reused the code and the hyperparameter values released by the authors in the github repositories for all baselines.
specifically for mcmd and mcmd nt we reused the released model checkpoints of come and cct5 which were trained on mcmd.
for mcmd nl which consists of five new languages we trained new models for come and cct5 and evaluated them on the test set of mcmd nl.
for subjective evaluation we sampled commits from each test set of mcmd and mcmd new commits per language totaling commits.
we evaluated the reference messages and the messages generated by the best performing llms and the best performing baseline for these sampled commits.
to enhance the diversity of samples we divided each test set into clusters and randomly selected one commit from each cluster.
we used the openai embedding model for vectorization and k means for clustering.
in total we sampled code diff commit message pairs to score.
objective evaluation results table iv table v and table vi show the objective evaluation results on mcmd mcmd nt and mcmd nl respectively.
comparison among llms.
we find a large performance variation among different llms.
specifically gpt .
turbo and deepseek v2 chat exhibit comparable performance statistically outperforming other llms across all metrics andtable iv objective evaluation results on mcmd.
t he deeper color means better performance .
modeljava c c python javascript average bleu met.
rou.
cid.
bleu met.
rou.
cid.
bleu met.
rou.
cid.
bleu met.
rou.
cid.
bleu met.
rou.
cid.
bleu met.
rou.
cid.
nngen .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
cct5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
come .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
gpt .
turbo .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
claude haiku .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
qwen1.
7b chat .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
deepseek v2 chat .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
codeqwen1.
7b chat .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
deepseek coder v2 instruct .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table v objective evaluation results on mcmd nt.
t he deeper color means better performance .
modeljava c c python javascript average bleu met.
rou.
cid.
bleu met.
rou.
cid.
bleu met.
rou.
cid.
bleu met.
rou.
cid.
bleu met.
rou.
cid.
bleu met.
rou.
cid.
nngen .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
cct5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
come .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
gpt .
turbo .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
claude haiku .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
qwen1.
7b chat .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
deepseek v2 chat .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
codeqwen1.
7b chat .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
deepseek coder v2 instruct .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table vi objective evaluation results on mcmd nl.
t he deeper color means better performance .
modelphp r typescript swift objective c average bleu met.
rou.
cid.
bleu met.
rou.
cid.
bleu met.
rou.
cid.
bleu met.
rou.
cid.
bleu met.
rou.
cid.
bleu met.
rou.
cid.
nngen .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
cct5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
come .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
gpt .
turbo .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
claude haiku .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
qwen1.
7b chat .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
deepseek v2 chat .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
codeqwen1.
7b chat .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
deepseek coder v2 instruct .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
datasets.
given the privacy and security risks with api access to closed source models the open source deepseek v2 chat model is a good choice for generating commit messages.
interestingly although additional pre training on general llms can enhance their performance on code generation it does not yield statistically significant improvements in commit message generation and may even degrade performance.
for example deepseek v2 chat outperforms its code specific counterpart by .
.
.
and .
in mcmd nl in terms of bleu meteor rouge l and cider respectively.
this may be because code llms are primarily trained on code rather than commits making it difficult for them to capture the subtle differences between two code snippets which is essential to generate effective commit messages.
thus integrating commit data into llm training is vital to improve their performance on commit message generation.
finding gpt .
turbo and deepseek v2 chat are the best performing llms for the commit message generation task.
moreover additional code pre training on general llms does not yield better performance.
comparison between llms and baselines.
we observe that the best performing baseline i.e.
come surpasses the best performing llms on the mcmd dataset potentially due to the low quality reference messages in mcmd whichwill be further discussed in sec.
iv b3.
however on the new mcmd nt dataset we observe that the best performing llms outperform come on all metrics.
the wilcoxon signed rank test shows that the performance difference is statistically significant at the confidence level of .
for example gpt .
turbo outperforms come by .
.
.
and .
on average in terms of bleu meteor rouge l and cider respectively.
this indicates that come has poor generalization ability when faced with new data that may exhibit a different distribution from its training set.
regarding the mcmd nl dataset we observe that the bestperforming llms achieve comparable performance to come.
it is worth noting that come was fine tuned on mcmd nl which is time consuming and resource intensive.
in contrast llms utilize only a few demonstrations to acquire taskspecific knowledge without model tuning.
this underscores the flexibility of llms in adapting to new tasks.
finding the best performing llms statistically significantly outperform the best performing baseline on mcmd nt indicating better generalization.
moreover they have comparable performance to the best performing baseline on mcmd nl without model tuning.
subjective evaluation results table vii shows the subjective evaluation results for the reference messages andtable vii subjective evaluation results .
evaluation bymessage frominformativeness conciseness expressiveness mcmd mcmd new mcmd mcmd new mcmd mcmd new humanreference .
.
.
.
.
.
come .
.
.
.
.
.
gpt .
turbo .
.
.
.
.
.
deepseek v2 chat .
.
.
.
.
.
llmreference .
.
.
.
.
.
come .
.
.
.
.
.
gpt .
turbo .
.
.
.
.
.
deepseek v2 chat .
.
.
.
.
.
table viii spearman and kendall correlations between automatic evaluation metrics and human evaluation .
metricsinformativeness conciseness expressiveness average bleu .
.
.
.
.
.
.
.
meteor .
.
.
.
.
.
.
.
rouge .
.
.
.
.
.
.
.
cider .
.
.
.
.
.
.
.
gpt .
.
.
.
.
.
.
.
the messages generated by the best performing llms and the best performing baseline i.e.
come.
we observe that the best performing llms outperform come in both human and llm based evaluation.
the wilcoxon signed rank test shows that the performance difference is statistically significant at the confidence level of .
the results demonstrate the ability of llms to generate concise and readable commit messages with more comprehensive semantics.
specifically gpt .
turbo outperforms come in terms of informativeness conciseness and expressiveness by an average of .
.
.
and .
.
.
on the two datasets respectively.
notably despite come s superior performance in objective evaluation on mcmd it falls behind in subjective evaluation.
this discrepancy is attributed to the low quality reference messages in mcmd.
as a learning based method come tends to reproduce such low quality messages leading to better objective evaluation results but poorer performance in human judgment.
additionally we observe that the scores of reference messages in mcmd new are higher than those in mcmd indicating that our created mcmd new is better in data quality.
we further investigate the correlation between automatic evaluation metrics and human evaluation by calculating spearman s and kendall s .
the results are shown in table viii.
we observe that llm based evaluation has much higher correlations with human judgment than other automatic evaluation metrics on both spearman s and kendall s .
this suggests that llm based evaluation is more reliable for evaluating the quality of commit messages.
finding the best performing llms statistically significantly outperform the best performing baseline in human and llm based evaluation.
among automatic evaluation metrics llm based evaluation has the strongest correlation with human evaluation indicating its superior reliability in evaluating the quality of commit messages.
fig.
.
results of root cause analysis.
c. rq3 root cause analysis setup in this rq we aim to further understand the root causes of underperformance for the best performing llms focusing on cases that received zero scores on all objective evaluation metrics in rq2.
we sampled such cases from each of the best performing llms using the same sampling method as in subjective evaluation totaling cases.
two authors carefully read the code changes reference messages and generated messages by llms of selected cases to identify the root causes of underperformance.
following the initial analysis the two authors discussed their disagreements to reach a consensus.
when there is no agreement between the two authors another author is introduced to discuss and resolve the disagreement.
results fig.
presents the results of root cause analysis which includes two major categories.
inaccurate measurement refers to false positives where the messages generated by llms are correct based on our manual inspection but the objective evaluation metrics are low.
three types of root causes were identified in this category low quality reference messages lrm where the reference messages have little useful information.
for example as shown in fig.
the reference message fails to describe what was changed whereas the message generated by llms is more informative and gives a better summary.
semantically similar ss where the message generated by llms is semantically similar to the reference message.
for example as shown in fig.
all messages correctly convey the intent of the change.
however the message generated by llms scores zero because it fails to use the exact words present in the reference message.
reasonable improvement ri where the message generated by llms has a reasonable improvement over the reference message.
for example as shown in fig.
the message generated by llms more precisely conveys the specific intent of the code change compared to the reference message.
finding .
of llm s underperforming cases were caused by inaccurate measurement which indicates the limitation of traditional metrics and the urgent need for new metrics to accurately evaluate the performance of llm based commit message generation approaches.fig.
.
an example of a low quality reference message.
fig.
.
an example of a semantically similar message.
incorrect prediction refers to true positives where the messages generated by llms fail to accurately reflect the change intention.
we identified three types of root causes in this category lack of contextual knowledge lck refers to cases where only code changes are not enough to provide the necessary contextual knowledge to generate correct commit messages.
additional information such as related issues and pull requests is required to fully understand the code changes.
adverse demonstrations ad refers to cases where the reference messages in demonstrations are of low quality or the code changes in demonstrations have low similarity to the code change query.
the former leads llms to learn and generate uninformative messages while the latter prevents llms from understanding the desired behavior.
specifically we observe that deepseek v2 chat is more susceptible to adverse demonstrations.
model fallacy mf refers to cases where llms fail to correctly generate commit messages due to the deficient ability of the model itself.
we further investigated potential mitigation strategies to enhance llm s performance in cases of ad and mf as lck requires more information.
for ad we refrained from providing demonstrations for llm and prompted it to generate commit messages based solely on the code changes i.e.
zeroshot learning .
the results showed that all cases were resolved.
formf we utilized gpt a more advanced llm to generate commit messages leading to resolving .
cases.
fig.
.
an example of a reasonable improvement.
finding the main root causes of llm s underperformance are lack of contextual knowledge adverse demonstrations and model fallacy.
two potential mitigation strategies were providing high quality demonstrations and improving large language models.
v. d iscussion a. implications large language models are few shot committers.
our empirical study reveals that llms are capable of generating high quality commit messages leveraging only a few demonstrations.
our results show that the best performing llms statistically significantly outperform the best performing baseline in both human and llm based evaluation.
this indicates that developers could utilize llms to help them automatically generate commit messages.
specifically we recommend developers use the open source deepseek v2 chat model to avoid privacy and security concerns.
for researchers this also indicates that the comparison with llms is necessary when evaluating new commit message generation approaches.
however we also observe that additional code pre training on general llms does not yield better performance on the commit message generation task.
further research is needed to explore llms specifically designed for code changes.
high quality datasets are urgently needed.
the quality of datasets for commit message generation has not been thoroughly verified.
these datasets are usually crawled from open source projects and subjected to simple data cleaning.
our results show that of llm s underperformance was caused by low quality reference messages.
recent work has developed automated classifiers to identify low quality commit messages which pave the way to construct highquality datasets.
another way to enhance dataset quality is leveraging llms to generate high quality pseudo training examples based on their rich knowledge .
commit messages can be written in diverse ways but current datasets only have one reference message which limits the diversity of the generated messages.
furthermore our results show that .
of llm s underperformance was caused by lack of context.researchers can utilize additional information such as issues to construct richer datasets with multiple reference messages.
llm based evaluators are promising.
automatic evaluation metrics such as bleu simply assume that the reference messages are the gold standard and provide a quick assessment by quantifying the overlap of words or characters between the generated and the reference messages.
however these metrics often fail to capture semantic quality like informativeness or usefulness and their reliability can be further undermined if the reference messages are of poor quality.
human evaluation on the other hand can overcome the limitation but is impractical to evaluate the whole test set due to its time consuming and laborious nature.
recent studies reveal that the llmbased evaluators achieve high alignment with human judgment which is in line with our finding .
therefore we advocate using llm based evaluators as a reliable alternative for evaluating the quality of generated messages combining the benefits of both automatic metrics and human evaluation.
b. threats to validity potential data leakage since llms are trained on extensive data one potential threat is data leakage i.e.
the studied llms have seen the commit messages in the test set.
for example the training corpus of chatgpt includes opensource projects before sep. .
however we observe that llms do not perform well with zero shot learning indicating a low probability of direct memorization.
to further mitigate this threat we created a new dataset mcmd new with opensource projects after jan. to evaluate the studied llms.
randomness of llms and sample selection the randomness of llm inference is a potential threat.
to mitigate this we set the temperature to to generate deterministic outputs and ran experiments five times for all open source models which yielded more reliable and stable results.
due to budget constraints we did not run multiple times for closedsource models in rq2.
moreover the randomness of sample selection in subjective evaluation and root cause analysis is another potential threat.
to mitigate the threat and ensure diverse and representative samples we divided the dataset into multiple clusters and selected samples from each cluster.
subjectivity of human the subjective nature of human evaluation in rq2 inevitably presents a potential threat.
to reduce the threat we invite three volunteers including postgraduates majoring in computer science and industry professionals with relevant experience to evaluate the commit messages and the final result is the average of the three volunteers.
moreover the subjectivity of manual root cause analysis in rq3 is another potential threat.
to address this we obeyed a rigorous annotation process with two authors independently annotating each sample and a third author resolving any inconsistencies or conflicts through discussion.
vi.
r elated work a. commit message generation several approaches have been proposed to automatically generate commit messages which can be categorized as rule based retrieval based and learning based approaches.
rulebased approaches extract information from code changes and generate commit messages with predefined templates .
for example cort es coy et al.
extracted the stereotype type and impact set of a commit then filled predefined templates with extracted information to generate commit messages.
retrieval based approaches find the most similar code changes in the dataset and reuse their commit messages .
for example liu et al.
represented code change as bag of words vectors and then calculated the cosine similarity of them to find similar code changes.
learning based approaches design neural machine translation models to translate code changes into commit messages .
for example nie et al.
pre trained the transformer model to learn the contextualized representations of code changes and then fine tuned the model for downstream commit message generation.
dong et al.
represented code changes with a fine grained abstract syntax tree and used graph neural networks to extract features and generate commit messages.
b. llms for software engineering large language models llms have garnered significant attention and adoption in both academic and industrial domains including software engineering due to their exceptional performance across a wide range of applications.
for example geng et al.
investigated the feasibility of utilizing llms to address multi intent comment generation.
the most related work is gao et al.
which evaluated the capability of icl using chatgpt on code related tasks such as bug fixing.
however code related tasks naturally differ from code change related tasks like commit message generation.
recently several studies have evaluated llm based commit message generation .
however these studies primarily evaluated the performance of chatgpt in a simple zero shot setting with basic prompts.
in contrast our study selected six mainstream llms and explored their performance in complex few shot settings with different prompt and demonstration designs.
in addition we conducted subjective evaluations and analyzed the root causes of underperformance.
vii.
c onclusion and future work in this paper we conduct an empirical study on commit message generation using large language models llms via in context learning icl .
specifically we assess the impact of prompt and demonstration settings and examine llm s effectiveness on a popular dataset and a new dataset we created.
our study highlights the capability of llms to generate commit messages through icl and identifies several directions for future research.
in the future we intend to develop an llm integrated tool that seamlessly fits into the existing software development lifecycle aiding developers in efficiently generating high quality commit messages.
acknowledgment we thank the anonymous reviewers for their valuable comments.
this work was supported by ant group research fund.