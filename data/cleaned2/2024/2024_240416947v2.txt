fuzzing mlir compilers with custom mutation synthesis ben limpanukorn university of california los angeles blimpan cs.ucla.edujiyuan wang university of california los angeles wangjiyuan cs.ucla.eduhong jin kang university of california los angeles hjkang cs.ucla.edu zitong zhou university of california los angeles zitongzhou cs.ucla.edumiryung kim university of california los angeles miryung cs.ucla.edu abstract compiler technologies in deep learning and domainspecific hardware acceleration are increasingly adopting extensible compiler frameworks such as multi level intermediate representation mlir to facilitate more efficient development.
with mlir compiler developers can easily define their own custom irs in the form of mlir dialects.
however the diversity and rapid evolution of such custom irs make it impractical to manually write a custom test generator for each dialect.
to address this problem we design a new test generator called s ynth fuzz that combines grammar based fuzzing with custom mutation synthesis.
the key essence of s ynth fuzz is two fold it automatically infers parameterized context dependent custom mutations from existing test cases.
it then concretizes the mutation s content depending on the target context and reduces the chance of inserting invalid edits by performing kancestor and prefix postfix matching.
it obviates the need to manually define custom mutation operators for each dialect.
we compare s ynth fuzz to three baselines grammarinator a grammar based fuzzer without custom mutations mlirsmith a custom test generator for mlir core dialects and neuri a custom test generator for ml models with parameterization of tensor shapes.
we conduct this comprehensive comparison on four different mlir projects.
each project defines a new set of mlir dialects where manually writing a custom test generator would take weeks of effort.
our evaluation shows that s ynth fuzz on average improves mlir dialect pair coverage by .
which increases branch coverage by .
.
further we show that our context dependent custom mutation increases the proportion of valid tests by up to .
indicating that s ynth fuzz correctly concretizes its parameterized mutations with respect to the target context.
parameterization of the mutations reduces the fraction of tests violating the base mlir constraints by .
increasing the time spent fuzzing dialect specific code.
index terms grammar based fuzzing program synthesis program transformation mlir compiler testing code patterns i. i ntroduction deep learning compilers are a critical component of ai workflows that enable pytorch and tensorflow models to be compiled to a variety of hardware architectures.
one of the leading technologies powering such dl compiler development is llvm s multi level intermediate representation mlir framework.
unlike llvm which defines a single common intermediate representation ir mlir enables developersto extend the underlying ir through the concept of mlir dialects .
each mlir dialect defines a new ir consisting of a unique set of operations types and attributes with domain specific semantics.
for example the ir for machine learning is modeled a computation graph while the ir for llvm is modeled as a sequence of program instructions.
take the circuit ir compilers and tools circt project as an example.
it leverages the mlir framework to build a compiler for heterogeneous compilation by defining new dialects with new operations.
for example circt uses a generic hardware abstraction by defining the hwand comb dialects with operations to represent abstract hardware modules and combinational logic.
listing shows a snippet of mlir representing a hardware module containing a custom mlir operation called comb.add which represents combinational addition.
such fast evolution and diversity of underlying custom irs presents challenges for developing custom test generators.
google reported in that over dialects have been internally developed .
in the four years since the mlir project s initial public release public downstream projects like circt have also each contributed multiple custom dialects .
general purpose fuzzers such as afl fail to effectively generate or mutate mlir due to its highly structured form.
for instance syntactically correct mlir must have proper nesting of operations the correct number of operands and outputs for each operation valid type annotations and valid attribute names and values.
grammar based fuzzers such as grammarinator use a context free grammar to constrain input generation however there are two limitations for this domain.
first developers must supply a refined grammar that is specific to each mlir dialect which is a derivative of the base mlir grammar.
this refined grammar must include specialized production rules for each dialect s operation names attribute names and the number of inputs and outputs.
for instance to generate an onnx.conv2d operation the refined grammar would need a specialized production rule with operation name onnx.conv2d that has output inputs and nested attribute names called kernel size padding and more.
creating such refined grammars byarxiv .16947v2 aug hw.module hardware module definition 2 bb0 a1 i2 bit integer input defines a new constant with value bitwidth c1 hw.constant value i2 i2 adds the constants and outputting o1 comb.add a1 c1 twostate i2 i2 i2 hw.output o1 i2 output of module is extra boilerplate omitted listing .
this mlir code snippet uses a new hardware dialect comb for combinational logic in the circt project.
hand is impractical.
for example the four mlir projects in our evaluation collectively define dialects with unique operations.
each operation would require at least one production rule in the refined grammar.
second in addition to defining a refined grammar developers must externally encode semantic constraints required by each custom dialect.
custom generator based fuzzers in the vein of csmith nnsmith and mlirsmith manually encode semantic constraints in terms of imperative code.
however the large and continually increasing number of mlir dialects makes it prohibitively expensive to write custom test generators manually.
we observe that the fast evolving compiler infrastructure of irs requires a test input generator that can learn semantic constraints automatically.
to this end we propose a new approach called s ynth fuzz drawing inspiration from techniques for automated patch synthesis .
these techniques synthesize code edits from examples learn the code contexts in which the transformations are appropriate and then concretize the code edits to the matching code contexts.
like generator based fuzzers s ynth fuzz is capable of preserving context sensitive constraints such as the cardinality of operation arguments and return values the def use relationships of values and the consistency of type annotations.
the key novelty is that s ynth fuzz can do so without the significant manual effort required to write custom generators by hand.
s ynth fuzz accomplishes this by synthesizing parameterized mutations from seed test cases.
colored pairs in listing represent the def use relationships and type consistency that needs to be satisfied.
a parameterized mutation derived from listing would encode the knowledge that the operation comb.add is nested within thehw.module denoted as ancestor k1 is preceded by one hw.constant operation denoted as l1 and followed by onehw.output operation denoted as r1.
this knowledge enables s ynth fuzz to select an appropriate context to apply the mutation by matching the k ancestors and l siblings and r siblings of the comb.add operation.
the parameterized mutation also encodes the knowledge that comb.add takes two arguments a1 and c1 and returns one value o1 all of which have the same type i2.
synth fuzz parameterizes these arguments and types and then re concretizes them based on the target context to which the mutation is applied.
we compare the effectiveness of s ynth fuzz against grammarinator mlirsmith and neuri.
grammarinator is a representative grammar based fuzzer.
mlirsmith is a representative custom generator for mlir core dialects.
neuriis a custom test generator with limited parameterization i.e.
it parameterizes the tensor shapes and operation s numerical attributes.
we evaluate s ynth fuzz on four mlir based compiler projects llvm onnx mlir triton and circt.
these are chosen as representative mlir projects that define and custom dialects respectively.
for all dialects except the core dialects targeted by mlirsmith and the oneonnx dialect that can be targetted by neuri no custom test generators exist.
writing test generators for these custom dialects is time consuming.
as an example mlirsmith s implementation totals lines of code with lines of code per dialect on average .
we assess s ynth fuzz s fault detection potential by measuring code coverage and mlir dialect pair coverage.
dialect pair coverage is defined as the number of unique pairs of operations dialects that have a data dependency or control dependency.
averaged across over four mlir compiler projects s ynth fuzz outperforms grammarinator mlirsmith and neuri in terms of branch coverage by .
.
and .
respectively.
in terms of dialect pair coverage s ynth fuzz outperforms grammarinator mlirsmith and neuri on average by .
.
and .
.
compared to mlirsmith and neuri s ynth fuzz is capable of covering new custom dialects defined by the four mlir projects.
s ynth fuzz discovers a previously undiscovered bug in circt.
we also perform a case study on the potential of s ynthfuzz to generalize beyond mlir dialects to another domain by automatically generating valid aws cloudformation cf templates that can pass the validity checks ofcfn lint .
s ynth fuzz generates .
greater proportion of valid cf templates compared to grammarinator which demonstrates that s ynth fuzz s custom mutation synthesis can provide significant benefits to learn semantic constraints automatically.
in summary this paper makes the following contributions we design a novel compiler fuzzing technique that obviates the need for defining custom mutations apriori which is impractical when the target ir is highly extensible and constantly evolving.
our method automatically synthesizes and applies multiedit dependence aware custom mutations on the fly.
the key enabler is the construction of parameterized mutations from existing tests and the concretization of the mutations after positioning the context through ancestor path or prefix postfix matching.
we show that our method achieves .
greater code coverage and .
greater dialect coverage within the same time budget compared to existing baseline fuzzers.
the remainder of this paper is organized as follows.
section ii introduces mlir and a motivating example.
section iii presents the design and implementation of s ynth fuzz.
section iv provides the design of our experiments and their results.
section v discusses possible threats to validity.
section vi presents related work.
finally we draw the conclusions of our work in section vii.ii.
b ackground a. mlir multi level intermediate representation multi level intermediate representation mlir is a modular compiler framework that differs from traditional approaches by enabling developers to extend the intermediate representation.
rather than defining a single monolithic ir with a fixed set of types and instructions like llvm s ir mlir is extensible by design.
compiler developers may define new mlir dialects consisting of custom operations and types tailored to the domain language or architecture the compiler targets.
mlir dialects can be progressively lowered forming a modular compilation pipeline in contrast with traditional compiler infrastructure that offers limited extensibility.
however this presents a challenge for test generation since dialectspecific operations also introduce new constraints that are dialect specific.
take the circuit ir compilers and tools circt as an example.
circt is a unified framework built on mlir that enables optimized hardware design across different backends catering to the needs of heterogeneous compilation.
it defines new dialects with new operations including the comb andhwdialects that define low level hardware operations.
an example of the comb add operation is shown on line of listing .
the full name of this operation iscomb.add where comb is a dialect name and add is the operation name.
as shown in the snippet the operation takes two operands arg0 and c1 and returns a single value o1.
its type signature indicates that the operation takes an input operands of type i2 bit integers and produce an output of type i2.
b. motivating example existing mutation strategies such as recombining test fragments frequently fail to generate test cases capable of exercising deeper compiler logic.
this failure is caused by the large proportion of invalid test cases generated which violate early checks made by the compiler.
for example a the definition of identifiers needs to exist before they are used def use b the types of variables need to remain consistent through the test case type consistency and c the number and type of arguments that match what is required by an operation signature consistency .
to address the limitation of existing fuzzers we present an approach that synthesizes parameterized mutations aiming to implicitly capture these constraints.
consider the following seed test cases a donor program pdin listing that contains the comb.add operation to be inserted in the recipient program prin listing .
we demonstrate how grammar based generation and recombination are unlikely to produce a valid program shown in listing .
a grammar based mutator following a base mlir grammar is unlikely to produce listing because the grammar does not include operation semantics defined by different dialects.
for any given operation the generic mlir grammar only specifies that the syntax of an operation must have a name e.g.comb.sub zero or more return values arguments and attributes and a type signature.
therefore a grammar based1 hw.module 2 bb0 arg0 i2 c1 hw.constant value i2 i2 o1 comb.add arg0 c1 i2 i2 i2 hw.output o1 i2 listing .
a donor program pdfrom which a mutation for inserting the comb.add operation is synthesized from.
hw.module 2 bb0 arg0 i4 arg1 !hw.array 2xi2 hw.bitcast arg1 !hw.array 2xi2 i4 comb.sub arg0 i4 i4 i4 hw.output i4 listing .
a recipient program prto which the mutation for inserting the comb.add operation from listing should be applied to.
hw.module 2 bb0 arg0 i4 arg1 !hw.array 2xi2 hw.bitcast arg1 !hw.array 2xi2 i4 comb.sub arg0 i4 i4 i4 o1 comb.add arg0 c1 i2 i2 i2 hw.output i4 listing .
a test case created by grammarinator s recombine operation.
it deletes line4 and adds line5.
this test case is invalid as it violates the def use relation and the type consistency.
hw.module 2 bb0 arg0 i4 arg1 !hw.array 2xi2 hw.bitcast arg1 !hw.array 2xi2 i4 comb.sub arg0 i4 i4 i4 comb.add arg0 i4 i4 i4 hw.output i4 listing .
a test case created by s ynth fuzz s context dependent parameterized mutation.
this mutation replaces an operation comb.sub with comb.add .
the test case is valid as s ynth fuzz matched a corresponding context before concretiz ingitsmutation to the target context.
fuzzer generating a comb.add operation would be unaware of the signature of the operation as defined by the comb dialect.
without a refined grammar for each dialect the fuzzer would be unaware of the associations between variables and their types e.g.
arg0 0with type i4 and comb.add having exactly two input values and one return value.
another common grammar based mutation strategy is to recombine the fragments of existing tests with other test cases.
to illustrate comb.sub at line of the recipient program pr in listing is replaced with comb.add from line of the donor program pdin listing producing the mutated test in listing .
however after the replacement the values and types of the comb.add operation are inconsistent with its new surrounding context.
the resulting test will be rejected early by the compiler as it violates the def use constraint since the value c1 was not defined before it was used.
inferring such semantic constraints often turing complete is challenging as discussed in prior work .
listing highlights the changes required to adapt the code using comb.add from pdtopr.
to satisfy the def use constraint the values referenced as arguments to an operation e.g.
0on line must be previously defined e.g.
0on line .
to satisfy type consistency the initially assigned types such as i4 for 0on line must remain consistent in itssubsequent