chatgpt inaccuracy mitigation during technical report understanding are we there yet?
salma begum tamanna salmabegum.tamanna ucalgary.ca university of calgary calgary canadagias uddin guddin yorku.ca york university toronto canadasong wang wangsong yorku.ca york university toronto canadalan xia lanxia ca.ibm.com ibm markham canadalongyu zhang longyu.zhang ibm.com ibm markham canada abstract hallucinations the tendency to produce irrelevant incorrect responses are prevalent concerns in generative aibased tools like chatgpt.
although hallucinations in chatgpt are studied for textual responses it is unknown how chatgpt hallucinates for technical texts that contain both textual and technical terms.
we surveyed software engineers and produced a benchmark of q a pairs from the bug reports of two oss projects.
we find that a rag based chatgpt i.e.
chatgpt tuned with the benchmark issue reports is .
correct when producing answers to the questions due to two reasons limitations to understand complex technical contents in code snippets like stack traces and limitations to integrate contexts denoted in the technical terms and texts.
we present chime chatgpt inaccuracy mitigation engine whose underlying principle is that if we can preprocess the technical reports better and guide the query validation process in chatgpt we can address the observed limitations.
chime uses context free grammar cfg to parse stack traces in technical reports.
chime then verifies and fixes chatgpt responses by applying metamorphic testing and query transformation.
in our benchmark chime shows .
more correction over chatgpt responses.
in a user study we find that the improved responses with chime are considered more useful than those generated from chatgpt without chime.
index terms chatgpt hallucination software issue reports i. i ntroduction the reliability of llms is often questioned due to their tendency to produce nonsensical or incorrect outputs a phenomenon commonly referred to as hallucination .
like any llm chatgpt can also suffer from hallucination issues like inconsistency in responses or factual inaccuracies.
these problems can arise even when the model is provided with the context as a document paragraph.
while progress is made to assess hallucinations in textual data we are not aware of how hallucinations can be detected and mitigated for software technical reports that contain both textual and technical terms e.g.
crash dumps code snippets etc.
this paper studies the detection and mitigation of chatgpt inaccuracies in technical reports.
we pick software bug reports for our study because bug reports often contain a blend of descriptive text technical terminology code