automated unsupervised and auto parameterized inference of data patterns and anomaly detection qiaolin qin heng li ettore merlo maxime lamothe dept.
of computer and software engineering polytechnique montreal montreal canada qiaolin.qin heng.li ettore.merlo maxime.lamothe polymtl.ca abstract with the advent of data centric and machine learning ml systems data quality is playing an increasingly critical role for ensuring the overall quality of software systems.
data preparation an essential step towards high data quality is known to be a highly effort intensive process.
although prior studies have dealt with one of the most impacting issues data pattern violations these studies usually require data specific configurations i.e.
parameterized or use carefully curated data as learning examples i.e.
supervised relying on domain knowledge and deep understanding of the data or demanding significant manual effort.
in this paper we introduce riolu regex inferencer auto parameterized learning with uncleaned data.
riolu is fully automated automatically parameterized and does not need labeled samples.
riolu can generate precise patterns from datasets in various domains with a high f1 score of .
exceeding the state of the art baseline.
in addition according to our experiment on five datasets with anomalies riolu can automatically estimate a data column s error rate draw normal patterns and predict anomalies from unlabeled data with higher performance up to .
improvement in terms of f1 than the state of the art baseline even outperforming chatgpt in terms of both accuracy .
higher f1 and efficiency less inference time .
a variant of riolu with user guidance can further boost its precision with up to .
improvement in terms of f1.
our evaluation in an industrial setting further demonstrates the practical benefits of riolu.
index terms pattern anomaly detection pattern based data profiling unsupervised learning supervised learning i.introduction data are an essential part of modern software .
in particular data centric or ai enabled software systems incorporate massive data to provide querying or intelligent services to users.
many studies have discussed the importance of data quality in software engineering poor data quality can harm software engineering efforts.
in particular prior studies have stressed the importance of data quality assurance for the overall software quality .
for example a recent study discusses the data inconsistency issue in mobile apps which can confuse users and cause app quality degradation .
according to an industrial investigation by stonebraker and rezig data preparation could take over of the development time when executed manually .
to reduce the labor cost of data preparation companies and researchers aim to find automatic ways to understand the quality and detect anomalies in their data.
corresponding author.data pattern related anomalies or pattern anomalies are a major type of flat structural data anomaly and are widely studied by researchers given their hardto detect and hard to debug nature .
unlike log anomalies and other time series anomalies which describe abnormal system events or status and typically require feature sets for anomaly detection data pattern anomalies describe anomalies in the data structures themselves which may arise in features.
they occur when a record violates predefined or implicit patterns .
prior work defines data patterns as regular expressions regexes that succinctly describe the syntactic variations in the strings .
for example a data pattern for a dataset of dates could be d d d .
while pattern based data profiling focuses on data description with patterns pattern based anomaly detection leverages the patterns to detect anomalies i.e.
data instances that violate the data patterns .
however two critical challenges face automated data profiling and pattern anomaly detection how can we automatically infer data patterns without prior data format knowledge given its infinite possibilities?
how can we ensure the health of the inferred patterns i.e.
avoid anti patterns that cover anomalies themselves ?
this work addresses these challenges and provides an automated unsupervised and automatically parameterized solution for pattern inference and anomaly detection.
prior work has proposed different approaches for patternbased data profiling i.e.
pattern inference and anomaly detection .
these approaches use either declarative i.e.
manually designed or automatically inferred regular expressions to capture the patterns in the data .
although declarative tools such as google s tfdv and amazon s deequ are straightforward to use manually designing the regular expressions for each data column is still very effort and time consuming .
to reduce human effort researchers designed tools that can automatically draw patterns from the data and identify anomalies.
for example xsystem split the structure into three layers of representation i.e.
branch token symbol to incrementally extract regular expressions from a provided table.
flashprofile clusters the records by their syntactic similarity and assigns patterns for each cluster it then filters anomalies by dropping low frequency e.g.
less than patterns after synthesizing column patterns.
auto validate treats pattern quality as a global notion and ensures the generalizability of a patternarxiv .05240v1 dec 2024by learning it from one column and testing its generalization ability on other similar columns.
however these tools still suffer from two shortcomings parameter configuration that requires domain expertise for example xsystem requires a predefined number of branches for pattern generation which necessitates prior knowledge of the number of valid patterns insufficient precision for example the similarity based approach used by flashprofile cannot distinguish records that are seemingly close to each other leading to inaccurate patterns.
to overcome the difficulty of parameter configuration and improve the precision we propose riolu regex inferencer auto parameterized learning with uncleaned data a pattern inference and anomaly detection approach that can be adaptively parameterized.
inspired by auto validate riolu regards high quality patterns as patterns that can cover adequate healthy samples inspired by potter s wheel riolu selects the most expressive and concise patterns.
our work uses statistical heuristics and k means clustering to automatically infer parameters thus avoiding manual parameter setting.
moreover the approach is built using a rule based progressive structure which enhances the pattern quality.
to assess the usefulness of riolu we address the following two research questions rqs in this study rq1 how well can riolu profile data in different domains?
deriving precise patterns is a prerequisite for pattern anomaly detection.
this rq aims to assess riolu s ability in deriving patterns for data description from all records a.k.a.
data profiling .
our results show that riolu outperforms flashprofile a state of the art open sourced data profiler and gpt .
turbo on data in various domains.
rq2 how precisely can riolu detect pattern anomalies?
pattern anomalies are common in uncleaned data.
this rq assesses riolu s ability to learn patterns and detect anomalies in such data.
we observe that riolu can effectively detect pattern anomalies in public datasets and an industry setting outperforming state of the art baselines.
our work makes the following main contributions we proposed a fully automated approach riolu for data pattern inference and anomaly detection without supervision or parameter configuration.
we demonstrated that a human guided variant of riolu guided riolu can further improves the performance of the fully automated version i.e.
auto riolu1 .
we evaluated auto riolu on real world commercial data with practitioners confirming its ability to generate precise patterns for data quality verification and anomaly detection.
practitioners and researchers can leverage riolu to understand their data and improve data quality reducing their time and effort in data preparation or data quality assurance.
our data and code are published in a replication package .
ii.
b ackground riolu targets pattern based data profiling and anomaly detection.
this section first defines patterns in our context 1we use the name auto riolu to refer to the fully automated version in order to distinguish it from the guided riolu version.followed by the definitions of the two pattern based tasks.
a. patterns following song and he we define patterns as structured sequences containing basic components.
in english based patterns basic elements include upper letters lower letters digits and symbols .
according to ilyas et al.
symbols serve as a natural delimiter in splitting the patterns.
for example mar is a string containing all these pattern elements and can be represented as digit symbol upper lower symbol digit .
such patterns can be used to match or describe a data domain.
b. pattern based data profiling the goal of pattern based data profiling is to distinguish patterns in the records by describing the data using derived regular expressions.
compared to previous pattern based data profiling tools such as microsoft s sql server data tools ssdt and ataccama one which heavily relies on a small predetermined set of atoms flashprofile uses a bottomtop technique to cluster records through syntactic similarity and then extract patterns from each cluster it requires no userdefined domains or base patterns e.g.
dates phone numbers to reach a high precision.
however we noticed that the tool may not be robust when the differences between records are subtle.
for example a column of state province abbreviations should only contain two upper letters i.e.
.
due to poor data quality the column may also contain noisy records with lower letters e.g.
qc on or incorrect string lengths e.g.
m abb .
given the small syntactic distances between different spelling types they may be identified as one cluster and the profiling result would be which fails to precisely describe the underlying pattern.
c. pattern anomaly detection pattern anomalies indicate records that violate pre defined or inferred regex like patterns .
following huang and he we define pattern anomaly detection as the process of filtering inconsistent data i.e.
anomalies using regular expressions.
as introduced in sec i existing tools provide automatic inference approaches for pattern anomaly detection.
although these tools have largely reduced human labor they always have the problem of requiring domaindependent parameter settings.
for example xsystem requires the user s prior knowledge of the number of branches i.e.
number of acceptable pattern types which could be hard to fix at the first use and may not be constant across domains e.g.
a column for id can only accept one branch as d while a column for date may accept both patterns of d d d and d d d written as and respectively as valid .
flashprofile require users to define the threshold for low frequency patterns i.e.
a pattern with a frequency lower than the threshold is considered as abnomal .
however a threshold may not hold for different datasets and it is difficult for users to determine a solid threshold without fully comprehending the dataset.annotations column input column subset r cov estimation unsupervised supervised pattern generation pattern selection patterns for data profiling patterns for anomaly detection anomaly detection data profiling column sampling constrained template generation r cov both tasks content collection constraint inferencefig.
.
an overview of riolu s structure.
iii.
a pproach a. overview the input of riolu is a two dimensional table with random numbers of columns and rows.
the structure of each column is undefined for example they can be names ids or urls.
the goal of riolu is to automatically derive the patterns e.g.
yyyy mm dd of each column without prior knowledge the patterns can then be used to detect data anomalies e.g.
invalid urls .
as shown in fig the approach of riolu for determining patterns and detecting anomalies in a column can be decomposed into the following steps column sampling sample a subset of data from the column to generate the patterns.
coverage rate rcov estimation estimate the percentage of healthy values rcov in each column.
constrained template generation generate raw templates for each record with a granularity constraint.
pattern generation generate pattern constraints for each template according to the coverage rate.
pattern selection select patterns based on some heuristics e.g.
their generalizability .
as shown in fig all five steps are needed for the anomaly detection task.
on the other hand the data profiling task only involves the steps of column sampling constrained template generation and pattern generation as all data are assumed to be healthy for this task i.e.
rcovis constantly .
estimating the portion of healthy data i.e.
rcov is an essential process in generating patterns for anomaly detection as healthy patterns should only be learned from healthy data.
since automated coverage rate estimation depends on template generation pattern generation and pattern selection we introduce this step after explaining the other three steps.
b. column sampling typically a data column e.g.
a column about dates only covers a subset of the entire data domain all possible values of dates that may come through the data pipeline.
ideally highquality patterns should not only cover all the samples in the available data that can be seen during the pattern generation process but also be able to generalize to unseen future data .
therefore we use a subset from the column as the pattern generation subset and use the whole data column for generalizability evaluation and pattern selection.
to ensure the representativeness of the sample and the pattern generation efficiency we use a sample size ntrwhich is calculated using a z score with a confidence level of and an error marginof ntrsamples are randomly drawn from a column whose size is nwithout replacement.
c. constrained template generation template generation without constraints may result in an over abundance of templates.
indeed the basic structure of patterns can be captured by raw templates that contain token s and delimiters .
we could finely split all records using all of their symbol strings i.e.
symbols containing no tokens within such as in fig.
as delimiters.
however overly fine grained splitting of the records may result in under generalization a problem in anomaly detection .
to prevent under generalization we establish an exact matching raterem to control the granularity of raw templates.
rem controls the portion of records that need to be fully split thus determining the required maximum number of delimiters and the granularity of raw templates.
as illustrated in the right part of fig fully splitting all records i.e.
rem results in the last two templates matching only one record.
the scattered raw template distributions may cause the generated patterns to have low frequency leading to a biased result in the pattern selection step see section iii e .
constrained template generation aims to convert records to raw templates under the constraint of rem.
for the data profiling task rem is fixed to as we aim to capture all template structures and do not require pattern selection.
for data anomaly detection we should only obtain templates for the exact matching of the healthy data.
hence we set the exact matching rate with the same value as the estimated coverage rate i.e.
the estimated percentage of healthy data see iii f rem rcov with the intuition that all the healthy data would have exact template matching.
fig is an example of a datetime column with five records from a sampled set ntr .
first we calculate the number of records to be fully split using rem.
the number shall be calculated with ntr rem and rounded to an integer.
in our example when setting rem to we should fully split all five samples when rem .
four samples are to be fully split.
when rem we determine the samples to be fully split using the number of delimiters they contain.
according to the minimum description length principle we capture the minimum number of needed delimiters to fulfill the rem constraint.
the numbers of delimiters in the example are .
hence when rem .
we accept five as the maximum number of acceptable delimiters as it is the minimum number of delimiters required to fully split four samples.
01t00 02t00 gmt 01t00 06t00 20t00 am r em max d templates t t t t t t t t t t t t t t t t t t .
t t t t t t t t t t tfig.
.
an example of date time records and the templates generated under different exact matching rates rem .
t in templates represents tokens.
maxd stands for the maximum number of delimiters accepted under certain rem.
after determining the maximum number of acceptable delimiters we generate raw templates by iterating through all the records.
for records containing less or equal to the maximum number of delimiters we fully split them and take all their symbol strings as delimiters.
otherwise we stop splitting when the maximum value is reached.
under the setting of rem .
the second record contains six delimiters and will be split as t t t t t t i.e.
matching the template of the last record .
the portion t t in the raw template is merged as a t after reaching the maximum delimiter number.
d. pattern generation each record is matched to a raw template after the constrained template generation stage.
we further elaborate on the content constraints and form patterns based on the templates.
raman et al.
use token range and token length to constrain the tokens.
instead of using the tokens ilyas et al.
split the tokens into characters and discuss whether the character slot contains specific static characters or covers one or more character type .
inspired by these approaches we consider these four types of constraints for pattern generation token range token length static character and character type.
we use a waterfall structure to infer the constraints at the four layers with an order from stricter to looser token range determination token length determination static character determination and static type determination.
the intuition is simple if a token has a stricter constraint e.g.
with a token range constraint being either am or pm then we do not need to infer the looser constraints e.g.
then token length or character level constraints .
we iterate records in the sampled set to collect all the contents on the four layers and select content constraints using rcovas a threshold.
we use fig as a running example and assume rcov rem .
.
content collection.
fig illustrates five date time records that match with a template t t t t t .
each token in the template is matched to a string in the records.
for example the first t in the first record corresponds to .
following this method we collect the contents of each token from the five examples as shown in the tokens frame.
these contents are used for token range and token length constraint inference.
the tokens can be further decomposed into character slots for static character and character type inference.
in fig we decompose the third token as an example.
the character collection is similar to token content collection we iterate through the tokens and fit their contents into the corresponding slot position as shown in the character slots for t3 frame.constraint inference.
when selecting content constraints auto validate aims to find patterns that capture most of the values under a small tolerance value e.g.
for anomalies in the training set.
however we argue that using a constant predefined is hardly precise.
instead the value should be adjusted based on the specific error rate of each dataset.
therefore we estimate the percentage of healthy values rcovto guide the constraint selection stage for the anomaly detection task.
as mentioned in sec iii a for data profiling the coverage rate is given that the goal is to describe all data fully.
token range determination constant token contents e.g.
am or pm are highly frequent .
we use a two class k means clustering technique to cluster the token values based on their frequency.
we choose k means due to its efficiency and wide use for data clustering.
the frequency range that can be reached by a token is ntr .
to ensure that two clusters i.e.
the high and low frequency cluster can always be created we manually insert and1 ntrinto the frequency list.
the frequencies in the high frequency cluster are then summed to compare with the coverage rate rcov if the sum is larger than rcov then the high frequency values are making an adequate match and thus a token range is found.
otherwise we shall further determine whether the token has a length constraint.
in our date time example the template contains five tokens.
as shown in fig all the values for t4andt5share the value of the token value has a frequency of .
hence a token range is assigned to these two tokens.
conversely t1 t2 and t3do not have any token range since each token value has a frequency of .
and is clustered with1 .
i.e.
the low frequency cluster .
token length determination while a specific content range may not be applicable for a token the token may have a limitation to the length.
a static length len for a token is determined if and only if most token contents have a fixed length if the system fails to detect a static length for the current token the minimum token length is captured as lenmin.
in our running example in fig we should detect token length constraints for t1 t2 and t3.
all the contents fort1andt2are in a fixed length while t3contains four character contents and one character content.
under the setting of rcov .
we determine that t3also has a token length constraint since of the contents satisfy the constraint of having character slots.
static character determination we use k means clustering based on frequencies to determine static characters in the pattern similar to the strategy for token range determination.
the frequencies are calculated by counting the existence frequency for a character on a certain slot e.g.
for the first character slot in t3 the frequency of is .
while the frequency for is .
.
we also insert and1 ntrinto the frequency list to ensure the split.
for static length tokens i.e.
with len we detect static character ranges for all len characters for tokens without static length constraints we detect static character ranges for the first lenmin characters.
we decompose t3in fig to demonstrate the approach.
thecharacter slots for t3 records template final pattern token range for t4 t5 static type for slot1 static char for slot3 01t00 06t00 19t00 07t00 13t0 tokens t t t t t 01t00 06t00 19t00 07t00 13t0 t t t t t t d d d d d t00 determine token range determine token length determine static char determine static type ... ... t t t t t slot1 slot2 slot3 slot4 slot5 slot3 slot4 slot5 t t slot1 slot2fig.
.
a date time example of using riolu generating patterns rcov .
.
t stands for tokens.
the green paths indicate a constraint has been found for the input i.e.
token or character slot on the corresponding layer while the red paths suggest that the input does not have a constraint on this layer.
constant length constraint is for this token.
according to the collected character in the character slots for t3 frame the last three slots have static characters but no static character is detected for the first two slots.
static type determination characters are usually categorized into upper lower letters digits and symbols.
static type is to be detected if and only if when a range of static characters i.e.
a stricter constraint fails to be determined.
for each character slot the character types are ranked according to their frequencies from high to low and the types are selected until their cumulative frequency exceeds rcov i.e.
the types cover the majority of the space .
if all types appear in the majority of current character slot the slot would have no constraint.
following the intuition in static character determination for static length tokens the type constraint applies to alllencharacters for tokens without static length constraints we detect character types for the first lenmin characters.
for the example token t3in fig since the first two slots in t3 do not have static characters we check their static character type.
both slots have a static type of digits written as d .
pattern composition after constraints on the four layers are detected we compose them into a pattern.
for tokens with token range constraints we directly replace the token using the range e.g.
the last two tokens in fig are replaced by .
otherwise we write the regular expression for the token using detected constraints.
take t3as an example using the d constraints for the first two character slots and the t00 constraints for the last three slots the regular expression fort3is d t00 .
following this procedure a pattern is generated as illustrated in the final pattern frame in fig .
e. pattern selection a pool of candidate patterns is constructed after the patterngeneration step.
however not all patterns are considered as healthy patterns.
as shown in fig we further perform pattern selection for the data anomaly detection task.
similar to padhi et al.
we consider patterns with low matching rates on the dataset as anomalies.
for each pattern pi its matching rate i.e.
frequency on the whole dataset is calculated through the number of records matched using regex.
we then apply k means clustering to split the patterns into two clusters based on their matching rate automatically the high frequency and low frequency clusters.
the clustering technique could avoid the domain dependent threshold determination problem raised in sec ii c. we assume that at least one pattern shall be accepted for the corresponding column.
thus we insert a low matching rate noise i.e.
n to ensure a low frequency cluster is created.
patterns labeled as high frequency are selected as healthy patterns whereas those labeled as low frequency are not further used.
finally the healthy patterns are used to detect anomalous records in the column records that do not match any healthy pattern are identified as anomalies.
f .
coverage rate rcov estimation coverage rate rcovis essential in template generation and pattern generation.
similar to column sampling sec iii b for the sake of generalizability and efficiency we use a statistically representative sample of size ntr of a data column to estimate the corresponding rcov either in a supervised or an unsupervised way.
guided riolu supervised estimation.
with human involved it is feasible to notice pattern inconsistency in the sampled data and label them with domain knowledge.
users are required only to label whether a record in the sampled set of size ntr is a pattern anomaly in the column.
the labeled data can be used to calculate the portion of healthy data which is the portion of data to be covered rcov .
column pattern generation on subsets assumption r cov init .
pattern selection for each pattern pool r cov matching rate calculation for each pattern pool average subset2 subset4 subset3 subset1 subset5 fig.
.
the coverage rate estimation process in unsupervised approach.
auto riolu unsupervised estimation.
our intuition for pattern anomalies is that these anomalies cannot form large pattern clusters similar to padhi et al.
.
conversely healthy patterns are highly frequent and robust enough to form a large cluster.
as illustrated in fig we apply a three step approach to estimate the coverage rate.
we first randomly draw five subsets i.e.
nsubset from the considered data column to ensure measurement stability each with size ntr.
then we generate five initial pattern pools from these five subsets with the assumption of rcovinit .
following the steps described in iii c and iii d. we choose rcovinit .
as the initial overage rate since prior work shows that the anomaly data rates are typically less than .
we further evaluate the sensitivity of our approach for the settings of the rcovinitvalue and the nsubset value in iv c. in the second step we select patterns with high matching rates from the five initial pattern pools through the pattern selection process as described in iii e .
in the last step each selected pattern pool is used to calculate the portion of matches they can create i.e.
matching rate on the whole dataset.
the matching rates provide us with an estimation of the portions of large clusters i.e.
potentially healthy clusters noise patterns provide few matches and will be dropped in the pattern selection stage.
using five subsets can statistically suppress the randomness of the evaluation process.
hence we take the average matching rate as the estimated rcov.
iv.
e xperiments this section describes our experiment design and results for evaluating riolu through our two research questions.
rq1 evaluates riolu s capacity to produce high quality patterns in a pattern based data profiling setting.
rq2 evaluates riolu s capacity to detect pattern anomalies from uncleaned data in an unsupervised or semi supervised manner.
a. rq1 how well can riolu profile data in different domains?
patterns extracted for data profiling should adequately and precisely describe the corresponding data domain.
further it does not assume that the dataset is dirty in nature.
hence the coverage rate to be reached in data profiling tasks shall be and all the generated patterns should be accepted to describe the domain.
through the data profiling quality analysis we could gain insight into riolu s pattern generation capability.
dataset we choose a set of public datasets used to evaluate data profiling in prior work flashprofile .
to ensure generalizability we consider the entire domains data with datasets with to around 20k records from a variety of domains e.g.
amino structure software configuration ip address the files along with detailed domain information are listed in our replication package.
using these datasets allows us to have a fair comparison with prior work .
methods compared we compare riolu with the state of the art open source tool for data profiling flashprofile as well as chatgpt which has recently shown promising results for regex generation .
chatgpt .
llms have shown their power in various tasks including regex inference .
we specified the pattern based data profiling task to the gpt .
turbo apitable i the average performance of chatgpt f lash profile and riolu for data profiling tp average true positive rate fp average false positive rate p average precision r average recall f1 average f1score .
tp fp p r f1 chatgpt .
.
.
.
.
flashprofile .
.
.
.
.
riolu .
.
.
.
.
fig.
.
riolu s profiling quality on flashprofile domains dataset.
the white peaks in the upper part denote the false negative rate and the red peaks in the lower part indicate the false positive rate.
and obtained the responses as it was one of the most powerful and affordable at the experiment time.
the regexes are manually extracted from the responses.
we provide the prompt template and responses in the replication package.
flashprofile .
flashprofile is the state of the art opensourced tool for pattern based data profiling.
flashprofile clusters record using syntactic similarity and draw a pattern for each cluster.
it significantly outperforms previous stateof the art tools including microsoftssdt and ataccama one .
riolu .
data profiling does not require estimating the coverage rate given that the patterns should cover all data.
to use riolu for data profiling we set the coverage rate rcovto and skip the pattern selection step.
evaluation and metrics the goal of data profiling is to extract patterns that can fully and solely describe a domain.
we followed the evaluation settings used in prior work .
of the data from each dataset is randomly extracted as the pattern generation dataset.
while prompting chatgpt we observed that for datasets containing many i.e.
10k records or long content e.g.
protein structures a random sample of of the data exceeded the token limit of chatgpt.
thus we only use of these datasets for training data.
the true positive rate under this scenario is the portion of matches produced by the pattern pool in the remaining or for datasets with chatgpt data from the same domain.
an equal number of records are randomly drawn from other domains and the false positive rate is the matching rate on records belonging to the other domains.
the intuition is that patterns drawn from a dataset shall describe all the records in this domain while not over generalizing to other domains.
we also calculate the precision recall and f1 score based on these metrics.
evaluation result table i shows the average performance of riolu and the baselines for data profiling.
fig illustrates riolu s profiling quality in the same manner asflashprofile does showing the detailed performance for each individual dataset.
as reported in flashprofile has an average true positive rate of .
an average false positive rate of .
with an average f1 score reaching a high value of .
.
according to table i riolu pushed the accuracy higher with an average f1 score of .
.
the average true positive rate obtained by riolu is around .
while the false positive rate is .
less than of that of flashprofile .
on the other hand the overall performance of chatgpt is worse than both flashprofile and riolu the average f1 score is only .
.
in particular chatgpt s false positive rate is about .
times that of riolu.
besides chatgpt generated one pattern that encountered issues during regex compiling i.e.
bad character range issue in a piece of regex and patterns that failed to match any record in the domain e.g.
for code records with only characters .
the results indicate that the patterns created using our raw templates and water flow constraint selection approaches can accurately capture pattern features for each domain and distinguish them from other domains.
moreover we noticed that data driven pattern profiling methods tend to be more accurate than chatgpt.
we also observe a general trend of improved performance of riolu when the data size increases in fig .
this is because a small data size may fail to sample adequate supportive records as all the patterns may have a low frequency.
therefore these patterns cannot be accurately captured.
when the data size increases the trend of highly frequent patterns becomes significant and thus riolu can learn more robust patterns.
summary.
riolu performs well on the data profiling task on datasets from various domains.
in particular the false positive rate of riolu is less than of that of the state of the art baseline flashprofile and about of chatgpt s indicating that auto riolu can describe the data more precisely and avoid over generalization which is essential for data anomaly detection.
b. rq2 how precisely can riolu detect pattern anomalies?
to answer this question we evaluate riolu s capacity for detecting data pattern anomalies using three sets of data opensourced tabular data java project method names and commercial data.
we first compare auto riolu and guided riolu with the baselines on five open sourced tabular datasets.
we also explore the method naming patterns in popular java projects and examine the inconsistencies using auto riolu.
on the commercial dataset we predict patterns using autoriolu in different domains and ask the data analysis team in our industry partner companyx to validate the patterns.
evaluation on open source datasets.
dataset to estimate the anomaly detection performance of riolu we collected five publicly available datasets in different domains and with various sizes.
these datasets contain multiple types of errors including pattern violations and other types of data quality issues.
we obtained the original data and their cleaned versions from previous dataquality studies and selected the columns containing pattern anomalies.
table ii shows the data size error rate and description of each dataset.
the data columns domains include date time postal code phone number etc.
table ii the information about the five public datasets .
the size of each dataset is written as rows columns .
dataset size error rate description hosp 1k .
hospital data from us department of health human services.hosp 10k .
hosp 100k .
flights .
flight data from airline related websites.
movies .
imdb film data.
methods compared we compare auto riolu and guided riolu with chatgpt and two versions of flashprofile the state of the art open sourced approach.
chatgpt .
constrained by the token limit of chatgpt we randomly sample records for each column and present them to gpt .
turbo api for regex inference.
for certain columns with a larger number of tokens we fed records or records to meet the token limit.
the inferred regexes are then used for anomaly detection on the entire dataset.
the sampled dataset and zero shot prompt template can be found in our replication package.
flashprofile .
as mentioned in iv a flashprofile clusters data with different patterns and infers regular expressions for each cluster.
pattern anomalies can be found using a frequency threshold patterns that are rare tend to be anomalies.
we follow the suggestion provided in the instructions and set the threshold to .
.
d flashprofile dynamic flashprofile .
d flashprofile is a combination of flashprofile with our automated pattern selection step see sec iii e .
based on our pattern selection step we avoid a fixed threshold but dynamically select the patterns based on their frequencies.
the cluster with a higher frequency is accepted as the healthy pattern pool.
auto riolu .
auto riolu learns and selects patterns using automated coverage rcov estimation on unlabeled data sec iii f .
guided riolu .
guided riolu estimates rcovaccording to the error rate of a labelled subset of the data sec iii f .
we also considered xsystem for evaluation.
however given that the system heavily relies on determining the number of branches i.e.
the number of different patterns which requires domain expertise we failed to obtain reliable patterns.
thus we excluded the tool from the comparison.
evaluation and metrics as anomalies have the nature of being rare in the wild the healthy and anomaly records are often imbalanced an approach that constructs a broad pattern and predicts only the majority class e.g.
predicting every record as normal using .
can achieve high accuracy without actually detecting any anomalies which can give a biased impression of the performance.
thus we do not evaluate the accuracy.
anomaly detection aims to identify all the anomalies while not misidentifying any healthy data.
hence we leverage precision recall and f1 score for the evaluation.
precision indicates the percentage of predictedtable iii the experiment results for quantitive analysis on five datasets .
p precision r recall f1 f1 s core hosp 1k hosp 10k hosp 100k flights movies p r f1 p r f1 p r f1 p r f1 p r f1 chatgpt .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
flashprofile .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
dynamic flashprofile .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
auto riolu .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
guided riolu .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table iv the inference time statistics on the datasets by second of four compared tools .
time s chatgpt flashprofile auto riolu guided riolu average .
.
.
.
anomalies that are actually anomalies recall indicates the percentage of anomalies that are correctly predicted the f1 score is the harmonic average of precision and recall.
quantitative evaluation result we ran the experiment on a mac desktop with an core cpu m2 chip and 16g memory.
the time used for pattern inference is reported in table iv.
the response time of chatgpt was captured as the inference time.
we did not include the execution time of using the generated patterns with regex matching to detect anomalies since all tools use the same piece of regex matching code for the detection and the time consumption is neglectable.
we did not include d flashprofile s inference time because this approach combines flashprofile and our pattern selection component the time consumption is larger than flashprofile.
since the ground truths have been obtained from previous studies the labeling time is excluded for guided riolu.
both versions of riolu require less inference time on average auto riolu requires .
less time than chatgpt and guided riolu requires .
less .
the performance results generated by the five tools compared are shown in table iii.
to suppress the bias caused by random sampling the results for both versions of riolu are the average on five detection runs.
according to the table both versions of riolu outperform all the baselines on four out of five datasets.
chatgpt obtained high precisions and showed great potential in anomaly detection on the hosp 100k dataset chatgpt detected more true positives in the state code domain and obtained a higher f1 score than auto riolu.
we noticed that guided riolu can boost performance on four datasets using a set of labeled data samples achieving the best performance on all datasets.
on four datasets guidedriolu obtained up to .
improvement of f1 over autoriolu.
the sample size to be labeled is less than .
of the whole dataset.
with the labeled small subset riolu can estimate the coverage rate more accurately.
it is also shown in table iii that combining our pattern selection step with flashprofile i.e.
dynamic flashprofile enhanced its f1 score on three out of the five datasets the automated selection of patterns can increase the quality of pattern pools on the three hospital datasets proving the efficiency of the selection step.
however we noticed a performance dropin the flights dataset.
the ground truth of the flights dataset contains scattered patterns i.e.
records with different patterns are accepted as ground truths .
since all the patterns do not have high coverage i.e.
with coverage around they cannot be well clustered based on their frequency.
table v patterns generated by chatgpt f lash profile dynamic flash profile d f lash profile a uto riolu and guided riolu for four example data domains .
state code zip phone duration chatgpt ?
!
d d ?
d ?
d d s hr .
s d s min flashprofile min d flashprofile min auto riolu d d d d min guided riolu d d d d min qualitative evaluation result we extracted the patterns generated by chatgpt flashprofile d flashprofile auto riolu and guided riolu in four domain fields to analyze the results qualitatively.
the first three domains are state code zip and phone number from the hosp 100k dataset and the duration domain from the movies dataset.
we examine the quality of the patterns using domain knowledge gained through observing the records.
according to table v guided riolu can generate and select correct patterns corresponding to prior knowledge for all four domains.
given that the coverage rate is automatically estimated auto riolu provides a false positive pattern i.e.
strings containing more than two upper letters are accepted as state code .
chatgpt has external domain knowledge from pre training.
hence it inferred and selected the patterns based on both the provided column and its training data.
for state code it included a false positive regex matching any single uppercase letter that is not immediately followed by another uppercase letter.
for zip code chatgpt accepted another format i.e.
d ?
d apart from five digits.
the duration in the movies dataset should use minutes as units instead of hours.
however chatgpt accepted the records with hours as a unit based on its external knowledge of time calculation leading to more false negatives in anomaly detection.
it can be observed that flashprofile accepts several suboptimal patterns with the default frequency threshold e.g.
phone numbers shall only contain digits however it provides us with three patterns including either letters or symbols .
with our pattern selection module attached the patterns learned by dynamic flashprofile are more precise.
although the length constraints cannot be determined accurately due to the pre clustering technique of flashprofile dynamic flashprofile eliminated patterns with significantly wrong types for example the three phone number patterns containing letters or symbols are dropped after clustering.
the qualitative result suggests that although auto riolu can learn constraints on character types and length constraints the learning quality is still to be improved with a more accurate estimation of the coverage rate.
the estimation can be more accurate with humans involved to label a small subset e.g.
with around samples .
we also found that riolu s accuracy can be limited when patterns are scattered i.e.
when a data column has many low frequency healthy patterns .
in such cases riolu may fail to distinguish anomalies from low frequency healthy patterns also see sec vi .
detection of naming inconsistency in java projects.
identifier e.g.
variable or method naming has been widely recognized as an essential software engineering task .
indeed inappropriate naming can cause problems in developer comprehension and even code quality degradation .
oracle suggests that java method names should start with an upper letter and with the first letter of each internal word capitalized.
we want to evaluate the ability of auto riolu to infer project wise naming conventions and detect anomalies i.e.
inconsistencies automatically thus we do not specify any naming convention with auto riolu.
to this end we collected method names in the top java projects from the codeattention training set .
we ran auto riolu with default settings obtained the inferred patterns and detected anomalies.
for all projects our inferred healthy patterns match the oracle java method naming convention.
in other words all the detected normal names match the standard naming convention i.e.
no false negatives .
thus we only manually analyze the detected anomalies to understand the categories of anomalies and potential false positives.
given that the liferayportal project contains more than 20k detected anomalies we randomly sampled anomaly examples i.e.
confidence level with error rate for labeling.
table vi shows the detection and labeling results.
the labels can be found in our replication package.
as shown in table vi out of the projects have a consistent naming rate higher than .
there are two categories of naming anomalies true positives method names containing symbols such as and and method names starting with an uppercase letter or a digit.
for the project with the lowest consistent naming rate i.e.
liferayportal .
of the detected anomalies contain symbols.
the detailed statistics can be found in our replication package.
for out of the projects auto riolu s false positive rates are relatively low ranging from to .
.
for one project liferay portal the false positive rate in the detected anomalies is which may be caused by its low anomaly rate there are only detected anomalies.
among these projects the false positives are mainly caused by short names e.g.
go f auto riolu tends to treat such short names as anomalies since typical java method names tend to have more than three characters.
.
.
.
.
.
.
.
initial r cov0.
.
.
.
.
.0f1 score .
.
.
.
flights hosp 1k hosp 10k hosp 100k movies average f1 number of subsets0.
.
.
.
.
.0f1 score .
.
.
.
.
.
.
.
.
.
flights hosp 1k hosp 10k hosp 100k movies average f1fig.
.
the impact of choosing different rcovinit values .
.
.
and .
and nsubset range from to for auto riolu.
evaluation on companyx database.
the data pattern inconsistency would block the automatic software pipeline and lead to software quality problems.
our industrial partner companyx uses a large data warehouse to manage data from various sources and domains the data are produced by some software sub systems e.g.
web applications and consumed by other sub systems e.g.
data analytics pipelines .
companyx faces the problem of data patterns not being aligned.
the data pattern inconsistency would block the automatic software pipeline and lead to software quality problems.
however given the large volume of data it is a great challenge to design all the regular expressions for each data domain manually.
we deployed riolu on the data warehouse management platform databricks and evaluated its ability in generating patterns and detecting potential anomalies.
we performed our evaluation using three representative large tables i.e.
tables containing over columns and 500k records .
before executing the tool we cooperated with the data analysis team to determine columns in target tables that require pattern validation.
according to our discussion the columns to be checked cover various domains including date id emails province code etc.
the error rate of each column remains unknown before our detection.
we then used auto riolu with the default setting to automatically generate the data patterns and detect potential anomalies for these columns.
the team manually verifies the validity of each generated pattern by inspecting records in the corresponding columns.
autoriolu can generate robust patterns for most columns and detect inconsistent data records i.e.
anomalies .
for instance based on auto riolu s results we noticed that .
of liability codes are unknown which may need the production team s attention account ids should contain a fixed number of digits but .
of the records have abnormal patterns and require further investigation.
we observed that only email pattern generation had been complex according to the heterogeneity of email addresses e.g.
john.doe school.edu and johndoe school.com can both be valid addresses but they belong to different templates due to the symbol .
existing in the first email address .
future work is needed to improve riolu for data with such such heterogeneous patterns.
c. ablation study sensitivity analysis several design choices and parameters may affect the performance of riolu.
thus we carry out an ablation study and a sensitivity analysis to discuss their impacts.table vi the method names consistent rate i.e.
the portion of naming matched with the inferred pattern the number of anomalies detected and the false positive rate .
project name presto wildfly libgdx intellij community gradle liferay portal spring framework cassandra hibernate orm hadoop common elasticsearch consistent rate .
.
.
.
.
.
.
.
.
.
.
detected anomalies fp rate .
.
.
.
.
.
.
.
.
.
ablation study.
as described in sec iii riolu takes five steps to automatically infer patterns and detect pattern anomalies column sampling coverage rate rcov estimation constrained template generation pattern generation and pattern selection.
to understand the impacts of these steps and the design decisions therewithin we carried out an ablation study on auto riolu for pattern anomaly detection using the public datasets in rq2.
the ablation study considers five variants of auto riolu in the column sampling step sampling of a column s records similar to flashprofile instead of using a statistically representative sample removing the rcovestimation step and using a static default rcovof .
in the constrained template generation step removing the constraint generating templates that give exact match for every record i.e.
rem using a static pattern selection threshold .
suggested by the online document of flashprofile instead of k means clustering and selecting all generated patterns i.e.
no pattern selection .
table vii presents the results of our ablation study.
overall the original auto riolu has the highest and most stable performance.
we observed that removing or modifying the pattern selection step the no p.s.
and static p.s.
threshold rows in table vii leads to the most significant performance drop especially when the error rate is too high e.g.
the flights dataset with f1 from .
to .
an .
drop without proper pattern selection anti patterns may be selected thus rcovcannot be accurately estimated.
besides subsampling of the records might lead to unstable results as a fixedrate sampling may lead to under or over generalization of the original population.
in addition fixing rcovto an initial default value such as .
i.e.
without automated rcovestimation will allow riolu to over optimistically assume that a fixed percentage e.g.
of the records are healthy leading to sub optimal results constantly setting rem to can overgenerate trivial patterns and reduce their generalizability.
sensitivity analysis.
as introduced in sec iii f the rcov estimation relies on the rcovinitand the number of subsets sampled from the column nsubset .
to validate the impact of our default choice i.e.
rcovinit .
nsubset we carried out a sensitivity analysis.
we vary rcovfrom .
to .
in increments of .
except the last increment to .
and nsubset from to in increments of .
we observe that the performance of auto riolu is relatively stable across different parameter settings in fig.
.
the average f1 score across the datasets only changes .
when we vary the rcovinit from .
to .
further increasing thercovinit i.e.
an initial too high assumption of the health rate would notably decrease the performance.
the optimal choice of rcovinitvaries across datasets given that their errortable vii the ablation study of auto riolu.
t he bold values indicate the highest and the italic values show the lowest f1scores for each dataset respectively .
p attern selection is abbreviated as p.s.
.
hosp 1k hosp 10k hosp 100k flights movies original .
.
.
.
.
column sampling .
.
.
.
.
static rcov .
.
.
.
.
.
static rem .
.
.
.
.
static p.s.
threshold .
.
.
.
.
no p.s.
.
.
.
.
.
rates are drastically different.
nevertheless we recommend the use of the default parameter setting rcovinit .
as it provides generalizable and near optimal performance for most of the datasets including those not used when building our tool.
while changing the number of subset samples nsubset auto riolu also shows a relatively stable performance.
we observed a peak of average f1 score when setting nsubset to a smaller number may cause the samples being less representative whereas a larger number would decrease the efficiency and may cause too many overlaps among the samples.
hence we selected as the default nsubset value.
summary.
the fully automated version of riolu autoriolu outperforms the baselines on four out of five public datasets and achieves better efficiency.
guided riolu further improves the performance with up to .
improvement of f1 over auto riolu with the guidance of a small set of labeled data.
our extended evaluation on java naming inconsistency detection and in an industry setting further demonstrates riolu s generalizability in detecting data pattern anomalies hence contributing to the overall quality of software that relies on data.
v. r elated works data quality has incited tremendous interest in the software engineering e.g.
and data engineering e.g.
communities.
in particular pattern violation is a major type of data quality issue as it directly impacts the quality of downstream software .
below we discuss prior work related to pattern based data profiling pattern anomaly detection and work on language mining which shares some similarity with pattern inference.
pattern based data profiling.
the goal of pattern based data profiling is to describe data with a set of patterns to help understand the data .
such comprehension of data is important for improving and maintaining data for software development and operations .
pattern based data profiling tools use syntactic structures to describe data.
microsoft ssql server data tools pioneered learning descriptive regular expressions but they lack extensibility and comprehensiveness.
ataccama one is a commercial offering that profiles the data using a set of base patterns.
potter s wheel describes the data domain using the most frequent pattern.
flashprofile clusters records according to syntactic similarity and assigns a pattern to each cluster.
however we observe that these approaches e.g.
flashprofile are not sensitive to subtle data noises they could lead to false positives caused by over generalization.
hence we propose a novel approach to generate precise and general patterns even when noises exist in the data riolu can automatically distinguish healthy patterns from noises .
pattern anomaly detection.
pattern based anomaly detection leverages manually designed or automatically inferred patterns to detect data anomalies .
tfdv and deequ require manual design of regular expressions as patterns.
these approaches are not effective when users lack knowledge of their data and would be time consuming for a large volume of data.
data scanner 4c considers a human in the loop method and lets users select healthy examples for pattern inference and inconsistency identification.
however example selection can still be labor consuming since users must inspect the dataset for multiple loops.
to save manual effort some studies focus on automated pattern inference.
potter s wheel leverages minimum description length to extract patterns for each column xsystem infers pattern for every column with a branch merge technique and flashprofile first clusters records by domain using syntactic similarity then assign patterns to each cluster.
auto detect and autovalidate both consider pattern inference with multiple columns by leveraging knowledge from similar columns aiming to infer precise and generalizable patterns and suppress the false positive rate for anomaly detection.
however we noticed that these previous approaches require domain specific thresholds to identify pattern anomalies posing difficulty for users to use them for different data domains.
therefore in this work we propose a fully automated unsupervised and autoparameterized approach for pattern bassed anomaly detection.
language mining.
the task of pattern inference is related to the field of language mining and general grammar induction.
for example the l algorithm is an impacting method for inferring language using labeled samples.
recent works such as glade mimid and arvada further enhanced the generalizability and speed of context free grammar inference with positive examples there is also specific research on grammar inference for ad hoc parsers .
however these works do not explicitly support the detection of anomalies and do not offer an unsupervised variant.
users have to determine whether an input is a positive sample before feeding it into the algorithm.
moreover although context free grammars are expressive the rules are more difficult for non expert users to understand validate and modify.
conversely regexes are easier to write and expressive to validate tabular data with patterns e.g.
datetime url thus are used by it companies such as microsoft and amazon .vi.
t hreats to validity construction validity.
given the potential noise in the data and randomness in our approach e.g.
from column sampling the pattern inference results and the performance of riolu may fluctuate.
thus we ran the tests on riolu five times to suppress the impact and collect the average results.
programming language and network latency may impact the efficiency comparison among the tools riolu was coded in python while flashprofile was written in .net which is a more time efficient language chatgpt inferred the patterns with powerful server hardware and ran remotely while other tools were run locally.
the time efficiency of riolu can be further improved with more efficient language or powerful hardware.
internal validity.
we assume that the health data patterns should cover more records than anomaly patterns.
thus we use k means to find a natural threshold for the patterns coverage rate.
k means is used given its low time complexity and wide use for data clustering.
we acknowledge that more advanced clustering approaches e.g.
deep learning based ones exist their combination with riolu can be verified in the future.
we recognize that the generated patterns quality may be limited by the ignorance of data types e.g.
generating non existing patterns for free text or some records with rare legitimate patterns may be flagged due to their statistical minority we suggest that these problems could be feasibly fixed through human validation on the corresponding domain.
external validity.
we tested riolu s performance on data in various domains and sizes e.g.
the anomaly detection datasets have to 100k records .
however our findings may be limited to the datasets that we considered.
our work would benefit from future work that evaluates riolu on more diverse datasets especially in practical settings.
vii.
c onclusions this work proposes a fully automated pattern inference and anomaly detection approach riolu which does not require human labeling or parameter configuration.
riolu leverages statistical heuristics and automated clustering to mitigate the problem of parameter configuration and uses a rule based progressive structure to ensure the precision of pattern inference.
our experiment results show that riolu can precisely generate patterns to describe data from various domains and detect pattern anomalies in a fully automated manner outperforming the baselines including chatgpt in terms of both accuracy and efficiency.
moreover we found that guided riolu a variant of riolu with the guidance of a small labeled dataset can further improve the anomaly detection performance.
our evaluation in an industrial setting proves the usefulness of our approach in a practical setting.
riolu has the potential to benefit software and data engineering communities by improving the quality of the ever growing data and the software built upon them.
for example our lightweight approach may be incorporated into the continuous integration pipeline of data centric systems to support unit tests for data.