llm agents driven automated simulation testing and analysis of small uncrewed aerial systems venkata sai aswath duvvuru and bohan zhang department of computer science saint louis university saint louis usa venkatasaiaswath.duvvuru bohan.zhang.
slu.edumichael vierhauser department of computer science university of innsbruck innsbruck austria michael.vierhauser uibk.ac.atankit agrawal department of computer science saint louis university saint louis usa ankit.agrawal.
slu.edu abstract thorough simulation testing is crucial for validating the correct behavior of small uncrewed aerial systems suas across multiple scenarios including adverse weather conditions such as wind and fog diverse settings hilly terrain or urban areas and varying mission profiles surveillance tracking .
while various suas simulation tools exist to support developers the entire process of creating executing and analyzing simulation tests remains a largely manual and cumbersome task.
developers must identify test scenarios set up the simulation environment integrate the system under test sut with simulation tools formulate mission plans and collect and analyze results.
these labor intensive tasks limit the ability of developers to conduct exhaustive testing across a wide range of scenarios.
to alleviate this problem in this paper we propose a utosimtest a large language model llm driven framework where multiple llm agents collaborate to support the suas simulation testing process.
this includes creating test scenarios that subject the sut to unique environmental contexts preparing the simulation environment as per the test scenario generating diverse suas missions for the sut to execute and analyzing simulation results and providing an interactive analytics interface.
further the design of the framework is flexible for creating and testing scenarios for a variety of suas use cases simulation tools and sut input requirements.
we evaluated our approach by a conducting simulation testing of px4 and ardupilot flightcontroller based suts b analyzing the performance of each agent and c gathering feedback from suas developers.
our findings indicate that a utosimtest significantly improves the efficiency and scope of the suas testing process allowing for more comprehensive and varied scenario evaluations while reducing the manual effort.
index terms simulation testing ai for se suas i. i ntroduction simulation testing is a critical step in the small uncrewed aerial systems suas development process to validate the behavior of suas across a variety of real world scenarios .
this includes for example adverse weather conditions e.g.
wind rain or fog diverse terrains e.g.
hilly flat urban and open fields and varying mission profiles e.g.
longrange short range surveillance and tracking .
despite the availability of various suas simulation tools and software applications simulation testing remains predominantly a manual or at best partially automated process.
as a result current suas simulation testing practices result in three main pain points for suas application developers.first identifying and designing simulation test scenarios that clearly specify a operational contexts including weather conditions environmental settings terrain variability b suas sensor specifications including how noisy each sensor is expected in the scenario c mission objectives including higher order tasks suas mission modes and d core safety properties to test is a challenging and time consuming task.
due to the fact that suas operate in complex environments and are expected to perform reliably in unique or even unimaginable real world scenarios developers domain knowledge of suas use cases plays an important role.
an example where a software issue caused an incident is a recent case of a drone crashing in the woods while autonomously tracking a person on an electric scooter.
the individual was moving at a moderate speed when suddenly a dog crossed their path.
the drone mistakenly switched its tracking from the person to the dog and crashed into a tree.
video of this drone crash is available on reddit1.
such unique scenarios are particularly difficult for suas developers to imagine and accurately test during the development phase.
second suas developers must model each identified scenario to align with the system under test sut requirements and configure the simulation tool to simulate environmental context.
this process is often manual repetitive timeconsuming and prone to human error.
third once simulation tests are executed analyzing flight logs to diagnose abnormal behavior is labor intensive requiring a deep understanding of thousands of flight controller parameters .
while tools like px4 flight review plot sensor data automatically developers still need to interpret the plots and identify issues manually which requires specific skills and in depth knowledge of the flight controller.
furthermore there is no automation framework that enables suas developers to analyze simulation artifacts automatically.
these challenges highlight the need for automated tools and supporting frameworks capable of systematically specifying generating executing and analyzing diverse simulation tests to achieve scalability and sufficient simulation test coverage.
therefore in this paper we present a novel framework called solid crash while tracking myself onthe arxiv .11864v1 jan 2025env agent m agent s agent analyticsagent sutsuas simulator sut setup configtest propertiesrule based validator rule based validatorautomated feedback automated feedbackphase scenario blueprint construction phase scenario specification and execution phase scenario analysis interactive simulation analysistest promptsimulator setup config analysis reportenvironmental description mission descriptionsuas developerscenario blueprint manual feedback manual validationfig.
overview of our a utosimtest framework with the main phases scenario blueprint construction manual validation and feedback blue scenario specification validation and execution green and scenario analysis yellow .
autosimtestas a first step towards automating the simulation testing process for suas.
we leverage our experience in simulation and field testing within this domain to construct and implement this framework employing recent advancements in generative ai more specifically large language models llms to a generate unique test scenarios that incorporate environmental context mission objectives and core safety properties to test b model suas mission in sut expected language c configure the simulation environment based on the test scenario s environmental context and d produce a simulation report based on the collected data from scenario execution as well as provide interactive methods for analyzing simulation results for suas developers.
the contributions of this paper are as follows autosimtest this is the first work to propose a novel multi llm agents based framework to automate the suas simulation testing process.
automated flight analysis as part of the framework we introduce a novel analytics agent enabling automated and interactive analysis of suas flight logs.
performance and developers perception extensive experiments have demonstrated that a utosimtestis capable of testing diverse sut effectively analyzing flight logs and providing valuable support to both novice and experienced developers in better interpreting the simulation data.
the remainder of the paper is laid out as follows.
in section ii provide a brief introduction to relevant llm techniques and then in section iii introduce our a utosimtest framework.
subsequently in section iv and section v we discuss the different llm agents part of our framework.
we then in section vi describe our evaluation setup for addressing feasibility generalizability and agent performance and report on results in section vii.
finally we discuss several lessons learned in section viii threats to validity in section ix related work in section x and conclude in section xi.
ii.
b ackground this section briefly introduces the llm techniques used in the design of our a utosimtestframework cf.
section iii .a.
retrieval augmented generation rag rag is a method that combines the strengths of pretrained language models with external knowledge retrieval.
instead of solely relying on the internal knowledge of the language model rag retrieves relevant information from external data sources e.g.
a vector database serving as a knowledge library that the generative ai models can understand.
this approach allows the agents to utilize contextually relevant information especially in specialized domains such as suas to generate text.
rag consists of four primary steps query generation document retrieval context integration and response generation.
first the input query is processed to generate search queries which are then used to retrieve relevant documents from an external knowledge base.
these documents are integrated back into the language model as additional context.
finally the model produces a final response that combines its internal knowledge with the retrieved information to generate a more precise and contextually relevant output.
b. prompt engineering prompt engineering is a critical technique in the field of natural language processing nlp that involves designing and crafting input prompts to elicit desired outputs from language models .
the fundamental principle of prompt engineering is to provide clear and context rich instructions that align with the underlying model s training data.
key strategies to write quality prompts include a providing examples to illustrate the request b specifying the desired output format to ensure clarity and c incorporating relevant contextual information to help the llm understand the prompt and produce the intended output.
prior research in the domain of nlp and generative ai has shown that the quality of prompt to llm model greatly influences the quality of generated content .
iii.
a utosimtest framework overview our a utosimtestframework supports three main phases of performing suas tests.
phase the scenario blueprint construction phase the scenario specification and execution and finally phase the subsequent scenario analysisand interpretation of results.
these phases aim to enhance the suas simulation testing process by a reducing manual effort in designing and executing scenarios b providing automated analytics support for better understanding of simulation results while at the same time minimizing reliance on domain specific knowledge and finally c achieving faster and more frequent simulation testing cycles during development.
figure provides an overview of the three phases of the framework.
each phase consists of specialized llm based ai agents to automate parts of the testing process.
all agents share the generated data to provide the necessary automation support.
in the following we provide a brief overview of the main functionality of each phase and discuss the role of each ai agent before we provide further design and implementation details in section iv and section v. a. phase scenario blueprint construction as a starting point the suas developer provides the highlevel objectives of the simulation tests that should be created in natural language as textual input.
depending on the focus of the simulations to be performed this input can range from very specific goals such as testing the computer vision model of the system under foggy weather conditions to more general objectives such as evaluating the navigation capabilities of the system .
this crucial input then serves as a part of the prompt for the first agent the scenario gen ai agent sagent which is tasked with generating a scenario blueprint based on its knowledge of past suas incidents in the real world.
the s agent utilizes a retrieval augmented generation approach to consider real world suas incidents to generate relevant scenario blueprints cf.
section iv .
the output of the s agent includes the textual specifications of the environment the mission that the sut must execute and importantly test properties to evaluate the sut.
for instance a developer might provide input to the s agent as test the ability of a drone to track a missing person during a searchand rescue mission .
in response the s agent generates commen al environmental description a densely forested area with tall trees uneven terrain and potential obstacles like fallen logs branches and wildlife activity alongside dynamic conditions such as varying light levels fog and light rain.
commen al suas mission description the sut should be tasked with locating a missing hiker in a forest environment navigating through waypoints including search patterns obstacle avoidance maneuvers and target identification.
commen al test properties specific metrics such as target detection and identification accuracy obstacle avoidance efficiency flight stability in varying weather sensor performance under different lighting and overall mission completion time.
before being fed into the second phase of the pipeline a stage gate is added where the scenario blueprints undergo manual inspection and validation by a human for example developers testers or safety engineers.
this ensures that the generated scenarios are accurate and do align with testing needs and respective safety related requirements e.g.
simulatorsettings weather rainintensity .
windspeed winddirection visibility .
vehicles drone 1 vehicletype quadrotor pose x y z roll pitch yaw homelocation latitude .
longitude .
altitude fig.
env agent output mission cruisespeed hoverspeed items amslaltaboveterrain null altitude altitudemode autocontinue true command frame params null .
.
type simpleitem ... ... ... plannedhomeposition .
.
fig.
m agent output operational boundaries of suas related to environmental conditions .
developers can provide feedback to s agent to refine and adjust specific components of the blueprint such as increase the mission complexity or add lighting variability to the environment .
this feedback process ensures early identification and correction of scenarios that do not meet developers objectives.
b. phase scenario executable script generation this phase focuses on automatically generating the scenario execution scripts as per the suas developer s testing infrastructure.
this phase consists of two specialized agents each taking over and creating one crucial part required for scenario execution the sut mission execution script and simulation tool configuration script env agent simulation tools such as airsim gazebo and dronereqvalidator require configuration scripts typically in json or xml format to initialize the simulation environment.
these scripts define various parameters necessary for the simulation such as weather conditions the origin of the simulation environment drone characteristics e.g.
quad copter or fixed wing and their home or starting geolocations.
therefore the env agent takes the high level textual environment description from phase provided by the s agent as input and utilizes it as a primary prompt to generate a configuration script.
this generated output is used to initialize the 3d environment of the simulation tool according to the scenario execution requirements.
for instance if a scenario needs to be executed in foggy weather or an urban environment the env agent will generate the necessary tool configuration to simulate these conditions.
however the fidelity of the weather conditions and the realism of the environment e.g.
buildings depends on the fidelity of the simulation tool itself.
figure shows a sample script generated by the agent to initialize the airsim simulation tool with weather conditions and a vehicle.
m agent missions define the tasks and actions that suas must perform serving as inputs to the sut.
these missions specify tasks like searching tracking or flying in patterns tocapture imagery.
typically suas developers use open source tools such as qgroundcontrol or missionplanner for creating these missions.
these tools allow users to specify waypoints set altitudes define actions at each waypoint e.g.
taking photos or adjusting speed and incorporate safety protocols such as return to home triggers or no fly zones.
however the manual process is time consuming and prone to human error due to the vast number of settings and configurations required.
moreover the complexity of these settings increases with the increasing complexity of the sut.
therefore similar to the env agent the m agent takes the high level textual mission description from the s agent and transforms it into specific executable missions for the sut.
figure shows a sample output to execute a px4 mission.
validation and error correction given the susceptibility of current llms to issues such as hallucinations it is important to validate the generated scripts.
for this purpose we introduce an automated validation step before scripts are executed using a set of pre defined rules.
developers can create these rules based on their suas mission and simulation tool requirements.
for instance rules can be designed to detect basic syntactical errors in the generated scripts such as incorrect formatting missing fields or invalid out of range parameter values in mission.
when issues are detected the validator provides immediate feedback to the respective agent prompting it to regenerate a corrected script with specific error details.
this feedback loop ensures that any errors introduced by the llms are resolved within the framework and do not propagate to subsequent stages.
finally after the mission is validated for execution and the simulation environment is initialized with environmental context as per the scenario either a developer can manually trigger the simulation execution in sut or it can also be automated using automation scripts.
c. phase scenario analysis generation the execution of a simulation scenario typically generates simulation results in the form of sut logs which developers record in their code base for debugging purposes.
these sut logs contain high level explanatory messages such asmission dispatched successfully and moving to the next waypoint as well as low level log messages that include event names and their timestamps.
these logs are crucial for developers to understand and debug any issues related to sut execution as per the system requirements.
in addition to sut logs the underlying flight controller used by developers such as px4 or ardupilot generate their own logs called flight logs.
these flight logs contain time series sensor data and warnings or messages generated by flight controllers.
this data is crucial for conducting lower level analysis and understanding the behavior of the vehicle.
developers analyze these logs to interpret the simulation results.
analyzing time series data from hundreds of flight controller parameters however requires extensive domain expertise and knowledge.
therefore the primary objectives of analytics agent are twofold automated scenario analysis generate scenario analysis reports from data produced as part of the scenario execution.
bridge developer s knowledge gap in analysis provide developers with interactive methods to explore and understand complex simulation logs.
this interactive analysis aims to bridge the knowledge gap by allowing developers to ask high level analytics questions and receive automated analysis from analytics agent in the form of a small set of relevant parameters to investigate out of hundreds of parameters.
iv.
a gents knowledge base and prompts in this section we discuss two main components of our framework the knowledge base and custom prompt design.
a. knowledge base s agent knowledge base to automatically generate scenarios that aim to detect specific vulnerabilities in the sut especially the ones that are common in the real world we created a robust database drawing from publicly available information on suas real world incidents.
the sources we used to build the s agent s knowledge base are listed in table i. we used web scrapers to collect the data from sources and semi autonomously cleaned them to create the knowledge base.
this knowledge base provides the agent with realistic scenarios and contexts that have previously caused issues in suas operations.
this data driven approach enables the sagent to generate simulation testing scenario blueprints that are informed by actual events and historical data.
this datadriven approach is also extensible to other domains e.g.
autonomous vehicles further discussed in section viii .
no.
incident source incident token uk air accident investigation rep. wikipedia list of ua v incidents nasa asrs report table i scenario incident sources to build s agent knowledge base.
analytics agent knowledge base with extensive simulation data and hundreds of parameters in each log the analyticsagent must select the relevant parameters and log sections to generate relevant responses.
this requires the agent to have domain knowledge to interpret the log data and understand its content and meaning.
to build this second knowledge base we developed a dataset that encodes the meaning and interpretation of hundreds of parameters from the two most popular open source flight controllers px4 and ardupilot.
the px4 flight controller codebase2provides a comprehensive description of each logged parameter with detailed comments explaining the meaning and interpretation of each parameter.
we used a python script to parse the data structure of each message which contains the parameter name and a brief description as comments.
this domain specific information enables the analytics agent to leverage rag techniques to identify and analyze contextually relevant parameters within the flight logs based on user queries and scenario test properties.
s agent m agent env agent a agent automated a agent interactive agent goalsact as a test engineer to generate scenario blueprints that include environment mission test propsact as an automation engineer to generate a sut mission script based on scenario blueprintact as an automation engineer to generate sim tool script based on scenario blueprintact as a data analyst to analyze simulation log data and explain how the test properties are affectedn a user goalsexample generate a search and rescue mission scenariosn a n a n aexample explain all px4 parameters that might have an impact of high wind samplecf.
example blueprint in section iii acf.
figure cf.
figure 2n a n a rules blueprint completeness script format validity valid geo loc.
valid waypoints velocity mph altitude 400ft script format validity wind mph light intensity analysis completeness analysis completeness ext.
datalist of past real world suas incidentsn a n aflight controller codebase and documentation table ii overview of the prompt design for four agents part of a utosimtest.
b. prompt design we use a pattern based approach to design prompts for each agent.
our prompt design for agents in the framework consists of four essential patterns agent s goals we utilize the persona pattern to instruct the agents about their primary goals when generating output.
for instance we use the act as a simulation testing engineer persona to design suas scenarios or act as a data analyst to analyze the simulation logs.
the persona pattern helps the agent to identify the details it should focus on when generating the output.
sample of expected output we utilize the template pattern to instruct the agent about the format of the expected response.
the s agent m agent and env agent include this component especially because the output is directly fed into another sub system and must match the sub system s input format for automated execution.
user goals we utilize the context manager pattern to instruct the llm about the user goals to fulfill when generating the output.
the prompt includes instructions from the user s perspective to tailor the output such as generating a scenario blueprint for a complex search andrescue mission or a simpler package delivery mission.
the s agent andanalytics agent include this component in their prompt since the user initiates the process using s agent and consumes interacts with the output of the framework using the analytics agent .
further these agents utilize rag to include the contextual information from their respective knowledge base based on user goals to augment targeted and relevant information.
rules additionally we utilize the error identification pattern to force the llm to validate the generated output against a set of predefined rules or conditions.
for instance the suas mission script should not allow suas to fly over 400ft safe altitude .
this validation process ensures that the generated output meets the required criteria and maintains consistency in the llm output.table ii summarizes the prompt design for each agent including the four components and the data utilized from their knowledge base to generate a response.
the following sections provide details about the design and architecture of each agent.
v. a gents design a. s agent first the agent uses an embedding model to vectorize the knowledge documents cf.
table i and store them in a vector database rag knowledge store to perform search queries on it.
second when an suas developer provides input to the agent to generate a scenario blueprint as per their testing needs the agent vectorizes the textual input prompt vector and performs a cosine similarity search in the rag knowledge store to identify a set of real world suas incidents relevant to the user s testing context.
for example if a user wishes to test a search and rescue operation the agent will find the details of accidents during search and rescue operations and utilize that to formulate user goals.
in addition to the user goals the agent incorporates other components agent s goals sample of expected output and rules as described in table ii of the prompt to form a comprehensive prompt and sends it as input to the llm to generate a scenario blueprint.
ection m agent andenv agent the m agent andenv agent agents utilize specific parts of the scenario blueprint as their goals to generate scripts.
the m agent uses the mission description section as its goals while the env agent uses the environment section.
since these agents act as translators converting textual descriptions into a language that the sut and simulation tool can interpret they don t require user goals .
however to ensure their generated output is compatible with the sut and simulation tool they include sample of expected output andrules to follow when generating output.
the detailed breakdown of the prompt design is shown in table ii.analyticsagent agent goalsprompt user goalsprompt scenario blueprint query vector relevant parameters to analyze data plotter vision llmrules automated mode extract test properties suas developer vectorize vectorize vectorize domain knowledge to analyze logs knowledge baseinteractive mode relevant param search store vectorsuser query provide input analysis reportgenerateread param listouptuttime series plotsfig.
schematic overview of the main parts of the analytics agent .
b. analytics agent the design of the analytics agent is divided into two main logically separated components as shown in figure .
the automated mode component is responsible for analyzing simulation logs based on the test properties generated by sagent and producing a comprehensive scenario analysis report without direct interaction with the user.
complementing this first part the second part of the analytics agent the interactive mode component allows suas developers to start new analyses that may not be part of automated mode analysis.
both components utilize a shared knowledge base to identify the essential simulation log data to analyze.
the primary distinction between the two modes lies in the formulation of prompts.
in automated mode the agent uses agent s goals and rules whereas interactive mode relies on user input designated as user goals and rules in the prompt.
based on the analysis goals whether from the user or the agent the agent performs a semantic search within the knowledge base to identify flight controller parameters to analyze such as velocity pitch angles gps position and altitude.
upon identifying the core flight parameters the agent initiates an automated script to plot these parameters with the xaxis representing the timestamp and the y axis representing the parameter values.
these plots along with the prompt are then provided as input to the vision llm model to analyze the plot images and generate an analysis report.
analytics agent saves this report for developers debugging and analysis purposes.
vi.
e valuation we used our a utosimtest framework to investigate the following research questions rq1 to what extent is our framework applicable for conducting real world suas simulation testing and how well can it be applied across diverse sut that use different flight controllers and simulation tools for testing?
to answer this research question we conduct experiments to first evaluate a the feasibility of a utosimtest to test different sut that utilize widely used flight controllers such as px4 and ardupilot and second b its ability handle diverse common suas use cases ref table iii .
rq2 to what extent can our framework with rag be utilized for generating relevant test scenarios blueprints across suas use cases and investigating suas failures?
we conducted a second set of experiments to quantitatively assess the performance and effectiveness of our rag approach for both the s agent and the analytics agent .
we used the standard retrieval augmented generation assessment ragas framework a reference free rag pipeline evaluation method that utilizes a separate llm as a critic to judge the responses generated by our agents.
we collect the following metrics.
context precision and recall these metrics measure the effectiveness of rag in retrieving relevant information from the knowledge base.
context precision evaluates the proportion of retrieved information that is relevant while context recall evaluates the proportion of relevant information.
response faithfulness this metric assesses whether an aiagent generate outputs that are grounded in the information gathered from their knowledge bases.
response relevancy this metric evaluates the completeness of the final responses generated by an agent.
to compute response relevancy and faithfulness we employ the llama llm a trillion parameter model as a critic to evaluate responses of s agent andanalytics agent .
further we also investigate the quality and correctness of the analyticsagent output using flight logs as described in section vi a. rq3 how do developers perceive the generated scenarios and does the interactive analysis support developers in better understanding the simulation results?
to address this research question we conducted interviews each approx.
lasting hour with suas developers to understand current challenges in suas simulation testing and their perception about a utosimtestand especially how the interactive analysis can support them during simulation log analysis.
we deployed our entire framework on the cloud and asked participants to especially interact with s agent and analytics agent .
participants provided feedback and opinions on the framework s usability in suas simulation testing.
next we discuss our strategy tools and techniques we utilized to implement the framework that we utilize for conducting experiments.
a. evaluation setup autosimtest framework implementation we leverage the advanced capabilities of the open source phi family of llms designed by microsoft to implement our framework.
specifically we utilize the phi medium 128k instruct llm a model with billion parameters.
in order to support analysis of image data we use the phi vision 128kinstruct model which integrates an image encoder connector projector and a phi mini language model all within .
billion parameters.
for embedding and similarity matching we use sentence transformers all mpnet base v2 model and faiss for vector database management.
we deployed our framework on gradio.app a cloud platform for mlmodels.
we use this deployment in our perception study to gather feedback from suas developers.
figure shows the interface our participants used in the study.
sut we designed two suas sut px4 and sut ardu based on px4 and ardupilot flight controllers respectively.
these flight controllers were chosen due to their widespread use comprehensive documentation and open source license.
for simulation we employed airsim compatible with px4 and ardupilot sitl compatible with ardupilot .
we developed our sut for city surveillance use case with two features way point based navigation sut follows predefined waypoints to surveil an area.
autonomous navigation sut determine its grid based flight path based on the shape of surveillance area.
suas use cases e examined the effectiveness of a utosimtest across popular suas use cases as summarized in table iii.
these specific use cases were selected due to their popularity in both academic research and industrial products.
suas use case