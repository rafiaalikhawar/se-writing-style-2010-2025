your fix is my exploit enabling comprehensive dl library api fuzzing with large language models kunpeng zhang shuai wang b jitao han xiaogang zhu xian li shaohua wang b and sheng wen the hong kong university of science and technology zkp0625 outlook.com shuaiw cse.ust.hk central university of finance and economics hanjitao1 gmail.com davidshwang ieee.org the university of adelaide xiaogang.zhu adelaide.edu.au swinburne university of technology xli1 swen swin.edu.au abstract deep learning dl libraries are widely used to form the basis of various ai applications in computer vision natural language processing and software engineering domains.
despite their popularity dl libraries are known to have vulnerabilities such as buffer overflows use after free and integer overflows that can be exploited to compromise the security or effectiveness of the underlying libraries.
while traditional fuzzing techniques have been used to find bugs in software they are not well suited for dl libraries.
in general the complexity of dl libraries and the diversity of their apis make it challenging to test them thoroughly.
to date mainstream dl libraries like tensorflow and pytorch have featured over apis and the number of apis is still growing.
fuzzing all these apis is a daunting task especially when considering the complexity of the input data and the diversity of the api usage patterns.
recent advances in large language models llms have illustrated the high potential of llms in understanding and synthesizing human like code.
despite their high potential we find that emerging llm based fuzzers are less optimal for dl library api fuzzing given their lack of in depth knowledge on api input edge cases and inefficiency in generating test inputs.
in this paper we propose dfuzz a llm driven dl library fuzzing approach.
we have two key insights with high reasoning ability llms can replace human experts to reason edge cases likely errortriggering inputs from checks in an api s code and transfer the extracted knowledge to test other new or rarely tested apis.
with high generation ability llms can synthesize initial test programs with high accuracy that automates api testing.
dfuzz provides llms with a novel white box view of dl library apis and therefore can leverage llms reasoning and generation abilities to achieve comprehensive fuzzing.
our experimental results on popular dl libraries demonstrate that dfuzz is able to cover more apis than sota llm based fuzzers on tensorflow and pytorch respectively.
moreover dfuzz successfully detected bugs with already fixed and replicated by the developer but still under investigation.
i. i ntroduction to date deep neural networks dnns and their enabled applications have been extensively utilized in various realworld scenarios such as autonomous driving software vulnerability detection and medical diagnosis .
typically those dnn applications are bcorresponding author.built on top of deep learning dl libraries such as tensorflow and pytorch which offer a comprehensive set of api functions to facilitate developing dnn models and execute them.
given that dnn applications have been actively employed in reliability sensitive scenarios it is demanding to thoroughly test dl libraries and uncover underlying bugs in those api functions.
among all popular methods fuzzing is deemed the mainstream approach with high potential given its high flexibility and capability in detecting real world defects .
while traditional fuzzing techniques have been used to find bugs in software recent works show that they are not well suited for dl libraries .
in general the complexity of dl libraries and the diversity of their apis make it challenging to test them thoroughly.
to date mainstream dl libraries like tensorflow and pytorch have featured over apis and this number continues to increase.
performing fuzzing on all of these library apis is a challenging endeavor particularly when taking into account the input data complexity emerging data types and diverse api implementation patterns.
recent dl library fuzzing works primarily rely on manually crafted mutators which require expensive human efforts and are less scalable.
large language models llms are transformer based neural networks that have achieved state of the art sota performance in a wide range of natural language and code processing tasks .
it is shown that llms can reason and generate human like code given that they have been trained on large scale corpora which often subsume common sense knowledge and programming expertise.
recent works use llms to perform dl library api fuzzing with promising results.
however we find that those emerging llm based fuzzers often suffer from a lack of indepth knowledge on api input edge cases and inefficiency in generating test inputs most test inputs are not helpful .
this is mainly due to the fact that llms were merely employed to mutate api inputs using either scheduling algorithm based schemes or based on historical data .
as a result they 1arxiv .04312v1 jan 2025essentially treat dl library api fuzzing as a black box problem where the underlying api implementations are neither well understood norutilized.
our insights.
this paper presents a novel and comprehensive dl library fuzzing approach by bridging de facto llms with a white box view of dl library apis.
we have two key insights llms manifest high reasoning ability and possess pre knowledge over a considerable number of apis.
they can reason those api input checks e.g.
torch check in pytorch widely seen in api low level code and infer edge cases likely error triggering api inputs accordingly.
moreover these edge cases are transferable to effectively stress other similar apis and likely uncover more bugs rapidly.
llms also manifest high generation ability in that with proper guidance and feedback they can synthesize programs invoking a target api with high accuracy thus automating the api testing process with high efficiency.
with the above insights we propose dfuzz a novel llmbased fuzzing framework for dl libraries.
dfuzz features a three step approach to perform effective api fuzzing with the usage of llms in different steps.
dfuzz first employs llms to summarize input checks e.g.
torch check found in the low level code of a dl library api.
dfuzz forms edge cases that can provoke those checks and further lift the edge cases to an abstract context free form that can be easily transferred to test other apis with similar input types.
next dfuzz uses llms to synthesize initial test programs to invoke a target api where we form a feedback driven process to guide the generation.
third with edge cases and initial test programs on hand dfuzz employs llms to synthesize diverse inputs to fuzz a given api.
we further design a set of optimizations to make df uzz highly efficient and practical.
we evaluate dfuzz to fuzz two mainstream dl libraries tensorflow and pytorch.
we compare dfuzz with sota llm based fuzzers titanfuzz and fuzzgpt and sota type aware mutation based fuzzer ivysyn .
we show that dfuzz achieves higher api coverage with much lower llm usage.
in fuzzing pytorch and tensorflow dfuzz uses only .
and .
of titanfuzz s llm usage in initial program generation respectively yet covers and more apis than titanfuzz respectively.
moreover we show that dfuzz finds bugs in the latest versions of pytorch and tensorflow with already fixed and replicated by the developer but still under investigation.
more than bugs exist in previous versions of tensorflow and pytorch which have been extensively tested by titanfuzz fuzzgpt and ivysyn yet none of these fuzzers discovered them.
in sum our contributions are as follows we for the first time advocate a white box view in the context of llm based dl library api fuzzing.
the high reasoning and generation abilities of llms facilitate inferring edge cases and generating test programs which are essential for comprehensive dl library api fuzzing.
we present a novel llm based fuzzing framework dfuzz that implements the above insight.
dfuzz features a three step approach to delivering fuzzing.
dfuzzfeatures a set of design principles and optimizations to make it highly efficient and practical.
our results show that dfuzz can consistently outperform sota llm based fuzzers in terms of api coverage and bug finding.
bugs have been found in the latest versions of tensorflow and pytorch with bugs already existing in tensorflow pytorch fuzzed by previous tools.
ii.
m otivation a. related work and limitations fuzzing dl libraries is a challenging and demanding task faced by the community.
based on seed differences existing research on fuzzing dl libraries can be categorized into two types model level and api level fuzzers .
model level fuzzers primarily test apis related to model construction and training treating complete dl models as inputs while simultaneously testing multiple apis .
the testing scope of api level fuzzers encompasses all apis within the target dl framework generating programs for each api and subsequently applying mutations .
dfuzz belongs to the latter category focusing on api level fuzzing given its high comprehensiveness over apis.
however with the high complexity and diversity of dl library apis we find that sota methods in this category feature a rather opaque understanding of the underlying api usage patterns and codebases even if llms may have been employed.
to elaborate existing dl framework fuzzing methods can be categorized into the following three types 1scheduling algorithm based approach such as titanfuzz .
such a method pre defines basic mutation operators and then applies these operators to mutate programs based on a scheduling algorithm.
however the optimization goal of such methods is often not bug discovery but rather metrics like the complexity of generated programs.
the mutation is undirected and lacks guidance to effectively stress apis.
importantly given that there are often thousands of apis in real world dl frameworks our tentative experiments show that such methods fail to allocate sufficient testing time to each api making them hardly comprehensive to test each api instance.
for example in titanfuzz s experiments only seconds were performed to each api likely struggling to achieve good results.
2approach based on predefined input mutators such as ivysyn and docter .
these methods require experts to analyze existing bug codes summarize features and thus construct suitable mutators.
in comparison to mutations based on predefined mutators are more directed and targeted.
however the construction of mutators is labor intensive and time consuming and the mutators are often not comprehensive enough to cover all possible edge cases.
in ivysyn researchers manually analyzed cves and summarized dozens of mutators.
however this relies on heavy expert experience and human resources for analysis thus lacking automation and scalability.
in docter the tool relies on the provided documentation to extract input constraints for api functions.
3approach based on historical bug triggering code such as fuzzgpt .
holistically history based methods can be seen 2as an enhanced version of where the mutation is guided by historical bug cases.
nevertheless we find that such approaches require collecting a large number of error codes from the internet such error codes may be likely incomplete and not representative of the entire api input space.
moreover there exists a noticeable gap between bug codes and test program generation.
bug codes often consist of a limited number of statements that are genuinely pertinent to the bug whereas other statements are irrelevant and frequently lead to erroneous program generation.
also due to the substantial differences in the syntactic forms and usages of apis error triggering features gathered from existing bug codes may be likely unusable when fuzzing other apis.
dfuzz achieves notably better results than fuzzgpt as shown in our evaluation sec.
v .
b. insights derived from a white box perspective conceptually we view that the limitations of existing methods can be addressed by shedding a white box perspective on the dl library codebase.
in general dl frameworks like tensorflow and pytorch typically comprise multiple layers of abstraction and at the lowest level native low level apis implement specific operations e.g.
tensor operations .
importantly these implementations include extensive checks on input parameters to ensure they meet the expected conditions of the api.
in case of errors corresponding error messages are generated.
we see that these checks contain a rich set of information about edge cases extreme or special situations of api inputs that are likely to trigger bugs.
to ease understanding we present a real example below.
tensor abs tensor self torch check !self.is complex in place abs is not supported for complex tensors.
return unary op impl self at abs out fig.
.
sample source code in pytorch.
real world example.
fig.
illustrates the low level source code of pytorch api torch.tensor.abs .
this api computes the absolute value of a tensor and modifies the tensor in place without creating a new one.
since torch.tensor.abs does not yet support handling tensors of complex type i.e.
an edge case line checks whether the input tensor is complex if so an error is thrown.
the check statement line can be abstracted into an edge case a tensor parameter is a complex tensor which likely crashes the api without the check.
moreover our manual analysis shows that when testing another api torch.all which has the same input parameter type as abs feeding the edge case into torch.all triggers an unknown bug.
this example implies that an edge case extracted from one api is often valuable for testing other apis.
we present the following definition and key observations that motivate our approach.
edge case.
through analyzing the source code of pytorch and tensorflow we find that the source code of dl frameworks contains a rich set of information that can be used to infer edge cases.
this offers a unique opportunity to leverage llms to extract edge cases from the source code and then transfer the extracted knowledge to test a group of apis with high similarity.table i defines edge cases considered which are three special conditions over api inputs.
using such edge cases to test apis can effectively stress them and likely uncover defects if the apis are not properly handling those edge cases.
table i edge case categories and examples .
category input example expected edge case special type x is int x is int x is string abnormal value x is int x x special type attribute x is tensor x.dtype is int x.dtype is float observation i api checks reflect edge cases.
in developing dl frameworks developers commonly incorporate check statements to ensure that the input parameters meet the expected conditions of the api or alert users if certain inputs are yet supported.
for instance in the pytorch low level code we see many check statements such as torch check andatcheck where each checks one or several edge cases.
this suggests that checks contain rich information to reflect edge cases.
this paper extracts edge cases by analyzing check statements and then infer the edge cases based on the context input types and error messages.
however in practice dl libraries like pytorch and tensorflow often have two kinds of check statements those encode the edge conditions directly and those encode expected conditions of the api inputs.
dfuzz indeed considers both types of check statements to extract edge cases to extract the former we directly ask the llm to infer the edge case sec.
iii c to extract the latter we ask the llm to infer an edge case that violates the expected conditions by slightly tweaking prompts used in sec.
iii c .
to avoid verbosity we unified the two types of checks as edge cases in this paper.
observation ii apis with same input types often sharing edge cases.
importantly the edge cases that an api needs to handle are often determined by their input types.
for example in the case of torch.all andtorch.abs where both apis take a tensor as its input they share the same edge cases to handle undefined and empty tensors.
we view this illustrates an important insight if apis have the same input parameter types they shall likely share edge cases.
thus we can transfer knowledge of edge cases learned from one api to test another api with the same input types.
moreover since edge cases are determined by api input types we see that knowledge transfer of edge cases also occurs in a cross framework setting e.g.
an edge case learned from pytorch can be used to test tensorflow apis and vice versa.
we validate this insight empirically in sec.
v b we extract edge cases from pytorch core libraries and use these edge cases to test tensorflow to detect bugs.
c. pilot study despite the promising observations pytorch tensorflow has thousands of apis where each api may contain multiple checks.
extracting the edge case from a check statement is not trivial which requires reasoning the check context input types error messages and possibly data flow constraints.
relying on manual analysis would require a significant amount of time 3and effort and certain pattern match e.g.
regular expression based approaches are also not feasible.
having that said we see the encouraging potential of llms in this context.
llms are trained on millions of lines of code available on the internet and are shown to manifest promising understanding and reasoning capabilities in relevant software engineering tasks like code completion summarization and bug fixing .
in fact we use the code from fig.
to form a prompt and ask gpt .
chatgpt to extract edge cases from line .
we find that gpt .
can output the edge case tensor self is a complex tensor in an accurate and succinct manner.
this above result is promising and suggests that llms can be used to extract edge cases from check statements.
at this step we conduct a pilot study to quantify the feasibility of llms in extracting edge cases from dl framework source code.
without loss of generality we take pytorch and gpt .
as representative examples to validate our approach.
the pytorch framework comprises front end back end and bindings.
the front end primarily the python apis empowers users to construct and debug dl models with ease.
the back end centered around the c apis implements elementary operations.
through bindings pytorch s front end interacts with the c implementation in the back end blending python s user friendly interface with c s high performance.
in the back end aten pytorch aten src aten serves as the underlying tensor library supporting tensor operations of various purposes.
most pytorch high level apis and functionality are built upon aten making it a core component of the entire framework.
we collect aten s native functions aten native for our study.
aten s native functions serve as the contemporary approach for integrating operators and functions into aten .
we randomly select functions from aten native and for each function we extract its function header and all check statements in the form of pytorch check to form a code block.
then for each block we ask gpt .
four questions how many check statements are contained in each block?
what are the variables related to each check statement?
what are the types of variables related to each check statement?
and extract the edge cases checked by each check statement.
we also manually analyze the selected code blocks to obtain the ground truth.
then we assess the accuracy of llm outputs whose results are in table ii.
table ii assessing gpt .
across four tasks to comprehend checks .
tasks accuracy success total the number of checks in a code block the involved variables of a check .
the involved variable types of a check .
the edge cases corresponding to a check .
there are a total of code blocks.
there are a total of checks among all code blocks.
gpt .
achieves an accuracy of over on all four tasks.
llm can infer the meaning of callee functions based on the context provided.
among all torch checks involve callee functions.
the errors occurred only in analyzing twocode blocks including three checks .
one error occurs because the target code block contains too much irrelevant information leading the llm to inaccurately analyze it.
the other one was misled by the unclear error message string target in the check.
we confirm that without the error message string the llm can correctly analyze the edge case grad output is a 3d tensor .
overall we interpret the pilot study as highly promising and the results suggest that llms can be used to extract edge cases from dl framework source code with high accuracy.
iii.
d esign of dfuzz analysisetype pattern tensor context free edge cases an empty tensor ...etype pattern int context free edge cases ...context free edge cases pytorchprogram generation mutation buginitial programapi infocheck related code blocksstep i extraction of context free edge cases step ii debug based initial program generationstep iii edge cases based program mutation fig.
.
the workflow of df uzz.
with the key insights presented in sec.
ii we now present the design of dfuzz a framework for comprehensive api fuzzing.
as in fig.
df uzz comprises three core steps step i edge case extraction.
following the discussion in sec.
ii we use llm to extract edge cases from the source code of dl library apis.
we identify and address several domain challenges in this step see details in sec.
iii a whose outputs will be edge cases conditions over input parameters in a context free format.
this way the extracted edge cases can be smoothly used to test other similar apis despite various differences in the input syntactic forms like variable names.
step ii initial program generation.
the next step prepares an initial test program that invokes a target api for fuzzing.
despite the fact that llms may fail to generate valid programs for certain apis we propose an iterative procedure where errors encountered during execution are given to the llm to regenerate the program and enhance chances of success.
step iii edge cases based mutation.
with edge cases and initial test programs dfuzz performs edge cases centric mutation.
per our observation we deem an edge case beneficial for testing a target api if the input types of the target api match or subsumes the types of the edge case.
with this heuristic we instruct the llm to select beneficial edge cases for fuzzing achieving high comprehensiveness in covering even rarely tested and new apis as long as their input types match certain edge cases.
whenever a bug is found we record the case and report it to the developers for fixing.
application scope.
dfuzz is designed for testing dl libraries it is fully automated and requires no additional resources besides the source code of the target dl framework.
in contrast recent works may require input templates pre defined by human experts or historical bug reports that are collected online .
dfuzz can be used by developers and also normal users who want to test dl libraries but may lack expertise 4in library implementation .
while dfuzz is evaluated over two mainstream dl libraries pytorch and tensorflow it can be applied to other dl libraries when source code is available.
dfuzz extracts edge cases from the input checks available in the low level code of the dl frameworks.
we clarify that such inline checks are the primary way to handle edge cases in dl libraries.
therefore analyzing those checks offers a comprehensive coverage of edge cases.
looking ahead dfuzz may be extended to test other types of software such as those financial software where inline checks appear also prevalent .
a. edge case extraction dfuzz extracts edge cases from the source code of dl libraries and reuses them to test other apis.
the most straightforward approach is to split the check statements from the source code and ask llm to reason edge cases reflected by these check statements.
for instance we can use prompts like extract edge cases detected by the following check statement for fuzzing apis to guide llms.
nevertheless our tentative exploration shows the following challenge.
transferability.
llms often focus solely on triggering edge cases within a specific context without considering whether these extracted edge cases can be applied to testing other apis.
for instance in fig.
the edge case extracted from line is self is a complex tensor.
however this edge case can only be used when testing a specific api torch.tensor.abs due to the reason that other apis may not have an input variable named self .
even worse our analysis focuses on the low level implementation of apis where variable names may presumably differ from those at the high level.
for example the variable self at the high level python layer corresponds totensor input .
therefore directly utilizing those edge cases extracted by llms is not effective.
to address the challenge we propose a three step approach to extracting context free edge cases.
we refer the three steps as code extractor analyzer and standardizer.
code extractor retrieves check related code blocks from the dl framework source code including check statements and the interface information function name and input parameters of their respective functions.
then for each check statement within the extracted code block analyzer uses an llm to extract a context based edge case and the context description the former refers to the involved input parameters and their description and the latter encodes the names and types of the related parameters.
we further convert context based edge cases into context free ones.
for each context based edge case and its context description standardizer first removes variable names in an edge case and then uses the data types of the involved variables in an edge case referred to as etype pattern to form a context free edge case.
after analyzing all check statements we cluster all etype pattern context free edge case tuples tuples in each cluster share the same etype pattern.
code extractor code extractor extracts code blocks related to edge cases whose approach is generally applicable to various c libraries containing check statements.
to ease presentation consider the aten library in pytorch that uses thetorch check macro to form check statements.
we locate all torch check statements across source code files under aten.
while there exists a large number of torch check statements we only need to consider cases where the input parameters of apis are directly checked by torch check statements given that these checks reflect edge cases that can be triggered by mutating input parameters.
furthermore for each function we assemble a check related code block which consists of the function interface and all torch check statements within the function.
for instance for the function pool2d in fig.
a the code extractor only retains the function interface and the targeted torch check statements resulting in the checkrelated code block shown in fig.
b .
analyzer after extracting the check related code blocks analyzer extracts the context based edge case for each check statement.
to do so one approach is to directly employ llms to analyze each check statement and determine its corresponding edge case.
though it is feasible the analysis results are challenging to use subsequently because the randomness of the output format makes it difficult to extract information in a unified manner.
for instance our employed llm gpt .
sometimes provides edge cases directly corresponding to each check while at other times it divides the analysis outputs into multiple paragraphs making our subsequent analysis tedious.
lacking relevant variable information makes it difficult to convert context based edge cases into context free forms.
for example the edge case extracted by llm for fig.
b is the stride arg should be empty.
yet for other apis it is unclear what stride arg refers to.
prompt design.
to form context free edge cases we instruct the llm to reason edge cases and the context description variable names and types associated with each check.
the prompt is shown in fig.
.
we divide the analysis of a torch check into four steps .
what variables does the torch check examine?
.
what are the data types of these variables?
here we consider seven most commonly used types tensor int bool str float scalar list but it is easy to extend to other types.
.
what edge cases does thetorch check check?
.
to standardize the output and reduce irrelevant information summarize the output in json format.
after each json item we also provide the expected output format and examples.
as an example for the checkrelated block in fig.
b the analysis results are in fig.
c .
standardizer analyzer uses llm to obtain the contextbased edge cases.
however as previously noted context based edge cases are hardly useful.
many context based edge cases are redundant and since variable names remain in an edge case they can only be used in specific syntactic contexts.
additionally these context based edge cases are extracted from low level code whereas our api fuzzing will be launched at the python layer noted in sec.
ii b .
analyzer has prepared sufficient information for standardizer to convert context based edge cases in a format usable at the python layer i.e.
context free edge case.
in particular for each context based edge case we gather the types of relevant variables into a set called the etype pattern .
etype pattern 5extract code blocks with check statements remove unnecessary code linescode extractor tensor abs t ensor self torch check !self.is complex in place abs is ... query the llm for the code block s informationanalyzer remove the variable information extract the applicable input dtype patternstandardizer related variables self corresponding types t ensor tensor self should be complex check related code blockcontext based edge case context descriptioncontext free edge case tensor the t ensor should be complexetype patternall context free edge cases cluster based on etype pattern etype pattern t ensor context free edge cases the t ensor should be complex the t ensor should be 3d the t ensor should be a scalar the t ensor should be empty ......fig.
.
the workflow for extracting edge cases.
tensor pool2d ...... intarrayref stride arg ...... ...... torch check !stride arg.empty stride cannot be empty!
...... return convert v output tensor pool2d ...... const intarrayref kernel arg ...... torch check !stride arg.empty stride cannot be empty!
a originalcodes b extractedcheck relatedcodeblock the 1st torch check statement the statement torch check !stride arg.empty ...... the checked variables stride arg list thechecked edge case the stride arg should beempty the 1st torch check statement etype pattern list context freeedge case the list should beempty c analyzer sample outputs.
d standardizer sample outputs.
fig.
.
edge case extraction example.
reflects the types of input parameters that an api expects in accordance with the analyzed edge case.
for example suppose an edge case s etype pattern is tensor tensor then this edge case can be used to test apis that expect two tensor inputs such as torch.add input tensor other tensor .
and to obtain context free edge cases we replace all variable names in the context based edge cases with the corresponding types.
for example for the context based edge cases and the context descriptions extracted from pool2d in fig.
b we present the extracted etype pattern and context free edge cases in fig.
d .
finally for all etype pattern context free edge cases tuples we eliminate duplicates and cluster them based on etype pattern.
b. initial program generation to fuzz a target api i we need to generate a test program that invokes i. the test program should be non trivial i.e.
it can pass input values to the target api and check the output.
with the high generation ability we employ llms to synthesize the program thus alleviating the burden of manually writing test programs.
however our tentative exploration implies that certain apis are presumably not covered by llm training data especially for newly added or less commonly used apis.
this raises a hurdle for llm to generate valid test programs.
to tackle this challenge we divide the initial program generation into two stages the generation stage and the debug stage.
in the generation stage we integrate the interfacealgorithm initial program generation algorithm input the target api api.
the interface information of the target api apiinfo .
output a valid program that invokes api p. 1init cnt 2while init cnt init max do dialogue prompt construct prompt api api info dialogue.append prompt p llm dialogue status msg exec p debug cnt while debug cnt debug max do ifstatus success then ruturn p error info getmsg msg dialogue.append p dialogue.append error info regenerate p llm dialogue status msg exec p debug cnt debug cnt init cnt init cnt information of the target api into a prompt that is fed into the llm to synthesize the initial test program.
the prompt design is shown in fig.
.
however per our observation generating a program that is both syntactically and semantically correct is challenging the llm generated programs are often not executable.
therefore we propose a debug stage where we instruct the llm to reflect the error message and regenerate the program.
as shown in alg.
dfuzz first generates an initial program based on the interface information of the api and runs it to obtain the execution status lines .
if there is an error we extract the error function along with the specific error message line .
then we feed this error information into the llm to regenerate a program that can resolve the current error lines .
if debugging debug max times still fails to obtain a valid program we ask llm to regenerate a new initial program back to line .
this is important as due to the stochastic nature of llm the same prompt can yield programs of different quality helping us to avoid getting stuck in local minima lines .
based on our preliminary exploration we set init max to and debug max to to balance the trade off between the quality of the generated program and the time cost.
see our evaluation in sec.
v a. c. edge case based mutation through the previous steps we have obtained the contextfree edge cases and the initial program to invoke a target api check related code block for each statement containing torch check answer the following questions which input variables does the torch check statement check?
list the name of the input variables.
what type are these variables respectively?
considering only the following variable types tensor int bool str float scalar list .
what edge case does the torch check statement check?
output the results in the following format the nth torch check statement the statement in the source code the statement containing torch check in the source code the checked variables such as var1 type1 var2 type2 .
the checked edge case follow variable name should requirement such as the input t should be a 2d tensor .
fig.
.
the prompt used by analyzer.
write a program to generate input data and invoke api name to process input data.here are some information of this api api info fig.
.
the prompt of initial program generation.
i. to determine which context free edge cases are presumably beneficial for testing i we collect the etype pattern of i denoting a combinatorial set of its parameter types.
for example for torch.add input tensor other tensor its etype pattern is tensor tensor .
then we iterate each subset of the etype tensor tensor tensor tensor and search for all collected context free edge cases to find a match.
once matched we concretize the type names in the context free edge cases with the names of the input parameters in the target api.
for example when testing torch.all input tensor we then transform tensor is a complex tensor to input is a complex tensor .
finally we assemble the collected context free edge cases the definition of the target api i and the corresponding initial program that invokes iinto a prompt a sample is in fig.
.
then we let the llm generate a test program capable of triggering the edge cases to see if it can uncover any bugs.
as shown in alg.
dfuzz begins by retrieving the initial program and etype pattern of the target api lines .
then leveraging the etype pattern dfuzz identifies the context free edge cases applicable to the api line .
to reduce overhead we select a subset of matched edge cases for mutation line .
more specifically we categorize edge cases into two types based on the etype pattern individual edge cases and compound edge cases.
the former s etype pattern is only related to a single variable while the latter involves multiple variables.
in selecting individual cases we prioritize variables that appear earlier in the api as they are considered more crucial.
particularly for the first two variables all individual edge cases related to them will be selected.
of the individual edge cases related to variables in positions of the api will be randomly selected while the remaining variables will be chosen at a rate of .
.
due to the relatively limited quantity of compound edge cases all matched compound edge cases will be selected for mutation.
finally any bugs triggered by the mutated program are then collected for further analysis lines .
following the convention in this field we consider two types of bugs crashes and cpu gpu inconsistency.
the former asserts that fuzzing triggers any crashes including aborts segmentation faults etc.
for the latter we run mutated programsalgorithm edge case based mutation algorithm input the target api api.
a library of context free edge cases cec .
output a set of programs that trigger bugs bug .
1bugs 2pi getinit program api 3etype pattern getetypes api 4matched edge cases match etype pattern cec 5selected edge cases select matched edge cases 6foredge cas inselected edge cases do pm mutation pi edge case based on figure status msg exec pm ifstatus bug then bugs.append pm separately on the cpu and gpu to check if the outputs are consistent.
.
the initial program that invoke api init program .
the information of api api info mutate the initial program to generate a program satisfy that following requirements edge case fig.
.
the prompt of mutation.
iv.
i mplementation the core testing process of dfuzz is implemented in python with approximately lines of code see our artifact at .
check identification.
without losing generality we extract edge cases from the checks found in the source code of pytorch.
we see that pytorch inline checks are well structured in the format of torch check making it easier to analyze and validate.
also pytorch updates frequently resulting in considerably more checks than other dl libraries on the market.
per our investigation pytorch s tensor operation library aten contains a substantial number of checks over different api input types and attributes.
this is reasonable aten is one core library of pytorch that has been developed and constantly updated by the pytorch team for decades.
aten contains a wide range of computation operations such as matrix multiplication convolution and activation functions.
these operations are the foundation of pytorch s high level apis such as torch.nn and torch.optim .
thus we decide to use pytorch s aten library pytorch aten src aten native to extract edge cases.
we find that using this library offers a good balance between the number of checks large enough to cover a wide range of apis and the complexity of the code not too complex to analyze .
we extracted a total of kinds of etype patterns and their associated edge cases.
the extracted etype patterns include not only basic types tensor int bool str float scalar 7list but also compound types composed of multiple basic types such as tensor tensor int int and so on.
in compound types there are often constraints between variables such as tensor has a larger last dimension than tensor 2orboth integers int and int are negative .
with these extracted context free edge cases we conducted tests on the apis of pytorch in sec.
v. moreover it is important to note that the extracted types and edge cases are transferable to other dl libraries we also test them on tensorflow in sec.
v. v.evaluation we aim to answer the following research questions rq1 how does dfuzz perform in terms of api coverage?
rq2 how many new bugs can be discovered by the dfuzz?
and rq3 how effective is dfuzz when using alternative llms?
we also conduct an ablation study in rq4 .
dl libraries.
we test two mainstream dl libraries pytorch and tensorflow for the following two reasons pytorch and tensorflow are the most widely used dl libraries and bugs discovered within them hold greater value.
pytorch and tensorflow have already been tested by many fuzzing tools.
if we can still uncover long standing bugs from them it convincingly shows the effectiveness of dfuzz.
as a fair comparison we use a consistent set of target apis with titanfuzz and fuzzgpt.
in rq1 we test api coverage using the same versions of pytorch v1.
and tensorflow v2.
as those in the titanfuzz and fuzzgpt papers.
in rq2 we conduct tests on the latest versions of pytorch v2.
.
and tensorflow v2.
to uncover previously unknown bugs.
environment.
all evaluations are conducted on a system with gb ram and running ubuntu .
with one nvidia geforce rtx gpu.
llms.
the current prototype of dfuzz uses gpt .
gpt .
turbo for various llm based reasoning and generation tasks.
we set the temperature to to get the best results.
in rq3 we study the feasibility of replacing gpt3.
with llama2 7b chat and llama2 70b chat further discussions are in sec.
vi.
fuzzers.
when conducting rq1 we compare dfuzz with sota tools titanfuzz and fuzzgpt.
they are both llmbased fuzzers and have successfully tested a wide range of apis on pytorch and tensorflow.
for bug discovery rq2 we compare dfuzz with titanfuzz fuzzgpt and ivysyn.
ivysyn is another sota fuzzing tool for dl frameworks.
we have introduced the design of these sota tools in sec.
ii a. in line with these previous tools we mainly use two metrics api coverage rq1 and number of detected bugs rq2 to assess df uzz and compare with previous tools.
a. rq1 api coverage api coverage is an important metric for dl library fuzzing given that a bug is triggered only when the related apis are invoked .
evaluating api coverage reflects the quality of the initial programs generated by dfuzz.
we first report api coverage below then measure the debugging procedure of dfuzz when synthesizing these initial programs.we compare dfuzz with fuzzgpt and titanfuzz both of which are llm based fuzzers.
ivysyn focuses on testing low level native dl c c code.
it synthesizes code snippets in high level languages e.g.
python only for the buggy code identified in the low level code.
consequently ivysyn s approach isn t optimized for api coverage and therefore we do not compare it.
as a fair comparison we conduct experiments on pytorch v1.
and tensorflow v2.
the same setup used in the fuzzgpt and titanfuzz papers .
more specifically we conduct experiments directly within the docker environment provided by titanfuzz.
we use the default settings of titanfuzz and fuzzgpt.
table iii the comparison of api coverage .
pytorch has a total of api s while tensor flow has api s. target librarydfuzz fuzzgpt titanfuzz under cov1times2cov times cov times test pytorch v1.
unknown tensorflow v2.
unknown 1cov api coverage.2times number of times llm has been invoked.
we measure the number of covered apis and the number of llm invocation times for each tool whose results are in table iii.
dfuzz and titanfuzz both comprise two stages initial program generation and mutation.
as the mutation phase does not lead to additional gains in api coverage we collect api coverage and llm usage times from the initial program generation stage for both these fuzzers.
for fuzzgpt we directly use its final coverage data.
dfuzz offers the highest api coverage in both pytorch and tensorflow.
in particular dfuzz covers and more apis on pytorch compared to fuzzgpt and titanfuzz respectively.
meanwhile dfuzz invokes significantly less amount of llms than that of titanfuzz.
fuzzgpt has not been open sourced at the time of writing and its llm usage counts are unknown to us.
on pytorch and tensorflow the llm usage of dfuzz is only .
and .
of titanfuzz s respectively.
we believe the results are promising given that dfuzz is able to cover more apis with such fewer llm invocations.
recall our synthesis follows a trial and fix strategy where a synthesis process deems failed if dfuzz cannot successfully debug an initial program within a certain number of attempts alg.
.
here we report the number of successful and failed debugging attempts when generating the initial programs in table iv.
note that successful debug refers to the scenario where the program first synthesized by the llm is invalid yet after debugging a valid program is obtained failed debug has been noted above.
table iv debug success evaluation .
target library successful debug failed debug success rate pytorch .
tensorflow .
overall dfuzz s debugging success rates on pytorch and tensorflow are .
and .
respectively.
pytorch has a total of apis where for the apis dfuzz successfully generates valid programs with just one query to the 8llm and the remaining apis require debugging.
among these apis yield corresponding invocation programs after debugging while the remaining apis cannot be debugged successfully.
we report that debugging each pytorch api requires invoking the llm approximately .71times.
with further analysis we find that the gap between pytorch and tensorflow is due to the complexity of the apis.
in particular there are more rarely used apis in tensorflow for which our employed llm appears to lack related knowledge.
this makes the overall generation and debugging process more challenging compared to pytorch.
overall we believe the results are promising indicating that when integrating error information into prompts we effectively guide the llm to generate valid programs.
b. rq2 bug discovery we conducted bug discovery on the latest versions of pytorch v2.
.
and tensorflow v2.
by the time of writing.
note that due to the lack of automated tools for bug analysis confirming each bug requires significant human effort.
therefore we only analyze the fuzzing results of dfuzz.
however to compare with other fuzzers we examine if the bugs detected by dfuzz exist in the older pytorch and tensorflow versions tested by other fuzzers.
if so it implies that those fuzzers were unable to identify these bugs otherwise they would have been fixed .
we report the bug discovery findings in table v. on tensorflow dfuzz identified bugs with already fixed and replicated by the developer but still under investigation.
importantly bugs exist in tensorflow v2.
which has been tested by titanfuzz and fuzzgpt.
nevertheless neither titanfuzz nor fuzzgpt were able to detect them.
meanwhile bugs exist in tensorflow v2.
which has been tested by ivysyn.
on pytorch dfuzz discovered ten bugs with already fixed and replicated by the developer but still under investigation.
among these eight bugs exist in pytorch v1.
which was tested by titanfuzz and fuzzgpt while six bugs already exist in pytorch v1.
tested by ivysyn.
the above results demonstrate that dfuzz not only efficiently discovers bugs but also identifies long standing bugs.
bug characteristics.
dfuzz uncovers bugs in total the last row in table v which can be categorized into four types abort signals segfaults runtime errors and inconsistent output.
runtime errors include internal assert failed mkl fft error and cufft error .
we calculate the number of bugs for each category whose results are in table vi.
most of the bugs discovered by dfuzz are due to the lack of relevant validation checks in apis.
since we test various possible edge cases over input variables most apis tend to trigger abort signals or segfaults directly when encountering unexpected inputs resulting in fewer cases of inconsistent outputs.
among all found bugs cause crashes.
as noted in docter despite receiving invalid inputs dl api functions should not crash.
instead they are expected to handle such inputs 1at this step we also search for bug information in the community e.g.
pytorch forums to make sure they have not been reported by others before.gracefully e.g.
through throwing an exception .
thus aligned with related works focus e.g.
docter and ivysyn we deem these crash bugs as critical.
transferability.
while our edge cases were extracted from pytorch we successfully discover bugs in the latest version of tensorflow.
this indicates that the edge cases we extracted have good transferability and can be used across platforms.
in fact one could even interpret that the edge cases extracted from pytorch are more effective in uncovering bugs in tensorflow than that of pytorch itself.
we believe this is due to the fact that pytorch has been tested by many fuzzing tools and the community is actively fixing bugs still we find ten bugs .
moreover by transferring edge cases across platforms dfuzz for the first time enables a highly comprehensive fuzzing of tensorflow which is hardly achieved by existing tools most of those bugs are notfound by previous works .
extracted edge case types.
the dfuzz is capable of extracting a wide variety of edge cases.
except the three types in table i there are some other types edge cases related to multiple parameters such as tensor 1has a larger last dimension than tensor some special parameter attributes related to program logic such as sparse dense conjugate and contiguous for tensor.
restriction between the attributes of a parameter such as tensor 1with input.shape input.shape .
llms employed by dfuzz can handle these cases properly.
c. rq3 alternative llms in this rq we evaluate if dfuzz can effectively employ smaller llms for edge case based mutation.
in our experiments the general llms have demonstrated greater proficiency in understanding our tasks particularly in creating edge cases for a target api.
thus we conduct experiments using two widelyused general open source llms llama2 70b chat and llama27b chat .
recall we have obtained a set of bug triggering programs in rq2 we collect the prompts which are used to generate these programs and then feed those prompts into llama2 70b chat and llama2 7b chat.
with the same prompts we check if they can also generate bug triggering programs.
we do not have powerful servers to run llama2 70b chat and therefore we use the apis provided by replicate .
replicate provides apis capable of running open source models.
we set the temperature to .
for each prompt and repeat the generation process three times for each prompt.
the results are in table vii.
using llama2 70b chat we can still discover three bugs on pytorch and ten bugs on tensorflow respectively.
llama2 7b chat results in discovering two bugs on pytorch and four bugs on tensorflow.
interestingly llama2 7b chat discovers two bugs on tensorflow that are not found by llama2 70b chat.
upon further analysis we find that llama2 7b chat exhibits more uncertainty in generating complex tensors.
this increased uncertainty occasionally leads to correctly generating bug triggering programs.
d. rq4 ablation study indfuzz each test case is formed by conducting two tasks initial program generation and edge case based mutation.
to 9table v thebugs found by dfuzz.
target library total fixed replicatedexisted in pytorch v1.
tf v2.
existed in pytorch v1.
tf v2.
not found by titanfuzz fuzzgpt not found by ivysyn pytorch tensorflow total torch meta func fmin const tensor self const tensor other torch check !self.is complex ...... tensor fft r2c cufft const tensor self ......bool onesided torch check self.is floating point ......input data torch.tensor result torch.all input data print result ......grad tf.constant dtype tf.float64 ......result tf.raw ops.resourceapplyproximaladagrad ...... grad grad target tf.raw ops.resourceapplyproximaladagradtarget torch.all a b a complex tensora floating pointtensor fig.
.
the case study.
table vi bug types .
library abort signals segfaults runtime error inconsistent pytorch tensorflow total table vii evaluating other llm s. target library chatgpt .
llama 70b chat llama 7b chat pytorch tensorflow total analyze the contribution of each component to bug discovery we analyze the bugs discovered by each component.
we find that allbugs are discovered through the edge case based mutations.
this is unsurprising because the api list we select is aligned with both titanfuzz and fuzzgpt.
titanfuzz and fuzzgpt have already tested these apis and discovered some rather easy to trigger bugs.
given dfuzz s initial program generation is mainly to offer a program invoking the target apis and thus expanding api coverage it is expected to not discover any new bugs.
on the other hand the results show the effectiveness of our edge case based mutation.
although these apis have already been tested by other tools our edge case based mutation approach still detected ten and bugs respectively in the latest versions of pytorch and tensorflow where most of them also exist in older versions tested by previous fuzzing tools .
this illustrates that our tool effectively benefits from diverse edge cases consequently leading to the discovery of more bugs.
e. case study we present case studies to better illustrate the insights of dfuzz.
fig.
presents two examples to show how dfuzz extracts context free edge cases from the source code and successfully triggers bugs.
in fig.
a within torch meta func torch check checks if the variable with the tensor type deems a complex tensor.
accordingly we extract an edge case of tensor type complex tensor .
since the input parameter of apitorch.all is also of tensor type we use the extracted edgecase to fuzz it by setting its input parameter as a complex tensor.
this way we discover an inconsistent output bug in pytorch v1.
running on gpu and cpu due to that torch.all does not consider the case of complex tensor.
given that torch.all is widely used our finding received immediate attention from developers and was set as high priority for fixing.
in fig.
b we extract edge cases from pytorch and apply them in fuzzing tensorflow this shows the transferability.
within fft r2c cufft torch check checks if the variable with tensor type deems a floating point tensor.
similarly we extract an edge case of tensor type floating point tensor and use the extracted edge case to fuzz tf.raw ops.resourceapplyproximaladagrad .
we find that this api has a severe core dumped issue in handling floating tensors.
it is difficult for previous fuzzing tools to detect these two bugs.
titanfuzz applies basic mutation operators to mutate programs based on a scheduling algorithm.
however it lacks guidance for triggering edge cases making it infeasible to trigger those two bugs.
similarly for fuzzgpt to discover these bugs three prerequisites must be met first there must be bug codes with similar root causes on the internet.
second fuzzgpt needs to properly crawl and collect these related bug codes.
third fuzzgpt must correctly extract the root cause from these bug codes and apply it to test tf.raw ops.resourceapplyproximaladagrad .
however these two bugs already existed in pytorch v1.
and tensorflow v2.
which have been tested by fuzzgpt this indicates that fuzzgpt overlooked these bugs.
overall the aforementioned three conditions are uneasy to meet in practice making it difficult for fuzzgpt to discover these bugs.
in contrast dfuzz manifests two advantages over fuzzgpt dfuzz does not depend on information bug codes from the internet instead it only needs the source code of dl libraries.
it can comprehensively test apis uncovering more edge cases that developers may not have considered nor reported online.
ivysyn manually designs mutators through the analysis of known cves.
this is a substantial effort yet hard to be comprehensive.
particularly for tensor types ivysyn features a pool of tensor mutations containing tensors with large positive and negative values tensors with empty shapes and tensors containing random dimensions.
nevertheless it does not include cases of complex tensors and floating point tensors.
as a result 10ivysyn fails to find these two bugs although these bugs already exist in pytorch v1.
and tensorflow v2.
that have been tested by ivysyn.
this illustrates the inherent challenge faced by manual efforts it is difficult to be comprehensive and also keep up with the rapidly evolving dl libraries.
in contrast dfuzz automatically extracts edge cases from the source code not only eliminating the need for expert experience but also capturing a more comprehensive set of edge cases.
vi.
d iscussion extension.
holistically dfuzz leverages the reasoning and generation abilities of llms to achieve comprehensive fuzzing of dl libraries.
in this regard we envision several promising directions to extend dfuzz.
first we can further improve the reasoning ability of llms by incorporating more domainspecific knowledge.
for example we can leverage the existing knowledge in the form of api documentation code comments and bug reports to guide the reasoning process.
as a common practice this rich information can be dumped into local vector databases and then be used to guide the reasoning process .
second we can enhance the generation ability of llms by incorporating more advanced program synthesis techniques.
recent advances in llm based program synthesis have shown promising results in generating programs under various scenarios .
nevertheless we clarify that the benefit of enhancing generation ability may be marginal given that when preparing the initial test programs we often do not need to synthesize complex programs.
threat to validity.
one potential threat to the validity of our study is the generalization of our findings.
while we have demonstrated the effectiveness of dfuzz on two popular dl libraries tensorflow and pytorch it is unclear whether dfuzz can be generalized to other dl libraries.
to mitigate this threat we illustrate the high transferability of dfuzz by showing that its extracted edge cases from pytorch can be effectively used to test tensorflow.
this suggests that dfuzz can be generalized to other dl libraries.
another potential threat is the reproducibility of our findings.
llms can be sensitive to the training data the model architecture and the hyperparameters.
particularly commercial llms like gpt .
only offer remote apis which might undermine its reproducibility.
to mitigate this threat we have made our artifact available and we have assessed the performance using alternative open source llms.
from another perspective we observe that a certain level of randomness is helpful to fuzz e.g.
our findings in sec.
v c .
vii.
c onclusion in this paper we present dfuzz a novel white box approach for llm based dl library api fuzzing.
using llms we infer edge cases and generate initial test programs which offer effective and efficient dl library api fuzzing.
evaluations show that dfuzz consistently outperforms existing dl library fuzzers for pytorch and tensorflow.