software model evolution with large language models experiments on simulated public and industrial datasets christof tinnes siemens ag garching bei m unchen germany christof.tinnes siemens.comalisa welter saarland university saarbr ucken germany welter cs.uni saarland.desven apel saarland university saarbr ucken germany apel cs.uni saarland.de abstract modeling structure and behavior of software systems plays a crucial role in the industrial practice of software engineering.
as with other software engineering artifacts software models are subject to evolution.
supporting modelers in evolving software models with recommendations for model completions is still an open problem though.
in this paper we explore the potential of large language models for this task.
in particular we propose an approach ramc leveraging large language models model histories and retrieval augmented generation for model completion.
through experiments on three datasets including an industrial application one public open source community dataset and one controlled collection of simulated model repositories we evaluate the potential of large language models for model completion with ramc.
we found that large language models are indeed a promising technology for supporting software model evolution .
semantically correct completions on real world industrial data and up to .
type correct completions .
the general inference capabilities of large language models are particularly useful when dealing with concepts for which there are few noisy or no examples at all.
i. i ntroduction models play an important role in modern software and system development software documentation system architecture simulation and industrial automation .
in practice all artifacts in software and system development are subject to evolution which also applies to software models1 software models must evolve because of changing requirements but they are also subject to bugfixes and refactorings .
from the perspective of a modeling tool we can understand the evolution of a software model as a sequence of edit operations to change or evolve the model the user executes edit operations e.g.
using mouse clicks and keyboard strokes provided by the tool.
supporting tool users in accomplishing various software model evolution tasks is clearly desirable in practice .
for the evolution of software models modeling tools typically provide an initial set of edit operations e.g.
adding an attribute to a model element .
nevertheless since the usage of a domain specific language is also subject to evolution and since project specific usage patterns might emerge this initial set of edit operations is likely not exhaustive.
equal contribution1in our work to avoid confusion it s crucial to differentiate between software models and machine learning models.for example in object oriented design design patterns are widely used and are not part of uml but could be provided as edit operations by a uml modeling tool.
for source code modern integrated development environments already support writing and evolving source code by auto completion .
most notably the use of large language models llms has become state of the art for the autocompletion of source code .
the world of software models seems to be lagging behind and no general approach for software model auto completion is ready for industrial application.
it has been even argued that the so called cognification of use cases in model driven software engineering might turn the difference between perceived added value and cost from negative to positive .
problem statement.
notably for a few domain specific languages rule based approaches exist that use pre defined edit operations or patterns for model completion .
using a specification language for defining edit operations poses three challenges though.
first specifying new edit operations requires knowledge about the specification language and the domain specific language.
second domain specific edit operations are often not explicitly known that is they are a form of tacit knowledge .
externalizing the knowledge is hard or even impossible for domain experts.
third edit operations can change over time for example because the metamodel changes.
in the light of these challenges mining approaches that retrieve edit operations are especially appealing since they do not require any manual specification no hand crafting of examples as in model transformation by example and they are not limited to well formedness rules that can be derived out of the metamodel.
unfortunately existing approaches such as applying frequent subgraph mining to software model repositories are not scalable and mining approaches lack abstraction capabilities .
clearly from the perspective of software model evolution it is desirable to have context dependent auto completions rather than utilizing a fixed set of edit operations.
we posit that generative language models exhibit a deep understanding of language and hold comprehensive knowledge across various domains which is a result of their training on vast corpora.
thisarxiv .17651v5 dec 2024capability enhances their potential to interpret and complete software models effectively which usually encompass a vast amount of natural language data.
while recent research suggests that llms could be utilized for model completion we go beyond and utilize model evolution data from model repositories to capture real world complexities.
it is important to note that in our work we explicitly acknowledge the complexity of real world data which is due to the close collaboration with our industry partner who also contributes a case study .
contributions.
by leveraging existing software model histories2 and by defining an encoding for serializations of model difference graphs we study to what extent retrievalaugmented generation i.e.
we provide examples as context in the prompt can be used for software model completion.
we find thatramcis indeed a promising approach for software model completion with .
of semantically correct completions.
we furthermore propose to use fine tuning i.e.
the llm s weights are adapted by training on parts of our data for software model completion and compare it to our retrievalbased approach ramc.
llm s general inference capabilities prove especially helpful in handling noisy and unknown context and real time capabilities enabled by llms are beneficial for stepwise model completion.
we conclude that using llms for software model completion is viable in practice despite various complexities but further research is necessary to provide more task and domain knowledge to the llm.
in summary we make the following contributions as a foundation for applying llms we formalize the concept of software model completion based on change graphs and their serialization.
we propose a retrieval augmented generation approach ramc for software model completion.
we evaluate ramcqualitatively and quantitatively on three datasets including an industrial application one public open source community dataset and one controlled collection of simulated model repositories.
we compare our approach with the most recent advancements in model completion as well as to the alternative of finetuning a pre trained llm.
we find that for all three datasets llms are a promising technology for software model completion with up to .
correct completions for the synthetic dataset and .
of semantically correct completions on the industrial dataset.
notably our approach improves significantly over the state of the art .
furthermore it appears that fine tuning can be an alternative to retrieval augmented generation that is worthwhile investigating.
source code for the experiments scripts public datasets and results are publicly available see section vii .
2note that we use the terms software model repositories andsoftware model histories interchangeably and we assume that the repository contains several revisions of a software model.ii.
r elated work various approaches have been proposed for software model completion ranging from rule based approaches to data mining techniques and more sophisticated machine learning approaches.
an overview of recommender systems in modeldriven engineering is given by almonte et al.
.
some of the previous work studies recommending model completions by utilizing knowledge bases such as pattern catalogs or knowledge graphs .
consequently these research efforts are often domain specific as they require the provision of domain specific catalogs a.k.a.
the cold start problem such as for uml or business process modelling .
another common approach is to use already existing model repositories and employ techniques such as frequency based mining association rule mining information retrieval techniques and clustering to suggest new items to be included in the model or new libraries for use .
memorec and morgan are frameworks that use a graph based representation of models and a similarity based information retrieval mechanism to retrieve relevant items such as classes from a database of modelling projects.
however their graph based representation does focus on the relationship between a model element and its attributes but it does not capture relationships between different elements in the model and consequently may not capture the essential semantics and constraints of the model and modelling languages.
repository mining and similarity based item recommendation techniques are often combined .
k ogel et al.
identify rule applications in current user updates and find similar ones in the model s history.
more generally one could automatically compute consistency preserving rules or pattern mining approaches to derive a set of rules to be used in conjunction with a similar association rule mining approach.
another strategy to generate model completion candidates that comply with the given metamodel and additional constraints involves using search based techniques .
without knowledge about higher level semantics these approaches are more comparable to the application of a catalog of minimal consistency preserving edit operations .
regarding the application of natural language processing nlp and language models burgue no et al.
propose an nlp based system using word embedding similarity to recommend domain concepts.
weyssow et al.
use a transformer based language model to recommend metamodel concepts without generating full model completions.
di rocco et al.
introduce a recommender system using an encoderdecoder neural network to assist modelers with editing operations.
it suggests element types to add but leaves the specification of details values and names of these elements and operations to the human modeler.
gomes et al.
use natural language processing to translate user intents expressed in natural language into actionable commands for developing and updating a system domain model.
while code completion and model completion are closely related recent research has mainly concentrated on code completion where llms seemto be the state of the art .
considering the close connection to code and model completion it s essential for us to explore further how generative approaches such as llms operate within the context of software model completion of complex real world models.
most closely to this work is an approach by chaaben et al.
which utilized the fewshot capabilities of gpt for model completion by providing example concepts of unrelated domains.
in contrast our approach takes a different avenue leveraging model evolution from model repositories.
c amara et al.
further extend on chaaben et al.
s research by conducting experiments to assess chatgpt s capability in model generation.
ahmad et al.
explore the role of chatgpt in collaborative architecting through a case study focused on defining architectural significant requirements asrs and their translation into uml .
in the appendix a table summarizing related work on model completion is provided.
a slightly different but similar research area focuses on model repair .revision uses so called consistency preserving edit operations to detected inconsistencies and then uses the pre defined edit operations to recommend repair operations.
iii.
f ormal definitions in this section we describe the fundamental concepts essential for the subsequent approach and analysis.
a.software models edit operations and model completion in model driven engineering the language for a software model i.e.
its abstract syntax and static semantics is typically defined by a metamodel tm.
we denote by mthe set of all valid models according to some metamodel .
this can be formalized using typed attributed graphs .
definition iii.
abstract syntax graph .anabstract syntax graph gmof a model m m is a attributed graph typed over an attributed type graph tggiven by metamodel tm.
the idea of typed graphs is to define a graph homomorphism i.e.
a function from the typed graph gto the type graph tg .
details of this formalization are given by biermann et al.
.
the abstract syntax graph of a model and its type graph contain all information that a model holds.
in this paper we are concerned with model repositories.
we assume that the modelling tool takes care of checking the correct typing of the software models.
furthermore we work with a simplified graph representation of the models in which the abstract syntax graph is alabeled directed graph with node and edge labels equal to a textual representation of corresponding classifiers and relationships of the abstract syntax graph cf.
definition iii.
.
definition iii.
labeled directed graph .given a label alphabet l alabeled directed graph gis a tuple v e where vis a finite set of nodes eis a subset of v v called the edge set and v e lis the labeling function which assigns a label to nodes and edges.rather than working directly on the abstract syntax graph of the models we will mostly be working with model differences.
definition iii.
structural model difference .astructural model difference mnof a pair of model versions mandn is obtained by matching corresponding model elements in the model graphs gmandgn using a model matcher e.g.
emfcompare or sidiff .
there are added elements the ones present in gnbut not in gm removed element the ones present in gmbut not in gn and preserved elements which are present in gmandgn.
we assume that this matching is deterministic that is given two models m n m we obtain a unique structural model difference mn.
the difference can be represented as adifference graph g mn .
more concretely we add the change type add preserve or remove in the node and edge labels and matching elements i.e.
the preserved ones from gmandgnare unified present only once .
we define a simple change graph to be the smallest subgraph comprising all changes in the difference graph g mn.
definition iii.
simple change graph .given a difference graph g mn asimple change graph scg mn g mnis derived from g mnby first selecting all the elements in g mn representing a change i.e.
added removed nodes and edges and second adding preserved nodes that are adjacent to a changed edge.
definition iii.
endogenous model transformation .anendogenous model transformation is a pair t m n m m .
we call mthesource model andnthetarget model of the transformation and tdef m m the space of endogenous model transformations.
next we define a function scg t g that takes a model transformation i.e.
a pair of models as input and returns the simple change graph for the corresponding model difference.
we can use scg to define an equivalence relation on tby t1 m n t2 k l scg mn scg kl.
it is straightforward to see that this relation indeed defines an equivalence relation i.e.
the relation is reflexive symmetric and transitive .
we can therefore define the quotient set t .
by construction there is bijection from the quotient set to the range of scg .
we can therefore use this construction to formally define the concept of an edit operation .
definition iii.
.
anedit operation is an equivalence class in the set edef t .
an edit operation is therefore a set of model transformations that have the same simple change graph.
remark.
the graph labeling function allows us do define the scope of the edit operation.
for example if we are interested only in the type of nodes and edges we can omit the attributes from the label.
likewise if we are interested in the attributes or only want to set them during execution time we can define placeholders for the attribute values inuniversit t des saarlandes31m !m m !
a 1b figure visual presentation of our example taken from the repair vision dataset a evolutionary view user performs edit operations one by one.
b evolution can be performed by a user or by using a completion approach.
the labels.
therefore we define edit operations only up to the concrete label representation which leaves some freedom for templating.
in this work we do make use of placeholders only during the evaluation e.g.
checking for type correctness .
given an edit operation and a model m one can perform the removal of remove nodes and the gluing of add nodes as defined by the simple change graph corresponding to and then set concrete attributes.
this yields the corresponding model nwith m n .
this way an edit operation e can be interpreted as a template for a model transformation which is in line with previous constructions .
we write m n to denote a concrete element i.e.
a model transformation in the equivalence class e. we are interested in completing software models.
that is for an existing evolution m n we want to find a completion e such that m n cis a realistic completion meaning in some real world scenarios it actually will be done by a modeler.
definition iii.
model completion .given a set of model transformations t model completion is a computable function c t t that given a model transformation m nfrom a source model mto a partial target model n computes a model transformation c m n n c. we call the edit operation asoftware model completion .
given a model completion we denote the application of to model nby m e t where m n c .
in general for an edit operation there might be zero or more applications to a given model m m .
nevertheless given that the matching in nis fully defined by the application of there is a uniquely defined candidate n c t. b. language models language models as generative models have the capability to produce new sequences of text based on their training data.
definition iii.
language model .alanguage model is a conditional probability distribution p c for a sequence of token s given a sequence of context tokens c.the probability distribution is typically derived from a corpus of documents containing some of the tokens.
with the success of transformer architecture llms have become quite popular now and are used in plenty of domains including software engineering .
there are two tactics available to feed domain knowledge or context into a generative language model fine tuning and retrieval augmented generation.
retrieval augmented generation includes additional knowledge in the context or prompt .
fine tuning adjusts the llm s weights based on additional training data.
iv.
a pproach in this section we describe ramc our approach of how to employ llms to auto complete software models.
a. running example consider the motivating example depicted in figure which originates from one of our datasets repair vision further explained in section v b. in a we show the evolution of its abstract syntax graph3.
in this evolution scenario a modeller adds the uml profiles mechanism cf.
uml specification chapter .
to the ecore metamodel4of uml .
.
.
step by step the modeller extends the existing uml metamodel with additional functionality currently focusing on the eclass extension in the uml package.
in a first step the modeller adds an operation getstereotype responsible for accessing the sterotype of the extensions associated with an element in the meta model .
as defined in the uml specification every extension has access to the metaclass it extends realized in ecore by the eoperation getmetaclass .
this eoperation is implemented by the modeller in a second step.
these steps in the evolution of the uml metamodel could be performed via edit operations by a human user or likewise recommended in the form of a model completion as depicted in b of figure .
b. overview and design choices utilizing llms for software model completion gives rise to several challenges addressed by ramc how to provide context such as domain knowledge to the llm how to serialize software models and how to deal with limited context5?
regarding context we opt for retrieval augmented generation and compare the approach to fine tuning in one of our experiments.
the next important design decision is that we do not work on the software models directly but on the simple change graphs described in section iii.
the basic idea is that simple change graph completions can be straight forwardly interpreted as model completions i.e.
generating a new added node corresponds to adding a new model element to the model .
working with the concept of a simple change graph has several advantages first we do not have to work with the entire software model representation but we can focus on slices of 3due to obvious space constraints only a small part of the original model only one out of classifiers and out of operations is shown4uml according to the meta object facility is itself a model according to its meta metamodel ecore and therefore covered by the present work.
5software models can become huge compared to the limited number of tokens that can be given to a llm.fewshotexamplesinstructiont 292e1 changetype add type reference referencetypename eoperations changetype preserve type object classname eclass attributes id ftha796teei97md7gk1rma name extension epackage uml abstract false interface false eidattribute name egenericsupertypes changetype add type object classname eoperation attributes id lu7gf96teei97md7gk1rma name getstereotype ordered false unique true lowerbound upperbound many false required false etype stereotype egenerictype stereotype econtainingclass extension eyouarean assistantthatisgivena listofchangegraphsin an edgeformat.
thatis thegraphisgivenedgebyedge.
the graphsaredirected labeledgraphs.
an edgeisserializedas esrc idtgt idedge labelsrc labeltgt label labels aredictionaries.
ifa nodeappearsin morethanoneedge thesecondtime itappearsitisreplacedby toavoidrepetition.e.g.
e0 a b bare1 bla foothe secondedgeherewouldbeequivalentto e1 blabar foo .
therearesomechangegraphsgivenasexamples.
graphs areseparatedby n n n n .the last graphin thislistofgraphsisnot yetcomplete.
exactlyoneedgeismissing.
yourtaskisit tocompletethelast graphbyguessingthelast edge.
youcanguessthistypicallybylookingat theexamplesand tryingtodeducethepatternsin theexamples.
givethismissingedgein theformat esrc idtgt idedge labelsrc labeltgt label .
note thatthebeginning e isalreadypartoftheprompt.
changetype add type reference referencetypename eoperations changetype add type object classname eoperation attributes id lu7gft6teei97md7gk1rma name getmetaclass ordered false unique true lowerbound upperbound many false required false etype metaclass egenerictype metaclass econtainingclass extension approach generated prompt response!
!
serializedchatgptapi figure detailed prompt and simple change graph serialization of the r amcapproach corresponding to the example given in figure exact few shot examples are provided in the appendix.
the models around recently changed elements.
this is one tactic of dealing with the common problem of the limited context of a llm.
for example in our running example the entire serialized uml metamodel is huge and would not fit in the context of contemporary llms.
second simple change graph completions also include attribute changes and deletions of model elements and are not limited to the creation of new model elements.
ramcis capable of suggesting semantically appropriate changes such as renaming an attribute or altering the type of an attribute.
additionally it recommends specific attribute values that are beyond predefined options for example values for string type attributes.
although alternative representations besides simple change graph can influence the outcome choosing simple change graph was a deliberate design decision we made.
an overview of the approach is depicted in figure the computation of model differences figure and simple change graphs figure is explained in section iii.
their serialization will be addressed in the next subsection.
based on the terminology in section iii the formalization of our approach r amcis given in the appendix.
figure overview of r amc.c.
pre processing both training phase and generation phase work on serializations of simple change graphs.
we describe how these serializations are derived based on the example given in figure .
input to this procedure are two successive revisions of a model output is a serialization of their simple change graphs.
these revisions can originate either from the model the user is working on in the generation phase or from our training data.
in the first step a model difference is computed for each pair of successive revisions of a model figure .
regarding our running example in a of figure we also highlighted these model differences by color that is added model elements are depicted in green.
from this model difference we compute a partial simple change graph see definition iii.
and figure .
finally the simple change graph is serialized as a list of edges figure .
to this end we defined a graph serialization called edgelist for directed labeled graphs.
figure presents the prompt generated from our approach alongside the corresponding response which was retrieved via api access to chatgpt.
it also shows an example of this graph serialization e.g.
last part of the prompt which contains all kinds of attribute information.
it can quickly become verbose and noisy in real world examples.
common formats such as the graphml6are less suitable for llms since they list vertices before edges.
this requires guessing all nodes first added deleted and preserved before generating edges.
d. training phase the input to the training phase is a set of serialized simple change graph components.
the output is a vector store of serializations with a key for retrieval figure .
we retrieve relevant simple change graphs from model repositories by utilizing a similarity search based on sentence embeddings .
the serializations are stored in a vector database together with their sentence embedding figure 4and5 .
e. generation phase the input to the generation phase is a set of serialized simple change graph components capturing the difference of a new model snapshot i.e.
local changes and the previous model revision m1 m2 as well as the vector store from the training phase.
the output is a list of completion s in the form of edgelist serializations which are suggested to the user after being parsed an example is given in figure at the bottom under response .
retrieval.
the vector store is queried for simple change graph serializations via a similarity based retrieval.
note that in our case the retrieved context can be interpreted as fewshot examples because we retrieve complete simple change graphs that is completed partial simple change graphs from the history.
the few shot samples from figure are detailed in the appendix.
to ensure a diversity of samples we use a procedure similar to maximum marginal relevance explained in detail in the appendix.
as few shot samples we select up to simple change graphs we investigate the dependency on the number of few shot samples in section v. prompt formulation.
the prompt input to the llm used by our approach consists of an instruction at the beginning followed by the few shot samples retrieved from the vector store joined via a separation token and finally the partial simple change graph serialization is concatenated see figure .
sampling new edges.
we can sample multiple completion candidates from the llm by using a beam search or by instructing the llm to generate multiple edges.
details of the edge sampling are given in the appendix.
f .
implementation we have implemented the computation of model differences and simple change graphs on top of the eclipse modeling framework using sidiff for matching and diffing.
the other parts are implemented in python mainly utilizing network x7for handling graphs.
we use lang chain8 for the handling of language models and retrieval augmented generation.
we use the all minilm l6 v29language model for the sentence embeddings since it performed well in preliminary experiments.
as vector store we use chroma db10.
as language model we use gpt version since it performed best in preliminary experiments.
we use a dedicated deployment of openai on microsoft azure that is certified for the classification level of the industrial data.
v. e valuation we evaluate to what extent our approach is able to derive structurally and semantically correct completion operations from the software model history.
this includes in particular their applicability in industrial scenarios.
we aim at a systematic evaluation of llms for model completion in a controlled setting.
this allows us to concentrate on the core effectiveness of llm technology while controlling for confounding factors such as tool use and human aspects e.g.
ux design facets .
this is also the reason why at this stage conducting a user study settled in a specific application context would be not opportune but needs to follow at a later stage .
however by applying our approach to a real world context at our industry partner who expressed clear interest in and demand for this technology we establish a solid methodological and empirical foundation before considering the development of sophisticated and potentially costly tools.
a. research questions to understand the merits of language models for model completion we want to answer the following research questions rq to what extent can pre trained language models and retrieval augmented generation be used for the completion of software models?
a general pre trained language model is typically not aware of the syntax and domain specific semantics of the simple change graph serializations per se .
this includes the definition of the graph serialization format the definition of simple change graphs the metamodel and the domain specific semantics of the software models not already encoded in the metamodel.
for example a generated completion might be invalid according to the metamodel e.g.
invalid combination of edge source and target node labels or could even result in an invalid directed labeled graph serialization e.g.
they do not adhere to the edgelist format .
rq what influence does semantic retrieval have on the performance of ramc?
as motivated in section iv providing context that is semantically close to a to be completed change could improve the correctness of retrieval augmented generation.
we therefore want to understand the influence of the similarity based retrieval on model completion.
that is we want to compare semantic retrieval and random retrieval of few shot examples and to analyze the influence of the number of few shot examples.
rq how does ramccompare against the state of the art chaaben et al.
?
we evaluate the accuracy of our proposed approach ramc by comparing it to the closely related work of chaaben et al.
which we use as a baseline.
their study focuses on few shot learning to suggest new model elements providing the same unrelated few shot examples independently of the current model to be completed.
our investigation centers on the prediction improvements that can be realized by providing semantically similar examples from the model history as context to the llm for the model completion task.
rq what are limitations of using llms for model completion in a real world setting?
while quantitative results provide insights into the merits of llms on model completion we also want to investigate when and why model completion fails.
from simple examples and simulated changes it is hardly possible to make assertions for real world changes.
we therefore take a closer look at a sample set from real world changes .
from our observations we will derive research gaps and hypotheses for future research.
rq what insights can be gained when comparing domainspecific fine tuning to our retrieval based approach ramc?
an alternative to retrieval augmented generation is domainspecific fine tuning.
we explore its viability considering dataset properties and training specifics e.g.
epochs and base llm .
b. datasets to answer our research questions we make use of three datasets balancing internal and external validity.
basic statistics about the datasets are given in table i.table i basic statistics for the datasets.
model size is measured in terms of the number model elements.
changes include added deleted and modified model elements.
dataset no.
no.
avg.
avg.
no.
public models revisions model size changes industry no repair vision yes synthetic yes industry dataset.
we have extracted this dataset from a repository of sysml models in magic draw11for a train control software used by a large product line of trains of our industry partner.
the dataset stems from an industry collaboration where we tackle several challenges related to the management of large industrial software product lines.
the model for the train control software comprises several submodels such as drive and brake control interior lightning exterior lightning sanitary facilities hv ac etc.
in a preprocessing step we have removed confidential information e.g.
the models contain requirement owner information and other personal information of involved engineers .
the models themselves as well as the average number of changes between revisions in this dataset are large cf.
table i .
the large number of changes originates from many attributes changes such as renamings and typically long time periods between two revisions.
the industry dataset with its domain specific and projectspecific concepts helps to understand to what extent we can use llms for software model completion in a complex real world setting.
it allows us to assess the effectiveness in navigating the noisy complex and often irregular nature of real world data a critical aspect often overlooked in existing research.
repair vision dataset.
the repair vision dataset is a public dataset12of real world open source models containing histories of ecore repositories such as uml2 or bpmn2.
the repair vision dataset plays a crucial role in our evaluation in assessing how effectively llms can be employed for software model completion in real world settings.
similar to the industry dataset the serialized change graphs in this dataset can become verbose and noisy and reflect the difficulties of real world model completion see figure .
its public availability facilitates reproducibility comparability and public accessibility fundamental aspects that ensure our research can be examined and extended by others.
synthetic ecore dataset.
with the first two datasets we aimed at external validity and a real world setting.
at the same time we had only little control over potentially influential factors of the dataset impairing internal validity.
to obtain a dataset for which we can control several properties of the model repositories we simulated the evolution of a software model similar to tinnes et al.
we used a metamodel that resembles a simple component model as used in modelling system architecture with components implementations ports connectors and requirements .
some predefined edit operations 11magic draw is a modeling tool commonly used in industries for uml and sysml system modeling .
been randomly applied to a revision of a software model to obtain a new revision of the software model.
this way we were able to control the number of edit operations that are applied per model revision i.e.
and the number of model revisions in one dataset i.e.
or .
we furthermore randomly applied perturbations.
that is with a certain probability i.e.
we slightly modified the edit operation by a successive application of an additional edit operation that overlaps with the original edit operation.
the repositories in this dataset contain only changes at the type level that is we do no include attributes or changes thereof.
the synthetic dataset gives us more control over several properties of a model repository allowing us to specifically understand how fine tuning is affected by the properties of the model repositories this way increasing internal validity.
c. operationalization we conduct four experiments one per research question.
for all significance tests we use a significance level of .
.
experiment rq to answer rq we preprocess all three datasets from section v b and generate a collection with training and testing samples more specifically simple change graphs to ensure a systematic evaluation.
we then select13between samples depending on the dataset from the testing set and for each we select between to few shot samples from the training set.
the reason to choose between samples is to obtain a sample set of a manageable size that we can manually analyze and that induces acceptable costs for the llm usage and to obtain a large enough set to draw conclusions.
first we analyze the correctness of the generated completions with respect to the ground truth.
a simple change graph contains a change that actually occurred in the modeling history.
from the change graph we randomly remove edges to obtain a partial change graph with the full change graph being the corresponding ground truth.
this approach improves over previous methods that involves arbitrarily removing elements from a static snapshot.
by focusing on model histories we create a realistic setting selecting subsets of changes that have actually occurred in real world scenarios.
we consider different levels of correctness structural correctness ensures that the graph structure is correct with properly directed sourced and targeted nodes.
change structure correctness builds on this by additionally requiring correct types of changes to the model such as whether elements should be modified added or removed.
lastly type structure correctness demands further an exactly correct type and changetype .
an illustrative example for these types is given in figure under response .
we automatically check the format structural correctness change semantics and type correctness for all datasets.
for the industry dataset we additionally manually evaluate the generated completions to also check for semantic correctness.
in our manual analysis of semantic correctness a solution was deemed correct if the llm s proposed completion matched the ground truth in meaning and purpose.
this check cannot 13the selection procedure is explained in detail in the appendix.be automated due to the extensive use of natural language in our data and application specific identifiers e.g.
user chosen attribute names .
for example in figure naming a new operation getextension or getext is a matter of preference while their semantic meaning is the same.
we addressed potential errors and bias in our manual analysis by having two of the authors independently evaluate the proposed solutions.
any mismatches in their evaluations were discussed and a consensus was reached on the correct interpretation.
for the base llm we use gpt version in a dedicated azure deployment to complete our prompts.
experiment rq in rq we investigate whether the correctness from correct format to semantic correctness depends on the number of few shot samples.
for the industry dataset we have the information on whether a few shot sample s change is of a similar class as the test simple change graph.
we also investigate how this affects correctness that is whether the similarity based retrieval in ramcaffects the correctness of completions.
to this end we compare semantic sampling with few shot samples that have been randomly retrieved from the training data.
we evaluate this for semantic correctness.
for this reason and also to reduce the llms usage costs we perform this analysis only for the industry dataset.
experiment rq to address rq we selected the publicly available revision dataset.
this selection not only enhances reproducibility but also allows for comparisons with future methodologies such that ongoing research advancements can be directly compared to our ramcand the work by chaaben et al.
.
their approach recommends new classes their associations and attributes.
accordingly the present experiment specifically targets these aspects.
we excluded samples that did not fall into these categories resulting in test examples from the revision dataset for comparison.
to replicate the approach introduced by chaaben et al.
which we denote as a baseline we use their few shot examples serialization of concepts and incorporate the partial models similarly into the prompt.
further details are available in the appendix.
we query gpt text davinci several times and suggest the most frequently occurring concept.
experiment rq we answer rq by manually investigating completions that have been generated in the first experiment for the industry dataset.
we go through all prompt and completion pairs and identify common patterns where the model completion works well or does not and we aim at interfering causes that led to the results.
since this analysis is time consuming we focus on the industry dataset a domain and project specific real world dataset.
we report on the identified strengths and weaknesses of the approach given this real world scenario and point to research gaps and formulate hypotheses for future research and improvements.
experiment rq to investigate whether fine tuning is 14note that we experimented with several llms from the gpt family of models and also observed changes in the specific model s performance over time .
at the time of execution gpt using a small introductory prompt that explains the tasks see appendix was performing best on a small test set and we therefore fixed the llm in r amcto gpt .a viable alternative to few shot prompting see experiment we fine tune models from the gpt family of language models on the synthetic dataset.
the reasons why we restrict this analysis to the synthetic datasets are manifold the main reason is that we want to understand how the performance of the fine tuning approach depends on various properties of the dataset in a controlled setting.
furthermore we have a limited budget for this experiment and fine tuning is costly.
we also control for the number of fine tuning epochs and the base language model used for the fine tuning.
for every repository of the dataset we split the data into training set and testing set and we use the test set to report on the performance of the completion task.
the fine tuning of the models optimizes the average token accuracy15.
to compare the retrieval augmented generation to fine tuning we run both for the same test samples.
for the few shot training samples we also use the same training samples used to fine tune the language models.
we assess the correctness with regard to the ground truth.
due to the unique characteristics of the synthetic dataset the ground truth correctness is defined by the graph structure change structure and type structure.
d. results experiment rq addressing rq which explores the extent to which pre trained llms and retrieval augmented generation can be utilized for software model completion our findings on the correctness of ramcare detailed in table ii.
we list the different levels of correctness for all datasets.
we see that more than of the completions have a correct format and even more than of completions are type correct that is completed edges have the right source and target nodes and type and the types of the source and target node are correct.
even at a semantic level of the generated completions are correct for the industry dataset.
for the synthetic dataset type correctness is equivalent to semantic correctness.
consequently of the results are correct for this dataset.
table ii different levels of correctness in percent of the entire test set for all three datasets.
change type total dataset format structure structure structure semantic count industry .
.
.
.
.
repair vision .
.
.
.
synthetic .
.
.
.
experiment rq regarding the relationship between the number of few shot samples and correctness we conducted a one sided mann whitney u test for the overall and type semantic correct distributions over the number fewshot samples.
for every dataset we do not find any significant relationship between the number of few shot samples and correctness smallest p value is .2for the type correctness 15at the time of experiment execution evaluating with any self defined test metrics was not possible using the fine tuning apis provided by openai.
this metric is not aware of any specifics of the dataset and even a single wrong token in a serialization can produce a syntactically wrong serialization while the token accuracy for the incorrect completion would still be high.of the repair vision dataset .
furthermore we find that test samples where a similar class of changes is among the few shot samples perform significantly better than overall correctness p .
using a mann whitney u test p .
using a binomial test .
finally we find that similarity based retrieval performs significantly better than random retrieval for type correctness p using a binomial test as well as for semantic correctness p .
by a binomial test16 .
table iii different levels of correctness in percent of ramcand random retrieval on the i ndustry dataset.
change type total approach format structure structure structure semantic count ramc .
.
.
.
.
random .
.
.
.
experiment rq to obtain a clear picture of the pros and cons of ramc baseline and random retrieval we independently report the accuracy of the correct concepts classes and the correct association.
we further split correct concepts in correct type same class in table iv and correct name see appendix for details .
we perform binomial tests our random baseline against ramcand chaaben et al.
to compare the effectiveness of our approach.
we found that in all cases ramcperforms significantly better than random which in turn performs even significantly better than baseline table iv .
table iv different levels of correctness of r amc random retrieval and b aseline on the r evision dataset.
approach same class same name same concept same assoc.
ramc .
.
.
.
random .
.
.
.
baseline .
.
.
.
p .
p .
experiment rq to better understand when and why the retrieval augmented generation succeeds or fails when completing software models we separate our analysis here in two parts successful completions and unsuccessful ones.
reoccurring patterns success several of the successful completions follow repeating completion patterns.
for example there is a move refactoring where a package declaration with type definitions is moved from one package to another package .
since this happened quite often in the past repository histories the correct new parent package could be deduced even though thispackage is not yet part of the incomplete test sample.
complex refactorings success furthermore more complex refactorings have also been be completed correctly for example a redesign of a whole part decomposition including packages andsysml block definitions has been correctly performed.
similarly we find correctly completed refactorings dealing with inheritance of port types .
16for semantic correctness we rely on the fact that the number of semantically correct samples is smaller than the number of type correct samples.
thus we are able to compute an upper bound for the p value using the type correct random retrieval samples.project specific concepts success even project specific concepts such as a special kind of tagging concept to mark software components as frozen are correctly inferred from the few shot examples or co changes of components are correctly identified likewise.
no memorization success we also observe correct handling of structure in non trivial cases.
for example correct combinations of source and target node ids are generated can not be observed in the few shot examples.
noise success we also observe that the language model is able to infer concepts among noise that is unrelated changes.
for example there are correctly completed instances of the add interface block andtype reference concept where similar fewshot samples are only present with lots of entangled changes.
regarding unsuccessful cases we observe two main reasons for failure incorrect structure and incorrect semantic.
structural conflicts failure for incorrect structure we find examples where conflicts occur because a node with the same node id is already present.
furthermore sometimes correct model elements or packages are added to the incorrect parent package in most cases we see a tendency of the llm to flatten hierarchies .
structure incorrect failure there are several instances where correct edge source and target node types are generated but their ids and consequently the structure is incorrect.
semantics wrong b c copy paste failure one cause for incorrect semantic completions is that parts of few shot samples are incorrectly copied and pasted.
this typically occurs when the llm lacks sufficient context to generate the correct completion leading it to mistakenly copy and paste segments from the provided examples.
semantics wrong b c unknown evolution missing context failure for example in the case of functional project specific evolution it might be hard to guess the right completion without further knowledge or the semantic retrieval might fail to retrieve instances of the correct change pattern.
interestingly in some of these cases the llm is guessing well but not perfect e.g.
added subsystem instead of external subsystem .
conceivable but unobserved evolution failure another interesting instance of incorrect semantic completion is a completion where a comment in german should be removed but instead a comment in english has been added.
in the project there were many renamings from german to english and in this case a future change has been correctly anticipated.
experiment rq to compare our retrieval augmented generation based to fine tuning we perform an analysis at the token level and we also compare the completions on a graphstructural and semantic level.
at the token level we find an average token accuracy of .
with a minimum of .
and a maximum of .
on our test data sets test ratio .
we can observe strong correlation of the average token accuracy with the number of fine tuning epochs.
also larger models perform better with respect to the average token accuracy.
regarding the repository properties we only find significant negative correlations with the perturbation probability.
that is more diverse repositories are typically harder for the modelcompletion using fine tuning.
exact numbers are given in our appendix.
when comparing the distributions of the edges removed in the simple change graph for incorrect and correct completions we see that the average number of removed edges for the incorrect i.e.
no exact match completions .
is significantly larger than the average number of removed edges for the correct ones .
.
similarly we find a significant relationship for the distributions of the total simple change graph size .89for the incorrect completions and .39for correct completions .
accuracies of the comparison of our approach to the fine tuning approach are given in table v. table v different levels of correctness in percent for fine tuned models compared to the retrieval based approach in multi edge software model completion on s ynthetic .
dataset method correct edge s exact match batch r amc .
.
text ada .
.
batch r amc .
.
text curie .
.
we conducted a mann whitney u test to compare the performance of retrieval augmented generation and the finetuned text curie andtext ada models from the gpt3 family.
in terms of producing at least one correct edge neither fine tuning nor retrieval augmented generation exhibit statistical significance in outperforming the other.
in terms of exact matches text ada p .
and text curie001 p outperform retrieval augmented generation.
regarding exact matches the impact of different sampling methods used in fine tuning and ramcbecomes substantial algorithms are provided in the appendix .
while ramcoften produces more edges than required the sampling procedure used with the fine tuning models is more conservative.
e. discussion overall we find that both ramcand fine tuning of llms are promising approaches for model completion and the general inference capabilities of llms are useful can handle noisy contexts and provide real time capabilities.
we will next discuss the results outline hypotheses for potential future research and describe threats to validity in section v f. rq in the experiment we observed promising correctness values across all datasets.
not only are more than of completions correct w.r.t.
the serialization format but we also find more than of semantically correct completions in the real world industrial setting .
this indicates that retrievalaugmented generation is a promising technique for model completion.
token processing times fall within the millisecond range and time required for semantic retrieval is negligible even for larger models.
the approach s real time capability is significant given the stepwise model completion use case.
rq in the retrieval augmented generation setting we do not find any significant relationship between the number of few shot samples and correctness.
we find that similaritybased retrieval boosts the correctness of the approach and thatit significantly performs better if a similar relevant change following a similar pattern is available in the context.
it also worthwhile mentioning that real world datasets are typically biased with respect to the change pattern and semantic retrieval can avoid sampling from large but irrelevant change pattern.
rq we have observed that in all instances where new elements with associations are recommended ramc consistently outperforms random retrieval and chaaben et al.
.
these results reinforce our findings from rq namely that leveraging llms with retrieval augmented generation represents a viable approach for model completion.
rq we have seen that our approach can be used to provide completions that are correct to a large extent for simple reoccurring patterns but also more complex refactorings.
even project specific concepts can be deduced from fewshot examples.
in many cases generated edges are also structurally correct.
the general inference capabilities of llms are useful for example in dealing with concepts for which there are few or no similar examples.
furthermore also with noise retrieval augmented generation often provides correct completions.
regarding usefulness of the completions our manual analysis reveals that many of the completions appear useful for the modeler.
for example ramcwas able to perform a translation of several german comments to english because the engineering language of the project has been changed.
furthermore ramcwas able to complete project specific refactorings.for a further investigation of these observations we formulate the following hypothesis.
hypothesis llms and retrieval augmented generation are able to handle noisy training examples leverage domain knowledge from pre training adapt to project specific concepts and provide useful software model completions.
we found completions that are incorrect from a structural viewpoint as well as incorrect from a semantic viewpoint.
as for structurally incorrect completions we identified cases where existing node ids are incorrectly reused where incorrect containment hierarchies would have been created or where completed edges are correct from a type perspective but do not connect the right nodes.
it is worth further investigating how these structural deficiencies could be overcome in particular given that llms are designed for sequential input not for graph inputs.
this leaves us with the following hypothesis.
hypothesis conceivable remedies for the structural deficiencies include fine tuning of llms combining graph neural networks designed for graph like input with llms providing multiple different graph serialization orders or a positional encoding that reflects the graph like nature of the simple change graph serializations.
regarding semantics we found incorrect completions that were related to a lack of domain knowledge in the pre trained model or the few shot examples respectively.
for example we found cases of functional evolution where the languagemodel is missing domain knowledge or requirements or cases of a refactoring without any relevant few shot sample.
we further identified cases where a conceivable completion has been generated but was not the one from the ground truth.
hypothesis conceivable remedies for the semantic deficiencies include strategies to further fuse the approach with context knowledge e.g.
fine tuning providing requirements or task context in the prompt leveraging other project data in repositories etc.
.
furthermore providing a list of recommendations may cure some identified deficiencies.
rq we found that a more fine tuning epochs are beneficial for the average token accuracy.
more diverse repositories increase the difficulty for the software model completion.
the larger the simple change graph and the more edges we omit for the completion the higher the probability of an incorrect completion.
the reason that fine tuning has a higher exact match accuracy is more due to the edge sampling algorithm than to the method itself when analyzing the percentage of correct edges it becomes clear that we cannot conclude that one approach outperforms the other.
instead we hypothesize a strong dependency on the edge sampling procedure which deserves further investigation.
while the retrieval augmented generation often generated more edges than necessary the sampling procedure used with the fine tuned models from the gpt family takes a more conservative approach prioritizing the generation of edges with high confidence.
comparison to code completion.
note that llms for source code completions show similar results to our findings in experiment and ranging from for perfect prediction of entire code blocks to for a few tokens in a single code statement .
drawing a direct comparison between code and model completion is not straightforward though.
f .
threats to validity with respect to construct validity we made several design choices that may not be able to leverage the entire potential of llms for software model completion including our definition of simple change graphs the serialization of the simple change graph the strategy of how to provide domain knowledge to the language model and the choice of the base llm.
to increase internal validity we incorporated the synthetic ecore dataset into our experiments controlling for properties of software model repositories.
still we were not always able to completely isolate every factor in our experiments.
for example fine tuning and few shot learning use different edge samplings.
this is due to the api that we used to access the language models.
in future research an ablation study for the design choices in the algorithms shall be performed.
to address the potential variability that llms may exhibit we checked and confirmed that the completions were stable.
regarding external validity we included two real world datasets repair vision andindustry and we study realworld change scenarios from the observed history in these repositories.
we have chosen our test samples to be small enough to perform manual semantic analysis but large enoughto draw conclusions.
to minimize costly manual checks our semantic analysis was confined to our most challenging dataset theindustry dataset.
extending the analysis to the other datasets would enhance validity.
however we are confident that having analyzed hundreds of samples we have struck a reasonable compromise.
we are therefore certain that our results have an acceptable degree of generalizability for the current state of research.
in any case user studies shall investigate the usefulness of our completions in practice.
investigating merits of llms for model completion is an emerging topic and many questions are open.
still our results set a lower bound for the potential of llms in this area with promising results insights and hypotheses for further research.
vi.
c onclusion we presented and investigated an approach to software model completion based on retrieval augmented generation ramc and compared it to fine tuning during our evaluation.
our experiments on a simulated a public open source ecore and an industrial sysmldataset for a train control software product line show that indeed llms are a promising technology for software model completion.
the real time capability of our approach is especially beneficial for stepwise model completion highlighting its practical utility.
we achieved a semantic correctness in a real world industry setting of .
which is comparable to earlier results with llms for source code completion.
further investigation revealed that similaritybased retrieval significantly enhances the correctness of model completions and that fine tuning is a viable alternative to retrieval augmented generation.
all in all the general inference capabilities of llms are beneficial particularly in dealing with concepts for which only scarce or even no analogous examples are provided.
we have identified concrete causes for the technology to fail and formulated corresponding hypotheses for future research.
of utmost importance for future research is to compare technology such as graph neural networks that has been designed for processing graph like data e.g.
our simple change graphs especially for structural aspects of software model completion.
also marrying approaches that are strong for structural aspects and llms that are typically strong for semantic aspects of model completion is worth further investigation.
vii.
d ata availability we provided all data excluding the industry dataset and the python code of our approach on a supplementary website.
we cannot include the industry dataset because it contains sensitive data including intellectual property of products on the market.
we provide r scripts and jupyter notebooks to replicate our statistical evaluation.