fidelity of cloud emulators the imitation game of testing cloud based software anna mazhar saad sher alam william x. zheng yinfang chen suman nath tianyin xu cornell university ithaca ny usa university of illinois urbana champaign urbana il usa microsoft research redmond wa usa abstract modern software projects have been increasingly using cloud services as important components.
the cloud based programming practice greatly simplifies software development by harvesting cloud benefits e.g.
high availability and elasticity .
however it imposes new challenges for software testing and analysis due to opaqueness of cloud backends and monetary cost of invoking cloud services for continuous integration and deployment.
as a result cloud emulators are developed for offline development and testing before online testing and deployment.
this paper presents a systematic analysis of cloud emulators from the perspective of cloud based software testing.
our goal is to understand the discrepancies introduced by cloud emulation with regard to software quality assurance and deployment safety and address inevitable gaps between emulated and real cloud services.
the analysis results are concerning.
among apis of five cloud services from azure and amazon web services a ws we detected discrepant behavior between the emulated and real services in of the apis.
these discrepancies lead to inconsistent testing results threatening deployment safety introducing false alarms and creating debuggability issues.
the root causes are diverse including accidental implementation defects and essential emulation challenges.
we discuss potential solutions and develop a practical mitigation technique to address discrepancies of cloud emulators for software testing.
i. i ntroduction modern software projects have been increasingly using cloud services as important components for storage database data processing etc.
such cloud based programming practice greatly simplifies software development by harvesting cloud benefits e.g.
high availability and elasticity and software deployment by reducing the cost of purchasing and managing large scale systems and infrastructures.
today all major cloud providers offer various services to support cloud based software and these cloud services are widely used .
despite its benefits cloud based programming imposes new challenges for software testing and analysis due to opaqueness of cloud backends and monetary cost of invoking cloud services during continuous integration and deployment ci cd .
first unlike other types of dependencies like libraries which are linked as a part of the software program cloud services areexternal to cloud based software invoked via rest api calls and their backend implementations are opaque.
it is hard to reason about the correctness of cloud based software independently especially its end to end behavior.
for example regressions of cloud backend implementations can directly affect dependent software that invokes corresponding apis.second testing cloud based software with real services can be costly especially with ci cd.
cloud services charge users based on the number of api invocations storage and additional features like transaction support .
so extensive testing on the cloud is expensive.
for example the test suite of orleans issues 120k azure api calls.
under ci cd tests are continuously invoked .
we expect even higher costs in the near future as cloud services are increasingly adopted by software projects and new tests are being added.
cloud emulators are developed for cloud based software development and testing before online testing and deployment.
nine out of ten projects we studied iii use emulators for ci tests.
we are also informed by a major cloud service provider that emulators are widely used by customers who use their service apis.
a cloud emulator offers local simulation of large complex cloud services.
for example a fault tolerant persistent key value storage service can be emulated by a centralized in memory hash table .
cloud emulators enable developers to conduct prompt cost efficient offline testing and debugging .
they are transparent to software under test using emulators requires no code change but a simple setup to connect to emulated services.
cloud emulators are typically developed or supported by cloud service providers.
for example microsoft provides emulators for azure services e.g.
azurite for azure storage services .
ideally emulators should behave the same as real cloud services so that software quality assurance like testing can rely on emulators.
however it is prohibitively difficult for emulators to achieve perfect fidelity considering the complexity scale and distributed nature of cloud services .
in practice emulators implement specifications of cloud service apis ii .
however as shown in our study v a specifications of today s cloud services and their apis are often incomplete and limited.
without formal enforcement of emulator compliance with real cloud service it is unclear how much fidelity today s emulators could realize.
we use the term discrepancies to refer to emulator behavior that deviates from specified behavior of cloud services.
we observed that discrepancies are constantly reported to affect testing of cloud based software .
in this paper we analyze the discrepancies between cloud emulators and cloud services to understand the fidelity of cloud emulation in practice and its impacts on cloud based software quality assurance and developer experience.
specifically we apply differential testing against two widely used cloud em emulatorcloud 400blob created successfullyclientdeployment safety violation201blockblob upload source url one of the http headers is not in the correct format emulatorcloud 200incompatible location constraintclientfalse alarms400putbucketinventoryconfiguration bucket id request accepted emulatorcloud 400exception while calling s3.copyobjectclientdebuggabilityissues500copyobject bucket copysource key invalid copy source object key root cause emulator azurite lacks url format validation for this api resulting in a successful response to an invalid request.root cause emulator localstack inaccurately simulates s3 s regional access rules causing error response to a valid request.
root cause emulator localstack fails to handle exceptions properly when invalid object keys are encountered.azure blobaws s3 aws s3 a deployment safety violation emulatorcloud 400blob created successfullyclientdeployment safety violation200blockblob upload source url one of the http headers is not in the correct format emulatorcloud 200incompatible location constraintclientfalse alarms400putbucketinventoryconfiguration bucket id request accepted emulatorcloud 400exception while calling s3.copyobjectclientdebuggabilityissues500copyobject bucket copysource key invalid copy source object key root cause emulator azurite lacks url format validation for this api resulting in a successful response to an invalid request.root cause emulator localstack inaccurately simulates s3 s regional access rules causing error response to a valid request.
root cause emulator localstack fails to handle exceptions properly when invalid object keys are encountered.azure blobaws s3 aws s3 b false alarm emulatorcloud 400blob created successfullyclientdeployment safety violation200blockblob upload source url one of the http headers is not in the correct format emulatorcloud 200incompatible location constraintclientfalse alarms400putbucketinventoryconfiguration bucket id request accepted emulatorcloud 400exception while calling s3.copyobjectclientdebuggabilityissues500copyobject bucket copysource key invalid copy source object key root cause emulator azurite lacks url format validation for this api resulting in a successful response to an invalid request.root cause emulator localstack inaccurately simulates s3 s regional access rules causing error response to a valid request.
root cause emulator localstack fails to handle exceptions properly when invalid object keys are encountered.azure blobaws s3 aws s3 c debuggability issue fig.
implications of discrepancies between cloud emulators and cloud services with regards to software testing.
ulators azurite for azure storage services including blob table and queue and localstack for amazon web services including s3 and dynamodb .
we record discrepant behavior between the cloud emulator and the cloud services and analyze the root cause of each discrepancy.
we focus on basic functional correctness instead of performance or fault tolerance e.g.
data consistency and crash consistency which are beyond the expectation of local emulation.
our analysis results are concerning.
among apis of five cloud services from azure and amazon web services aws we detected discrepant behavior between the emulated and real services in of the apis.
these discrepancies have profound implications on deployment safety and developer experience code that passes tests with emulators may fail in production when cloud services are used test failures with emulators can be false alarms and debugging with emulators can be hard due to discrepant feedback e.g.
error code and messages .
figure shows three examples we discovered in our analysis.
we further analyze ten open source cloud based software projects five of them are affected by discrepancies some of their tests have inconsistent results when running on the cloud emulator versus the cloud services.
in one project durabletask of the tests are affected.
the root causes of discrepancies are diverse but can be categorized into incompleteness of existing specifications unspecified behavior and implementation defects such as bugs and missing features .
while these root causes reflect essential software engineering challenges we believe that many discrepancies could be addressed by more comprehensive testing and more systematic specification.
we discuss potential solutions and mitigations ranging from practical formal methods to new system level support vi .
we explore hybrid cloud emulator testing as a short term mitigation and develop a simple tool named e tto selectively run tests on emulators versus cloud services based on whether the test invokes discrepant apis vii .
e toffers different policies depending on whether discrepant api information is known as apriori or being done via in situ analysis.
through et we show that hybrid testing yields considerable cost savings compared to running all tests with cloud services.
the paper makes the following main contributions a discussion on the challenges of testing cloud based software and the discrepancies introduced by emulators.
a systematic analysis of discrepancies between cloud ser vices and their emulators including their characteristics root causes and impacts on software testing.
a discussion on solutions to discrepancies and a mitigation tool to selectively run tests with emulators.
we reported bugs that caused the discrepancies so far six have been confirmed and five have been fixed.
research artifact ii.
b ackground a. cloud services and their apis modern cloud services are programmatically accessed via rest apis defined as follows on top of http s .
definition rest api .
in this paper we defined a rest api as an http method plus the resource.
for example get blog posts id and put blog posts id are considered two unique rest apis of a web blogpost service.
a service typically exposes several tens of rest apis.
for example aws s3 exposes rest apis and azure blob service exposes rest apis .
to ease developer programming cloud services provide software development kits sdks with high level language specific library apis.
typically sdk apis wrap raw rest apis and for a service more sdk apis are built on top of the rest apis.
most existing cloud based applications invoke sdk apis to interact with the cloud services instead of calling raw rest apis.
api specification the rest apis are commonly described using specification languages such as openapi specification .
the specification describes the api version request uri content type input parameter output format error code and messages etc.
the api specifications are used by cloud emulators ii b to develop emulated apis.
we find that the api specifications are often incomplete.
for example the openapi specification of azure blob services only specifies value constraints including data types for of parameters across all azure blob apis.
besides all the specifications are on data type value range and default value with no behavior semantics e.g.
async or not .
sdks often include additional checks on parameter values of api calls over the api specifications values that satisfy the api specification could be rejected by the sdk checks.
1we follow the rest api definition used in azurite .
this definition can be inconsistent with other definitions e.g.
an api in our definition is referred to as an api operation in .
we choose this definition because it resembles sdk apis faced by developers e.g.
read a blog post and create a new blog post are corresponding to two different sdk apis .
pricing cloud services are expensive.
despite different pricing models of cloud services pricing typically depends on the amount of data to be stored and the cost of operations.
take azure blob service as an example.
the price for 100tb month ranges from depending on the access tiers .
azure blob service then charges for read write iterative read and iterative write operations separately .
for example the price for write operations varies from .
.
per writes depending on the tier.
other features such as redundancy further increase the cost.
with the current pricing model testing cloud based software incurs non trivial monetary costs.
to demonstrate the cost we run the tests of orleans a cloud based software project for azure storage services with standard configuration.
orleans has tests that issue 120k azure api calls over unique apis.
we run these tests times which costs .
us dollars we expect times to be a reasonable time in ci cd of large software projects .
b. cloud emulator to reduce cost and get prompt feedback emulators are developed to assist developers in offline development and testing.
emulators can also be used for debugging production problems.
emulators run as local daemons that simulate cloud services.
cloud based software programs transparently interact with the emulator in the same way they interact with cloud services.
using an emulator only needs a simple configuration that switches the connection from a cloud handle to localhost listened to by the emulator no code change is needed.
most cloud services provide developers with official emulators.
for example microsoft provides emulators for azure storage and cosmosdb and aws provides emulators for dynamodb and step functions.
moreover third party emulators are developed.
one successful example is localstack which emulates many aws services such as s3 and dynamodb.
compared with official emulators localstack provides a more usable integrated development environment .
our study deliberately selects an official emulator azurite and a third party emulator localstack .
for compliance with the target cloud services cloud emulators are commonly built on top of api specifications.
for example azurite uses autorest to generate stub code from the openapi specification of azure storage services .
localstack employs weekly github action checks to detect any changes of the api specifications of aws .
c. emulation versus mocking mocking is a common practice used in unit tests to simulate dependencies of code under test .
unlike emulators mocked objects are not required to rigorously satisfy api specifications because mock uses behavior verification .
for this reason mock cannot help with state verification.
emulation is fundamentally different from mocking.
an emulator is expected to conform to api specifications of the cloud services for proprietary services api specifications are the contract .
the emulator is designed as a drop in replacementtable i emulators and cloud services studied in this paper only aws services studied in this paper are listed .
emulator service loc commits developer azurite blob queue table 591k official localstack s3 dynamodb 449k third party for the actual service so developers can move from testing to deployment by simply changing a connection string.
emulators are closer to fakes than mocks.
an emulator must maintain states so producer consumer dependencies are expected to be the same.
the emulator does not have to behave exactly the same or maintain the same internal states as the actual service beyond api specifications.
without high fidelity emulators cloud based software developers can only run tests with the actual services at certain time points during ci cd.
however this creates a difficult trade off between cost and effectiveness.
running tests against actual services frequently may incur high costs.
infrequent testing with actual services leads to big bundles of commits affecting ci cd effectiveness and reducing developer experience.
therefore emulator fidelity is important.
iii.
m ethodology we use differential testing to discover discrepancies between cloud emulators and real cloud services.
basically we issue the same rest api calls to the emulated service and the cloud service independently and check the resulting behavior including the return values error codes or messages if any and states of key data objects such as blobs and containers.
any inconsistent behavior indicates a discrepancy.
studied emulators.
we select two cloud service providers with the highest market share microsoft azure and amazon web services aws .
for these two providers we choose to study the most commonly used emulators azurite and localstack they represent state of the art.
moreover azurite represents the official emulators provided by cloud service providers while localstack represents third party emulators developed by companies of cloud based integrated programming environments.
importantly both emulators are open sourced which enables us to debug discovered discrepancies.
table i lists the information of the two emulators.
studied services.
for the two emulators we select five widely used cloud services blob queue and table services from the azure storage services and s3 and dynamodb from aws.
azurite only supports azure storage services blob queue and table for localstack we pick dynamodb and s3 as popular and commonly used aws services.
test workloads.
we use two complementary test workloads.
first we leverage api fuzzing to generate sequences of rest api calls.
each api call sequence is a test workload and the workloads collectively cover all the rest apis provided by the target cloud services.
the api fuzzing workloads help us understand the discrepancies in each rest api and characterize a broad range of apis.
3the fuzzing is done against sdk apis not raw rest apis.
we in fact started from rest api fuzzing using restler .
however we found that certain discrepancies are not possible if the software under test uses sdks which have additional checks ii .
in practice developers do not commonly craft rest api calls directly but mostly call sdk apis ii a .
since our goal is to understand discrepancies in the context of software development rather than security analysis we choose to fuzz sdk apis.
basically we focus on analyzing discrepancies faced by cloud based software developers.
we also use the test suites of existing cloud based software projects as the test workloads.
many tests invoke cloud service apis.
these tests help understand the impact of discrepancies on testing real world software projects which is complementary to fuzzing from the api perspective.
fuzzing sdk apis.
we implemented a grammar aware api fuzzer to generate diverse sdk api calls as test workloads.
we start from default or predefined parameter values for each sdk api and the fuzzer mutates parameter values based on value constraints defined in openapi specifications of rest apis the grammar .
to do so we establish the mapping from the parameters of rest apis to those of the corresponding sdk apis the mapping process is straightforward because sdk apis are mostly wrappers over raw rest apis.
the grammar based mutation ensures that generated sdk api calls are mostly valid and can reach emulated or real cloud services.
our fuzzer implements the fuzzing approach of restler inferring producer consumer dependencies among request types e.g.
api yshould be called after api x because ytakes as an input a resource id produced by x and taking dynamic feedback from responses during testing e.g.
learning that a api ycalled after a sequence x yis refused and avoiding this combination in the future .
we monitor the response of each api call.
if inconsistent responses are returned by the emulator and the cloud services including both http response status code like and as well as error code and message if the response returns an error we capture and record the discrepancy and abort the test.
otherwise we progress to the next api call in the generated sequence.
we also check the key data objects before and after the api calls e.g.
the number of blobs for azure blob services to capture discrepancies with no immediately observable manifestation such as resource leaks iv b .
those checks are service specific.
using existing tests.
to understand the impact of discrepancies on real world software projects we perform differential testing using test suites of existing projects.
we select ten open source projects table ii that use the studied cloud services.
the ten projects are selected because they are mature and widely used based on the total number of commits and star counts developed by reliable sources such as companies like microsoft orleans durabletask nuget insights and petabridge alpakka and are actively maintained and use recent versions of the studied cloud service apis.
in our study we select tests that interact with the cloudtable ii cloud based software projects.
tests refers to tests that invoke cloud services apis refers to unique apis.
project services loc tests apis alpakka queue .5k attachmentplugin blob .9k durabletask blob queue table .0k identityazuretable table .7k insights blob queue table .8k ironpigeon blob .8k orleans all services but s3 .8k servicestack dynamodb s3 .2k sleet blob s3 .2k streamstone table .6k services.
the selection is done by monitoring the http traffic of each test in a reference run using the emulator.
we check whether a test outputs inconsistent results when running with emulators versus cloud services.
table ii shows the number of tests that invoke cloud services and the number of unique apis invoked by the test suite of each project.
discrepancy analysis process.
we inspect every observed discrepancy during the aforementioned testing.
a discrepancy is recorded if it manifests via inconsistent test results the results of a test are different when running with the emulator versus the real services.
for each discrepancy we verify it by deterministically reproducing its manifestation and impact and debug it to localize the root cause in emulator source code.
the process helped us minimize human errors during the analysis and subjectiveness in the interpretation and categorization.
iv.
d iscrepancy characteristics a. prevalence of discrepancies our analysis shows that discrepancies are prevalent in the two cloud emulators azurite and localstack .
the five cloud services we studied expose a total of apis.
among these apis our api fuzzer iii discovered discrepancies in .
table iii shows the number of discrepant apis of each service.
we define a discrepant api as follows definition discrepant api .
an api is discrepant if it can expose inconsistent behavior in the scope of its specification when invoked on the emulator versus on the real cloud service.
both azurite and localstack have a considerable percentage of discrepant apis among all the apis they support and across the services showing that discrepancies are not specific to one emulator implementation or specific to apis of a particular service.
rather emulator fidelity is a common challenge.
these discrepancies have different implications as exemplified in figure including deployment safety violations 1a false alarms 1b and debuggability issues 1c .
table iv categorizes the implications of the total discrepancies discovered in the discrepant apis one discrepancy can have different implications .
the results show diverse implications of these discrepancies.
we measure their impacts on real world test cases in iv c. surprisingly we find that of the discrepant apis are certified by the emulators and considered fully supported.
4table iii discrepant apis with respect to the cloud services services emulator total apis discrepant apis azure blob azurite azure table azurite azure queue azurite aws s3 localstack aws dynamodb localstack total table iv impacts of discrepancies across emulators.
impact azurite azure localstack a ws total deployment safety false alarms debuggability issues total localstack adopts five methods to certify emulated apis including both internal and external integration tests e.g.
snapshot tests .
despite extensive efforts out of discrepant apis from localstack are certified by all five testing methods while out of discrepant apis are certified with at least one test method.
similarly out of discrepant apis from azurite are certified to be fully supported by azurite .
the results show the challenges faced by existing testing based practices in detecting discrepancies.
finding .
discrepancies between modern cloud emulators and real cloud services are prevalent discovered in of apis on average and even in certified apis .
the implications of discrepancies are diverse including unsafe deployment false alarms and debuggability issues.
b. discrepancy manifestations a notable observation is that discrepancies are manifested through not only inconsistent responses to the api calls but also inconsistent remote cloud side states.
the latter creates significant challenges to observe and understand discrepancies especially with short running test cases.
we implemented domain specific checks to compare the remote states maintained by the emulators and the corresponding cloud services iii .
for example we check the states of each container maintained by the emulators and cloud services before and after each container related api call.
seven discrepancies have the same response to api calls but create inconsistent remote states.
for example when invoking an azure blob api container restore to recover an early deleted container both azurite and the azure blob service return the same response however the blob service faithfully restores the deleted container while azurite creates a new empty container.
such discrepancies may not be easy to capture without fine grained checks.
we also find discrepancies that cause inconsistent responses and inconsistent remote states.
for example when calling a blob api blockblob stageblockfromurl with an invalid url azurite succeeds by creating a new blob while the blob service fails with invalidheadervalue .table v software tests that are affected by discrepancies.
project discrepant discrepant impact results tests safety vio.
false alarms alpakka durabletask orleans servicestack streamstone total table delete testsblockblob upload testsqueue delete testscontainer delete testsblob acquirelease tests62 apis are not used by any tests fig.
popularity of azure storage apis measured by the number of tests that use an api across all the studied projects.
finding .
discrepancies that only manifested via inconsistent remote states are hard to observe fine grained state checks are needed to capture those silent discrepancies.
c. impact on real world tests we measure the impacts of discrepancies on real world test suites of cloud based software projects table ii .
the impact is reflected by inconsistent test results when running the same test with the emulator versus the cloud service.
we define discrepant tests as follows definition discrepant test .
a test is a discrepant test if it invokes any discrepant apis.
note that a discrepant test may or may not output discrepant test results depending on whether api call statements are executed and whether discrepancyinducing parameters are used during the test execution.
among the ten projects we evaluated table ii we discovered discrepant test results in of them five projects as shown in table v. different projects are affected at different levels ranging from to of tests that invoke cloud services.
the variation is attributed to the usage characteristics of the cloud service apis.
specifically though we discovered a large number of discrepant apis iv a not all these apis are equally invoked by the test cases.
figure depicts the popularity of all azure storage apis in total invoked by all the tests of the studied projects where discrepant apis are marked in red.
popularity is measured by the number of tests that use the api.
among the apis only of them are invoked by at least one test.
only apis out of involved in the tests are discrepant while discrepant apis in total are discovered for azure see table iii .
note that the number of discrepant tests is much larger than the number of discrepant test results manifested table iii .
5the reason is that many discrepant tests are only manifested when certain parameter values are used.
the results have two important implications.
first addressing discrepancies can leverage api usage characteristics in the field to prioritize widely used apis.
oftentimes fixing discrepancies of a few apis can eliminate a large number of discrepant tests or test results.
we take all the tests using azure services as the example by resolving the top five discrepant azure apis in figure discrepant tests drop by from to and discrepant results drop by from to .
the small drop in discrepant results is caused by tests in durabletask utilizing multiple discrepant apis.
if the top seven discrepant azure apis are resolved out of the discrepant results caused by durabletask will be eliminated.
second fine grained parameter level analysis can further capture discrepancies.
although our analysis stays at the api level instead of parameters we build on these implications when designing mitigation solutions vii .
finding .
five out of ten studied software projects reveal discrepant test results caused by discrepancies between emulators and cloud services.
those discrepant tests are caused by a small set of discrepant apis.
not all discrepant apis manifest during testing if triggering parameters are not used.
we further categorize the implications of discrepant tests into deployment safety violations 1a and false alarms 1b as broken down in table v. debuggability issues are not applicable here as the tests all pass in the default setup.
the majority of test discrepancies would lead to deployment safety violations the test that passes with the emulator would fail when running with the cloud service i.e.
passing the test provides no safety guarantee on the cloud .
for example a testcreatetaskhub in durabletask uses the azure blob api container create to re create a previously deleted blob container.
this test fails when running with the cloud service due todurabletaskstorageexception the specified container is being deleted try operation later because container deletion is asynchronous and provides no guarantee for the time to finish.
however this test always passes when running with the azurite emulator as azurite always deletes the container synchronously before the api returns.
false alarms are relatively less common than deployment safety issues table v .
two out of four false alarms are caused by brittle assertions on the error messages returned by the api calls which are discrepant between the emulator and the cloud service .
such discrepancies can be addressed by enforcing the consistencies of the error messages.
one false alarm is caused by a flaky test the non deterministic flaky behavior only manifests when running with the emulator not with the cloud service due to order differences caused by discrepant timing of api calls.
we fixed the flaky tests by adding await to enforce the order.
the last false alarm is caused by resource discrepancy the stress test in orleans exhausted the socket limit of localstack which passes with the cloud service .
such resource discrepancies are essential table vi root causes of observed discrepancies.
service incomplete spec.
unspecified defects in impl.
azure blob .
.
.
azure queue .
.
.
azure table .
.
.
aws s3 .
.
.
aws dynamodb .
.
.
and stress tests should not use emulators in the first place.
finding .
deployment safety violations are the major implications of discrepant tests while false alarms also appear in testing results.
tests of cloud based software projects need to carefully decide to run on emulators versus cloud services.
v. r oot cause analysis we discuss the discrepancies from the specification perspective.
conceptually both the emulator and the cloud services are implementations of the api specification in practice emulators implement the api specifications defined by the cloud services .
so discrepancies are the result of defects in either specification or implementation.
based on the existing api specifications ii we categorize the discrepancies into incomplete specification unspecified behavior that is not considered in existing specifications and implementation defects in the emulators or the cloud services.
table vi shows the three categories of discrepancy root causes.
during the project we detected ten bugs in the two studied emulators of which six have been confirmed and five fixed .
we also detected two bugs in the cloud backend implementations which have been reported to the cloud service providers.
a. incomplete specifications as discussed in ii existing cloud service api specification focuses on parameter value constraints and error codes and messages from which emulators automatically generate stub code that adheres to the specifications e.g.
using autorest .
however we still find that a significant percentage of discrepancies are caused by inconsistent validity checks of parameter values as well as inconsistent error code and messages.
the reason is incomplete specifications.
table vi shows that incomplete specifications can cause up to .
of the discovered discrepancies in a service.
parameter value constraint ideally the api specification should define allthe value constraints of every input parameter.
in reality api specifications are deficient.
we observe discrepant value constraint checks in twelve out of discrepant azure storage apis and ten out of aws s3 apis.
we observe no such discrepancy in dynamodb.
figure 3a shows such an example from azure blob where the value of the x ms proposed lease id parameter of the blob changelease api should be in the guid format .
the blob service implements a format check while azurite does not.
as a result an invalid api call of blob changelease will be returned successfully by the emulator but rejected by 6invalidheadervalueemulatorcloud oklease id should be guid formatno check for lease idclientdeployment safety violationblob changelease400 emulatorcloud 400clientdebuggability issuesbad request the number of queue msg should be within to empty messagemessages peekoutofrangequeryparametervalue 400azure blob azure queue a parameter value constraints invalidheadervalueemulatorcloud oklease id should be guid formatno check for lease idclientdeployment safety violationblob changelease400 emulatorcloud 400clientdebuggability issuesbad request the number of queue msg should be within to empty messagemessages peekoutofrangequeryparametervalue 400azure blob azure queue b error response fig.
discrepancies caused by deficient specifications the cloud.
however such value constraint is not specified in the openapi specification of azure blob services.
in another common discrepancy case across azure and aws cloud service apis require authorization to private resources or sensitive operations e.g.
security configuration like putbucketacl .
in its absence these requests are denied by the cloud service.
however our results revealed that emulators often overlook this constraint accepting such requests with a ok response resulting in of discrepancies.
we find that the requirement of authentication is commonly included in the text descriptions which is not machine checkable.
although this is not a defect as far as openapi is concerned it is not enforced by auto generated stub code.
from a codegen machine checkability perspective the specification is incomplete.
our experience of examining azure and aws openapi specifications shows that text based api descriptions often includes constraints that are not machine checkable.
error response we also find that specifications can be incomplete in the expected error code and messages and fail to associate them with the apis leaving emulator developers to interpret discrepant error messages.
figure 3b shows such an example.
when a request is made with an out of range value for the numofmessages parameter the azure queue service provides a detailed message pinpointing the error.
in contrast azurite only responds with a bad request error code offering no specific guidance and impeding debuggability.
discrepant error responses were particularly prominent in azure storage apis accounting for out of of the total discrepancies.
when we examined the azure api specifications we found that the error codes were not associated with the apis but were defined in a separate list.
differently we found that aws specifications have a more structured approach to error code definitions which were also part of the related api definitions.
the latter directly translates to the emulator code.
in dynamodb api specifications we found structured definitions of unique error codes including their error messages exception flags and documentation.
hence discrepant error responses are rare in dynamodb and s3.
environment dynamics409cloud acceptedemulatorclient deployment safety violationroot causeimpactmanifestationdeletecreate the specified container is being deleted.
try operation later.
container is deleted instantly.clientdeletecreate201 createdcontainerbeingdeleted202 acceptedazure blob fig.
discrepancies caused by unspecified behavior techniques for generating and enforcing machine checkable api specifications may potentially close the gaps.
recent work demonstrates the potential of synthesizing formal specifications from text .
techniques that infer specifications from code can help differential analysis of api specifications from emulator and service code to find discrepancies.
certainly api is a form of abstraction.
so overly detailed specifications that describe internal behavior could be considered leaky abstractions.
a key challenge is to define the right level of specifications as an effective api contract.
finding .
the completeness of machine checkable specifications is still a fundamental challenge even for simple specifications such as parameter value constraints and error code.
without an effective way towards comprehensive specifications we expect such discrepancies to remain prevalent.
b. unspecified behavior a few discrepancies were caused by api behavior out of the scope of the existing specification and thus is unspecified.
we observed two patterns of unspecified behavior discrepancies.
we mentioned the first pattern in iv c whether an api is synchronous or asynchronous.
for example azure blob s api container delete which deletes container resources in cloud services is an asynchronous api.
for efficiency consideration the deletion is not guaranteed to finish before the api returns.
instead the time to finish the deletion depends on the amount of resources to be deleted.
conversely emulators always finish deletions before returning the api calls.
figure depicts such discrepancies.
the result is that api sequences involving creating a container deleting it and then attempting to recreate it with the same name yielded different results the cloud service returned containerbeingdeleted while the emulator allowed immediate container recreation with created .
this pattern also appears in sequences following a deletion api call the emulator would return a not found after deletion while the cloud busy the deletion would non deterministically depending on timing issue a success response or a conflict message the specified container is being deleted.
try operation later.
the second pattern is unspecified api behavior on null