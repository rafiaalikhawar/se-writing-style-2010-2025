magika ai powered content type detection yanick fratantonio googleluca invernizzi googleloua farah googlekurt thomas googlemarina zhang google ange albertini googlefrancois galilee googlegiancarlo metitieri google julien cretin googlealex petit bianco googledavid tao googleelie bursztein google abstract the task of content type detection which entails identifying the data encoded in an arbitrary byte sequence is critical for operating systems development reverse engineering environments and a variety of security applications.
in this paper we introduce m agika a novel ai powered content type detection tool.
under the hood m agika employs a deep learning model that can execute on a single cpu with just 1mb of memory to store the model s weights.
we show that m agika achieves an average f1 score of across over a hundred content types and a test set of more than 1m files outperforming all existing content type detection tools today.
in order to foster adoption and improvements we open source m agika under an apache license on github and will make our model and training pipeline publicly available.
our tool has already seen adoption by the gmail email provider for attachment scanning and it has been integrated with virustotal to aid with malware analysis.
we note that this paper discusses the first iteration of magika and a more recent version already supports more than content types.
the interested reader can see the latest development on the m agika github repository available at github.com google magika.
i. i ntroduction content type detection is a fundamental computing task that identifies the data encoded in an arbitrary byte sequence.
this allows an application to distinguish source code c python etc.
media pdf jpg etc.
binaries exe elf and a variety of other file formats.
as such content type detection impacts a wide range of downstream use cases including software development tools security tools browsers and media players.
for example development environments e.g.
visual studio code vs code for short rely on contenttype detection to decide which syntax highlighters and plugins to use.
security applications rely on content type detection for policy enforcement e.g.
email providers prohibiting executable attachments for forensic analysis and recovery and for routing samples to the most capable content type specific threat analysis scanners e.g.
virustotal and other anti viruses have specialized scanners for binaries scripts or pdfs these scanners are too resource intensive to be run on all samples .
the need for content type detection stems from byte sequences lacking an intrinsic trustworthy indicator of their underlying file format.
while some approaches to built in indicators exist such as file extensions mime types magic let function const m yheading .quer yselect or h setusername m yname please ent er y our name.
!m yname setusername .set it em name m yname m yheading.t e xtcont ent m yname is cool documentpr ompt if else localst or age bundle st art time hi .log hi functionconsole bundle st art time hi .log hi functionconsolelibmagicmagika javascriptascii textascii textjavascriptjavascriptjavascriptspaces have been removedfig.
example of the frailty of signature based content type detection when applied to distinct code snippets taken from javascript basics of the mozilla s mdn .
file which relies on regular expressions for content type detection imprecisely labels each snippet as ascii text unless the spaces around the sign are removed.
as we show our proposed content type detector m agika overcomes these robustness limitations.
bytes and metadata they can easily be omitted when a file s content is copied or transmitted or otherwise spoofed e.g.
to evade security systems that prohibit certain content types .
the task of content type detection was first tackled with thefile command in bell labs unix over five decades ago .
since then developers have created a variety of tools the vast majority of which rely on manually written signatures e.g.
rules or regular expressions that are updated as new file types and versions emerge.
examples include the latest version offile powered by libmagic exiftool andtrid .
unfortunately signature based approaches are frail a single whitespace change or other subtle byte sequence modification can result in inaccurate content type detection as shown in figure .
beyond signatures guesslang employs a simple tensorflow text model to distinguish source code content types which is integrated into vs code .
in this paper we discuss the design and implementation of a novel ai powered content type detection tool called m agika .
built using a deep learning model m agika automatically infers the content type of a byte sequence without any reliance on human expertise with respect to the intricacies of different file formats.
the model takes as input three sequences of bytes drawn from the beginning middle and end of a file s content automatically identifies patterns unique to contentarxiv .13768v1 sep 2024types and outputs the most probable content type.
we trained magika to identify canonical content types using 24m file samples sourced from github and virustotal .
we find that m agika achieves an average f1 score of on a holdout test dataset of .2m samples.
for comparison we benchmark m agika against four existing tools file exiftool trid and guesslang to compare their accuracy speed and overall resource usage using the same test dataset of .2m samples.
we find that magika outperforms every existing tool with a f1 gain over the best tool for binary content types and a f1 gain over the best tool for text content types and f1 gain overall.
the f1 gain increases respectively to and when considering all content types in our benchmark instead of just those supported by existing tools.
for bulk inferences m agika requires .77ms to yield a decision per file outperforming all other existing tools except for file .75ms .
likewise m agika requires only 1mb of memory to represent model weights and achieves the aforementioned performance with just a single cpu no gpu is required.
in order to foster wider adoption and accelerate improvements we open source m agika under an apache license on github .
upon release we reached 4k stars on github in less than a week we currently total more than .7k stars.
in terms of real world deployments m agika has already been integrated within gmail and google drive to predict the content type for hundreds of billions of files every week .
magika has also been integrated with virustotal to assist with malware analysis and we are in discussions to integrate magika with vs code as a replacement for guesslang .
this positive reception reflects the real world improvements magika provides over existing software engineering toolsets.
ii.
r elated work content type detection is a well studied longstanding problem.
before presenting our new approach we discuss both traditional approaches used by existing tools as well as research proposals to integrate machine learning into contenttype detection.
a. traditional approaches to content type detection the conventional approach to content type detection relies on manually crafted signatures e.g.
regular expressions .
the most prominent tool employing signatures is file which is regarded as the default command line utility for detecting content types.
file provides support for binary file formats which are particularly amenable to a signaturesbased approach including highly specialized formats e.g.
file formats employed in specific video games for storing saved games textures etc.
.
file also possesses a range of signatures for textual content types e.g.
c java python .
anecdotal evidence suggests that file exhibits higher accuracy for binary formats than textual ones but its performance has not been systematically evaluated prior to our study.
by default file outputs a textual description of the detected content type and a number of additional metadata.for example when processing a pdf file file will output its content type pdf but also the pdf version number how many pages it has and so forth.
since this output is challenging to parse programmatically file also supports outputting a mime type.
mime types are easier to parse and more codified than human descriptions and thus it may be more appropriate when integrating file in automated pipelines.
however dealing with mime types has its set of challenges which we document in section iv.
other popular tools include trid and exiftool .
the latter was originally designed to detect image content types exclusively but it has since expanded its scope.
apart from these general purpose content type detection tools there are a number of specialized tools that trade breadth for depth on a specific domain.
for instance peid is a tool dedicated to detecting pe packers whereas detect it easy facilitates fine grained inspection of pe files and other executable file formats.
we omit these special purpose tools from our comparative analysis due to their limited support of the content types we evaluate with magika .
lastly numerous libraries re implement a subset of existing features to offer content type detection to various programming languages often aiming to be free of dependencies to simplify integration.
examples include mime types a library specializing in a limited number of content types particularly for javascript clients polyfile a pure python implementation of libmagic .filetype a golang library for file type identification.
filetype.py a python library for file type identification and file type another binary file format detection tool for the javascript ecosystem.
given the derivative nature of many of these tools we omit them from our comparative evaluation as well.
b. machine learning approaches to content type detection recently researchers have proposed a number of approaches that leverage machine learning to replace manual signatures.
sceadan proposes using support vector machines to model features based on unigram and bigram frequencies.
fitzgerald et al.
suggest natural language processing techniques for file fragment classification whereas wang et al.
propose using sparse coding and unsupervised learning to classify file fragments in the context of memory forensics.
more recent approaches employ appropriately state of theart techniques such as recurrent and convolutional neural networks .
these proposals have not found wide adoption possibly due to their relatively low overall accuracy ranging from to limited support for different content types between and primarily binary file formats and competition from existing tools which already provide adequate support for binary file formats.
as such we treat them as out of scope for our comparative evaluation.
the most recent and successful approach is guesslang an open source tool that detects programming languages from a snippet of source code.guesslang focuses exclusively on textual content types and employs a wide deep tensorflow model for the detection task .
notably vs code utilizes guesslang to infer the programming language when a user creates a new file without specifying a file extension .
while the original guesslang is currently unmaintained last commit in september and relies on deprecated tensorflow abstractions vs code developers maintain a node.js client that facilitates the testing of the underlying model .
iii.
d ataset we curate a novel dataset of 26m files drawn from a diverse set of content types to act both as a benchmark for evaluating existing content type detectors and to train our deep learning content type detector.
we describe the origin of this data our validation steps and limitations with our approach.
dataset sources.
in real world settings the distribution of content types varies from environment to environment.
for example social media uploads are dominated by videos and images whereas digital signing platforms predominantly receive pdfs.
rather than tailor our dataset to any single environment we incorporate a sample of content types that may be present across a variety of environments including source code executables documents media archives and more.
we build a stratified dataset in which every type has equal representation.
when reporting performance metrics throughout this work we break our results down per content type to account for this sampling strategy.
in practice any deployment environment can estimate the efficacy of m agika by taking a weighted average of our reported results with weighting taking into account the frequency of content types within a specific setting.
to gather our stratified samples we identified github a popular source code development platform and virustotal a popular binary and file scanning platform as two promising sources as each covers a complementary set of content types github includes mostly text based files such as source code e.g.
c c java python configuration files e.g.
json yaml ini and a variety of text files for documentation e.g.
markdown rtf .
conversely virustotal includes file archives e.g.
zip tar binaries e.g.
exe dll elf as well as documents e.g.
pdf doc xls .
content types.
we selected prevalent content types based on available public metrics.
using a public mirror of github designed for large scale analysis and virustotal s public api we first calculate for each the top most prominent file extensions across both platforms.
we then identified the associated content type if any for each of these file extensions.1given the potential biases of our data sources due to which potentially relevant content types do not appear in the most frequent file extensions we augment this list by consulting existing resources and adding file 1not all file extensions are associated to a content type for example a significant portion of samples on virustotal have the .file or.virus extension which is not indicative of any content type.ai apk appleplist asm asp batch bmp bzip c cab cat chm coff crx cs css csv deb dex dmg doc docx elf emf eml epub flac gif go gzip hlp html ico ini internetshortcut iso jar java javabytecode javascript jpeg json latex lisp lnk m3u macho makefile markdown mht mp3 mp4 mscompress msi mum odex odp ods odt ogg outlook pcap pdf pebin pem perl php png postscript powershell ppt pptx python pythonbytecode rar rdf rpm rst rtf ruby rust scala sevenzip shell smali sql squashfs svg swf symlinktext tar tga tiff torrent ttf txt unknown vba wav webm webp winregistry wmf xar xls xlsb xlsx xml xpi xz yaml zip zlibstream fig.
list of content types in our dataset.
extensions if at least one of the authors believed to be critical to make a first release of practical utility.
in total we consider file extensions in our study.
as a simplification we manually group some extensions into a single category.
for example we group both jpeg and jpg into a jpeg content type.
similarly we group both exe and dll into a pebin content type.
as a notation mechanic we format these canonical content types as type throughout this paper whereas we refer to file extensions as type.
this process reduces our final set of categories from file extensions to canonical content types.
of these are binary content types and are text content types.
figure shows the full list of selected content types.
we consider our selection of content types to be sufficiently large and diverse to demonstrate the generalizability of our approach.
we leave the extension of our approach to even more content types to future work with the open source community.
sampling validating content types.
for each content type we query github and virustotal to obtain a random sample of files associated with the content type s file extensions.
to minimize the risk of mislabeled content types in our ground truth we perform a number of validation checks before adding a sample to our dataset.
we specifically avoid using existing content type detection tools as part of our validation otherwise we might oversimplify content type detection in the case of requiring agreement across all tools or propagate detection errors in the case of trusting one tool above others .
we use four heuristics for validation file size magic bytes for binary files character encoding for text files and file trustworthiness.
for file size we require any sample in our dataset to consist of at least bytes.
for magic bytes we apply a set of necessary but not sufficient rules to validate a file extension.
note that these rules are by design straightforward as we would otherwise risk the introduction of biases in our dataset.
for example all pebin the main microsoft windows executable format must start with the string mz 0x4d 0x5a .
not all files starting with mzare pebin e.g.
they could be textual files which happened to start with the characters mz but if a sample s file extension claims to be an exe or dll we verify this condition holds.
for text files where checks on magic bytes are not applicable wemerely verify the encoding to ensure the file contains only text characters.
finally for samples from virustotal we ensure that no anti virus engines flags the sample as malicious due to such samples having an increased risk of obfuscated content types.
independent of these workflows we also create synthetic samples for two content types unknown and txt.
for unknown each sample consists of a random byte sequence of arbitrary length.
for txt each sample consists of a random string of text characters of arbitrary length.
these synthetic samples are used as a form of data augmentation to train the model.
we do so to help the model handle files with content types it has not been trained on following best practices rather than forcing the model to choose of n valid content types it can select unknown .
we empirically observed that this approach has a negligible impact on accuracy on our dataset .
accuracy improvement .
nevertheless in adherence to best practices we adopt this technique as it does not introduce any disadvantages.
the interested reader can find all the details about the entire dataset creation process in our open source release.
final dataset.
our final dataset consists of .
million samples.
we randomly split these into a training validation and testing dataset.
the latter serves as a uniform benchmark for all content type detection tools including our deep learning model but is never exposed to our model during training nor has it been used to select the model hyperparameters or thresholds for which we used the validation split following best practices .
in total we selected 10k samples per content type for our testing benchmark with more than 1m samples in total making our testing dataset large enough to obtain robust evaluation metrics 10k samples for model validation and 10k samples for model training capping at million per content type .
there are two exceptions to this for isoand odp content types we have only 14k samples total.
as we show later this does not have a material difference on our deep learning detection accuracy.
in total our testing benchmark consists of .
million samples our training dataset consists of million samples and our validation dataset consists of .
million samples.
limitations.
as with any measurement or machine learning study our methodology incurs a number of limitations.
first our dataset consists of only content types.
we acknowledge the selected list is necessarily incomplete and may not encompass content types relevant to all deployment enviroments.
while there is a long tail of many other content types in use today acquiring and validating a representative sample is prohibitively expensive in terms of manual overhead.
as such we focus on selecting a diverse and large enough number of content types to prove the generalizability of our deep learning approach on the most popular content types leaving the extension of our technique to a more comprehensive set of content types to future work with the open source community.
second despite our best efforts at validation and sampling our benchmark dataset may contain mislabeled content types or be fig.
cdf of the sample sizes in our benchmark dataset.
biased towards the samples available on virustotal or github.
as such performance metrics may vary for other deployments though our sample size should be suitably large to provide insights into the limitations of existing content type detection tools as well as our own.
iv.
b enchmarking existing tools we benchmark the performance of existing content type detection tools to motivate the need for more robust detection.
our measurements rely on the .2m samples in our test benchmark dataset.
the dataset includes samples of various sizes as shown in figure .
as part of our evaluation we assess both the number of content types supported per tool overlapping with the types in our benchmark and the accuracy of predictions per tool.
as we show the best existing tool achieves an f1 score of only with no single tool supporting all of the content types we evaluate.
a. tools selection our benchmark evaluates four popular content type detection tools used for a variety of applications file the default command line tool for detecting content types for a variety of files.
we note that file can be optionally queried to return the mime type which we also benchmark as the two modes can yield distinct results denoted file mime .
exiftool a tool originally developed for detecting image content types but that has since expanded to a variety of binary and text content types.
trid a tool for detecting a wide range of content types.
guesslang a tool for detecting the programming language based on an excerpt of source code.
we select the first three because they represent the statusquo they are based on well established signatures based approaches readily maintained and are widely adopted.
we select guesslang as a representative of emerging content type detection tools based on machine learning.
guesslangis unique as it focuses exclusively on textual content types and is robust enough for real world deployment in vs code .
b. metric selection for each tool our benchmark assesses the precision and recall of inferred content types compared to the golden labels of our test dataset.
given this is a multi class classification problem we estimate per type precision as tp tp fp .
here a true positive tp indicates the tool predicts the correct golden label 2while a false positive fp indicates the tool predicts a specific content type but in fact the golden label is any other content type.
we calculate per type recall as tp tp fn .
here a false negative fn means a tool failed to detect a sample as the golden canonical content type predicting any other label.
for ease of presentation we simplify these metrics into an f1 score which provides equal weight to both per type precision and recall f1 precision recall precision recall we calculate this f1 score per content type and then average f1 scores across text content types binary content types and overall content types to provide a variety of assessments on how well existing tools perform.
c. automating large scale evaluations the text string outputs of existing content type detection tools are not designed for cross comparison.
as such we develop a harness to programmatically query millions of samples while normalizing the outputs to match one of the canonical content types in our benchmark.
normalizing detected content types.
unfortunately there are no canonical naming conventions for content types across tools or even the same version of the same tool.
nor is there a hierarchy of how content types relate.
for example valid command line outputs of file for xml documents include xml document xml .
document and xml .
document text among others.
we also find that different versions of the same tool will silently update naming conventions.
for example file silently changed the output for javascript files from node.js script text executable to node.js script executable dropping the text keyword and potentially breaking automated workflows .
similarly we find that the mime types generated by tools despite being machine oriented also do not follow canonical naming conventions for several reasons mime types change over time e.g.
text x markdown vs. text markdown tools silently update their output e.g.
for pebin file .
outputs app x dosexec while file .
outputs app vnd.microsoft.portable executable 2given multiple fine grained content types may be associated with a canonical content type e.g.
javascript and typescript javascript we treat a prediction as accurate if it matches any fine grained content types in our mapping.tool name binary types text types all types of of of file file mime exiftool trid guesslang table i number of content types in our benchmark that are supported by existing content type detection tools.
some content types have multiple valid mime types e.g.
for xml both application xml andtext xml are valid tools databases have typos e.g.
text pyton or arbitrary variations e.g.
text python37 vs.text python .
we used a manual iterative process to address all naming discrepancies.
for each tool we gathered the default command line outputs for every file in our benchmark.
we then grouped these by output frequency mapped the most popular types to one of our canonical content types using a set of handwritten rules and iteratively updated the set of rules until of the default outputs mapped to a canonical type.
in the case a tool produced a content type outside our canonical set we flag it with a special error code.
enumerating supported content types.
of the tools we evaluate only exiftool andguesslang provide a list of supported content types.
to determine which tool supports which content type we use the normalized outputs over every sample as previously discussed.
if a tool never flags any sample as one of our canonical content types we make a simplifying assumption that it does not support that content type.
this provides a fairer comparison with existing tools we omit unsupported samples from some of our metrics rather than giving a tool an f1 score of for such content types.
table i shows a breakdown of the content types in our benchmark supported by existing tools across text binaries and overall.
d. results we report a high level summary of our benchmark results for each existing tool in table ii.
for space reasons we share per content type results for only a sample of the content types we benchmark in table iii.
full results can be found at .
overall if we restrict our view to content types supported by each tool we find that file mime achieves the best performance with an average f1 score of with exiftool andtrid scoring just lower.
for text exclusively guesslang achieves the best performance with an average f1 score of .
for binaries exclusively file mime achieves the best performance with an average f1 score of .
as we demonstrate shortly m agika is able to outperform all of these tools achieving an overall average f1 score of .
we explore the performance of each existing tool in detail below.
file andfile mime .we find that file is highly performant for supported binary content types with an average f1content type metric file file mime exiftool trid guesslang binary supported types text supported types overall supported types binary all types text all types overall all types table ii performance of existing content type detection tools measured as an f1 score averaged across all binary text and overall content types.
for fairness we include two scopes for our metrics performance restricted to supported content types and performance on all content types.
score of when using the mime flag and otherwise.
however its accuracy decreases for some specific binary types such as apk jar or dmg .
for supported text types the average f1 score drops to likely due to the limitations of signatures when applied to text.
we find the accuracy of supported text content types can vary widely with an f1 score of for ruby for lisp and just for powershell .
support is also missing for some programming languages such as rust and scala .
we note that the mime flag is critical for some text contexts with accuracy dropping from to for doc without the flag.
exiftool andtrid .we find that trid andexiftool provide similar overall accuracy with an average f1 score of each.
however trid supports far more content types than exiftool .
both trid andexiftool see drops in accuracy for text content types with average f1 scores falling to and respectively.
for instance trid produces inaccurate inferences for json sql and yaml despite supporting each content type.
likewise exiftool struggles with python and ruby among other text content types.
as with file the high variance of f1 scores makes it challenging for clients to understand the quality of content type detection from a tool absent benchmarks.
guesslang .as previously mentioned guesslang focuses exclusively on text content types achieving an average f1 score of for supported types.
while guesslang is highly accurate for some programming languages like rust or scala it nevertheless struggles with html and xml among others.
one limitation specific toguesslang is that it does not support an unknown verdict.
as such it still predicts a random incorrect content type for our synthetic txt samples.
we note that vs code developers ran into this exact problem as discussed in a github issue .
instead vs code implements a number of heuristics to artificially penalize automatic recognition of content types prone to false positives .content type file file mime exiftool trid guesslang apk asm asp batch bmp c cs css csv dmg doc docx elf go html ini jar java javascript jpeg json macho makefile markdown pdf pebin perl png powershehll python ruby shell sql txt unknown vba xls xlsx xml yaml zip table iii performance of existing content type detection tools measured as an f1 score per content type for a sample of content types in our benchmark.
v. m agika in light of the limitations of existing content type detectors we design and train a new deep learning classifier called magika that distinguishes between content types without the need for signatures created by experts.
a. requirements we design m agika to support two distinct deployment scenarios with a single model a command line tool that can replace established utilities and a bulk inference tool that can scale to analyzing billions of samples per day.
these scenarios constrain how we approach accuracy speed and resource utilization.
concat onehot encoding dense spatialdropout reshape layernormalization dropout dense dense globalmaxpooling1d layernormalization dense content type beginning middle end dropout fig.
architecture of m agika .
the input and the output are depicted in blue and green respectively.
the model s layers are in yellow.
the layers in purple are used only in training.
the numbers next to the layers names indicate the size of their outputs.
accuracy.
magika should provide equivalent or better accuracy at distinguishing between the content types in our dataset.
furthermore detection should depend exclusively on a file s content regardless of the presence of a file extension or metadata.
as before we use an f1 score to evaluate accuracy.
speed.
bulk inference should return a decision within 10ms per file excluding any initialization overhead for loading a model into memory.
command line users may be sensitive to such initialization costs.
as such the total overhead of initialization and inference should should remain 100ms.
this speed also includes the time spent reading a file which has design implications for how best to detect the content type of large files e.g.
100mb without necessarily reading the whole file.
resources.
in order to maximize potential deployment scenarios m agika should meet our speed and accuracy requirements even on a single cpu.
this resource constraint is atypical of machine learning which often assumes access to a gpu.
however we cannot expect command line users to have access to a gpu.
likewise requiring large scale platforms to dedicate gpu resources for the simple task of content type detection would be prohibitively expensive especially considering the required scale e.g.
potentially billions of files processed daily .
b. model architecture we present the architecture of the deep learning model that powers m agika in figure .
the overall flow consists of transforming the contents of a file into a fixed sized vector representation processing the vectors via a neural network and interpreting the model outputs to predict the most probable content type.
inputs.
the model s inputs consists of three vectors that encode a sequence of bytes selected from the beginning middle taken from the center of the file with no randomness and end of an input file.
we encode each byte as an integer in the range .
to maximize the utility of the inputs made available to the model we strip any leading or trailing whitespace from the beginning and end of a file s contentsbefore selecting bytes for encoding.
as the model s input has constant size i.e.
3x512 integers if an input file is too small we pad the encoding with a special character represented by the integer .
we then concatenate the three input vectors to form a single vector of 3x512 integers which we one hot encode and embed into a float vector using a dense layer.
while we considered using longer sequences or more sequences of bytes from a file in practice this incurs additional resource requirements and slows down processing due to having to seek over more portions of the input file e.g.
files 100mb .
by using a fixed size input rather than a whole file we ensure inference is constant time.
despite limiting the model s visibility we demonstrate we can still achieve robust accuracy.
trunk.
the model trunk consists of two components.
first the output of the previous dense layer is reshaped to 384x512 dimensions effectively reorganizing the previous output into small four byte chunks.
each chunk is represented by a dimension vector that the model can treat as a single entity instead of having to consider each byte separately which we found to improve both performance and efficiency.
then the model contains two dimensional dense layers with gelu activation .
a global max pooling layer performs downsampling and reduces the dimensionality back to 1d.
during training we apply layer normalization a dropout rate and a of spatial dropout rate for regularization throughout the model.
this final model represents hundreds of design iterations across architectures that are simple enough to be small and fast on a cpu.
we also ran extensive hyperparameter tuning experiments using the validation dataset and tested various model configurations that evaluated the embedding dimension the size of the reshape layer the number size and activation of the dense layers the normalization type and the amount of dropout applied before converging on this specific design.
outputs.
the final dense layer uses softmax activation with size equal to the number of canonical content types in our dataset.
the output vector represents a probability distribution over each of the potential content types.
we can select the most likely output with an argmax function or apply individual confidence thresholds per content type ultimately yielding the single most likely content type among those that meet a minimum confidence threshold.
c. training we trained the m agika model using categorical crossentropy loss which is suited for multi class classification settings with adam as the optimizer batch size learning rate .
1 .
2 .
and 1e .
additionally we use cutmix for data augmentation although our experiments did not highlight a significant boost in validation accuracy.
however since cutmix is used only during training and does not affect model inference we continue using a rate for cutmix because it has no downsidesfig.
validation loss and validation accuracy as the training progresses in terms of the number of epochs.
we find accuracy increases up to around epochs.
fig.
average f1 score after one epoch of training with increasing number of samples per content type across binary text overall content types.
and could make the model more robust to out of distribution samples.
we trained the model on a machine with a core amd ryzen 7950x cpu one nvidia geforce rtx and 126gb of ram.
we implemented the model and training pipeline in tensorflow and keras .
the training pipeline handles hundreds of millions of examples in a scalable manner with dataset sharding and shuffling to ensure that batches are balanced across the different content types.
we train the final model for epochs on the training dataset which took days and hours total on our setup.
figure shows how the loss and accuracy progress with the number of epochs computed on the validation split of the dataset .
note how the model achieves a very high validation accuracy already starting from the first few epochs.
figure shows how the average f1 score increases with the number of training samples per content type.
note how the average fig.
precision recall curve for a fixed threshold applied to all content type predictions.
note that the truncated scale for precision and recall is different.
f1 score for binary content types surpasses with only 10k samples but that more samples are needed to boost the average f1 score among text content types.
d. setting confidence thresholds a classification threshold is the cut off point whereby we treat the probability output from our final dense layer as suitably high confidence to yield a content type prediction.
selecting an optimal threshold involves balancing the trade offs between precision and recall which is made more complicated by a multi class setting.
for example we empirically found that a probability value of .
was robust to assert that a file ishtml whereas the same threshold for pdf yielded poor accuracy.
we approached this problem by first considering a single threshold for all content types.
figure contains the precisionrecall curve for this scenario.
the curve shows that while there is a trade off between precision and recall we can configure the model to obtain a precision and a recall that are both higher than .
.
as a further optimization we instead compute per contenttype thresholds.
we determine each threshold by fixing precision at and then choosing the maximum recall of the remaining thresholds.
at inference time we then select the argmax of content type predictions that exceed the type s custom threshold.
if the model yields no confident outputs we output either txt orunknown depending on the nature of the content type the model was the most confident with.
we discuss the overall performance of this approach shortly.
in practice clients using m agika can set their own confidence thresholds but by supplying tuned defaults we avoid a pain point encountered by users of other tools .
e. performance optimizations per our design requirements m agika must also be performant in terms of the time it takes to load the model into content type metric f1 supported all binary text overall table iv performance of m agika measured as an f1 score averaged across all binary text and overall content types.
to simplify comparison across tools we calculate the delta in f1 score between m agika and the most accurate existing tool per metric.
memory and yield a prediction.
we implement a number of performance optimizations whereby a client can access our deep learning model either through a command line or api.
as part of this we use onnxruntime instead of tensorflow and keras which we use for training because it is roughly 15x faster in loading the model while having a similar model inference time thus significantly reducing the initialization cost which is critical for command line use cases.
likewise we support batching for clients performing multiple inferences to parallelize processing.
our first cli prototype has been written in python but due to performance reasons we have also implemented a version in c for production use cases and a version in rust as the next version of our command line tool.
we have also implemented a prototype based on tensorflowjs which allows one to run m agika entirely within a browser.
vi.
e valuation in order to demonstrate the value of m agika we benchmark it against existing tools in terms of accuracy and speed.
a. accuracy we report a high level summary of our benchmark results for m agika in table iv.
the reported accuracy metrics have been computed using the testing dataset which following best practices has not been used for any step involving the creation or tuning of m agika .
as a source of further insights we share per content type results for a sample of the content types in our benchmark in table v. full results can be found in our source repository .
regardless the context be it text binary or overall content types we find that m agika outperforms all existing tools.
this demonstrates the generalizability of our model architecture and training approach.
accuracy gains.
for binary content types we find that magika yields a modest f1 gain of when compared to the best existing tool file mime limited to content types supported by both m agika andfile mime .
this f1 gain extends to if we consider all content types in our benchmark.
for text content types m agika yields even better f1 gains of over guesslang limited to content types supported by both m agika andguesslang .
this extends to for all content types in our benchmark.
this is especially true of markdown gain vba gain and xml gain .
likewise m agika is better than allcont.
type f1 apk asm asp batch bmp c cs css csv dmg doc docx elf go html ini jar java javascript jpeg json latex lisp cont.
type f1 macho makefile markdown pdf pebin pem perl png powershell python ruby rust scala shell sql txt unknown vba xls xlsx xml yaml zip table v performance of m agika measured as an f1 score per content type for a sample of content types in our benchmark.
to simplify comparison across tools we calculate the delta in f1 score between m agika and the most accurate existing tool per content type.
models at exhibiting uncertainty with an f1 score of for unknown synthetic random byte sequences and for txt synthetic random text strings .3we also note that magika has high accuracy and often shows a double digit gain in f1 even for content types that were shown to be problematic for guesslang s integration in vs code such asbatch csv ini makefile sql and yaml .
no need for preprocessing.
we find that m agika is able to handle various forms of packing and compression.
for example doc xls ppt are all variants of the same composite document file binary format.
likewise docx xslx pptx apk and jarare all instances of zip file formats.
most existing tools deal with these content types by first unpacking the file before applying any signature based rules.
conversely magika requires no pre processing or domain knowledge to yield accurate predictions.
accuracy vs. sample size.
figure shows how the accuracy of magika and existing tools change depending on the samples size.
the results show how m agika achieves a stable accuracy for most samples with a slight degrade in accuracy for very small samples of less than bytes upon manual inspection we find that this loss of accuracy is due to m agika outputting a generic content type such as txt orunknown due to lack of strong confidence.
it is also interesting to see how the average accuracy of existing tools is also somewhat stable across samples size but with much higher variance.
3in practice this txt includes both txt files as well as our synthetic examples.
this label is only valid if no other text content types apply e.g.
csv html etc.. fig.
average accuracy vs. samples size for m agika and existing tools for simplicity the figure only shows tools with an average accuracy higher than .
limitations.
our evaluation is affected by a few limitations.
first our dataset is balanced across content types this is standard practice especially when there is no onereference real world distribution as it allows one to determine how each content type is supported regardless of whether they are rare in practice.
this makes the results more transparent and easier to inspect e.g.
when using an imbalanced dataset a model with poor support for a rare content type would still have a very high average accuracy .
however we acknowledge that the reported accuracy metrics are not directly applicable but given that m agika is open source a user with a very unique setting can tune and evaluate m agika to its needs for example by mapping our results to her distribution this is the standard way to adapt models to an imbalanced environment.
the second limitation is that as it is common with works based on deep learning on large datasets we did not perform cross validation as it is computationally prohibitively expensive as reported a single training run takes about one week .
however cross validation is deemed as critical only when dealing with very small datasets in which a specific pick of the testing dataset may be accidentally very biased this is not a concern in our situation in which the testing set alone has more than 1m samples.
we also performed an additional experiment in which we split our testing dataset in random smaller sets the average accuracy is .
.
showing very low variance which supports the robustness of our evaluation.
b. speed per our design requirements the time it takes to infer a content type is equally important to accuracy.
there are two relevant dimensions to processing latency initialization e.g.
loading a signature database or model and inference e.g.
via matching signatures or running a model .
for m agika we can measure the breakdown between these two aspects directly but it is challenging for the other tools.
thus we measure the breakdown indirectly by measuring the time required totool name one sample all samples per request ms in one request ms file .
.
file mime .
.
exiftool .
.
trid .
.
guesslang .
.
magika .
.
table vi average execution speed of m agika and existing tools when scanning samples content types samples per type .
the first column captures the initialization and inference cost for content type detection on a single sample averaged across all samples .
the second column captures the amortized initialization and inference costs of content type detection for all samples in a single request normalized to the total number of samples.
evaluate one sample at a time and to evaluate multiple samples all at once.
we evaluate all of the existing tools and m agika as follows.
we select a fixed subset of our benchmark dataset that consists of samples ten for each of the content types.4this avoids any bias in the event a tool is faster for one content type versus another e.g.
due to requiring decompression or more complex signatures .
as a first experiment we send a single request to each tool for each sample in our evaluation.
as a second experiment we pass all samples as a single request to each tool e.g.
we provide each sample s path as arguments to the same command line invocation .
in both cases we limit the number of available cpus to one using the taskset utility .
we estimate the overall running time for both modes using hyperfine executing each experiment ten times and averaging the results with three warm up rounds to minimize the impact of external factors such as caches .
our environment consists of an isolated docker container within an idle virtual machine an e2 highmem instance on google cloud with 8x amd rome cpus and 64gb of ram .
table vi shows the results.
we find that file is the fastest tool with or without the mime flag followed by magika exiftool trid and guesslang .
apart from file every tool has a non negligible initialization cost on the order of 100ms or more.
for clients scanning files in bulk the amortized processing time of m agika drops to .77ms better than all existing tools apart from file .
the significant performance gain of m agika overguesslang the only other tool to use a model is likely due to the latter s reliance on tensorflowjs whereas m agika uses onnxruntime.
we note that our implementation of m agika allows clients to take advantage of multiple cpus without having to wrap requests via parallelization which is not true of existing tools.
for example our client achieves an average of .39ms per sample when running on cpus while the amortized processing time of the other tools remains the 4we selected the first ten samples for each content type.
however one of the zsamples causes exiftool .
to hang indefinitely thus we excluded this sample and replaced it with another one.same.
while many other optimizations are feasible our results demonstrate that m agika can quickly infer content types even when executing a neural network exclusively via a cpu.
vii.
r eal world adoption we recently released the m agika model and a command line wrapper as open source under an apache license .
we share information on its reception as well as real world deployments that now use m agika .
github release.
we reached 4k stars on github in less than a week and currently total more than .7k stars.
m agika was featured in github trending projects and we have received more than open issues and pull requests from external contributors.
likewise a number of developers have reached out to integrate m agika into their workflows such as for validating datasets or for enhancing security malware analysis pipelines e.g.
assemblyline .
m agika s python package currently averages over 3k downloads per day.
email attachment scanner.
we worked with a large email provider gmail to integrate m agika into their attachment scanner that detects and blocks malicious attachments .
this scanner processes hundreds of billions of files every week underscoring the computational performance of our approach.
m agika allows the email provider to enforce content type policies on potentially obfuscated files e.g.
prohibiting executable binary content types as attachments .
it also allows the email provider to route specific samples to anti virus scanners based on the content type detected which might otherwise be prohibitively expensive to run on all samples.
for example we have estimated that on a daily basis m agika routes to ms office specific malware scanners several millions samples that would have otherwise not being scanned when using existing content type detection systems or file extensions.
m agika has now been running in production for more than six months with no significant concerns to be reported.
virustotal file scanner.
the virustotal service has recently integrated our work every submission to virustotal is now processed with m agika whose result can be seen in the details tab of each submission alongside other content type detection tools.
this can be used by virustotal for improved indexing and to optimize which files are sent to the platform s code insight functionality which employs generative ai to analyze and detect malicious code.
for examples m agika s accurate detection of powershell means that virustotal can now isolate such files and specialize any generative analysis exclusively to powershell .
vs code.
we have reached out to vs code developers and we have presented our model for consideration as a potential replacement for guesslang .
during our discussions the developers acknowledged the limitations and challenges encountered with guesslang expressing openness to consider alternatives.
we subsequently engaged in a dialogue regarding current feature gaps primarily focusing on the necessity formore fine grained detection of specific content types e.g.
c vs.c javascript vs.typescript inivs.toml which we have been exploring with very promising results.
viii.
d iscussion our benchmark and integration stories highlight that content type detection remains an important problem.
we discuss future design directions for further improving m agika and potential risks.
handling new content types.
we acknowledge that the current version of m agika supports a limited number of content types and that our selected list is not necessarily the most representative in all scenarios.
moreover new content types are constantly emerging requiring the authors of contenttype detection tools to continuously maintain and add new signatures.
m agika sidesteps the necessity of understanding the intricacies of different file formats e.g.
to write signatures and to ensure they do not collide with previous signatures .
instead handling new content types with m agika requires two steps sourcing a sufficiently large number of training samples and retraining the model with a new dense output sized to the number of supported content types.
we find in practice that m agika can use as few as 10k samples to achieve robust accuracy for binary content types though text content types require more samples.
sourcing files is also fairly trivial via github and virustotal.
while re training a new model can take several days it is not a concern as the entire process can be automated.
multiple valid content types.
magika currently outputs only a single inferred content type which is enough for the vast majority of files.
however one could craft so called polyglot files which are syntactically valid for multiple content types e.g.
interpretable as python php and bash .
such polyglots use unspecified holes in the file format specifications making accurate detection more challenging.
more trivially multiple valid content types might arise due to a file containing code snippets from different programming languages.
in practice users of m agika may be able to examine the top n most likely content types though we leave evaluating the accuracy of m agika on multi content type files to future work.
adversarial risk.
as with other content type detection tools magika is potentially susceptible to evasion.
here evasion means tricking a content type detection tool to infer an incorrect type but where the intended application can nevertheless process the file s contents correctly.
for signature based rules this might be as simple as adding whitespace per our earlier example in figure .
for m agika as the model is open source attackers might instead add small perturbations to a file s contents to cause our neural network to infer an incorrect content type.
adding such noise in a programmatic way while still yielding a file that is interpretable by the intended application may be non trivial however this analysis falls under the area of adversarial machine learning and significant work is required to make the approach resilient to attackers that specifically attempt to bypass the detection.ix.
c onclusions in this work we presented m agika a novel ai powered content type detection tool.
trained on 24m samples and canonical content types we showed how our deep learning architecture can achieve an average f1 score of .
our approach outperforms all existing content type detection tools with a f1 gain over the best tool for binary content types and a f1 gain over the best tool for text content types.
for bulk inferences m agika requires .77ms to yield a decision per file with just a single cpu making it suitable for a variety of deployment scenarios.
our tool has already seen adoption in the real world in a number of critical pipelines.
to foster adoption and improvements parts of m agika are already available as open source under an apache license and we plan to release the remaining material in the near future .