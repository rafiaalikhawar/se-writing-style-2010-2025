formally verified cloud scale authorization aleks chakarov amazon aleksach jaco geldenhuys amazon jgeldenh matthew heck amazon mcheck michael hicks amazon mwhicks sam huang amazon srhuang georges axel jaloyan amazon gjaloyan anjali joshi amazon anjalijs k. rustan m. leino amazon leino mikael mayer amazon mimayere sean mclaughlin amazon seanmcl akhilesh mritunjai amazon amritun clement pit claudel amazon pitclauc amazon.ch sorawee porncharoenwase amazon soraweep florian rabe amazon florrabe marianna rapoport amazon rapopor giles reger amazon reggiles cody roux amazon codyroux neha rungta amazon rungta robin salkeld amazon salkeldr matthias schlaipfer amazon schlaipf daniel schoepe amazon schoeped johanna schwartzentruber amazon jjsch serdar tasiran amazon tasirans aaron tomb amazon aarotomb emina torlak amazon torlaket jean baptiste tristan amazon trjohnb lucas wagner amazon lgwagner michael w. whalen amazon mww remy willems amazon rwillems tongtong xiang amazon ttx tae joon byun meta taejoon umn.edujoshua cohen princeton university jmc16 cs.princeton.eduruijie fang university of texas at austin rjf abstractpredicates.orgjunyoung jang mcgill university junyoung.jang mail.mcgill.ca jakob rath tu wien jakob.rath tuwien.ac.athira taqdees syeda university of melbourne hira.syeda unimelb.edu.audominik wagner ntu singapore dominik.wagner cs.ox.ac.ukyongwei yuan purdue university yuan311 purdue.edu abstract all critical systems must evolve to meet the needs of a growing and diversifying user base.
but supporting that evolution is challenging at increasing scale maintainers must find a way to ensure that each change does only what is intended and will not inadvertently change behavior for existing users.
this paper presents how we addressed this challenge for the amazon web services a ws authorization engine invoked billion times per second by using formal verification.
over a period of four years we built a new authorization engine one that behaves functionally the same as its predecessor using the verification aware programming language dafny.
we can now confidently deploy enhancements and optimizations while maintaining the highest assurance of both correctness and backward compatibility.
we deployed the new engine in without incident and customers immediately enjoyed a threefold performance improvement.
the methodology we followed to build this new engine was not an off the shelf application of an existing verification tool and this paper presents several key insights rather than prove correct the existing engine written in java we found it more effective towrite a new engine in dafny a language built for verification the email addresses of amazon affiliated authors end with amazon.com.
the work by non amazon affiliated authors was done while they were at amazon.from the ground up and then compile the result to java.
to ensure performance debuggability and to gain trust from stakeholders we needed to generate readable idiomatic java code essentially a transliteration of the source dafny.
to ensure that the specification matches the system s actual behavior we performed extensive differential and shadow testing throughout the development process ultimately comparing against production samples prior to deployment.
our approach demonstrates how formal verification can be effectively applied to evolve critical legacy software at scale.
i. i ntroduction to control access to their data and resources aws customers write policies that express fine grained permissions.
a cloud hosted authorization engine evaluates incoming requests against those policies to either grant or deny access.
it must ensure that decisions are both fast to not slow down normal processing too much and correct by adhering to public documentation and meeting customer expectations.
this authorization engine is used to secure access to over fully featured services invoked over billion times per second.
today any change to the authorization engine undergoes a rigorous evaluation of both design and implementation 1leveraging unit and integration testing and multiple rounds of code review.
changes are also reviewed by a central security team and undergo extensive third party penetration testing.
while effective in we decided even more assurance was needed to allow our authorization system to evolve rapidly to meet aws s increasing scope and scale.
indeed the challenge of ensuring the engine s speed and correctness had increased over time causing us to wrestle with the question how do we confidently deploy enhancements to this security critical component and know that backward compatibility and performance for existing customer workflows are preserved?
this challenge is not unique to our engine it is a core challenge for any critical system developed over a long period of time in response to a growing and diversifying user base.
we sought to gain this confidence through the application of formal methods which involve writing a formal specification and proving that code correctly implements that specification.
while the obvious benefit of formal methods is assured correctness it also endows two other benefits understanding a precise formal specification is easier to read and understand than an optimized implementation.
thus we can be more confident that changes we make are the right ones since they are expressed as simple changes to the specification.
agility formal proofs make changes easier to deploy.
this is because proofs give a strong guarantee that changes we make to the production code always precisely match our specification i.e.
they do what we intend.
for example if we want to optimize the engine we can prove that the optimized version still meets the existing specification.
if we wish to add a new feature we can prove that the new specification is backward compatible with the old one before proving the new code satisfies it.
over a period of four years we developed a new formally verified authorization engine called authv2 that is behaviorally equivalent to the original engine called authv1 .
the effort split evenly between i specification proofs and implementation ii compilation and iii testing and validation.
authv2 was successfully deployed to production in early .
it computes authorization decisions three times faster than authv1 .
in addition using our new formal methodsdriven development process we have already been able to confidently and swiftly make changes to authv2 after it was deployed.
we believe authv2 to be the first successful application of formal verification to build a performant functionally equivalent replacement for a highly scaled legacy system.
the core contribution of this paper is to show what was required beyond a straightforward application of an existing verification tool to achieve this result we had to develop new solutions integrate multiple tools and languages and use them at scale.
the paper recounts the four phase process we followed to successfully launch authv2 reverse engineer the formal specification section iv build a high quality verification aware 2authv1 andauthv2 are pseudonyms.implementation and prove it correct section v generate idiomatic code from that implementation section vi and validate the generated code for performance and correctness under deployment conditions section vii .
there are several takeaways from our experience that were key to its success.
first we needed to rewrite authv1 to be amenable to formal verification.
it is non trivial to formally verify software written in a general purpose language that has not been designed with formal verification in mind.
authv1 is deployed in java for a jvm based environment.
it relies on features that make formal verification challenging such as complex exception handling use of mutable state and heap manipulation.
therefore following the lead of several past projects we elected to write authv2 in a language built to support formal verification from the ground up.
our language of choice was dafny which leverages smt solving to automate much of the proof task.
while dafny has been applied to several prior projects e.g.
we present novel insights from using it in practice and at scale e.g.
explaining how we managed the problem of proof brittleness aka proof instability .
second to compile the verified dafny code to java we developed our own idiomatic compiler that generates readable and performant code rather than use dafny s existing java compiler.
so made authv2 s java code reviewable by the operators of authv1 who were not dafny experts helping build their trust in its launch readiness.
idiomatic compilation also makes authv2 easily debuggable in the context of the larger java codebase it operates within.
and it preserves the optimizations expressed in authv2 s dafny source code.
our compiler goes beyond the scope of prior idiomatic compilers for verified code by supporting more sophisticated source language constructs and environment models and leveraging annotation based static checks.
third while much focus in the formal methods community is on proof verification technology it is no less important to be sure your proofs consider the right specification .
we used extensive systematic differential testing to ensure the specification was accurate and complete and thus that authv2 now and in the future exhibits no deviations from behavior established by authv1 .
in particular we wrote the initial authv2 specification to be executable and then extensively tested it against the legacy authv1 on synthetic and logged data.
we also shadow tested authv2 against authv1 on production authorization data ultimately testing against 1015production samples prior to launch.
so was pivotal to ironing out last corner case mismatches.
this paper details the four phase process we followed to build authv2 highlighting these takeaways along the way.
it concludes with a detailed comparison of related verification efforts section viii and ideas for further work section ix .
ii.
b ackground a. cloud service authorization the legacy authorization engine of aws known as authv1 is written in java and is part of a software development kit sdk used by customer facing teams to authorize requests.
it provides the following api answerevaluate list policy ps request r here a policy is the java representation of a list of authorization rules called statements written in a purpose built declarative language.
each statement is a five tuple effect principal action resource condition .
it specifies under which conditions a principal is permitted or forbidden to take an action on a resource corresponding to effect allow or deny respectively.
the condition is optional and provides a way to specify constraints using typed operators.
for example here is a policy containing a single statement which states a user named eve may perform read write actions on jpg files inside the eve folder managed by storage service whose name starts with photo .
effect allow principal action resource condition stringlike filename photo .jpg therequest type represents a request to some service api.
it is a tuple p a r c that asks is principal pallowed to take action aon resource rgiven context c?
where c is a key value list with additional request data.
theevaluate function evaluates the request against all provided policies and returns the answer asexplicitdeny allow orimplicitdeny .
in particular a request is allowed if some policy allows and no policy denies it.
in the absence of applicable policies a request is denied implicitly.
the above behavior is publicly documented in semiformal statements such as an explicit deny in any policy overrides any allows.
we call this property deny trumps allow and revisit it in the following subsection.
b. dafny a verification aware programming language after authv1 had been in use for about a decade we felt that our current software assurance process was reaching its limit.
it was becoming increasingly difficult to convince ourselves to deploy complex changes including optimizations that might disrupt our many customers due to a fault that got missed in either the design or the implementation.
we decided the best path forward was to develop a formally verified version of authv1 we would write a formal specification that described authv1 s behavior and an implementation that we proved conformed to that spec.
when we changed the implementation in the future we would either prove it against the existing spec e.g.
if the change was an optimization or we would prove it against an updated spec that in turn we proved was backward compatible with the existing one.
with the decision made to formally verify the question became how?
our first thought was verify authv1 s java source code directly using a verifier such as openjml or key .
however at the time these tools struggled with complex usageof java features present in authv1 such as exception handling mutable state and heap manipulation.
when we looked at prior successful verification efforts section viii has details we found that most leveraged a programming environment such as isabelle hol the rocq prover or f built from the ground up to support formal proof.
following their lead we elected to rewrite authv1 in dafny an opensource programming language with an integrated static verifier and then compile the resulting code to java.
dafny features java like declarations and statements such as variable assignments loops classes and inheritance.
moreover it supports functional features such as algebraic datatypes higher order functions mathematical sets and maps and a strict distinction between side effect ful and side effectfree parts of the code.
one often leverages the latter when writing logical specifications and the former when writing efficient imperative code but dafny allows freely mixing both styles.
dafny specifications are akin to the code contracts in eiffel consisting of method pre and post conditions affixed to methods and loop invariants and are statically checked by the dafny verifier.
for example we can use the following code snippet to specify the evaluate function function evaluate ps seq policy r request answer ifmatchesdeny ps r then explicitdeny else if matchesallow ps r then allow else implicitdeny and attach it as a post condition to the imperative implementation of the function.
users can write explicit lemmas whose pre post condition pair represents an implication about the inputs.
for example we can formalize the deny trumps allow property as lemmadenytrumpsallow ps seq policy r request p policy s statement requires pinps s inp.statements matches s r s.effect deny ensures evaluate ps r explicitdeny proof dafny s verifier leverages external smt solvers to discharge proof obligations as well as manual proofs when automation reaches its limits.
dafny can compile the behaviorally verified code into various target languages.
it also provides a foreignfunction interface that makes it possible to interface with pre existing target language code.
while its ecosystem is not nearly as rich as e.g.
java s it includes advanced tools like a test generator and a specification auditor.
iii.
o verview how does one evolve a legacy system to use formal methods?
our approach took place in roughly four phases developing a formal specification building a performant proved correct implementation compiling the implementation to idiomatic deployable code in the programming language of the legacy system and deeply validating that implementation under deployment conditions.
as discussed in 3section ii b our particular experience with authv2 leverages dafny a dafny implementation authv2impl dafny is proved correct against a specification authv2spec dafny and is translated to authv2 java the java code which runs in production.
subsequent development on authv2 happens at the dafny level changes to the dafny implementation are re proved against the specification which itself can change subject to proofs of key properties and then compiled to java and deployed.
we give an overview of each phase we followed to build authv2 here and discuss each phase in depth in the subsequent sections.
a. formalizing the specification the first phase was to develop authv2spec dafny the formal specification for authv2 that captures authv1 s behavior.
we wanted authv2 s specification to satisfy three properties.
it should be simple so that it is easy to understand formal so that we can prove properties about it and executable so that we can already test the specification against the legacy authorization engine.
these properties strengthen our confidence that the specification truly reflects authv1 s behavior.
to write the specification authv2spec dafny we inspected authv1 s java implementation and documentation internal as well as public and expressed what we found using dafny.
we ultimately proved lemmas about the specification that we expected should hold including the deny trumps allow property stated in section ii a. failed proofs would have signaled an incorrect specification or potentially unexpected authv1 behavior.
we extensively tested authv2 against authv1 using the latter s existing unit tests and differential tests at the whole system level driven by a test generator.
section iv presents our process in detail.
b. implementing a proved correct authorization engine the next phase was to build a performant verificationaware dafny implementation and prove that it conforms to the specification.
while the specification is purely functional and inefficient the dafny implementation can be imperative and aggressively optimized.
because authv1 runs in a java environment authv2 needed to be written against a model of the standard java libraries.
we therefore developed formal dafny interfaces for java arrays lists strings etc.
and relevant library methods.
proofs of correctness leverage these interfaces and the final engine links against the real libraries.
we faced two key challenges during this phase.
first we needed to write these library interfaces in a way that bridges the semantics of the purely functional built in dafny types for sequences etc.
with java s imperative ones while also ensuring the dafny specification of these foreign functions which amount to axioms from the perspective of dafny correctly model the behavior of the java functions.
second we needed to impose some engineering discipline to cope with proof brittleness a phenomenon common in automated reasoning based systems in which apparently inconsequential changes cause previously working proofs to fail.
we discuss these challenges and our solutions in section v.c.
generating deployable code with a proved correct dafny implementation in hand the next phase was to make sure that the implementation s code is deployable.
while authv2 is written in dafny authv1 is part of a larger system written in java.
therefore the ultimate deliverable of the project is java code authv2 java generated from authv2 .
a critical requirement was for authv2 javato meet all operational and performance requirements that had been previously gathered for authv1 throughout the usual software engineering process authv2 javahas to be human readable.
this ensures that code review can independently ascertain the correctness and security of authv2 java.
therefore we decided early on not to use the existing dafny to java compiler which covers all of dafny but produces java code whose performance and idiomaticity are hard to predict for the dafny programmer.
instead we developed a custom dafny compiler see section vi for a fragment of dafny that corresponds to idiomatic java.
it generates authv2 javaasreadable java code basically as a transliteration ofauthv2 .
thus the authv2 developers could anticipate the exact java code that would be produced allowing them to optimize for idiomaticity and efficiency.
ultimately this proved crucial to building the trust necessary to launch authv2 .
table illustrates the size of our dafny formalization and the generated java code for authv2 that now runs in production.
the ratio of the size of the dafny specification and proofs to the size of the dafny implementation is cf.
for ironclad apps and for the initial version of sel4 .
additionally the generated java code is larger than the compilation targeted non specification dafny code because it includes auto generated methods like equals and hashcode in java.
table authv2 formalization and generated code size feature loc dafnyspecification proofs library axiomatization implementation total java generated implementation d. extensive validation under deployment conditions the final phase was to subject authv2 javato real world conditions prior to launch.
to do so we deployed our authorization engine as a shadow in production environments and differentially tested authv2 javaagainst authv1 on production data comparing their functionality and performance.
while our work prior to this phase gave us significant confidence in authv2 this final phase assured us of a seamless deployment.
we shadow tested throughout first comparing the two engines on only .
of samples and steadily raising the sampling rate to as mismatches became less frequent.
so revealed corner case correctness problems where 4authv2 s specification had not modeled all the subtleties of authv1 .
we deployed 8iterations of authv2 javato shadow mode and the final iteration agreed with authv1 on1015 production authorization samples indicating it was ready to launch in production.
moreover while in lab differential testing suggested early on that authv2 javaoutperformed authv1 profiling on real world traffic identified additional performance optimization opportunities.
section vii details our validation efforts.
iv.
f ormal specification the first phase of developing a proved correct version of a legacy system is developing its specification i.e.
a formal description of the intended behavior.
an often overlooked challenge is to ensure that the specification is indeed the intended one.
we build confidence about the specification s correctness by taking two steps.
first we make it executable and validate it using the legacy system s data and tests.
second weprove properties about it that correspond to key statements in the documentation e.g.
those that help the legacy system s users gain intuition about the semantics of the policy language.
a. writing a testable specification if we were starting the project today we would write a purely functional executable specification directly in dafny.
however back in early dafny lacked a critical feature a compiler to java the deployment language required for authv2 .
java code was essential to integrate the specification with the existing authv1 authorization engine s test frameworks and test generators.
to address this limitation we started two parallel efforts developing a temporary purely functional implementation in scala chosen for its ability to interface with java via the jvm that closely mirrored the dafny specification.
the development took about two months and comprised .
kloc.
building a dafny to java compiler to automate the generation of java code from dafny specifications.
for example consider the stringlike operator implementation in scala which enables policies to match request attributes against strings with wildcards defwildcardmatch pattern string text string boolean valescapedregex escaperegexchars pattern s escapedregex .r .findfirstin text .nonempty to gain confidence in the correctness of the specification we tested the scala model in two ways first we applied authv1 s existing unit and integration tests.
second we used junitquickcheck to differentially test the scala model against authv1 .
the latter required developing a fuzzer to systematically inject arbitrary input data into the two models.
the main difficulty here is randomly generating allowed policy request pairs because the request must match the various policyconditions to be allowed random input data would be denied immediately and exercise only a fraction of the source code.
to maximize code coverage we hand wrote input generators so that policies and requests match with high probability.
our generators also ensured data satisfy semantic constraints e.g.
that ip addresses contain realistic values and that service names are realistic.
after about 3person months we achieved about instruction coverage and branch coverage and differential testing uncovered differences between the early version of the specification and authv1 that we were able to correct.
this proved extremely valuable because it was not until later in the development cycle that verification and shadow testing see below produced actionable feedback for repairing the specification.
once the dafny compiler and ffi models were ready we ported the specification to dafny and repeated validation testing in the same way.
note that the example specification above still used external library functionality for regular expressions similar to authv1 s implementation.
that was sufficient for testing the specification.
after porting to dafny we refined the model further to enable verifying it in depth.
this involved e.g.
also modeling the exact semantics of string matching without external dependencies predicate wildcardmatch pat string txt string ifpat txt then true else if pat then false else if txt thenpat wildcardmatch pat txt else if pat then wildcardmatch pat txt wildcardmatch pat txt else pat txt pat ?
wildcardmatch pat txt b. proving properties about the specification we used dafny to prove lemmas about our specification such as the deny trumps allow lemma mentioned in section ii properties about expected behavior of string matching such as case sensitivity and the semantics of composed conditions.
as an example consider the property that consecutive stars in a pattern can be collapsed into a single star without affecting the outcome of the match lemmacollapsestar pat string txt string requires pat requires pat pat ensures wildcardmatch pat txt wildcardmatch pat txt automatically discharged proof v. v erified implementation after developing the specification and gaining confidence in its correctness the next phase is to develop a productionready implementation and prove it correctly implements the specification.
by production ready we mean the implementation enjoys good performance and the code is available in a mainstream language in idiomatic style which 5makes it easier for operators to analyze e.g.
when debugging an operational issue.
we achieve production readiness by writing dafny code in an imperative style and compiling it to idiomatic java.
the code is written against a hand crafted environment that maps constructs types methods etc.
in the java standard library to corresponding dafny constructs so the generated code links against those standard libraries.
in this section we discuss the basic approach how we modeled the java environment and challenges we faced when carrying out the proofs.
a. basic approach we developed in dafny an efficient imperative verificationaware implementation of the program and we proved it equivalent to the functional specification.
the dafny implementation has essentially one imperative method for every function in the specification each with a post condition that it returns the same value as the function on all inputs.
this makes the verification modular i.e.
every implemented function is verified individually.
each method s body contains the operational program logic as well as ghost code .
the latter includes loop invariants and assertions that help the prover along in establishing the post condition.
the ghost code is erased during compilation and does not affect execution.
while the specification uses dafny s built in types exclusively the dafny implementation also makes use of models of external types.
we define each modeled type with a function tospec listing which lifts the type to a built in dafny type.
methods from external libraries e.g.
the java standard library are stubbed out as body less methods with pre postconditions that model the functionality of the external methods.
we explain how we model these external dependencies in section v b. for instance consider the specification of wildcardmatch defined in section iv a. it has exponential worst case complexity whereas our imperative dafny implementation excerpt in listing is only quadratic.
the complete implementation listing verified imperative dafny implementation of o n2 wildcard matching algorithm methodwildcardmatch pat string txt string returns res bool ensures res wildcardmatch pat.tospec txt.tospec ifpat.length returntxt.length vari nat31 j nat31 whilei txt.length loop invariants ifj pat.length pat.charat j j j ... of this method requires lines of dafny lines of which is ghost code.
the example takes inputs of type string which models the java standard library string class and its methods and is different from dafny s built in string whichis a type synonym for a dafny sequence of characters.
by callingtospec onstring we lift it to the corresponding specification type a dafny string and prove equivalence of the results.
b. interfacing with the java target language every large software project needs to interface with external dependencies.
the standard dafny compiler compiles builtin types to types defined in a java runtime library djlib which comes with the dafny distribution.
one then writes wrapper methods in java that translate djlib types to native java types used in apis of the external dependency.
the types in djlib preserve the properties such as immutability and apis of the dafny types which makes compilation simple.
for instance a dafny seq is compiled to dafnysequence and one would manually translate the dafnysequence to say a linkedlist expected by an external api.
however dafny s generated code can be slow compared to handwritten optimized java code that uses native java types and the wrappers that translate djlib types to native types add additional overhead.
we initially considered compiling built in dafny types to native efficient java types for instance compiling seq to a linkedlist .
this would make the compiler implementation difficult the compiler needs to make sure that the translation from pure dafny to mutable java collections is sound while also generating idiomatic code.
to ensure soundness while preserving idiomaticity operations on dafny collections needed to be restricted which in turn limited expressivity.
in addition compiling native dafny collections did not allow the dafny programmer to choose different concrete collection implementations in java for example dafny has a single pure sequence type but in java one might choose an arraylist instead of a linkedlist based on the specific requirements of an algorithm.
we opted to address the above problems by defining bindings for java types and axiomatically modeling their behavior in dafny.
our compiler discussed in more detail in section vi disallows use of dafny s built in types in dafny implementation code not in the erased ghost code unless there is a direct counterpart in java e.g.
bool .
for illustration listing shows an excerpt of how we model arraylist and its supertype list in dafny which we use instead of seq in dafny implementation code.
the base field whose type is the lifted dafny type models the current value of the mutable java object.
it is used in the pre postconditions of the modeled methods.
we changed dafny s default extern behavior so that dafny identifier paths can be rewritten to java ones from the dafny source code without needing any handwritten java code to interface with external dependencies.
working with these types is ergonomic it is easy to write the dafny code in a way that produces the java code the developer expects see fig.
.
adding new extern dafny types as needed is easy and does not require changes to the compiler see section vi for exceptions .
additionally the generated 6listing modeling java list andarraylist in dafny.
nat31 is a bounded subtype of dafny s int and compiles to java s int.
module extern java.util util trait list t extends object ghost var base seq t function axiom size res nat31 requires base nat31 max reads this ensures res base ghost function tospec seq t reads this base class arraylist t extends list t constructor axiom ensures base method axiom addall x list t modifies this ensures base old base x.base figure a small dafny implementation method on the left with generated java code on the right shows that closely modeling the target language leads to predictable generated code.
methodclonetoal l list nat31 returns r arraylist nat31 ensures l.tospec r.tospec r new arraylist nat31 r.addall l arraylist integer clonetoal list integer l arraylist integer r new arraylist r.addall l returnr java code can directly interface with extern dependencies.
no wrappers for translating the generated java types are needed and no runtime cost is incurred.
this keeps the compiler simple the dafny code ergonomic and the compiled code readable and efficient.
in total we wrote around loc modeling dependencies with their types and apis as needed forauthv2 .
c. challenge proof brittleness and continuous integration as the project grew proof brittleness became a significant challenge echoing a well identified burden in other largescale verification efforts .
this brittleness can be informally described as proofs breaking due to unrelated changes in the environment tool versions or code base.
the consequences were severe it slowed the team down added overhead in fixing broken proofs and in some cases even prevented the merging of code updates until proofs were repaired.
ultimately most code or environment changes broke at least one proof causing delays in deployment by up to days.
we implemented three levels of mitigation against proof brittleness.
first by using a centralized cloud based infrastructure we could ensure a standardized and consistent environment across all verification tasks eliminating environment based proof variability.
furthermore by leveraging the massively parallel capabilities of the infrastructure we could keep the wall clockverification time from to below minutes significantly speeding up code reviews and deployments.
second we added brittleness checks to our continuous integration continuous deployment ci cd pipeline to enforce proof stability across toolchain updates and code changes.
those checks raise a warning if a given brittleness threshold is exceeded.
the dafny documentation gives a measurement of brittleness for a vc as the coefficient of variation of resource counts an abstract measurement of verification cost that depends only on the input formula and exact smt solver configuration for different seeds passed to the solver.
at its maximum verification conditions vcs were flagged as brittle.
additionally we designed and implemented a new proof oriented report generator detailing the brittleness levels of all vcs.
this report is reviewed during pull requests and prevents merging commits that would significantly degrade the stability of proofs.
third we refactored the most brittle proofs to significantly reduce their resource count using a set of dafny guidelines for reducing dependence on automation .
an interesting find is that reducing resource count of a proof is a good strategy for reducing its brittleness.
those guidelines consist mostly of making functions opaque which hides their definitions and using the reveal statement which manually re introduces them to the verification context.
as an experiment we gathered metrics on line of code loc count and the ratio of proof versus dafny implementation for our string processing module before and after refactoring.
in dafny before refactoring the module was lines long of which were dedicated to proofs.
after refactoring the module increased to lines of which were proofs.
overall this reduced the resource count by two orders of magnitude on the brittle parts of the code base.
this led to a sharp reduction of brittle vcs in our project from to .
vi.
i diomatic compilation dafny s standard compiler was designed to favor a general compilation process that can support multiple target languages including c c and java.
unfortunately this design means that it tends to produce java code that is neither readable performant nor maintainable which are requirements for authv2 .
having readable java code allows experts in authorization to review the logic without needing to understand dafny.
ensuring the java is performant is a baseline expectation we wanted authv2 to match or exceed authv1 s performance.
maintainability has two aspects.
first the generated code must be compatible with various versions of java and java libraries and support standard error handling mechanisms such as exception handling used in authv1 .
this keeps the experience for end users consistent with authv2 .
second traceability between the java and dafny implementations is important during debugging especially when dealing with emergent issues that involve complex interactions between client code and authv2 .
7to meet our readability performance and maintainability requirements we developed a separate dafny to java compiler that is able to produce idiomatic output i.e.
output that looks human written.
in essence the compiler works by transliterating the dafny source into java thus preserving java style idioms expressed in the dafny without introducing temporary variable names or corrupting control flow structures.
as a result the dafny developer is able to ensure that the final java code is of good quality.
a. non idiomatic code figure shows a snippet of the java output generated with the standard dafny compiler .
the generated code is hard to review or debug due to its use of boxed types extra variables opaque generated names redundant parentheses and excessive public access modifiers.
in addition dafnygenerated code can be inefficient because dafny s abstractions do not always naturally map to preferred abstractions in java.
for example dafny types such as option and result commonly used in functional programming are compiled to wrapper classes.
the extra pointer indirection adds overhead.
moreover dafny programs must pay the cost to translate from exceptions to result when interfacing with external java libraries with exception throwing apis.
as another example dafny uses purely functional collection implementations rather than imperative ones such as those found in java s standard libraries.
as a result the compiler does not leverage java s optimized collection classes.
b. an idiomatically compilable fragment of dafny to address these issues we wrote an idiomatic compiler whose design follows three key principles restrict support to subset of source language allow only a subset of dafny as input which we call dafnylite .
dafnylite drops support for idiosyncratic dafny features and retains features that are straightforward to embed into java.
this reduces the complexity of the compilation compared to dafny s built in compiler and thus increases dafnylite s compilation trustworthiness.
perform idiomaticity transformations apply small single purpose transformations that make the output more idiomatic.
model target language in source language model javaspecific features and libraries in dafny and develop special transformations that pick up on these models.
for example code manipulating a dafny exception trait is compiled into java exceptions.
thus by defining a subset of dafny that closely models the intended java code the dafnylite compiler achieves readability and performance while avoiding complex optimizations.
the dafnylite compiler is written in f .
it links against the dafny compiler written in c which handles parsing type checking name resolution and desugaring .
the dafnylite compiler then converts the dafny abstract syntax tree ast to the dafnylite intermediate representation dlir which includes only expression fragments relevant tojava compilation and excludes ghost code.
the right side of figure compares this compiler s output to the standard one.
the compiler performs a series of self contained typepreserving dlir to dlir passes.
each pass executes a finegrained action such as transforming a dafny feature to a java feature simplifying java code or increasing the idiomaticity.
for example the compiler consolidates variables adjusts visibility annotations shortens identifier paths flattens nested blocks and inverts if conditions when it reduces visual distraction.
the resulting dlir syntax tree contains only java idioms and is printed out as java source code that is formatted and compiled with javac .
the dlir transformations also serve to enforce the restricted dafnylite subset.
for instance dafnylite supports only a restricted subset of algebraic datatypes that map easily to java classes or enums.
it limits result typed expressions to return positions or immediate pattern matching ensuring sound and idiomatic compilation to java s exceptions and try catch constructs.
to avoid java s performance overhead from anonymous functions it does not support dafny lambdas.
instead of compiling dafny s immutable collections we explicitly model java library collections in dafny ensuring interoperability readability and performance.
additionally the compiler avoids syntactic sugar constructs like assignments with object destructuring that can be easily encoded differently to reduce complexity.
to ensure idiomatic output the dafnylite compiler performs transformations on dlir to adhere to java conventions.
even with direct java analogues some dafny features need specific transformations to be idiomatic.
for example dafny datatypes with only nullary constructors become java enums result expressions turn into try catch blocks and option types under restrictions compile to nullable types.
c. translation validation and fuzzing producing idiomatic target code is more complex than simply generating correct executable code.
to add confidence that we are preserving dafny s correctness guarantees we invest extra effort into validation and testing beyond unit and regression tests.
we use the java checker framework to validate correctness properties that should be ensured by the translation.
the checker framework enhances java s type system through annotations we use it to confirm the absence of nullpointer exceptions absence of unintended side effects correct usage of signed computations and correct usage of reference equality .
additionally we employ fuzzing and differential testing inspired by irfan et al.
to validate that the dafnylite compiler follows the semantics of the standard dafny compiler.
we use xdsmith to randomly generate and compile programs ensuring their compilations are correct.
the non idiomatic compilers from the open source distribution of dafny serve as the ground truth in our differential testing setup.
this approach helped us identify and fix multiple bugs in the dafnylite compiler.
8public static boolean wildcardmatch dafny.
dafnysequence ?extends dafny.codepoint pattern dafny.
dafnysequence ?extends dafny.codepoint text boolean r false if java.math.biginteger.valueof pattern .length .signum r java.math.biginteger.valueof text .length .signum returnr int 0 i int 1 j int rhs0 int rhs1 0 i rhs0 1 j rhs1 while 0 i text .cardinalityint if 1 j pattern .cardinalityint dafny.codepoint pattern .select 1 j .value 1 j int 1 j ...static boolean wildcardmatch stringpattern stringtext if pattern.length returntext.length inti intj while i text.length if j pattern.length pattern.charat j j ... figure excerpt from standard dafny compilation of listing left and the corresponding dafnylite generated code right vii.
v alidating for deployment the third and final phase is to validate that authv2 java matches authv1 s behavior under deployment conditions using extensive production data.
so has several benefits.
first it validates that the specification truly is accurate and complete.
second it validates that the dafny verifier s proofs are sound.
third it validates that the semantics of authv2 were preserved by our idiomatic compilers when generating authv2 java.
every mismatch between authv2 andauthv1 can be investigated to determine the root cause and response e.g.
a change to the specification a bugfix to a tool etc.
fourth the testing reveals performance under production workloads and helped us identify opportunities for further authv2 optimizations.
finally it helped earn trust with stakeholders that not only was the deployed system correct but that it also integrated with the build and production systems of the services that use it.
we used shadow testing a technique where a candidate system runs alongside the existing system in production to collect the data necessary to convince ourselves authv2 javawas ready for production.
a. shadow testing earlier phases of development used traditional testing and synthetic fuzzing inputs to compare authv1 and authv2 java.
although these approaches were effective at finding issues our concern was that we were not seeing rare but meaningful differences that would impact customers at a scale of billions requests per second.
shadow testing addressed this shortcoming by comparing authv2 javatoauthv1 on production data as it was occurring trillions of times per day and emitting metrics if a difference was encountered.
in total shadow testing exposed mismatches owing to inaccuracies or incompleteness of our specification no issues were found with tools or proofs.
as an example one mismatch exposed a difference in how authv1 andauthv2 javahandled date and time objects occurring in policies.
when developing authv2 java we made the decision to drop millisecond resolution in date and time objects to simplify the specification and proofs.
our thinking here was that cloud is a globally distributed system and policy propagation operations can takep50 v1 p50 v2 p95 v251015 s p50 v1 p50 v2 p95 v2510 s p50 v1 p50 v2 p95 v2510 s p50 v1 p50 v2 p95 v251015 s figure authorization latency histograms of high traffic services for week periods before and after the switch to authv2 .
the graphs show the performance improvement provided by authv2 across various workloads.
the dashed red graph represents authv1 the solid green one represents authv2 .
the x axis is log scaled and shows the latency with the median for each engine and p95 for authv2 highlighted.
the y axis shows the percentage of calls at a latency range .
the long tails of the graphs are trimmed at p99.
.
seconds to complete making the need for millisecond resolution unnecessary.
however authv1 retains millisecond level precision and under the right circumstances this can and did result in different authorization results than authv2 java.
other mismatches were related to corner cases of wildcard matching and unicode string representation.
we fixed all mismatches we encountered as we found them.
we issued shadow iterations of authv2 java each a refinement of the previous.
the final iteration of authv2 java agreed with authv1 on1015inputs with no observed differences.
at this point we launched authv2 javaas the production authorization engine and ran authv1 as a shadow to detect any differences that might have evaded our previous shadow testing.
after a month of running authv1 as a shadow and finding no differences we turned it off completely.
b. performance testing a key goal was to ensure that authv2 java s performance met or exceeded the performance of authv1 both in terms 9ofaverage latency reflecting per request costs and high percentile latencies p99 p99.
reflecting worst case costs.
while we got some sense for authv2 java s performance using synthetic data during development we did not get the full picture until we did shadow testing.
in particular we found that service workloads varied a fair bit where this difference owed to their different types of policies and data and to their different deployment environments.
these differences could in turn introduce threshold effects owing to just in time compilation garbage collection and cpu load.
measuring performance during shadow testing gave us a more accurate picture of all this diversity.
shadow testing also gave us an opportunity to use our formal methods driven development process to improve performance.
we started from an improved baseline after the initial rewrite of authv1 but noticed some regressions or room for improvement in corner cases.
in response we optimized the authv2 code proved it correct and then deployed the change during the next shadow testing iteration.
we never during this process observed a correctness related difference owing to an optimization.
examples of optimizations include removing redundant computations specializing standard library methods on a subset of input data and leveraging heap predicates to avoid copying data when it is not mutated.
while we cannot share absolute performance metrics fig.
provides latency histograms for four high traffic services on both authv1 and authv2 .
on these four services average latency improved by p99 by and p99.
by .
moreover authv2 s p95 latency is always less than authv1 s p50 latency.
these improvements are fairly representative of the improvements across all services which showed a latency improvement on average.
viii.
d iscussion and related work the ideas of formal software verification as well as their ideals and criticisms see e.g.
hoare de millo et al.
and fetzer are many decades old.
mechanical tools that aid in the process also had an early start followed by specification facilities being directly integrated into programming languages or associated modeling languages .
the last fifteen years have seen a major expansion of both formal verification technology and the use of that technology in practice .
several substantial software systems have been verified including operating systems os kernels like sel4 and freertos compilers like compcert and cakeml cryptography components or libraries like openssl hmac evercrypt erbsen et al.
almeida et al.
and the aws encryption sdk esdk and communication or routing protocols or components thereof like quic scion and everparse .
all of this software has seen at least some industrial adoption with the most noteworthy perhaps being the evercrypt libraries which are deployed in firefox and the linux kernel both of which are highly security sensitive.these efforts have leveraged a variety of frameworks to yield proofs of thread and memory safety functional correctness and properties about security.
generally speaking there are basically three development approaches write the code in a mainstream language and use a tool to prove properties specified in the language itself and or as annotations.
write the code in a mainstream language and use a separate framework to specify and prove properties about anembedding of that program in the framework.
write the code in a proof oriented programming language ppl a special purpose language in which the code specification and proofs can be expressed.
the ppl code is made executable by stripping its logical annotations and compiling it often in a lightweight manner to an executable target language.
to support the first approach researchers have built verifiers for programs written in languages such as c framac verifast and vcc java openjml and key go gobra ada spark rust prusti creusot and verus and several others.
these tools often work by translating program statements and properties to prove into logical formulae whose truth can be determined by an smt solver.
building these tools to be precise at scale is challenging mainstream languages are rich in features including challenging ones such as mutation manual memory management and multi threading.
of the projects listed above the scion internet router comes closest to taking this approach it used isabelle to verify the correctness of the scion protocol and then used gobra to verify that the go code correctly implements the protocol.
one particular challenge for us is that verification tools for java key and openjml consume specifications written in jml which does not offer the flexibility we needed to both execute specifications to test against a legacy implementation and state and prove properties about the specifications.
other tools also have non executable specification languages to varying degrees and so also suffer this limitation.
to support the second approach one must express the syntax and semantics of the mainstream language using the language of a proof assistant such as isabelle the rocq prover or lean .
this approach allows specifying properties using the full power of the proof assistant s language and proving them interactively and or with automation.
of the above listed verified software the sel4 freertos and openssl hmac developments are examples of this approach all of them considered c programs mechanized in isabelle for the first two and the rocq prover for the last .
this approach is flexible and appealing rather than internally compiling from source to intermediate language it essentially exposes the intermediate language as the object of interactive proof leveraging a highly engineered proof environment.
that said it still suffers in that the full source language could be too richly featured for feasible proofs.
another downside is that this can be difficult from an engineer s perspective they have to 10work with programs as embedded terms in the proof assistant rather as programs in their native syntax with ide support for definition lookup code highlighting etc.
the third approach carries out proof on a program written directly in a ppl but compiles that program to a mainstream language.
compcert erbsen et al.
esdk and evercrypt everparse are examples of this approach with code written in the rocq prover s programming language gallina dafny and the low sublanguage of f .
an advantage of these languages is that they were designed for proof from the start so while they have many familiar constructs they also specifically elide features to make proofs easier to construct whether manually or automatically .
a key challenge of the third approach is compiling to efficient debuggable code.
we met this challenge by writing an idiomatic compiler for dafnylite a subset of dafny we defined.
previously the f team developed low a subset of full f that can be compiled to c using a compiler called karamel .
compared to our compiler the goal is the same intuitive compilation from the limited source to the target language but the details are quite different.
c has manual memory management undefined behavior etc.
which imposes greater limitations on the source language and because evercrypt has few library dependencies karamel has little need to deal with an extensive environment model.
in contrast our dafnylite compiler maps dafny s classes inheritance and error handling as well as our modeled java environment into idiomatic counterparts in java.
our compiler is also unique in its use of translation validation via the java checker framework.
our work is unusual in that we developed a formally verified version of a bespoke legacy system in highly active operation.
most of the other work discussed above focused on verifying the implementation of a published standard e.g.
for a language compiler or a cryptographic algorithm.
thus they had more reliable and meticulous documentation from which to develop the specification and clearer properties to prove.
for us extensive property based and differential testing including against production data was critical to ensure our specification accurately captured the expected behavior and achieved good performance.
another authorizer developed recently with the help of formal methods is cedar .
rather than develop a provedcorrect implementation of cedar via one of the three approaches discussed above the cedar developers took a verification guided approach comprising three parts hand write and prove key properties on an executable specification of cedar in the lean verification aware language hand write an implementation of cedar in rust and use differential random testing to assure that the lean specification and rust implementation behave the same.
verficationguided development resembles the approach we took with authv2 but instead of compiling aproved correct lean implementation to rust as we compiled a proved correct dafnylite implementation to java the cedar developers directly wrote the rust implementation and tested its correctness againstthe lean specification.
this means authv2 offers comparatively higher assurance that its implementation authv2 javatruly matches its specification authv2spec dafny.
this assurance relies on the assumption that the dafnylite compiler produces a correct result whereas cedar relies on the correctness of the lean compiler since the lean spec is compiled to c in order to differentially test it.
we gained substantial assurance in the dafnylite compiler s correctness on authv2 through extensive shadow testing of authv2 javaagainst authv1 through the use of the checker framework and by being able to manually review the generated code which is quite similar to the dafny original .
another difference between cedar and authv2 is that cedar was not a legacy system so it did not involve the same reverse engineering challenges or the far more extensive testing needed for ensuring compatibility and stakeholder buyin that authv2 s development involved.
ix.
c onclusion in this paper we presented a major success story to add to the catalog of formal method wins.
thirty years ago formal methods were still regarded as a technological outsider too difficult too limited too expensive and generally unnecessary.
the strongest evidence for this is the somewhat defensive stance of its proponents .
since then tools and techniques improved significantly and combined with advances in computing power the gap between theory and applications has gradually closed.
this has led to broader adoption and consequently many more successes .
we shared our experience formally specifying and replacing a long running heavily utilized correctness and securitycritical piece of legacy software the aws authorization engine.
this experience included overcoming logical technical and social challenges.
our effort has resulted in one of the most thoroughly analyzed pieces of software at amazon.
we employed a three pronged approach that combines formal verification human code review and systematic largescale testing.
it required three handwritten implementations the development of an idiomatic compiler rigorous manual reviews of the output by our team and stakeholders at every stage and comprehensive testing approaches such as fuzzing regression testing and shadow mode deployment.
the benefits are clear we can now confidently make changes and introduce new features to the software swiftly while preserving robustness performance and backward compatibility.
11references wolfgang ahrendt bernhard beckert richard bubel reiner h ahnle peter h. schmitt and mattias ulbrich editors.
deductive software verification the key book from theory to practice volume oflecture notes in computer science .
springer .
jos e bacelar almeida manuel barbosa gilles barthe benjamin gr egoire adrien koutsos vincent laporte tiago oliveira and pierreyves strub.
the last mile high assurance and high speed cryptographic implementations.
in ieee symposium on security and privacy sp .
amazon web services.
aws encryption sdk for dafny .
https github.com aws aws encryption sdk dafny.
vytautas astrauskas aurel b l y jon a s fiala zachary grannan christoph matheja peter m uller federico poli and alexander j. summers.
the prusti project formal verification for rust.
in nasa formal methods symposium .
springer .
patrick baudin franc ois bobot david b uhler lo c correnson florent kirchner nikolai kosmatov andr e maroneze valentin perrelle virgile prevosto julien signoles and nicky williams.
the dogged pursuit of bug free c programs the frama c software analysis platform.
communications of the acm .
lennart beringer adam petcher katherine q. ye and andrew w. appel.
verified correctness and security of openssl hmac.
in proceedings of the 24th usenix security symposium usenix security pages washington d.c. .
usenix association.
usenixsecurity15 technical sessions presentation beringer.
timothy bourke matthias daum gerwin klein and rafal kolanski.
challenges and experiences in managing large scale proofs.
in proceedings of the 11th conference on intelligent computer mathematics cicm pages berlin heidelberg .
springer verlag.
jonathan p. bowen and michael g. hinchey.
seven more myths of formal methods.
ieee software .
bochmann csi5174 coursenotes literature bowen 20more 20myths.pdf.
robert s. boyer and j strother moore.
a computational logic .
academic press inc. .
boyer acl.pdf.
lilian burdy yoonsik cheon david r. cok michael d. ernst joseph r. kiniry gary t. leavens k. rustan m. leino and erik poll.
an overview of jml tools and applications.
international journal on software tools for technology transfer sttt .
mernst pubs jml tools sttt2005.pdf.
roderick chapman claire dross stuart matthews and yannick moy.
co developing programs and their proof of correctness.
communications of the acm .
roderick chapman and florian schanda.
are we there yet?
years of industrial theorem proving with spark.
in interactive theorem proving pages berlin heidelberg .
springer verlag.
https proteancode.com keynote.pdf.
checker framework.
the checker framework .
https checkerframework.org .
nathan chong and bart jacobs.
formally verifying freertos interprocess communication mechanism.
embedded world exhibition conference .
formally verifying freertos interprocess communication mechanism.
koen claessen and john hughes.
quickcheck a lightweight tool for random testing of haskell programs.
in proceedings of the fifth acm sigplan international conference on functional programming icfp pages new york ny .
association for computing machinery.
nr cs257 archive john hughes quick.pdf.
ernie cohen markus dahlweid mark a. hillebrand dirk leinenbach micha moskal thomas santen wolfram schulte and stephan tobies.
vcc a practical system for verifying concurrent c. in stefan berghofer tobias nipkow christian urban and makarius wenzel editors theorem proving in higher order logics 22nd international conference tphols volume of lecture notes in computer science pages .
springer .
david r. cok gary t. leavens and mattias ulbrich.
java modeling language jml reference manual .
the coq development team.
the coq proof assistant .
https coq.inria.fr.
joseph w. cutler craig disselkoen aaron eline shaobo he kyle headley michael hicks kesha hietala eleftherios ioannidis john kastner anwar mamat darin mcadams matt mccutchen neha rungta emina torlak and andrew m. wells.
cedar a new language for expressive fast safe and analyzable authorization.
proc.
acm program.
lang.
oopsla1 april .
the dafny community.
the dafny programming language .
https dafny.org .
richard a. de millo richard j. lipton and alan j. perlis.
social processes and proofs of theorems and programs.
communications of the acm may .
leonardo de moura and sebastian ullrich.
the lean theorem prover and programming language.
in automated deduction cade pages berlin heidelberg .
springer verlag.
ipd.kit.edu uploads publikationen demoura21lean4.pdf.
antoine delignat lavaud c edric fournet bryan parno jonathan protzenko tahina ramananandro jay bosamiya joseph lallemand itsaka rakotonirina and yi zhou.
a security model and fully verified implementation for the ietf quic record layer.
in proceedings of the ieee symposium on security and privacy sp pages washington dc .
ieee computer society.
security model verified implementation quic record layer .
xavier denis jacques henri jourdan and claude march e. creusot a foundry for the deductive verification of rust programs.
in proceedings of the 23rd international conference on formal engineering methods icfem .
springer verlag .
werner dietl stephanie dietzel michael d. ernst kivanc mus lu and todd w. schiller.
building and using pluggable type checkers.
in proceedings of the 33rd international conference on software engineering pages .
craig disselkoen aaron eline shaobo he kyle headley michael hicks kesha hietala john kastner anwar mamat matt mccutchen neha rungta bhakti shah emina torlak and andrew wells.
how we built cedar a verification guided approach.
in companion proceedings of the 32nd acm international conference on the foundations of software engineering pages new york ny usa july .
association for computing machinery.
andres erbsen jade philipoom jason gross robert sloan and adam chlipala.
simple high level code for cryptographic arithmetic with proofs without compromises.
acm sigops operating systems review .
james h. fetzer.
program verification the very idea.
communications of the acm .
.
.
jean christophe filli atre l eon gondelman and andrei paskevich.
the spirit of ghost code.
formal methods in system design .
robert w. floyd.
assigning meanings to programs.
proceedings of the symposium on applied mathematics .
eecs.berkeley.edu necula papers floydmeaning.pdf.
the f development team.
f a proof oriented programming language .
the f development team.
fstarlang karamel karamel is a tool for extracting low level f programs to readable c code.
.
.
j. v .
guttag j. j. horning s. j. garland k. d. jones a. modet and j. m. wing.
larch languages and tools for formal specification .
spring verlag .
www publications larchbook.pdf.
anthony hall.
seven myths of formal methods.
ieee software .
examples hall7myths.pdf.
chris hawblitzel jon howell manos kapritsos jacob r. lorch bryan parno michael lowell roberts srinath t. v .
setty and brian zill.
ironfleet proving safety and liveness of practical distributed systems.
communications of the acm .
chris hawblitzel jon howell jacob r. lorch arjun narayan bryan parno danfeng zhang and brian zill.
ironclad apps end to end security via automated full system verification.
in jason flinn and hank levy editors 11th usenix symposium on operating systems design and implementation osdi pages .
usenix association .
gernot heiser gerwin klein and june andronick.
sel4 in australia from research to real world trustworthy systems.
communications of the acm .
csiro full text heiser ka .pdf.
michael g. hinchey and jonathan p. bowen.
industrial strength formal methods in practice .
springer science business media .
c. a. r. hoare.
the mathematics of programming.
in foundations of software technology and theoretical computer science pages berlin heidelberg .
springer verlag.
https ora.ox.ac.uk objects uuid dcd96ff3 8fd9 892641107d63 files m88f2b80f31ed89dead6c09c65011ce07.
wei huang ana milanova werner dietl and michael d. ernst.
reim reiminfer checking and inference of reference immutability and method purity.
acm sigplan notices .
ahmed irfan sorawee porncharoenwase zvonimir rakamaric neha rungta and emina torlak.
testing dafny experience paper .
in proceedings of the 31st acm sigsoft international symposium on software testing and analysis issta pages new york ny .
association for computing machinery.
science publications testing dafny experience paper.
bart jacobs and frank piessens.
the verifast program verifier.
technical report cw department of computer science katholieke universiteit leuven belgium august .
gerwin klein june andronick kevin elphinstone gernot heiser david a. cock philip derrin dhammika elkaduwe kai engelhardt rafal kolanski michael norrish thomas sewell harvey tuch and simon winwood.
sel4 formal verification of an operating system kernel.
communications of the acm .
gerwin klein june andronick matthew fernandez ihor kuz toby murray and gernot heiser.
formally verified software in the real world.
communications of the acm .
trustworthy.systems publications csiro full text klein akmhf .pdf.
ramana kumar magnus o. myreen michael norrish and scott owens.
cakeml a verified implementation of ml.
in proceedings of the 41st acm sigplan sigact symposium on principles of programming languages .
andrea lattuada travis hance chanhee cho matthias brun isitha subasinghe yi zhou jon howell bryan parno and chris hawblitzel.
verus verifying rust programs using linear ghost types.
proc.
acm program.
lang.
oopsla1 april .
thierry lecomte.
applying a formal method in industry a year trajectory.
in formal methods for industrial critical systems pages .
berlin heidelberg .
citeas.
k. rustan m. leino.
dafny an automatic program verifier for functional correctness.
in proceedings of the 14th international conference on logic for programming artificial intelligence and reasoning lpar pages berlin heidelberg .
springer verlag.
dafny automatic program verifier functional correctness .
xavier leroy sandrine blazy daniel k astner bernhard schommer markus pister and christian ferdinand.
compcert a formally verified optimizing compiler.
in erts embedded real time software and systems 8th european congress toulouse france .
see.
d. c. luckham s. m. german f. w. v. henke r. a. karp p. w. milne d. c. oppen w. polak and w. l. scherlis.
stanford pascal verifier user manual.
stanford verification group .
pub cstr reports cs tr cs tr .pdf.
christopher a. mackie.
preventing signedness errors in numerical computations in java.
in proceedings of the 24th acm sigsoft international symposium on foundations of software engineering pages .
william m mckeeman.
differential testing for software.
digital technical journal .
sean mclaughlin georges axel jaloyan tongtong xiang and florian rabe.
enhancing proof stability.
in proceedings of the dafny workshop popl .
association for computing machinery .
enhancing proof stability.
bertrand meyer.
object oriented software construction .
series in computer science.
prentice hall international .
com wp content uploads oosc2.pdf.
microsoft.
shadow testing .
code with engineering playbook automated testing shadow testing .
f. l. morris and c. b. jones.
an early program proof by alan turing.
annals of the history of computing .
tobias nipkow markus wenzel and lawrence c. paulson.
isabelle hol a proof assistant for higher order logic .
springer verlag berlin heidelberg .
paul r. holser jr. junit quickcheck property based testing junit style .
jo ao c. pereira tobias klenze sofia giampietro markus limbeck dionysios spiliopoulos felix a. wolf marco eilers christoph sprenger david basin peter m uller and adrian perrig.
protocols to code formal verification of a next generation internet router.
technical report arxiv .
jonathan protzenko bryan parno aymeric fromherz chris hawblitzel marina polubelova karthikeyan bhargavan benjamin beurdouche joonwon choi antoine delignat lavaud c edric fournet natalia kulatova tahina ramananandro aseem rastogi nikhil swamy christoph m. wintersteiger and santiago zanella b eguelin.
evercrypt a fast verified cross platform cryptographic provider.
in proceedings of the ieee symposium on security and privacy sp pages washington dc .
ieee computer society.
https ef ac 81ed cross platform cryptographic provider .
jonathan protzenko jean karim zinzindohou e aseem rastogi tahina ramananandro peng want santiago zanella b eguelin antoine delignat lavaud c at alin hrit cu karthikeyan bhargavan c edric fournet and nikhil swamy.
verified low level programming embedded in f .proceedings of the acm on programming languages icfp .
tahina ramananandro antoine delignat lavaud c edric fournet nikhil swamy tej chajed nadim kobeissi and jonathan protzenko.
everparse verified secure zero copy parsers for authenticated message formats.
in proceedings of the 28th usenix security symposium usenix security pages santa clara ca .
usenix association.
usenixsecurity19 presentation delignat lavaud.
alastair reid luke church shaked flur sarah de haas maritza johnson and ben laurie.
towards making formal methods normal meeting developers where they are.
in presented at the hatra workshop .
suhrid satyal ingo weber hye young paik claudio di ciccio and jan mendling.
shadow testing for business process improvement.
in on the move to meaningful internet systems.
otm conferences confederated international conferences coopis c tc and odbase valletta malta october proceedings part i volume of lecture notes in computer science pages .
gerald schermann j urgen cito and philipp leitner.
continuous experimentation challenges implementation techniques and current research.
ieee software .
amazon web services.
iam policy elements condition operators .
policies elements condition operators.html.
nikhil swamy tahina ramananandro aseem rastogi irinia spiridonova haobin ni dmitry malloy juan vasquew michael tang omar cardona and arti gupta.
hardening attack surfaces with formally proven binary format parsers.
in proceedings of the 43rd ac siglan international conference on programming language design and implementation pldi pages new york ny .
association for computing machinery.
hardening attack surfaces with formally proven binary format parsers .
benjamin weyers michael d. harrison judy bowen alan j. dix and philippe a. palanque.
case studies.
in the handbook of formal methods in human computer interaction pages berlin heidelberg .
springer verlag.
.
felix a. wolf linard arquint martin clochard wytse oortwijn jo ao c. pereira and peter m uller.
gobra modular specification and verification of go programs.
in alexandra silva and k. rustan m. leino editors computer aided verification cav volume of lecture notes in computer science pages .
springer .
yi zhou jay bosamiya jessica li marijn heule and bryan parno.
context pruning for more robust smt based program verification.
in 13proceedings of the formal methods in computer aided design fmcad conference october .
yi zhou jay bosamiya yoshiki takashima jessica li marijn heule and bryan parno.
mariposa measuring smt instability in automated program verification.
in proceedings of the formal methods in computeraided design fmcad conference .