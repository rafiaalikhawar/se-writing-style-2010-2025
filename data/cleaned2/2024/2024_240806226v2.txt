accepted at international conference on software engineering icse a large scale study of model integration in ml enabled software systems yorick sens henriette knopp sven peldszus thorsten berger ruhr university bochum germany chalmers university of gothenburg sweden abstract the rise of machine learning ml and its integration into software systems has drastically changed development practices.
while software engineering traditionally focused on manually created code artifacts with dedicated processes and architectures ml enabled systems require additional data science methods and tools to create ml artifacts especially ml models and training data.
however integrating models into systems and managing the many different artifacts involved is far from trivial.
ml enabled systems can easily have multiple ml models that interact with each other and with traditional code in intricate ways.
unfortunately while challenges and practices of building ml enabled systems have been studied little is known about the characteristics of realworld ml enabled systems beyond isolated examples.
improving engineering processes and architectures for ml enabled systems requires improving the empirical understanding of these systems.
we present a large scale study of open source ml enabled software systems.
we classified and analyzed them to determine system characteristics model and code reuse practices and architectural aspects of integrating ml models.
our findings show that these systems still mainly consist of traditional source code and that ml model reuse through code duplication or pre trained models is common.
we also identified different ml integration patterns and related implementation practices.
we hope that our results help improve practices for integrating ml models bringing data science and software engineering closer together.
index terms machine learning ai engineering se4ai i. i ntroduction many recent breakthroughs in machine learning ml have given rise to ml enabled software that was not realizable before.
consider cyber physical systems such as autonomous vehicles or unmanned aerial vehicles as well as software in finance or healthcare .
all these systems benefit from advances in ml model architectures and training algorithms.
nevertheless developers still struggle when developing software systems that integrate ml models .
in fact industrial surveys report that of ml projects stall while others say as many as fail .
many factors can cause such failures including development processes data quality system architecture tool support quality assurance and the integration of ml models with traditional software components the focus of our work.
integrating ml models in software systems is challenging.
executing the models requires boilerplate code and extra infrastructure .
the lack of behavioral guarantees in ml models being probabilistic by nature requires additional safeguards especially in safety critical systems .
safeguards are typically implemented manually to prevent undefined behavior for certain inputs as well as invalid resultsthat would violate domain restrictions.
these challenges are further exacerbated by systems that integrate multiple models.
extreme examples are perception systems in autonomous driving some of which boast up to different ml models that interact with each other and are safeguarded with manually implemented heuristics.
furthermore training and developing ml models is often costly which made ml model reuse a common practice in particular reusing pre trained ml models allows leveraging larger models trained on more data but does not fit well with current software engineering practices.
unfortunately empirical insights on reuse practices are missing.
research on engineering ml enabled systems has focused on challenges and practices of engineering such systems .
researchers also created novel workflows and tooling e.g.
experiment management tools to address the challenges.
much work also focused on dependability and safety of ml models .
however building ml enabled systems requires holistic methods and tools to systematically integrating traditional software and data science artifacts in a whole system as well as maintaining and evolving such systems.
progress in this area has been limited so far which can be attributed to a lack of understanding of the current practices and concrete problems of developing mlenabled software systems.
in other words while researchers have focused on ml engineering still little is known about how ml models are integrated into ml enabled systems in practice.
we present an empirical study of ml enabled software repositories on github characterizing the use of ml in software systems.
we focused on the reuse of ml models across software systems as well as the integration of ml models.
our analysis addresses three main research questions rq1 what are characteristics of ml enabled software systems?
to understand such systems we studied what role ml plays in them and what characterizes them.
for instance what is the proportion of ml code in relation to non ml source code what types of software systems employ ml e.g.
libraries or end user oriented applications and how?
rq2 how are ml models reused across software systems?
the growing size e.g.
number of trainable parameters of ml models both enables and necessitates their reuse across software systems as they become more general in their capabilities but also more difficult to train .
we explored how reuse is done in practice like reuse of source code has been studied in previous works .
rq3 how is the integration of ml models reflected in architectural aspects of software systems?
we explored howarxiv .06226v2 feb 2025ml models are integrated into systems and what functionality they realize.
we identified how many models are used how they interact and how they are integrated with code for instance with pre and post processing methods.
we contribute insights on the share of ml related code of ml enabled systems and its distribution showing that even in mlenabled systems traditional source code makes up for most parts of the system insights into ml reuse practices highlighting a strong reliance on pre trained models as well as the practice of copying source code implementing ml models between systems acatalog of ml integration patterns and coding practices derived from an in depth analysis of applications that use ml to provide end user oriented functionality and a dataset of ml enabled software systems of which we manually classified according to the type of the system and their relation to ml.
for replication and further studies we provide a replication package including the dataset and analysis scripts .
we hope that our findings will help practitioners and researchers better understand the integration of ml models in ml enabled systems and that our data will be useful for further research.
ii.
m otivation and research questions we present the necessary background the state of research as well as motivate and refine our research questions.
ml enabled systems contain ml based functionality ranging from utility functions to application logic.
this functionality can include predictions data analysis or generative content creation .
ml enabled systems incorporate various ml technologies such as deep learning dl convolutional neural networks cnns or transformers .
this integration of ml into software poses new challenges for development processes architectures and testing among others .
many originate from the fundamentally different nature of ml models compared to traditional software .
ml models are probabilistic essentially constituting unreliable functions while traditional software is more deterministic.
characteristics of ml enabled software systems rq1 .
while there exists extensive theory in data science the majority of techniques is model centric.
however integrating ml models into software systems to provide value to end users requires system centric engineering methods.
effective methods need to be tailored towards the actual characteristics and properties of ml in the context of systems.
to this end we need to improve our empirical understanding of ml enabled systems in general.
furthermore software systems can be of different types such as applications libraries and frameworks which impacts the use of ml.
rq1.
what types of systems use ml and how are they related to it?
first determines what systems e.g.
applications or libraries integrate ml models and what ml is used for e.g.
for end user functionality or purely conceptual contributions .
this puts theresults of our study into context and helps identify differences in the use of ml as well as implications for practice.
ml enabled systems rely on ml libraries and frameworks.
the most popular ones are pytorch tensorflow and scikit learn .
many others build upon them.
while tensorflow and pytorch offer dl scikit learn offers different techniques mostly traditional ml ranging from supervised methods e.g.
linear regression and perceptron to unsupervised methods e.g.
clustering and pca .
tensorflow and pytorch are frameworks designed for a modular implementation of ml models.
a class module baseestimator in scikit learn represents either complete models or their building blocks e.g.
layers .
developers typically extend this class often using predefined modules such as linear or convolution layers.
ml enabled systems contain additional artifacts called ml assets in the remainder including model implementations model binaries ml training code and other ml files.
these ml assets must be integrated with code and co evolved.
rq1.
how much of ml enabled systems is specific to ml?
elicits details on the relevance type and characteristics of ml assets.
finally while quality assurance is an integral part of ml model training also the system has to be considered.
however the concrete quality assurance practices in ml enabled systems have not been captured on a large scale beyond individual qualitative studies .
to obtain insights on these practices rq1.
what are quality assurance practices of ml enabled systems?
investigates quality assurance on a large scale.
reuse of ml models rq2 .
like reuse in traditional software developers reuse ml assets.
it is commonly known that there is reuse of established ml models allowing developers to use them without the extensive effort required to build a new model .
such models are called pre trained models as they are optimized and fully functional.
pre trained models for specific tasks are distributed through model hubs.
jiang et al.
identified model hubs when studying the content of their provided artifacts and related security risks pytorch hub tensorflow hub hugging face model zoo onnx model zoo model hub nvidia ngc and matlab model hub .
while we know all these sources from which pre trained models could be reused we know little about the actual practices neither how prevalent this reuse is nor how it is realized.
rq2.
to what extent are pre trained models used?
addresses this gap.
apart from full models other ml assets e.g.
scripts model binaries can be reused as well.
however there are currently no studies on such reuse and its relation to model reuse e.g.
reusing implementations of ml models as an alternative to pretrained models.
related studies focus on how to identify model plagiarism or on reuse practices of models from specific libraries or platforms .
consequently we need to improve our empirical understanding to what degree ml models are currently reused what other assets are also reused and how that reuse is realized addressed by rq2.
to what extent are ml assets reused between ml enabled systems?
.
architectural aspects rq3 .
the architecture of ml enabled systems revolves around ml models ml assets and traditionalgithub original ml related systems python loc starsdataset systems qa related assetssearch results sample of dataset systems a dataset extraction b quantitative analysis applied systems systems search for systems system classification loading of pretrained modelscode duplicateslegend artifact processdescriptive statisticsml assets manual in depth analysisapply filtering criteria c qualitative analysisfig.
methodology overview software which must be integrated together.
however little is known on how the interaction of ml models can impact the behavior of a system.
while some architectures for ml enabled systems have been proposed they usually do not focus on concrete interactions and topologies of ml models but rather on high level system abstractions.
this includes for instance a microservice architecture where ml models are modularized and interact with the rest of the system via rest .
others suggest separating ml logic from the business logic of a system .
ml models are typically encapsulated in software modules that contain the ml model binary along with source code for model loading and execution including required pre and postprocessing steps.
common anti patterns include excessive glue code complicated data processing pipelines dead experimental code paths and lack of abstraction .
since ml models are updated or replaced regularly i.e.
by an optimized version which in turn affects the remaining system e.g.
thresholds in glue code need to be adjusted such anti patterns hinder optimization.
little is known about best practices in such situations.
to derive such rq3.
how are ml models embedded into traditional code?
investigates current ml model integration patterns.
complex systems can integrate many ml models.
recall perception systems with up to models integrated in complex topologies with non trivial interactions and dependencies between models .
while current research discusses the implications of such topologies e.g.
propagating error computational complexity it is important to study these in a larger dataset and define common patterns addressed by rq3.
what are interaction patterns of ml models?
.
iii.
m ethodology our methodology summarized in fig.
comprised the mining of repositories with ml enabled systems from github and their quantitative and qualitative analysis.
while some sub rqs could be answered by analyzing the whole dataset automatically other sub rqs required qualitative analysis of random samples.
a. subject selection we mined repositories from github as follows considering typical mining strategies .
inclusion criteria.
we used github s dependency graph to obtain repositories that use one of the three most popular mllibraries ic1 and that are implemented in the currently most popular programming language for ml python ic2 .
ic1 the repository depends on tensorflow pytorch or scikit learn .this criterion aimed to identify systems implementing or using ml technologies.
ml libraries support developing especially training ml models but are also needed to execute the models.
ic2 the main programming language is python.
this criteria aimed to ensure the relevance of our analysis since python is the currently most popular language for mlenabled systems and the comparability of the subject systems by focusing only on one language.
applying ic1 yielded repositories of which dependent on pytorch on tensorflow and on scikit learn and ic2 in python repositories.
exclusion criteria.
we then filtered the obtained repositories to create a high quality set of ml enabled software systems of a certain size and maturity.
we filtered out repositories if at least one of the following criteria was met.
ec1 the repository is not original i.e.
forked or duplicated from another repository in the dataset.
forks would distort the results as popular projects with many forks would be counted multiple times and disproportionately affect the results without providing additional insights.
ec2 the repository has fewer than stars.
sufficient popularity ensures to avoid practically irrelevant projects.
ec3 the codebase contains fewer than lines of python code.
this criterion aimed at ensuring a certain size of the code excluding tutorials and toy projects.
the thresholds for ec2 and ec3 were chosen based on previous studies and manual exploration of a sample.
we set them relatively low to exclude simple toy projects but still obtain a large representative dataset.
we excluded forks with ec1 in two steps first we removed projects that were forked directly from github which can be retrieved through github s rest api.
second since we found that the dataset still contained a significant number of duplicates we identified them based on identical readme files and kept only the repository that was created first.
this selection resulted in a final dataset of repositories.
b. quantitative analysis we analyzed the ml enabled systems by creating analysis scripts and using automated tools.
that was possible for the following research questions which we answered as follows.
descriptive statistics.
to put all our results into context and to determine to what kinds of systems they apply we first created descriptive statistics.
typical descriptive statistics are scale and popularity of the subjects .
for popularity we used github stars which can also be seen as an indicator of maturity.
to determine the systems scales we used common code metrics number of commits source files lines of code number of classes and functions.
these metrics help to contextualize results and highlight the scale and relevance of our dataset.
we also provide statistics on the use of ml libraries also whether multiple libraries are used.general characteristics rq1 .the first research question that can be answered quantitatively on the full dataset is rq1.
how much of ml enabled systems is specific to ml?
since we were able to automatically identify ml assets use metrics to measure their characteristics and compare those to traditional software assets.
to identify the ml related parts of our subject systems we extracted ml assets by searching for the following typical ml assets implementations of ml models recall that models are implemented by extending the class module pytorch and tensorflow or baseestimator scikit learn sec.
ii .
we searched for class definitions that inherit from these base classes or any of their subclasses provided in the libraries or defined in the subject systems.
ml related functions we extracted all functions and class methods that either use an api of an ml library or instantiate an ml model.
functions defined within an ml model are considered ml related regardless of whether the specific function directly interacts with an ml library.
stored model files ml models are often stored in binary files for later reuse typically using the extensions .pt or .pth pytorch and .h5 or .pb.
tensorflow .
we counted the number of such files present in the systems.
we compared the general code metrics of the traditional code assets to those of the identified ml assets.
this analysis provides insights into the extent of the ml specific assets compared to traditional software assets.
we also answered rq1.
what are quality assurance practices of ml enabled systems?
quantitatively on our full dataset using automated techniques.
we analyzed to what degree the source code especially the ml related part is covered by unit tests and how the models are evaluated.
to identify test cases we searched the source code for implementations of python s unittest framework.
we then identified the unit under test by analyzing the method calls invoked.
this allowed us to measure the number of functions directly covered by tests for ml and non ml functions respectively.
to investigate quality assurance of ml models we extracted usages of ml validation functions following another work analyzing ml related projects .
reuse of ml models rq2 .the research question rq2.
to what extent are pre trained models used?
was answered quantitatively on the full dataset since we were able to detect the presence of pre trained models automatically.
recall that using pre trained models distributed through model hubs sec.
ii is a systematic way of model reuse.
we relied on the list of jiang et al.
.
from each hub s documentation we extract a list of apis used to load pre trained models in python.
the full list can be found in our replication package .
we then identified which systems use the apis and where the model is loaded in the system collecting a list of all related files.
furthermore we analyzed the source of the api calls in the system by categorizing them as test demonstration and other directories according to their name.
we report the number of systems that contain an api used to load pre trained models and where they are loaded.
we also analyzed reuse of ml assets at a large scale amongall systems in our full dataset answering rq2.
to what extent are ml assets duplicated between ml enabled systems?
quantitatively.
specifically to identify duplicated ml code we applied a code clone detection tool to the ml enabled systems.
we decided to use jplag which supports python and is ranked as one of the best tools in multiple literature reviews .
in a pairwise comparison a match is found when a sequence of tokens of a certain minimum length for two files match.
we used a threshold of tokens to avoid false positives.
based on these results we identified the most prominent sources of duplicated model implementations by sorting the subject systems by the number of duplicated tokens shared with all others systems.
then starting from the projects with the highest amount of duplicated code we manually inspected each system to determine if they are original or what system they are based on.
to this end we analyzed the copied source files and searched for hints on the original system.
usually the copyright information was left in place crediting the original repository.
this analysis was repeated until we encountered no more original systems for times in a row.
we report the number of code duplicates from each original system in our dataset.
since some original systems were not contained in the dataset e.g.
because they were not stored on github we use another system that only contains a complete duplicate from this system and no other as a placeholder to report the number of duplicates from the original one.
c. qualitative analysis system classification for our qualitative analysis we selected a random sample of systems .
of our dataset and answered the following research questions requiring qualitative analysis.
to answer rq1.
what types of systems use ml and how are they related to it?
we classify systems inspired by studies that investigated properties of ml enabled systems such as ml model development stages and quality assurance.
more specifically we classify according to the following two dimensions type of systems i.e.
application or library and their relation to ml i.e.
what are the ml based functionalities and how are they used .
we manually examined and labeled the sample in several iterations in which the definitions of system type and relation to ml were continuously adjusted.
two authors classified batches of systems independently and then discussed their disagreements.
a third author was the mediator if no consensus was reached.
we started with projects to develop an understanding of what is present in the dataset and to define the labels.
thereafter the two authors classified batches of systems independently.
we used krippendorff s alpha for the agreement score.
as the score was below .
in the first iterations the authors discussed their disagreements.
after four iterations we achieved an alpha of .
for the system type and an alpha of .
for the relation of ml confirming a common knowledge base and correctness of the assigned labels.
based on the improved understanding the previous systems including the initial sample were reclassified by one author with the second author checking the results.d.
qualitative analysis manual in depth analysis to answer rq3 and its sub research questions rq3.
how are ml models embedded into traditional code?
andrq3.
what are interaction patterns of ml models?
a manual analysis of the source code was necessary to identify relations of ml models and traditional software components.
since analyzing systems manually is not feasible we selected all systems that are end user oriented applications and that use ml to realize business logic type application and label businessfocused applied cf.
sec.
iii b .
for the selected systems we collect insights on architectural patterns and report concrete details through examples.
our analysis is inspired by peng et al.
who studied various aspects of model integration in an autonomous driving perception system cf.
sec.
ii .
upon the research questions and our experience we defined relevant aspects to extract.
we then systematically examined the implementation of each system and documented observations related to the aspects the raw data is in our appendix .
we time boxed the manual analysis to two hours per system.
if we could not gather enough information in time for instance due to insufficient documentation or poorly structured source code we noted the respective aspect as unknown.
to structure the analysis we first identified the code related to the ml models as described in sec.
iii b as entry points for manual exploration.
for each entry we analyzed the surrounding code to understand the model integration.
then using the ide s call graph navigation we determined where models are instantiated.
we disregarded instantiations that are tests or demonstrations or that occur in files that are no longer used but are kept by developers.
for instantiations that occur in actual production code we then analyzed the surrounding code to determine the input and output of the model.
if necessary we followed the method calls further to clarify the data format of the input and output as well as possible pre and post processing steps characterizing the data flow and topologies of the models in a system.
after checking each instantiation for each model we summarized the results to obtain the total number of models used.
all projects are details in our online appendix .
we collected the following aspects.
the former are descriptive aspects putting the results from the quantitative analysis iii b into context.
the latter are related to model integration answering rq3.
and interaction patterns answering rq3.
.
descriptive aspects.
while these aspects are related to general characteristics of ml enabled systems rq1 and practices on model reuse rq2 our qualitative analysis goes into much more technical detail on the sample of systems.
ml functionalities and tasks what are the ml models used for?
what functionality is implemented with ml?
based on which ml task is the functionality implemented?
number and type of models how many ml models are used?
what is their model architecture cnn rnn etc ?
how are they trained supervised unsupervised etc.
?
model origins is the model custom built completely pretrained or pre trained with additional fine tuning?
besides quantitative statistics about model reuse rq2 we comple mented these with qualitative aspects e.g.
how tailored .
model embedding.
the following aspects address rq3.
how are ml models embedded into traditional code?
storing loading of models are the models stored locally in the repository or loaded from a remote source and how?
models need to be instantiated e.g.
load model parameters .
while we investigated this aspect quantitatively with rq2 we went into more detail and analyzed model instantiation qualitatively e.g.
what and how apis are used to understand their integration into traditional code.
model in output what is the input and output of ml models?
what data types are used?
how ml models are embedded into a system also depends on the data being processed.
we examine the type of input and output data to understand what data is used in ml enabled functionality.
pre postprocessing what pre and postprocessing steps are implemented for the ml models?
since different parts of a system typically use different data formats to actually use data as input for an ml model and to use its output it often needs to be processed before and after.
also other functions that need to be performed on this data such as safeguards are often considered essential.
this aspect aims to understand the specifics needed for integrating an ml model.
interaction patterns.
our final aspect addresses rq3.
what are integration patterns of ml models?
.
model interaction how do multiple models interact?
we specifically analyzed systems that contain multiple ml models with respect to the interaction of their models.
as described above we followed call graphs to comprehend the code and to determine data flows between the models.
we sketched the topologies and when similar ones appeared in at least two systems defined these as a pattern.
the patterns from the case study by peng et al.
provided a basis.
e. threats to validity internal validity personal biases of the authors e.g.
expectancy bias may have affected the selection of subject systems and how these were analyzed quantitatively.
to mitigate this threat we based selection criteria on external sources such as studies that identify the most popular ml libraries .
similarly we built up our analysis on previous works and reused existing tooling e.g.
jplag to avoid internal bias in the analysis.
the classification of the systems may be subject to author bias potentially impacting how the systems are classified and different perspectives of the labeling authors may impact the validity of the labels.
to address author bias we involved multiple authors in labeling the systems and held frequent discussions on the labels involving a third author as mediator.
to address different perspectives we only continued with independent labeling after reaching a sufficiently high krippendorff s alpha.
author bias may also affect the manual analysis of the selected systems sec.
iii d i.e.
impacting what is considered relevant.
to mitigate this threat we structured the analysis using aspects and questions and frequently discussed our findings among three authors.
stars102103104105 commits100101102103104 source files101102103104 loc104105106107 classes100101102103104 functions102103104105 fig.
general statistics of the systems in our dataset external validity our quantitative analysis is limited to python systems and three ml libraries pytorch tensorflow and scikit learn which threatens the generalizability of the results.
however since both python and the selected libraries are most widely used for ml enabled systems our research covers the majority of relevant systems.
due to the sample size of of systems the observed distributions of system type and relation to ml are only representative with a relatively high margin of error up to .
at the confidence level for conceptually applied .
the manually analyzed subset of systems is so small that we can only present qualitative observations.
our automated analysis relies on parsing python code into asts which failed for individual files but the error rate on average .
of the files is negligible.
analyzing the number of ml related functions is difficult due to the use of wrappers e.g.
one function loads a model and another one uses it.
as a result we only show a lower bound on the number of ml functions and more accurate qualitative observations as part of our manual analysis.
the sometimes generic names of the apis used to load pre trained models may lead to false positives when identifying the use of pre trained models.
to address this we only consider api calls in the context of a model hub and are not ambiguous about the loaded object.
iv.
s ystem characteristics rq1 we now present characteristics of the systems including their types ml assets and quality assurance practices.
descriptive statistics.
we mined ml enabled systems that vary in scale significantly see fig.
.
on average they have source files median loc median classes median and functions median .
they are of substantial popularity with an average of stars median and commits median .
outliers are the algorithms 181k stars a collection of ml algorithms for educational purposes and azure sdk for python with the largest codebase of 10m loc.
figure shows the used ml libraries.
pytorch is most common of systems followed by scikit learn and tensorflow of systems .
more than half of the systems use more than one library indicating that different types of ml are used.
number of projectspyt orchscikit learnt ensorflowlibrary230715251114 number of projects123 libraries13351166426 fig.
ml libraries used in the ml enabled systemstable i types of systems and their relation to ml total1application library framework plugin total132 business conceptual ml tool 1total number of systems of a given type of relation to ml independent of the other dimension.
each system can have multiple labels in each category.
a. system classification rq1.
upon our random sample of systems we derived labels for system types and their relation to ml.
our appendix has detailed label descriptions examples and labeling guidelines.
table i shows the results of the classifications.
system types.
we define the following four types of systems.
applications are standalone executable systems that provide end user oriented functionality through a user interface e.g.
gui or command line .
they do not require users to have programming skills.
applications solve problems and provide solutions to end users implementing business logic to do so.
libraries provide functionality intended to be used in other programs via code apis.
except for the provided functionality they contain no application logic and cannot run standalone.
frameworks provide general application logic but are designed to be extended by concrete functionality orchestrated by the framework.
to this end frameworks take control of the code that uses the framework according to the principle of dependency inversion .
similar to libraries frameworks usually provide generic helper functions that are frequently used in the context of the framework.
plugins extend applications with functionality via extension points and only function through them.
users interact with plugins through the ui of the applications.
our dataset consists mainly of libraries followed by applications and frameworks and almost no plugins.
relation to ml.
we identified three ways in which an mlenabled system can be related to ml i.e.
how these use ml.
business focused applied refers to systems that apply ml on a real world problem and provide a benefit to end users.
they are generally focused on processing or generating data linked with business logic to encapsulate ml.
conceptually applied refers to systems that demonstrate how ml technologies can be used to solve a variety of problems.
they are focused on evaluating an ml model from a data scientist s perspective and have a technical focus.
ml tools refers to tools that support building ml models usually libraries or frameworks that implement ml functions and algorithms such as loss functions and optimizers e.g.
tensorflow but also tools that support the general ml development processes such as experiment management tools .
the relation of the systems to ml is almost equally distributed with of the systems using ml conceptually supporting building ml models and actually using ml to provide end user oriented functionality.
we further investigated common combinations of project type and relation to ml.
notably the majority of systems arelibraries that apply ml conceptually .
most applications and all plugins are business focused but most business focused systems are libraries .
frameworks are mainly ml tools that allow building ml models .
overall ml projects on github appear to mainly demonstrate ml functionalities and provide it to other software.
b. extent of ml specific assets and relation to code rq1.
ml assets.
the most prevalent assets in our full dataset are model implementations in code which are present in systems .
the average number of classes per project that inherit from module or baseestimator is the median is .
these classes are either complete models or components.
nevertheless it is safe to assume that the average project contains multiple ml models since one model is unlikely to be composed of modules.
additionally entire ml models can be loaded from binary files containing the weights for configuring models in code.
these are however only contained in repositories with each of these containing on average such files median .
figure gives an overview of implemented modules and model binary files.
since some forms of ml e.g.
pca clustering requires no models to be stored some systems contain no such assets.
relation of ml and non ml code.
to determine the extent of ml related code we measured the amount of ml related and non ml related code in our full dataset.
as shown in fig.
on average of the source files median and of the functions median are ml related.
so traditional code still makes up most of the systems.
our dataset contains systems with different relationships to ml including those that apply ml to deliver functionality business focused those that demonstrate possible applications conceptual as well as ml tools that support the development of other systems.
we investigated on our manually classified sample how this affects the share of ml related files and functions.
while the share of ml code is the highest for conceptual ml systems on average of files and of functions even in these systems traditional source code such as utility functions data preprocessing tests and demonstrations makes up most of the system.
in business focused applied ml systems code related to ml is the lowest on average of files and of functions among all types of systems investigated since ml is used only for individual features.
similarly ml tools provide many additional features such as uis but usually focus more on ml technology.
for these systems the average share of ml code is of the files and of the functions which is in between the other two categories.
t otal classes ml module classes model binary files fig.
number of ml modules model binaries in the systems files functions full dataset0.
.
.0share of ml related files functions files functions business focused sample .
.
files functions conceptual sample .
.
files functions ml tools sample .
.5fig.
share of ml code for the full dataset and the classified sample according to the projects relation to ml c. testing rq1.
to investigate testing practices on the full dataset we analyzed the prevalence of unit tests and the proportion of functions directly covered by them.
the overall prevalence of unit tests is very low with only of the subject systems containing at least one unit test.
we calculated the number of functions invoked in these test cases.
among the systems containing at least one test file the units tests directly cover a median of of all functions and .
of the ml functions see fig.
.
test coverage of ml enabled systems is generally rather low with no differences between ml and non ml code.
while of the systems train custom ml models evaluate their models.
to this end training is implemented in a median of files and evaluation in files.
accordingly some systems do not trust pretrained models but evaluate them in their application context.
the majority of ml enabled systems on github are libraries and frameworks.
end user oriented applications are still a minority.
the systems use ml in different ways either developing new ml technology or applying ml on concrete problems.
a small but still substantial number of tools that support ml engineering exist.
the ml enabled systems vary in size and contain many different ml assets but also large amounts of non ml code.
quality assurance is neglected.summary rq1 system characteristics v. r euse of ml m odels and code rq2 we now discuss the extent of model reuse in the form of pre trained models and duplicated model implementations.
a. prevalence of pre trained models rq2.
pre trained models loaded from model hubs are used in of the systems .
the systems contain on average api calls to the model hubs.
figure shows the results.
due to the high number of api calls per system we investigated individual systems manually finding that often not full models are loaded but components that are used together e.g.
encoder and decoder models in transformers or generator and discriminator in generative adversarial networks .
by analyzing the directory names of code files with api calls we determined the location in the system and potential .
.
.
.
.
.
share of functions directly invoked in unit testsall functionsml functions fig.
test coverage for systems with 1unit test100101102103 api callssystems files with api callothertest dirdemo dirtotaldirectory category fig.
number of api calls for loading of pre trained models per system and number of files with api call per category purpose for which models are loaded.
out of all files that contain an api call to load pre trained models per system .
are located in test directories and .
in directories named demo or experiment.
the remaining .
are located in other directories containing code such as model implementations utility functions or application logic.
ml models are loaded in systems for neither testing nor demonstration purposes.
systems load pre trained models only in demonstration directories.
this can be explained by the fact that some systems in our dataset are ml tools cf.
sec.
iv a which do not implement ml based functionality but might demonstrate pre trained models.
b. extent of ml asset duplication rq2.
to determine how model implementations are reused we examined for each system how much of the source code implementing ml models is duplicated from other systems in the dataset.
we found this type of reuse in of the subject systems .
these contain an average of files that are at least partially duplicated which is more than half of the average of model implementation files in the subject systems.
we also observed that a few repositories provide the basis for most copies and that many of these belong to the same organizations notably microsoft openmmlab and hugging face.
table ii shows the projects from which is copied the most.
reuse of ml models is prevalent among the subject systems.
of the analyzed systems use pre trained models and systems copy ml implementation code.
the copied code originates mostly from a small set of repositories maintained by an even smaller set of organizations.summary rq2 model reuse vi.
a rchitectural aspects rq3 we answer rq3 based on insights from our in depth analysis of ml enabled applications cf.
sec.
iii d .
table ii overview of the most duplicated projects 1original project microsoft unilm microsoft lmops iscyy yoloair huggingface transformers open mmlab mmsegmentation 1original project microsoft lora locuslab convmixer open mmlab mmdetection facebookresearch fairseq stability ai stablediffusion 1number of projects that copy model code from the given original.
incl.
models1234 systemsfig.
distribution of the number of models sample a. descriptive aspects of ml models we start by characterizing the role of ml models in the systems contextualizing the results of rq1 andrq2 .
ml functionalities and tasks.
common functionalities provided by ml models include image processing e.g.
tiatoolbox chatbots e.g.
gentopia video processing e.g.
videoto3dposeandbvh data analysis e.g.
corpustools androbotic navigation e.g.
sunnypilot .
other systems use ml to transcribe music infer metadata and play games.
table iii shows the distribution of ml functions and tasks.
on a technical level the underlying ml tasks in table iii are generative ai e.g.
h2ogpt is a chatbot classification e.g.
falldetection openpifpaf classifies videos decision making e.g.
sunnypilot segmentation e.g.
videoto3dposeandbvh segments videos dimensionality reduction e.g.
corpustools with systems not fitting into established categories.
for example tournesol uses linear regression to visualize correlations.
four systems combine tasks for instance video chatgpt is achatbot with video processingcapabilities that allows user to query the content of videos.
number and type of ml models.
the majority of the systems employ multiple ml models see fig.
with the highest number being h2ogpt .
half of the systems use more than models.
three systems allow to select arbitrary models from model hubs making the number of models practically infinite.
for example gentopia allows the user to configure a chatbot by selecting a model from hugging face.
the ml technologies found are shown in table iii.
the most common type of model is a deep neural network dnn i.e.
dl found in systems.
classical ml not dl was found times.
among the dnns we observed multiple variations including transformers 6convolutional neural networks cnn 3fully connected neural networks fcnn and one long short term memory lstm .
cnns are used exclusively for image processing whereas transformers and lstms are mainly intended for natural language processing although we found transformers that are used for image generation e.g.
controllora .
table iii ml functions tasks and technologies sample ml function image processing video processing chatbot data analysis navigation 3ml task generative ai classification decision making segmentation dimensionality reduction clustering 1ml technology transformer cnn fcnn lstm pca rl clustering 1number of systems assignment is not exclusive so the numbers do not add up to table iv storage and origin of ml models sample model origin total1pre trained fine tuned custommodel storagetotal111 local remote na 1number of systems with a given origin or storage independent of the other dimension a system can have multiple models with different origin or storage.
ml can be further divided into supervised learning sl unsupervised learning ul and reinforcement learning rl .
ul was found only in the form of classical ml such asprincipal component analysis pca andclustering .
two systems use supervised classical ml.
the dnns are mostly used for sl but rl systems were also found.
in contrast to dl classical ml is mostly used for auxiliary functions e.g.
corpustools uses pca for data visualization.
model origins.
systems use pre trained models directly out of the box while systems fine tune them and systems use completely custom implemented models.
dnns are usually pre trained or fine tuned whereas classical ml models are always custom trained.
the likely reason is that the more complex a model becomes the more training data and effort it requires so pre trained models relieve developers of end useroriented systems of this burden .
we found systems that use custom trained dl models of which use rl for which few pre trained models exist.
other custom built ml models are used for specialized use cases such as dreamartist stablediffusion which uses custom models for prompt optimization.
storing loading of models.
we observed local and remote storage times each with systems supporting both.
for example vlog uses pre trained models from different sources some stored locally some downloaded from hugging face.
since ul does not need models to be stored systems have no storage.
we visualize the relationship between storage and origin of the models as a matrix in table iv.
custom models are always stored locally pre trained models usually remotely with some exceptions such as stable diffusionwebui depthmap script orvideoto3dposeandbvh .
b. model embedding rq3.
ml models are embedded in traditional code by using codelevel apis.
the main interaction with models is via their inputs and outputs which often require pre or postprocessing.
however we observed that developers often resort to ad hoc solutions to embed ml models.
we report on observed inputs and outputs and pre and postprocessing practices.
model in output.
we found a wide range of data types used as inputs and outputs to ml models see table v .
inputs include text e.g.
paperless ng images e.g.
imagepy and video e.g.
videoto3dposeandbvh .
less frequent are audio sheetsage or an environment state for rl agents in the form of structured numeric data deep learning and thegame ofgoorsarl star .
the outputs are more diverse commonly text e.g.
sheetsage number of occurencesfusionsequentialindependentalternativefig.
distribution of model interaction patterns sample images e.g.
clip glass labels e.g.
paperless ng which classifies text documents actions e.g.
sunnypilot or other robotics systems bounding boxes e.g.
viame and video e.g.
vlog .
while we mostly observed standard data formats special application scenarios sometimes require less common formats e.g.
videoto3dposeandbvh extracts animations from video and saves in the bvh format.
structured numeric data is outputted by the model of sunnypilot containing inferred information about the environment like the lane lines or the distance to the lead vehicle.
the most common combination of input and output is text to text systems all chatbots .
other common combinations are text to image or image to text.
clip glass is an example of both providing image captioning and image generation from text.
pre postprocessing.
almost all systems contain pre and post processing steps ranging from generic operations e.g.
text tokenization to more complex content manipulations often tailored to the needs of a specific system.
we clearly identified preprocessing code in systems systems contained no preprocessing while the remaining were inconclusive.
common preprocessing steps include normalization feature extraction and vectorization.
some processing functions such as image normalizations were scattered throughout the system.
postprocessing was found in systems contain none were inconclusive.
again the majority are simple data conversions such as projecting tokens back to text.
more complex tasks were also observed e.g.
validating actions chosen by an rl agent deep learning and thegame ofgo .
particularly interesting is tiatoolbox which provides users with extensive configuration options including pre and postprocessing methods.
while it provides default functions it also supports the integration of user written functions added to the model as callbacks.
dreamartist stable diffusion implements some security functionalities that ensure input integrity.
c. interaction patterns rq3.
in the systems with multiple models see sec.
iv we observed various types of interaction between these models.
triangulating with related work we extracted interaction patterns that describe the observed topologies.
figure shows the frequency of each pattern in the sample.
alternative use of models is the most common pattern systems where multiple models are offered for the same task and chosen either automatically or by the user.
for example sarl star provides multiple rl agents to control a robot.
table v data types as input and output to models sample text image video labels actionsnumeric databounding boxesaudio input output independent models for unrelated functionalities are found in systems.
larger systems in particular provide the user with multiple functions some of which require ml.
for example viame offers image processing tools e.g.
for segmentation orclassification to be used independently by the user.
sequential integration of ml models was observed in systems.
the output of one model is used as input to another model possibly with additional processing steps in between.
this pattern is used for more complex tasks such as perception systems e.g.
home robot uses a model to processes sensor input and feeds the results i.e.
detected obstacles to an rl agent for navigation.
sometimes additional data is added to the subsequent models e.g video chatgpt uses a transformer model to summarize videos which is the input for a large language model llm together with text prompts.
joining integration of ml models is the most complex pattern and was found twice in different variants.
results of two or more models are combined either by another model which also includes the sequential pattern or code logic.
we observed one variant in vlog which has a complex pipeline of models to transform videos into text descriptions.
it includes a model transcribing audio and one generating captions for each frame.
both results are fed into an llm which assembles a final text document.
the other variant was observed in eynollah where two segmentation models process an image with each model being used for different parts of the image.
the segmented image is further processed by other models.
multiple of these patterns are implemented in systems.
for instance viame offers multiple independent ml functionalities but has alternative models for some of them.
in vlog data processing starts with a feature extractor model followed by a segmentor sequential to divide a video into chunks.
the chunks are then processed by two models one for subtitle generation and the other for segmentation and classification.
in parallel the audio track is translated into text by a third ml model.
the outputs of the three models are then assembled by an llm joining into a text description of the video.
most systems use supervised dl with pre trained models followed by deep reinforcement learning and classical unsupervised learning.
while most ml models process text or images other types of input are also present.
using multiple models is very common.
although most models are either used as alternatives or independently some systems apply multiple models sequentially.
pre and postprocessing steps are scattered throughout the codebase and there are no common practices for integrating ml models.summary rq3 architectural aspects vii.
r elated work prior research has characterized ml enabled systems and examined challenges related to the integration of ml and relevant aspects.
we complement with the first investigation of model reuse and architectural aspects on a large dataset.
similar to how we study system characteristics gonzales et al.
analyze ml projects compare them to non mlprojects and identify unique properties such as programming languages library dependencies and developer collaboration.
we confirm some of their findings for instance that ml tools are still more popular than applied ml projects.
our study however has a much wider scope including a qualitative analysis that we relate to the quantitative analysis.
jiang et al.
study model reuse on a dataset of pre trained models.
their dataset is limited considering only two major model hubs whereas we consider seven.
we also investigate reuse of ml assets through code duplication.
gorton et al.
suggest a conceptual ml architecture which we did not observe in the analyzed systems.
peng et al.
examine the architecture of an autonomous driving system and identify patterns of interaction between ml models and source code.
we complement their findings with a broader perspective covering ml enabled systems.
similar to us nahar et al.
combine repository mining with a manual analysis of a subset.
they find that only a fraction of systems using multiple ml models contain interacting models.
however their analysis is at a more abstract level than ours focusing on processes and collaboration rather than implementation details such as the type of ml model used.
they also focus on qualitative methods while we combine these with quantitative analysis.
houerbi et al.
studied ci pipelines in open source ml projects and identified significant issues.
in fact in our dataset of repositories use continuous integration use github actions travis and jenkins confirming their observations about adoption.
openja et al.
investigate testing practices by manually analyzing test files from open source ml projects highlighting used test types and tested ml related functionalities.
while we took a static view on ml enabled systems others analyze their evolution focusing on how ml assets evolve in relation to source code and the types of changes contributed by forks of ml repositories .
simmons et al.
analyze issues in open source ml projects finding that ml related issues take longer to resolve than non ml issues.
finally munappy et al.
investigate challenges in data management for deep learning models identifying challenges and possible solutions.
biswas et al.
characterize data science pipelines finding that in practice they differ from each other and are often more complicated than the theoretical models.
this is however beyond the scope of our study.
viii.
d iscussion we now discuss our findings and formulate actionable recommendations for researchers and practitioners.
ml adoption in open source software.
despite growing attention the adoption of ml in open source systems is lower than expected.
most systems are prototypes e.g.
consistentteacher a research prototype to improve object detection and proofs of concept demonstrating new ml technologies e.g.
disrupting deepfakes which demonstrates how images can be protected from manipulation .
applications providing end user oriented functionality e.g.
paperless ng a tool for organizing scanned text documents by inferringmetadata are a minority and often appear prototypical as well e.g.
home robot refers to itself as a research tool .
this aligns with others findings which suggest that open source ml applications often resemble startup style projects where ml is not a central component.
while the analyzed applications offer diverse capabilities based on ml these are often based on ad hoc solutions to integrate ml models e.g.
falldetection openpifpaf uses if statements to load alternative models .
this indicates that developers are still exploring the possibilities of ml cf.rq1.
and highlights the need for design methods that consider the specifics of ml and more structured techniques to integrate ml models into software systems cf.
rq3.
.
effective ml adoption requires a holistic perspective on building ml enabled systems.
further studies are needed to understand ml adoption barriers e.g.
lack of use cases engineering practices or resources and develop methods and tools for adoption.recommendation ml asset reuse.
ml assets are extensively reused across software systems often employing bad reuse practices such as code cloning.
we identified reuse as an essential practice as of systems reuse ml assets.
this reuse can be easily motivated e.g.
due to lack of data science expertise among software engineers .
however the observed reuse is rarely systematic i.e.
cloning model code which poses maintainability issues as observed in code clones in general .
although pre trained models should provide a manageable form of reuse of the systems using pre trained models contain also code clones.
while we confirm the importance of pre trained models in practice other studies also highlight issues such as insufficient security measures missing attributes or artifacts and discrepancies in model performance .
we show that code duplication is not predominant in a single ml technology but prone to all areas i.e.
transformers stable diffusion and others cf.
table ii .
our in depth analysis of a sample of ml enabled applications revealed no other patterns of model reuse.
a possible reason for cloning model implementations is that developers modify the model implementations slightly which is supported by the fact that many of the identified code duplicates are not complete matches.
however this may also result from duplicates using outdated model versions.
increase developers awareness of proper reuse mechanisms particularly for ml assets and promote their adoption.
this must be accompanied by academic studies to identify the reasons for the observed bad practices and the development of effective easy to use reuse mechanisms.recommendation topologies of ml models.
many ml enabled systems integrate multiple models.
however the actual embedding of ml models into the systems and the integration of multiple models is realized in an ad hoc manner.
best practices aremissing.
in line with other studies the analyzed systems often employ multiple ml models that interact in various ways through complex code logic.
for example nahar et al.
show that of th systems they investigated have multiple models and contain interacting models.
in our sample even of the systems contain multiple models and involve some form of interaction.
still we did not observe any systematic integration strategy but ad hoc implementations.
especially pre and post processing of data lacks clear structure e.g.
tiatoolbox allows for arbitrary pre and postprocessing methods other systems e.g.
gentopia contain none .
other observations include heavy use of wrappers for loading ml models and extensive if statements for loading alternative models falldetection openpifpaf contains an if with alternatives .
this lack of methods missing architectures or templates may be a core reason why advancements in ml do not carry over to building systems in fact most projects stall or fail in the development phase .
researchers can assist by providing guidelines on best practices and architectures for developing complex topologies of ml models.
additionally tools and libraries such as model hubs should be improved to provide adequate support for developing ml enabled systems particularly considering multiple ml models.
current tools such as experiment management tools are not only model centric but lack support for multiple models .
complex ml model topologies are part of ml enabled systems and need to be considered in future research.
this requires design patterns and tools that can manage multiple models lifting the current model centric to a system centric perspective in the development of ml enabled systems.recommendation ix.
c onclusion we presented a large scale study of ml model integration in ml enabled software systems.
we quantitatively analyzed systems in terms of their characteristics comparing ml and non ml assets and reuse practices.
we manually classified a random sample of systems in terms of their relationship to ml and system type and analyzed the applications of this sample in depth regarding their provided functionality model reuse practices and architectural aspects.
among others we find that ml is used for a variety of functionalities but mostly in conceptual prototypes.
we discovered extensive but unstructured reuse of ml models.
although we observed a lack of structured approaches to the architecture of ml enabled systems we identified four patterns of interaction between ml models.
we contribute recommendations for researchers and tool builders.
acknowledgment we thank kevin hermann for providing feedback and constanze ohrem for the help with labeling the dataset.
this work was partially funded by the german federal ministry for education and research bmbf under the project privacye2e.