exlong generating exceptional behavior tests with large language models jiyang zhang the university of texas at austin usa jiyang.zhang utexas.eduyu liu the university of texas at austin usa yuki.liu utexas.edupengyu nie university of waterloo canada pynie uwaterloo.ca junyi jessy li the university of texas at austin usa jessy austin.utexas.edumilos gligoric the university of texas at austin usa gligoric utexas.edu abstract many popular programming languages including c java and python support exceptions.
exceptions are thrown during program execution if an unwanted event happens e.g.
a method is invoked with an illegal argument value.
software developers write exceptional behavior tests ebts to check that their code detects unwanted events and throws appropriate exceptions.
prior research studies have shown the importance of ebts but those studies also highlighted that developers put most of their efforts on happy paths e.g.
paths without unwanted events.
to help developers fill the gap we present the first framework dubbed exlong that automatically generates ebts.
exlong is a large language model instruction fine tuned from codellama and embeds reasoning about traces that lead to throw statements conditional expressions that guard throw statements and non exceptional behavior tests that execute similar traces.
we compare exlong with the state of the art models for test generation cat lm and one of the strongest foundation models gpt 4o as well as with analysis based tools for test generation randoop and evosuite .
our results show that exlong outperforms existing models and tools.
furthermore we contributed several pull requests to open source projects and ebts generated by exlong were already accepted.
index terms test generation large language models program analysis exceptional behavior tests i. i ntroduction many popular programming languages including c java and python support exceptions .
exceptions are thrown during program execution if an unwanted event happens e.g.
a method is invoked with an illegal argument value.
to throw an exception a developer writes a throw statement in their code.
these throw statements are commonly guarded with conditional statements e.g.
if as exceptions should be thrown only under exceptional circumstances.
figure 1a shows a code snippet in java that throws an illegalstateexception line when the next character parsed from argument request is identified as a special atom line but is neither an opening line nor a closing parenthesis line .
software developers write exceptional behavior tests ebts to check that their code properly detects unwanted events and throws desired exceptions.
figure 1b shows an example ebt.
an ebt similar to a non exceptional behavior test nonebt first performs necessary setup of the system under test e.g.
creates objects lines then invokes a method under1class searchcommandparser extends commandparser static final char chr space static final char chr cr r public searchterm searchterm imaprequestlinereader request throws protocolexception ... char next while next request.nextchar !
n next !
chr cr next request.consumeall chr space if isatomspecial next if next ... else if next ... else throw new illegalstateexception unsupported atom special char next ... return handleoperators target throw statement a method under test searchterm .
1public class searchcommandparsertest private searchterm parse string line throws protocolexception final byte bytes line.endswith n ?line line n .getbytes bytearrayinputstream ins new bytearrayinputstream bytes return new searchcommandparser .searchterm new imaprequestlinereader ins null test expected illegalstateexception.
class public void testunsupportedatomspecialchar throws protocolexception parse method under test exceptional behavior test b exceptional behavior test written using junit that covers the highlighted statement above.
fig.
an ebt testunsupportedatomspecialchar from greenmail mail test greenmail and the target throw statement.
test line and finally checks the expected behavior line .
for an ebt the expected behavior is that an exception was thrown and the type of the exception matches the one specified by a developer.arxiv .14619v3 dec 2024prior research has studied ebts in practice and observed that most projects already have some ebts but that the number of ebts is not as high as the number of non ebts.
simply put developers focus on happy paths and have limited time to test exceptional behaviors.
furthermore through interviews and surveys prior studies confirmed the importance of ebts and developers desire to improve the testing of exceptional behaviors.
sadly tool support for automatically generating ebts is limited.
most existing analysis based test generation tools e.g.
randoop and evosuite and learningbased test generation tools e.g.
cat lm and teco have no special settings for targeting ebts and are primarily evaluated on non ebts.
random test generation tools can be guided by reinforcement learning to target exceptional behaviors but the generation works only on the entire codebase and not for a specific throw statement that a develop might select.
additionally tests produced by analysis based tools lack readability .
we present the first framework dubbed exlong an instruction fine tuned large language model llm that automatically generates ebts.
llms are shown to be effective in code generation including test generation .
such strong prior provides a good foundation yet is not enough.
ebts contribute to only a very small percentage in existing codebases i.e.
they are not well represented in llm training data.
the special conditions that trigger an ebt during execution are not included in the training phase of standard code llms thus they do not perform well on the task of generating ebts.
using codellama as its base exlong is finetuned with a novel task instruction and finetuning data designed specifically to embed the reasoning about a context that includes a traces that lead to target throw statements b guard expressions i.e.
conditional expressions that guard those throw statements and c non ebts that execute similar traces.
this context is used as the input to generate an ebt that triggers the target throw statement.
the ebt that we already showed in figure 1b was generated by exlong.
we assess the power of exlong using two use cases.
in the first use case which we call developer oriented use case a developer selects a method under test and a target throw statement as well as a destination test file.
exlong takes these inputs and automatically generates an ebt that executes the target throw statement.
in this use case we compare exlong with the state of theart models for test generation cat lm and strongest foundation models gpt3.
and gpt 4o .
we use a number of standard metrics bleu codebleu edit similarity and exact match as well as metrics specific to code including percentage of compilable tests executable tests and executable tests that cover the target throw statement.
our results show that exlong generates .
and .
more executable ebts than cat lm and gpt3.
respectively.in the second use case which we call machine oriented use case a developer uses exlong to automatically generate ebts for the entire codebase with the goal to cover all existing throw statements with one ebt per statement .
exlong takes the entire codebase as input finds throw statements that are in public methods already covered by at least one nonebt and generates one ebt for each of the throw statements.
this use case is similar to the traditional test generation setup targeted by analysis based generation tools.
in this use case we compare exlong with popular analysis based test generation tools randoop and evosuite .
although tools complement each other i.e.
each tool can generate ebts for some target throw statements that other tools cannot our findings show that exlong outperforms randoop and evosuite.
additionally we built exlong on gpt 4o a state of theart language model without fine tuning and evaluated it in developer oriented use case.
our results show that exlong gpt 4o outperforms gpt 4o by up to .
.
this emphasizes that our technique is generalizable to the most advanced proprietary llms.
finally we selected a subset of ebts generated by exlong and created pull requests for several open source projects.
by the time of this writing tests generated by exlong have already been accepted by developers of those projects.
the key contributions of this paper include task .
we define a novel task for llms generating exceptional behavior tests ebts .
model .
we designed and implemented exlong an instruction fine tuned llm built on codellama which reasons about traces to methods that contain throw statements guard expressions and non ebts that cover similar traces.
use cases .
we recognized two use cases for exlong developer and machine oriented use cases.
evaluation .
we assess the power of exlong in both use cases.
in developer oriented use case we compare exlong with existing models for code and test generation.
in machine oriented use case we compare exlong with analysis based testing tools randoop and evosuite.
we find that exlong outperforms existing state of the art models and tools.
dataset .
we developed a novel dataset for the presented task and this dataset is publicly available.
exlong is available on github at engineeringsoftware exlong.
ii.
u secases at a high level exlong is designed to help software developers write ebts that cover the throw statements within the given repository.
we propose two use cases for exlong developer oriented use case section ii a and machineoriented use case section ii b .
note that in this work we donotconsider generating ebts that cover throw statements in the dependency libraries of the given repository e.g.
arithmeticexception thrown from the java.lang.math asfig.
overview of exlong.
two use cases for exlong developer oriented use case and machine oriented use case.
in the developer oriented use case a developer specify the method under test a target throw statement and a destination test file and ask exlong to generate an ebt that cover the target throw statement.
in the machine oriented use case a developer gives an entire repository to exlong.
we assume that those throw statements are already covered by ebts available in dependency libraries developers can configure exlong to include such throw statements if necessary.
a. developer oriented use case in the developer oriented use case box 1in figure a developer will specify the method under test mut a target throw statement and a destination test file then ask exlong to generate an ebt that invokes mut and triggers the target throw statement.
when the developer specifies a target throw statement that is not within the mut exlong will first use a static program analysis technique to find all possible throw statements reachable through a sequence of method calls starting from the mut and then prompt the developer to select the target statement from the list.
b. machine oriented use case in the machine oriented use case box 2in figure a developer gives an entire repository to exlong.exlong will first find all public methods and throw statements in those methods.
furthermore exlong will heuristically select a destination test file.
then exlong will create public method throw statement destination test file triples.
finally it will generate one ebt to test each triple without requiring any developer intervention.
exlong uses static dynamic program analyses to obtain necessary context such as traces leading to throw statements and guard expressions.
iii.
exlong figure shows the workflow of exlong.
given an mut mut a target throw statement s and a destination test file d exlong reasons about the following context collected using static and dynamic program analyses stack trace r the sequence of method invocations that start from the mut and lead to the target throw statement.
guard expression g the logical formula representing the constraints on the symbolic variables that must be true to follow that particular trace.
relevant non ebts tneb a subset of non ebts that invoke the given mut or are in the given destination test file their snippets and coding styles help with generating desired ebts.
the above constitutes the prompt p mut s d r g tneb that encompasses the task inputs and the task context .
during training exlong is instruction fine tuned over a base llm to produce ebt teb conditioned on the input p. during inference exlong generates the ebt given the prompt.
while the format of the llm s input p and output teb is static the steps for preparing the task inputs and reasoning about the task context differ for training and inference and differ slightly between the developer and machine oriented use cases.
we describe the methodology for training exlong in section iii a and inference in section iii b. a. training to perform supervised fine tuning sft on exlong s foundation llm we collect a corpus c of sft data c p teb mut s d r g tneb teb from a set of repositories with developer written ebts and non ebts.
algorithm shows the process of collecting the training corpus for exlong.
identifying ebts and non ebts for each repository in the training set we first parse the source code in the repository to identify test methods and categorize them into ebts and non ebts.
a test method is categorized as an ebt if it conforms to one of the four patterns that are widely used by developers test expected assertthrow expectedrule tryfailcatch.
all the other test methods are categorized as non ebts.
the set of ebts and non ebts are used as thealgorithm collecting training corpus.
inputs teb tneb existing ebts and non ebts sut the system under test outputs c the training corpus of mut s d r g tneb teb procedure collect training corpus teb tneb sut c forteb tebdo d getfile teb r execute instrumentprintexception teb instrumenting and executing ebt to get stack trace r excludetestandutilmethods r d mut r .method mut comes first in stack trace s getsourcecode r last stack trace item points to target throw statement g computeguardexp r tneb initialize set of relevant non ebts c c mut s d r g tneb teb sut instrumentprintmethod sut instrumentation for getting methods covered by non ebts fortneb tnebdo mut execute tneb get mut directly invoked by non ebt forc c do ifc.mut mut c.d getfile tneb then c. tneb c. tneb tneb return c inputs to the training corpus collection algorithm line in algorithm .
executing ebt and collecting stack trace each ebt will be expanded to one sft example c in the training corpus c .
naturally the file that contains the ebt is the destination test file line in algorithm .
to avoid data leakage problems we remove all test methods in the test file d and only keep the test class structure and utility methods.
for training stack trace is the sequence of method invocations from ebt non inclusive that lead to the target throw statement under test inclusive .
line in algorithm shows how to collect the stack trace first instrument the ebt by adding print exception .getcause to the code location after the exception is thrown and caught by the ebt then execute the instrumented ebt to get the printed stack trace.
to avoid duplicate information we exclude ebt itself and any utility methods in the destination test file from the stack trace line in algorithm .
the first method invoked in the stack trace is the mut by definition line in algorithm .
the last method invocation and line number in the stack trace point to the target throw statement line in algorithm .
computing the guard expression stack trace provides the sequence of method invocations that lead to the target throw statement but knowing only the names of the methods is insufficient for generating ebts.
to aid the reasoning about the setup of the system under test which lead to exceptional behaviors we propose guard expression a logical formula representing the constraints on the symbolic variables that must be true to follow the particular code trace.
specifically we use conjunctions of expressions extracted from the invoked methods in the stack trace to form the guard expressions.algorithm collect ast nodes along the stack trace.
inputs r the stack trace for a target throw statement outputs n the collected nodes procedure collect nodes r n for m lineno reversed r do current getsource code m lineno parent current n n current while current mdo ifparent is forstmt then n n parent .compareexpression ifparent is ifstmt then ifparent .thenstmt current then n n parent .conditionexpression ifparent .elsestmt current then n n parent .conditionexpression other cases while switch block and assignment statements are in supplementary material.
current parent parent parent .getparent return n algorithm compute guard expression based on stack trace.
inputs r the stack trace for a target throw statement outputs g the guard expression for the target throw statement procedure compute guard exp r n collect nodes r collect all condition if while etc.
and assignment assign method call nodes along the stack trace e the set of conditions in the guard expression forn ndo ifnis conditionalexpr then e e n ifnis assignstmt then e merge e n.lhs7 n.rhs ifnis methoddeclaration then n n process with next node method call ifnis methodcallexpr then argmap forarg argname n.getargs n .getparams do argmap argmap argname arg e merge e argmap returnve procedure merge e argmap e fore e do forname7 expr argmap do ife.contains name then e e.replace name expr e e e return e for instance the guard expression for the throw statement highlighted in figure 1a is present in the figure box .
the first step in the computation is to collect the list of guard related ast nodes along the stack trace starting from the target throw statement to the mut as described in algorithm .
we traverse each method in the stack trace inreversed order line .
inside each method we start from the ast node specified by the line number in the stack trace.
we always include this node into the list of collected nodes line because the variable names in that statement may be used by the next step for replacing method call arguments.
starting from the statement we traverse the ast by maintaining the pointer current that constantly moves from the child ast node to its parent node.
ast nodes entailing condition expressions within the for loop lines to if statement lines to while loop and switch statement will be added to n. we also collect the assignment statements method call expressions and method declarations.
some of these are omitted from algorithm to keep it simple.
the second step of computing guard expressions is to process the collected nodes by propagating the symbolic variables through the stack trace as described in algorithm .
the nodes are visited in the order being collected and conditional expressions are directly added to the guard expression line .
assignment statements method declaration and method call expressions will trigger a merge operation line .
the goal of the merge operation is to review the current guard expression and replace the symbolic variables e.g.
variables appearing in the target throw statement with their corresponding values e.g.
constant values or mut s arguments .
therefore the guard expression reflects the mut s arguments and public fields that are usable by ebt rather than local variables that ebt may not have access to.
for assignment statement line we replace the left hand side variable in ewith the right hand side expression.
for method declaration and method call expression lines to these two nodes always appear in pairs in the collected nodes we replace the method declaration s argument names in ewith the actual arguments in the method call expression.
connecting ebts to relevant non ebts we add the relevant non ebts to the prompt lines to in algorithm to encourage the llm to reason about the procedures to set up the object under test and the condition under which the exception will be triggered grounding the existing nonebts.
additionally we believe the non ebts in the same repository will promote the consistency between the generated code and the existing code in terms of the both format and coding conventions.
given an mut we use two approaches to retrieve the relevant non ebts non ebts that directly invoke the same mut used first when the context window of the llm cannot fit all relevant non ebts and non ebts that are already present in the destination test file line in algorithm .
if there is no relevant non ebt retrieved this part of the prompt is left empty.
instruction fine tuning we use codellama instruct7b an open source foundation model designed for code generation and instruction following as the foundation model ofexlong.
codellama is pretrained on auto regressive and infilling objectives enabling tasks like code completion and document generation.
we fine tune codellama on our collected training corpus of sft data c p teb with the novel instructions shown in figure .
given thealgorithm prepare the pool stack traces from non ebts and assemble the prompt for evaluating exlong.
inputs tneb existing non ebts sut the system under test outputs q stack traces to reach target throw statements procedure collect stack trace set tneb sut sut instrumentprinttrace sut q fortneb tnebdo forr execute tneb do r excludetestandutilmethods r getfile tneb fors getthrowstmts r do q q r tneb s return q global var q c ollect stack trace set tneb sut inputs mut s d task inputs selected by developers or inferred tneb existing non ebts outputs p the prompt to give to the llm procedure assemble prompt mut s d r tneb forq q do ifmut q.r q.s sthen r r q.r ifmut q.r .method then tneb tneb q.tneb ifr then r randomselect r g e xtract guard exp r tneb tneb tneb getfile tneb d return mut s d r g tneb instruction that includes the collected context the model is expected to produce the ebt llm p teb.
we fine tune the codellama model using the parameterefficient low rank adaptation lora technique .
rather than updating the entire set of parameters in the llm lora injects trainable low rank matrices into each layer of the model.
this approach dramatically reduces the number of trainable parameters and the amount of required computational resources.
b. inference the inference workflow of exlong is different from its training workflow in that we cannot rely on executing ebts to collect the context e.g.
stack trace as our goal is to generate those ebts.
instead exlong reasons about the context based on task inputs mut target throw statement and destination test file and leveraging existing non ebts for the system under test.
algorithm describes the key steps in preparing the inference prompt of exlong.
collecting non ebts stack traces to reach potential target throw statements we first prepare a set of stack traces from the execution of non ebts that can reach potential target throw statements in the repository lines to .
this only needs to be done once per repository.
specifically we first instrument all methods that have throw statements to log the current stack trace usingtable i statistics of the collected dataset.
muts is the number of unique method under test exception types is the number of unique exception types tested by the ebts.
projects tests ebts muts exception types all train valid eval thread .currentthread .getstacktrace upon invoking those methods line .
then we execute all non ebts and collect the logged stack traces lines to .
the execution of each non ebt may generate multiple stack traces as it may cover multiple methods with throw statements.
selecting task inputs next we select the task inputs mut target throw statement and destination test file which can be specified by the developer or inferred by heuristics depending on the use case that exlong is targeting developer oriented use case developer specify the mut and target throw statement to generate the ebt for and the destination test file where the ebt should be placed.
machine oriented use case given a repository exlong locates all throw statements and generates one ebt for each of them.
for a target throw statement the mut is the method containing the throw statement and the destination test file is selected based on a file name matching and b test coverage analysis.
specifically similar to prior work given a code file named fnm we search for test file named fnmtest ortestfnm .
if there is no result based on file name matching we run the existing non ebts to find any existing test class that cover the mut or the class of mut.
if there is again no result based on test coverage analysis exlong will not generate an ebt for this target throw statement and it will move to the next one .
the selected task inputs are then used to assemble the prompt for exlong line .
assembling the prompt we first need to find stack traces from the set of non ebts stack traces that match the given mut and target throw statement line .
if multiple matching stack traces are found we randomly select one line .
given the stack trace we use the same algorithm in section iii a3 to compute the corresponding guard expression line .
the relevant non ebts are selected using the similar criteria as section iii a4 i.e.
having the same mut line or in the same destination test file line .
however if no matching stack trace is found exlong will not generate an ebt for the given inputs.
iv.
d ataset in this section we describe details on collecting the dataset section iv a as well as the statistics of our dataset used for training and evaluation section iv b .table ii statistics of the evaluation dataset for developeroriented use case.
mut is the number of unique method under test exception types is the number of unique exception types tested by the ebts throw statements is the number of unique throw statements covered by the ebts.
ebts mut exception types throw statements developer oriented table iii statistics of the evaluation dataset for machineoriented use case.
throw statements is the number of target throw statements we extracted from the repository according to section iii b2.
exception types is the number of unique exception types thrown by the target throw statements.
throw statements exception types machine oriented a. dataset collection following prior work we collect data from java projects from codesearchnet which are available on github and satisfy the following use the maven build system compile successfully do not have test failures have at least one ebt that follows one of the four patterns section iii a1 and have a license that permits the use of its data.
requirements simplify the automation steps and ensure that we can run existing tests to collect dynamic data e.g.
stack traces as well as run ebts that we generate.
b. dataset statistics the statistics for the collected dataset are presented in table i. in total we collected tests from projects where of these tests are ebts.
collected ebts cover a range of unique exception types e.g.
runtimeexception illegalargumentexception .
the dataset is randomly split by projects into training train validation valid and evaluation eval sets where the training set is the sft data used to instruction fine tune exlong the validation set is used for early stopping the training process and guiding our design decision of exlong and the evaluation set is used for evaluating the performance ofexlong and baselines.
table ii presents the statistics of the evaluation data for developer oriented use case.
note that this is a subset of the last row from table i. under developer oriented use case we benchmark exlong on the subset of examples for which we are able to extract stack traces.
in this paper we focus on cases where accurate stack traces can be extracted by executing existing non ebts.
when such non ebts are not available namely the stack traces cannot be obtained developers can first write or generate non ebts for the mut with the help of other test generation tools and then use exlong to generate ebts.table iii presents the statistics of the evaluation data for machine oriented use case.
note that this is a subset of the last row from table i. for machine oriented use case we evaluate on examples as we filter the data for which we were not able to locate the destination test file with our designed heuristics section iii b2 .
v. e valuation design we assess the performance of exlong by answering the following research questions rq1 how does exlong perform under the developeroriented use case compared with the state of the art models?
rq2 how much do stack traces and guard expressions help exlong in generating ebts?
rq3 how much does the selection of non ebts help exlong in generating ebts?
rq4 how does exlong perform with different underlying llm model?
rq5 how does exlong perform under the machine oriented use case compared with analysis based test generation tools?
we next describe metrics used to compare models and tools section v a and then describe the baselines used in our comparison section v b .
we answer all research questions in section vi.
a. evaluation metrics developer oriented use case for developer oriented use case we compare using data shown in table ii the generated ebts against the developer written ebts by benchmarking on similarity based and functional correctness metrics.
following prior work on learning based test generation we use the following similarity based metrics to compare generated ebts and ground truth i.e.
developerwritten ones exact match accuracy xmatch the percentage of the predictions that are exactly the same as the ground truth.
bleu the number of n grams in the prediction that also appear in the ground truth.
codebleu adapted version of bleu score for code.
in addition to n grams overlapping it also computes the overlap of ast nodes nodes in the data flow graph between the prediction and ground truth.
edit similarity calculates levenshtein distance which is the minimum number of character level edits insertions deletions or substitutions required to change the prediction into the ground truth.
the similarity metrics only capture the surface level similarity between the prediction against an existing ebt among them xmatch is the most strict one as it requires perfect matches while the others account for partial matches.
however such surface metrics do not adequately capture the functional validity of the generated ebt e.g.
whether the code can be compiled or executed especially since the developerwritten ebts may not be the only correct implementation tocover a specific target throw statement.
thus we additionally include the following functional correctness metrics compilable percentage of the generated ebts that can be compiled.
being compilable is a basic functional requirement for the generated tests.
matched e percentage of ebts that check the specified exception type.
namely whether the exception class following test expected is the same as user specified one.
this metric checks if the model hallucinates the exception type.
runnable percentage of ebts that check the specified exception type and can be compiled and executed without any error.
this metric unlike others requires the generated ebts to be semantically valid.
throwcov out of all developer specified target throw statements table ii the percentage of target throw statements with successfully generated ebts i.e.
compilable runnable and checking the specified exception type.
this is the strictest metric ensuring that the generated ebts are semantically valid and are targeting the throw statement specified by developers.
machine oriented use case for machine oriented use case we benchmark tools ability to cover the throw statements within a given repository throwcov out of all target throw statements selected in repositories table iii the percentage of the target throw statements with successfully generated ebts i.e.
compilable runnable and checking the correct exception type.
b. baselines learning based tools we compare exlong with one of the strongest foundation models and one llm that is specifically pretrained to generate tests.
gpt3.
we instruct gpt3.
to write ebts by first providing one random example from the training data.
namely one prompt and the corresponding ground truth ebt.
the prompt we use to query gpt3.
includes the mut the target exception type to test the method containing the target throw statement one relevant non ebt and the destination test file.
we sample a single ebt from the output.
cat lm cat lm is an llm pretrained on java and python repositories.
it is pretrained with a novel objective that considers the mapping between source code and the corresponding test files.
cat lm is pretrained to generate the remaining test methods given a code under test and the beginning of the test file.
it has shown strong performance on several test generation tasks.
to be consistent with its pretraining objective and intended use case we prompt catlm with the mut followed by the destination test file one randomly selected relevant non ebt and the test annotation test expected encouraging the model to complete the ebt.
just like in other cases we sample a single ebt.
automatic test generation tools in machine oriented use case we compare exlong with two widely used analysisbased test generation tools.
randoop randoop is a random test generation tool that creates tests by randomly generating inputs and recording the sequences of method calls.
we run randoop with a timelimit of seconds per class to generate unit tests for each project per the randoop user manual we set seed to usethreads to true and other options to their default values.
evosuite evosuite is a search based test generation tool that randomly generates inputs and employs a genetic algorithm to evolve these inputs aiming to maximize code coverage.
we run evosuite for seconds per class as suggested in a recent sbst competition .
we also set theseed to .
unlike randoop which generates tests for the entire project to generate more ebts for the target throw statements within the time limit we generate tests on a subset of classes when running evosuite.
starting from classes that contain throw statements we use jdeps to retrieve all classes that transitively depend on these initial classes thereby creating a targeted subset for evaluation.
c. hardware we run exlong s program analyses part randoop and evosuite on a machine with intel core i7 11700k .60ghz cores threads cpu gb ram ubuntu .
java and maven .
.
.
we perform exlong s llm fine tuning and generation as well as cat lm on a server with nvidia a100 gpus amd milan .
ghz.
we run finetuning and generation for exlong and baselines three times with different random seeds and report the average numbers across three runs.
vi.
r esults in this section we present the evaluation results and answer each research question.
a. rq1 developer oriented use case to answer rq1 we compare the ebts generated by exlong with developer written tests.
the results of exlong and baselines are shown in tables iv and v. table iv presents the results when we inform llms the method name of the target ebt while in table v we do not.
we describe the last row in these tables in a later subsection.
exlong outperforms all the baselines on both similaritybased metrics left side in tables and functional correctness metrics right side in tables .
exlong achieves higher performance than baselines for both generating executable ebts runnable and ebts that cover the target throw statements throwcov .
this highlights that exlong can generate more ebts that can be directly adopted by developers.
in table iv we can see that exlong outperforms gpt3.
by .
and .
on runnable and throwcov respectively.
similarly we can see that exlong outperforms cat lm by .
and .
on runnable and throwcov respectively.
this further underlines the benefit of the stack traces and guard expressions extracted via program analysis and exlong s capability of reasoning about them.
to further understand the performance difference we inspect the ebts generated by gpt3.
and exlong.
although gpt3.
generates comparable number of compilable ebtspublic static filewriter createfilewriter string classname logfilepath logfilepath compressioncodec codec secorconfig config throws exception return createfilereaderwriterfactory classname config .
buildfilewriter logfilepath codec a the mut to be tested.
!filereaderwriterfactory.
class .isassignablefrom class.
forname classname b the guard expression for the target throw statement.
test expected illegalargumentexception.
class public void testfilewriterconstructormissing throws exception reflectionutil.createfilewriter missingclass mlogfilepath null msecorconfig c compilable but failing ebt generated by gpt3.
test expected illegalargumentexception.
class public void testfilewriterconstructormissing throws exception reflectionutil.createfilewriter java.lang.string mlogfilepath null msecorconfig d compilable and runnable ebt generated by exlong fig.
ebt testfilewriterconstructormissing generated by gpt3.
and exlong.
the ebt generated by exlong covers the target throw statement satisfying the correct condition.
asexlong it struggles to cover the correct target throw statements especially when they are not in the mut but could be reached through a sequence of method calls .
for example in figure 3b we show the guard expression extracted by exlong to trigger the illegalaccessexception with regard to the first argument classname of the mut createfilewriter in figure 3a.
the ebt generated by gpt3.
can be compiled but fails to check the illegalaccessexception figure 3c .
exlong uses the correct class java.lang.string that satisfies the condition and successfully covers the target throw statement figure 3d .
comparing functional correctness metrics in table iv and table v performance of both gpt3.
and catlm declines significantly when the ebt method name is omitted.
this result aligns with expectations as the method name frequently implies the conditions under which the exception is supposed to be thrown e.g.
should fail iftime provider isnull .
in contrast exlong demonstrates robustness regarding the inclusion or exclusion of the test method name maintaining consistent performance on functional correctness metrics.
this further emphasizes the reasoning ability of exlong on the stack traces and guard expressions.
b. rq2 ablation study of stack traces and conditions to evaluate the contribution of the components in exlong we perform an ablation study.
in table vi we show the results while including ebt s name in the prompt.
we find that ablating each component deteriorates performance espe table iv results on developer oriented use case with ground truth ebt s name in the prompt.
models bleu codebleu editsim xmatch compilable matched e runnable throwcov gpt3.
few shot .
.
.
.
.
.
.
.
cat lm .
.
.
.
.
.
.
.
codellama zero shot .
.
.
.
.
.
.
.
exlong .
.
.
.
.
.
.
.
exlong sample .
.
.
.
.
.
.
.
table v results on developer oriented use case without ground truth ebt s name in the prompt.
models bleu codebleu editsim xmatch compilable matched e runnable throwcov gpt3.
few shot .
.
.
.
.
.
.
.
cat lm .
.
.
.
.
.
.
.
codellama zero shot .
.
.
.
.
.
.
.
exlong .
.
.
.
.
.
.
.
exlong sample .
.
.
.
.
.
.
.
cially across functional correctness metrics.
removing stack traces slightly hurts the performance of exlong in terms of functional correctness which is expected because a guard expression is in a way the summary of a stack trace.
we observe a small drop in compilable but a rather larger drop in throwcov if removing both stack trace and guard expression from the context.
this underlines the importance of the condition information present by these two components on which exlong reasons about when generating ebts that cover the target throw statements.
relevant non ebts mostly contribute to the compilable because the relevant non ebt covers the same mut which gives model a starting point to construct the ebts.
the ablation study underscores the importance of each component in exlong.
c. rq3 selection of non ebts in addition to randomly choosing one relevant non ebt to add to the prompt for exlong we try running the inference ofexlong for no more than times each time with a different relevant non ebt and reporting the best performance dubbed exlong sample in tables iv and v. we see that that performance differences can be substantial.
to study how the diversity of non ebts used in the prompt affect the exlong performance our ablation study on the number of different relevant non ebts is shown in table vii.
we present results for exlong generating only one ebt for each target throw statement exlong sample but using the same non ebt as in multiple inference runs exlong sample using different non ebts in multiple inference runs.
for a fair comparison we keep the number of generated ebts for and same and we report the best of kmetrics on a subset of examples on which we extracted more than one relevant non ebt.
we find that increasing the number of sampled ebts improves the performance when compared to only generating one ebt which can be attributed to the randomness.
moreover sampling with a diverse set of non ebts further boost the performance across most of the metrics under the same sample size.
this motivates a new research question on how to choose 36randoop evosuite exlong a covered throw statements on a subset of projects.
36randoop evosuite exlong b covered throw statements on all projects.
fig.
venn diagram that shows target throw statements coverage by exlong randoop and evosuite.
better non ebts to the prompt for ebt generation which we leave for future work.
d. rq4 exlong built with gpt 4o our technique which assists llms in generating ebts can be transferred to other llms with significant benefits.
in addition to results with open sourced llms in prior sections we further present the results of exlong built on gpt4o currently one of the most powerful proprietary llms.
table viii compares the performance of exlong gpt 4o with baseline gpt 4o under the developer oriented use case.
the inputs to gpt 4o includes the mut the target exception type the method containing the target throw statement one relevant non ebt and the destination test file.
we instruct exlong gpt 4o with the prompt containing mut stack trace guard expression relevant non ebts and the destination test file.
one example pair of input and ground truth ebt is provided for both models.
results show that exlong gpt4o outperforms gpt 4o across both similarity metrics and functional correctness metrics.
e. rq5 machine oriented use case in table ix we present the throw statement coverage rate of exlong sample and two analysis based test generation tools.
evosuite could generate tests for all projects while randooptable vi ablations on different context of exlong.
models bleu codebleu editsim xmatch compilable matched e runnable throwcov exlong .
.
.
.
.
.
.
.
no stack trace .
.
.
.
.
.
.
.
no stack trace guard expression .
.
.
.
.
.
.
.
no stack trace guard expression non ebt .
.
.
.
.
.
.
.
table vii comparison between using different non ebts to sample and using the same non ebt but sampling multiple times.
note that we add the target ebt s name to the prompt and we only report results on examples that have more than one candidate non ebt.
models bleu codebleu editsim xmatch compilable matched e runnable throwcov exlong .
.
.
.
.
.
.
.
exlong sample w same non ebt .
.
.
.
.
.
.
.
exlong sample w different non ebt .
.
.
.
.
.
.
.
table viii comparison between gpt 4o few shot and exlong gpt 4o.
models bleu codebleu editsim xmatch compilable matched e runnable throwcov gpt 4o few shot .
.
.
.
.
.
.
.
exlong gpt 4o .
.
.
.
.
.
.
.
table ix throw statements coverage rate for exlong randoop and evosuite.
toolsthrowcov subset projects all projects exlong .
.
evosuite .
.
randoop .
.
could not generate tests for three projects.
we inspected the issues and found that randoop crashed on opennms newts because this project kept throwing runtime exceptions all related to com.codahale .metrics .scheduledreporter randoop crashed on pinterest secor because this project requires the configuration of kafka randoop could not load a class and crashed on openhft chronicle map.
we report results on the subset of projects where all the tools can be run successfully subset projects and results on all projects all projects .
among the given target throw statements exlong achieves higher throw statement coverage rate than analysis based tools.
figure illustrates the overlap and difference among the sets of target throw statements covered by exlong evosuite and randoop.
all three tools cover different sets of throw statements.
exlong covers the most target throw statements that other two cannot.
vii.
c ase study we performed a case study where we submitted the ebts generated by exlong to the open source projects where the data was extracted from to collect developers feedback.
among the evaluation set for machine oriented use case exlong generated ebts across projects that are runnable and cover the correct throw statements.
we selected a subset of projects that are actively maintained i.e.
they had at least one commit accepted pull request pr or responded issue within the past six months at the time of the papersubmission .
we found that the generated ebts of projects were the same as those added by developers on later commits commits after the ones we used during evaluation thus refrained from submitting prs to them.
in total we submitted prs which include ebts one pr per project .
among them prs ebts have been accepted and prs ebts are still pending.
no pr was rejected.
in one instance a developer responded and merged our pr only minutes after we create the pr.
this was encouraging and future tool development should integrate exlong into an ide such that exlong continuously provide ebts for code that a developer is editing.
viii.
l imitations we discuss several limitations and potential future work.
programming language .
in this work we focused on supporting the java programming language which is among the most popular languages nowadays.
we expect no substantial differences in our approach for similar programming languages e.g.
c .
future work could evaluate and tune our model for dynamically typed languages e.g.
python.
project boundaries .
in our evaluation we generate ebts for throw statements within a single project and we ignore throw statements that are in libraries used by the project.
we could not come up with a use case that targets throw statements in libraries so we left it out of our work.
if we were to target such a case we would need to collect context throw statement and conditions from those libraries.
one could take several directions e.g.
finding code of those libraries building a model on bytecode level or decompiling code and then extracting the context.
destination test file .
not every mut has a destination test file.
we leave the problem on finding or generating destination test file for any given method under test as future work.llms .
we built exlong around codellama a recent open source model.
we believe that building on codellama provides reproducibility guarantees that will help us and others to build on this work.
our contributions are the task definition of a context for the task tools for extracting the context instruction fine tuned model and extensive evaluation.
we expect that building exlong on other open source llms would lead to similar results.
ix.
r elated work there has been significant work on test generation and code generation .
we cover related work on llm based test generation generating tests for exceptional behavior and other test generation techniques.
llm based test generation .
transformer models have been used to generate tests and test oracles .
cat lm is a .7b model that is pretrained on a large dataset of java and python projects.
it outperforms existing test generation tools starcoder and codegen 16b in terms of the number of valid tests and test completion tool teco .
so we compared our work with cat lm in this paper.
conditions are useful for guiding the generation of tests and finding bugs .
symprompt introduced path constraint prompting to guide llms to generate high coverage tests without additional training.
they collect constraints from each possible execution path in the target method and prompt the llm to generate tests that cover those paths.
we extract guard expressions by analyzing multiple methods along the stack trace starting from the throw statement to the target method for generating ebts.
existing test cases including the setup and teardown methods serve as a useful context to guide the generation .
haji et al.
empirically studies the effectiveness of generating tests using github copilot and discovers that using existing test cases as context can increase the passing rate of generated tests by .
.
our work uses existing nonebts as context and collects stack traces from those tests to guide the generation of ebts.
generating tests for exceptional behavior .
exception handling is an important aspect of software development.
there are several techniques to generate ebts.
however they either generate ebts from specifications or use random based or search based strategies to generate ebts.
our work is the first to use llms to generate ebts.
also prior work generates tests for the whole program while our exlong allows users to specify which throw statements to cover.
guo et al.
introduces boundary coverage distance bcd to evaluate the quality of test inputs which can be used to guide the random generation of test inputs by minimizing bcd.
goffi proposed throw statement coverage to measure the effectiveness of test inputs in triggering exceptions.
we also use throw statement coverage in our evaluation.other test generation techniques .
other techniques incorporate random based search based and constraint based strategies to automatically generate tests.
tests can be derived from multiple sources including the code under test error messages and specifications like comments .
randoop generates tests by randomly generating inputs and saving the sequences of method calls.
evosuite is a search based test generation tool that randomly generates inputs and uses a genetic algorithm to evolve the inputs to maximize code coverage.
during the test generation process both randoop and evosuite create tests that cover normal as well as exceptional behaviors.
however since they generate inputs randomly they do not guarantee to generate tests with high coverage or meaningful and readable inputs.
moreover there is no assurance that these inputs will successfully trigger certain exceptional behaviors.
evosuitefit adapted evosuite s search algorithm with reinforcement learning to generate exceptional tests.
unfortunately we cannot directly use evosuitefit has error invalid or corrupt jarfile .
x. c onclusion we presented the first work on generating tests for exceptional behavior ebts using large language models.
we introduced exlong that builds on top of codellama to embed reasoning about traces that lead to throw statements conditional expressions along those traces and non exceptional tests that cover similar traces.
we evaluated exlong in two use cases developer oriented use case i.e.
generate ebt for a given method under test and a target throw statement and machine oriented use case i.e.
automatically generate tests for all throw statements available in a repository .
our results show that exlong outperforms existing test generation models and analysis based test generation tools.
we contributed a number of tests generated by exlong to open source projects and ebts are already accepted.
we believe that exlong targets an important task has good performance and helps developers increase code quality assurance by automatically providing high quality ebts.