knowledge enhanced program repair for data science code shuyin ouyang king s college london london uk shuyin.ouyang kcl.ac.ukjie m. zhang king s college london london uk jie.zhang kcl.ac.ukzeyu sun institute of software chinese academy of sciences beijing china zeyu.zys gmail.comalbert merono penuela king s college london london uk albert.merono kcl.ac.uk abstract this paper introduces dsrepair a knowledgeenhanced program repair approach designed to repair the buggy code generated by llms in the data science domain.
dsrepair uses knowledge graph based rag for api knowledge retrieval and bug knowledge enrichment to construct repair prompts for llms.
specifically to enable knowledge graph based api retrieval we construct ds kg data science knowledge graph for widely used data science libraries.
for bug knowledge enrichment we employ an abstract syntax tree ast to localize errors at the ast node level.
we evaluate dsrepair s effectiveness against five state of the art llm based repair baselines using four advanced llms on the ds dataset.
the results show that dsrepair outperforms all five baselines.
specifically when compared to the second best baseline dsrepair achieves substantial improvements fixing .
.
.
and .
more buggy code snippets for each of the four evaluated llms respectively.
additionally it achieves greater efficiency reducing the number of tokens required per code task by .
.
.
and .
respectively.
index terms code repair large language model knowledge graph data science i. i ntroduction data science is crucial in driving innovation and decisionmaking across various domains leveraging data to uncover insights and inform strategic actions.
nevertheless the complexity of data science libraries and the expertise required to use them can pose significant barriers to lay users.
large language models llms have emerged as powerful tools to generate data science code automatically democratizing access and accelerating development processes.
despite their potential the widely acknowledged shortcomings with llms such as hallucination and the lack of specialized knowledge of certain domains e.g.
long tail api usage and specific code context remain significant obstacles.
these issues are particularly critical in the data science domain where the code heavily relies on libraries for accurate and efficient data processing and analysis making precision and contextual accuracy essential for robust outcomes.
existing studies have applied feedback based iterative self repair to improve the reliability of llm generated code.
nevertheless these approaches are not designed for data science code.
recently retrieval augmented generation rag has also emerged as a widely adopted technique to inject external knowledge into llms to facilitate more coherent code generation.
rag combines the strengths of information retrieval and llms to enhance code generation.
existing rag basedcode generation studies commonly follow a standard rag architecture where the retriever component retrieves relevant plain text from a vast corpus or database using a vector similarity search.
this retrieved information is then fed into an llm which uses this context to produce more accurate and relevant code .
however these textbased rag approaches are not well suited for code generation tasks as they rely on unstructured or semi structured plain text which lacks the semantic relationships and structured representation needed for complex code understanding and generation.
specifically text based retrieval relies on vector similarity search which often retrieves irrelevant or loosely related information due to ambiguities in natural language plain text does not explicitly represent the relationships between apis their dependencies or their attributes e.g.
parameters return types .
as a result text based rag approaches may fail to provide the comprehensive contextual knowledge required for resolving an issue retrieved plain text often contains redundant descriptions or ambiguities which can confuse large language models llms or lead to suboptimal code generation.
this paper introduces dsrepair a knowledge enhanced approach for repairing incorrect data science code produced by llms through knowledge graph based rag and bug information enrichment.
we construct ds kg datascience knowledge graph a set of knowledge graphs for the seven most widely adopted data science libraries i.e.
numpy pandas scipy scikit learn matplotlib pytorch and tensorflow .
for the buggy code generated by an llm dsrepair uses the api name appeared in the code as the query to obtain the correct usage of corresponding api functions by accessing ds kg.
it then uses the returned result to guide the llm in repairing code.
compared to text based rag our kg based rag naturally captures more complex relationships and dependencies within api documents which is essential in the data science domain.
the rich semantic relationships stored in knowledge graphs enhance the reliability and efficiency of code generation and repair.
for instance in matplotlib s api document an api named matplotlib.pyplot.subplots has a parameter called gridspec kw.
the value of gridspec kwmust be passed to another api object called matplotlib.gridspec.gridspec .
if an error occurs in gridspec kw querying knowledge from matplotlib.gridspec.gridspec is more helpful than querying only from matplotlib.pyplot.subplots .
a well designed kg can 1arxiv .09771v1 feb 2025infer such dependency naturally.
dsrepair also uses enhanced bug information to improve the program repair effectiveness.
data science code often contains multiple function calls and data operations within a single line.
therefore to obtain fine grained bug information dsrepair uses abstract syntax tree ast and test case execution information to localize errors at the ast node level.
we evaluate dsrepair on two widely used general purpose llm i.e.
gpt .
turbo and gpt 4o mini and two state of the art coding llms i.e.
deepseek coder and codestral based on ds the data science code generation benchmark spanning seven data science libraries.
our results show that dsrepair outperforms all the five baseline llm based repairing approaches.
in particular compared to the second best baseline dsrepair correctly fixes .
.
.
and .
more buggy code snippets for the four llms respectively while reducing token usage per code task by .
.
.
and .
.
to summarize this paper makes the following contributions we present dsrepair a novel llm based program repair approach for data science code leveraging knowledge graphbased rag and enriched bug information.
we construct and release ds kg data science knowledge graph a comprehensive set of knowledge graphs tailored to the seven most widely used data science libraries.
we propose an ast based bug information enriching approach that can pinpoint errors at the ast node level.
we conduct an empirical study using four llms and five baselines demonstrating that dsrepair significantly outperforms all baselines in repairing data science code.
we release our data code kg data dump and results at our homepage .
the rest of the paper is organized as follows.
section ii outlines our methodology.
section iii describes the design of the experiments including research questions benchmarks baselines selected models and measurements.
section iv presents the results and highlights notable findings based on our empirical results.
section v discusses the threat to validity the limitation and the generalizability of our work.
section vi introduces the related work of our study.
section vii concludes.
ii.
m ethod fig shows an overview of dsrepair.
given a code problem description we first let llm i.e.
gpt .
turbo generate code.
if the code fails to pass the test cases dsrepair constructs a repair prompt and requests llm to regenerate the code see section iii b for details .
as shown in the figure dsrepair involves four main steps api kg construction where a knowledge graph ds kg is built for popular data science libraries e.g.
numpy and pandas to capture detailed api usage and relationships api knowledge retrieval where api calls are extracted from the buggy code and queried from ds kg with the results verbalized into natural language for llm prompts bug knowledge enrichment which localizes errors at the ast node level using test case execution to provide fine grained bug information and prompt construction extracted api ast api knowledge retrieval enriched bug knowledge tests repair prompt repaired code llm api documents ds kg coding task buggy code api kg construction api knowledge retrieval bug knowledge enrichment prompt construction fig.
overview of dsrepair.
where all gathered information is structured into a detailed prompt to guide the llm in generating effective repairs.
a. api kg construction we develop ds kg a knowledge graph tailored to widely adopted data science libraries such as numpy pandas scipy scikit learn matplotlib pytorch and tensorflow.
its primary purpose is to assist llms in repairing buggy code by providing structured information about the correct usage of apis.
following standard kg construction procedures we begin by creating an ontology to define the schema for ds kg .
existing ontologies are unsuitable for representing api documentation in the context of data science code repair.
to address this gap we manually design a domain specific ontology schema drawing insights from the structure of api documentation.
api documentation typically provides details such as an api s name expression explanation parameters and return types.
our ontology captures these attributes for individual api functions enabling precise and structured queries based on error information extracted from buggy code.
inspired by prior work in code ontology design we represent each api function as a unique entity within ds kg.
the ontology includes two types of relations attribute relations describe links between api entities and their attributes such as has name has expression and has explanation .
dependency relations capture the hierarchical structure and dependencies of apis such as belongstolibrary and belongstomodule .
fig illustrates an example of ds kg construction from the numpy api document.
each api object introduced on a webpage such as numpy.flipud and numpy.array split is treated as an entity.
detailed information about an api object such as its name expression explanation and parameters returns is used to build rdf triples with attribute relations.
for example the api object numpy.flipud has such an rdf triple numpy.flipud has expression numpy.flipud m .
new entities are created for the parameters and return values of each api object each with attributes like argument position data type and explanation.
for example the parameter min numpy.flipud m has the following rdf triple numpy.flipud parameter m hastype array like .
using the prefix of the api entity derived from the name and 1in this paper we ignore the owl prefixes in rdf triples subject predicate object to make the article more concise.
2web pages api document design ontology ontology construction numpy.array split has name numpy.array split numpy array split belongstolibrary numpy numpy array split has expression numpy.array split ary indices or sections axis numpy array split has ex planation split an array into multiple sub arrays ... numpy.flipud has name numpy.flipud numpy.flipud belongstolibrary numpy numpy.flipud has expression numpy.flipud m numpy.flipud has explanation reverse the order of elements along axis up down .
... rdf triples data extraction ds kg b np.flipud np.array split a len a ncol incorrect code api extraction query construction valueerror setting an array element with a sequence.
the requested array has an inhomogeneous shape after dimensions.
the detected shape was inhomogeneous part.
numpy.flipud has expression numpy.flipud m numpy.flipud has explanation reverse the order of elements along axis up down .
... numpy .array split has expression numpy.array split ary indices or sections axis numpy .array split has explanation split an array into multiple sub arrays.
... retrieved rdf triples api knowledge numpy.flipud has full expression numpy.flipud m numpy.flipud reverse the order of elements along axis up down .
... numpy.array split has full expression numpy.array split ary indices or sections axis numpy.array split split an array into multiple sub arrays.
... retrieved api knowledge triple verbalization select ?subject ?predicate ?object where graph dskg ?subject a dskg function.
?subject dskg has name ?functionname.
?subject ?predicate ?object.
filter ?functionname numpy.flipud ?functionname numpy.array split sparql query fig.
details on api kg construction step and api knowledge retrieval step in dsrepair.
the code raises an error because of the mismatched array shape between np.flipud s required input and np.array split s output.
dsrepair extracts the api call in the error line and builds a sparql query to search the relevant rdf triples in the ds kg which is constructed from api documents and guided by the ontology.
finally dsrepair maps the returned rdf triples to natural language which will be used as a part of the repair prompt.
webpage url we construct rdf triples with dependency relations.
for instance the api object numpy.flipud is linked to its library through the rdf triple numpy.flipud belongstolibrary numpy .
b. api knowledge retrieval dsrepair integrates ds kg to enhance the repair of buggy code by retrieving relevant api knowledge and incorporating it into the repair process.
dsrepair extracts all api invocations in the buggy code snippet using regular expressions e.g.
identifying np.flipud or np.array split .
it resolves api prefixes e.g.
mapping np to numpy and uses the full api name for queries accounting for the common use of abbreviations in data science libraries.
using the resolved api name dsrepair constructs sparql queries to retrieve rdf triples from ds kg.
these triples encapsulate knowledge specific to the queried api such as its attributes dependencies and parameter details.
to ensure compatibility with llms we transform the retrieved rdf triples into natural language sentences using triple verbalization techniques .
these sentences provide human readable explanations including a description of the api s purpose and syntax details about parameters and returns together with their data types and explanations.
the retrieved api knowledge is concatenated and included in the api knowledge section of the repair prompt provided to the llm.
in section iv f we demonstrate that incorporating only the full api expression as knowledge yields the best performance for data science code repair.
thus by default dsrepair includes the full api expression in the prompt.
this approach balances the richness of information and efficiency ensuring llms receive sufficient contextual guidance without overwhelming them with unnecessary details.
c. bug knowledge enrichment bug knowledge enrichment aims to provide llms with extra bug information to help llms better repair the bug without requesting extra tests.
we use only the example tests provided in the coding task description.
traditional fault localization approaches such as spectrum based fault localization and mutation based fault localization are not applicable here for two reasons.
first data science code generation benchmarks usually provide a very limited number of tests e.g.
.
tests on average per problem in ds since the annotators need to define program inputs with complex objects such as square matrices classifiers or dataframes second traditional approaches are often filelevel or line level fault localization while data science code tends to contain multiple function calls and data operations in one line.
therefore different from traditional approaches dsrepair uses ast node level bug information to provide llms with more fine grained bug information.
fig.
shows a specific example of our bug information enrichment procedure.
firstly test cases are extracted from the coding task description provided.
these tests are essential for validating the correctness of the code and are used later in the bug knowledge enrichment process.
we then transform the incorrect code snippet into its ast representation.
once the ast is generated dsrepair iterates within a namespace that includes all necessary libraries and the extracted test cases.
this iteration involves executing nodes in the ast sequentially.
3generated code b np.flipud np.array split a len a ncol test case input np.array output np.array enriched bug knowledge last exec part np.array split a len a ncol last exec part type class ast.call last exec part value array array array last unexec part np.flipud np.array split a len a ncol last unexec part type class ast.call stderr traceback most recent call last ... file test demo.py line in module b np.flipud np.array split a len a ncol ... valueerror setting an array element with a sequence.
the requested array has an inhomogeneous shape after dimensions.
the detected shape was inhomogeneous part.
bug information enrichment module b np.flipud np.array split a len a ncol assign b np.flipud np.array split a len a ncol call np.flipud np.array split a len a ncol call np.array split a len a ncol array array array attribute np.flipud variable name np variable name b stderr can only localize error at the line level at here the code can be executed fig.
bug knowledge enrichment example.
stderr standard error information can only localize the bug at the line level while our bug knowledge enrichment could enrich the error information to the ast node level.
to gain detailed bug information the system identifies the last unexecuted node in the ast.
we classify all the bugs into two categories runtime errors.
if the code contains bugs that prevent it from being executed the system will run each ast node until it encounters an error.
the ast node that was executable before the failure occurred is noted as the last executed node.
the node immediately following this which causes the failure is where the bug is likely located.
the error is between these two nodes the last executable node and the first unexecutable node.
assertion errors.
if the entire code can be executed but the results do not match the expected output such an issue can be due to an assertion error.
in this case the system captures the final value returned by the code execution.
by comparing this actual output with the desired result the system can provide information to llms about why the code is incorrect.
the comparison highlights discrepancies offering insights into potential logical errors in the code.
d. prompt construction this step uses information obtained from the previous steps and organizes it into a structured prompt which is then fed to the llm for code repair.
as shown in fig the final prompt includes the following components problem description incorrect code stderr information api knowledge bug knowledge fact checking and response format.
we first put the problem description and llm generated incorrect code in the prompt.
error messages are cleaned by removing local file paths and deleting warnings.
to some extent this action protects the privacy of users operation system environment and ensures that only relevant error information is included focusing on critical errors that hinder code execution.
we extract useful api knowledge from the ds kg query results specifically the api expression or signature.
this expression includes all parameters both compulsory and optional highlighting potential errors related to parameter usage and function calls.
this comprehensive parameter information is crucial as it often points to the source of errors in the code.
for bug knowledge we leverage the results from bug enrichment.
this involves providing the test case the last unexecutable ast node and the last executable ast node along with the executed result value.
by comparing the actual first request first response dsrepair problem description incorrect code api knowledge bug knowledge fact checking return format incorrect code problem description fig.
dsrepair prompt example.
the prompt contains structural and rich information to guide llms for code generation.
output with the desired output we can pinpoint the exact location and nature of the error.
this detailed local context helps in understanding the specific issues within the code.
the fact checking component identifies where the existing code logic violates the corresponding requirements outlined in the problem description.
this step is essential to redirect the llm s attention back to the problem description ensuring that the solution aligns with the original requirements and constraints.
the complete prompts that we use are available on our homepage .
iii.
e xperimental design a. research questions our evaluation answers the following questions rq1 how effective is dsrepair in repairing llm generated data science code?
this rq investigates the buggy code fix rate of dsrepair compared with other state of the art program repair approaches.
rq2 how do dsrepair s bug fixes overlap with the five baselines?
this rq investigates whether dsrepair could fix unique bugs that the baselines fail to address.
rq3 what is the cost of dsrepair?
this rq investigates token usage and money spent on dsrepair and our baselines.
rq4 how is dsrepair s performance affected by different prompt designs?
to understand how different prompt designs 4affect dsrepair we conduct an ablation study to analyze each key prompt component s contribution to dsrepair.
rq5 how do different knowledge retrieval approaches affect dsrepair?
this rq aims to explore the advantage of knowledge graph based rag against plain text based rag.
rq6 how does the richness of api knowledge affect dsrepair?
this rq studies whether different types of api knowledge e.g.
whether the knowledge contains explanation or parameters given in dsrepair will affect its performance.
rq7 how does the non determinism of llm affect our experiment results?
this rq studies the influence of llm s inherent randomness on our experiment results.
b. data science benchmark our evaluation uses ds the state of the art benchmark specifically designed for benchmarking llms in data science code generation.
the ds benchmark was specifically constructed to mitigate concerns about data leakage.
in particular the dataset applies perturbation e.g.
text rephrasing and semantic perturbation so that models cannot answer them correctly by memorizing the solutions from pre training .
other data science code generation benchmarks are not applicable because they are not based on realistic problems and have no dedicated test cases to evaluate the correctness of the code they use exact match or bleu score .
ds comprises one thousand diverse and practical data science problems sourced from stackoverflow covering seven essential python libraries numpy pandas scipy sklearn matplotlib pytorch and tensorflow .
the version of each library can be found on our homepage .
in our experiments we let gpt .
turbo generates code for each of the coding tasks in ds of the generated programs fail to pass the test cases and are regarded as repair targets of dsrepair.
c. baseline as far as we know there are no existing repair approaches that are specifically designed for data science code generation.
therefore in our evaluation we compare dsrepair against the following state of the art llm based approaches that are capable of repairing general types of code.
while these generalpurpose approaches are effective in many scenarios they are not tailored to address the unique challenges posed by data science specific bugs.
by addressing the distinct requirements of data science code we aim to demonstrate dsrepair s enhanced ability to handle data science specific repair tasks in comparison to these baselines.
code search code search guides code repair by searching for similar code in the code base and adding the search result as a suggestion to the prompt.
following the practice in the paper we use the code problem description as the query and lucene as searching engine to conduct code search in the code base pytorrent .
chat repair chat repair leverages the code execution result to check code correctness.
if the code cannot pass thetest cases chat repair incorporates the execution results in the prompt to provide richer information for code debugging.
self debugging s self debugging s s represents simple enriches the prompt with the simplest information a sentence that indicates the code s correctness without more detailed information.
for instance the generated code is incorrect.
please fix the code.
self debugging e self debugging e e represents explanation first requests llm to generate a line by line explanation about intermediate execution steps of the generated code.
then it requests llm again to generate code based on the line by line explanation of the incorrect code.
self repair self repair first leverages error information produced by test execution to make llm produce a short explanation of why the code failed.
then it uses the explanation as part of the prompt to request llm to improve the incorrectly generated code.
d. llms we use two widely used general purpose llms gpt .5turbo and gpt 4o mini and two state of the art coding llms deepseek coder and codestral .
we access all the llms by using their commercial apis.
the details of the four llms are shown in table .
we choose python as our programming language for the code generation tasks because ds is based on python.
to control the randomness we set the temperature of all the llms to .
for each approach we let the llms generate code for each coding task ten times3.
we select the result with median overall performance as the final result .
our rq7 in section iv g is about the influence of llm s inherent randomness on our experiment results.
table llms used in the evaluation.
llm version input token price output token price gpt .
turbo gpt .
turbo .
1m tokens .
1m tokens gpt 4o mini gpt 4o mini .
1m tokens .
1m tokens deepseek coder deepseek coder v2 .
1m tokens .
1m tokens codestral codestral .
1m tokens .
1m tokens e. measurement we introduce the following metrics for measuring the performance of dsrepair and baselines.
effectiveness we measure the effectiveness of different approaches by checking their capability in fixing incorrectly generated code including the absolute number of fixes anf and fix rate fr .
the former is the absolute number of coding tasks whose code is successfully fixed.
the latter is the ratio of anf against all the buggy code snippets.
for anf two of the authors conduct manual verification on the correctness of the patches to make sure that the reported fixes are not overfitted.
cost we measure the cost by token usage tu and money spent ms which are the most widely used metrics 2we do not use gpt because it comes at a significantly higher cost.
3we repeat experiments for deepseek coder v2 three times only because the api is no longer available after .
5for measuring cost for llm based approaches .
tu refers to the total token usage when using llm to finish one complete request on average including input token usage and output token usage.
ms refers to the money cost for llm to receive and return those tokens.
below is the formula for the ms ms nx n token i n pi token o n po where piandporefer to the input and output token price token i nandtoken o nrefer to the input token usage and output token usage at certain request n and nrefers to the total number of llm requests.
iv.
r esults this section introduces the experimental results as well as the analysis and findings for each rq.
a. rq1 effectiveness of dsrepair to answer rq1 we report the results of absolute number of fixes anf and fix rate fr for dsrepair and all the baselines with each llm.
dsrepair initially generates patches that successfully pass the tests from all the llms.
after manual checking two of the patches generated by gpt4o mini are overfitted4and have been removed from the repaired set.
table shows the ultimate results.
we can observe that dsrepair significantly outperforms all the baselines in terms of anf and fr across all four llms we study.
specifically dsrepair can fix the buggy code for and coding tasks for the four llms respectively while the second best results are and respectively.
for specific data science libraries dsrepair outperforms the baselines for most libraries.
for example for gpt .
turbo dsrepair has the highest fix rate in numpy scipy sklearn matplotlib and pytorch.
for codestral dsrepair performs the best on numpy pandas sklearn matplotlib and pytorch.
fig shows an example from codestral where the error can be solved by dsrepair but cannot be solved by self repair.
the purpose of this code problem is to only turn on minor ticks on the x axis.
self repair generates an incorrect fix while dsrepair generates the correct fix.
in this problem the buggy code uses the function plt.minorticks on short for matplotlib.pyplot.minorticks on with parameter axis x .
however as stated in the matplotlib official document the full expression of plt.minorticks onis matplotlib.pyplot.minorticks on with no parameters which means that plt.minorticks oncan control the display of minor ticks on both x axis and y axis but there is no optional parameter to control the display on x axis or y axis only.
with dsrepair by enriching the prompt with knowledge of how to use plt.minorticks oncorrectly llm is more likely to realize that putting parameters in function plt.minorticks onis incorrect.
the correct solution uses plt.gca .xaxis.set minor locator instead to reach the goal of the code problem.
4an overfitted patch passes the test cases but is actually incorrect.
problem no.
library category matplotlib import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns x np.random.rand y np.random.rand plt.scatter x y how to turn on minor ticks on x axis only solution start incorrect solution plt.minorticks on axis x plt.savefig output.png bbox inches tight correct solution import matplotlib.ticker as ticker turn on minor ticks on x axis plt.gca .xaxis.set minor locator ticker.auto minorlocator plt.savefig output.png bbox inches tight fig.
a code problem example from ds .
the incorrect solution is generated from self repair and the correct solution is generated from dsrepair.
by incorporating knowledge of the invoked api dsrepair can assist llms in generating solutions with correct api usage.
looking deeper into the buggy code that dsrepair cannot fix we identify two primary reasons.
firstly the presence of multiple errors in the code poses a significant challenge.
dsrepair is designed to address specific errors highlighted by standard error messages.
however when a code segment contains hidden bugs that come out only after fixing one bug our approach struggles to resolve all issues in a single request.
secondly the insufficiency of information provided from the test cases in the description limits the repair effectiveness.
some descriptions lack accompanying test cases which are crucial for identifying and fixing errors.
for instance if the buggy code triggers an assertion error the absence of concrete test cases impedes the llm s ability to generate a precise fix.
even when generated code passes the given test cases it may still fail during actual evaluation.
simply informing the llm that the code is incorrect without detailed guidance is often inadequate for effective repair.
answer to rq1 dsrepair significantly outperforms all the baselines in fixing buggy data science code.
specifically dsrepair demonstrates notable improvements across four llms by fixing and buggy programs respectively with improvement rates of .
.
.
and .
compared to the second best baseline respectively.
please note that our comparison with baselines for detecting general software logic bugs is not meant to imply that dsrepair outperforms these baselines in all domains.
rather we show that approaches not specifically designed for data science 6table rq1 effectiveness of dsrepair against the baselines.
values are shown in the format anf fr .
anf is the absolute number of fixes.
fr is fix rate.
the results indicate that dsrepair outperforms the baselines for the majority of the libraries.
model approach numpy pandas scipy sklearn matplotlib pytorch tensorflow total gpt .
turbocode search .
.
.
.
.
.
.
.
chat repair .
.
.
.
.
.
.
.
self debugging s .
.
.
.
.
.
.
.
self debugging e .
.
.
.
.
.
.
.
self repair .
.
.
.
.
.
.
.
dsrepair .
.
.
.
.
.
.
.
gpt 4o minicode search .
.
.
.
.
.
.
.
chat repair .
.
.
.
.
.
.
.
self debugging s .
.
.
.
.
.
.
.
self debugging e .
.
.
.
.
.
.
.
self repair .
.
.
.
.
.
.
.
dsrepair .
.
.
.
.
.
.
.
deepseek codercode search .
.
.
.
.
.
.
.
chat repair .
.
.
.
.
.
.
.
self debugging s .
.
.
.
.
.
.
.
self debugging e .
.
.
.
.
.
.
.
self repair .
.
.
.
.
.
.
.
dsrepair .
.
.
.
.
.
.
.
codestralcode search .
.
.
.
.
.
.
.
chat repair .
.
.
.
.
.
.
.
self debugging s .
.
.
.
.
.
.
.
self debugging e .
.
.
.
.
.
.
.
self repair .
.
.
.
.
.
.
.
dsrepair .
.
.
.
.
.
.
.
struggle with addressing data science bugs.
by addressing the unique needs of data science code dsrepair can significantly improve repair outcomes in the context of data science.
b. rq2 overlap with baseline in this rq we conduct an overlap analysis by comparing the solved buggy code snippets between dsrepair and the baselines.
fig shows the upset plots for different approaches and the intersection of their anf.
we can observe that the fixed buggy code overlaps between dsrepair and baselines are overall less than of the bug fixes from dsrepair.
this means that about half of the code fixes from dsrepair could not be fixed by baselines.
for example in fig a dsrepair can fix buggy code snippets while self repair can only fix buggy code snippets.
the overlap between their fixed code snippets is only which means that dsrepair has code snippets that self repair cannot fix and self repair has code snippets that dsrepair cannot fix.
the overlap among the six baselines is quite low for gpt .
turbo for gpt 4o mini for deepseek coder and for codestral .
overall each baseline shows the uniqueness of buggy code repair.
answer to rq2 dsrepair uniquely fixes approximately of buggy code snippets that baselines are unable to fix.
c. rq3 cost of dsrepair to answer rq3 we assess the financial costs associated with using dsrepair by quantifying the us dollar spent on interactions with using the apis of the four llms.
the cost of each request to these models depends directly on the number of tokens processed including both the tokens used for input andthose generated as output.
we calculate the expenses incurred during these interactions by measuring the token usage tu of dsrepair and then converting this usage into actual money spent ms comparing these against the cost of our baselines.
table shows the tu and ms for different approaches.
fig shows a scatter plot of tu and fr.
we observe that dsrepair costs less token usage than the second best baseline.
for example dsrepair uses only .
.
.
and .
tokens per code problem while self repair needs .
.
.
and .
tokens per code problem.
based on the real time price in table the money spent on each request is .
.
.
and .
for using gpt .
turbo gpt 4o mini deepseekcoder and codestral as llm respectively.
we can also observe that dsrepair s token usage with gpt4o mini deepseek coder and codestral is higher than when using gpt .
turbo.
this is because the return of these code llms may not follow the prompt s output format instruction.
the responses typically contain more information such as line by line code comments natural language explanation of the code and an analysis of why the first generated code is incorrect all of which contribute to extra costs.
answer to rq3 compared to the second best baseline dsrepair uses fewer tokens .
.
.
and .
saving .
.
.
and .
tokens per code task respectively across different llms.
d. rq4 influence of prompt design to figure out how different prompt components influence dsrepair we conduct an ablation study.
in dsrepair there are two key components i.e.
api knowledge and bug knowledge.
in the ablation study we compare dsrepair s performance 7dsrepaircode searchchat repairself debugging sself debugging eself repair0102030intersection size1938 a gpt .
turbo dsrepaircode searchchat repairself debugging sself debugging eself repair020406080intersection size7387 b gpt 4o mini dsrepaircode searchchat repairself debugging sself debugging eself repair020406080intersection size5088 c deepseek coder dsrepaircode searchchat repairself debugging sself debugging eself repair0102030405060intersection size65 d codestral fig.
rq2 upset plots for overlap analysis.
for example in a the first column indicates that buggy code snippets that can be fixed by both dsrepair and code search.
table rq3 cost of different approaches.
tu refers to token usage input token usage output token usage and ms refers to money spent for llm receiving the prompt and generating the response.
approachgpt .
turbo gpt 4o mini deepseek coder codestral tu ms tu ms tu ms tu ms code search .
.
.
.
.
.
.
.
chat repair .
.
.
.
.
.
.
.
self debugging s .
.
.
.
.
.
.
.
self debugging e .
.
.
.
.
.
.
.
self repair .
.
.
.
.
.
.
.
dsrepair .
.
.
.
.
.
.
.
t oken usage0.
.
.
.
fix rategpt .
turbo t oken usage0.
.
.
.
gpt 4o mini t oken usage0.
.
.
.
fix ratedeepseek coder t oken usage0.
.
.
.
codestralcode search chat repairself debugging s self debugging eself repair dsrepair fig.
rq3 scatter plot of tu token usage and fr fix rate .
dsrepair is the optimal approach the star markers compared with baselines.
with the performance of no knowledge prompt without api and bug knowledge api knowledge only prompt without bug knowledge provided by tests and bug knowledge only prompt without api knowledge we use dsrepair w o api bug dsrepair w o bug and dsrepair w o api to represent no knowledge api knowledge only and bug knowledge only for short.the results of the ablation study are shown in table .
when using gpt .
turbo as llm the overall performance of dsrepair .
fr is better than dsrepair w o bug .
fr and dsrepair w o api .
fr .
the overall performance for gpt 4o mini of dsrepair .
fr is better than dsrepair w o bug .
fr and dsrepair w o api .
fr .
when using deepseek coder as our llm dsrepair still stands for the best with .
total fr.
however dsrepair w o bug has better overall performance than dsrepair w o api where using dsrepair w o bug has .
fr while using dsrepair w o api only has .
fr.
using codestral as llm dsrepair has .
fr which is higher than both dsrepair w o bug .
and dsrepair w o api .
.
interestingly we observe that the fr declines in dsrepair w o bug gpt .
turbo dsrepair w o bug and w o api gpt 4o mini and dsrepair w o api deepseek coder compared with dsrepair w o api bug.
answer to rq4 both enriched api knowledge and enriched bug knowledge in the prompt contribute to the final effectiveness of dsrepair.
e. rq5 comparison of different knowledge retrieval approaches to answer rq5 we examine the impact of various knowledge retrieval approaches on the performance of dsrepair.
specifically we compare knowledge retrieval through kg dsrepair with knowledge retrieval through plain text searching.
for plain text searching we extract api knowledge using 8table rq4 results of ablation study.
dsrepair w o api bug is for prompt without api and bug knowledge dsrepair w o bug is for prompt without bug knowledge and dsrepair w o api refers to prompt without api knowledge.
anf is the absolute number of fixes.
fr is fix rate.
model prompt anf fr gpt .
turbodsrepair w o api .
dsrepair w o bug .
dsrepair w o api bug .
dsrepair .
gpt 4o minidsrepair w o api .
dsrepair w o bug .
dsrepair w o api bug .
dsrepair .
deepseek coderdsrepair w o api .
dsrepair w o bug .
dsrepair w o api bug .
dsrepair .
codestraldsrepair w o api .
dsrepair w o bug .
dsrepair w o api bug .
dsrepair .
invoked api names as keywords.
the api knowledge is retrieved as a window of text encompassing tokens per keyword.
this window length was chosen to match the average size of the retrieval results from ds kg for each keyword ensuring a fair comparison.
all other experimental settings are kept consistent with those used in dsrepair.
table shows the results of different knowledge retrieval approaches for dsrepair.
we can see that retrieving knowledge from plain text only has .
.
.
and .
fix rate for four tested llms.
retrieving knowledge from plain text uses .
.
.
and .
tokens per buggy code which is higher than retrieval from ds kg and thus has higher money spent on four llms.
answer to rq5 knowledge graph based retrieval outperforms plain text based retrieval in fixing buggy data science code.
the former s fix rate is .
.
.
and .
for gpt .
turbo gpt4o mini deepseek coder and codestral respectively compared to .
.
.
and .
for the latter.
f .
rq6 influence of api richness to address rq6 we assess how varying the richness of api knowledge impacts the performance of dsrepair.
in our dsrepair setup we use only the full expressions of the invoked api to enrich the prompts.
our queries also yield additional information about correct api usage including explanations of functions and details about parameters and returns.
to explore the potential benefits of this enriched api knowledge we design experiments with different richness levels of api information dsrepair explanation dsrepair parameter return and dsrepair explanation parameter return.
dsrepair explanation in table rq5 knowledge retrieval comparison between plain text and kg.
retrieval from kg is better than from plain text.
fr refers to fix rate tu refers to token usage input token usage output token usage and ms refers to money spent for llm receiving the prompt and generating the response.
model knowledge retrieval fr tu ms gpt .
turboplain text .
.
.
knowledge graph .
.
.
gpt 4o miniplain text .
.
.
knowledge graph .
.
.
deepseek coderplain text .
.
.
knowledge graph .
.
.
codestralplain text .
.
.
knowledge graph .
.
.
corporates explanations of the invoked api into the api knowledge.
dsrepair parameter return adds information about the function s parameters and returns.
dsrepair explanation parameter return combines both types of information into the api knowledge.
table presents the performance results of these different levels of api knowledge richness.
the results are evaluated in terms of effectiveness as measured by the fix rate fr and cost as quantified by token usage tu and money spent ms .
the data shows that dsrepair achieves the highest fix rate across all richness levels with .
on gpt .5turbo .
on gpt 4o mini .
on deepseek coder and .
on codestral.
this suggests that the additional information may complicate the prompt without necessarily improving the effectiveness of the repair.
in terms of cost dsrepair generally exhibits lower token usage and monetary cost compared to its enriched counterparts.
for example on gpt .
turbo dsrepair uses .
tokens and incurs a cost of .
whereas dsrepair explanation parameter return uses .
tokens and costs .
.
this pattern holds across the other models as well indicating that increasing the complexity of the api knowledge may lead to higher money costs without a proportional gain in repair effectiveness.
answer to rq6 using full expressions of invoked api from the retrieval results in dsrepair performs the best in fixing bugs.
g. rq7 influence of llm non determinism to investigate rq7 we explore the effect of nondeterminism in llms on our experimental results.
as outlined in section iii we conduct each experiment ten times to account for variability in llm responses.
from these iterations we select the median performance result as our final data point for analysis.
to further understand how llm nondeterminism might influence our results we calculate the mean and standard deviation of the results across the ten trials.
table shows the mean fr fix rate and the standard deviation of code repair results.
we observe that the standard 9table rq6 influence of api knowledge richness on dsrepair.
fr refers to fix rate tu refers to token usage input token usage output token usage and ms refers to money spent for llm receiving the prompt and generating the response.
api knowledge richnessgpt .
turbo gpt 4o mini deepseek coder codestral fr tu ms fr tu ms fr tu ms fr tu ms dsrepair explanation .
.
.
.
.
.
.
.
.
.
.
.
dsrepair parameter return .
.
.
.
.
.
.
.
.
.
.
.
dsrepair explanation parameter return .
.
.
.
.
.
.
.
.
.
.
.
dsrepair .
.
.
.
.
.
.
.
.
.
.
.
deviations of dsrepair are not big which indicates the stability of our experiment results.
additionally with the standard deviation dsrepair still outperforms the baselines in its mean fr with .
.
.
.
.
.
.
.
for using gpt .
turbo gpt 4o mini deepseekcoder and codestral as llm respectively.
answer to rq7 despite the randomness of llms dsrepair consistently outperforms the baselines with greater stability across multiple trials.
it achieves mean fix rates of .
.
.
.
.
.
.
.
across gpt .
turbo gpt 4omini deepseek coder and codestral respectively.
v. d iscussion in this section we discuss the threats to validity the limitations and the generalizability of our research.
a. threats to validity the threats to internal validity mainly lie in the implementation of our prompt design.
to reduce this threat we design dsrepair with the idea of structuring prompts adapted from a handy template for structuring prompts called co star framework .
considering key aspects that influence the effectiveness and relevance of an llm s response dsrepair can lead llms to generate more optimal responses for code purposes.
in addition we design research questions such as rq4 and rq6 to study the influence of the different prompts on our final performance.
the threats to external validity mainly lie in the datasets and llms used in our study.
to reduce the threat regarding datasets we carefully choose to use ds as our experiment dataset which is the state of the art benchmark tailored to address data leakage concerns with realistic and diverse data science problems with testing methods checking both execution semantics and surface form constraints .
to reduce the threat regarding llms we use four widely studied llms to mitigate the potential bias that certain llms can bring to the experiment results.
in addition to mitigate the inherent randomness of llms we experiment ten times for each approach and choose one with the median overall performance as our final result which could further mitigate the non determinism of llms.
moreover we exclusively design rq7 to study whether the non determinism of llms will affect our experiment findings.b.
limitation the effectiveness of dsrepair depends largely on the quality and completeness of the knowledge it provides.
our approach demonstrates capability in addressing runtime errors by eliminating the initial error.
however this repair process can sometimes introduce or trigger new errors.
this phenomenon is particularly evident when the repaired code successfully executes but subsequently results in assertion errors.
the reliance on high quality test cases in the problem description is crucial in their absence dsrepair may guide llms to generate code that closely mirrors the incorrect code.
this occurs because the llms are provided with api knowledge that can inadvertently reinforce the use of incorrect or irrelevant apis present in the original code.
despite this there are instances where upon receiving api knowledge the llms deviate from the incorrect apis opting instead for alternative solutions such as using different apis or defining new functions.
maintaining the ds kg presents significant challenges.
our ds kg only reflects the correct knowledge of apis based on a specific version.
the rapid pace at which online api documentation is updated complicates the task of ensuring the ds kg remains up to date.
consequently keeping the dskg up to date demands substantial effort and resources.
this maintenance burden is a critical consideration as outdated or incomplete knowledge can adversely affect the accuracy and reliability of the repairs generated by dsrepair.
with the assistance of api document s release notes we could manage the updating of ds kg by leveraging library development logs to automate the process.
these logs often document changes and updates made to api libraries allowing us to efficiently identify and integrate the necessary modifications into the kg.
another notable limitation of dsrepair is the time cost associated with knowledge retrieval.
when compared to plain text searching retrieval using the ds kg incurs a significant time overhead averaging .
more time approximately .
seconds per task.
while this increase in retrieval time may seem marginal it can accumulate and impact the overall efficiency of the repair process particularly in scenarios requiring rapid iteration and testing.
c. generalizability of dsrepair in this paper dsrepair is specifically designed to enhance the repair of data science code.
nevertheless dsrepair s underlying methodology leveraging knowledge enhanced retrieval and structured bug information can be generalized to broader coding tasks.
the key innovation of dsrepair lies 10table rq7 mean and standard deviation of the anf absolute number of fix expressed as mean anf standard deviation.
the small standard deviation indicates the reliability of our results and our experimental conclusions in rq1 remain valid within this range.
approach gpt .
turbo gpt 4o mini deepseek coder codestral code search .
.
.
.
.
.
.
.
chat repair .
.
.
.
.
.
.
.
self debugging s .
.
.
.
.
.
.
.
self debugging e .
.
.
.
.
.
.
.
self repair .
.
.
.
.
.
.
.
dsrepair .
.
.
.
.
.
.
.
in its use of knowledge graph based retrieval augmented generation rag which is not inherently limited to data science apis.
by replacing the domain specific ds kg with a knowledge graph covering general purpose programming languages and software libraries dsrepair could be adapted to repair a wide range of buggy code across different domains as well as to improve other coding tasks other than repair such as bug localization and code generation.
dsrepair also has the potential to be extended to support project level code generation and repair where understanding dependencies across multiple files and context knowledge with the same project are crucial.
by constructing a project specific kg that encodes function definitions module dependencies and code architecture dsrepair can enhance code generation and repair in large scale software development.
vi.
r elated work a. code repair the goal of automated program repair is to automatically identify and fix bugs or defects in the software.
leveraging llms such as bert codebert codex and gpt series for code repair can achieve promising performance in generating patches for various kinds of bugs and defects.
these models are adept at grasping the core meaning and relationships within code resulting in the generation of precise and functional fixes without the need for compilation.
using llms for fixing code speeds up the identification and resolution of bugs freeing software developers to tackle more intricate issues.
this contributes to improved software reliability and upkeep.
chatgpt in particular stands out among llms because of its built in interactive nature which fosters an ongoing loop of feedback producing patches that are more polished and appropriate to the context .
b. prompt engineering prompt designing is an increasingly important skill set needed to leverage effectively with llms such as chatgpt.
similar to software design the design of prompt aims at offering reusable solutions to specific problems by providing a codified approach to customizing the output and interactions of llms.
abukhalaf et al.
conduct an empirical study on object constraint language based constraint generation by comparing the codex generated constraints and humane written constraints.
xia et al.
specificallyexamined prompts for automatic code repair.
more specifically white et al.
focus on combatting mistakes and improving generated code quality by designing prompt patterns.
borji et al.
examine the quality of generated answers and code from llms and conclude the existing failures from the experiment.
our research work draws inspiration from these explorations and prompts that could be used to generate code candidates with better quality and fewer errors.
c. retrieval augmented generation rag aims to address the limitations of generative models including issues related to outdated knowledge a deficiency in long tail knowledge and the potential for private training data leakage .
early research in code generation concentrated on code to code retrieval using dual encoder models with the retrieved outputs subsequently inputted into autoregressive language models .
repocoder enhances retrieval processes by employing iterative incremental generations .
knm leverages in domain code databases and applies bayesian inference to finalize the generated code.
rag also can be used to build prompts for transformerbased generative models with retrieved information including similar examples relevant api details documentations and imports .
vii.
c onclusion we propose dsrepair a novel knowledge enhanced approach for data science code repair.
we perform experiments with four llms and five baselines in data science code repair and find that dsrepair significantly outperforms all the baselines in repairing data science code.
by integrating api knowledge retrieval and bug information enrichment we can guarantee better performance in code repair and gain people s trust in using llms for coding.
in future work we also plan to explore a multi agent framework with interactive feedback to enhance the performance of dsrepair while focusing on optimizing feedback steps and resource use to ensure scalability cost efficiency and robust data science code repair.
viii.
a cknowledgement this work was supported by the ukri centre for doctoral training in safe and trusted artificial intelligence ep s023356 the nsfc and the national natural science foundation of china .
11references e. bolyen j. r. rideout m. r. dillon n. a. bokulich c. c. abnet g. a. al ghalith h. alexander e. j. alm m. arumugam f. asnicar et al.
reproducible interactive scalable and extensible microbiome data science using qiime nature biotechnology vol.
no.
pp.
.
h. hassani and e. s. silva the role of chatgpt in data science how ai assisted conversational interfaces are revolutionizing the field big data and cognitive computing vol.
no.
p. .
s. hong y .
lin b. liu b. wu d. li j. chen j. zhang j. wang l. zhang m. zhuge et al.
data interpreter an llm agent for data science arxiv preprint arxiv .
.
m. nejjar l. zacharias f. stiehle and i. weber llms for science usage for code generation and data analysis arxiv preprint arxiv .
.
y .
lai c. li y .
wang t. zhang r. zhong l. zettlemoyer w. t. yih d. fried s. wang and t. yu ds a natural and reliable benchmark for data science code generation in international conference on machine learning .
pmlr pp.
.
d. zan b. chen y .
gong j. cao f. zhang b. wu b. guan y .
yin and y .
wang private library oriented code generation with large language models arxiv preprint arxiv .
.
d. zan b. chen z. lin b. guan y .
wang and j. g. lou when language model meets private library arxiv preprint arxiv .
.
y .
ge w. hua k. mei j. tan s. xu z. li y .
zhang et al.
openagi when llm meets domain experts advances in neural information processing systems vol.
.
c. s. xia and l. zhang keep the conversation going fixing out of bugs for .
each using chatgpt arxiv preprint arxiv .
.
k. gupta p. e. christensen x. chen and d. song synthesize execute and debug learning to repair for neural program synthesis advances in neural information processing systems vol.
pp.
.
x. chen m. lin n. sch arli and d. zhou teaching large language models to self debug arxiv preprint arxiv .
.
m. fu c. k. tantithamthavorn v .
nguyen and t. le chatgpt for vulnerability detection classification and repair how far are we?
in2023 30th asia pacific software engineering conference apsec .
ieee pp.
.
q. zhang t. zhang j. zhai c. fang b. yu w. sun and z. chen a critical review of large language model on software engineering an example from chatgpt and automated program repair arxiv preprint arxiv .
.
p. lewis e. perez a. piktus f. petroni v .
karpukhin n. goyal h. k uttler m. lewis w. t. yih t. rockt aschel et al.
retrievalaugmented generation for knowledge intensive nlp tasks advances in neural information processing systems vol.
pp.
.
m. r. parvez w. u. ahmad s. chakraborty b. ray and k. w. chang retrieval augmented code generation and summarization arxiv preprint arxiv .
.
j. li y .
zhao y .
li g. li and z. jin acecoder utilizing existing code to enhance code generation arxiv preprint arxiv .
.
s. zhou u. alon f. f. xu z. wang z. jiang and g. neubig docprompting generating code by retrieving the docs arxiv preprint arxiv .
.
m. liu t. yang y .
lou x. du y .
wang and x. peng codegen4libs a two stage approach for library oriented code generation in 38th ieee acm international conference on automated software engineering ase .
ieee pp.
.
t. ahmed k. s. pai p. devanbu and e. barr automatic semantic augmentation of language model prompts for code summarization inproceedings of the ieee acm 46th international conference on software engineering pp.
.
s. lu n. duan h. han d. guo s. w. hwang and a. svyatkovskiy reacc a retrieval augmented code completion framework arxiv preprint arxiv .
.
z. tang j. ge s. liu t. zhu t. xu l. huang and b. luo domain adaptive code completion via language models and decoupled domain databases in 38th ieee acm international conference on automated software engineering ase .
ieee pp.
.
f. zhang b. chen y .
zhang j. keung j. liu d. zan y .
mao j. g. lou and w. chen repocoder repository level code completion through iterative retrieval and generation arxiv preprint arxiv .
.
p. yin b. deng e. chen b. vasilescu and g. neubig learning to mine aligned code and natural language pairs from stack overflow in proceedings of the 15th international conference on mining software repositories pp.
.
p. yin w. d. li k. xiao a. rao y .
wen k. shi j. howland p. bailey m. catasta h. michalewski et al.
natural language to code generation in interactive data science notebooks arxiv preprint arxiv .
.
d. guo q. zhu d. yang z. xie k. dong w. zhang g. chen x. bi y .
wu y .
li et al.
deepseek coder when the large language model meets programming the rise of code intelligence arxiv preprint arxiv .
.
e. simperl and m. luczak r osch collaborative ontology engineering a survey the knowledge engineering review vol.
no.
pp.
.
m. c. su arez figueroa a. g omez p erez and m. fern andez l opez the neon methodology for ontology engineering in ontology engineering in a networked world .
springer pp.
.
q. liang z. kuai y .
zhang z. zhang l. kuang and l. zhang misusehint a service for api misuse detection based on building knowledge graph from documentation and codebase in ieee international conference on web services icws .
ieee pp.
.
i. abdelaziz j. dolby j. mccusker and k. srinivas a toolkit for generating code knowledge graphs in proceedings of the 11th knowledge capture conference pp.
.
e. prudhommeaux sparql query language for rdf w3.
org tr rdf sparql query .
p. blinov semantic triples verbalization with generative pre training model in proceedings of the 3rd international workshop on natural language generation from the semantic web webnlg pp.
.
r. abreu p. zoeteweij and a. j. van gemund on the accuracy of spectrum based fault localization in testing academic and industrial conference practice and research techniques mutation taicpartmutation .
ieee pp.
.
m. papadakis and y .
le traon metallaxis fl mutation based fault localization software testing verification and reliability vol.
no.
pp.
.
r. brate m. h. dang f. hoppe y .
he a. mero no pe nuela and v .
sadashivaiah improving language model predictions via prompts enriched with knowledge graphs in dl4kg iswc2022 .
r. agashe s. iyer and l. zettlemoyer juice a large scale distantly supervised dataset for open domain context based code generation arxiv preprint arxiv .
.
match.
m. post a call for clarity in reporting bleu scores arxiv preprint arxiv .
.
j. chen x. hu z. li c. gao x. xia and d. lo code search is all you need?
improving code suggestions with code search in proceedings of the ieee acm 46th international conference on software engineering pp.
.
m. bahrami n. shrikanth s. ruangwan l. liu y .
mizobuchi m. fukuyori w. p. chen k. munakata and t. menzies pytorrent a python library corpus for large scale language models arxiv preprint arxiv .
.
t. x. olausson j. p. inala c. wang j. gao and a. solar lezama demystifying gpt self repair for code generation arxiv preprint arxiv .
.
s. ouyang j. m. zhang m. harman and m. wang llm is like a box of chocolates the non determinism of chatgpt in code generation arxiv preprint arxiv .
.
plot.
q. zhang c. fang w. sun y .
liu t. he x. hao and z. chen appt boosting automated patch correctness prediction via fine tuning pretrained models ieee transactions on software engineering .
t. le cong d. m. luong x. b. d. le d. lo n. h. tran b. quanghuy and q. t. huynh invalidator automated patch correctness assessment via semantic and syntactic reasoning ieee transactions on software engineering .
z. fan x. gao m. mirchev a. roychoudhury and s. h. tan automated repair of programs from large language models in ieee acm 45th international conference on software engineering icse .
ieee pp.
.
m. jin s. shahriar m. tufano x. shi s. lu n. sundaresan and a. svyatkovskiy inferfix end to end program repair with llms in proceedings of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering pp.
.
y .
wu n. jiang h. v .
pham t. lutellier j. davis l. tan p. babkin and s. shah how effective are neural networks for fixing security vulnerabilities in proceedings of the 32nd acm sigsoft international symposium on software testing and analysis pp.
.
m. lajk o v .
csuvik and l. vid acs towards javascript program repair with generative pre trained transformer gpt in proceedings of the third international workshop on automated program repair pp.
.
c. s. xia and l. zhang conversational automated program repair arxiv preprint arxiv .
.
d. sobania m. briesch c. hanna and j. petke an analysis of the automatic bug fixing performance of chatgpt in ieee acm international workshop on automated program repair apr .
ieee pp.
.
j. white q. fu s. hays m. sandborn c. olea h. gilbert a. elnashar j. spencer smith and d. c. schmidt a prompt pattern catalog to enhance prompt engineering with chatgpt arxiv preprint arxiv .
.
e. gamma r. helm r. johnson and j. vlissides design patterns elements of reusable object oriented software .
pearson deutschland gmbh .
s. abukhalaf m. hamdaqa and f. khomh on codex prompt engineering for ocl generation an empirical study arxiv preprint arxiv .
.
j. white s. hays q. fu j. spencer smith and d. c. schmidt chatgpt prompt patterns for improving code quality refactoring requirements elicitation and software design arxiv preprint arxiv .
.
a. borji a categorical archive of chatgpt failures arxiv preprint arxiv .
.
a. mallen a. asai v .
zhong r. das d. khashabi and h. hajishirzi when not to trust language models investigating effectiveness of parametric and non parametric memories arxiv preprint arxiv .
.
n. carlini f. tramer e. wallace m. jagielski a. herbert v oss k. lee a. roberts t. brown d. song u. erlingsson et al.
extracting training data from large language models in 30th usenix security symposium usenix security pp.
.
w. chu l. li l. reyzin and r. schapire contextual bandits with linear payoff functions in proceedings of the fourteenth international conference on artificial intelligence and statistics .
jmlr workshop and conference proceedings pp.
.