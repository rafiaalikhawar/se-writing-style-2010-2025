deep learning based code reviews a paradigm shift or a double edged sword?
rosalia tufano alberto martin lopez ahmad tayeb ozren dabi c sonia haiduc gabriele bavota software institute usi universit a della svizzera italiana switzerland florida state university united states rosalia.tufano alberto.martin ozren.dabic gabriele.bavota usi.ch atayeb2 shaiduc fsu.edu abstract several techniques have been proposed to partially automate code review.
early support consisted in recommending the most suited reviewer for a given change or in prioritizing the review tasks.
with the advent of deep learning in software engineering the level of automation has been pushed to new heights with approaches able to provide feedback on source code in natural language as a human reviewer would do.
also recent work documented open source projects adopting large language models llms as co reviewers.
although the research in this field is very active little is known about the actual impact of including automatically generated code reviews in the code review process.
while there are many aspects worth investigating e.g.
is knowledge transfer between developers affected?
in this work we focus on three of them i review quality i.e.
the reviewer s ability to identify issues in the code ii review cost i.e.
the time spent reviewing the code and iii reviewer s confidence i.e.
how confident is the reviewer about the provided feedback.
we run a controlled experiment with professional developers who reviewed different programs with without the support of an automatically generated code review.
during the experiment we monitored the reviewers activities for over hours of recorded code reviews.
we show that reviewers consider valid most of the issues automatically identified by the llm and that the availability of an automated review as a starting point strongly influences their behavior reviewers tend to focus on the code locations indicated by the llm rather than searching for additional issues in other parts of the code.
the reviewers who started from an automated review identified a higher number oflow severity issues while however not identifying more highseverity issues as compared to a completely manual process.
finally the automated support did not result in saved time and did not increase the reviewers confidence.
index terms code review controlled experiment i. i ntroduction code review is an essential activity in both industrial and open source projects.
its benefits have long been recognized and studied and include improved code quality and a reduced incidence of bugs among others .
however code review can also be time consuming and costly and over the past decade researchers have been studying ways to reduce the cost of this activity while maintaining its benefits.
some early efforts included recommending the most suitable reviewer for a given change predicting the defectiveness of a patch before or after being reviewed and classifying the usefulness of review comments .
equal contributor.
author order determined by coin flip.over the last few years along with the rise of deep learning dl came a new wave of approaches aimed at reducing the costs of code review by exploiting dl techniques to automate the code review process.
these approaches have been able to provide natural language review comments about source code similar to what a software developer would provide .
moreover recent work reported open source projects adopting large language models llms as co reviewers .
given the tremendous potential shown by llms in helping developers with various software engineering tasks it seems natural to involve them also in the code review process with the goal of reducing developer effort and software cost.
however little is known about the actual impact of including automatically generated code reviews in the code review process.
for example it is hard to anticipate how using automated code reviews as a starting point could impact the knowledge transfer between developers or the quality of the final code review or if using automated reviews could lead to biases or blind spots in developers analysis of the code if it leads to a lower code review cost etc.
studying the impact of all these factors goes beyond the scope of a single paper.
however we aim to make the first steps in this direction by focusing on three specific aspects namely i code review quality i.e.
a reviewer s ability to identify issues in the code ii code review cost i.e.
the time spent by developers reviewing the code and iii reviewer s confidence i.e.
how confident the reviewer is about the provided feedback.
to determine the impact that using automated code reviews can have on these three aspects we present a controlled experiment involving developers.
the participants performed a total of reviews across six projects written either in python or java in which we injected quality issues representative of problems usually found during code review .
participants have been assigned to one of the two languages based on their expertise.
each review was performed with one of three treatments.
the first named manual code review mcr assumes no availability of an automatically generated review as starting point thus reflecting the classic code review scenario.
the second named automated code review acr provides reviewers with a review automatically generated by chatgpt plus which they can use as a starting point for their final reviews e.g.
they could discard generated comments rephrase them or complement the set of identified issues .arxiv .11401v3 nov 2024this scenario is representative of the current state of the art in automated code review .
the third named comprehensive code review ccr aims at simulating a hypothetical scenario in which tools for the automated generation of code reviews reached a whole new level where they can correctly identify all the important issues in the code.
to simulate this scenario we provide participants with a code review correctly pointing to all issues we injected presenting it as automatically generated.
the latter scenario while not realistic nowadays given the current technology allows us to observe the extent to which reviewers would trust an automated tool by adopting its suggestions and the impact it would have on reviewing time.
our main findings can be summarized as follows reviewers considered valid most of the issues identified by chatgpt plus .
on average of the issues automatically identified by the llm have been kept by the reviewers in their final review.
the availability of an automated review as a starting point strongly influences the reviewer s behavior .
reviewers mostly focused on the code locations pointed out in the automatically generated review they were provided with this holds for both acr and ccr treatments .
while we observed substantial variability in the code locations commented on by reviewers who inspected a program manually the ones who started from an automated review tended to focus on the code locations already provided in it.
the automated code review generated by the llm does not help in identifying more high severity issues as compared to a completely manual process.
we only observed a significant difference in the number of low severity issues in the code in favor of the treatment adopting the chatgpt based review .
even assuming an excellent support in terms of automated review ccr treatment reviewers do not save time with respect to a fully manual process .
this is due to the fact that they need to interpret the automatically generated comments and check for their correctness in the code.
providing reviewers with automatically generated reviews both in the acr and ccr treatment does not impact their confidence which is comparable to that observed in the mcr treatment .
this might be due to the fact that the provided review does not help in better understanding the program but only highlights potential issues.
study material and data are publicly available .
ii.
s tudy design the goal of the study is to assess the impact that having access to automatically generated code reviews has on the code review process.
more specifically we aim to understand how the availability of an automated code review affects the quality of the final review written by the reviewer the time they spend reviewing the code and their confidence about the submitted review.
note that our study focuses on a scenario in which the output of the dl based approach is provided to the reviewer as a support for the review writing.however these tools could also be seen as a possible support for the contributor i.e.
a first feedback loop before submitting the code for the actual human review see e.g.
.
while this work focuses on the perspective of reviewers and the bias they may be subject to when using automated code reviews future work could certainly investigate the contributors perspective i.e.
do automated reviews affect the quality of the code reviewed by humans?
.
the study addresses the following research questions rq rq is there a statistically significant difference in the characteristics of the code reviews written by developers with without automated support?
this preliminary rq aims at quantitatively comparing the code reviews submitted by developers with without access to automatically generated reviews.
we analyze various aspects including the number of issues reported the length of the code review number of sentences and the problematic code locations identified e.g.
number of different lines in which issues have been found .
rq to what extent does having access to automated code reviews increase the likelihood of identifying code quality issues?
we inject quality issues in the code of our software projects and assess the extent to which participants were able to identify them with without the support of automated code review.
since we manually inspect all code reviews written by developers we also analyze discuss and compare among the different treatments the additional quality issues that were not injected by us but were found by participants.
rq to what extent does the availability of automated code reviews save time during the review process?
we compare the amount of time spent by reviewers with without the support of automated code review.
in particular we analyze the time spent i to complete the review task i.e.
overall time ii inspecting the code and iii writing the actual review.
rq does the availability of automated code reviews increase the reviewers confidence?
at the end of each reviewing task we ask reviewers to rate their confidence in the submitted review and we investigate if the availability of automated code reviews has an impact on the reviewers confidence.
a. context selection participants we used convenience sampling to recruit participants who i are currently professional developers of them or ii have worked in the past as professional developers and are now enrolled in a cs graduate program one of them .
we did not include any cs student without at least one year of industrial experience.
although this limited the number of participants we could involve we consider industrial experience essential in any study on code review given its common use in industrial practice.
for simplicity in the following we use the term developers when referring to participants even though one of them is not currently working as a developer.
we invited developers to participate in our study asking each of them to contribute three code reviews each using a different treatment details in section ii b .table i summary of the object programs .
project id language source loc issues maze generator java rosetta maze generator python rosetta number conversion java rosetta number conversion python rosetta stopwatch java apache stopwatch python translated tic tac toe java rosetta tic tac toe python rosetta todo list java artificial todo list python artificial word utils java apache word utils python translated the invitation email available in our replication package asked them to accept the invitation if they i are familiar with code review and ii have experience with at least one of the two subject programming languages i.e.
java and python .
their availability was collected using a google form.
only if they accepted our invitation we asked them four questions.
the first was please select the programming languages in which you have expertise check both of them if you are familiar with both with possible answers being python and java.
the information collected was used to assign participants to the code review tasks described in section ii a2 i.e.
to ensure that they were only allocated code review tasks involving a programming language they were familiar with .
then the developers answered three questions related to their expertise their years of experience in programming their current role position and whether they took part in the past in the code review process as a reviewer as a developer whose code was reviewed in both roles or in none of them.
we received an answer from developers all of whom accepted to participate in the study.
however in the end only completed at least one of the tasks assigned i.e.
at least one code review .
from these we selected participants for our analyses in such a way as to have the same number of participants per system and treatment details in section ii d .
on average the participants have .
years of programming experience median min max three of them selected java as programming language six python and checked both languages.
finally three of them have not been involved in code review in the past while still being familiar with it one only as a reviewer one only as a contributor while have covered both roles.
programs table i shows the programs that we asked participants to review.
we considered six different projects each available in both programming languages i.e.
java and python.
our object programs are taken from different sources see column source in table i .
we selected three programs maze generator number conversion andtic tac toe from rosetta code a repository of programming tasks written in multiple languages including java and python.
two java programs stopwatch and word utils were taken from utility classes of the apache commons lang library and then translated by the authors into python.one program todo list was created from scratch for both java and python.
to ensure that the implementation of the selected programs was of high quality each of them was reviewed by two authors of the paper.
additionally all programs were reviewed by a professional developer with seven years of experience and high familiarity with both python and java.
the selection of the programs was guided by two main goals i to ensure code reviews are manageable in terms of both time and complexity for our study participants we therefore chose programs that are relatively small in terms of lines of code refer to the loc column in table i and ii to avoid requiring specific domain knowledge from the participants for this reason we selected programs that any seasoned developer could easily understand and review.
the chosen programs include maze generator which creates a random maze in the console based on user specified dimensions number conversion enabling the conversion of decimal numbers into binary octal hexadecimal and roman numeral formats stopwatch a basic program that performs stopwatch functions like start stop reset and split time tic tac toe an implementation of the corresponding game played via command line interface cli against an agent programmed not to lose todo list a cli based to do list manager supporting adding removing prioritizing and listing tasks and word utils a set of utility functions offering string manipulation utilities such as capitalization case swapping and abbreviation.
we then manually injected a number of quality issues in the selected programs given that among other things we aim at assessing the extent to which an automated code review increases the chances of identifying code quality issues.
the column issues in table i shows the number of issues injected into every program.
overall we injected issues across the programs.
the complete list of issues injected and their description is available in our replication package and includes code duplication structural defects e.g.
overly long methods documentation issues e.g.
mismatches with respect to the implementation and logic bugs among others.
we paid attention not to inject issues which can be very easily detected by participants e.g.
bugs that make the program crash since they might not represent realistic simulations of code submitted by a developer for review.
this also meant that we did not inject a fixed number of issues per project because we found a different number of issues to be suitable for different programs.
the injected issues are inspired by the taxonomy of issues found in code reviews documented by m antyl a and lassenius .
in particular m antyl a and lassenius found of the issues identified by reviewers to be related to evolvability defects e.g.
documentation issues sub optimal implementation choices with the remaining pertaining functional issues e.g.
wrong implementation logic .
by classifying the type of issues we injected according to the definitions given by fregnan et al.
we injected evolvability issues and defects in the object java python programs.public string tostring stringbuilder builder new stringbuilder for int row row height row builder.append tostring row builder.append h wall closed.repeat width .append corner .append n return builder.tostring public string tostring string result for int row row height row result tostring row result h wall closed.repeat width corner n return result if part and part in delimiters result.append part elif part and re.match regex pattern part result.append part if part and part in delimiters or re.match regex pattern part result.append part java pythonfig.
.
examples of injected issues in java and python.
the top part of each example represents the original code the lower part is the code after injection.
as an additional note the injected issues cover of the issue types in the taxonomy by m antyl a and lassenius .
for example we did not inject visual representation issues since code formatting usually depends on each project s practices.
fig.
shows two examples of injected issues one per language .
the top part of each example shows the original code while the bottom part reflects the code after the issue injection.
the java example represents the injection of an issue related to performance we replaced the use ofstringbuilder with string concatenation creating a degradation of performance.
the python code on the other hand exemplifies the injection of a structural defect we introduced an unnecessary nested condition also creating duplicated code.
each object program also featured i a main file allowing to run it and ii test cases exercising its basic functionalities.
b. code review treatments to understand the impact of supporting developers with code review automation we defined three treatments .
the first named manual code review mcr resembles the classic code review process performed by developers without any automated support.
the second named automated code review acr provides the participant with a code review automatically generated by chatgpt plus i.e.
gpt .
the prompt used to create code reviews was provide a detailed code review of the following java python program code .
the third treatment called comprehensive code review ccr is designed to simulate the scenario where the reviewer is provided with an automated code review which correctly identifies all issues we injected.
unlike the first two treatments which represent realistic scenarios in industry this ccr treatment simulates an ideal hypothetical scenario where the automated code review is able to identify all quality issues in a given code.while this scenario is not yet fully attainable with today s technology studying it can provide valuable insights into how reviewers behaviors might change if they had access to a future tool capable of identifying every quality issue in the code e.g.
would they trust the tool enough to significantly save time?
.
to simulate this scenario participants in the ccr treatment were told that the reviews were automatically generated even though this was not the case.
more specifically we first manually performed the code review and made sure to capture all the issues we injected in the review.
then we used chatgpt plus to rephrase the code reviews we wrote using the prompt rephrase the following code review comment as if you are generating it comment .
the comment refers to the following java python code code .
note that while the ccr reviews include exactly ncomments identifying the nissues injected in each program the acr treatment may identify comments that only address some or none of the injected issues.
additionally acr could include comments on other quality issues we did not introduce.
the code reviews generated or rephrased by chatgpt plus for acr and ccr projects languages treatments are publicly available .
in summary the three treatments compare the classic manual code review process mcr with the automation available in practice nowadays acr and a utopian scenario we hope to reach one day in the field ccr .
c. experimental setup and procedure our study is comprised of different code review tasks projects languages treatments .
as previously mentioned we asked participants to review three programs each with a different code review treatment.
the three tasks were all in the same programming language but related to different projects.
we provided each participant with instructions to connect via the remote development plugin of visual studio code vs code to a server we set up with the environment needed to run the study.
we took care of installing the versions of java and python .
needed to run any of the object programs.
also we installed in vs code the java extension pack and python .
once connected the participants could see the review tasks assigned to them directly in the ide without the need of installing configuring anything.
in particular the participants were presented with three projects already imported in the ide two of which included a code review for the treatments automated code review andcomprehensive code review .
the code review was presented through the code review plugin in vs code which also allows to mark source files with review comments and to modify delete the already provided comments part of the provided code review.
all projects featured a readme file with instructions on where to find a description of the program to review how to use the code review plugin how to run the program and associated tests and a reminder to rate the confidence of their review at the end of each code review task by simply writing a score from very low confidence to very high confidence at the end of the readme file.this was the only action required from the participants at the end of each task.
the code review plugin of vs code took care of storing the final version of their code review on our server i.e.
the code review including all comments the participants manually wrote plus for the acr and ccr treatments the comments they decided to keep as is or by rephrasing modifying them from the provided reviews.
each comment is linked to a file and a range of selected text in terms of line column numbers .
lastly besides the final code review and the self assessed confidence we also monitored the participants behavior in the ide using tako a vs code plugin collecting the actions performed in the ide.
tako records events such as opening and closing files and tabs switching between them and editing files among others.
this allows us to perform the time based analyses needed to answer rq .
d. data analysis out of the reviews we assigned participants treatments of them were completed by participants.
as a result we ended up with a different number of reviews for different programs for each treatment.
in order to compare the treatments fairly we systematically selected the highest possible number of reviews per treatment such that all treatments featured exactly the same number of reviews on exactly the same programs thus being comparable .
we prioritized reviews from participants who completed two participants or three tasks.
this led to the selection of reviews i.e.
per treatment from participants.
two authors independently inspected each review to extract data needed to answer our rqs which cannot be automatically collected.
at the end they had a meeting to discuss about differences in the extracted data and agree on the correct data to report.
the manually collected data include the total number of reported quality issues in each review.
this information cannot be automatically extracted by counting the number of comments in the code review since each comment may point to several quality issues even in the same code location.
the number and percentage of injected issues identified in the code review .
the number and percentage of quality issues identified in the initial reviews provided to participants which have been kept in the finally submitted code review .
this metric has only been collected for code reviews resulting from the acr and ccr treatments.
the additional quality issues i.e.
unrelated to the injected ones present in the final code review .
out of the reviews manually inspected by two authors they disagreed only on three reviews about the total number of reported quality issues and about the number of injected issues identified.
conflicts were solved via open discussion.
we answer our rqs by comparing the code reviews output of the three treatments.
table ii reports the dependent variables considered in each rq the independent variable being the same for all rqs i.e.
treatment and the control variables.table ii variables used in our study .
variable description dependent variables rq number of reported quality issuesthe total number of quality issues reported in a submitted review length of the code review the number of sentences in a submitted review covered code locationsnumber of lines subject of at least one review comment dependent variables rq is injected issue identified the participant found the injected issue dependent variables rq total timethe total time spent on the whole code review session time reviewingthe time spent on the actual code to review i.e.
excluding the time spent writing the review or running the program time writingthe time spent writing review comments or reading the ones already provided in the automated review dependent variables rq confidencethe confidence score provided by the participant for the submitted review independent variables treatmentthe treatment used to perform the code review one among mcr acr ccr control variables experience years of experiencethe years of experience in programming of the participant involved in code reviewwhether the participant has been involved in the code review process as a reviewer as a developer whose code was reviewed in both roles or in none of them.
control variables review task programming languagethe language in which the code to be reviewed is written java or python programthe program on which the review task must be performed issue typethe type of the issue to spot classified according to the work by fregnan et al.
for rq we compare i the total number of reported quality issues ii the length of the code review in terms of number of sentences and iii the covered code locations in terms of number of lines subject of at least one review comment we also differentiate between code statements and code documentation .
for answering rq we compare the ability of participants to identify injected issues during code review.
in this case a boolean dependent variable is injected issue identified has been used to indicate whether each of the bugs injected in the programs under study has been identified.
concerning rq we exploit the data collected by tako to compare i the total time spent on the whole code review session ii the time spent on the actual code to review i.e.
excluding the time spent writing the review or running the program and iii the time spent writing review comments or reading the ones already provided in the automated review.
finally for rq 3we compare the confidence scores provided by participants to check whether the availability of the automated code review had an impact on the perceived confidence.
for all comparisons we use boxplots to visualize the distributions.
in addition to that we run the following statistical analyses.
when needed we opted for non parametric tests since all our distributions are not normally distributed shapiro wilk test .forrq we build a multivariate logistic regression model having is injected issue identified as the dependent variable andtreatment as independent variable with mcr set as the reference level to more easily look at the impact of introducing automation i.e.
acr and ccr treatments in the code review process.
we include all control variables listed in table ii.
concerning the other rqs i.e.
rq rq and rq we use multivariate linear regression to build seven models one for each dependent variable in table ii.
for example to answer rq0 three regression models have been built each using one of the three dependent variables relevant for this rq.
the independent variable is always treatment while in terms of control variables we use all the ones in table ii but the issue type.
indeed the dependent variables used in rq rq and rq3 differently from the one used in rq are meaningful only when applied to a whole code review e.g.
the time spent to complete the code review the confidence of the participant when submitting the review .
since a single review concerns several issues usually having a different type for these rqs we do not consider the issue type as control variable.
iii.
r esults and discussion a. rq differences in reviews output of different treatments fig.
shows boxplots comparing the final version of the code reviews submitted by developers under the three treatments considered in our study i.e.
manual code review mcr automated code review acr and comprehensive code review ccr .
from left to right we report boxplots comparing the number of quality issues reported in the review the length of the review in terms of number of sentences the overall line coverage of the review over the entire program code and comments as well as its coverage when considering only the source code and only the documentation comments .
the two leftmost boxplots show clear differences among treatments in terms of the number of issues reported in the final reviews and in their length.
reviews resulting from treatments including an automated support acr and ccr generally report more issues as compared to mcr.
the multivariate regression model multiple r2 .
table iii confirms the significant role played by the acr treatment p .
in the number of reported issues with the dunn s test showing a statistically significant difference when comparing acr vsmcr p value .
after benjaminihochberg correction .
worth mentioning is also that participants tended to report more quality issues for some of the subject programs.
this is expected considering that i we injected a different number of quality issues in each program and ii chatgpt identified a different number of quality issues in the programs influencing the number of quality issues reported in the acr treatment.
the final reviews of the acr treatment identified on average .
issues median .
against the .
of ccr median and the .
of mcr median .
.
such a result could be explained in part by the fact that reviewers kept most of the issues already present in the automated acr and in thecomprehensive ccr code review.indeed the acr and ccr reviews we provided to participants featured on average .
median and median issues reported respectively and reviewers kept on average .
median .
and .
median of these issues respectively.
this leads to a first outcome of our study reviewers considered as valid most of the issues identified by chatgpt plus by keeping them in their final review .
the fact that acr and ccr reviews contained issues that reviewers kept in most cases had an impact on the final length of the submitted reviews with acr and ccr reviews being longer than those in the mcr treatment see fig.
.
the regression model in table iii multiple r2 .
reports a significant impact of the acr treatment p .
with the dunn s test confirming the statistically significant difference in length when comparing reviews output of the acr and ccr treatments p value .
and those output of acr and mcr p value .
p values adjusted with benjamini hochberg correction .
to get a better understanding of the magnitude of such differences the final reviews in the mcr treatment include on average .
sentences median .
compared to the .
median in the final reviews of the acr treatment.
interestingly as our results show a more verbose review does not necessarily mean a review that covers more code locations.
the three rightmost boxplots in fig.
illustrate this phenomenon there is no clear difference in the coverage overall on code and on comments between the final reviews of the three treatments as also confirmed by the regression model in table iii .
moreover the reviews written by participants without automated support had a higher variability in terms of the lines commented on by the different reviewers while those starting from automated reviews tended to stay focused on the lines of code highlighted in the provided reviews.
this is illustrated in the venn diagram in fig.
which shows the total number of different lines covered by all reviews belonging to each treatment.
as observed mcr reviews covered a total of distinct lines and of these lines were unique to mcr i.e.
not covered by the acr nor ccr final reviews.
this was followed by ccr reviews which covered lines of which were unique to final reviews within this treatment and lastly acr reviews covering distinct lines of which being found only in acr reviews.
fig.
shows an example of the code lines covered by three participant reviews clear gray gray black from the acr left and mcr right treatments for the number conversion program in java.
each rectangle denotes a single issue identified in the reviews possibly spanning multiple lines thicker rectangles .
red rectangles denote issues covering unique code locations among the three reviews of that treatment .
while for the acr treatment there were only three issues from two reviewers covering unique locations for the mcr treatment all three reviewers found a total of eight issues covering unique code lines not covered in the other reviews.
these findings result in two additional takeaways of our study.length of the code review sentences number of reported quality issues overall line coverage code coverage documentation coverage mcr acr ccrmcr acr ccrmcr acr ccrmcr acr ccrmcr acr ccrfig.
.
rq characteristics of the final code reviews under the three treatments.
table iii rq0 m ultivariate linear regression models estimate std.
error significance .
n. of reported quality iss.
length of the code review covered code locations estim.
s.e.
sig.
estim.
s.e.
sig.
estim.
s.e.
sig.
intercept .
.
.
.
.
.
acr .
.
.
.
.
.
ccr .
.
.
.
.
.
years of experience .
.
.
.
.
.
involved in code review contributor reviewer .
.
.
.
.
.
involved in code review none .
.
.
.
.
.
involved in code review reviewer .
.
.
.
.
.
programming language python .
.
.
.
.
.
program number conversion .
.
.
.
.
.
program stopwatch .
.
.
.
.
.
program tic tac toe .
.
.
.
.
.
program todo list .
.
.
.
.
.
program word utils .
.
.
.
.
.
sig.
codes p .
p .
p .
mcr acrccr785687562631811868.
.
.
.
.
.
fig.
.
rq number of distinct lines covered i.e.
commented on by participants in the final reviews of the three treatments and their overlap.
1102030405060708090100110116acr reviewsncr reviewscode lineslegend review 1review 2review 3issue in uniquecode location acr reviewsmcr reviews fig.
.
example of distinct code lines covered by different reviews.
first reviews obtained with the support of automated tools might be more expensive to process for the contributor i.e.
the developer submitting the code for review since they are significantly more verbose as opposed to those manually written while commenting on a similar amount of code lines.
mcr acr ccrmcr acr ccrnumber of injected issue identified of injected issue identifiedfig.
.
rq number and percentage of identified injected issues.
this might indicate an additional cost on the contributor s side which complements the analysis we will present in rq when assessing the time spent by reviewers under the three treatments.
second the availability of an automated review influences the reviewer s behavior who will mostly focus on the code locations commented on in the provided review.
this also results in a lower variability in the types of issues identified by reviewers for the same program.
b. rq impact on quality issues found fig.
shows boxplots with the number top and percentage bottom of injected issues identified in the final reviews of the three treatments.
table iv reports the results of the logistic regression model using the is injected bug identified as dependent variable.
as expected the final reviews submitted under the ccr treatment usually report of injected issues as these were already present in the review initially provided to them and kept in the final review with the logistic regression confirming the significant influence of ccr on the odds of identifying the injected bug .table iv rq1 logistic regression model .
estim.
s.e.
sig.
intercept .
.
acr .
.
ccr .
.
years of experience .
.
involved in code review contributor reviewer .
.
involved in code review none .
.
involved in code review reviewer .
.
programming language python .
.
program number conversion .
.
program stopwatch .
.
program tic tac toe .
.
program todo list .
.
program word utils .
.
issue type evolv.
docum.
textual .
.
issue type evolv.
structure org.
.
.
issue type evolv.
structure solution app.
.
.
issue type funct.
check .
.
issue type funct.
interface .
.
issue type funct.
logic .
.
sig.
codes p .
p .
p .
more interestingly the final acr reviews submitted by participants uncovered a median of of the injected issues i.e.
the same amount uncovered in the mcr reviews which were written from scratch by participants despite the fact that the automated reviews initially provided in the acr treatment already reported on average of the injected issues.
this unveils a few important findings of our study the current state of the art in terms of automated code review does not lead to a higher number of injected issues being identified compared to fully manual code reviews.
this is due to two factors.
first chatgpt produced reviews in which more than half of the injected issues on average were not identified.
second the exposure that developers had to these automated reviews before writing their final review seems to have biased their behavior leading to them barely identifying any additional injected issues compared to the ones already included in the automated review.
these findings should act as a clear warning sign for companies interested in adopting automated support in code review.
a possibility to consider is to provide the automated review only once the reviewer already submits their comments thus not being biased by the ones already provided.
since the final reviews of acr report the highest number of issues without however identifying a higher number of injected issues a question arises about the relevance of the additional issues identified in the acr reviews.
indeed it is possible that additional issues reported are just as relevant as the injected ones.
to assess this we asked two developers not involved in the study to assess the severity of the i injected issues as documented in the reviews provided as starting point in the ccr treatment ii non injected issues automatically identified in acr reviews and iii non injected issues manually identified in mcr reviews.
the two developers have and years of programming experience respectively and were instructed to provide a severity assessment on a scale from low severity to high severity with the former indicating issues which they do not consider mandatory to address for approving the code and the latter indicating showstoppers.while we acknowledge the subjectivity of this assessment the two developers who performed it had a strong disagreement i.e.
1vs3 in only of the inspected issues.
the weighted kagreement was .
.
the findings of this analysis indicated that the issues we injected were the ones assessed with the highest severity q1 .
q2 .
q3 .
mean .
followed by the additional ones manually identified by developers q1 .
q2 .
q3 .
mean .
and the additional ones recommended by chatgpt q1 .
q2 .
q3 .
mean .
.
we also factored in the issue severity as a further cofactor in the logistic model without however observing any impact of it nor changes in the significant variables output of the model available in .
the difference between the severity of the injected issues and the additional ones identified either manually or automatically is statistically significant p value .
with a medium effect size mann whitney test and the cliff s delta .
an example of an automatically identified issue classified as low severity by both developers is this method provides an interesting feature by .
the logic is sound though it involves several conditionals and might benefit from comments explaining the rationale for each case.
.
in this case while comments are indeed missing the code is quite selfexplanatory.
on the other hand some high severity injected issues were missed by chatgpt such as the following one provided in the initial review of the ccr treatment there is a critical bug.
for hexadecimal it erroneously uses base instead of .
it should be decimaltoanybase num .
.
rq1 s findings lead to the conclusion that the reviews output of the acr treatment identify a higher number of low severity issues while however not making a difference when it comes to spotting high severity issues i.e.
the ones we injected as compared to a full manual process.
this supports our former suggestion that given the current state of automation automatically generated reviews may be considered as a useful complement at the end of a manual review process.
c. rq impact on review time fig.
shows from left to right the time in seconds that reviewers spent on the whole code review process the time spent on the code to review i.e.
not including running the program or the tests and the time writing the review comments.
for the acr and ccr treatments the latter also includes the time spent reading the originally provided reviews.
while we also created multivariate linear regression models we found that none of the involved independent variables treatments and cofactors plays a statistically significant role on any of the time based dependent variables.
thus for the sake of space we only report these models in our replication package .
this finding debunks one of the motivations for automated code review i.e.
saving time for reviewers.
in fact fig.
shows that reviews performed completely manually mcr took less time mean median minutes than those supported by automation in the acr mean median minutes and ccr mean median minutes treatments.total time s time reviewing s time writing s mcr acr ccrmcr acr ccrmcr acr ccrfig.
.
rq time spent in reviews across different treatments.
this might be due to the fact that the introduction of an automated review even if correct ccr comes with a price namely the reading understanding and double checking of the provided comments.
this seems to be the case especially for ccr reviews where reviewers spent an average of minutes median reading and writing reviews compared to an average of minutes median for reviews written from scratch see rightmost boxplot in fig.
.
it is also interesting to note that the variability in the time spent on the code to review and on the overall review process is higher for the acr and the ccr treatments as compared to mcr.
we presume that this is due to two factors namely i the length of the automatically generated reviews where the shortest pointed to no issues acr review of stopwatch in python while the longest pointed to issues acr review of maze generator in java and ii the trust that reviewers put in the automated support which may vary from one reviewer to another.
longer case studies in which developers have time to build their opinion about the review automation tool are needed to corroborate or contradict our findings especially when it comes to what we observed in terms of time spent.
table v rq3 linear regression model .
estim.
s.e.
sig.
intercept .
.
acr .
.
ccr .
.
years of experience .
.
involved in code review contributor reviewer .
.
involved in code review none .
.
involved in code review reviewer .
.
programming language python .
.
program number conversion .
.
program stopwatch .
.
program tic tac toe .
.
program todo list .
.
program word utils .
.
sig.
codes p .
p .
p .
d. rq impact on reviewer s confidence after each code review task reviewers scored their confidence in the review they submitted on a scale from very low confidence to very high confidence .
reviews from the mcr treatment were scored with an average confidence of .
median those from the acr treatment with an average confidence of .
median and those from the ccr treatment with an average confidence of .
median .indeed as shown in the linear regression model in table v multiple r2 .
there is no significant impact of the treatment on the confidence score reported by reviewers.
we thus conclude that providing an automated code review as a starting point even one being able to identify several high severity issues i.e.
ccr treatment does not have a significant effect on the confidence of the reviewer.
this might be due to the fact that while the automated code review may point the reviewer to relevant code locations it does not help in understanding the code which in the end is what we expect to mostly influence a reviewer s confidence.
combining automated code review with llm based code summarization may help in that direction.
e. actionable recommendations based on our findings we distill the following actionable recommendations for reviewers designers of tools aimed at automatically generating code reviews and researchers.
reviewers we observed that the availability of an automated review strongly influences the reviewer s behavior who will mostly focus on the code locations commented on in the provided review.
also automated reviews result in a lower variability in the types of issues identified by different reviewers for the same program.
based on these findings we recommend to adopt automated reviews as a further check only after the manual inspection.
this will not save time but it could help in identifying additional quality issues.
tools designers while most of the issues reported by chatgpt have been considered valid by reviewers i.e.
kept in the final review we found that these issues tend to have a quite low severity.
tools tailored for the identification of specific high severity quality issues would be a valuable asset.
also automatically generated reviews are much more verbose than those manually written.
this is a non negligible cost that should be considered in the design of tools e.g.
by making a best effort to keep the generated reviews concise.
researchers we did not observe any time saved thanks to the availability of automated reviews.
thus the motivation for introducing these tools in a code review process may be more related to a more comprehensive code inspection rather than to save time .
also given the strong bias in reviewers behavior we identified studies investigating the impact on practitioners behavior when exploiting ai based tools to semi automate se tasks are very much needed.
iv.
t hreats to validity construct validity.
a major challenge was the time measurement in rq since interruptions were possible while the participants were performing the code reviews.
we instructed participants to not interrupt a code review task and to take breaks only when changing treatment.
still in the time based analysis we excluded data points out of since it was clear that they represented errors in the measurement when compared to the other timings.
the removed data points were balanced across the treatments for ncr for acr and for ccr and did not change the final outcome of our study.analyses reported in rq 0and rq 1are based on data manually extracted from the submitted reviews e.g.
number of issues reported in the reviews thus involving subjectivity risks.
to partially address them we made sure that each review was independently inspected by two evaluators.
internal validity.
participants who only contributed one or two of the assigned reviews out of contributed three reviews made the study diverge from our initially planned within subject design.
still we addressed the issue by only considering of the reviews we collected in our analysis with the goal of balancing within each treatment the number of reviews performed on each project.
while we collected some demographic data about participants there are several other factors that could have influenced our findings.
these include i educational background ii work experience iii the lack of knowledge some participants may have about the inspected programs iv no past experience of participants in using code review automation tools thus not being able to properly calibrate their confidence in accepting rejecting the recommendations and v possible time constraints that participants had while performing the review task.
our replication package features a causal diagram showing all measured and unmeasured variables which may have played a role on the measured dependent variables.
external validity.
as a design choice we decided to only involve in our study professional developers.
this resulted in a limited number of participants who took part to our study for a total of data points i.e.
submitted review per treatment.
we acknowledge that our study could be statistically underpowered thus leading to biased conclusions.
for this reason replications are needed in order to corroborate or contradict our preliminary observations.
another threat to the generalizability of our findings concerns the representativeness of the issues we injected as representative of those actually found in industrial code review.
for example concerns may arise about the triviality of the injected issues also considering the subject programs which for the sake of limiting the study time were limited in terms of size.
nonetheless we took inspiration from the taxonomy of issues found in code reviews documented by m antyl a and lassenius trying to inject a proportion of evolvability and functional issues close to the one they document i.e.
out of issues found in code review do not pertain the visible functionality of the program .
also our results show that when not running the task in the context of the ccr treatment i.e.
a review identifying the injected issues was available participants were able to identify all injected issues in only out of the code review tasks.
this addresses concerns about the triviality of at least identifying the issues.
v. r elated work a. code review automation most of the prior work focused on classification tasks such as recommending the best suited reviewer s for a given change classifying the sentiment of review comments and their usefulness etc.more relevant to our work are the generative code review tasks recently automated via dl.
the two most commonly addressed tasks in the literature are review comment generation and code refinement .
the former consists in automatically generating review comments in natural language for a given piece of code similar to those that a human reviewer would write.
the input of the dl model is represented by the code to review while its output consists of a set of natural language comments pointing to issues in the code.
in the code refinement task the dl model takes as input the code submitted for review and the natural language comments written by a human reviewer.
the goal of the model is to refine the code to address the reviewer s comments.
the evaluations of these tasks showed promising results e.g.
correct predictions in the code refinement task while still pointing to the need for major improvements before these tools can be considered for industrial adoption .
given the documented evidence of chatgpt s adoption in open source projects as a co reviewer in our experiment we adopted chatgpt as the representative for code review automation tools supporting the comment generation task.
b. controlled experiments on code review there are several prior controlled experiments on code review reported in the literature.
table vi offers a brief overview of these works and reports i the related reference ii the number and type of participants involved in the study where type can be p practitioners s students or both iii the subject programming languages iv the number of total review tasks collected in some studies participants performed more than one task while in others only one and v a short description of the manipulated independent variables and measured dependent variables.
the last row of table vi reports the same information for our study.
as illustrated all works focus on independent variables different from the one tackled in our study i.e.
the presence absence of an automated code review .
several works also differ in terms of measured dependent variables focusing on the reviewer s comprehension level.
we discuss in the following only the most related controlled experiments being the ones sharing with us the measured dependent variables.
khandelwal et al.
study how the usage of gamified code review tools can improve the usefulness of the code review comments and the identified quality issues.
the study involved undergraduate students who had to write i a program which on average was composed by lines of code and ii reviews for peer written programs using one of five code review tools three being gamified and two not .
this led to a total of code review tasks which showed no impact of gamification on the code review quality.
hanam et al.
present an experiment involving practitioners and students to investigate the impact of change impact analysis on the ability of identifying bugs during code review.
each participant performed two code review tasks one using a change impact analysis tool named semcia and one not using it leading to code reviews collected.table vi controlled experiments on code review for participants type s indicates students p practitioners .
reference participants programming review independent variable main dependent type languages tasks manipulated variables measured runeson and wohlin p s c three approaches for estimating the number of bugs in the code being reviewedhow close the approaches were in terms of estimates after the review was performed tao and kim s java using not using an approach to automatically partition composite changes submitted for reviewcode review correctness review time zhang et al.
s java using not using an interactive approach to inspect code changescomprehension level of reviewed change assessed via a questionnaire khandelwal et al.
s python using a gamified non gamified code review tool usefulness of code review comments identified bugs code smells huang et al.
s java using not using a code differencing tool to simplify the understanding of code changescomprehension level of reviewed change assessed via a questionnaire huang et al.
p s java providing not providing reviewers with information about the silent class in a commit i.e.
the one triggering changes to other modified classes comprehension level of reviewed change assessed via a questionnaire hanam et al.
p s javascript using not using a tool performing change impact analysisidentified bugs review time spadini et al.
p s java showing not showing already existing review comments to a reviewer starting their code inspectionidentified bugs fregnan et al.
p s java manipulating the inspection order of files impacted by a changeidentified bugs in files appearing in different positions our study p java python three treatments providing not providing reviewers an automatically generated reviewidentified bugs review time reviewer s confidence using semcia participants were able to perform the code review quickly while also identifying more bugs.
spadini et al.
study how showing already existing review comments to a reviewer starting their code inspection influences the number of identified bugs.
this experiment involved participants including practitioners and students.
the authors asked each participant to perform one code review for a total of collected reviews.
the authors show that existing review comments can help in identifying specific types of bugs that otherwise would have been overlooked.
finally fregnan et al.
investigate whether the order in which files submitted for review are presented to the reviewer impacts the number of bugs identified in them.
surprisingly they found that by just changing the file position the odds of identifying bugs in the inspected files can substantially change with those inspected first having higher odds.
the study has been conducted with among practitioners and students.
also in this case each participant contributed to only one treatment with a total of reviews collected.
as highlighted in table vi and previously mentioned the novelty of our work as compared to the existing literature lies in the investigated independent variable focusing on the novel generation of dl based code review automation tools.
vi.
c onclusion and future work in this study we explored the effects of incorporating automatically generated code reviews particularly those produced by chatgpt plus gpt into the code review process.
our controlled experiment involved professional developers who reviewed code in three settings manually having an automated review provided by chatgpt plus as a starting point or starting from a review that was manually crafted by the authors to capture all major issues in the code but rephrased by chatgpt plus to seem generated by it.reviewers generally accepted the validity of issues identified by the llm adopting of them on average in their final reviews.
however the presence of an automated review influenced reviewers to concentrate on the highlighted code locations potentially overlooking other areas.
our findings also indicate that using automated chatgpt plus reviews as a starting point leads to less severe issues being identified compared to manual reviews but unveils more trivial issues.
additionally while automated reviews are expected to save time our findings show that in practice reviewers still had to spend time verifying the accuracy of the automated comments negating any potential time savings.
at the same time having access to automated reviews as a starting point did not lead to changes in the confidence of reviewers.
these findings suggest that while current llms can play a valuable role in identifying part of the code issues their use as automated co reviewers does not necessarily improve the efficiency or effectiveness of the code review process from a holistic perspective.
the tendency of automated reviews to focus reviewer attention on specific areas of the code coupled with their limited impact on identifying high severity issues and inefficiency in reducing review time represent important challenges that should be addressed in future work to allow llms to become integral parts of the code review process.
acknowledgment this project has received funding from the european research council erc under the european union s horizon research and innovation programme grant agreement no.
and from the swiss national science foundation snsf under the project parsed grant agreement no.
.
we are deeply grateful to the participants who took part in the study for their time and effort.