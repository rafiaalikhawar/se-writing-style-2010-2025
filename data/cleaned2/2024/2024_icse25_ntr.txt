template guided program repair in the era of large language models kai huang technical university of munich germany huangkevinapr outlook.comjian zhang nanyang technological university singapore jian zhang ntu.edu.sgxiangxin meng beihang university china mengxx buaa.edu.cnyang liu nanyang technological university singapore yangliu ntu.edu.sg abstract recent advancements in automated program repair apr have been significantly driven by the application of large language models llms .
in particular the integration of llms with traditional template based repair methods has demonstrated effective outcomes.
despite this the synergy between the strengths of traditional methods and llms remains underexploited.
this oversight originates from the indiscriminate use of templates and their insufficient coverage.
also using small scale llms within the zero shot learning context proves to be suboptimal.
to alleviate the limitations we propose ntr neural template repair a two stage repair framework including template selection and patch generation both of which are under the fine tuning paradigm.
in the template selection phase we formulate it as a multiclass classification problem and fine tune million level llms for better selecting possible templates.
during the patch generation phase we leverage the chosen templates as probable directions e.g.
mutate conditional expression to guide the finetuning process of llms at the billion level scale for precise patch creation.
moreover we incorporate a unique template to signify the absence of a suitable template and employ a probability based prioritization of templates thereby optimizing patch generation.
this framework not only effectively addresses template mismatch issues but also enables the billion level llms to explore the patch space more efficiently despite the gpu memory constraints.
we evaluate ntr with different foundational models on defects4j v1.
and humaneval java the framework consistently demonstrates significant effectiveness.
when utilizing starcoder as the foundational model for patch generation ntr fixes and bugs in defects4j and humaneval outperforming the best baseline apr tool by and bugs.
with the larger codellama model the fixed bugs rise to and respectively exceeding the baseline by and bugs.
notably the performance stems not only from the foundational models but also benefits greatly from our ntr framework.
specifically ntr s implementation with starcoder and codellama leads to and additional fixes which is beyond what the models achieve on their own.
this emphasizes the success of our new perspective on utilizing templates to unlock the bug fixing potential of llms.
index terms automated program repair large language models fine tuning repair template i. i ntroduction automated program repair apr techniques have been under development for nearly two decades spawning several methodological paths such as search based constraint based template based and learning based apr techniques.
in this field learning based apr techniques have achieved considerable progress as highlighted in recent studies .
corresponding author jian zhang jian zhang ntu.edu.sg .recently the advent of large language models llms presents new opportunities for the apr research.
llms typically undergo an extensive pre training phase which enables them to acquire the rich domain knowledge for a wide range of downstream tasks.
inspired by this researchers have started to explore the capabilities of llms for apr .
unlike earlier learning based apr efforts that rely on traditional neural models recent works have adopted llms as foundational models and achieved greatly improved results.
among the llm based approaches the incorporation of template has shown impressive performance as evident by alpharepair and gamma which can represent the state of the art apr work.
these tools adopt the zero shot learning paradigm which allows the model to directly predict the correct code for areas masked by predefined repair templates in the buggy code.
however existing template based llm solutions for apr still suffer from three primary drawbacks.
indiscriminate template use initiatives like alpharepair and gamma typically do not prioritize among templates during the selection process often employing all available templates indiscriminately.
specifically gamma utilizes an ast based matching method to choose repair templates which risks prioritizing incorrect templates over suitable ones.
this can lead the model to miss opportunities of correct patch synthesis and exacerbate the patch overfitting problem.
inadequate template coverage the effectiveness of template based apr tools is inherently limited by their template scope .
for instance gamma utilizes a set of repair templates derived from tbar for masking code in the patch synthesis process.
if a bug s expected repair behavior falls outside these predefined templates these tools can struggle to generate the correct patch.
limitations of small scale llms under zero shot paradigm current approaches rely on zero shot learning with smaller llms e.g.
codebert 125m while the potential of billion level llms with templates under the finetuning paradigm remains untapped.
in fact previous studies have shown that simply fine tuning and using larger llms can significantly improve repair capabilities .
unfortunately the infilling code mask methods are largely confined to masked language modeling based mlm based llms which restricts their applicability to decoder only models without infilling capabilities such as codellama .therefore in the era of llms it is necessary to reconsider how templates can be more effectively integrated into largescale llms to further enhance their repair capabilities.
we present the following key insights to mitigate the above limitations.
insight inspired by the template based approach in transfer training a model to rank templates has proven to be an effective strategy.
particularly in the context of llms we can develop a template ranking model that leverages the code comprehension strengths of llms presents a promising avenue.
insight previous work has shown that the neural machine translation nmt workflow circumvents the template coverage issue by conceptualizing the repair task as a translation rather than an infilling task.
in this workflow we can design a special template that represents bug fixes beyond traditional template set and adopt the nmt model to learn these complex repair behaviors e.g.
multi hunk bugs .
insight recent studies suggest that applying nmt fine tuning strategies can overcome the limitations associated with the zero shot learning paradigm.
given that llms are not inherently tailored for repair tasks we can impart bugfixing knowledge of templates into llms during fine tuning.
moreover the flexibility of the nmt workflow allows it to adapt to various model architectures without being restricted to mlm based llms .
based on above insights we propose a novel templatebased framework namely neural template repair ntr .
ntr divides the repair task into two phases template selection and patch generation .
in the template selection phase ntr formulates it as the multiclass classification problem and fine tunes lightweight llms as the template selection model to perform template ranking insight .
it takes the buggy method as input and predicts the most appropriate repair templates for repairing it.
in the patch generation phase we adopt the nmt workflow and format the selected templates as guidance.
we select an llm with billions of parameters and fine tune it under the nmt workflow insight .
during fine tuning we introduce symbols to denote the buggy code the template and the fixed code.
this enables llms to synthesize patches in an autoregressive manner naturally following the logic of the selected templates.
to address the template coverage issue we set up a special template othertemplate to capture repair behaviors beyond the scope of existing templates insight .
furthermore when performing inference the prioritization of selected templates allows ntr to iteratively produce a broader array of candidate patches than approaches that do not leverage templates.
this approach allows for better exploration of the patch space guided by repair templates and alleviates the limitations present in previous work.
as an added benefit the selection then generation framework empowers large scale llms to expand the patch space even with small beam sizes.
this adaptation is crucial because setting a large beam size as done in traditional non pre trained model based work becomes impractical due to the billions of parameters in these models and the limitations of gpu memory .
in summary the main contributions of this paper are as follows technique.
we introduce ntr a novel framework that combines the strengths of both templates and large scale llms by guiding llms through a two phased approach of template selection and patch generation.
to the best of our knowledge starcoder 15b or codellama 70b utilized in ntr represents the largest llm applied to fine tuned apr research so far.
our work showcases for the first time a promising path toward integrating these large scale llms with templates.
extensive study.
we performed extensive evaluations on defects4j v1.
and humaneval java.
the results demonstrate that ntr outperforms previous apr tools and further boosts the repair capabilities of llms.
for instance with starcoder as the foundational model ntr successfully repairs and bugs in defects4j and humaneval respectively surpassing the top baseline apr tool chatrepair and llm incoder by and bugs.
similarly utilizing the more substantial codellama model ntr fixes and bugs in defects4j and humaneval respectively outperforming the best baseline by and bugs.
notably ntr s application with starcoder fixed more bugs than the foundational model marking a .
improvement.
similarly its use of codellama led to fixing more bugs achieving a .
improvement over the foundational model.
open science.
to promote open science we have released starcoder 15b and codellama 70b two llms that have been fine tuned with the ntr framework and basic nmt fine tuning strategy.
we encourage future researchers to leverage these foundational models to develop more powerful apr tools.
our source code and data are available at ii.
approach this section will describe how ntr combines templatebased and learning based technical paths to build a new program repair workflow.
as shown in figure the workflow of ntr is divided into two main parts i.e.
model finetuning and model inference which is in line with most of the supervised learning based apr workflows .
in particular ntr has two modules phases on design choice namely template selection and patch generation .
briefly speaking ntr first trains a template selection model to prioritize appropriate repair templates for the buggy code and then trains a patch generation model to perform patch synthesis under the guidance of templates.
the design of this two phase repair framework for ntr is the main focus of this paper which we will elaborate on in the following sections.
a. model fine tuning this section describes how we train the template selection or template ranking model and the patch generation model.
template selection model training let s first review the workflow of template based apr techniques.
typically this approach employs a two phase repair strategy that includes template extraction and patch generation.
earlier template based works did not consider the prioritization of repairfix code returfix code publictype 1method 1 type 2var 1 fix code if var 1.method 2 int 1 bug code publictype 1method 1 type 2var 1 if var 1.method 2 int returntype 3.method 3 var 1 repair template insert cast checkerbug code publictype 1method 1 type 2var 1 if var 1.method 2 int returntype 3.method 3 var 1 repair template mutate conditional expressionbug code publictype 1method 1 type 2var 1 if var 1.method 2 int returntype 3.method 3 var 1 repair template mutate conditional expression 2 3 5 4 bug code publictype 1method 1 type 2var 1 if var 1.method 2 int returntype 3.method 3 var 1 repair template insert cast checkerbug code publictype 1method 1 type 2var 1 if var 1.method 2 int returntype 3.method 3 var 1 repair template mutate conditional expressionbug code publictype 1method 1 type 2var 1 if var 1.method 2 int returntype 3.method 3 var 1 repair template mutate conditional expression fig.
the workflow for neural template repair.
template usage.
they used all repair templates sequentially to generate patches.
however the original order of templates can have a negative impact on the effectiveness of patch generation.
in the worst case such a practice could leave the most possible template at the end of the patch space which would fail to generate the correct patch due to the limited budget of patch validation .
subsequently some works tried to optimize template selection by building probabilistic models or simple mlp models .
these models facilitate the selection of repair templates that are more likely to be correct offering a promising direction.
given that these neural models must learn from scratch using training data their performance could be limited.
meanwhile in the current era llms have shown extraordinary capabilities in code understanding and generation tasks .
hence a practical approach to enhancement involves leveraging the comprehension abilities of llms to aid in the selection of suitable repair templates.
specifically we adopt the nmt fine tuning paradigm to train a specialized template selection model denoted as mtemplate .
we outline the concrete steps as follows.
bfp extraction .
before model training we construct training samples by extracting data in the form of bug fix pairs bfps from a collection of bug fix histories.
here we used the transfer dataset collected by meng et al.
as the training corpus from which we extracted method level bfps.
we chose the transfer dataset because it has been successfully applied to template based apr efforts which could be a robust foundation for deploying ntr s framework.
letddenote a dataset which comprises npairs of bug code xiand fix code yi d xi yi xibug code yifix code i n here we have a specific instance dtransfer for the transfer dataset.
this dataset serve as the basis for extracting templates and training our template selection model.table i repair templates no.
from previous work and special template no.
from ntr settings.
no.
repair templates no.
repair templates insert cast checker mutate class instance creation insert range checker mutate integer division operation insert null pointer checker mutate operators insert missed statement mutate return statement mutate conditional expression mutate variable mutate data type move statement mutate literal expression remove buggy statement mutate method invocation expression other template template extraction .
after obtaining bfps it is necessary to deduce fix templates based on specific bug fixing behaviors that is code changes.
to achieve this we perform template extraction based on state of the art template based apr works tbar transfer and tenure .
specifically on the dtransfer dataset we initially utilized the repair templates as shown in table i more details please see transfer provided by previous works represented as ttransfer and conducted template extraction using the sc4ft tool developed by meng et al.
.
we selected these repair templates because they encompass most of the bug repair behaviors and prior research has employed these templates to achieve advanced results.
however due to the diverse nature of software bugs creating a comprehensive set of templates that covers all possible bugs is unfeasible.
therefore previous methods like gamma relying solely on a predefined set of templates might not address a significant number of complex bugs including multi hunk bugs.
to tackle the challenge during the extraction process we designate other template to label repair behaviors beyond the scope of the repair templates mentioned above.
in addition this special template also contains multi hunk bug fixing behavior.
thus it can be used to mitigate the template coverage problem and allow the model to gain repair capability of multi hunk bugs.
model training tuning .
we denote the training dataas follows xrepresents the set of bug code samples and t represents the set of repair templates.
each training sample expressed as xi ti consists of a bug code xiand its corresponding repair template ti.
the core objective of the template selection model referred to as mtemplate is to predict the appropriate repair template tibased on a given bug code xi.
this objective can be expressed as mtemplate xi ti.
the fine tuning objective aims to minimize the cross entropy lossltemplate for the template selection model mtemplate optimizing the model s parameters by reducing the discrepancy between the predicted templates tiand the actual templates ti for all bug code snippets xi.
this is formulated as min ltemplate x ix jtijlog tij where tijdenotes the model s predicted probability that label jis the correct classification for bug code xi and tijis a binary indicator of whether label jis the correct template for the given instance.
during the training of the template selection model we use small scale llms such as codet5 as the foundation model.
the rationale for the choice of million level codet5 that has an encoder instead of a billion level llm like decoder only starcoder is as follows.
template selection fundamentally a task of program understanding involves predicting the appropriate repair template for given buggy code.
findings from a prior study indicate that gpt like models decoderonly including gpt c and codegpt underperform in classification tasks such as this.
conversely encoderdecoder models like codet5 have demonstrated superior performance in classification challenges according to the same study .
also our preliminary experiments confirmed that starcoder performs poorly on this type of task.
therefore we use codet5 for demonstration purposes in this work.
the template selection task can be viewed as a multi class classification problem where each template serves as a class.
given the buggy code xiand the model mcodet5 the vector representation of the selection model be described as follows ri mcodet5 xi where is the prefix in front of the input and the corresponding hidden state can represent the semantics of the whole buggy function based on the attention mechanism.
a softmax function is then applied to these logits to derive a probability distribution over all repair templates p t xi softmax ri .
here p t xi denotes the predicted probability distribution over repair templates tfor the given buggy code xi.
the repair template with the highest probability is selected as the prediction ti arg max t p t xi .
it is true that a model can not always perfectly predict the most appropriate template.
however our model can also be used for template prioritization based on probabilities allowing the inclusion of correct patch at the patch generation phase.
patch generation model training when generating patches existing template based llm approaches including alpharepair and gamma transform fix templates commit before int getval int array int len int index bug start if index len bug end return array else return index commit after fix start if index len index fix end bug hunk fix hunk commit msg mutate conditional expression output inputfig.
the input and output of the patch generation model.
into masked donor code and directly use million level llms to fill the masks with zero shot learning.
this practice could suffer from the limitations of the models small scale and misalignment between the code generation and program repair.
typically the nmt fine tuning paradigm of llms can address the problems.
as a representative built on traditional neural model tenure follows the nmt workflow while considering templates.
different from the above work tenure jointly generates a template along with the fixed code during training and inference.
theoretically the template generation can be largely affected by the joint cross entropy since the fixed code is generally much longer than a template see table i .
this one step strategy potentially hampers the model s ability to accurately select the most appropriate template let alone effectively prioritize among templates thereby diminishing the benefits of template based guidance.
we will further analyze this problem in the ablation study of section iv b. therefore ntr implements a two phase repair strategy that distinctively coordinates template selection and patch generation by optimizing them independently.
in the patch generation phase we specifically incorporate the chosen repair templates alongside the buggy code as inputs while designating the fixed code solely as the output.
this dual input method makes the patch synthesis process aligns more closely with the guidance offered by the selected templates.
figure illustrates the path generation process.
in the fine tuning phase we format the buggy code the template and the fixed code as commit before bug method commitmsg repair template commit after fixcode .
this approach is inspired by the pre training methodologies of existing llms such as starcoder which learn the commit behaviors from open source projects in this manner.
since bug fixing often involves commits in practice the format is naturally suited for fine tuning as it closely matches the structure used during pre training.
model training tuning .
based on both repair templates and the buggy code the data dextracted from step bfp template extraction are restructured to align with therequirements of the patch generation task.
let x d represent the set of buggy code samples tdenote the set of repair templates and y d signify the set of fix code samples.
each training sample denoted as xi ti yi consists of a buggy code xi its corresponding repair template ti.
the objective of the patch generation model referred to as mpatch is to autoregressively synthesize the appropriate fix code yigiven xiandti.
this can be represented as yi mpatch xi ti .
during fine tuning the objective is to optimize the model s parameters by maximizing the log likelihood of the autoregressive generation process across all training samples.
this involves calculating the likelihood of each token yij in the predicted fix code yi based on the actual fix code yi up to the current token alongside the bug code xi and the selected repair template ti.
the aggregated log likelihood function representing the sum of log probabilities for correctly predicting each token in the sequence is given by min l x ix jlog p yij yi j xi ti here p yij yi j xi ti denotes the model s predicted probability of token yijgiven the preceding tokens yi j in the actual fix code under the condition of the bug code xiand the repair template ti.
the goal of fine tuning is to maximize this function thereby enhancing the model s ability to accurately predict the next token in the sequence.
we choose starcoder 15b codellama 70b as the foundation model for fine tuning.
there are two main reasons for this decision.
fundamentally program repair is inherently a code generation task and based on prior studies larger llms tend to exhibit more powerful repair capabilities.
in fact it is revealed that llms show a clear pattern called the emergent capability .
that is the performance is nearrandom until a certain critical threshold of scale is reached e.g.
10b after which performance increases to substantially above random.
hence we select billion level llms for the patch generation task.
this enable it to better deal with the unseen and complex bugs in real world projects.
through template guided training ntr s patch generation model acquires repair behaviors aligned with the diverse directions outlined in the templates.
importantly this training approach enables conditional generation allowing the largescale llms to extend beyond the confines of explicitly specified templates.
this is particularly true for the special template other template which broadens the llms ability to learn repair behaviors beyond those predefined templates.
b. model inference after the fine tuning process is complete we obtain a template selection model and a patch generation model.
it is worth noting that during inference traditional apr methods can generate hundreds of patches constituting a vast patch space by setting a large beam size in the beam search algorithm .
theoretically we could also adopt a large beam size for our generation model relying solely on the predicted template from the selection model to maintain competitiveness.
however this strategy encounters two significant challenges.
firstly the accuracy of the predicted template is not guaranteed which could mislead the generation model into producing invalid patches.
secondly the constraint of gpu memory poses a substantial hurdle especially for large scale llms such as codellama 70b potentially leading to out of memory oom errors and thus a constrained patch space.
these considerations drive ntr to adopt a dual model approach implementing template prioritization and iterative patch generation during the inference phase to effectively navigate these challenges.
template prioritization to mitigate the risk of inaccurate template selection we implement template prioritization ordering the templates according to the model s output probabilities.
the inputs to the template selection model include the buggy code x and the candidate repair templates t which are derived from the model s output.
the model is adapted to compute the probabilities for each repair template as p tj x mtemplate x tj .
in this expression p tj x denotes the likelihood of choosing repair template tjbased on the buggy code x. this phase ends up with a sorted candidate template space as tsorted.
this space is derived by ordering the templates according to their predicted probabilities from the template selection model tsorted sort p tj x tj t desc here sort desc represents the sorting operation in descending order of the probabilities p tj x which are the chances of selecting each repair template tjgiven the buggy code x. the output tsorted is subsequently fed into the patch generation model in the next step.
iterative patch generation given the sorted template spacetsorted we iteratively concatenate each repair template tij from this space with the buggy code xito form model inputs.
these concatenations are then fed into the patch generation model mpatch following the nmt paradigm to predict the corresponding repair patches.
this strategy of iteratively combining different repair templates tijwith the bug code xican effectively explore the patch space.
the underlying idea is that multiple solutions may exist for fixing a bug.
therefore ntr can synthesize correct patches using different repair templates leveraging the inherent advantage of template based apr techniques .
moreover the patch generation model s flexibility with the nmt workflow enables it to overcome template coverage limitations potentially synthesizing the correct patch even when repair template prediction fails or corresponding repair templates are lacking which combines the strengths of learning based apr techniques .
for each of the krepair templates tijwhere j k tsorted our goal is to generate mpatches represented as y1 ij y2 ij .
.
.
ym ij mpatch xi tij here y1 ij y2 ij .
.
.
ym ijdenote the mgenerated patches for the input pair xi tij using the patch generation model mpatch.
by employing the subset of sorted repair templates kand adjusting the number of patches mgenerated per template we can also maximize the patch space even with the constraintof gpu memory.
for example we might be restricted to generating only candidate patches at a time due to potential oom.
however by associating repair templates with a single bug code we can iteratively generate diverse groups of the candidate patches expand the candidate patch space to include potential patches.
in summary this method not only mitigates the computational constraints but also leverages the model s capacity to explore a broader range of repair possibilities substantially increasing the chances of generating effective patches.
iii.
e xperiment setup a. research questions rq1 how well does ntr fix common bugs?
we will explore ntr s repair capabilities in java bug repair scenarios and compare with baselines.
repair effectiveness rq2 how much does ntr s design choice contribute to the overall repair capability?
ntr uses a two stage repair strategy and here we will explore the impact of different repair strategy settings on the results.
ablation study rq3 how good is ntr at fixing security vulnerabilities?
to further evaluate ntr s generalization ability we conducted experiments in vulnerability fixing scenarios.
generalizability study b. dataset training dataset transfer dataset .
in the main experiment we use the transfer dataset to implement model training and tuning.
it contains about one million bug fix pairs and corresponding fix templates that have been used to drive transfer and tenure .
for example the template based apr work tenure selected about 570k of these data for training.
considering the huge cost of training llms we randomly selected 100k samples from them for ntr.
the size of each set is as follows train val test .
recoder dataset .
in the generalizability study we use the recoder dataset to implement model fine tuning.
we chose the recoder dataset because wu et al.
have successfully applied the recoder dataset to vulnerability repair tasks with effective results.
in addition their work also indicates that some vulnerabilities still share similar repair patterns with general bugs.
the size of recoder dataset is as follows train val test .
vulgen dataset .
in the generalizability study we also use the latest vulgen dataset to implement model training and tuning.
we chose the vulgen dataset because it is the most comprehensive collection of c c vulnerability repair datasets currently available.
in particular vulgen has successfully extracted templates from the dataset for vulnerability generation.
the size of vulgen dataset is as follows train val test .
testing benchmark defects4j v1.
.
defects4j v1.
contains java bugs and is one of the most popular testing benchmarks .
since the training set transfer dataset contains multi hunk fix examples ntr can get the multi hunk repair capability and we use both single and multi hunk samples from defects4j v1.
in our evaluation.
humaneval java .
humaneval java contains single hunk java bugs and has no risk of data leakage.
here we use all samples in humaneval java.
vul4j .
vul4j contains java vulnerabilities and has been applied to llm4apr .
here we followed the baseline work by selecting single hunk samples.
cbrepair .
cbrepair contains c c vulnerabilities and has been successfully applied to vulnerability repair tasks .
here we selected single hunk samples.
c. baselines apr baselines .
we collected recent apr works to serve as baselines.
this includes chatrepair fitrepair gamma tenure tare repatt alpharepair rap gen knod recodert tbar jiang et al.
.
following the common practice in the apr community we reuse the reported results from previous studies instead of directly running the apr tools.
llm baselines .
to present more clearly the improvement of ntr s strategy for llm s repair capability we implement nmt fine tuning of llms to provide additional baselines which we call llm starcoder codellama .
since ntr employs multiple candidate templates to guide patch generation it can obtain a larger patch space with a small beam size.
in order to more fairly compare the performance of ntr against llms we borrow from previous work by sampling the model multiple times with distinct sets of temperature parameters i.e.
llm .
this comparison is fair because ntr utilizing candidate templates generates an equivalent patch space to that of llm through rounds of sampling.
d. implementation template selection as previously mentioned codet5 was selected for ntr s template selection model due to its superior performance in classification tasks among millionlevel models .
specifically the template selection model underwent full parameter fine tuning with codet5 220m .
following the insights from previous work we conducted training for epochs and subsequently chose the checkpoint with the best perplexity.
this approach addresses the limitation that a single epoch of fine tuning is insufficient for millionlevel llms preventing them from achieving convergence and adequately learning repair templates.
regarding the training hyper parameters we configured the learning rate to 5e and set the maximum input output sequence length to .table ii different implementations of ntr.
implementationmodel components template selection model patch generation model ntr cs codet5 220m starcoderbase 15b ntr cl codet5 220m codellama 70b patch generation considering the pivotal role of patch generation in ntr which demands advanced natural and programming language understanding for effective patch synthesis we selected two of the foremost llms for code families currently available codellama and starcoder as identified on the big code models leaderboard .
these models offer the robust capabilities necessary for guiding the synthesis process.
the patch generation model for ntr was parameter efficiently fine tuned using starcoderbase 15b codellama 70b we implemented the fine tuning using the lora and bit quantization also called qlora techniques to reduce memory and computational costs and trained only one epoch based on the findings from previous work .
this is because for large scale llms one epoch can already make the model converge and more epochs will destroy the model s generalization ability thereby adversely affecting its ability to effectively perform repairs.
in the training hyper parameter settings we set the learning rate to 5e and the maximum input output length to in the model inference phase we set the beam size to or to generate or candidate patches for each template and used top10 templates to expand the patch size or .
the chosen size of the patch space is deemed suitable for apr tasks as supported by previous studies that typically involve hundreds to thousands of candidate patches.
note that due to gpu memory constraints the maximum beam size we can set for starcoder and codellama is and respectively.
fault localization and patch validation in order to avoid additional biases introduced by the fault localization tool our experiments were conducted under conditions of perfect fault localization.
as noted by previous works this is the preferred evaluation setting as it eliminates any differences in performance caused by running fault localization tools.
prior to patch validation duplicate patches within the patch space are removed.
the validation process for ntr aligns with established apr methodologies as documented in prior works .
specifically for each bug version we allocate a maximum of hours for the validation run retaining only the first top plausible patch candidate that successfully passes all test cases.
should a candidate meet these criteria the validation for that bug ceases immediately.
finally the plausible patches will be manually checked to verify whether they are semantically correct or not.
we summarize the different implementations of ntr in table ii.
the above is implemented based on python .
and pytorch .
and all models are from huggingface.
we conduct the main experiments on a core workstation withintel r xeon r bronze cpu gb ram and a tesla a100 gpu running ubuntu .
.
lts.
iv.
e xperiment result a. rq1 repair effectiveness setup in the main experiment we aim to explore the effectiveness of ntr in common bug fixing scenarios.
here we select the transfer dataset for model fine tuning and defects4j v1.
humaneval java for testing.
note that when testing the repair capabilities of the ntr implementation on defects4j v1.
we set the maximum beam size to for the starcoderbased ntr implementation ntr cs and for the codellamabased ntr implementation ntr cl .
this is reasonable because we use the beam size is smaller than or equal to most baseline works .
in addition when testing the repair capability of the ntr implementations on humaneval all ntr implementations set the beam size to which is consistent with the compared baseline work .
the default setting for ntr is to use top candidate templates generating at most candidate patches when the beam size is set to and at most candidate patches when the beam size is set to .
result table iii and table iv present the repair results of the specific implementation of ntr on defects4j v1.
and humaneval java.
we will now proceed to analyze the repair results as outlined below.
defects4j v1.
.
as shown in table iii the implementations of ntr outperform the current best baseline work the chatgpt based apr tool chatrepair .
specifically the starcoder based ntr csfixes more bugs vs. than chatrepair and the codellama based ntr clfixed more bugs vs. than chatrepair.
importantly the results show a clear enhancement of the ntr strategy on the llms repair capability when comparing the difference in the repair capability of the llm based ntr implementation over the llm baselines.
specifically the starcoder based ntr csfixes more bugs vs. than starcoder achieving a .
improvement the codellama based ntr clfixes more bugs vs. than codellama achieving a .
improvement.
humaneval java .
as shown in table iv the implementations of ntr outperform all the baselines from the llm4apr study .
specifically the starcoder based ntr csfixes more bugs vs. than incoder 6b the codellamabased ntr clfixed more bugs vs. than incoder6b.
however due to the relatively small size of incoder comparing it directly to ntr does not yield meaningful insights.
therefore our analysis primarily concentrates on evaluating the improvements offered by the llm based ntr implementation over the foundational model.
specifically the starcoder based ntr csfixes more bugs vs. than starcoder achieving a .
improvement the codellama based ntr clfixes more bugs vs. than codellamax10 achieving a .
improvement.
multi hunk fixes .
we draw on previous work to mark out repair behaviors at multiple bug locations which allowstable iii repair results for ntr and baselines on defects4j v1.
under the perfect fault localization.
apr tool ntr cl codellama ntr cs starcoder chatrepair fitrepair gamma tenure tare repatt alpharepair rap gen knod recoder t tbar beam size patch size chart closure lang math mockito time total corr.
plaus.
table iv repair results for ntr and baselines on humaneval java under the perfect fault localization.
apr tool ntr cl codellama ntr cs starcoder incoder 6b incoder 1b codegen 6b codegen 2b codet5 large cure rewardrepair recoder knod beam size patch size total corr.
plaus.
table v multi hunk bug repair on defects4j v1.
.
multi hunk bugshercules iter ntr cs ntr clmulti hunk bugshercules iter ntr cs ntr clbug id edits bug id edits chart math chart math closure math closure math closure math closure math closure math closure math closure math lang math lang math lang math lang math lang math lang math lang math math total bugs ntr to learn and gain multi hunk repair capabilities.
as shown in table v we extract the repair results of iter and hercules that can repair multi hunk bugs and compare them with the specific implementation of ntr.
the results show that ntr successfully repairs or out of multihunk bugs which rivals the repair results of the state of the art multi hunk repair tool iter.
note that iter and hercules do not use perfect fault localization.
therefore we wouldn t say that ntr is superior in repairing multi hunk bugs.
the motivation for conducting experiments on multi hunk bugs is that existing template based approaches e.g.
alpharepair and gamma are often limited to single hunk bugs due to the use of templates.
thus we aim to demonstrate that our templateguided approach ntr has multi hunk repair potential as well.
in summary ntr achieves effective results on both defects4j v1.
and humaneval java which is better than many current llms as well as learning based and traditional apr tools.
and ntr also shows potential for multi hunk fixes.
in particular ntr further enhances the repair capability of the llm.
ntr csrepairs more bugs than the foundation model achieving a .
improvement ntr clrepairs more bugs than the foundation model achieving a .
improvement.
these results indicate that ntr s strategy is successful which is a feasible path to continuously optimize llm4apr research.
b. rq2 ablation study setup in the ablation experiment we aim to explore the impact of ntr s different design choices on the repair results.
therefore we designed several variants to reveal the effects of different fine tuning strategies.
specifically we designed six fine tuning strategies basic fine tuning strategy wetable vi repair results of different fine tuning strategies on defects4j v1.
.
variants repair strategy beam size patch size result starcoder basic fine tuning starcoder t one stage template based fine tuning starcoder basic fine tuning with multiple sampling ntr ns ntr without template selection ntr cs ntr without the special template ntr cs ntr table vii repair results of repair strategies and based llm implementation on defects4j v1.
.
llms starcoder 15b codellama 13b codegen25 7b incoder 6b codegeex2 6b beam size strategy strategy used the nmt fine tuning strategy from the recent llm4apr studies to present the repair ability of directly fine tuning llms.
one stage template based fine tuning strategy the closest to our work is the recent tenure which uses a one stage strategy to simultaneously predict templates and patches while ntr employs a two stage strategy to optimize template selection and patch generation tasks separately.
here we implement tenure s strategy on top of llms to evaluate its effectiveness against ours.
basic fine tuning strategy with multiple sampling as previously discussed to achieve a fair comparison with ntr we enhance the basic fine tuning model s patch space by sampling it multiple times.
ntr strategy without template selection to clarify the template selection component s role in enhancing the repair results we remove the template selection model from ntr and follow the practice of previous work to indiscriminately apply repair templates one by one.
ntr strategy without the special template to show the importance of setting the special template we remove othertemplate from ntr and compare its performance with the complete ntr strategy.
ntr strategy for reference we include the original outcomes achieved by the two stage ntr strategy.
due to the computational costs associated with fine tuning llms and the need for a diverse set of test samples we select starcoder as the foundational model and defects4j v1.
as the testing benchmark for our ablation study.case token.mod if rval error diagnostictype.error jsc divide by 0 error divide by right return null result lval rval break case token.div if rval error diagnostictype.error jsc divide by 0 error divide by right return null closure 78fig.
developer patch for closure .
table viii average correct template rank on defects4j v1.
.
project chart closure lang math mockito time average ntr ns .
.
.
.
.
.
.
ntr cs .
.
.
.
.
.
.
improvement .
.
.
.
.
.
.
result table vi presents the repair results of llm variants on defects4j v1.
using different fine tuning strategies.
next we will analyze the impact of detailed design choices.
the limitations of one stage template based repair strategy.
by comparing the repair results of repair strategies and starcoder vs. starcoder t we can conclude whether tenure s one stage strategy is beneficial for basic nmt fine tuning.
to our surprise the results in table vi show that starcoder t is even worse than starcoder with fewer bugs fixed.
this result is confusing because tenure performs well on the non pre trained model rnn of its original paper yet it performs even worse on the llm.
to further confirm this finding we select top foundation model families on the big code models leaderboard for additional experiments.
according to table vii tenure s one stage strategy still performs worse than the basic finetuning.
in analyzing the underlying causes we identified a significant issue the one stage strategy tends to repeatedly predict the same templates during the simultaneous prediction of templates and patches.
this redundancy limits the diversity of template types considered increasing the likelihood of overlooking the correct template.
to illustrate we present an example where the bug is successfully fixed by starcoder but not by starcoder t. as shown in figure the repair behavior of closure removes buggy statements and thus its correct repair template should be removebuggystatement .
however we found that the starcoder t s predicted repair template is always othertemplate which makes it miss the chance to synthesize the correct patch.
overall tenure s strategy proves suboptimal for predicting repair templates mainly due to the joint cross entropy that tends to favor the longer fixed code over the template as detailed in section ii a2.
this demonstrates that using an independent template selection template ranking step is a more reasonable strategy.
the benefits of two stage template guided strategy .
by comparing the repair results of repair strategies starcoder vs. ntr csand ntr nsvs.
ntr cs we can assess the contribution of ntr s two stage strategy in expanding the patch space as well as improving the repair effectiveness.
ntr csvs.
starcoder limited by gputable ix average correct patch rank on defects4j v1.
.
project chart closure lang math mockito time average ntr ns .
.
.
.
.
.
.
ntr cs .
.
.
.
.
.
.
improvement .
.
.
.
.
.
.
public int parsearguments parameters params throws cmdlineexception string param null try string param params.getparameter param params.getparameter catch cmdlineexception e if param null setter.addvalue true closure fig.
developer patch for closure .
public int parsearguments parameters params throws cmdlineexception string param null try string param params.getparameter param params.getparameter catch exception e if param null setter.addvalue true closure ntr fig.
ntr cspatch for closure .
memory we set the maximum beam size to which means at most patches are generated at a time.
indeed how to extend the patch space for enhancing the repair capability of llms is a challenge that needs to be solved in the era of llms .
ntr is designed to extend the patch space by using multiple repair templates starcoder extends the patch space by sampling multiple times.
as shown in table vi ntr csfixes more bugs than starcoder which indicates that ntr s patch space extension strategy is more effective.
ntr motivates the llm to explore towards different repair behaviors by combining different repair templates which is more effective than blindly exploring the patch space by multiple sampling.
ntr csvs.
ntr ns some template based apr works simply use all templates in a fixed order ignoring the importance of template prioritization.
conversely ntr performs template ranking by training a template selection model.
as shown in table vi ntr csfixes more bugs than ntr ns.
besides ntr cssuffers from a milder degree of patch overfitting than ntr ns.
specifically we calculated the degree of overfitting using the number of overfitting patches divided by the number of plausible patches ntc csvs.
ntr ns vs. .
vs. .
.
we hypothesize that this improvement results from the template selection model effectively elevating the rank of the correct repair template within the candidate template space.
this in turn indirectly boosts the rank of the correct patch within the candidate patch space.
as shown in table viii ntr cswith the template selection model achieves a .
reduction in the average ranking of correct templates.
in addition table ix shows that ntr csalso gets a .
reduction in the average ranking of correct patches compared to ntr ns.
overall these results lend support to our hypothesis indicating that the template selection component within ntr plays a crucial role in reducing patch overfitting and improving the repair effectiveness.table x repair results for ntr on vul4j and cbrepair.
apr tool ntr cs starcoder vulmaster codex 12b vulrepair beam size vul4j .
cbrepair the importance of the special repair template .
by comparing the repair results of repair strategies and ntr csvs.
ntr cs we can clarify the importance of setting the special template othertemplate .
as shown in table vi ntr cswithout the special templates fixes fewer bugs than ntr cswith the special templates which suggests that the setting of the special templates is important to the ntr strategy.
recall that the introduction of the othertemplate in ntr aims to address the template coverage issue facilitating the learning and generation of more complex repair behaviors.
we present an example where the ntr strategy aided by the special template successfully fixed a bug.
as shown in figure the correct repair behavior of closure involves complex changes in replacement and insertion.
consequently approaches with template coverage issues like gamma and alpharepair those focusing on single line repairs like tenure and ntr variants without the special template ntr cs all failed to fix it.
in contrast figure shows that ntr cseffectively utilizes the special template to synthesize the correct patches.
this evidence further supports that ntr cs mitigates the template coverage problem through the special template enabling the synthesis of patches for complex repair behaviors.
c. rq3 generalizability study setup the strategy employed by ntr is theoretically general.
to validate its generalizability we conduct experiments on vulnerability repair.
specifically we use starcoder as the foundation model recoder and vulgen datasets as the training dataset and vul4j and cbrepair as the test benchmarks.
since sc4ft is java specific we manually labeled templates for c c samples in vulgen to accommodate language differences.
we extracted the best repair results from the recent vulnerability repair work and re implemented vulrepair in our dataset for comparison.
we set the same beam size of as in recent work and considering that vulrepair uses a small scale model we set the beam size to for it.
the purpose of this experiment is to explore whether ntr can work effectively in vulnerability repair scenarios.
result table x presents the repair results of ntr on vul4j and cbrepair.
in total ntr repaired more vulnerabilities than the foundation model starcoder achieving a .
improvement.
this result again demonstrates the effectiveness of the ntr strategy.
in addition ntr fixed more samples than the sota vulnerability repair work vulmaster on vul4j achieving a .
improvement.
this shows that the llm based implementation of ntr can generalize well to other similar tasks.v.
t hreats to validity data leakage the pre training data of llms may contain correct patches for buggy projects which could affect the evaluation.
we mitigate this threat from three ways first the llm we used starcoder provides a page that can detect data leakage which helps us detect and understand how our experimental results suffer from data leakage.
specifically we detected the correct patches generated by ntr cson defects4j v1.
in our main experiment and found that closure lang math of them were threatened by data leakage.
even removing these bugs ntr csstill repairs bugs which is still ahead of the best baseline chatrepair vs. .
second our main experiment was also conducted on humaneval java a synthetic benchmark designed to prevent data leakage.
third since the ntr strategy is designed to further improve the repair capability of llms we have compared ntr with the foundation model.
regardless of potential data leakage in the llm one strategy can be deemed successful as long as it demonstrates enhanced repair results.
as shown in table iii table iv and table x the results from ntr consistently outperform the foundational model and thus the threats of data leakage is minimized.
repair efficiency the substantial size of llms implies more time and memory costs for training and inference.
we recognize this as a potential impact on repair efficiency which we intend to analyze through two ways time cost .
during the model fine tuning phase the starcoder based ntr implementation ntr cs took hours and the codellamabased ntr implementation ntr cl took hours.
fortunately model fine tuning is a one time cost and the fine tuned model can be used directly in the repair workflow.
in the model inference phase it took an average of .61s for ntr csto generate a patch and .76s for ntr cl.
it is undeniable that using a larger scale codellama costs more time while bringing better repair capabilities.
in the future we expect more model acceleration techniques to mitigate this problem.
memory cost .
we use qlora to save memory without sacrificing performance.
specifically we use bit quantization and lora for starcoder so that only about 20g of memory is needed to load the model we use bit quantization and lora for the larger codellama which allows us to load the model with 40g of memory.
such memory costs allow researchers to deploy these llms on consumer grade gpus e.g.
two rtx 24g .
also ntr iteratively guides patch synthesis which expands the patch space under memory constraints.
overall we mitigate the memory cost threat using the qlora technique and ntr s design choices.
vi.
r elated work apr research has entered the era of llms.
researchers are devoted to designing novel repair strategies to maximize the repair potential of llm.
based on the methodologies of patch generation we categorize llm based apr approaches into mask prediction sequence transformation and other methods.mask prediction based i.e.
infilling works typically draw on specifically designed repair templates to guide the llm to fill in masked locations by constructing masking prompts based on these templates.
this approach uses pre training tasks e.g.
mlm for the llms to generate patches directly.
the earliest work was alpharepair which introduced the zeroshot learning paradigm to the apr task yielding surprisingly effective results.
subsequent works such as fitrepair and repilot also use this paradigm.
similarly gamma leverages templates derived from prior template based studies and applies the zero shot learning paradigm.
another similar work is typefix which automatically mines templates from the repair history and focuses only on python type errors.
however most advanced llms such as codellama 70b are not designed for cloze tasks making it difficult to directly leverage them for mask prediction.
sequence transformation based i.e nmt works typically tag the fault location and simultaneously feed the buggy code along with its context into the model.
this enables the model to generate the appropriate transformations patches .
for example vulrepair inferfix and recent empirical studies have adopted this approach.
the advantage of this approach is that the model is able to generate patches based on the defective code and its context and has a broad applicability.
however merely fine tuning llms might not represent the most effective strategy for automated program repair.
compared to the above work our approach effectively incorporates prior domain knowledge i.e.
templates into the fine tuning process thereby enhancing its overall effectiveness.
additionally llms with language comprehension and generation capabilities can also be utilized to automatically generate fixes in an interactive way.
for instance chatrepair interleaves patch generation with instant feedback to perform apr in a conversational style.
more recently the focus has shifted toward developing agents that enable llms to emulate the human debugging process to address real world bugs exemplified by repairagent fixagent and autocoderover .
these approaches enhance automated repair systems by incorporating interactive tools and iterative refinement processes.
in the future building frameworks for multi agent collaboration to tackle real world programming challenges appears to be a promising direction.
vii.
c onclusion this paper presents ntr a fully llm powered templateguided repair framework that enriches the template based apr method by fine tuning llms for enhanced template selection and patch generation.
this two phrase strategy effectively empowers llms to synthesize accurate patches under the direction of repair templates which further unlocks their generative capabilities.
furthermore ntr s patch generation is not confined by template coverage limitations.
through experiments ntr achieves sota results demonstrating the effectiveness of our approach.acknowledgement we thank the reviewers for their insightful comments and suggestions.
this research is supported by the national research foundation singapore and the cyber security agency under its national cybersecurity r d programme ncrp25 p04taicen .
any opinions findings and conclusions or recommendations expressed in this material are those of the author s and do not reflect the views of national research foundation singapore and cyber security agency of singapore.