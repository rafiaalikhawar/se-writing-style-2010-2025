testing and understanding deviation behaviors in fhe hardened machine learning models yiteng peng daoyuan wu zhibo liu dongwei xiao zhenlan ji juergen rahmel and shuai wang the hong kong university of science and technology hong kong china hsbc hong kong china ypengbp daoyuan zliudc dxiaoad zjiae shuaiw cse.ust.hk juergen.rahmel hsbc.com.hk abstract fully homomorphic encryption fhe is a promising cryptographic primitive that enables secure computation over encrypted data.
a primary use of fhe is to support privacypreserving machine learning ml on public cloud infrastructures.
despite the rapid development of fhe based ml or he ml the community lacks a systematic understanding of their robustness.
in this paper we aim to systematically test and understand the deviation behaviors of he ml models where the same input causes deviant outputs between fhe hardened models and their plaintext versions leading to completely incorrect model predictions.
to effectively uncover deviation triggering inputs under the constraints of expensive fhe computations we design a novel differential testing tool called hed iff which leverages the margin metric on the plaintext model as guidance to drive targeted testing on fhe models.
for the identified deviation inputs we further analyze them to determine whether they exhibit general noise patterns that are transferable.
we evaluate hed iff using three popular he ml frameworks covering different combinations of models and datasets.
hed iff successfully detected hundreds of deviation inputs across almost every tested fhe framework and model.
we also quantitatively show that the identified deviation inputs are visually meaningful in comparison to regular inputs.
further schematic analysis reveals the root cause of these deviant inputs and allows us to generalize their noise patterns for more directed testing.
our work sheds light on enabling robust he ml for real world usage.
i. i ntroduction in recent years there has been rapid growth in fully homomorphic encryption fhe algorithms .
as one of the most exciting cryptographic breakthroughs fhe enables privacy preserving machine learning that allows normal users to encrypt their data locally and then directly upload the ciphertext data to a remote server for processing.
the server performs computation on the encrypted data and returns the encrypted results to the users.
thus users can protect their confidential data without compromising the convenience of machine learning as a service mlaas .
commercial vendors like intel and ibm are actively developing and promoting their he ml frameworks greatly spurring real world ml applications with privacy considerations .
despite the prosperous development of he ml there is still a lack of systematic understanding of the quality of he ml applications.
in general the design of fhe primitives is often subtle and complex .
even worse fhe protocols are well known for their slow speed and we find that modern corresponding authors.he ml frameworks often aim to extensively support complex dnn operators tensor computations and imperative programming .
hence various optimization and approximation schemes are involved in converting standard floatingpoint computations into fhe supported forms.
while these efforts substantially reduce the hurdle of employing fhe primitives in mlaas applications they may increase the burden on developers to deliver bug free he ml applications.
in fact our preliminary studies in iii have shown that he ml models may yield specious outputs under certain inputs.
in this paper we aim to systematically test and understand the deviation behaviors in fhe hardened ml models.
we target deviations where the same input causes deviant outputs between fhe models and their plaintext versions leading to completely incorrect model predictions.
inspired by the widely used differential testing dt technique in the software engineering se community which aims to find bugs in similar software a straightforward approach to finding deviation behaviors is to conduct dt directly.
this technique will systematically explore the input space and detect inputs that lead to differing outputs between the fhe and plaintext models.
however directly using dt is expensive as a prediction in fhe model often requires several orders of magnitude more computation than in the plaintext model e.g.
half a minute vs. less than .
millisecond .
to deliver an efficient dt specific to fhe models this paper first conducts a preliminary study to identify a proper metric which serves as the feedback to drive guided test input selection and mutation.
through empirical analysis we eventually choose margin defined as the difference between the largest and the second largest prediction values in the model s inference result as our metric.
we present details of this preliminary study in iii.
based on the insights above we design a novel differential testing tool called hed iff which leverages the margin metric on the plaintext model as guidance to drive targeted testing on fhe hardened models.
specifically hed ifffirst conducts margin based input filtering to identify only those promising ones as candidate seeds for mutation.
then hed iffadds a small amount of noise to each seed and mutates them in ways that could potentially cause deviant behaviors.
inspired by the success of the projected gradient descent pgd algorithm in finding adversarial examples aes we adapt pgd to mutate seed inputs in a way that reduces the margin value for our scenario of finding deviation behaviors.
furthermore 1with detected deviation inputs on hand we present a schematic analysis to understand the root cause of deviant behaviors in fhe models and also generalize the noise patterns of these deviation inputs.
the generalized patterns can be treated like weak universal adversarial perturbations uaps enabling further directed identification of deviation inputs.
this can be analogous to exploiting the correlations between similar codes to find new defects by utilizing patterns of previously discovered defects in traditional software.
our evaluation encompasses three mainstream he ml frameworks including tenseal concrete ml and helayers .
these frameworks are developed and maintained by industrial vendors such as openmined ibm and zama.
we launch testing towards similar neural network architecture to prominent fhe friendly ml models and multilayer perceptron mlp trained for general and domain specific tasks with different activation functions.
with real world datasets including bank credits mnist and digits we simulate scenarios where sensitive data of different types are processed by he ml models.
hed iff generates about mutated inputs in total to test heml models.
during approximately 54hours of testing we detected a total of deviation inputs across almost all the tested fhe frameworks and models.
we show that these inputs have high visual similarity compared with normal inputs thereby uncovering defects that may cause substantial confusion for users of these fhe frameworks in their daily usage.
our further study depicts the root cause of these defects.
looking ahead besides using standard test data for accuracy validation developers can use hed iffto test potential deviation behavior in their he ml models before releasing them to end users.
in sum we make the following contributions we for the first time study flaws introduced in fhe a highly visible cryptographic primitive that enables secure computation on encrypted data.
we reveal flaws within the context of he ml models which can cause confusion or adversarial manipulations during usage.
we present hed iff an automated testing tool designed to uncover inputs that cause deviated outputs in he ml models compared to their plaintext counterparts.
hed iff employs carefully designed margin based feedback to guide test input selection and mutation.
our large scale evaluation of mainstream he ml frameworks and models exposes a substantial number of flaws.
we also discuss the root causes and demonstrate the potential consequences of exploiting these defects.
we maintain hed iffto benefit future research at .
ii.
b ackground and motivation this section introduces the mechanisms and core components that enable efficient fhe computation.
we then illustrate how fhe can be employed to enable privacy preserving ml inference and the significance of he ml in practice.
note that we do not specifically distinguish fhe and he in this paper and to ease the reading we refer to fhe based ml as he ml.a.
homomorphic encryption he he is a cryptographic primitive that enables computation on encrypted data.
a major application scenario for he is cloud computing where clients send their data to computationally powerful but untrusted servers for computation.
the servers can compute the encrypted data and return encrypted results to the clients while learning nothing about the plaintext data or results.
denote the encrypted data as ccorresponding to a plaintext message m i.e.
encrypt m c. the equivalence of computation on encrypted data and plaintext data constitutes the fundamental property of he as shown as follows decrypt f c decrypt f encrypt m f m where fis the function that the clients intended to compute on their data.
the function fis typically composed of addition and or multiplication operations.
depending on the type and the number of supported operations he can be classified into different categories including partially homomorphic encryption phe somewhat homomorphic encryption she and fully homomorphic encryption fhe .
phe supports either addition or multiplication operations but not both while she supports both but is constrained by the total number of operations.
fhe supports an arbitrary number of addition and multiplication operations.
due to its versatility fhe is generally referred to as cryptography s holy grail .
as such this paper focuses on fhe schemes.
however hed iff is not limited to fhe and has high generality across different he algorithms by treating he ml frameworks as a black box.
the privacy guarantee of fhe is based on the hardness of certain mathematical problems particularly the learning with errors lwe problem .
given an n element secret vector s fn q where fqdenotes a finite field the client samples a matrix a fn n q from a uniform distribution and a noise vector e fn qfrom a small variance gaussian distribution.
based on the lattice theory it is nearly impossible for the server to uncover the secret vector sfrom the term as e. hence the client can use sas a key to encrypt its message mby computing b as 2e m then sends the ciphertext c a b to the server.
the server can operate on the ciphertext cby addition and multiplication operations without learning the plaintext message m and returns the final computed ciphertext c to the client.
the client can then decrypt the ciphertext c to obtain the plaintext result m .
hurdles of applying fhe to ml.
although fhe serves a promising solution to enable privacy preserving computation applying fhe to ml is non trivial because computation on real number.
ml models typically work with real numbers while fhe can only naturally support integer operations.
one viable solution is to convert all real numbers at the application level in which the server rounds the real numbers into integers before applying fhe with techniques such as model quantization .
such an approach can yield compatible accuracy with the real number counterpart .
another solution is to support real numbers at the cryptographic level in which the fhe scheme natively supports real number 2operations.
many fhe schemes have been proposed to support real number operations the most prominent of which are ckks and rns ckks .
such schemes are often referred to as approximate fhe schemes as they can only support approximation of real number operations.
the accuracy of the approximation is related to the number of bits used to represent the real number.
the more bits used the higher the accuracy and the greater the computational cost.
non linear functions.
fhe requires the homomorphic function fto be expressed in terms of addition and multiplication operations or some other operations like bit shift that the underlying fhe schemes can support.
however in the machine learning domain many widely used functions like relu and sigmoid are non linear which does not directly map to fhe primitive operations.
a plethora of works approximate non linear functions with polynomials of the form akxk .
.
.
a1x a0 which can be expressed in terms of addition and multiplication operations.
some heml frameworks like concrete ml provide built in support for non linear function conversion while others like tenseal require users to manually convert the non linear functions to polynomial form.
besides conversion the model can also be trained directly with the approximated function in an fhefriendly format to avoid the noise introduced in the conversion process.
our evaluation covered all these three scenarios.
b. fhe in mlaas to date fhe is typically employed in mlaas and commercialized by platforms like amazon aws and google cloud .
often the cloud holds an ml model and provides the ml inference service to a client.
the client has sensitive data and does not want to reveal it to the cloud but wants to use the cloud s model for inference.
meanwhile the cloud does not want to reveal internal service implementations particularly themodel architecture andparameter weights to the client as designing the model architecture can cost considerable effort and the model might be trained on proprietary datasets.
to use fhe the cloud first trains a plaintext model using standard ml training techniques.
the plaintext model is then transformed into a fhe compatible format with the help of he ml frameworks like tenseal finally encrypted and deployed to the cloud.
to use the deployed model the client first encrypts his sensitive data locally then sends the encrypted data to the cloud.
the cloud runs ml inference on the ciphertext from the client and returns the encrypted result.
after that the client decrypts it to obtain the result in plaintext.
during the process the cloud learns nothing about client data due to the security guarantee of fhe while the client cannot either learn the model architecture and parameter weights of the cloud.
c. research motivation and position motivation.
as aforementioned there has been high interest in commercializing and deploying fhe in practice.
several industrial giants and startups are actively developing and promoting their he ml solutions in the market .
he ml has also been adopted in various highly regulated sectors includinghealthcare finance and government .
overall the market size of fhe has reached million usd in and is expected to hit million by .
despite the promising adoption of fhe in practice the correctness of he ml frameworks and corresponding fhehardened models has not been systematically studied and their potential vulnerabilities have not been well understood.
we anticipate that the defects in he ml frameworks may lead to severe consequences in fhe hardened models including missed detections of tax evasion credit fraud or even misdiagnosis of patients.
even worse our tentative explorations show that defects in he ml frameworks are common and hard to diagnose given the algorithmic obscureness of fhe the complexity of he ml frameworks and the high computational cost of testing fhe.
hence it is imperative to understand the reliability of he ml frameworks and corresponding fhehardened models and to develop effective testing techniques to uncover potential defects.
position.
we aim to develop automated testing to uncover defects in the inference process of he ml models denoting the most common usage of he ml in practice.
to our knowledge few he ml frameworks support he enabled model training given the prohibitive computation overheads.
the training of ml models is typically performed offline where the training data is assumed available in plaintext.
the trained model is then transformed by he ml frameworks into an fhe compatible format and deployed e.g.
on the cloud to perform inference.
we also assume that the original model is well trained i.e.
it has been trained on a large amount of high quality data and has achieved high accuracy on the validation set.
yet we assume the predictions of fhe hardened well trained ml models may be incorrect under certain inputs and aim to detect inputs that cause these mispredictions in he ml models.
iii.
p reliminary study and observation this section conducts a preliminary study to summarize insights that guide the design of hed iff which will be presented in iv.
a. deviation inputs and preliminary testing definition.
following the discussion in ii c the objective of our preliminary testing is to analyze the characteristics of deviation triggering inputs or simply called deviation inputs which are inputs ithat can make the fhe hardened model mfto predict the wrong label while its plaintext version mp to output the ground truth label.
specifically deviation inputs need to satisfy the following constraints argmax mf i argmax mp i argmax mp i label where mf i andmp i denote the logits output of the fhehardened and plaintext models respectively and label is the ground truth of input i.argmax is used to choose the class with the highest logits i.e.
the prediction class of the model.
example.
table i depicts the logits output the output without softmax of the plaintext and fhe hardened models using the concrete ml framework .
the class prediction i.e.
the class with the maximum value of the logits of the plaintext 3table i logits output of the same image from the plaintext and fhe hardened ten class classification model.
the class predictions of the two models are bolded.
label class no.
plain logits .
.
.
.
.
.
.
.
.
.
fhe logits .
.
.
.
.
.
.
.
.
.
table ii deviation inputs identified during preliminary testing.
the columns show the number and ratio of deviation inputs under three mainstream he ml frameworks.
datasets models tenseal concrete ml helayers mnist cryptonets square .
.
.
digits cryptonets sigmoid .
.
.
credit mlp sigmoid .
.
.
bank mlp apprelu .
.
.
model is meaning the prediction of the fourth position was selected.
in contrast the class prediction of the fhe hardened model is i.e.
the prediction at the ninth position .
preliminary testing.
we find that deviation inputs are not rare.
rather we observe such inputs across different encryption schemes he ml frameworks models and datasets.
here we launch a standard differential testing dt task we iterate through the training dataset to count deviation inputs with respect to the objective in eq.
.
this way we measure the ratio of deviation inputs residing in the standard training dataset and study their characteristics for further efficient testing.
table ii shows the results of preliminary testing across four datasets four models and three he ml frameworks including tenseal concrete ml and helayers .
specifically we iterate through the whole training dataset for digits credit and bank datasets on tenseal and helayers and randomly select several samples for testing from the mnist dataset and the datasets on concrete ml due to their extremely time consuming.
details of models and framework configurations will be further introduced in v. reflection and challenge.
while we find deviation inputs across different settings the ratio of deviation inputs existed in the original training dataset is low.
however this does not mean that this phenomenon is unimportant on the contrary we view this problem as critical albeit difficult to detect especially in fhe hardened models where the inference computation is timeconsuming and blanket searching is hardly feasible.
overall given the objective defined in eq.
and the observations above one might expect to employ dt to drive automated testing where we compute and compare the predictions of both the fhe hardened and plaintext models.
however considering that fhe computation is slow directly applying dt techniques to this scenario is less feasible.
we need a highly targeted approach to minimize computation on fhe hardened models.
below in iii b we explore leveraging certain metrics to guide the search of deviation inputs.
b. explore efficient deviation inputs searching to deliver efficient dt and uncover deviation inputs in fhehardened models the key is to minimize the expensive fhe computation.
as such we attempt to identify a particular metrictable iii the number and ratio of deviation inputs that cause the predictions to change to the second most likely class.
the deviation inputs are collected from the results in table ii.
datasets models tenseal concrete ml helayers mnist cryptonets square .
.
.
digits cryptonets sigmoid .
.
credit mlp sigmoid .
.
.
bank mlp apprelu .
.
fig.
statistics of margin value on deviation inputs and normal inputs.
the x axis denotes margin values and the yaxis denotes the number of normal and deviation inputs.
of deviation inputs on the plaintext model that could be treated as a mirror.
by calculating and observing this mirror metric solely on the plaintext model we would have confidence that the corresponding input is more likely to trigger deviant outputs in the fhe models.
based on this insight we now analyze the characteristics of deviation inputs collected in iii a and explore possible metrics that satisfy the above conditions.
observation.
through a detailed analysis of the identified deviation inputs we find that nearly all of them trigger the model to predict the second most likely class in the plaintext model.
we thus measure and report the ratio of deviation inputs that fall into this category.
as shown in table iii the ratio is high across nearly all datasets and models.
moreover when comparing the logits of the plaintext model and the fhehardened model for each normal deviation input the absolute difference in logits for the prediction classes with the two highest probabilities is almost the same.
for instance consider the example in table i where the fourth and ninth classes have the highest probabilities in the plaintext and fhe hardened model.
the difference in the logits of the fourth and ninth classes is about .
and .
respectively.
in other words if the difference between the logits of the most likely and the second most likely classes is marginal deviation inputs may be more likely to exist when the logits of these classes change by approximately the same absolute values.
margin.
through the above observation and analysis it is natural to consider the margin as our metric given margin is defined as the difference between the largest prediction value and the second largest prediction value in the model s inference result.
formally given a model mand an input x we have margin x pred i pred j where i argmax m x pred i m x iandj argmax m x k i pred j m x j. in deep learning theory margin is deemed the minimum distance of an input to the decision boundary .
while deviation behavior is due to the difference in the decision boundaries between fhe hardened and plaintext models data 4that is close to the decision boundary of the plaintext model is more likely to cause deviation as a slight difference in the decision boundary of fhe hardened model near this data may induce an altered prediction i.e.
deviation behavior.
thus margin as a metric for the distance to the decision boundary should be a good choice to identify deviation inputs.
empirical validation.
we conduct an experiment to validate the effectiveness of the margin metric.
specifically we compare the margin of each deviation input and normal input from a statistical perspective.
we chose the deviation inputs found from credit dataset as shown in table ii.
we then count the number of normal and deviation inputs within different margin value ranges.
as in fig.
we observe a clear trend that most deviation inputs denoted with the blue bar have small margin values.
we interpret that inputs with smaller margin values are closer to the decision boundary and therefore are more likely to reflect the boundary differences between the plaintext and fhe hardened models through subtle mutations.
formalization.
to further illustrate the effectiveness of the margin metric we formalize the relationship between margin and deviation inputs.
let margin be i top1 logits p i top2 logits p i where top1andtop2are used to get the max and second max value in logits p i which means the logits value of plaintext model given input i. consider the max error i introduced in fhe hardened model s prediction for each value of class jinlogits f i we have logits f i j logits p i j i i.e.
logits p i j i logits f i j logits p i j i for the difference of two classes j1andj2 we have logits p i j1 logits p i j2 2 i logits f i j1 logits f i j2 logits p i j1 logits p i j2 2 i suppose j1andj2correspond to the max and second max value in logits p i respectively we have i 2 i logits f i j1 logits f i j2 i 2 i as i by the definition of margin and error i is necessarily small since large error would compromise both the usability of fhe computations and the fhe hardened model s accuracy the smaller the i the higher probability that i 2 i 0andlogits f i j1 logits f i j2 which will make the prediction label in fhe hardened model change.
in short we consider margin to be a straightforward yet highly effective metric to guide the search for deviation inputs.
iv.
d esign of hed iff margin s effectiveness for exploring deviant outputs in an fhe hardened ml model as illustrated in iii motivates us to design a novel margin guided tool named hed iffto test and understand deviation behaviors in fhe hardened models.
overview.
fig.
shows a high level workflow of hed iff which consists of three main steps.
1given a set of original data inputs ofrom the given dataset hed iffperforms marginbased seed filtering iv a to select inputs swith the lowest margin values on the plaintext model as seeds.
the insight is that these seeds are more likely to trigger deviant outputs in thefhe hardened model.
2hed iffthen conducts margin guided differential testing iv b aimed at mutating the inputs towards lower margin values on the plaintext model and employs dt as an oracle to identify seeds dthat are deviation inputs.
hed iffdoes not stop at identifying individual deviation inputs but also attempts to understand them by generalizing the noise patterns of these deviation inputs iv c .
the generalized patterns can be treated like weak uaps and enable further identification of deviant inputs.
for example for the s d seed inputs that originally do not produce deviant outputs in the second step we can further mutate them based on the noise patterns generalized from the ddeviant outputs.
a. margin based input filtering for seeds not all data inputs from the given dataset are suitable to be evolved into deviation inputs.
we need to identify only those promising ones as candidate seeds for further mutation in iv b .
specifically we found that inputs with lower margin values of the prediction outputs on the plaintext model are more likely to evolve into deviation inputs than those with higher margins making them more suitable as seeds .
for example our experiment in vi b shows that if we use the top inputs in the dataset with the lowest margin values as seeds the effectiveness of using them to find deviation inputs is greater than that of using the randomly selected inputs.
with this observation hed iffsorts the training data inputs oby their margin values on the plaintext model and selects the samples swith the smallest margin values as seeds which have a higher potential to evolve into deviation inputs.
we will describe the concrete configuration of oandsin vi.
b. margin oriented differential testing with the selected seed inputs s we need to mutate them in a way that they can eventually cause deviant behaviors.
to achieve this we add a small amount of noise to each seed and observe whether such a mutated input evolves toward potential deviation.
according to insights obtained from iii such a mutation should evolve towards a decreased margin value of the generated prediction output on the plaintext model so that the mutated inputs have a higher chance of causing deviant behaviors in fhe hardened models.
toward this objective a straightforward approach is to use random mutation.
for example for each seed input we could randomly mutate it ten times and keep only those with decreased margin values on the plaintext model.
however it is hard for such a random approach to always identify suitable mutated inputs.
moreover it requires more computation due to its blind testing nature.
therefore we seek a more targeted mutation method.
given that the pgd algorithm has demonstrated its effectiveness in finding aes for subjects like images we explore how to customize pgd in our scenario to target the mutation of seed inputs towards the direction of lowering the margin value.
to clarify although we apply the idea of well known pgd in our work we are not looking for aes but rather deviation inputs that cause incorrect predictions in fhe hardened models.
more discussions from conceptual 5origin dataset metric sort...seed listpopmargin guided mutationsame pred.diff.
pred.
m!m iv.aiv.b deviation inputsnon deviation for each iv.c try k nearest noise fig.
a high level workflow of hed iff.
algorithm margin oriented differential testing.
function marginpgd mp i iteration noise bound i i stepsize lossfunc x margin predict vector mp x for1...steps do i i clamp stepsize sgn lossfunc i stepsize stepsize noise i i obtain added noise from the mutation return noise function margindt mp mf corpus of seed inputs s max mutation times nmax iteration noise bound total noise bound q s d cntmu while cntmu nmax andq do i q .pop predp label p predict vector mp i predf label f predict vector mf i iflabel p label fandlabel p i.label then add iind collect deviation inputs else i i marginpgd mp i margin oriented mutation i clamp noise i control cumulative noise scale add i inq cntmu cntmu return d technical and empirical perspectives on the difference between aes and deviation inputs are provided in vii.
the standard form of pgd seeks to improve the loss function value of specific data on the target model by adding multiple steps of noise along the gradient.
similarly we treat the loss function as a negative indicator of the margin value and quickly reduce the margin value according to the pgd algorithm with random start.
however if we only modify the loss function the pgd algorithm will easily fall into localized perturbations thus failing to reduce the margin value making this algorithm even less effective than using random mutation.
this issue does not occur with the original pgd algorithm because the original loss function does not have a bound and can be continuously increased.
however the minimum value of the margin is and thus it is highly susceptible to falling into perturbations around due to larger step sizes.
therefore we use an adaptive step size in place of the fixed step in the pgd algorithm so that we can continuously and rapidly decrease the margin value.
with the support of the margin oriented pgd algorithm we now present the complete dt algorithm in alg.
.
it consists of a building block function marginpgd to mutate the seed inputs towards lower margin values on the plaintext model and a main function margindt that employs dt as an oracle to eventually identify deviation inputs dfrom the given seeds s.specifically in lines marginpgd takes the plaintext model mp a normal input iand an iteration bound as inputs and returns a noise that makes i noise has a lower margin value on the plaintext model.
to achieve this we define our loss function as the negative margin in line when we increase the loss function with pgd we decrease the margin value of the corresponding data with respect to mp.
in lines we iteratively add noise to i in the direction of the gradient of decreasing margin value.
clamp constrains the noise in the range of to if a value in the noise is less than or greater than it is set to or .
then we use an adaptive step size to avoid falling into localized perturbations in line .
inmargindt we first initialize the seed pool qwith the filtered seed inputs sand the deviation inputs pool das empty in line .
we then iteratively mutate the seed inputs in quntil the number of mutations cntmureaches a predefined threshold nmax or all seeds in qare mutated to deviation inputs in line .
for each iteration we pop a least recently used seed input ifrom queue qin line and predict the output vectors of the plaintext model mpand fhe hardened model mfin lines .
if the prediction labels of mpandmfare different and the prediction label of mpis the same as the ground truth label of i we add ito the deviation inputs pool din line .
otherwise we use marginpgd to generate a mutated input i from iin line .
at line c lamp noise constrains the cumulative noise of i within the range of to where is the total noise bound note is the per iteration noise bound .
i will be added to the seed pool qin line .
finally we return the deviation inputs pool din line .
c. generalizing and leveraging noise patterns as mentioned earlier we not only identify individual deviation inputs but also try to understand these inputs specifically whether they exhibit general noise patterns.
the generalized patterns can be treated like weak uaps enabling further identification of deviation inputs.
to this end we first conduct a theoretical analysis on how the noise pattern of a deviation input could be generalized to other inputs.
we then use this understanding to further drive hed iffto identify more deviation inputs from the seeds that originally do not produce deviant outputs.
theoretical analysis.
fig.
shows the schematic view of our theoretical analysis.
given an input x2that cannot be transformed into a deviation input during differential testing we can find its nearest input x1that can be transformed into a 6decisionboundaryofplaintext modeldecisionboundaryoffhe modeln!x!x!
n!x x fig.
illustrating how the noise pattern of a deviation input x1can be generalized and used to transform another input x2 that originally does not produce deviant outputs during dt.
algorithm pattern based mutation.
function patternmutation mp mf corpus of non deviant seed inputs snon corpus of deviation inputs p total noise bound dp forsi snon do n1 n2 ... n k findnearest noise si p forni n1 n2 ... n k do i findscale factor mp si ni if i none then s i si clamp i ni predp label p predict vector mp s i predf label f predict vector mf s i iflabel p label fandlabel p si.label then add s iindp b reak return dp deviation input with noise n1.
as these two points x1andx2 are close to each other perturbed inputs with noise n1 x1 n1 andx2 n1 should also be similar.
as we know x1 n1is the deviation input i.e.
x1 n1is in the gap between the decision boundary of the plaintext model and the fhe hardened model as illustrated in fig.
.
therefore x2 n1 being close to x1 n1 has a high probability of also being in this gap.
however if we directly use x2 n1 this may not result in a deviation input because the decision boundaries of the plaintext and fhe models are non linear.
therefore instead of directly applying n1 we consider it as a meaningful noise direction and we need to scale it to make it more effective.
with the observation in iii we can still use the margin value ofx2 n1on the plaintext model as a guide for finding an appropriate scale factor i.e.
we can find the that makes the margin value of x2 n1on the plaintext model as small as possible while keeping the prediction unchanged.
using the noise patterns.
as an application of the generalized noise patterns we further drive hed iffto identify more deviation inputs from the seeds snon s d that originally do not produce deviant outputs during dt in iv b .
we present our pattern based mutation algorithm in alg.
.
specifically for each non deviant seed si snon we can find the knearest deviation inputs with the function findnearest noise in line which computes the l2 distance between siand each diviation input xiwithout noise and return kdeviation noises n1 n2 ... nksuch that sim l2 si xj j ...k are the klowest.
then in lines for each noise ni we find the corresponding scale factor iusing the function findscale factor .
this function tries to find an ithat makes the margin value of si i nion the plaintext model low enough while the prediction still equals si.label with binary search.
in line after finding a proper i we constrain each value of noise i niwith the function clamp to avoid any noise value exceeding the noise bound like what we did in alg.
.
finally we compare the result ofsi i nion the fhe hardened model with si.label to determine whether si i niis a deviation input in lines .
if so we add it to the deviation inputs pool dpand then break the loop.
v. i mplementation and setup hed iffis written primarily in python with about lines of code.
to facilitate the reproducibility of results we provide the source code and the datasets used in the evaluation at .
we will maintain hed iffand update it with new features and more documents to benefit future research.
hed iffdoes not require any modifications to the models or the he ml frameworks which makes it easy to launch on different settings.
there is no special configuration for seed filtering and marginguided mutation.
as for noise patterns when collecting them we only use the noise pattern of the nearest data to the input data and do not consider if its label is the same as the input data for the seek of balancing time consumption and scalability.
details will be discussed further in vi c .
below we present the evaluation setup in line with the statistics in table iv.
he ml frameworks and fhe schemes tested.
hed iff can test various he ml models implemented using different he ml frameworks.
as mentioned earlier in iii we target three mainstream he ml frameworks tenseal concreteml and helayers.
tenseal is a library for tensor homomorphic encryption operations based on microsoft seal .
based on the encryption library seal tenseal provides vector operation support for bfv and ckks schemes.
ckks which supports real number operations is commonly considered the most suitable fhe scheme for ml applications.
therefore we also use this scheme to encrypt the tested ml models.
additionally we select parameters to support the full depth of multiplications to ensure that models errors do not arise from decryption failures.
to handle the non linear function sigmoid we follow tenseal s recommendation to approximate the non linear function with a polynomial function.
concrete ml provided by zama a cryptography company focusing on fhe is built on their fhe compiler concrete.
concrete ml supports the tfhe scheme .
tfhe is a scheme for computations on integers and provides the programmable bootstrapping pbs mechanism which reduces noise with bootstrapping and can compute arbitrary functions using a lookup table.
this way we can avoid manually approximating non linear activation functions after quantizing the model into an integer model.
instead we can directly utilize concrete ml s functionality to implement activation functions.
helayers developed by ibm bridges ml frameworks like pytorch and tensorflow and basic fhe 7table iv the evaluation setup of hed iffand the overall statistics.
framework model datasetsplaintext encrypted avg.
inference initial total deviation input total deviation accuracy accuracy time per input seeds mutations after filtering inputs tensealcryptonets square mnist .
.
.30s cryptonets sigmoid digits .
.
.99s mlp sigmoid credit .
.
.32s mlp apprelu bank .
.
.43s concrete mlcryptonets square mnist .
.
.02s cryptonets sigmoid digits .
.
.38s mlp sigmoid credit .
.
.41s mlp apprelu bank .
.
.70s helayerscryptonets square mnist .
.
.83s cryptonets sigmoid digits .
.
.58s mlp sigmoid credit .
.
.13s mlp apprelu bank .
.
.12s libraries like heaan and seal .
similar to the previous two frameworks helayers depends on third party libraries to provide the fhe functionalities.
however unlike the previous two helayers offers a more flexible way to customize the low level fhe library rather than just using the default one.
in the evaluation we also choose seal as our underlying library and test its ckks scheme.
as helayers does not support automatic conversion for sigmoid we use the same method as in tenseal to approximate the sigmoid function.
datasets and models.
we assess hed iffacross various data types such as images and tabular data and choose widely recognized ml tasks including image classification credit score prediction and deposit subscription prediction.
specifically we chose representative datasets mnist digits credit and bank .
the mnist dataset is widely used for handwritten digits and consists of 28x28 pixel grayscale images labeled from to .
the digits dataset is similar to mnist but with smaller 8x8 pixel images.
for these two image datasets we employ the neural network architecture similar to a popular fhe friendly model cryptonets yet with different activation functions.
in particular for mnist we use the popular square activation function in the he ml field and for digits we use the activation function sigmoid to investigate the impact of non linear activation functions.
the credit dataset is a tabular dataset commonly used for credit score prediction.
it comprises various features related to individuals credit profiles such as age income and credit history.
the task is to predict whether a given individual is likely to have good or bad credit based on these features.
the bank dataset is another tabular dataset used for predicting deposit subscriptions containing information about bank clients including their demographic economic and banking features.
we use similar models for these two tabular datasets.
specifically we use undersampling to balance these two datasets and then use a two layer mlp with sigmoid activation for the credit dataset and a two layer mlp with an approximation of the relu activation for the bank dataset to investigate the impact of training with polynomially approximated non linear activation functions.
clarification on datasets.
one may wonder about the choice of datasets in our evaluation as contemporary dnn testing works often use more complex datasets like imagenet or traffic scenes .
we clarify that in practical fhe scenarios computation over sensitive tabular data is the mainstream.
incontrast conducting fhe over complex media data like highresolution images or videos is not practical.
nevertheless we still use two image datasets mnist and digits to evaluate hed iffand show its generality over different data types.
vi.
e valuation our evaluation follows the setup noted in v. all experiments are conducted on a server with an amd ryzen 3970x cpu and 256gb ram.
we aim to answer the following rqs.
rq1 can hed iffeffectively find deviation triggering inputs for he ml models?
rq2 does the margin guided approach outperform the random approach in seed filtering and mutation?
rq3 can noise patterns of the nearest data be effectively used to find new deviation triggering inputs?
a. rq1 effectiveness of hed iff to answer rq1 we present the evaluation results of hed iff in table iv with the configuration described above.
in the evaluation on tenseal and helayers for datasets like minst credit and bank which have more than 3k data points in training dataset we set the seed number at 1k and the maximum mutation number at 5k.
for the small dataset digits which has less than 2k data points we set the seed number at and the maximum mutation number at .5k.
for the extremely time consuming concrete ml we halve the seed and maximum mutation numbers.
for comparison between plaintext and encrypted models we use the entire test dataset for models on tenseal and helayers and sample 1k test data for datasets except digits on concrete ml.
the subtle differences between the prediction accuracy of plaintext and encrypted models demonstrate the proper configurations of evaluated fhe frameworks indicating that he ml models do not change the functionality of the plaintext model and the deviation inputs found by hed iffare not caused by incorrect security parameter configurations.
this is also illustrated by table ii which shows the low ratio of deviation inputs in the original training data.
as shown in table iv results suggest that although plaintext and encrypted models have similar prediction accuracy this similarity does not equate to the robustness of the encrypted models.
hed iffcan still filter and generate a large number of deviation inputs from the seed data for most settings.
cryptonets square and mlp apprelu are two linear models that do not involve non linear activation function conversion.
this way the main root causes of deviations are imprecise 8plain model 5tenseal plain model 5tenseal 3plain model 9helayer 9plain model 9helayer fig.
examples of added noise.
table v average l2 and maximum distance of noise.
we show the average l2 and maximum distance per pixel for image datasets and for tabular datasets we show the average l2 and maximum distance per column.
a dash in the table means that no deviation was found during the mutation process.
he ml mnist digits credit bank framework l2 max l2 max l2 max l2 max tenseal .
.
.
.
.
.
.
.
concrete ml .
.
.
.
.
.
.
.
helayers .
.
.
.
.
.
weights in the encrypted models introduced by encoding floats to fixed point real numbers in ckks and model quantization in tfhe along with accumulated noise during encrypted computation.
as shown in the results of the table helayers performs best suggesting that helayers may introduce the least noise in encoding and computation for these two models.
besides deviations localized by seed filtering we also observe that hed iffcan generate many deviation inputs based on non deviation seeds.
an outlier case is the mlp sigmoid model on tenseal and helayers i.e.
with ckks.
although we can localize a high ratio of deviation inputs during seed filtering nearly no deviation inputs are found in the mutation process.
this is likely because with a rather conservative noise threshold it is difficult to move these data to the gaps between the decision boundaries of the plaintext and encrypted models.
although we found hundreds of deviation inputs with hed iff it is worth emphasizing that the noise introduced to deviation inputs is generally mild and does not affect the visual content of those deviation triggering inputs.
this is made possible by our cumulative noise control introduced in iv b .
such stealthily tweaked deviation inputs can likely cause high confusion of he ml frameworks and applications in practice.
below we present both quantitative and case studies.
fig.
exhibits two deviation inputs with their original seeds and added noise for mnist with tenseal and digits with helayers.
as can be seen the deviation inputs manifest high visual similarity with the original inputs yet these deviation inputs result in different outputs of tenseal helayers hardened models.
due to limited space we present more examples online in our repository .
moreover we quantify the scale of noise generated by hed iffin table v. each column of data has two values representing the average l2 distance between the original seed and generated deviation inputs and the average maximum noise value respectively.
overall we interpret the average l2 and maximum distances are being reasonably low indicating that the produced deviation inputs are close to normal inputs across different models and data types.answer to rq1 although fhe hardened models show satisfactory performance with a negligible accuracy downgrade and within a reasonable testing time hed iffeffectively finds plenty of deviation inputs that may mislead and confuse users across nearly all scenarios.
b. rq2 effectiveness of margin s guidance in iv we proposed two margin guided approaches for seed filtering and mutation and a method to generalize the noise pattern of deviation inputs.
in this section to answer rq2 we investigate the effectiveness of two margin guided approaches across different datasets and he ml frameworks.
we will discuss the generalization of noise patterns in vi c .
specifically we compare the margin guided approaches with the random approaches in seed filtering and mutation.
in the mutation process we further consider the standard form of the pgd algorithm implemented by torchattacks as another stronger baseline.
we set the seed number at and the mutation number at for datasets on tenseal and helayers concrete ml with results reported in table vi.
the results in table vi show that margin guided filtering can localize many more deviation inputs in seeds in comparison to random filtering.
from another perspective margin guided mutation can find more deviation inputs than random mutation under either margin guided or random filtering strategies.
while pgd mutation finds more deviation inputs than random mutation margin guided mutation outperforms pgd in most cases especially when synergized with margin guided seed filtering.
we also report that while random mutations are slightly faster than the other two mutation strategies pgd mutation and margin guided mutation take about the same time to find deviation inputs.
to clarify the computation of these mutation strategies only differ in plaintext models while the main time consumption comes from the encrypted inference of the fhe hardened models.
in other words the choice of different mutation strategies has only a negligible impact on the speed of our testing.
overall we recommend using margin guided seed filtering and mutation together which can presumably yield the most effective testing results in a satisfactory time frame.
answer to rq2 compared with the random approach and pgd mutation hed iff s margin guided design significantly contributes to its effectiveness and can substantially help users uncover more potential deviation inputs in fhehardened models.
c. rq3 characteristics of noise patterns in this section we answer rq3 by starting with validating the assumption that close data will also have similarity in the direction of the noise pattern as illustrated in fig.
.
specifically we choose the tenseal framework as our testbed and compare the approach of choosing knearest data i.e.
the approach used in hed iff with other three approaches selecting krandom data and selecting knearest random data with the same label.
for each approach denoted as a 9table vi filtering and mutation strategy effectiveness.
random filter margin filter refers to random marginguided filtering and random pgd margin refers to random pgd margin guided mutation.
numbers in the o.columns denote deviation inputs found during seed filtering and numbers in the t.columns denote deviation inputs found after mutation.
we mark the largest deviation inputs using blue and yellow under random filter and margin filter respectively.
he mldatasetsrandom filter margin filter random pgd margin random pgd marginframeworko.
t. o. t.o.
t. o. t. o. t.o.
t. tensealmnist digits credit bank concrete mlmnist digits credit bank helayersmnist digits credit bank table vii noise pattern similarity.
topk random w w o means the similarity of k nearest random data with without the same label consideration.
metric kmax mnist digits bank topk w1 .
.
.
.
.
.
.
.
.
topk w o1 .
.
.
.
.
.
.
.
.
random w1 .
.
.
.
.
.
.
.
.
random w o1 .
.
.
.
.
.
.
.
.
we measure its noise pattern direction similarity performance sim ausing the method described below.
given a set of deviation inputs d for each deviation inputs xd nd d we choose kdeviation inputs x1 n1 ... xk nk d xd nd using approach a and calculate the average cosine similarity between n1 nkandndassim d. then sim ais defined as the average of sim dfor all deviation inputs.
this can be formulated as sim a d x xd nd d1 kkx i 1nd ni nd ni we compare the performance of knearest data with and without same label and random kdata with and without same label and the results are shown in table vii.
we choose k in the experiment.
table vii shows that noise patterns of closer data are more likely to be similar.
this is consistent with our intuition in fig.
.
also we find that the similarity when k is higher than when k or5 meaning that the noise pattern of the nearest data is also the most similar among the noise patternstable viii noise pattern effectiveness.
numbers in the dev.
and q. columns mean deviation inputs found and total queries to fhe hardened models respectively.
metric kmaxmnist digits bank dev.
q. dev.
q. dev.
q. topk w1 topk w o1 random w1 random w o1 ofknearest data.
moreover when comparing data with and without the same label topk wandtopk w o we find that the similarity of noise patterns is not strongly affected by the label of the data suggesting that the noise pattern is more likely to be determined by the data itself rather than the label.
we further compare the effectiveness of finding new noise patterns with the approach described in iv c using noise patterns from knearest random data with without the same label.
the results are shown in table viii.
both topk w oand topk wperform similarly well in finding new deviation inputs compared to random approaches which is consistent with the results in table vii.
considering that topk w ohas the best similarity performance when k and good performance in finding new deviation inputs when kis higher we choose topk w owithk as the noise pattern leveraging strategy to make a trade off between query times and scalability.
in other words we use a small kto save on query times in the default setting and use topk w oto keep the potential of finding more deviation inputs with fewer queries as kincreases.
answer to rq3 we show that normal input data with a close distance are very likely to be transformed into deviation inputs with similar noise patterns.
this further allows hed iffto mutate inputs in a directed manner improving its effectiveness in finding more deviation inputs.
vii.
d iscussion deviation inputs vs. aes.
deviation inputs can mislead heml models from making correct predictions.
although similar to adversarial examples aes we clarify that they are distinct from ml aes found in prior works .
conceptually conventional aes specifically alter the predictions of plaintext dnn models only.
in contrast according to our definition in iii a deviation inputs cause the fhehardened model and its plaintext version to produce different predictions typically the plaintext version s prediction is correct while the fhe hardened version s prediction is incorrect.
in addition conventional aes are pervasive in dnns and are believed to stem from inadequate training and unsmooth classification boundaries .
differently hed iff s findings are specific to fhe frameworks and detected deviation inputs are 10primarily caused by the subtle differences between classification boundaries of fhe hardened and plaintext models.
technically hed iffuncovers inputs that trigger distinct predictions in he ml models compared to plaintext models.
the deviation inputs retain the same prediction labels in the plaintext models but alter the predictions in the he ml models.
due to the extreme time consuming of fhe computations hed iffconsiders plaintext models as mirrors and uses information provided by plaintext models to guide the search for deviation inputs.
in contrast aes typically change plaintext model predictions and are detected mainly with the information from the plaintext model itself.
empirically while the community is alert to the risks from aes we lack proper attention to potential deviations caused by fhe.
the gap between regular and fhe hardened models can mislead developers about their robustness since a model that performs well with plaintext may not maintain the same level of robustness when adapted for fhe.
such a misunderstanding can potentially introduce security risks in financial medical and other sensitive areas.
bug localization and mitigation.
we envision the potential feasibility of localizing certain root causes of the detected deviations.
as analyzed in ii a the he ml models may yield deviant outputs due to approximations in the non linear functions and the conversion from floating point to fixed point numbers or integers.
this way it may become possible to localize the root neurons or layers in he ml models.
moreover with those localized root neurons layers we can fine tune them to increase accuracy and robustness.
we deem this interesting yet challenging for future work primarily due to the contemporary he ml frameworks not offering fine grained control e.g.
neuron level over the fhe approximations and as a tradeoff increasing accuracy and robustness may likely lead to a decrease in the inference speed.
we advocate for the se and he ml community to explore this direction based on the insights from hed iff.
alternative testing feedback.
in this paper hed iffgradually finds inputs to decrease the logits margin until the deviation is large enough to flip model predictions.
this design of testing feedback abstracts the he ml model internals e.g.
orthogonal to the types of approximations employed for the non linear functions making the testing pipeline of hed iffmore general and applicable to various he ml settings.
we also notice prior dnn testing works that leverage the model internals layer wise or neuron wise outputs to guide input mutation.
while this may offer finer grained feedback e.g.
focusing on the most critical neurons speed is the primary concern when applying such methods to test fhe hardened models.
hooking into model internals e.g.
every neuron incurs unacceptable overhead particularly in the he ml setting where substantial computation resources are used for encrypted computations.
we believe that the feedback currently provided on model outputs issufficient and consider it an interesting area for future work to explore finer grained testing feedback.viii.
r elated work privacy preserving machine learning.
besides he ml many other approaches have emerged to achieve privacypreserving machine learning.
secure multiparty computation mpc is one of the promising solutions in which multiple non colluding parties privately complete computation without revealing individual components.
early works have explored mpc based linear regression logistic regression and neural networks .
other existing works leverage federated learning techniques to safely aggregate gradients from multiple users .
furthermore recent research has also studied using trusted hardware environments like arm trustzone and intel sgx to protect ml models .
testing privacy enhancing technologies.
given the prosperous development of privacy enhancing technologies the task of checking the correctness of these privacy related algorithms and protocols is demanding.
recent works propose the use of symbolic execution constraint solving and fuzzing to test and verify differential privacy dp programs .
besides dp mpc compilers and their machine learning applications have also been tested by the community .
zero knowledge proof zkp compilers and applications due to their industry adoption in second level blockchains are receiving extensive attention for testing and verifying their correctness .
in contrast this work initializes the first effort to test and understand defects in he ml.
testing machine learning systems.
deep learning has become the crux of many real world applications and the reliability of deep learning systems has attracted increasing attention.
some existing works have explored testing deep learning models with the idea of coverage from traditional software testing .
morever some works have studied the correctness of deep learning compilers and libraries .
however the testing of he ml has not been well studied.
this work fills this gap by proposing a novel testing approach to efficiently detect defects in he ml systems.
ix.
c onclusion we present hed iff a margin guided dt framework for he ml models.
hed iffsuccessfully uncovered thousands of inputs that cause deviant outputs in commonly used heml models.
we show that the uncovered defects can cause great confusion in the daily usage of he ml models and the noise patterns generated by margin oriented mutation have the potential to be generalized to find more deviations.
we conclude by discussing the applications of hed iffand possible mitigation strategies for the uncovered defects.
acknowledgement we thank the anonymous reviewers for their valuable feedback.
we also thank the developers of the he ml projects we used in our evaluation.
the hkust authors were supported in part by a rgc grf grant under the contract rgc crf grant under the contract c6015 23g and research fund provided by hsbc.
11references c. gentry fully homomorphic encryption using ideal lattices in proceedings of the forty first annual acm symposium on theory of computing pp.
.
a. silverberg fully homomorphic encryption for mathematicians women in numbers research directions in number theory vol.
p. .
j. h. cheon a. kim m. kim and y .
song homomorphic encryption for arithmetic of approximate numbers in advances in cryptology asiacrypt 23rd international conference on the theory and applications of cryptology and information security hong kong china december proceedings part i .
springer pp.
.
a. benaissa b. retiat b. cebere and a. e. belfedhal tenseal a library for encrypted tensor operations using homomorphic encryption .
zama concrete ml a privacy preserving machine learning library using fully homomorphic encryption for data scientists https github.com zama ai concrete ml.
e. aharoni a. adir m. baruch n. drucker g. ezov a. farkash l. greenberg r. masalha g. moshkowich d. murik h. shaul and o. soceanu helayers a tile tensors framework for large neural networks on encrypted data privacy enhancing technology symposium pets .
f. boemer a. costache r. cammarota and c. wierzynski ngraph he2 a high throughput framework for neural network inference on encrypted data in proceedings of the 7th acm workshop on encrypted computing applied homomorphic cryptography pp.
.
w. ao and v .
n. boddeti autofhe automated adaption of cnns for efficient evaluation over fhe in 33rd usenix security symposium usenix security pp.
.
r. ran x. luo w. wang t. liu g. quan x. xu c. ding and w. wen spencnn orchestrating encoding and sparsity for fast homomorphically encrypted neural network inference in international conference on machine learning .
pmlr pp.
.
i. chillotti n. gama m. georgieva and m. izabach ene tfhe fast fully homomorphic encryption over the torus journal of cryptology vol.
no.
pp.
.
l. bergerat a. boudi q. bourgerie i. chillotti d. ligier j. b. orfila and s. tap parameter optimization and larger precision for t fhe journal of cryptology vol.
no.
p. .
e. aharoni n. drucker g. ezov h. shaul and o. soceanu complex encoded tile tensors accelerating encrypted analytics ieee security privacy vol.
no.
pp.
.
w. m. mckeeman differential testing for software digital technical journal vol.
no.
pp.
.
a. madry a. makelov l. schmidt d. tsipras and a. vladu towards deep learning models resistant to adversarial attacks arxiv preprint arxiv .
.
i. j. goodfellow j. shlens and c. szegedy explaining and harnessing adversarial examples arxiv preprint arxiv .
.
s. m. moosavi dezfooli a. fawzi o. fawzi and p. frossard universal adversarial perturbations in proceedings of the ieee conference on computer vision and pattern recognition pp.
.
r. gilad bachrach n. dowlin k. laine k. lauter m. naehrig and j. wernsing cryptonets applying neural networks to encrypted data with high throughput and accuracy in international conference on machine learning .
pmlr pp.
.
s. moro p. cortez and p. rita a data driven approach to predict the success of bank telemarketing decision support systems vol.
pp.
.
i. c. yeh and c. h. lien the comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients expert systems with applications vol.
no.
pp.
.
y .
lecun l. bottou y .
bengio and p. haffner gradient based learning applied to document recognition proceedings of the ieee vol.
no.
pp.
.
a. asuncion uci machine learning repository university of california irvine school of information and computer sciences ics.
uci.
edu mlearn mlrepository.
html .
research artifact d. j. wu fully homomorphic encryption cryptography s holy grail xrds crossroads the acm magazine for students vol.
no.
pp.
.
o. regev on lattices learning with errors random linear codes and cryptography journal of the acm jacm vol.
no.
pp.
.
y .
cheng d. wang p. zhou and t. zhang a survey of model compression and acceleration for deep neural networks arxiv preprint arxiv .
.
a. stoian j. frery r. bredehoft l. montero c. kherfallah and b. chevallier mames deep neural networks for encrypted inference with tfhe in cyber security cryptology and machine learning s. dolev e. gudes and p. paillier eds.
cham springer nature switzerland pp.
.
j. h. cheon k. han a. kim m. kim and y .
song a full rns variant of approximate homomorphic encryption in selected areas in cryptography sac 25th international conference calgary ab canada august revised selected papers .
springer pp.
.
a. al badawi c. jin j. lin c. f. mun s. j. jie b. h. m. tan x. nan k. m. m. aung and v .
r. chandrasekhar towards the alexnet moment for homomorphic encryption hcnn the first homomorphic cnn on encrypted data with gpus ieee transactions on emerging topics in computing vol.
no.
pp.
.
a. brutzkus r. gilad bachrach and o. elisha low latency privacy preserving inference in international conference on machine learning .
pmlr pp.
.
w. z. srinivasan p. akshayaram and p. r. ada delphi a cryptographic inference service for neural networks in proc.
29th usenix secur.
symp pp.
.
aws cryptographic computing amazon web services aws .
m. guevara expanding our fully homomorphic encryption offering google developers blog .
duality duality technologies secure data collaboration products .
h. n. security google leverages open source fully homomorphic encryption library help net security duality technologies google fhe transpiler .
duality our partnership with aws brings together the power of our innovative duality platform and the global reach and scalability of aws providing our customers with seamless access to our solutions and services .
c. byrne new privacy enhancing technology revolutionizing healthcare digital cxo hnology revolutionizing healthcare .
c. d. magazine why tackling financial crime calls for a privacy first approach cyber defense magazine .com why tackling financial crime calls for a privacy first approach .
t. w. s. journal banks start using information sharing tools to detect financial crime mation sharing tools to detect financial crime .
a. t. insight protecting data privacy in the fight against financial crime a team fight against financial crime ?brand ati .
g. t. review exclusive duality technologies enters trade finance fraud prevention echnologies enters trade finance fraud prevention .
a. authority darpa awards duality technologies multimillion dollar contract to accelerate machine learning on encrypted data ity.com machine learning darpa awards duality technologies multimill ion dollar contract to accelerate machine learning on encrypted data .
w. examiner defense officials eye anti quantum encryption to shore up protection of classified material washington examiner https nti quantum encryption to shore up protection of classified material .
d. gaikwad homomorphic encryption market is predicted to witness over cagr by encryption market predicted witness over gaikwad .
g. elsayed d. krishnan h. mobahi k. regan and s. bengio large margin deep networks for classification advances in neural information processing systems vol.
.
microsoft seal release .
jan. microsoft research redmond wa.
j. fan and f. vercauteren somewhat practical fully homomorphic encryption cryptology eprint archive .
h. chen r. gilad bachrach k. han z. huang a. jalali k. laine and k. lauter logistic regression over encrypted data from fully homomorphic encryption bmc medical genomics vol.
pp.
.
i. chillotti m. joye and p. paillier programmable bootstrapping enables efficient homomorphic inference of deep neural networks in cyber security cryptography and machine learning 5th international symposium cscml be er sheva israel july proceedings .
springer pp.
.
a. paszke s. gross f. massa a. lerer j. bradbury g. chanan t. killeen z. lin n. gimelshein l. antiga et al.
pytorch an imperative style high performance deep learning library advances in neural information processing systems vol.
.
m. abadi p. barham j. chen z. chen a. davis j. dean m. devin s. ghemawat g. irving m. isard et al.
tensorflow a system for large scale machine learning in 12th usenix symposium on operating systems design and implementation osdi pp.
.
r. e. ali j. so and a. s. avestimehr on polynomial approximations for privacy preserving and verifiable relu networks arxiv preprint arxiv .
.
j. deng w. dong r. socher l. j. li k. li and l. fei fei imagenet a large scale hierarchical image database in cvpr09 .
d. seita bdd100k a large scale diverse driving video database the berkeley artificial intelligence research blog.
version vol.
p. .
h. kim torchattacks a pytorch repository for adversarial attacks arxiv preprint arxiv .
.
k. leino z. wang and m. fredrikson globally robust neural networks ininternational conference on machine learning .
pmlr pp.
.
k. pei y .
cao j. yang and s. jana deepxplore automated whitebox testing of deep learning systems in proceedings of the 26th symposium on operating systems principles ser.
sosp .
new york ny usa acm pp.
.
x. xie l. ma f. juefei xu m. xue h. chen y .
liu j. zhao b. li j. yin and s. see deephunter a coverage guided fuzz testing framework for deep neural networks in proceedings of the 28th acm sigsoft international symposium on software testing and analysis pp.
.
o. goldreich s. micali and a. wigderson how to play any mental game or a completeness theorem for protocols with honest majority in providing sound foundations for cryptography on the work of shafi goldwasser and silvio micali pp.
.
a. wigderson m. or and s. goldwasser completeness theorems for noncryptographic fault tolerant distributed computations in proceedings of the 20th annual symposium on the theory of computing stoc pp.
.
q. jia l. guo z. jin and y .
fang preserving model privacy for machine learning in distributed systems ieee transactions on parallel and distributed systems vol.
no.
pp.
.
p. mohassel and y .
zhang secureml a system for scalable privacypreserving machine learning in ieee symposium on security and privacy sp .
ieee pp.
.
v .
nikolaenko u. weinsberg s. ioannidis m. joye d. boneh and n. taft privacy preserving ridge regression on hundreds of millions of records in ieee symposium on security and privacy .
ieee pp.
.
b. mcmahan e. moore d. ramage s. hampson and b. a. y arcas communication efficient learning of deep networks from decentralized data in artificial intelligence and statistics .
pmlr pp.
.
s. hardy w. henecka h. ivey law r. nock g. patrini g. smith and b. thorne private federated learning on vertically partitioned data via entity resolution and additively homomorphic encryption arxiv preprint arxiv .
.
q. yang y .
liu t. chen and y .
tong federated machine learning concept and applications acm transactions on intelligent systems and technology tist vol.
no.
pp.
.
f. mo a. s. shamsabadi k. katevas s. demetriou i. leontiadis a. cavallaro and h. haddadi darknetz towards model privacy at the edge using trusted execution environments in proceedings of the 18th international conference on mobile systems applications and services pp.
.
l. hanzlik y .
zhang k. grosse a. salem m. augustin m. backes and m. fritz mlcapsule guarded offline deployment of machine learning as a service in proceedings of the ieee cvf conference on computer vision and pattern recognition pp.
.
z. ding y .
wang g. wang d. zhang and d. kifer detecting violations of differential privacy in proceedings of the acm sigsac conference on computer and communications security pp.
.
y .
wang z. ding d. kifer and d. zhang checkdp an automated and integrated approach for proving differential privacy or finding precise counterexamples in proceedings of the acm sigsac conference on computer and communications security pp.
.
h. zhang e. roth a. haeberlen b. c. pierce and a. roth testing differential privacy with dual interpreters arxiv preprint arxiv .
.
b. jayaraman and d. evans evaluating differentially private machine learning in practice in 28th usenix security symposium usenix security pp.
.
q. pang y .
yuan and s. wang mpcdiff testing and repairing mpchardened deep learning models.
ndss .
y .
li d. xiao z. liu q. pang and s. wang metamorphic testing of secure multi party computation mpc compilers.
esec fse .
r. yarlykov solving scalability with zksync a blockchain review the scalability problem a review of the zksync blockchain .
s. pailoor y .
chen f. wang c. rodr guez j. van geffen j. morton m. chu b. gu y .
feng and i. dillig automated detection of underconstrained circuits in zero knowledge proofs proceedings of the acm on programming languages vol.
no.
pldi pp.
.
h. wen j. stephens y .
chen k. ferles s. pailoor k. charbonnet i. dillig and y .
feng practical security analysis of zero knowledge proof circuits in 33rd usenix security symposium usenix security pp.
.
j. liu i. kretz h. liu b. tan j. wang y .
sun l. pearson a. miltner i. dillig and y .
feng certifying zero knowledge circuits with refinement types in ieee symposium on security and privacy sp .
ieee pp.
.
d. xiao z. liu y .
peng and s. wang mtzk testing and exploring bugs in zero knowledge zk compilers in ndss .
j. kim r. feldt and s. yoo guiding deep learning system testing using surprise adequacy in ieee acm 41st international conference on software engineering icse .
ieee pp.
.
z. ji p. ma y .
yuan and s. wang cc causality aware coverage criterion for deep neural networks in ieee acm 45th international conference on software engineering icse .
ieee pp.
.
y .
yuan q. pang and s. wang revisiting neuron coverage for dnn testing a layer wise and distribution aware criterion in ieee acm 45th international conference on software engineering icse .
ieee pp.
.
d. xiao z. liu y .
yuan q. pang and s. wang metamorphic testing of deep learning compilers proceedings of the acm on measurement and analysis of computing systems vol.
no.
pp.
.
q. shen h. ma j. chen y .
tian s. c. cheung and x. chen a comprehensive study of deep learning compiler bugs in proceedings of the 29th acm joint meeting on european software engineering conference and symposium on the foundations of software engineering pp.
.
h. v .
pham t. lutellier w. qi and l. tan cradle cross backend validation to detect and localize bugs in deep learning libraries in ieee acm 41st international conference on software engineering icse .
ieee pp.
.
h. ma q. shen y .
tian j. chen and s. c. cheung fuzzing deep learning compilers with hirgen in proceedings of the 32nd acm sigsoft international symposium on software testing and analysis pp.
.