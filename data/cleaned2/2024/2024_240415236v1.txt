revisiting unnaturalness for automated program repair in the era of large language models aidan z.h.
yang sophia kolak vincent j. hellendoorn ruben martins claire le goues carnegie mellon university pittsburgh united states email aidan cmu.edu sdkolak andrew.cmu.edu vhellendoorn cmu.edu rubenm cs.cmu.edu clegoues cs.cmu.edu abstract language models have improved by orders of magnitude with the recent emergence of transformer based large language models llms .
llms have demonstrated their ability to generate natural code that is highly similar to code written by professional developers.
one intermediate value an llm can emit is entropy which measures the naturalness of a token of code.
we hypothesize that entropy can be used to improve the performance of automated program repair apr tasks.
while much progress has been made in automated program repair apr fault localization techniques suffer from a lack of diversity in ranking scores patch generation tools tend to be inefficient as all tests need to run before determining if a patch is likely to be correct and patch ranking often suffers from the test suite over fitting problem.
however using an llm directly for apr introduces concerns for training data leakage.
in this work we introduce a novel way of using the entropy of llms in combination with prior apr tools to improve all stages of apr.
by using only the prefix and suffix context of a line or block of code to describe naturalness we can use llms to localize faults and rank patches all while eliminating the dependency for test suites.
we show that entropy is highly complementary with prior fault localization tools.
our proposed re ranking method achieves a top score improvement over sbfl.
we propose a patch naturalness measurement entropy delta to improve the efficiency of template based repair techniques by ranking plausible patches before undergoing testing.
when using entropy delta for patch ranking and classification our proposed method can rank correct patches more effectively than stateof the art machine learning tools with an improvement in top .
our work suggests that llms can be an effective addition to compliment prior apr tasks while minimizing both the testsuite overfitting problem and the llm data leakage problem.
i. i ntroduction the problem of software quality has motivated the development of a variety of techniques for automatic program repair apr .
at a high level dynamic apr approaches use test cases to define a defect to be repaired and functionality to retain and to localize the defect to a smaller set of program lines.
apr techniques generate candidate patches in a variety of ways such as by heuristically instantiating pre defined repair template or by customizing symbolic techniques to synthesize new code .
meanwhile the recent advances in machine learning and ai including but by no means limited to advances in transformer based language models have produced ordersof magnitude performance improvements over previous ml techniques for code generation .
ml therefore affords promising opportunities for program repair and fault localization .
the applicability of language models to the repair process makes sense these models are trained on large volumes of code in which defects are relatively rare.
since their training objective encourages next token prediction well trained language models tend to simultaneously perceive faulty code as unlikely or unnatural and to produce code that is correct as correct code is more natural .
the naturalness of code and unnaturalness of buggy code is now a well established phenomenon .
however the bulk of prior research on this topic relied on relatively simple n gram language models .
compared to present day llms these models provided a very poor estimator of code predictability.
the unnaturalness of buggy lines was therefore mainly useful as an explanatory metric but showed limited utility for precisely localizing defects let alone repairing programs.
the recent advancement of much larger and more sophisticated llms have decreased model perplexities by multiple orders of magnitude.
this makes them a much more accurate adjunct both for estimating naturalness and for fault localization or correct patch identification .
in this paper we revisit the idea of un naturalness for program repair.
the fundamental idea behind using an lm alone even a hypothetically optimal one for repair treats predictability as ultimately equivalent to correctness.
this assumption is specious llms adopt preferences based on a corpus with respect to training loss that rewards imitation.
beyond the fact that llms necessarily train on buggy code llms generate and score text one token at the time.
given that they may well prefer a subtly incorrect implementation spread across several readable lines over a correct but difficultto understand one line solution as the per token surprisal of the former may be strictly lower than the latter.
judgement of code correctness requires substantially more context than an llm has access to including but not limited to test cases test behavior and developer intent.
although some of this information could be provided as context it will lie outside the training distribution.
this implies that llms can only go so far on their own in reasoning about and fixing buggy code.
it moreover motivatesarxiv .15236v1 apr 2024the use of traditional tools which compress such information as a complement to llms in repair which has indeed shown promising recent results for the patch generation stage in particular acknowledging the risk of training data leakage in any such experiment .
we go beyond prior work by interrogating the role of entropy as a complement to traditional repair at every stage fault localization fl .
end to end dynamic apr relies on fault localization to narrow a bug to a smaller set of source locations.
improving fault localization accuracy is key to improving repair efficiency .
although fl accuracy is improving both commonly used and state of the art ml based techniques still suffer from the tendency to assign the same fl score to large amounts of code.
for example we find that ochiai sbfl assigns the same suspiciousness score to lines of code in the dataset defects4j an average of .
ties per bug and transferfl assigns the same suspiciousness score to lines of code in the same dataset an average of .
ties per bug .
we show that by incorporating entropy into fault localization the variance of suspicious scores increase by for ochiai sbfl and overall accuracy increases as well e.g.
the top score improves from to .
plausible patch generation.
apr approaches typically generate multiple potential code changes in search of plausible patches that cause the program to pass all tests.
executing tests and to some extent compiling programs to be tested dominates repair time the template based approach tbar spends about of its total time creating patch templates generating patches from templates and running tests on generated patches.
regardless of the patch generation method e.g.
symbolic techniques template instantiation or machine learning models repair efficiency is best approximated in terms of the number of patches that must be evaluated to find a good repair .
we show that entropy when used to order candidate patches for evaluation can improve the efficiency of generic templatebased repair by tested patches per bug on average.
patch correctness assessment.
plausible patches are not always correct in that they can fail to generalize to the desired behavior beyond what is tested in the provided test suite .
some recent work aims to address this in the form of a post processing step that identifies and filters plausible butincorrect patches typically by combining program analysis and machine learning .
however techniques to date are typically trained on the same test suites used for patch generation imposing a project specific training burden and an expensive one when dynamic signals are required and posing a significant risk of overfitting .
we show that entropy can rank correct patches more effectively in top than state of the art patch ranker shibboleth without using any project specific training data.
in summary we make the following contributions.
end to end entropy apr.
we propose a technique that uses a combination of an llm s inherent ability to detect4473 xyitemrenderer r getrendererfordataset d sbfl .
e .
... collection c getrenderer .getannotations sbfl .
e .
if r !
null collection c r.getannotations e .
... a chart buggy code and developer fix.
collection c getrenderer .getannotations e .
if r null return null e .
b chart buggy code and test failing tbar patch .
collection c getrenderer .getannotations e .
if r null continue e .
c chart buggy code and test passing tbar patch .
fig.
chart bug from defects4j with its developer written fix a test failing patch generated by tbar and a test passing patch generated by tbar .
we show the incoder produced entropy of code in each patch.
naturalness i.e.
entropy and an llm s generation ability to predict faulty lines rank untested patches and classify tested but potentially incorrect patches.
entropy delta for efficient template based patch generation.
we introduce entropy delta as a patchnaturalness measure that can rank patches before running tests.
we show that entropy delta can be used to immediately filter out test failing patches and on average reduce running tests for patches for each bug in our dataset.
we combine entropy delta patch ranking with a prior template based program repair technique tbar and release a more efficient version of tbar for future research.
artifact availability .
our data tool and results are available and will be released as open source.
ii.
i llustrative example consider the buggy and fixed versions of chart from defects4j shown in figure 1a.
the original buggy code is missing a null check which the developer fixed by adding if r !
null around the implicated code at line .
thetbar template based program repair technique produces candidate patches by repeatedly instantiating applicable templates at program statements ordered by ochiai sbfl suspiciousness score.
for example figure 1b a tbar generated patch that does not cause the tests to pass and so is discarded the search continues.
given chart s associated test suite the ochiai sbfl approach assigns line the highest suspicious score in chart of .
line a suspiciousness score of .
and .
to lines and onward.
using only sbfl for fault localization ranking the actual faulty line at line is ranked as 10th most suspicious.
this does not prevent tbar from considering it but does cost time.
tbar can produce patches that pass all tests for this bug such as the one shown in figure 1c.
in the interest of reasoning about efficiency we hold fault localization constant given that this is the 19th patch attempted.
however although this patch prevents the null pointer exception it does not generalize beyond the provided tests to capture the apparent developer intent.
importantly tbar can produce the correct patch from figure 1a if configured to execute beyond the first testpassing patch found it is the 70th patch attempted but only the second that passes all tests.
llm based entropy provides useful clues here however.
first consider fault location we use incoder 3to measure the entropy of every line in this file.
rank ordering them line is ranked 8th most surprising.
this is better than the sbfl technique on face.
however their real utility appears to lie in combination re ranking the lines receiving the top sbfl suspicious scores by incoder entropy values puts line at rank .
we investigate how entropy in conjunction with sbfl performs for fault localization across multiple bugs and projects as well as how different llms affect its performance.
we can also measure the naturalness of generated patches such as by calculating the change in entropy which we call entropy delta e between the original buggy line of code and the proposed patches.
the efor the test failing patch is .
for the test passing but still incorrect patch is .
and for the correct patch is .
.
the entropy delta scores do not perfectly predict behavior note that the test failing patch has a higher score than the test passing but incorrect patch but it still suggests entropy delta can improve efficiency by suggesting the order to test patches.
test execution time is the dominant cost in program repair.
by using entropy to rank potential patches before testing them to suggest the order in which to do so both test passing patches can be found within attempts improving on to the first test passing in the default mode and to find the second correct patch .
entropy delta can potentially help disambiguate plausible but incorrect from genuinely correct patches.
we evaluate these relationships in detail in the rest of this work showing how entropy can usefully complement traditional approaches to automatic localization and transformation in the context of program repair.iii.
a pproach we ask and answer the following three research questions about the utility of llm entropy for apr.
rq1 how can entropy improve fault localization?
we perform an empirical evaluation of prior state of the art fault localization tools and observe whether and how they benefit from the use of entropy scores.
rq2 how can entropy improve patch generation efficiency?
to measure how entropy can be used for patch generation efficiency we use it to rank proposed patches generated by an apr technique before running tests.
rq3 how well does entropy deltas identify correct patches?
we investigate if entropy delta can differentiate plausible patches patches that passes all tests and correct patches patches that correctly fix the bug .
this section describes how we use entropy for fault localization section iii a our development of entropy delta for evaluating patches section iii b and our modifications to tbar to enable our study of improved patch efficiency section iii c .
the next section describes datasets and metrics.
a. entropy for fault localization we integrate raw entropy scores into prior fault localization techniques.
figure overviews the approach.
we take suspiciousness scores provided by a given prior fl tool for a given file.
for consistency with prior studies of fault localization we focus on top n identified lines as developers do not typically inspect more than candidates .
we choose and for n and name the two approaches filter and filter as these are two quantities near the top that can still significantly impact top scores.
we then query an llm for entropy scores for each line of code in that file.
we tokenize the entire file iteratively masking each line and querying the model for each line s entropy.
we used a sliding context window with tokens i.e.
the maximum attention window of our smallest selected llm surrounding the mask as suffix and prefix context.
this allows us to assign entropy based scores for all code in a file even those longer than a given llm s context window.
we then re rank the suspiciousness code identified by sbfl by entropy score and validate the ranked list according to the actual fault location in the dataset.
we incorporate entropy into three previous fl techniques sbfl using the ochiai formula transferfl and llmao .
ochiai is a common formula in sbfl used in traditional apr practices e.g.
tbar .
transferfl and llmao are the current state of the art fl techniques using transfer learning and llms respectively.
2using sbfl fault localization tbar produces these patches at and respectively.
3when prompted with the code and asked to fix the bug directly incoder does not produce a test passing patch in few shot setting.
note that gpt4 fixes the bug correctly and reports the git commit associated with the fix implicating data leakage.input for prior fl entropy source code test results .
prior fl suspicious lines .
llm e line a gen0 ...genn e line b gen0 ...genn ... e line x gen0 ...genn line a line b ... line x entropy ranked lines generations .
patch generation patch ranking input for entropy direct source code fig.
fault localization pipeline using entropy.
we take a prior fl suspicious score list query each code line for entropy values and re rank the list using llm entropy scores.
generation query prefix prefix suffix suffix a an example of entropy delta query from a code line deletion patch.
the entropy delta value of the deleted line is the difference between the original line and a blank line.
b an example of entropy delta query from a code line replacement patch.
the entropy delta value of the replaced line is the difference between the original line and the replacement line.
fig.
example entropy delta queries from an llm.
the mask tokens enable models to learn the contextual relationships between tokens and make entropy predictions for missing new or replacement tokens.
the eom token is a special token that indicates the end of a mask.b.
entropy delta to evaluate patch naturalness which we use in both patch prioritization during generation evaluation and patch correctness prediction we introduce the concept of an entropydelta .
entropy delta describes how code replacement changes the naturalness of a block of code.
figure 3a and figure 3b give examples for our usage of entropy delta for assigning a ranking score for patches.
figure 3a shows the process of masking out a deleted line of code and querying the llm for the change in entropy using that mask i.e.
the change in entropy without the original line .
figure 3b shows the process of querying the llm for the change in entropy if the tokens of the original line of code is replaced with new tokens of patch code.
if the patch is an insertion of a blank new line we query the entropy delta between the newline token and the original line of code.
for the case of an insertion we measure the entropy delta between the new code line and the original blank line.
an entropy delta is simply the change in entropy before and after a line in code is replaced.
for instance if the line s original entropy is .
and the replacement line s entropy is .
then the line has an entropy delta of .
as in replacing that line lowered entropy by .
.
a significant reduction in entropy large positive entropy delta means that the replacement code lowered the entropy implying both that the original statement may have been buggy and that the patch is more natural for that region of code.
a large negative entropy delta means that the replacement code increased entropy meaning that the patch is less natural at that location.
an entropy delta of means that the patch has the exact same naturalness as the original code.
c. modified tbar our patch efficiency experiments ask how entropy can speed up patch generation and evaluation.
we evaluate itin context of tbar the best performing template based program repair technique in the existing literature.
we avoid using ml based apr techniques even though some may outperform tbar because our goal is a controlled evaluation of entropy without learned patterns from the test suite.
evaluating based on a technique that otherwise also relies on trained ml models fails to isolate the effect of entropy per se.
tbar is a template based patch generation technique integrated with defects4j v1.
.
our experiments require several modifications to the codebase.
first we enable tbar to continue seeking patches after the first test patching patch is found.
second we enable tbar to generate patches or evaluate them in a customized order such as one provided by an entropy delta ranking .
our tbar extension also includes some refactoring for modifiability extensibility as well as a more accurate patch caching mechanism caching the patched source code rather than the patch alone .
we provide the modified code with our replication package.
iv.
d atasets and metrics in this section we describe the models we use for entropy section iv a the bug and patch datasets considered section iv b as well as evaluation metrics section iv c .
a. llms we used incoder starcoder and code llama2 .
the three llms were trained on open source code and are capable of infilling with bidirectional context.
the incoder model is a large scale decoder only transformer model with .
billion parameters.
the model was trained on a dataset of code from public open source repositories on github and gitlab as well as from stackoverflow.
incoder was primarily trained for code infilling which involves the insertion of missing code snippets in existing code using a causal masked objective during training.
however its versatility enables it to be utilized in a variety of software engineering tasks including automatic program repair.
starcoder and llama were trained with a similar autoregressive plus causal mask objective as incoder.
starcoder was trained with .
billion parameters.
code llama2 have three versions available 7b 13b and 34b.
we choose the 7b version as it is the closest in size to the other two models.
although the three llms were not specifically trained for repair their large architectures and training objectives could imply that their entropy values on a particular region of code could suggest code naturalness.
for all experiments we set the llm temperature to .
.
b. dataset we use the defects4j dataset as the basis of our experiments.
defects4j is a well established set of documented historical bugs in java programs with associated tests and developer patches.
it is commonly used in apr testing and fault localization research.
however each research question requires a different subset of the data.
table i shows the number of bugs in each project that have at least one patchtable i defects4j bugs with at least one patch passing tests rq2 efficiency and a developer fix rq3 patch correctness .
defects4j v1.
bugs defects4j v2.
bugs patch efficiency rq2 patch correctness rq3 incl.
total incl.
total chart closure lang math mockito time total passing tests for analyzing patch efficiency and a developer fix for analyzing patch correctness along with plausible but incorrect patches.
in total we analyze bugs from defects4j v1.
for patch efficiency and bugs from defects4j v2.
for patch correctness.
we used defects4j v1.
for the fault localization and patch generation experiments.
we do this because off the shelf tbar as well as prior fault localization tools replication packages are all only compatible with defects4j v1.
.
the fault localization experiments consider all bugs in defects4j v1.
.
we choose not to use defects4j v2.
for fault localization because prior tools replication packages are only compatible with defects4j v1.
.
for patch generation the goal is to evaluate the degree to which entropy can improve repair efficiency we therefore focus on the subset of defects4j v1.
bugs on which vanilla tbar succeeds at least once.
for patch correctness ranking we use curated datasets from prior tools replication packages directly namely shibboleth and panther .
shibboleth and panther are both tools that leverage static and dynamic heuristics from both test and source code to rank and classify plausible patches built on top of the updated defects4j v2.
dataset.
we use a dataset of plausible patches on defects4j v2.
curated by ghanbari et al.
.
for patch classification we use a dataset of plausible patches on defects4j v2.
curated by tian et al.
.
the patches from tian et al.
were generated by seven different apr techniques.
each bug in the data set has one correct patch and several plausible i.e.
test passing but incorrect ones.
we calculate the change in entropy between the section of code in the original buggy file and the patched version.
note that both datasets only contain patches in projects chart closure lang math mockito and time of defects4j v2.
s total projects to compare with prior work built on defects4j v1.
.
instead of the total number of bugs in defects4j v2.
we only consider the bugs in the projects included by shibboleth and panther shown in table i .
c. metrics fault localization and patch ranking.
we measure the effectiveness of both fault localization ranking and patch ranking by counting the number of correct faults or patches that appear intable ii top n scores on bugs from defects4j v1.
.
from prior tools and re ranking with entropy from three pre trained llms incoder 6b llama 7b and starcoder .5b fl type re rank filtertechnique top top top entropy entropy llama2 entropy starcoder entropy incoder sbflsbfl filterentropy llama2 entropy starcoder entropy incoder filterentropy llama2 entropy starcoder entropy incoder transferfltransferfl filterentropy llama2 entropy starcoder entropy incoder filterentropy llama2 entropy starcoder entropy incoder llmaollmao filterentropy llama2 entropy starcoder entropy incoder filterentropy llama2 entropy starcoder entropy incoder the top n position.
the top n measure has been widely used in apr research .
existing studies showed that over of developers inspect only the top suggested elements.
we use top top and top for fault localization ranking.
we only use top and top for patch ranking following ghanbari et al.
as some bugs in our dataset only have plausible patches available.
patch generation efficiency.
we measure the effect of reranking generated potential patches in terms of the number of patch evaluations saved by so.
patch evaluations are established as a hardware and program independent measure for apr efficiency and a proxy for compute time.
patch correctness.
for patch classification tasks we convert entropy delta values into binary labels.
we label patches with a positive entropy delta as more natural i.e.
more likely to be correct and patches with a negative entropy delta less natural i.e.
less likely to be correct .
to measure entropy s ability to isolate correct and incorrect patches we use recall and recall.
recall measures to what extent correct patches are identified while recall measures to what extent incorrect patches are filtered out.
we use accuracy precision and f1 scores to assess classification effectiveness over the entire dataset.
v. r esults in this section we present results on the performance of entropy and entropy delta on our three research questions rq1 can entropy improve fault localization?
rq2 can entropy improve patch generation efficiency?
rq3 how well does entropy deltas identify correct patches?
rq1 can entropy improve fault localization?
in this research question we compared different configurations for fault localization.
our analysis aims to determine the most effective approach for identifying the buggy statement in a series of one line bugs.
we first measure entropy directly for fault localization with our three selected llms codellama2 starcoder and incoder.
we then measure the fault localization accuracy of three prior fault localization tools sbfl transferfl and llmao .
finally we use entropy to re rank prior fault localization tools and observe that entropy re ranking largely improves prior tools.
table ii shows the top n scores n on all configurations of our experiment.
we observe that the entropy of incoder the smallest llm in our lineup is the most effective for fault localization.
this is consistent with results from xia et al.
who found that incoder trained with an objective of predicting missing code from a bidirectional context is more effective at program repair tasks than larger but purely causal generative llms.
sbfl.
from table ii we observe an overall decrease in top n scores using either code llama2 or starcoder entropy with a filter.
however all top n scores improve with the filter.
in particular the top score of for llama2 entropy improves upon sbfl by and the top score of for starcoder entropy improves upon sbfl by .
using incoder entropy to re rank sbfl shows substantial improvements across all top n and the two types of filters.
incoder entropy sbfl with a filter achieves a top score of improvement and filter achieves a top score of improvement .
similarly the top and top scores improve by and respectively for incoder entropy sbfl with a filter.
the top and top scores improve by and respectively for the filter.
transferfl.
as seen in table ii we observe an improvement from entropy on transferfl s top and top scores using a filter.
in particular filter incoder entropy with transferfl top is improvement and filter llama2 entropy with transferfl top is improvement .
however filter incoder entropy with transferfl yields a top score of which is a decrease in performance than transferfl by itself.
as compared to state of the art machine learning based fl techniques we observe that entropy scores perform worse on top .
llmao.
similar to the results of transferfl re ranking with entropy only improves fault localization results using the filter.
furthermore only entropy calculated using incoder shows an improvement over llmao alone for top and top with a and improvement respectively.
since llmao is already an llm based fl tool llm entropy reranking shows marginal improvements as compared to prior non llm based fl tools.
llmao finetunes on codegen 16b which is a larger llm than our three chosen llms.table iii entropy delta ranking scores of plausible patches generated by tbar perdefects4j project.
the mean rank decrease is and the median is .
project improves ranking lowers ranking chart closure lang math mockito time total chart closure lang math mockito time020406080100patch ranking ranking method t bar e delta fig.
entropy delta and tbar ranking lower is better of test passing patches on defects4j bugs.
our results indicated that sbfl benefits the most with incoder s entropy re ranking.
since sbfl has the most amount of tied suspicious scores .
ties per bug on average the additional suspiciousness from entropy values helps to break ties.
transferfl and llmao benefit from entropy re ranking mostly when using a filter.
these findings suggest that incorporating entropy as a heuristic in fault localization can improve the accuracy of identifying the buggy statement particularly when used in conjunction with sbfl.
rq1 summary we leverage entropy for fault localization in defects4j programs and show that while entropy alone is only somewhat useful for finding defective lines the measure is highly complementary when combined with prior fault localization tools which highlights the importance of combining llmbased methods with techniques from prior apr approaches.
rq2 can entropy improve patch generation efficiency?
in this section we discuss the observed relationship of entropy and test passing patches.
we use entropy from incoder the most successful llm in rq1 s fault localization.
we measure the impact of entropy delta on patch generation efficiency with two methods measuring each successful test passing patch s ranking as ranked by original tbar and chart closure lang math mockito time0255075100125150175200median number of patches triedentropy reranking tbarfig.
median number of patches tested lower is better per project before succesful patch using tbar original ranking and entropy delta re ranking of test passing patches on defects4j bugs.
entropy delta re ranked tbar and incorporating entropydelta into tbar and measuring the total number of patches generated to pass all tests.
we first configured tbar to generate only patches per each defects4j bug assuming perfect fault localization.
of the tbar patches we generated passed all tests contained in their bugs respective repositories e.g.
all tests written for project chart .
finally we calculated the entropy delta score for each patch and the test passing patch s original ranking according to tbar .
as seen in table iii entropy delta improves out of the rankings as compared to tbar s original ranking.
on average we observed a mean rank decrease of meaning that using entropy delta to rank the generated tbar patches can reduce a mean of full test iterations i.e.
each potential patch must run through all test cases in the repository before knowing if it is a plausible patch .
liu et al.
compared apr techniques and found that tbar exhibits one of the highest number of patches generated but also the highest rate of bug fixing across defects4j .
we posit that entropy delta s efficiency improvement over tbar significantly boosts template based apr s overall utility.
figure compares the tbar ranking and entropy delta ranking.
each bar represents the rank of test passing patches compared to all generated patches per defects4j project.
a lower rank signifies a more efficient repair process as the repair process ends when a test passing patch is found.
as seen in figure tbar s original ranking for test passing patches is higher than entropy delta s ranking across all projects.
entropy delta shows a higher disparity on ranking between test passing and test failing patches i.e.
a lower median rank for all test passing patches .
in particular patches from projects chart and time show the largest improvement from re ranking patches with entropy delta.
successful patches in chart and time typically require multi line edits and with a wider rangechart lang mockito time closure math10 0510entropy delta correct patch false truefig.
entropy delta across correct and incorrect patches on defects4j projects.
a higher entropy delta signifies a less surprising patch to the llm and a lower entropy sometimes negative entropy delta signifies a more surprising patch to the llm.
of templates to choose from entropy delta can make a greater impact in reducing the number of patches tested.
we then configured tbar to use entropy delta ranked patches directly and measured the total number of patches required until a successful bug fix i.e.
passing all tests .
figure shows the median number of patches tested per project before a successful patch using tbar original ranking and entropy delta re ranking.
we observe that entropy delta reranking reduces the median number of patches tested across all projects except for mockito.
mockito has only three single line bugs that tbar can fix.
with a smaller total number of patches to try on a single template e.g.
total possible patches for mockito entropy delta re ranking does not have as large of an impact on apr efficiency.
rq2 summary we show that entropy can be used to rank patches before going through the entire test suite thereby reducing the test overhead for template based repair technique tbar by a mean of patches tested.
entropy delta can both reduce the median number of patches tried before finding a fix and consistently rank test patching patches higher than testfailing patches without any dependency on the test suite.
entropy delta is most useful for bugs that require multi line patches.
rq3 how well does entropy deltas identify correct patches?
in rq2 we saw that entropy delta can improve the efficiency of patch generation by reducing number of patches tested.
however it is important to note that a test passing patch is not necessarily correct.
to further explore the issue of correctness we investigated the ability of entropy deltasto distinguish between correct and incorrect patches both of which are test passing.
patch ranking we evaluate a dataset of patches generated by prior apr methods collected by ghanbari et al.
.
for each bug the data set includes some number of plausible i.e.
test passing patches where exactly one is correct and the rest are incorrect.
we attempt to isolate the true correct patch from the incorrect patches.
we then rank each patch according to its entropy delta querying the model for the entropy of the entire patch region before and after the replacement.
table iv shows the top and top results of our approach on the labeled dataset of patches.
we see from table iv that entropy delta outperforms both sbfl and shibboleth on top across all projects and entropy delta outperforms shibboleth on top across all projects but chart top as compared to shibboleth s top .
overall we see that entropy delta improves upon shibboleth by for top and for top .
the difference in entropy reduction between correct and plausible but incorrect patches is shown in greater detail in figure .
we see a clear difference in entropy delta across correct and incorrect patches.
in particular the correct patches for all six projects have a median entropy delta value of above and the incorrect patches for all six projects have a median entropy delta value of below .
a correct patch tends to appear more natural to the llm as compared to its original buggy line.
patch classification table v shows our classification results on a labeled dataset of plausible patches curated by tian et al.
for classifying patches as correct or incorrect.
entropy delta improves upon the accuracy score of patch sim and panther but only slightly improves recall score over both patch sim and panther.
for recall entropy delta performs better than patch sim by but performs worse than panther by .
entropy delta slightly improves accuracy over panther by .
and over patch sim.
entropy delta improves precision over panther by and patch sim by .
finally entropy delta performs better than both patch sim and panther on f1 score by and respectively.
as compared to the state of the art entropy improves classification performance on true positives more than true negatives.
our analysis focused on comparing the degree of entropy reduction between true correct patches and plausible test passing patches.
as shown in table iv table v and figure our results suggest that correct patches tend to lower entropy i.e.
increase naturalness more than incorrect patches.
specifically entropy delta ranks more correct patches in the top than the state of the art patch ranker shibboleth and entropydelta can classify correct patches with an higher precision than the state of the art patch classifier panther.
these findings suggest that entropy deltas can be a valuable heuristic for distinguishing between correct and incorrect patches.table iv ranking results of plausible patches per defects4j project using ranking methods sbfl shibboleth and entropy delta project patches correct incorrect top n sbfl shibboleth entropy delta chart 182top top closure 205top top lang 185top top math 474top top mockito 1top top time 46top top total 1093top top table v classification scores of plausible patches on defects4j projects using classification methods patch sim panther and entropy delta score patch sim panther entropy delta accuracy .
.
.
precision .
.
.
recall .
.
.
recall .
.
.
f1 .
.
.
rq3 summary the entropy delta from an llm distinguishes between correct and plausible test passing but incorrect patches with higher precision and accuracy than state of the art patch disambiguation tools.
vi.
r elated work we discuss in the following sections the most recent advances in llm for code fault localization and patch ranking.
a. llm for code language models have been used for code generation bug detection and patch generation.
recent language models finetune on code as training data and can perform code completion and generate code based on natural language with impressive results.
large language models llms such as codex gpt neo and llama2 have raised performance on these tasks by using more trainable parameters and training data.
ray et al.
study the relationship between bugginess and llm entropy.
ray et al.
empirically showed that n gram models trained over a large corpus of code will find buggy statements more surprising as indicated by a high entropy score.
kolak et al.
revisit the question of naturalness i.e.
the humanreadability of patches in the era of large language models.
kolak et al.
experimented with models ranging from 160m to 12b parameters and measured the similarly between llm generated patches and developer written patches.
their resultsshow that larger models tend to generate test passing lines at a higher rate.
additionally llm generated patches tend to be more similar to the human written patch as model size increases.
xia et al.
directly applied llms for aprs and found that llms can suggest multi line fixes with higher accuracy than state of the art apr tools.
our study performs an empirical evaluation of how code naturalness i.e.
entropy can improve prior apr tools across three different stages of automated program repair fault localization patch generation and patch ranking.
b. fault localization prior fault localization tools use test output information code semantics and naturalness of code to achieve a high degree of confidence on bug detection.
spectrum based fault localization sbfl uses a ratio of passed and failed tests covering each line of code to calculate its suspiciousness score in which a higher suspiciousness signifies a higher probability of being faulty.
recent advances in deep learning created a spur of research on using graph neural networks gnns for fault localization.
grace deepfl and dear encode the code ast and test coverage as graph representations before training deep learning models for fault localization.
transferfl combined semantic features of code and the transferred knowledge from open source code data to improve the accuracy of prior deep learning fault localization tools.
llmao finetuned a light weight bidirectional layer on top of code tuned llms to show that llms can detect both bugs and security vulnerabilities without the use of test cases.
our work builds on top of the topperforming prior fault localization tools and show that entropy can be used as a light weight re ranking tool that improves fault localization scores without a dependency on test cases.
c. patch correctness similarly to prior fault localization tools prior patch disambiguation tools leverage test output information and code information both code syntax and code semantics for rankingor classifying patches.
qi et al.
analyzed the reported bugs of three generate and validate apr tools genprog rsrepair and ae systems to find that producing correct results on a validation test suite is not enough to ensure patch correctness.
smith et al.
performed an experiment that interrogates whether or not automatically generated patches are prone to overfitting to their test suite.
borrowing the concept of training and test sets from machine learning they found that automated program repair apr typically used the same test suite for both training generating the patch and testing validation .
smith et al.
found that both the coverage rate of the test suite as well as the assignment of test train sets between the two suites impact the degree of overfitting in repair.
to counteract the overfitting problem ye et al.
proposed ods overfitting detection system a novel system to statically classify overfitting patches.
xiong et al.
generated both execution traces of patched programs and new tests to assess the correctness of patches.
ghanbari et al.
used both the syntactic and semantic similarity between original code and proposed patch and code coverage of passing tests to rank patches.
shibboleth was able to rank the correct patch in top and top positions in of their curated dataset.
tian et al.
proposed machine learning predictor with bert transformer based learned embeddings for patch classification.
tian et al.
found that learned embeddings of code fragments with bert cc2vec and doc2vec yield similarity scores that given a buggy code substantially differ between correctly patched code and incorrectly patched one.
the most relevant work to our study of patch correctness is yang et al.
.
yang et al.
found that state of the art learning based techniques suffered from the dataset overfitting problem and that naturalness based techniques outperformed traditional static techniques in particular patch sim .
our work uses plausible patches collected in past the llm training data cutoff of all our chosen llms which lowers the risk of llm training data leakage.
our work performs an empirical study on entropy against the most recent state of the art patch disambiguation techniques panther and shibboleth on top of patch sim .
motivated by liu et al.
our work is the first to use llm entropy on plausible patches before undergoing testing to achieve more efficient apr on prior template based techniques.
finally we introduce a new naturalness measurement for patches entropydelta which achieves state of the art results for plausible patch disambiguation without depending on the test suites of buggy programs which lowers the risk of dataset overfitting.
vii.
t hreats external validity.
a threat to the external validity of our study is the potential selection bias of our three selected llms.
we chose a representative set of llms with a range of trainable parameters.
we chose the three based on their infill ability and built in bidirectional attention mechanism.
much larger llms 20billion parameters might have a stronger ability to reason over faulty code lines and patches but require much larger computation power and time for entropy calculation.
another threat to external validity is our usage of defects4j data throughout our empirical evaluation.
we chose defects4j for our target bugs for fault localization and patch disambiguation due to the data available and the aim to compare against related work in apr.
data leakage of defects4j as training data for our selected llms is possible.
we mitigate this risk by using entropy in combination with prior apr techniques instead of direct llm prompting for patch generation and applying entropy delta on untested or plausible patches that are not available online i.e.
recently generated and not used as an official patch for bug fixing .
internal validity.
an internal validity is the manual labeling of plausible patches.
we used manually labeled data released by prior works in which the authors followed clear and reproducible decision criteria.
although mistakes could still be made on which plausible patches are correct or incorrect we use the same labeling for all prior tools studied as well as entropy to create a standardized baseline on patch classification.
construct validity.
one source of construct validity is the measurements we chose for our empirical evaluation.
we used top n as a ranking measurement for both bugs and patches following prior apr work.
to overcome some limitations of top n we also use multiple patch classification measurements accuracy precision recall and f1 on a separate set of labeled patch data to strengthen generalizability.
viii.
c onclusion in this work we propose the use of unnaturalness of code for automated program repair through the measurement of entropy generated by code tuned llms.
we also introduce the term entropy delta which measures the difference in entropy between a proposed code insert i.e.
a patch and the original code.
using three llms and three prior fault localization tools we show that entropy can improve top and scores after re ranking the first potential bug localization.
we use entropy delta on untested patches to save an average of test runs per bug for the template based apr technique tbar.
we show that entropy delta can improve upon state of theart patch ranking by for top and classify plausible patches with a higher precision.
our results indicate that llms can be a powerful addition to state of the art apr tools without the dependency on tests and the usage of llm codegeneration.
the reduction in both test suites and llm codegeneration results in the reduction in model over fitting and training data leakage.