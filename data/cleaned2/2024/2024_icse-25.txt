lissa toward generic traceability link recovery through retrieval augmented generation dominik fuch tobias hey jan keim haoyu liu niklas ewald tobias thirolf anne koziolek kastel institute of information security and dependability karlsruhe institute of technology kit karlsruhe germany dominik.fuchss hey jan.keim haoyu.liu koziolek kit.edu niklas.ewald alumni.kit.edu tobias.thirolf student.kit.edu abstract there are a multitude of software artifacts which need to be handled during the development and maintenance of a software system.
these artifacts interrelate in multiple complex ways.
therefore many software engineering tasks are enabled and even empowered by a clear understanding of artifact interrelationships and also by the continued advancement of techniques for automated artifact linking.
however current approaches in automatic traceability link recovery tlr target mostly the links between specific sets of artifacts such as those between requirements and code.
fortunately recent advancements in large language models llms can enable tlr approaches to achieve broad applicability.
still it is a nontrivial problem how to provide the llms with the specific information needed to perform tlr.
in this paper we present lissa a framework that harnesses llm performance and enhances them through retrievalaugmented generation rag .
we empirically evaluate lissa on three different tlr tasks requirements to code documentation to code and architecture documentation to architecture models and we compare our approach to state of the art approaches.
our results show that the rag based approach can significantly outperform the state of the art on the code related tasks.
however further research is required to improve the performance of rag based approaches to be applicable in practice.
index terms traceability link recovery large language models retrieval augmented generation i. i ntroduction in the complex task of software development developers and other stakeholders handle numerous artifacts such as requirements code documentation and models.
consequently success in development depends in part on understanding how software artifacts interrelate.
to deal with the interrelations of these artifacts researchers and practitioners actively investigate the creation and recovery of so called trace links between these artifacts.
traceability link recovery tlr identifies correspondences between elements in artifacts and makes them explicit.
thus tlr helps to reduce the complexity of many development tasks such as change impact analysis bug localization and maintenance .
moreover trace links can also be used to identify inconsistencies between artifacts .
typically automated approaches are task specialized i.e.
they are designed to recover specific types of trace links such as requirements to code documentation to code issues to commit requirements to requirements or architecture documentation to architecture models .
however the approaches performances vary depending on the tasks.
often approaches do not achieve the necessary performance to be used in practice .
large language models llms offer promising capabilities regarding natural language understanding.
furthermore llms have already proven to be effective in a variety of software engineering tasks including code generation code summarization or api recommendation .
in particular llms could lead to a generic approach for tlr.
nevertheless llms face the challenge that they do not know the project and its context.
in particular an llm cannot handle the complete content of a larger software project due to the project s size and limited input lengths.
fortunately the processing can be augmented by retrieval techniques called retrieval augmented generation rag .
here the first step is retrieving relevant artifacts based on information retrieval ir techniques.
the llm then generates the answer using the retrieved artifacts.
in this paper we propose using rag with llms to perform tlr between various artifacts.
our rag based approach for tlr is called lissa linking software system artifacts .
we focus on three different tlr tasks that cover diverse settings to generate insights about the performance of rag based approaches for tlr tracing requirements to code tracing architecture documentation to code and tracing architecture documentation to architecture models.
the example project in figure illustrates these tasks.
the project contains requirements some classes from the source code the component based architecture and a part of the documentation of a system that provides and processes images.
we consider the four artifacts as different sources of information about the system.
the requirements contain the need for a rest api that is implemented in the code asrestfacade .
thus there is this trace link between the requirement and the code file.
additionally the component diagram shows the database component that is realized in code as the package db .
this results in a trace link between the architecture and the code.
lastly the documentation describes the use of a rest interface to process the images.
this defines trace links from the documentation to the code and to thethe system shall use a rest api.the meta data shall be stored in the db.requirements image dbconnection dbdriver connect conn restfacade getimg id int source code package dbcomponent database component image processingarchitecture we use rest to process the images.documentation fig.
.
example project with requirements an architecture diagram natural language documentation and source code image processing component in the architecture.
we select these three different tlr tasks to empirically evaluate lissa on a variety of artifact types and trace links.
moreover these tasks have been actively researched in the past and datasets are available to evaluate tlr performance.
this motivates us to ask rq1 is a generic rag based tlr approach better than task specific state of the art approaches?
moreover as recent research indicates that chain of thought cot prompting can be more effective than other zero shot prompts in certain scenarios we want to further explore how the performance is affected by these llm prompting techniques.
thus we ask rq2 is cot prompting more effective than a simple classification prompt?
rag heavily depends on the ability to retrieve relevant information i.e.
parts of artifacts.
consequently we investigate the impact of different preprocessing techniques for this retrieval and since related research shows that fine grained mappings can improve the performance of tlr tasks we ask rq3 does retrieval and mapping on a fine grained sub artifact level improve the tlr performance compared to mapping on an artifact level?
furthermore we investigate rq4 does rag improve the tlr performance compared to embedding based ir tlr?
we make the following contributions c1 we provide lissa an rag based approach for tlr between different types of software artifacts.
the approach datasets and results are publicly available in our replication package .
c2 we evaluate the performance of lissa on three different tlr tasks and compare it to state of the art approaches.
c3 we investigate the influence of prompt techniques and preprocessing on the performance of lissa.
the remainder of this work is structured as follows in section ii we present related work and discuss the stateof the art for different tlr tasks.
afterward in section iii we present our rag based approach lissa.
in section iv we discuss the evaluation of retrieval augmented llms for tlr and their performance on three established tlr tasks.
afterward we discuss our findings and their implications for practice and research in section v. lastly we conclude the paper in section vi.
ii.
r elated work in this section we focus on the state of the art in the different tlr tasks and discuss the various existing approaches.
we also cover recent advances in using llms for tlr.
a. traceability link recovery in the following we discuss the state of the art approaches for the three types of tlr tasks we consider requirements to code documentation to code and architecture documentation to architecture models.
requirements to code a significant amount of research in tlr focuses on the relationship between requirements and code.
within this context methods from natural language processing serve as fundamental components of various approaches.
notably vector space models latent semantic indexing and latent dirichlet allocation are frequently employed techniques .
these models consider semantic similarity between requirements and code to bridge the semantic gap and recover trace links.
to measure similarity important features such as common terms between artifacts and semantic vectors of artifacts are primarily considered.
however developers might use different terms to refer to the same concept across different artifacts .
this might result in mismatched vocabulary making trace links hard to recover.
to address this problem gao et al.
use co occurring term pairs biterms from both requirements and code to enrich the artifact texts from both sides.
the idea is that biterms indicate the consensus between the artifacts.
hey et al.
show that a more fine grained interpretation of artifacts could improve recovery results.
their approach ftlr uses sentences from requirements and methods from code files as units for the similarity mapping rather than the whole requirements and source code files.
ftlr represents the semantics of the fine grained elements through word embeddings.
moreover they discovered that selecting the appropriate information is crucial for enhancing the performance of the approach.
hey et al.
also show that the relevant information can be derived automatically with an llm based classifier .
the consensus of these approaches is the ambition to semantically understand the artifacts to bridge the semantic gap.
llms promise deep language understanding of the input text making them a plausible alternative for tlr.
another option to bridge the semantic gap is using transitive links between artifacts.
the underlying idea is that intermediate artifacts can find implicit tracing relationships .
nishikawa et al.
generate trace links between two artifact types aand b by utilizing an intermediate artifact type c. they use a vector space model to create trace links between artifacts of types aandcand between artifacts oftypes bandc.
artifacts from aandbthat both trace to the same artifact of type care considered connected.
moran et al.
introduced comet a bayesian inferencebased method for tlr.
the method involves three stages initially combining multiple text similarity metrics to generate preliminary trace links then incorporating developer feedback to refine these links and finally utilizing information from transitive links to enhance the results.
however when there are multiple transitive paths between artifact pairs it remains a challenge how to aggregate them.
this challenge is explored by rodriguez et al.
.
their approach aggregates multiple paths using one of three methods the maximum score the sum or a weighted sum.
rodriguez et al.
found that an optimal transitive technique requires specific tuning for each project and possibly for different trace paths within a project.
transitive links can also help within a single artifact type like code.
panichella et al.
showed the benefits of including call and inheritance dependencies for tlr.
kuang et al.
then demonstrated that data dependencies provide complementary information to call dependencies that are beneficial for linking requirements to code.
the idea is that close methods are probably linked to similar requirements.
documentation to code the second type of tlr task is the recovery of trace links between documentation and code.
in their study on recovering trace links between code and manual pages antoniol et al.
found that programmers often use meaningful names for program items.
as such they use a unigram language model and a bayesian classifier and compared it to a vector space model.
marcus and maletic apply latent semantic indexing achieving higher precision and recall.
the approach transarc by keim et al.
uses a component based architecture model as an intermediate artifact to recover trace links between architecture documentation and code.
this transitive approach yields improved results by combining tlr approaches that need to bridge smaller semantic gaps.
because architecture models are not always present they also provide a baseline for their performance without using the intermediate artifact namely ardocode.
documentation to models the third type of tlr task is the recovery of trace links between documentation and architecture models.
for this task cleland huang et al.
propose enhancement strategies to maximize recall and precision while maintaining a high recall.
to achieve this they discover that links that have a medium probability a.k.a.
low confidence links are hard to distinguish between true and false links resulting in bad performance.
to mitigate this problem they propose the usage of hierarchical information and logical groupings of artifacts to enhance precision.
for tracing documentation to business process model and notation bpmn lapena et al.
found that both the linguistic particularities of bpmn models and their execution traces can help recovering lost links.
keim et al.
propose ardoco for tlr of architecture documentation to component based architecture models.
they use heuristics to identify word clusters that could define aknowledge identifier type contentelement granularity artifactparent .. fig.
.
data model knowledge elements and artifacts model element.
ardoco then traces these clusters to the model elements.
their dataset for this task is publicly available .
b. using llms for tlr prompting and rag until now llms have only rarely been employed for tlr.
lin et al.
fine tune a codebert llm for the task of recovering trace links between issues and commits.
however their approach requires initial trace links from a project for fine tuning.
therefore they tackle a different kind of tlr problem than our approach.
recently rodriguez et al.
studied the application of prompt engineering on tlr.
they showed the llm s ability to comprehend domain specific knowledge such as acronyms.
furthermore they showed that small modifications of prompts can result in differences in tlr results.
in contrast to our approach neither the work by rodriguez et al.
nor other existing approaches explore the application of rag for different tlr tasks.
since tlr handles large amounts of data it can be expensive or even impossible due to size restrictions to process all data naively.
therefore retrieval augmented generation is one possible approach to make tlr applicable to large datasets.
moreover we focus on the application of prompting techniques and different preprocessing instead of prompt engineering.
finally we also apply our approach to different tlr tasks to evaluate its applicability on various tlr tasks.
iii.
t helissa a pproach in this section we present the lissa approach and discuss its different components.
the approach uses multiple steps to recover trace links between different artifacts.
as input lissa takes a set of artifacts e.g.
requirements documentation or architecture models.
the output of lissa is a set of trace links between the elements within the artifacts or between artifacts e.g.
sentences to source code files or text files to code files.
a. preprocessing embedding lissa distinguishes artifacts elements and knowledge .
their relationship is shown in figure .
we define artifacts as the inputs that can be processed by lissa.
artifacts consist of the original content of files and have a specific artifact type.
elements are the traceable units of the artifacts and thus are mainly the parts that are used to create the trace links.
besides their identifier type and content they also have a granularity.
this granularity is used to define the level of abstraction of the trace links.
furthermore elementssource code requirements architecture docs architecture modelscode like artifacts nl artifacts model like artifactsartifacts preprocessing embedding vector store fig.
.
preprocessing embedding of artifacts nl for natural language can have a parent.
the child parent relationship is directly reflected by the granularity of the elements.
for example a sentence child is part of a text file parent .
knowledge defines all data that can be used by lissa to create trace links.
it is defined by an identifier a type and its content.
in this work we regard both the contents of the artifacts and their elements as knowledge.
we have kept this base class knowledge intentionally general because we designed lissa as an extensible framework.
most important knowledge does not need to be artifacts or elements.
this makes it possible to enrich prompts with information that is not directly part of them.
it also enables more sophisticated retrieval techniques and ensures the extensibility of the framework.
after loading the relevant artifacts the approach first preprocesses the artifacts.
as shown in figure the approach currently distinguishes three types of artifacts natural language artifacts code artifacts and model artifacts.
natural language artifacts describe text based artifacts that mainly consist of written natural language text.
examples include requirements and documentation.
these artifacts are typically preprocessed by splitting them into lines sentences paragraphs or arbitrary chunks of text.
code artifacts define artifacts that are written in a programming language.
this could either be source code test code or shell scripts.
code artifacts are preprocessed based on their programming language.
for this paper we focus on java code since the obtained datasets from related work mostly contain java code.
the approach extracts elements of the code artifacts by splitting the code based on methods classes or files.
model artifacts like architecture models are instances of a certain metamodel.
in our case we focus on uml component diagrams that are used to describe the architecture of a software system.
we select these models because they are used by one of the state of the art approaches allowing us to compare our approach to them.
model artifacts are preprocessed by extracting the elements like components and interfaces.
in general the approach preprocesses elements from the different artifact types by transforming them into a textual representation that can be used for retrieval.
as shown in figure after transformation lissa uses embedding models to create a vector representation of each element.
the embeddings of elements from artifacts are stored in a vector store.type uml component name filestorage interface realization ifilestorage operation getfiles operation storefile uses filesystem fig.
.
example textual representation of a model element preprocessors in detail in the following we present the preprocessors of lissa that we identified as relevant for the artifacts of the given tlr tasks.
the decision is informed by the datasets from related work cf.
section ii .
a none the first preprocessor does not change the artifact and only creates the embedding.
this means that no splitting is done and the whole artifact is used as one element.
one issue in this preprocessing is that artifacts might be longer than the maximum input size of the embedding model or the llm.
to deal with this we cap the artifacts w.r.t.
the input size of the models.
in the future this issue might dissolve with models that support increased maximum input sizes.
b sentence the second way of preprocessing is tailored to text based natural language artifacts.
the artifact can be split into sentences and each sentence is then defined as an element.
this way the approach supports fine grained trace links between sentences and other elements.
c chunk n when preprocessing text based artifacts like source code one method to split is chunking.
this means the text is divided into chunks of a given size n. for source code this chunking can also be tailored to the programming language by splitting the text at language dependent keywords to create chunks of approximately the desired size.
in our evaluation we also assess the impact of different chunk sizes.
d method a second way to split source code is to split it based on method declarations.
methods including their body signature and documentation define a unit that shall fulfill a certain task.
consequently the preprocessor splits the code before each method definition including its method documentation like javadoc.
the underlying idea is to treat methods as meaningful atomic units and not possibly split in the middle of a method.
the resulting elements could also be used to generate trace links on the method level.
e models feature extraction compared to the other kinds of artifacts we preprocess models differently.
models are instances of a certain metamodel.
in our case we focus on uml component models based on the eclipse modeling framework emf .
the model preprocessor is tailored to this specific type of model and extracts features from a given model instance.
the extractor identifies components interfaces their operations methods and their dependencies realizations and usages .
these extracted features are used to create a textual representation of a model element component or interface that can be used for the retrieval process.
the creation of the textual representation is based on templates that are filled with the information of extracted features.
figure shows an example of the textual representation of a model element.target artifactpreprocessing target target elements source artifactpreprocessing source source elementsembeddingvector store target elements finding similar elementstarget element candidatesk prompting aggregation trace linksretrieval mapping fig.
.
overview of the retrieval mapping.
data is in orange prompting is in blue and other processing is in white.
b. retrieval as shown in figure the next step after preprocessing the artifacts is the retrieval.
the retrieval step is responsible for finding the top kmost similar elements for a given source element in the vector store containing the target elements.
for the comparison of embeddings the approach uses the cosine similarity of vectors.
these most similar elements are then used as target element candidates.
only these candidates can be potential targets of a trace link for the given source element.
because the source elements are used to retrieve the target elements the retrieval step defines the direction of the tlr task.
we call a to b tlr e.g.
requirements to code tlr when a is the source artifact and b is the target artifact.
in summary the retrieval step takes the elements from a preprocessed source artifact creates the embeddings and identifies for each source element the top kmost similar target elements as candidates.
the preprocessing steps strongly influence the retrieval because the retrieval defines the possible mappings between elements.
for example in requirements to code tlr with the sentence preprocessor for requirements and the method preprocessor for code the candidate mappings are between single sentences and single methods.
c. mapping after the retrieval comes the mapping process.
the mapping takes a source element and its respective target element candidates and uses an llm to classify which of the target element candidates belong to the source element.
the approach then aggregates the identified mappings to create trace links on the desired level of abstraction.
consequently the whole mapping process consists of two steps prompting and aggregation.
prompting in the prompting step the source element and the target element candidates are combined to create prompts for the llm.
the llm s responses are then used to decide for each candidate if the source element belongs to the target element and therefore if there should be a trace link.
the approach supports different prompting techniques.
the prompting relies on the granularity of the source and target elements.
for example if there is requirements to code tlr and the approach uses the sentence preprocessor for requirements and the method preprocessor for code then the llm classifies if a sentence belongs to a method.
we base our prompts on the work of rodriguez et al.
.
based on their work we derive two prompts the kiss promptprompt kiss question here are two parts of software development artifacts.
source type source content target type target content are they related?
answer with yes or no .
prompt chain of thought below are two artifacts from the same software system.
is there a traceability link between and ?
give your reasoning and then answer with yes or no enclosed in trace trace .
source type source content target type target content and the cot prompt.
the kiss prompt is a simple classification prompt that gives the llm the freedom to interpret its task.
the cot prompt is a zero shot prompt that is more complex and requires the llm to reason about the elements.
we detail these prompts in the following.
a kiss prompt the kiss prompt is a simple zero shot prompt see prompt .
the prompt first defines the software development domain.
afterward the prompt states the types of elements and their contents.
lastly the prompt contains a yes or no question if the source element belongs to the target element candidate.
in the llm s response the approach looks for the term yes.
if found the approach traces the source element to the target element.
b cot the second prompt is the cot prompt see prompt .
this prompt also is a zero shot prompt.
it starts with the setting of the context by defining that the llm gets two artifacts from a software system.
afterward the prompt contains the same yes or no question as the kiss prompt.
in contrast to the kiss prompt the prompt asks the llm to provide reasoning.
lastly the prompt defines the output format and provides the source and target elements.
again the approach looks at the output of the llm and if the llm approves the approach again traces the source element to the target element.
the reasoning itself is disregarded.table i overview of dataset for requirements to code tlr.
it m arks italian and en e nglish natural language descriptions and tl s tands fortrace link language of artifacts dataset domain nl programming req code tl smos education it java etour tourism en java itrust healthcare en java dronology re aerospace en java python dronology dd aerospace en java python aggregation the final step of the mapping is the aggregation.
in this step the traced pairs of source and target elements are combined to trace links that match the defined level of abstraction.
the following example illustrates this based on the configured preprocessors the prompting step has traced sentences of requirements and methods of code classes.
the desired granularity for trace links is requirement file to code file.
the aggregator creates the trace links if there is at least one correspondence between a sentence of a requirement file and a method of a certain class.
by so the aggregator can output the trace links on the desired level of abstraction.
iv.
e valuation in this section we present the evaluation of our approach.
we evaluate the performance of our approach on the three different tlr tasks and compare it to state of the art approaches.
therefore we discuss the used datasets metrics used models baselines and the performance of our ragbased approach.
we also perform significance tests to provide a statistical analysis of the results.
finally we address the potential threats to validity.
we provide all our results and datasets in our replication package .
a. datasets to reduce the bias of the creation of a new dataset we use existing datasets from the literature.
as already discussed this is one factor in our selection of the respective tlr tasks.
tracing requirements to code for tracing requirements to code we use a dataset from the replication package by hey .
we use the projects smos etour and itrust as these projects are used for the evaluation of the state of the art approaches ftlr and comet.
these projects were gathered by the center of excellence for software systems traceability coest and cover different domains education tourism and healthcare .
additionally we use the dronology dataset aerospace domain and apply our approach and ftlr on it.
table i shows the characteristics of these projects such as the used natural nl and programming languages and the number of requirements source code files and trace links in the datasets.
note that all projects except for smos provide natural language descriptions in english.
the smos project has requirements and source code identifiers in italian.
for smos etour and itrust we can use the information on traceability links between requirements and code directly.table ii number of artifacts pertype and number of trace links in the gold standard foreach architecture traceability project .
ad stands for architecture documentation .
artifact type ms ts tm bbb jr ad sentences architecture model elements source code files ad to model trace links ad to code trace links however the dronology dataset contains different types of natural language artifacts such as requirements re and design definitions dd .
we use their summary and description as well as their relationship to tasks and code to derive two ground truths a mapping from requirements to code and a mapping from design definition similar to low level requirements to code.
tracing architecture documentation to architecture models for tracing architecture documentation to architecture models we use the ardoco dataset .
the dataset contains component based architecture models their documentation and the gold standard for documentation to model trace links for five projects.
the projects are mediastore ms teastore ts teammates tm bigbluebutton bbb and jabref jr .
again the projects cover a variety of domains media micro services education web conferencing and reference management .
table ii gives an overview of the artifacts in the dataset.
the artifacts of all projects are written in english.
we use these projects to evaluate our approach against the state of the art.
tracing documentation to code for tracing documentation to code we use the dataset provided in transarc s replication package .
the dataset expands the ardoco dataset by the corresponding code for the five projects and a gold standard for architecture documentation to code trace links.
we use the projects bigbluebutton mediastore and teastore to compare our approach to the state of the art approaches.
furthermore we discuss challenges regarding large projects like the two remaining projects of the dataset jabref and teammates separately.
these projects are characterized by the fact that they have many architecture documentation to code trace links.
while the projects bigbluebutton trace links mediastore trace links and teastore trace links contain on average up to .4trace links per sentence of documentation teammates contains .4and jabref .8trace links per sentence on average.
the code artifacts of all projects are also written in english and the gold standard covers only the java and shell script parts of their code.
b. metrics to evaluate the performance of our approach we use commonly used metrics for tlr precision recall and f score.
we use the f score as both precision and recall are essential for fully automated tlr.
for semi automatictlr where recall is more critical to avoid missing links we also report the f score.
by so we can compare our approach s performance to the reported results of the state ofthe art approaches.
the information also gives us insights into future applications of an rag based approach for tlr.
c. significance tests to provide a statistical analysis of the results we perform significance tests.
however we have chosen not to test all configurations for two reasons.
one payment for llm services is an added and non negligible cost in the experiments.
and two with a view to the practical application of lissa it makes sense to test only the on average best configuration.
in practice a ready to use solution would be required.
thus we consider only the configuration that achieved the best average f1 score.
we use the two sided wilcoxon signed rank test with a significance level of .
.
in order to test the significance of the results we run the experiment configuration 5additional times with a different random seed each time.
the full configuration is documented in the replication package .
d. models for the evaluation we use openai s recent models.
we use text embedding large as the embedding model.
we opted for that model based on the datasets we used for the evaluation.
the datasets do not only contain english language artifacts but also artifacts in other languages.
furthermore the vendor of the embedding model states that this model is their newest and best performing embedding model.
for the large language model for classification we use gpt 4o gpt 4o andgpt 4o mini gpt 4o mini2024 .
openai claims that gpt 4o is their most advanced model.
gpt 4o mini is openai s most recent model at the time of writing that is potent cheap and fast.
e. baselines in order to evaluate the performance of our approach we compare it to state of the art approaches.
retrieval only a baseline approach for all our tlr tasks is the retrieval only approach.
this approach does not use the classifying llm but only the retrieval step of our approach.
we then mock the classifier by always stating that a source element belongs to all found target element candidates.
by so we maximize the overall recall of the approach for a given amount of ktarget elements that should be retrieved.
this allows the analysis of what effects the classifier step has and whether the classifier is beneficial.
tracing requirements to code for tracing requirements to code we compare our approach to the state of the art approaches ftlr and comet.
both approaches have already been discussed in section ii.
additionally we compare our approach to a vsm and lsi baseline.
for ftlr we provide the results of the best configuration with the originally defined thresholds.
it uses method comments call dependencies and filter based on use case templates and functional aspects.
additionally we take a look at the results of thebest per project optimized threshold configuration ftlr opt illustrating the upper boundary of ftlr s performance.
this configuration only uses requirements filter based on use case templates and functional aspects and no method comments or call dependencies.
for the baseline comet we also provide the results for the best per project optimized configuration comet opt .
the results for smos etour and itrust are already part of the replication package by hey .
thus we report these results for comparison.
dronology has not been evaluated with ftlr so far so we apply ftlr to it.
unfortunately we could not run and evaluate comet on the dronology dataset.
for the vsm and lsi baselines we make use of the code provided by gao et al.
in their replication package.
the code only produces ranked lists of target artifacts per source artifact without a fixed threshold for determining the final trace links.
therefore we again calculated the perproject optimized f scores vsm opt lsi opt by varying the cutoff threshold between r0 1sin .
steps.
the results thus again show the upper boundary of their performance.
tracing documentation to code for this task we compare our approach with the work of keim et al.
see also section ii a2 .
since we focus on the direct linking of documentation to code we do not compare to transarc.
transarc uses intermediate artifacts and thus extra knowledge that is not available for the direct linking approach.
consequently we use the best known approach for direct linking from keim et al.
ardocode.
ardocode is based on heuristics that have been originally optimized for the tlr from documentation to architecture models.
nevertheless ardocode achieved the best results for the direct tlr from documentation to code i.e.
without using architecture models.
we compare the results of our rag based approach directly with the results reported by keim et al.
tracing architecture documentation to architecture models for tracing between architecture documentation and architecture models we compare our approach with the stateof the art approach ardoco .
ardoco is based on heuristics and tailored to this tlr task cf.
section ii a3 .
we again compare our results directly with the results reported by the authors of ardoco without any modifications.
f .
results this section presents and discusses the evaluation.
our replication package contains all detailed results.
tracing requirements to code table iii shows the detailed results using gpt 4o.
for the tlr from requirements to code we consider eleven different configurations.
in the first group there are three configurations without preprocessing the artifacts.
these three configurations compare the effects of the different classifiers introduced in section iii c i.e.
using no llm at all using prompt kiss prompt and using prompt cot prompt .
the second group of results shows the effects of preprocessing the input by reducing the requirements to sentences and splitting the code into chunks.
as chunk size we define a value of200 characters.
this value should ensure a fine grainedtable iii results forrequirement to code tlr w ithgpt o t op p rep.for preprocessor cls.for classifier art.for artifacts smos etour itrust dronology re dronology dd approach p r f f2 p r f f2 p r f f2 p r f f2 p r f f2 vsm opt .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
lsi opt .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
comet opt .
.
.
.
.
.
.
.
.
.
.
.
ftlr .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ftlr opt .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
prep.
cls.none noneno .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
p1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
p2 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.309sentence chunk no .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
p1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
p2 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
art.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.305sentence methodno .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
p1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
p2 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
art.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
splitting where also parts of the methods are transformed into elements.
in this setting we evaluate four classifiers.
again we evaluate the use of no llm the kiss prompt and the cot prompt.
additionally we evaluate how the provided elements affect the performance of the approach.
therefore we defined a variant where the llm is provided with the original artifact instead of the elements from the retrieval.
this means that we use the elements i.e.
parts of artifacts to retrieve relevant artifacts.
the llm then uses the whole artifact to decide.
the final group shows the results where the preprocessor for code is replaced with the method preprocessor tailored to java source code.
similar to the previous group of results we evaluate four classifiers and their performance.
in the first group as expected the configuration using no llm at all achieves the highest recall.
this configuration defines the upper bound for recall of a retrieval augmented tlr approach for a defined preprocessing.
the next two rows in the table show the results for the two prompt types.
except for smos the configurations using the two prompt types outperform the no llm configuration both in f score and f 2score.
in many cases for gpt 4o prompt the cot prompt outperforms the kiss prompt regarding f score.
on average this is our overall best configuration w.r.t.
f score.
in the second group with chunks consisting of 200characters cot prompts outperform the kiss prompts.
furthermore the table shows that the use of target artifacts instead of target elements increases the performance of the approach in f 1score and f score.
nevertheless this preprocessing performs slightly worse compared to the variant without preprocessing.
in the third group with method chunking the performance of the prompts varies between the projects.
our overall best results in f score and f score for smos are achieved using this preprocessing.
for etour this preprocessing achieved the worst results.
the other projects are comparable to the resultstable iv comparison oftheaverage andweighted average f scores using gpt oandgpt o mini forrequirement to code tlr using top r etrieval avg.
w. avg.
approach gpt 4o gpt 4o mini gpt 4o gpt 4o mini prep.
cls.
f1 f2 f1 f2 f1 f2 f1 f2none noneno .
.
.
.
.
.
.
.
p1 .
.
.
.
.
.
.
.
p2 .
.
.
.
.
.
.
.315sentence chunk no .
.
.
.
.
.
.
.
p1 .
.
.
.
.
.
.
.
p2 .
.
.
.
.
.
.
.
art.
.
.
.
.
.
.
.
.314sentence methodno .
.
.
.
.
.
.
.
p1 .
.
.
.
.
.
.
.
p2 .
.
.
.
.
.
.
.
art.
.
.
.
.
.
.
.
.
baselines f1 f2 f1 f2 vsm opt .
.
.
.
lsi opt .
.
.
.
ftlr .
.
.
.
ftlr opt .
.
.
.
in the previous groups.
this indicates that there might be some characteristics in the projects that affect the performance of the approach.
smos is the only project written in italian and therefore another configuration might be more suitable for projects in english.
etour s method comments are often inconsistent with the code they describe .
this may negatively impact the performance of the embedding and therefore the retrieval.
on etour we observe a decrease in recall from .815to0.597in the retrieval only configurations if considering methods as elements to embed.table v p v alues of the wilcoxon signed rank test two sided for f1 f2 scores of requirement to code tlr w ithgpt o none none p2 a ndtop .
b old marks a significant difference ours better baseline better .
dr stands for dronology .
approach smos etour itrust dr dr avg.
w. avg.
re dd vsm opt .03 .03 .03 .03 .03 .03 .03 lsi opt .03 .03 .03 .03 .03 .03 .03 comet opt.
.03 .03 .
.03 ftlr .03 .03 .03 .03 .03 .03 .03 ftlr opt .03 .03 .03 .03 .03 .03 .03 .
.03 table iv provides an overview of the results for the two models gpt 4o andgpt 4o mini in comparison to ftlr vsm and lsi.
we report the average and weighted average for the f score and f score.
the overall best results regarding both scores are achieved by our approach without splitting the artifacts.
the larger model outperforms both the smaller model and the state of the art approach on average and on weighted average f scores.
regarding f scores the results show that on average the best configuration is achieved by the gpt 4o model using prompt without preprocessing.
except for the weighted average f score using gpt 4o applying classifiers improve our performance in both scores.
there the best performance is achieved by using no llm.
however this result is mainly influenced by the smos project.
interestingly for smos gpt 4o mini achieves an f 1score of .425using the kiss prompt for sentence methodpreprocessing.
thereby gpt 4o mini outperforms gpt 4o for this project.
this means that for some projects gpt 4o mini can outperform the state of the art and the larger gpt 4o.
overall gpt 4o mini is comparable to the state of the art.
we perform significance tests for the best configuration of the gpt 4o model no preprocessing and prompt .
the p values of the significance test are shown in table v. the results with a significant difference between our results and the baseline are marked in bold font.
if our result is better than a baseline we mark it with .
if a result is worse we mark it with .
on average our approach is significantly better than all state of the art approaches.
however the results also show that our on average best prompt is not significantly better for the smos dataset.
here we emphasize that this is the only project of the dataset that contains italian language artifacts.
the only non significant differences occur for perproject optimized baseline configurations.
it may be expected that the actual performance of these approaches in practice is lower exemplified by ftlr vs. ftlr opt.
the results for f1 score and f score are similar.
the only differences are for comet opton smos and itrust as well as ftlr opt on etour and in weighted average.
in f score lissa is significantly better than the baselines on etour and itrust.
in summary the approach lissa outperforms the stateof the art approaches ftlr and comet using gpt 4o.
in contrast to the gpt 4o mini model this comes with the disadvantage of high costs.
considering the results for gpt 4otable vi comparison oftheaverage andweighted average f scores for software architecture documentation to code tlr o n thesmaller projects bigbluebutton teastore media store using gpt o mini andtop r etrieval approach avg.
w. avg.
preprocessor classifier f1 f2 f1 f2 sentence noneno llm .
.
.
.
prompt .
.
.
.
prompt .
.
.
.
sentence chunk no llm .
.
.
.
prompt .
.
.
.
prompt .
.
.
.
sentence chunk no llm .
.
.
.
prompt .
.
.
.
prompt .
.
.
.
baselines f1 f2 f1 f2 ardocode .
.
.
.
mini the approach performs similarly to ftlr and performs comparable to gpt 4o.
in the end we ve spent approx.
for the evaluation of this task with gpt 4o excluding the repetitions for the significance tests .
gpt 4o mini only cost us approx.
for worse but comparable results.
tracing documentation to code in the second task we evaluate tlr from documentation to code.
as discussed in section iv a3 we use the transarc dataset for this evaluation.
since this task is defined as tracing single sentences to code files we always use the sentence preprocessor for the documentation.
regarding code we evaluate the configurations for no preprocessing small chunks with characters and larger chunks with characters.
we first evaluate the performance of the smaller projects bigbluebutton mediastore and teastore .
the results for those using top retrieval are shown in table vi.
similar to the evaluation of requirements to code tlr we report the results for the two different prompt techniques and the retrieval only no llm for each preprocessing configuration.
on average we can see again that the combination of cot and no further preprocessing achieves the best results regarding f .
for this configuration the wilcoxon signed rank test shows that the f scores of the averages are significantly better than the baseline ardocode and the no llm configuration both p .
.
the baseline ardocode is better in f score due to high recall.
however the average precision is only .
.
for the smaller projects the approach already achieves similar or even better results than the baseline.
nevertheless we also uncover challenges for larger projects.
with large projects the retrieval is problematic on the two largest projects jabref and teammates the best retrievalonly configuration for top elements achieves a recall of .027for jabref and .061for teammates.
additionally the bad retrieval for the large projects affects the weighted average heavily the weighted average f score of the best configuration including the large projects is .
without table vii comparison oftheaverage andweighted average f scores for architecture model to architecture documentation m2d and architecture documentation to architecture model d2m tlr u sing top r etrieval andgpt o mini avg.
w. avg.
approach d2m m2d d2m m2d features cls.
f1 f2 f1 f2 f1 f2 f1 f2 name no .
.
.
.
.
.
.
.
p1 .
.
.
.
.
.
.
.
p2 .
.
.
.
.
.
.
.
name no .
.
.
.
.
.
.
.
interfaces p1 .
.
.
.
.
.
.
.
p2 .
.
.
.
.
.
.
.
name no .
.
.
.
.
.
.
.
interfaces p1 .
.
.
.
.
.
.
.
usages p2 .
.
.
.
.
.
.
.
baselines f1 f2 f1 f2 ardoco .
.
.
.
.
.
again the best configuration in this case is the same as for the smaller projects cot prompt without preprocessing.
in summary our approach lissa achieves significantly better results in f score than state of the art approaches on smaller projects.
for larger projects the performance of the retrieval needs to improve.
tracing architecture documentation to architecture models the final tlr task is recovering trace links between architecture documentation and architecture models.
because there are comparably few model elements and few sentences we additionally analyzed the effect of the direction for tlr.
this means we evaluate both tlr for documentation to model d2m and tlr for model to documentation m2d .
for both cases we use a retriever that retrieves the most similar target elements for each source element.
since the tlr task requires a trace link between a sentence and a model element i.e.
for this tlr task a component we used the sentence preprocessor to split the documentation.
we evaluate the effect of the prompt type and the effect of the features that are extracted from the model preprocessor.
the results of the evaluation are shown in table vii.
on average the cot prompt outperforms the kiss prompt and using retrieval only in both scores.
for the d2m tlr task the displayed f score comes from a very high recall of almost .0but a very low precision.
when examining the flipped task the recall decreases while the precision increases substantially.
nevertheless the results vary on the projects.
we assume that this is mainly caused by the retrieval.
the retrieval always tries to select the topkmost similar target elements.
considering m2d tlr this means that for each component in the model the approach retrieves the top kmost similar sentences.
nevertheless the value kmight depend on the project and also the direction of the tlr task.
depending on the project the assumption that a component is described in at most ksentences can be fulfilled.considering the different features on average the approach s performance worsens with the number of used features.
we assume that this is due to the kind of trace links.
the llm has to decide if a component belongs to a sentence.
too much information that might not be directly related or that is unique to the component can negatively influence distract the llm.
this then leads to decreased precision.
overall our approach does not outperform the state of theart approach ardoco.
g. threats to validity in this part we discuss threats to validity and base this discussion on the guidelines by runeson et al.
.
we also consider the work by sallou et al.
that focuses on threats for experiments with llms in software engineering.
regarding construct validity there is the threat that the datasets are biased.
to mitigate this threat we use the same datasets as the state of the art approaches.
the datasets cover different domains and project sizes.
nevertheless the datasets mostly contain open source projects that may have other characteristics compared to closed source projects.
in this work we do not focus on prompt engineering i.e.
we do not extensively modify prompts and compare their performance.
consequently a potential threat to the validity of our results regarding rq1 is the selection of the prompts.
however we closely follow the suggestions of rodriguez et al.
.
another threat is a biased selection of metrics.
to diminish this risk we use commonly used metrics in tlr research.
overall we try to reduce potential confounding factors that could prevent us from effectively addressing our rqs.
considering internal validity there is the threat that other factors might influence our experiments.
these could influence our interpretations and lead to wrong conclusions.
to mitigate this threat we follow established practices.
we ensure that we use datasets and projects from state of the art approaches.
furthermore we clearly state the origins of the projects and ground truths.
this also reduces the risk of selection bias.
since the datasets mostly consist of open source projects they still might vary in quality.
the quality and consistency within one project can affect the performance of the approach.
regarding external validity there are various threats.
first we only evaluate the approach on a limited number of projects and a limited number of tlr tasks.
the results can vary for other projects and other tlr tasks.
to reduce this threat we use established datasets and projects from different domains and sizes.
second we evaluate the approach with only one embedding model and two llms.
to mitigate this threat we use state of the art models.
the third threat is the use of closed source models for evaluation.
since we do not know the training data of the models we cannot ensure that there is nodata leakage .
in order to mitigate this threat we provide our code the prompts embeddings and the responses of the llms in our replication package to ensure transparency.
the fourth threat is the non determinism of the llms.
to mitigate this threat we set the temperature of the llms to to reduce the randomness of llms.with the aforementioned evaluation on established datasets using established metrics we address the reliability of our research.
therefore we do not introduce a threat regarding the manual creation of the datasets.
nevertheless the gold standard might be imperfect as they are manually created.
to improve reliability we also base our prompts on the literature.
v. d iscussion f uture work in this section we analyze our results on rag based tlr concerning challenges impediments and implications in both research and practice.
we also provide a brief overview of planned future improvements.
one challenge is defining trace links so that llms can effectively interpret them.
terms such as related or traceability link can be too ambiguous for llms and might need further refinement.
another challenge involves optimizing the retrieval process which sets the upper limit for recall.
a significant issue arises when a single sentence maps to hundreds of code files as observed in larger projects like jabref.
in such cases a high number of elements must be retrieved to ensure all relevant code files are included pushing the retrieval process to its limits.
to address this we suggest focusing on improved retrieval techniques.
possible solutions to enhance the process include dynamic thresholds alternative data structures for retrieval or artifact summarization.
our results show that the performance in f score and f2 score of the proposed rag based tlr does not yet enable fully automated tlr in practical applications even for semi automated settings f score the performance remains insufficient for practice .
nevertheless state of the art approaches perform worse on code related tlr tasks.
rag presents several opportunities to improve performance future research could focus on prompt engineering on incorporating more context and on exploring advanced rag techniques.
our framework lissa serves as a foundation here.
lissa already outperforms code related tlr approaches though it does not yet address these future directions.
therefore we see significant potential for rag in these tasks.
in the future we aim to expand our lissa approach in several ways.
our findings show that our current fine grained mappings do not enhance performance motivating us to explore more advanced aggregation techniques.
for example we plan to adapt methods like majority voting from ftlr to improve the aggregation step.
moreover there is a clear need for more sophisticated preprocessing methods for code.
sentences in architecture documentation can map to hundreds of code files because they refer to higher level structures such as packages.
to address this we intend to investigate summarization techniques and clustering methods for code preprocessing.
lastly we aim to improve the classification process during the prompting step.
currently the decision to create a trace link is based solely on the llm s classification.
in the future we will explore incorporating the llm s reasoning into this decision making process.
we also plan to enhance prompts by integrating additional contextual information such as project specific or domain specific knowledge.vi.
c onclusion this paper presents a novel approach for traceability link recovery tlr using large language models llms with retrieval augmented generation rag .
the benefit of our rag based approach linking software system artifacts lissa is its applicability to various tlr tasks.
lissa uses preprocessors to prepare artifacts according to their type to create traceable elements of the source and target artifacts.
the approach then uses the embeddings of these elements to retrieve similar target elements for each source element.
lissa then generates prompts for the llm to classify if source and target elements are related.
if classified as related the approach aggregates the elements to finally create the trace links.
to answer our research questions we evaluate lissa on established datasets and compare it to state of the art approaches for three different tlr tasks.
regarding rq1 which examines the performance of ragbased tlr compared to state of the art our evaluation shows that our rag based approach significantly outperforms stateof the art approaches for requirements to code tlr.
for smaller projects the rag approach achieves significantly better f scores than the state of the art approach for documentation to code tlr.
however the approach underperforms for larger projects.
for tlr from documentation to architecture models lissa does not outperform the state of the art.
for rq2 about the effectiveness of cot prompting we find that cot prompting outperforms simple classification prompts in f score.
this is consistent with the findings by rodriguez et al.
.
looking at the different preprocessing techniques and the impact of fine grained mappings rq3 we show that the preprocessing of artifacts is on average not beneficial.
in particular the preprocessing does not improve the performance of the tlr from requirements to code.
however the bestperforming approach is project dependent.
regarding rq4 that investigates the effects of the classification step we showed that on average a llm based classification improves the performance over retrieval only in all considered tlr tasks for both scores.
in summary this research contributes to the field of traceability by providing a novel rag based approach for tlr tasks.
we provide insights into how different prompt types and preprocessing techniques influence the performance.
our findings open up new research directions for the application of llms to tlr tasks.
to ensure replicability transparency and extensibility we provide the source code of lissa the used datasets and our results as part of our replication package .
we aim to facilitate further research in this field and want to enable other researchers to build upon our work.