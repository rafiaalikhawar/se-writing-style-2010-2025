towards better answers automated stack overflow post updating yubo mai zhejiang university hangzhou china zju.edu.cnzhipeng gao shanghai institute for advanced study of zhejiang university shanghai china zhipeng.gao zju.edu.cnhaoye wang hangzhou city university hangzhou china wanghaoye hzcu.edu.cn tingting bi the university of western australia perth australia tingting.bi uwa.edu.auxing hu zhejiang university hangzhou china xinghu zju.edu.cnxin xia zhejiang university hangzhou china xin.xia acm.orgjianling sun zhejiang university hangzhou china sunjl zju.edu.cn abstract utilizing code snippets on stack overflow so is a common practice among developers for problem solving.
although so code snippets serve as valuable resources it is important to acknowledge their imperfections reusing problematic code snippets can lead to the introduction of suboptimal or buggy code into software projects.
so comments often point out weaknesses of a post and provide valuable insights to improve the quality of answers while so comments are usually missed and or ignored leaving these problematic code snippets untouched.
in this work we first investigate the task of automatic so posts updating based on their associated comments.
we introduce a novel framework named s oup stack o verflow u pdator for p ost for this task.
s oup addresses two key tasks valid comment edit prediction vcp and automatic post updating apu .
we fine tuned a large language model codellama using low rank adaptation techniques to complete the vcp task and constructed a dataset containing 78k valid comment edit pairs for the apu task.
subsequently we tested the performance of multiple large language models on the apu task.
extensive experimental results show the promising performance of our model over a set of benchmarks.
moreover we also perform an in the wild evaluation on stack overflow we submitted edits generated by our approach to stack overflow posts and of them have been verified and accepted by so maintainers further proving the practical value of s oup.
index terms stack overflow large language models post updating data quality i. i ntroduction as one of the most popular programming q a communities today stack overflow so offers a wealth of knowledge for solving various programming issues which is scattered throughout its question and answer posts .
however not all so posts are perfect many code snippets in the posts are suboptimal e.g.
needing code simplification or extension problematic e.g.
using obsolete apis or libraries or even buggy e.g.
security flaws .
reusing these code snippets can mislead answer seekers and induce potential problems and or bugs into their working code base decreasing software quality and maintainability.
this is the corresponding author question post can you find all classes in a package using reflection?
votes answer post without using any extra libraries ......url unpackage cl.getresource pack datainputstream dis new datainputstream inputstream upackage.getcontent string line null while line dis.readline !
null ......comment1 i get a null when using this code.
seems to only work if your jar is an executable nov comment2 if you obtained the package name fromstring pack getpackage .getname then you have to addpack pack.replaceall may fig.
.
stack overflow post example fortunately so comments provide useful information to enhance the associated answers from a diverse range of perspectives such as pointing out weaknesses and suggesting improvements.
however these so comments can be easily overlooked or ignored in contrast to answers therefore most comments are rarely integrated back into their associated answers .
fig.
illustrates such an example the most recent edits of this answer occurred in all subsequent comments e.g.
comment were overlooked and never integrated into answers.
moreover since so only displayed five comments to users these newly posted comments can be hidden by so and never be noticed.
comments are the main channels for so users to discuss and communicate potential problems in answer posts.
ideally when a comment is posted to point out problem s it should trigger an update in corresponding answers e.g.
code snippets and thus improve their quality.
nonetheless because digesting comments and manually maintaining answers after a long time being posted are time consuming and laborintensive users seldom update code snippets based on comments.
as replied by an user in stack overflow when his her obsolete answer was found feel free to update the answer yourself if you like.
i honestly would but i don t have the time.
therefore it is highly desirable to have a tool that automatically updates so code snippets according to the givenarxiv .09095v1 aug 2024comments however developing such a tool poses significant challenging as outlined below high quality dataset .
some previous works built datasets for so post updates for example tang et al.
first proposed a matching based approach to map comments to their related edits building a dataset containing 248k comment edit pairs.
however due to the limitations of their approach the quality of their constructed dataset can not be guaranteed.
the recall of their method for identifying valid comment edit pairs is only .
while the precision is .
.
the relatively low quality data greatly hinders the model s performance trained on it.
to create such a dataset it requires to extract valid commentedit pairs from so i.e.
a comment and its corresponding edit that addressed this comment.
currently there is no high quality dataset available for training models that automatically update posts.
post updating models .even tang et al.
proposed the first comment edit dataset they ended up with creating the dataset without further utilizing it.
to the best of our knowledge there has been no research delving into the automatic updating so posts based on their comments.
to address the aforementioned challenges we define two key tasks as follows the validcomment edit prediction vcp .
for a given so answer post numerous comments can be added under the same post.
however not every comment leads to an edit.
in order to automatically construct a dataset that can be used to train models for automatic post updating we defined this preliminary task.
vcp is defined as predicting whether a comment edit pair is valid and a valid commentedit pair is defined as i the comment is relevant to the edit ii the edit addresses implements all the issues suggestions raised in the comment iii the edit does not make changes beyond what is mentioned in the comment.
in this type of data the updated code fully complies with the comments suggestions and therefore can be used to train the apu model.
the automatic postupdating apu .
in stack overflow an edit is associated with a pair of posts before and after modification.
the apu task is defined as giving a post before modification and its associated comment whether the post after modification can be correctly generated.
since developers are more interested in code snippets this paper focuses only on the updating of code snippets in the posts.
inspired by the great advancements and potential of large language models llms in code generation in this work we proposed a novel llm based framework named soup stack overflow updator for post to perform the vcp and apu tasks.
for the vcp task we first manually annotated 5k comment edit pairs we then finetuned a llm for this task and the trained model is denoted as s oup p. then the well trained model s oup pis used to automatically create a high quality dataset consisting of 78k valid comment edit pairs.
this high quality dataset is used for fine tuning a llm on the apu task and the trained model isdenoted as s oup u. our study aims to answer the following four research questions rq1 how effective is our approach in predicting valid comment edit pairs?
we compared our s oup p with the matching based method proposed by tang et al.
the experimental findings demonstrate that our approach achieved an impressive .
precision and .
recall on the vcp task marking a substantial improvement over tang s method by and respectively.
rq2 how effective is our approach in automatically updating so code snippets based on their comments?
the experimental results show that our approach achieved a .
exact match rate particularly excelling in solving improvement type updates.
rq3 to what extent does the dataset influence model performance in the apu task?
we performed a cross dataset evaluation between different models the experimental results show that the model trained on our dataset significantly outperformed the same one trained on tang s proposed dataset.
rq4 how acceptable are our updated so posts in real world scenarios?
in this rq we conducted an in the wild evaluation to evaluate the effectiveness of soup in practice.
we randomly sampled unaddressed comments from so we then manually submitted s oup generated edits to these posts updates are accepted by so maintainers.
overall our paper makes the following contributions we propose a novel llm based framework soup to accurately predict valid comment edit pairs and autonomously update so code snippets based on associated comments.
s oup is expected to become a real time updating tool for code forum websites ultimately improving the code quality of such sites.
.
we build a high quality dataset for so post updating which contains valid comment edit pairs for java the experimental results show that our constructed high quality dataset can significantly improve model s performance on the apu task.
we extensively evaluate our approach by submitting soup suggested edits to actual so posts.
edits are accepted verifying the practical value of our approach.
our contribution also lies in the provision of both the source code and dataset for s oup enabling fellow researchers to replicate our findings and explore their own concepts with ease.
ii.
m otivation although so code snippets may not always be flawless the platform s comments serve as valuable resources for identifying potential issues and offering constructive feedback.
fig.
shows several comment edit examples in so to deliver helpful information and knowledge for different problems.
even popular so answers may contain problematic code snippets.ex1.
comment linkedlist should be replaced by arraylist but linkedhashmap is necessary because it preserves insertion order.
postid public class maputil public static k v extends comparable ?
super v map k v sortbyvalue map k v map list map.entry k v list new linkedlist maparraylist map.entry k v map.entryset ..... ex3.
comment i know this is an old post but it is worth mentioning to other visitors that the above code has a bug.
one must use thenexttokenwhen calling tosnsclient.listtopics otherwise you will have an infinite loop provided you have more than topics .
postid public class testsns ..... while nexttoken !
null listtopicsresult snsclient.listtopics nexttoken ......ex2.
comment use the proper add ... method.
the add string component method has been obsolete for over a decade since jdk1.
.
postid jpanel sidepanel new jpanel new borderlayout .....sidepanel.add borderlayout.center new jscrollpane log borderlayout.center sidepanel.add buttonpanel borderlayout.north buttonpanel ......fig.
.
motivating examples for example ex.
presents a suboptimal code snippet the question post sort a map key value by values received more than .8k votes and has been visited for more than .8m times since its creation and this code snippet received more than 1k votes by so users.
however as suggested by a developer this code snippet is suboptimal by using linkedlist instead of arraylist .
after the comment was posted the answer post was subsequently updated accordingly.
however it is worth noting that this problematic code had been present on so for over eight years before any editing took place.
so answers may become obsolete or outdated as a result of the rapid evolution of software systems.
ex.
demonstrates an example where the code snippet used an outdated method add which has been obsolete for over a decade.
so answers may contain buggy code.
an example is shown in ex.
as the commenter pointed out i know this is an old post but it is worth mentioning to other visitors the above code has a bug.
one must use nexttoken ... otherwise you will have an infinite loop .
this answer post was created in august but this bug has not been discovered until july .
during this time this code snippet was visited by so users thousands of times which can mislead developers and cause the introduction of bugs in their subsequent development.
so comments provide valuable information to improve the code snippet s quality however prior studies found that answers are rarely updated after comments are posted and only .
of the answers are edited after any comments .
in other words the valuable information hidden in comments is mostly ignored and the comments are rarely integrated back into answers.
therefore in this work we aim to fill this gap by proposing a tool to automatically update code snippets based on their associated comments which can enhance the answer quality and reduce the likelihood of introducing bugs.
iii.
p reliminary investigation tang et al.
conducted the first study on how to map a comment with its associated edit.
they proposed a simple false negative example comment hello thank you butidon t wanna close thewhole program justthewindow where iam.
postid if salir !
system.exit nameofyourframe.dispose false positive example comment rather than declare theactivity assingleinstance consider using flag activity clear top flag activity single top astheflags fortheintent .this will finish intervening activities and start orbring back totheforeground thedesired activity .
postid ...... intent.addflags intent.flag activity clear top flag activity new task intent.flag activity clear task ......fig.
.
preliminary investigation examples matching based method to determine whether an edit is related to a comment.
specifically their method matches a comment to an edit based on three heuristic rules the comment occurred before the edit the comment mentions a code term that gets added or removed from a code snippet within the edit the commenter and the editor are different users.
to evaluate their method performance tang et al.
manually annotated comment edit pairs from so posts to create a ground truth dataset for estimating the precision and recall of their approach.
as they reported their matchingbased method achieved precision and recall on this ground truth dataset.
they further analyzed positive cases including java cases predicted by their method and reported a precision of .
to gain a deeper understanding of their method and dataset we manually investigated java comment edit pairs predicted as relevant by their method and found the following limitations tang s method of using human defined rules to predict the relevance of edits and comments has two disadvantages first it ignores the semantic relationship between code terms and edits second relevant comment edit pairs are not necessarily valid comment edit pairs .
fig.
shows a false positive example the comment mentions the code term flag activity clear top that was deleted in the edits tang s method predicted this comment edit pair as relevant.
however the edit did not modify according to the comment s suggestions useflag activity single top .
such pairs are considered as relevant by tang et al.
but failed to meet our valid comment edit pair definition.
we relabeled these pairs with standards of our valid comment edit pairs and reevaluated tang s method the precision of tang s method significantly dropped from to .
there are a large number of so comments that caused an edit but did not contain any code terms resulting in a large number of false negative samples.
fig.
shows a false negative example where the comment doesn t mention any code terms.
tang s method can t identify such comment edit pairs because the comment does not use explicit code terms but rather explains the problem and how the code can be fixed.
this limitation makes their method miss a significant number of valid comment edit pairs resulting in notably low recall scores.
although tang et al.
constructed the first relevant comment apu task vcp taskvalid comment edit predictionheuristic rules comment pre edit post editfull scale dataset 304k human annotated dataset 5k comment pre edit post edit feed forward layer normalization masked multi self attention ...... input embeddingoutput predictionllm prompt learning soup p comment pre edit post edithigh quality dataset 78k soup estack overflow post edit history automatic post updating soup framework with codejavaclosest editsingle editfig.
.
workflow of our approach edit pair dataset including pairs the proportion of valid comment edit pairs is only .
in other words nearly half of their dataset samples are noise the low quality of their dataset poses substantial threats and risks for researchers or studies intending to reuse it.
therefore it is necessary to build a high quality dataset of valid commentedit pairs.
iv.
a pproach in this section we first formally define our two key tasks i.e.
valid comment edit prediction referred to as vcp and automatic post updating referred to as apu .
we then describe the details of our approach to solving these two tasks.
the workflow of our approach is shown in fig.
.
a. task definition valid comment edit prediction on so an edit eis associated with a pair of posts before and after editing denoted aspre eandpost e. formally let cbe the candidate comment to be checked the vcp task is to find a function predict so that predict pre e post e c 1for triplet v 0forotherwise where triplet vdenotes that the triplet pre e post e c is valid according to the definition in the introduction .
automatic post updating the apu task is to update code snippets in so posts based on their associated comments.
more formally it aims to find a function so that update pre e c post e given the post s pre editing version pre eand its associated comment cas input update aims to generate a post s postediting version post eas output.
s oup framework leverages the llm based model to approximate predict andupdate respectively.
b. valid comment edit prediction the goal of the vcp task is to obtain a high quality dataset of valid comment edit pairs.
we translate this goal into a valid comment edit prediction task.
for this task we first collected a full scale dataset for potential comment edit pairs by applying a set of pre defined rules.
after that wesampled comment edit pairs from the full scale dataset to perform manual annotation.
such a sample guarantees a margin of error of with a confidence level of .
lastly we used of the annotated comment edit pairs to fine tune llms codellama in this study for this vcp task adapting the llm to effectively distinguish valid comment edit pairs and invalid ones.
the approach details are as follows full scale dataset preparation.
to perform vcp task we first make a full scale dataset for comment edit pairs by mining edit histories of so posts.
to make a fair comparison we use the same version of sotorrent dataset september as tang et al.
which captures the edit history of all so posts.
following tang s research we fetch all commentedit pairs.
subsequently we make our full scale dataset by filtering with the following criteria we exclusively focus on so java posts in this study since java is one of the most widely used programming languages.
it is worth mentioning that our work can be readily extended to other programming languages.
our goal is to update code snippets based on their corresponding comments.
therefore we only consider answer posts with code snippets.
an answer post can be associated with multiple code snippet blocks in this study we only consider answer posts that contain edits to a single existing code block.
edits to multiple code blocks at the same time are excluded.
the edit must occur after the comment and if multiple edits occur after a given comment we paired the comment with its closest edit.
it is worth mentioning that we didn t consider the second rule i.e.
a comment mentions a code term that gets removed or added in a later edit and the third rule i.e.
the commenter and editor are different users proposed by tang et al because we want to make a full scale dataset to include as many potential comment edit pairs as possible and covering wider usage scenarios comments without code terms and their associated edits .
based on the above criteria we filtered out potential java comment edit pairs as our full scale dataset.
human annotation after collecting the full scale dataset of so comment edit pairs for java we sampled pairs for manual annotation.
considering the limitation of our hardware resources we set the maximum sequence length of the comment edit pairs to tokens in this study.
since we focus on updating code snippets we removed code comments within the code snippets.
then two authors are asked to perform the manual annotation on the above comment edit pairs independently.
these two annotators are software engineering researchers with more than years of java programming language and have used stack overflow for more than years.
for each comment on an answer the two annotators separately analyze the commentedit pair to determine if it is valid based on the definition provided in the introduction.
the total time spent by thetable i dataset information dataset label train validation test human annotatedvalid invalid high quality two annotators to complete the manual annotation costs .
hours and .
hours respectively with an average of 1min 32s and 2min 8s for labeling one comment edit pair.
the cohen s kappa coefficient for the two annotators is .
indicating substantial agreement between them .
after each annotator performs the above labeling process independently there are .
instances of disagreements.
in this study we resolve the disagreements as follows when different opinions are met the first author will act as a mediator to discuss with two annotators then make his her own decision on this conflict case.
the final label will be determined by majority voting.
finally out of comment edit pairs are labeled as valid while the remaining edit comment pairs are labeled as invalid.
notably the label distribution of the dataset is slightly imbalanced according to the research of krawczyk et al.
the slight imbalance ratio has a relatively small impact on the model performance.
in addition considering that our dataset reflects the real world label distributions we directly use this slightly imbalanced dataset to train and evaluate the model.
after constructing the human annotated dataset we randomly split the dataset into train validation test set by the train set is used to fine tune llms for the vcp task the validation set is used to minimize the overfitting and the test set is only used for evaluating the model performance on vcp.
the details of train validation and test set of the human annotated dataset are displayed in the first two rows of table i. vcp modeling the manual annotation process is timeconsuming and label intensive to create a high quality dataset we need to design effective models to perform valid commentedit prediction automatically.
inspired by the great potential of large language models llms in code comprehension we fine tune a llm to perform the vcp task with prompt learning by using our human annotated dataset.
we refer our model on the vcp task as s oup p. recently prompt learning has been widely used with llms to support various tasks .
prompt learning combines the advantage of prompt engineering and fine tuning.
it utilizes the prompt to elicit the domain knowledge of llm and fine tuning to adapt their learned representations to specific downstream tasks.
in order to design an effective prompt for the vcp task we followed the prompt engineering guidelines mentioned in the previous studies to construct and iteratively optimize the prompt based on the validation set which included writing specific judgment criteria based on the task definition and the annotation experience of two annotators asking fora structural output e.g.
response label s using delimiters to clearly indicate distinct parts e.g.
use to wrap the code analyzing the error cases on the validation set to iteratively optimize the prompt.
we detailed the description of our prompt for judging valid comment edit pairs in table ii the complete prompt can be found in our replication package .
regarding llms we chose codellama as the base llm in our research.
codellama is a decoder only transformer model created by further training llama2 on code specific datasets.
as one of the most popular open source code llms it supports multiple programming languages including java and has shown impressive capabilities in code related tasks.
considering the computation resource limitation we chose the codellama instruct model with 13b parameters in this study and fine tuned it using the low rank adaptation lora method .
table ii prompt for valid comment edit prediction prompt instantiation input s please check if the code before comment and code after simultaneously meet the following four conditions.
condition the comment points out a general or specific flaw in code before or provides a general or specific suggestion for code before or raises a general or specific question about code before.
condition code after has optimized all the flaws pointed out in the comment implemented all the suggestions given in the comment and resolved all the questions raised in the comment.
condition code after has not made any changes beyond those mentioned in the comment.
condition code after has not introduced any new errors.
please answer with yes or no whether they meet the aforementioned four conditions simultaneously.
output response label s c. automatic post updating high quality dataset construction the s oup pmodel is trained to label valid comment edit pairs automatically and effectively.
according to our evaluation results s oup phas achieved a precision of .
and a recall of .
on the vcp task details in section v .
considering that there is also an .
inconsistency in the manual annotation process we consider s oup pas an effective model for identifying valid comment edit pairs and has achieved a human like table iii prompt for automatic post updating prompt instantiation input s i want you to act as a code optimizer you need only to return the optimized code snippet to me.
below is an instruction that describes a task.
write a response that appropriately completes the request.
code snippet comment input please optimize the code snippet based on the comment.
output response code after s performance.
therefore given its remarkable performance of soup pfor predicting valid comment edit pairs we apply soup pback to our full scale dataset 304k comment edit pairs to automatically identify valid pairs among them and construct a large scale high quality dataset.
particularly for a given potential comment edit pair from the full scale dataset we use s oup pto automatically label it as valid or invalid.
finally we obtain valid comment edit pairs as our large scale high quality dataset.
s oup pachieves a precision of .
which means that there is .
noise data in this dataset.
to reduce the noise data ratio one possible way is to conduct manual verification according to our method of manual annotation but it is too time consuming and laborintensive.
another way is to craft manually defined rules for different types of noisy data but it requires expert knowledge and additional annotation efforts.
we plan to design automated methods that do not rely on manual annotations or rules in the future to further improve the model s precision and reduce the noise ratio of the dataset.
after constructing the large scale high quality dataset we randomly split the dataset into train validation test set by the ratio of .
the train set is used to fine tune llms on the apu task the validation set is used to assess how well the model generalizes to new samples.
the test set is used to estimate the model s performance on unseen data.
the details of the train validation and test set of the high quality dataset are demonstrated in the last row of table i. apu modeling similar to vcp modeling for apu we fine tuned codellama with prompt learning on the highquality dataset built by s oup pusing lora.
the only difference is the input prompts the prompts designed for apu task is shown in table iii.
we refer to our model on the apu task as s oup u. it is worth mentioning that our framework is not restricted to codellama codellama can be easily replaced by other llms or even pre trained language models plms according to different users computational resources or preferences.
d.implementation details in this work we use the hugging face library for implementation and fine tuning.
the fine tuning was conducted on a server equipped with four nvidia a800 80gb gpus.
the fine tuning process follows these hyperparameters settings learning rate of 2e batch size of for a total of epochs.
the models with the best performance on the validation set are selected for final evaluation.
v. e valuation in this section we evaluate how effective is our proposed framework s oup for vcp and apu tasks.
we aim to answer the following four key research questions rq1 how effective is our approach for predicting valid comment edit pairs?
rq2 how effective is our approach for automatically updating so posts based on their comments?
rq3 to what extent does the dataset influence model performance in the apu task?
rq4 how acceptable are our updated so posts in realworld scenarios?
a. rq1 effectiveness of soup on vcp experimental setup regarding the vcp task we finetuned codellama with of our human annotated dataset i.e.
train set .
then the model with best performance on the validation set is chosen as s oup pfor this vcp task and the test set is used for evaluating our model and baselines.
evaluation metrics and baselines the vcp task can be regarded as a binary classification task we use precision recall and f1 score as evaluation metrics.
these metrics effectively gauge both the accuracy and comprehensiveness of the resulting apu dataset.
we use the manually curated test set from table i to evaluate the performance of different models which are as follows tang s tang et al.
first proposed a matching based approach for related comment edit prediction in particular their method determines whether a comment is related to an edit as follows if the comment mentions a code term that gets added or deleted in the comment then this pair is predicted as related.
we run tang s method on our human annotated test set for evaluation.
soup p codellama trained on the training set from table i which achieved the best performance on the validation set.
experimental results the experimental results for rq1 is shown in table iv.
from the experimental results we can see that tang s method achieved a recall of .
the reason is that their method was unable to identify valid comment edit pairs in which comment did not involve code element additions or deletions.
as shown in ex1 of fig.
the comment doesn t even contain any code elements.
the extremely low recall rate indicates that tang s method misses a large number of valid pairs.
tang s method achieved a precision of .
which is consistent with the precision in our preliminary experiments.
the reason is that the matchbased method cannot capture the semantics between comments and edits making it difficult to determine whether they are valid comment edit pairs.
as shown in ex2 of fig.
tang s method wrongly predicted it as a positive case.
although the updated code deleted the code element new mentioned in the comment it had nothing to do with comment suggestions e.g.
the builder must build a client .
in other words this is an invalid comment edit pair.
our s oup plabeled this pair as negative i.e.
invalid by successfully capturing the semantic relationship between the comment and edit.
our method i.e.
s oup p outperforms tang s method in terms of all evaluation metrics.
we attribute this to the following reasons firstly compared with tang s method our approach is based on llm which shows great potential for code comprehension.
in other words llm has its own ability to capture the semantic relationship between the comment editex1.comment ifyou have noother custom bindings just eliminate that line.asamatter offact it sirrelevant totheanswer soiamremoving it.
postid class test mock activity activity ..... install new testmodule new bindeverythingelse ...... ex2.comment trynew httpclientbuilder .create .build .execute new httpget chkurl .the builder must build aclient and with client execute thecall.
postid httpresponse urlresp new httpclientbuilder .create .execute new httpget chkurl intresp code urlresp.getstatusline .getstatuscode ex3.comment that works except youhave distinctly written newsitr and newsiter .iadjusted itin mycode .
postid iterator newsitr news.descendingkeyset .iterator while newsiter.hasnext out.println news.get newsiternewsitr.next br fig.
.
examples of errors in different methods pairs.
secondly our well designed prompts further elicit the knowledge from codellama and adapt it to our vcp task.
table iv results of different methods on vcp approach precision recall f1 score tang s .
.
.
soup p .
.
.
manual analysis we manually analyzed all the false positive cases cases and false negative cases cases predicted by our approach in rq1.
the false positive cases are mainly caused by the following reasons the comment raised multiple issues or suggestions and the edits only addressed part of them cases .
fig.
ex.
demonstrates such a case where the comment points out the inconsistency between the variable names newsitr andnewsiter .
the updated code only modifies one instance of newsiter but the issue still persists in the while statement.
therefore it does not meet the definition of a valid comment edit pair as it does not fully address the issue raised in the comment and should be considered a negative example.
the model may consider such comments helpful for code changes thus classifying them as positive cases.
comments are not beneficial to code changes cases .
these comments usually contain some actions related to editing e.g.
i will edit it later which mislead the model into thinking that they have contributed to the editing thus determining them as positive cases.
the false negative cases are mainly caused by the following reasons code changes occur within tokens cases .
minor changes may not provide sufficient context for the model to infer thus judging comments as irrelevant to the edits.
the functionality of the code has undergone significant changes after editing cases .
the comments of such edits are usually new functional requirements.
these code changes or updates are too complicated for our model to perfectly handle thus wrongly determining them as negative cases.
to alleviate the above issues a possible optimization direction is to utilize data augmentation techniques to further enhance llm s code comprehension capabilities on such data samples.table v results of different methods on apu approach em cb codellama pr .
.
chatgpt .
.
soup codellama .
.
soup coditt5 .
.
answer to rq1 how effective is our approach for predicting valid comment edit pairs?
soup pachieved an accuracy of .
and a recall rate of .
on the vcp task far surpassing the current state of the art method.
b. rq2 effectiveness of soup on apu experimental setup for the automatic post updating task we fine tuned codellama with the train set of the high quality dataset the model with best performance on the validation set is chosen as s oup ufor this apu task.
then the test set is used for estimating the performance of our approach and other baselines.
evaluation metrics and baselines the apu task aims to generate the correct updating code for so posts which can be regarded as a generation task.
therefore we choose the widely used exact match em and codebleu cb as evaluation metrics for this rq.
emmetric is used to evaluate whether the model prediction exactly matches the ground truth.
cbis a common metric that measures lexical overlap for text code generations.
the following baselines are employed soup codellama we fine tuned codellama with prompt learning on the training set of the high quality dataset this model is denoted as soup codellama .
soup coditt5 zhang et al.
fine tuned a model coditt5 for software related editing tasks using the pretained language model codet5 .
their approach showed promising performance on various downstream tasks e.g.
comment updating bug fixing and automated code review .
the apu task is a similar task related to software editing the difference is our task aims to update so code snippets based on their comments.
we further fine tuned coditt5 on the apu task with our s oup framework denoted as soup coditt5 .
chatgpt chatgpt has demonstrated powerful ability on code generation test generation or bug fixing .
applying prompt engineering to guide chatgpt to complete particular tasks was widely explored by different researchers.
in our work we adopted chatgpt based on gpt .
turbo as a baseline and evaluated chatgpt s performance on the apu task by using the similar prompt as shown in table iii.
codellama pr in order to explore the effectiveness of employing the fine tuning strategy we directly tested the performance of codellama on the apu task without extra fine tuning only using the prompts from table iii.table vi comment categories for manually analyzed examples category positive negative improvement weakness inquiry addition total ex1.comment ithad bad initialicing trywith intminrating integer .max value intmaxrating integer .min value postid public static void main string args ...... intminrating integer.max value int maxrating integer.min value ...... ex2.comment donotuse!
with strings !
postid label ... sizechoice !
small !
small .equals sizechoice ... prediction ... sizechoice !
small !sizechoice.equals small ... fig.
.
positive and negative examples of soup on apu experimental results the experimental results of rq2 are demonstrated in table v. from the table we observe the following points directly using the prompt engineering approach with llms is not suitable for our apu task.
for example codellama pr 13b achieved the worst performance and even using chatgpt 175b only reached a .
exact match indicating that relying solely on prompts is insufficient to complete the apu task.
after fine tuning with our high quality dataset codellama s emvalue sharply increased from .
to .
i.e.
the result of s oup codellama .
this demonstrates that fine tuning llms with large scale high quality dataset can effectively adapt llm s learned feature representation to specific tasks significantly enhancing their performance on the apu task.
fine tuning llms plms with our s oup framework achieve best performance on this task.
after fine tuning s oup coditt5 with only 220m parameters can achieve a comparable or even better performance than s oup codellama.
our s oup framework is highly flexible and can be integrated with any open source llms or plms benefiting users to choose according to their own hardware resources and preferences.
manual analysis we sampled from the results of soup codellama for manual analysis considering s oupcodellama is more robust to noisy data and generalizes better on new datasets in rq3 .
considering the same comment can be addressed with different correct updatings we manually examined exact matching cases denoted as positive and non exact matching cases denoted as negative for analysis.
the positive cases are samples with codebleu equal to while the negative cases are samples with codebleu score less than .
specifically we sampled negative samples with codebleu less than .
and the other negative samples with codebleu larger than .
.
we carefully investigate these samples to see if their edits address the comment or not.
after our manual analysis non exact matching samples wereincorrectly labeled by s oup pin the data preparation stage these are invalid comment edit pairs that should be removed from training or testing.
only non exact matching samples were verified as valid comment edit pairs this is close to the precision .
we found in rq1.
furthermore we categorized these positive and negative examples according to the primary comment categories defined by zhang et al.
and the results are shown in table vi.
from the table it can be seen that soup uis good at updating so posts with improvement comment cases .
this is because improvement comments typically contain specific suggestions for changes which the model can easily understand the intention to modify.
fig.
ex1 shows one such case where the comment explicitly suggested the use of statements int minrating integer.max value and the according generated edits fully implements this suggestion.
soup uis not so good at handling post updates with comments categorized as weakness.
regarding the weakness comment type comments do not agree with the original code s functionality addressed his her concerns such as the comment above solutions doesn t work if the list is empty but i still need to handle that case .
the edits corresponding to these comments usually implement new features and the code undergoes significant changes before and after the update.
the model must infer the actual functionality requests of the commenter and modify the original code thoroughly or even regenerate code from scratch.
these edits are too complicated for soup uto perfectly handle.
a possible solution is to add more contextual information for these updates such as the question post.
it would be interesting to address these limitations but it is beyond the scope of this work.
there are negative cases that fully address their comments equivalently but not with exact matches.
ex2 in fig demonstrates such a case.
the codebleu between our generated edit and the ground truth edit is only .
.
the ground truth edit is !
small .equals sizechoice while the edit generated by our approach is !sizechoice.equals small these two edits are equivalent and both can fully address its comment.
this illustrates the em metric just estimates the lower bound of our model if we include these equivalent samples the model performance can be further improved.
answer to rq2 how effective is our approach for automatically updating so posts based on their comments?
soup uachieved a .
exact match rate particularly excelling in solving improvement type updates.
c. rq3 cross dataset evaluation experimental setup in this research question we aim to investigate whether our constructed high quality dataset can benefit the downstream apu task.
as introduced previously tang et al.
proposed a dataset of comment edit pairs based on their matching based approach.
however due to the limitations of their manually defined rules their datasettable vii information on datasets constructed by different methods dataset train validation test tang data hq data contains significant invalid comment edit pairs more than according to our preliminary study .
these invalid pairs introduce considerable noise that could negatively affect model training for the apu task.
therefore we conducted a crossdataset evaluation between different models.
in particular we prepared two datasets for this rq tang data we apply tang s matching method on our full scale dataset to obtain comment edit pairs finally comment edit pairs are extracted denoted as tang data.
notably tang data is smaller than the dataset presented in their paper this is because we only consider single code block edit in this study while tang et al.
collected edits with multi code blocks in their original paper.
hq data this is the high quality dataset constructed by s oup p the information of which is displayed in the last row of table i. we split tang data into train validtion test set by the ratio of the details of these two datasets are displayed in table vii.
we perform a cross dataset evaluation for this research question the models trained with hq data s train set are also evaluated on tangdata s test set and vice versa.
evaluation metrics and baselines we reuse the em evaluation metric for this research question.
we fine tuned coditt5 and codellama with the train sets of above two datasets respectively denoted as s oup coditt5 and s oupcodellama.
the models with their best performance on the validation set are chosen for test set evaluation.
experimental results from table viii it can be seen that the models trained with our hq data outperform the models trained with tang data.
for example s oup coditt5 trained with hq data achieves .
for em evaluation metric on its test set while s oup coditt5 trained with tangdata only achieves .
emon its test set.
this is because hq data has its advantage over tang data in terms of data quantity such as our hq data contains more than 62k data samples for training while tang data only contains 13k training samples which misses a large number of valid pairs.
by directly applying models trained with our hq data to tang data s test set we can still achieve a far better performance.
for example s oup coditt5 trained with hqdata achieve .
emon tang data s test set even better than the models trained with tang data itself e.g.
em of .
.
this is because our hq data has its advantage over hq data in terms of data quantity and data validity according to evaluation results reported in rq1 tang data contains only .
of valid data which means the dataset contains nearly half of the noise.
the models trained with tang data can hardly adapt to our hq data s test set.
for example when applying s oup coditt5 trained with tang data to our hq table viii theresults of cross dataset evaluation approach tang data test hq data test model train em em soup coditt5tang data .
.
hq data .
.
soup codellamatang data .
.
hq data .
.
data s test set the em significantly dropped from .
to .
.
this is reasonable because our constructed hq data covers wider usage scenarios e.g.
the valid comment edit pairs that no code terms are mentioned or modified.
models trained with tang data hardly learn these features and are unable to generate correct edits on these unseen samples.
answer to rq3 to what extent does the dataset influence model performance in the apu task?
our constructed dataset is superior to the dataset constructed by tang s method in terms of quantity validity and variety which is beneficial to improving the performance of various models on the apu task in different scenarios.
d. rq4 in the wild evaluation experimental setup the final goal of our s oup framework is to help stack overflow users and or developers automatically update answer post code snippets based on their comments in this research question we perform an in thewild evaluation to evaluate the effectiveness of our s oup for updating code snippets in real world stack overflow posts.
on stack overflow users can edit a post when they want to improve the post quality such as fixing typos or code related issues.
after edits are suggested it will be peer reviewed by several senior stack overflow users whether the edits should be accepted or not.
once accepted the posts will be updated with newly posted edits.
to perform this in the wild evaluation we collected code snippet comment pairs from stack overflow answer posts.
we first set a time window of one year from february to february to focus on recently raised comments as they are more likely to remain unaddressed.
after that we filtered comments containing specific keywords e.g.
deprecate outdate error issue as these comments often point out weaknesses within code snippets and indicate a need for timely updates.
we matched these comments with their code snippets initially identifying candidate pairs.
due to the presence of noisy samples e.g.
comments without editing purposes we further manually reviewed these pairs and only retained posts that needed updating.
considering the time and resource limitations we finished the in the wild dataset construction when the sample size reached .
for a given code snippet comment pair we then applied our s oup umodel to generate the updated code snippet based on its comment.
two authorsex2.
comment you should close the stream in the finally block or use a try with resource.public static void main string s ...... catch fis.close ioexception e e.printstacktrace catch ioexception e finally e fis .
printstacktrace close ex3.
comment did no one in the last years notice that this doesn t work?for double d double data should befor double d double data .....for double d double double data temparray double d ......ex1.
comment shouldn t it besystem.exit instead ofsystem.exit .
means a success return code whick doesn t seem to be the case if all the tasks couldn t be terminated......if !executor.awaittermination timeunit.microseconds system.out.println still waiting after 100ms calling system.exit ... system.exit ...... ex4.
comment although this is a highly voted and accepted answer it does not actually answer the question.
... if the idea was to demonstrate that bytearrayoutputstream has a tobytearray method then the intermediate baos.writeto myoutputstream should instead be more like baos.write new byte ...bytearrayoutputstream baos new bytearrayoutputstream baos.writeto myoutputstream write new byte baos.tobytearray fig.
.
in the wild evaluation examples then posted these generated edits to the original posts for updating.
experimental results we submitted edit requests generated by our s oup uto their corresponding stack overflow posts and of them have already been confirmed and accepted by stack overflow senior developers and maintainers.
to avoid subjective bias the developers were unaware of the edits were generated by our approach.
fig.
demonstrates three accepted examples i.e.
ex1 ex2 ex3 and one rejected example i.e.
ex4 from our in the wild evaluation.
for the sake of double blind reviewing we hide the details of the postid .
ex.
demonstrates an accept answer post that obtains votes the latest edits occurred a decade ago july however a developer recently february pointed out that it should use exit instead of exit because 0means a success return code our approach successfully fix this problem with editing.
before our updating this error has existed in stack overflow for more than years which can mislead developers in their daily development.
ex.
shows another edits verified by developers.
the commenter mentioned you should close the stream in the finally block our model successfully identifies fis in the try block and moves it to the finally block.
notably no code terms are mentioned in this comment and our model can capture the semantic meaning of the comment and generate correct edits successfully.
in ex.
multiple code terms are mentioned and our model successfully captures tiny differences for updating.
our model does not always generate correct edits one common failed situation is that the comments are too complex and or do not provide valuable editing information e.g.
just posting error messages when reusing this code .
ex.
demonstrates a rejected suggestion.
in this case our model wronglyupdated this code snippet because the edits were mentioned as an example by the commenter and so maintainers rejected our edits because the edit does not improve the quality of the post answer to rq4 how acceptable are our updated so posts in real world scenarios?
out of the suggested edits generated by soup were accepted with an acceptance rate of further demonstrating its practical value.
vi.
d iscussion a. data leakage issues our research examined three llms coditt5 codellama and chatgpt.
coditt5 based on the codet5 model and pretrained on the codesearchnet dataset from github was fine tuned for comment updating bug fixing and automated code review without using so data.
codellama built on llama2 incorporated stackexchange data in its pre training.
since chatgpt is a closed source model its use of so data is unclear.
however the low performance of codellama and chatgpt in our study em scores of .
and .
respectively suggests they may not utilize the so editing dataset in their model training or fine tuning.
b. ethical issues as we utilize the so dataset and genai in this study there are potential ethical issues related to data privacy intellectual property and forum quality as well as so policy compliance.
so dataset is publicly available we used the data solely for research without revealing user identities we also aligned with so s terms for non commercial use.
in terms of the quality concerns which were addressed through careful review by experienced so users and four authors and a limited experiment scale cases .
despite so s current policy against genai generated content our research aims to enhance so post quality through llms strictly for research.
overall we argue the ethical issues of this study are limited.
c. llms versus stack overflow although llms have made significant progress in coderelated tasks so still provides valuable insights that current llms e.g.
chatgpt cannot fully replicate.
on one hand so contains a wide range of practical problems that may exceed the capabilities of llms for example chatgpt only achieved .
em score in our apu task even our model can only achieve .
em score which means llms still have a large room for improvement.
on the other hand the quality of the llms generated content still varies significantly.
for example the ai generated answers can be false or misleading while so answers are more trustworthy since they have been voted on and peer reviewed by community members and experts.
therefore how to combine llms with so to better help developers for solving their daily technical problems will be an interesting research direction in the future.vii.
r elated work a. mining stack overflow posts currently many studies have been conducted on stack overflow including post recommendation query reformulation and content quality analysis .
zhang et al.
found that .
of outdated so answers were obsolete upon posting with only .
updated upon discovery.
ragkhitwetsagul et al.
reported that of users face issues with reused so answers such as mismatches and bugs.
wang et al.
linked so badge incentives to minor edits which are often reverted.
baltes et al.
created sotorrent dataset to refine so post edit history analysis.
diamantopoulos et al.
mined common edit patterns from sotorrent.
adaji et al.
found that high quality answers usually contain more comments.
zhang et al.
categorized so comments noting their informativeness for answer optimization.
son et al.
found that only .
of comments led to updates with .
of update requests ignored.
tang et al.
developed a method to match comments with edits.
while these studies highlight so post issues and the value of comments for code optimization they lack effective strategies for using comments to enhance so posts.
their work provides a good research motivation for the apu task we proposed.
b. code maintenance code review datasets for code maintenance often derived from programming competitions like marmoset introclass quixbugs codeflaws or open source projects such as ibugs defects4j bugsjs are pivotal for bug fixing tasks .
however these datasets are typically limited in scale or quality due to manual construction or heuristic based methods.
our work introduces a novel llm based framework s oup p which automates dataset construction yielding a dataset that surpasses existing ones in both quality and scale.
code review datasets on the other hand focus on methodlevel changes and are used to generate revised code from comments and existing code aiming to reduce maintenance costs.
unlike the apu task which deals with fine grained often uncompileable code snippets code review datasets are not directly applicable to the apu task due to differing data characteristics.
nevertheless successful model practices in code review such as the transforme model by tufano et al.
t5 s performance evaluation and the coditt5 model by zhang et al.
provide valuable insights.
moreover zhao et al.
have explored the efficacy of various large models including chatgpt codellama and codereviewer in code review identifying optimal prompts and models.
while these llms have shown promise in code review their applicability to the apu task remains untested which our paper addresses.
we are confident that our apu dataset will contribute to code maintenance and code review at a fine grained level and we will further investigate the applicability of the apu dataset in other domains in the future.viii.
t hreats to validity threats to internal validity.
the use of manually curated datasets in the vcp task introduces a risk of bias.
we addressed this by engaging multiple annotators to reduce subjectivity in the annotation process.
additionally the preliminary dataset analysis by the first author may carry inherent bias which we mitigated by validating with objective datasets in rq1 and rq3 confirming the consistency of our results.
the potential bias in manual error example selection was countered through stratified sampling ensuring a more objective representation of results.
threats to external validity.
our study s focus on java snippets from stack overflow could limit broader applicability.
to combat this limitation we highlight the adaptability of our approach to other platforms and languages with plans to explore this in future work.
threats to construct validity.
as our manual analysis section shows those code snippets that do not precisely match the true code snippets can also be reasonable.
the exact match metric merely reflects a lower bound of the model s ability to generate meaningful code snippets.
furthermore those code snippets with high codebleu scores might also be incorrect.
since most code snippets from stack overflow are difficult to run directly implementing an automatic functional equivalence assessment is very challenging.
therefore we further analyzed randomly selected error examples to further verify the correctness of the code generated by s oup.
ix.
c onclusion in this work we introduced s oup a llm based framework to automatically update posts in stack overflow.
our key contributions are twofold firstly we constructed a high quality dataset of valid comment edit pairs.
secondly we proposed a novel framework s oup to perform so post updating the extensive experiments show the superiority of our approach over a set of baselines.
our work first attempts to automatically update knowledge on platforms such as stack overflow aiming to improve code quality and reliability for so users and developers.
acknowledgment this research is supported by the starry night science fund of zhejiang university shanghai institute for advanced study grant no.
sn zju sias .
this research is supported by the national key research and development program of china no.
2021yfb2701102 .
this research is partially supported by the shanghai sailing program 23yf1446900 and the national science foundation of china no.
no.
no.
no.
and u20a20173 .
this research is partially supported by the ningbo natural science foundation no.
2023j292 and zhejiang provincial natural science foundation of china no.
lq24f020017 .
this research was also supported by the advanced computing resources provided by the supercomputing center of hangzhou city university.
the authors would like to thank the reviewers for their insightful and constructive feedback.