coca generative root cause analysis for distributed systems with code knowledge yichen li yulun wu jinyang liu zhihan jiang zhuangbin chen guangba yu michael r. lyu the chinese university of hong kong hong kong sar china.
email ycli21 ylwu24 jyliu zhjiang22 lyu cse.cuhk.edu.hk yugb5 mail2.sysu.edu.cn school of software engineering sun yat sen university guangzhou china.
email chenzhb36 mail.sysu.edu.cn abstract runtime failures are commonplace in modern distributed systems.
when such issues arise users often turn to platforms such as github or jira to report them and request assistance.
automatically identifying the root cause of these failures is critical for ensuring high reliability and availability.
however prevailing automatic root cause analysis rca approaches rely significantly on comprehensive runtime monitoring data which is often not fully available in issue platforms.
recent methods leverage large language models llms to analyze issue reports but their effectiveness is limited by incomplete or ambiguous user provided information.
to obtain more accurate and comprehensive rca results the core idea of this work is to extract additional diagnostic clues from code to supplement data limited issue reports.
specifically we propose coca a co de knowledge enhanced root c ause a nalysis approach for issue reports.
based on the data within issue reports coca intelligently extracts relevant code snippets and reconstructs execution paths providing a comprehensive execution context for further rca.
subsequently coca constructs a prompt combining historical issue reports along with profiled code knowledge enabling the llms to generate detailed root cause summaries and localize responsible components.
our evaluation on datasets from five real world distributed systems demonstrates that coca significantly outperforms existing methods achieving a .
improvement in root cause localization and a .
improvement in root cause summarization.
furthermore coca s performance consistency across various llms underscores its robust generalizability.
i. i ntroduction the reliability of complex distributed systems is critical for ensuring seamless operation and user satisfaction.
however despite best efforts users may still encounter various issues and report them through issue tracking systems such as jira .
these reports often contain essential information such as issue descriptions runtime logs and stack traces.
it is crucial for developers to identify the root causes and mitigate them promptly to maintain system reliability.
however manually understanding these reports and identifying the underlying root causes can be labor intensive and error prone given the complexity of modern distributed systems.
significant research has been devoted to developing automatic root cause analysis rca approaches .
extensive approaches focus on using comprehensive run time monitoring data e.g.
logs metrics and traces to locate root causes .
they typically compare the fault suffering data collected guangba yu is the corresponding author.from the faulty states with the fault free data collected from the normal state of the system to identify root causes nevertheless this monitoring data is often unavailable in practice because it requires extensive infrastructure to collect the required data.
for instance commonly used issue tracking systems such as jira generally contain only issue descriptions and fault suffering data in reports submitted by users .
another related work is fault localization which pinpoints specific locations in the code responsible for software failures.
however bug reproduction based fault localization techniques face challenges for localizing the faulty code in distributed systems which is extremely challenging with only issue reports available .
furthermore fault localization techniques are limited to addressing code related issues and cannot diagnose the non code related issues such as network failures.
to address the limitations of monitor data driven rca approaches recent approaches leverage the semantic comprehension and reasoning capabilities of natural language processing llms techniques to identify root causes from issue reports particularly when only limited run time monitoring data is available .
unfortunately the information contained in user submitted issue reports is often incomplete or ambiguous .
users typically can only provide symptom related details such as brief issue descriptions and snippets of log messages as the complexity of the distributed systems makes it difficult for them to detail the underlying execution logic directly responsible for the issue.
this lack of detailed execution information including the execution logic proceeding to failure architectural and interaction patterns embedded in system code limits the ability of llms to recommend the accurate root causes.
to bridge this gap the core idea of this work is to accurately retrieve diagnostic clues e.g.
execution logic from the complicated source code of a distributed system to supplement rca in data limited issue reports.
given the limited information in an issue report e.g.
issue description logs and stack traces our approach deals with the problem by reconstructing the execution logic preceding the issue by identifying and analyzing the relevant code.
this allows us to infer root causes even with incomplete textual data in the issue report.
inspired by the capabilities of llms to comprehend both code and natural language our intuition is to leverage llms to reasonarxiv .23051v1 mar 2025the execution paths prior to issue and understand the logic from relevant code.
however it is non trivial to integrate knowledge from codebase for diagnosing root causes in distributed systems without execution.
we have identified three major challenges.
linking logs directly to specific code positions is difficult because detailed metadata like line numbers is often missing and runtime log messages are usually different from the original logging statements .
the complexity of distributed systems with various invocation mechanisms such as static calls and remote procedure calls rpcs makes it hard to piece together execution paths from code positions.
due to the context window limitations llms struggle to understand long codebases making it difficult to use llms to analyze complex and extensive codebases of distributed systems during rca.
our work.
to address these challenges we propose coca the first co de knowledge enhanced root c ause a nalysis approach.
to address the first challenge the logging source retrieval phase employs a static analysis based backtracking approach to accurately identify the source logging statements even when the actual log messages differ significantly from the original logging statements.
these logging statements then provide code snippets directly related to the issue reports such as the lines of code executed immediately before the issue occurs.
next we design an execution path reconstruction step to incorporate method invocations thereby providing a more comprehensive execution context prior to the occurrence of an issue.
beyond analyzing call graphs between methods we propose an rpc bridging method aiming to reconstruct interactions prior to failure via rpcs.
then the failurerelated code profiling phase employs code snippet indexing and retrieval methods to analyze all obtained code snippets which aims to profile the snippets into a more compact form this allows them to fit within a reasonably sized window suitable for processing by llms.
finally during the root cause inference phase we construct a prompt based on historical issue reports the target issue report and all profiled code knowledge.
this prompt is then used to query the llm which generates a thorough root cause summary and localized responsible components.
to evaluate coca we conduct a comprehensive evaluation based on the dataset collected from five real world distributed systems.
our results demonstrate that coca achieves the best performance overall metrics in both root cause summarization and localization.
more precisely coca surpasses the current leading method by .
in root cause localization as measured by exact match and by .
in root cause summarization e.g.
bleu .
additionally coca consistently enhances performance across various underlying llms e.g.
llama .
thereby affirming its robust generalizability.
moreover we explore the individual contributions of each phase and provide the corresponding reasoning.
1a widely used protocol in distributed systems that allows a program to run a procedure on another machine as if it were local.
bug id closed majorjobclient fails removes files and in turn crashes mr am version .
.
resolution fixed assignee maintainerreporter reporter namedescription we ran into this multiple times.
mr jobclient crashes immediately.
the application was created and added to the list of apps.
so if the client contacts the rm it returns the application which perhaps is still in new state.logs info mapreduce.jobsubmitter number of splits xxx info mapred.yarnrunner appmaster capability memory info mapred.yarnrunner command to launch container for am info mapred.resourcemgrdelegate submitted application id to rm info mapreduce.jobsubmitter cleaning up the staging area stack traces at local trace org.apache...yarnremoteexceptionpbimpl failed to run job at org.apache...createyarnremoteexception at org.apache...getremoteexception at org.apache...submitjob at org.apache...submitjobinternal ...fig.
example of an issue report.
this paper s contributions are summarized as follows to our best knowledge coca2is the first work incorporating code knowledge into the automatic root cause analysis framework for issue reports in distributed systems.
we proposed and implemented coca with multiple generalized novel tools e.g.
rpcbridge which solve several critical challenges in rca field.
we collected and labeled a real world issue dataset of distributed systems for rca which includes real world issues covering five widely used distributed systems.
we extensively evaluate the performance of coca on realworld datasets.
the results demonstrate the effectiveness of coca and the adaptability of coca with different backbone models.
ii.
b ackground and motivation a. background issue reports in distributed systems.
issue reports including bug reports github issues and cloud incident tickets are reported by users to track and manage various types of system problems.
we unify the terms throughout the paper to issue reports for distributed systems.
as shown in figure when users encounter a system issue e.g.
jobclient fails they typically report the problem on the system s issue tracking platform as an issue report.
an issue report usually contains attributes such as the issue title description logs stack trace and meta information including version severity and status .
the description is a free text written by users that describes the problem aiding in understanding the issue.
logs offer a detailed record of system events and actions preceding the failure.
each log message typically includes a timestamp indicating when the event occurred a text template and dynamic variables that provide context specific information e.g.
request ids .
log sequences can reflect the execution path of the system within a specific time frame .
for instance the provided logs in the issue report suggest that the jobclient failure occurred 2after posting the application to rm and the staging area clean up.stack traces act as a snapshot of the call stack at the point where a failure occurred to pinpoint the context of the failure.
addressing an issue report manually can be a timeconsuming and tedious procedure .
it requires rich domain knowledge from maintainers to understand reproduce and eventually fix the problem.
this difficulty is further amplified in complex distributed systems.
b. motivating study in this section we present a motivating example to demonstrate our key insight using code knowledge to enhance system failure diagnosis.
specifically we present a real world issue report mapreduce from the widely used distributed system mapreduce .
a mapreduce job fails immediately after the user submits the job from the client machine.
to diagnose this issue a common practice is to investigate the error log messages to understand what went wrong.
for example the stack trace indicates that this job failed after submission.
once this error is identified the next step is to trace back through the logs to find preceding events that might lead to the issue.
in this case line is particularly suspicious because a cleaning upaction is performed after the job is submitted.
while log analysis sheds some light it falls short of revealing the underlying cause of the failure.
to find out the rationale behind this problem we need to understand the code execution logic that occurs before the issue arises.
specifically 1the client submits an application to the resource manager rm and 2immediately requests an application report.
however at the time of the request 3the rm has not completed setting up the application in the staging area i.e.
a temporary workspace to organize the necessary resources to run an application returns the null to the client.
due to the incomplete setup 4the client then mistakenly perceives the application as removed and initiates a cleanup process while printing the log message cleaning up the staging area... .
this action occurs while the rm is still reading the staging area after receiving the application submitted from the client.
this race condition results in a failed job run.
in this case existing solutions which solely rely on the analysis of reports exhibit a significant limitation they can only confirm job failure and pinpoint the suspicious cleanup action.
however accurately determining the root causes without a comprehensive understanding of the job s execution is hard without the involvement of the relevant code.
to address this limitation we introduce an innovative approach for a more thorough rca.
our approach synergies runtime log data with the corresponding executed code thereby providing a deeper comprehension of job failures.
specifically we plan to leverage the capabilities of llms which have demonstrated proficiency in comprehending both incident tickets and code to enhance the issue diagnosis process.
challenges while the integration of code knowledge into the rca procedure seems intuitive it poses three main challenges challenge sourcing log messages.
while stack traces provide specific class names file names and line numbers for precise location sourcing the original code lines of log messages is complex .
ideally we could use the above mentioned metadata e.g.
line numbers provided by logging libraries for precise localization.
however obtaining such detailed metadata is usually not feasible in real world scenarios .
furthermore runtime log messages often differ significantly from the logging statements in the code making it difficult to for a precise match.
in particular the variable in the logging statement may consist of multiple dynamic variables across different branches .
challenge reconstructing execution paths.
the complexity of execution paths in distributed systems makes their reconstruction from logs and stack traces in issue reports a formidable task .
building the dependencies between these code pieces is highly complex due to the diverse nature of dependencies in distributed systems including static calls grpc calls and others.
these complexities block the accurate determination of the code executed prior to failures.
challenge profiling failure related code snippets.
assuming the ability to reconstruct the execution paths prior to failures this would involve a vast amount of code.
failurerelated code knowledge comes not only from the executed code but also from its dependencies .
the absence of these dependencies could render code slices hard to comprehend.
however including all executed and dependent code significantly expands the context length potentially exceeding the model s input limit leading to high costs and model confusion.
iii.
m ethodology a. overview to address the above challenges we propose coca a framework that enhances rca of issue reports in distributed systems through code knowledge.
as illustrated in figure coca takes the issue report along with the associated project code of the corresponding version as input and generates the diagnostic result through a four phase process.
i in logging source retrieval phase coca localizes the code positions i.e.
the line numbers of the corresponding logging statements of log messages in issue reports and maps out the code points spreading across the systems.
ii in execution path reconstruction phase the identified code points are used to reconstruct the execution paths leading up to the failure with a call graph patched with rpc edges and inter method analysis.
iii in failurerelated code profiling phase coca analyzes and indexes the extracted method level code snippets with execution orders with signatures and documents.
the llm can retrieve the full method code of the code snippet based on indexes and initial comprehension of the issue.
iv during root cause inference phase in conjunction with similar historical issue reports 3status submitclient.submitjob ...if status null log.info cleaning up the staging area.... delete the corresponding directory jtfs.delete submitjobdir ture jobsubmitter.submitjobinternal rmdelegate.submitapplication ... rmdelegate.getapplicationreport ... yarnrunner.submitjob ....job.submit info .jobsubmitter number of splits info .yr appmaster capability memory info .yr command to launch container for am is ... info .rmgrdelegate submitted application id to rm info .jobsubmitter cleaning up the staging area path remotetrace at local trace org...yarnremoteexceptionpbimpl failed to run job org...createyarnremoteexception ...pbimpl.java at org...yarnrunner.submitjob yarnrunner.java at org...submitjobinternal jobsubmitter.java at org...run job.java .... retrievercalogscodejobsubmitteryarnrunnerrm jobclient submitstaging area readingcleanuprequestnull4fig.
a motivating example mapreduce .
project codebase issue reportbug idclosedmajorjobclient failsversion .
.
descriptionlogsinfo number of splits ...info submitted application id to .... info cleaning up the staging area ...stack traces at local trace at... at... infonumberofsplits ...infosubmittedapplication....infocleaningupthestaging...logs at localtraceatorg.class1....atorg.class2...stack traces template tree phase i logging source retrieval historical issues execution order ...code indexes signature docs signature docs code prompt rc summary ...rc localization ...rca result code pointclass quorumcallmethod waitforline code pointclass quorumcallmethod waitforline code pointclass quorumcallmethod waitforline llm phase ii execution path reconstructionphase iii failure related code profiling code snippetlongcounter var log.warn ... .... code snippetclasscall key .....qr.execute job publicstate execute .....log.error ... code snippetanalyzingrpcindexingrestoringretrievalmatchingretrievalgenerationphase iv root cause inference user fig.
the workflow of coca.
string msg string.format waited s ms timeout s ms for a response for s waited millis operator if !successes.isempty msg .
succeeded so far string.join successes.keyset if successes.isempty exceptions.isempty msg .
no responses yet.
log.warn msg hdfs qjournal client quorumcall.java fig.
an example of logging statement restoring.
coca diagnoses the issue using all acquired knowledge and outputs a comprehensive root cause summary and components.
b. logging source retrieval to address the challenge and identify the precise logging statement sources from the code of log messages in the given issue report coca employs a two step approach after execrating all logging statements and their corresponding code positions which is achieved by analyzing the project s logging library .
the first step involves restoring the logging statements that contain constructed variables back to primitive.
for the second step we introduce a template match algorithm to match log messages with restored logging statements of the corresponding system version.
restoring logging statements this step involves restoring logging statements by resolving constructed variables e.g.
log.warn msg which is further used for matching log messages to their corresponding logging statements.
asshown in figure the raw logging statement only indicates the constructed variable msg instead of the primitive template with constant strings and original variables.
to tackle this issue we analyze data dependency and control flow to obtain the primitive template set.
inspired by previous work coca employs a backtracking approach based on static analysis to identify and restore variables paths allowing coca to restore logging statements and obtain uncompressed string constants and variables from a simple msg.
specifically variable msg is constructed through several steps via conditional branches.
coca restores msg back to its original constant strings and variables for further matching.
specifically after restoring the logging statement in figure is reinstated to four different primitive templates e.g.
waited ms timeout for a response for .
no responses yet.
for each control branch.
log template matching upon restoring all the primitive log templates the subsequent step involves matching log messages to templates for determining the code positions.
during the process of log message matching a single log message could potentially be matched to multiple logging templates .
for instance there are two templates t1 job created executing and t2 job created executing on node .
every log message generated from template t2could be potentially matched by t1.
this might be due to the fact 4algorithm template matching procedure input template prefix tree log message l output matched template tm function match root l results if len l and root is a leaf node then add template of root to results return if l in root.children then match root l results if in root.children then for i to len l do if l in root .children then match root l results 12root root node of the template tree 13l results split tokens log message 14match root l results 15tm most static template results return tm that for some complexly constructed variables e.g.
msg it is challenging to restore them thoroughly leaving some unsolved variables to have such a general template.
to address this issue we proposed a template matching algorithm to match log messages back to logging statements accurately.
different from general logging parsing problems coca performs a non pruneable recursive matching to review all templates within the prefix tree maintaining a global perspective.
when a log message can match several log templates coca prioritizes the template with the most static parts i.e.
non characters .
noted that coca currently only analyzes logs from the system being diagnosed without analyzing logs from third party libraries as coca focuses on helping maintainers of target systems.
c. execution path reconstruction after successfully matching the code positions of log messages in a given issue report the subsequent task is to figure out how to incorporate these code points into the rca process.
an intuitive approach is extracting the surrounding lines of logging statements to profile the logs.
however this method presents challenges when applied to distributed systems where logs are not densely populated .
for two adjacent log messages numerous methods might execute across different components.
merely extracting surrounding lines without reconstructing and incorporating the executed code may miss important system runtime information .
in order to comprehend the pre failure execution state of the system we need to address the challenge reconstructing the execution paths prior to failure based on identified code points.
following the previous works coca constructs the inter procedure control flow graph icfg for connecting the code points with execution code paths.
typically the construction of the icfg involves both inter method server stub idl fileclient stub network utility idl compiler requestresponse requestresponse rpc bridging client invocation server implementationfig.
a common rpc call invocation process.
and intra method analysis.
inter method analysis primarily focuses on identifying the call graph of the system which includes determining the relationships of invocations between different methods both direct and indirect.
conversely intramethod analysis resolves the control flow within each individual method to identify its potential execution paths.
additionally in distributed systems simply analyzing the call graph to model the invocation relationships is insufficient given the widespread use of rpc .
rpc is a prevalent mechanism for facilitating interactions between individual instances .
these calls are made dynamically i.e.
via network utilities making it hard to statically capture the call edges between instances.
worse still existing approaches leave this issue unsolved .
to address this we propose an rpc bridging method suitable for common rpc frameworks such as grpc and thrift to improve the precision of constructing call graphs in distributed systems.
figure shows the typical invocation process for a remote call under common rpc frameworks.
initially rpc functions defined by the interface description language idl are transformed into client andserver stubs.
these stubs provide an interface for handling remote calls including serialization and message sending.
subsequently the client invokes the client stub which ultimately connects to the server stub to call the server s service functional implementation.
in this process the client stub invokes the server implementation via network utilities.
however as the actual call targets are determined at runtime this dynamic binding introduces significant uncertainty for static analysis thus breaking the call stack between client and server.
to address this uncertainty we leverage the implementation patterns in the call process to bridge the call stack coca first identifies client invocation to rpc functions by parsing the idl files e.g.
proto files for grpc and thrift files for thrift to obtain the rpc function names and interface names.
coca then searches for matched callees in the call graph confirming them as client to server rpcs.
second coca extracts the server side implementation classes which typically have names similar to their interface counterparts e.g.
the interface resourcetrack and its implementation resourcetrackservice .
we use substring matching for fuzzy identification.
to prevent false positives we further examine the declarations of the matched classes to ensure the implementation of corresponding interface classes.
finally we replace the callee 5codeprompt instruction please review the issue report and its execution paths then identify and return the method signature that requires detailed review for further diagnosis of the issue report.issue report ...executionpaths method a method b method c...codeindexes signature jobstatus submit job job cluster cluster docs internal method for submitting jobs to the system... param.... throws ioexception signature ... returned method signatures ...... rcapromptinstruction please infer localize and summarize the root cause of above input for further issue solving.
specifically you have two tasks root cause...issue report ...executionpaths method a method b method c...codesnippets jobstatus submit job job cluster cluster ...historicalissues issue report a with diagnosis result fig.
example prompt in coca.
of client to server rpcs with the corresponding server side implementation functions completing the client to server call stack bridging as shown by the dashed line in figure .
with this patched call graph and inter method analysis coca identifies multiple execution paths based on specified code points for the subsequent step.
note that while coca currently supports grpc framework it can be readily adapted to other rpc e.g.
thrift frameworks by replacing the code that parses the rpc definition files i.e.
idl files d. failure related code profiling upon reconstructing the execution paths based on code points the failure challenge emerges how to deal with thousands of lines of executed code?
the code profiling phase aims at profiling the code information while concurrently reducing redundant failure unrelated code snippets and minimizing the cost i.e.
context length of coca.
to retrieve the code snippets that contribute to failure comprehension or are prone to failure and forward them to the backbone llm coca utilizes a two staged approach that involves indexing and retrieval.
code snippet indexing to streamline the process of retrieving relevant code snippets we implement a methodlevel indexing framework for executed paths.
the indexing framework creates individual indexes for each method in the execution paths equipping the backbone model with sufficient key information to make selections without the need to process the entire method code.
given the experiment datasets seen section iv a were collected from well maintained systems we use standard method docs along with the method signature generated by soot as indexes as shown in code prompt part of figure .
if docs are unavailable we can generate standard docs using llm based methods .
code snippet retrieval we provide the llm with a comprehensive understanding of the issue report and analyzed execution paths along with indexes for subsequent retrieval.with the initial comprehension of the target issue report and execution paths preceding to the failure coca will allow llm to identify code snippets that contribute to failure comprehension or are prone to failure.
to accomplish this we input both the issue report and the code snippet indexes of all analyzed execution paths into the backbone llm for retrieval from the codebase.
as shown in figure it will return the method signatures that require detailed review.
based on these generated method signatures the corresponding method code is then extracted preparing for the next phase.
e. root cause analysis in context learning to learn the historical experiences from previous issue reports of current system coca employs the in context learning icl strategy which has proved its effectiveness in bug diagnosis task .
the icl strategy improves the effectiveness of llms based on historical experience.
specifically this strategy provides a few historical issue reports sampled from the collected and labeled dataset which was detailed in section iv a however the inherent heterogeneity of issue reports presents the challenge of finding similar and useful historical examples.
embedding based similarity metrics though proved effective for certain applications may not perform optimally in the context of diverse issue reports .
we use the bm25 similarity to select these examples.
the bm25 function based on the term frequency inverse document frequency tf idf method places greater emphasis on keywords making it particularly suitable for component matching among issue reports.
we specifically select the top five historical issue reports with the highest similarity scores.
diagnosis result inference as illustrated in figure after comprehensively analyzing the issue report reconstructed execution paths and retrieved code snippets coca is wellequipped to infer the root cause of the issue.
coca structures the prompt by beginning with the issue report followed by the relevant knowledge obtained from the analysis that contributes to the root causes analysis.
specifically coca focuses on two key tasks root cause summarization and localization.
the goal of root cause summarization is to provide a clear and concise guide for maintainers in their subsequent mitigation and fixing efforts.
in contrast root cause localization aims to provide the analyzed scope for the actual root cause facilitating further fixes by enumerating the potential root cause components ranked by likelihood.
iv.
experiment setup we want to answer the following research questions rqs .
rq1 how effective is coca in rca compared with existing methods?
rq2 what are the impacts of different phases in coca?
rq3 how generalizable is coca with different backbone models?
6table i details of the datasets.
system sloc failure types issues mapreduce 721khangs inconsistent state incorrect result resource exhaustion resource leak unexpected termination hdfs 706kdata loss hangs inconsistent state incorrect result resource leak unexpected termination hbase 912k data loss hangs inconsistent state incorrect result unexpected termination cassandra .1m hangs inconsistent state incorrect result unexpected termination zookeeper 184k hangs incorrect result resource leak total data loss hangs inconsistent state incorrect result resource exhaustion resource leak unexpected termination sloc is calculated based on the latest version.
the actual sloc may vary depending on the version in which the issue occurred.
a. dataset collection our experiments utilize datasets from a previous study collected from the jira a public issue tracking platform.
as shown in table i the systems from which these issues were collected range from 144k to 3m lines of code with to years of development.
these systems are distributed and include computing storage and configuration frameworks.
all the issues we studied are distributed issues meaning the failure propagation involves multiple nodes or components which makes rca particularly challenging.
to make our analysis more rigorous we excluded issues that lacked run time logs and filtered out those that lacked a detailed root cause analysis upon the closure.
labelling all the issues in our study have been fixed and thoroughly investigated .
the discussions and patches related to these issues are publicly available which facilitates the labeling process.
two experienced developers both are contributors of at least one above system undertook the labeling procedure jointly.
to label root cause summary annotators read and summarize the root cause identified from the discussions in the issue report and previous studies .
they then distill the diagnostic procedure from the discussion refining the summary based on mitigation strategies and fixing items.
the goal of the summary is to provide a clear and concise guide for maintainers to diagnose the issue.
for root cause localization due to differences in the granularity we standardized the process by identifying all components mentioned within the discussion and postmortem studies allowing all approaches to select from them.
these components include code related elements such as specific classes e.g.
jobclient as well as non code related components including system resources e.g.
network rpc and abstract concepts e.g.
race conditions deadlocks .
the ground truth for root cause localization is derived from the studied root cause and the corresponding fixing patches and it consists of one or two components.
b. evaluation metrics root cause summarization we use bleu meteor and rouge metrics consistent with previous research to evaluate the quality of the generated root cause summarization from the aspect of text similarity with ground truth.
we also leverage widely usedembedding models openai embedding to embed the root cause summary and calculate the semantics similarity.
following previous works we also employ human evaluation usefulness indicates whether the summary accurately explains why the issue occurred and how to fix it.
the same experts who annotated the datasets conducted this evaluation ensuring consistency and a high level of expertise.
they are offered the summary from all baselines alongside the corresponding ground truth for each issue.
the evaluators are instructed to compare these results against the ground truth and rate the usefulness for diagnosing the given issue of each summary on a scale from to .
we then calculate the average score among evaluators for certain baseline.
root cause localization we use the following two metrics to evaluate the effectiveness of coca and baselines.
exact match measures whether the generated root cause components exactly match the ground truth set of components.
any mismatch either fewer or more components than the ground truth results in a score of .
top k measures whether the ground truth components are completely included within the top k k mostlikely root cause components generated by models.
for this metric we prompt the model to generate the k most likely root cause components.
c. comparative methods we deliberately refrained from comparing coca with traditional and aiops methodologies that utilize fault free data during the procedure.
typically when issues are reported and tracked using platforms such as jira obtaining fault free comparative data is not feasible .
additionally we excluded autofl because solely based on ticket it was hard to reproduce the bugs of distributed system and obtain the runtime coverage.
we selected rcacopilot and react as our primary baselines because of their similar rca procedures and capabilities in handling both localization and summarization tasks.
both models will retrieve the historical issue reports as knowledge sources to enhance the performance of root cause analysis.
since neither model is open source we reimplemented their methods based on the corresponding papers.
we also included the base model with five fixed examples to demonstrate the task for comparison.
all methods utilize the same backbone llm to ensure a fair comparison.
7d.
implementation details the static analysis module in coca has been implemented using approximately lines of code written in both java and python.
this development employs soot and eclipse jdt core facilitating joint analysis of both java bytecode and source code.
the experiments of coca and all baselines were conducted on a linux machine ubuntu lts .
.
for gpt 4o gpt .
llama .
gemini .5pro and claude .
we use the public apis provided by openai deepinfra google and anthropic corresponding to the model gpt 4o gpt .5turbo0125 llama .
405b gemini .
pro andclaude .5sonne respectively.
we set the temperature of all models as zero to ensure the reproduction.
the default number of sampled examples is set to for all approaches.
the depth of analyzed constructed execution path is .
v. evaluation results a. rq1 effectiveness of coca to evaluate the performance of coca in rca tasks we conduct a thorough evaluation with comparison to other baselines.
the results of this evaluation are detailed in table ii and all approaches employ gpt 4o as the uniform backbone model.
we analyze the evaluation results from two dimensions root cause summarization androot cause localization .
root cause summarization we compare coca with baselines in terms of root cause summarization from the aspects of syntax similarity semantic similarity and humanevaluated metrics.
regarding syntax similarity we observe that coca outperforms the best performing approach rcacopilot showing improvements of .
in bleu .
in rouge and .
in meteor scores on average.
these enhancements are even more pronounced in specific system for instance when analyzing issues from zookeeper coca surpasses the best performing approach by .
in bleu4.
for semantic similarity coca consistently outperforms the best performing baseline with improvements ranging from .
to .
across all evaluated systems providing another dimension of effectiveness against the baselines.
in terms of usefulness which could directly measure the help for the maintainers coca exhibits substantial enhancements.
this underscores the practical impact of integrating code knowledge in real world distributed system maintenance.
root cause localization according to the evaluation results it is clear that coca outperforms all baselines in faulty components localization.
coca outperforms the best performing baseline in all evaluated systems across almost all metrics.
specifically when averaged across all systems coca outperforms rcacopilot by .
.
and .
for metric exact match top andtop respectively.
notably despite utilizing the same foundational model i.e.
gpt 4o coca achieves a remarkable .
improvement of the base model in exact match the most challenging metric underscoring the effectiveness in the task of root cause localization.
these results underscore the importance of integrating internalsystem knowledge i.e.
code over relying solely on historical data e.g.
react in rca tasks.
answer to rq1.
by incorporating the code knowledge into the procedure of rca coca demonstrates superior performance in both dimensions of root cause summarization and localization significantly outperforms all baselines and especially the base model.
b. rq2 impact of different phases in coca to evaluate the individual contribution and parameter setting of each phase within the coca we conducted an ablation study by creating three variants of coca each missing a different phase.
specifically to evaluate the framework without the logging source retrieval phase we adopted a new codeenhanced method that retrieves the top methods calculated by openai embedding similarity from the project codebase for comparison.
additionally to estimate the impact of analyzed execution depth we also investigated execution depth parameters ranging from to .
table iii demonstrates the experimental results.
the results indicate that without conducting the logging source retrieval phase the overall performance of coca generally declines across all metrics.
there is a decrease of in exact match while meteor andusefulness fall by .
and .
respectively.
more critically some metrics e.g.
meteor top5 even fall below those of the raw llm.
this suggests that when the retrieved code snippets are irrelevant to the failure at hand they can potentially mislead the model.
additionally when execution path reconstruction is not conducted the overall performance of coca also deteriorates significantly.
the bleu score drops from .
to .
and the usefulness score from .
to .
representing decreases ranging from .
to .
respectively.
it is noteworthy that the exact match performance is even lower than search based method.
this indicates that without analyzing the execution paths and relying solely on the log surrounding code snippets it is challenging for the llm to accurately identify the root cause.
as shown by our experiments with different execution depths where each invocation is treated as one degree degree paths may miss critical code information while degree paths may introduce too much irrelevant code.
the experiment results suggest that degree paths default setting of coca provide optimal performance in this scenario.
additionally regarding rpc invocations approximately .
of the invocations in a reconstructed execution path are rpc invocations demonstrating the effectiveness of proposed rpcbridge.
furthermore when the phase of failure related code profiling is omitted the performance of coca for root cause localization drops obviously .
reflected by exact match while the performance of root cause summarization remains relatively high.
furthermore it is puzzling to observe that the top accuracy remains at a similar level with top accuracy.
upon investigating this phenomenon we discovered that it is due to extremely long code contexts.
these results underscore the critical role of the failure related code profiling phase.
8table ii root cause analysis results from both summarization and localization dimensions for each system.
system modelroot cause summarization root cause localization bleu rouge meteor semantics usefulness exact match top top mapreducebase model .
.
.
.
.
.
.
.
rcacopilot .
.
.
.
.
.
.
.
react .
.
.
.
.
.
.
.
coca .
.
.
.
.
.
.
.
hdfsbase model .
.
.
.
.
.
.
.
rcacopilot .
.
.
.
.
.
.
.
react .
.
.
.
.
.
.
.
coca .
.
.
.
.
.
.
.
hbasebase model .
.
.
.
.
.
.
.
rcacopilot .
.
.
.
.
.
.
.
react .
.
.
.
.
.
.
.
coca .
.
.
.
.
.
.
.
cassandrabase model .
.
.
.
.
.
.
.
rcacopilot .
.
.
.
.
.
.
.
react .
.
.
.
.
.
.
.
coca .
.
.
.
.
.
.
.
zookeeperbase model .
.
.
.
.
.
.
.
rcacopilot .
.
.
.
.
.
.
.
react .
.
.
.
.
.
.
.
coca .
.
.
.
.
.
.
.
table iii ablation study of coca.
system settingroot cause summarization root cause localization bleu rouge meteor semantics usefulness exact match top top allcoca .
.
.
.
.
.
.
.
w o logging source retrieval .
.
.
.
.
.
.
.
w o execution path reconstruction .
.
.
.
.
.
.
.
w degree execution path reconstruction .
.
.
.
.
.
.
.
w degree execution path reconstruction .
.
.
.
.
.
.
.
w o failure related code profiling .
.
.
.
.
.
.
.
only w full jira discussion .
.
.
.
.
.
.
.
answer to rq2.
the ablation study indicates that the elimination of any component notably reduces the overall performance.
particularly the results demonstrate that merely searching the codebase without the coca framework falls short of expectations emphasizing that each component contributes to the overall performance.
c. rq3 generalizability of coca in this rq we evaluate the performance of coca by utilizing various llms in conjunction with our framework.
we have selected three representative llms that are frequently used in research specifically gpt .
gpt 4o llama .1405b gemini .
pro and claude .
sonnet .
we selected versions of these models with enough parameter size to ensure the capability of complex prompt understanding .
the experimental results are shown in table.
iv.
our observations indicate that the coca framework can consistently improve the performance of all utilized base models in terms of all metrics by a large margin.
on average all models have been improved by .
.
and .
in localizing the root causes based on the metrics of exact match top and top respectively.
furthermore when it comes to summarizing the root causes we observed an average improvement of .
as reflected by bleu .
as reflected by meteor and .
as reflected by usefulness .
particularly noteworthy is the performance of the claude .
sonnet model within the coca framework which surpasses the raw model in top .
the experiments compared the performance of the latest and most powerful llms in root cause analysis tasks.
the results indicate that although gpt4ois not the most advanced raw model from some aspects e.g.
exact match the enhancements provided by coca are still the most significant.
the results not only demonstrate the advantage of coca s design but also demonstrate the generalizability for different backbone models.
we believe that the performance of coca can be further improved with the development of llms.
furthermore the experiments with full jira discussion in table.
iii further demonstrate coca s generalizability.
the results show dramatic improvements after incorporating detailed jira issue discussions indicating that those llms have limited memorization of jira issue results.
moreover table.
iv shows that all five llms perform moderately when provided with only the raw issues which more closely resembles their training data format .
however after incorporating the knowledge provided by coca all models exhibit consistent and significant improvements in performance.
9table iv the performance of coca with different backbone models.
system frameworkroot cause summarization root cause localization bleu rouge meteor semantics usefulness exact match top top gpt 4obase .
.
.
.
.
.
.
.
coca .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
gpt .5base .
.
.
.
.
.
.
.
coca .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
llama .
405bbase .
.
.
.
.
.
.
.
coca .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
claude .
sonnetbase .
.
.
.
.
.
.
.
coca .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
gemini .
probase .
.
.
.
.
.
.
.
coca .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rs the root cause of the scanner id collision issue in hbase arises from rapid restarts of the rs.
when rs restarts quickly it will reuse scanner ids that had been assigned to clients previously due to the unstable random number generation method.
since each client and server pairing operates on a unique scanner id to track data sequence numbers through variable nextcallseq moreover if the new rs happens to assign a new client the same scanneridas the old one a nextcallseqerror may occur causing potential collisions errors.root cause summary from coca info server client tried to access missing scanner num1info server client tried to access missing scanner num2error hregionserver outoforderscannernextexception expected nextcallseq but the nextcallseq got from client request scanner id num3 number of rows close scanner false next call seq the root cause of the issue appears to be related to the scanner id in hbase.
the logs show instances of clients trying to access missing scanners indicating that the scanner ids they are trying to access are no longer valid.
additionally an exception is thrown indicating that the sequence of scanner calls from the client is out of order.
this could be due to a client side issue such as a bug in the client code or a delay in the client side processing causing the scanner to expire.root cause summary from gpt 4o while true scannerid rand.nextlong if scannerid continue ... ....hregionserver.scan hregionserver.addscanner ... request12scannerid usingnextccallseq clients fig.
case study of hbase .
answer to rq3.
coca consistently enhances the performance of both root cause summarization and localization regardless of the type of backbone models employed.
moreover our experiments demonstrate the limited data leakage further highlighting the generalizability of coca.
vi.
d iscussion a. case study to demonstrate the practical value of coca for system maintainers we analyze hbase 9824as a practical example.
this case illustrates how coca significantly improves the efficiency and accuracy of root cause analysis for system maintainers.
the issue involves scanner id collisions in the hbase system where multiple clients attempt to access nonexistent scanner ids resulting in nextcallseq mismatch errors.
while gpt 4o provides a surface level diagnosis based solely on the issue report and logs identifying only that the scanner ids are invalid and clients are unaware of this condition coca delivers substantially more actionable insights for maintainers by leveraging code knowledge extracted from logs and traces.
coca determines the root cause of the issue is reuse of scanner ids then concludes that scanner ids are generated through random numbers by reviewing the code.
when an hregionserver rs quickly restarts it is treated as a new rs and is assigned the same scanner id as before due to the unreliable random number generation logic without dynamic seeds.
consequently this leads to inconsistencies of scanner ids for clients resulting in a system crash.
compared to solely relying on issue reports the integration of code knowledge can substantially enhance the information available to the llm prior to making an inference thereby generating a more comprehensive and detailed rca result.
to demonstrate coca s benefits for maintainers we conducted a controlled experiment of this case with an experienced annotator who is also the contributor to hbase.
when presented with gpt 4o s rca summarization the annotator spent over hours analyzing the system s execution path and related code based on the traces and logs available in the issue report which coca had already automatically summarized.
notably this issue took a full day of active discussion on the jira forum before reaching the conclusion.
this real world example demonstrates how coca accelerates the diagnosis process by automatically reconstructing and analyzing system execution paths with llm that would otherwise require hours of manual investigation by skilled maintainers.
b. practicality of coca coca is designed to facilitate the diagnosis of issues and incidents in real world distributed systems such as modern cloud systems e.g.
azure and aws .
these mature systems have employed issue tracking platforms such as icm in microsoft jira used by apache to streamline issue management and enhance system reliability.
when an incident or issue is reported coca will outperform current methods in three aspects.
in terms of effectiveness our extensive experiments on real world jira issues demonstrate coca s strong performance in production environments.
the evaluation results show high accuracy in root cause analysis consistent performance across different 10types of issues which is the strong evidence of coca s practical capability in real world systems.
additionally coca s extensibility allows for seamless adaptation to various issue tracking systems without the need to reproduce the issue.
these platforms share similar settings with user reported issues including descriptions logs and traces.
besides for efficiency it typically takes maintainers several hours to days based on average response time from jira issue maintenance records .
however coca provides rca results with an average response time of .
seconds while automatically identifying and analyzing an average of .
methods from execution paths in hundreds of lines of logs and stack traces.
c. threats to validity potential data leakage.
one of the main concerns of this work is the potential data leakage issue due to the utilization of public issue reports.
specifically the target issue report may be trained during the pretraining or retrieved by our sampling algorithm leading it to memorize rather than infer the result.
however this risk may be mitigated by several factors.
firstly many of the root causes are not available on jira which reduces the chances of data leakage.
secondly our complex prompt format is unlikely to appear directly in the training dataset especially compared to raw issue reports as the experiment results shown in table iv for representative llms.
thirdly as shown in table iii the performance improves significantly after feeding the raw jira issue discussions demonstrating that those llms have limited memorization of rca results.
moreover during the dataset collection phase we manually reviewed each issue s discussion and linking relationships to ensure that there were no duplicate issue reports with the same root cause.
the precision of executed path reconstruction.
static analysis has inherent limitations in its analysis boundary and struggles with dynamic bindings .
this affects its accuracy in constructing call graphs especially when dealing with network interactions and the use of reflection proxies and multi threading in distributed systems.
to mitigate this we propose and implement a novel rpc bridging method to integrate rpcs within coca to enhance the its analytical capabilities.
in addition ablation studies also show that compared with similarity based code base retrieval methods reconstructing execution paths can maximize the introduction of fault related code fragments and restore the system execution state as much as possible.
vii.
r elated work monitor data driven rca.
most of the existing rca approaches only use monitor data e.g.
logs or traces as input.
they typically compare whether the runtime data pattern changes under normal and abnormal system states to figure out the root cause.
for instance logcluster utilizes a clustering algorithm to collect log clusters which are then employed to discern log abnormal patterns by comparing them with test log sequences.nezha contrasts event graphs from fault free and faultaffected systems to deduce the root causes.
however this type of approach has three limitations they rely on the normal runtime data pattern of the system which is not readily available in practice.
they usually only output the root cause module and lack a root cause summarization.
they do not incorporate related code as contextual information leading to underperforming diagnostic results.
llm based rca.
the recent advent of llms has facilitated the proposal of several llm based rca methodologies since llms inherently possess domain knowledge of systems and corresponding failures.
rcacopilot which was introduced as an llm based rca approach equipped with retrieval tools.
by analyzing historical incidents react can understand complex system issues thereby continuously enhancing its performance.
compared to existing llm based rca approaches coca extends its diagnostic capabilities beyond runtime information and historical bug reports by incorporating code knowledge.
coca can retrieve and analyze internal system knowledge i.e.
code without requiring issue reproduction to obtain code coverage during execution which fault localization methods did but is impractical in distributed systems .
from the perspective of fault types coca address a variety of issues including both code bugs and non code problems e.g.
resource limitations misconfigurations or environmental factors by providing code knowledge to enhance the comprehension of the system thereby facilitating the rca process while fault localization methods focus solely on localizing reproducible code bugs.
regarding analysis scope coca performs root cause analysis by summarizing and localizing runtime problems.
in contrast fault localization techniques are restricted to localizing faults within the code itself without addressing non code or environmental factors.
thus coca overcomes the limitations of previous llmbased approaches by providing a more thorough and contextualized rca method.
viii.
c onclusion in this paper we propose coca the first work incorporating code knowledge into an automatic root cause analysis framework for issue reports in distributed systems.
experimental results show that coca outperforms all baselines and can be generalized to various llms.
coca demonstrates its promising future in the context of the evolving llm techniques elevating the upper limit of llm s capability in rca task and benefiting both researchers and maintainers.
acknowledgment the work described in this paper was supported by the research grants council of the hong kong speci al administrative region china no.
cuhk of the general research fund and rgc grant for theme based research scheme project rgc ref.
no.
t43 n and the national nature science foundation of china under grant no.
.
11references apache jira mar .
.
available org jira secure dashboard.jspa g. yu p. chen y .
li h. chen x. li and z. zheng nezha interpretable fine grained root causes analysis for microservices on multi modal observability data in proceedings of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering pp.
.
x. li p. chen l. jing z. he and g. yu swisslog robust and unified deep learning based log anomaly detection for diverse faults in ieee 31st international symposium on software reliability engineering issre .
ieee pp.
.
q. lin h. zhang j. g. lou y .
zhang and x. chen log clustering based problem identification for online service systems in proceedings of the 38th international conference on software engineering companion pp.
.
c. m. rosenberg and l. moonen spectrum based log diagnosis inproceedings of the 14th acm ieee international symposium on empirical software engineering and measurement esem pp.
.
f. lin k. muzumdar n. p. laptev m. v .
curelea s. lee and s. sankar fast dimensional analysis for root cause investigation in a large scale service environment proceedings of the acm on measurement and analysis of computing systems vol.
no.
pp.
.
t. hirsch and b. hofer root cause prediction based on bug reports in2020 ieee international symposium on software reliability engineering workshops issrew .
ieee pp.
.
j. huang z. jiang j. liu y .
huo j. gu z. chen c. feng h. dong z. yang and m. r. lyu demystifying and extracting fault indicating information from logs for failure diagnosis in ieee 35th international symposium on software reliability engineering issre .
ieee pp.
.
w. yuan s. lu h. sun and x. liu how are distributed bugs diagnosed and fixed through system logs?
information and software technology vol.
p. .
s. kang g. an and s. yoo a quantitative and qualitative evaluation of llm based explainable fault localization fse .
s. kang b. chen s. yoo and j. g. lou explainable automated debugging via large language model driven scientific debugging arxiv preprint arxiv .
.
r. just d. jalali and m. d. ernst defects4j a database of existing faults to enable controlled testing studies for java programs in proceedings of the international symposium on software testing and analysis pp.
.
d. yuan y .
luo x. zhuang g. r. rodrigues x. zhao y .
zhang p. u. jain and m. stumm simple testing can prevent most critical failures an analysis of production failures in distributed data intensive systems in 11th usenix symposium on operating systems design and implementation osdi pp.
.
y .
chen h. xie m. ma y .
kang x. gao l. shi y .
cao x. gao h. fan m. wen et al.
automatic root cause analysis via large language models for cloud incidents in proceedings of the nineteenth european conference on computer systems pp.
.
k. an f. yang l. li z. ren h. huang l. wang p. zhao y .
kang h. ding q. lin et al.
nissist an incident mitigation copilot based on troubleshooting guides arxiv preprint arxiv .
.
d. roy x. zhang r. bhave c. bansal p. las casas r. fonseca and s. rajmohan exploring llm based agents for root cause analysis arxiv preprint arxiv .
.
t. ahmed s. ghosh c. bansal t. zimmermann x. zhang and s. rajmohan recommending root cause and mitigation steps for cloud incidents using large language models in ieee acm 45th international conference on software engineering icse .
ieee pp.
.
z. jiang j. liu j. huang y .
li y .
huo j. gu z. chen j. zhu and m. r. lyu a large scale benchmark for log parsing arxiv preprint arxiv .
.
v .
bushong r. sanders j. curtis m. du t. cerny k. frajtak m. bures p. tisnovsky and d. shin on matching log analysis to source code a systematic mapping study in proceedings of the international conference on research in adaptive and convergent systems pp.
.
y .
peng c. wang w. wang c. gao and m. r. lyu generative type inference for python in 38th ieee acm international conference on automated software engineering ase .
ieee pp.
.
y .
ding z. wang w. ahmad h. ding m. tan n. jain m. k. ramanathan r. nallapati p. bhatia d. roth et al.
crosscodeeval a diverse and multilingual benchmark for cross file code completion advances in neural information processing systems vol.
.
k. papineni s. roukos t. ward and w. j. zhu bleu a method for automatic evaluation of machine translation in proceedings of the 40th annual meeting of the association for computational linguistics acl pp.
.
meta llama .
.
.
available meta llama z. chen y .
kang l. li x. zhang h. zhang h. xu y .
zhou l. yang j. sun z. xu et al.
towards intelligent incident management why we need it and how we make it in proceedings of the 28th acm joint meeting on european software engineering conference and symposium on the foundations of software engineering esec fse pp.
.
j. liu s. he z. chen l. li y .
kang x. zhang p. he h. zhang q. lin z. xu et al.
incident aware duplicate ticket aggregation for cloud systems arxiv preprint arxiv .
.
y .
huo y .
li y .
su p. he z. xie and m. r. lyu autolog a log sequence synthesis framework for anomaly detection in 38th ieee acm international conference on automated software engineering ase .
ieee pp.
.
b. chen j. song p. xu x. hu and z. m. jiang an automated approach to estimating code coverage measures via execution logs inproceedings of the 33rd acm ieee international conference on automated software engineering pp.
.
j. liu j. huang y .
huo z. jiang j. gu z. chen c. feng m. yan and m. r. lyu scalable and adaptive log based anomaly detection with expert in the loop arxiv preprint arxiv .
.
d. yuan h. mai w. xiong l. tan y .
zhou and s. pasupathy sherlog error diagnosis by connecting clues from run time logs in proceedings of the fifteenth international conference on architectural support for programming languages and operating systems pp.
.
j. huang j. liu z. chen z. jiang y .
li j. gu c. feng z. yang y .
yang and m. r. lyu faultprofit hierarchical fault profiling of incident tickets in large scale cloud systems in proceedings of the 46th international conference on software engineering software engineering in practice pp.
.
j. dean and s. ghemawat mapreduce simplified data processing on large clusters communications of the acm vol.
no.
pp.
.
z. yang z. zhao c. wang j. shi d. kim d. han and d. lo what do code models memorize?
an empirical study on large language models of code arxiv preprint arxiv .
.
s. gao x. c. wen c. gao w. wang h. zhang and m. r. lyu what makes good in context demonstrations for code intelligence tasks with llms?
in proceedings of the 38th international conference on automated software engineering ase .
s. he j. zhu p. he and m. r. lyu loghub a large collection of system log datasets towards automated log analytics arxiv preprint arxiv .
.
z. jiang j. liu j. huang y .
li y .
huo j. gu z. chen j. zhu and m. r. lyu a large scale evaluation for log parsing techniques how far are we?
in proceedings of the 33rd acm sigsoft international symposium on software testing and analysis .
x. wang x. zhang l. li s. he h. zhang y .
liu l. zheng y .
kang q. lin y .
dang et al.
spine a scalable log parser with feedback guidance in proceedings of the 30th acm joint european software engineering conference and symposium on the foundations of software engineering pp.
.
d. schipper m. aniche and a. van deursen tracing back log data to its log statement from research to practice in ieee acm 16th international conference on mining software repositories msr .
ieee pp.
.
y .
li x. zhang s. he z. chen y .
kang j. liu l. li y .
dang f. gao z. xu et al.
an intelligent framework for timely accurate and comprehensive cloud incident detection acm sigops operating systems review vol.
no.
pp.
.
y .
li y .
peng y .
huo and m. r. lyu enhancing llm based coding tools through native integration of ide derived static context arxiv preprint arxiv .
.
y .
li y .
huo z. jiang r. zhong p. he y .
su and m. r. lyu exploring the effectiveness of llms in automated logging generation an empirical study arxiv preprint arxiv .
.
z. jiang j. liu z. chen y .
li j. huang y .
huo p. he j. gu and m. r. lyu lilac log parsing using llms with adaptive parsing cache proceedings of the acm on software engineering vol.
no.
fse pp.
.
j. huang z. jiang z. chen and m. r. lyu ulog unsupervised log parsing with large language models through log contrastive units arxiv preprint arxiv .
.
p. he j. zhu z. zheng and m. r. lyu drain an online log parsing approach with fixed depth tree in ieee international conference on web services icws .
ieee pp.
.
x. zhao y .
zhang d. lion m. f. ullah y .
luo d. yuan and m. stumm lprof a non intrusive request flow profiler for distributed systems in 11th usenix symposium on operating systems design and implementation osdi pp.
.
t. jia l. yang p. chen y .
li f. meng and j. xu logsed anomaly diagnosis through mining time weighted control flow graph in logs in2017 ieee 10th international conference on cloud computing cloud .
ieee pp.
.
y .
li y .
huo r. zhong z. jiang j. liu j. huang j. gu p. he and m. r. lyu go static contextualized logging statement generation proceedings of the acm on software engineering vol.
no.
fse pp.
.
y .
wu z. yu m. wen q. li d. zou and h. jin understanding the threats of upstream vulnerabilities to downstream projects in the maven ecosystem in ieee acm 45th international conference on software engineering icse .
ieee pp.
.
z. zhang m. k. ramanathan p. raj a. parwal t. sherwood and m. chabbi crisp critical path analysis of large scale microservice architectures in proceedings of the usenix annual technical conference j. schindler and n. zilberman eds.
pp.
.
a. fang r. zhou x. tang and p. he rpcover recovering grpc dependency in multilingual projects in 38th ieee acm international conference on automated software engineering ase .
ieee pp.
.
grpc grpc jul .
.
available thrift thrift jul .
.
available y .
li t. tan and j. xue understanding and analyzing java reflection acm trans.
softw.
eng.
methodol.
vol.
no.
pp.
.
n. grech and y .
smaragdakis p taint unified points to and taint analysis proc.
acm program.
lang.
vol.
no.
oopsla .
b. r. bruce t. zhang j. arora g. h. xu and m. kim jshrink in depth investigation into debloating modern java applications in proceedings of the 28th acm joint european software engineering conference and symposium on the foundations of software engineering pp.
.
r. vall ee rai p. co e. gagnon l. hendren p. lam and v .
sundaresan soot a java bytecode optimization framework in cascon first decade high impact papers pp.
.
d. nam a. macvean v .
j. hellendoorn b. vasilescu and b. a. myers using an llm to help with code understanding in proceedings of the 46th ieee acm international conference on software engineering pp.
.
y .
wang h. le a. gotmare n. d. q. bui j. li and s. c. h. hoi codet5 open code large language models for code understanding and generation in proceedings of the conference on empirical methods in natural language processing h. bouamor j. pino and k. bali eds.
pp.
.
z. wang z. liu y .
zhang a. zhong l. fan l. wu and q. wen rcagent cloud root cause analysis by autonomous agents with toolaugmented large language models arxiv preprint arxiv .
.
s. robertson h. zaragoza and m. taylor simple bm25 extension to multiple weighted fields in proceedings of the thirteenth acm international conference on information and knowledge management pp.
.
t. leesatapornwongsa j. f. lukman s. lu and h. s. gunawi taxdc a taxonomy of non deterministic concurrency bugs in datacenter distributed systems in proceedings of the twenty first international confer ence on architectural support for programming languages and operating systems pp.
.
h. s. gunawi m. hao t. leesatapornwongsa t. patana anake t. do j. adityatama k. j. eliazar a. laksono j. f. lukman v .
martin et al.
what bugs live in the cloud?
a study of issues in cloud systems inproceedings of the acm symposium on cloud computing pp.
.
z. li j. chen r. jiao n. zhao z. wang s. zhang y .
wu l. jiang l. yan z. wang et al.
practical root cause localization for microservice systems via trace analysis in ieee acm 29th international symposium on quality of service iwqos .
ieee pp.
.
s. banerjee and a. lavie meteor an automatic metric for mt evaluation with improved correlation with human judgments in proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and or summarization pp.
.
c. y .
lin rouge a package for automatic evaluation of summaries intext summarization branches out pp.
.
openai.
openai embeddings.
.
available openai.com docs guides embeddings h. wang x. xia d. lo j. grundy and x. wang automatic solution summarization for crash bugs in ieee acm 43rd international conference on software engineering icse .
ieee pp.
.
a. saha and s. c. hoi mining root cause knowledge from cloud service incident investigations for aiops in proceedings of the 44th international conference on software engineering software engineering in practice pp.
.
c. lee t. yang z. chen y .
su and m. r. lyu eadro an endto end troubleshooting framework for microservices on multi source data in ieee acm 45th international conference on software engineering icse .
ieee pp.
.
e. foundation eclipse java development tools jdt core .
.
available openai.
introducing chatgpt.
.
available https openai.com blog chatgpt g. deepmind gemini .
pro .
.
available https deepmind.google technologies gemini pro anthropic anthropic .
.
available anthropic.com deepinfra deepinfra .
.
available com j. xu r. yang y .
huo c. zhang and p. he prompting for automatic log template extraction arxiv preprint arxiv .
.
y .
huang y .
li w. wu j. zhang and m. r. lyu do not give away my secrets uncovering the privacy issue of neural code completion tools arxiv preprint arxiv .
.
w. li j. ming x. luo and h. cai polycruise a cross language dynamic information flow analysis in 31st usenix security symposium pp.
.
j. wang and h. wang nativesummary summarizing native binary code for inter language static analysis of android apps in proceedings of the 33rd acm sigsoft international symposium on software testing and analysis .
j. samhi j. gao n. daoudi p. graux h. hoyez x. sun k. allix t. f. bissyand e and j. klein jucify a step towards android code unification for enhanced static analysis in 44th ieee acm 44th international conference on software engineering pp.
.
p. chen y .
qi p. zheng and d. hou causeinfer automatic and distributed performance diagnosis with hierarchical causality graph in large distributed systems in ieee infocom ieee conference on computer communications .
ieee pp.
.
a. amar and p. c. rigby mining historical test logs to predict bugs and localize faults in the test logs in ieee acm 41st international conference on software engineering icse .
ieee pp.
.
z. jiang j. huang z. chen y .
li g. yu c. feng y .
yang z. yang and m. r. lyu l4 diagnosing large scale llm training failures via automated log analysis arxiv preprint arxiv .
.
s. shan y .
huo y .
su y .
li d. li and z. zheng face it yourselves an llm based two stage strategy to localize configuration errors via logs arxiv preprint arxiv .
.