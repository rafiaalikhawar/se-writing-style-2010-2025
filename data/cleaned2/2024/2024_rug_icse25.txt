rug turbo llm for rust unit test generation xiang cheng fan sang yizhuo zhai xiaokuan zhang and taesoo kim georgia institute of technology george mason university cxworks fsang yzhai60 taesoo gatech.edu xiaokuan gmu.edu abstract unit testing improves software quality by evaluating isolated sections of the program.
this approach alleviates the need for comprehensive program wide testing and confines the potential error scope within the software.
however unit test development is time consuming requiring developers to create appropriate test contexts and determine input values to cover different code regions.
this problem is particularly pronounced in rust due to its intricate type system making traditional unit test generation tools ineffective in rust projects.
recently large language models llm s have demonstrated their proficiency in understanding programming language and completing software engineering tasks.
however merely prompting llm s with a basic prompt like generate unit test for the following source code often results in code with compilation errors.
in addition llm generated unit tests often have limited test coverage.
to bridge this gap and harness the capabilities of llm we design and implement rug an end to end solution to automatically generate the unit test for rust projects.
to help llm s generated test pass rust strict compilation checks rug designs a semantic aware bottom up approach to divide the context construction problem into dependent sub problems.
it solves these sub problems sequentially using an llm and merges them to a complete context.
to increase test coverage rug integrates coverage guided fuzzing with llm to prepare fuzzing harnesses.
applying rugon real world rust programs average loc we show that rugcan achieve a high code coverage up to .
closely comparable to human effort .
.
we submitted unit tests generated by rugcovering the new code of them have been accepted were rejected and are pending for review.
i. i ntroduction unit testing is essential to ensure program quality throughout the development process with the aim of comprehensively testing a function in all possible branches and execution paths.
it has become an integral part of the software development cycle and many companies such as google and meta have a strict coverage requirement .
a good unit test includes the calling context to successfully execute the function the proper input triggering different paths and clear assertions reflecting the correctness of the result.
consequently it is widely recognized that the production of high quality unit tests requires substantial human energy and effort.
for example in the united states software testing labor is estimated to cost billion dollars per year .
to reduce developers workload several approaches have been proposed to automate test generation including traditional approaches like search based software testing sbst fuzzing program synthesis and latest large language model llm based methods .
traditional approaches usually leverage program analysis to build the testing context and search for appropriate testinput to trigger different paths.
these tools achieve good testing coverage on programming languages such as java e.g.
evosuite python e.g.
pynguin and c c e.g.
afl .
recently many llm based tools such as codamosa chatunitest testgen llm have been proposed to automate testing generation.
commercial products like cody and copilot are available for daily development.
rust an emerging system programming language that performs strict compilation checks is gaining traction for its performance and memory safety advancements.
it is adopted in crucial projects such as operating system kernels device drivers web browsers etc.
rust also has a plethora of over 130k packages to date.
however rust packages are not well tested.
we conducted a survey on crates.io rust s package repository computing the unit tests code coverage for the top 30k most downloaded crates1.
from our study we found that many crates are rarely tested and .
of the crates have less than test coverage which underscores the serious problem lack of unit testing in rust projects.
the difficulty of rust testing is due to its complex type systems.
for example rust s ownership system defines the single owner of each memory value and the borrow checker ensures that each value should only have one writable reference at a time.
these language features impose extreme stringency in compiler checks making it challenging even for experienced developers to craft valid code that passes compilation .
when applying existing approaches in rust traditional approaches such as sbst and fuzzing suffer from complex type dependencies and the potential huge searching space leading to limited test cases and test coverage.
for llm based approaches rust s strict compiler checks and complex program conditions largely undermine the ability of llm s to generate valid testing code with high code coverage.
this underscores the necessity of developing an automated tool specifically for the rust programming language and pinpoints two key challenges that must be addressed a passing compilation checks.
and b expanding code coverage.
in this paper we propose rug which leverages llm to automatically generate compilable high coverage unit tests for rust projects.
to solve the challenge a rugproposes a bottom up approach to divide the context construction problem into dependent sub problems and iteratively interact with llm to solve each sub problem.
each subsolution will be verified 1in rust packages are called crates.
1and memorized from the bottom of the dependency graph to the top.
the sub solutions will be merged at the end to generate the final test context.
for challenge b rugtransforms the generated tests into fuzzing harnesses without breaking test body and leverages fuzzing to improve the test coverage.
regarding the fuzzing corpus rugprompts llm to prepare sample test data during context generation and reuses them as initial corpora for the fuzzing process.
we evaluated rugon the most frequently downloaded crates of rust average loc .
the result shows that rug generates high quality tests with better code coverage.
using the latest gpt model rugachieves .
code coverage which is comparable to human practice .
and even achieves higher test coverage than human practice on crates.
we submitted unit tests generated by rugcontaining code regions that developers failed to test as pull requests prs and of them have been merged into the project taking .
of all the tests reviewed.
contributions.
this paper makes following contributions2 a bottom up context building algorithm.
we propose a semantic aware bottom up algorithm which simplifies the context construction problem for llm and improve correctness of generated code by .
.
fuzzing based input exploration.
we demonstrate that fuzzing tools can be used to compensate for llm s weakness in reasoning program conditions and expand testing coverage by .
to .
.
an automatic unit test generation tool for rust.
we present rug an automatic unit test generation tool for rust that leverages a deep combination of llm and program analysis.
rugimproves the code coverage of .
to .
and .
to .
when compared to existing traditional approaches and existing llm based approaches respectively.
practical usage.
we submitted the generated unit tests to different popular rust projects of them are merged by maintainers with positive feedback taking the .
of all the reviewed unit tests.
ii.
b ackground in this section we first outlines the issue of test generation alongside the most current state of the art approaches.
then we introduce the basics of the rust programming language which guarantees memory safety and performance through strict compiler checks.
furthermore we highlight the obstacles encountered in automatic testing code generation for rust.
a. automatic unit testing although unit tests have been proven to be helpful in program quality during software development it is time consuming for human developers to write unit tests for each unit.
in order to address this issue there are many efforts to automate the 2we will open source r ugupon publication.unit test generations including the sbst fuzzing approach and learning based approach.
sbst approach.
searching based software testing sbst leverages the source code including the library code and application code to find the possible function sequences and input data that can test new code.
it utilizes different search algorithms to automatically generate tests guided by the coverage goal and evolutionary algorithms are demonstrated powerful for this searching process.
however when applying thesbst to rust there are two drawbacks due to the complex type system of rust the mutation operators can not guarantee the correctness of the generated code.
the evolutionary algorithms primarily relies on existing user program to mutate limiting their ability to generalize and adapt to broader testing scenarios.
fuzzing.
fuzzing has grown in popularity for software testing due to its efficiency and reproducibility.
it generates and mutates the input to test the target program and collect code coverage as feedback.
based on coverage feedback and domain knowledge modern fuzzers are proficient in exploring paths within testing programs and can even uncover bugs that have existed for years .
with its ability to identify input data that cover new code fuzzing holds potential in aiding the generation of test data for unit tests.
however utilizing fuzzing solely for unit test generation presents the following issues.
first fuzzers require specific fuzzing harnesses to work with and the quality of the fuzzing harnesses largely affects fuzzer performance.
although program analysis tools such as rulf rpg are proposed to automate this process it still suffers from the ultimate search process and lack of readability.
second the number of corpora generated during the fuzzing process is large requiring post processing steps to filter and select.
finally the readability of the fuzzing corpora is poor and is difficult to directly adopt into the repository.
llm approach.
large language models like chatgpt are state of the art language models that are trained on vast amounts of language data.
these advanced language models with their neural architectures and expansive understanding of languages are able to capture intricate patterns and relationships in language usage.
as a result they express an impressive capabilities in the software related activities including unit test generation coding assistance and program debugging .
provided the context and questions as prompts llm is able to generate the relevant test program.
to evaluate the capabilities of llm to generate unit tests yuan et al.
conducted a thorough evaluation in the context of ja v a. their study reveals that chatgpt is able to write a reasonable number of unit tests but a large proportion .
encountered diverse compilation errors.
although the rest of the tests have been successfully compiled only .
of the executions succeed because the generated assertions are incorrect.
finally the unit test generated from llms exhibited commendable readability.
all three findings are consistent with our results under the rust context and in this work we propose our solutions for these problems.
2b.
rust programming language rust is an emerging programming language for low level and system development with memory safety guarantees.
it has two parts safe rust is designed to achieve native performance in a memory safe way guarded by the compiler and unsafe rust requires developers help for memory safety.
safe rust3 employs a robust type system that imposes strict disciplines effectively mitigating security concerns and ensuring memory safety.
in addition to its advanced type system rust supports traits to define the common behaviors and generic parameters to extend its usability.
next we will describe some necessary language features and concepts used in the latter context and highlight the challenges they present.
type checks.
rust is a statically typed language all the variables are assigned a specific type at the compilation time.
rustc compiler will reason and check the type correctness and bounds for every variables in the program.
as a result the unit test generation tool needs to ensure the correctness of variable types and bounds.
ownership.
rust s ownership ensures that every variable has a relative memory it binds to and this memory is immediately reclaimed when the owner variable goes out of scope.
this ownership can also be transferred and the original variable loses the access to the value ensuring the variable is valid when it s being accessed.
traits and bounds.
to further empower the flexibility rust supports generic parameters to allow developers reuse the functions.
trait bounds are used as desired restrictions for the generic parameters.
therefore when synthesising programs for generic functions synthesizers are expected to find qualified candidate types as generic parameters otherwise the rustc will find the missing or mismatch of generic parameters.
the aforementioned features along with other compiler checks e.g.
mutability lifetime etc.
introduce new challenges for test statement construction or program synthesis in rust.
for instance the recombination operator in a genetic algorithm takes the parent programs e.g.
program a b as inputs and breeds them to generate child programs.
in java or c c language with the help of the type analysis synthesizers generate a program with cross usage of variables a variable is created in aand used as a parameter in b .
however this step does not always hold in rust because except type correctness the variable s ownership mutability and lifetime need to be correct as well.
we argue that managing these requirements simultaneously is not trivial under the constraints of safe rust.
furthermore since rust performs these checks during compile time it is difficult to bypass these challenges.
iii.
c hallenges in this section we use a motivating example in listing to showcase the challenges to automatically generate unit tests.
based on the root causes of these challenges we categorize them into two classes and propose our solutions in iv.
3the goal of code generation is for safe rust we still call it rust for simplicity.listing .
motivating examples for encode function.
details of struct trait defintions are omitted.
the challenge comes from lines which provides an implicit definition of config trait.
1fnencode e encoder self char encoder e result encodeerror target function impl for encoder trait 4impl w writer c config encoder for encoderimpl 5pub struct encoderimpl w writer c config impls for writer trait 7impl writer for slicewriter 8impl writer for iowriter proxy impls for config trait 10impl t config for twhere t r1 r2 r3 def for configuration impls r1 r2 r3 12pub struct configuration r1 r2 r3 a. motivating example the motivating example in listing shows a target testing function encode in line and the related data structures in thebincode crate .
the function encode takes the first argument self as a char type serializes it from memory into raw bytes and stores them in the specified location specified in encoder .
in order to test the target function developers need to prepare two concrete variables one is a simple variable with char type and the other is a complex instance of e encoder trait.
in this code snippet encoderimpl is a candidate implementation of encode .
however encoderimpl itself depends on two other traits w writer and c config representing any general writer orconfig .writer controls the output location of the raw bytes and config controls the way to encode the memory object e.g.
big endian or little endian .
in lines we can resolve a concrete instance of writer named iowriter orslicewriter .
as for the config trait instead of providing a direct definition developers need to deduce that the proxy definition of config on line implies that any type t satisfying the union of rntraits can be an implementation of config .
thus rugneeds to search across the source codebase for the intersections of candidates implementing rntrait which isconfiguration .
after getting a valid writer andconfig a valid testing context is built for the encode function and ready to test.
generating a unit test for function encode highlights the two previous challenges.
first to pass the compiler check the generation tool needs to infer the correct instance of the trait based on the function declaration ensuring that the generated code adheres to the complex compiler rules.
through our evaluation on gpt .
and gpt even all relevant source code rustdoc and sample code are presented in the prompt it is still difficult for llm to generate the correct tests.
second to enhance the coverage of the code the generated code should encompass various regions within the function.
even if the test generation step succeeds both the llm and sbst approaches face challenges in reasoning about the conditions and thus cannot effectively improve code coverage.
by analyzing the generated tests and their failure reasons we identified the root 3causes of these two challenges and classified them by their sources as the challenges caused by rust and the challenges caused by the llm models.
b. challenges to pass the compiler check the rust compiler enforces strict checks for all variables making it challenging even for humans to write rust code let alone automatic tools.
two common mistakes that often require significant time for human engineers to diagnose and resolve are trait errors and path errors.
traits and bounds.
trait is extensively used in rust to define the shared behaviors across different structs while trait bound serves as the restrictions of available generic parameters can be used in rust library.
for example in listing the e encoder indicates the generic paramter e must satisfy the encoder bound.
therefore to generate a concrete test it is vital that traits and trait bounds are filled with appropriate instances4.
in the motivating example the second parameter encoder needs to be an instance of encoder trait which further relies on the writer trait and config trait.
moreover the config trait only has a proxy definition shown in line in listing .
therefore these complex composition and proxy of trait trait bounds become the burdens for llm to correctly reason the proper candidates which lead to compilation errors of type mismatch .
besides rust s separation of data definition and implementations makes it more difficult for llm s to identify correct type information from the mixed source code.
definition paths.
to ensure successful compilation all type and function definitions within the testing program must be explicitly imported into the context.
this is hard for the purely llm based approach.
in the motivating example the target test function encode is part of the encode trait which is implemented for the char type as the self parameter.
hence to trigger the encode function for char the encode trait must be explicitly imported into the testing context.
cascading errors.
the unit test composition process involves several steps.
first we need to figure out the calling context e.g.
reasonable input data for each variable.
second we need to properly passing those context to the target function.
finally we need to resolve the correct assertion statement.
when prompt llm through this process it requires complicated reasoning chain from the model.
even a minor error at any step could culminate in an inaccurate final prediction.
therefore llm must accurately navigate the challenges posed by the rust language as mentioned previously.
c. challenges of low code coverage even after overcoming the challenge of generating unit tests that pass the compiler s checks low code coverage remains a significant issue when composing unit tests for rust programs.
4although rust provides dynkeyword to delay these bound checks until runtime it s rarely used in the rust crates.
rust projecttype dependencygraphdependencygraph constructiontest code .a.
.a.2bottom up context building harness transformation .b.1fuzzing harnessfuzzing corporafiltering selection unit tests .b.2rugfig.
.
the general workflow and components of rug.
after building the type dependency graph from the input rust project rugleverages bottom up context building to handle the compilation challenges and fuzzing to resolve the coverage challenges.
difficulty in path exploration.
in contrast to traditional approaches llm currently lacks the capability to examine the execution of the program.
this limitation results in challenges in reasoning about branch conditions and exploring various code paths.
moreover in the context of test assertions the oracle is responsible for verifying the correctness of the function execution.
while llm is able to generate appropriate fields for verification because of failure to statically reason the path that will be executed they may yield incorrect values as oracles.
such a deficiency in accurately reasoning about data conditions and outcomes further complicates the llm s capability to produce valid and reliable test assertions underlining the need for enhanced techniques.
iv.
r ugdesign to address the challenges of test generation and coverage we propose a new tool called rug which uses a semantic aware approach to guide the llm building the test and leverages fuzzing to expand testing coverage.
the workflow of rugis shown in fig.
.
taking a rust project as input rugfirst constructs a type dependency graph for the parameters of the target function then it resolves the concrete types and the corresponding code for each node in the dependency graph from the bottom.
after the root node is resolved the unit test context is complete.
finally ruguses fuzzing to explore valid inputs and enhance code coverage.
in this section we first introduce the context building approach of rugin iv a and show the fuzzing process in iv b. a. testing context construction to build the unit test context rugfollows a two step process for each target function rugapplies static analysis to construct a type dependency graph for the parameters of the target function starting from the leaf node rugadopts the bottom up approach to automatically build the concrete code with the help of llm.
constructing type dependency graphs to build the unit test context rugfirst statically builds the type dependency graph.
to better explain our approach we use g v e to denote the type dependency graph where gis a directed 4encoderimpl config writer t r1 r2 r3 slicewriter iowriter writable ... r1r2r3configuration encoderimpl config writer configuration resolved iowriter resolved help me write unit test for encode function function body for the 1st argument the type is encoderimpl the context with source code is listed below encoderimpl writer iowriter .. slicewriter config t r1 r2 r3 .. for the 2nd argument ... ... prompt configuration prompt all resolved nodes rug s sub problem baseline problem proxy node trait node instance node unit test for encode code to build configuration unit test for encode fig.
.
rug s bottom up context building example.
the left side represents the baseline s one shot approach requiring llm to generate the test with a long context leading to buggy output.
rugautomatically divides the task into subproblems and simplifies the task iteratively.
the content of the rugprompt is in fig.
and fig.
.
graph vdenotes the nodes in the graph and edenotes the edges.
the details of the graph are defined as follows v v v vtrait vinstance vproxy where vtrait denotes the vertices with trait bounds constraints vinstance represents the vertices with concrete types like struct orenum vproxy is a kind of special vertices for a proxy definition line in listing denoting the logical substitution of type compositions.
e e eis a directed edge indicating that the start vertex depends on the end vertex.
for example in listing line can be represented as encoderimpl writer encoderimpl and writer are vertices in the graph.
in this graph the leaf vertices are defined as those with concrete types.
for example in fig.
which illustrates part of the type dependency graph for listing the vertex writer is considered an instance vertex but since it does not have a concrete type it is not a leaf vertex.
bottom up code generation given the type dependency graph g treat each node in the graph as a subproblem and the unit test generation problem can be modeled as follows given a type dependency graph gand parameters piof the target function for each pi the context of the unit test to be generated is a subgraph g g such that s s sstarts withpiand ends with primitive type nodes where sdenotes all topological sequence permutations of g. in fig.
the subgraph of encoderimpl is the scope of the target parameter to be solved.
for existing llm based auto testing tools testing code generation is finished in one shot like the left part in fig.
they collect the context and ask for the testing code then check the output and apply automatic fixes or retries if necessary.
this one shot approach usually produces a long context for llm to infer increasing the difficulties of code generation due to the strict compiler checks and cascading errors in iii b.rugdivides the generation of the test code into small steps as checkpoints reducing the difficulties of each subproblem and guiding llm to solve the final problem.
like the right part of fig.
rugbegins with nodes that can be directly instantiated using llm to construct the unit test context.
once the unit test code for a node is generated the results are used to resolve its parent node in the graph.
a node is ready to be resolved once all its dependent nodes have been addressed.
for example in fig.
after resolving the rntypes rug starts with the configuration node which can be resolved using the output of rntype sub problems.
with the prompt shown in fig.
the llm generates the concrete code for configuration .
to ensure the correctness of the generated code for each node ruguses an oracle compiler to verify that the code is compilable.
once validated the results are used to generate the test code for the type tas a proxy node denoting the composition of type r1 r2 r3 .
when providing concrete types to the llm different strategies are employed for different types of nodes.
for instance nodes rugresolves the correct definition paths for each type within the context and recursively gathers relevant items including structure definitions target function definitions trait definitions structure field types implementation relationships.
for trait nodes rugqueries the intersection of bound constraints to identify all candidate types.
if no valid candidate is found ruggenerates a prompt describing the trait and uses llm to implement it.
finally for proxy nodes rug applies its proxy definitions within the compilation context to find candidate types.
after resolving and validating all the unit test codes for each parameter rugleverages a test generation prompt to ask llm to generate the unit test code.
the generated code is then validated using the compiler.
the whole algorithm is shown in algorithm getdependent finds direct dependents if any of the input type for instance nodes and valid candidate 5please help me fill in the sample code by creating an initialized local variable named var name with type var type using its constructor method or structural build in crate name crate s file location file.
fill in any sample data if necessary ... rust sample code to fill optional for the dependent type please reuse below sample code to construct.
dependent code from previous round fig.
.
prompt template rugused for each sub problem.
the optional paragraph is reusing the previous output to cut the context.
types for trait proxy nodes descriptiongen generates the prompt description of the target type g means accessing the value in the map gby the key k and dis a set of verified candidates for the input parameter type t. algorithm rug s context construction algorithm.
tis the target parameter type gis the type dependency graph.
corner cases are omitted for simplicity.
procedure build context t g d s getdependent t g fors sdo if not sis resolved then build context s g d g iftis instance then g llmrequest t d if not compilev erify t g then g descriptiongen t else if dis empty then d llmrequest t descriptiongen t g d return g corner cases although rug s bottom up building approach improves the quality of the generated code by minimizing the relevant context there are several corner cases need to be handled.
one of the corner cases is the loop in the subgraph g indicating the presence of cyclical type dependencies.
rugwill break the cycle by randomly determining orders and will remove the sample code in the prompt shown in fig.
.
in addition for trait proxy nodes sometimes there is no valid instance type in the compilation scope and rugwill prompt llm to implement the traits.
meanwhile for the trait proxy nodes with multiple instances rugprovides different candidate selection strategies according to the user s preferences.
the occurrence of nodes with multiple candidates available takes around .
and by default rugwill choose the candidate in the local crate.
finally due to the uncertainties of llm sometimes the the target function is fn name in crate name crate s file location file its definition path is def path and source code is like below target test function for n th argument parameter n type can be used please use following sample code to construct it dependent code sample to reuse please help me build unit test fig.
.
prompt final test generation template rugused to combine the sub solution together and build the target unit test.
model fails to give a correct answer rugwill mark the subtask as unfinished and continue the next step with descriptions in natural language.
in practice we found even without a sample code that sometimes llm can generate the correct code for the current sub problem.
b. fuzzing for input exploration although the divided context building approach helps llm to write executable tests the generation of useful test data is another burden for llm .
in iii c we show that the root cause of this challenge is lack of the ability to inspect the state of the program during execution so it is struggling for llm to generate the corresponding data to trigger different conditions in the code path.
in our motivating example for the char type to encode llm usually prepares a valid ascii symbol or one or two simple utf characters as test data.
with the help of fuzzing the test input is quickly scaled to complex utf characters and triggers the missed region.
thus without fuzzing it is challenging for llm to prepare reasonable test data for high code coverage.
in this section we demonstrate how we prepare the fuzzing harness from generated tests and how we handle the redundant fuzzing corpora as postprocessing.
fuzzing harness transformation fuzzers need fuzzing harnesses to test with where all input data is provided by the fuzzers as raw bytes.
to convert existing test code into a fuzzing harness rugleverages program transformation to construct test data from raw inputs and preserve the semantics of the test body.
during the transformation process rugfirst identifies all the primitive data in the original test code and replaces them with local variables of the same type built from the fuzzer input bytes.
meanwhile rugrecords these initial data as seeds and saves them as the initial corpus for the fuzzing process.
second to ensure the graceful execution of fuzzer rugdisabled all assertions to help fuzzer finish its execution.
although this step may miss some bugs it ensures the graceful execution of the fuzzer and improves the code coverage.
after fuzzing and postprocessing rugwill review these assertions and try to replace the value based on the fuzzing result.
fuzzing corpora selection by applying fuzzers to an existing test program we can efficiently expand our test coverage through the new input data found by the fuzzers.
because of its efficiency a few seconds of fuzzing can execute the program thousands of times with hundreds of corpora 6generated which is far beyond the requirement for the number of tests.
to further manage these corpora we develop a source code coverage based path weight guided corpora selection algorithm to filter and rank the corpora based on their coverage.
the post processing goes with two stages.
first rugfilters the corpora based on source code coverage instead of fuzzers bitmap coverage to remove the redundant inputs.
second rug uses weight to measure the importance of each code region and the overall weight for each corpora to rank them.
to calculate the weight rugassigns a default weight of for each code region.
for all the ncorpora that touched this region they will share the weight gaining the score of1 n. the goal of this weight sharing design is to find the corpus that covers more unique code regions and the algorithm is shown in algorithm src covis a function that takes a specific corpus and returns a set of code regions rthat are covered by the given input.
wcontains the weights for each input corpus.
finally based on the weight score and the number of code regions covered rugranks the corpora and selects them according to the given threshold.
algorithm rug s corpora ranking algorithm.
iis the input corpora as a set.
procedure rank corpora i w c d fori ido r src cov i d r forr rdo c getordefault c r fori ido weights forr d do weights weights .
c w weights sort iby w i iin descending order return i v. i mplementation we implemented r ugfor the rust toolchain nightly .
as shown in fig.
rugleverages llm and fuzzer as black boxes implements a testing context builder that deeply combines static analysis and llm fuzzing harness transformer and corpora postprocessor.
a. static analysis wrapped as a compiler plugin rugworks with middlelevel intermediate representation mir in rustc to retrieve the semantic information.
specifically for the target types in context rugrecursively collect the relevant items including all the types definitions all the definitions of the inner types all the types that implement the relevant traits and 5the effort to support other toolchains is to handle the rust compiler s mir changes across different versions.
all the function definitions and the rustdoc of the relevant types.
rug s searching scope is limited to the target cage and when a foreign trait or type outside of the current compilation scope is included r ugstops searching for its relevant items.
b. bottom up test context generation with llm rugis implemented in python.
during the evaluation we use gpt .
turbo 16k model as gpt .
and gpt model as gpt .
we set presence penalty to to encourage the model to reuse the context provided by us and leave the other configurations unchanged6.
to handle the uncertainties of llm outputs rugadds system prompts asking llm to conform to specific formats and always provides a template code to fill out which helps postprocess generated code.
however due to the nondeterministic nature of llm sometimes it is difficult for llm to find the correct answer so we set a maximum limitation of three times to try for each individual question.
after three attempts even if the answer is still wrong rugwill cache the result and use natural language descriptions as hints for the next questions.
after receiving the answers from llm rugchecks the correctness of the answer by compiling the code and detecting the types of local variables using an oracle compiler plugin.
c. fuzzing transformation and postprocessing rugimplements harness transformer with the help of rustc compiler and syn crate to transform the testing code into fuzzing harnesses while semantically preserving the test logic.
for a given unit test rugfirst identifies and extracts the primitive test data as local variables then converts these local variables from the input of the fuzzer so that the fuzzers can manipulate them for testing.
the original test data are saved as the initial corpus seed for the fuzzer.
in addition rugtemporally disabled all the assertions in the original test since these assertions may not hold because the test data are changed.
finally rugutilizes bolero and libfuzzer to launch fuzzing testing.
after the fuzzing process rugapplies the corpora filtering algorithm to filter redundant corpora and the ranking algorithm to order corpora according to their importance.
vi.
e valuation to demonstrate the effectiveness of rug we conducted a comprehensive evaluation motivated by the following research questions rq1 how does the test generation performance of rug in terms of coverage compare to traditional tools?
rq2 compared with other llm based tools does rug show any benefits?
rq3 how does each factor contribute to rug s testing coverage?
rq4 how is r ug s usability for real world applications?
rq5 how robust is rugin different scenarios?
specifically how does rugperform on crates that the llm has not been 6ruguses default values for temperature top p frequency penalty .
7trained on?
additionally how do different type selection approaches impact the results?
a. rq1 comparison with traditional tools to answer q1 we compare rugwith four of the latest different test generation tools for rust including rustyunit as asbst approach syrust as a constraint solving program synthesizer and rulf rpg as fuzzing based tools.
comparison with general searching based tools.
rustyunit is a sbst approach that leverages the dynamosa algorithm to mutate the existing rust codebase and generate the unit tests.
and syrust is a semantic aware program synthesizer that uses a sat solver to generate valid rust programs.
to ensure a fair comparison we applied rugto their original benchmarks separately and used llvm source code coverage to measure the code region coverage and function coverage.
when calculating the code coverage we considered only functional code excluding testing code and compiler generated code.
we conducted our experiments on servers equipped with two amd epyc cpus and 256gb of memory running ubuntu .
.
the rust toolchain version used is nightly2022 and all tools are assessed using their default configurations.
since rustyunit provided different evolutionary algorithms we selected the most powerful dynamosa algorithm which achieves the highest code coverage for comparison with rug.
for syrust we run the synthesizer with their default timeout of hours for each project7.
the evaluation results are shown in table i. rugachieves .
coverage on rustyunit s and .
on syrust s outperforming all the works.
rustyunit s seeded dynamosa algorithm which is part of the genetic algorithm family relies on existing code as parents to mutate unit test statements.
however because the usage of each function within a crate is often imbalanced this approach struggles with less frequently used functions.
syrust on the other hand requires a manually crafted template of function arguments for synthesis necessitating developer input and limiting the synthesis to the template input.
overall rug s superior coverage is primarily due to its ability to construct calling contexts for a wider range of target functions.
comparison with fuzzing tools.
besides comparing with sbst tool and program synthesis approach rugis evaluated against fuzzing based tools rulf and rpg .
rulf constructs type dependency graphs to assist in generating fuzzing harnesses producing a sequence of api calls through graph traversal.
rpg extends rulf by adding support for generic parameters and prioritizes functions containing unsafe regions using a pool based generator.
we run all tools on rulf s benchmarks under default configurations to ensure a fair comparison.
for each individual fuzzing harness we set a timeout of hours for rulf and hours for rpg since rpg generates much more fuzzing harnesses compared with 7the crates are bitvec crossbeam dashmap imrc ndarray num rational slab for data structure and csv core encode unicode encoding rs hcid sval urlencoding utf8 width for encoding .cratefunc region func region rustyunit rug gamie .
.
.
.
humantime .
.
.
.
lsd .
.
.
.
quick xml .
.
.
.
tight .
.
.
.
time .
.
.
.
mean .
.
.
.
syrust rug data structure .
.
.
.
encoding .
.
.
.
mean .
.
.
.
table i coverage comparison between rug rusty unit and syrust.
rug out performs rusty unit by .
and syrust by .
.
rulf8.
for tests generated by rug we conduct fuzzing for seconds per function which is a reasonable time for unit test generation.
all fuzzing experiments were repeated three times and the average values are reported.
the results are shown in fig.
where rugachieves an average coverage of .
outperforming rulf s .
and rpg s .
.
rulf s harnesses generation process is an np complete problem using a heuristic threshold to limit the generation process.
besides rulf does not implement static analysis for generic parameters preventing it from triggering functions that use traits e.g.
json crate .
rpg improves upon rulf by supporting generic parameters and utilizing a pool based generator to produce more fuzzing harnesses.
however rpg encounters a similar search problem as rulf and prioritizes functions with unsafe regions during the generation process which affects the total number of harnesses.
in addition if no valid candidate is found for the given generic parameters the rpg may not locate a candidate outside the current compilation scope leading to a missing fuzzing harness.
in contrast rugleverages llm to generate the testing code avoiding the search problem and covering more testing functions.
b. rq2 comparison with llm based tools to answer the rq2 and demonstrate rug s effectiveness in tackling the two challenges we select crates from the most downloaded on crates.io averaging loc per crate.
beyond their popularity theese crates represent a diverse spectrum of rust applications in system development mio crc32fast for system call and hardware instructions json toml bincode for data serialisations hashes uuid for crypto computations and num traits ryufor numerical computations.
for the baseline approach we integrate three llm based approaches chatunitest for code generation testgenllm for oracle compiler check and rustassistant for code repair.
the process is as follows we collect the relevant source code context of the target function and send it 8this setting outfits rpg s evaluation settings of to hours total timeout for each project.
8clap flate regex proc time semver url tui r syntax s parser json http xi editor mean020406080 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.9code region coverage rulf rpg rug fig.
.
coverage comparison between r ugand rulf rpg.
rpg encounters an unexpected crash while running for http crate.
crate name downloads tests ac rejgpt .
gpt human test coveragebase rug base rug newly api cov rate w o w. fuzzing w o w. fuzzing w o w. fuzzing w o w. fuzzing bincode 49m .
.
.
.
.
.
.
.
.
.
chrono 128m .
.
.
.
.
.
.
.
.
.
hashes 266m p .
.
.
.
.
.
.
.
.
.
humantime 98m p .
.
.
.
.
.
.
.
.
.
itoa 221m .
.
.
.
.
.
.
.
.
.
json 203m .
.
.
.
.
.
.
.
.
.
mio 145m .
.
.
.
.
.
.
.
.
.
nom 114m p .
.
.
.
.
.
.
.
.
.
num traits 185m .
.
.
.
.
.
.
.
.
.
demangle 93m p .
.
.
.
.
.
.
.
.
.
crc32fast 104m .
.
.
.
.
.
.
.
.
.
ryu 185m .
.
.
.
.
.
.
.
.
.
semver 168m .
.
.
.
.
.
.
.
.
.
textwrap 134m .
.
.
.
.
.
.
.
.
.
time 200m p .
.
.
.
.
.
.
.
.
.
toml 125m .
.
.
.
.
.
.
.
.
.
uuid 108m .
.
.
.
.
.
.
.
.
.
mean .
.
.
.
.
.
.
.
.
.
identifier p a b c d e f g h i j table ii code coverage evaluation for rug on 17popular rust crates .
p indicates the pr is still pending for response .
the three dimensions for sensitivity tests are gpt model versions generation approaches and whether applying fuzzing .
the newly api coverage denotes the api s that baseline failed to test and only covered by rug.
along with the prompt to the llm to generate test cases.
the generated test code is then checked for correctness by an oracle compiler.
if the llm output fails to pass the compiler checks we collect the error message line numbers and reasons for the failure then ask llm to fix the problems within the same context.
since chatunitest and testgen llm are not implemented for rust we reimplemented them specifically for rust during the preparation of the baseline experiment.
we treat this pipeline of three approaches as the baseline representing the best existing llm based practice.
compared to baseline rugdoes not have an additional error fixing stage and employs a bottom up building algorithm to generate the compilable test code.
in addition rugtransforms the test code into a fuzzing harness and takes advantage of fuzzing as a final step.
the results across different experiment configurations are shown in table ii.
compared with adand eh we show thatrug s overall approach improves the testing coverage by .
and .
under different llm s. the improvement number of gpt is relatively smaller because the remaining untested code regions are limited especially we only focus on the actual functional code and do not generate tests for derivedfunctions9.
c. rq3 ablation study to demonstrate the effectiveness of rug s two techniques bottom up building and fuzzing and its sensitivity to different llm models we conducted sensitivity experiments for each of them showing their individual contributions to the final results shown in table ii.
bottom up building.
to demonstrate the effectiveness of the bottom up building approach of rug we evaluate the newly covered apis shown in i representing target apis that the baseline tools failed to generate the test context but can be resolved by the bottom up approach of rug.rugsuccessfully covers .
of these apis highlighting its effectiveness.
in addition comparing between acand eg we show that bottom up building helps improve code coverage by .
and .
which accounts for about half of the total coverage improvement.
fuzzing approach.
to show the improvement contributed by fuzzing we launch rug s fuzzing harness transformer on both 9rustc allows to generate default implementations like clone which will be counted as separate functions in llvm source code coverage.
9generated tests.
the result shows that the fuzzing component increases code coverage by .
in cdand .
in gh.
even when the initial method achieved high coverage such as .
in eand .
in g fuzzing still provided an additional coverage boost of around .
this reinforces our insight that applying fuzzing can effectively enhance code coverage.
large language model versions.
finally we test rug s sensitivity to different version of llm models gpt .
and gpt .
clearly gpt is more intelligent than gpt .
by showing about improvement between aand e. the experiment result shows code coverage are all further improved by applying rugfor .
in gpt .
and .
in gpt .
besides in terms of testing coverage applying rugon gpt .
dachieves a higher coverage number than vanilla gpt approach e. d. rq4 practical usability evaluation in this section we evaluate the usability of rugby comparing its generated tests with human written tests to answer rq4.
for the crates in table ii we note that due to their popularity these crates are actively maintained and well tested by the developers achieving an average coverage of .
.
we refer to their coverage as the best that human developers can achieve in practice and evaluate rug s generated tests against the human written tests to demonstrate its practical usage.
code coverage.
as shown in the column j the developers achieve an average of .
code coverage on these crates and by leveraging the latest gpt rugcan achieve .
inh which is comparable to human tests.
in addition among the crates rugand human developers both achieve higher coverage on crates and get the similar testing coverage forhashes indicating the potential usage of rugin the software testing process.
in addition for crates like chrono nomandtime ruglargely expands the testing coverage than developers showing the effectiveness of r ug.
for crates where rug does not outperform e.g.
bincode toml the main reasons are as follows these crates are template libraries for de serialization with few type implementations in the code base making it difficult for rugto find the valid candidates.
the code requires highly structured input to be fuzzed effectively which is challenging for fuzzers without concrete structure definitions.
readability.
to evaluate rug s tests readability we collect the test coverage of rugand human developers and find ten missing tests for existing human written tests.
we directly leverage rug s generated tests without changing test bodies and send them as prs to the open source projects.
to our surprise the developers are happy to merge these machine generated tests.
ruggenerated a total of unit tests of which we submitted to the corresponding crates based on their quality and priority.
so far of these unit tests have been merged with positive feedback.
developers chose not to merge tests for two main reasons first the target functions are imported from externalcrates base humanrug ls rr uf metrics evaluation .
.
.
.
.
coolssh .
n a .
.
.
cbored .
.
.
.
.
rust mc proto .
n a .
.
.
atomic waitgroup .
.
.
.
.
osrm client .
.
.
.
.
europe elects csv .
n a .
.
.
xelis hash .
.
.
.
.
behindthename .
.
.
.
.
utapi rs .
n a .
.
.
mean .
.
.
.
.
table iii robustness evaluation of rug on unlearned crates after the llm training cut off.
the n a denotes the crates don t have tests .
the rug with ls strategy achieves .
excluding the no test crates close to the .
coverage by developers .
libraries and the developers do not intend to include tests second the submitted tests are closed after a long pending period .
we are still awaiting feedback on the remaining unit tests.
in general .
of the tests reviewed are merged by the developers.
token consumption.
although llm s are getting more and more cheap nowadays10 automatic tools quickly consume large number of tokens by sending relevant source code as prompts.
for example under the gpt model the cost of the baseline approach is around .
however with the help of typeaware caching rugonly takes .
of the total tokens in the baseline approach and improves the coverage of the testing by .
.
rugconsumes less tokens because for each method under the same structs with dependent types rugfeeds each unique dependency to the llm only once caching and reusing the result without saving the prompt.
this approach saves tokens for subsequent functions with the similar dependencies.
in contrast baseline approaches must feed the entire context including transitive dependencies each time leading to higher token usage.
e. rq5 robustness evaluation to evaluate rug s robustness we conduct two experiments to measure rug s performance under unlearned crates and different candidate selection strategies for corner cases in iv.
for gpt .
turbo its knowledge cut off is september and we carefully select ten crates that are created after january from crates.io as our benchmark coolssh xelis hash for algorithm calculations behindthename cbored utapi for ffi wrappers metrics evaluation europselects csv for string parsing osrm mc proto for binary data paring and atomic waitgroup for concurrency.
unlearned crates.
the evaluation result is shown in table iii.
compared with the baseline llm approach rugachieves an average improvement of .
.
when comparing against human written tests excluding the four crates without human tests rugachieves .
on the remaining crates just .
10as of march the cost of 1m tokens as inputs is for gpt and .
for gpt .
10less than the developers tests.
in addition rugachieves a higher coverage rate on half of the crates compared to human developers showing r ug s robustness on unlearned crates.
candidate selections.
regarding candidate selections we compare three selection strategies for the same crates used in the data leakage analysis.
those selection strategies are widely used local crate selection ls round robin rr and unsafe first uf .
the ls is the default strategy of rug trying to cover more local types and only conducts random selection when there are multiple local candidate types.
in addition we further evaluated two more strategies rr tries to fairly cover the candidate types while selection and uf leverages the program analysis to find the candidate type with highest number of unsafe regions which is close to rpg s ranking algorithm.
the result of different candidate selections are shown in the table iii compared with rr and uf local crate selection ls prefers to use the instance from the current crate increasing the chance of local code coverage.
for rr and uf they may select the outside instance as final candidate leading to the potential drop in the local testing coverage.
vii.
r elated work a. llm based tools for rust automatic testing the powerful induction ability of llm s shows their application in software engineering especially in code generation testing .
among these works rustassisstant leverages the iterative communications with llm to fix rust compilation errors.
rustgen took the nature english description as input and output a reasonably functional rust code.
rust lancet automatically fixes the compiler errors resulting from the violation of the rust ownership rules.
they are all different applications of llm compared with rug.
regarding unit test generation codamosa integrates the traditional sbst approach with llm to generate unit test code by asking llm for mutation seeds when the search process is stuck.
however codamosa is tailored for the python language which due to its weak typing does not present the same challenges associated with inferring correct types as seen in strongly typed languages like rust.
therefore the general difficulties related to the synthesis of rust programs remain unaddressed leading to low code coverage.
in contrast therugapproach effectively addresses these challenges using static analysis to guide llm to infer types.
b. traditional automatic testing tools before the emergence of the llm there is already a lot of work dedicated to building the automatic testing tools including searching based software testing sbst and fuzzing.
sbst approach requires the existing calling context for the target function as input .
then it uses evolutionary algorithms to develop new unit tests aiming to cover sections of the code that have not been previously tested .
for example rustyunit utilizes the dynamosa algorithm to automatically generate the unit test for rust.
besides sbst coverage guided fuzzing approaches are used to assess different areas of the software.these approaches areparticularly favored because they are capable of generating new inputs thus extending the coverage of the code .
for instance afl.rs is a fuzzing tool that depends on manually crafted fuzzing harnesses to probe the software.
the combination of fuzzing with llm can bypass the manual work of building a readable fuzzing harness and automate testing .
fuzz4all is a recent work that proposes a universal fuzzy loop powered by llms to find bugs in different languages.
however fuzz4all aims to find bugs in language compilers and rarely touches on the problem of low code coverage for generated code snippets.
rugbuilds a fuzzing harness transformer and adds fuzzing to further expand the coverage of the generated code.
besides rulf and rpg are the two related works that focus on automatically generating the fuzzing harnesses leveraging the api dependency graph.
we note that compared to the api dependency graph of rulf or rpg the dependency graph of type rugis simplified by removing the function vertices leading to a significant reduction in search space during the graph searching process.
with this simplicity rugutilizes llm to decide the proper sequences of functions to test and prepare the test sample data which solves the two challenges of traditional approaches huge searching space and lack of an initial fuzzing corpus.
viii.
l imitation in this section we discuss the potential limitations of rug and possible future improvements.
rugrelies on static type dependency analysis to divide the context for the following steps.
therefore for other strong typed languages such as java there is no fundamental burden to transplant rug.
for weak typed languages like python the accuracy and scope of the static analysis will affect the quality of the generated tests and we argue that this is a general issue for these languages and other automatic testing tools have similar problems.
in addition rugis bonded to the specific version of rust and a general implementation can save time to accommodate changes.
to expand the code coverage of the generated tests rug uses fuzzing to explore the input space.
in order to launch the fuzzer rugtemporally disables all assertions which might miss some bugs while the fuzzing process.
apart from that rug s corpora selection algorithm may lose the code coverage based on the number limitation of generated tests.
for the generated test rugdoes not consider the framework and coding style of the existing tests which can be given as a sample prompt to llm for better code quality.
ix.
c onclusion in this work we propose rug which leverages llm and fuzzing to automatically generate testing code for rust projects.
rugproposes a bottom up approach to address the difficulties of rust program synthesis and fuzzing of the rust program to expand the coverage.
evaluation shows that rug achieves higher coverage than existing tools with efficiency and scalability.
11references f. taufiqurrahman s. widowati and m. j. alibasa the impacts of test driven development on code coverage in 1st international conference on software engineering and information technology icoseit .
ieee pp.
.
m. siniaalto and p. abrahamsson a comparative case study on the impact of test driven development on program design and test coverage infirst international symposium on empirical software engineering and measurement esem .
ieee pp.
.
m. davis s. choi s. estep b. myers and j. sunshine nanofuzz a usable tool for automatic test generation in proceedings of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering pp.
.
v .
tymofyeyev and g. fraser search based test suite generation for rust in international symposium on search based software engineering .
springer pp.
.
american fuzzy lop.
.
available y .
takashima r. martins l. jia and c. s. p as areanu syrust automatic testing of rust libraries with semantic aware program synthesis in proceedings of the 42nd acm sigplan international conference on programming language design and implementation pp.
.
x. wu n. cheriere c. zhang and d. narayanan rustgen an augmentation approach for generating compilable rust code with large language models .
z. xie y .
chen c. zhi s. deng and j. yin chatunitest a chatgpt based automated unit test generation tool arxiv preprint arxiv .
.
g. fraser and a. arcuri evosuite automatic test suite generation for object oriented software in proceedings of the 19th acm sigsoft symposium and the 13th european conference on foundations of software engineering pp.
.
s. lukasczyk and g. fraser pynguin automated unit test generation for python in proceedings of the acm ieee 44th international conference on software engineering companion proceedings pp.
.
c. lemieux j. p. inala s. k. lahiri and s. sen codamosa escaping coverage plateaus in test generation with pre trained large language models in ieee acm 45th international conference on software engineering icse .
ieee pp.
.
n. alshahwan j. chheda a. finegenova b. gokkaya m. harman i. harper a. marginean s. sengupta and e. wang automated unit test improvement using large language models at meta arxiv preprint arxiv .
.
sourcegraph cody .
github github copilot your ai pair programmer features copilot .
rust for linux contributors.
rust for linux linux.
github.
.
available the register.
microsoft to explore windows code written in rust.
online article.
.
available microsoft windows rust rust gpu contributors.
rust gpu rust cuda.
github.
.
available google security blog.
supporting use of rust in chromium.
blog post.
.
available supporting use of rust in chromium.html s. zhu z. zhang b. qin a. xiong and l. song learning and programming challenges of rust a mixed methods study in proceedings of the 44th international conference on software engineering pp.
.
h. zhu p. a. hall and j. h. may software unit test coverage and adequacy acm computing surveys csur vol.
no.
pp.
.
k. serebryany oss fuzz google s continuous fuzzing service for open source software .
j. jiang h. xu and y .
zhou rulf rust library fuzzing via api dependency graph traversal in 36th ieee acm international conference on automated software engineering ase .
ieee pp.
.
z. xu b. wu c. wen b. zhang s. qin and m. he rpg rust library fuzzing with pool based fuzz target generation and generic support in proceedings of the ieee acm 46th international conference on software engineering pp.
.
openai.
introducing chatgpt.
.
available blog chatgpt a. fan b. gokkaya m. harman m. lyubarskiy s. sengupta s. yoo and j. m. zhang large language models for software engineering survey and open problems arxiv preprint arxiv .
.
z. yuan y .
lou m. liu s. ding k. wang y .
chen and x. peng no more manual tests?
evaluating and improving chatgpt for unit test generation arxiv preprint arxiv .
.
m. wermelinger using github copilot to solve simple programming problems in proceedings of the 54th acm technical symposium on computer science education v .
pp.
.
s. kang b. chen s. yoo and j. g. lou explainable automated debugging via large language model driven scientific debugging arxiv preprint arxiv .
.
bincode org bincode a binary encoder decoder in rust com bincode org bincode .
p. deligiannis a. lal n. mehrotra and a. rastogi fixing rust compilation errors using llms arxiv preprint arxiv .
.
m. sch fer s. nadi a. eghbali and f. tip an empirical evaluation of using large language models for automated unit test generation ieee transactions on software engineering .
c. llvm.
source based code coverage clang documentation.
llvm project.
.
available sourcebasedcodecoverage.html camshaft bolero fuzzing and property testing front end framework for rust.
.
available openai platform models.
.
available com docs models gpt turbo p. nie r. banerjee j. j. li r. j. mooney and m. gligoric learning deep semantics for test completion in ieee acm 45th international conference on software engineering icse .
ieee pp.
.
n. rao k. jain u. alon c. le goues and v .
j. hellendoorn catlm training language models on aligned code and tests in 38th ieee acm international conference on automated software engineering ase .
ieee pp.
.
v .
guilherme and a. vincenzi an initial investigation of chatgpt unit test generation capability in proceedings of the 8th brazilian symposium on systematic and automated software testing pp.
.
y .
deng c. s. xia h. peng c. yang and l. zhang large language models are zero shot fuzzers fuzzing deep learning libraries via large language models in proceedings of the 32nd acm sigsoft international symposium on software testing and analysis pp.
.
j. liu c. s. xia y .
wang and l. zhang is your code generated by chatgpt really correct?
rigorous evaluation of large language models for code generation advances in neural information processing systems vol.
.
w. yang l. song and y .
xue rust lancet automated ownership ruleviolation fixing with behavior preservation .
j. c. king symbolic execution and program testing communications of the acm vol.
no.
pp.
.
l. a. clarke a system to generate test data and symbolically execute programs ieee transactions on software engineering no.
pp.
.
m. harman the current state and future of search based software engineering in future of software engineering fose .
ieee pp.
.
m. harman p. mcminn j. t. de souza and s. yoo empirical software engineering and verification.
chapter search based software engineering techniques taxonomy tutorial .
p. mcminn search based software test data generation a survey software testing verification and reliability vol.
no.
pp.
.
o. buehler and j. wegener evolutionary functional testing of an automated parking system in proceedings of the international conference on computer communication and control technologies ccct and the 9th.
international conference on information systems analysis and synthesis isas florida usa vol.
.
citeseer .
z. li m. harman and r. m. hierons search algorithms for regression test case prioritization ieee transactions on software engineering vol.
no.
pp.
.
k. r. walcott m. l. soffa g. m. kapfhammer and r. s. roos timeaware test suite prioritization in proceedings of the international symposium on software testing and analysis pp.
.
d. you z. chen b. xu b. luo and c. zhang an empirical study on the effectiveness of time aware test case prioritization techniques in proceedings of the acm symposium on applied computing pp.
.
l. zhang s. s. hou c. guo t. xie and h. mei time aware test case prioritization using integer linear programming in proceedings of the eighteenth international symposium on software testing and analysis pp.
.
m. khatibsyarbini m. a. isa d. n. jawawi and r. tumeng test case prioritization approaches in regression testing a systematic literature review information and software technology vol.
pp.
.
g. fraser and a. arcuri whole test suite generation ieee transactions on software engineering vol.
no.
pp.
.
c. pacheco and m. d. ernst eclat automatic generation and classification of test inputs in ecoop object oriented programming 19th european conference glasgow uk july .
proceedings .
springer pp.
.
a. nistor q. luo m. pradel t. r. gross and d. marinov ballerina automatic generation and clustering of efficient random unit tests for multithreaded code in 34th international conference on software engineering icse .
ieee pp.
.
s. thummalapenta j. de halleux n. tillmann and s. wadsworth dygen automatic generation of high coverage tests via mining gigabytes of dynamic traces in tests and proofs 4th international conference tap m laga spain july .
proceedings .
springer pp.
.
s. thummalapenta t. xie n. tillmann j. de halleux and w. schulte mseqgen object oriented unit test generation via mining source code in proceedings of the 7th joint meeting of the european software engineering conference and the acm sigsoft symposium on the foundations of software engineering pp.
.
a. arcuri restful api automated test case generation with evomaster acm transactions on software engineering and methodology tosem vol.
no.
pp.
.
y .
zheng x. xie t. su l. ma j. hao z. meng y .
liu r. shen y .
chen and c. fan wuji automatic online combat game testing using evolutionary deep reinforcement learning in 34th ieee acm international conference on automated software engineering ase .
ieee pp.
.
q. guo x. xie y .
li x. zhang y .
liu x. li and c. shen audee automated testing for deep learning frameworks in proceedings of the 35th ieee acm international conference on automated software engineering pp.
.
g. fraser and a. arcuri a large scale evaluation of automated unit test generation using evosuite acm transactions on software engineering and methodology tosem vol.
no.
pp.
.
b. jeong j. jang h. yi j. moon j. kim i. jeon t. kim w. shim and y .
h. hwang utopia automatic generation of fuzz driver using unit tests in ieee symposium on security and privacy sp .
ieee pp.
.
american fuzzy lop plus plus.
.
available aflplusplus aflplusplus r. f. team rust fuzz arbitrary generating structured data from arbitrary unstructured input.
.
available rust fuzz arbitrary t. rust fuzz team afl.rs fuzzing rust code with afl rust fuzz afl.rs accessed .
c. yang z. zhao and l. zhang kernelgpt enhanced kernel fuzzing via large language models arxiv preprint arxiv .
.
c. s. xia m. paltenghi j. le tian m. pradel and l. zhang fuzz4all universal fuzzing with large language models proc.
ieee acm icse .
r. meng m. mirchev m. b hme and a. roychoudhury large language model guided protocol fuzzing in proceedings of the 31st annual network and distributed system security symposium ndss .
j. eom s. jeong and t. kwon covrl fuzzing javascript engines with coverage guided reinforcement learning for llm based mutation arxiv preprint arxiv .
.
l. huang p. zhao h. chen and l. ma large language models based fuzzing techniques a survey arxiv preprint arxiv .
.