arxiv .15150v1 aug 2024 prl a mutation testing pipeline for deep reinforcement learning based on real faults deepak george thomas matteo biagiola nargiz humbatova mohammad wardat gunel jahangirova hridesh rajan paolo tonella dept.
of computer science iowa state university iowa usa dgthomas iastate.edu universit a della svizzera italiana lugano switzerland matteo.biagiola nargiz.humbatova paolo.tonella usi.ch dept.
of computer science and engineering oakland univers ity michigan usa wardat oakland.edu king s college london london united kingdom gunel.jaha ngirova kcl.ac.uk school of science and engineering tulane university loui siana usa hrajan tulane.edu abstract reinforcement learning rl is increasingly adopted to train agents that can deal with complex sequentia l tasks such as driving an autonomous vehicle or controlling a humanoid robot.
correspondingly novel approaches are nee ded to ensure that rl agents have been tested adequately before going to production.
among them mutation testing is quite promising especially under the assumption that the inject ed faults mutations mimic the real ones.
in this paper we first describe a taxonomy of real rl faults obtained by repository mining.
then we present the mutatio n operators derived from such real faults and implemented in t he tool prl.
finally we discuss the experimental results showing that prl is effective at discriminating strong from weak test generators hence providing useful feedback to developers about the adequacy of the generated test scenarios.
index terms reinforcement learning mutation testing real faults i. i ntroduction reinforcement learning rl is being applied to various safety critical systems such as traffic control drone navi gation and power grids .
due to its relevance in such systems rl developers need to make sure that their rl agent is tested thoroughly.
mutation testing is a powerful technique to ensure that the test set exercises the system in an adequate w ay but existing attempts to apply mutation testing to rl a re limited and do not cover the full spectrum of faults that may affect an rl agent.
in this paper we construct a comprehensi ve taxonomy of real faults identified by repository mining we analysed posts .
times more than previous work and we develop an rl mutation tool named prl which implements new mutation operators mos mimicking the real faults in the taxonomy.
our taxonomy of real rl faults targets rl developers who use well known frameworks such as stablebaselines3 and openai gym for their projects.
we mined stackexchange and github posts and then manually analysed the relevant artifacts to identify real bugs that developer s experience.
then we derived mutation operators from the taxonomy and implemented of them in the tool prl.
these operators have been evaluated on four environments provide dby openai gym and highwayenv cartpole parking lunarlander and humanoid .
the humanoid environment is a particularly challenging robotics enviro nment with a high dimensional observation space thereby requiri ng extensive computational resources.
in the evaluation we a pplied prl to environments with both discrete and continuous action spaces and we considered popular deep rl drl algorithms including dqn ppo sac and tqc .
experimental results indicate that our mutation tool prl is useful in discriminating strong from weak test scenario generators and achieves a high sensitivity to the test set quality substantially higher than that of the state of the a rt tool rlmutation .
we also show that the new fault types introduced in our taxonomy are major contributors to the increased sensitivity of our mutation operators.
ii.
r elated work we organise the related works into those analysing deep learning dl rl faults and those mutating dl rl models.
a. fault classification dl faults humbatova et al.
proposed a dl faults taxonomy using stackoverflow and github artifacts.
islam et al.
studied the frequency root cause impact and pattern of bugs while using deep learning software.
rl faults nikanjam et al.
developed a fault taxonomy for rl programs.
they studied rl artifacts obtained from stackoverflow and github.
while analysing github they went over issues from the following frameworks openai gym dopamine keras rl and tensorforce.
their work focuses on mistakes developers make while writing an rl algorithm from scratch.
however rl algorithms are notoriously hard to implement and even small implemen tation details have a dramatic effect on performance .
our work considers the perspective of software developers who use rl as a tool to address an engineering problem.
therefore we focus on bugs that arise while using reliable r l implementations or bugs that can be mapped 1to those occurring in reliable implementations .
we compar e with the taxonomy of nikanjam et al.
in section iii d. andrychowicz et al.
studied the effect of more than design choices on an rl agent s performance by training around kagents.
they used this study to recommend various hyperparameter choices.
b. mutation testing for ai based systems mutation testing for dl deepmutation and munn were the pioneers in recognising the need for mutation operators specifically tailored to dl systems.
sub sequently deepmutation was extended to a tool called deepmutation focusing on operators that can be applied to t he weights or structure of an already trained model.
jahangiro va and tonella performed an extensive empirical evaluati on of the mutation operators proposed in deepmutation and munn.
deepcrime differs from deepmutation in that it uses a set of mutation operators derived from real faults in dl systems .
such operators are applied to a dl model before training.
mutation testing for rl there are currently two papers dedicated to mutation testing of rl systems.
lu et al.
introduce a set of rl specific mutation operators includin g element level mutation operators and agent level mutation operators.
the element level mutations consider the injec tion of perturbations into the states and rewards of an agent.
age ntlevel mutations introduce errors into an already trained ag ent.
if a trained agent s policy is represented by a q table an agent level mutation would fuzz the state action value estimations stored in the table.
if the policy of a trained ag ent is implemented by a neural network an agent level mutation would remove a neuron from the input or output layer of the network.
in addition the authors propose operators that af fect the exploration exploitation balance by for instance mu tating the exploration rate of the agent during training.
however such mutation operators are not based on any real world fault taxonomy or existing literature.
moreove r mutation killing is computed on a single repetition of the experiment not accounting for randomness in the training process.
previous literature shows that rl algorithms are sensitive to the random seed suggesting that statisti cal evaluations are needed to draw reliable conclusions .
tambon et al.
explore the use of higher order mutants i.e.
the subsequent application of different mutations to a program under test in the context of rl.
the mutation operators they propose implemented in the tool rlmutation are based on a number of sources.
as a basis the authors have adopted the existing operators for rl and dl systems and complemented them with operators extracted from the state of the art taxonomies of rl faults and real dl faults .
they divided the obtained operators into three broad categories environment level agent level and policylevel operators.
mutations at the environment level include faults related to the observations the agent perceives in th e environment for instance due to faulty sensors or delibera te attacks.
operators at the agent level stem from the faults thatdevelopers make while implementing rl agorithms such as omitting the terminal state or selecting a wrong loss functi on.
finally policy level mutations focus on the agent s policy network mutating activation functions or number of layers .
these mutations are used to create first order mutants.
amon g them those that are not trivial i.e.
that are not killed by all the test environments are considered for higher order mutati on.
our mutation tool prl differs substantially from both lu et al.
and tambon et al.
because it is grounded on a novel taxonomy of real faults experienced by developers that rely on existing mature frameworks for the creation of rl agents.
this rules out the syntactic state reward polic y perturbations introduced by lu et al.
and the mistakes made by programmers when implementing rl algorithms from scratch that are instead considered by tambon et al.
.
since rlmutation includes also real faults that may occu r when developers rely on existing rl frameworks we conduct a detailed comparison with the existing taxonomy and tool in section iii d and section v c respectively.
iii.
t axonomy of real rl faults we constructed a taxonomy of real rl faults in a bottom up way starting from the collection of artefacts obtained th rough software repository mining.
we then labeled such artefacts to eventually organise the labels into a taxonomy.
a. mining of software artefacts in our preliminary investigation we observed that most discussions about faults reported by developers implement ing an rl agent happen in stack exchange.
we also noticed several commit messages about rl faults in github.
hence we mined these two repositories.
mining github the foremost challenge while mining github repositories was identifying popular rl frameworks .
nikanjam et al.
used keras rl dopamine tensorforce and openai gym .
however openai gym is only used to simulate the interactions between an agent and the environment and does not provide rl algorithms.
while investigati ng the remaining frameworks we found that tensorforce is no longer maintained and keras rl did not get updated since november .
therefore to identify popular rl frameworks we checked top tier machine learning and software engineering conferences such as icml icse esec fse and ase.
we manually inspected papers dropping all papers that focused on model based rl inverse rl and multi agent rl.
this filtering step left us with papers from se conferences and papers from icml that provided actionable insights.
while many were custom implementations of rl algorithms the majority of the paper s that used frameworks used stablebaselines3 overall .
we followed humbatova et al.
s approach to mine github repositories.
we searched for files containing the st ring stable baselines3 using the github search api and found files.
since the api has a limit of kresults per query we searched for file sizes between and kbytes with a step size of .
we identified repositories 2corresponding to these files.
we then dropped all repositori es that had less than stars commits forks and contributors which left us with repositories.
as the nex t step we manually verified whether they were related to rl and dropped the repositories containing tutorials and code examples using stablebaselines3 obtaining the list of the final repositories.
these repositories were then used to ext ract issues pull requests prs and commit messages.
while extracting the issues we only extracted those that contain ed the label bug defect or error in them.
following isla met al.
we only selected commits that contain the term fix .
to automate the process of extracting relevant artifacts fr om github we followed humbatova et al.
we combined all issues pr titles descriptions and comments along wit h commit messages into a text dump.
we did data cleaning on the words within the text dump dropped stop words non word characters etc and counted the frequency of each remainin g word.
words that had a frequency lower than raised from to obtain a manageable list of words were dropped resulting in a list of words.
we divided the final list of words among authors to identify relevant rl words and obtained .
we then selected the corresponding issues pr s and commits a total of .
mining stack exchange to include questions on data science and artificial intelligence which might be relevan t for our taxonomy we mined both stackoverflow so and stack exchange s se data science ds and artificial intelligen ce ai q a websites with so falling under the umbrella of se .
table i number of unique tags and posts in se and so tags all tags rl posts artificial intelligence ai se data science ds se stackoverflow so se total we used se s data explorer to extract posts from so and se.
we first extracted all tags from these websites then we filtered all tags without the term reinforcement in thei r respective name excerpt i.e.
short description or wik ibody i.e.
detailed description .
the resulting tags were t hen used to select all posts from se.
next we excluded all posts without an accepted answer .
we also filtered out the post s whose title contained the terms how install or build to discard how to questions.
during manual inspection we fou nd that many posts were not rl specific but rather related to machine learning in general.
therefore we dropped all posts t hat had only one tag and it was machine learning .
following th is procedure we obtained posts see table i .
given the large number of posts we performed various pilot studies to gauge the data quality of selected samples and ma nually filtered out irrelevant posts.
these pilot studies yie lded a large number of false positives.
upon a closer examination o f the dataset we found that the posts from so contained around of the total posts and the tag artifical intelligence w aspresent in so posts without dropping duplicates .
a big chunk of the posts within this category contained posts concerning classical ai such as the a algorithm.
therefore we dropped all posts that either contained the tag artificia lintelligence or a combination of artificial intelligenc e and machine learning without another rl tag.
lastly follo wing islam et al.
we kept only posts that contained code.
this brought our final dataset size down to .
manual labelling one group of labellers manually analysed the artifacts and dropped false positives.
five au thors participated in the labeling process for taxonomy construc tion.
each post in the dataset was randomly assigned to two authors .
we used the procedure by humbatova et al.
for the labeling process.
the authors therein developed a tool that help ed them manage the labels.
this tool allowed each assessor to pick a label generated by their colleagues.
in case none of the labels matched the post description they created their own label and added it to the tool which then became accessible t o others.
we pre loaded all labels created by nikanjam et al.
into the labeling tool to be consistent with and build upon the existing rl fault taxonomy .
furthermore to check the disagreement between various participants we measure d krippendorf s alpha which handles more than two raters with each rater only labeling a subset of the posts.
krippendorf proposed to reject data where the confidence interval of the reliability falls below .
.
ideally the v alue of alpha should be .
but variables with values greater than .
could be relied upon .
during labeling the two raters of each post met together to resolve conflicts when any such conflict arouse.
when no resolution between two raters could be reached for a certain post the overall group made the final decision through voting .
while the average agreement krippendorf s alpha was .
before the consensus meeting it raised to .
after the meeting.
finally all the authors went through all the posts together for a final pass.
b. taxonomy construction we followed islam et al.
s approach to build the taxonomy tree wherein we built our tree on top of another existing rl fault tree by nikanjam et al.
s marked in orange blue in figure .
for new labels marked in green in figure we followed humbatova et al.
s approach to group them into higher level categories.
c. the final taxonomy reward function.
this category is related to faults affecting the reward function guiding the rl agent towards the learning objective.
defining the reward function designing the reward function is critical to achieve good performance in rl.
we found the following faults associated with it.
suboptimal reward function defining a good reward function is challenging especially for complex tasks involving multiple constrain ts.
for instance learning a robust policy for quadrupedal robo ts requires a complex reward function encouraging linear and 3rl fault reward function defining the reward function suboptimal reward function se gh sparse reward se gh reward function not defined for entire range of behaviour se gh potential of terminal step not fixed to zero se gh normalising the reward function missing reward normalisation scaling se gh unnecessary reward normalisation se gh wrapping reward after observation normalisation se gh suboptimal discount factor se gh training process training budget small number of mcts simulations se gh not enough episodes iterations training se gh rl function approximator design missing policy gradient clipping se gh suboptimal learning start se gh non random starting state se gh non stochastic policy se gh use of inappropriate function approximator for the given environment se gh use of variable horizon in environment where fixed horizon is more appropriate se gh suboptimal number of rollout steps se gh normalisation missing action normalisation se gh suboptimal missing normalisaton of observations se gh wrong normalisation of advantage se gh inference misconfiguration of the agent to a deterministic inference se gh misconfiguration of the agent to a stochastic inference se gh rl fault environment setup termination flags flags indicating successful termination or timeout not set properly se gh missing check for dones se gh action selection suboptimal action space se gh state representation subtoptimal frame skip parameter se gh suboptimal scaling of features se gh suboptimal features for state representation se gh suboptimal state space se gh suboptimal replay buffer suboptimal replay buffer design se gh suboptimal replay buffer size se gh suboptimal sampling from replay buffer se gh network update suboptimal network update frequency se gh biased target network se gh suboptimal polyak constant value se gh suboptimally balancing value and policy networks se gh wrong network update wrong update rule se gh environment exploration suboptimal exploration rate epsilon se gh suboptimal exploration decay se gh missing exploration se gh suboptimal minimum exploration rate se gh suboptimal application of dirichilet noise se gh suboptimal noise sampling se gh missing reset close environment se gh regularisation suboptimal entropy coefficient se gh fig.
taxonomy of real rl faults green indicates new fault types orange and blue indicate fault types in common with th e previous taxonomy blue indicates the ones that we renam ed.
se gh are preceded by the number of instances found in stackexchange github.
4angular velocities while penalising vibrations and energ y consumption .
manually setting weights for these componen ts is not an easy task for developers while they may greatly impact the learning effectiveness.
sparse reward this deals with cases where the reward is provided on rare occasions for instance at the end of each episode rather than at each timestep.
in some environments a sparse reward makes training ineffective or even impossible .
reward function not defined for the entire range of behavior this fault occurs when the reward function does not account for all of the trajectories that the agent might take.
potential of terminal step not fixed to zero this fault is related to the process of reward shaping wherein the agent is provided wit h supplemental rewards to make the learning smoother.
when the shaping function is based on a potential the value of suc h potential should be zero at the terminal step.
normalising the reward function a class of faults associated with reward functions is related to normalisati on.
missing reward normalisation scaling reward scaling typically involves taking the product of the environment reward s with a scalar r r .
in certain environments clipping is an alternative to scaling.
existing studies show that the choice of reward scaling clipping has a large impac t on the output of the training process.
unnecessary reward normalisation this fault occurs when reward function normalisation is not required and its use is actually detriment al to training.
wrapping reward after observation normalisation in this fault observations are normalised before a wrappe r is applied to the reward function making the wrapper suboptimal.
suboptimal discount factor the discount factor is a critical parameter used to trade off future and immediat e rewards.
when is close to the agent focuses on actions that maximise the short term reward whereas when is close to the agent privileges actions that maximise future rewa rds.
training process.
this category consists of faults that affect the training process of the rl agent.
training budget this category concerns faults related to the number of iterations used to train the rl agent.
small number of mcts simulations this fault was observed in the context of the alphagozero algorithm .
this algorithm uses monte carlo tree search mcts to learn optimal actions.
having a low number of mcts simulations may lead to suboptimal actions being selected .
not enough episodes iterations training this fault occurs when the number of training iterations for the rl algorithm is low.
th is prevents the rl algorithm from learning a good policy.
rl function approximator design a critical element in rl is the function approximator learnt during training.
thi s category consists of faults that occur due to the design choi ces related to the selection of the function approximator.
missing policy gradient clipping incorporating policy gradient clipping improves the performance of actor critic algorithms .
suboptimal learning starts when the training process starts the agent is allowed to take a series of random actions withou t learning.
the learning starts hyperparameter is a critic al parameter that controls when the agent starts learning aft erthe training process has begun.
non random starting state starting at the same state each time the agent is reset preve nts it from exploring the surrounding states and leads to overfit ting.
non stochastic policy certain rl algorithms require a stochastic policy and therefore the function approximato r must be stochastic in nature.
implementing a deterministic function approximator could lead to a drop in performance.
use of inappropriate function approximator for the given environment rl problems of different sizes in terms of state and action spaces require different approximation techni ques.
relatively smaller problems might be solved using tabular methods whereas larger problems might require linear or non linear function approximators such as neural networks .
use of variable horizon in environment where fixed horizon is more appropriate this fault occurs when reward learning is adopted during rl training.
for effective reward learnin g e.g.
from human preferences a fixed episode length was found to be often a better choice .
suboptimal number of rollout steps this fault occurs in the context of on policy algorithms and refers to the number of rollout steps per environment used to update the policy.
this hyperparameter significantly affects the algorithm s performance .
normalisation this category is related to normalisation in the context of training.
missing action normalisation action normalisation has been found to be helpful especial ly when the actions are continuous .
suboptimal missing normalisaton of observations andrychowicz et al.
recommend to always normalise observations.
as per their experiments this was critical for achieving high per formance in almost all the environments.
wrong normalisation of advantage rl algorithms such as ppo and a3c compute the advantage function i.e.
an estimate of the val ue of a certain action in a given observation.
normalising this estimate with a single sample or a few samples may result in diverging computations nan and to gradients with larg e variances.
inference.
this category deals with faults that occur at inference time i.e.
after the rl algorithm has been traine d. misconfiguration of the agent to a deterministic inference during inference forcing an agent to take deterministic actions when it was trained with a stochastic policy might lead to a drop in performance .
in fact for problems wher e appreciable generalisation is required at inference time e.g.
when there is a substantial development to production shi ft a better policy may be a stochastic one.
misconfiguration of the agent to a stochastic inference forcing an agent that was trained with a deterministic policy to become stochasti c at inference time thereby carrying out exploratory behavi or can also lead to a performance drop.
environment setup.
faults related to the environment setup fall under this category.
termination flags flags that denote when an episode has ended might be set incorrectly.
flags indicating successful termination or timeout not set properly flags for termination and timeout should only be set in terminal states.
terminal 5states are critical for calculating state values and these values are recursively used to compute the values for previous stat es.
missing check for dones during training the algorithm needs to correctly check whether a state is terminal i.e.
done as this determines how the targets for the optimisation proble m are computed.
action selection this fault occurs when the user defines an action space that makes learning difficult or impossible e.g.
representing actions as discrete integers vs bit vec tors .
state representation this category deals with faults associated with the definition of environment states.
suboptimal frame skip parameter the frame skip parameter forces an action to be repeated for a specific number of frames.
this parameter was found to have a significant impact in terms of learning efficiency in environments requiring high frequ ency policies such as atari games and robotic applications .
suboptimal scaling of features state features must be scaled appropriately in order for an rl algorithm to learn efficien tly.
suboptimal features for state representation in order to speed up learning rather than feed in raw state inputs and expect the learning algorithm to identify useful patterns develo pers could use their domain knowledge and engineer the state to include relevant possibly higher level features.
suboptimal state space the rl paradigm assumes that the environment the agent operates in follows the markov property i.e.
th at the current state the agent perceives summarises all the pa st interactions of the agent with the environment.
in other wor ds all the information the agent needs to make optimal actions need to be in the state space of the agent.
if some crucial information are hidden from the agent the markov property does not hold and the agent cannot learn optimally .
suboptimal replay buffer.
off policy rl algorithms typically use a replay buffer during training.
suboptimal replay buffer design catastrophic forgetting might be caused by the under representation of dat a for specific tasks in the replay buffer.
this fault is common in multi tasks rl settings i.e.
when the rl agent has multiple objectives but also in single environments that c an be decomposed in sub objectives or levels .
suboptimal replay buffer size the replay buffer size is a non trivial tunable hyperparameter.
while a smaller replay buffer may lead to relevant data getting replaced too quickly a large buffer might lead to older and irrelevant data getting sampl ed reducing learning efficiency .
suboptimal sampling from replay buffer it is crucial that the sampling process maintains the i.i.d.
identical and independently distributed prop erty of the data and that the sampled data are not temporally correlated.
if this property does not hold the learning pro cess might be negatively affected.
network update.
this category refers to updating the parameters of the neural networks implementing the functio n approximators.
suboptimal network update frequency the frequency of the target network updates is too low high impacting the le arning effectiveness .
biased target network this fault occurs when the target network parameters are not independent fromthe online network s. the target network prevents the polic y from exploring alternative solutions while the online netw ork is being updated .
this fault prevents the target networ k from converging to the optimal one.
suboptimal polyak constant value an alternative to duplicating the online network weights as target network weights is to perform soft update s by polyak averaging.
the critical hyperparameter controll ing such soft updates is the polyak update coefficient .
suboptimally balancing value and policy networks this fault occurs while using the alphago algorithm .
this algorithm uses mcts to select actions by utilising value and policy networks.
the parameter balances the decisions of these two networks.
a fault occurs when a poor value of the hyperparameter is set .
wrong network update wrong update rule new data cannot be optimally learned by the rl algorithm e.g.
because the learning rate of the neur al networks decays too quickly .
environment exploration.
we found a variety of critical exploration hyperparameters in various rl algorithms.
exp loring too little may cause the rl algorithm to be unable to discover high reward states and actions exploring too much prevents the agent from exploiting what it has learned.
suboptimal exploration rate epsilon this hyperparameter refers to the suboptimal setting of the epsilon paramet er present in various rl algorithms .
suboptimal exploration decay during the start of rl training the algorithm is expected to explore states extensively to identify promis ing states and actions.
however as the algorithm progresses t he amount of exploration is typically reduced so that the agent can exploit its existing knowledge.
missing exploration this label refers to the case where the agent does not explore at all .
suboptimal minimum exploration rate once the exploration parameter has been completely decayed it shou ld be left to a value that is still greater than zero.
this ensure s that the agent continues to explore for the remaining traini ng budget.
however too large values will interfere with learn ing while a value that s too low will prevent experiencing new states and actions.
suboptimal application of dirichilet noise dirichilet noise is used by the alphago algorithm for exploration.
the noise sampled from the dirichilet distribut ion which requires careful setting is added to the root node s prior probabilities .
suboptimal noise sampling this fault was found in the usage of the deep deterministic policy gradient ddpg algorithm.
ddpg incorporates exploration during training by adding noise to actions.
the choice of the distribution to sample the noise affects the exploratio n efficacy .
missing reset close environment this fault deals with forgetting to reset or to close the environment during training or inference .
regularisation policy regularisation improves the performance of rl algorithms .
suboptimal entropy coefficient the asynchronous actor critic and ppo algorithms incorporate the policy s entropy to the loss function to improve exploration.
there fore the entropy coefficient hyperparameter becomes critical to control the exploration rate of the agent.
6d.
comparing prior work with our taxonomy nikanjam et al.
s final taxonomy has fault categories.
our taxonomy contains five of these categories with orange background in figure plus one which we renamed with blue background in figure .
the remaining five categories do not match any category in our taxonomy for at least one of the following reasons the fault could not be mapped to an rl framework i.e.
it only affects re implementations o f rl algorithms the fault is not rl specific e.g.
the fau lt is a generic dl fault there is no evidence for the fault in our mined posts e.g.
the associated posts contain a how to question instead of describing actual issues and discussi ng possible solutions the fault is a generic coding error the associated post does not refer to any code implementi ng the rl agent.
for the matching fault types we used the same labels as nikanjam et al.
except for suboptimal exploration rate which we renamed to suboptimal exploration decay specifying more precisely that the fault is related to how fast slow the exploration rate is decayed over time.
iv.
m utation analysis a. mutation operators table ii list of proposed mutation operators in prl.
group operator id is reward functionchange discount factor sdf y make reward sparse spr n change reward scale srs n training processchange number of rollout steps snr y change learning start sls y reduce episodes iterations nei y introduce deterministic start state nrs n remove normalisation of actions mna n remove normalisation of observations mno n regularisation change entropy coefficient sec y network updatechange network update frequency snu y change polyak constant value spv y suboptimal replay buffer change replay buffer size sbs n environment explorationchange minimum exploration rate smr y change exploration rate ser n to define a set of mutation operators we analysed all of the unique fault types in the rl taxonomy of real faults see section iii c .
the extraction of mutation oper ators was organised into three stages.
first two of the authors independently went through the whole list of faults types an d each derived potential mutation operators mos from the inspected faults.
then they have performed conflict resolu tion between themselves and produced a list of proposed operato rs.
at the next step two other authors have separately inspecte d the set of candidate mos and the faults that did not inspire any mo.
both of the authors have shown full agreement with the initial list of the operators i.e.
have not proposed any new mos or rejected the existing ones.
at the last stage two authors one from each stage have gone through the list of mos to document their feasibility and applicabilityscenarios.
the mo extraction process as well as the comment s on the possible implementation approaches are available i n our replication package .
in total we propose mutati on operators with of them implemented in our tool prl.
table ii enlists the final set of proposed mos which are divided according to the corresponding top category of the taxonomy column .
column provides a short description of each mo while column specifies a short abbreviated name.
the last column is which stands for implementatio n status shows whether an operator is implemented or not.
th e operators that are domain specific i.e.
that require a cust om implementation for each case study have not been implemented as they are not generally applicable.
for instance the make reward sparse operator requires knowledge of how the reward function is implemented in a given environment while the missing normalisation operators are not applic able in environments where actions are discrete or observati ons are not normalised by default.
in total the operators span six out of the eight top categori es of the taxonomy.
training process is the most populated category with six of the proposed operators.
most of the operators stem from one fault type in the taxonomy with the name of the mo corresponding to the name of the taxonomy leaf.
change reward scale is an exception to this rule as it corresponds both to the missing reward normalisation sca ling and unnecessary reward normalisation fault types.
b. mutation analysis procedure to ensure reliable and statistically sound evaluation of th e quality of test sets we adopt the definition of statistical killing a mutant is killed by a test set if the prediction accuracies of original and mutated model computed on such test set differ in a statistically significant way.
however rl presents numerous differences w.r.t.
dl models that we need to account for when evaluating an rl agent.
in rl since the agent is trained online a test can be represented as an initial configuration of the environmen t where the agent operates .
let us consider the cartpole subject environment consisting of a cart moving to keep a pole vertically aligned this is the classical inverted pen dulum problem .
its initial configuration eis a dim vector e wherexis the initial position of the cart xis the initial velocity of the cart is the initial angle of the pole w.r.t.
the vertical axis and is the initial angular velocity of the pole.
trained rl agents are typically evaluated using a s et of randomly generated initial environment configurations to test their generalisation capabilities.
as there is no ex plicit test set to evaluate rl agents in a given environment we refe r to a test environment generator or test generator tgfor short rather than a test set a random tg which randomly generates initial environment configurations is one example of tg.
letabe the rl agent under test.
to support statistical analysis of mutation killing we train ninstances of aforntime steps each.
for each instance an arbitrary random seed is chosen for the generation of a reproducible sequence of initial environment configurations environment 7configurations for short which are used to train the agent within the ntime steps budget.
then for any given mutation operator pand its configuration p j j jp we train n mutant instances by reusing the same set of random seeds and as a result the same sequence of initial environment configurations used to train the original agent instances.
i n this way we create npairs of original and mutant instances oi mi that are trained on the same sequence or sequence prefix if a shorter sequence is used of initial environment s. killability we first define the killed predicate for an mo parameter configuration p j and then we use it to define the notion of killable and its complement likely equivalent mo configuration p j .
to decide whether a mutant is killed by a test generator tg we execute each pair oi mi of original and mutant agents in test mode on the sequences of environment configurations generated by tg.
since tgmight generate different sequences depending on the agent under test with no loss of generality we assume that two different test sequences toi tg oi and tmi tg mi are obtained when applying tgto either the original agent oior the mutant mi.
when such a dependency does not hold i.e.
the dependency between tgand the agent under test there is a single test sequence t toi tmi.
this happens for instance when using a random tgor a predefined sequence of environment configurations tas test set.
it is also convenient to assume that the two test sequenc es have the same length l toi tmi .
we represent the result of the execution of each pair oi mi on the corresponding sequences toi tmi as a 4tuple soi foi smi fmi wheresoiandfoiare the number of successes and failures for the i th original agent instance oi withsoi foi l the variables smiand fmistore these measurements for the paired i th mutant instancemi withsmi fmi l. given the contingency table for each pair oi mi we apply the fisher s test to decide whether the mutant instance miis killed or not the killed predicate kbeing defined as k oi mi pvalue .
.
note that mutating the original agent may result in a mutant that improves over the performance of the original weaker agent in this case a certain pair can be killed because fo i fm i i.e.
the original instance o ifails more often than the mutated instance m i. all such pairs o i m i are discarded and for them the killed predicate is conventionally set to zero i.e.
k o i m i .
the killed predicate kof a given mutant configuration p j is calculated for a given test generator tgbased on the number of killed instance pairs over the total number of pairs n w wherewis the number of pairs where the original instance is weaker i.e.
fails more often than its mutated instance pa ir k tg p j braceleftbigg true ifkr tg p j .
false otherwise wherekr tg p j n w summationtextn i 1k oi mi is the killing rate i.e.
proportion of killed mutant instances .
a certain mutant configuration p j iskilled if at least half ofits pairs is killed excluding those pairs where the origina l instance is weaker than the mutant instance.
the notion of likely equivalent and its complement killable mutant that we use in our mutation procedure is based on the one proposed in deepcrime a mutant is likelyequivalent if the training data cannot capture the differences between the original and mutant model.
hence to decide whether the mo parameter configuration p j iskillable we check if it is killed by the training data.
specifically we se t tg trsoi i.e.
we replay both oiandmion the set of training environment configurations trsoiand compute the killing predicate k trsoi p j .
as trsoiwas used to train oiand at least a prefix of trsoiwas used to train mi we expect trsoito be highly discriminative between original and mutant .
a mutation operator piskillable if at least one of its mutant configurations p j is killable.
if a mutation operator is not killable it is deemed likely equivalent and discarded.
triviality we are also interested in checking whether a certain mo generates mutants that are trivial to kill.
to evaluate triviality of each mutant we reuse the results of the replay of each original agent and mutant pair oi mi on their set of training environment configurations trsoi from killability analysis.
from trsoi we select the subset of environment configurations where the original agent instan ce oisucceeds and check in how many of these environment configurations the mutant instance mifails.
we calculate the average proportion of failing environment configurations o ver all the pairs and if it exceeds we consider the mutant t o be trivial.
we exclude trivial mutants from our analysis as they would inflate the mutation score without being discriminati ve.
mutation score once the likely equivalent mutants are sorted out for each pair oi mi we generate ltest environment configurations using the given test generator tg.
the mutation score of a test generator tg for a given mutation operatorp is the average killing rate kracross all mutant configurations p j .
given an rl mutation tool the overall mutation score msfor a test generator tgis calculated as the average across the tool s mos ms tg op op summationdisplay p op1 jp summationdisplay j jpkr tg p j v. e mpirical evaluation rq are prl s mutation operators useful?
do they discriminate between test environment generators o f different qualities?
in the first research question we investigate whether the mu tation testing pipeline in prl is able to generate non trivial killable and discriminative mutants i.e.
mutants that would tell apart test environment generators of different qualit ies.
metrics.
to answer rq 1we measure triviality and killability for each mutation operator in each subject environment and rl algorithm.
we also measure the mutation scores of two 1considering the killing rate krrather than the killed predicate k ensures that the mutation score computation is more fine grained.
8test generators details provided in section v b namely weak tgw and strong tgs .
to evaluate the discriminative power of the mutants we measure the sensitivity between the weak and strong test generators as defined in deepcrime when ms tgs op ms tgw op sensitivity ms tgs op ms tgw op ms tgs op while we set it to zero when ms tgs op ms tgw op .
rq how does prl compare with the stateof the art rlmutation approach?
in this research question we compare our tool with an existing mutation tool for rl namely rlmutation .
metrics.
to answer rq we compare prl and rlmutation on the same subject environments and rl algorithms used for rq by measuring sensitivity of tgwandtgson the mutants produced by both approaches.
rq what is the impact of the new fault types identified in our taxonomy and implemented in prl ?
in this research question we evaluate the specific contribu tion of the new fault types that emerge from our taxonomy w.r.t.
existing rl fault taxonomies in the literature.
in pa rticular we consider the impact of the five new mutation operators namely snr sls nei sec and spv .
metrics.
to answer rq we compute the sensitivity of the newly introduced fault types for each pair subject environm ent env rl algorithm a .
a. subject environments and rl algorithms subject environments.
we evaluated our approach using two subject environments used in previous work namely cartpole and lunarlander to be able to compare our approach with rlmutation and we added two new subject environments one concerning the driving domain i.e.
parking and a robotic environment namely humanoid both of which are commonly used in the rl literature.
each environment has an initial configuration that is generated randomly at the beginning of each episode according to the constraints determined by each environment.
rl algorithms.
we selected four rl algorithms that are widely used in the literature.
dqn is representative of value based algorithms while ppo is a widely used policy gradient algorithm.
sac and tqc are hybrid algorithms i.e.
blending value based and policy gradie nt techniques.
dqn sac and tqc are off policy algorithms while ppo is an on policy algorithm.
the different characteristi cs of these four rl algorithms allow for the application of all mos of prl.
b. procedure rq .
for each subject environment we trained the original agents with the applicable rl algorithms using the hyperparameters provided by raffin et al.
.
dqn is only applicable to cartpole and lunarlander as it only supports discrete action spaces while sac and tqc only support continous action spaces hence they are only applicable onparking and humanoid.
ppo cannot be applied on parking as it is a goal based environment that requires an off polic y algorithm.
we discarded the ppo algorithm on humanoid as with the default hyperparameters the agent had a near zero success rate in repeated training instances.
we trained n original agents for each pair subject environment rl algoritm env a to account for the randomness of the training process.
then for each applicable mutation operator mo pgiven the pair env a we randomly sampled j mutant configurations.
when designing the sampling range for each mo we started from the corresponding search space already defined by raffin et al.
for hyperparameter tuning but we adjusted it to increase the chance of generating challenging mutant configurations.
for categorical search spaces concerning six out of eight mos we decreased by the upper and or lower bounds of the original hyperparameter search space for sdf we did not increase the upper bound as the original highest value .
is very close to the theoretical maximum .
.
three exceptions concern the sls nei not available in the original hyperparameter search space and snu operators where we considered the corresponding mutation search spac e as relative to the training time steps budget.
after training the original agents and the corresponding mutant configurations for each pair env a we replay the training environment configurations.
for each mutation ope ratorp we select the configuration p j that is killable nontrivial and closest to the original value.
the next step after replay is building the weak tgw and strong tgs test environment generators.
to obtain tgw we first generated test environment configurations at random and executed them on the original agent.
during the execution we track the quality of control qoc of the trained agent as a way to measure the confidence of the agent during a certain episode.
for instance in cartpole th e agent can fail in two ways i.e.
either if it brings the cart too far from the center .
m or if the pole it is controlling falls beyond a certain angle .
at each time step tthe qoctis the minimum between two normalised distances i.e.
the absolute distance between the current position of the cart and .
and the absolute difference between the curren t angle of the pole and .
likewise the qoc metric can be defined for the other subject environments.
then for each test environment configuration we take the minimum qoc value and we rank the test environment configurations in descending order of qoc .
finally we select the first test environment configurations as those generated by tgw.
to obtain tgs we resort to the approach by uesato et al.
and biagiola et al.
which consists of training a failure predictor on the training environment configuratio ns to learn failure patterns of the trained agent in order to generate challenging test environment configurations.
sin ce our objective is to kill mutants we train one neural network as failure predictor for each selected mutant configuration.
t hen we use the trained neural network predictor in each mutant configuration to select promising test environment confi g9table iii results for rq rq rq .
gray cells indicate that the specific mutation operator is n ot applicable to a certain env a pair.
boldfaced values indicate that an operator is killabl e while the symbol stands for not available .
underlin ed operators indicate new fault types w.r.t.
existing taxonom ies and avg new refers to the average computed only for underlined operators.
the sensitivity of rlmutation is reported in the last row.
cartpole lunarlander parking humanoid dqn ppo ppo dqn sac tqc sac tqc ms ms ms ms ms ms ms ms trivial killableweakstrongsensitivity trivial killableweakstrongsensitivity trivial killableweakstrongsensitivity trivial killableweakstrongsensitivity trivial killableweakstrongsensitivity trivial killableweakstrongsensitivity trivial killableweakstrongsensitivity trivial killableweakstrongsensitivitysec .
.
.
.
.
.
.
smr .
.
.
.
.
.
.
.
.
.
sdf .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
sls .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
snr .
.
.
.
.
.
.
nei .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
snu .
.
.
.
.
.
.
.
.
.
spv .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
avg .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
avg new .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rlmut.
.
.
.
.
.
.
.
.
.
.
.
.
urations where each selected test environment configurati on is chosen to maximise the probability of the failure predict or out of candidates generated at random.
rq .
to compare prl with rlmutation we considered all the mutants produced by rlmutation which are publicly available.
then for each killable mutant of rlmutation we executed the test environment configuration s generated by tgwand tgs.
for each original and mutant pair we used rlmutation to compute the killed predicate we then computed the mutation score for tgwandtgs as the ratio between the number of killed mutants and the total number of killable mutants.
c. results rq .
table iii shows the evaluation of prl for all subject environments and rl algorithms.
rows represent the mos while columns show the results of our mutation pipeline for each mo.
for each pair env a we report the percentage of trivial mutant configurations for each mo trivial the percentage of killable configurations killable the mutation scores ms for the tgw weak and tgs strong test generators and the individual sensitivity sensitivity .
for instance in cartpole dqn the sls mutation operator 4th row has of mutant configurations that are trivial of configurations that are killable while t he mutation score for tgw .50and tgs .
hence sensitivity is .
.
we compute the mutation score for a give n operator only if the operator is killable and non trivial.
for instance in cartpole ppo the sec operator is non killable killable .
while in humanoid sac the sdf operator is killable killable .
but all the killableconfigurations are trivial trivial .
hence we do not compute the mutation score for them.
we observe that for cartpole the sensitivity is quite low i.e.
.
for dqn and .
for ppo.
indeed the dqn agent in cartpole is very weak such that even tgwis effective at killing mutant configurations its mutation score is .
on average while the mutation score of tgsis .
.
on the other hand the ppo agent on cartpole is very hard to kill for training environment configurations on average th e percentage of killable mutant configurations is .
and for killable configurations tgshas only a slight edge w.r.t.
tgw.
however for the remaining subject environments which are more complex than cartpole i.e.
these environments are harder to learn for an rl agent the average sensitivity ran ges from a minimum of .
in humanoid tqc to a maximum of .
in humanoid sac .
rq overall the mutation operators of prl are effective at discriminating strong from weak test gener ators especially in complex environments where the minimum sensitivity is .
and the maximum is .
.
rq .
the last row of table iii shows the average mutation score and sensitivity of rlmutation on the common pairs of subject environments and rl algorithms.
we observe that in all cases the mutants created by prl are more sensitive than the mutants of rlmutation whose maximum sensitivity is .
for lunarlander ppo .
rq in all subject environments and rl algorithms prl create mutants that are more sensitive than rlmutation s. 10rq .
in table iii we underline the mutation operators coming from the new fault types that are not present in existing rl fault taxonomies.
the avg new row shows the average metric values considering only the underlined mutation operators.
for instance for the pa ir cartpole dqn the average sensitivity across all mutation operators is .
while the average sensitivity only consi dering the new fault types is .
.
overall the mutation operat ors associated to new fault types have higher sensitivity than t he total average in cases the same sensitivity in cases in o ne case sensitivity is not computable for the new operators .
rq mutation operators associated to new fault types contribute substantially to increase the sensitivity of prl.
vi.
t hreats to validity internal validity.
an internal threat to the study s validity might come from the labeling of the artifacts.
we addressed this threat by having at least two labelers independently la bel each post.
we also fixed the disagreements within labelers and used krippendorff s alpha to quantify the disagreement s. furthermore we initially conducted pilot studies to refine the labeling process.
an additional internal validity thre at concerns the subjective bias while constructing the taxono my tree from the generated labels.
this was alleviated by all th e authors providing feedback on the final tree.
external validity.
an external validity threat is related to the generalizability of the bugs found on stack exchange and github.
while the sources of the bugs might be limited we cross referenced top conferences and highly cited works to ensure that such issues have been studied in the literature.
the selection of conferences to identify a popular rl framework also poses an external validity threat.
while we considered the top tier ml conference i.e.
icml considering other top ml conferences could have given us a better picture of popular rl frameworks in the ml community.
generalization might also be affeced by our choice of mining github only considering stablebaselines3.
although stablebaselines3 is the rl framework used by the majority of the papers in the selection of conferences we considered by not investigating other rl frameworks we might lose out on a variety of important faults .
lastly an additional external validity threat is related t o the limited number of subject environments we considered in the evaluation.
we selected cartpole andlunarlander to enable comparisons with previous work.
additionally we consider ed parking andhumanoid which are also heavily used in drl research.
overall this selection of environments suppor ting both discrete and continuous action spaces allowed us to ap ply four foundational drl algorithms namely dqn ppo sac and tqc.
conclusion validity.
conclusion validity threats are related to how random variations in the experiments are handled and the inappropriate use of statistical tests.
since rl algori thms are notoriously sensitive to the random seed we train original and mutated agents multiple times i.e.
n and we used rigorous statistical tests i.e.
the fisher s t est to decide whether the mutated agent is killed.
recently agarwal et al.
proposed bootstrap sampling to overcome the uncertainty given by a few run rl training regime.
we acknowledge that our experimental setting may benefit from bootstrap sampling and by using the rliable library we re computed the killability predicate on all .
kmutant instances using bootstrap sampling.
in particular we comp uted the probability of improvement and estimated the confidenc e intervals using ksamples.
we found that the killed predicate computed using bootstrap sampling agrees with the killed predicate based on fisher s test of the time.
moreover when in disagreement of the times the mutant instance is killed by the fisher s test indicating a higher statisti cal power than bootstrap sampling.
hence these results sugges t that bootstrap sampling would bring minimal benefit to our experimental setting although it can be used as an alternat ive to the fisher s test for the killed predicate.
vii.
c onclusions and future work we present a taxonomy of real rl faults.
using this taxonomy we extracted mutation operators and implemented them in our tool prl.
we evaluated its effectiveness in discriminating strong and weak test generators on a diverse set of environments using popular rl algorithms.
our tool also achieves higher sensitivity compared to the prior work rlmutation with a significant contribution from the operat ors that are derived from new taxonomy branches.
viii.
d ata availability our taxonomy labeling results are available on our replication package .
we also share the code of prl .
ix.
a cknowledgments we are grateful for the help provided by breno dantas cruz.
we also acknowledge the icse reviewers for their valuabl e feedback.
this work relied on the grants provided by the national science foundation ccf cns cns ccf ccf and nrt .
this work used explore access at texas high performance research computing through allocation cis240181 from the advanced cyberinfrastructure coordination ecosy stem services support access program which is supported by national science foundation grants and .
matteo biagiola was partially supported by the h2020 project precrime funded under the erc advanced grant program erc grant agreement n. .