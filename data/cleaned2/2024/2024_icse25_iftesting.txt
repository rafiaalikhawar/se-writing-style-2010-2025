dissecting global search a simple yet effective method to boost individual discrimination testing and repair lili quan tianlin li xiaofei xie zhenpeng chen sen chen lingxiao jiang xiaohong li tianjin university tianjin china nanyang technological university singapore singapore management university singapore nankai university tianjin china quanlili tju.edu.cn tianlin001 e.ntu.edu.sg xfxie smu.edu.sg zhenpeng.chen ntu.edu.sg tigersenchen .com lxjiang smu.edu.sg xiaohongli tju.edu.cn abstract deep learning dl has achieved significant success in socially critical decision making applications but often exhibits unfair behaviors raising social concerns.
among these unfair behaviors individual discrimination examining inequalities between instance pairs with identical profiles differing only in sensitive attributes such as gender race and age is extremely socially impactful.
existing methods have made significant and commendable efforts in testing individual discrimination before deployment.
however their efficiency and effectiveness remain limited particularly when evaluating relatively fairer models.
it remains unclear which phase of the existing testing framework global or local is the primary bottleneck limiting performance.
facing the above issues we first identify that enhancing the global phase consistently improves overall testing effectiveness compared to enhancing the local phase.
this motivates us to propose genetic random fairness testing grft an effective and efficient method.
in the global phase we use a genetic algorithm to guide the search for more global discriminatory instances.
in the local phase we apply a light random search to explore the neighbors of these instances avoiding time consuming computations.
additionally based on the fitness score we also propose a straightforward yet effective repair approach.
for a thorough evaluation we conduct extensive experiments involving testing methods datasets models including naively trained repaired and quantized for on device deployment and sixteen combinations of sensitive attributes showing the superior performance of grft and our repair method.
index terms individual discrimination fairness dnns i. i ntroduction deep learning dl has achieved remarkable success and demonstrated great potential in tackling complex tasks particularly in socially critical decision making applications .
however they often exhibit unfair behaviors in sensitive application areas raising significant social concerns and violating the fairness requirements of software .
for example studies show that pedestrian detectors are significantly biased at night with more females going undetected than males causing severe security concerns perpetuating gender discrimination and exacerbating social inequalities.
therefore it is crucial to enhance fairness throughout the this work was done during lili quan s visit to singapore management university.
tianlin li and xiaohong li are the corresponding authors.entire development and deployment process of dl systems particularly in socially critical scenarios.
specifically existing studies on deep neural network fairness can be primarily divided into two categories individual discrimination and group fairness .
individual discrimination requires similar decisions for similar individuals while group fairness requires equal treatment for different groups based on a protected attribute.
individual discrimination is capable of identifying discriminatory behaviors that may be ignored by group fairness measures.
specifically individual discrimination allows for a more powerful and granular examination of discriminatory behavior that arises due to changes in only the sensitive attribute in various contexts.
in contrast group fairness measures may fail to detect discrimination when the model treats the same group oppositely in different situations.
thus in this work we primarily focus on addressing the issue of individual discrimination in deep neural networks dnns .
to thoroughly test individual discrimination in dnns existing methods primarily generate individual discriminatory instances idis in a two phase generation framework.
recognizing that the neighbors of idis are likely to be discriminatory they first identify diverse instances as global seeds the global phase .
then they iteratively search the neighbors of these global seeds to uncover as many as possible idis the local phase .
specifically adf and eidig guide the search direction through gradients in both phases.
neuronfair and dice improve upon previous methods by optimizing gradient guidance using neuron behaviors or information theoretic characterization of discrimination.
expga collects high quality global seeds through interpretable methods and then uses a genetic algorithm to search for more idis in the local phase.
despite the advanced nature of current efforts their efficiency and effectiveness remain limited particularly when evaluating fairer models even with white box access e.g.
neuronfair takes seconds to generate only idis for models improved by faire as detailed in table ii .
to understand which phase is the key bottleneck we conducta preliminary study comparing the impact of enhancing the global versus local phases under the same search budget details in section ii c .
our findings show that enhancing the global phase is significantly more rewarding increasing global idis consistently improves overall testing effectiveness than enhancing the local phase.
motivated by this we mainly focus on improving the global search algorithm to enhance the overall effectiveness and efficiency of fairness testing.
considering this factor we propose genetic random fairness testing grft a straightforward yet effective and efficient fairness testing method.
in the global phase we utilize a genetic algorithm that effectively guides the search process toward generating more global idis from a single seed.
the fitness function is designed based on the model output differences between the original and corresponding mutated cases.
this significant increase in generated global idis allows us to adopt an efficient and less sophisticated search strategy in the local phase while maintaining high effectiveness.
thus in the local phase we perform a light method i.e.
random search to explore the neighbors of the identified global idis avoiding time consuming computations.
additionally our proposed method requires only black box access to the target model.
the fitness score used in grft typically measures the degree of unfairness or bias in a model s predictions.
this insight inspires us to improve model fairness by reducing the model s fitness scores.
specifically we propose a straightforward yet effective repair approach that constructs instance pairs from the original training data and introduces a novel loss function aimed at directly reducing differences in model outputs between these pairs.
this new loss function ensures that while the model continues to perform well on the primary task it also becomes less likely to exhibit biased behavior.
to conduct a thorough investigation we extend the performance study beyond existing methods that primarily evaluate vanilla models i.e.
naively trained models before any repair which can lead to misleading conclusions about the effectiveness of these testing and repair methods.
we comprehensively assess the fairness of models across three dimensions the vanilla model before repair the repaired model and the quantized model which indicates performance when deployed on a device.
the extensive experiments reveal that grft can find more idis more quickly than all baselines across all models and datasets.
for example on average across all datasets compared to the best performing baseline eidig grft takes only .
i.e.
seconds of the time to discover .
times i.e.
more idis in the models repaired by faire.
moreover compared to existing repair methods our repair method significantly improves the fairness performance of the model.
for example grft on average discovers .
idis in the models repaired by our method which is a .
reduction compared to the flipping based retrained models.
contributions.
in summary this research makes the following contributions this paper highlights that increasing the number of globalidis improves testing effectiveness more than enhancing the local phase under the same budget.
we propose grft a novel method that uses genetic algorithms in the global phase and random search in the local phase for more efficient and effective idi generation.
we propose a straightforward yet effective repair method by introducing a novel loss function to reduce differences in model outputs between instance pairs.
we conduct extensive experiments involving testing methods models including naively trained repaired and quantized models datasets and combinations of sensitive attributes to demonstrate the efficiency and effectiveness of our method.
we release our testing and repair tool in https sites.google .com view faireness testing grft.
ii.
b ackground p reliminary study a. deep neural networks deep neural networks dnns inspired by the neural networks of the human brain are renowned for their exceptional performance .
these networks typically consist of multiple layers of interconnected neurons.
definition a deep neural network dnn fconsists of multiple layers l0 l1 .
.
.
l k lo where l0is the input layer lo is the output layer and l1 .
.
.
l kare hidden layers.
the inputs of each layer are from the outputs of the previous layer.
in this work we mainly focus on the classifier f x y where xis a set of inputs and yis a set of classes.
given an input x x we use fl x to represent the internal features extracted by the layer l i.e.
the neuron output values at l .
b. problem definition individual discrimination there are various metrics for evaluating the fairness of machine learning models .
among them individual fairness asserts that similar inputs differing only in protected attributes should not lead to discriminatory outcomes.
we adopt the definition of individual discrimination from .
let the attribute set of the dataset bea a1 a2 an with p arepresenting protected attributes e.g.
gender race age and np denoting nonprotected attributes.
we define a discriminatory instance as follows.
definition discriminatory instance x a1 a2 an is an arbitrary instance in dataset where airepresents the value of attribute ai.
we define xas a discriminatory instance of a dnn when there is a x a a a nin the instance space that satisfies the following conditions p p s.t.
a p a p q np s.t.
a q a q f x f x repairing individual discrimination the repair could be defined as given a dnn fthat suffers from individual discrimination we aim to repair the dnn fas a fairer dnn f .
given any input x if we change some values of thetable global idis0 local iteration1000500250200eidig015438193322305824834adf017694216232534127525neuronfair013232162231929620984 global idis total number of idis07500150002250030000 number of global idis0 eidig adf neuronfair eidigadfneuronfair 1fig.
result of preliminary study protected attributes as x the classification output should be not changed i.e.
f x f x .
the definition could be f x f x f x f x where f means the new model with the learned parameters .
for the input xandx we expect that the decisions of the dnn on these two inputs rely on the features that are as similar as possible and the original functionality is not affected.
c. preliminary study existing methods typically generate midis during the global phase and then perform nlocal iterations for each idi resulting in a total of m niterations.
we ignore the cost of the global phase here since it is often minimal due to significantly fewer global iterations e.g.
global iterations versus local iterations .
however it remains unclear given a fixed m nbudget how should resources be allocated to maximize testing effectiveness?
to explore it we investigate the following two strategies strategy enhance the global phase by increasing m the number of idis while reducing n the number of local iterations per idi.
strategy enhance the local phase by giving a smaller mand increasing n thus focusing on more iterations per idi.
existing methods predominantly follow strategy adopting a lightweight global phase e.g.
generating only one idi per seed input with iterations and performing a more intensive local phase e.g.
iterations per idi .
however the effectiveness of this approach compared to strategy remains unclear.
to address this gap we conducted a preliminary study to compare these two strategies.
using a fixed budget of m n we evaluated four configurations where each pair represents m n .
as shown in figure our findings reveal that increasing m the number of global idis consistently improves testing effectiveness.
iii.
m ethodology a. testing individual discrimination based on findings in section ii c that enhancing the global phase yields greater performance rewards than enhancing the local phase we designed grft a simple and effective method that combines a more efficient global search algorithm i.e.
genetic algorithm with a lightweight local search algorithm i.e.
random .
dataset global generation mutation crossover selection discrimination check non sensitive sensitive... ... ...datasetcluster seed ... ... ... ... ... ... model ... ... discriminatory samples discriminatory sampleslocal generation local perturbation discrimination checkseed sample selection ... ... ... ... modelfig.
overview of grft algorithm global generation input s a seed sample max iter maximum number of iterations output d idis set const m size of population r1 crossover rate r2 mutate rate 1d 2x initpopulation s m 3iter 4while iter max iter do calculate fitness values for the chromosomes in x forx xdo ifdiscriminationcheck x true then d d x iflen d 0then return d fori do ifisbestfitness x then continue select x1 x2using tournament selection x crossover x1 x2 r1 x mutate x r2 iter 18return d overview figure provides an overview of grft.
to ensure diverse instances we first cluster the original training dataset using algorithms like k means and select seed samples from the clusters.
grft then discovers idis from each seed through two phases global generation and local generation.
the global phase takes the seed samples as input and employs a genetic algorithm ga to efficiently search for more idis through population initialization selection crossover and mutation operations.
based on the globally generated idis the local stage aims to discover idis near them through local perturbation and search iterations.
global generation existing studies often generate idis by guiding the search process using the gradient of the loss function or model output with respect to input x which can be time consuming.
moreover findings in section ii c indicate that improving the global phase leads to greater performance gains.
motivated by the high convergence speed and strong search capabilities of ga we adopt ga in the global phase.
algorithm presents the ga procedure for generating idis with its key components detailed as follows.algorithm tournament selection input fitness fitness values x current population tournament size size of tournament output x1 x2 selected samples for crossover 1tournament randomselect population tournament size 2tournament randomselect population tournament size 3x1 getbestfitnessindvidual fitness tournament 4x2 getbestfitnessindvidual fitness tournament 5return x1 x2 population construction this step constructs the initial population based on a seed sample.
in this paper we focus on tabular datasets.
for the encoding of ga we consider a whole sample instance as a chromosome and each attribute as a gene.
given a sample x we generate a new sample x through random perturbation of non protected attributes chosen from x. since the attributes are all preprocessed as categorical values in tabular samples the perturbation is done by increasing or decreasing the value by sgunit random select from .c is used to constrain the difference between the attribute value ofxandx .
the generated new chromosome will be checked to ensure that each attribute value complies with the constraints of the dataset.
attribute values that exceed the valid range will be clipped.
as the initial step line in algorithm we randomly generate msamples as the initial population.
fitness function intuitively if two instances differ only in protected attributes a larger discrepancy in their model outputs suggests that these attributes significantly influence the model s inference.
this implies that such instances are more prone to becoming discriminatory under slight perturbations.
based on this intuition we design a fitness function fitness x for a sample xas shown in equation .
it quantifies the absolute difference between the model s output probabilities for xandxkusing l1 norm.
xkis derived from xandnis the number of samples that differ from xonly in the protected attributes.
the higher the fitness value the more likely the sample xis to be mutated into idis.
in each search iteration this function is used to calculate the fitness value for each chromosome in the population x line in algorithm .
fitness x pn k f x f xk .
selection this step selects high quality chromosomes to generate the next population based on fitness score.
in this paper we adopt the tournament strategy where the chromosome with the best fitness score in each tournament is selected for crossover line in algorithm .
algorithm details the selection process.
specifically tournament size chromosomes are randomly selected from the top ranked individuals in the current population as a tournament.
then the chromosomes with the highest fitness score are selected from tournament 1andtournament 2respectively.
crossover and mutation the crossover randomly exchanges the attribute values under the correspondence index x1 x20 4crossover a0a1a2a3a4a5a9a6a7a8a10a11 x3 x40 3mutationa0a1a2a3a4a5a9a6a7a8a10a11 1fig.
the example of crossover and mutation line in algorithm .
next the mutator randomly selects three non protected attributes to increase or decrease sgunits same as population construction at a predefined mutation rater2 line in algorithm .
the mutated chromosome is then checked to ensure compliance with dataset constraints and any attribute values exceeding the valid range are clipped.
figure illustrates an example of crossover and mutation.
chromosomes x1andx2 used for crossover are generated based on the dataset census income.
the protected attributes a0 a6 and a7correspond to age race and gender respectively and are shown in the blue boxes.
by randomly crossing the attributes a1 a2 a4 a6 a9 ofx1andx2 a new chromosome x3is obtained.
then the non protected attributes a3 a8 and a10ofx3are mutated to and respectively resulting in the final new chromosome x4.
local generation based on the idis found in the global phase the local phase aims to explore the idis near them.
existing methods that rely on gradient calculations and iterative searching are time consuming.
however our more effective global search algorithm allows us to adopt an efficient and less sophisticated search strategy in the local phase while maintaining high performance.
furthermore neighbors of idis are often also likely to be discriminatory.
therefore we employ a lightweight random search in the local phase.
algorithm shows the process of local generation.
for each discriminatory instance dgenerated in the global phase this phase iteratively searches through max iter random samples line .
in each iterative search one non protected attribute ofdis randomly selected line and either increased or decreased by lines and .
the mutated instance is then checked to determine if it is still discriminatory line .
if it is no longer discriminatory the search continues from the original discriminatory instance d0 line .
b. repairing individual discrimination the fitness score equation used in ga typically quantifies the degree of model unfairness.
higher fitness scores indicate a greater likelihood of discriminatory behavior while lower scores suggest better fairness.
to improve fairness we reduce the model s tendency to produce high fitness scores by adding a loss term.
we use lclsto denote the cross entropy loss item and the retraining loss item could be revised as l lcls x xx xfitness x where is used to modulate the weight of the two items.
this revised loss function integrates traditional classification accuracy with a fairness focused term ensuring strong performance on the primary task while reducing biased behavior.
note that we only need to sample xfrom the original trainingalgorithm local generation input dg idis generated in global phase max iter maximum number of iterations np non protected attributes output dl idis set 1dl 2iter 3ford dgdo while iter max iter do d0 copy d attribute randomselect np ifrandom.uniform .5then d else d ifdiscriminationcheck d true then dl dl d else d d0 iter data x which eliminates the need for a large volume of generated idis.
iv.
e valuation in this section we evaluate the performance of the proposed testing and repair method.
we first outline the experimental setup and then we introduce our evaluation aiming to answer the following research questions rq1 how does grft outperform other baselines across vanilla repaired and quantized models?
rq2 how does grft perform in both the global and local phases?
rq3 how efficient is grft?
rq4 how effective is the proposed repair method?
a. experimental setup datasets and models.
we conduct experiments on five popular datasets including census income bank marketing german credit compas lsac .
these tabular datasets are commonly used in individual fairness testing .
census income.
the dataset created by barry becker from the census database contains instances and attributes.
the original aim is to determine whether a person makes over 50k a year.
among these attributes race gender and age are defined as protected attributes.
bank marketing.
the dataset contains over instances and attributes with age being the only protected attribute.
the original aim of this dataset is to assess whether a bank term deposit product would be subscribed to yes or not no .
german credit.
this dataset provides an assessment of creditworthiness based on personal and financial records.
it contains examples with attributes.
compas .
compas correctional offender management profiling for alternative sanctions is a commercial algorithm used by judges and parole authoritiesto predict recidivism.
a year follow up study showed that the algorithm is biased against black inmates and favors white defendants.
lsac.
the lsac dataset tracks students who entered law school in the fall of through three or more years of law examinations.
national race and gender specific bar passage data are available for analysis and study.
we train six layer fully connected dnn models following prior work .
more details on our website .
testing baselines we select state of the art testing approaches applicable for dnns to compare with grft.
the white box baselines include adf eidig neuronfair and the black box baselines include expga and limi .
the white box method dice is not selected because it uses the same gradient guided strategy as adf with shannon entropy to quantify individual discrimination qid generating multiple non idis in the global phase high qid value .
this greatly increases time costs in the local phase taking over four times longer for only a slight increase in idis.
the results are available on our website .
repairing baselines following faire we select the following repairing baselines for comparison.
idis retraining .
adf and eidig incorporate their generated idis into the training data to retrain the model and improve fairness.
as shown in eidig it outperforms adf in retraining performance thus we primarily use eidig to generate retraining data.
following eidig all idis across protected attribute combinations are added to the training data forming the baseline mdis.
flipping based retraining .
a basic method for augmenting the training data involves flipping the protected attributes of each instance e.g.
generating a new instance by changing the gender from woman to man .
during the learning process the model learns to be insensitive to the protected attributes because the ground truth remains unchanged while only the protected attributes vary.
we denote this baseline as mflip.
multitask learning .
a common approach to mitigate protected features uses a multi task setting training a protected attribute classifier alongside the original task.
the loss function minimizes the original crossentropy loss l1 while maximizing the classifier s crossentropy loss l2 .
the final loss is l 1l1 2l2.
we denote this method as mmt.
faire .
this method identifies protected neurons and non protected neurons.
a condition layer is added to penalize protected neurons and promote non protected neurons at each layer.
the model is then fine tuned using the original data freezing hidden layers weights and training only the condition layers denoted as mfaire .
care .
this method localizes faulty neurons through causality analysis and optimizes their parameters to mitigate misbehavior denoted as mcare .
ruler .
this method balances accuracy and fairness by employing a two phase training procedure with an iterative adversarial approach denoted as mruler .
evaluation metrics we design the following metrics for our evaluation discriminatory instance number .
this metric tracks the number of idis identified serving as both a testing and repair metric.
for testing a higher count indicatesthorough detection of fairness issues.
for repair the goal is to reduce this number reflecting improved model fairness.
time .
this metric measures the total time needed for fairness testing to identify idis.
it covers both global and local phases.
a shorter time indicates higher efficiency which is important for practical applications.
accuracy .
the accuracy of the repaired model is an important indicator for measuring its utility.
the objective of the repair method is to improve the individual fairness of the given models without sacrificing too much accuracy.
parameter setting recall that grft involves important parameters including mandcfor population construction tournament size for sample selection r1for the crossover and r2for the mutation.
we configure m c tournament size r1 r2to .
.
according to our experimental experience for the tabular dataset.
the parameter is set to for the repair method.
additionally before the global generation phase we employ the k means algorithm to cluster the training set.
across all experiments following adf and eidig we set the number of clusters to and select samples as seed and set the maximum number of iterations for the global phase and local phase to and respectively.
additionally we use the best settings reported in the papers for all testing and repair baselines.
we take an average of multiple runs for all experiments to ensure robust results.
under most parameter settings the pvalues .
.
it shows our results are statistically significant.
b. rq1 performance of grft across vanilla repaired quantized models vanilla models the number of idis found by each fairness testing method on vanilla models are shown in table i. we can see that all the testing methods yield different results across various datasets and attribute combinations.
adf and eidig detect the most idis on the bank dataset while expga and grft detect the most on the lsac dataset.
however grft consistently outperforms all other methods in detecting more idis across every dataset and attribute combination.
its superior performance is particularly notable in complex attribute combinations and larger datasets such as the lsac dataset and the attribute g r. furthermore our black box testing method grft detects an average of idis across all datasets and attribute combinations significantly higher than the next best method eidig which identifies .
these results demonstrate grft s robustness and effectiveness in identifying idis.
repaired models table ii compares testing methods across repaired models showing the average number of idis identified across all protected attributes.
a indicates no idis were found.
more details are available on our website .
we observe that on most repaired models gradient based white box methods like adf eidig and neuronfair outperform black box methods such as expga and limi.
however onmflip limi detects more idis than adf and eidig.
this may be because models closer to fairness have smoothertable i number of idis found by each fairness testing method on vanilla models dataset attr adf eidig neuronfair expga limi grft censusa g r a g a r r g avg bank a credita g a g avg compasg r g r avg lsacg r g r avg total gradients making them less effective for guiding idi searches.
notably all methods failed to find idis on mmtfor the credit and compas datasets due to the lower model accuracy which produced identical outputs for all inputs.
limi could not find idis on mfaire because the surrogate decision boundary could not be generated due to the instability of gan models.
compared to existing methods grft identifies the most idis across all repaired models with a significant margin over the next best method.
for example in mdis grft finds idis for the lsac dataset a considerable improvement over the best gradient based method adf which finds idis.
similarly in mflip grft identifies .
idis significantly surpassing other methods.
these results underscore grft s robustness in handling diverse fairnessenhanced models and highlight its potential as a powerful tool for fairness testing in machine learning.
quantized models to assess the impact of quantization on model fairness we apply quantization and testing to the repaired models.
for each model we consider three quantization levels i.e.
and .
we then use all fairness testing methods to detect idis.
the results show that grft outperforms other baselines across all datasets and quantized models.
table iii shows the number of idis found by grft on the best performing quantized models i.e.
quantized mflip where indicates the model that is not quantized.
more detailed results are shown in our website .
from the table it is evident that quantization impacts the number of idis detected.
specifically as the quantization level increases the number of idis generally increases across most datasets and attributes.
for instance in the census dataset the attribute combination of age and race a g shows a steady increase in idis from at quantization to at .
similarly in the credit dataset the attribute combination of gender and age a g shows an increase fromtable ii number of idis found by fairness testing methods on repaired models datasetmdis mmt adf eidig neuronfair expga limi grft adf eidig neuronfair expga limi grft census bank credit compas lsac avg .
.
.
.
.
.
.
.
.
.
datasetmflip mfaire adf eidig neuronfair expga limi grft adf eidig neuronfair expga limi grft census bank credit compas lsac avg .
.
.
.
.
.
.
.
.
.
.
datasetmcare mruler adf eidig neuronfair expga limi grft adf eidig neuronfair expga limi grft census bank credit compas lsac avg .
.
.
.
.
.
.
.
table iii number of idis found by grft on quantized mflip dataset attr censusa g r a g a r r g bank a credita g a g compasg r g r lsacg r g r at quantization to at .
this pattern suggests that quantization generally exacerbates the fairness issues in the models leading to an increase in discriminatory instances detected.
the effect is particularly pronounced in datasets like credit where the number of idis increases significantly with higher quantization levels.
answer to rq1 grft consistently outperforms all testing baselines in detecting more idis across vanilla repaired and quantized models.
moreover quantization seems to harm model fairness as indicated by the rise in idis with higher quantization levels.c.
rq2 performance on global phase and local phase to understand the reasons for grft s effectiveness we conducted a detailed analysis of its performance in the global and local phases.
global phase table iv presents the number of idis detected by each testing method during the global phase across vanilla repaired and quantized models.
due to space limitations and the similar results observed on the quantized and repaired models we only present results for the top testing baselines and the best performing quantized mflip.
limi is excluded as it only contains one random exploration phase.
a in the table indicates that no idis are found.
we can see that in the global phase grft significantly outperforms other state of the art methods across vanilla repaired and quantized models.
specifically for initial seeds and the mflip ga based grft detects an average of idis per dataset significantly surpassing adf i.e.
eidig i.e.
neuronfair i.e.
and expga i.e.
.
grft can detect more than idis which exceeds the number of seeds.
this is because in each iteration the population size is set to .
therefore for each seed more than one discriminatory instance can be detected.
in contrast existing gradient based methods generate at most one discriminatory instance per seed.
for the census credit compas and lsac datasets expga produces the fewest idis as it does not search for idis but instead uses an interpretability model to generate high quality samples.
these results demonstrate that grft significantly outperforms other testing methods in the global phase primarily due to the superior search capabilities and fast convergence of the ga.table iv number of idis found by each fairness testing method in the global phase datasetmvan mdis mmt mflip adf eidig neuronfair grft adf eidig neuronfair grft adf eidig neuronfair grft adf eidig neuronfair grft census bank credit compas lsac avg mfaire mcare mruler quantized mflip datasetadf eidig neuronfair grft adf eidig neuronfair grft adf eidig neuronfair grft adf eidig neuronfair grft census bank credit compas lsac avg local phase despite using only a random search strategy without directional guidance grft detects a significant number of idis in the local phase.
for vanilla models grft detects i.e.
instances on average accounting for .
of the total instances detected while formflip it detects i.e.
instances comprising .
.
in comparison adf eidig and neuronfair detect and instances in the local phase contributing .
.
and .
of their respective totals.
notably expga generates fewer idis as its global phase provides too few high quality seeds highlighting the effectiveness of grft in leveraging even a simple random search for local phase detection.
ablation analysis to evaluate the contributions of the global and local phases we replace the search algorithms in grft with those from the two best performing baselines eidig and neuronfair forming variants shown in the first row of table v. specifically grft neuronfair represents the variant combining grft s global search with neuronfair s local search while eidig grft represents the variant combining eidig s global search with grft s local search.
for the evaluation samples from the census dataset were randomly selected as seeds with and iterations set for the global and local phases respectively.
each experiment was repeated three times and the average results are reported in table v where idis denotes the number of idis found and idis persecond measures efficiency representing the average number of idis generated per second.
the metric idis persecond was introduced to ensure a fair comparison as different methods require varying amounts of time to complete the same number of iterations.
when replacing random in grft with the local search algorithm in neuronfair and eidig more idis were generated after iterations vs. vs. .
however the time required increased significantly .79s vs. .92s vs. .82s making them and times slower.
conversely when replacing neuronfair and eidig with simple random they became much faster .43s and .58s but produced fewer idis.
these results highlight that enhancing the global phase while using lightweight search in the localtable v performance of variant methods ga random grft grft neuronfairgrft eidigeidig grftneuronfair grft idis time seconds .
.
.
.
.
idis persecond .
.
.
.
.
table 1adfeidigneuronfairexpgalimigrftmvan40572.
.
.
.
.4m dis43388.
.
.
.
.159m mt53435.
.
.
.
.
.6m flip11815.
.
.
.
.
.2m faire13160.
.
.
.
.284m care152446150.
.
.
.
.5m ruler18402.
.
.
.
.
.9quantized11815.
.
.
.
.
.
.
.
.
.
.
.844time seconds model typemvanmdismmtmflipmfairemcaremrulerquantized mflip .
.
.
.
.
.
.
.
.
.
.
.
.5adfeidigneuronfairexpgalimigrftvalue axis10100100010000100000 mvanm dism mtm flipm fairem care adfeidigneuronfairexpgalimigrft fig.
comparison of total time required by fairness testing methods across vanilla repaired and quantized models phase leads to higher idis especially when the same time budget is applied.
answer to rq2 this significant increase in generated global idis allows us to adopt an efficient and less sophisticated random search strategy in the local phase while maintaining high effectiveness.
d. rq3 the efficiency of grft to evaluate the efficiency of grft we calculated the total time required for each testing method to complete the search for each model and initial seeds.
figure shows the average result on vanilla repaired and quantized models.
due to space limitations and the similar results observed on the quantized and repaired models we only present results for the best performing quantized mflip.
more detailed results can be found on our website .
we can see that for all models grft completes the search in under seconds while adf eidig and neuronfair take over seconds at least times longer than grft .
even the relatively faster limi requires at least seconds.
it is worth noting that the times required by baselines are higher than reported in their papers because we measure the total time to generate allidis from seeds global and local iterations whereas the papers report the time for generating only idis.
compared to the vanilla models the testing methods generally complete the search faster on repaired models as fewer idis are detected in the global phase in fairer models limiting the number of searches in the local phase.
these results indicate that grft completes the search process faster than all baselines at least times faster than gan based limi and even times faster than gradient guided methods.
additionally table i shows that grft identifies more idis across most datasets and models compared to all baselines.
these results indicate that grft can discover more idis in less time.
for instance on vanilla models grft identifies idis in an average of .
seconds while the next best method eidig takes .
seconds to find instances.
similarly grft takes only seconds to discover .
times more idis i.e.
than eidig does in the models repaired by faire.
we believe the primary reason for grft s high performance is the combination of the ga and the random iterative search algorithm.
first the ga significantly enhances the effectiveness of the global phase allowing us to adopt an efficient and less complex search strategy in the local phase while maintaining high effectiveness.
second the use of an unguided random iterative search in the local phase reduces time costs by avoiding the complex and time consuming gradient computations required by existing methods and by processing all seeds in batches.
in contrast other baselines require mutations based on the gradient of input pairs making batch processing a challenge.
answer to rq3 grft significantly reduces the time required to identify idis because they do not require gradient calculations demonstrating superior efficiency over other methods.
e. rq4 performance of our repair method as shown in table ii a considerable number of idis can still be detected in mdis mmt and mfaire .
particularly more idis can be detected in mmtthan in the vanilla models.
for example adf detects about idis in mmt which is .
times the number found in the vanilla models i.e.
.
this may stem from the instability of the multitask learning paradigm in the repair method.
while it improves fairness on the original test set it may fail to enhance robustness and can even reduce it if misdirected.
as observed in faire the adversarial branch often converges to fixed outputs during training failing to counter discriminatory patterns effectively.
notably for mmt no idis are detected on the credit and compas datasets due to the lower accuracy resulting in the same output for all inputs.
as a comparison the number of detected idis in models repaired by our method is shown in table vi.
we can see that our repair method is highly effective in reducing the number of idis across all datasets.
particularly in the census bank compas and lsac datasets adf eidig andneuronfair detected fewer than idis.
expga was unable to find discriminatory inputs in the bank and credit datasets.
while grft identifies a relatively higher number of idis in our repaired models due to its rigorous nature the overall reduction in bias achieved by our repair method surpasses that of existing repair methods.
for example grft on average discovers .
idis in the models repaired by our method which is a .
reduction compared to the flipping based retrained models.
these findings clearly demonstrate that our repair method is effective in mitigating bias and improving the fairness of deep learning models making it a valuable tool in the development of equitable ai systems.
table vi total discriminatory instance number found in our repair models dataset adf eidig neuronfair expga limi grft census bank credit compas lsac avg .
.
.
.
.
table vii accuracy of vanilla and repaired models dataset mvan mdis mmt mflip mfaire mcare mruler ours census .
.
.
.
.
.
.
.
bank .
.
.
.
.
.
.
.
credit .
.
.
.
.
.
.
.
compas .
.
.
.
.
.
.
.
lsac .
.
.
.
.
.
.
.
avg .
.
.
.
.
.
.
.
in addition the accuracy of vanilla and repaired models are shown in table vii.
from the table we can observe that compared to the vanilla model most repaired models showed a decrease in accuracy.
specifically the mdis mcare and our repaired models maintain relatively high accuracy with an average accuracy exceeding .
across all datasets which represents a decrease of less than compared to the vanilla models.
however mmtachieves an average accuracy of .
across all datasets representing a decrease of approximately compared to the vanilla models.
specifically on the compas dataset the accuracy of mmtdrops to .
a reduction compared to the vanilla models significantly impairing the model s performance.
these results indicate that our repair method preserves the model s original accuracy while resulting in fewer new idis.
this demonstrates that our repair method effectively addresses fairness issues in models.
answer to rq4 our repair method not only preserves the original accuracy of the model but also results in fewer new idis.
v. d iscussion a. quality of synthetic data the perturbations of attributes may produce unrealistic inputs leading to false positives and an overestimation ofdiscrimination.
to address this limitation following previous work each input attribute is constrained to its minimum and maximum range in the training dataset.
furthermore we evaluate the perturbation distance measured with cosine distance and distribution distance measured with maximum mean discrepancy between the generated idis and the original data by randomly selecting seeds from the census dataset and generating idis with all methods over rounds repeating each experiment times.
the results show that the average perturbation distance across all methods ranged from .
to .
with grft achieving a value of .
which is closer to the lower bound of the range demonstrating comparable quality to the baselines.
similarly the average distribution distance ranging from .
to .
indicates that all methods preserve the original data distribution.
this is due to the shared mutation strategy that introduces small valid perturbations.
notably we also analyzed how many idis found by grft are new or previously discovered by the baselines as shown in figure .
grft uniquely discovered idis significantly more than the baselines while only instances are commonly identified by adf eidig or neuronfair.
this advantage arises from grft s more efficient global search strategies which allow it to explore a broader input space.
fig.
idis generated by eidig adf neuronfair and grft b. deeper insights into performance our method s high performance is due to two main factors.
first the ga in the global phase generates multiple discriminatory instances from each seed boosting search effectiveness.
second the random iterative search in the local phase avoids the time consuming gradient calculations used by other methods.
this combination allows our method to outperform existing approaches in both speed and effectiveness.
notably combining the ga with a gradient based local search might uncover more discriminatory instances but it would significantly increase the time cost.
c. generalizability grft is a black box testing method that can be easily extended to other models i.e.
cnn and data types e.g.
text and image datasets .
unlike tabular data the attributes of the image and text are difficult to modify directly from the input domain.
therefore we slightly adjust the modificationtable viii the testing performance of each method on the celeba and sst datasets.
dataset celeba sst method adf eidig neuronfair limi grft expga grft idis time seconds time per idi .
.
.
.
.
.
.
of the protected attribute in grft following neuronfair and expga respectively.
for a given image xand classifier f x there are three steps following neuronfair first we build a sensitive attribute classifier fsa x that can distinguish the image s protected attributes e.g.
gender .
next we use the classic fgsm adversarial attack to modify the images protected attributes and flip their predicted results by generating senatt as follows senatt sign fsa x pred x satisfying that fsa x fsa x senatt where is a hyperparameter to determine perturbation size sign is a signum function return or .
finally we leverage grft to generate bias and then determine whether the instance pair x bias x senatt bias satisfy definition .
for a given text sample xcontaining a sensitive word ai p following expga we modify the protected attribute by substituting aiwith a pair of semantically opposite words aiand ai.
for non protected attributes we randomly replace non sensitive words with semantically similar words identified using the word embedding tool glove .
in this study we evaluate a resnet model trained on the image dataset celeba for smile detection and a layer cnn model trained on the text dataset sst with gender as the protected attribute.
for the experimental setup seeds are randomly selected and iterates are set to and in global and local phases respectively.
for the image dataset only the global phase is applied as idis generated in the local phase differ by only a few pixels from the global phase contributing minimally to improving classifier fairness.
during image mutation n pixels are randomly adjusted by .
baselines are evaluated using the optimal settings from neuronfair and expga.
the results in table viii show that grft outperforms all baselines by identifying more idis with higher efficiency.
on the image dataset grft discovers an idi .3x .6x .7x and .63x faster than adf eidig neuronfair and limi respectively.
limi is faster due to its pre trained gan but may be slow in real world use.
on the text dataset grft generates idis in seconds compared to only idis in seconds by expga.
notably other baselines are not included in this comparison as they do not support text datasets.
furthermore we measure the bias perturbation biasusing thel2 norm for generated image data.
grft achieves a significantly lower bias .
compared to the baselines where adf eidig limi and neuronfair have biasvalues of .
.
.
and .
respectively.
this isbecause grft perturbs a smaller subset of pixels in each search step while baselines modify larger pixel regions in a single step.
these results underscore grft s practical advantages in handling high dimensional image and text data with superior efficiency and effectiveness.
d. future work while grft excels in fairness testing identifying the root causes of dnn unfairness and enhancing idi diversity remain open challenges.
clustering based analysis reflects idi diversity but does not directly correlate with the causes of dnn unfairness.
furthermore the reliance on similar mutation techniques across methods limits input diversity highlighting the need for future research on explanation methods and advanced mutation strategies.
e. threats to validity limited model and datasets.
in this paper we follow commonly used settings to primarily evaluate the effectiveness of grft on tabular data.
however as demonstrated in section v c grft can be extended to other domains showcasing its potential and generalizability to image and text data.
vi.
r elated work there is a substantial body of work focusing on testing and improving group fairness where examples are grouped according to a particular sensitive attribute and statistics are calculated across groups.
notable studies include .
however evaluations of group fairness and individual fairness are fundamentally different .
in our paper we focus on individual fairness.
a. fairness testing most existing individual fairness testing work uses a twostep approach that first perturbs random sample instances from the input dataset to find a discriminatory instance and then locally perturbs those instances to further generate biased test cases.
these methods can be divided into two categories black box and white box .
in the black box setting define fairness and discrimination and develop themis a tool to generate efficient test suites for measuring discrimination.
to address themis s inefficiencies caused by the random sampling process propose aequitas which discovers idis through random exploration in the global phase and perturbs instances using three strategies in the local phase to generate more idis.
employs lime to generate local perturbation samples to build a decision tree and then analyzes each tree path to generate test inputs.
similar to expga employs interpretable methods to collect high quality initial seeds in the global phase and then adopt ga to search discriminatory sample candidates in the local phase.
recently proposes limi to generate more natural individual idis with the help of a generative adversarial network gan however training gans is inefficient and their performance is limited by the assumption that they accurately capture the decision boundary which is not always true.in the white box setting adf utilizes the gradient of the loss function to search idis near the decision boundary of dnn.
eidig enhances adf by incorporating momentum into the global phase and improving the local phase with the effective vicinity explorer to boost exploration effectiveness.
neuronfair and dice enhance performance by calculating gradients only for identified biased neurons and quantifying fairness based on protected information used in decision making respectively.
however all methods rely on gradient calculations limiting their efficiency.
though these methods have made progress in fairness testing their extensive gradient calculations iterative searches and additional model training raise efficiency concerns.
moreover they often improve performance by enhancing the local phase focusing on more iterations per global idi.
in contrast our grft focuses on enhancing the global phase with the strong search capabilities of ga to quickly identify many global idis and employs a simple random perturbation strategy in the local phase to reduce time costs.
b. fairness repair a series of research integrate generated idis to retrain models and improve fairness.
however they are inefficient due to the significant time overhead caused by the generation process.
to avoid using generated discriminatory data propose care a causality based technique for repairing neural networks by localizing faults and using pso to adjust the weights of identified neurons.
similarly adds a condition layer after each hidden layer to penalize neurons linked to protected features and promote those tied to non protected features.
this method modifies the model architecture but results in a greater accuracy trade off.
unlike them we improve model fairness by introducing a novel loss function term while ensuring accuracy on the original task.
vii.
c onclusion this paper investigates individual discrimination in dnns and introduces grft as an effective testing method along with a simple yet effective repair approach for mitigating discrimination.
extensive experiments involving six testing methods models five datasets and sensitive attribute combinations demonstrate the generalizability of our approach.
further refinement and expansion of these methods can contribute to the development of fairer deep learning models and enhance trust in ai driven decision making.
acknowledgment this work was partly supported by the national key research and development program of china 2023yfb3107100 the national research foundation singapore the cyber security agency under its national cybersecurity r d programme ncrp25 p04 taicen .
any opinions findings and conclusions or recommendations expressed in this material are those of the author s and do not reflect the views of national research foundation singapore and cyber security agency of singapore.