large language models as configuration validators xinyu lian yinfang chen runxiang cheng jie huang parth thakkar minjia zhang tianyin xu university of illinois urbana champaign urbana il usa meta platforms inc. menlo park ca usa lian7 yinfang3 rcheng12 jeffhj minjiaz tyxu illinois.edu parthdt meta.com abstract misconfigurations are major causes of software failures.
existing practices rely on developer written rules or test cases to validate configuration values which are expensive.
machine learning ml for configuration validation is considered a promising direction but has been facing challenges such as the need of large scale field data and system specific models.
recent advances in large language models llms show promise in addressing some of the long lasting limitations of ml based configuration validation.
we present the first analysis on the feasibility and effectiveness of using llms for configuration validation.
we empirically evaluate llms as configuration validators by developing a generic llm based configuration validation framework named ciri.
ciri employs effective prompt engineering with few shot learning based on both valid configuration and misconfiguration data.
ciri checks outputs from llms when producing results addressing hallucination and nondeterminism of llms.
we evaluate ciri s validation effectiveness on eight popular llms using configuration data of ten widely deployed open source systems.
our analysis confirms the potential of using llms for configuration validation explores design space of llmbased validators like ciri and reveals open challenges such as ineffectiveness in detecting certain types of misconfigurations and biases towards popular configuration parameters.
i. i ntroduction modern software systems undertake hundreds to thousands of configuration changes on a daily basis .
for example at meta facebook thousands of configuration file diffs are committed daily outpacing the frequency of code changes .
other systems such as at google and microsoft also frequently deploy configuration changes .
such velocity of configuration changes inevitably leads to misconfigurations.
today misconfigurations are among the dominating causes of production incidents .
to detect misconfigurations today s configuration management systems employ the configuration as code paradigm and enforce continuous configuration validation ranging from static validation to configuration testing and to manual review and approval .
the configuration is first checked by validation code aka validators based on predefined correctness rules in practice validators are written by engineers .
after passing validators configuration changes are then tested with code to check program behavior .
lastly configuration changes commonly in the form of a configuration file diff is reviewed before deployment.
the aforementioned pipeline either relies on manual inspection to spot misconfigurations in the configuration file diffs or requires significant engineering efforts to implement co primary authors.and maintain validators or test cases.
however these efforts are known to be costly and incomprehensive.
for example despite that mature projects all include extensive configuration validators recent work repeatedly shows that existing validators are insufficient.
the reasons are twofold.
first with large scale systems exposing hundreds to thousands of configuration parameters implementing validators for every parameter becomes a significant overhead.
recent studies report that many parameters are uncovered by existing validators even in mature software projects.
second it is non trivial to validate a parameter which could have many different correctness properties such as type range semantic meaning dependencies with other parameters etc.
encoding all of them into validators is laborious and error prone not to mention the maintenance cost .
recently using machine learning ml and natural language processing nlp to detect misconfigurations has been considered as a promising approach to addressing the above challenges.
compared to manually written static validators ml or nlp based approaches are automatic easy to scale to a large number of parameters and applicable to different projects.
several ml nlp based misconfiguration detection techniques were proposed .
the key idea is to first learn correctness rules from field configuration data or from documents and then use the learned rules to detect misconfigurations in new configuration files.
ml nlp based approaches have achieved good success.
for example microsoft adopted peerpressure as a part of microsoft product support services pss toolkits.
it collects configuration data in windows registry from a large number of windows users to learn statistical golden states of system configurations.
however ml nlp based misconfiguration detection is also significantly limited.
first the need for large volumes of systemspecific configuration data makes it hard to apply those techniques outside corporations that collect user configurations e.g.
windows registry or maintain a knowledge base .
for example in cloud systems where the same set of configurations is maintained by a small devops team there is often no enough information for learning .
moreover prior ml nlpbased detection techniques all target specific projects and rely on predefined features templates or models making them hard to generalize.
recent advances on large language models llms such as gpt and codex show promises to address some of the long lasting limitations of traditional ml nlp basedmisconfiguration detection techniques.
specifically llms are trained on massive amounts of public data including configuration data configuration files in software repositories configuration documents knowledge based articles q a websites for resolving configuration issues etc.
hence llms encode extensive knowledge of both common and project specific configuration.
such knowledge can be utilized for configuration validation without the need for manual rule engineering.
furthermore llms show the capability of generalization andreasoning and can potentially understand configuration semantics.
for example they can not only understand that values of a port must be in the range of but also reason that a specific configuration value represents a port e.g.
based on the name and description and thus has to be within the range.
certainly llms have limitations.
they are known for hallucination and non determinism .
additionally llms have limited input context which can pose challenges when encoding extensive contexts like configuration file and related code.
moreover they are reported to be biased to popular content in the training dataset.
fortunately active efforts are made to address these limitations.
in this paper we present the first analysis on the feasibility and effectiveness of using llms such as gpt and claude for configuration validation.
as a first step we empirically evaluate llms in the role of configuration validators without additional fine tuning or code generation.
we focus on basic misconfigurations those violating explicit correctness constraints which are common misconfigurations encountered in the field .
we do not target environment specific misconfigurations or bugs triggered by configuration discussed in vii .
to do so we develop ciri an llm empowered configuration validation framework.
ciri takes a configuration file or a file diff as the input it outputs detected misconfigurations along with the reasons that explain them.
ciri integrates different llms such as gpt claude and codellama.
ciri devises effective prompt engineering with few shot learning based on existing configuration data.
ciri also validates the outputs of llms to generate validation results coping with the hallucination and non determinism of llms.
a key design principle of ciri is separation of policy and mechanism.
ciri can serve as an open framework for experimenting with different models prompt engineering training datasets and validation methods.
we study ciri s validation effectiveness using eight popular llms including remote models gpt gpt .
claude3 opus and claude sonnet and locally housed models codellama 7b 13b 34b and deepseek .
we evaluate ten widely deployed open source systems with diverse types.
our study confirms the potential of using llms for configuration validation e.g.
ciri with claude opus detects out of real world misconfigurations outperforming recent configuration validation techniques.
our study also helps understand the design space of llm based validators like ciri especially in terms of prompt engineering with few shot learning and voting.
we find that using configuration data as shots can enhance validation effectiveness.
specifically few shot learningusing both valid configuration and misconfiguration data achieves the highest effectiveness.
our results also reveal open challenges within the scope of target configurations ciri struggles with certain types of misconfigurations such as dependency violations and version specific misconfigurations.
it is also biased to the popularity of configuration parameters causing both false positives and false negatives.
in summary this paper makes the following contributions a new direction of configuration validation using pre trained large language models llms ciri an llm empowered configuration validation framework and an open platform for configuration research an empirical analysis on the effectiveness of llm based configuration validation and its design space ciri is released at ii.
e xploratory examples we explore using llms to validate configuration out of the box.
we show that vanilla llms can detect misconfigurations.
however they are prone to both false negatives and false positives that require careful handling.
figure presents four examples two of which the llm successfully detects misconfigurations and two of which the llm misses the misconfiguration or reports a false alarm.
these examples were generated using the gpt .
turbo llm .
detecting violation of configuration dependency.
validating dependencies between configuration parameters has been a challenging task in highly configurable systems .
llms can infer relations between entities from text at the level of human experts which allows llms to infer dependencies between parameters in a given configuration file based on their names and descriptions.
figure example presents a case where values of two dependent parameters were changed i.e.
buffer.size and bytes.per.checksum .
after understanding the value relationship dependency between these two parameters the model determines that the change in bytes.per.checksum has violated the enforced dependency and provides the correct reason for the misconfiguration.
detecting violation with domain knowledge.
a state of theart llm is trained on a massive amount of textual data and possesses basic knowledge across a wide range of professional domains.
an llm thus could be capable of understanding the definition of a configuration parameter and reasoning with its semantics.
when the llm encounters a configuration parameter such as ip address permissions and masks it invokes the domain knowledge specific to the properties of those parameters.
figure example presents a case where an http address has been misconfigured to a semantically invalid value.
the model detects the misconfiguration reasons that its value is out of range and further suggests a potential fix.
missed misconfiguration and false alarm.
llms as configuration validators are not without errors.
examples and in figure show two cases where the llm make mistakes.
in example the configuration file has provided a description of the changed parameter hostname.verifier and explicitly name io.file.buffer.size name value value description the size of buffer for use in sequence files during read and write operations.
description name io.bytes.per.checksum name value value description the number of bytes per checksum.
must not be larger than io.file.buffer.size.
description ...q any mistakes in the above configuration file?the value of io.bytes.per.checksumshould be less than or equal to io.file.buffer.size.config.
a name dfs.journalnode.httpaddress name value .
.
.
value description the address and port thejournalnodehttp server listenson.
description ...the port number is too big.
the port number should be less than .example llm catches a dependency violation name hadoop.ssl.hostname.verifier name value strict i8 value description the hostname verifier to providefor httpsurlconnections.
validvalues are default strict strict i6 default and localhostand allow all description ... name io.mapfile.bloom.error.rate name value .
value description the rate of false positives inbloomfilter s used in bloommapfile.as this value decreases the sizeof bloomfilter s increasesexponentially.
this value is theprobability of encountering falsepositives default is .
.
description ...example llm misses an invalid optionexample llm reports a false alarm the value of the property hadoop.security.
crypto.cipher.suiteis wrong.the value of the property io.mapfile.bloom.
error.rateis .
which is too low.example llm catches an invalid port value any mistakes in the above configuration file?any mistakes in the above configuration file?any mistakes in the above configuration file?fig.
examples and show the llm correctly catches and reasons the misconfigurations.
examples and show the llm misses the misconfiguration or reports a valid configuration as erroneous.
listed the valid value options of the parameter.
however the model fails to realize that the parameter is misconfigured to an invalid non existent option strict i8 .
in example the description suggests that the parameter bloom.error.rate ranges from to percentage whereas the actual scale is to fraction .
this inconsistency supposedly confuses the model making it mark .
a valid value as invalid.
both examples show that directly using off the shelf llms as configuration validators would result in false negatives and false positives.
the incorrect validation results can be attributed to hallucination .
a simple explanation is that llms are exposed to potentially contradictory data during training which causes confusion to the model at the inference time.
iii.
c iri anllm empowered configuration validation framework we develop ciri an llm empowered configuration validation framework.
ciri takes a configuration file or a file diff as the input and outputs a list of detected misconfigurations along with the reasons to explain the misconfigurations.
if no misconfiguration is detected ciri outputs an empty list.
ciri supports different llms such as gpt claude codellama and deepseek .
figure gives an overview of ciri.
ciri turns a configuration validation request into a prompt to the llms iii a .
the prompt includes the target configuration file or diff a few examples aka shots to demonstrate the task of configuration validation code snippets automatically extracted from the code base and directive question and metadata.
to generate shots ciri uses its database that contains labeled configuration data including both valid configurations and misconfigurations.
ciri sends the same query to the llms multiple times and aggregates responses into the final validation result iii b .
ciri applies to any software project even if it has no labeled configuration data of that project in its database regardless their file format or complexity.
ciri exhibits transferability using data from one project and applying it to others the ability to transfer configuration related knowledge across projects when using configurations from different projects as shots finding .
ciri s configuration validation effectiveness can also be further 1adding a new llm in ciri takes a few lines to add the query apis.
ciri config.ciriquery llmshot file selectionlabeled misconfiguration shotvalid configuration shot answer...are there any mistakes... name io.bytes.per.checksum name value value q configuration shot fileconfig.a prompt generation config.shots config.to validateresult generation validation v otingresponse configuration database userresult automatic code retrievalcode cachecode retrieval code snippetfig.
system overview of ciri.
improved by generating quality shots finding and code snippets finding .
a. prompt engineering prompt structure ciri generates a prompt that includes four elements the content of input configuration file or file diff the shots as valid configurations or misconfigurations with questions and ground truth responses for few shot learning code snippets automatically extracted from available codebase and a directive question for llm to respond in formatted output.
figure shows an illustrative example of the prompt generated by ciri.
it contains nshots the content of to be validated configuration file and the code snippet enclosed within usage followed by the directive question.
ciri phrases the prompting question as are there any mistakes in the above configuration for version ?
respond in a json format similar to the following ... .
the and are required inputs of ciri because the validity of configuration can change by project and project version .
this prompt format enforces the llm to respond in a unified json format for result aggregation iii b .
however responses from llms sometimes may still deviate from the anticipated format .
in such cases ciri retries a new query to the llm.
few shot learning ciri leverages the llm s ability to learn from examples at inference time aka few shot learning to improve configuration validation effectiveness.
to do so ciri simply inserts shots at the beginning of each prompt.
each shot name yarn.resourcemanager.hostname name value .
.
.
value description address of applications manager interface in the rm.
description ...question any mistakes in the above configuration file for yarn version .
.
?
respond in a json format similar tothe following haserror boolean true if there are errors false if none errparameter list containing properties with errors reason list containing explanations for each error configuration file shot answer haserror true errparameter reason each octet segment of the ip address separated by dots must be in the range of to .
name yarn.webapp.address name value nm.company.com value description nm webapp address.
description usage port conf.get yarn.webapp.address .split usage question .........configuration file shot nto be validated configuration filefig.
an example prompt generated by ciri.
contains a configuration snippet the prompting question and its corresponding ground truth.
figure shows an example where there are nshots.
configuration file shot is the first shot in which the parameter yarn.resourcemanager.hostname is misconfigured.
this shot also contains the prompting question orange box and the ground truth blue box .
shot generation ciri maintains a database of labeled valid configurations and misconfigurations for generating valid configuration shots referred to as validconfig and misconfiguration shots referred to as misconfig .
the database can be easily customized or extended e.g.
new configuration data can be added for projects that do not have built in shot files.
a validconfig shot specifies a set of configuration parameters and their valid values.
a valid value of a parameter can be its default value or other valid values used in practice.
a misconfig shot specifies a set of parameters and their values where only one of the parameter values is invalid.
for a given configuration of a specific project ciri by default generates shots using configuration data of the same project.
if ciri s database does not contain configuration data for the target project ciri will use data from other projects to generate shots.
as shown in finding llms possess transferrable knowledge in configuration across different projects.
ciri supports multiple methods for selecting data to generate shots including randomized selection category based selection and similarity based selection selecting data from configuration with the highest cosine similarity .
we did not observe major differences when using different selection methods.
so ciri uses randomized selection by default.
augmenting with code recent work shows that retrievalaugmented generation rag can enhance llms by incorporating additional information .
in the context of configuration each configuration parameter has correspondingprogram context such as data type semantics and usage.
in figure the code snippet enclosed within usage is automatically extracted from the source code which uncovers semantics that the parameter value is expected to include a symbol with the split segment representing a port.
ciri uses a simple but effective code retrieval strategy to retrieve the most effective code snippet ciri employs the following strategy searching codebase with parameter names and retrieving relevant snippets prioritizing code over comments and documents selecting the longest snippet if multiple options are available as longer snippets tend to be more comprehensive with details and deduplicating retrieved snippets.
the retrieved code snippet is cached for efficiency.
the retrieval performs text search and analysis on local source code files which typically takes less than a few seconds.
the code retrieval strategy is effective in improving the effectiveness of configuration validation finding .
addressing token limits llms limit input size per query by the number of input tokens.
for example the token limits for gpt .
turbo are .
to navigate these constraints ciri compresses the prompt if its size exceeds the limit.
ciri first tries to put the target configuration and the directive question in the prompt then maximizes three misconfig shots with one validconfig shot finding to fit into the remaining space.
if the configuration cannot fit into the token limit ciri transforms it into a more compact format e.g.
transforming an xml file into ini format.
if the compressed input still cannot fit ciri aborts and returns errors.
in practice configuration files and diffs are small and can easily fit existing limits.
for example prior study inspects configuration files collected from docker where each file contains to parameters with eight on average .
for very large configurations ciri can split them into multiple snippets and validate them separately.
b. result generation the json response from llms contains three primary fields haserror a boolean value indicating whether misconfigurations are detected errparameter a list of misconfigured parameters and reason a list of explanations of the detected misconfiguration corresponding to errparameter .
the llm s ability of explaining the reasoning is crucial to its usability and is a key advantage of llm empowered detection over traditional approaches.
validation against hallucination we employ a few rules to address the hallucination of llms.
for example if haserror is false both errparameter andreason must be empty.
similarly ifhaserror returns true errparameter andreason must be nonempty with the same size.
the answer to errparameter must not contain repeated values.
if a response fails these rules ciri retries until the llm returns a valid response.
voting against inconsistency llms can produce inconsistent outputs in conversation explanation and knowledge extraction .
to mitigate inconsistency ciri uses a multi query strategy querying the llm multiple times using the same prompt and aggregating responses towards a result that is both representative of the model s understanding and 4more consistent than a single query.
ciri uses a frequency based voting strategy the output that recurs most often among the responses is selected as the final output .
in our evaluation of responses were rejected by the valdiation.
note that the reason field is not considered during voting due to the diverse nature of the response.
after voting ciri collects reasons from all responses associated with the selected errparameter .
the reason field is important as it provides users with insights into the misconfiguration which is different from the traditional ml approaches that only provide a binary answer with a confidence score.
ciri clusters the reasons based on tf idf similarity and picks a reason from the dominant cluster.
we find that this mechanism is robust to hallucination hallucinated reasons were often filtered out as they tended to be very different from each other.
c. ciri configuration ciri is customizable with a key principle of separating policy and mechanism.
users can customize ciri via its own configurations.
table i shows several important ciri configurations and default values.
the default values are chosen by pilot studies using a subset of our dataset iv .
table i configuration of ciri and its default values.
parameter description default value model backend llm.
also allows users to add other llms.
gpt temperature tradeoff between creativity and determinism.
.
shots the number of shots included in a prompt.
dynamic queries the number of queries with the same prompt.
iv.
b enchmarks and metrics our study evaluates ten mature and widely deployed opensource projects alluxio django etcd hbase hadoop common hdfs postgresql redis yarn zookeeper which are implemented in a variety of programming languages java python go and c .
they also use different configuration formats xml and ini with a large number of configuration parameters.
table ii lists the version sha and the number of parameters at that version.
we evaluate ciri on the aforementioned projects with eight llms gpt turbo gpt .
turbo claude opus claude3 sonnet codellama 7b 13b 34b and deepseek .7b which differ in model sizes and capabilities.
all of these models have also been trained with a large amount of code data where prior work has demonstrated their promising capability in handling a number of software engineering tasks.
a. configuration dataset our study uses two types of datasets real world misconfiguration datasets and synthesized misconfiguration datasets.
real world misconfiguration to our knowledge the ctest dataset is the only public dataset of real world misconfigurations it is used by prior work .
the dataset contains real world configuration induced failures of five open source projects among which are misconfigurations and are bugs.
we discuss the results of ciri on real world misconfigurations in finding .table ii evaluated projects and the configuration datasets validconfig and misconfig for shot pool and evaluation.
projectversion paramsvalidconfig misconfig sha shot eval shot eval alluxio 76569bc django 67d0c46 etcd 946a5a6 hbase 0fc18a9 hcommon aa96f18 hdfs aa96f18 postgresql 29be998 redis d375595 yarn aa96f18 zookeeper e3704b3 synthesized misconfiguration since real world configuration dataset iv a is too small to systematically evaluate configuration validation effectiveness we create new synthetic datasets for each evaluated project.
first we collect default configuration values from the default configuration file of each project and real world configuration files from the ctest dataset collected from docker images for those projects included in ctest.
we then generate misconfigurations of different types.
the generation rules are from prior studies on misconfigurations which violates the constraints of configuration parameters table iii .
notably prior studies show that the generation rules can cover .
of parameters across four projects .
for each project we build two distinct configuration sets.
first we build a configuration dataset with no misconfiguration denoted as validconfig to measure true negatives and false positives table iv .
we also build a configuration dataset denoted as misconfig in which each configuration file has one misconfiguration to measure true positives and false negatives table iv .
note that a misconfiguration can be a dependency violation between multiple parameter values.
table ii shows the size for validconfig and misconfig datasets for each project.
to create the misconfig data for each project we first check if its configuration parameters fit any subcategory in table iii and if so we apply rules from all matched subcategories to generate misconfigurations for that parameter.
for example an ip address parameter fits both syntax ip address and range ip address .
we do so for all parameters in the project.
then we randomly sample at most five parameters in each subcategory that has matched parameters and generate invalid value s per sampled parameter.
for each subcategory we further randomly select one parameter from the five sampled ones.
we use the selected parameter to create a faulty configuration as a misconfig shot iii for that subcategory and add it to the project s shot pool.
for the other four parameters we use them to create four faulty configurations for that subcategory and use them for evaluation.
for django certain subcategories have fewer than five matched parameters resulting in the ratio of eval to shot less than .
we separate the evaluation set and shot pool to follow the practice that the training set does not overlap with the testing set .
we create the validconfig dataset for each project using the 5table iii misconfiguration generation we use generation rules from prior work which reflects real world misconfiguraions .
subcategory lists rules to generate different misconfiguraions for the same configuration parameter.
category subcategory specification generation rules syntaxvalue set integer float long... generate a value that does not belong to the value setdata typenumbers with units generate an invalid unit e.g.
nounit path ?
generate a value that violates the pattern e.g.
hello world url .
generate a value that violates the pattern e.g.
file ip address .
generate a value that violates the pattern e.g.
.x0.
.
port data type value set octet generate a value that does not belong to the value set permission data type value set octet generate a value that does not belong to the value set rangebasic numeric valid range constrainted by data type generate values outside the valid range e.g.
integer.max v alue bool options value set true false generate a value that does not belong to the value set enum options value set enum1 enum2 ... generate a value that doesn t belong to set ip address range for each octet generate a value outside the valid range e.g.
.
.
.
port range generate a value outside the valid range permission range generate a value outside the valid range dependencycontrol p1 v p2 generate invalid control condition p1 v value relationship p1 p2 generate invalid value relationship p1 p2 version parameter change v1 pset v2 pset pset 1 pset 2generate a removed parameter in v2or use an added parameter in v1 aforementioned methodology for the misconfig dataset except that we generate valid values.
b. metrics we evaluate ciri s effectiveness at both configuration file andparameter levels at the file level we check if ciri can determine if a configuration file contains misconfigurations at the parameter level we check if ciri can determine if each parameter in the configuration file is valid or not.
table iv describes our confusion matrix.
we compute the precision tp tp fp recall tp tp fn and f1 score at both file and parameter levels.
if not specified we default to macro averaging since each project is regarded equally.
we prioritize parameter level effectiveness for fine grained measurements and discuss parameter level metrics by default in the evaluation.
table iv definitions for confusion matrix.
level metric definition filetp a misconfigured file correctly identified fp a correct file wrongly flagged as misconfigured tn a correct file rightly identified as valid fn a misconfigured file overlooked or deemed correct param.tp a misconfigured parameter correctly identified fp a correct parameter wrongly flagged as misconfigured tn a correct parameter rightly identified as valid fn a misconfigured parameter overlooked or deemed correct v. e valuation and findings we present empirical evaluation results on the effectiveness of llms as configuration validators with ciri v a .
we analyze how validation effectiveness changes with regard to design choices of ciri v b .
we also present our understanding of when ciri produces wrongful results v c and biases v d .
a. effectiveness of configuration validation finding .
ciri shows effectiveness of using state of theart llms as configuration validators.
it achieves file and parameter level f1 scores up to .
and .
respectively.ciri exhibits remarkable capability in configuration validation.
table v shows the f1 score precision and recall for each project using llms with three misconfig and one validconfig shots finding .
the results show that ciri not only can effectively identify configuration files with misconfiguration with an average f1 score of .
across llms but also pinpoint misconfigured parameters with explanations with an average f1 score of .
across llms .
certainly the parameter level f1 scores are about lower than filelevel f1 scores i.e.
pinpointing fine grained misconfigured parameters is a more challenging task for llms compared to classifying the entire file as a whole.
the average f1 score of .
at the parameter level may not seem high.
the reason is that certain types of misconfigurations e.g.
dependency and version violations are hard to be detected by ciri see finding .
on the other hand ciri effectively detects misconfigurations of syntax and range violations with average an f1 score of .
and .
table ix .
this effectiveness is noteworthy given that prior research has identified syntax and range violations as common types of misconfigurations in the field .
therefore we believe that ciri is already useful in practice.
finding .
ciri detects out of real world misconfigurations outperforming recent configuration validation techniques including learning based and configuration testing .
we conduct experiments to evaluate how ciri compares with existing validation techniques on a real world dataset.
for this evaluation we choose the top five llms ranked by f1 score at the parameter level based on the results from table v. the real world dataset contains misconfigurations in total iv among which ciri can detect misconfigurations as shown in table vi.
ciri successfully detected using claude opus.
the six undetected misconfigurations include three due to parameter dependency violations discussed further in v c and the other three are environment related issues that are beyond ciri s current capability.
notably ciri seldom 6table v f1 score precision and recall of ciri evaluated on ten projects with eight llms as configuration validators.
modelsf1 score precision recall file level f.l.
parameter level p.l.
f.l.
p.l.
f.l.
p.l.
al.
dj.
et.
hb.
hc.
hd.
po.
rd.
ya.
zk.
avg al.
dj.
et.
hb.
hc.
hd.
po.
rd.
ya.
zk.
avg avg avg avg avg gpt turbo .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
gpt .
turbo .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
claude opus .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
claude sonnet .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
codellama 34b .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
codellama 13b .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
codellama 7b .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
deepseek .7b .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table vi a comparison of ciri confminer and ctest in detecting real world misconfigurations.
n.r.
not reported.
technique correct incorrect missed runtimedetection detection ciri claude opus .
sec ciri gpt turbo .
sec ciri codellama 34b .
sec ciri gpt turbo .
sec ciri claude sonnet .
sec confminer .
n.r.
n.r.
n.r.
ctest .
n.r.
n.r.
min reports incorrect detection.
note that the real world dataset only contains misconfigurations without valid configuration preventing the calculation of f1 score as defined in methodology due to the absence of negative cases.
we compare ciri s results with a recent learning based validation technique confminer which was evaluated on the same dataset.
confminer utilizes the file content and commit history to identify patterns in configuration to detect misconfigurations.
confminer can detect out of misconfigurations which is less than ciri.
unlike llms that are trained on extensive text data and can comprehend the context of configurations confminer relies on regular expressions to identify patterns.
this approach limits its ability in complex scenarios such as identifying valid values for enumeration parameters and understanding the relationships between different parameters.
we also compare ciri with a recent configuration testing technique namely ctest .
ctest detected of the real world misconfigurations without rewriting test code ciri outperforms ctest by .
.
the reasons are twofold.
first testing relies on adequacy of the test cases.
we find that existing test suites do not always have a high coverage of configuration parameters.
on the other hand llms can validate any parameter.
second llms detected silent misconfigurations that are not manifested via crashes or captured by assertions e.g.
several injected misconfigurations silently fell back to default values and passed the test llms detected them likely because they violated documented specifications .
certainly ctest can detect a broader range of misconfigurations such as the environment related issues that ciri cannot.
we do not intend to replace configuration testing with llms.
instead our work shows that llms can provide much quicker feedback for common types of misconfigurations so tools like ciri can be used in an early phase e.g.
configurationtable vii effectiveness of llms without using shots.
modelsf1 score precision recall f.l.
p.l.
f.l.
p.l.
f.l.
p.l.
gpt turbo .
.
.
.
.
.
.
.
gpt .
turbo .
.
.
.
.
.
.
.
claude opus .
.
.
.
.
.
.
.
claude sonnet .
.
.
.
.
.
.
.
codellama 34b .
.
.
.
.
.
.
.
codellama 13b .
.
.
.
.
.
.
.
codellama 7b .
.
.
.
.
.
.
.
deepseek .7b .
.
.
.
.
.
.
.
authoring before running expensive configuration testing.
as shown in table vi for a configuration file in the real world dataset ctest takes to minutes to finish while ciri only takes to seconds.
in summary our results show that llms like gpt claude and codellama 34b can effectively validate configurations and detect misconfigurations with a sensibly designed framework like ciri.
ciri can provide prompt feedback complementing other techniques like configuration testing.
b. impacts of design choices ciri plays a critical role in llms effectiveness of configuration validation.
we explore its design choices and impacts.
finding .
using configuration data as shots can effectively improve llms effectiveness of configuration validation.
shots including both valid configuration and misconfiguration achieve the highest effectiveness.
using validation examples as shots can effectively improve the effectiveness of llms.
table vii shows the results of llms when the validation query does not include shots.
in particular comparing table vii to table v as indicated by the numbered arrows in table vii the average f1 score of the llms has decreased by .
.
at the file level and decreased by .
.
at the parameter level.
we also study ciri s effectiveness with different shot combinations.
we evaluate six n shot learning settings where nranges from to .
for example to evaluate ciri with a two shot setting three experiments will be performed two validconfig shots one validconfig shot plus one misconfig shot two misconfig shots.
in total we experiment with shot combinations.
due to cost we only run experiments .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.65file level .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.39param level validconfig shot misconfig shotfig.
f1 scores under different shot combinations.
on gpt .
turbo to hcommon.
we find that only using validconfig shots leads to a decrease in precision while only using misconfig shots reduces recall.
clearly text distribution in the query affects llms .
llms can be biased if the shots are all misconfigurations llms will be overly sensitive to the specific patterns in the shots known as overfitting which causes llms miss other types of misconfigurations if the shots are all validconfig llms face challenges in accurately identifying incorrect parameters within the file leading to false alarms.
as shown in figure using both misconfig and validconfig in few shot learning mitigates the biases and achieves the highest effectiveness and including three misconfig shots and one validconfig shot in the prompt achieves the highest f1 score at both the file and parameter levels.
finding .
using configuration data from the same project as shots often leads to high validation f1 score.
however even without access to configuration data from the target project using configuration data from a different project can lead to a improved validation score than zero shot.
in situations where configuration data is unavailable e.g.
due to confidentiality we evaluate whether using configuration data from other systems as shots can improve configuration validation effectiveness on the target system.
table viii shows the results of using data from other projects as shots for configuration validation on hcommon.
by comparing shot hcommon with other columns in table viii we see that using shots from other projects is not as effective as using shots from the target system.
however the average f1 score is still higher than zero shot indicating that using shots from other projects can improve the effectiveness over zero shot.
our observations highlight that ciri with llms can transfer configuration related knowledge across different projects for effective configuration validation compared to traditional approaches.
finding .
ciri s code augmentation approach can help llms to better understand the context of the configuration and improve the validation effectiveness.
we compare the results of gpt .
turbo with and without code augmentation with four shots.
the results show an improvement in f1 scores by .
at both the file and parameter levels.
figure exemplifies code snippets retrieved from thetable viii f1 score on hcommon hc.
using shots from different systems e.g.
hb.
refers to using hbase shots.
s and s means using four shots and no shots respectively.
modelsfile level f.l.
parameter level p.l.
hc.dj.
et.
hb.
avghc.dj.
et.
hb.
avg4 s s s s gpt .
turbo .
.
.
.
.
.
.
.
.
.
.
.
claude sonnet .
.
.
.
.
.
.
.
.
.
.
.
conf.setint tfile.fs.input.buffer.size fsinputbuffersize ex1 conf.setboolean fs.automatic.close false port conf.get yarn.nodemanager.webapp.address .split kerberos .equals conf.get hbase.security.authentication ex2 ex3 ex4 fig.
code snippets retrieved by ciri to aid llms.
codebase by ciri which could improve llms comprehension of the configuration context examples and delineate the parameter types as integer and boolean respectively.
example highlights that the parameter should include a symbol with the latter segment representing a port.
example shows that kerberos is one valid value for the parameter.
finding .
code specialized llms e.g.
codellama exhibit much higher validation scores than generic llms e.g.
llama2.
moreover further scaling up the code specialized llms leads to a continuous increase in validation scores.
in table v we observe a notable trend within the codellama model family the 13b model demonstrates an improvement in f1 score at the parameter level by .
over the 7b model this trend continues with the 34b model which exhibits a further .
enhancement in f1 score over the 13b model.
the observed performance gains can be primarily attributed to the increased capacity for learning and representing complex semantics of configuration values as model size scales.
this involves deep comprehension beyond syntax and range violations which are more common in practice .
we further evaluated the effectiveness of the llama model which is identical to codellama in structure but lacks codespecific training.
the llama 13b is not effective with an average f1 score of .
at the parameter level.
this result underscores the role of code specific training which enhances llm s comprehension of configuration in the context of code.
c. limitations and challenges finding .
with ciri llms excel at detecting misconfigurations of syntax and range violations with an average f1score of .
across subcategories.
however llms are limited in detecting misconfigurations of dependency and version violations with an average f1 score of .
across subcategories.
table ix shows ciri s validation effectiveness per misconfiguration type.
the f1 score on detecting misconfigurations of syntax and range violations is consistently above .
across projects and often reaches .
.
however f1 score rarely exceeds .
on 8table ix parameter level f1 score by misconfiguration types from table iii.
n.a.
means no evaluation samples.
category sub categorygpt turbo claude opus codellama 34b al.
dj.
et.
hb.
hc.
hd.
po.
rd.
ya.
zk.
avg al.
dj.
et.
hb.
hc.
hd.
po.
rd.
ya.
zk.
avg al.
dj.
et.
hb.
hc.
hd.
po.
rd.
ya.
zk.
avg syntaxdata type .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
path .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
url .
.
.
n.a.
.
.
n.a.
n.a.
n.a.
n.a.
.
.
.
.
n.a.
.
.
n.a.
n.a.
n.a.
n.a.
.
.
.
.
n.a.
.
.
n.a.
n.a.
n.a.
n.a.
.
ip address .
n.a.
n.a.
.
.
.
n.a.
.
.
.
.
.
n.a.
n.a.
.
.
.
n.a.
.
.
.
.
.
n.a.
n.a.
.
.
.
n.a.
.
.
.
.
port .
n.a.
n.a.
.
.
.
n.a.
.
n.a.
.
.
.
n.a.
n.a.
.
.
.
n.a.
.
n.a.
.
.
.
n.a.
n.a.
.
.
.
n.a.
.
n.a.
.
.
permission .
n.a.
n.a.
.
.
.
n.a.
n.a.
n.a.
n.a.
.
.
n.a.
n.a.
.
.
.
n.a.
n.a.
n.a.
n.a.
.
.
n.a.
n.a.
.
.
.
n.a.
n.a.
n.a.
n.a.
.
rangebasic numeric .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
bool .
.
.
.
.
.
n.a.
.
.
.
.
.
.
.
.
.
.
n.a.
.
.
.
.
.
.
.
.
.
.
n.a.
.
.
.
.
enum .
n.a.
.
.
.
.
.
.
.
n.a.
.
.
n.a.
.
.
.
.
.
.
.
n.a.
.
.
n.a.
.
.
.
.
.
.
.
n.a.
.
ip address .
n.a.
n.a.
.
.
.
n.a.
.
.
.
.
.
n.a.
n.a.
.
.
.
n.a.
.
.
.
.
.
n.a.
n.a.
.
.
.
n.a.
.
.
.
.
port .
n.a.
n.a.
.
.
.
n.a.
.
n.a.
.
.
.
n.a.
n.a.
.
.
.
n.a.
.
n.a.
.
.
.
n.a.
n.a.
.
.
.
n.a.
.
n.a.
.
.
permission .
n.a.
n.a.
.
.
.
n.a.
n.a.
n.a.
n.a.
.
.
n.a.
n.a.
.
.
.
n.a.
n.a.
n.a.
n.a.
.
.
n.a.
n.a.
.
.
.
n.a.
n.a.
n.a.
n.a.
.
dependencycontrol .
n.a.
.
.
.
.
.
.
.
n.a.
.
.
n.a.
.
.
.
.
.
.
.
n.a.
.
.
n.a.
.
.
.
.
.
.
.
n.a.
.
value relationship .
n.a.
n.a.
.
.
.
.
n.a.
.
n.a.
.
.
n.a.
n.a.
.
.
.
.
n.a.
.
n.a.
.
.
n.a.
n.a.
.
.
.
.
n.a.
.
n.a.
.
version parameter change .
n.a.
.
.
.
.
.
.
.
n.a.
.
.
n.a.
.
.
.
.
.
.
.
n.a.
.
.
n.a.
.
.
.
.
.
.
.
n.a.
.
name hbase.security.authentication name value simple value description controls whether secure authentication is enabled for hbase.
possible values are simple no authentication and kerberos .
description ... name hbase.auth.key.update.interval name value value description the update interval for authentication tokens in milliseconds.
used when hbase security is enabled.
description ... fig.
misconfiguration that violates control dependency that llms cannot detect.
the update interval for authentication is set but the secure authentication is disabled.
misconfigurations of dependency and version violations.
under these two categories llms achieve f1 scores of .
.
for misconfigurations that violate value relationship which is higher than the other two subcategories control and parameter change however it is still much lower than others.
the difference can be attributed to the inherent nature of different types of misconfigurations.
misconfigurations of syntax and range violations are more common in practice from which llms learned extensive knowledge.
in such a case domain specific knowledge from llms is sufficient to detect these misconfigurations.
but misconfiguration of dependency and version violations is often project specific as exemplified in figure .
they are tied to detailed history and features of the project and thus hard to be captured or memorized by llms if the llm is not fine tuned on projectspecific data.
this discrepancy between misconfiguration types exposes existing llm s limitation.
in summary despite our efforts to incorporate examples of configuration dependencies in few shot learning and utilize code augmentation to provide code snippets containing dependency information dependencyrelated misconfigurations still remain a challenge.
finding .
among the detected misconfigurations llms correctly explained reasons for .
of the misconfigurations meanwhile .
of the reasons are misleading.
when an llm detects a misconfiguration ciri also asks the llm to explain the reason.
the quality of the reasoning is crucial to the usability of llm empowered configuration validaalluxio.underfs.gcs.default.mode 888the property alluxio.underfs.gcs.default.mode has the value which is not a valid octal number.config.a1 the property alluxio.underfs.gcs.default.mode has the value which exceeds the range of an integer.a2 fig.
correct and incorrect reasons returned by llms.
tion as it aids debugging and fixing the misconfiguration .
to evaluate these explanations we randomly select one answer in which the misconfiguration is correctly detected per subcategory project llm tuple and collect a total of answers resulting from queries .
upon careful manual review we determined that .
of the reasons given by the llms are clear and explain the misconfigurations.
.
of the answers contain a mix of correct and incorrect reasons across queries.
ciri filters out incorrect reasons using the voting mechanism iii b as correct reasons are dominating.
figure presents an example of mixed reasons with the second reason being an instance of hallucination.
in short with frameworks like ciri llms can effectively validate configurations for syntax or range violations but are less effective for the configurations that involve dependencies between parameters and software versions showing the challenges for llms to reason about interactions between parameters and between configuration and code .
to address those misconfigurations one can re train or fine tune llms with data related to dependency and versions.
d. biases finding .
llms are biased to popular parameters ciri is more effective in detecting misconfigurations of popular parameters but also reports more false alarms on them.
to measure the popularity of a configuration parameter we count the number of exact match search results returned by google when searching the parameter name and call it g hits .
we study the correlation between a parameter s g hits and the effectiveness of llms in detecting the misconfigurations.
for each configuration file in the misconfig dataset we track the frequency of llms detecting the parameter s misconfigurations 9g hits log100 fig.
the g hits distribution of the correctly detected misconfigurations orange and the g hits distribution of the missed misconfigurations blue .
the bars in box plots indicate medians.
cl refers to codellama.
1st 2nd 3rd 4th 5th 6th 7th 8th 1st 2nd 3rd 4th 5th 6th 7th 8thcodellama claude deepseek gpt times the param is identified during evaluationparam of ith highest g hits fig.
frequency of the identified parameter with ithhighest g hits in a configuration file.
with the ithhighest g hits in each file where i ... .
we separate cases when the misconfigured parameter is detected versus missed.
as shown in figure the median g hits of misconfigured parameters being detected is higher than the median g hits of misconfigured parameters being missed.
we also study the frequency of false alarms across different ranking positions of g hits within the file.
specifically for each configuration file in validconfig dataset across all ten projects we evaluated we track the frequency of llms mistakenly identifying the parameter with the ithhighest g hits in each file where i ... .
we group the results by the model family as shown in figure .
the distributions reveal a clear skewness towards parameters with higher g hits indicating that llms are more prone to report false alarms on popular parameters.
the biases can be attributed to the training data of llms which are from public domains easily accessible by search engines like google.
topics or parameters that are popularly discussed are more likely to be memorized by the llms due to more frequent presence in the training data.
so for configuration validation llms can be less effective for parameters that are not commonly referenced online.
vi.
t hreats tovalidity external threats.
external threats come from evaluated projects datasets and llms.
to mitigate threats of evaluated projects we select ten mature widely used projects of different types.
these systems are commonly used in prior studies .
to account for bias inthe evaluated configuration data we include many types of configuration parameters and their generation rules based on prior work .
our results cannot generalize to misconfiguraions of different types such as environment related misconfigurations discussed in vii .
moreover we evaluate ciri with eight state of the art llms to mitigate threats on evaluated models.
we expect the overall trend to be general but the precise numbers may vary with other llms projects and configuration data in the field.
internal threats.
the internal threats lie in potential bugs in the implementation of ciri and experimental scripts for evaluation.
we have rigorously reviewed our code and multiple authors cross validated the experiment results.
construct threats.
the threats to construct validity mainly lie in metrics iv b .
to reduce such threats we use the popular f1 score precision and recall and define our confusion matrices at both configuration file and parameter levels.
vii.
d iscussion and future work detecting multiple misconfigurations in one file.
in principle ciri can find multiple misconfigurations as its design such as prompt and voting is not specific to a single parameter.
for example the prompt asks for a list of erroneous configuration parameters instead of one parameter.
our evaluation focuses on one misconfiguration as it is reported that .
.
of parameter misconfigurations can be attributed to a single parameter .
we further conduct an experiment where we inject multiple errors in the target configuration file ranging from two to four.
the experiment is done on hcommon the project with the largest dataset using claude sonnet.
the results show an f1 score of .
and .
when the number of misconfigurations per file is and respectively.
specifically ciri detected nearly all misconfigurations within a file when the misconfigurations were in the syntax and range categories.
improving effectiveness of llms as validators.
despite the promising results using llms directly as configuration validators like ciri is a starting point to harness the ability of llms for configuration validation.
specifically there are circumstances where llms show limitations and biases v c v d .
one intricate aspect of configuration validation is understanding configuration dependencies.
integrating llms with configuration dependency analysis could be beneficial.
we plan to investigate advanced prompting techniques such as chain of thoughts cot .
for configuration validation cot prompting can potentially mimic the reasoning process of a human expert.
by eliciting llms to generate intermediate reasoning steps toward the validation results it makes the validation more transparent and potentially more accurate.
we also plan to explore extending ciri into a multiagent framework where ciri can interact with additional tools such as ctest and cdep through agent frameworks such as langchain and autogen .
lastly integrating user feedback loops can be valuable.
with user feedback on validation results the iterative procedure can refine llms over time leading to more accurate responses.
10detecting environment related misconfigurations.
while our study primarily targets misconfigurations that are common in the field the validity of configuration files can vary across deployment environments.
for instance a configuration parameter can specify a file path so the file s existence permission and content decide its validity.
to address such configurations llms can be used to generate environmentspecific scripts to run in the target environment.
for example given the configuration file as input the llm can generate python scripts as simple tests as follows.
try with open path to file r asf data json.loads f.read print valid configuration except print invalid configuration such llm generated scripts can help identify issues like misconfigured paths unreachable addresses missing packages or invalid permissions.
notably these scripts offer a lightweight alternative to configuration tests .
detecting source code related misconfigurations.
many misconfigurations are rooted in the interactions between configuration values and the code that consumes them .
we have explored augmenting llms with code snippets finding which can reveal parameter types and semantics.
this approach can be further improved by integrating advanced program analysis to present both configuration and relevant source code to the llm.
techniques like static or dynamic program slicing can help identify the relevant code.
such approach is specifically important for configurations like feature flags where bugs are often not located in changed configurations but manifested through code paths enabled by the new configurations.
detecting domain specific misconfigurations.
we mainly explored llm empowered validation for common types of software configurations see iv .
domain specific configurations such as access control security configuration and network configuration need more domainknowledge for reasoning and rely more on system network states and environments and thus are more challenging to validate.
we believe that llms can be useful components if sufficient inputs and guidance are provided.
fine tuning llms for configuration validation.
we also plan to explore fine tuning to tackle system specific configuration problems which is hard to address with common sense knowledge.
specifically configuration related software evolution is prevalent which introduces new parameters and changes the semantics and constraints of existing parameters .
a promising solution is to fine tune llms on new code and data and make llms evolution aware.
viii.
r elated work prior studies developed frameworks for developers to implement validators and test cases as well as techniques to extract configuration constraints .
however manually writing validators and tests requires extensive engineering efforts and is hard to comprehensively cover various properties of different configurations .
ml nlp based configuration validation techniques have been investigated to reduce the cost.
traditional ml nlp based approaches learn correctness rules from configuration data and documents and then use the learned rules to conduct validation.
however these techniques often face data challenges and rely on predefined learning features and models making them hard to generalize to different projects and deployment scenarios.
we explore using llms for configuration validation which can potentially address the limitations of traditional ml nlp based techniques towards automatic effective configuration validation solutions.
in addition to proactive configuration validation misconfigurations troubleshooting is a related area and has been under active research .
traditional approaches analyze failure symptoms runtime executions and system states to reason about root causes in configuration or program code.
recent work on llm empowered root cause analysis shows promise in leveraging llms to reason about failures based on symptoms and execution traces.
the high level principle may apply to troubleshooting misconfigurations as failure root causes.
a recent work has explored this direction .
ix.
c oncluding remarks as a first step to harvest llms for software configuration we develop ciri as an open platform to experiment with llms as configuration validators and present the important design choices.
through ciri we analyze llm empowered configuration validators.
our analysis shows the potential of using llms for configuration validation ciri demonstrates the effectiveness of state of the art llms as configuration validators for common types of misconfigurations.
despite the encouraging results our study reveals the limitations of directly using llms as configuration validators they are limited to a few types of misconfigurations they can validate and they are ineffective in detecting misconfigurations that violate dependencies and version related misconfigurations meanwhile inducing biases to popular parameters.
we hope that our work shed light on further research of using llms for software configuration research.