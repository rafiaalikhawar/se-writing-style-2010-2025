context conquers parameters outperforming proprietary llm in commit message generation aaron imani university of california irvine irvine usa aaron.imani uci.eduiftekhar ahmed university of california irvine irvine usa iftekha uci.edumohammad moshirpour university of california irvine irvine usa mmoshirp uci.edu abstract commit messages provide descriptions of the modifications made in a commit using natural language making them crucial for software maintenance and evolution.
recent developments in large language models llms have led to their use in generating high quality commit messages such as the omniscient message generator omg .
this method employs gpt to produce state of the art commit messages.
however the use of proprietary llms like gpt in coding tasks raises privacy and sustainability concerns which may hinder their industrial adoption.
considering that open source llms have achieved competitive performance in developer tasks such as compiler validation this study investigates whether they can be used to generate commit messages that are comparable with omg.
our experiments show that an open source llm can generate commit messages that are comparable to those produced by omg.
in addition through a series of contextual refinements we propose local message generator omega a cmg approach that uses a bit quantized 8b open source llm.
omega produces state of the art commit messages surpassing the performance of gpt in practitioners preference.
index terms large language model llama3 commit message generation gpt4 i. i ntroduction commit messages cm play a crucial role in documenting changes in version control systems facilitating maintenance and evolution of the software .
researchers have leveraged natural language processing techniques to develop automatic methods for commit message generation cmg with the goal of improving the quality of human written messages .
in addition to detailing the changes what information and their rationale why information researchers have identified additional expected criteria from practitioners perspective .
thus cmg can be regarded as a complex reasoning task requiring a comprehensive and precise understanding of the commit context as well as the ability to formulate a cm that meets all the quality criteria developers expect.
given this complexity cmg methods have significantly advanced with the introduction of large language models llm trained using reinforcement learning from human feedback rlhf such as gpt .
the enhanced reasoning capabilities of these models have resulted in state of the art performance in cmg .
notably li et al.
utilized gpt4 a proprietary llm with .
trillion parameters to outperform traditional state of the art cmg approaches .
they introduced omniscient message generator omg areact agent that produces high quality cms by leveraging six contextual pieces of information about a commit.
gpt is employed in omg to generate three out of the six contextual pieces of information and it also serves as the reasoning engine in the react chain.
although omg produces cm with state of the art quality its reliance on a proprietary llm introduces certain organizational and environmental risks.
third party proprietary llm apis have been introducing privacy risks for companies and research has been done to investigate llms privacy and security implications .
for instance a developer at samsung accidentally shared sensitive internal source code with chatgpt leading samsung to ban the use of all proprietary chatbots due to the difficulty in accessing and deleting shared information .
omg requires sharing sensitive source code information including the bodies of affected methods and classes before and after the commit with a third party api gpt .
this introduces privacy risks making its adoption by the industry problematic and limiting its practical application.
additionally researchers have shown that making requests to proprietary llms can lead to annual carbon emissions greater than the emission during training such llms .
omg makes llm requests in preparing commit context thoughts in react number of available tools to the agent initial react thought to generate a cm.
a study on popular java repositories has shown that on average a developer pushes commits daily .
assuming this number holds in industrial cases a company with developers using omg would generate over a million cms for a single project annually resulting in million calls to a proprietary llm.
this makes the wide adoption of omg a sustainability threat that introduces environmental risks.
these limitations in omg motivate the need for a shift away from a proprietary llm based approach.
open source llms ollm offer a cost effective and privacy conscious alternative as they mitigate the privacy concerns associated with proprietary models.
deployable on local gpus due to their smaller number of trained parameters ollms are more sustainable for adoption in llmbased automations .
software engineering researchers have attempted to use ollms to address several development tasks.
while they have shown promising results in some areas a performance gap has been noted when compared toarxiv .02502v2 nov 2024proprietary llms .
however with the advent of new ollms that achieve performance on par with proprietary llms across various benchmarks and leaderboards the likelihood of their successful adoption for a wide range of tasks is increasing.
since prior work has not investigated the application of ollms in generating cms that meet practitioners expectations and are able to achieve performance similar to the state of the art cmg technique our study aims to take the first step in replacing gpt in omg with an ollm and to assess the feasibility of generating cms comparable to those produced by omg.
to address this goal we formulated our first research question.
rq1 can an ollm generate cms comparable to a state of the art llm gpt ?
to address our first research question we experimented with using an ollm instead of gpt to generate llmderived commit context and produce high quality cms.
we utilized automated machine translation evaluation metrics and practitioner surveys to measure the quality of the generated cms.
based on our results although the ollm produced cms with quality comparable to omg in various aspects it did not meet practitioners expectations in one key aspect comprehensiveness .
this indicates that the ollm generated cm missed details that were covered by omg.
to address this shortcoming we propose our second research question.
rq2 how can we bridge the comprehensiveness gap between the cms produced by an ollm and the cms generated by a state of the art llm gpt ?
to answer this rq we introduced change based multiintent method summarization cmms to provide refined contextual information that bridge the comprehensiveness gap by refining the commit context.
through automated evaluation and a second survey we examined the refined context s effectiveness in addressing rq2.
while the improved prompt with the enhanced commit context leads to cms even closer to those generated by omg the running the ollm for inference requires at least an nvidia a6000 gpu with 48gb of vram.
this high resource demand can limit the practical usefulness of our cmg approach for individual developers or smaller teams with limited budgets.
furthermore adopting an ollm with fewer trained parameters results in a more sustainable cmg approach by minimizing its carbon emission aligning with one of the primary objectives of this study.
thus we aimed to explore the possibility of generating comparable cms using a smaller ollm slm that can run on a local gpu with as little as 8gb of vram reducing the gpu vram requirement by .
this investigation is formulated as the third research question of this study.
rq3 can a smaller ollm slm produce cms comparable to a state of the art llm gpt ?
to address this research question we initially employed the same prompt and context as we used to answer rq2 in an attempt to achieve comparable results.
however asanticipated the automated scores for the generated cms by our tested slms were considerably lower compared to those produced by the larger ollm.
the poor scores despite the enhanced context led us to hypothesize that the slm is not capable of correctly understanding the underlying changes in a diff.
having validated our hypothesis through an analysis study we developed two commit diff augmentation techniques diff narrator and fine grained interactive diffexplainer fidex designed to clarify the changes in a diff.
as with previous research questions we used automated metrics and a third practitioner survey to evaluate the effectiveness of our diff augmentation techniques in closing the gap between the commit messages generated by the slm and those produced by the ollm.
in summary our study makes the following contributions we demonstrate that replacing gpt with an ollm using the same commit context as omg produces comparable cms in all human cm quality evaluation criteria except comprehensiveness.
we introduce a new method summarization approach called change based multi intent method summarization cmms for software engineering tasks that rely on code changes.
we propose two augmentation techniques for commit diff diff narrator and fine grained interactive diff explainer fidex that boost slm s performance in the cmg.
we propose the state of the art cmg approach l ocal messag e g enerator omega that employs a 4bit quantized slm with 8b trained parameters that runs on a local gpu with as little as 8gb vram to generate cms that are preferred by practitioners over those generated by omg.
the remainder of this paper is structured as follows.
in section ii we review related research pertinent to our work.
in section iii we detail our methodology for addressing all research questions.
section iv presents the results of our experiments and surveys.
in section v we highlight potential threats to the validity of our findings and the measures taken to mitigate them.
finally section vi concludes our study and outlines potential future work.
ii.
r elated work a. commit message generation over the past few years several studies have aimed to improve the state of the art in cmg by exploring various methods to represent the changes in a commit such as the diff abstract syntax tree ast paths issue states and modification embedding .
however these cmg methods did not account for the impact of botgenerated and uninformative cms during training which has been shown to render their reported performance inaccurate .
accordingly researchers proposed filtering the adopted cm datasets to include only good practice cms when training the deep learning models.
however these methodsrelied on the quality of human written cm which has been reported to lack the required quality and did not align their generated cm with practitioners expectations of a good cm .
to address these shortcomings in previous cmg methods li et al.
conducted surveys and data mining to understand practitioners expectations for a good cm .
they proposed an llm based cmg approach called omg that uses the react prompting framework with gpt to meet the identified expectations.
based on their findings the commit diff which was the primary artifact used in traditional cmg methods is not enough to generate a cm that aligns with developers expectations.
hence they utilized six different contextual pieces of information about a commit resulting in cms that surpassed the quality of those generated by the previous state of the art fira as determined through human evaluation.
despite achieving superior results the authors did not consider using an llm that addresses privacy and sustainability concerns.
instead they employed the most advanced proprietary llm available during the development of omg.
our study builds upon omg in terms of the commit context provided to the model and employing an llm.
however we seek a different goal.
our objective is to generate high quality cms without relying on state of the art proprietary llms.
b. ollms in software engineering researchers have investigated the effectiveness of ollms in solving various software engineering tasks and compared them with proprietary llms yielding both positive and negative results across different areas .
yin et al.
evaluated ollms in identifying software vulnerability.
they reported that while ollms demonstrate some capability in specific areas they still require further improvement to be truly effective in addressing software vulnerabilityrelated tasks .
pan et al.
investigated the effectiveness of llms in code translation .
among the evaluated models including ollms and gpt the best performing ollm starcoder achieved a .
successful translation rate compared to .
by gpt .
on the other hand ollms have demonstrated comparable results with gpt in certain areas.
in a study by liu et al vicuna 13b equipped with the author s proposed online log analysis approach logprompt showed comparable performance with gpt .
zhong and wong inspected the reliability and robustness of the llm generated code.
they found that although meta llama2 achieved a low api misuse rate its compilation rate was reported significantly lower than the other models in the study.
however instructiontuned deepseek coder .7b achieved comparable results in terms of the balance between compilation rate and api misuse rate.
munley et al.
explored various llms capabilities in generation of a validation and verification test suite for highperformance computing compilers from a standard specification.
their results indicated the instruction tuned deepseek coder 33b produced the most passing tests followed by gpt4 turbo .
given the varied performance of ollms across different software engineering tasks it is essential to investigate their ability to generate cms.
our study aims to fill this gap in existing research through investigating the potential of ollms in producing high quality cms that meet practitioners expectations.
iii.
m ethodology in this section we present our methodology in answering our research questions.
figure presents the changes we made to various aspects of omg in answering each research question.
a. base commit context in addition to the commit diff we utilized six different contextual pieces of information about a commit that were proposed and utilized by omg .
specifically for each commit we provided the following contextual information to the ollm slm associated issues on the version control system or issue tracking service associated pull requests relative importance of changed files the software maintenance activity type of the commit multi intent method summaries of changed methods summary of changed classes.
since the latter three components of the commit context are generated by an llm we refer to them as the llm derived commit context .
we used this context as the basis for our study.
however we made specific refinements when addressing our rqs which we detail in subsection f. one of the contextual refinements we propose in this work is altering how method summaries were generated for the affected methods.
therefore it is important to discuss the original approach adopted by omg.
omg employs the multiintent method summarization mms technique proposed by geng et al.
to summarize the methods affected by a commit mms is an llm based method comment generation approach that uses few shot prompting to generate the summaries for a method from five different aspects developer s intents as follows what describes the functionality of a method why explains the reason why a method is provided or the design rationale of the method how to use describes the usage or the expected set up of using a method howit is done describes the implementation details of a method property asserts properties of a method including preconditions or post conditions of a method.
later in this section see subsection f we discuss how mms is employed by omg why it is not fit for the cmg task and how we can overcome its limitations.
b. datasets since our work builds on omg our dataset should include practitioner evaluated cms generated by omg.
this ensures our ground truth cms have high human evaluation scores .
the dataset includes commits from apache projects in java.
we used this dataset to compare our cms generated usingllama 70b a wq all in context zer o shotgpt react none documentation removalmms cmmsrq1 rq2 rq3 omg oursllm prompting t echnique aspectmethod class pr eprocessing method summarization none documentation removalmms cmmsmethod class pr eprocessing method summarization llama 8b a wqgpt 4llm fidexnonediff augmentationllama 70b a wq all in context zer o shotgpt reactllm prompting t echnique all in context zer o shotreactprompting t echniquefig.
.
overall methodology.
modified aspects of omg are represented using .
for each changed aspect shows the value for omg while represents ours.
our approach with those generated by omg using automated evaluation metrics and practitioner surveys.
additionally we used the same dataset to evaluate the performance of our candidate models in producing llmderived commit context as discussed earlier in this section .
specifically for the software maintenance activity type classifier we used the same dataset of commits in java manually labeled with three maintenance activities corrective perfective and adaptive .
to evaluate the performance of candidate ollms slms in class summarization similar to the approach taken by li et al.
due to budget constraints and the high cost of using gpt we sampled classsummary pairs confidence level margin of error from the class summary dataset used by omg.
lastly to examine our candidate ollms slms in generating method summaries we used the test set utilized by omg.
all these datasets are provided in the supplementary .
c. evaluation metrics automated metrics omg generated cms have been evaluated by practitioners and achieved high human evaluation scores .
therefore we postulated that if we make our cms similar to those generated by omg they would achieve acceptable results when evaluated by practitioners.
this approach allowed us to use the similarity to omg generated cms as an initial quality assurance measure before conducting human evaluations.
accordingly we used standard evaluation metrics that are used to compare cms with state of the art machine generated cms .
specifically we used bleu meteor and rouge l to measure the similarity between the cms generated by an ollm and slm with those generated by omg.
additionally following the common practices in cmg research we reported automated metrics by comparing our cms with human written cms.
human metrics in order to evaluate our cms by practitioners we opted for using the four human evaluation metrics proposed by li et al.
in our surveys.
these metrics were developed through careful study of cmg literature and discussions among researchers .
the metrics are rationality which assesses whether a cm provides a logical explanation for the code change and identifies the software maintenanceactivity type.
comprehensiveness which evaluates whether the message summarizes what has been changed and includes relevant important details.
conciseness which measures the brevity of a cm.
expressiveness which examines the grammatical correctness and fluency of the cm.
d. experimental setup we utilized a linux server equipped with an nvidia a6000 gpu with 48gb of vram to run the ollm inference engine.
the 48gb of gpu vram limited our experiments to ollms with up to billion trained parameters in full precision or up to a bit quantized billion parameter ollm .
therefore we had to select a quantization method to quantize ollms with more than billion trained parameters.
among the state of the art quantization methods we chose activation aware weight quantization a wq due to its minimal impact on model s perplexity and the inference speedup it provides for the quantized llm .
for the remainder of this paper a quantized model refers to an ollm that has been quantized to bit using the awq technique.
to efficiently manage and execute an ollm we chose vllm due to its efficient memory management compatibility with our server and ease of deploying a wide range of ollms.
vllm provides an openai compatible api facilitating its use with llm app developments such as langchain which we used to develop the llm agents utilized in this study.
however at the time of our experiments vllm lacked the embedding inference capability.
consequently we could not use the original class summarization approach employed by li et al.
which relied on embedding the changed classes and using retrieval based question answering to summarize them.
instead we utilized zero shot prompting to summarize the affected classes.
lastly to ensure consistent output we set the temperature to when using the deployed ollm for inference similar to previous work .
e. survey overall we conducted three practitioner surveys to answer our research questions.commits sampling we adopted a similar approach as li et al.
while conducting our surveys to keep the workload manageable for participants.
specifically evaluating cms for the entire commit dataset which involves assessing candidate cms for commits from four perspectives would be impractical for participants.
therefore we randomly sampled commits for each survey resulting in commit messages cms generated using our approach and cms generated using the approach we are comparing with for participants to compare and evaluate.
this is the same total number of cms that were evaluated by the survey participants in the evaluation of omg ensuring a fair workload to achieve a high completion rate .
participant recruitment we adopted the snowball sampling approach to recruit the participants for our surveys .
specifically we began by distributing the survey to our industry contacts with at least two years experience in java development and sent periodic reminders to encourage participation.
additionally we asked them to share the survey with other developers who have relevant programming backgrounds.
this approach ensured that participants possessed the necessary knowledge to accurately assess the cms generated for java projects.
survey design all three surveys were designed to comparatively evaluate cms written for a sample of commits.
for the first and last surveys we presented two candidate cms for each commit cm and cm .
we randomly shuffled the questions to eliminate any bias towards an option due to an observable pattern in the candidate cms.
the survey was hosted on questionpro and participants were provided with definitions of the human evaluation metrics.
following the definitions we presented the commits with all the commit context along with the two candidate cms.
for each human evaluation metric we asked the participants to select their preferred cm or choose identical if there was no clear preference.
the identical option was provided to see if the cms generated by the ollm slm are comparable to those produced by omg.
additionally for each commit we asked the participants to select their overall preferred cm.
the second survey was designed differently as its purpose was to validate the comprehensiveness of the cms after making llm derived commit refinements.
for each commit we presented the cms before and after the refinements.
similar to the other surveys we randomly shuffled the questions.
for each commit we asked the participants to choose a candidate cm that is more comprehensive.
we did not provide an identical option for this survey as our goal was to assess whether the enhanced context produced more comprehensive cms not just comparable ones.
f .
answering rqs in the following subsections we detail the steps taken to address each research question.rq1.
can an ollm generate cms comparable to a state ofthe art llm gpt ?
ollm selection to answer rq1 we needed to compile a list of candidate models.
given the reliance of omg on code summarization and the necessity of code understanding to generate high quality cms candidate models had to demonstrate strong performance on code related tasks by scoring high on relevant benchmarks.
we selected four ollms as candidates to answer rq1.
all selected models held leading rankings in the evalplus leaderboard and the big code models leaderboard at the time of compiling the candidate list.
evalplus is a code synthesis evaluation framework designed to benchmark the functional correctness of llm synthesized code.
the big code models leaderboard evaluates the performance of base multilingual code generation models on the humaneval benchmark and multipl e. these benchmarks are commonly referenced by researchers when selecting llms .
we chose a quantized instruction tuned llama3 70b and a quantized instruction tuned deepseek coder 33b .
we selected instruction tuned llama3 70b since its score on the humaneval benchmark was comparable to the top models on the big code models leaderboard .
we also selected the instruction tuned deepseek coder 33b because it was ranked as one of the top models in the evalplus benchmark.
in addition we used the autoawq library to quantize codefuse deepseek 33b and opencodeinterpreter ds 33b as awq quantized versions of these models were not available on huggingface.
when sorting the models on the big code models leaderboard by their performance in generating correct java code codefuse deepseek 33b and opencodeinterpreter ds 33b were ranked among the top models.
in order to ensure the quality of the llm derived commit context similar to li et al.
we evaluated the candidate ollms performance on class summary generation mms and classifying software maintenance activity types.
based on the evaluation results we selected the top performing model quantized instruction tuned llama3 70b for our experiments to answer rq1 and rq2.
prompting method in answering rq1 our goal was to determine the feasibility of replacing gpt with an ollm in the original implementation of omg.
therefore although the adopted prompting method react does not align with one of the primary objectives of our study sustainability of the cmg approach we chose to begin our experiments with this prompting method.
this decision was made to limit the changes to the adopted llm.
however we observed poor automated scores when comparing the cms generated using react by our selected ollm to those generated by omg.
this led us to question the capability of the ollm in generating useful thoughts to reason about each contextual piece of information about a commit as noted by other researchers .
to address the poor performance we avoided using moreadvanced prompting techniques to ensure the results did not stem from improvements in the adopted prompting approach.
hence we utilized the simplest prompting method zero shot prompting .
instead of expecting the ollm to request each piece of commit context as in react we provided all the available context about a commit along with cmg instructions in one prompt.
this change in prompting technique increased the bleu score achieved by the ollm by .
the high automated scores meteor and rouge l above from zero shot prompting suggested that the generated cms resembled those produced by omg.
to verify their quality we conducted a survey with practitioners to compare the cms from the selected ollm with those from omg using human metrics.
the survey results are discussed in section iv.
the survey verified our assumption.
however practitioners preferred omg generated cms for their comprehensiveness which led to our second research question.
rq2.
how can we bridge the comprehensiveness gap between the cms produced by an ollm and the cms generated by a state of the art llm gpt ?
to answer rq2 we initially expanded our zero shot prompt to ask the ollm to comprehensively detail all the changes that occurred in the commit without missing anything.
however this not only failed to increase the automated metrics but also moderately decreased them.
therefore since a change in the prompt did not address rq2 we shifted our focus to the provided context.
specifically we investigated the effectiveness of the llm derived commit context namely the summaries of changed classes and methods and the software maintenance activity type.
our cms were structured similarly to omg they included a header with the software maintenance activity type and a brief subject followed by the body.
consequently the software maintenance activity type contributed only one word to the cm i.e.
one of refactor fix style or feat .
therefore we concentrated on the class and method summaries positing that improvements in these should help the ollm write more comprehensive cms.
according to the literature mismatched code comments cause the models to produce summaries that reflect the comments rather than the actual code functionality .
hence we added a preprocessing step to the class and method bodies passed to the ollm for summarization by removing all the documentation comments and javadocs .
furthermore we changed the way the mms was done for modified methods methods that exist in both pre and postcommit states .
originally omg generated these summaries separately for the pre commit and post commit bodies of affected methods and provided these as the summaries of the affected methods.
however we hypothesize this approach is not suitable for a code change based task like cmg as it does not ask the ollm to generate summaries based on how the changes in the diff impact each of the five different aspects of affected methods.
instead this approach assumes llm sability in generating different summaries with slight changes to the method bodies.
in order to validate our assumption we randomly sampled commits from the commits in which the changes affected a method confidence level margin of error .
two authors independently compared the generated summaries for the pre and post commit bodies of the affected methods to see if the changes are correctly captured yes no in the summaries for any of the five aspects.
we observed that in of the commits inter rater agreement of there was no conceptual difference in the generated method aspects recall the five aspects of a method that mms provides by the ollm for the method bodies before and after the commit.
this confirmed our hypothesis.
to address the identified shortcoming of mms generating semantically identical summaries for affected methods before and after the commit we introduce change based multiintent method summarization cmms .
specifically for a modified method we first generated the pre commit multiintent summaries using the original approach proposed by geng et al.
.
next we passed the pre commit summary along with a list of changes to the pre commit method body produced by a python script that parses the method body before and after the commit.
we then asked the ollm to explain how each method aspect of the pre commit method body would be affected by these upcoming changes.
for instance changes to a method s input arguments affect the how to use aspect of it.
we conducted an ablation study to examine the effectiveness of each refinement documentation removal and cmms separately.
both refinements improved all automated metrics using omg as the reference.
we report the automated evaluation results in section iv.
since the omg generated cms were perceived as more comprehensive by the participants in our first survey this increased similarity to omg generated cms led us to infer that the produced cms had become more comprehensive.
to validate this assumption we conducted a second survey and asked participants to compare the cms produced with the old commit context to those resulting from the enhanced commit context.
the survey results verified our hypothesis.
the survey results are reported in section iv.
rq3.
can a smaller ollm slm produce cms comparable to a state of the art llm gpt ?
slm selection to maintain a fair comparison between the performance of a selected slm and the quantized llama3 70b we only considered the quantized versions of any slm.
given the acceptable performance of the quantized llama3 70b we included the lighter version of the llama3 family the quantized instruction tuned llama3 8b as one of the candidate slms.
additionally we considered the quantized versions of conversation tuned codeqwen1.
7b and instructiontuned mistral v0.
7b .
at the time of gathering the candidate slms these three models had the highest rankings among ollms with under billion trained parameters inthe repoqa benchmark which evaluates llms capability in long context code understanding tasks .
similar to our approach for ollm selection in rq1 we evaluated the candidate slms on class summarization mms and software maintenance activity classification tasks.
initially we selected the conversation tuned codeqwen1.
7b due to its superior performance in generating the llm derived commit context .
however after observing its output in an initial cmg experiment we noticed that the model often repeated the same sentences to fill the allowed maximum tokens.
while we could have used a repetition or frequency penalty to mitigate this issue best practice from the relevant research dictates that the model should not get penalized for reusing tokens .
hence we used the second best model quantized instructiontuned llama3 8b for the remaining experiments without any penalties.
cmg experiments given the improved comprehensiveness achieved in rq2 and the overall good performance of our zero shot prompting in rq1 we experimented with the selected slm under zero shot prompting using the enhanced llm derived commit context .
however the automated scores were considerably lower than those achieved by the selected ollm.
given the identical commit context we posit that the lower automated metrics are due to the model s inability to correctly and comprehensively understand the changes introduced by the commit.
dong et al.
found a similar issue in learning based cmg approaches by observing that their poor quality lies in strong attention weights for marks white spaces line prefixes in a diff .
nonetheless they did not evaluate llms understanding of a git diff and its impact on the generated cm.
accordingly to validate our hypothesis we randomly sampled commits and asked the selected slm to explain the changes in the diff of those commits.
next two authors independently evaluated the slm s answers and marked each answer as correct if all the changes in the diff were correctly covered otherwise incorrect.
we found out that in of the cases the llm s response was not correct.
the inter rater agreement for this analysis study was .
fidex in light of these findings we realized the necessity of a diff augmentation approach that helps the slm correctly understand the changes in a diff.
this augmentation approach should address specific considerations.
firstly it should be able to accurately comprehend all the changes that occurred in the diff without any errors c1 .
additionally research suggests that explicit prompting boost the performance of llms in inferential reasoning tasks such as ours which is inferring the changes in a diff during cmg.
hence the diff augmentation approach should be able to remove this inference step for the slm in the cmg task by explicitly stating the differences between the old and new versions c2 .
furthermore relevant studies in cmg suggest that providing fine grained details of the changes in a diff can boost the performance .
therefore the diff augmentation approach should provide details about the differences c3 .
we did not find a similar approach that addresses these considerations.therefore we propose fine grained interactivediffexplainer fidex a hybrid llm based approach to produce a detailed explanation of a diff highlighting all differences between the pre and post commit versions of affected java files.
fidex is a hybrid approach since it leverages a deterministic solution to understand the changes in a diff c1 and uses an llm based solution while explaining the differences between the old and new versions c2 c3 .
specifically since we observed the slm s inaccuracy in understanding the changes in a diff we devised a deterministic solution to minimize the hallucinations by fidex in understanding the changes c1 .
particularly we developed a python script named diff narrator .
given a commit diff diff narrator outputs a diff narrative which is a numbered list of change items .
change items are basic units of changes in a diff and can be either of the following cases addition chunk consecutive lines with mark removal chunk consecutive lines with mark replacement chunk a removal chunk immediately followed by an addition chunk.
to explain the differences between the old and the new version of affected java files c2 c3 fidex prompts an llm.
we adopted the role playing prompting technique in fidex which has been shown to outperform zero shot prompting in several reasoning benchmarks .
specifically fidex consists of two phases prompt construction and output construction.
figure illustrates the fidex approach as a conversation between the user and an llm.
in the prompt construction phase we prepare the role prompts to be passed to the llm.
the prompts start with a role setting prompt where we define the llm as a senior java developer and instruct it to answer questions factually and precisely.
following this we include two rounds of user llm interactions.
in the first round the user asks the llm to provide instructions on how to read a diff.
the llm s response includes predefined instructions for reading a diff.
in the second round the user asks the llm to follow you ar e a senior java developer .
answer my questions factually and precisely .
follow the instructed steps for the following diff thank you!
so what ar e the differ ences between the old and new versions of each changed file?
be car eful about the differ ent statement types e.g.
javadoc method call variable declaration etc.
be mindful about the order of lines in the diff.
be car eful about any indentations or code style formatting changes.
diff explanation how can i r ead a git diff?prompt construction output constructiondiff narrator fig.
.
fine grained interactive diff explainer.
values wrapped in square brackets are hard coded.
except the last message other messages are fed into the llm s memory.based on the pr ovided diff the differ ences between the old and new versions of the jarmojo.java file ar e .
the line adddir ectory includes package.html basedir has been r eplaced with adddir ectory includes package.html .svn basedir .
the change is in the third argument of the adddir ectory method.
the old version includes only package.html while the new version includes package.html and .svn .
this suggests that the new version of the code is including the .svn dir ectory in the package.diff git a maven mboot2 sr c main java jar jarmojo.java b mavenmboot2 sr c main java jar jarmojo.java index 0ecfceb9a..c1cf57026 a maven mboot2 sr c main java jar jarmojo.java b maven mboot2 sr c main java jar jarmojo.java public class jarmojo map includes new linkedhashmap adddir ectory includes package.html basedir adddir ectory includes package.html .svn basedir cr eatejar jarfile includes file maven mboot2 sr c main java jar jarmojo.java has been modified in this commit.
following changes have been made to the file .
following line s adddir ectory includes package.html basedir is replaced with with following line s adddir ectory includes package.html .svn basedir fig.
.
example of a diff its diff narrative and its fidex generated diff explanation using quantized llama3 8b from top to bottom .
the highlighted parts show how diff explanation outlines the difference between the old and new version in fine detail.
these instructions and describe all the changes in the input diff.
to generate the llm s response for this prompt we use the diff narrative produced by diff narrator.
lastly the output construction stage is where the final interaction between the user and llm takes place.
the user asks the llm to describe the differences between the old and new versions of each affected file c2 while considering specific cautions to ensure accuracy and comprehensiveness.
the cautions are designed to warn the llm about different fine grained statement types c3 such as javadocs method declarations etc.
to avoid confusion between documentation and code changes .
additionally the llm is instructed to be mindful of the order of changes to highlight reordering changes and to pay attention to the sequence of lines in the diff.
the final consideration is asking the llm to differentiate between code style or formatting changes which helps identify stylistic changes and correctly differentiate between different software maintenance activity types in the commit .
figure provides an example of a raw diff the diff narrative produced by the diff narrator in grey and a diff explanation that is generated by the selected slm through using the fidex.
we augmented the raw diff with the fidex generated diff explanation to observe its impact on the similarity of the generated cms to those generated by omg.
in addition we used thediff narrative as a diff augmentation to determine if the additional details provided by fidex help the slm write cms more similar to those produced by omg.
lastly we conducted a final practitioner survey to measure the effectiveness of cmg omg softwar e maintenance activity t ype relative importance of changed filesraw diff retrieval based class summary gpt 4react pr omptingmms linked issues prssoftwar e maintenance activity t ype relative importance of changed filesaugmented diff zero shot class summarycmms linked issues prschanged methods and classes documentation removal fidex bit quantized llama3 8b instructzero shot pr omptingchanged methods and classesomegafig.
.
differences between omg and omega using the selected slm.
figure highlights the differences between omg and our final cmg approach omega.
iv.
r esults in this section we address our research questions by presenting the results of both automated and human evaluations.
rq1.
can an ollm generate cms comparable to a state ofthe art llm gpt ?
automated evaluation to ensure the quality of the llmderived commit context similar to li et al.
we evaluated the candidate ollms performance on class summarization mms and classifying software maintenance activity types.
table i presents the results of automated evaluation of candidate ollms in generating llm derived commit context.
among the candidate ollms the quantized instruction tuned llama3 70b scored highest in classifying software maintenance activity type and class summarization leading to its selection for experiments in rq1 and rq2.
as discussed in the previous section we experimented with react and zero shot prompting techniques to answer rq1.
table ii presents the automated evaluation results for these two prompting approaches.
based on the automated scores using zero shot prompting resulted in a bleu score that was .
times higher than that achieved by adopting react.
therefore we used the cms generated through zero shot prompting technique in our first survey to assess their quality by practitioners.
human evaluation in our first survey practitioners participated.
the cms were randomly sampled using all incontext zero shot prompting with the original omg commit context.
ollm generated cms were preferred in of responses while omg generated cms were selected as the overall preferred cm in of responses.
in of the responses the cm generated by the selected ollm was perceived as more concise.
overall there was no aspect in which omg was selected by the majority of responses over the selected ollm quantized instruction tuned llama3 70b .
however ollm generated cms were selected in fewer responses intable i automated evaluation results for generating llm derived commit context .
sma stands for software maintenance activity .
we used the value reported by li et al.
to avoid incurring additional costs .
bold models are the selected candidates.
model candidate for class summarization method summarization sma classification bleu meteor rouge l bleu meteor rouge l accuracy gpt turbo .
.
.
.
.
.
codefuse deepseek 33b rq1 .
.
.
.
.
.
deepseek coder 33b instruct rq1 .
.
.
.
.
.
opencodeinterpreter ds 33b rq1 .
.
.
.
.
.
llama3 70b instruct rq1 .
.
.
.
.
.
codeqwen .
7b chat rq3 .
.
.
.
.
.
mistral 7b instruct v0.
rq3 .
.
.
.
.
.
llama3 8b instruct rq3 .
.
.
.
.
.
table ii automated evaluation results for rq1 and rq2 using instruction tuned awq quantized llama3 70b.
before and after values for commit enhancements are separated by slash.
rows with bold context were used in the survey conducted for the rq.
rq prompt commit context reference omg reference human bleu meteor rouge l bleu meteor rouge l 1react same as omg .
.
.
.
.
.
zero shot same as omg .
.
.
.
.
.
zero shotomg documentation .
.
.
.
.
.
.
.
.
.
.
.
mms replaced by cmms .
.
.
.
.
.
.
.
.
.
.
.
refined .
.
.
.
.
.
.
.
.
.
.
.
terms of comprehensiveness ollm of responses and omg .
this observation led to the formulation of rq2 which we addressed through llm derived commit context enhancements.
an ollm can produce cms that are comparable overall to those generated by a state of the art llm except in terms of comprehensiveness.
rq2.
how can we bridge the comprehensiveness gap between the cms produced by an ollm and the cms generated by a state of the art llm gpt ?
automated evaluation table ii presents the automated evaluation results for our ablation study see section iii highlighting the impact of each contextual refinement in bridging the identified comprehensiveness gap from our first survey.
removing documentation from affected method and class bodies significantly improved all automated scores increasing the bleu score of the cms from .
to .
a improvement.
replacing mms with cmms without docu mentation removal also led to similar improvement although the bleu score when comparing cms to human written ones degraded by .
given the effectiveness of each standalone enhancement we combined them resulting in a bleu score of .
when compared to omg a higher score than the score achieved by cms used for our first practitioner survey.
human evaluation our second survey was designed to assess the effectiveness of our strategy to bridge the identified comprehensiveness gap in the first survey.
hence participants compared randomly sampled cms that were generated before the llm derived context enhancements with those generated by the enhanced context.
based on the survey results of responses found the cm generated through the enhanced context more comprehensive.
replacing mms with cmms along with a documentation removal step before summarizing affected methods and classes mitigates the comprehensiveness gap.
table iii automated evaluation results for rq3 using instruction tuned awq quantized llama3 8b.
the last row presents the automated scores of omega which was used for survey .
prompt commit context diff augmentation technique reference omg reference human bleu meteor rouge l bleu meteor rouge l zero shot refinednone .
.
.
.
.
.
diff narrator .
.
.
.
.
.
fidex used in omega .
.
.
.
.
.26rq3.
can a smaller ollm slm produce cms comparable to a state of the art llm gpt ?
automated evaluation similar to rq1 we evaluated the candidate slms performance on class summarization mms and classifying software maintenance activity types.
table i presents the automated scores achieved by each candidate slm in generating llm derived commit context.
although the quantized conversation tuned codeqwen 7b was the best performing model based on the automated scores we chose the second best slm the quantized instruction tuned llama3 8b after observing the problem of repeated sentences by the quantized conversation tuned codeqwen 7b in cmg see previous section for details .
we present the automated scores for our experiments in rq3 in table iii.
as shown in the table each augmentation to the raw diff consistently improves all automated scores.
two key observations can be made from this table.
firstly the fidex generated diff explanation enhances the bleu score when compared to human written cms by .
secondly the difference between various diff augmentation approaches compared to the raw diff is more significant than the differences among these methods themselves.
we posit that this is because providing the diff narrative gives sufficient context for simple diffs and the cms for those diffs do not significantly change with the fidex approach.
nevertheless appending the diff with our fidex generated diff explanation improves the bleu score by which is only lower than the score achieved by the selected ollm without diff augmentation rq2 .
human evaluation our last survey aimed to evaluate the performance of omega by assessing the impact of augmenting commit diff with fidex generated explanations compared to omg.
a total of practitioners participated in this survey.
according to the findings the selected slm outperforms gpt4 in generating cms that meet practitioners expectations except in conciseness.
slm generated cms were preferred in of responses while omg generated cms were preferred in of responses.
notably the preference for slmgenerated cms in all metrics except conciseness was higher than that for the quantized instruction tuned llama3 ollm .
this highlights the effectiveness of the llm derived commit context enhancements rq2 and the fidex generated diff summaries rq3 in enabling a quantized ollm to produce superior cms compared to gpt .
augmenting the commit diff with an explanation generated by fidex enables an slm with just .
of the trained parameters to outperform gpt in cmg.
v. t hreats to validity we adopted measures to mitigate potential threats to the validity of our work which we outline in this section.
construct validity relying on automated machine translation metrics for cm evaluation has been shown not to align percentageoverall preferencecomprehensivenessconcisenessexpressivenessrationalitycriterion human evaluation results of omega vs omg omega identical omgfig.
.
human evaluation results of omega vs omg with human preferences .
to minimize our reliance on these metrics we used them solely as a preliminary step to ensure the generated cms similarity to omg written ones.
the quality of the cms were ultimately assessed by practitioners using state of the art human evaluation metrics .
there is a possibility that survey participants may have misunderstood the human evaluation criteria when assessing the candidate cms.
to mitigate this risk we provided definitions for each human evaluation criterion at the beginning of the survey.
lastly our choice of quantization method may have hampered the selected model s performance.
however among the existing quantization methods at the time of conducting our study our selected approach was the one with the minimal impact on the model s performance .
internal validity to ensure that our results solely stem from the contextual refinements we made and are not influenced by the selection of llm inference parameters we set the temperature to and avoided any repetition or frequency penalties.
this approach makes the ollms responses reproducible and deterministic.
external validity similar to omg our study is limited to apache projects written in the java programming language.
however given the multilingual programming datasets used to train our selected ollms we posit that our results are generalizable to other programming languages as well.
additionally the choice of ollm slms may affect the generalizability of our findings.
however to mitigate this threat we tested our approach with multiple candidate ollm slms before selecting the best performing one.
vi.
c onclusion this study investigated the feasibility of generating stateof the art cms that meet practitioners expectations using ollms.
our results show that an ollm can produce cms comparable to those generated by proprietary models though with poorer comprehensiveness performance.
by refining the llm derived commit context we bridged this performance gap.
additionally our findings demonstrate that even a smaller llm slm can perform as well as if not better compared to a large proprietary llm or large ollm.
our findings encourage researchers to explore ways to carefully curate task specific contextual information for llmsto achieve better results rather than relying solely on larger proprietary llms with poorly devised contexts.
our approach to generating high quality cms using an slm offers significant advantages for the industry.
the lower hardware requirements of slms make them adaptable for companies and practitioners with limited computational resources and facilitate the avoidance of sharing sensitive information with external providers.
we provide the source code and datasets that were used in our experiments in the supplementary .