does genai make usability testing obsolete?
ali ebrahimi pourasad and walid maalej department of informatics universit at hamburg hamburg germany ali.ebrahimi.pourasad walid.maalej uni hamburg.de abstract ensuring usability is crucial for the success of mobile apps.
usability issues can compromise user experience and negatively impact the perceived app quality.
this paper presents ux llm a novel tool powered by a large vision language model that predicts usability issues in ios apps.
to evaluate the performance of ux llm we predicted usability issues in two open source apps of a medium complexity and asked two usability experts to assess the predictions.
we also performed traditional usability testing and expert review for both apps and compared the results to those of ux llm.
ux llm demonstrated precision ranging from .
and .
and recall between .
and .
indicating its ability to identify valid usability issues yet failing to capture the majority of issues.
finally we conducted a focus group with an app development team of a capstone project developing a transit app for visually impaired persons.
the focus group expressed positive perceptions of ux llm as it identified unknown usability issues in their app.
however they also raised concerns about its integration into the development workflow suggesting potential improvements.
our results show that uxllm cannot fully replace traditional usability evaluation methods but serves as a valuable supplement particularly for small teams with limited resources to identify issues in less common user paths due to its ability to inspect the source code.
index terms app development large language model foundation models usability engineering ai4se recommender systems quality requirements ai inspired design.
i. i ntroduction with the rapid growth of the app market over the last decade developing good apps has become crucial to vendor success .
studies have shown that users tend to favour apps that perform as expected and are easily understandable .
software usability is a key factor that crucially influences how users perceive the quality of an app .
usability can be broadly defined as a concept that essentially refers to how easy it is for users to learn a system how efficient they can be once they have learned it and how enjoyable it is to use it .
usability issues are problems that compromise the usability of an app hindering a positive user experience .
it is thus crucial for developers to thoroughly detect and address usability issues for improving their apps .
there are different ways to systematically identify usability issues.
conventional usability evaluation methods include usability testing in labs with users as well as theoretical analyses with experts .
once usability issues are identified developers can address them and offer a refined app to their users .
however it can be challenging especially for small app development teams to channel the resources and expertise needed for implementing an effective usability evaluation .generative artificial intelligence genai is a maturing technology that is increasingly getting attention for the purpose of automating various tasks in different domains as it is capable of generating meaningful texts images and videos .
particularly foundational models such as large language models llms process a user textual prompt and construct responses by predicting next tokens in a partially formed context .
recently foundation models gained considerable interest in the software engineering domain as they can be employed for a variety of purposes including generating code and documentation or fixing bugs .
this work investigates the extent to which genai can support or even automate usability evaluation for mobile apps.
we introduce ux llm a novel open source tool that uses foundation models to detect usability issues in the separate views of native ios apps.
as input ux llm requires a brief description of the app context the source code and an image of the analysed view.
section ii introduces the implementation details of ux llm and the underlying prompt engineering.
this constitutes our first contribution.
to evaluate ux llm performance and how it compares with conventional usability evaluation methods we conducted a multi method study consisting of expert assessments expert reviews and usability testing for two open source ios apps of medium complexity a quiz and a to do app.
to understand how development teams perceive the support of such tools and explore possible concerns we conduced a focus group within an app development project.
section iii presents the design of our evaluation study.
the encouraging results confirm that llm based approaches are able to identify valid app usability issues.
the results also indicate that genai approaches do not fully replace traditional methods but rather complement them highlighting the potential as a supportive tool that can enhance usability evaluations during the development process.
section iv reports on the results of our evaluation study which constitutes together with the data our second contribution.
the remainder of the paper discusses the work limitations and the threats to validity in section vi related work in section v and summarises the findings with their implication in section vii.
ii.
ux llm a pproach ux llm is an application that predicts usability issues for a view of an ios mobile app.
for instance in the case of a registration view ux llm might predict the absence ofarxiv .00634v2 mar 2025fig.
user interface of ux llm.
placeholders for input fields leaving users uncertain about what information to input.
as input ux llm requires the app context source code and an image of the analysed view.
the app context consists of two texts a brief overview of the app e.g.
as found on app pages in app stores and the user task which describes the main goal of interacting with the view.
for example when looking at a meditation app the overview could be a meditation app focused on improving stress relief and wellness .
when analysing the progress tracking view the user s task could be review meditation history and achieved milestones .
in addition the source code provided needs to be swiftui1code from the view components and logic.
the input image can be a screenshot from the running app or from the design file.
the input data is packaged into a prompt using prompt engineering techniques and sent to a multimodal llm namely openai s gpt turbo with vision2.
finally the output is a list of predicted usability issues with brief explanations.
figure displays the gui of ux llm showing an example where usability issues were predicted for a view of a quiz app.
a screenshot of the view is provided on the left side.
on the right side three text fields contain the app context and the source code as mentioned earlier.
below a start button initiates the testing to generate usability issues.
under this button the identified issues are displayed offering insights into potential areas of improvement in the user interface.
a. prompt engineering prompt engineering optimises the input to an llm to enhance the output performance.
white et al.
describe it as the means by which llms are programmed via prompts .
user prompt i have an ios app about the user s task in this app view is about inserted user task .
an image of the app view is provided.
below is the incomplete swiftui code for the app view.
this code includes the view s user interface and a view model for logic handling.
it may also include additional components like subviews models or preview code.
source code openai s api3accepts a list of messages as input for their llms.
ux llm utilises two types of messages one for the system and another for user input which can be seen in listings and respectively.
the system prompt provides high level instructions on the behaviour expected from the model.
conversely the user message is assembled using the information provided by the user about their app view.
listing system prompt you are a ux expert for mobile apps.
your task is to identify usability issues with the information you get for an app s view.
an example of a usability issue could be lack of visual feedback on user interactions .
respond using app domain language you must not use technical terminology or mention code details.
enumerate the problems identified add an empty paragraph after each enumeration no preceding or following text.
using several app projects which we had access to and were familiar with but different from those used in the evaluation we experimented with various prompt engineering strategies and tactics from the openai documentation4to enhance the llm performance.
one tactic is to ask the model to adopt a persona which we used in the system prompt where the llm is instructed to act as a ux expert.
furthermore the model was instructed to respond using domain specific language avoiding any mention of code.
this helps focus the responses facilitates more user like feedback and ensures accessibility to non technical stakeholders.
using the strategy write clear instructions we tested different user and system prompts.
we particularly tested more detailed system prompts where the llm was guided to pinpoint issues using a pre defined set of usability attributes such as effectiveness.
however in the end we opted for a more open ended question allowing the llm to freely identify any usability issues it could find thereby enabling a more exploratory discovery of potential issues.
startllmbuttonview backgroundviewinputtextfieldsview appoverview string usertask string sourcecode stringimagedropview nsimage nsimage?
interface llmcaller call configuration llmcallerconfiguration llmresponsecontentview viewmodel llmoutput string?
isloading bool startgeneratingusabilityissues usabilityissuepresentationview usabilityissuestext string isloading bool basicpromptgenerator tinfyimagecompressornetworkservice tinfyshrinkurl urlopenaillmcaller openaiurl urlllmcallerconfiguration modelid string base64encodedimage string?
systemcontent string usercontent stringpromptconfiguration appoverview string usertask string sourcecode string?
hasimage bool interface imagecompressor resizeandshrink imagedata data size cgsize data interface promptgenerator generateusercontent configuration promptconfiguration string generatesystemcontent stringservicesgui interface llmcaller call config llmcallerconfiguration llmresponsecontentview viewmodel llmoutput string?
isloading bool startgeneratingusabilityissues basicpromptgenerator tinfyimagecompressornetworkservice tinfyshrinkurl urlopenaillmcaller openaiurl urlllmcallerconfiguration modelid string encodedimage string?
systemcontent string usercontent stringpromptconfiguration appoverview string usertask string sourcecode string?
hasimage bool interface imagecompressor resizeandshrink imagedata data size cgsize data interface promptgenerator getusercontent config promptconfiguration string getsystemcontent stringservicesguicontentview textfields inputtextfieldsview imagedropview imagedropview startbuttonview startllmbuttonview issueview usabilityissuepresentationview background backgroundviewfig.
simplified class diagram of ux llm architecture.
another tactic employed is provide examples commonly referred to as few shot prompting .
it enhances the llm results by directing the output towards a desired direction .
similarly we implemented one shot prompting by presenting an example usability issue in the system prompt lack of visual feedback on user interactions .
another tactic we used is use delimiters to clearly indicate distinct parts of the input .
this is particularly useful in organising the user prompt thus dividing the different sections app overview user task image and source code.
based on the documentation this structure should help the llm understand and process each part of the prompt effectively.
furthermore specify the desired length of the output is a tactic to ensure that the responses are concise and focused.
in the system prompt we initially asked the llm to list exactly identified usability issues.
however in the testing we ran into cases where less than usability issues were found and the llm fabricated non existent issues.
also we found that when more than issues could be found the model would discard valuable information.
in the end we left out the exact number of issues to identify allowing an unrestricted discovery.
b. implementation ux llm is a macos application developed using swiftui .
when entering an image it uses tinypng api5to compress it.
while waiting for the llm response a shader animation is shown which is sourced from the inferno project by twostraws6.
select open source appsapply ux llmexpert reviewusability testingusability issuesusability issuesusability issuesexpert assessmentperformance analysismethods comparisoncase selectionapply ux llmfocus groupperception evaluationusability evaluation rq2 rq3 rq1 fig.
overview of our evaluation study.
figure shows a simplified class diagram of ux llm architecture.
the diagram particularly abstracts the complexity of the swiftui mvvm architecture by condensing the gui layer and modelling combinations of views and their viewmodel s as one.
the primary focus is on the service layer which operates using the bridge pattern to separate the abstraction the high level logic from the implementation the low level details allowing them to change independently.
this separation enhances modularity and makes the system more flexible and maintainable.
the three services promptgenerator llmcaller and imagecompressor are abstracted from their actual implementation allowing ux llm to easily adapt to different models or apis without significant changes.
additionally the bridge pattern has simplified gui testing by enabling the substitution of mock objects for actual services.
iii.
e valuation design we conducted a thorough multi method evaluation to assess the potential and limitation of genai powered usability evaluation focusing on the following research questions rq1 how accurately can ux llm predict usability issues?
rq2 how do ux llm predicted issues compare with those identified by traditional usability evaluation methods and to what extent can ux llm replace these methods?
rq3 how would an app development team perceive ux llm support during app development?
to answer rq1 and rq2 we applied ux llm to two apps and asked usability experts to assess the predicted issues.
we also performed usability testing and expert review on the same apps and compared the results to ux llm.
to answer rq3 we applied ux llm to an ongoing app development project and conducted a focus group with its development team where we discussed the results and limitations.
figure overviews the entire evaluation study which we discuss in the following.
a. performance analysis and methods comparison rq1 for the evaluation of the ux llm prediction performance we first had to select evaluation apps which we did not use during the development and prompt engineering.
the evaluation apps must have accessible source code that is allowed to befig.
reference apps for the evaluation.
left quiz app right to do app.
processed by openai s gpt .
we weren t able to use apps from partner companies as none wanted to share their code with openai due to security and privacy concerns.
consequently we focused on open source apps particularly structurally straightforward apps of medium complexity to ensure a comprehensive yet manageable evaluation.
furthermore it was essential to have ux experts assess the predicted usability issues generated by ux llm to calculate precision and recall scores which are established benchmarks for a tool like uxllm .
in this context precision calculates of all the issues identified by ux llm how many were actual usability issues.
recall calculates of all the actual usability issues available how many did ux llm successfully identify.
for a comprehensive evaluation of ux llm it is also crucial to compare its results with them of other methods used to identify usability issues rq2 .
to the best of our knowledge no dedicated dataset of open source apps with pre identified usability issues to compare ux llm to was available when this research was conducted.
we also explored github issues and app store reviews as these could have been used to extract usability issues.
many open source apps lacked a substantial user base resulting in limited feedback documented on github or the app store.
while large open source apps like firefox offered extensive user feedback they were unsuitable due to not being written in swiftui and their rather high complexity.
we followed a systematic approach for the app selection.
we first searched github for swiftui and google for open source swiftui .
we then manually checked apps looking for structurally straightforward apps of medium complexity to ensure a comprehensive yet manageable evaluation.
we excluded niche very specialized or inactive apps.
finally we chose two app a quiz app7and a to do app8shown in figure .
the quiz app consists of four screens.
initially the user selects a category in which to take a quiz e.g.
geography.
next they set up the quiz by adjusting settings such as the length.
then they proceed to take the quiz.
in the end there is a score screen that displays the results with options to either retry the quiz or return.
the to do app consists of two screens.
swiftuiparticipant age gender occupation p1 m quality assurance specialist p2 f civil engineer p3 m marketing consultant p4 m civil engineering student p5 m law student p6 f teacher p7 m software developer p8 m informatics student p9 m software developer p10 f dentist table i participants in usability testings of reference apps.
the first screen provides a list of all the tasks that have been added.
the second screen is a task detail view that is used to create or edit a task.
after app selection we conducted three usability evaluations in parallel on each app applying ux llm expert reviews and usability testings.
first we gathered the source code of the two mentioned apps from github and generated usability issues for each view with ux llm.
two ux experts assessed these generated issues to check their correctness and enable calculating precision and recall .
usability testing according to nielsen principles of usability testing participants should represent actual users .
the selected apps are widely recognised quiz apps are popular and to do apps often come pre installed on devices .
our screening process also ensured that all participants had prior experience with using similar apps.
according to nielsen tests with five participants uncovers about of usability issues .
however this approach is debated with some studies suggesting significant variability in the number of usability issues identified with five participants .
as we aim at identifying as many usability issues as feasible we conducted tests for each app with participants whose details are depicted in table i. we recruited participants within our personal network and met at convenient locations to conduct the testing such as at home or in the office.
the participants were seated at a table and used an iphone13 to interact with the apps.
the iphone was configured to record the screen ensuring a comprehensive capture of the interactions.
one author sat next to the participants with a laptop documenting their behaviour.
the test sessions followed four steps.
first we welcomed and thanked participants and explained the obligatory details such as the option to opt out whenever they want.
second we explained the purpose of the study and demonstrated the think aloud protocol they should use.
we stressed that the focus of the study was on the apps and their usability ensuring that participants did not feel they were being tested.
third we gave participants the following list of usual tasks to perform for the quiz app t1 put the app in light mode.
goal find use light mode t2 take a short quiz on a topic that interests you.
goal take a quiz t3 you want to test your knowledge again on the same topic.
user goal replay quiz t4 next week you will have an exam in geography.
take a quiz to boost your knowledge.
goal search and setup a specific quiz for the to do app t5 prepare for your upcoming days.
some to do s on your mind are buying groceries watering plants and taking out the trash.
goal write down simple tasks t6 you must plan your vacation next friday.
goal write down a high priority task with a reminder t7 groceries have been bought except for one item bread .
goal edit task t8 you have emptied the trash and watered your plants.
goal mark tasks complete t9 remove the watering plants task from your list since it is no longer needed.
goal remove one task clear the entire list to make room for new tasks.
goal remove all tasks fourth participants performed the tasks using think aloud while we observed and took notes.
this setup aimed to foster a comfortable environment that resembled typical usage scenarios for the participants.
the full list of resulting usability issues can be found in the replication package .
expert review and expert assessment as the second usability evaluation method we conducted expert reviews with two ui ux professionals male female each with six years of work experience.
their daily work is to conduct usability evaluations and ux design.
the first session was conducted in person and the second via zoom.
each session was divided into two distinct phases with an introduction at the beginning to explain the obligatory details and the purpose of the study.
in the first phase the experts independently reviewed the two reference apps without knowing about ux llm starting with the quiz app.
they navigated through the interfaces vocalising their observations and identifying usability issues similar to the think aloud protocol.
the identified usability issues are listed in the replication package .table ii usability issues of ux llm see replication package annotated with assessments from ux experts.
id e1 e2 id e1 e2 id e1 e2 id e1 e2 c1 a a c14 a a c27 b a c40 b b c2 c b c15 a a c28 d d c41 b a c3 d d c16 a a c29 b b c42 a a c4 a a c17 c b c30 b b c43 a a c5 b b c18 a b c31 d d c44 a a c6 a b c19 b a c32 b a c45 a a c7 a a c20 a a c33 a a c46 a a c8 a a c21 a c c34 d d c47 a a c9 b a c22 a a c35 a a c48 b c c10 c a c23 a a c36 a a c49 c a c11 a b c24 a a c37 a a c12 a a c25 c a c38 b b c13 a b c26 b a c39 b b legend id issue id e1 ux expert e2 ux expert a usability issue b no usability issue c uncertain d irrelevant incorrect statement.
the second phase was dedicated to assessing the usability issues identified by ux llm listed in details in the replication package .
the experts completed a questionnaire where for each usability issue identified by ux llm they had to to classify it into one of the following categories usability issue no usability issue uncertain or irrelevant incorrect statement .
their assessments are listed in table ii.
b. perception of a development team rq3 to address rq3 we conducted an in depth focus group as part of a post graduate university capstone project to develop an app for real clients from industry.
the team was developing a native ios transit app focusing on visually impaired users for a large public transport company.
at the time of the focus group the students had one month remaining to complete their project out of five .
they already had performed extensive requirements design and development work with several prototypes.
the six person team was organised into three pairs each responsible for a project part.
the focus group included three participants one from each pair.
the participating graduate students had at least two years of development experience working part time.
before meeting with the group we gathered their app code and evaluated it with ux llm preparing usability issues for its seven main views included in .
then we ran the focus group for approx.
hours in a university collaboration space along four steps first we interviewed the group about the project details their roles and their usability evaluation work so far.
second we demonstrated ux llm and asked about initial impression and preliminary thoughts.
third we presented the identified usability issues asked the group to rate their usefulness and discussed them briefly one by one.
last we reflected with the group about integrating ux llm into their workflows as well as any remaining concerns they had.
we took notes of the session for later analysis.
the entire list of questions is included in the replication package .
when answering semantic scale questions we used an approach similar to the planning poker from agile practices .
usability issueno usability issueuncertainincorrect irrelevant statement ux expert 1ux expert table 1ux expert 1ux expert 2usability issue2731no usability issue1312uncertain52incorrect irrelevant statement44usability issues count27 assessment categories proportion relative to the total of issues 1fig.
experts assessment of the issues identified by ux llm.
participants simultaneously displayed their chosen numerical rating between and using hand gestures after a countdown from three ensuring unbiased and independent responses.
iv.
e valuation results a. performance analysis figure presents a bar chart of the ux experts assessments of the usability issues generated by ux llm.
the two ux experts provided their evaluations in the four categories mentioned above.
expert labelled samples as actual usability issues as non usability issues as uncertain and as incorrect irrelevant statements.
expert labelled samples as usability issues as non usability issues as uncertain and as incorrect irrelevant statements.
the top of each bar shows the number of issues identified by each expert in each category with percentages indicating the relative ratio to the total of issues assessed.
this indicates that around of usability issues identified by ux llm are valid.
however we noted some discrepancies in the experts evaluations.
for instance the experts collectively identified issues as usability issues while individually they reported and issues.
this is highlighted by cohen s kappa measure a widely accepted metric for assessing evaluator agreement which was .
.
taking into account established benchmarks for interpreting cohen s kappa such as the one proposed by landis and koch or altman a value of .53suggests moderate agreement between the ux experts.
looking at the differences the main reason for disagreement seems to be the subjective nature of usability evaluation and a wide room for interpretation .
for example the experts disagreed on issue c19 about the absence of a description label on the progress bar during the quiz.
one labelled it as an issue while the other argued that it should be intuitive to users what a progress bar in a quiz app represents.
similarly c27 mentions that after the quiz a more detailed feedback despite the score could enhance the learning experience which one expert viewed as a suggestion for a new feature rather than an existing usability issue.
likewise issue c32 denotes the lack of clear separation between tasks since tasks were listedwithout distinct dividers or spacing in the overview of the todo app.
this was seen differently by experts one classified it as a usability issue that could overwhelm users while another considered it a minor aesthetic choice assuming that users can navigate through lists regardless of visual grouping.
next we calculated the precision and recall for uxllm.
due to variations in the expert assessments different metrics for each expert are calculated.
to calculate these metrics we need a definition for true positives tp false positives fp and false negatives fn in the context of ux llm.
true positives are all usability issues generated by ux llm that an expert has identified as a usability issue during their evaluation.
samples identified as no usability issue and incorrect irrelevant statement are defined as false positives.
the issues labeled as uncertain are excluded from the calculations because the experts said that they might provide valuable insights even if they only offer a change in perspective.
for example issue c2 critiques the dark mode colour scheme which was difficult for the experts to assess but both agreed on the positive impact of highlighting it to developers.
false negatives are defined as all usability issues not identified by ux llm but found during the usability testings or expert reviews.
to avoid counting the same usability issue multiple times we manually matched the same issues from each method as presented in table iii.
from this the precision and recall values for expert e1 and expert e2 are as follows precision e1 tpe1 tpe1 fpe1 .
recall e1 tpe1 tpe1 fn .
precision e2 tpe2 tpe2 fpe2 .
recall e2 tpe2 tpe2 fn .
to directly address research question one rq1 the precision scores which range from .61to0.
are encouraging.
this suggests that the usability issues generated by ux llm are generally valid while maintaining false positives at a manageable level.
the recall scores being on the lower side ranging from .35to0.
were expected given the complexity of detecting usability issues.
this was highlighted by molich et al.
who demonstrated that even simple websites can contain more than a hundred usability issues.
b. comparison of usability evaluation methods to compare ux llm with usability testing and expert review we matched the issues capturing the same problem possibly with a varying level of details as shown on table iii.
looking at the issues there is a clear difference intable iii matching of issues in reference apps identified by usability testing a expert review b and ux llm c .
view u. testing expert expert ux llm category a1 b4 b4 view a2 b5 quiz app a3 c6 b2 c1 c12 c20 c23 b6 c4 setup view a5 b10 b11 b14b9 c8 c14 quiz app a6 b13 a7 c11 quiz view a9 a10 b21 c15 c21 quiz app a14 b20 c22 b18 b18 c19 score view a15 a16 b29 c25 quiz app b23 b26 c24 list view a17 b35 to do app a18 b37 b37 c37 a19 b36 a21 c33 a22 b43 c35 b38 c32 b39 c36 task detail a25 b57 c44 c47 view a26 b54 c46 to do app a27 b58 b44 c42 b51 b51 c41 b53 c43 the detail captured.
issues identified from usability testings generally provide a broad impression while those from expert reviews are sharper with more precise focus.
for instance issue a5 identified in a usability test describes that users were overwhelmed by a chaotic looking screen.
this is matched with four issues from the expert reviews b9 b10 b11 and b12 that provide more details revealing reasons why the screen appears overwhelming such as overload of content inconsistent grid layout and texts inconsistent use of capitalisation .
the venn diagram shown in figure is obtained by matching duplicate similar issues.
in the issue set of ux llm all samples were included where at least one expert identified them as actual usability issues.
the diagram shows the overlap and unique contributions of the usability testing the expert review and ux llm.
of the total issues the usability testings uncovered issues with unique to it.
the expert review pointed out including unique issues.
ux llm identified issues contributing unique insights.
the diagram shows that only issues were identified by all three methods.
there are issues that both the testings and the experts identified but ux llm did not.
the usability testing and ux llm together identified issues that the expert review missed.
similarly the expert review and ux llm together found issues that were not detected through the usability testing.
it is noteworthy that the expert review identified a greater number of distinct issues.
they break down overarching usability issues into several smaller usability issues as in fig.
venn diagram showing the overlap of usability issues identified by usability testings expert reviews and ux llm.
the example above b9 b12 .
this allows for more targeted improvements as more focused issues make resolution more actionable .
further they point out minor design issues such as a label being slightly misaligned vertically b24 or minor copy writing inconsistencies where the same feature is referred to with different words b30 .
these issues likely went unnoticed in the usability testing as they were too minor to significantly impact the user experience for an evaluator to notice but are certainly still relevant .
the intersection between ux llm and the expert review seems to contain issues that are difficult to extract from usability testings due to the absence of specific user groups or testing conditions.
for instance issue c1 refers to a text that is hard to read for users with visual impairments or when viewing in bright light .
another type of issues identified are those that could not be stimulated with the the usability testing tasks.
for example issue b38 notes that completed tasks do not visually stand out from pending tasks .
this might have gone unnoticed in the testings as the maximum number of tasks displayed on the overview was four and the problem might become significant only as the list expands.
finally looking at the issues only ux llm has identified it seems to spot issues other methods might miss by analysing code.
it identifies e.g.
problems on less common user paths such as the app performance under slow internet conditions.
for example issue c7 states when the app is fetching categories there is no visual indication to the user that data is being loaded which could lead to confusion .
this scenario was neither encountered in the usability testing nor the expert reviews as they had fast internet connections.
however it is valid forusers with a limited internet connection.
another example is issue c18 which notes the question text does not have a maximum line limit which may cause issues if the question is too long leading to the text being cut off or the layout looking cluttered .
this issue was also not encountered because the test questions were not overly long.
with different longer questions this could become a usability issue.
however ux llm did not identify broad context or navigation related issues likely since it analyses only one view at a time.
issue b30 e.g.
that involves inconsistent wording of the same feature across different screens was beyond ux llm capability.
c. perception of a development team rq3 current practices of the team the focus group started with a waterfall like approach initially creating a high fidelity prototype which then served as the foundation for development.
later the prototype was discarded and design changes were directly implemented in the app bypassing adjustments to the design files.
in addition they conducted several usability evaluations through testings and walk through engaging directly with representatives of the visually impaired community.
predicting usability issues with ux llm during the interview we briefly demonstrated ux llm to the team by using it on one of their app views.
their initial impression of its potential to enhance the user experience of an app was moderately positive with an average rating of .
.
one participant stated some issues feel a bit generic and some don t make sense since they are addressed in previous screens .
more specifically ux llm criticised the text size for being too small which was just personalised in the onboarding before.
the other participant said i appreciate the fresh perspectives it offers.
even incorrect usability issues can be valuable as they make me reevaluate design decisions referring to an issue where ux llm criticise the colour scheme as not accessible .
we examined all issues identified by ux llm within the seven main app views removing results that were clearly false positives .
the students evaluated the usefulness of these issues resulting in a positive average rating of .
.
they found some of the issues particularly insightful even uncovering bugs they had not recognised before.
one participant said the feedback on the button bug was spot on it s not something we would have thought about by ourselves .
the other acknowledged that the issues are properly phrased so that they can directly be imported as to dos on our task board .
also on some screens we assumed something is not ideal but we did not know what the problem was these issues are very helpful .
overall they felt that pre filtered results significantly improved the usefulness and appeal of ux llm.
integrating ux llm into development workflows regarding the ease of integrating ux llm the team gave it a mildly positive average rating of .
.
a developer mentioned that it would be a further burden for him to fill out ux llm required fields.
he stated i m a laid back person so it would annoy me to have to use another application beside my ide .
in addition he said i m not sure if i d regularly go throughthese points indicating that it could be tedious to go over the issues for every app view over the course of development.
the group considered having a team member consistently review the app views with ux llm and share the feedback with the rest.
one of them said personally i wouldn t mind using the tool as i being the project manager could regularly use ux llm and incorporate its findings into our task board .
although doable with their team size they recognised the challenge of adopting another task given their current time pressure.
the ux designer stated it s great to see an overview of what s available you can quickly eliminate unnecessary issues and reflect on them.
in the end it saves a lot of time as it is easier than conducting usability evaluations ourselves indicating her excitement about the tool and that she feels like she can easily filter out the false positives.
additional concerns and improvement suggestions for applying ux llm in daily work the group proposed embedding it as a plugin in the ide or in a continuous integration ci pipeline to allow for automatic operation in the background.
they emphasised that an ide plugin just makes sense as the xcode ide9already has the code and view s preview side by side .
in addition the team expressed the desire to not only identify issues but also provide solutions including alternative designs of their input image.
the designer said when it criticised the accessibility of the colours it would be nice if it could also show what colours to use instead .
interestingly they also stated a limitation that could result from the policy of the llm used.
a discussed example is when creating an app about wine as some llms might not be allowed to answer queries about alcohol.
furthermore the presentation of usability issues was relevant to the team as they found it demotivating to receive a lengthy list pointing out their product deficiencies.
also the developer was sceptical about ux llm capability to identify usability issues in features reliant on hardware interactions involving user input or external factors such as camera functions and voice input.
lastly they emphasised the importance of a holistic analysis to detect broader inconsistencies and navigation issues.
this approach would also reduce predicted usability issues which are already addressed in other sections of the app.
overall the team perspective on ux llm was positive ending the session saying it has identified issues that we overlooked and not just a few .
the tool was credited with uncovering valuable issues that had previously gone unnoticed besides conducting usability evaluation methods.
v. r elated work there is a huge body of knowledge around usability and usability engineering including a recent comprehensive review of usability for mobile apps .
the closest areas to our work are software analysis for usability user data and feedback analytics and genai for gui development.
code analysis for usability research has proposed static and dynamic code analysis to identify usability issues.
for example mathur et al.
proposed an analysis framework to check the source code of android apps against pre configured usability guidelines and validation cases e.g.
all password fields should provide an option to reveal the clear text .
similarly uxllm only uses development artefacts notably source code but focuses on ios apps and does not require pre configuration.
in a study with mobile app companies mathur et al.
found that developers often skip usability evaluation due to needed effort lack of resources and know how .
this is a main motivation for our ai assisted usability evaluation.
the authors also found their framework to be effective and helpful but developers were sceptical about the extra work to build own validation cases.
similar methods are capable of detecting predefined issue categories which can be beneficial e.g.
when evaluating accessibility requirements .
however they require initial setup and maintenance as development libraries guidelines and usage scenarios evolve over time .
as ux llm the main advantage of code analysis approaches is their applicability during the development phase to predict and prevent usability issues before such issues impact real users.
usage data analytics app usage logs deliver great value for developers as they enable analysing the user behaviour .
for example they can identify which app features are used most and focus usability evaluation on those areas .
mobile tracking aka analytics tools like firebase 10ortelemetrydeck11enable exploring runtime engagement and navigation data or tracking actions such as button presses.
however such tracking remains exploratory and raises privacy concerns depending on its implementation .
to detect issues park et al.
generated a representative real user activity model from execution traces and an expected activity model from developers execution of intended usage scenarios.
the authors matched both models to reveal discrepancies between intended and actual usage uncovering four types of usability issues unexpected action sequence unexpected gesture repeated gesture and exceeded elapsed time .
similarly jeong et al.
presented a graph based approach to identify dissimilarities in user behaviour assuming that when users face usability issues they show different behavioural patterns.
evaluating the approach on issues identified from usability testings of two android apps the authors found a correlation between areas of high behavioural dissimilarity and identified usability issues.
unlike ux llm usage data analytics often does not require access to the source code but actual users executing the app.
this provides a strong evidence about the user issues but prevents the applicability to earlier development stages e.g.
before releases.
moreover tracking user activity could raise privacy concerns and only some user segment might participate .
user feedback analysis research has shown that for many developers app store reviews represent a primary source user feedback .
these reviews enable gathering issues including on usability and help developers improve their apps enabling user driven quality assessment .
users also report issues in other feedback channels too as social media and product forums .
thus monitoring and processing feedback looking for usability related reports is an active research field that has received a strong boost with the availability of recent llms .
while certainly an important complementary source for monitoring usability issues actual user priorities and novel ideas user feedback analytics also requires releasing the app and having a significant user base whereas ux llm can be used during the development as it simulates common user knowledge coded in foundation models.
ux llm is also beneficial for less popular apps which do not have enough users for a meaningful analysis .
moreover feedback analytics requires manual tuning and interpretation to extract more valuable insights .
feedback can vary greatly in quality with up to of app store reviews consist of praise rather than constructive feedback .
generative ai for gui engineering recent advance in ai has also benefited usability research and ui design in general.
beltramelli presented pix2code which generates code from a gui image input.
this approach uses convolutional neural networks to identify distinct elements positions and poses in gui images.
it then employs recurrent neural networks and a decoder to generate executable code.
the rapid raise of generative ai brought significant improvements in how models solve the design2code task.
si et al.
.
conducted a benchmarking with state of the art models and showed that gpt turbo with vision performs best on this task compared to other models like gemini pro.
their study annotators claimed that gpt generated websites which can replace the original reference in terms of visual appearance and content in of cases.
in of cases the annotators stated that gpt even generated better websites than the original reference.
other recent approaches has suggested to generate gui images.
lee et al.
proposed a hybrid neural network to generate design layouts that meet user specified constraints e.g.
a design with one headline and two images .
in their experiments they demonstrated that the generated layouts are visually similar to real design layouts.
wei et al.
took a step further to generate the designs using ui diffuser and layoutdm a transformer based model for layouts generation.
users provide a simple prompt of what the gui should represent e.g.
a music player.
their preliminary results indicate that this could be a cost effective approach for creating mobile gui designs.
however the guis are far from being directly directly reusable need improvements and usability evaluation.
uizard12is a similar recent commercial product which generates mobile gui designs with a simple prompt.
mattisson et al.
demonstrated that uizard can significantly enhance designers efficiency boost creativity in idea generation improve communication with stakeholders.
particularly the editable designs with nested components allow for continuous customisation and adjustments e.g.
after usability evaluation.
in another recent study wei et al.
tuned a vision language model to retrieve relevant gui designs from a large constructed dataset.
this approach seems to be more effective yielding more relevant and higher quality screens highlighting the importance of the human in the loop for gui recommendation.
the works are are perhaps closest to ours are of kuang et al.
and of kocaballi .
to explore how ux evaluators use ai assistant kuang et al.
conducted a study with participants using a simulated ai assistants.
they found that participants asked the bot for five categories of information such as user actions and mental model and observed other trends.
the authors finally derive design considerations for future conversational ai assistants for ux evaluation.
in hypothetical design project kocaballi used chatgpt to generate personas simulate interviews with fictional users and create new design ideas.
the work highlights drawbacks such as forgotten information partial responses and a lack of output diversity suggesting the importance of human oversight in the design process.
to the best of our knowledge this work is the first to use generative ai for predicting actual usability issues in apps while comparing the performance with conventional usability testing and evaluation by humans.
vi.
t hreats to validity and limitations internal and external validity for the usability testing we aimed to recruit diverse participants with ages ranging from to years and various occupations.
although this represents a broad spectrum some participants have a technical affinity which could potentially limit the range of usability issues identified.
furthermore only three participants were female and three had only limited ios experience which could potentially obscure the results as unfamiliarity with platform features might be mistakenly perceived as usability issues .
in addition no participant had impairments potentially missing some issues.
replicating exact real world conditions is a perpetual challenge.
although usability tests also ours are designed to match real world conditions than laboratory tests they are never entirely the same .
some usability issues might have been overlooked due to artificial usage environment or participants motivation to use the apps with pre defined tasks.
another potential threat to internal validity is that only two ux professionals were recruited for the expert review although the evaluations were thoroughly conducted as the result details show involving additional experts might lead to more diverse perspectives on ux llm and its results.
the selection of two reference apps restricts a broader reliability of our findings.
the chosen quiz and to do app are simpler compared to the wide range of more complex available apps.
as we primarily aimed to support smaller app teams it made more sense to focus the evaluation on simpler apps.
evaluating ux llm for niche domains like ireland covid tracker would rather limit the results.
the two apps used in our study provided the best compromise between broadly used and maintained app.
clean swiftui model view viewmodel architecture to inject into ux llm.
general apps not too niche domain also to enable the usability testing .
.
nevertheless to increase the generalisability it is important to replicate our study with apps from various categories embodying varying levels of complexity and incorporating different types of user interactions .
the amount and quality of code entered into ux llm can also have a major impact on its performance.
we chose expert reviews and usability testings as comparative usability evaluation methods.
while these methods are wellestablished for a comprehensive analysis ux llm should also be compared with additional usability evaluation methods such as questionnaires or feedback usage data and code analysis techniques discussed in section v. in addition the focus group involved a small student development team working on a capstone university project app.
the insights gained do not fully represent the experiences of more diverse professional development teams working on a commercial app affecting the perceived utility and integration potential of ux llm in various development contexts.
for more in depth results it would be beneficial to have developers use the tool over time to gather more evaluation experiences.
construct validity unlike the usability testing and the expert assessment which followed strict protocols during the expert reviews we refrained from instructing the experts to follow a certain process.
a more structured review would likely increase the replicability and agreement rate.
in fact we considered using the more structured approach called heuristic expert reviews where evaluators are aided by a checklist.
however we decided for the general review form mainly to not interfere or direct the experts to a certain direction .
moreover heuristic evaluations tend to miss context specific issues .
during the expert assessment the survey had four options to categorise ux llm s usability issues usability issue no usability issue uncertain and incorrect irrelevant statement .
the appropriateness of these categories could also be questioned considering that they might not capture all possible reactions.
to mitigate this potential threat we took detailed notes of all expert comments.
furthermore during performance evaluation the recall was calculated by defining false negatives as all usability issues not identified by ux llm but found during usability testings and expert reviews.
there remains ambiguity about whether all usability issues have been uncovered with a strong likelihood that some issues have gone unnoticed.
this was highlighted by molich et al.
who demonstrated that even simple websites can contain more than a hundred usability issues.
similar to park et al.
multiple studies skipped the recall calculations entirely stating that it is nearly impossible to define a totally complete set of .
accordingly we combined issues from both expert evaluation and the testingssuggest using precision as main metric to demonstrate the method accuracy and recall rather as indicative.
to compare and match the overlap of usability issues found by the three methods we constructed table iii.
although we took special care and applied a strict guideline that only the same or sub super issues are matched other ux experts might come to slightly different matching which we encourage assessing by share our evaluation data.
only usability issues between the methods were matched.
matching issues within each method could also lead to additional insights.
finally for the studies on ux llm only openai s gpt4 turbo with vision was used.
using different llms and different prompts could significantly change the performance.
moreover during all studies we ignored the severity of issues comparing the issue severity would likely lead to additional insights but requires additional studies due to its subjective variability .
we also did not examine the determinism of ux llm results.
generating different outputs from the same input could impact performance and usefulness .
vii.
d iscussion and conclusion our results show that with fairly low effort and widelyavailable models genai can predict in source code valid usability issues that can easily be reviewed and fixed before releasing the app avoiding to dissatisfy users and compromise their experience.
this saves time and resources particularly for smaller app teams and freelancers who tend to ignore usability evaluation .
however our study also shows that relevant issues particularly identified by usability experts were missed by ux llm.
this suggests that traditional usability evaluation can certainly be boosted with genai but not completely replaced still requiring a ux expert in the loop .
usability evaluation is a rather continuous iterative subjective process .
it is thus unlikely that one method or one person can identify all usability issues .
this should be kept in mind when interpreting the rather limited recall values of ux llm calculated conservatively.
relaxing the interpretation e.g.
including issues rated undecided or confirmed by one expert would lead to higher performance values.
our qualitative analysis e.g.
from the focus group also shows that debatable issues or rather feature ideas can also be insightful for developers to re evaluate their designs.
we think that method triangulation or hybrid approaches are particularly appealing to gather various perspectives and increase evaluation effectiveness and efficiency.
ux llm seems to outperform when access to certain users is difficult e.g.
impaired users or rare scenarios and tasks are not specified section iv b .
experts are particularly able to reason about the broader pictures e.g.
issues across multiple views or inconsistent terminology which is a limitation of ux llm.
automated analysis of user feedback usage data and code discussed in section v represent additional complementary techniques.
however a central question remains unanswered how and when to effectively combine the various usability evaluation techniques.
our comparative study and the focus group contributed only preliminary insights.
this need to bere evaluated at a broader scale and including multiple apps domains programming frameworks and numerous issues.
at the time of our research there was no public datasets available with source code and actual validated usability issues from various evaluation methods.
this was a major rationale to focus our work on an in depth study creating such data for apps rather than a benchmarking different foundation models and prompt engineering techniques.
we hope that by replicating our study and augmenting our dataset with additional apps and issues studies on model tuning and advance prompt engineering including e.g.
more custom issue examples or techniques as chain of thoughts will be become possible.
this will likely lead to more precise genaiassisted usability evaluation tools and will enable meta studies of hybrid approaches.
we think that the research community should focus on this in near future.
finally integrating usability issue prediction in the ide as a usability debugger as well as suggesting potential fixes are promising directions for a higher acceptance of the tool in daily development work.
acknowledgement this work was partly supported by maibornwolff gmbh.
we thank all participants in the usability testing the ux experts as well as the participants of the focus group for their time and feedback.