an empirical study on automatically detecting ai generated source code how far are we?
hyunjae suh university of california irvine irvine ca usa hyunjas uci.edumahan tafreshipour university of california irvine irvine ca usa mtafresh uci.edujiawei li university of california irvine irvine ca usa jiawl28 uci.edu adithya bhattiprolu university of california irvine irvine ca usa abhattip uci.eduiftekhar ahmed university of california irvine irvine ca usa iftekha uci.edu abstract artificial intelligence ai techniques especially large language models llms have started gaining popularity among researchers and software developers for generating source code.
however llms have been shown to generate code with quality issues and also incurred copyright licensing infringements.
therefore detecting whether a piece of source code is written by humans or ai has become necessary.
this study first presents an empirical analysis to investigate the effectiveness of the existing ai detection tools in detecting ai generated code.
the results show that they all perform poorly and lack sufficient generalizability to be practically deployed.
then to improve the performance of ai generated code detection we propose a range of approaches including fine tuning the llms and machine learning based classification with static code metrics or code embedding generated from abstract syntax tree ast .
our best model outperforms state of the art ai generated code detector gptsniffer and achieves an f1 score of .
.
we also conduct an ablation study on our best performing model to investigate the impact of different source code features on its performance.
index terms large language model i. i ntroduction artificial intelligence ai such as machine learning techniques has been widely used to tackle software development tasks especially for the generation of source code .
more recently large language models llms that were pre trained on large and diverse data corpora have shown state of the art performance in code generation .
the generative llms such as chatgpt gemini pro and starcoder2 are able to generate code that is very similar to what a human developer would produce given the natural language specification.
while a large amount of previous research works have explored a variety of fine tuning prompting techniques to further boost model s performance in generating high quality source code numerous llm based tools i.e.
github copilot have been implemented to assist developers to design software architecture generate production code test cases and refactor the existing code base.
therefore leveraging llms for source code generation or assisting programming related tasks is becoming popular among software practitioners.however the wide adoption of llms for code generation has raised a variety of concerns among researchers and software practitioners.
researchers have questioned the evaluation process of the source code quality generated by llms and the correctnesss of the generated code could be easily impacted by the wording in the llms prompts .
in addition it s been proven that around of github copilot generated code snippets on github have security issues of various types indicating the security risks of the code generated by llms.
moreover the violations of intellectual property rights have also been found in llms such as generation of licensed code .
therefore it has become necessary to determine whether a code snippet is written by humans or generated by the llms.
while there exist a variety of automated tools i.e.
gptzero sapling and more to detect artificial intelligence generated content aigc such tools were built for detecting natural language texts and their performance in detecting ai generated source code still remains far from being perfect .
to fill this gap nguyen et al.
proposed gptsniffer by fine tuning codebert to classify a code snippet as either human written or llm generated.
however they only considered the code that is written in java programming language and is generated only by chatgpt.
it has been proven that different llms are good at generating code for different sets of coding problems namely they tend to perform differently given the same set of coding tasks.
thus gptsniffer s generalizability to code that s written in other programming languages or generated by llms other than chatgpt is not investigated.
a better detection approach for detecting ai generated source code is still missing.
in this paper we first conduct a comprehensive empirical study to evaluate the performance of existing aigc detectors for detecting ai generated source code.
the goal was twofold first as a complement to prior studies we investigate the widely adopted aigc detectors ability to detect aigenerated source code that is written in various programming languages generated by multiple popular generative llms arxiv .04299v1 nov 2024and from different domains i.e.
programming questions open source development tasks .
second we analyzed the performance of the current state of the art detector specifically for source code gptsniffer.
therefore we asked the following research questions rq1 how do existing aigc detectors perform on detecting ai generated source code?
rq2 how can we improve the performance of aigenerated source code detection?
rq3 how do the source code features captured by embeddings contribute to the overall effectiveness?
the significance of our contributions are following we show that existing aigc detectors for text perform poorly in detecting ai generated source code.
we show that the current state of the art technique for ai generated code detection gptsniffer fails to generalize effectively across different programming languages programming tasks and generative llms.
we built a variety of machine learning and llm based classifiers to detect ai generated code which outperform the compared techniques and show decent performance across multiple programming languages programming tasks and generative llms.
the remainder of this paper is organized as follows in section ii we provide related works of aigc detection and background about llm based code generation and pre trained source code embeddings.
we outlined our data collection model building and performance analysis in section iii.
next we present the evaluation results and observations in section iv.
then we discuss the implication for our study in section v. section vi shows potential threats to the validity of our approaches and findings.
finally we conclude with a summary of the findings in section vii.
ii.
r elated work b ackground a. large language models for code generation due to advancements in the field of natural language processing nlp llms have seen substantial progress in their performance and widespread use .
more recently source code has also been included to train the llms with the goal of helping with software development activities .
since these models such as codebert codet5 starcoder2 and chatgpt have been trained on vast and diverse datasets of source code and natural language from various domains they showed cutting edge effectiveness when being fine tuned or prompted to solve various downstream software engineering se tasks .
llms are widely adopted to directly generate source code given the docstring requirement in natural language code generation or complete the code for the developers based on the software context code completion .
to further ease the use of llms for code generation code completion and improve the quality of the generated code researchers have been investigating fine tuning prompting approaches to generate code of high quality that meets developers intentions .
in addition numerous software development tools for code generation code completion such as github copilot have been released and widely used by developers.
all these techniques significantly increase the chance that ai i.e.
llms writes a piece of code instead of a human developer.
in this study we specifically analyzed the code generated by gemini pro chatgpt gpt and starcoder2instruct 15b since they are among the state of the art llms and have been the subjects of many prior se studies in code generation .
we believe that this selection of llms covers not only the detectability of code generated by general llms i.e.
chatgpt gemini pro and gpt but also that of codespecific llms i.e.
starcoder2 instruct .
b. automated detection of artificial intelligence generated content aigc the emergence of generative llms such as chatgpt has surged the demand for accurately detecting aigc.
a variety of aigc detectors have been developed.
for example gptzero is a widely used commercial aigc detector while sapling generates the probability of whether each token in the input is aigc.
it reached accuracy in identifying ai generated texts.
in addition researchers and open source software practitioners have also been actively developing aigc detectors such as gpt detector detectgpt and giant language model test room gltr which achieved decent performance in detecting aigc.
however the aforementioned aigc detectors are only designed to detect ai generated natural language texts.
since source code has unique linguistic syntax and writing styles that differ from natural language these detectors may not perform well in determining whether a human or ai writes the source code snippet.
researchers have recently found that these text detectors showed limited effectiveness when detecting ai generated code .
to fill this gap nguyen et al.
proposed gptsniffer where they fine tuned codebert to classify whether a code snippet is written by ai or human.
however it could not generalize well to the data that it was not trained on section iii b .
in addition only chatgpt was queried to generate the code while java was the only programming language considered.
since there is a gap in terms of comprehensive analysis across different languages and generative llms in our study we investigated our approaches and the existing aigc detectors performance on the code generated by four widely used state of the art llms namely gemini pro chatgpt gpt and starcoder2instruct.
we also experimented with c and python in addition to java.
in order to ensure the generalizability of our approaches across different datasets we selected three widelyused code generation benchmarks mbpp humanevalx and codesearchnet .c.
pre trained source code embedding distributed numeric code representations pre trained code embeddings have been proven to be effective in various se tasks such as automated program repair vulnerability prediction and code clone detection .
various pre trained embedding models have been proposed by researchers to better capture syntactical semantic information of source code and improve downstream se tasks .
more recently researchers have started incorporating structural information i.e.
information from abstract syntax tree ast and the textual information of source code into code embeddings.
for example zhang et al.
first split each large ast into smaller statement asts and encoded these asts to numeric vectors by capturing the lexical and syntactical knowledge of statements.
they then used a bidirectional recurrent neural network rnn to leverage the statements naturalness and produce the embedding.
ding et al.
used a two step unsupervised training strategy to integrate the textual and structural information from the code to make the embeddings more generalized to different se tasks.
they found that including structural information could help improve the code embeddings quality.
since code embeddings are usually obtained by training models with large source code datasets to acquire knowledge about the semantic and syntactic meaning we posit machine learning models trained with such embeddings have the potential to perform better than models without such information when trying to differentiate human written code from ai generated code.
in this study we selected codet5 110m embedding model which shows state of the art performance on code understanding and generation tasks to generate embeddings for source code and ast to incorporate both textual and structural information.
then we trained our machine learning models with these embeddings to detect aigenerated code with the goal of achieving decent performance.
iii.
m ethodology our goal was to investigate the effectiveness of the current aigc detectors in detecting ai generated source code rq1 .
our other goal was to classify a code snippet as human written or ai generated rq2 .
finally we analyze the impact of various source code features on the performance of the bestperforming approach rq3 .
in the following subsections we detail the applied methodology.
figure shows an overview of our methodology.
a. data collection since we aimed to detect ai generated source code and evaluate the effectiveness of the current aigc detectors we targeted the code generation benchmark datasets that have been studied by previous research related to code generation .
specifically we selected three datasets namely mbpp humanevalx and codesearchnet .
mbpp contains crowdsourced python programming problems along with humanwritten python functions solving the specified problems.
fig.
overview of research method the problems range from simple numeric manipulations to tasks that require basic usage of standard library functions.
humaneval x consists of data samples function docstrings specifications and the corresponding human written code solutions in python c java javascript and go.
to make our evaluation more generalized and not restricted to human crafted coding questions we also included codesearchnet a dataset of million pairs of comment and human written code collected from publicly available opensource non fork github repositories.
in this study we aimed to investigate the generalizability of the aigc detectors and our approaches across multiple programming languages so we selected java c and python three most widely used programming languages by software practitioners .
we believe that selecting only three widely used languages instead of using all the languages available in the studied datasets would keep our experiments manageable.
it s worth pointing out that the mbpp dataset only has code written in python programming language while codesearchnet has java and python but does not include c .
to ensure there were no duplicates between different datasets we manually reviewed all specifications and code snippets.
additionally we used a clone detection tool nicad to identify any potential code clones.
consequently we found no duplicates or clones across different datasets.
to collect the ai generated counterparts for the humanwritten code in the datasets we adopted four state of theart generative llms that have been widely used in code generation literature namely chatgpt gemini pro gpt and starcoder2 instruct to generate source code based on the natural language specifications or comments in the selected datasets.
due to a limited financial budget and prohibitively expensive openai api instead of generating code for the complete codesearchnet dataset which consists of 457k python and 497k java codes we randomly sampled confidence level margin of error data instances each for python and java.
in addition the generation from the llms tends to be nondeterministic and creative when the temperature increases .
to make our evaluation more comprehensive which covers code generations with greater linguistic variety we generated multiple code snippets for each specification using the same llm with different temperatures.
to make our experiments controllable we set the temperature as andtable i collected dataset with ai generated code temperature chatgpt gemini pro gpt starcoder2 instruct mbpp python humaneval x python humaneval x java humaneval x c codesearchnet python codesearchnet java table ii collected dataset with ai generated code default temperature chatgpt gemini pro gpt starcoder2 instruct mbpp python humaneval x python humaneval x java humaneval x c codesearchnet python codesearchnet java the default value provided by respective llms for chatgpt gpt and starcoder2 instruct the default temperature is .
for gemini pro the default is .
to generate code based on the specifications.
after code generation we obtained the code generated by each selected llm.
combining the code generated by llm with the human written code which already existed in the original dataset we obtained datasets with twice the size of original one.
then we removed the data instances where the llms could not generate the source code given the specifications.
we also removed code snippets that contain syntax errors which would prevent us from extracting static code features later in section iii e. the statistics of the collected datasets are shown in table i and table ii.
b. compared aigc detectors a number of aigc detectors have been implemented to detect ai generated natural language texts.
similar to pan et al.
our goal was to investigate the effectiveness of these detectors in detecting ai generated source code and compare them with our approaches.
following their work we selected five aigc detectors gptzero gpt output detector detectgpt gltr and sapling .
we ran these detectors on human written code from our selected datasets and also on the ai generated code by the three llms with different temperatures.
more recently nguyen et al.
built a classifier named gptsniffer that specifically targeted the identification of aigenerated source code.
they fine tuned codebert with human written code and chatgpt generated code to classify a code into human written or ai generated.
in this study we include gptsniffer as a baseline to systematically evaluate its performance on ai generated code from different llms temperatures and programming languages.
c. evaluation settings and metrics similar to previous works we split each of the selected dataset using the ratio allocating for training for validation and for testing.
to prevent any data overlap between datasets that are from the same source but generated by different llms i.e.
humaneval c chatgpt humaneval c gemini pro humaneval c gpt we consistently splitted the datasets.
each of the source code has a ground truth label of either human orai representing that the code is either generated by humans or the llms.
the metrics were calculated based on the comparison between the ground truth labels and the predicted labels.
the specific metrics we used in this study include accuracy true positive rate tpr true negative rate tnr and f1 score .
in this study the positive label stands for human while the negative label represents ai.
detailed explanation of the metrics is as follows accuracy accuracy is calculated as the ratio of correct predictions to the total number of predictions.
it is calculated asaccuracy tp tn tp tn fp fn.
tp is the number of human written code predicted correctly.
tn denotes the count of correctly predicted samples of ai generated code.
we defined tn this way since its original definition corresponds to the count of correctly predicted negative class samples and in our classification scheme we set aias the negative label.
fp is the number of ai generated code incorrectly predicted as human written code and fn is the number of human written code incorrectly predicted as ai generated code.
tpr true positive rate recall represents the ratio of actual positive cases that are correctly identified as positive by the classification model.
it is calculated as tpr tp tp fn.
tnr true negative rate represents the ratio of actual negative cases that are correctly identified as negative by the classification model.
it is calculated as tnr tn tn fp.
f1 score f1 score is the harmonic mean of precision and recall where precision is calculated as precision tp tp fp.
however it can vary depending on which class i.e.
humanwritten or ai generated is designated as positive potentially leading to a misrepresentation of the model s performance.
for instance consider a scenario where the human class is designated as positive.
if the model accurately predicts all instances of human generated code but misclassifies all aigenerated code its f1 score would be zero.
thus we also calculated two variants of f1 scores by setting either human oraias the positive label.
we represented the f1 score where human is set as the positive label as human f1 score while the f1 score where aiis set as the positive label as ai f1 score .
finally with the two variants we calculated average f1 score which is the average of the two f1 score variants and is computed by taking the macro average of thehuman f1 score and ai f1 score .
it is calculated as average f human f ai f .
we believe this f1 score can better represent the overall effectiveness.
for each of our proposed techniques i.e.
models in this study we conducted the evaluation in two different settings.
first we trained the models on the training split of each of the datasets section iii a conducted hyper parameter tuning to select the model with the best performance and evaluated the model s performance on the testing split within evaluation setting .
to further evaluate the generalizability of the models across different datasets with programs in different domains and written in different programming languages we also testedthe models on the testing split of another dataset across evaluation setting .
for example we trained a model on mbpp s training data split and tested it on humanevalx s testing split.
for the baseline aigc detectors that we compared ours with we ran these detectors on the testing splits of our datasets to evaluate their performance.
d. llm based approaches since llms have shown state of the art performance on code classification tasks such as defect detection and clone detection we decided to harness the power of llms to detect ai generated code.
we used zero shot learning in context learning with retrieved demonstrations and finetuning.
in this study we selected chatgpt i.e.
gpt .5turbo as the model to perform prompting fine tuning since it is one of the llms that showed state of the art performance on a variety of se tasks .
we did not take gpt or gemini pro due to the unavailability of model fine tuning.
in this study we utilized three different representations of source code namely the textual content of the source code the ast representation by and the concatenation of textual code and ast representation.
specifically we followed the algorithm presented by guo et al.
to generate ast representation using tree sitter which starts from the root node and recursively traverses the ast appending node names with special suffixes i.e.
left right to the resulting sequence.
the models trained with this ast representation showed state of the art performance in various code understanding tasks .
in the following section we will use code only for textual code representation ast only for the ast representation by and combined for the concatenation of code only andast only .
in the zero shot learning setting we prompted chatgpt to determine whether the given source code snippet is generated by ai or human.
we followed the established best practices to design our prompt the prompt is provided in the replication package .
in the in context learning setting we provided demonstration examples from the training datasets in the prompt in addition to the zero shot setting.
following the best practices of setting in context learning examples for se tasks we used bm to retrieve four demonstration examples two instances of human written code and two instances of ai generated code from the training datasets that are the most similar to the code snippet to be predicted i.e.
test sample and we ordered the examples based on their similarity to the code snippet in ascending order.
in this study we prompted the model with one of the three code representations to analyze the models performance using different representations.
in the fine tuning setting the training data consists of one of the representations i.e.
code only ast only or combined and the ground truth labels.
openai api was called to fine tune the model i.e.
chatgpt .
in the following sections we will use fine tuned chatgpt to represent the chatgpt model that is fine tuned to detect aigenerated code with our datasets.
we evaluated the models in within evaluation setting only for the zero shot learningsetting as it does not require training data.
for in context learning and fine tuning we performed evaluation in both within evaluation setting and across evaluation setting .
e. machine learning classifiers with static code metrics despite the recent advancement in deep learning and llms shallow machine learning algorithms still showed decent performance in some se tasks given the appropriate features are provided .
in this study we also investigated the feasibility and effectiveness of a machine learning based classification approach for ai generated source code detection.
to construct the features from source code for training our machine learning models we used scitools understand to extract static source code metrics as the features since these metrics have been used in a number of previous works .
in addition we also collected features studied by aljehane et al.
which includes identifiers method and variable names names and operators in if else and while statements operators keywords arguments and method signatures.
we believe that these features also have the potential to be used to distinguish between human written and ai generated code since human developers focus on them when they review source code.
we used tree sitter to collect the code features for the three programming languages in our study.
since our datasets consist of three different languages we filtered the metrics to keep those that are commonly applicable to all three languages.
for instance python does not have semicolon so we removed all features that are related to semicolon.
in total we retained code features.
due to space constraints we provided the whole set of these features in our replication package .
in order to avoid multicollinearity across features we conducted variance inflation factor vif on all features.
vif is a statistical measure used to assess multicollinearity in regression analysis.
high vif values indicate strong multicollinearity which can lead to unstable and unreliable regression coefficients.
akinwande et al.
argued that a vif value between and indicates a high correlation that may be problematic.
thus we set our threshold value as and make vif values of our features below that value.
as a result we retained eight features.
the finalized set of features and their corresponding descriptions are provided in table iii.
we used widely used machine learning classifiers by researchers such as logistic regression lr k nearest neighbor knn multi layer perceptron mlp support vector machine svm random forests rf decision tree dt gradient boost gb and extreme gradient boost xgb .
we performed hyper parameter tuning on each trained model to optimize the performance using random grid search .
f .
machine learning classifiers with code embedding in this study we also leveraged code embeddings to capture the information in source code with the goal of better distinguishing ai generated and human written code.
we experimented with a pre trained source code embeddingtable iii collected source code features features definitions sumcyclomaticsum of cyclomatic complexity of all nested functions or methods.
avgcountlinecodeaverage number of lines contai ning source code for all nested functions or methods.
countlinecodedeclnumber of lines containing decl arative source code countdeclfunction number of functions maxnestingmaximum nesting level of if w hile for switch etc.
countlineblank number of blank lines keywordsthe ratio between the number of language keyword tokens to the number of total tokens operators in if else and while statementsthe ratio between the number of operators in if else and while sta tements to the number of total tokens model.
specifically codet5 110m embedding model was selected because it was the latest code embedding model at the time we conducted the experiments.
the generated embeddings were used as the features to train machine learning models.
in order to generate the embeddings that capture different aspects of source code we selected three code representations namely code only ast only and combined where we separated the concatenation of two representations with a special separator token following .
the goal was to explore the embeddings of various representations of source code i.e.
structural and textual information to investigate the most distinguishing representations captured by embeddings.
we used the same machine learning algorithms and steps used in section iii e including random grid search.
figure shows the overview of this approach.
fig.
overview of machine learning classifiers with embeddings furthermore to explore performance inconsistencies across various approaches we compared the similarity between aigenerated and human written code to explain classification performance.
we used semantic embeddings of code which capture the underlying meaning of the code providing a robust basis for comparison and understanding the differences between the two classes.
we computed the cosine similarity between the code embeddings of ai generated and human written code from the same specification in our dataset and averaged these similarity values.
the averaged cosine similarities between ai generated and human written code embeddings were then used to compare semantic similarity across the different llms.
this process was carried outrespectively for each of the four llms used in our study.
in addition given that discrepancies between training and testing datasets can affect classification performance we investigated these differences to understand why performance degrades in the across evaluation setting compared to the within evaluation setting.
we focused on the cosine similarity of ast only embeddings since models trained with these embeddings performed best in the within evaluation setting.
in the within setting we averaged the ast only embeddings separately for each dataset s training and testing splits.
we then measured the cosine similarity between these averaged embeddings.
we followed a similar process for the across setting averaging the ast only embeddings for the training and testing splits but comparing the cosine similarity with splits from different datasets.
we compared trainingtesting split combinations for each llm.
finally we averaged the cosine similarity values for each evaluation setting and compared the two sets of values.
g. ablation study since our embedding based machine learning models with ast only perform the best among all other approaches section iv d we conducted an ablation study to investigate the impact of different source code features on the models performance.
to do so we first took all the code features that are applicable to c java and python in section iii e. among these features we selected the ones that we can modify in the code without altering the code logic namely comment lines variable names method names and blank lines .
we eliminated blank lines from our experiments since removing blank lines does not have any impact on the ast representation of the source code.
next we established the following code variant types that do not affect the code logic based on each of the three features code with no comment line code with uniform variable names prefixed with var and numbered sequentially starting from var code with uniform method names prefixed with func and numbered sequentially starting from func in order to create the mentioned variants we used treesitter to parse the ast and change its relevant nodes.
to create code with no comment line we removed the comment andblock comment nodes from the ast.
to make the function names uniform code with uniform method names we changed the method declaration orfunction definition nodes given that each programming language has specific ast node types for function declaration definition.
some languagespecific functions such as the main method in c and java or the constructor and deconstructor methods i.e.
init in python remained untouched.
for renaming the variables code with uniform variable names we performed a similar approach as we did for code with uniform method names .
figure shows an example of code with uniform variable names .
different ast nodes were changed based on the ast structure and node types of each programming language.for instance we changed the nodes including identifier pattern list assignment typed parameter for python code while we modified local variable declaration formal parameter nodes for java and init declarator for c .
the complete implementation is available in our replication package .
after generating the code variants we produced the embeddings for the ast of these code variants.
finally we trained machine learning classifiers with these ast only embeddings of the code variants for each variant type.
then we performed welch s t test and calculated the effect size cohen s d using the average f1 scores of each variant compared to those of the original code.
fig.
example of code with uniform variable names variant iv.
r esults in this section we organized the results of this study based on our research questions in section i. due to space constraints we present the results for the default temperature value.
the result for temperature is included on the companion website .
we also include both the results for within and across evaluation settings to provide a comprehensive view of the results.
a. rq1 how do existing aigc detectors perform on detecting ai generated source code?
to answer rq1 we evaluate the performance of existing aigc detectors by running them on the testing splits of our datasets section iii c .
as mentioned in section iii five aigc detectors for ai generated natural language text gptzero gpt output detector detectgpt gltr and sapling and a state of the art ai generated source code detector gptsniffer are included as baseline approaches that we compare ours with in this study.
table iv a vg f1 stands for average f1 score shows the performance of all five aigc detectors for natural language text when the temperatures of the llms that generated the code are set as the default value for chatgpt gpt and starcoder2 instruct .
for gemini pro .
we average the values of all the metrics across all datasets whose generation llm is the same to obtain an overview of the performance.
we find that the performance of detecting code when the generation llms temperatures are set to and the default value is similar so we put the results when the temperatures of the llms are to our replication package due to space constraints.
similar to what researchers have found their accuracy is mostly less than .
indicating their ineffectiveness in detecting ai generated source code.
sincesource code has unique linguistic syntax and writing styles that are different from natural language the reasons for such low performance in detecting ai generated source code could include that these aigc detectors were trained with only natural language texts.
moreover a certain aigc detector can have a different performance in detecting code generated by different llms.
for example detectgpt tends to classify human written chatgpt generated and gemini pro generated code as aigenerated based on the high tnr and low tpr values but it does not have this issue when identifying gpt generated code.
we also find that some techniques tend to classify both human written and ai generated code as human written code such as gptzero on the datasets with code generated by gemini pro and chatgpt.
code generated by starcoder2instruct showed a relatively higher average f1 score compared to code generated by other llms across all aigc detectors.
overall all of them show limited effectiveness in detecting ai generated code.
in addition the average f1 score across different generative llms does not show a significant difference which indicates that aigc detectors perform poorly regardless of the llms used to generate the ai generated source code.
we also find a similar trend in terms of the temperature settings of the generative llms i.e .
llms used for code generation .
therefore neither the generative llms nor their temperature settings affect the capability of existing aigc detectors to detect ai generated source code.
observation existing natural language aigc detectors perform poorly in classifying human written and ai generated source code.
as for gptsniffer a state of the art aigc detector finetuned with source code we provide its performance results on all our datasets in table v. while gptsniffer has been fine tuned on chatgpt generated java code it still shows poor performance in detecting source code from different datasets containing code written in java i.e.
humaneval java and codesearchnet java other than its training data even for java code generated by chatgpt.
for example it only shows the accuracy of .
and average f1 score of .
on java code generated by chatgpt in humaneval dataset.
when it comes to other programming languages it shows limited performance on detecting python code i.e.
accuracy of .
and average f1 score of .
on mbpp dataset with code generated by chatgpt .
surprisingly it shows a superior performance in detecting ai generated code in c where it achieves accuracy of and average f1 score of on humaneval c dataset with chatgpt generated code.
the reason could be that the testing split of this dataset only contains data instances.
a relatively smaller number of instances may have led to overfitting.
for detecting code generated by llms other than chatgpt its accuracy fluctuates around .
moreover it tends to classify code as ai generated based on the high tnrtable iv performance of existing aigc detectors default temperature chatgpt gemini pro gpt starcoder2 instruct mean of every datasets acc tpr tnr a vg f1 acc tpr tnr a vg f1 acc tpr tnr a vg f1 acc tpr tnr a vg f1 acc tpr tnr a vg f1 gpt2 output detector .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
detectgpt .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
sapling .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
gptzero .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
gltr .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table v the evaluation of gptsniffer on our datasets default temperature dataset generation llm language acc tpr tnr a vg f1 mbpp chatgpt python .
.
.
.
mbpp gemini pro python .
.
.
.
mbpp gpt python .
.
.
.
mbpp starcoder2 instruct python .
.
.
.
humaneval chatgpt python .
.
.
.
humaneval chatgpt java .
.
.
.
humaneval chatgpt c .
.
.
.
humaneval gemini pro python .
.
.
.
humaneval gemini pro java .
.
.
.
humaneval gemini pro c .
.
.
.
humaneval gpt python .
.
.
.
humaneval gpt java .
.
.
.
humaneval gpt c .
.
.
.
humaneval starcoder2 instruct python .
.
.
.
humaneval starcoder2 instruct java .
.
.
.
humaneval starcoder2 instruct c .
.
.
.
codesearchnet chatgpt python .
.
.
.
codesearchnet chatgpt java .
.
.
.
codesearchnet gemini pro python .
.
.
.
codesearchnet gemini pro java .
.
.
.
codesearchnet gpt python .
.
.
.
codesearchnet gpt java .
.
.
.
codesearchnet starcoder2 instruct python .
.
.
.
codesearchnet starcoder2 instruct java .
.
.
.
average .
.
.
.
and low tpr values.
in addition we also trained and tested our best performing machine learning models section iv d on gptsniffer s data and we achieved similar performance average f1 score of .
thus we believe that gptsniffer does not show a decent performance on datasets from a different domain i.e.
open source projects in github code snippets written in programming languages other than java and generated by llms other than chatgpt which severely undermines its applicability to ai generated code detection.
observation a state of the art aigc detector for source code gptsniffer still performs poorly in classifying human written and ai generated source code.
b. rq2 how can we improve the performance of aigenerated source code detection?
llm based approaches to improve the effectiveness of detecting ai generated source code we first experiment with a variety of llmbased approaches.
in this section we evaluate the performance of chatgpt after zero shot learning in context learning and fine tuning fine tuned chatgpt where we prompted trained the model with three different representations of source code section iii d namely code only ast only and combined .
we average the values of all the metrics across all datasets whose generation llm is the same to obtain an overview of the performance as in table vi and vii for within evaluation setting and across evaluation setting respectively.
our detailed results for both evaluation settings are provided in our replication package .
for fine tuned chatgpt their accuracy and average f1 score can reach more than80 on some datasets such as datasets with chatgpt and gpt generated code in within evaluation setting which suggests that fine tuned chatgpt with our datasets can detect ai generated code significantly better than existing aigc detectors.
we also find that fine tuned chatgpt performs better in identifying ai generated code produced by llms with a high temperature than that of a low temperature.
however the fine tuned models only show around in terms of accuracy and average f1 score in across evaluation setting where the models are evaluated on the testing splits of the different datasets section iii c .
for example fine tuned chatgpt with mbpp dataset whose aigenerated python code is by gemini pro shows only in accuracy when being tested on codesearchnet dataset whose ai generated code is written in java default temperature for the generative llm .
this indicates that fine tuned chatgpt using code only as the input still lack generalizability across code snippets from different domains and different programming languages.
similar to fine tuned chatgpt oncode only fine tuned chatgpt onast only also significantly outperforms its zeroshot and in context learning counterparts.
however the finetuned chatgpt performs significantly worse when using ast only as input than that of code only .
for example the average f1 score for the fine tuned chatgpt with humanwritten and chatgpt generated code using ast only is .
when the default temperature is used for code generation while the score reaches .
for code only .
in addition fine tuned chatgpt onast only shows poor performance in across evaluation setting mean average f1 score of around across all datasets when the generative llms temperatures are and for default temperatures suggesting limited generalizability.
when we concatenate code only and ast only as input i.e.
combined we observe similar mean average f1 scores to those when code only is used as input in zero shot learning and in context learning.
for fine tuned chatgpt there is a notable performance drop compared to when code only is used as input.
for example fine tuned chatgpt oncode only outperform that of combined by more than in terms of mean average f1 scores in the default temperature setting.
thus we believe that ast representation of source code may not be suitable as the input for fine tuning chatgpt to detect ai generated code.
interestingly identifying source code generated by gemini pro presents the lowest mean average f1 score suggesting it is particularly challenging to identify the code generated by this generative llm using fine tuned chatgpt .
for example fine tuned chatgpt only has around in average f1 scoresand accuracy when detecting gemini pro generated code while it shows when detecting chatgpt gpt or starcoder2 instruct generated code when the temperature is set as .
observation fine tuned chatgpt significantly outperforms zero shot and in context learning.
in addition ast representation is not suitable as the input forfine tuned chatgpt to detect ai generated code.
c. rq2 how can we improve the performance of aigenerated source code detection?
machine learning classifiers with static code metrics we evaluated the performance of the machine learning classifiers on the testing splits of the datasets.
rf model shows the best performance when the temperature is set to while gb shows the highest mean average f1 score in the default temperature setting.
similar to llm based approaches we average the values of all the metrics across all datasets whose generation llm is the same to obtain an overview of the performance.
the results obtained under within evaluation setting are presented in table vi and the results under the across evaluation setting are presented in table vii.
the difference in accuracy and average f1 score across different generative llms shows that machine learning based detection techniques have varied effectiveness in detecting code generated by different llms.
for example when the generative llms temperatures are set to the rf classifier has more than in average f1 score and accuracy for detecting chatgpt generated code while it only has around in detecting gemini pro generated code.
however these classifiers still outperform existing aigc detectors.
we also find that our machine learning classifiers tend to perform better i.e.
higher overall mean average f1 score on detecting code generated with higher temperature i.e.
default temperature than code generated with a temperature of .
in across evaluation setting the machine learning classifiers have a mean average f1 score of around for both temperature of and default temperature which indicates the limited performance of the trained machine learning models when they are tested on different programming languages and datasets from other sources.
observation the machine learning classifiers trained with static code features can identify aigenerated code.
however they show varied effectiveness in detecting code generated by different llms.
d. rq2 how can we improve the performance of aigenerated source code detection?
machine learning classifiers with embeddings instead of using static code features as in section iv c we adopted the code embeddings generated from code only ast only and combined section iii d using codet5 as the feature vectors to train our machine learning models.
we take svm for the embedding of ast only mlp for code only andlr for combined when the generative llms temperatures are set to .
in the default temperature setting we select lr for all three representations.
we highlight the highest scores across all approaches in table vi.
our findings are machine learning models trained with the embeddings of ast only demonstrate the best performance across all the other approaches that we experimented with in this study indicating that machine learning models are able to capture the difference between ai generated and humanwritten code.
the mean average f1 score is .
when the temperature is set to and .
when the temperature is set to the default value.
thus our machine learning models trained with embeddings significantly outperform aigc detectors including gptsniffer.
however these models still show inconsistent performance in identifying ai generated code by different llms similar to llm based and machine learning models trained with static code features.
for example their accuracy reaches nearly i.e.
.
when being trained and evaluated on humanwritten and chatgpt generated code while it s only .
for gemini pro given the temperature is set as default.
our results on exploring performance inconsistencies across various approaches show that the cosine similarity between ai generated and human written code embeddings were as follows .
for chatgpt .
for gemini pro .
for gpt and .
for starcoder2 instruct.
these values indicate a high degree of semantic similarity between the code embeddings of ai generated and human written code.
consistent with our result that code generated by gemini pro was the hardest to detect the cosine similarity between embeddings of ai generated and human written code was the highest for gemini pro.
this indicates that gemini pro s code is semantically closest to human written code making it more difficult to distinguish.
in contrast we found that the cosine similarity for code generated by chatgpt and starcoder2 instruct was the lowest suggesting greater semantic differences.
this aligns with our finding that code generated by chatgpt and starcoder2 instruct was the easiest to detect.
therefore the varying semantic differences between ai generated and human written code across different llms may account for the performance inconsistencies observed in code detection.
as for the model s generalizability the results of the across evaluation setting show that it still struggles to detect ai generated code written in other programming languages and from other domains around in terms of mean average f1 score across all datasets whose generative llms temperatures are set to and for default temperature .
the results of analyzing the discrepancies of ast only embeddings between training and testing splits are shown in table viii.
on average the cosine similarity in the across evaluation setting was about lower than in the within evaluation setting .
vs. .
.
this finding suggests that dissimilarities between training and testing datasets contribute to performance degradation in the across setting.
however further in depth analysis is needed to fully understand all thefactors affecting performance in the across setting.
observation the machine learning models trained with code embeddings of ast only show the best performance among all the approaches.
however they show varied effectiveness in detecting code generated by different llms.
e. rq3 how do the source code features captured by embeddings contribute to the overall effectiveness?
ablation study in our ablation study we generate the code variants as described in section iii g and produce embeddings for these variants using codet5 .
specifically we used the bestperforming model across all the approaches we experimented with the model with the highest mean average f1 score in this ablation study as the ai generated code detection model namely the gb classifier trained with the embedding ofast only with code generated by llms in default temperature setting.
we use best model to represent this model for simplicity.
then we compare this model s performance with the performance of the gb classifiers trained on the embeddings of the code variants of the three variant types.
the goal is to investigate whether the change of the code features captured by code embeddings can have an impact on the overall effectiveness of the best model .
the results in table ix show that the variants of code with no comment line have the most impact on the performance of the best model where it decreases the mean average f1score by .
while variants of code with uniform variable names andcode with uniform method names have negligible effect.
we conducted t test and measured the effect size on theaverage f1 score of each variants against the average f1score of the best model to see if there were any statistically significant differences between the values.
for variants of code with no comment line we observed p value of .
and effect size of .
.
the results show that the impact is statistically insignificant with a small effect size.
observation removing code comments has an impact on the effectiveness of our best model.
however the impact is statistically insignificant with a small effect size.
v. d iscussion despite the fact that the llms such as chatgpt have shown state of the art performance using zero shot and incontext learning in a variety of se tasks including code summarization code refinement and android bug replay they both have relatively poor performance by showing around in average f1 scores andaccuracy across all datasets in detecting ai generated code.
thus we believe that even powerful llms such as chatgpt lack the capability to effectively identify ai generated source code if they are not fine tuned even when retrieved demonstration examples are provided in the prompt.based on our results in iv we are able to reach over in terms of accuracy and average f1 scores .
compared to existing aigc detectors and gptsniffer our models have achieved significantly better performance.
however there is still room for improvement since all our models perform poorly in the across evaluation setting where they fail to be effectively generalized to code written in programming languages other than the language of their training data and from different domains or sources.
this is not surprising as it also happens to other se tasks.
for example while there has been research improving the performance of defect prediction models for decades the task can still be challenging when the models are evaluated on data from software projects other than the project that they are trained with .
thus further research from the community is required to devise generalizable detection techniques.
for using different representations of source code to train our models code only ast only and combined there is no representation that always outperforms another across all our approaches.
code only shows the best performance in finetuning chatgpt while machine learning models trained with the embeddings of ast only achieve the highest accuracy andaverage f1 scores among all other approaches.
this may suggest the need for a multi modal representation of code for better ai generated code detection and further research is needed to explain this observation conclusively.
our models have different performances and they in general perform better in detecting code generated by chatgpt gpt and starcoder2 instruct than that of gemini pro.
some probable reasons behind this could be that gemini pro tends to generate code that resembles what humans would write or our features and embeddings could not capture the difference between gemini pro generated code and human written code.
as llms are being built and trained at an ever increasing rate with better performance more effective approaches with distinguishing features or embeddings should be implemented to detect ai generated code that s getting ever closer to human written one.
finally in our ablation study section iv e while removing comments from the source code has the most impact on the best model s performance the impact is statistically insignificant.
vi.
t hreats to validity we have taken all reasonable steps to mitigate potential threats that could hamper the validity of this study.
construct validity it is possible that the prompt we used to generate the source code with the llms might have impacted the generated code quality.
to mitigate this threat we followed the best practices of prompt design and the setting of incontext examples such as explicitly defining the persona of the llms instructing the task definition and providing necessary context information.
however we did not use any advanced prompting techniques such as chain ofthought which could potentially improve the code quality.
this omission may affect the validity of our results as thesetable vi overall performance comparison within default temperature chatgpt gemini pro gpt starcoder2 instruct overall average acc tpr tnr a vg f1 acc tpr tnr a vg f1 acc tpr tnr a vg f1 acc tpr tnr a vg f1 acc tpr tnr a vg f1 gpt .
zero shot code .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
gpt .
in context code .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
gpt .
fine tuned code .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
gpt .
zero shot ast .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
gpt .
in context ast .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
gpt .
fine tuned ast .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
gpt .
zero shot code ast .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
gpt .
in context code ast .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
gpt .
fine tuned code ast .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
code metrics gb .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
code embedding code .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
code embedding ast .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
code embedding code ast .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table vii overall performance comparison across default temperature chatgpt gemini pro gpt starcoder2 instruct overall average acc tpr tnr a vg f1 acc tpr tnr a vg f1 acc tpr tnr a vg f1 acc tpr tnr a vg f1 acc tpr tnr a vg f1 gpt .
in context code .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
gpt .
fine tuned code .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
gpt .
in context ast .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
gpt .
fine tuned ast .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
gpt .
in context code ast .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
gpt .
fine tuned code ast .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
code metrics gb .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
code embedding code .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
code embedding ast .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
code embedding code ast .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table viii comparison on the characteristics of training and testing dataset within vs. across default temperature chatgpt gemini pro gpt starcoder2 instruct overall average within across within across within across within across within across cosine similarity of ast embeddings .
.
.
.
.
.
.
.
.
.
table ix the result of ablation study chatgpt gemini pro gpt starcoder2 instruct overall average difference compared to base acc tpr tnr a vg f1 acc tpr tnr a vg f1 acc tpr tnr a vg f1 acc tpr tnr a vg f1 acc tpr tnr a vg f1 acc tpr tnr a vg f1 base ast code embedding .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
uniform method names .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
uniform variable names .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
no comment line .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
techniques might yield better quality code and influence the detection outcomes.
another concern is the potential bias arising from the specifications and code in the datasets which are collected from public platforms such as leetcode and github.
chatgpt gemini pro or starcoder2 instruct might have been trained on these datasets.
since llms are probabilistic generative models that create code based on specifications rather than retrieving it from their training datasets the risk of bias in ai generated code is likely reduced.
however this issue is more pronounced with human written code which is part of the dataset and thus more accessible for llms during their learning.
llms may therefore detect human written code more effectively than aigenerated code because they may have encountered humanwritten code during their pre training phase.
this potential bias could influence our findings.
internal validity the generation of the llms such as chatgpt or gemini pro is non deterministic especially when the temperature is set to .
this may negatively impact the reproducibility of our study.
there might have been possible mistakes in implementing the baseline aigc detectors that we compared with.
to alleviate this threat we directly used the code implementations published by the authors .
for commercial tools i.e.
gptzero and sapling we used their publicly available api.
external validity the findings in this study may only be valid for the three datasets and the three llms we selected.however the code snippets and specifications in the datasets come from a diverse range of sources.
the three llms are among the most widely used llms by software developers for code generation and they are built by different organizations i.e.
openai vs google .
moreover multiple programming languages that are widely adopted by software practitioners are considered in this study.
thus we believe the concern about the generalizability of our findings is mitigated.
vii.
c onclusion our study identified that existing aigc detectors perform poorly in detecting ai generated source code.
our results indicated that current techniques for detecting ai generated source code need to be improved underscoring the need for further research in this area.
from these findings we suggested the following three different approaches for ai generated source code detection llm based approach machine learning with code metrics and machine learning with code embeddings.
we evaluated our approaches on multiple datasets from different sources llm and languages.
among the approaches machine learning with code embeddings yielded the highest mean average f1 score of .
in within evaluation setting .
we conducted an ablation study with our best performing model to investigate the impact of different source code features on the model s performance.
our paper s findings unveil the current status of ai generated source code detection and propose pathways for improvement.