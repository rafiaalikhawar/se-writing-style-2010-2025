soen code generation by emulating software process models using large language model agents feng lin1 dong jae kim2 tse hsun peter chen1 1software performance analysis and reliability spear lab concordia university montreal canada 2depaul university chicago usa feng.lin mail.concordia.ca k dongja encs.concordia.ca peterc encs.concordia.ca abstract software process models are essential to facilitate collaboration and communication among software teams to solve complex development tasks.
inspired by these software engineering practices we present flowgen a code generation framework that emulates software process models based on multiple large language model llm agents.
we emulate three process models flowgen waterfall flowgen tdd and flowgen scrum by assigning llm agents to embody roles i.e.
requirement engineer architect developer tester and scrum master that correspond to everyday development activities and organize their communication patterns.
the agents work collaboratively using chain of thought and prompt composition with continuous selfrefinement to improve the code quality.
we use gpt3.
as our underlying llm and several baselines rawgpt codet reflexion to evaluate code generation on four benchmarks humaneval humaneval et mbpp and mbpp et.
our findings show that flowgen scrum excels compared to other process models achieving a pass of .
.
.
and .
in humaneval humaneval et mbpp and mbpp et respectively an average of improvement over rawgpt .
compared with other state of the art techniques flowgen scrum achieves a higher pass in mbpp compared to codet with both outperforming reflexion .
notably integrating codet into flowgen scrum resulted in statistically significant improvements achieving the highest pass scores.
our analysis also reveals that the development activities impacted code smell and exception handling differently with design and code review adding more exception handling and reducing code smells.
finally flowgen models maintain stable pass scores across gpt3.
versions and temperature values highlighting the effectiveness of software process models in enhancing the quality and stability of llm generated code.
index terms large language model code generation agents software process model i. i ntroduction the recent surge of large language models llms has sparked a transformative phase in programming and software engineering.
pretrained on vast repositories of code related datasets these llms have acquired a comprehensive understanding of code enabling them to excel in diverse coderelated tasks.
with tools like chatgpt or llama researchers have demonstrated the potential of llms in generating commit messages resolving merge conflicts generating tests method renaming and even facilitating log analytics .
among all development activities code generation has received much attention due to its potential to reduce devel opment costs.
as llms are becoming increasingly integral to software development various techniques have emerged in llm based code generation.
for example prompting techniques like few shot learning have been shown to improve code generation results.
in particular few shot learning coupled with few shot sampling or information retrieval augmented technique have been shown to improve code generation.
moreover one can integrate personalization in the prompt instructing llms to be domain experts in a specific field which can further improve llm responses .
such personalization techniques highlight the potential of using multiple llms working together to assist in complex software development activities.
given the complexity of software development llm agents stand out among various llm techniques.
agents are llm instances that can be customized to carry out specific tasks that replicate human workflow .
recently multi agent systems have achieved significant progress in solving complex problems in software development by emulating development roles .
metagpt introduced by hong et al.
integrated development workflow using standard operating procedures by assigning specific roles e.g.
a designer or a developer to llm agents.
dong et al.
developed self collaboration which assigns llm agents to work as distinct experts for sub tasks in software development.
qian et al.
proposed an end to end framework for software development through self communication among the agents.
despite the promising applications of llms in automating software engineering tasks it is pivotal to recognize that software development is a collaborative and multi faceted endeavor.
in practice developers and stakeholders work together following certain software process models like waterfall testdriven development tdd and scrum .
the process models help facilitate communication and collaboration to ensure the delivery of high quality products.
even though there is a common community agreement on the pros and cons of each process model the impact of adopting these process models for llm code generation tasks remains unknown.
in particular will emulating different process models impact the generated code quality in different aspects such as reliability code smell and functional correctness?
while some research has explored integrating multi agentsarxiv .15852v2 oct 2024within llm frameworks their research focus diverges from the influence of the software process model on code generations for several reasons xu et al.
do not adhere to specific process models and both dong et al.
and qian et al.
focus solely on waterfalllike models neglecting tdd and scrum which may have different impact on code generations.
importantly none of the aforementioned studies conduct a fine grain analysis of how different development activities affect code quality metrics such as code smell and reliability other than the pass score.
our study takes further steps to analyze the impacts of different agents within the process models on code generation and their influence on other code quality attributes.
this paper presents a novel multi agent llm based code generation framework named flowgen .flowgen integrates diverse prompt engineering techniques including chain ofthought prompt composition and selfrefinement with a focus on emulating the flow of development activities in various software process models.
specifically we implemented three popular process models into flowgen flowgen waterfall flowgen tdd and flowgen scrum.
each process model emulates a real world development team involving several llm agents whose roles i.e.
requirement engineer architect developer tester and scrum master correspond to common software development activities.
the agents work collaboratively to produce software artifacts and help other agents review and improve the artifacts in every activity.
we evaluate flowgen on four popular code generation benchmarks humaneval humaneval et mbpp and mbpp et .
we apply zero shot learning to avoid biases in selecting few shot samples .
to compare we also apply zero shot learning on gpt .
as our baseline rawgpt .
we repeat our experiments five times to account for variability in llm s responses and report the average value and standard deviation.
to study code quality in addition to pass we run static code checkers to detect the prevalence of code smells in the generated code.
our evaluation shows that flowgen scrum s generated code achieves the highest accuracy pass is .
for humaneval .
for humanevalet .
for mbpp and .
for mbpp et surpassing rawgpt s pass by .
to .
.
while flowgen in general is more stable than rawgpt flowgen scrum exhibits the most stable results with an average standard deviation of only .
across all benchmarks.
additionally we compare flowgen scrum with other stateof the art techniques codet and reflexion .
both flowgen scrum and codet outperform reflexion significantly in terms of pass across all benchmarks with flowgen scrum achieving a higher pass than codet in mbpp.
furthermore the integration of codet into flowgen scrum demonstrates the highest pass scores highlighting the potential of integrating other prompting techniques with flowgen for improved code generation.
we further study the impact of each development activity on code quality.
we find that removing the testing activity in the process model results in a significant decrease inpass accuracy .
to .
.
eliminating the testing activity also leads to a substantial increase in error and warning code smell densities.
we also find that the design and code review activities reduce refactor and warning code smells and improve reliability by adding more exception handling code.
nevertheless flowgen consistently outperforms rawgpt by reducing code smells and enhancing exception handling.
finally we find that the gpt model version plays a significant role in the quality of generated code and flowgen helps ensure stability across different versions of llms and temperature values.
we summarize the main contributions of this paper as follows originality we introduce a multi agent framework called flowgen incorporating software process models from real world development practice.
we integrate agents acting as requirement engineers architects developers testers and scrum masters and study how their interaction improves code generation and code quality.
technique we integrate prompt engineering techniques like chain of thought prompt composition and self refinement to facilitate interactions among the agents.
we implement three recognized process models flowgen waterfall flowgen tdd and flowgen scrum but the technique can be easily extended to emulate other process models or development practices e.g.
devops .
evaluation we conduct a fine grained evaluation on the quality of the generated code using four popular code generation benchmarks humaneval humanevalet mbpp mbpp et comparing agent interactions and their effect on both accuracy pass and other code quality metrics e.g.
smells .
we manually checked the generated code and discussed the reasons for test failures.
finally we examined how model versions and temperature settings affect code generation stability.
data availability to encourage future research in this area and facilitate replication we made our data and code publicly available online .
paper organization.
section ii discusses background and related work.
section iii provides the details of flowgen .
section iv evaluates our flowgen .
section v provides a discussion on future work.
section vi discusses threats to validity.
section vii concludes the paper.
ii.
b ackground r elated works in this section we discuss the background of software process models and llm agents.
we also discuss related work on llm based code generation.
a. background software development process.
software development processes encompass methodologies and practices that development teams use to plan design implement test and maintain software.
the primary goal of a software process is to assist the development teams in producing high quality software.
generally different software process models involve the sameset of development activities such as requirement design implementation and testing but differ in how the activities are organized.
because of the variation each software process model has its strengths and weaknesses based on the project type teams and experience .
in particular three well known and widely adopted software process models were created over the years waterfall test driven development tdd and scrum .waterfall is often used in safety critical systems where development teams must adhere to a linear path and each software development activity builds upon the previous one.
tdd and scrum are both variants of the agile development model.
compared to waterfall agile process models focus more on iterative and incremental development and adapting to change.
tdd emphasize writing tests before writing the actual code to improve software design and quality.
scrum highlights the importance of collaboration and communication in software development.
scrum prescribes for teams to break work into time boxed iterations called sprints.
during these sprints teams focus on achieving specific goals e.g.
user stories ensuring a continuous discussion among teams to handle any unexpected risks throughout the development process.
llm agents.
llm agents are artificial intelligence systems that utilize llm as their core computational engines to understand questions and generate human like responses.
llm agents can refine their responses based on feedback learn from new information and even interact with other ai agents to collaboratively solve complex tasks .
through prompting agents can be assigned different roles e.g.
a developer or a tester and provide more domain specific responses that can help improve the answer .
one vital advantage of agents is that they can be implemented to interact with external tools.
when an agent is reasoning the steps to answer a question it can match the question response with corresponding external tools or apis to construct or refine the response.
for instance an llm agent that represents a data analysis engineer can apply logical reasoning to generate corresponding sql query statements invoke the database api to get the necessary data and then answer questions based on the returned result.
when multiple llm agents are involved they can collaborate and communicate with each other.
such communication is essential for coordinating tasks sharing insights and making collective decisions.
hence defining how the agents communicate can help optimize the system s overall performance allowing agents to undertake complex projects by dividing tasks according to their domain specific skills or knowledge.
the software development process plays a crucial role in software development fundamentally involving communication among various development roles.
given the demonstrated capability of large language model llm agents to mimic domain experts in specific fields this study leverages llm agents to represent diverse development roles and conduct their associated duties.
our research establishes a collaborative team of llm agents designed to emulate these process models and roles aiming to enhance code generation.b.
related works code generation is a thriving field of research because of its potential to reduce development costs.
in particular promptbased andagent based code generation techniques are two of the most prevalent directions.
prompt based code generation.
prompt based code generation employs a range of techniques to refine prompts ultimately leading to the generation of expected code.
for example li et al.
propose using structured prompts containing code information e.g.
branch and loop structures to improve the generated code.
nashid et al.
retrieval code demos similar to the given task and include them in the prompt to improve code generation.
ruiz et al.
use translation techniques for program repair where buggy programs are first translated into natural language or other programming languages.
the translated code is used as a prompt to generate new fixed code with the same feature.
sch afer et al.
iteratively refine prompts based on feedback received from interpreters or test execution results.
kang et al.
provide specific instructions test method signature and bug report as part of the prompt for generating test code to reproduce bugs.
xie et al.
parse the code to identify the focal method and related code context which are given in the prompt for test code generation.
yuan et al.
apply a prompt composition technique by first asking an llm to provide a high level description of a method and then the description is used as part of the prompt to enhance test code generation.
chen et al.
introduced codet a framework that employs self generated tests to evaluate the quality of generated code.
shinn et al.
presented reflexion which utilizes an evaluator llm to provide feedback for enhancing future decision making processes.
agent based code generation.
agent based code generation emphasizes on the importance of role definition and communication among multiple llm agents.
some approaches incorporate external tools as agents.
for example huang et al.
introduce the test executor agent employing a python interpreter to provide test logs for llms.
zhong et al.
introduces a debugger agent that utilizes a static analysis tool to build control flow graph information guiding llms in locating bugs.
meanwhile other studies task llms as agents by emulating diverse human roles including analysts engineers testers project managers chief technology officers ctos etc.
nevertheless these studies miss key roles in the development activities e.g.
only has analysts coders and testers or focus more on the business side of the roles e.g.
employ cto and ceo .
in our work we try to follow the waterfall model that is proposed in the software engineering literature and create agents that correspond to every development activity.
these approaches follow the waterfall model to communicate among these roles with variation in the prompts and roles ultimately improving code generation.
in comparison our research leverages llm agents to emulate multiple software development process models while prior research focuses only on the waterfall model .
we implement several prompting techniques but more importantly we emphasize on how various process models and the associated development activities affect the generated code.
different from prior works which only study functional correctness we study several additional dimensions of code quality including code design code smell convention issues and reliability.
we also explore why the generated code fails the tests and the sensitivity of the results across llm model versions and temperature values.
iii.
m ethodology we propose flowgen an agent based code generation technique based on emulating different software processes.
figure shows the overview of flowgen define the roles and their responsibilities use llm agents to represent these roles and complete the interactions among these agents according to the software process models.
in each development activity we implement chain of thought and selfrefinement to improve the quality of the generated artifacts.
in particular we study and compare three software process models waterfall tdd and scrum .
nevertheless flowgen can be easily adapted to different process models.
we use zero shot learning in all our experiments to avoid biases in selecting data samples.
below we discuss flowgen in detail.
a. using llm agents to represent different development roles in software process models inflowgen we create llm agents who are responsible for the main development activities requirement design implementation and testing.
hence to emulate a software process model we reorganize the communication and interaction among different agents.
the benefit of such a design is that it maximizes the extensibility and reusability of the agents and flowgen can be easily adapted to different process models.
we implement four agents whose role corresponds to the common development activities requirement engineer architect developer and tester .
for scrum we introduce an additional role scrum master .
we designed these roles to use the same prompt template across different process models with different terms such as user stories v.s.
requirement to investigate the effectiveness of process models on code generation.
the exact words that we used for the prompts can be found online .
the role specific details of our prompt are role you are a responsible for instruction according to the context please role specific instruction context in this prompt template inspired by metagpt and self collaboration role refers to one of the roles e.g.
requirement engineer that corresponds to the development activity and task describes the duties for the role e.g.
analyze and create requirement documents .
instruction leverages chain of thought reasoning and refers to role specific instruction listed in steps such as analyzingthe requirement and writing the requirement documentation.
finally context contains the programming question the agent conversation history or the agent generated artifacts.
context includes all necessary information that helps the agents to make a next step decision based on the current conversation and generated results.
table i shows the tasks instructions and contexts for every development role.
in general every role takes the output from the prior development activities as input i.e.
context .
for example an architect writes a design document based on the requirement document generated by a requirement engineer.
we design developers and testers to have multiple tasks.
developers are responsible for writing code and fixing improving the code based on suggestions.
we design testers using a prompt composition technique which is shown to improve the llm generated result .
first testers design a test.
then testers write and execute the tests based on the design.
on average flowgen generates four tests for each problem before the review meeting and six after the meeting.
it is important to note that oracles are kept aside and never used in the code generation process.
finally testers generate a test failure report.
developers receive the test failure report to fix the code.
in addition to the tasks described in table i all roles have one common task which is to provide feedback to other roles for further improvement e.g.
for code review .
b. communications among agents one of the most important aspects of llm agents is how the agents communicate.
a recent survey paper shows that one common communication pattern is sequential i.e.
ordered where one agent communicates to the next in a fixed order.
another pattern is disordered where multiple agents participate in the conversation.
each agent gets the context separately and outputs the response in a shared buffer.
then the responses can be summarized and used in the next decisionmaking process.
based on the software process models and the two aforementioned communication patterns we implement three interaction models for the agents flowgen waterfall flowgen tdd and flowgen scrum figure .
the details of our multi agent communication history are available online .
flowgen waterfall flowgen waterfall follows the waterfall model and implements an ordered communication among the agents.
given a programming problem the problem goes through the requirement analysis design implementation and testing.
one thing to note is that the test result from our generated tests is redirected to the developer agent in our implementation of flowgen waterfall so developers can fix and improve the code.
flowgen tdd in the design of flowgen tdd we follow the ordered communication pattern and organize the development activities so that testing happens after design and before implementation.
once the tests are written the developer agent considers the test design when implementing the code.
when the implementationflowgen waterfal l flowgen tdd flowgen scru marchitect testerrequirement requirement engineerrequirement documentssuggests generatereviewtest execute bug fix software developertest script reportcode bug fixreview test failure report requirement engineer software architectsoftware developer testermeeting recordsscrum mastertask lists or suggestionssummarizes reviews review discussionselfrefinementroles software artifacts scrum mastertask lists suggestionsmeeting records requirement engineerrequirement documentuser requirement software architectdesign documentrequirement document software developercoderequirement design document tester test cases scripts reportrequirement design document developer testerdesign software architectdesign documentssuggests generatereviewarchitect testerimplement software develope rcodesuggests generatereviewrequirement engineer testertest design tester test casessuggests generatereviewtester generate architect testerrequirement requirement engineerrequirement documentssuggests generatereviewtest execute bug fix software developertest script reportcode bug fixreview test failure reportdeveloper testerdesign software architectdesign documentssuggests generatereviewrequirement engineer testertest design tester test casessuggests generatereviewarchitect testerimplement software developercodesuggests generatereviewtester generate architect testerrequirement requirement engineerrequirement documentssuggests generatereviewtest execute bug fix software developertest script reportcode bug fixreview test failure reportdeveloper testerdesign software architectdesign documentssuggests generatereviewrequirement engineer testertest design tester test casessuggests generatereviewarchitect testerimplement software developercodesuggests generatereviewtester generatesprintsprint meetingfig.
an overview of flowgen waterfall flowgen tdd and flowgen scrum.
table i tasks instructions and corresponding contexts that are used for constructing the prompts for the development roles.
role task instruction context requirement engineer analyze and generate requirement documentation from the context.
analyze the requirement and write a requirement document.
programming problem description.
architect design the overall structure and highlevel components of the software.
read the context documents and write the design document.
the design should be high level and focus on guiding the developer in writing code.requirement document.
developer write code in python that meets the requirements.
read the context documents and write the code.
ensure that the code you write is efficient readable and follows best practices.requirement and design documents.
fix the code so that it meets the requirements.
read the test failure reports and code suggestions from the context and rewrite the code.original code test failure report and suggestions for improvement.
tester design tests to ensure the software satisfies feature needs and quality.
read context documents and design test cases.
requirement and design documents.
write a python test script using the unittest framework.
read the context documents write a python test script and follow the input and output given by the requirement.test case design and requirement documents.
write a test failure report.
read the test execution result and analyze and generate a test failure report.test execution result.
scrum master summarize and break down the discussion into a task list for the scrum team.
read and understand the context and define the tasks for development roles.meeting discussion.
is finished we execute the tests.
if a test fails the developer agent is asked to examine the code and resolve the issue.
flowgen scrum compared to waterfall and tdd scrum involves one additional role the scrum master .
there are also additional sprint meetings among the agents.
note that different from flowgen waterfall we use the agile terminologies in the prompt e.g.
we use user story instead of requirement document when implementing flowgen scrum.
we follow a disordered communication pattern in the design of flowgen scrum because in sprint meetings every development role can provide their opinion e.g.
to simulate the planning poker process .every development role except the scrum master reads the common context e.g.
description of the programming problem from a common buffer.
then every role provides a discussion comment and is saved back in the buffer.
therefore every role is aware of all the comments.
then the scrum master summarizes the entire discussion and derives a list of user stories for each development role.
during the sprint similarly to waterfall and tdd the four development roles carry out the development activities in sequence.
at the end of the sprint the agents will start another sprint meeting to discuss the next steps such as releasing the code or needing to fix the code because of test failures.
self refinement we implement self refinement which tries to refine the llm generated result through iterative feedback to further improve the generated artifacts from every development activity.
in all three variations of flowgen we assign other agents to review the generated artifacts for every development activity and provide improvement suggestions.
we assign the agents from both the downstream activity and the tester to examine the generated artifacts and provide suggestions.
the suggestions are then considered for the re generation of the artifacts.
we include the tester in every development activity to emulate the devtestops practice where testers are involved in all development activities and provide feedback on the quality aspects.
for example once a requirement engineer generates a requirement document both the architect and tester would read the document and provide suggestions for improvement.
then the requirement engineer will re generate the requirement document based on the previously generated document and suggestions.
at the development and testing activity the tester will generate a test failure report if any of the llm generated tests fail or if the code cannot be executed e.g.
due to syntax error .
the test failure report is then given to the developer for bug fixing.
we repeat the process ttimes to self refine the generated code.
in our implementation we currently set t .
if the code still cannot pass the test we repeat the entire software development process.
c. implementation and experiment settings environment.
we use gpt3.
version gpt .
turbo as our underlying llm due to its popularity and wide usage in code generation research .
we leverage openai s apis version .
.
to interact with gpt.
we send prompts using json format and send all the conversation history as part of the prompt .
we set the temperature value to .
and explore the effect of the temperature value in rq3.
we implemented flowgen using python .
.
benchmark datasets.
we follow prior studies and evaluate the code generation result using four benchmarks humaneval humaneval et mbpp mostly basic python programming and mbpp et.
these benchmarks contain both the programming problems and tests for evaluation.
given a programming problem we consider that a generated code snippet is correct if it can pass all the provided tests.
humaneval has programming problems and mbpp has programming problems we use the sanitized version released by the original authors and three test cases for each problem.
we also use the dataset published by dong et al.
where they use the same problems as humaneval and mbpp but offer stronger evaluation test cases around test cases for each problem called humaneval et and mbpp et .
all these benchmarks use python as the programming language.
each programming problem contains the input pairs method signature method description invoke examples and expect the code as output.
evaluation metric.
to evaluate the quality of the generated code we use the pass k metric .
pass k evaluatestable ii average and standard deviation of the pass accuracy across five runs with the best pass marked in bold.
the numbers in the parentheses show the percentage difference compared to rawgpt .
statistically significant differences are marked with a .
humaneval humaneval et mbpp mbpp et rawgpt .
.
.
.
.
.
.
.
flowgen waterfall .
.
.
.
.
.
.
.
.
.
.
.
flowgen tdd .
.
.
.
.
.
.
.
.
.
.
.
flowgen scrum .
.
.
.
.
.
.
.
.
.
.
.
the first k generated code s functional accuracy i.e.
whether the generated code can pass all the test cases .
in this work we set k to evaluate if the first generated code can pass all the provided test cases.
a pass of means of the generated code can pass all the tests in the first attempt.
we use pass because it is a stricter criterion reflecting situations where developers do not have the groundtruth for automatically evaluating multiple attempts.
iv.
r esults we evaluate flowgen with four research questions rqs .
rq1 what is the code generation accuracy of flowgen?
motivation.
in this rq we emulate the three process models using llm agents and compare their results on code generations.
such results may provide invaluable evidence for future researchers seeking to optimize process models for code generation within their specific business domain.
approach.
as a baseline for comparison we directly give the programming problems to chatgpt which we refer to as rawgpt .
although prior works show that few shot learning can improve the results from llms they can be biased on how the few shot samples are selected .. hence we use zero shot learning in our experiment.
to control for randomness in the experiment we ensure all these experiments use the same temperature value t .
and the same model version gpt .
turbo .
finally we repeat each flowgen five times and report the average pass and standard deviation across the runs.
we also conduct a student s t test to study if flowgen s results are statistically significantly different from rawgpt .
result.
flowgen scrum shows a consistent improvement over rawgpt achieving .
to .
improvement in pass .
table ii shows the pass accuracy of flowgen across different process models on the benchmark datasets studied.
as shown in the table ii for humaneval and humanevalet all of the studied process models have .
to even .
improvement in pass compared to rawgpt and the improvements are all statistically significant p .
.
for mbpp and mbpp et flowgen scrum also has statistically significant improvements of .
to .
even though we see a slight decrease when adopting flowgen waterfall and flowgen tddto mbpp and mbpp et.
despite slight variations in code generation responses form llm across executions we find stable standard deviationstable iii flowgen test failure categorization.
failure types are generated from python interpreter .
darker red indicates higher percentages of the failure categories in the generated code across the models.
percentages are calculated by the ratio of specific failure types to the total number of failed tests across different process models.
failure categories benchmarks model assertion syntax name type index value recursion attribute total humaneval rawgpt waterfall tdd scrum humaneval et rawgpt waterfall tdd scrum mbpp rawgpt waterfall tdd scrum mbpp et rawgpt waterfall tdd scrum of pass ranging from .
to .
across all process models and benchmarks .
in particular flowgen scrum has the lowest standard deviation .
to .
an average of .
while rawgpt has the highest standard deviation .
to .
an average of .
following flowgen scrum flowgen waterfall has the second highest standard deviation with flowgen tdd is ranking third.
in conclusion although the models generally have consistent pass across runs flowgen scrum consistently produces the most stable results.
there are potential issues in the tests provided by the benchmarks which may hinder the pass of flowgen.
table iii provides a breakdown of failure types from the python interpreter across various process models and benchmarks.
for example indexerror happens when the generated code does not handle an out of bound index causing an exception to be thrown.
while we repeat our experiment times the standard deviation across runs is low hence we represent test failure from only one of the runs.
aligned with the findings from table ii flowgen scrum has the lowest assertionerror compared to other models i.e.
higher pass rate .
we also notice that syntaxerror is more evident inrawgpt as expected due to the absence of a code review and testing process.
however there are still higher test failures inflowgen waterfall andflowgen tddcaused by increased occurrences of valueerror typeerror indexerror and nameerror for mbpp and mbpp et as seen in table iii.
upon manual investigation of prevalent test failures types we discover that flowgen waterfall andflowgen tddintroduce various validations and enforce programming naming conventions in the generated code which may help improve code quality but cause tests to fail .
for example listing depicts the provided tests and the generated code for mbpp582 of which the objective is to write a function to check if a dictionary is empty or not .
while rawgpt passed theprovided tests flowgen waterfall andflowgen tddfailed.
this failure is because the generated code contains strict input validation to check that the input should be of type dict.
however the mbpp provided test uses an input of the type set which fails the validation causing a typeerror exception.
moreover flowgen waterfall andflowgen tddenforce the common naming convention format and more meaningful function name e.g.
my dict v.s.is dict empty both of which causes nameerror exception due to wrong function declaration causing test failure.
more interestingly we find that such code standardization may also be misled by the requirement provided by the benchmark itself.
for example the mbpp requirement specifies expected input as dict yet provides a set type as the input to the test.
the llm code generation indeed captures this correct requirement by validating that input must be of type dict .
such inconsistency in the benchmark may reduce the pass .
mbpp check if a dictionary is empty mbpp test case 3def test assert my dict false is a set not a dict rawgpt s answer 6def my dict dict1 return len dict1 waterfall tdd s answer enforces the input type must be a dictionary 9def is dict empty input dict function name is renamed from my dict if not isinstance input dict dict raise typeerror input is not a dictionary return true if not input dict else false listing mbpp test failure due to strict input validation andwrong function name .
in mbpp listing test cases provided by mbppet change the return value from a boolean as is the case in mbpp to the string match!
.
moreover in mbpp797 mbpp et s test capitalized the last word in uppercase range v.s.
r ange .
such non standard evaluation leads to unfair test results leads to failure which may bias the experimental results for mbpp et.
such bias suggests that the decrease in pass rates for mbpp et is not solely due to an increase in the number of provided tests.
example1 changed return type from boolean to string 2assert text starta endb aabbbb mbpp 3assert text starta endb aabbbb match!
mbpp et example2 capitalized the last character in function name 5assert sum in range mbpp 6assert sum in range mbpp et listing mbpp mbpp test failure due to irregularity in test cases.
fixing the issues largely improve flowgen s pass to improvement in humaneval to in humaneval et to in mbpp and to in mbpp et.
namely flowgen s pass can achieve over to across all benchmarks.
even though we also observe improvement in rawgpt s pass the three flowgen models had greater improvement.
on average the three models have a pass that is .
and better than rawgpt in humaneval humaneval et mbpp and mbpp et respectively.the results underscore the deficiencies in the benchmarks suggesting that the current pass score of flowgen could represent a lower bound.
these preliminary findings highlight the potential of flowgen and suggest that future research should improve the benchmarks by incorporating input checking or consistent naming convention into the tests and subsequently re evaluate existing code generation techniques.
flowgen scrum achieves the best results with a pass that is .
to .
better than rawgpt .flowgen scrum also has the most stable results average standard decision of .
across all benchmarks among all models.
notably while flowgen waterfall andflowgen tddenhance code quality such improvements may result in test failures.
rq2 how do different development activities impact the quality of the generated code?
motivation.
as observed in rq1 various process models can indeed affect the functional correctness pass of the generated code.
however it is equally crucial to understand code quality issues such as code smells and the impact of a development activity on the generated code.
this understanding is essential for assessing whether the generated code adheres to industry best practices.
moreover such insight may offer valuable opportunities for enhancing the design readability and maintainability in auto generated code.
approach.
to study the impact of each development activity on code quality we remove each activity separately and reexecute flowgen .
for example we first remove the requirement activity in flowgen waterfall and execute flowgen waterfall .
then we add the requirement activity back and remove the design activity.
we repeat the same process for every development activity.
note that we cannot remove the coding activity since our goal is code generation.
hence we removed code review at the end of the coding activity.
code quality considers numerous facets beyond mere functional correctness .
other factors such as code smells maintainability and readability are also related to code quality.
hence to gain a comprehensive understanding of how code quality changes we apply a static code analyzer to detect code smells in the generated code and study code reliability by analyzing the exception handling code.
to study code smell design and readability we apply pylint .
.
a python static code analyzer on the generated code.
pylint classifies the detected code smells into different categories such as error warning convention and refactor .
we study how the number of detected code smells in each category changes when removing an activity.
since the generated code may have different lengths we report the density of the code smells in each category.
we calculate the code smell density as the total number of code smell instances in a category e.g.
error divided by the total lines of code.
to study reliability we calculate the density of handled exceptions total number of handled exceptions divided by the total lines of code since exceptions are one of the most important mechanisms to detect and recover from errors .for better visualization we present the density results as per every lines of code.
we also ensure reliability of our results by repeating all of the aforementioned approach three times.
result.
testing has the largest impact on the functional correctness of the code while other development activities only have small impacts.
removing testing causes pass to decrease by .
to .
.
table iv presents changes in pass and the densities of code smell and handled exceptions.
we show the results for humaneval and mbpp because they share the same programming problems and generated code with the other two benchmarks.
among all development activities testing has the largest impact on pass where removing testing causes a large decrease in pass .
to .
decrease .
the finding implies that llm s generated tests are effective in improving the functional correctness of code.
in both benchmarks removing sprint meetings in flowgen scrum also causes pass to drop.
however removing other activities only has a small and inconsistent effect on pass .
for example in humaneval removing requirement design and code review generally causes pass to decrease except for flowgen scrum but removing these activities improves pass in mbpp.
in other words most development activities do not significantly contribute to the functional correctness of the generated code.
as shown in table iv eliminating test activities significantly boosts error andwarning smell densities by an average of .
and .
respectively.
omitting design raises refactor smell density by an average of .
and skipping code review leads to a .
average increase in warning density.
however removing other development activities shows either a small or inconsistent impact.
we also find some differences in the artifacts generated by different roles.
for example although both roles generate documents requirement engineers specify the acceptance criteria while architects address time space complexity.
in short the findings show that adding design testing and code review can help reduce the density of code smell in the generated code .
having design and code review activities significantly improves reliability by increasing the density of handled exceptions while other development activities only have small or no impacts.
removing design and code review activities separately causes the handled exception density to decrease from .
to .
and .
to .
respectively.
namely these two activities add exception handling in the generated code which may help improve reliability.
removing other activities shows a mixed relationship with the density of the handled exception.
for example removing testing in flowgen tddcauses an increase of handled exception density by .
in mbpp i.e.
testing removes exception handling code while causing a decrease of .
in humaneval i.e.
testing adds exception handling code .
while the effect of each development may be related to the nature of the benchmarks our findings show that in both benchmarks adding design and code review activities can help improve code reliability by handling more exceptions in the generated code .
flowgen shows consistent improvement over rawgpt intable iv pass and error warning convention refactor handled exception density per lines of code in the full flowgen with all the development activities and after removing a development activity.
a lower error warning convention refactor is preferred and a higher handled exception is preferred.
darker red indicates a larger decrease in percentages while darker green indicates a larger increase in percentages.
model dev.
activitieshumaneval mbpp pass error warning convention refactorhandledpass error warning convention refactorhandled exception exception rawgpt .
.
.
.
.
.
.
.
.
.
.
.
flowgen waterfallfull .
.
.
.
.
.
.
.
.
.
.
.
rm requirement .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rm design .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rm codereview .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rm test .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
flowgen tddfull .
.
.
.
.
.
.
.
.
.
.
.
rm requirement .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rm design .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rm codereview .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rm test .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
flowgen scrumfull .
.
.
.
.
.
.
.
.
.
.
.
rm requirement .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rm design .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rm codereview .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rm test .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rm sprintmeeting .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
the quality of the generated code decreasing the density of error warning convention refactor code smells .
to .
while significantly increasing handled exception density.
the code generated by rawgpt has higher error warning convention refactor code smell densities than that offlowgen waterfall flowgen tdd and flowgen scrum.
this finding shows all three models improve the quality of the generated code to different degrees.
specifically compared to rawgpt flowgen decreases the error code smell density by .
to .
warning density by .
to .
convention density by .
to .
and refactor density by .
to .
.
meanwhile rawgpt has fewer handled exceptions than flowgen .
as table iv shows in both humaneval and mbpp rawgpt has almost zero handled exception while flowgen waterfall generates the most handled exception .
and .
handled exceptions per every loc in the two benchmarks flowgen tdd ranks second .
and .
and then flowgen scrum .
and .
.
in short flowgen improves the quality of the generated code by reducing code smells while adding more exception handling code.
compared to rawgpt flowgen remarkably improves the quality of the generated code by reducing code smells and adding more exception handling.
testing has the most significant impact on pass and code smells among all development activities while having design and code review greatly improve the exception handling ability.
rq3 how stable is the flowgen generated code?
motivation.
in llm the stability of generated responses can be influenced by several parameters temperature affecting the randomness in the generated responses and model versions which may introduce variability due to changes in optimization and fine tuning .
understanding and improving uni00000016 uni00000013 uni00000014 uni00000019 uni00000014 uni00000016 uni00000014 uni00000014 uni00000013 uni00000019 uni00000014 uni00000015 uni00000018 uni00000030 uni00000052 uni00000047 uni00000048 uni0000004f uni00000003 uni00000039 uni00000048 uni00000055 uni00000056 uni0000004c uni00000052 uni00000051 uni00000003 uni0000002b uni00000058 uni00000050 uni00000044 uni00000051 uni00000048 uni00000059 uni00000044 uni0000004f uni00000015 uni00000013 uni00000017 uni00000013 uni00000019 uni00000013 uni0000001b uni00000013 uni00000033 uni00000044 uni00000056 uni00000056 uni00000023 uni00000014 uni00000016 uni00000013 uni00000014 uni00000019 uni00000014 uni00000016 uni00000014 uni00000014 uni00000013 uni00000019 uni00000014 uni00000015 uni00000018 uni00000030 uni00000052 uni00000047 uni00000048 uni0000004f uni00000003 uni00000039 uni00000048 uni00000055 uni00000056 uni0000004c uni00000052 uni00000051 uni00000003 uni00000030 uni00000045 uni00000053 uni00000053 uni00000015 uni00000013 uni00000017 uni00000013 uni00000019 uni00000013 uni0000001b uni00000013 uni00000016 uni00000013 uni00000014 uni00000019 uni00000014 uni00000016 uni00000014 uni00000014 uni00000013 uni00000019 uni00000014 uni00000015 uni00000018 uni00000030 uni00000052 uni00000047 uni00000048 uni0000004f uni00000003 uni00000039 uni00000048 uni00000055 uni00000056 uni0000004c uni00000052 uni00000051 uni00000003 uni0000002b uni00000058 uni00000050 uni00000044 uni00000051 uni00000048 uni00000059 uni00000044 uni0000004f uni00000010 uni00000048 uni00000057 uni00000014 uni00000013 uni00000015 uni00000013 uni00000016 uni00000013 uni00000017 uni00000013 uni00000018 uni00000013 uni00000019 uni00000013 uni00000033 uni00000044 uni00000056 uni00000056 uni00000023 uni00000014 uni00000016 uni00000013 uni00000014 uni00000019 uni00000014 uni00000016 uni00000014 uni00000014 uni00000013 uni00000019 uni00000014 uni00000015 uni00000018 uni00000030 uni00000052 uni00000047 uni00000048 uni0000004f uni00000003 uni00000039 uni00000048 uni00000055 uni00000056 uni0000004c uni00000052 uni00000051 uni00000003 uni00000030 uni00000045 uni00000053 uni00000053 uni00000010 uni00000048 uni00000057 uni00000014 uni00000013 uni00000015 uni00000013 uni00000016 uni00000013 uni00000017 uni00000013 uni00000018 uni00000013 uni00000035 uni00000044 uni0000005a uni0000002a uni00000033 uni00000037 uni0000003a uni00000044 uni00000057 uni00000048 uni00000055 uni00000029 uni00000044 uni0000004f uni0000004f uni00000037 uni00000048 uni00000056 uni00000057 uni00000003 uni00000027 uni00000055 uni0000004c uni00000059 uni00000048 uni00000051 uni00000036 uni00000046 uni00000055 uni00000058 uni00000050fig.
pass across gpt3.
versions.
the stability of llms is crucial for enhancing their trustworthiness thereby facilitating their adoption in practice.
therefore in this research question rq we investigate the stability of ourflowgen in pass across four benchmarks considering various temperature values and model versions.
approach.
we evaluate the pass of flowgen across four versions of gpt3.
turbo turbo turbo and turbo .
the latest version is turbo published in january and the earlier version is turbo published in march .
to avoid the effect of the model version when we vary the temperature we use the same model version turbo the version that we used in prior rqs to study the effect of temperature values.
we set the temperature to .
.
.
and .
in our experiment.
we execute rawgpt and the three variants of flowgen three times under each configuration and report the average pass .
result.
rawgpt has extremely low pass in some versions uni00000013 uni00000011 uni00000015 uni00000013 uni00000011 uni00000017 uni00000013 uni00000011 uni00000019 uni00000013 uni00000011 uni0000001b uni00000037 uni00000048 uni00000050 uni00000053 uni00000048 uni00000055 uni00000044 uni00000057 uni00000058 uni00000055 uni00000048 uni00000003 uni00000039 uni00000048 uni00000055 uni00000056 uni0000004c uni00000052 uni00000051 uni00000003 uni0000002b uni00000058 uni00000050 uni00000044 uni00000051 uni00000048 uni00000059 uni00000044 uni0000004f uni00000019 uni00000018 uni0000001a uni00000013 uni0000001a uni00000018 uni0000001b uni00000013 uni00000033 uni00000044 uni00000056 uni00000056 uni00000023 uni00000014 uni00000013 uni00000011 uni00000015 uni00000013 uni00000011 uni00000017 uni00000013 uni00000011 uni00000019 uni00000013 uni00000011 uni0000001b uni00000037 uni00000048 uni00000050 uni00000053 uni00000048 uni00000055 uni00000044 uni00000057 uni00000058 uni00000055 uni00000048 uni00000003 uni00000039 uni00000048 uni00000055 uni00000056 uni0000004c uni00000052 uni00000051 uni00000003 uni00000030 uni00000045 uni00000053 uni00000053 uni0000001a uni00000019 uni0000001a uni0000001b uni0000001b uni00000013 uni0000001b uni00000015 uni0000001b uni00000017 uni00000013 uni00000011 uni00000015 uni00000013 uni00000011 uni00000017 uni00000013 uni00000011 uni00000019 uni00000013 uni00000011 uni0000001b uni00000037 uni00000048 uni00000050 uni00000053 uni00000048 uni00000055 uni00000044 uni00000057 uni00000058 uni00000055 uni00000048 uni00000003 uni00000039 uni00000048 uni00000055 uni00000056 uni0000004c uni00000052 uni00000051 uni00000003 uni0000002b uni00000058 uni00000050 uni00000044 uni00000051 uni00000048 uni00000059 uni00000044 uni0000004f uni00000010 uni00000048 uni00000057 uni00000018 uni00000013 uni00000018 uni00000018 uni00000019 uni00000013 uni00000019 uni00000018 uni00000033 uni00000044 uni00000056 uni00000056 uni00000023 uni00000014 uni00000013 uni00000011 uni00000015 uni00000013 uni00000011 uni00000017 uni00000013 uni00000011 uni00000019 uni00000013 uni00000011 uni0000001b uni00000037 uni00000048 uni00000050 uni00000053 uni00000048 uni00000055 uni00000044 uni00000057 uni00000058 uni00000055 uni00000048 uni00000003 uni00000039 uni00000048 uni00000055 uni00000056 uni0000004c uni00000052 uni00000051 uni00000003 uni00000030 uni00000045 uni00000053 uni00000053 uni00000010 uni00000048 uni00000057 uni00000018 uni00000015 uni00000018 uni00000017 uni00000018 uni00000019 uni00000035 uni00000044 uni0000005a uni0000002a uni00000033 uni00000037 uni0000003a uni00000044 uni00000057 uni00000048 uni00000055 uni00000029 uni00000044 uni0000004f uni0000004f uni00000037 uni00000048 uni00000056 uni00000057 uni00000003 uni00000027 uni00000055 uni0000004c uni00000059 uni00000048 uni00000051 uni00000036 uni00000046 uni00000055 uni00000058 uni00000050fig.
pass across temperature values.
of gpt3.
while flowgen has stable results across all versions.
flowgen may help ensure the stability of the generated code even when the underlying llm regresses.
figure shows the pass for rawgpt and the three flowgen across gpt3.
versions.
in earlier versions of gpt3.
and rawgpt has very low pass on all benchmarks e.g.
to in humaneval and humaneval et .
in mbpp s pass is even lower with a value around .
the findings show that model version may have a significant impact on the generated code .
however we see that after adopting our agent based techniques all three variants of flowgen achieve similar pass across gpt3.
versions.
the results indicate that flowgen can generate similar quality code even if we have an underperformed baseline model.
all techniques have a relatively similar pass when the temperature value changes.
figure shows the pass for all the techniques when the temperature value changes.
there is a slight downward trend for rawgpt when tincreases but the changes are not significant pass is decreased by to .
for flowgen and especially flowgen scrum we see similar pass regardless of the temperature value.
although we see a slight increase in the pass of flowgen scrum when t .
to higher compared to when t .
across the benchmarks the difference is small and the pass is almost the same when tis either the lowest .
or largest value .
.
in short although temperature values may have an impact on the generated code the effect is relatively small for flowgen .
flowgen generates stable results across gpt versions while we see large fluctuations times difference in rawgpt s pass .
pass is generally consistent across all models when the temperature value changes.
rq4 how does flowgen compare with other code generation techniques?
motivation.
flowgen is designed to organize agents to emulate process models and can be combined with other code generation techniques.
however it is crucial to evaluate its performance relative to these techniques to assess the effectiveness in emulating software process models for code generation.
while many code generation results use the same benchmarks our evaluation results cannot be directly compared with other agent based code generation works due to missing information on model versions temperature values post processing steps specific prompts or the selection of few shot samples.
therefore in this rq we compare flowgen scrum with other llm based baselines under the same environment settings.
moreover we evaluate an integrated version of flowgen scrum to showcase how existing prompting techniques can be combined with flowgen .
approach.
we compare against two state of the art techniques codet and reflexion .
codet employs self generated tests to evaluate the quality of generated code which is similar to the testing phase of flowgen .reflexion is an agent based technique that achieves state of the arts pass on the benchmarks.
we apply these two techniques using their released code replacing the llm version and temperature settings with those used by flowgen scrum and repeat the experiment five times.
result.
while flowgen scrum and codet achieve similar results with flowgen scrum having a higher pass in mbpp they both have higher pass than reflexion on all benchmarks statistically significant .
table v shows the pass of the techniques.
flowgen scrum has a statistically significantly higher pass than codet in mbpp and similar pass in other benchmarks no statistically significant difference .
both techniques achieve higher pass than reflexion statistically significant .
reflexion s mbpp results are even worse than rawgpt .
this observation aligns with the original study where shinn et al.
reported similar performance degradation.
integrating codet to flowgen scrum further brings statistically significant improvements of pass by up to .
codet is a general technique where it repeats the code generation and selects the code that passes the most self generated tests.
hence as a pilot study we made the developer agent repeats the implementation activity multiple times to produce several versions of the code i.e.
flowgen scrum test .
the developer agent then generates multiple test assertions to identify the version with the highest pass rate.
the selected code is subsequently submitted to the next stage the testing activity.
our findings indicate that flowgen scrum test outperforms flowgen scrum andcodet achieving an average pass score of .
.
.
and .
on humaneval humaneval et mbpp and mbpp et respectively.
this provides statistically significant improvement over both flowgen scrum andcodet .
our finding highlights the potential of flowgen in boosting the performance of other code generation techniques and vice versa .
future studies can refine flowgen to incorporate enhancement to each activity for further improvement.
to support these efforts we have made our code publicly available to facilitate further adoption and allow researchers to experiment with different software process models.table v average and standard deviation of pass across five runs with the best pass marked in bold.
humaneval humaneval et mbpp mbpp et reflexion .
.
.
.
.
.
.
.
codet .
.
.
.
.
.
.
.
flowgen scrum .
.
.
.
.
.
.
.
flowgen scrum test .
.
.
.
.
.
.
.
both flowgen scrum and codet outperform reflexion in pass across all benchmarks with flowgen scrum and codet demonstrating similar results.
the incorporation of codet into flowgen scrum further enhances performance achieving the highest pass scores highlighting the potential of flowgen for code generation tasks.
v. d iscussion f uture works role of human developers in flowgen .although software process models were originally designed for human centric development rather than for llms our empirical findings suggest that certain elements of these processes can contribute to better code quality.
every activity in the process model also has different impacts on the generated artifacts.
future research should examine the incorporation of human developers into various phases of the code generation process.
specifically humans can play critical roles in the following stages pre execution of flowgen different process models exhibit varying quality e.g.
smell and accuracy .
humans are instrumental in selecting the most appropriate model for a given task.
humans are also essential in providing initial requirements and design specifications.
during flowgen execution humans can oversee review meetings and assist in reviewing improving generated artifacts.
for example humans can validate the generated requirements with product managers or verify the quality of the generated code by manual inspection and debugging.
the improvement in each activity can also impact the subsequent activities and hence affect the final artifacts.
post execution of flowgen following the code generation phase humans can either accept the generated artifact or request further refinements offering additional requirements as needed to better meet project goals.
quality of code generation benchmarks we manually validated all coding problems that failed the tests.
we found that the majority of quality issues within the benchmark were in mbpp and mbpp et e.g.
bad naming convention or inconsistent test definition .
these issues may contribute to reduced pass scores due to factors beyond logic in the code.
it is also important to acknowledge that other benchmarks might present unique challenges that could similarly affect pass evaluations.
hence a crucial research direction is to conduct a thorough evaluation of benchmarks for more diverse and accurate evaluations of code generation approaches.
vi.
t hreats to validity internal validity.
due to the generative nature of llm the responses may change across runs.
variables such as temper ature and llm model version can also impact the generated code.
we set the temperate value to be larger than because we want llms to be more creative and offer more advice during discussions.
to mitigate the threat we try to execute the llms multiple times.
as found in rq1 the standard deviation of the results is small so the generated results should be consistent.
in rq3 we conducted the experiments using different temperature values and model versions.
the temperature value only has a small effect on pass and model versions have a large impact on rawgpt .
in rq2 we study the impact of removing every development activity.
however having multiple development activities may have a tandem effect that further improves code quality.
future studies are needed to study the effects of different combinations of development activities in code generation.
external validity.
we conduct our study using two state ofthe art benchmarks.
however as we discussed there exist some issues in the provided tests.
moreover the programming problems are mostly algorithmic so the findings may not generalize to other code generation tasks.
future studies should consider applying flowgen on different and larger programming tasks.
we use gpt3.
as our underlying llm.
although one can easily replace the llm in our experiment the findings may be different.
future studies on how the results offlowgen change when using different llms.
construct validity.
we try to implement an agent system that follows various software development processes.
however there are many variations of the same process model and some variations may give better results.
future studies should further explore how changing the process models affect the code generation ability.
one limitation is that there is no guarantee of the correctness of the generated tests.
however we also found that the generated tests still contribute to improving the overall quality of the generated code.
similar findings are reported on codet where generating multiple tests helps improve code correctness.
however future studies should focus on further improving the generated tests by using traditional software engineering techniques to estimate the oracles or select higher quality tests e.g.
mutation testing .
vii.
c onclusion in this paper we emulate various roles in software development such as requirement engineers architects developers and testers using large language model llm agents and structuring their interactions according to established process models.
we introduce flowgen a framework that implements three renowned process models flowgen waterfall flowgen tdd and flowgen scrum.
we evaluated how these models affect code generation in terms of correctness and code quality on four benchmarks humaneval humaneval et mbpp and mbpp et.
our findings show that flowgen scrum notably enhances pass scores by an average of over rawgpt while maintaining the lowest standard deviation averaging .
.
moreover we find that development activities such as design and code review significantly reduce code smells and increase the presence of handled exceptions.this indicates that flowgen not only boosts code correctness but also reduces code smells and improves reliability.
compared with other state of the art techniques flowgen scrum and codet achieved similar results with both outperformingreflexion .
integrating codet into flowgen scrum further resulted in statistically significant improvements achieving the highest pass scores.
these insights pave the way for future research to develop innovative development models tailored for llm integration in software development processes.
in this study we introduced flowgen a framework designed to emulate software process models using large language model llm agents each representing roles such as requirement engineers architects developers and testers.
we implemented three variations of flowgen flowgen waterfall flowgen tdd and flowgen scrum.
our evaluation across four benchmarks humaneval humaneval et mbpp and mbppet demonstrated the superior performance of flowgen scrum achieving up to .
improvement in pass scores over rawgpt .
our results showed that incorporating software process models into llm based code generation significantly enhances code correctness reduces code smells and improves exception handling.
flowgenscrum consistently outperformed other models achieving the highest pass scores and the lowest standard deviation indicating more stable and reliable code generation.
additionally our comparative analysis with state of the art techniques revealed that flowgenscrum and codet achieved similar results both outperforming reflexion.
notably integrating codet into flowgenscrum resulted in statistically significant improvements achieving the highest pass scores.
this highlights the robustness and potential of combining structured software development practices with llm capabilities.
future research should focus on further refining these models incorporating more sophisticated interactions and expanding the scope of evaluation to include a broader range of software development tasks and environments.
by so we can better understand the capabilities and limitations of llms in software engineering and continue to improve their integration into practical development workflows.
these findings underscore the potential of llms in mimicking real world development practices to generate higherquality code and emphasize the importance of structured development practices in llm based code generation.