the power of types exploring the impact of type checking on neural bug detection in dynamically typed languages boqi chen ece mcgill university montreal canadajos e antonio hern andez l opez ida link oping university link oping swedengunter mussbacher ece mcgill university montreal canadad aniel varr o ida ece link oping university mcgill university link oping sweden montreal canada abstract automated bug detection in dynamically typed languages such as python is essential for maintaining code quality.
the lack of mandatory type annotations in such languages can lead to errors that are challenging to identify early with traditional static analysis tools.
recent progress in deep neural networks has led to increased use of neural bug detectors.
in statically typed languages a type checker is integrated into the compiler and thus taken into consideration when the neural bug detector is designed for these languages.
however prior studies overlook this aspect during the training and testing of neural bug detectors for dynamically typed languages.
when an optional type checker is used assessing existing neural bug detectors on bugs easily detectable by type checkers may impact their performance estimation.
moreover including these bugs in the training set of neural bug detectors can shift their detection focus toward the wrong type of bugs.
we explore the impact of type checking on various neural bug detectors for variable misuse bugs a common type targeted by neural bug detectors.
existing synthetic and real world datasets are type checked to evaluate the prevalence of type related bugs.
then we investigate how type related bugs influence the training and testing of the neural bug detectors.
our findings indicate that existing bug detection datasets contain a significant proportion of type related bugs.
building on this insight we discover integrating the neural bug detector with a type checker can be beneficial especially when the code is annotated with types.
further investigation reveals neural bug detectors perform better on type related bugs than other bugs.
moreover removing type related bugs from the training data helps improve neural bug detectors ability to identify bugs beyond the scope of type checkers.
index terms type checking neural bug detection dynamically typed languages i. i ntroduction bug detection is a crucial task in software engineering se .
studies suggest that between .
to bugs may occur per lines of industrial code .
the implications of these bugs in software systems can be profound leading to outcomes ranging from minor system malfunctions to potential loss of human lives .
bug detection is even more important in dynamically typed languages such as python and javascript partially supported by the frqnt b2x project file number it30340 mitacs accelerate and the wallenberg ai autonomous systems and software program wasp sweden programtype checker compiler has bug?yesneural bug detector report bugs yeshas bug?fig.
use of an nbd for statically typed languages which are more error prone than statically typed languages .
furthermore the absence of mandatory type checking in these languages often leads to more type related bugs compared to statically typed languages .
automated bug detection aims to identify bugs early in the software development cycle.
traditionally this task has been depending on static analysis which interprets the semantics of the source code .
recent advances in machine learning ml have increased interest in using neural networks for identifying various bugs by learning implicit bug patterns from faulty programs .
these bug detectors typically aim to identify bugs such as variable misuse or binary operator bugs in syntactically correct programs.
consequently their training and evaluation datasets comprise both correct and faulty programs that are syntactically valid.
in statically typed languages such as java and c type checking is integrated into the compiler s program analysis process.
hence previous research on neural bug detectors nbds for these languages has considered this aspect .
figure shows how developers can use a type checker together with an nbd for a statically typed language.
the process begins with type checking if a bug is identified by the type checker categorized as type related bug in our paper it is immediately reported and rectified.
when the type checker does not find any bugs the nbd evaluates the program.
it reports any discovered bugs otherwise the program is deemed correct.
hence neural bug detectors focus on programs for which the type checker does not detect any bugs .
thus the training and evaluation of nbds must be contextualized to ensure their relevance and efficiency.
first nbdarxiv .15368v2 jan 2025models should be evaluated exclusively on bugs not detectable by the type checker to ascertain their true effectiveness.
moreover the training of nbds should prioritize real world performance to identify bugs that elude type checkers thus enhancing their usefulness.
however when studying nbds for dynamically typed languages such as python previous research often disregards this crucial aspect due to the absence of inherent static type checkers .
as a result little is known about the impact of type checking on the performance of these nbds.
still the growing popularity of optional type annotations in these languages which facilitate static type checking suggests a shift towards integrating type checking in the development workflow .
moreover compared to nbds traditional type checkers generate more explainable outputs provide soundness guarantees with well studied theoretical frameworks and require less computational resources and no training data.
when a type checker is used for example as part of continuous integration existing performance assessments may not accurately represent the nbd s effectiveness in such settings.
besides training nbds on such bugs may inadvertently skew the model s detection capability towards less relevant types.
this paper explores the impact of type checking on contemporary nbds in the context of python a widely used dynamically typed language.
our analysis encompasses a broad spectrum of neural architectures including gated graph neural networks ggnn great codebert graphcodebert and unixcoder .
our study investigates variable misuse bugs which are frequently targeted by nbds as they are widespread in se practice .
for instance of the bugs in the manysstubs4j corpus are variable misuse bugs while around of build errors in google engineering systems can be attributed to this type of bugs .
additionally allamanis et al.
find such bugs in ravendb a popular nosql database highlighting their prevalence in open source projects.
we assess how type related bugs affect the training and evaluation of nbds using both synthetic and real world datasets.
specifically we address the following research questions rq1 how prevalent are type related bugs in variable misuse datasets?
motivation before examining how type checking influences the training and evaluation of nbds it is imperative to assess if bugs identifiable by a type checker exist within popular variable misuse datasets.
results we find that a notable proportion of bugs can be identified by a type checker in synthetic and real world datasets that include programs that are not annotated with types .
this proportion is even higher when programs are type annotated.
rq2 how does type checking influence the performance of nbds when they are used together?
motivation since existing variable misuse datasets contain a notable proportion of type related bugs identifiable by a type checker the aim of rq2 is to evaluate how effective it is to use nbds together with a type checker in terms of performance.results we find that when the programs are not annotated with types incorporating a type checker enhances the performance of ggnn and great.
using a type checker with some other neural networks results in increased recall more bugs detected but decreased precision more false alarms .
however when incorporating a type checker on annotated programs both recall and precision consistently improve.
rq3 how do type related bugs influence the performance evaluation of nbds?
motivation when incorporating a type checker into the development process nbds should prioritize detecting bugs that elude type checkers.
hence understanding their effectiveness in identifying this category of bugs is crucial.
results we observe that nbds exhibit notably better performance on type related bugs.
rq4 how do type related bugs influence the training of nbds?
motivation since nbds tend to perform better on type related bugs the existence of such bugs in the training data may bias the nbd towards the wrong type of bugs already caught by the type checker.
therefore in rq4 we investigate how filtering out type related bugs impacts the training of nbds.
results we find that nbds trained on programs excluding type related bugs perform better in identifying bugs beyond the scope of type checkers on synthetic and real world datasets.
we also observe a decrease in the precision of bug detection.
an improvement in joint performance is observed when the recall of bugs is prioritized.
added value.
our findings offer practical insights into the optimal use of a type checker with an nbd for dynamically typed languages particularly when recall for variable misuse bugs is more important than precision or when type annotations are present.
integrating a type checker with an nbd necessitates excluding type related bugs from evaluation to mitigate the potential of overly optimistic results.
likewise during training it is advisable to exclude these bugs to improve bug detection.
artifacts.
artifacts for this study are available online .
organization.
this paper is organized as follows section ii introduces the background and context of our study.
following this section iii examines the prevalence of type related bugs in popular datasets for variable misuse.
section iv evaluates the impact of type checking on the training and evaluation of nbds.
subsequently section v explores the potential implications of our findings.
finally section vi reviews related literature and section vii concludes the paper.
ii.
b ackground a. type checking for python type checkers are a category of bug detectors based on static analysis targeting to identify defects using the types of variables.
these type checkers are typically integrated into the compiler of statically typed languages and can be used as external tools in dynamically typed languages .
python is a dynamically typed language thus it only determines the type of a variable at runtime.
however starting with python .
optional type annotations are introduced to bothhelp the developer understand the code better and enable static type checkers to detect type related bugs .
there are two type checking frameworks in python gradual typing and type inference .
many popular type checkers such as mypy operate on the principles of gradual typing a framework that detects inconsistencies in types using explicit type annotations in the program .
however gradual typing often assigns unknown types to unannotated variables which limits its ability to detect type related bugs in the absence of type annotations.
type inference is a technique similar to gradual typing but with a key distinction it infers types for variables even without explicit type annotation in the code thereby enabling the detection of a broader spectrum of type related bugs.
given the differences between the two frameworks this paper uses pytype a popular opensource type inference based type checker when the program is not annotated with types and uses both pytype andmypy when type annotations exist.
recent advances have seen deep learning based approaches being applied to type inference in python.
the core concept involves using neural networks to learn typing patterns implicitly in programs subsequently annotating variables based on these patterns.
tools such as deeptyper and typilus have demonstrated the ability to infer complex types that are difficult to infer by traditional methods.
however these tools have not yet been widely adopted in practice due to their high computational demands and the challenges in explaining the rationale behind their type inferences.
thus we do not consider these types of inference tools in this paper.
b. variable misuse bugs a variable misuse bug occurs when a developer unintentionally uses an incorrect variable.
listing shows an example of a variable misuse bug.
in this snippet the developer mistakenly uses the variable first instead of last in line if assn !
first .
unlike in statically typed languages this program will not cause a syntactic error but will raise a runtime error when the function is executed.
def take last assignment source first true last none for assn in source iffirst last assn first false if assn !
first yield last last assn if last is not none yield last listing example synthetic faulty program extracted from a python variable misuse dataset.
incorrect variable use location is bold and colored in red in this paper we exclusively focus on variable misuse bugs due to their prevalence in se practice and being commonly targeted by nbds .
furthermore some of these bugs can potentially be caught by type checkers which makes this kind of bugs a good candidate for investigating nbds in a practical se setting.
we focus on detecting a single bug in a code snippet which is a common assumption in the literature of neural bug detection .
c. nbds for variable misuse this section overviews the task of detecting variable misuse and the neural network architectures used to tackle such task.
task given a program a bug detector targeting variable misuse bugs produces two outputs a decision on whether the program contains such a bug and the location of the variable misuse if the program contains a bug.
formally given a bug detector dand a program function p the output of the bug detector can be represented as d p d l where dis a boolean value indicating whether the program has a bug or not and lrepresents the location of the bug when d true .
architectures the three prevalent neural network architectures used to tackle the variable misuse task are transformers graph neural networks gnn and hybrid approaches.
transformers represent a class of neural networks that process inputs as sequences of tokens leveraging a multi head attention mechanism to capture global relationships within these tokens .
in this case the programs are represented as a sequence of tokens and the output bug location l corresponds to the token that causes the bug.
while it is possible to train transformers from scratch with randomly initialized weights they are typically pre trained on extensive data corpora and subsequently fine tuned for specific tasks.
these pre trained models are known as pre trained language models.
this paper focuses on encoder only language models as they are predominantly used in fine tuning for classification problems .
specifically in our experiments we use codebert graphcodebert and unixcoder all of which are pre trained models specialized in source code.
graph neural networks gnns process graphs with both structural and feature information .
hence the programs are depicted as program graphs encompassing details concerning control flows e.g.
branching and loops and data flows e.g.
last read and last write of a variable .
consequently the output bug location l corresponds to the node representing the token responsible for the bug.
the most frequently used gnns for bug detection are gated graph neural networks ggnn which use a gated recurrent unit gru to perform the neighbor aggregation .
hybrid approaches combine both transformer and graph neural network approaches.
in our study we explore the graph relational embedding attention transformer great architecture proposed by hellendoorn et al.
.
great harnesses the capabilities of both transformers and gnns by enhancing the attention mechanism with biases for different relationship types.
the central concept behind great is incorporating edge information into the transformer architecture by using a relation type bias term in the attention computation.
iii.
e xistence of type related bugs this section summarizes why type checking is important to be used for detecting python bugs and examines thepresence of type related bugs in popular synthetic and realworld datasets.
additionally it discusses the methodology for assessing the prevalence of type related bugs in these datasets.
a. motivation previous work in creating nbds for python has largely overlooked the potential presence of type related bugs in their datasets due to the lack of type annotations in python code.
however recent type checkers such as pytype based ontype inference can identify type related bugs even in the absence of explicit type annotations.
for example in listing the variable usage first at line can be detected by a type checker through type inference.
in this context first is expected to be a boolean variable.
attempting member access for a boolean variable is invalid resulting in an unsupported operand error for the program.
in order to understand the influence of such defects on the performance of nbds the first step is to assess the existence of such bugs in datasets used for training and evaluation.
we evaluate existing synthetic and real world bug detection datasets to address the following research question rq1 how prevalent are type related bugs in variable misuse datasets?
b. evaluation setup in this section we present our methodology for conducting type checks on datasets designed for python variable misuse bug detection.
we detail the datasets used in our study and provide an overview of our approach.
datasets we analyze two widely used synthetic bug detection datasets and two real world datasets.
furthermore we manually annotate a subset of the real world datasets.
eth py synthetic bugs synthetic is a dataset specifically crafted for the detection and correction of variable misuse bugs within python functions.
this dataset is synthetically generated from the eth python dataset by substituting correct variable usages with incorrect ones chosen randomly within the same function.
for each faulty function the dataset specifies the bug s location a list of candidate variables for repair and the correct variable in the candidates.
the dataset contains .7m training functions 200k for validation and 950k for evaluation.
synthetic training set synthetic is a training dataset constructed following a similar approach as synthetic .
instead of directly creating synthetic bugs from the programs in the eth py150 dataset this dataset is generated by extracting programs directly from the open source projects used to construct py150.
specifically repositories without a commit fix related to real world variable misuse bugs are used.
in total around 147k samples were constructed for both faulty and correct functions from repositories.
py150 real bugs real py150 is a dataset containing real variable misuse bugs extracted from open source python projects used to construct the py150 dataset by applying a set of heuristics to identify commits fixing these bugs.
however the amount of faulty programs is insufficient to train a nbd.
programis a faulty program?yescheckout the fix commit no checkout containing commit perform type annotation run type checkersrevert fixcollect datasetis a faulty program?
yesidentify program location nohas fixable irrelevant errors?
noyesfig.
type annotation process for the real dataset we include the correct programs from the test splits as well as faulty programs from all splits.
in total the dataset contains bugs and correct programs pypi real bugs real pypi contains faulty functions with different types of bugs found in all 285k packages from the python package index pypi .
we only filter out the variable misuse bugs and use them for evaluation.
this dataset provides commits to the fix of the bug.
we use this information to extract original faulty programs from project repositories.
while some projects used in the dataset were deleted we were able to extract samples of variable misuse bugs from this dataset.
we merge this dataset with the real py150 dataset to create the merged real dataset.
annotated dataset annotated while it is suggested to integrate type annotation and checking into the development workflow most programs in the datasets are still not annotated with types.
previous work has shown that type checkers may not identify all errors in unannotated programs .
to better understand the effect of type annotation in this case we manually annotate a subset of the real dataset referred to as annotated .
aligned with previous work we randomly select programs from the real dataset sourced from github for manual annotation.
we choose correct programs and buggy programs to balance the annotated dataset.
next we present the annotation process.
c. type annotation process we adopt an annotation process similar to previous work on type checking for python .
figure demonstrates this process for which the entire repository is used.
to ensure consistency one author of the paper annotated all programs and the results were discussed among all authors to resolve any discrepancies.
for a faulty program the version from the fix commit is used.
for a correct program we identify the most recent commit containing the exact same program as of the dataset s publication date.
this step ensures that the program used during the annotation is correct preventing bias in the procedure due to the existence of bugs in the program.
once the correct version is set the file of the function to be annotated is located using the metadata.
then type annotations are added to allvariables and functions used.
if necessary class and function stubs are added to the file to provide type annotations for symbols used but not defined in the function.no or other bug stype related bugs faulty programtype checkermissing modules?
yes add missing modules bug found?
nobug on the same line as the variable misuse?
no fig.
an overview of the type related bugs labeling process after the initial type annotation both mypy andpytype are run to check for any irrelevant errors.
if such errors can be fixed by refining the type annotations the annotations are updated accordingly.
this process is repeated until no errors are present or the remaining errors cannot be fixed by refining annotations such as false alarms from the type checker.
finally the fix commit is reverted for faulty programs so that any further errors identified by the type checker can be considered type related bugs.
both the function and the defined stubs are collected for evaluation.
d. addressing the rq for each faulty code snippet in the datasets we use a type checker to detect type related bugs within the function.
the unannotated code is only type checked with pytype while the annotated programs are checked both with pytype and mypy .
preprocessing and filtering are performed to ensure the bugs caught by the type checker are indeed the variable misuse bugs of interest.
figure shows an overview of the process.
preprocessing given that each unannotated code snippet within the datasets only contains one function and omits the imported packages for that function the type checker may detect undesired import errors .
to mitigate such issues our approach includes a dual phase of type checking.
initially the first phase of type checking is conducted to identify any absent packages.
subsequently all detected missing packages are imported at the beginning of the code snippet.
this step is not necessary for the annotated programs since the missing symbols are already added as stubs.
type checking and filtering then a second phase of type checking is executed.
to account for potential irrelevant issues raised by the type checker we ignore all errors related to imported packages and internal errors from pytype such aspyi errors indicating that pytype cannot produce type inference for the program.
finally only errors detected on the same line as the variable misuse bug location are considered identifiable by the type checker.
we conduct type checking on all datasets quantifying the proportion of bugs identifiable by a type checker.
we refer to these bugs identifiable by a type checker as type related bugs .e.
results the top of figure shows the percentage of bugs detectable by a type checker for all four datasets.
for the annotated dataset both results from pytype andmypy are presented.
in general all datasets contain a notable fraction of typerelated bugs.
synthetic datasets contain the highest fraction with roughly .
insynthetic and14.
insynthetic .
as these datasets are frequently used for training this fraction could potentially lead nbds to deviate their focus towards type related bugs which is undesirable in cases where developers use type checkers.
the type checker can also detect a notable portion of .
of real world bugs in real.
when annotations are added to the programs the estimated percentage of detected bugs increases significantly to around .
forpytype and18.
for mypy .
our results are consistent with previous work which indicates that mypy can identify around of bugs in annotated python programs.
our slightly higher recall may be attributed to focusing on variable misuse bugs.
the bottom of figure also offers a more detailed analysis of type related errors illustrating the distribution of such bugs in both synthetic and real world datasets.
since mypy gives different category names we manually convert these names to the same as in pytype .
in general there are five types of bugs that appear most frequently in unannotated datasets name error .
.
often arises when a variable name is used without prior declaration or in parallel with the declaration.
for example the definition and use of a variable are in different branches of an if else statement.
attribute error .
.
occurs when an attribute is accessed on a variable that does not have that attribute.
for example accessing an attribute of an integer variable.
wrong arg types .
.
is raised when a function is called with arguments of wrong types.
this is typically related to a built in function while type annotation exists such as using len to access the length of an integer.
unsupported operand .
.
is raised when an unsupported operation is performed on variables.
for example attempting to add a string to an integer.
not writable .
in synthetic only is raised when trying to modify an immutable variable such as an immutable tuple.
most type related bugs in unannotated programs are linked to trivial errors e.g.
undefined variables in name errors .
consequently nbds may excel in handling these instances potentially inflating overall performance metrics.
this poses a significant threat to evaluation as these bugs may never be analyzed by the nbds if a type checker is incorporated.
the error distribution of annotated differs from that of the unannotated datasets.
most detected bugs are attribute errors and wrong arg types which are closely related to variable types.
manual type annotation aids type checkers in detecting these errors more effectively.
indeed none of the new errors detected through added annotations are name errors .
also a new error type bad return type has been identified.
this errorsynthetic train .
eval .
synthetic train .
real eval .
annotated pytype eval .
annotated mypy eval .
fig.
distribution of type related bugs in the synthetic real world and annotated datasets.
in synthetic we plot only the distribution of the evaluation split the training split follows a similar distribution .
is related to the function return type thus requiring manual annotation in the function signature for detection.
rq1 a significant fraction of bugs are detectable by a type checker ranging from .
to19.
for the synthetic datasets and .
for real world bugs even when programs are not annotated.
this fraction becomes even higher for annotated real world programs ranging from .
to18.
depending on the type checker.
moreover the training datasets contain a notable amount of detected bugs potentially affecting the training of nbds.
iv.
e ffect of type checking a. motivation neural bug detection datasets contain a significant proportion of type related bugs indicating the potential application of type checkers in detecting these bugs.
in this section we investigate how and when a type checker should be used together with an nbd for dynamically typed languages.
moreover in scenarios where a type checker is used nbds are expected not to analyze bugs detectable by the type checker.
thus to offer recommendations for deploying nbds alongside a type checker it is crucial to understand the impact of typerelated bugs on the training and testing of these neural systems.
hence we address the following three research questions rq2 how does type checking influence the performance of neural bug detection when they are used together?
rq3 how do type related bugs influence the evaluation of nbds?
rq4 how do type related bugs influence the training of nbds?
b. experiment setup we introduce the neural networks used in the nbds in our experiments.
this is followed by datasets and evaluation metrics used to investigate the research questions.
nbds the first column of table i shows the nbds used in our experiments.
the second column illustrates the architectures corresponding to each considered model.
we cover three widely employed architectures transformers gnns and hybrid methods.
the third column denotes if the detector is trained from scratch i.e.
initialized with random weights or fine tuned i.e.
initialized with a pre trained neural network .table i bug detection neural networks used in the experiments model architecture training strategy ggnn gnn from scratch codebert transformer fine tuning graphcodebert transformer fine tuning unixcoder transformer fine tuning great hybrid from scratch datasets we adopt the methodology from prior studies concerning training with a synthetic dataset and assessment using both the synthetic test split from the same dataset and real world datasets .
specifically we select the synthetic dataset as our training dataset because of its larger volume of data necessary for training nbds.
the trained nbds are then evaluated on both annotated and unannotated samples from the real dataset.
since the real world datasets are constructed independently from the synthetic dataset some of the real world bug programs may also appear in the training set of synthetic .
to avoid such data leakage issues all potential duplicated samples are excluded from the real world datasets during evaluation.
specifically we use the metadata of each sample as a heuristic and remove all program samples that are from the same file in the same repository and have the same function signature as any samples from the synthetic training set.
in total .
correct programs and .
of bugs in the real py150 dataset and .
of faulty programs in the real pypi dataset are removed due to potential duplication with the training data.
metrics in this work we evaluate the performance of nbds as a classification problem.
compared to standard classification metrics we define stricter true positive cases for a bug detection task taking into account bug localization.
specifically given a set of faulty programs pfand a bug detector d the true positives tp are defined as the number of programs where the bug detector correctly identifies and localizes the bug tp d pf p p f dd p true ld p lp where dd p represent decision and ld p represent the bug location of the program ppredicted by the nbd.
then all other programs pwith dd p true are considered as false positives fp .
moreover false negatives fn are the number of faulty programs where the bug detector wronglyidentifies the program as bug free.
formally the false negative fn is defined as the following fn d pf p p f dd p false finally all other programs the nbd identifies as bug free are considered as true negative tn .
to evaluate the performance with nbds we use standard precision and recall as metrics precision tp tp fprecall tp tp fn to evaluate the joint performance of the bug detector the fscore is typically used.
however the f score treats precision and recall equally.
in a real world scenario a programmer may prefer a higher recall to ensure all bugs are detected even if it means more false positives or a higher precision to reduce the number of false alarms.
therefore we consider the f score which allows us to adjust the importance of precision and recall.
specifically the f score is defined as f 2 precision recall 2 precision recall while represents the relative importance of recall compared to precision.
a higher signifies more emphasis on recall.
when the f score is equivalent to the f score.
in an se context a higher is preferable when detecting and fixing bugs are costly and time consuming while a lower is appropriate when developers need to spend much time on checking false alarms of automated detection tools.
to measure the differences in performance of the two settings we use ratio change .
given the performance values of two settings paandpb the ratio change ofpbwith respect topais calculated as follows pb pa pa comparison to existing metrics.
existing work often uses correct program recall and localization accuracy as the metrics for an nbd to evaluate bug classification and localization separately.
in fact when treating bug detection as a classification problem localization accuracy can be seen as the recall in our metrics while correct program recall is covered by the true negative rate.
furthermore the f score takes both metrics into account and can provide a more comprehensive evaluation of the nbds end to end performance.
training settings previous work often trains the nbd with the joint training objective of both bug detection and repair .
in our preliminary experimentation we notice that training without the repairing loss results in either similar or degraded performance compared to the joint objective.
consequently we opt to use the joint training objective as done in previous studies.
when training ggnn and great models we use the same hyperparameter configurations as in previous work and train for and epochs respectively.
for fine tuning we train each model for epoch and use a learning rate of 1e with a linear learning rate scheduler.c.
rq2 influence of type checker addressing the rq in this section we aim to a evaluate the effect of integrating a type checker into the nbd similar to the ones used for statically typed languages as shown in figure and b investigate when and how the type checker should be used together with the nbd depending on the requirement of the developer.
all nbds are trained with the synthetic dataset on the variable misuse task.
to gain insights on the influence of type checkers in a practical scenario we evaluate the performance of the bug detectors on both the unannotated real and the annotated subset annotated .
in general for each nbd we consider two settings nn the nbd is used alone and pipeline the nbd is used together with a type checker in a pipeline as shown in figure .
the type checker used inpipeline ispytype for the real dataset as it can handle unannotated programs.
for annotated we also evaluate mypy to understand the impact of different type checkers.
since the neural networks are trained on code without annotations type annotations are removed when running the nbds.
we also evaluate the precision and recall of the type checker using the real dataset.
specifically we define tpas the count of buggy samples correctly identified by the type checker i.e.
bugs identified in figure .
fn represents buggy samples missed by the type checker.
the fpis determined by running type checkers on the correct programs.
a correct program is classified as fp if the type checker raises a type of bug that was found by the type checker in faulty programs e.g.
nameerror orattribute error in figure .
this criterion is adopted because the type checker s capability extends to detecting various bug types beyond variable misuse.
results for the real dataset figure 5a shows the precision and recall of the nbds in the real dataset for both thennandpipeline settings and results of the type checker pytype on the unannotated real dataset.
the type checker exhibits reasonable precision but low recall in identifying variable misuse bugs when the program is not annotated with types.
this limited performance may be attributed to using a single type checker configuration for all programs in the dataset and the lack of type annotation.
nbds in the pipeline configuration demonstrate higher recall than those in the nn setup.
additionally for certain models ggnn and great the precision also improves.
this is likely caused by the fact that the type checker has better precision than these nbds.
the recall improvement ranges from .
to3.
for all nbds indicating that integrating a type checker can help identify more bugs.
however the precision in the pipeline setting also decreases by between .
to4.
for all pre trained models due to the new false positives introduced by the type checker.
to better understand the trade off between precision and recall introduced by the type checker on unannotated programs we plot the f score for different values of in figure 5b.
the two full training methods are excluded as the pipeline approach improves both precision and recall.
in general when the nnsetting achieves a higher score compared tomodelprecision recall nn pipeline nn pipeline codebert .
.
.
.
graphcodebert .
.
.
.
unixcoder .
.
.
.
ggnn .
.
.
.
great .
.
.
.
pytype .
.
a precision and recall of the bug detectors on the unannotated real dataset.
numbers are in percentage.
the pipeline setting always improves recall but may reduce precision.
.
.
.
.
.
.
0. .
.
.
.
.36f score codebert graphcodebert unixcoder b f score for different values for nbds in nn solid andpipeline dashed settings.
the pipeline setting surpasses nnfor all nbds when .
.
fig.
a precision and recall on the real dataset.
b f scores for different values of table ii precision and recall of the bug detectors a and type checkers b on annotated annotated with types for the nbd and pipeline performance on the unannotated raw and annotated version.
f scores are omitted since both precision and recall improve consistently when programs are annotated.
numbers are in percentage.
modelprecision recall nnpipeline pytype raw pipeline pytype pipeline mypy nnpipeline pytype raw pipeline pytype pipeline mypy codebert .
.
.
.
.
.
.
.
graphcodebert .
.
.
.
.
.
.
.
unixcoder .
.
.
.
.
.
.
.
ggnn .
.
.
.
.
.
.
.
great .
.
.
.
.
.
.
.
a type checker precision recall pytype raw .
.
pytype .
.
mypy .
.
b thepipeline setting indicating that it may not be necessary to include type checking when the frequency of false alarms is a concern.
however when .
i.e.
recall is considered .62times more important than precision pipeline surpasses thennsetting for all nbds suggesting that the type checker can be beneficial when recall of bugs is more important.
for the rest of the paper we consider and .
on unannotated programs for cases where precision and recall hold equal importance and cases where recall is more critical.
results for the annotated dataset table ii displays the precision and recall of the bug detectors for both the nnand pipeline settings as well as the performance of both type checkers on the annotated dataset.
additionally we include the performance of these settings on the same dataset without type annotations raw .
this comparison is provided because the figures reported for the real dataset in figure 5a are not directly comparable due to the real dataset being highly imbalanced whereas the annotated dataset is balanced.
when the program is annotated with types the type checker pytype achieves a precision of .
and a recall of .
.
on unannotated samples the same type checker achieves a precision of and a recall of .
.
conversely mypy which only runs on type annotated samples achieves a higher recall of .
but a lower precision of .
when compared with pytype .
nevertheless the recall achieved by type checkers even with annotations indicates that these tools fail to detect many variable misuse bugs highlighting the need to use more advanced tools such as nbds alongside them.
surprisingly with the improvement in the performance of the type checker from type annotations pipeline achieves both higher recall and precision and thus higher f for all compared to nnandpipeline on unannotated programs.
on average the precision of the pipeline setting with pytype is improved by .
while the recall is improved by .
compared to the nn setting.
pipeline with mypy achieves even higher recall an average increase of .
and a similar improvement in the precision averaging around .
.
rq2 combining a type checker with nbds can improve recall by .
to .
but may lower precision by .
to4.
on unannotated programs.
on annotated programs integrating a type checker improves both precision and recall.
thus the pipeline setting is beneficial when recall is key or when type annotations are present.
d. rq3 evaluation on type related bugs addressing the rq since integrating type checkers can be beneficial if identifying bugs is key or when type annotations exist when using a type checker in combination with an nbd the nbd should mainly focus on the bugs that are not easily detectable by type checkers.
in this section we evaluate the performance of nbds on type related bugs compared to other bugs when trained with a synthetic dataset on the variable misuse task.
we divide the evaluation dataset into two groups based on the type checking results in the evaluation type related bugs and other bugs.
we then show the impact of the type related bugs on overall performance by evaluating all programs and programs filtering type related bugs.
in this experiment we consider synthetic real and theannotated datasets and filter the bugs with pytype .
results table iii shows the precision and recall of different nbds.
comparing the performance of the nbds ontable iii impact of filtering type related bugs on precision and recall.
numbers are in percentage.
f scores are omitted since both precision and recall decrease consistently.
modelsynthetic real annotated precision recall precision recall precision recall full filtered full filtered full filtered full filtered full filtered full filtered codebert .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
graphcodebert .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
unixcoder .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ggnn .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
great .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
synthetic and real world datasets we observe that these detectors perform much better on the synthetic datasets compared to the real world bugs consistent with previous work .
this discrepancy can be attributed to the distribution shift between synthetic and real world bugs .
to evaluate the impact of type related bugs on precision and recall we include the ratio of change in the filtered dataset compared to the full dataset.
we observe a decrease in both precision and recall for all nbds in the filtered evaluation sets hinting at a bias of nbds towards type related bugs.
for unannotated programs removing type related bugs from the evaluation dataset leads to an average reduction in precision .
and recall .
.
the nbds exhibit a more notable decrease in real compared to synthetic with an average of .
and4.
for precision and recall respectively.
with type annotations annotated the decrease is even more notable with the precision of nbds dropping by .
to12.
and recall dropping by .
to17.
.
these findings imply that type related bugs in the evaluation may lead to overly optimistic outcomes especially when a type checker is used in the pipeline.
furthermore the decline in performance is notably more significant for the real world datasets compared to the synthetic dataset.
this observation brings increased threats to evaluation validity in real world use cases compared to the synthetic dataset.
rq3 our findings reveal that bug detectors exhibit superior performance in detecting type related bugs compared to other types of bugs.
removing these bugs from the evaluation dataset leads to a reduction in both precision an average of .
and recall an average of .
even in the absence of type annotations.
when type annotation exists the dropping in performance is even more significant.
this suggests that the inclusion of type related bugs in the evaluation dataset particularly when using a type checker may yield overly optimistic evaluation results.
e. rq4 effects of type related bugs on training addressing the rq in this section we investigate the impact of filtering type related bugs from the training data on the performance of the nbds.
nbds trained with the full dataset are compared with those trained with the filtered dataset using precision recall and f on all samples from the evaluation datasets excluding type related bugs.
we aim to measure the performance of nbds when the type checker is used together as a pipeline.
thus we only focus on bugs beyond the scope of type checkers and filter all type relatedbugs from the datasets.
since manually annotating the training dataset would be overwhelming due to the large size the training dataset is filtered using unannotated programs only.
to ensure consistency between training and evaluation we exclude the annotated subset in this experiment and leave the exploration of type annotations in training as future work.
a naive approach to filter type related bugs from the training data is to remove all samples identified as type related bugs.
however this approach may result in a significant reduction in the size of the training data as well as in an imbalance in the correct and faulty programs.
to mitigate these issues we replace the type related bugs with other bugs in the training data by randomly oversampling other bugs.
this approach ensures that the size of the training data remains the same and the balance between correct and faulty programs is maintained.
results the performance change of nbds trained with the full dataset and the filtered dataset is shown in table iv.
values on the left side of the arrow show the performance of the full training dataset while the right side represents the performance of filtered training.
the better f scores of the two settings are highlighted in bold.
in general the recall is improved when the nbd is trained on the filtered dataset observed in all cases with an average improvement of .
.
this improvement indicates that the removal of type related bugs from the training data enables the nbds to focus more on other non type related bugs.
on the other hand the precision is also reduced in out of cases.
however the change is generally smaller with an average decrease of .
.
we hypothesize that this decrease may be attributed to the oversampling strategy we used rather than generating entirely new synthetic samples.
when assessing the f score we evaluate both cases of f and .
.
when using the f score where recall is equally important as precision filtering out typerelated bugs from the training data does not yield a visible improvement in overall performance as indicated by a better f score in only out of cases.
this finding aligns with our observation in section iv c that integrating type checkers may not be beneficial in these cases.
when recall is more important .
we note that the filtered dataset consistently outperforms the full dataset in all cases for the synthetic dataset.
this observation suggests that the reduction in precision is less pronounced compared to the increase in recall in this case.
this improvement extends to the real world dataset as well with the filtered dataset outperforming the full dataset in out of cases while not outperforming in the case of codebert.
the slighttable iv performance of detectors trained with full dataset filtered dataset .
numbers are in percentage.
in general filtering type related bugs in training improves the recall while decreasing the precision.
modelsynthetic real precision recall f f .
precision recall f f .
codebert .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
graphcodebert .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
unixcoder .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ggnn .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
great .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
variation in results for the real world dataset may stem from the distribution shift between synthetic and real world bugs.
rq4 removing type related errors from the training dataset causes a significant increase in recall.
although a decrease in precision is also noted the overall f score is improved in out of cases when .
.
v. d iscussion a. key implications we show that datasets for python variable misuse contain a significant portion of type related bugs.
moreover we conduct systematic experiments to evaluate the impact of type related bugs on the training and evaluation of nbds.
based on the results we provide practical takeaways for using nbds in detecting variable misuse bugs for dynamically typed languages.
takeaway in scenarios where developers include type annotations type checkers should be used in conjunction with nbds.
if no annotations are provided combining type checkers with nbds is beneficial when prioritizing recall.
many existing studies have demonstrated that type checkers can help identify bugs in dynamically typed languages when manual type annotation is provided .
our study builds upon these findings revealing that type checkers are capable of detecting a significant proportion of real world bugs around .
even in the absence of explicit type annotations.
if the programs are annotated the percentage of type related bugs detected increases to a range of .
to .
depending on the type checker used.
based on these observations we assess the combined use of type checkers and nbds on both unannotated and annotated programs.
analysis of this assessment indicates that if recall is more important than precision i.e.
when .
including type checkers is recommended.
when type annotations exist integrating type checkers is always beneficial.
takeaway when evaluating nbds type related bugs and other bugs should be evaluated separately.
existing assessments of nbds typically combine all bug types into a unified dataset.
our findings reveal that excluding type related bugs from the evaluation dataset significantly reduces the nbds performance.
thus to accurately estimate the effectiveness of nbds in conjunction with a type checker removal of these type related bugs from the evaluation is necessary.
conversely if the aim is to evaluate the nbd s standalone performance a more detailed bug categorization should be used to comprehensively understand their capabilities.
takeaway when training an nbd to complement type checkers eliminating type related bugs from the training datasets enhances the neural model s ability to detect nontype related bugs.
however this refinement often leads to a decrease in precision.
excluding type related bugs from unannotated datasets during the training phase results in general improvements in recall for all nbds within both synthetic and real world datasets.
however this improvement in recall comes at the cost of a reduction in precision.
thus the decision to exclude these bugs from training depends on the specific goal of the bug detector.
if recall is more important than precision then eliminating type related bugs during training is advantageous.
b. threats to validity a internal validity training and fine tuning of nbds can be sensitive to the choice of hyperparameters.
we mitigate this issue by reusing hyperparameters from previous work .
furthermore we use the same hyperparameters for the same model architecture when training with the full and filtered dataset.
moreover the training process can be nondeterministic and due to the cost of training we only used one random seed for each setting.
to mitigate this issue we experimented with multiple different neural network architectures and training paradigms.
when evaluating the impact of type related bugs in the training data we oversample bugs to keep the dataset balanced.
such a strategy provides significant improvement in recall.
another approach to further improve performance but left for future work could be to generate more synthetic bugs while filtering type related bugs.
we manually annotated a subset of the real dataset to evaluate the performance when type annotation exists.
to ensure accuracy and minimize bias we only use the correct version of the programs and follow procedures from previous work.
b external validity this paper exclusively concentrates on python due to the limited availability of variable misuse datasets for other dynamically typed languages.
consequently the insights drawn from this study might not be directly transferable to other dynamically typed languages.
we defer the exploration of such intriguing avenues to future research.
the impact of type checking may differ for nbds.
we studyvarious recent nbds and obtained similar effects.
besides we rely solely on type checkers to detect and classify typerelated bugs due to the lack of ground truth.
implementation errors in the type checkers may cause incorrect detection.
to mitigate this issue we use popular type checkers developed and maintained by google and the python community.
c construct validity selecting appropriate performance metrics to evaluate performance in the variable misuse task presents a potential threat to validity.
to tackle this concern we use standard metrics for classification by considering bug detection as a classification problem.
these metrics resemble existing metrics used for nbds.
in assessing the joint effectiveness of nbds we employed the f score.
the choice of values is tailored to balance the emphasis between identifying bugs and minimizing false alarms.
to address variability in performance we present results across a spectrum of values during pipeline performance evaluation.
furthermore we select two values to represent different scenarios in our analysis of the influence of type related bugs on training.
vi.
r elated work a. neural bug detectors a target bug types multiple studies have shown the effectiveness of using neural networks for bug detection.
besides variable misuse bugs studied in this paper neural networks have been used to detect other types of bugs such as operand swapping wrong binary operator and identifying wrong argument or return values of a function .
most of the work adapts the strategy of joint learning of detection and repair of the program together .
b proposed models the predominant models utilized for bug detection can be broadly categorized into transformers gnns and hybrid methods .
transformers process programs as sequences of tokens with the flexibility of being either trained from scratch or fine tuned e.g.
cubert codebert graphcodebert unixcoder etc.
.
gnn models on the other hand interpret programs in the form of graphs featuring diverse edges such as control flow data flow etc.
.
hybrid approaches combine both models.
for example great combines the transformer architecture with edge types from the program graph.
the sandwich model alternates between transformer layers and graph neural layers forming a cohesive approach.
this paper explores a range of models including codebert graphcodebert unixcoder ggnn and great thereby encompassing the entire spectrum of commonly employed approaches for bug detection.
c datasets due to the lack of real world bugs training on real world bugs alone is typically not sufficient to obtain a high performing bug detector .
as a result many approaches have been focused on generating synthetic bugs.
the most common approach is to randomly perturb a program with a set of predefined mutation rules .
recent work demonstrates that neural detectors trained on synthetic bugs do not perform well on real world bugs .
this difference in performance can be attributed to thedifferent characteristics between synthetic and real world bugs .
as a result many studies have experimented with mixing synthetic and real world bugs in training .
at the same time other approaches attempt to generate synthetic datasets that are more similar to real world bugs.
for example richter et al.
use a learning based approach to generate faulty programs taking the context of the original program into account.
semseed extracts a set of bug patterns from real world bugs and applies the pattern most similar to the original program to generate a synthetic bug.
our study takes a different perspective on analyzing the datasets for training nbds.
we investigate how type checking affects the performance of nbds and offer recommendations to enhance their practicality when deployed in such scenarios.
b. combining static analysis and neural networks static analysis has long been used to enhance the performance of neural networks used on code.
in nbds static analysis generates a program graph used as input to the neural network .
when repairing the bug feedback generated from the compiler is used as additional information to help the neural network .
another task commonly associated with static analysis is type inference.
when employing a neural network for variable type prediction particularly in dynamically typed languages the predicted types often lack guarantees of consistency with the type checker .
in such scenarios a type checker is used to guide the search space of types and ensure the coherence of the predicted types .
our work shows how and when a type checker should be used together with nbds to improve the overall bug detection performance.
we also show how bugs detectable by a type checker can influence the training and evaluation of nbds.
vii.
c onclusion and future work in this study we explore the impact of type checking on automated bug detection using neural networks focusing on variable misuse errors.
we found that over of synthetic bugs and around of real world bugs in datasets used for nbds can be identified by a type checker without explicit type annotations.
when type annotaion are present over of real world bugs can be detected.
via experiments with various nbds we find that the benefits of type checking are most pronounced when recall is more important than precision or when type annotations exist.
when the nbds are used together with a type checker it has been observed that type related bugs lead to an overly optimistic evaluation.
we also discover that excluding these bugs from training can enhance the detection capabilities of nbds while reducing their precision.
as a result the decision to exclude type related bugs from training depends on whether the priority is recall or precision we plan to extend the work from three aspects.
first the impact of type checking on dynamically typed languages beyond python and other types of bugs can be investigated.
then the impact of different bug types in type related errors on nbds can be studied.
finally the influence of neural type inference can be explored.