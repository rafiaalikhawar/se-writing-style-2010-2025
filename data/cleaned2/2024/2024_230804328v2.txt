the product beyond the model an empirical study of repositories of open source ml products nadia nahar haoran zhang grace lewis shurui zhou christian k stner carnegie mellon university carnegie mellon software engineering institute university of toronto nadian andrew.cmu.edu abstract machine learning ml components are increasingly incorporated into software products for end users but developers face challenges in transitioning from ml prototypes to products.
academics have limited access to the source of commercial ml products hindering research progress to address these challenges.
in this study first and foremost we contribute a dataset of open source ml products for end users not just models identified among more than half a million ml related projects on github.
then we qualitatively and quantitatively analyze open source ml products to answer six broad research questions about development practices and system architecture.
we find that the majority of the ml products in our sample represent more startup style development than reported in past interview studies.
we report findings including limited involvement of data scientists in many open source ml products unusually low modularity between ml and non ml code diverse architectural choices on incorporating models into products and limited prevalence of industry best practices such as model testing pipeline automation and monitoring.
additionally we discuss seven implications of this study on research development and education including the need for tools to assist teams without data scientists education opportunities and open source specific research for privacy preserving telemetry.
index terms open source dataset machine learning products mining software repositories software engineering for machine learning i. i ntroduction with the increasing popularity of machine learning ml more and more software products are incorporating ml components to enable various capabilities such as face recognition in photo sharing apps.
however incorporating ml into products is not just about developing the model there is a significant amount of effort to integrate the model into the system while considering aspects such as system architecture requirements user experience ux design safety and security system testing and operations .
developers find it challenging to convert ml prototypes into software products with ml components ml products .
unfortunately researchers rarely have access to the source code of ml products and hence face difficulty in a studying challenges in depth and b designing and evaluating interventions e.g.
tools and practices .
currently academic researchers rely primarily on an outside view gathered through interviews or surveys with industry practitioners .
some researchers perform research within a company and gain rich access but are limited to a single context and can rarely share details.
this inability to access and study ml products not just any ml projects poses a significant impediment to advancingtable i sample ml products for analysis from the curated dataset of ml products mobile p1 p10 desktop p11p20 and web applications p21 p30 id name descrption star cont.
u d p1 text fairy ocr scanner app 10m p2 seek by inaturalist app to identify plants and animals 1m p3 pocket code app for learning programming 7m p4 esp32 ai camera esp32 cam processing ai tasks 1k p5 notionai mymind app to store and search for items 1k p6 organic maps offline map app 500k p7 vertikin e commerce app n a p8 florisboard android keyboard n a p9 newsblur personal news reader 50k p10 tflite mnist handwritten digits classification n a p11 awips advanced weather processing system 97k mo p12 audiveris optical musical recognition app .7k mo p13 datashare doc.
analysis software for journalists .2k mo p14 algoloop algorithmic trading application n a p15 subtitle edit editor for video subtitles 293k mo p16 deepfacelab software for creating deepfakes n a p17 faceswap software for creating deepfakes 297k mo p18 ho helper for hattrick football manager 14m p19 bigbluebutton web conferencing system 88k mo p20 poseosc realtime human pose estimation n a p21 openbb terminal investment research software 94k mo p22 coffee grind size coffee particle analyzer n a p23 celestial detection classifier of celestial bodies n a p24 electricity maps greenhouse gas intensity visualizer 3m p25 galaxy data intensive science for everyone 187k mo p26 gridcal power systems planning software n a p27 honkling keyword spotting system .2k mo p28 jitsi meet app for video conferencing 10m p29 code.org professional learning program for cs 82m p30 basketball analys.
analyze basketball shooting pose n a users downloads there is no reliable way to calculate the number of users we report them using multiple ways if available such as downloads in google play store self reported on website or website traffic tracker similarweb.com to count average monthly users in value mo format research in this field leading to a wealth of academic literature identifying challenges through professionals testimonies but a dearth of research offering scientifically evaluated solutions or interventions at the intersection of software engineering se and ml.
historically the field of se has greatly benefited from opensource software in terms of research practice and education.
research on mining open source software repositories has allowed us to create vast datasets and study aspects that would otherwise be challenging to investigate such as effectiveness of continuous integration and pull requestbased development .
open source allows researchers to test interventions on software artifacts which has been foundational for many research areas such as program repair and software testing .
many innovations developed andarxiv .04328v2 aug 2024evaluated on open source are later adopted in industry e.g.
facebook s adoption of program repair and google s adoption of mutation testing .
beyond research and development access to open source on a larger scale has revolutionized education offering students and professionals the opportunity to study and illustrate practices in open source repositories .
in the same manner access to opensource ml products could open opportunities for research education and technology transfer.
to this end we identify a corpus of such ml products.
many past studies have tried to study ml projects in open source but usually a only focus on one or two specific examples or b use a dataset full of notebooks research projects homework solutions and demos which are not representative of real world industrial ml products.
while open source is useful for studying ml libraries and notebooks researchers struggle to find good examples of open source ml products several papers prominently highlight faceswap as an active end user open source ml product and study it in depth but it is usually the only clear example of an ml product ever identified or analyzed .
several papers rely on a dataset of projects labeled as ml applied but closer inspection reveals that most of these projects are research notebooks tutorial style projects and toy projects not a promising dataset for those interested in studying ml products.
we have two goals for this paper a to identify a corpus of ml products in open source beyond just faceswap and b to analyze the curated dataset to answer research questions of interest to the community.
the first goal turned out to be surprisingly difficult due to the abundance of open source ml projects that are not ml products making the keyword search approach used in related studies ineffective.
in response we designed a tailored pipeline strategized specifically for finding ml products .
we identified and manually verified a total of repositories .
while this is a smaller corpus than past datasets on ml projects it provides a considerable number of open source projects that build an end user product around a model have a development history and are fully transparent providing opportunities not achievable with interviews surveys and even industry collaborations.
second we analyze our dataset to answer existing research questions for which open source software might provide useful insights.
instead of a shallow quantitative analysis of all the ml products we conducted an in depth analysis of a 30product sample table i and reported findings around six research questions related to collaboration architecture process testing operations and responsible ai.
among others our findings reveal a often limited visible involvement of data scientists in developing open source ml products and a lack of clear boundaries between responsibilities for ml and nonml code b diverse architectural choices on incorporating often multiple models into products and c rare use of industry best practices such as pipeline automation model evaluation and monitoring.
while our findings suggest that open source ml products often mirror practices reported fromstartup style ml products in industry we also find a wide range of practices and products in this dataset.
our dataset offers ample research and educational opportunities such as developing tools to assist teams that lack access to data scientists tools and patterns to address the unexpectedly low modularity between ml and non ml code and open sourcespecific innovations for privacy preserving telemetry.
to summarize the primary contributions of this paper are a an open source dataset of ml products b a novel search strategy to identify the ml products from github and c findings around six broad research questions characterizing the nature of ml products in the dataset.
ii.
d efining ml p roducts throughout the paper we use the term ml product to describe software products for end users that contain ml components.
we explicitly distinguish ml products from other ml related projects and artifacts such as notebooks and models.
note that terminology in this field is not standardized or consistent as practitioners and researchers may refer to the libraries that train models e.g.
tensorflow the code to train models e.g.
in a notebook the deployed models e.g.
gpt3 or the products around those models e.g.
faceswap by names such as ml systems ml projects or ml applications.
our notion of ml product considers the entire software system including non ml components in line with past research that used terms like ml enabled systems or ml systems .
during our research we needed a clear definition for what we consider an ml product especially because we had to classify thousands of repositories .
in line with interactive systems in human computer interaction and products in the context of product management and after iteratively reviewing hundreds of projects cf.
sec.
iv we define ml product as follows ml product a machine learning product is a software project a for end users that b contains one or more machine learning components .
to be considered for end users the project must have aclear purpose and a clear target audience .
the purpose can be fun and the audience can be everybody.
the software must be complete usable polished and documented e.g.
install and usage instructions to the level typically expected by the target audience.
the product needs to use at least one machine learned model that provides major or minor functionality in the software.
the model can be developed from scratch or called using an existing library or api.
for contrast we define ml libraries and ml projects ml library libraries frameworks or apis that are used to perform ml tasks such as tensorflow and dvc1.
project ml project represents any software project that integrates some form of ml functionality or code.
examples include notebooks research artifacts associated with a paper and course homework.
all ml products are ml projects but most ml projects are not ml products.
scoping.
we exclude ml products targeting software developers and data scientists as end users from our corpus such as code completion tools.
these users have more technical expertise and may interact with products through different interfaces sometimes blurring the line between product and project.
researchers interested in ml products for developers could repeat our search process with a wider scope.
iii.
e xisting research and limitations building ml products is challenging and requires engineering beyond the model and ml pipeline.
many researchers have studied the challenges that practitioners face when turning an ml model or prototype into a product.
we recently collected challenges from papers that surveyed and interviewed practitioners regarding the software engineering challenges faced when building ml products .
these papers illustrated numerous challenges such as architectural issues due to lack of modularity in ml code increasing design complexity and team collaboration hindered by the absence of necessary skills .
while these studies with practitioners provide a high level understanding of the problems they often do not offer sufficient details or access to design specific interventions.
researcher tradeoff between internal and external validity for studying ml products.
for studying ml products not just models and ml projects researchers adopted different research designs.
some conducted interviews e.g.
while others focused on surveying practitioners at scale e.g.
.
while these studies provide a broad sense of the challenges maximizing external validity they rely on self reported data without access to artifacts.
there are also ethnographic studies industrial case studies and experience reports these can yield a deeper understanding of specific cases maximizing internal validity but at the cost of generalizability as they are usually based on a single case a common tradeoff .
access to an open source dataset of ml products can provide new opportunities enabling researchers to validate reported challenges on tangible data devise solutions and evaluate interventions across a larger number of cases.
academic interventions exist for models and pipelines where plenty of notebooks and libraries exist in open source to study but not for product level problems to which academics hardly have access.
academics can design and evaluate solutions when adequate data is available such as the millions of notebooks on github for example supporting studies and evaluations of interventions on dependency management documentation generation and collaboration practices in notebooks .
there are also many solutions for ml related components such as data validation training data creation modelbuilding and fairness assessment .
however without access to study ml products it is challenging to design and rigorously evaluate interventions at the product level incl.
architecture collaboration and documentation .
open source datasets exist for ml projects but those are not representative of ml products and not suitable for answering research questions related to ml product development.
several papers introduce datasets of ml projects on github not necessarily ml products .
for instance gonzalez et al.
collected over applied ai and ml repositories.
the quality of this dataset was criticized for inclusion of toy projects by rzig et al.
who then curated a new dataset with ml projects for studying the adoption of continuous integration ci practices.
similar datasets of ml projects were curated by van oort et al.
and tang et al.
for purposes such as discovering the prevalence of code smells and refactoring practices in ml projects.
widyasari et al.
shared a manually labeled dataset of engineered ml projects referring to the need for a higher quality dataset that should include engineering practices.
while all these datasets can provide various insights about ml projects they are not promising for studying concerns related to developing ml products we analyzed all of them to find only four projects that we consider as products .
we argue that insights about architecture technical debt and differences in addressing ml and non ml issue reports from studying ml projects might not well generalize to ml products.
despite curating many datasets of ml related projects presented above researchers have not succeeded in finding ml products in open source apart from one frequently highlighted example faceswap .
closest to our work wan et al.
curate a dataset of ml products where a model is used in the context of some non ml code in their case to test the code using the model .
unfortunately all their projects are toy projects such as a starred fire alarm application with three commits that uses an object detection model to get object labels from an input photo and prints alarm if it detects the keyword fire in the label .
existing datasets of ml projects and toy examples limit the generalizability of research to real world industry style ml products .
even worse if a reader is not aware of the types of projects in a dataset which is not obvious when the example provided is faceswap it is easy to incorrectly generalize findings to ml products .
thus there is value in a dataset specifically dedicated to ml products which motivates us to curate and study such a dataset.
iv.
c ontribution a c urating the dataset identifying open source ml products was surprisingly difficult and turned into a research project in itself.
searching with keywords like machine learning in readmes as in prior work collecting open source ml projects does not work because a the vast number of ml projects libraries notebooks research experiments demos is much larger than the much smaller number of ml products and b ml productsdo not always explicitly advertise their use of machine learning especially when used for smaller optional features.
for example only one of the top search results on github for machine learning is an ml product and of our analyzed ml products do not mention machine learning in their readme such as the video conferencing application p29 which uses facial expression detection as an add on.
instead we explored and iteratively refined a new search strategy combining domain knowledge code search and manual analysis in a process that is specifically designed to scale to search across all of github.
in a nutshell our approach is based on the following insights targeting end users ml products have a user interface mobile web desktop command line whereas most other ml projects do not.
we rely on code search to identify code relating to user interfaces.
ml is used in products usually through a small number of libraries and apis whether to train a custom model to load a serialized model or to call a remote api service.
we rely on code search to identify the use of ml in implementations.
the final distinction between ml products and ml projects requires human judgment all our attempts at automation yielded poor accuracy .
we develop heuristics to prioritize which projects to analyze to manage scarce resources for manual analysis.
code search at the scale of github is challenging.
we carefully design a multi step pipeline that incrementally reduces the search space eliminating many projects that are not ml products with cheaper analyses before more expensive analysis steps are required.
each insight makes assumptions that enable the search to scale and find relevant ml products but each assumption may lose some ml products that do not meet them such as user interface mechanisms not captured e.g.
game engines and models not detected e.g.
custom k nn implementations .
our approach cannot ensure finding an exhaustive list of all ml products it is a best effort attempt to collect as many ml products as possible with reasonable resources in the face of a very difficult search challenge see limitations below .
a. search space and scope we search for ml products on github.
github is by far the most popular platform for open source projects whereas more specialized platforms such as hugging face only host ml models.
we only include popular project repositories over stars that have been maintained recently updated after and that are documented in english constraints that are common in open source research.
we restrict our analysis to desktop and web applications written in javascript python java and c most popular languages for such applications in addition to mobile apps for android and ios.
b. search pipeline to scale the search we proceed in five steps with increasing per project analysis cost in each step.
.
api search.
we start with a very scalable step to retrieve a vast overapproximation of candidate projects with the github search api.
we retrieve all github repositories using any of the four programming languages as the primary language and all repositories matching the keywords android or ios.
we additionally restrict the search to stars and commit date as mentioned above.
where necessary we partition the search space by date to overcome github s maximum of search results.
at this stage we identified candidate repositories cf.
table ii .
.
metadata and readme filter.
we retrieve each candidate project s readme and github metadata including about description and tagged topics through the github api.
we exclude obvious non product repositories by matching keywords such as framework tutorial and demo in the description or readme.
in line with similar efforts we remove archived and deprecated repositories e.g.
keywords deprecated or obsolete forks and repositories with nonenglish descriptions using an off the shelf model .
we report the exact filters in the appendix .
we manually validated a random set of filtered projects finding no incorrectly filtered projects.
a total of repositories remained after applying this filter.
.
product filter.
to detect user interfaces we rely on code search performed locally after cloning each candidate repository.
we curated a list of code fragments indicative of common frameworks for user interfaces such as com.android.application in a gradle.build file for android mobile applications import javax.swing for java desktop applications and from flask import flask for web applications in python see appendix for details .
we remove repositories that do not contain any of these code fragments leaving us with potential products for further analysis.
.
ml filter.
to identify the use of machine learning we again rely on code search based on curated lists of code fragments indicative of ml libraries and apis.
we count occurrences of calls to any of ml libraries or apis e.g.
import caffe and of serialized models e.g.
files with .tflite and .mlmodel extensions .
in addition as a noisy last resort we count occurrences of ml keywords such as machine learning and nlp in any source or text files including comments and documentation to catch less common libraries and custom implementations.
at this point projects pass at least one ml related filter.
.
manual inspection.
the final step with by far the highest per repository cost is to manually validate whether a repository is an ml product.
one or more authors with extensive expertise in ml products inspected the repository its description and when needed its code to judge whether the repository is indeed an ml product this typically took seconds to minutes per repository.
our definition of ml product in sec.
is the result of multiple iterations and refinement for example establishing requirements for purpose and documentation for which we discussed early inspected projects as a grouptable ii number of retrieved projects after each step mobile app desktop app web apptotal android ios js py java c js py java github search initial filter product filter manual check based on the ml clue counts we inspected around 4k projects manually.
removing duplicates.
of which we considered to be ml products to arrive at a stable definition which provided us with a high interrater reliability n kappa .
.
a few repositories near the decision boundary were discussed by all authors until a unanimous consensus was reached.
we inspected about of the remaining repositories prioritizing our resources based on match counts for our ml filters stratified product category language and ml filter.
in each strata we stopped when we reached consecutive false positive repositories for example after inspecting android mobile apps.
c. limitations and threats to validity to make the search feasible we had to make various compromises arriving at the described design.
given the various heuristics our approach represents a best effort attempt and cannot claim producing an exhaustive or complete list of ml products.
as discussed we may have missed ml products in other languages using other gui frameworks or less common ml libraries.
additionally our approach involved manual inspection which despite best efforts opens the possibility of human error and subjectivity.
our search heuristics prioritize false positives over false negatives and we designed our approach accepting low precision discarding many repositories in the last manual validation step to ensure high recall.
while we would have preferred to formally evaluate recall i.e.
whether we missed any ml products by comparing our dataset against any existing ground truth dataset of ml products such a dataset does not exist.
as a substitute we attempted to collect ml products independently by seeking input from industry practitioners through platforms like quora reddit linkedin twitter and a 32k member slack channel in the field of data science but aside from numerous replies expressing interest in our dataset we only received suggestions for two repositories both of which we determined not to be ml products according to our definition.
additionally we compared our dataset to other existing datasets of ml projects but did not find any additional repositories that satisfy our definition of ml products in those datasets.
in fact those datasets only contained a total of four ml products all of which we detected in our dataset.
while all this raises our confidence we cannot formally assess recall.d.
the open source ml product dataset in total we found 2622ml products cf.
table ii full dataset in appendix .
the average ml product in our corpus has stars contributors and is 325mb in size.
over half of the ml products are written in python most are web applications.
the dataset comprises a diverse range of products some of which have a significantly larger number of users and a more professional look than others.
for instance seek p2 in table i is a mobile app for identifying plants and animals using image recognition downloaded over million times and reviewed by over 38k users with robust support from the established inaturalist community who maintains a dedicated website and continuously improves and maintains the app.
in contrast notionai mymind p5 an android app developed by a single contributor uses an ml classifier to automatically tag images and articles with a simple user interface rare updates and under downloads from the play store.
approximately half of the products in our dataset have a professional presentation like seek those generally have more stars and a larger codebase.
the others seem to be personalinterest projects released as a product.
v. c ontribution b l earning from the dataset we created this dataset due to limited access to industry products which hinders research advancements and education in the field.
now that we have this dataset we can finally attempt to answer numerous research questions about ml products that have accumulated in many past studies with open source products where previously we had to rely on interviews or experience reports from industry practitioners.
given the breadth of topics we cannot cover everything in a single study.
in this paper we explore a wide range of topics rather than going into depth on a single one to contribute new knowledge and achieve two secondary objectives characterize the dataset our analysis with broad research questions will implicitly characterize the products in our dataset and enable other researchers to effectively use it and interpret derived findings cf.
limitations of existing work in section iii this also helps to explore how similar the ml products in our dataset are to ml products described in interviews and experience reports.
identify when deeper analysis of the dataset is feasible and promising our research questions from different topics such as collaboration architecture and 2removing duplicates such as same repositories for android and ios apps.development process will identify what kind of questions are worth going into deeper before committing resources to indepth analyses for individual research questions.
a. research method first we curated a list of research questions relevant to the study and designed qualitative and quantitative strategies to answer them.
given the novelty of the dataset and questions we heavily rely on qualitative analysis involving substantial manual effort.
therefore we decided it would be more manageable to analyze a sample of products from the dataset.
deriving research questions.
we selected research questions that are not only of interest to the research community but can also be feasibly answered by analyzing open source ml products e.g.
we did not find artifacts that would allow us to answer how do data scientists elicit document and analyze requirements for ml systems?
.
for this selection we employed a two step process.
first we explored the existing literature to identify topics that are of interest to researchers in the field such as the challenges faced by practitioners building ml products collected in our recent meta survey of interviews and surveys .
we identified numerous topics of interest such as collaboration architecture process quality assurance mlops and responsible ai.
second we examined our dataset of ml products to identify potential research questions rqs that could plausibly be answered with open source data.
we explored randomly selected ml products qualitatively taking multiple pages of notes for each product.
we immersed ourselves in the source code documentation contributor profiles issues and any other available information provided on associated websites we identified the ml and non ml components to familiarize us with common structures.
this gave us a sense of what kind of questions can be reasonably answered.
in the end we selected six questions spanning the entire life cycle rq collaboration how interdisciplinary are open source ml product teams and how do they divide their work?
rq architecture how are open source ml products architected to incorporate models?
rq process what model product development trajectory do open source ml products follow?
rq testing what and how are the open source ml products and their parts tested?
rq operations how are opensource ml products designed for operation?
rq responsible ai what responsible ai practices are used in open source ml products?
while each identified topic could warrant a dedicated study and deeper analysis here we provide initial answers for each and explore opportunities for future research.
analysis and synthesis.
without existing established measures we found a manual mostly qualitative analysis to be more responsive and effective than a narrow quantitative analysis at scale .
to find answers to the rqs the ml products required an in depth examination of the code analysis of contributor activities and thorough inspection of related documents.
while we automated some measures such as contributor percentages and modularity scores designing them also required initial manual investigation and manual classification of ml components.
this made the process quite labor intensive requiring hours per product.
consequently we analyzed a sample rather than every product in the dataset.
sampling we analyzed the products shown in table i .
of the dataset which was manageable for our manual analysis.
rather than attempting statistical generalizations our goal was to gain rich insights from the dataset.
thus we aimed to sample a diverse range of products using case study research logic .
we used information oriented selection to select popular and large products extreme deviant cases because we expect them have a richer history to analyze and we included a random selection of other products for the average cases .
to represent different kinds of products we stratified selection across the three genres mobile desktop and web.
specifically we select two products with the most stars per genre p8 p9 p16 p17 p21 p28 two products with the most contributors per genre p3 p6 p14 p19 p24 p25 the product with the largest code size per genre p1 p11 p29 and five randomly selected products per genre.
the sample set of ml products vary in terms of stars avg.
5k min.
max.
42k no of contributors avg.
min.
max.
and loc avg.
564k min.
max.
2745k as shown in table i and appendix .
analyzing products and card sorting we conducted a comprehensive qualitative analysis of the sampled products.
we describe the specific analysis steps separately for each research question below but generally we follow the strategy two researchers carefully examined the github repositories addressing each research question individually for each product involving tasks such as reading documentation identifying ml and non ml components in the source code measuring modularity examining contributor profiles analyzing commit history and reviewing issues.
the entire research team regularly met to discuss clarify understanding and resolve disagreements and organize findings.
to organize and find patterns among the products we performed card sorting .
each product was represented by a card for each rq describing our findings for that particular rq and we iteratively grouped these cards to identify patterns within rqs.
additionally we searched for associations across patterns from different rqs.
we share analysis artifacts including a miro board and spreadsheets in the appendix .
threats to credibility and validity.
despite following best practices for qualitative research as we discussed this part of the research shares common threats encountered in qualitative research .
given the small sample size and sampling strategy statistical generalization is not suitable and not advised.
readers should compare this to a study with interviews.
while we followed standard practices for coding and memoing during the analysis of the products we cannotcompletely eliminate biases introduced by the researchers.
in addition we only access public information and do not have access to offline activities hence our findings should be interpreted accordingly.
rq how interdisciplinary are open source ml product teams and how do they divide work?
interdisciplinary collaboration is difficult as has also been found in building ml products .
the transparency of open source development allows researchers to study many aspects of collaboration as demonstrated by numerous past studies on team collaboration pull requests and diversity e.g.
.
to understand interdisciplinary collaboration in open source ml products we explore team composition in terms of the number of contributors and their backgrounds who works on ml and non ml code of the product and how tangled ml and non ml code are in terms of co changes.
the findings can help study existing challenges such as siloed development and guide further studies in collaboration.
method.
to analyze contributor backgrounds and numbers findings we collected contribution data from github and identified the core contributors as those collectively responsible for of all commits in line with past work .
we manually classified each contributor s background asse focused ml focused or other e.g.
physics finance based on public self description professional title and education history as found on their github profile linkedin profile and personal project or company websites if available.
if the classification was not obvious e.g.
because of limited public information we classify their background as unsure.
to study how developers from se and ml backgrounds contribute to ml and non ml code findings we separated the ml and non ml code after excluding documentation and binaries we manually categorized code associated with model training prediction and pipeline as ml related while all other software infrastructure and graphical user interface gui code fell under non ml typically at the granularity of files.
finally we automatically analyze the commit history to attribute code changes to contributors.
to analyze the coupling of ml and non ml code finding we analyze co edits in the product history known as logical coupling .
specifically we compute a relative coupling index that indicates whether the ml and non ml parts are more or less coupled than would be expected if all changes were randomly distributed across files.
a low relative coupling index indicates that changes are typically isolated to only ml code or only non ml code whereas a high relative coupling index indicates that ml and non ml code are often changed together signaling low modularity.
to compute relative coupling index we compute coupling using evolutionary coupling index eci and then divide eci by the probability of coupling of random edits to normalize the effect of size as ml and non ml code size differs significantly the average product has 971k loc of non ml code and 23k loc of ml code .finding most of the core contributors are software engineers .
finding many products have a single core contributor .
in our sample of products we identified core contributors among whom we could classify .
among the we found contributors as se focused and contributors spread across eight products as ml focused .
p19 p28 p25 p03 p29 p14 p21 p24 p06 p02 p13 p15 p26 p09 p16 p18 p23 p01 p04 p05 p07 p08 p10 p11 p12 p17 p20 p22 p27 p30 project018core contributor countbackground ml se unsure other for products where we could classify all core contributors many had exclusively se focused contributors.
we found a single contributor in p24 who selfidentified to be an expert in both se and ml.
finding there is little evidence of clear silos with core contributors commonly committing to both ml and non ml code regardless of background.
finding team responsibilities are rarely assigned and recognizable in the commit history.
in contrast to the p07 p30 p16 p17 p23 p04 p01 p10 p12 p20 p05 p13 p08 p14 p18 p15 p27 p09 p02 p24 p21 p26 p28 p29 p11 p03 p19 p25 p06 project100 non ml mlcode changes by core contributorscontributor background ml se unsure other widely reported problem of siloing in industry teams we do not find a clear delineation by background of who contributes to the ml and non ml code.
we often find contributors of either background working on both parts.
only five products p13 p21 p24 p25 p29 publicly documented team structures with assigned roles and responsibilities for team members but even then the assigned responsibilities do not always reflect the commit activities.
for instance even though p29 has an explicit data team we do not find commits from the data team to the ml repository but sefocused contributors change both the ml and non ml code.
we conjecture that some offline collaboration is not visible in the open source repository.
.
.
rel coupling idx01234567 products finding ml and non ml code are often changed together indicating low levels of modularity.
while models are usually assumed to be modular components in a system our analysis reveals that many products in our sample exhibit frequent co changes of ml and non ml code.
for example p27 has a very high relative coupling index between ml and non ml code we found that this product has a custom script for training a speech recognition model where the ml code directly updates the user interface ui button based on the prediction result any modification to the ui properties requires an update to the model script to accommodate the change causing frequent co changes of the ui files and the model script.
conversely although we indeed found many products that have coupling lower than random it is not as low as could be expected from fully modular components of a product.
discussion open source versus industry open source ml products mirror the startup style of development more than big tech projects.
in line with general trends in open source we find relatively small teams developing ml products in our sample in contrast to often large teams reported to build ml products in big tech companies .
our open source ml products are closer to the average team size of members in startups .
while prior studies report misaligned responsibilities that do not reflect developers abilities or preferences across all kinds of organizations building ml products the fluent and broad responsibilities and collective code ownership resemble characteristics commonly seen in startups.
overall the opensource ml products seem more reflective of activities in the vast majority of ml projects outside of big tech organizations that have their own distinct and often understudied challenges .
implication researchers should study the challenges of teams that do not have access to data scientists and explore providing assistance.
most studies on ml development focus on the challenges of data scientists perceived as the dominant or novel role.
open source ml products seem to be dominated by software engineers who adopt ml tools often with limited apparent participation from data scientists.
past interview studies have already established pitfalls of software engineers adopting ml without explicit training such as inadequate feature engineering and insufficient evaluation.
open source ml products provide an opportunity to study such problems in public artifacts.
in addition ml products developed without dedicated data scientists are likely common also in industry outside of big tech and likely to become more common as data science becomes more accessible e.g.
with automl and prompt engineering researchers should explore how to support software engineers with limited data science expertise in building ml products responsibly for example through analysis automation and smart assistants.
recent tools for detecting data smells and data leakage and for anticipating fairness issues provide encouraging starting points.
conversely a few ml products in our sample are developed by data scientists without software engineers such cases are better researched but can equally benefit from further studies and support.
implication researchers should investigate sources of non modularity and develop tools and guidance.
while interactions among multiple models are a well known problem changing anything changes everything models are usually considered natural modules in a software design with clear and simple interfaces .
yet we found surprisingly frequent co changes of ml and nonml code.
research should explore the sources of this nonmodularity and have a unique opportunity to do so with our dataset.
research should identify or create design strategies to isolate change possibly coded as design patterns current mlrelated design patterns rarely consider the interaction of ml and non ml code to guide practitioners toward more modular designs.
positive examples in the dataset could serve as illustrations in educational materials.
rq how are open source ml products architected to incorporate models?
researchers have highlighted how ml can influence the architecture of software products .
to comprehensively understand the product structures and the incorporated models we explore architecturally relevant aspects such as model type usage importance integration of multiple models pipeline automation documentation and big data infrastructure.
method.
to understand the overall structure of the model and product we conducted a comprehensive manual analysis of the ml and non ml code in the repositories.
we analyzed the code with a focus on the following artifacts code structure and data flow as it pertains to models identifying how the models are created where and how they are called and how the model predictions are processed and used.
we also reviewed their documentation relevant blogs and forums associated web pages and related repositories under the same personal or organizational account.
we then sorted our findings and grouped those into categories using card sorting techniques guidance from previous research and domain knowledge from our research team.
finding about half of the products rely exclusively on third party ml models .
we identified products that use third party models via libraries e.g.
tesseract ocr external apis e.g.
clarifyai or load pre trained model files from a remote repository.
in contrast products self train models .
two products use both third party orself trained models p12 p30 .
for instance the optical music recognition application p12 uses a self trained model to classify music symbols and an existing ocr library for classifying text.
finding the importance of the ml models to the product varies with about half using them as optional functionality only .
we found the importance of ml models to vary considerably across different products.
the model is the core functionality in products as there would be no product without the model e.g.
the ocr model in the ocr scanner app p1 .
there are products that may still provide value without the model but the model is a significant functionality e.g.
the ocr model in video subtitle editor p15 that could potentially operate on manual inputs .
in products the model provides optional functionality serving as a nice to have add on e.g.
facial expression recognition in video conferencing app p28 .
whether a third party model isused finding is not necessarily associated with the model s importance we found products investing substantial effort in self trained models for optional functionality e.g.
p6 p26 p29 and products relying on a third party model for core functionality e.g.
p1 p12 p13 .
products with models as core tend to be smaller and have fewer contributors avg.
112k loc .
contributors while optional models are often added to larger products with more contributors avg.
754k loc .
contributors .
finding automation using model predictions is uncommon with most products keeping humans in the loop.
a central question in human ai design is how to use or present model predictions and whether and how to keep humans in the loop .
we find only five products that use model predictions to fully automate actions e.g.
keyword spotting app p27 executes gameplay instructions based on recognized voice commands .
two products prompt users to confirm an action e.g.
deepfake software p17 asks for confirmations on image previews between each processing stage .
most products in our sample merely display predictions leaving decisions about actions entirely to users e.g.
trading app p21 graphically presents investment predictions .
finding most products use raw model predictions without any post processing .
finding products that automate actions are more likely to further process model predictions.
products may check process and integrate model predictions in many ways some now encoded as patterns such astwo phased predictions for resilient serving of models .
however most analyzed products trust model predictions and display them without any further processing.
only two products incorporate additional architectural tactics around model predictions plant identifier app p2 uses a twophase prediction system combining a local model with an online model for low confidence cases known as two phase prediction pattern and subtitle editor p15 performs extensive checks on texts predicted via optical character recognition and language translation before presenting them.
in addition three products incorporate a confidence score threshold to filter low confidence predictions p9 p27 p29 and another three offer a retraining option for the model if performance proves unsatisfactory p5 p12 p28 .
interestingly p11 uses machine learning to check the results from a non ml api.
the few products that automated decisions based on model predictions finding process predictions further by offering retraining mechanisms and confidence checks .
augment promptautomateretrain score check safeguardno checks finding many products use multiple models though those models are mostly independent .
interactions among multiple models is a frequently raised challenge in industry where a minor change in one model can trigger cascading changes across the product .
while 18products use multiple models those perform independent tasks in products3 products use models for separate functions unrelated to each other e.g.
p15 uses one model for ocr and another for speech to text and products provide alternative models for the same function e.g.
p26 provides a choice between two clustering models .
five products sequentially compose models e.g.
p12 passes text recognized by an ocr model to an entity recognition model .
two products use models for collective decision making e.g.
p9 combines multiple classifiers to generate personalized news feeds .
finding pipeline automation is not common in opensource ml products.
switching from a static mindset and notebooks to pipeline automation is a commonly reported challenge .
among the products that use selftrained models finding training is often not automated.
we did not find any model training pipeline for four products p2 p18 p22 p30 we cannot tell if training happens offline or in a private repository .
four products p6 p10 p12 p16 require manual execution of sequential training steps two products automate only data retrieval p11 p24 .
four products p9 p15 p27 p29 have gui integrated training pipelines that can be separately activated via gui actions.
only four products p14 p21 p23 p26 feature fully automated training pipelines to consistently fetch the latest data and deploy updated models.
finding we do not find much effort on data or model documentation.
both industry and academia view model and data documentation as important for among others collaboration accountability and reuse but adoption in industry is rare and perceived as challenging .
in our sample the products using self trained models finding provided minimal and mostly scattered documentation for models and data if any.
regarding model documentation only one product p29 provides high quality model documentation in the form of a model card .
other products have at most brief instructions for using the model api or descriptions of the model architecture for the data scientists.
data documentation was mostly limited to presenting a data schema occasionally mentioning the volume of the training data or simply providing a link to their data sources.
we found no use of datasheets or similar templates.
finding most products do not use big data infrastructure .
scalability is reported as a common important architectural challenge for ml products for handling large datasets and distributing expensive training and inference jobs resulting in frequent reliance on big data infrastructure.
however we did not find the use of local or self hosted big data infrastructure such as hadoop and spark in any of our sample ml products.
seven products contain code related to cloud services for storage computing monitoring and search e.g.
amazon s3 ec2 cloudwatch and elastic cloud .
discussion open source versus industry open source ml products share many of the development decisions and 3numbers do not add up as some products fall in multiple categories.challenges discussed in industry studies.
in line with the industry trends many open source projects leverage thirdparty models rather than building models from scratch a pragmatic choice given the cost time and skill requirements involved in developing models with limited resources .
we find models used for various tasks with varying levels of importance in a product reflecting the diversity of the ml products.
open source ml products with models ascore more resemble startup style projects whereas attempts to integrate models as enhancements to existing products are found throughout the industry including many established corporations.
while there are large variations within our dataset open source ml products tend to lean toward the less complex end of reported ml products in the industry with simpler architectures with few models limited automation and less need for massive scale.
the lack of pipeline thinking overly trusting model predictions and poor documentation mirror practices repeatedly criticized in industry projects and while the more experienced and well resourced companies work toward better practices those challenges are still common in many newer and smaller organizations.
implication researchers should study patterns and test interventions for different architectural choices.
while researchers and practitioners argue for the need to implement safeguards prepare for the evolution of thirdparty models design effective and safe humanai interaction models and integrate multiple models researchers rarely have access to enough products to detect patterns and validate solutions on a range of systems.
even if some of the practices are rare in open source our dataset has many and diverse projects to study existing patterns and to provide a testbed to evaluate the consequences of different design interventions such as design patterns to isolate models or data flow analyses to track whether and how multiple models in a product interact.
it also provides opportunities for deeper investigation in certain aspects such as employing firehouse studies to interview developers when certain events occur.
additionally it allows researchers to conduct longitudinal studies to understand the evolution of the team and the architecture over time.
we have not seen any prior longitudinal studies of projects in this field common in msr style research likely due to a lack of access.
implication educators should use open source to develop teaching materials.
in a field where access to concrete implementations is scarce and educators often rely on demo projects or second hand reports open source ml products can be a valuable educational resource to showcase system design strategies and challenges whether as illustrations in lectures and blog posts as foundations for homework assignments or as in depth case studies as in the architecture of open source application books .
the dataset has sufficient variety to cover simpler projects suitable for beginners as well as sophisticated products built by large teams to study architectural design decisions.implication companies foundations and governments should explore strategies to sustain model and big data infrastructure.
unlike revenue generating commercial products most open source ml products in our sample did not seek to monetize their products.
the potential high recurring cost for model apis and cloud computing may prevent opensource developers from scaling their products or from building certain products in the first place.
some open source products may find a path to secure funding for example seek by inaturalist p2 was supported by various nonprofit foundations before establishing its own nonprofit with a seed grant.
while companies foundations and governments often support open source e.g.
free hosting and ci on github sovereign tech fund nsf pose sustained support for model apis and cloud computing is less common.
such support seems essential to encourage open source innovations as alternatives to commercially dominated ml products.
rq process testing operations and responsible ai due to the page limit we only report brief findings from the remaining four research questions but refer the interested reader to our appendix for details on the methods findings and discussions.
the following findings relate to rq finding to rq findings to rq and finding to rq .
finding product first development is more common than model first .
finding when the model is thecore functionality it is always developed first.
in prior work we found that some projects start with models and later model first product first unsurecore optional significant build products around them model first whereas others adopt a product first approach each creating distinct challenges .
in our open source sample we observe a greater prevalence of the product first trajectory which may be attributed to most contributors being software engineers finding and many products adding machine learning for optional functionality to existing products finding .
noticeably products with models as core are always developed model first .
for example deepfakes software p17 created the model first and added a gui a year later to make the model accessible to end users.
finding testing regular software functionality is common model testing is notably scarce and data validation is rare .
standard software testing practices are widespread but model evaluation is less prevalent in our sample.
even among the eight products that included model evaluation scripts three p3 p9 p29 approached model testing like unit testing asserting that predictions match expected values.
the rare cases p6 p21 in which data validation is conducted involve only minimal checks for schema and value ranges.
finding only a few products have mechanisms for evolving models.
of the products with self trainedmodels five products p9 p16 p23 p27 p30 offer users the option to retrain products at run time and three products p14 p21 p26 continually retrain their models by fetching up to date data from their data sources.
this aligns with our recent finding that many product teams have a static view of models .
finding model monitoring is almost non existent .
despite the heavy emphasis on observability in industry and academic literature for detecting failures and degradation out of the products do not collect telemetry and have no monitoring infrastructure.
only p21 incorporated telemetry for financial forecasting.
in addition p9 uses amazon cloudwatch but not for model monitoring.
finding mlops tools are not used.
we did not find use of any popular mlops tools for tasks such as automating deployment testing monitoring and data cataloging in any of the products.
given that many of these products do not incorporate retraining mechanisms finding they may have less need for mlops automation.
finding responsible ai practices e.g.
fairness safety security are not apparent in open source ml products.
despite significant attention in academia we do not find adoption of any responsible ai practices in our sample.
only one product p17 discusses ethical usage in their readme largely limited to disclaimers.
several products include privacy policies and disclaimers unrelated to ml.
one educational product p29 covers responsible ai practices as a subject.
discussion open source versus industry in comparison to industry open source showcases similar and more bad practices associated with both model and product evaluation and maintenance.
similar to our observations on architecture open source ml products exhibit many of the characteristics criticized in past research with low adoption of tooling and interventions discussed by more experienced and well resourced organizations we find the same low adoption of model evaluation data validation monitoring and responsible ai practices .
the open source ml products seem to reflect the practices of new organizations and smaller teams more than those of big tech organizations they are likely reflective of the challenges that new teams will experience especially teams dominated by software engineers.
implication tool vendors have an opportunity to showcase the benefits of automation tooling.
rather than relying on testimonials and narrowly scoped tutorials tool vendors such as those of mlops tools can demonstrate their tools in forks of open source ml products or can even work with opensource developers to integrate them into their products.
for example while researchers have found some public mostly very small projects using the versioning tool dvc none of them were full ml products showcasing the integration of model and product versioning.
similar to implication tool vendors can provide resources to support open source communities.implication research should explore open source friendly monitoring approaches.
observability is a key focus of the mlops community many researchers and practitioners argue that the unreliable nature of ml and presence of data drift makes monitoring and testing in production crucial to responsible engineering .
we conjecture that the privacy conscious open source culture and a lack of centralized infrastructure contribute to the minimal adoption of monitoring among open source ml products.
researchers should explore privacy preserving and community operated monitoring solutions compatible with open source values ideally through co design processes with open source practitioners.
vi.
c onclusion we offer a dataset of open source ml products to facilitate research experiments that can benefit from access to the development history and artifacts of ml products and report findings and seven implications from six research questions.
the dataset is a valuable educational resource for both academics and practitioners in ml product development offering diverse study materials with both large and small ml products and it also provides ample research opportunities as described.
acknowledgment the authors would like to thank bogdan vasilescu rohan padhye and eunsuk kang for helping with the framing and their continuous suggestions and feedback.
the author would also like to thank the industry practitioners who responded to the queries about ml products on quora reddit linkedin twitter and slack.
k stner s nahar s and zhang s work was supported in part by the national science foundation zhou s work was supported in part by the natural sciences and engineering research council of canada nserc rgpin202103538 and lewis work was funded and supported by the department of defense under contract no.
fa8702 d0002 with carnegie mellon university for the operation of the software engineering institute a federally funded research and development center dm24 .