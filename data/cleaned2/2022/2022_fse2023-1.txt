dynamic data fault localization for deep neural networks yining yin ynyin smail.nju.edu.cn state key laboratory for novel software technology nanjing university nanjing chinayang feng fengyang nju.edu.cn state key laboratory for novel software technology nanjing university nanjing chinashihao weng shweng smail.nju.edu.cn state key laboratory for novel software technology nanjing university nanjing china zixi liu zxliu smail.nju.edu.cn state key laboratory for novel software technology nanjing university nanjing chinayuan yao y.yao nju.edu.cn state key laboratory for novel software technology nanjing university nanjing chinayichi zhang smail.nju.edu.cn state key laboratory for novel software technology nanjing university nanjing china zhihong zhao zhaozhih nju.edu.cn state key laboratory for novel software technology nanjing university nanjing chinazhenyu chen zychen nju.edu.cn state key laboratory for novel software technology nanjing university nanjing china abstract rich datasets have empowered various deep learning dl applications leading to remarkable success in many fields.
however data faults hidden in the datasets could result in dl applications behaving unpredictably and even cause massive monetary and life losses.
to alleviate this problem in this paper we propose a dynamic data faultlocalization approach namely dfaulo to locate the mislabeled and noisy data in the deep learning datasets.
dfaulo is inspired by the conventional mutation based code fault localization but utilizes the differences between dnn mutants to amplify and identify the potential data faults.
specifically it first generates multiple dnn model mutants of the original trained model.
then it extracts features from these mutants and maps them into a suspiciousness score indicating the probability of the given data being a data fault.
moreover dfaulo is the first dynamic data fault localization technique prioritizing the suspected data based on user feedback and providing the generalizability to unseen data faults during training.
to validate dfaulo we extensively evaluate it on cases with various fault types data types and model structures.
we also evaluate dfaulo on three widely used benchmark datasets.
yang feng and zhenyu chen are the corresponding authors.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse december san francisco ca usa association for computing machinery.
acm isbn .
.
.
.
results show that dfaulo outperforms the state of the art techniques in almost all cases and locates hundreds of different types of real data faults in benchmark datasets.
ccs concepts software and its engineering software testing and debugging .
keywords fault localization deep learning testing data quality acm reference format yining yin yang feng shihao weng zixi liu yuan yao yichi zhang zhihong zhao and zhenyu chen.
.
dynamic data fault localization for deep neural networks.
in proceedings of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering esec fse december san francisco ca usa.
acm new york ny usa pages.
introduction deep learning dl based systems have achieved tremendous success in a wide range of applications such as facial recognition autonomous driving medical diagnosis etc.
compared to traditional code driven software dl based systems adopt a datadriven programming paradigm and thus face extra security and reliability challenges related to data faults e.g.
mislabeled data samples .
such faults can be inherited by dl models that are trained on the data resulting in significant losses and even fatalities .
therefore guaranteeing the quality of datasets becomes a significant quality assurance aspect for dl based systems.
related work.
various quality assurance techniques specifically designed for dnn models have been proposed including diagnosing incorrect training code and hyperparameter settings locating misbehavior inside the model neurons generatingesec fse december san francisco ca usa yining yin yang feng shihao weng zixi liu yuan yao yichi zhang zhihong zhao and zhenyu chen adversarial inputs and repairing the dl model misbehavior .
the above techniques mainly aim at finding the vulnerability in a well trained model but neglect the root cause of such vulnerability i.e.
the faults in the dataset.
such faults may propagate to the models in an implicit way through the model s internal parameters and fundamentally influence the performance and quality of dl applications .
in the machine learning community learning with noisy labels lnl has been proposed to handle the potential noises in the dataset .
these works aim to train a robust and accurate model even with the presence of label noises.
however lnl techniques fail to debug the dataset and improve the data quality and thus cannot guarantee the performance of downstream dl applications .
according to a recent survey conducted by ibm of enterprises encounter data challenges in their ai projects and of them are not confident in ensuring data quality.
recently a few research groups have also conducted empirical studies and advocated focusing on the data aspect rather than the model aspect when developing dl applications.
for example liang et al.
reveal that the design and sculpting of the data used to develop ai often rely on bespoke manual work which critically affects the trustworthiness of the model.
sambasivan et al.
reported that a large number of experienced developers are impacted by at least one data cascade compounding event which leads to negative downstream effects from data issues.
curtis et al.
have found systematic labeling errors ranging from .
to .
in popular ai benchmark datasets which destabilizes the evaluation of dl benchmarks .
in a nutshell both industry and academia are calling for an effective and efficient data centric approach to improve the data quality for dnns.
our work.
in this paper we introduce dfaulo adynamic data faultlocalization approach for deep neural networks.
the key insight of dfaulo is to incorporate the manual inspection feedback into the inspection process and thus prioritize the suspicious data adaptively.
while existing techniques are designed to prioritize the data into a static queue for manual inspection dfaulo adopts a different method that dynamically updates the data queue based on the feedback of each round of inspections.
dfaulo migrates the classic mutation based code fault localization techniques to obtain effective data fault features.
by generating multiple dnn model mutants from different dimensions of the original model we propose and extract fault features based on the prediction behavior of each mutant.
then dfaulo models the suspiciousness model with extracted features and dynamically updates the model with collected feedback.
specifically the key insight of dfaulo is two fold.
first dfaulo amplifies and captures the difference between clean data and sparsely distributed faulty data in the dataset via model mutation .
in the classic mutation based code fault localization mutating the program statement could amplify the execution difference of the test suite.
that is faulty statements tend to perform differently compared to correct statements when two groups of mutants are generated one that mutates a faulty statement and the other that mutates a correct statement.
to apply this idea to data fault localization we introduce a set of model mutation strategies to fine tune the original dnn model into multiple slightly changed mutated models.
these modelmutation strategies ensure that dnn mutant has a similar prediction capability on clean data compared to the original model but may not well fit the sparse faulty data.
in other words the mutated model amplifies the difference between clean and faulty data.
second dfaulo spots the potential faulty data via an iterative way where the feedback obtained from the manual review is utilized to dynamically enhance its performance and generalizability to unseen data faults during the training.
specifically since data distributions change in practice and it is impossible to collect all types of data faults during training we propose a dynamic data fault localization workflow that can adapt to the real world distribution of data faults.
our approach employs a logistic regression model called susp to predict the suspicious faulty data.
we then use previously confirmed faulty data to continuously update the susp allowing the dfaulo to dynamically prioritize suspected data and improve the effectiveness of subsequent data fault localization.
to validate the effectiveness of dfaulo we conduct a comprehensive experiment on subjects of different data fault types model structures data types and dl tasks.
the experimental results show that dfaulo can effectively locate different types of data faults and the model performance can be improved by correcting the located faults.
additionally we also conduct a case study on three widely used benchmark datasets including emnistletter imagenet and mtfl .
compared with baseline methods dfaulo locates more real faults in the emnist letter dataset.
and for more complicated benchmark datasets hundreds of real faults of different types are confirmed by dfaulo .
contributions.
the contributions of this paper include technique.
we propose dfaulo a data fault localization technique for dynamically locating data faults in dl datasets.
to the best of our knowledge dfaulo is the first technique that incorporates the model mutation method to locate data faults in a dynamic manner.
since it is designed upon features extracted from general attributes of dnns dfaulo is applicable for various types of deep learning datasets.
tool.
we have implemented dfaulo into a tool that can automatically locate data faults hidden in the deep learning datasets.
we have also released the source code and presented a demonstration at .
study.
we have conducted a large scale experiment with subjects containing both text and image data to validate the effectiveness of dfaulo .
further we apply dfaulo to three widely used deep learning benchmark datasets and locate hundreds of real data faults.
this result further confirms the practicability of dfaulo .
preliminary and background this section introduces the background knowledge of deep learning and states the definitions of data fault and data fault localization.
.
deep learning .
.
model training.
a deep neural network dnn model can be denoted asf x y which is essentially a mapping from the input domainxto the output domain y. take image classification as an example.
the input x x is an image and the output y y is a one hot vector representing the image s label over the label space.
typically training a dnn model often requires a large dataset denoted as d x1 y1 x2 y2 ... xn yn .
given adynamic data fault localization for deep neural networks esec fse december san francisco ca usa dataset dd x y web searchautomatic collection......specialist crowdsourcingauto annotation model......datalabeloutlier datapoor qualityhuman errormodel errordata faults d!
x!
y!
d!
we have id lesser panda lesser pandaschipperkecurly coated retriever figure labelmismatch!
data faultsreference imagesincorrect labelpoor quality dataimproper data label2.
y y s.t.
x!
y s.t.x!
y .
y y!
id curly coated retriever id giant panda id soccer ball id schipperkeid 00035571id 00031356id 00014766ilsvrc2012 val figure detected data fault samples in the imagenet dataset.
loss functionl f x y such as cross entropy and an optimization strategy such as stochastic gradient descent sgd the model parameters can be updated based on the training set d. after sufficient epochs of training on d we can obtain proper internal parameters of the dnn model and deploy it in the application to predict labels for unseen inputs.
essentially just as the code faults explicitly influence the program behavior through test case execution defects hidden in the dataset impact the model s internal parameters during training which in turn leads to potential vulnerabilities in the model .
.
.
data preparation.
data preparation is an integral part of deep learning and it includes collecting cleaning and labeling the data samples.
among them data labeling is the most laborintensive.
benefiting from online crowdsourcing platforms such as amazon mechanical turk amt appen previously known as crowdflower and prolific the developers or data engineers of dl applications can hire a large number of crowd workers to manually label the raw data.
although such online platforms provide diverse solutions to help data engineers adjust their data labeling tasks it is still inevitable to include various errors biases and noises into the dataset .
therefore developers or data engineers of dl applications are required to ensure the data quality and thus improve the generalizability and reliability of dl models trained on the dataset .
.
problem statement .
.
data fault.
data faults refer to the mismatches between data and labels in the dataset which can occur due to poor data quality or mistakes made by human organizers or annotators .
ideally the data instance in the dataset should associate with and only with one label however in practice due to various noise or mistakes some data might associate with no or more than one label.
in that situation these data may be faulty and could threaten the performance of deep learning models.
thus to clarify the problem this paper is dealing with we assume each data is associated with a label set.
we define data faults as follows definition data fault .letxdenote the data instance set andydenote the label set.
suppose x xis an input data instance in a manually labeled dataset d x y x x y y we can denote the manual label of xasyand its ground truth label set as yx.
based on this definition we say x y is a correctly labeled data if and only ifyxcontains the only label y. therefore in the manuallyconstructed dataset dcontaining faults we say xf yf is adata fault if y yf y yx or yx .
moreover if there exists the only y y such that xf y is not a data fault we name the corresponding fault type as label noise otherwise the fault is categorized as data noise .
figure presents some examples of data faults detected in the image dataset imagenet .
the first example image lesserpanda is classified into the incorrect class which can be corrected by modifying the class label.
we name this type of data fault as label noise which assumes the data sample is labeled into the incorrect classes.
the second example image schipperke does not include all appropriate labels.
to correct such data the input should be cropped or modified before re annotating.
the third example image curlycoated retriever has poor quality and we cannot infer any label from its input.
to correct such data we should delete it from the dataset.
we name the fault type of the second and third examples as data noise because correcting such data fault requires modifying the input data.
.
.
data fault localization.
in classic software engineering research fault localization techniques aim to yield a ranking list of program statements according to their suspiciousness.
a higher suspiciousness score indicates the statement is more likely to contain faults i.e.
the buggy code that causes failures .
inspired by the classic fault localization problem we migrate the concept of fault localization into the dnn dataset debugging and propose to assist data engineers by ranking the data samples so that data faults can be prioritized for review and correction.
we next define the data fault localization problem definition data fault localization problem .let df dbe a subset of the dataset d which contains all data faults ofd.
the data fault localization problem is to find a suspiciousness functionsusp d rsuch that xf yf dfand x y d df we have susp xf yf susp x y .
approach this section presents the proposed approach dfaulo .
we first present the overall idea and then detail its three steps.
.
the key idea our approach is built upon the insight that the identified data faults represent the distribution of potential faults in the dataset and utilizing inspection feedback can improve the fault localization capability.
therefore designing a dynamic data fault localization technology that introduces feedback information into subsequent data inspection is a natural idea.
to locate data faults dynamically we need to model a suspiciousness function that can be updated with feedback.
by extracting the fault features of each data point we can dynamically fit a suspiciousness model susp based on the fault features.
therefore we have to design a fault feature extraction method that can reflect the differences between clean and faulty data.
considering that dnn models initially learn and fit the primary features of the dataset models fine tuned on the majority subset are expected to fluctuate on data faults that are 1this type of data is also known as outlier or out of distribution data.esec fse december san francisco ca usa yining yin yang feng shihao weng zixi liu yuan yao yichi zhang zhihong zhao and zhenyu chen model structure dl library training code train data outlier degree activation cluster prediction lossfinetune the model with subset datadnn mutated models select subsets with primary data featurecompose selection stateoutput resultprediction lossexecute all training data on original and mutated models extract fault features based on dnn behaviors fault features of each data train a regression model to produce the suspiciousness generate artificial faults collect detected real faults rank training data with suspiciousness regression model prepare positive negative samples manually inspect and repair data faults inspect data fault correctionrepair model subsets train data figure an overview of the data fault localization approach dfaulo .
sparsely distributed and account for a small proportion of the set.
based on this insight we introduce the concept of model mutation to amplify the behavior differences between clean data and data faults.
specifically dfaulo first fine tunes the original model to generate multiple model mutants then designs and extracts fault features for each data point based on the behavior divergence of model mutants.
with distinctive fault features and a dynamic iteration process design dfaulo can update the suspiciousness model susp based on feedback to locate diverse data faults in application.
the overview of our approach dfaulo is shown in figure which consists of three key steps.
first to amplify and capture the divergence between clean data and data faults in the training dataset d the primary features of the data from different model layers are sampled to generate model mutants which is described in detail in section .
.
second each data sample in dis predicted by the model mutants and their corresponding features related to the selection state the output result and the prediction loss are extracted as data fault features.
the feature extraction method is presented in section .
.
finally based on the extracted features a dynamic data fault location approach is proposed.
a logistic regression model susp is applied to map the fault features into suspiciousness scores for each data sample.
to improve the localization precision we propose to update the susp with feedback gathered from previously checked data which is detailed and expressed in section .
.
.
model mutation strategy locating faults via mutation has been discussed in conventional fault localization apporaches .
if the faulty program statement is mutated failed test cases will pass on mutated programs if the correct statement is mutated previously passed test cases may fail.
therefore the mutation strategy could amplify the behavior between clean components and faults.
migrating such an idea into deep learning if the original model is fine tuned and the prediction capability is almost unchanged the clean data will be predicted correctly as the original model and the fault data may be predicted differently if the model mutant is not over fitted to them.
the remaining problem is how to fine tune the model so as to maintain an almost unchanged model mutant.
to this end we design various mutation strategies that could keep the majority of the dataset to mutate the original model so that the model mutant is slightly different from the original one and outputs similar predictions to samples with typical features.to implement this design we sample the subset d based on the neuron outputs across the input hidden and output layers of the original dnn model f. because a dnn model contains multiple hidden layers and deeper layer outputs are considered to provide more abstract information we mainly employ the last linear layer to represent to hidden layer s information in this paper.
for each layer we identify a small ratio subset of data with sparse features which are denoted as d1 out d2 out andd3 out and discard them to obtaindi d di out.
after that the model mutants are generated by retraining the original model ffor several extra epochs on di to slightly change the original model which is lightweight than train the model from scratch.
denoting the predefined remove ratio as we next present the details to obtain each di out.
.
.
input layer .for the input layer we capture the data features by detecting input outliers from the input space.
specifically we adopt a classical method namely vae i.e.
variational autoencoder as the outlier detector.
it builds an auto encoder to extract the low dimensional features of each data.
if an input x x has a larger reconstruction error it is supposed to have more suspicious data features and be an outlier.
denote the reconstruction error ofxase x .
the data is sorted according to the value of e x and the prioritized of the largest errors is discarded.
the subset to be removed is formally denoted as d1 out x y x y d arg max x x x x e x .
the remaining samples d1 d d1 outis then applied for train the model mutantf1.
.
.
hidden layer .the hidden neurons of the model carry the features learned from the training data and the deeper layer of the network represents more abstract information about model behaviors .
in the training dataset neuron activation states of the clean data corresponding to the same label are similar while a small amount of noise data may trigger different neuron activation .
therefore we choose to remove the data with deviations in the distribution of neuron activation states.
for each input data x x we collect the activation vector of the last linear layer for analysis which is denoted as f x .
then we adopt k means to cluster the activation states as f x x x into two clustersas1 as2.
obviously the sparse features would belong to a minor cluster thus for each dataset we remove the data in the smaller cluster to construct the subset.
suppose as1 as2 thedynamic data fault localization for deep neural networks esec fse december san francisco ca usa table features extracted for each x y in datasetd.
type formalization description ss1d1 x y data x y belongs tod1 1d2 x y data x y belongs tod2 1d3 x y data x y belongs tod3 ory original label yof inputx f x output result of input xon modelf f1 x output result of input xon modelf1 f2 x output result of input xon modelf2 f3 x output result of input xon modelf3 pll f x y prediction loss of data x y on modelf l f1 x y prediction loss of data x y on modelf1 l f2 x y prediction loss of data x y on modelf2 l f3 x y prediction loss of data x y on modelf3 subset to be removed is formally defined as d2 out x y x y d f x as2 .
similar to the input layer mutation we apply d2 d d2 outto train the second model mutant f2.
.
.
output layer .we analyze the model from the output perspective based on prediction loss.
if the dnn fails to fit the data due to unrecognizable or mismatched labels the corresponding prediction loss will be large.
hence based on the output layer we choose to remove the data with high prediction losses.
denoting the loss function of dnn as l f x y the removed data is formalized as d3 out x y x y d arg max x x x x l f x y .
the subsetd3 d d3 outis applied to train the mutant f3.
specifically for general dl tasks we construct the model mutant based on the above strategy and for categorical tasks we discard data for each category to construct the mutants.
.
fault feature extraction after we generate mutants that amplify the differences between clean and faulty data we extract a series of fault features for each data sample.
the fault features are listed in table which can be categorized into three classes selection state ss output result or and prediction loss pr .
the selection state ss indicates whether corresponding data belong to a sparse feature in the training set i.e.
whether we apply it for model mutation.
for each data x y in the original training set the selection state is marked as 1if the model mutant applied it for training otherwise it is marked as .
therefore the selection state of each model mutant is a discrete one dimensional vector.
we use the characteristic function 1xto indicate the selection state and x is regarded as the training dataset.
the output result or represents the output vector of the dnn model when predicting each data which reveals the execution differences between mutants and the original model.
thus the or features are continuous vectors with the same dimension as the dnn model s output.
for prediction loss pl features we adopt the same loss function used in the model optimization process.
the loss function may be variant for different models and tasks but we can determine the unique loss function lapplied for a specific f. the pl features are continuous onedimensional variables.
compared to or features which preserve detailed differences between mutants pl features directly indicate the gap between prediction and label.
in summary these fault features can be easily extracted through prediction and reveal the behavior differences from diverse aspects.because these features are general attributes of a dnn model our approach could be applied to dnn datasets trained on arbitrary model structures or loss functions.
.
dynamic data fault localization for a dataset under review without a standard reference dataset trusted clean data and real faults are both scarce.
meanwhile due to the differences in data collection methods data sources annotator labeling habits etc.
data faults are distributed differently among various datasets.
as a result designing a universal suspiciousness score based solely on static scores fault features is not appropriate for detecting diverse data faults.
therefore inspired by traditional fault localization techniques we choose to model a dynamically updated logistic regression model to predict the suspiciousness score.
denoting the employed model as susp the feature set of data sample d x y extracted from table as f d f1 d ... fm d and the weights correspond to each variable as w w0 w1 ... wm the fault probability computed by logistic function is susp f d e w0 m i 1wifi d .
with a set of positive and negative samples the internal weights wof thesusp could be optimized.
the basic idea is to adjust the susp using already checked data which is a natural and convenient process for data collection.
in the following we will explain how to initialize and dynamically update the susp .
initialization.
initially the entire dataset has not been checked by workers and thus we do not have enough reliable positive data fault and negative clean data samples for training the susp .
to overcome this issue we use the following strategies to initialize the susp generate artificial positive samples by constructing noisy vectors prepare artificial negative samples by randomly sampling from the original dataset.
the reason is two fold noisy vector distributes more outlier than real data faults making them effective for macroscopic modeling of fault features our method is based on the assumption that the majority of the dataset is correctly labeled and thus most of the raw data are clean.
supposing the artificial positive and negative data are d0pandd0nrespectively.
their corresponding fault features are denoted as f d0p f d0n .
therefore without acquiring any manual feedback we train the susp model on these artificial samples to initialize it.
then the initializedsusp computes the suspiciousness score for each data sample x y dand ranks them in descending order.
updating.
based on the trained susp all unchecked data are ranked and a batch of high suspiciousness score data is provided to the annotators.
with the assistance of data collection platforms workers can review and report incorrect or improper data samples.
the platform allows testers to analyze the results and determine the next batch of data to be checked.
inspired by online learning we propose a dynamic data ranking workflow that adjusts thesusp based on feedback from annotators.
the model adapts to the specific distribution patterns of the current dataset producing a more accurate ranking for subsequent data.
for the k th iteration our goal is to sample a subset dkfor manual checking.
first we take the union of previously checked subsets d1 ... dk denoted asd1 k .
the identified data faults in d1 k 1are flagged as real positive samples d1 k p other samples are flagged as real negativeesec fse december san francisco ca usa yining yin yang feng shihao weng zixi liu yuan yao yichi zhang zhihong zhao and zhenyu chen table experiment combinations conducted in this paper.
id dataset model domain task fault type mnist lenet image classify digits random label noise mnist lenet image classify digits specific label noise mnist lenet image classify digits random data noise mnist lenet image classify digits specific data noise mnist lenet image classify digits random label noise mnist lenet image classify digits specific label noise mnist lenet image classify digits random data noise mnist lenet image classify digits specific data noise cifar10 resnet image classify objects random label noise cifar10 resnet image classify objects specific label noise cifar10 resnet image classify objects random data noise cifar10 resnet image classify objects specific data noise cifar10 vgg image classify objects random label noise cifar10 vgg image classify objects specific label noise cifar10 vgg image classify objects random data noise cifar10 vgg image classify objects specific data noise agnews lstm text classify corpus random label noise agnews lstm text classify corpus specific label noise agnews lstm text classify corpus random data noise agnews lstm text classify corpus specific data noise agnews bilstm text classify corpus random label noise agnews bilstm text classify corpus specific label noise agnews bilstm text classify corpus random data noise agnews bilstm text classify corpus specific data noise mtfl tcdcnn image face alignment label noise mtfl tcdcnn image face alignment data noise samples d1 k n .
the model susp is then trained using f d1 k p andf d1 k n .
we apply the updated susp to compute the suspiciousness score for all remaining unchecked data x y d d1 k and rank them in descending order.
the subset dkis sampled by the updated susp .
utilizing the feedback of previously checked data this process is repeated until the review budget is reached and the final data queue for manual checking is dynamically generated.
experiment settings this section presents the experimental setup including the description of experiment subjects baseline methods parameter settings and evaluation metrics.
.
experiment subjects we employ combinations of datasets dnn models and data fault types as experiment subjects.
table lists all combinations that show the diversity in three aspects a data types and tasks.
we apply dfaulo to two image classification datasets mnist and cifar one text dataset agnews and one face alignment dataset mtfl.
mnist is a class handwritten digits dataset which contains 28greyscale images for training and for testing.
cifar is a colored class dataset and each input is a channel 32image.
agnews is a subset of ag s corpus of news articles constructed by assembling titles and description fields of articles from the largest classes.
multi task facial landmark mtfl dataset contains face images with five coordinates of facial landmarks.
the first three datasets i.e.
id are constructed for classification tasks and the last dataset mtfl id is for regression tasks.
b network structures.
we implement models with different network structures in the experiment including convolutional neural networks cnns for image domain datasets i.e.
lenet lenet resnet vgg and tcdcnn as well as classical recurrent neural networks rnns for text domain datasets i.e.
lstm and bilstm .
c fault types.
since data faults have different forms for each model dataset combination we use multiple ways to simulate different types of data faults.
as defined in definition we considertwo types of data faults i.e.
label noise anddata noise .
additionally based on the observation that the distribution of data faults impacts the classification effectiveness we also simulate two types of data distributions i.e.
symmetric and asymmetric .
symmetric noise also known as random noise supposes the faults are uniformly distributed among each class .
for random label noise we randomly select data from each class and change their label into a random label other than the original one for random data noise we randomly select data from each class and replace their inputs with irrelevant data.
in practice label noise often exists between similar classes and data noise can be associated with only a specific class rather than all classes.
to simulate this phenomenon we introduce asymmetric noise.
for specific label noise we select data from two classes and exchange their labels for specific data noise we randomly select data from one class and replace it with irrelevant data.
let a brepresent that we use dataset ato replace data in dataset b. we apply famous datasets in our experiments to inject data faults denoted as cifar mnist imdb agnews and coco mtfl.
.
baselines we employ seven baselines including three learning with noisy label lnl methods cleanlab semifeat and ncnv two test prioritization techniques deepgini and deepstate one advanced outlier detection method dif and one model uncertainty metric denoted as uncertainty .cleanlab is an open sourced label issue detection tool based on the confident learning algorithm.
it can be applied to classification datasets with any ml models and data types.
semifeat is a data centric noisy label clean technique aimed at identifying corrupt data labels in datasets without relying on training a model.
ncnv is a method called neighborhood collective noise verification which measures the degree of inconsistency to filter out the noisy data.
difis a novel outlier detection method which applies neural networks and isolation forests to detect outliers.
test prioritization is designed to select the data predicted incorrectly by the model.
since dnns with good generalization should also make incorrect predictions on faulty data these methods are valuable as baselines for evaluating their effectiveness in detecting data faults.
deepgini is a simple but effective test prioritization method for dnn with the softmax output layer.
deepstate is a test selection method specifically designed for recurrent neural networks rnns which selects error tests based on the internal states of the rnns.
the above methods cannot be applied to the regression model and we introduce a general method uncertainty as a baseline method which evaluates the model uncertainty for dnn with a dropout layer.
other prioritization and lnl techniques are omitted because they require to be trained on a trusted training set or do not involve a rank for data inspection .
besides other detection techniques such as concept drift detection which do not focus on wrongly labelled data are not considered either.
.
parameter settings to implement dfaulo we need to set some hyper parameters.
first for model mutation strategy the implementation of vae is based on the open sourced python tool pyod .
the remove ratio is set as .
we mutate the original model by directly retraining the original model the retraining epoch is set as and thedynamic data fault localization for deep neural networks esec fse december san francisco ca usa learning rate is the same as the setting of model training.
to simulate the updating process of susp we set the iteration batchsize as and update the model susp per batch to prioritize the next batch of unchecked data.
the maximum ratio used for updating susp i.e.
review budget is after that we rank all the unchecked data based on the last susp .
finally the whole data rank is reordered based on the iteration batch to evaluate the effectiveness of dfaulo .
.
evaluation metrics we employ three metrics i.e.
pobl rauc and auc roc to evaluate the effectiveness of baselines and dfaulo .
pobl refers to the proportion of located data faults to all data faults in the top data it measures the effectiveness of localization with a fixed manual budget.
furthermore to measure the overall localization effectiveness we plot the localization performance of each subject into a figure where the x axis is the ratio of data ranked bydfaulo or baselines and the y axis is the ratio of fault already detected.
we also plot the theoretically best localization performance in which all faults are prioritized in front of other data.
we calculate the ratio of the area under the curve for the data fault localization approach to the area under the curve for the ideal data rank as another evaluation metric named rauc .
rauc ranges from to and a larger rauc value indicates better performance.
considering the fact that the fault localization problem can be naturally modeled as a binary classification task we also employ the auc roc metric to evaluate the performance.
different from rauc auc roc indicates the effectiveness of selecting a threshold for classifying positive and negative samples.
results and discussions in the experiment we mainly focus on the following four research questions rqs rq1 effectiveness.
how effective is dfaulo in locating data faults in the dl datasets?
rq2 ablation study.
what is the impact of each component in the dfaulo design on the effectiveness?
rq3 efficiency.
what is the computational efficiency of dfaulo compared to baselines?
rq4 potentials.
to what extent can dfaulo improve the performance of dnn models via locating the data faults?
additionally to evaluate the effectiveness in real application scenarios we conduct a case study using benchmark datasets to explore thedfaulo s superiority in locating natural existence data faults.
.
rq1 effectiveness tables and show the results on subjects with classification and regression tasks respectively.
to mitigate bias we run the whole experiment five times and report the average results.
in table and we list the results based on data fault types.
additionally figure shows localization curves indicating the proportion of located faults versus the proportion of prioritized data.
results.
lnl techniques including cleanlab ncnv and semifeat perform well rauc .
in most subjects with label noises table row which is significantly better than other baselines.
for datasets with random data noises row most baselines show competitive performance rauc .
but ncnv anddifperforms poor on lenet rauc .
and agnews rauc .
respectively.
compared with cnn models the resultsof all methods on rnn models are less effective.
the rnn specific method deepstate performs better on data noise than label noise.
in table uncertainty shows poor performance in all evaluation metrics pobl .
rauc .
and auc roc .
.
for most of the experiment subjects the rauc and auc roc metrics of dfaulo are more effective and stable all results exceed .
compared to baselines.
additionally for the evaluation metric pobl dfaulo even reaches the maximum value of .
on some subjects which means it locates all data faults in the top list.
it can also be observed that the curves of dfaulo in figure are closer to the theoretically best curves than other baselines.
discussion.
according to the results dfaulo outperforms other baselines in both classification and regression datasets with consistent effective results.
the results suggest that uncertainty has no significant effect on locating data faults which is likely due to the model s overfitting to noisy data in the training set.
for the first three groups of fault types table row lnl techniques are more effective than other baselines and cleanlab performs better than other lnl techniques.
all lnl techniques failed to demonstrate effectiveness and perform even worse than random ranking for specific data noises.
on the other hand we notice that outlier detection method difis good at locating image data noises but shows bad performance on locating label noises and text data noises.
two test prioritization methods deepgini anddeepstate are effective in identifying random data noise but are not sufficiently sensitive to label noise.
considering the fact that all of test prioritization techniques are designed for selecting the unlabelled data for manual labelling we speculate this experiment result fits their design choice well.
besides ncnv requires to train dnn models with specific designed process the default training setting may not convergence for ncnv this property illustrate its poor results on mnist lenet .
our findings indicate that existing methods based on static data ranking become useless in finding faults in dataset with more diverse data faults.
additionally combined with the results of table and table dfaulo shows stable effectiveness in all fault types dataset domain model structure and task types demonstrating its superior generalizability.
.
rq2 ablation study we construct an ablation study to investigate the effectiveness of dfaulo s designs.
specifically we evaluate the localization performance of the suspiciousness function susp without utilizing all model mutants and without dynamic iteration respectively.
to mitigate bias and reach a statistically significant result we repeat experiments on each subject times.
also we employ the wilcoxon rank sum test and cliff s analysis to analyze whether there are significant differences between the ablation setting and the design of dfaulo .
following the conventions of previous work if the rank sum significance level p .05and effect size .
we mark the corresponding setting is significantly worse or better than dfaulo .
limited by space we select experiment subjects and report the average rauc in table .
results.
for most subjects the initialized susp model without feedback shows competitive performance rauc .
compared to the best baselines in table and for challenging experimental settings such as id specific data noise in agnews bilstm without the aid of feedback the initialized suspesec fse december san francisco ca usa yining yin yang feng shihao weng zixi liu yuan yao yichi zhang zhihong zhao and zhenyu chen table the overall effectiveness of dfaulo and baselines on classification datasets with different fault types.
faultdataset modelpobl rauc auc roc dfaulo baselines dfaulo baselines dfaulo baselines type cleanlab semifeat nvnc dif deepgini deepstate cleanlab semifeat nvnc dif deepgini deepstate cleanlab semifeat nvnc dif deepgini deepstaterandom label noisemnist lenet1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
mnist lenet5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
cifar10 resnet20 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
cifar10 vgg16 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
agnews lstm .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
agnews bilstm .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.666sepcific label noisemnist lenet1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
mnist lenet5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
cifar10 resnet20 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
cifar10 vgg16 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
agnews lstm .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
agnews bilstm .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.686random data noisemnist lenet1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
mnist lenet5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
cifar10 resnet20 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
cifar10 vgg16 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
agnews lstm .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
agnews bilstm .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.895specific data noisemnist lenet1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
mnist lenet5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
cifar10 resnet20 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
cifar10 vgg16 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
agnews lstm .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
agnews bilstm .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
for each experiment subject we bold the maximum evaluation results to highlight the technique with the best performance.
deepstate can only be applied to rnn models.cifar resnet .
.
.
.
.
.
percentageoftestcaseexecuted0.
.
.
.
.
.0percentageoffaultdetected theory best deepgini cleanlab dif simifeat ncnv random dfaulo a random label noise .
.
.
.
.
.
percentageoftestcaseexecuted0.
.
.
.
.
.0percentageoffaultdetected theory best deepgini cleanlab dif simifeat ncnv random dfaulo b specific label noise .
.
.
.
.
.
percentageoftestcaseexecuted0.
.
.
.
.
.0percentageoffaultdetected theory best deepgini cleanlab dif simifeat ncnv random dfaulo c random data noise .
.
.
.
.
.
percentageoftestcaseexecuted0.
.
.
.
.
.0percentageoffaultdetected theory best deepgini cleanlab dif simifeat ncnv random dfaulo d specific data noise figure data fault localization curves.
x axis indicates the percentage of prioritized data with the highest suspiciousness and y axis indicates the percentage of located data faults.
table data fault localization effectiveness on face alignment task of different fault types.
idfault pobl rauc roc auc type dfaulo uncertainty dfaulo uncertainty dfaulo uncertainty 25label0.
.
.
.
.
.
noise 26data0.
.
.
.
.
.
noise also shows a poor result rauc .
like other static baseline methods.
besides employing all three model mutants to construct susp model can more effectively locate data faults and dfaulo wins out of comparisons rows compared to the last row .
even with only human feedback the localization performance can be improved especially for difficult data fault types the average rauc of id is improved from .
to .
.
discussion.
first using dnn model mutants generated from different dimensions of the model provides dfaulo with effective fault features for locating diverse types of faults.
besides when susp models leverage no human feedback the result can be seen as a static technique.
in this case such static faulo exhibits both performance and bottlenecks similar to state of the art static baseline methods.
the dynamic iterative design of dfaulo is the key to breaking through the predicament of static techniques.
for those data fault types that are difficult to analyze and identify by static techniques dfaulo can promptly utilize feedback to learn unknown features of data faults.
finally taking advantage of moretable average rauc results of ablation study on experiment subjects.
models feed id applied back?
input .
.
.
.
.
.
.
.
.
.
.
.
hidden .
.
.
.
.
.
.
.
.
.
.
.
output .
.
.
.
.
.
.
.
.
.
.
.
all .
.
.
.
.
.
.
.
.
.
.
.
dfaulo .
.
.
.
.
.
if the result wins dfaulo webold the corresponding result and if it loses to dfaulo we mark it as gray.
feedback can better improve dfaulo s performance.
thus introducing the dynamic iterative design into online crowdsourcing tasks can maximize the dfaulo s advantages over other static methods.
.
rq3 efficiency we collect and report the average execution time of different methods in table .
on the one hand we compare dfaulo with other baseline methods on the other hand since dfaulo composes multiple steps we also collect the separate computation time of dfaulo .
the top half of the table reports the time cost by baselines and the last half of the table reports the overall row all time and separate time cost for each operation of dfaulo .dynamic data fault localization for deep neural networks esec fse december san francisco ca usa table average execution time seconds of baseline methods and dfaulo .
dataset mnist cifar agnews mtfl model lenet lenet resnet vgg lstm bilstm tcdcnnbaselinedeepgini deepstate uncertainty dif cleanlab semifeat ncnv dfauloall select subset mutation extraction initializesusp .
.
.
.
.
.
.
updatesusp .
.
.
.
.
.
.
results.
table shows deepgini deepstate uncertainty anddiftake less than seconds for all experiments which are the most efficient techniques.
notwithstanding being effective in rq1 lnl techniques cleanlab ncnv and semifeat manifest poor efficiency.
while dfaulo s efficiency may not be the best it is acceptable as all subjects can be ranked within an hour 3000s .
model mutation and feature extraction exhaust the majority of time indfaulo while initializing and updating the susp model is rather efficient which only requires less than seconds.
discussion.
deepgini anddeepstate are efficient due to their use of simple model behaviors during the prediction and calculation of static scores for ranking.
however the original model may overfit the training dataset and such static scores are not effective for data fault localization.
cleanlab adopts cross validation to train multiple models and compute confusion matrices which requires much larger training epochs and has limitations in terms of applicable dataset types.
ncnv needs to train two models with a specific method from scratch which takes the longest training time.
semifeat does not need to train the model but it expends much more computation on data processing and repeated execution.
dfaulo requires retraining the original model but only epochs are sufficient making its efficiency acceptable.
in terms of dynamic iteration efficiency updating susp after each iteration is quite efficient .
11s as all fault features are extracted statically beforehand.
therefore incorporating dfaulo into the data correction workflow would not consume excessive time for testers.
.
rq4 potentials intuitively if we can locate faults in the dataset and correct them we can obtain a clean dataset.
therefore we are curious about whether we can remedy some defective behaviors by retraining the model with a cleaned dataset.
based on the data ranking computed by each method we correct the data faults in the top as follows inputs with incorrect labels are corrected and irrelevant inputs are deleted.
then we retrain the original model and compare the accuracy fluctuations between different methods.
in addition we also analyze the effect of combining dfaulo with other training methods in table .
to evaluate whether dfaulo can enhance the effectiveness of such methods we implement two state of the art model centric lnl methods namely nlnl and dividemix and compare the model accuracy with and without dfaulo correction.
we prepared two training sets for each fault type one is with data faults injected and the other is corrected by dfaulo for the top data.table repairing defective dnns with each method.
fault type ori.dfaulo baselines cleanlab semifeat nvnc dif deepgini deepstatemnist lenet1 labelrand.
.
.
.
.
.
.
.
spec.
.
.
.
.
.
.
.
datarand.
.
.
.
.
.
.
.
spec.
.
.
.
.
.
.
.
lenet5 labelrand.
.
.
.
.
.
.
.
spec.
.
.
.
.
.
.
.
datarand.
.
.
.
.
.
.
.
spec.
.
.
.
.
.
.
.
cifar resnet20 labelrand.
.
.
.
.
.
.
.
spec.
.
.
.
.
.
.
.
datarand.
.
.
.
.
.
.
.
spec.
.
.
.
.
.
.
.
vgg16 labelrand.
.
.
.
.
.
.
.
spec.
.
.
.
.
.
.
.
datarand.
.
.
.
.
.
.
.
spec.
.
.
.
.
.
.
.
agnews lstm labelrand.
.
.
.
.
.
.
.
.
spec.
.
.
.
.
.
.
.
.51datarand.
.
.
.
.
.
.
.
.
spec.
.
.
.
.
.
.
.
.05bilstm labelrand.
.
.
.
.
.
.
.
.
spec.
.
.
.
.
.
.
.
.77datarand.
.
.
.
.
.
.
.
.
spec.
.
.
.
.
.
.
.
.
mtfllabel noise .
.
baseline .
data noise .
.
uncertainty .
for each subject we bold the top boxes with the highest accuracy increment.
table test accuracy on cifar resnet of different training methods with and without dfaulo .
training label noise data noise method random specific random specific ori.
.
.
.
.
dfaulo .
.
.
.
nlnl81.
.
.
.
dfaulo .
.
.
.
dividemix81.
.
.
.
dfaulo .
.
.
.
results.
table reports the model accuracy fluctuations when retraining the original dnn model with different fault localization methods.
for all combinations retraining the model with the dfaulo corrected dataset could improve the accuracy of the model.
consistent with the results of rq1 compared with other baseline methods dfaulo achieve the highest accuracy improvement in most subjects out of subjects .
table presents the effectiveness of dfaulo combined with lnl methods.
for the dividemix method trained on the noisy dataset the model accuracy is .
.
.
and .
respectively.
if dividemix is applied on a dataset cleaned by dfaulo the model accuracy increases to .
.
.
and .
respectively.
discussion.
based on table we observe that most defective dnn models can be well repaired if the data faults are corrected.
referring to table we draw the conclusion that dfaulo has better potential in repairing defective models via locating more data faults.
additionally the accuracy of different types of data fault indicates that specific data noise is most harmful to the performance.
based on the results of table dfaulo can assist in the lnl methods to overcome the defects in the dataset and improve the model performance.
this phenomenon also demonstrates the necessity of debugging the dataset before model training.esec fse december san francisco ca usa yining yin yang feng shihao weng zixi liu yuan yao yichi zhang zhihong zhao and zhenyu chen given patas suppose gorilla given barn suppose n a given toilet tissue suppose hamster given tub suppose jeans given cloak suppose n a given tub suppose n a given quilt suppose tabby catgiven grocery store suppose dungeness crab given hand held computer suppose space bar given rocking chair suppose n a given safety pin suppose strawberry given printer suppose desktop computergiven cockroach suppose cat mtfl given goose suppose n aimagenet emnist letter 800number of manual reported data faults random deepgini dif semifeat ncnv cleanlab dfaulo static dfaulo dynamic given oo suppose qq given gg suppose qq given nn suppose mm given bb suppose dd given jj suppose ii given bb suppose kk given mm suppose n a given bb suppose n a given nn suppose ee given nn suppose hh given jj suppose ss given ff suppose zz given gg suppose n a given vv suppose yy given gg suppose cc given ff suppose n a figure data faults detected by dfaulo in widely applied benchmark datasets.
.
case study on benchmark datasets .
.
comparing dfaulo with baselines.
to prove the superiority ofdfaulo over other baseline methods we deploy all methods on the same dataset emnist letter .
emnist letter is a dataset containing samples of handwritten letters.
the sota dnn model of emnist letter wavemix is employed for methods that require a dnn model.
for each baseline the top of the data is prioritized for manual inspection.
for our method we collect data prioritized by the initialized susp model noted as static faulo as well as data dynamically sampled by dfaulo .
each data sample is allocated for independent crowd workers to check.
each sample is scored on a scale of with higher scores indicating higher quality data and workers are asked to diagnose the type of fault and provide alternative labels.
if a sample receives three or more negative scores we mark it as a fault and use them for updating the susp .
the top left of figure reports the number of data faults found by each method.
detailed subsets of each method with the scores and diagnostic information for each data sample can be found on our website.
compared with the estimated data fault rate .
only two baseline techniques difandncnv failed to prioritize real data faults.
we assume this may be caused by their limited design intentions only designed to identify outlier noises or data with incorrect labels .
in addition cleanlab performs the best among all baseline methods with a fault rate of .
of the selected set.
consistent with the conclusions in rq2 the initialized susp denoted as static faulo almost achieves the best static method performance .
fault rate .
and when we introduce the dynamic design the dfaulo shows outstanding results than other methods.
dfaulo locates data faults out of manually inspected data and the corresponding fault rate is .
which demonstrates the superiority of dfaulo in real world application scenarios.
.
.
applying dfaulo on complicated benchmark datasets.
after demonstrating the superiority of dfaulo in identifying emnist data faults we further apply it to more complex benchmark datasets to uncover more diverse data faults.
for the classification task we conduct the case study on the large scale imagenet dataset utilizing a pre trained model with resnet architecture provided by pytorch .
we apply dfaulo on the validation set of imagenet e.g.
ilsvrc2012 which includes classes and images for each class.
consistent with the procedure described above after initializing the susp model the top data samples are prioritized for independent crowd workers to check.
for the regression task the mtfl dataset is selected for the case study we first follow the official suggestion to pre process the images to get a face bounding box and then apply dfaulo to locate data faults.
limited with resources we repeat the dynamic iteration times for imagenet and once for mtfl.
figure presents some detected real faults in these benchmark datasets.
the supposed label may be a substitute for the given label or coexist with the given label.
n a represents that the corresponding image cannot be categorized into a specific class.
in the mtfl dataset we present the images in which the automatically generated bounding boxes mismatch the face alignment labels.
based on the results in figure we find that both two fault types defined in definition actually exist in reality.
some input images lack recognizable features or have more than one object to classify.
other data have recognizable features but are labeled to the incorrect categories.
for example in the first row of figure the first image is labeled as barn while the main object in the image is an owl.
though the background of the image may indeed be a barn this input still lacks sufficient features of the barn for workers to label.
however owl is not a candidate label in imagenet ilsvrc2012 .
thus the crowd workers assume it is irrelevant data which not belong to the dataset.
besides if a suitable label exists we recommenddynamic data fault localization for deep neural networks esec fse december san francisco ca usa workers suppose an alternative label.
the second image in the first row labeled as tub is reported to be better labeled as jeans .
the third image in the first row is labeled as hand held computer while it can also be categorized into other classes such as space bar .
moreover comparing the data faults between imagenet and emnist we observe that the complexity of the input and tasks also leads to more diverse data faults.
compared to the study which used cleanlab to manually review half of imagenet s data dfaulo discovered many unnoticed data faults.
out of imagenet data received negative scores from at least one worker in our study but were not prioritized by cleanlab as the top half of the possible faulty data.
these newly found faults are highlighted in figure and more detailed information is available on our website.
threats to validity we noticed some threats in our study.
the internal threat of dfaulo comes from the errors committed by crowd workers.
the dfaulo leverages the outcomes of human feedback to dynamically update the modelsusp and imperfections in human feedback could negatively influence the effectiveness of the dfaulo .
we mitigate this threat by evaluating dfaulo on benchmark datasets which utilize authentic human feedback that inherently reduces inaccuracies and inconsistency for susp updating.
the external threat to validity lies in the subjects used in our experiment.
the selection of datasets and simulated data faults could influence the performance of the results.
to reduce this threat we construct model and dataset combinations with different data types and tasks network structures and fault types in the experiment.
moreover we conduct a case study on benchmark datasets to investigate the performance ofdfaulo in detecting real data faults.
all of these results show consistent effective results of dfaulo .
another threat comes from the randomness of all localization techniques.
first the randomness of dnn prediction could influence the ranking of faulty data which poses a threat to effectiveness comparison.
besides the randomness when initializing the susp model also poses a threat to the ablation study of dfaulo .
to reduce this threat we repeat all methods multiple times to collect average results and introduce statistical methods for analysis.
related works this section reviews some related works in fault localization and mitigation for deep learning based software testing.
repairing dl software codes.
similar to traditional software testing defects may also exist in the program codes of dl based software.
first faults may exist in dl libraries and then be inherited by a concrete implementation of the dl model.
pham et al.
proposed cradle to detect and localize bugs in dl libraries with differential testing.
wang et al.
proposed lemon to debug dl libraries by generating effective dl models via guided mutation.
wei et al.
proposed a fuzzing technique freefuzz to debug dl libraries based on open source apis automatically.
second code faults also exist in the model construct and training codes .
wardat et al.
proposed deeplocalize to identify the root cause code for dnn errors and the internal values are analyzed in both feedforward and backpropagation phases to identify and locate the incorrect codes and parameters.
zhang et al.
introduced autotrainer to test and repair the mode training process.adjusting neuron weights.
neuron networks rely on numerous hidden neurons to make a prediction.
once the model finishes training and is deployed to the application scenario its internal weights are frozen.
hence compared to code fault for a deployed trained dl model more potential defects may hide in the trainable parameters of the neurons.
eniser et al.
proposed deepfault to identify suspicious neurons whose weights are not calibrated correctly and impact the dnn performance.
sun et al.
proposed care to locate and modify the weights of suspicious neurons whilst maintaining general accuracy.
to reduce defect inheritance in transfer learning zhang et al.
designed a relevant model slicing method remos to avoid the model inheriting the neuron faults hidden in the teacher model.
in addition extensive deep learning testing papers attempt to improve dl model quality by retraining or fine tuning the model and the research subjects range from test selection to test generation .
debugging training data.
most of the abovementioned techniques assume that we have a trusted dataset for testing and repairing the dl model.
however such an assumption is fragile and hard to guarantee.
li et al.
proposed ltdd to debug feature values in training data and improve the fairness of machine learning models.
northcutt et al.
proposed an open source data debugging tool cleanlab to estimate and find label errors for classification datasets.
based on cleanlab northcutt et al.
conducted a large scale empirical study on the test sets of classical ml datasets and estimated there are at least .
errors across the datasets.
the relationship of dfaulo to the related work can be summarized into three folds it improves dl software quality in parallel with techniques that fix the code faults in the dl program it provides reliability guarantees for techniques that adjust the internal model weights with datasets instead of focusing on biased features which are hard to capture and repair dfaulo is designed to locate buggy data that can be interpreted and corrected manually.
conclusion this paper proposes a dynamic data fault localization technique named dfaulo .
data faults in the deep learning dataset can be dynamically prioritized by dfaulo and then workers can check and correct them without reviewing the whole dataset.
we demonstrate the effectiveness of dfaulo on a wide range of experiment subjects.
moreover we also apply dfaulo to widely used benchmark datasets and detect various real data faults.
our technique complements the gap in data quality assurance for the testing of dl based software systems and we will explore automatically repairing these located data faults in the future.
data availability all datasets used in the experiment can be easily accessed on their official websites.
the dfaulo and experiment source codes are released at we have also released the detected data faults in the case study and remaining experiment results at