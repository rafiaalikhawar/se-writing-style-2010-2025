trust enhancement issues in program repair yannic noller national university of singapore singapore yannic.noller acm.orgridwan shariffdeen national university of singapore singapore ridwan comp.nus.edu.sg xiang gao national university of singapore singapore gaoxiang comp.nus.edu.sgabhik roychoudhury national university of singapore singapore abhik comp.nus.edu.sg abstract automatedprogramrepairisanemergingtechnologythatseeks to automatically rectify bugs and vulnerabilities using learning search and semantic analysis.
trust in automatically generatedpatches is necessary for achieving greater adoption of program repair.towardsthisgoal wesurveymorethan100softwarepractitionerstounderstandtheartifactsandsetupsneededtoenhance trust in automatically generated patches.
based on the feedback from the survey on developer preferences we quantitatively evaluateexistingtest suitebasedprogramrepairtools.wefindthatthey cannotproducehigh qualitypatcheswithinatop 10rankingand anacceptabletimeperiodof1hour.thedeveloperfeedbackfrom ourqualitativestudyandtheobservationsfromourquantitative examination of existing repair tools point to actionable insights to drive program repair research.
specifically we note that producing repairswithinanacceptabletime boundisverymuchdependentonleveraginganabstractsearchspacerepresentationofarichenoughsearch space.
moreover while additional developer inputs are valuable for generating or ranking patches developers do not seem tobe interested in a significant human in the loop interaction.
acm reference format yannicnoller ridwanshariffdeen xianggao andabhikroychoudhury.
.trustenhancementissuesinprogramrepair.in 44thinternationalconferenceonsoftwareengineering icse may21 pittsburgh pa usa.acm newyork ny usa 13pages.
introduction automated program repair technologies are getting increased attention.inrecenttimes programrepairhasfounditswayinto the automatedfixing of mobile apps in the sapfixproject in facebook automatedrepairbotsasevidencedbytherepairnator joint first authors alternate email gaoxiang9430 gmail.com permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation onthe firstpage.copyrights forcomponentsof thisworkowned byothersthan the author s mustbehonored.abstractingwithcreditispermitted.tocopyotherwise or republish topostonserversortoredistributetolists requirespriorspecificpermission and or a fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn ... .
and has found certain acceptability in companies such as bloomberg .
while all of these are promising large scale adoption of program repair where it is well integrated into our programming environments is considerably out of reach as of now.
in this article we reflect on the impediments towards the usage of program repair by developers.
there can be many challenges towards the adoption of program repair like scalability applicability anddeveloperacceptability.alotoftheresearchonprogram repairhasfocusedonscalabilitytolargeprogramsandalsotolarge search spaces .
similarly there have been various worksongeneratingmulti linefixes orontransplanting patches from one version to another to cover various use cases or scenarios of program repair.
surprisingly thereisverylittleliteratureorsystematicstudies fromeitheracademiaorindustryonthedevelopertrustinprogramrepair.inparticular whatchangesdoweneedtobringintothepro gramrepairprocesssothatitbecomesviabletohaveconversationsonitswide scaleadoption?partofthegulfintermsoflackoftrust comes froma lack ofspecifications since the intendedbehavior of the program is not formally documented it is hard to trust that theautomaticallygeneratedpatchesmeetthisintendedbehavior.
overall we seek to examine whether the developer s reluctance touseprogramrepairmaypartiallystemfromnotrelyingonautomaticallygeneratedcode.thiscanhaveprofoundimplicationsbecauseofrecentdevelopmentsonai basedpairprogramming whichholdsoutpromiseforsignificantpartsofcodinginthefuture to be accomplished via automated code generation.
inthisarticle wespecificallystudytheissuesinvolvedinenhancingdevelopertrustonautomaticallygeneratedpatches.towards this goal we first settle on the research questions related to developertrustinautomaticallygeneratedpatches.thesequestions are divided into two categories a expectations of developers from automatic repair technologies and b understanding the possible shortfall of existing program repair technologies with respect to developer expectations.
to understand the developer expectations from program repair we outline the following research questions.
rq1to what extent are the developers interested to apply automated program repair henceforth called apr and how do they envision using it?
rq2can software developers provide additional inputs that would cause higher trust in generated patches?
if yes what kind of inputs can they provide?
1github copilot ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa yannic noller ridwan shariffdeen xiang gao and abhik roychoudhury rq3what evidence from apr will increase developer trust in the patches produced?
for a comprehensive assessment of the research questions we engageinbothqualitativeandquantitativestudies.ourassessment of the questions primarily comes in three parts.
to understand the developer expectations from program repair we conduct a detailed survey with questions among more than professional software practitioners.
most of our survey respondents are developers witha fewcoming frommore seniorroles suchas architects.the surveyresults amounttoboth quantitativeandqualitative inputs on the developer expectations since we curate and analyze respondents comments on topics such as the expected evidence for patch correctnessprovidedbyautomatedrepairtechniques.basedonthe surveyfindings wenotethatdevelopersarelargelyopen minded in terms of trying out a small number of patches no more than fromautomatedrepairtechniques aslongasthesepatchesareproduced within a reasonable time say less than hour.
furthermore the developers are open to receiving specifications from the program repair method amounting to evidence of patch correctness .
theyarealsoopen mindedintermsofprovidingadditionalspecificationsto driveprogramrepair.themostcommon specifications the developers are ready to give and receive are tests.
basedonthecommentsreceivedfromsurveyparticipants we then conduct a quantitative comparison of certain well known program repair tools on the widely used manybugs benchmarks .
tounderstandthepossibledeficiencyofexistingprogramrepair techniqueswithrespecttooutlineddeveloperexpectationsasfound from the survey we formulate the following research questions.
rq4can existing apr techniques pinpoint high quality patches in the top ranking e.g.
among top patches within a tolerable time limit e.g.
.
hours ?
rq5what is the impact of additional inputs say fix locations and additional passing test cases on the efficacy of apr?
wenotethatmanyoftheexistingpapersonprogramrepairuseliberal timeout periods to generate repairs while in our experiments thetimeoutisstrictlymaintainedatnomorethanonehour.weare alsorestrictedtoobservingthefirstfewpatches andweexamine theimpactofthefixlocalizationbyeitherprovidingandnotproviding the developer location.
based on a quantitative comparison of well known repair tools angelix cpr genprog prophet andfix2fit weconcludethatthesearchspace representationhasasignificantroleinderivingplausible correct patches within an acceptable time period.
in other words an abstract representation of the search space aided by constraints that are managed efficiently or aided by program equivalence relations is at least as critical as a smart search algorithm to navigate the patch space.
we discuss how the tools can be improved to meet developerexpectations eitherbyachievingcompilation freerepairor bynavigating suggestingabstractpatcheswiththehelpofsimple constraints such as interval constraints .
lastbutnottheleast wenotethatprogramrepaircanbeseen as automated code generation at a micro scale.
by studying thetrustissuesin automated repair wecan alsoobtainaninitialunderstanding of trust enhancement in automatically generated code.
specifications in program repair the goal of apr is to correct buggy programs to satisfy given specifications.inthissection wereviewthesespecificationsand discuss how they can impact patch quality.
test suites as specification.
apr techniques such as genprog andprophet treattestsuitesascorrectnessspecifications.the test suite usually includes a set of passing tests and at least one failing test.
the repair goal is to correct the buggy program to pass allthegiventestsuites.althoughtestsuitesarewidelyavailable theyareusuallyincompletespecificationsthatspecifypartofthe intendedprogrambehaviors.hence theautomaticallygenerated patchmayoverfitthetests meaningthatthepatchedprogrammaystillfailonprograminputsoutsidethegiventests.forinstance the following is a buggy implementation that copies ncharacters from sourcearray srctodestinationarray dest andreturnsthenumber of copied characters.
a buffer overflow happens at line when the sizeof srcordestislessthan n.bytakingthefollowingthreetests oneofthemcantriggerthisbug asspecification aproducedpatch index n index n index canmaketheprogram pass the given tests.
obviously the patched program is still buggy on test inputs outside the given tests.
int lenstrncpy char src char dest int n if src null dest null return int index while index n dest src buffer overflow return index type src dest n output expected output passing sof com passing dht app0 failing app0 dqt crash constraints as specification.
instead of relying on tests another lineofaprresearch e.g.
extractfix andcpr takeconstraintsascorrectnessspecifications.constraintshavethepotentialtorepresentarangeofinputsoreventhewholeinputspace.driven byconstraints thegoalofapristopatchtheprogramtosatisfy theconstraints.
however unlike thetestsuite theconstraintsare notalwaysavailableinpractice forthisreason techniqueslikeangelix andsemfix taketestsasspecificationsbutextract constraints from tests.
certain existing apr techniques take as inputcoarse grainedconstraints suchasassertionsorcrash freeconstraints.forinstance extractfixreliesonpredefinedtemplates toinfer constraintsthat cancompletely fixvulnerabilities.for the aboveexample accordingtothetemplateforbufferoverflow theinferred constraint is index sizeof src index sizeof dest .
once the patched program satisfies this constraint it is guaranteed that the buffer overflow is completely fixed.
guarantees from such fixing of overflows crashes do not amount to a guarantee of the full functional correctness of the fixed program.
code patterns as specification.
besides test suites andconstraints code patterns can also serve as specifications for repair systems.
specifically givenabuggyprogramthatviolatesacodepattern the authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
trust enhancement issues in program repair icse may pittsburgh pa usa repair goal is to correct the program to satisfy the rules defined by thecodepattern.thecodepatternscanbemanuallydefined from static analyzers automatically mined from large code repositories etc.
similar to the inferred constraints code patterns cannot ensure functionality correctness.
survey methodology since constructing formal program specifications is notoriously difficult the specifications used by apr tools cannot ensure patch correctness.
unreliable overfitting patches cause developers to lose trust in apr tools.
this motivates us to enquire survey developers on how apr can be enhanced to gain their trust.
wedesignedandconductedasurveywithsoftwarepractitioners specifically to answer the first three research questions rq1 .
in june we distributed a questionnaire to understand howdevelopers envision the usage of automated program repair and whatcanbeprovidedtoincreasetrustinautomaticallygenerated patches.
note that we followed our institutional guidelines and received approval from the institutional review board irb of our organization prior to administering the survey.
survey instrument.
we asked in total questions about how trustworthyaprcanbedeployedinpractice.ourquestionsarestructured into six categories c1usageofapr rq1 whetherandhowdeveloperswouldengage with apr.
c2availabilityofinputs specifications rq2 whatkindofinput artifacts developers can provide for apr techniques.
c3impact on trust rq2 how additional input artifacts would impact the trust in auto generated patches.
c4explanations rq3 what kind of evidence explanation developers expect for auto generated patches.
c5usage of apr side products rq3 what side products of apr are useful for the developers e.g.
for manual bug fixing.
c6background the role and experience of the participants in the software development process.
c1willprovideinsightsforrq1 c2andc3forrq2 andc4andc5 for rq3.
the questions are a combination of open ended questions like how would you like to engage with an apr tool?
and closeended questions like would it increase your trust in auto generated patches if additional artifacts such as tests assertions are used during patching?
withmultiplechoiceora5 pointlikertscale.thequestionnaire itself was created and deployed with microsoft forms.
a completelistofourquestionscanbefoundintable1andinour replication package .
participants.
we distributed the survey via two channels amazon mturk and personalized email invitations to contacts from global wide companies.
as incentives we offered each participant on mturk usd as compensation while for each other participant we donated usd to a covid charity fund.
we received responses from mturk.
to filter low quality and non genuine responses wefollowedtheknownprinciples andusedqualitycontrolquestions.inparticular wemanuallyinspectedallresponses and filteredout answersthat areirrelevant tothe actualquestion wecheckedforsuspiciousanswers whichoverloadkeywords e.g.
manyresponsesincludedamessageonannualpercentagerate !
!
!
!
figure1 responsesforq6.
whatisyour main roleinthe software development process?
figure responses for q6.
how long have you worked in software development?
apr insteadofautomatedprogramrepair andthen wechecked the consistency of the responses with quality control questions e.g.
please describe briefly your role in software development and nameyourprimary activityinsoftwaredevelopment atthe beginning of the survey.
after this manual post processing we ended up with validresponses from mturk.
from our company contacts we received responses from which all have been genuineanswers.fromthetotalof115validresponses weselected relevantresponses which excluded responses from participants whoclassifiedthemselvesasprojectmanager productowner data scientist orresearcher.ourgoalwastoincludeanswersfromsoftwarepractitionersthathavedaily hands onexperienceinsoftware development.figure1and2showtherolesandexperiencesforthe final subset of the participants.
analysis.
forthequestionswitha5 pointlikertscale weanalyzed thedistributionofnegative 1and2 neutral andpositive 4and5 responses.forthemultiplechoicequestions weanalyzedwhich choices were selected most while the open ended other choices were analyzed and mapped to the existing choices or treated as newonesifnecessary.forallopen endedquestions weperformedaqualitativecontentanalysiscoding tosummarizethethemes andopinions.thefirstiterationoftheanalysisandcodingwasdone by oneauthor followedby thereview ofthe other authors.in the following sections we will discuss the most mentioned responses and indicate in the brackets behind the responses how often the topicsarementionedamongthe103participants.weusethechisquare goodness of fit test .
to check that our results are significant and not a random observation.
we also show thesignificance of the obtained trends majorities with the binomial test .
.wepresentthecorresponding pvalues.alldata statistics and codes are included in our replication package .
survey results .
developer engagement with apr rq1 in this section we discuss the responses for the questions in categoryc1andquestionq2.
whichwasexplicitlyexploringhow authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa yannic noller ridwan shariffdeen xiang gao and abhik roychoudhury table complete list of questions from the developer survey.
in total questions in categories.
category question type q1.
are you willing to review patches that are submitted by apr techniques?
point likert scale q1.
how many auto generated patches would you be willing to review before losing trust interest in the technique?
selection other... c1 usage of q1.
how much time would you be giving to any apr technique to produce results?
selection other... apr q1.
how much time do you spend on average to fix a bug?
selection other... q1.
do you trust a patch that has been adopted from another location application where a similar patch was already accepted by other developers?
point likert scale q1.
would it increase your confidence in automatically generated patches if some kind of additional input e.g.
user provided test cases were considered?
point likert scale q1.7besidessomeadditionalinputthatistakenintoaccount whatothermechanismdoyouseetoincreasethetrustinauto generated patches?open ended q2.
can you provide additional test cases i.e.
inputs and expected outputs relevant for the reported bug?
point likert scale c2 availability q2.
can you provide additional assertions as program instrumentation about the correct behavior?
point likert scale of inputs q2.
can you provide a specification for the correct behavior as logical constraint?
point likert scale q2.
would you be fine with classifying auto generated input output pairs as incorrect or correct behavior?
point likert scale q2.
how many of such queries would you answer?
selection other... q2.
for how long would you be willing to answer such queries?
selection other... q2.
what other type of input e.g.
specification or artifact can you provide that might help to generate patches?
open ended q2.8pleasedescribehow youwouldliketo engagewithanaprtool.for exampleshortlydescribethedialogue between you as user of the apr tool and the apr tool.
which input would you pass to the apr tool?
what do you expect from the apr tool?open ended q3.
would it increase your trust in auto generated patches if additional artifacts such as tests assertions are used during patching?
point likert scale c3impactontrust q3.
which of the following additional artifacts will increase your trust?
multiple choice q3.
what are other additional artifacts that will increase your trust?
open ended q4.
would it increase your trust when the apr technique shows you the code coverage achieved by the executed test cases that are used to construct the repair?
point likert scale c4 explanations for generatedq4.
would it increase your trust when the apr technique presents the ratio of input space that has been successfully tested by the inputs used to drive the repair?
point likert scale patches q4.
what other type of evidence or explanation would you like to come with the patches so that you canselectan automatically generated patch candidate with confidence?open ended q5.
which of the following information i.e.
potential side products of apr would be helpful to validate the patch?
multiple choice c5 usage of apr q5.
what other information i.e.
potential side products of apr would be helpful to validate the patch?
open ended side products q5.
which of the following information i.e.
potential side products of apr would help you to fix the problem yourself without using generated patches ?multiple choice q5.
what other information i.e.
potential side products of apr would help you to fix the problem yourself without using generated patches ?open ended q6.
what is your main role in the software development process?
selection other... c6 background q6.
how long have you worked in software development?
selection q6.
how long have you worked in your current role?
selection q6.
how would you characterize the organization where you are employed for software development related activities?
selection other... q6.
what is your highest education degree?
selection other... q6.
what is your primary programming language?
selection other... q6.
what is your secondary programming language?
selection other... q6.
how familiar are you with automated program repair?
point likert scale q6.
are you applying any automated program repair technique at work?
yes no q6.
which automated program repair technique are you applying at work?
open ended !
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
figure results for the questions with the point likert scale responses .
the participants want to engagewith an apr tool.
first of all a strong majority of the responses p .
indicate that the participants are willing to review auto generated patches see q1.
in figure .
this finding generally confirms the efforts in the apr communitytodevelopsuchtechniques.only7 oftheparticipants arereluctanttoapplyaprtechniques intheirwork.asshowninfigure we note that p .
of the participants want to review only up to patches while only would review up to patches.
furthermore mention that it would depend on the specificscenario.atthesametime theparticipantsexpectrelatively quickresults p .
wouldnotwaitlongerthanonehour of which the majority of them p .
prefer to not even authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
trust enhancement issues in program repair icse may pittsburgh pa usa figure cumulative illustration of the responses for q1.
how many auto generated patches would you be willing toreview before losing trust interest in the technique?
wait longer than minutes.
the expected time certainly depends ontheconcretedeployment e.g.
repaircanalsobedeployedalong anightlycontinuousintegration ci pipeline butourresultsindicatethatdirectsupportofmanualbugfixingrequires quickfix suggestionorhints.infact p .
oftheparticipantsstate thattheyusuallyspendnotmorethan2hoursonaveragetofixa bug and hence the apr techniques need to be fast to provide a benefitforthedeveloper.toincreasetrustinthegeneratedpatches p .
agree that additional artifacts e.g.
test cases which are provided as input for apr are useful see q1.
in figure .
as a consistency check we asked a similar question at a later point seeq3.1infigure3 andobtainedthateven84 p .
agree that additional artifacts can increase trust.
the most mentionedother mechanisms to increase trust are the extensive validation of thepatcheswithatestsuite andstaticanalysistools the actualmanualinvestigation ofthepatches the reputation oftheaprtoolitself the explanation ofpatches and the provisioning of additionally generated tests .
rq1 acceptability of apr additional user provided artifacts like test cases are helpfulto increase trust in automatically generated patches.
however our results indicate that fulldeveloper trust requires a manual patch review.
at the same time test reports of automated dynamic and static analysis as well as explanations of the patch can facilitate the reviewing effort.
theresponsesfortheexplicitquestionaboutdevelopers envisioned engagement with apr tools q2.
can be categorized into four areas the extent of interaction the type of input the expected output andtheexpected integration intothedevelopmentworkflow.
interaction.
mostparticipants p .
mentionthatthey preferarather lowamountofinteraction i.e.
afterprovidingthe initial input to the apr technique there will be no further interaction.
only a few responses mention theone time option toprovide moretestcasesor somesortofspecificationto narrow downthesearchspacewhenaprrunsintoatimeout orthegenerated fixes are not correct.
only participants envision a high level of interaction e.g.
repeated querying of relevant test cases.
input.many participants appear ready to provide failing test cases or relevant test cases .
others mentioned that apr shouldtakea bugreport asinput whichcanincludethe stacktrace detailsoftheenvironment andexecutionlogs.some also mentioned that they envision only the provision of the bare minimum i.e.
the program itself or the repository with the source code .output.besidesthegeneratedpatches themostmentionedhelpful output from an apr tool is an explanation of the fixed issue includingitsrootcause .thisanswerisfollowedbytherequirement to present not only one patch but a list of potential patches .
additionally some participants mentioned that it would be helpful to produce a comprehensive test report .
integration.
themostmentionedintegrationmechanismistoinvolveaprsmoothlyinthe devopspipeline e.g.
whenever a failing test is detected by the ci pipeline the apr would be triggeredtogenerateappropriatefixsuggestions.adeveloperwould manuallyreviewthefailedtest s andthesuggestedpatches.along withtheintegrationtheparticipantsmentionedthattheprimary goal of apr should be to save time for the developers .
rq1 interaction with apr developers envision a low amount of interaction with apr e.g.
by only providing initial artifacts like test cases.
apr should quickly within min min generate a smallnumber between and of patches.
moreover apr needs to be integrated into the existing devops pipelines to support the development workflow.
.
availability impact of artifacts rq2 inthissection welookmorecloselyinthecategoriesc2andc3toinvestigatewhichadditionalartifactscanbeprovidedbydevelopers and how these artifacts influence the trust in apr.
we first explore the availability of additional test cases positive p .
program assertions positive p .
and logical constraints positive p .
see the results for q2.
q2.
and q2.
in figure .furthermore p .
oftheparticipantsarepositiveabout answering queries toclassify generated tests asfailing or passing.
thiscanbeunderstoodasparticipantswanttohavelowinteraction i.e.
asking questions to the tool but if the tool is able to issue queries they areready toanswer someof them typicallyrespondents prefer to answer no more than queries p .
.
based on the results for open ended question q2.
the majority of the participants p .
donotseeanyotheradditionalartifacts beyondtests assertions logical constraints user queries thattheycouldprovidetoapr.themostmentionedresponsesbyotherparticipantsaredifferentformsof requirementsspecification e.g.
writteninadomain specificlanguage executionlogs documentation of interfaces with data types and expected value ranges errorstacktraces relevant sourcecodelocations andreferencesolutions e.g.
existingsolutions for similar problems.
rq2 artifact availability software developers can provide additional artifacts like test cases program assertions logical constraints execution logs and relevant source code locations.
regarding an increasein trust in patches through the incorporation of additional artifacts driving repair p .
of the participantsagreethatadditional testcasesarehelpful seefigure5 .
thisisalsointerestingfromtheperspectiveofrecentautomated repairtools whichperformautomatedtestgenerationto achieve less overfitting patches.
logical constraints p .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa yannic noller ridwan shariffdeen xiang gao and abhik roychoudhury !
figure responses for q3.
which of the following additional artifacts will increase your trust?
and program assertions p .
perform worse in this respect.
although user queries allow more interaction with the apr technique they would not necessarily increase trust more than the other artifacts.
only p .
agreed on their benefit.
mostoftheparticipants p .
didnotmentionatrust gain by other artifacts.
however some participants mentionednon functionalrequirementslikeperformanceorsecurity aspects which is related to a concern that auto generated patches mayharmexistingperformancecharacteristicsorintroducenew security vulnerabilities.
rq2 impact on trust additional test cases would have a great impact on the trustworthiness of apr.
there exists the possibilityofautomaticallygeneratingteststoincreasetrustin the auto generated patches.
.
patch explanation evidence rq3 in this section we explore which patch evidence and apr sideproductscansupporttrustinapr seecategoriesc4andc5 .we first proposed two possible pieces of evidence that could be pre sentedalongwiththepatches the codecoverage achievedbythe executedtestcasesthatareusedtoconstructtherepair andthe ratioofinputspace thathasbeensuccessfullytestedbytheautomated patch validation.
p .
of the participants agree that code coverage would increase trust and p .
agree with the input ratio see q4.
and q4.
in figure .
the majority of the participants p .
do not mention other types of evidence thatwouldhelptoselectapatchwithconfidence.nevertheless the mostmentionedresponseisa fixsummary i.e.
anexplanation of what has been fixed including the root cause of the issue how it has been fixed and how it can prevent future issues.
other participantsmentionthe successrate incaseofpatchtransplants anda testreport summarizingthepatchvalidationresults .
these responses match the observations for rq1 where weaskedhowdeveloperswanttointeractwithtrustworthyapr and what output they expect.
rq3 patch evidence software developers want to see evidenceforthepatch scorrectnesstoefficientlyselectpatchcandidates.developerswanttoseeinformationsuchascodecoverage as well as the ratio of the covered input space.
a straightforward way to provide explanations and evidence is toprovideoutputsthatarealreadycreatedbyaprasside products.
welistedsomeofthemandaskedtheparticipantstoselectwhichof them would be helpful to validate the patches see results in figure .
p .
agree that the identified faultandfix locations !
figure responses for q5.
which of the following information i.e.
potentialside productsofapr wouldbehelpfulto validate the patch?
!
!
!
!
!
!
!
!
figure responses for q5.
which of the following information i.e.
potential side products of apr would help youtofixtheproblemyourself withoutusinggeneratedpatches ?
are helpful to validate the patch followed by the generated test caseswith p .
agreement.
in addition a few participants emphasizetheimportanceofa testreport anexplanationof theroot cause and thefix attempt .
finally weexplorewhichside productsaremostusefulfordevelopers evenwhenaprcannotidentifythecorrectpatch.figure7 showsthattheidentifiedfaultandfixlocationsareofmostinterest p .
followed by the generated test cases p .
.
veryfewparticipantsaddthatanissuesummary andthe potentialresultsofadataflowanalysis couldbehelpfultoo.
rq3 apr s side products our results indicate that sideproductsofaprlikethe faultandfixlocations andthegenerated testcases canassistmanualpatchvalidation andhence enhance trust in apr.
evaluation methodology we nowinvestigate towhich extent existingapr techniquessupport the expectations and requirements collected with our survey.
notallaspectsofourdevelopersurveycanbeeasilyevaluated.for example theevaluationoftheamountofinteraction theintegration into existing workflows the output format for the efficient patch selection and the patch explanations require additional case studiesand furtheruser experiments.in thisevaluation wefocus on the quantitative evaluation of the relatively short patching time min the limited number of patches to manually investigate 5to10 handlingofadditionaltestcasesandlogicalconstraints andtheabilitytogeneratearepairataprovidedfixlocation.we explorewhetherstate of the artrepairtechniquescanproducecorrect patches under configurations that match these expectationsandrequirements.specifically weaimtoprovideanswerstothe research questions rq4 and rq5.
apr representatives.
in our evaluation we selected tools to represent a wide spectrum of state of the art apr techniques searchbased genprog semantic based angelix thecombination of search based and learning based prophet and the integrationoftestinginsiderepairtotackleoverfitting fix2fit authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
trust enhancement issues in program repair icse may pittsburgh pa usa cpr .
we further selected tools that apply on c due to our evaluationsubjects.
genprog isasearch basedprogramrepair tool that evolves the buggy program by mutating program statements.itisawell knownrepresentativeofthegenerate andvalidate repair techniques.
angelix is a semantic program repair technique that applies symbolic execution to extract constraints which serve as a specification for subsequent programsynthesis.
prophet combines search based program repair with machine learning.
it learns a code correctness model from open source software repositories to prioritize and rank the generatedpatches.fix2fit combinessearch basedprogramrepair with fuzzing.
it uses grey box fuzzing to generate additional test inputs to filter overfitting patches that crash the program.
the test generationprioritizesteststhatrefineanequivalenceclassbased patch space representation.
cpr uses semantic program repair andconcolic test generationfor refining abstractpatches and for discarding overfitting patches.
it takes a logical constraint as additional user input to reason about the generated tests inputs.
subjectprograms.
weusethemanybugs benchmark which isawell establishedbenchmarkinapr andalloftheconsidered techniques tools also use some of these subjects in their eval uation.
therefore it is a benchmark for which it is known thatthe examined tools can identify patches.
our goal is to evaluate whethertheycan stillidentifypatches withchanged limitedenvironmental conditions e.g.
timeout set of available test cases etc .
thebenchmarksetconsistsof185defectsin9open sourceprojects.
for each subject manybugs includes a test suite created by the originaldevelopers.
note thatallofthestudiedrepairtechniques requireand orcanincorporateatestsuiteintheirrepairprocess.
for our evaluation we filter the defects that have been fixed bythedeveloperatasinglefixlocation.weremovedefectsfrom valgrind and fbc subjects due to the inability to reproduce the defects.
finally we obtain defects in different open source projects see table .
table experiment subjects and their details program description loc defects tests libtiff image processing library 77k lighttpd web server 62k php interpreter 1046k gmp math library 145k gzip data compression program 491k 12python interpreter 407k experimental configurations and setup.
all tools are configured to run in full exploration mode which will continue to generate patches even after finding one plausible patch until the timeout or thecompletionofexploringthesearchspace.tostudytheimpactof fixlocationsandtestcasevariations seerq5 weevaluateeachtool usingdifferentconfigurations seetable3 .ineachconfiguration we provide the relevant source file to all techniques however with developerfixlocation weprovidetheexactsourcelinenumberas well.notethateachsetupusesa1 hourtimeout whichischosen based on our survey responses p .
of all participants would expect results within hour.table experiment configurations id fix location passing tests timeout ec1 tool fault localization 1hr ec2 developer fix location 1hr ec3 developer fix location 1hr ec4 developer fix location 1hr evaluationmetrics.
inordertoassessthetechniquesandsupport theansweringofourresearchquestions weconsiderthefollowingeightmetrics whichareinspiredbyexistingstudiesinapr m1the search space sizeof the repair tool m2the number of enumerated explored patches m3theexplored ratiowithrespect to the search space m4the number of non compilable patches m5 the number of non plausible patches i.e.
patches that have been explored but ruled out because existing or generated test cases are violated m6the number of plausible patches m7the number of correctpatches and m8thehighest rankofacorrectpatch.m1 m6 help to analyze the overall search space creation and navigation of each technique.
the definition of the search space size m1 for the defect aswellasthedefinitionofanenumerated exploredpatch m2 varyforeachtool.weincludeallexperimentprotocolsinour replicationartifact whichdescribeshowtocollectthesemetricsfor each tool.
m7 m9 assess the repair outcome i.e.
the identification of thecorrectpatch.
we define a patch as correctwhenever it is semantically equivalent to the developer patch that is provided in our benchmark.
to check for the correct patch we manuallyinvestigated only the top ranked patches because our survey concludedthatdeveloperswouldnotexplorebeyondthat.notethatnotalltechniquesprovideapatchranking e.g.
angelix genprog and fix2fit .
in these cases we use the order of generation.
hardware.
all our experiments were conducted using docker containersontopofaws amazonwebservices ec2instances.we usedthec5a.8xlargeinstancetype whichprovides32vcpuprocessing power and 64gib memory capacity.
replication.
ourreplicationpackagecontainsallexperimentlogs andsubjects aswellasprotocolsthatdefinethemethodologyused to analyze the output of each repair tool .
in particular we describe how to retrieve each evaluation metric for the specific repair techniques.
experimental setup our experiments are meant to investigatespecificaspectsconcerningtheincreaseof programrepair adoption based on the results of our developer survey.
we assumethatthedeveloper userisnotanaprexpert andhence wouldusethe defaultparameter settingsinsteadoffine tuning orextendingthetools.furthermore ourexperimentsuse strict timeoutsand computation power restrictions.
other setups can lead to different and better results.
evaluation results table4summarizesourevaluationresults.foreachaprtechnique we show its performance under the given experimental configuration see table .
each cell shows pplaus pcorr where pplaus authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa yannic noller ridwan shariffdeen xiang gao and abhik roychoudhury table4 experimentalresultsforthevariousconfigurations.eachcellshowsthenumberofsubjects forwhichthetechnique was able to identify at least one plausible correct patch with regard to the specific configuration.
please also see threats to validity of experimental results in section to understand the context of these results fully.
subject def.angelix prophet genprog fix2fit cpr ec1 ec2 ec3 ec4 ec1 ec2 ec3 ec4 ec1 ec2 ec3 ec4 ec1 ec2 ec3 ec4 ec2 ec3 ec4 libtiff lighttpd php gmp gzip python overall table5 experimentalresultsfortheaverageexplorationratio pexpl for ec1 and ec2.
subjectangelix prophet genprog fix2fit ec1 ec2 ec1 ec2 ec1 ec2 ec1 ec2 libtiff lighttpd php gmp gzip python overall is the number of defects for which the tool was able to generate at least one plausible patch i.e.
m6 and similarly pcorr is the numberofdefectsforwhichthetoolwasabletogenerateacorrectpatchamongthetop 10plausiblepatches.forexample thelibtiff project has defects for which angelix was able to generate plausibleand1correctpatchforthesetupec1 i.e.
hourtimeout tool fault localization and all available test cases .
due to limitations in its symbolic execution engine klee angelix and cpr do not support lighttpd and python and the corresponding cellsare marked with .
for cpr we are not able to produce resultsfor ec1 because it does not have its own fault localization andhence requires the fix location as an input.
additionally table presents the average patchexploration enumeration ratio pexpl of the techniques with respect to the patch space size computed as a percentage of m2 m1 for each defect considered in each subject.
.
apr within realistic boundaries rq4 the numbers in table show that the overall repair success is limited.
for example fix2fit can generate plausible patches for defectswith ec1 whilecpr can generatecorrect patches for8 defectsgiventhecorrectfixlocation.comparedtopreviousstudies the number of plausible patches is lower in our experiments mainly due to the hour timeout.
prior research on program re pair have experimented with hour hour and hour timeouts anddeterminedwhetheracorrectpatchcan beidentifiedamong allgeneratedplausiblepatches.thefocusof these prior experiments was to evaluate the capability to generate apatch whereas inourwork wefocusontheperformancewithin atolerabletimelimitsetbydevelopers.notonlythetimeoutbutalso a scenario specific parameter fine tuning can affect the resultsgreatly.forexample whenwemodifythe synthesis level parameter of angelix a parameter that modulates the back end synthesisof the tool and hence can affect the search space we can seeadditionalpatchesbeinggenerated suchasforadefectinlibtiff 3edb9cd intheec3configuration.ourreportedexperimentsonly use thedefault parameters.
in future for a full investigation of the repairtools capabilities it willthereforebenecessarytoconduct an exploration of the parameter choices in each repair tool which has not been done in this paper.
rq4 repair success under ourtight constraints i.e.
the 1hourtimeoutandthetop 10ranking andtheirdefaultparametersetups currentstate of the artrepairtechniquescannotidentify many plausible patches for the manybugs benchmark.
automated program repair tools are only beginning to gain adoption andarestillanemergingtechnology.wewanttoidentify what it would take to increase the adoption of program repair.
ingeneral therepairsuccessofanaprtechniqueisdeterminedby its search space the exploration of this search space and the ranking of the identified patches.
in a nutshell this means ifthecorrectpatchisnotinthesearchspace thetechnique cannot identify it.
if the correct patch is in the search space but aprdoesnotidentifyitwithinagiventimeoutorotherresource limitations itcannotreportitasaplausiblepatch.ifitidentifiesthe patchwithintheavailableresourcesbutcannotpinpointitinthe potentiallyhuge spaceofgeneratedpatches theuser developer will not recognize it.
by means of these impediments for repair success in real world scenarios we examine the considered repair techniques.
our goal is to identify the concepts in apr that arenecessary to achieve the developers expectations and hence to improve the state of the art approaches.
search space.
table shows that angelix explores almost its complete search space within the hour timeout while table shows thatitcanidentifyplausiblepatchesforonlyonedefect withec1 .
asdescribedin theprogramtransformations tobuild explore thesearchspace byangelixonlyincludethemodificationofexist ing side effect free integer expressions conditions and the additionofif guards.therefore weconcludethatangelix ssearchspaceistoolimitedtocontainthecorrectpatches.theothertechniques on theotherhand considerlargersearchspaces.prophetalsoconsiders the insertion of statements and the replacement of function authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
trust enhancement issues in program repair icse may pittsburgh pa usa calls.genprogcaninsert removeanyavailableprogramstatement.
fix2fit uses the search space by f1x which combines the search spaces of angelix and prophet to generate a larger search space.cprusesthesameprogramtransformationsasangelixbutis designed to easily incorporate additional user inputs like custom synthesis components to enrich its search space.
rq4 search space successful repair techniques need to consider a wide range of program transformations and should be able to take user input into account to enrich the search space.
searchspaceexploration.
prophetandgenprogshowarelatively low exploration ratio with and respectively see ec1 in table5 whichleadstoalownumberofplausiblepatchesinour experiments.
instead fix2fit fully explores the patch search space formostoftheconsidereddefects exceptforphp whichleadstoahighpossibilityoffindingaplausiblepatch.cpr notshowninthetable fullyexploresitssearchspaceinourexperiments.incontrast to prophet and genprog fix2fit and cpr perform grouping and abstracting of patches to explore them efficiently.
fix2fit groups the patches by their behavior on test inputs and uses this equiv alence relation to guide the generation of additional inputs.
cprrepresents the search space in terms of abstractpatches which are patch templates accompanied by constraints.
cpr enumerates abstractpatchesinsteadofconcretepatches andhence canreasonaboutmultiplepatchesatoncetoremoveorrefinepatches.prophet and genprog however explore and evaluate all concrete patches which causes a significant slowdown.
reduction of the patch validation time is possible if we can validate patches without the need to re compile the program for each concrete patch .
rq4 patch space exploration a large rich search space requires an efficientexploration strategy which can be achieved by e.g.
using search space abstractions.
patchranking.
althoughfix2fitbuildsarichsearchspaceandcan efficientlyexploreit itstillcannotproducemanycorrectpatchesin our experiments.
one reason is that fix2fit can identify a correct patchbutfailstopinpointitinthetop 10patchesbecauseitonly applies a rudimentary patch ranking which uses the edit distance betweentheoriginalandpatchedprogram.forinstance fix2fit generates the correct patch for the defect 865f7b2in the libtiff subjectbutranksitbelowposition10 andhence itisnotconsidered in our evaluation.
furthermore fix2fit s patch refinement and rankingisbasedoncrash avoidance whichisnotsuitableforatestsuiterepairbenchmarksuchasmanybugsthatdoesnotinclude manycrashingdefects.cprimprovesonthatbyleveragingtheuserprovided logical constraint to reason about additionally generated inputs while the patch behaviors on these inputs are collected and used to rank the patches.
but still overall it cannot produce many correct patches within the top .
we also investigated how many ofthecorrectpatchesarewithinthetop 5because72 p .
of oursurveyparticipantsfavoredreviewingonlyupto5patches see figure .
we observed that most identified correct patches within thetop 10arerankedveryhighsothatthereisnotmuchdifferenceifatop 5thresholdisapplied.recentworks proposetheuseof the test behavior similarity between original patched programs to rank plausible patches which is a promising future direction.
rq4 patch ranking after exploring the correct patch an effective patch ranking is the last impediment for the developer.
.
impact of additional inputs rq5 providing fix location as user input.
in table the column ec1 showstheresultswiththetool sfaultlocalizationtechnique and columnec2showstheresultsbyrepairingonlyatthedeveloperprovided correct fixlocation.intuitively oneexpectsthatequipped with the developer fix location the results of each repair technique should improve.
however the results by angelix and genprog do not change except for one more plausible patch with angelix .
from the previous discussion about the search space we conclude thattheprogramtransformationsbyangelixarethemainlimitingfactortotheextentthateventheprovisionofthecorrectfixlocation has no impact.
for genprog we know from the ec3 configuration that there is at least one correct patch in the search space see table .
therefore we conclude that genprog suffers from its inefficient space exploration so that even the space reduction by settingthefixlocationhasnoimpact.prophetinsteadcangenerate two additional correct patches in ec2 and hence benefits from the precise fixlocation.the exploration ratio in table5 shows thatprophet almost fully explores its search space in ec2 indicating a smaller search space.
fix2fit can generate one more correct patch as compared to ec1.
similar to prophet fix2fit benefits fromthe precise fix location and can explore more of its search space.
note that cpr is not included in the comparison between ec1 and ec2 because it does not apply for ec1.
however for ec2 it can generatethehighestnumberofcorrectpatches.besidesitsefficientpatchspaceabstraction weattributethistoitsabilitytoincorporate additionaluserinputslikethefixlocationandtheuser provided logical constraint.
rq5 fixlocation ourresultsshowthattheprovisionofthe precise and correct fix location does not necessarily improve the outcome of the state of the art apr techniques due to their limitationsinsearchspaceconstructionandexploration.however being amenable to such additional inputs can significantly improve the repair success as shown by results from cpr.
varyingpassingtestcases.
toexaminetheimpactofthepassing testcases weconsiderthedifferencesbetweenthecolumnsec2 ec3 andec4intable4.ingeneral morepassingtestcasescanlead tohigh qualitypatchesbecausetheyrepresentinformationabout thecorrectbehavior.inlinewiththis weobservethatmorepassing test cases lead to fewer plausible patches because the patch validationcanremovemoreoverfittingpatches.forangelixhowever weobservethatthereisnodifferenceduetoitslimitedsearchspace.
cprisalsonotaffectedbythevaryingnumberofpassingtestcases.
it uses the failing test cases to synthesize the search space and the passing test cases as seed inputs for its input generation.
but since cpr always fullyexploresthe searchspace inour experiments the variationoftheinitialseedinputshasnoeffectwithinthe1hour.
overall we observe three different effects a for techniques with authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa yannic noller ridwan shariffdeen xiang gao and abhik roychoudhury a limited search space e.g.
angelix passing test cases have very lowornoeffect.
b fortechniquesthatsufferfrominefficientspace exploration strategies e.g.
genprog and prophet having fewer passing test cases can speeduptherepairpro cess and lead to more plausible possiblyoverfitting patches.
c otherwise e.g.
fix2fit variationsinthepassingtestcasescanstillinfluencetheranking.
whether more tests are better depends on the apr strategy and its characteristics asdiscussedinsection6.
.therefore wesuggest that apr techniques incorporate an intelligent test selection or filtering mechanism which is not yet studied extensively in thecontext of apr.
recently suggested applying traditional regression test selection and prioritization to achieve better repair efficiency.
furtherdeveloping andusing sucha mechanism represents a promising research direction.
note that in the discussed experiments thefix locationwas definedbeforehand.
however if aprtechniques useatest basedfault localization technique like in ec1 thetest cases havean additional effect on thesearch space and repair success.
rq5 test cases variation of passing test cases causes differenteffectsdependingonthecharacteristicsoftheaprtechniques.
overall one needs an intelligent test selection method.
threats to validity externalvalidityofsurvey.
althoughwereachedouttodifferent organizations in different countries we cannot guarantee that our survey results can be generalized to all software developers.
tomitigatethisthreat wemadeallresearchartifactspubliclyavailable sothatotherresearchersandpractitionerscanreplicate our study.
to reduce the risk of developers not participating or thevolunteerbias wedesignedthesurveyforashortcompletion time 20min andprovidedincentiveslikecharitydonationsand in the case of mturk monetary compensation.
another potential threat to validity is that only of all participants responded that they arefamiliarwithapr seeq6.
.
thisis tobe expected as apr is not yet heavily applied in theindustry with exceptionslikefacebookandbloomberg .toensurethattheparticipantshave an idea of apr we added a description and a link to an illustrative video at the beginning of our survey form.
we note that we are exploring what it would take for developers to try out programrepair since developers may have general preconceived notions.
byfindingoutwhatwouldmakethedeveloperscomfortabletouse apr we can hope to increase adoption.
construct validity of survey.
in our survey to encourage candid responsesfromparticipants wedidnotcollectanypersonallyidentifyinginformation.additionally weappliedcontrolquestionsto filter non genuine answers.
to mitigate the risk of wrong interpretationofthecollectedresponses weperformedqualitativeanalysis coding for which all codes have been checked and agreed by atleast two authors.
although we found general agreement across participants for many questions we consider our results only as a first step towards exploring trustworthy apr.
internalvalidityofsurvey.
ourparticipantscouldhavemisunderstood our survey questions as we could not clarify any particulars due to the nature of online surveys.
to mitigate this threat weperformedasmallpilotsurveywithfivedevelopers inwhichwe askedforfeedbackaboutthequestions thesurveystructure and the completion time.
additionally there is a general threat that participantscouldsubmitmultipleresponsesbecauseoursurvey was completely anonymous.
threatstovalidityofexperimentalresults.
inourempiricalanalysis wedonotcoverallavailableaprtools butinstead wecoverthe main apr concepts search based semantics based and machinelearning basedtechniques.withmanybugs wehavechosen a benchmark that is a well known collection of defects in open source projects.
additionally it includes many test cases whichare necessary to evaluate the aspects of test case provision.
the metricsinourquantitativeevaluationmeasurethepatchgeneration progress measuringrepairefficiency effectivenessviavariations in configurations ec1 ec4 .
to mitigate the threat of errors in oursetupofexperiments weperformedpreliminaryrunswithasubset of the benchmark and manually investigated the results.
ourexperimentalresultsinsection5explorethecapabilityofthe repair tools to produce patches within a hour timeout.
different results may be observed if a different timeout is chosen.
more importantly it is possible to get significantly better results from the repairtoolsbyfine tuningtheparametersoftherepairtools.for example whenwemodifythe synthesis level parameterof angelix aparameterthatmodulatestheback endsynthesisofthetool and hence canaffectthesearchspace wecanseeadditionalpatches being generated such as for a defect in libtiff.
in our experiments wedidnotfine tunesuchparametersbutinsteadusedthedefault parameter settings to simulate the experience of novice apr users.
it is entirely possible that more expert apr users will be able tousethetoolsmoreeffectivelytogetbetterresults.theimpactof parameter choices can also be rather nuanced e.g.
angelix is built ontopofkleesymbolicexecutionengineandkleehasparameter settings of its own.
furthermore we only share the results for the hour timeout as it is closer to the time tolerance mentioned by our study participants.
related work ourrelatedworkincludesconsiderationsoftrustissues and studies about the human aspects in automated program repair userstudiesaboutdebugging andempiricalstudiesaboutrepairtechniques .
with regard to human aspects in automated program repair our surveystudycontributesnovelinsightsaboutthedevelopers expectations on their interaction with apr and which mechanisms help toincreasetrust.withregardtoempiricalstudies ourevaluation contributes a fresh perspective on existing apr techniques.
trust aspects in apr.
trust issues in automated program repair emergefromthegeneraltrustissuesin automation.leeandsee discuss that users tend to reject automation techniques whenever theydonottrustthem.therefore forthesuccessfuldeployment of automated program repair in practice it will be essential tofocus on its human aspects.
with respect to this our presented surveycontributestotheknowledgebaseofhowdeveloperswantto interact with repair techniques and what makes them trustworthy.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
trust enhancement issues in program repair icse may pittsburgh pa usa existingresearchontrustissuesinaprfocusesmainlyonthe effectofpatchprovenance i.e.
thesourceofthepatch.ryanand alarconetal.
performeduserstudies inwhichtheyasked developers to rate the trustworthiness of patches while the researchers varied the source of the patches.
their observations indicatethathuman writtenpatchesreceiveahigherdegreeoftrust thanmachine generatedpatches.bertrametal.
conductedan eye tracking study to investigate the effect of patch provenance.
theyconfirmadifferencebetweenhuman writtenandmachinegenerated patches and observe that the participants prefer humanwritten patches in terms of readability and coding style.
our study ontheotherhand explorestheexpectationsandrequirementsof developers for trustworthy apr.
the work of weimer et al.
proposed strategies to assess repaired programs to increase human trust.
ourstudy resultsconfirm thatan efficientpatch assessment is crucial and desired by the developers.
we note that focuses on how to assess apr while we focus on how to enhance improve apr in general specifically in terms of its trust.
humanaspectsinapr.
otherhumanstudiesintheaprcontextfocus on how developers interact with apr s output i.e.
the patches.
cambronero et al.
observed developers while fixing software issues.
they infer that developers would benefit from patch explanationandsummariestoefficientlyselectsuitablepatches.they propose toexplain theroles ofvariables andtheir relationto theoriginal code to list the characteristics of patches and to sum marize the effect of the patches on the program.
tao et al.
exploredhowmachine generatedpatchescansupportthedebugging process.they conclude that compared to debuggingknowing only the buggy location high quality patches can support the debuggingeffort whilelow qualitypatchescanactuallycompromise it.liang et al.
concluded thateven incorrect patchesare helpful if they provide additional knowledge like fault locations.fry et al.
explored the understandability and maintainability of machine generated patches.
while their participants labelmachine generated patches as slightly less maintainable than human written patches they also observe that some augmentation of patches with synthesized documentation can reverse this trend.
kimetal.
proposedtheirtemplate basedrepairtechniquepar andevaluatedthepatchacceptabilitycomparedtogenprog.allofthesepreliminaryworksexplorethereactionsontheoutputofapr.
while our findings confirm previous hypotheses e.g.
that fault locations are helpful side products of apr or that an efficient patchselectionisimportant ourworkalsoconsidersthe input to apr the interaction with apr during patch generation and how trust can be accomplished.
debugging.
parninandorso investigatetheusefulnessofdebuggingtechniquesinpractice.theyobservethatmanyassumptions madeby automateddebugging techniquesoften donot hold inpractice.johnsonetal.
explorebarriersforthewideadoptionofstaticanalysistoolsandhowwellsuchtoolsfitintoactual development workflows.
they conduct interviews with developers and discuss their feedback to identify how those techniques can be improved.
although we focus on automated program repair our research theme is related to and .
we strive to understand howdeveloperswanttouseautomatedprogramrepairandwhether current techniques support these aspects.empiricalevaluationofapr.
thelivingreviewarticleonautomated program repair by martin monperrus lists at the point of timewewrotethispaper 43empiricalstudies.mostofthemare concerned about patch correctness to compare the success of apr techniques.
other frequently explored aspects are repair efficiency the impact of fault locations and the diversity of bugs .
less frequently studied aspects are the impact of the test suite and its provenance specificallytheproblemoftest suiteoverfitting andhow closethegeneratedpatchescometohuman writtenpatches .
ourempiricalevaluationisnotjustanotherempiricalassessmentof aprtechnologies.itisspecificallylinkedtothecollecteddeveloper expectations from our survey.
it limits the timeout to hour only explores the top patches and explores various configurations of passingtestsaswellastheimpactoffixlocations.togetherwith our survey results our empirical quantitative evaluation provides thebuildingblockstocreatetrustworthyaprtechniques which will need to be validated via future user studies with practitioners.
discussion inthispaper wehaveinvestigatedtheissuesinvolvedinenhancingdevelopertrustinautomaticallygeneratedpatches.through adetailedstudywithmorethan100practitioners weexplorethe expectations and tolerance levels of developers with respect toautomated program repair tools.
we then conduct a quantitativeevaluation of existing repair tools to simulate the experience ofnovice apr users.
our qualitative and quantitative studies indicatedirectionsthatneedtobeexploredtogaindevelopertrustin patches lowinteractionwithrepairtools exchangeofartifacts suchasgeneratedtestsasinputsaswellasoutputofrepairtools and paying attention to abstract search space representations over and above search algorithmic frameworks.
each repair tool has many parameters and we only used the default parameter settings aswouldbeexpectedfromnoviceusers wedidnotexplorethe various parameter settings.
to understand the full capability of the repair tools in future it would be worthwhile to systematically explore a large number of parameter settings and try out the tools with various different timeouts.
we note that increasingly there is a move towards automated code generation such as the recently proposed github copilot but thisraisesthequestionofwhethersuchautomaticallygenerated code can be trusted.
developing technologies to support mixed usageofmanuallywrittenandauto generatedcode whereprogram repair can improve the automatically generated code could be an enticing research challenge for the community.
dataset from our work ourreplicationpackagewiththesurveyandexperimentartifacts is available on zenodo .
acknowledgment this research is supported by the national research foundation prime minister s office singapore under its campus for researchexcellence and technological enterprise create programme.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa yannic noller ridwan shariffdeen xiang gao and abhik roychoudhury