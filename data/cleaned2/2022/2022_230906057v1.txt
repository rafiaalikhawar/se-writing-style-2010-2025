rap gen retrieval augmented patch generation with codet5 for automatic program repair weishi wang weishi001 e.ntu.edu.sg salesforce ai research nanyang technological university singaporeyue wang wang.y salesforce.com salesforce ai research singapore shafiq joty sjoty salesforce.com salesforce ai research usasteven c.h.
hoi shoi salesforce.com salesforce ai research singapore abstract automatic program repair apr is crucial to reduce manual debugging efforts for developers and improve software reliability.
while conventional search based techniques typically rely on heuristic rules or a redundancy assumption to mine fix patterns recent years have witnessed the surge of deep learning dl based approaches to automate the program repair process in a data driven manner.
however their performance is often limited by a fixed set of parameters to model the highly complex search space of apr.
to ease such burden on the parametric models in this work we propose a novel retrieval augmented patch generation framework rap gen by explicitly leveraging relevant fix patterns retrieved from a codebase of previous bug fix pairs.
specifically we build a hybrid patch retriever to account for both lexical and semantic matching based on the raw source code in a language agnostic manner which does not rely on any code specific features.
in addition we adapt a code aware language model codet5 as our foundation model to facilitate both patch retrieval and generation tasks in a unified manner.
we adopt a stage wise approach where the patch retriever first retrieves a relevant external bug fix pair to augment the buggy input for the codet5 patch generator which synthesizes a ranked list of repair patch candidates.
notably rap gen is a generic apr framework that can flexibly integrate different patch retrievers and generators to repair various types of bugs.
we thoroughly evaluate rap gen on three benchmarks in two programming languages including the tfix benchmark in javascript and code refinement and defects4j benchmarks in java where the bug localization information may or may not be provided.
experimental results show that rap gen significantly outperforms previous state of the art sota approaches on all benchmarks e.g.
equal contribution.
corresponding author wang.y salesforce.com.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse december san francisco ca usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
the accuracy of t5 large on tfix from .
to .
repairing more bugs and repairing more bugs on defects4j bugs.
further analysis reveals that our patch retriever can search for relevant fix patterns to guide the apr systems.
ccs concepts software and its engineering software testing and debugging computing methodologies natural language processing .
keywords automated program repair neural networks retrieval augmented generation pretrained language models acm reference format weishi wang yue wang shafiq joty and steven c.h.
hoi.
.
rapgen retrieval augmented patch generation with codet5 for automatic program repair.
in proceedings of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering esec fse december san francisco ca usa.
acm new york ny usa pages.
introduction program repair is one of the most important stages to maintain software quality which however is a time consuming and costdominating process in modern software development .
therefore there have been huge needs for automatic program repair apr tools to ease the difficulty and cost of program repair for developers with use cases including search of patches at program development time build time or run time .
a notable class of conventional techniques for apr is known as search based also referred to as generate and validate approach .
they often search for repairs based on the fix patterns mined via manual heuristic rules or redundancybased techniques .
the latter group of approaches make a redundancy assumption that the fixed patch can often be found or reconstructed from elsewhere in the codebase a donor code snippet .
this hypothesis has been validated empirically by studies showing that a significant proportion of commits are indeed composed of existing codebase.
meanwhile with the recent advancement in deep learning technologies numerous deep learning dl based apr approaches sep 2023esec fse december san francisco ca usa weishi wang yue wang shafiq joty and steven c.h.
hoi error msg expected an object to be thrown.
buggy code if .isundefined exp throw expected a value got undefined instead before fix evaluate function throw undefined for looping expressions after fix evaluate function throw new error undefined for looping expressions if .isundefined exp throw new error expected a value got undefined instead you face a bug one bug fix example in codebase retrieve fix figure one motivating example of how developers fix a bug by referring to a retrieved fix pattern in codebase.
have been proposed to automate the repair process via parametric models in a purely data driven manner.
in this paradigm the apr task is typically formulated as a neural machine translation or sequence to sequence learning problem in order to translate a buggy source program into a correct target version.
despite their promising results in software intelligence tasks their performance is often limited by the fixed set of model parameters to learn the highly complex distributional patterns for program repair even with several hundreds of million parameters .
to ease such burden on the parametric neural models in this work we propose a novel retrieval augmented patch generation framework called rap gen to additionally leverage relevant fix patterns from a patch retriever.
earlier apr techniques based on the redundancy assumption have shown that mining fix patterns from existing codebase or even external q as from stackoverflow can serve as crucial repair ingredients for apr.
our model which is semi parametric in nature aims to combine both benefits of the implicit parametric end to end program repair learning and the explicit non parametric fix pattern mining.
one distinction from prior fix pattern mining work is that we utilize the top relevant bug fix pair as a guiding fix pattern for a buggy patch instead of clustering the fix templates with hand crafted heuristics.
this retrieval guiding strategy is also motivated by debugging behaviours of program developers who often search for relevant bug fix examples to distill some repair clues for bug fixing.
fig.
illustrates a motivating example where we can find that the retrieved previous repair example informs a fix pattern of wrapping the string with an error object for the throw statement which guides the developer to fix the target bug under consideration.
in addition we propose to adapt a transformer based encoderdecoder model codet5 as the unified foundation model of rap gen for both patch retrieval and generation tasks.
codet5 is a generic code aware language model pretrained on large source code corpora in eight popular programming languages including javascript and java curated from github achieving state of theart sota performance in both code understanding and generation tasks.
rap gen adopts a stage wise learning strategy to connect the patch retriever and patch generator the patch retriever first searches for a relevant bug fix pattern and then pass it to the codet5 patch generator to synthesize a ranked list of fix patch candidates based on both the source buggy code and the retrieved external bug fix knowledge.
while such retrieval augmented generation paradigm has been explored in other tasks such as question answering and code generation and summarization we are the first to investigate its effectiveness for apr systems based on large scale pretrained language models for code.for the retrievers we propose a hybrid approach that accounts for both lexical and semantic matching through sparse bm25 and dense dpr retrieval based on the raw source code.
we employ codet5 s encoder as our dense dpr retriever and propose to train it with a contrastive learning objective using previous bug fix pairs as the fix patch often shares most of semantics with its buggy patch.
the dense dpr retriever is expected to capture deeper code semantics while the sparse keyword based bm25 retriever focuses more on the lexical similarity which is sensitive to the choice of naming for code identifiers.
notably the hybrid retriever is language agnostic as it does not require any code specific features such as abstract syntax trees asts .
experiments reveal that our patch retriever is able to retrieve lexically and semantically relevant fix patterns to guide apr systems.
we investigate the effectiveness of rap gen in different apr scenarios including javascript linter raised diagnostics tfix java bug fix commits code refinement and real java bugs accompanied with test cases in open source projects defects4j .
among these benchmarks we formulate the apr problem as that given a buggy code patch the apr model learns to predict a fix patch that repairs the bug from a codebase of previous bug fix pairs written by developers.
the correctness of the predicted fix patches are validated against either static analyzers tfix or unit testing defects4j or via a direct comparison with the groundtruth fixes written by developers.
overall extensive experimental results show that our rap gen significantly outperforms existing dl based methods on all these three apr benchmarks.
in summary the paper makes the following contributions we propose a novel retrieval augmented patch generation framework rap gen for apr.
it is a generic framework that can be easily integrated with any sequence to sequence learning models.
to the best of our knowledge this is the first work to leverage the power of retrieval in fix pattern mining for dl based apr systems.
we present a hybrid patch retriever for fix pattern mining that accounts for both lexical and semantic matching through a combination of sparse and dense retrievers.
it is a languageagnostic patch retriever using raw source code which does not require any code specific features.
we propose to adapt a generic pretrained code aware language model codet5 as a foundation model for rap gen to fix various bugs.
moreover we leverage it for both patch retrieval and generation task in a unified manner.
we extensively evaluate rap gen on three apr benchmarks in javascript and java.
results show rap gen significantly outperforms sota dl based methods on all benchmarks.
particularly our best model yields substantial improvements .
.
on exact match accuracy and .
.
on error removal accuracy on tfix over the previous sota t5large model with a .5x larger model size than ours.
on code refinement rap gen sets new sota exact match results of .
and .
over codet5 s .
and .
for the small and medium subsets.
on defects4j rap gen achieves new sota performance repairing more bugs with perfect fl and more bugs without perfect fl than the previous sota models.rap gen retrieval augmented patch generation with codet5 for automatic program repair esec fse december san francisco ca usa fix fix bug fix bug bug fix bug figure retrieval augmented patch generation rap gen framework with codet5 for automatic program repair.
we first retrieve the relevant bug fix patterns from the codebase through our hybrid patch retriever which takes both lexical and semantic similarity into account.
we then concatenate the top retrieved bug fix pattern with the query buggy patch for our patch generator to synthesize a ranked list of fix patch candidates for developers to verify.
related works .
automatic program repair in the past decades automatic program repair apr has attracted growing attention and various apr techniques have been proposed to reduce the manual efforts in debugging.
a notable class of conventional techniques for apr is known as search based or generateand validate approach .
earlier search based apr techniques are often based on program modification or mutation with heuristic algorithm or genetic programming to produce a large pool of candidate fixes for validating with unit tests.
the search strategy has been further extended to adopt fix patterns mined using redundancy based techniques .
these approaches make a redundancy assumption that the fixed patch can often be reconstructed from elsewhere in the codebase which has been validated empirically by studies showing that a significant proportion of commits are indeed composed of existing codebase.
more redundancy based techniques have shown that mining fix patterns from existing codebase or even external q as from stackoverflow can largely benefit apr systems.
recently with the recent advancement in deep learning dl approaches for natural language processing nlp many dl based apr techniques have been proposed to automate the program repair process in an end to end data driven manner.
motivated by the success of neural machine translation nmt these techniques often formulate apr as a sequence tosequence nmt problem which is to translate a buggy programinto a fixed version.
various neural architectures have been explored in learning based apr techniques.
earlier techniques are based on recurrent neural networks which is further extended to convolution neural networks in coconut and transformer based models by many recent dl based models including tfix cure recoder rewardrepair and selfapr .
notably many of these dl based approaches explore improving apr by leveraging code specific features such as abstract syntax trees asts and test execution diagnostics .
specifically recoder learns the syntax guided edits over the asts to ensure the syntactic correctness of the generated fix patch while dear uses tree based long short term memory lstm model to better encode the code structure and constructs a suitable fixing context using surrounding ast subtrees.
for the use of test execution information selfapr encodes test execution diagnostics into the input representation while rewardrepair improves apr with a loss function based on both program compilation and test execution information.
in terms of apr benchmarks the most popular one would be defects4j which contains real bug fix patches from open source github projects and has been widely adopted by a large body of apr work .
one notable feature of this benchmark is that it contains a test suite to validate whether the bugs are fixed or not.
however as these apr approaches rely on test cases they are inapplicable to newly discovered bugs or bugs difficult to test for deterministically .
additionally it remains a key challenge to obtain a large scale apr dataset with test cases e.g.
one of the largest one defects4j only contains less than 1000esec fse december san francisco ca usa weishi wang yue wang shafiq joty and steven c.h.
hoi bugs and another popular one quixbugs only have bugs.
to get rid of the requirement of test cases there is another group of apr research focusing on static analysis bugs or violations which can be flagged by static analysis tools and is easier to curate much more bug fix data.
besides another type of apr is based on the bug fixing commits by checking whether the commit comments contain some keywords such as repair and fix .
we consider all these types of apr use cases in this work.
.
pretrained language models for code pretrained language models lms like gpt bert and t5 have significantly boosted performance in a broad set of nlp tasks.
inspired by their success much recent work attempts to adapt the nlp pretraining methods to programming language.
they often rely on either an encoder only bert style models codebert and graphcodebert or decoder only gpt style models codegpt and codex or encoder decoder models plbart and codet5 .
particularly codet5 is a unified language model pretrained with a code aware pretraining objective on large scale code corpora covering different programming languages which has been shown to achieve sota performance on a wide range of code understanding and generation tasks .
compared to previous dl based apr approaches such as cure and tfix that utilize lms pretrained primarily on natural language corpus we propose to leverage the code aware lms of codet5 for apr with better code understanding capability.
there are recent attempts to explore few shot learning of large language models llms for apr.
according to prenner et al.
their method based on codex achieves em compared to the finetuned t5 s on a random sample of instances from tfix showing that there is still a gap between few shot learning and finetuning results.
besides few shot learning of llms requires more engineering efforts for prompting tuning and post processing which is labor intensive.
another concern is that llms such as codex are not open sourced and it might be expensive to use their apis e.g.
the davinci version costs .
for every 1k tokens1.
.
retrieval augmented generation a general retrieval augmented generation paradigm is comprised of three components including information retrieval data augmentation and generation model .
it has been widely studied in nlp and shown to achieve sota performance in a wide range of nlp tasks including question answering and question generation and machine translation .
inspired by their success much research work adapts this paradigm also referred as retrieveand edit refine framework to benefit software intelligence tasks including code autocompletion code summarization and code generation .
approach we propose rap gen a novel retrieval augmented patch generation framework for apr which aims to improve apr performance by leveraging a relevant bug fix pattern retrieved from a codebase of previous bug fix pairs.
as shown in fig.
our rap gen framework of three stages a patch retriever training stage to learn a hybrid retriever that can find relevant code patches based on the lexical and semantical similarity a patch generator training stage to train a codet5 model to produce the fix patch based on both buggy input and retrieved bug fix examples an inference stage to predict multiple fix patches where the top ranked one will be passed to developers for verification.
note that while retrieval augmented generation techniques have been explored in many nlp tasks it is not trivial to adapt such techniques to apr tasks and requires systematic adaptation to address some unique challenges.
the first challenge is how to retrieve relevant fix patterns for effectively guiding apr where we build a hybrid retriever based on both lexical and semantic information which is analyzed and compared with other retrievers in table .
the second challenge is how to build a top performing apr model for various languages and apr scenarios.
we leverage a language agnostic pretrained model codet5 for both retrieval and patch generation which is a more unified approach compared to prior work requiring a different retriever and generator.
in the remainder of this section we first introduce the task formulation of the retrieval augmented patch generation for apr in section .
and then revisit the backbone model of codet5 in section .
followed by detailing the hybrid patch retriever in section .
and the retrieval augmented patch generator in section .
.
.
task formulation letd xi yi d i 1be a program repair dataset consisting of d bug fix pairs xi yi wherexiandyiare thei th buggy and fixed program patch respectively.
assume that we have a codebase containing a large collection of previous bug fix pairs c bj fj c j where bj fj denotes the j th previous bug fix pair.
given a buggy program patch xiind a retriever retrieves the most relevant bug fix pair bj fj in the codebasecbased on a relevance scoring function f xi bj parameterized by .
then the original input sequence xiis augmented with the retrieved bug fix pair to form a new input sequence xi xi bj fj where denotes the concatenation operation.
the sequence to sequence seq2seq generator then generates yifrom xiin an autoregressive manner.
formally we aim to learn the following probability with the patch seq2seq generator parameterized by p yi xi n k 1p yi k xi yi yi k whereyi yi k 1is the previous sequence before the k th token andndenotes the number of tokens in the target sequence yi.
note that we regard the external codebase cas a non parametric memory and the retrieved bug fix pair as a guiding fix pattern for the generator.
in probabilistic terms the retrieval can be formulated as a latent variable zj bj fj which is approximated by top in our case.
formally the probability can be decomposed as p yi xi c j 1p zj xi z retrieverp yi xi zj z generator p yi xi z j wherez jis the top retrieved output from the retriever p zj xi .
we adopt such top approximation as marginalization over large krap gen retrieval augmented patch generation with codet5 for automatic program repair esec fse december san francisco ca usa makes the training and inference complicated and inefficient .
we also tried to employ top k k with the fusion indecoding or fid method but did not observe a salient performance improvement.
.
revisiting codet5 codet5 is a unified pretrained transformer based encoderdecoder language model that achieves sota results in both code understanding and generation tasks.
it is pretrained on .
million functions in different programming languages i.e.
ruby javascript go python java php c c collected from github.
codet5 employs a set of identifier aware pretraining objectives to incorporate the code specific knowledge into the language model.
in this work we adapt codet5 as our dense dpr retriever and patch generator to harness its powerful code understanding capability.
bpe subword tokenization .one benefit of using codet5 is that it provides a code specific byte pair encoding bpe tokenizer.
it can avoid the prevalent out of vocabulary oov problems in the code domain as programmers tend to write arbitrary identifiers and it is impossible to build a fixed vocabulary to accommodate arbitrary tokens commonly known as open vocabulary problem .
bpe is an algorithm that learns how to efficiently split tokens into subwords based on their frequency distribution.
it can also help reduce the vocabulary size as it will split rare tokens into multiple subwords instead of directly adding the whole tokens into the vocabulary.
additionally as the codet5 tokenizer is pretrained and optimized for eight popular programming languages the resulting tokenization generalizes well.
as pointed out by it reduces the tokenized sequence by on average compared to the default t5 tokenizer .
encoder and decoder architecture .codet5 consists of a stack of transformer layers for its encoder and decoder.
each transformer layer contains a multi head self attention for feature aggregation followed by a feed forward layer over the output of previous layer.
the final layer produces the hidden states for all input tokens which can be employed as the code presentation for classification or generation tasks.
for the codet5 encoder it utilizes bidirectional attention masks to learn better contextualized representation similar to bert while the codet5 decoder employs causal attention masks to ensure each token can only attend to the previous tokens for better sequence generation.
in rap gen framework we adapt the codet5 as the patch generator and its encoder specifically for the dense retriever.
.
hybrid patch retriever the retriever module in rap gen aims to retrieve relevant fix patterns to guide the apr process.
it builds on a relevance scoring functionf xi bj to compute the relevance between the query bugxiindand a previous key bug bjin the codebasec.
as shown in we utilize a hybrid approach to combine a lexicalbased bm25 retriever and a semantic based dpr retriever to take both lexical and semantic information into account.
prior work like show that sparse and dense retriever can complement each other for more robust text retrieval.lexical based retriever .we employ bm25 a well known term based retriever that uses sparse vector representation for lexical matching.
bm25 converts each code patch as bag of words representation and computes a lexical similarity between the query patchxiand a candidate patch bj.
the computed similarity score is represented as f xi bj bm25 xi bj .
as a sparse term based retriever bm25 is sensitive to the choice of identifier naming in source code which does not impact the code semantics.
semantic based retriever .we employ dense passage retriever dpr to retrieve relevant patches via measuring their semantic similarity.
to encode the code patch we use a transformer based encoder to map each patch to a fixed size dense vector.
specifically we initialize the dpr from a pretrained codet5 encoder and train it for a code to code retrieval task.
for training the dpr we propose to use the bug fix pairs in the codebase by considering the buggy codebjas the query and the corresponding fixed code fjas the key.
this is based on the assumption that the buggy patch and its fixed patch often shares similar semantics e.g.
identifiers and code structures .
this trick avoids the massive manual annotation efforts needed to curate a bug to bug search dataset.
for each query patch and candidate patch we prepend a special token of into its tokenized sequence and employ the final layer hidden state of the token as the patch representation.
we use a shared dpr to separately encode the query patch xiin dand a candidate patch bjincasclsxiandclsbj respectively.
then the similarity is computed by the inner product between these two patch representations as the following f xi bj sim xi bj t for training the dpr retriever we leverage the in batch negatives to optimize an infonce contrastive loss defined as follows lnce nn i logexp sim bi fi exp sim bi fi j m j iexp sim bi fj wheremis the current minibatch and ndenotes the number of positive training examples in the minibatch.
this objective aims to maximize the similarity between positive examples while minimizing the similarity between negative examples.
each positive example will have m 1negative samples.
note that we do not adopt the hard negative mining strategy as in due to the noisy nature of the training data.
in the inference stage given a query buggy patch xi the dpr retrieves a relevant bug fix pair bj fj by computing the similarity betweenxi query and bj key .
we also tried to base on the similarity between xiandfjbut it did not yield better results.
hybrid retriever .to take both lexical and semantic information into account we utilize a hybrid approach following to combine the bm25 and dpr.
the similarity score is computed as f xi bj sim xi bj bm25 xi bj where is a weight to balance the two retrievers and was empirically set to in our experiment.
based on this combined similarity score we select the top relevant bugfix pair bj fj as a fix pattern to guide the patch generator for bug fixing.
the hybrid retriever is expected to be more robust compared to retrievers that rely only on either lexical or semantic information.esec fse december san francisco ca usa weishi wang yue wang shafiq joty and steven c.h.
hoi error information fix guard for in the body of a for in should be wrapped in an if statement to filter unwanted properties from the prototype.patch difference for i i data.updates.length i ext data.updates for e in data.updates if data.updates.hasownproperty e ext data.updates buggy code var e ext for e in data.installs ext data.installs fixed code var e ext for e in data.installs if data.installs.hasownproperty e ext data.installs a tfix b code refinement c defects4j chart source inputretrievalcodet5rap genpatch difference if endindex endindex startindex endindex if endindex endindex startindex buggy code else if changeindex endindex if changetype listevent.insert endindex updates.addinsert changeindex startindex fixed code updates.addupdate changeindex startindex public class timeseries extends series implements cloneable serializable endindex endindex this is first item after end period endindex endindex so this is last item before end if endindex if endindex endindex startindex emptyrange true if emptyrange patch difference private synchronized void method 1 java.lang.string var 1 if var 1 !
null type 1 .
i var 2 string 1 var 1 var 3 var 1 private synchronized void method 1 java.lang.string var 1 var 3 var 1 buggy code public void method 1 java.lang.string var 1 type 1 .
i var 2 string 1 var 1 string 2 var 3 var 1 fixed code public void method 1 java.lang.string var 1 var 3 var 1 private synchronized void method 1 java.lang.string var 1 type 1 .
i var 2 string 1 var 1 var 3 var 1 figure bug fix examples on three apr benchmarks where rap gen successfully fix bugs while codet5 fails to do so.
.
retrieval augmented patch generator as shown in fig.
given a buggy patch xi we search for a top relevant fix pattern bj fj and pass it to the patch generator to generate a fixed code patch yi.
we adopt a simple yet effective strategy to augment xiinto xi xi bj fjvia appending the retrieved bug fix pair into the source buggy patch.
note that the patch generator module can be any sequence generation model.
different from prior studies that directly adopt a generator optimized on natural language we propose to employ codet5 a code aware programming language model optimized for code.
training .we prepare the retrieval augmented input to codet5 patch generator as xi xi bj fj where and are special tokens to separate the retrieved bug fix pair from the buggy patch.
codet5 s encoder takes xias input and emits the fixed patch yifrom its decoder in an autoregressive manner see section .
.
we consider two settings of the buggy patch xi where it may or may not contain bug localization information.
if it contains error information like error type error message and error line the buggy patch will be augmented to error information xi to incorporate error information to help fix the bugs.
to train the patch generator we adopt teacher forcing to minimize the cross entropy loss lceover all training instances defined as lce d i 1log p yi xi in teacher forcing the decoder uses ground truth context for faster convergence.
we use the training set as the search codebase following .
to avoid information leakage we do not allow the retriever to access the ground truth bug fix pair otherwise the training loss would easily drop close to as the generator can directly copy the retrieved fix as the target output.
this strategy makes the training and evaluation process more compatible as the evaluation sets are not overlapped with the training set as well.inference with beam search .during inference as shown in fig.
we employ beam search to generate a ranked list of fixed patch candidates for an input buggy patch where the number of predictions is determined by the beam size b. concretely at each decoding timestep the beam search selects the most bpromising fix candidates with the highest probability using a best first search strategy.
the search process is terminated when an token notifying the end of sentence is emitted.
the top ranked fix patch will be examined for its correctness by comparing with groundtruth fix patches or by validating against test suites or by manual verification by software developers.
experimental design .
dataset we evaluate rap gen on three apr datasets namely tfix in javascript code refinement and defects4j v1.
in java.
all datasets are originally collected from open source github commits but based on different criteria for bug identification where tfix is based on diagnostics from a javascript static analyzer code refinement is based on repair related commit message and defects4j is based on running the test suites.
we report their data statistics in table .
.
.
tfix.
tfix is a large scale program repair dataset comprising javascript code patch pairs curated from .
million github commits.
it includes error types see table detected by a static analyzer eslint2 .
in addition to error types it provides rich error annotations such as error message and localized error line so that there is no need for fault localization like prior work .
to prepare the input sequence as illustrated in fig.
a we follow to combine all error information together with the buggy code patch into a single piece of text as the following fix error type error message error context code line n buggy line n code line n retrieval augmented patch generation with codet5 for automatic program repair esec fse december san francisco ca usa table statistics of three program repair benchmarks.
benchmark version train valid test tfix original tfix deduplicated code refinement small code refinement medium defects4j v1.
defects4j v2.
where error context consists of the given localized error line and its two neighboring code lines to form a buggy code patch.
for the target sequence it is obtained by replacing the error line into a fixed line in the error context.
during data processing we observed aduplication issue inside each data split and between data splits.
specifically there are and duplicates in the train validation and test split respectively and and duplicates for inter split duplicates between train and test train and test validation and test splits respectively.
we filtered all these duplicates to get a deduplicated version of tfix as shown in table .
baseline models.
we compare rap gen with existing dl based apr models including sequencer and coconut .
besides we compare a large pretrained model t5 large which has been finetuned on tfix to achieve the sota performance by .
evaluation metrics.
we report exact match em accuracy and bleu score to evaluate program repair performance following on tfix.
bleu is a looser metric to evaluate the degree of subword overlapping while em is a more strict metric requiring the prediction to be identical to the ground truth patch in a real commit.
as a buggy program might have different ways to repair we further employ error removal metric following to take various forms of fixes into account.
the prediction is counted as correct for error removal if the existing error is removed and no new errors detected by the static analyzer eslint is introduced after the fix.
for all metrics we present their results on a scale of and a higher score represents better performance.
.
.
code refinement.
code refinement contains bug fix pairs at the function level which are originally collected from public github archive3between march and october .
they use google bigquery apis to identify all java commits having a message containing the patterns fix or solve and bug or issue or problem or error to ensure the quality of the collected bug fix function pairs.
they normalized the functions via obfuscating identifiers with indexed tokens such as type1 var1 method1 etc.
one data example can be found in fig.
b .
the dataset contains two data subsets which are determined by the number of tokens i.e.
of code tokens for the small set and of code tokens for the medium set.
since the bug localization is not provided the entire code fragment is taken as the source input sequence of our model.
the target sequence is the refined version of the whole code snippet.
baseline and metrics.
we compare our rap gen with pretrained programming language models based on transformers .
one group of these models is the encoder only models such as roberta comparison results of codet5 on the original tfix.
modelem w spaces em w o spaces avg.
w. avg.
avg.
w. avg.
naive copy .
.
.
.
sequencer .
coconut .
t5 small .
.
.
.
t5 base .
.
.
.
t5 large .
.
.
.
codet5 small .
.
.
.
error information .
.
.
.
codet5 base .
.
.
.
error information .
.
.
.
code codebert and graphcodebert .
these encoderonly models require a randomly initialized decoder to generate the fix.
besides we compare with encoder decoder transformer models such as plbart and cotext .
nsedit is a language model with encoder and decoder initialized from codebert and codegpt respectively.
it is finetuned to generate the fix via a neural symbolic editing sequence and ranks as the current sota model on code refinement.
we follow to apply bleu and exact match to evaluate the code refinement datasets.
.
.
defects4j.
defects4j has been one of the most widely adopted apr benchmarks which contains real bug fix patches in open source github projects.
each bug fix example is accompanied with test cases to validate the fix.
one example of defects4j bugs can be found in fig.
c where denotes a buggy line to be fixed and represents the correct fix committed from a developer.
a buggy line and its corresponding code context are combined to form the source input sequence while the target sequence is the fixed line.
as defects4j only has the test set we use the projectspecific training data curated by selfapr using self supervised learning methods.
specifically ye et al .
proposes perturbation rules on the correct past version of defects4j to construct synthetic bug fix java patches.
we use a subset of training data that is available online.4for testing we follow their exact settings to evaluate our models on bugs from both defects4j v1.
and v2.
which covers both settings with ground truth fault localization perfect fl and with predicted fls from spectrum based fl tools such as gzoltar .
baselines and metrics.
we compare rap gen with a broad set of sota dl based apr models including sequencer coconut cure rewardrepair recoder dlfix dear buglab and selfapr .
for evaluation we compute how many bugs can be correctly fixed on defects4j based on unit testing and manual verification following prior work.
we first run test suites to automatically identify plausible correct patches for each bug followed by manual checking to completely verify its correctness.
the correct predictions from our rap gen are included in our artifact.
for results of baselines we cite the results of dlfix and dear from dear and other results from selfapr .
december san francisco ca usa weishi wang yue wang shafiq joty and steven c.h.
hoi table performance of rap gen on the deduplicated tfix.
results of t5 large and codet5 base are different from table due to the deduplication.
model em bleu t5 large tfix .
.
codet5 base .
.
rap gen .
.
.
implementation details we adopt codet5 base that contains encoder layers and decoder layers with the parameter size of 220m for rap gen. we implement rap gen using pytorch and train it with adamw optimizer.
for the training of its neural components we run these experiments with nvidia a100 40g gpus on the google cloud platform.
for each benchmark we finetune a dpr retriever for epochs using the contrastive loss linfonce using a batch size of and a learning rate of 2e .
we finetune rap gen generator for epochs using a sequence generation loss lceusing a batch size of with a learning rate of 5e .
these best settings are obtained through a grid search for hyper parameter tuning batch size in and learning rate in 1e 5e 2e .
the training time of dpr retriever is hours depending on the training size of the dataset and the training time of rap gen generator is within days.
for lexical based retrievers we use an open sourced python library5of bm25 which can be efficiently trained on cpu within one hour with multi processing.
during inference we employ beam search with a beam size of for the tfix and code refinement and for the defects4j.
.
research questions to investigate the effectiveness of rap gen on apr tasks we seek to answer the following research questions rqs rq1 comparative study with dl based apr models on tfix.
how does rap gen perform to repair javascript linter flagged coding errors on tfix compared with other dl based apr approaches?
rq2 analysis of rap gen predictions on tfix.
how does rapgen repair tfix bugs for different error types and patch lengths?
what fix operations do rap gen adopt in repairing bugs?
rq3 comparative study with dl based apr models on code refinement.
how does rap gen perform to repair java commitrelated bugs compared with other dl based apr approaches?
rq4 analysis of our hybrid patch retriever.
can our hybrid patch retriever find relevant fix pattern to guide apr?
rq5 comparative study with dl based apr models on defects4j?
how does rap gen perform to repair java bugs in open source projects compared with other dl based apr approaches?
experimental result .
rq1 comparative study with dl based apr models on tfix .
.
improved tfix evaluation.
the original tfix benchmark employs the direct average of exact match em accuracy across performance breakdown on error types on tfix.
error type samples t5 large rap gen error type samples t5 large rap gen no new symbol .
.
no extra bind .
.
no compare neg zero .
.
no case declarations .
.
no ex assign .
.
no fallthrough .
.
for direction .
.
no inner declarations .
.
no unsafe finally .
.
no array constructor .
.
use isnan .
.
no constant condition .
.
no class assign .
.
generator star spacing .
.
no dupe class members .
.
no extra boolean cast .
.
no func assign .
.
no cond assign .
.
no empty pattern .
.
no process exit .
.
no unused labels .
.
no empty .
.
no duplicate case .
.
no dupe keys .
.
getter return .
.
prefer spread .
.
no sparse arrays .
.
no useless escape .
.
no const assign .
.
no console .
.
no global assign .
.
guard for in .
.
no new wrappers .
.
no throw literal .
.
no this before super .
.
no debugger .
.
no unsafe negation .
.
prefer rest params .
.
require yield .
.
no unreachable .
.
no extend native .
.
no extra semi .
.
no new object .
.
no redeclare .
.
no caller .
.
comma style .
.
constructor super .
.
no unused vars .
.
valid typeof .
.
no undef .
.
no self assign .
.
no invalid this .
.
sum w. avg.
.
.
error removal exact match bleu 4405060708090test results78.
.
.
.
.
.99rap gen t5 large figure error removal comparison on tfix where error removal is well aligned with exact match and bleu scores.
error types as the main metric.
however as shown in the table these error types have a rather imbalanced distribution e.g.
the major error type no invalid this has instances while the least error type no new symbol has only instances.
as such it is more reasonable to employ the weighted average to take the error type distribution into account.
besides we spot another limitation of its exact match evaluation that if the predicted fix contains one more whitespace such as a space or new line than the ground truth fix it would be regarded as a wrong exact match.
however extra whitespaces do not impact the correctness for javascript programs.
therefore we propose to use the weighted average of em w o spaces which normalizes the whitespaces before computing the em to exclude the effects of the mismatch in whitespaces.
as we find there is a duplication issue in the tfix dataset we also report the results on its deduplicated version.
.
.
codet5 results.
we compare codet5 models with other dlbased baselines on tfix and show results in table .
for the original metric of average em w spaces codet5 base .
also yields a better accuracy than t5 large .
given that it has much larger model size .
of codet5 base 770m vs. 220m .
if we focus on a more reasonable average em w o spaces codet5 base significantly boost the performance with around absolute accuracy improvement .
.
over t5 large.
based on the weighted average em w o spaces both codet5 small .
and codet5 base .
outperform all the baselines including t5 large .
.
this shows codet5 models with code aware pretraining on large scale sourcerap gen retrieval augmented patch generation with codet5 for automatic program repair esec fse december san francisco ca usa table analysis of error line removal operation on tfix.
t5 large codet5 rap gen ground truth el removal predicted el removal correct el removal false positive precision .
.
.
recall .
.
.
f1 .
.
.
number of tokens0.
.
.
.
.
.0cumulative fraction a exact match rap gen correct rap gen wrong number of tokens050100150number of correct fix b exact match rap gen t5 large figure a cumulative fraction of programs by number of tokens in the source buggy patch grouped by whether rap gen can accurately fix.
b distribution of correct fix over number of tokens for rap gen and t5 large.
code have a better understanding of program.
for tfix evaluation we employ em to denote the weighted average em w o spaces.
we perform an ablation study to remove the error information including error type and error message from the input sequence where we observe both codet5 small and codet5 base models have a consistent performance downgrade revealing that it is helpful to inform which types of error they need to fix for apr models.
.
.
rap gen results.
we report the results of our rap gen model on the deduplicated tfix benchmark in table where the results are slightly different due to data size changes after duplication.
results show that rap gen significantly outperforms t5 large .
.
em .
this indicates retrieval augmented generation is a viable and effective approach for apr and both semantic information and lexical information are crucial to retrieve relevant fix patterns.
we present one case in fig.
a where we can observe rap gen successfully repairs the bug with the guidance of retrieved fix pattern while codet5 without retrieval gives a wrong fix.
.
.
error removal evaluation.
though exact match can ensure correctness of machine generated patches it might be a too strict metric to consider other forms of correct fixes.
therefore we follow to employ the error removal metric where a fix is counted as correct if the error is removed and no new error is introduced.
the error detection is based on a static analyzer eslint.
we report error removal together with em and bleu results on a large subset of instances6in fig.
.
we observe that rap gen significantly improves error removal accuracy over t5 large .
.
.
the larger gain compared to em and bleu implies that rap gen is more capable of producing various forms of good fixes.
additionally em is well aligned with the looser metric of error removal.
6some source files are unavailable to reproduce this metric on the full test set.table performance of rap gen on the code refinement.
modelsmall medium em bleu em bleu naive copy .
.
.
.
lstm .
.
.
.
transformer .
.
.
.
roberta code .
.
.
.
codebert .
.
.
.
graphcodebert .
.
.
.
plbart .
.
.
.
cotext .
.
.
.
nsedit .
.
.
.
codet5 .
.
.
.
rap gen .
.
.
.
.
rq2 analysis of rap gen on tfix .
.
performance breakdown on error types.
we list the performance breakdown for error types on the deduplicated tfix in table .
rap gen outperforms the previous sota t5 large in error types.
especially for the major error type no invalid this rap gen improves its exact match from t5 large s .
to .
i.e.
repairing more instances.
in total rap gen correctly repairs more bugs than t5 large with a much smaller model size.
.
.
fix operation analysis.
we analyze what fix patterns are performed by our models on tfix.
we observe a large proportion of fix consists of deletion operations compared to the code insertion and replacement operations.
we find that fix operations consist of code insertion .
replacement .
deletion .
insertion and replacement .
insertion and deletion .
replacement and deletion .
and all three manners .
.
earlier studies also reflect that the deletion operation is one of the most common fix patterns.
besides we find one dominating fix operation is error line el removal which is to simply remove the error line from the buggy code and accounts for around in the test set.
we show how models perform this operation in table .
we observe rap gen achieves the best precision recall and f1 scores with a lowest false positive count of compared to codet5 s and t5 large s .
this indicates that rap gen is able to learn more diverse bug fix patterns instead of over relying on the trivial error line removal pattern.
.
.
patch length analysis.
we analyze the impacts of patch length fig.
.
fig.
a shows the cumulative fraction of buggy patches by its patch length grouped based on their outcome.
we find the patches successfully repaired by rap gen tend to be shorter than those where it fails.
fig.
b shows the distribution of correct fixes by its buggy patch length where rap gen can repair more bugs than t5 large especially for patches with to tokens.
.
rq3 comparative study with dl based apr models on code refinement we report the comparison results on code refinement in table .
all baseline results are directly obtained from their original papers.esec fse december san francisco ca usa weishi wang yue wang shafiq joty and steven c.h.
hoi table effects of retriever modules in rap gen. retriever tfix refine small refine medium no retriever .
.
.
random .
.
.
bm25 .
.
.
codebert .
.
.
codet5 .
.
.
hybrid bm25 codet5 .
.
.
we first observe that naive copy gives a pretty high bleu score but with a zero exact match indicating the buggy code and its fix has a large overlap and exact match should be employed as the primary metric.
among the baselines nsedit is a very competitive one with a best result .
em on the small subset and codet5 gives the best result .
em on the medium set.
the lower results on the medium set compared to the small set indicates that longer buggy functions are more difficult to fix which is aligned with observations in fig.
a .
overall rap gen achieves new sota results on two subsets with .
em for small set and .
em for medium set.
this again confirms that retrieved fix patterns provide helpful signals to guide the program repair and the hybrid retriever is more robust by using both lexical and semantic information.
fig.
b shows one case where the retrieved fix pattern error line removal helps rap gen to successfully fix the bug.
.
rq4 analysis of hybrid patch retriever we investigate how different retrieval modules affect the apr performance in the retrieval augmented generation setting in table .
we first compare with a random baseline via randomly retrieving bug fix pairs from the codebase.
the consistent performance downgrade compared to no retriever implies that randomly retrieved fix patterns cannot provide useful guiding signals for apr.
then we compare our hybrid retriever in rap gen with different retrievers sparse bm25 retrievers and dense retrievers based on codebert or codet5.
we observe that codet5 based retrievers outperforms either bm25 or codebert based retrievers while our hybrid retriever combining both bm25 and codet5 achieves the best apr performance validating the effectiveness of our retriever module design in rap gen. we further analyze the performance of our retrievers in terms of lexical and semantic matching between the query and the top retrieved patches.
we employ the bleu score to measure their subtoken overlap for lexical matching while for semantic matching we compute the cosine similarity cossim between their dense vectors encoded by our fine tuned dpr retriever.
table shows the performance of our retrievers on both tfix and code refinement benchmarks.
the first row indicates the lower bound performance via randomly retrieving bug fix pairs from the codebase where we observe this random baseline achieves much lower scores in both lexical and semantic matching.
for lexical matching bm25 outperforms dpr codet5 based on tfix but underperforms on two code refinement subsets.
we anticipate that it is due to the data difference between tfix and code refinement where the latter employs obfuscated identifierstable lexical bleu and semantic cossim retrieval matching results on tfix and code refinement benchmarks.
retrievertfix refine small refine medium bleu cossim bleu cossim bleu cossim random .
.
.
.
.
.
bm25 .
.
.
.
.
.
dpr .
.
.
.
.
.
hybrid .
.
.
.
.
.
e.g.
var1 var2 ... that hinders the performance of the lexicalbased bm25 retriever.
the hybrid retriever achieves the best lexical matching on all datasets revealing the semantic information can complement to the lexical information.
for semantic matching dpr achieves the best results on all datasets which is not surprising as it is optimized towards the identical objective.
notably our hybrid retriever achieves slightly lower results than dpr but much better results than bm25 implying it can balance both lexical and semantic information and be more robust than the lexical based retrievers which are sensitive to the choices of identifier naming.
.
rq5 comparative study with dl based apr models on defects4j .
.
rap gen results.
we compare rap gen with other sota dl based apr baselines on defects4j v1.
and v2.
in table .
we consider two settings with spectrum based fault localization fl and with the perfect fl.
note that all the baseline results are cited from selfapr and dear .
for a fair comparison we follow common practice to adopt the same hour timeout a beam size of an ensemble strategy as recoder for rap gen. as shown in table our rap gen achieves new sota performance under perfect fl by repairing the largest set of bugs bugs in v1.
and bugs in v2.
compared to other baselines.
particularly it repairs and more bugs than the previous sota selfapr in v1.
and v2.
respectively.
for the results with spectrum based fl rap gen achieves the second best performance which are very competitive to the sota models on both v1.
vs. recoder s and v2.
vs. selfapr s .
considering both v1.
and v2.
bugs it repairs bugs in total surpassing either recoder s or selfapr s bugs.
overall both results with or without perfect fl validate the superiority of our rap gen over other dl based baselines.
notably compared to many of these models our rap gen exhibits another advantage of being a language agnostic model that can generalize to other apr use cases.
by contrast recoder requires to learn edits over ast and selfapr requires the test execution diagnostics making them inapplicable or limited to deal with fragmented code snippets that cannot be parsed into asts or other apr scenarios without test cases.
we investigate to what extent rap gen can complement existing apr models including recoder rewardrepair and selfapr .
compared with these sota dl based apr approaches rap gen repairs and unique bugs for defects4j v1.
and v2.
respectively which are never correctly addressed by any other dlbased apr approaches veifying that our rap gen can complement to other top performing apr approaches.
we further show a caserap gen retrieval augmented patch generation with codet5 for automatic program repair esec fse december san francisco ca usa table performance of rap gen on defects4j v1.
and v2.
.
modelspectrum based fl perfect fl v1.
v2.
v1.
v2.
sequencer buglab dlfix coconut rewardrepair dear cure recoder selfapr codet5 rap gen in the table cells it represents the number of correct patches.
indicates data unavailability.
in fig.
c and find that rap gen successfully fixes the chart bug but in a different form with the developer s fix.
.
.
effects of retrieval from various fix patterns.
we analyze how retrieving a bug fix sample from various fix patterns will affect the apr performance.
for this analysis as shown in fig.
we select bugs from defects4j v1.
and v2.
which codet5 cannot fix red and rap gen can fix green under the setting with perfect fl.
for the categorization of fix patterns we base on the perturbation rules devised from selfapr and use its training set for each rule as a separate retrieval codebase.
we retrieve the top bug fix sample from the codebase for each rule or fix pattern denoted as p1 to p16 and examine whether it can improve codet5 s performance after using the guiding signals from such retrieval in rap gen. from fig.
we observe that retrievals from various fix patterns in rap gen are generally helpful in correcting codet5 s predictions on defects4j bugs.
we find that most of bugs in v1.
can be fixed after retrieval from many different patterns while for v2.
there are some bugs where only a few fix patterns are applicable e.g.
the p16 for closure and p5 for jacksondatabind .
across various fix patterns we find that the p13 of insert an existing block and p14 of delete statement are applicable to most bugs indicating these are key fix patterns for repairing defects4j bugs.
threats to validity construct validity.
we evaluated rap gen on three apr benchmarks tfix in javascript code refinement and defects4j in java.
on tfix we spotted a duplication issue and removed intra split or inter split duplicates out of total data instances.
this might slightly impact the comparison between our model and the tfix t5 large model.
we mitigate this threat by reporting the results of our model on the original tfix dataset and also the results of tfix model on the deduplicated test set.
on code refinement unlike the pairs in tfix can be validated by a static analyzer its bug fix pairs are curated from github commits with a bug fix related commit message and only a portion of them are manually figure effects of retrievals from various fix patterns over bugs from defects4j v1.
and v2.
which rap gen can fix green and codet5 cannot fix red .
we represent each bug on the x axis and use the color to denote its fixing outcome under different retrieval schemes on the y axis.
verified .
there is a chance that some pairs are invalid not related to the bug fix which brings potential threats to the reliability of the evaluation on this dataset.
internal validity.
the threats to internal validity mainly lie in the hyper parameter search stage for rap gen. as a neural model its performance is highly affected by the choice of hyper parameters.
to alleviate such threats we conduct a grid search to tune a better set of hyper parameters but we still cannot claim they are the best.
external validity.
we only evaluated our rap gen model on javascript and java programs and do not study its generalization to other programming languages pls .
however our approach is language agnostic as we do not employ any code specific features like asts and can be applied in a drop in fashion to other pls.
besides our evaluation on three apr datasets in two pls should be comprehensive enough to verify the effectiveness of our approach.
conclusion we present a novel retrieval augmented patch generation rapgen framework for automatic program repair a fundamental task in software engineering to reduce developers manual efforts in debugging.
rap gen consists of two components a hybrid patch retriever to retrieve relevant fix patterns for a query buggy patch and a patch generator to synthesize the fixed patch based on both buggy patch and its retrieved guiding fix patterns.
in addition we propose to leverage a powerful code aware pretrained language model codet5 as the backbone of rap gen to facilitate both patch retrieval and generation in a unified manner.
comprehensive results on three diverse apr benchmarks in javascript and java have demonstrated the effectiveness and superiority of our rap gen model over existing deep learning based apr approaches.
data availability our code and models can be found in this link s a4e95baee01bba14bf4b to reproduce the results in this paper.esec fse december san francisco ca usa weishi wang yue wang shafiq joty and steven c.h.
hoi