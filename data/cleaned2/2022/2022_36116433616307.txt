a practicalhuman labeling method for online just in time so f twaredefect prediction liyansong songly sustech.edu.cn southernuniversity ofscienceand technology shenzhen chinaleandroleiminku l.l.minku bham.ac.uk theuniversity ofbirmingham birmingham uk congteng mail.sustech.edu.cn southernuniversity ofscienceand technology shenzhen chinaxinyao xiny sustech.edu.cn southernuniversity ofscienceand technology shenzhen china abstract just in timesoftwaredefectprediction jit sdp canbeseenas an online learning problem where additional software changes produced over time may be labeled and used to create training examples.
these training examples form a data stream that can be used to update jit sdp models in an attempt to avoid modelsbecomingobsoleteandpoorlyperforming.however labeling procedures adopted in existing online jit sdp studies implicitly assumethatpractitionerswouldnotinspectsoftwarechangesupon adefect inducingprediction delayingtheproductionoftraining examples.
this is inconsistent with a real world scenario where practitioners would adopt jit sdp models and inspect certain softwarechangespredictedasdefect inducingtocheckwhetherthey really induce defects.
such inspection means that some software changes would be labeled much earlier than assumed in existing work potentiallyleadingtodifferentjit sdpmodelsandperformance results.
this paper aims at formulating a more practical humanlabelingprocedurethat takesintoaccounttheadoptionof jit sdpmodelsduringthesoftwaredevelopmentprocess.itthen analyseswhetherandtowhatextentitwouldimpactthepredictive performance of jit sdp models.
we also propose a new method totargetthelabelingofsoftwarechangeswiththeaimofsaving human inspection effort.
experiments based on github projects revealed that adopting a more realistic labeling procedure led to significantlyhigherpredictiveperformancethanwhendelayingthe labelingprocess meaningthatexistingworkmayhavebeenunderestimatingtheperformanceofjit sdp.inaddition ourproposed method to target the labeling process was able to reduce human effortwhilemaintainingpredictiveperformancebyrecommending liyan song cong teng and xin yao corresponding author are with research instituteoftrustworthyautonomoussystems southernuniversityofscienceand technology shenzhen china and guangdong provincial key laboratory of braininspired intelligent computation department of computer science and engineering southern universityof scienceand technology shenzhen china.
leandro lei minku corresponding author is with school of computer science the universityof birmingham edgbaston birmingham uk esec fse december san francisco ca usa copyright heldby theowner author s .
acm isbn .
to inspect software changes that are more likely to inducedefects.weencouragetheadoptionofmorerealistichuman labeling methods in research studies to obtain an evaluation of jit sdp predictive performance that is closerto reality.
ccsconcepts softwareanditsengineering riskmanagement software defect analysis computing methodologies online learning settings classification and regressiontrees bagging.
keywords just in timesoftwaredefectprediction onlinelearning verification latency waitingtime human labeling human inspection acm reference format liyan song leandro lei minku cong teng and xin yao.
.
a practical humanlabelingmethodforonlinejust in timesoftwaredefectprediction.
inproceedings of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering esec fse december san francisco ca usa.
acm new york ny usa 13pages.
introduction just in time software defect prediction jit sdp is a type of sdp specialized at the code change level with the aim of predicting whether or not a software change is defect inducing at commit time just in time .
it is of practical relevance as a decision supporting tool for improving software quality by automatically alerting developers of potential defects at a very early stage as soon as a software change is produced and at a relatively fine granularitycomparedtomodule basedsdp.assuch ithasbeen attractingincreasinginterestfrombothacademia andindustry .
from the machine learning viewpoint jit sdp can be seen as a binary classification problem for which models are constructedbasedontrainingexampleslabeledas defect inducing class or clean class thatcan then be used topredict whether ornot newsoftware changes wouldinducedefects.
inpractice trainingexamplescorrespondingtosoftwarechanges arrive sequentiallyinorder overtimeandthus jit sdp shouldbe takenasanonlinelearningtask wherejit sdpmodelsareupdated withnew incoming training examples .existing literature has revealed that updating jit sdp models with such training examples sothatthemodelscancapturethelatestdatageneration thiswork islicensedunderacreativecommonsattribution4.0international license.
esec fse december3 san francisco ca usa liyan song leandro lei minku congteng xinyao status canleadtobetterpredictiveperformancecomparedtomodels built with obsolete training examples .
updating jit sdp modelsovertimeisparticularlyimportantgiventheexistenceof conceptdrift whicharechangestakingplaceinthecodedefectgeneration process.
concept drift has been shown to occur in jit sdp and may significantlydeteriorate predictive performanceof jit sdpmodels if they are not updatedover time .
dealingwithconceptdriftinjit sdpisneverthelessparticularly challengingastheactuallabelsofsoftwarechangesonlybecome available long after their commit time an issue referred to as verificationlatency inmachinelearning .thiscandelay the model updates potentially affecting the ability of such models to react to concept drift.
in particular a training example can only belabeledasacleansoftwarechangeafterenoughtimehaspassed from its commit time for one to be confident on its clean status or as a defect inducing training example when a defect is found to be associatedtothischange whicheverisshorter .suchlabeling process is referred to as the waiting time method and has been adoptedinrecent onlinejit sdpstudies .
however suchlabelingprocessimplicitlyassumesthatpractitionerswouldnotinspectsoftwarechangesuponadefect inducing prediction.thisisinconsistentwithareal worldscenariowhere developersadoptjit sdpmodelsduringthesoftwaredevelopment process.
specifically when a jit sdp model predicts a new software change to be defect inducing the developer who has just completed the change may inspect the change to check whether or not it really induces any defect.
ignoring such inspection means assuming that practitioners would always completely ignore the warnings given by the jit sdp model and wait until defects are foundmuchlater whenthedefectismoredifficulttobefixed.in other words it means completely ignoring the main point of using jit sdpmodelsinpractice whichistoinspectsoftwarechanges predicted as defect inducing at or close to commit time in an attempt to eliminate their defects when the code change is still fresh in the developers minds.
this may in turn negatively impact theevaluationofpredictiveperformanceofjit sdpmodels asit resultsinadelayinthelabelingprocessthatislargerthanthedelay that would likely occur in practice potentially preventing jit sdp models from reactingto concept drift inamore timely manner.
therefore a more realistic labeling procedure that takes into account the adoption of jit sdp models during the software development process and the resulting human inspection of software changespredictedasdefect inducingshouldalsobetakenasanimportantpartofthejit sdpprocess.thispaperaimsatformulating such labeling procedure and investigating whether and to what extentitwouldimpactthepredictiveperformanceofjit sdpmodels.
we refer to this labeling procedure as immediate humanlabeling forsoftwarechangespredictedasdefect inducing humla .togetherwiththedelayedwaitingtimemethod itformsalabeling processthatisclosertotherealitythatwouldbeadoptedinpractice whenjit sdpmodelsareusedduringthesoftwaredevelopment process.
it is noteworthy that the term immediate in our paper does not mean that the developers has to give up all other work that they had planned in their schedule to immediately inspect the softwarechange.whatwemeanisthatthedeveloperwilllabelthe change as partof the process of inspecting thischange atan early stage which is inherent fromthe adoptionof jit sdp and this isa more immediate labeling than the waiting time procedure.
moreover theprocessofinspectingachangepredictedasdefect inducing isthesameasthelabelingprocess.inotherwords ifadeveloper opts for inspecting a change predicted as defect inducing as part oftheadoptionofjit sdpintheirproject organization labeling this software change does not increase this effort further.
that said inspectingallsoftwarechangesthatarepredictedasdefectinducing as part of the adoption of jit sdp could result in high inspection labeling effort.
so we also propose a human labeling methodtosavehumanlabelingcostsbyhelpingpractitionerstotargettheirinspectionefforttowardsspecificdefect predictedchanges calledeffort conservative humanlabeling eco humla .
our study answers the following research questions rqs rq1howtoformulateajit sdplabelingmethodwithimmediate human labelingofchanges predictedas defect inducing?
rq1.1how does this labeling method affect the jit sdp predictiveperformancecomparedto thedelayedwaitingtime methodfor jit sdp?
rq1.2to what extent the quality of human labels impacts the predictive performance when using this method?
rq1.3howdoestheamountofhumaneffortaffectthepredictive performance when using this method?
rq2how can we target the labeling process towards specific softwarechangestoreducetheamountofrequiredhuman inspection effort?
rq2.1how does this labeling method affect the jit sdp predictive performance?
rq2.
howmuchhuman effortcan this methodsave?
rq2.3howhelpfulisthismethodinencouragingdefectstobe foundwhen savingeffort?
the main contributionsof this paper are listedbelow we are the first to formulate the immediate human labeling method humla into jit sdp being closer to the reality that wouldbe adoptedinpractice rq1 .
we show based on experiments with datasets that this practicallabelingmethodcansignificantlybenefitpredictive performanceofjit sdp when thehuman label quality and quantity are above a given threshold rq1.
.
.
studies thatdonottakesuchlabelingprocessintoaccountmaythus beunderestimatingthepredictiveperformanceofjit sdp models that wouldbe obtainedinpractice.
we propose an effort conservative human labeling method eco humla toprioritizethemostconfidentlydefect predictedsoftware changes for practitioners to inspect rq2 .
we show experimentally that eco humla can substantially reducehumaneffortbyaround50 whilemaintainingjitsdp predictive performance rq2.
.
.
eco humla encourages a higher number of defects to be found through inspection thanabaselineeffortreduction method rq2.
.
related work .
jit sdp earlystudiesusuallymodeledjit sdpasanofflinelearningtask assumingthatalltrainingexamplesareavailablebeforehandand nofurtheradjustmentorevaluationofthejit sdpmodelswould 606a practical human labeling methodforonline just in timeso f tware defectprediction esec fse december3 san francisco ca usa happen over time.
a landmark study is that of kamei et al.
who summarized14featuresextractedfromcommitsandbugreports andshowedthemtobegoodindicatorsforyieldinghighpredictive performanceinjit sdp .manylaterstudieswereconducted basedonthesefeatures .variouslearningmachineshave been investigated for jit sdp among which tree based methods wereshowntohavepotentialinyieldinggoodperformance .techniquessuchasoversamplingandundersampling have alsobeen adoptedto deal with the class imbalance issue usuallysufferedbyjit sdp wherethedefect inducing clean class istypicallythe minority majority .
however studies on offline jit sdp disregard the chronology of softwarechangesthatarrivesequentiallyovertimeinpractice.tan etal.showedthatoverlookingchronologycanresultindeceptively higher predictive performance than the prediction models could achieve in practice posing serious threats to validity in jit sdp studies .
later studies further found that predictive performance of jit sdp models can deteriorate over time as a resultofconceptdrift.therefore jit sdpapproachesbeingableto learnovertimeandreducedropsinpredictiveperformancethatare potentiallycausedbyconceptdrifthavebeenproposed .
.2chronology preservinglabelingprocedures whentakingchronologyintoaccountinjit sdp onealsoneeds to consider the issue of verification latency when labeling training examples as explained in section .
in particular tan et al.
adoptedalabelingprocedurewhereafullbatchofsoftwarechanges is labeled after a pre defined amount of time so that there is an increasedchancethatdefectsassociatedtosoftwarechangeswould have been found to produce defect inducing training examples.
however this procedure does not consider that defect inducing trainingexamplescouldbeproducedassoonasdefectsarefoundto be associated to them which potentially happens before the end of this pre definedperiodoftime.therefore this labelingprocedure can unnecessarily delay the training process of jit sdp models potentiallyslowing downreactionto concept drift.
cabral et al.
proposed a method that overcomes this issue.
it makes use of a pre defined parameter called waiting time that defineshowlongonewouldwaitafterasoftwarechangeiscommittedtolabelitasclean.ifnodefectisfoundwithinthiswaitingtime acleantrainingexampleisproducedattheendofthewaitingtime.
if a defect is found to be associated to this software change within thewaitingtime adefect inducingtrainingexampleisproducedat themomentwhenthisisfound.ifadefectisfoundafterthewaiting time for a change that had been previously labeled as clean a new defect inducingtraining example isproducedfor this change.
suchwaitingtimestrategyhasbeenusedasalabelingmethod foranonlinejit sdplearningprocedureasillustratedinfigure a .
consideraninitialjit sdpmodel m0 thathasbeenproduced withexistingdata.whenanewsoftwarechange u1d44b u1d461iscommitted at test time step u1d461 where u1d44b u1d461 r u1d451denotes a u1d451 dimensional feature vector representing the software change the latest model m u1d461 is adopted to predict whether u1d44b u1d461would be defect inducing class orclean class formulated as hatwide u1d466.alt u1d461 m u1d461 u1d44b u1d461 .
once that is done new training examples are produced based on the waiting time methoduntilanewsoftwarechangearrivestobepredicted.such a jit sdp with the waiting time method adopted in recentonlinejit sdp studies.
b jit sdp with humla proposed in thispaper.
figure jit sdp with differentlabeling methods.
trainingexamplesare usedto producean updatedjit sdpmodel m u1d461 .whenthenewchangearrivestobepredicted theprocedure isrepeatedfortesttimestep u1d461 u1d461 .thisiterativetest then train processbasedonthewaitingtimemethodcontinuesthroughout the process of jit sdp.
several other studies have adopted this procedure withthe waiting time method .
however allpreviousstudieshaveoverlookedthefactthatpractitioners would be inspecting software changes predicted as defectinducingatcommittimewhenjit sdpisadoptedinpractice to checkiftheyarereallyassociatedtoadefect.eventhoughawaiting time isnecessary for labelingsoftware changes predicted asclean byajit sdpmodel changespredictedasdefect inducingwould likely be labeled much earlier.
therefore this more immediate human labeling procedure should be taken as an important part of thejit sdpprocesstoformulatealabelingprocedurethatiscloser to the realitythat wouldbe adoptedinpractice.
immediatehuman labeling injit sdp .
humla figure1 b illustratestheprocedureofimmediate humanlabeling forsoftwarechangespredictedasdefect inducing humla formulatedtoanswerrq1.inhumla ifasoftwarechangeispredicted asdefect inducing thischangewillbemoreimmediatelylabeledby humans whereasachangepredictedascleanislabeledfollowing the waiting time labelingprocedure explainedinsection .
.
in particular when a test example u1d44b u1d461 r u1d451is predicted as defect inducing class at test time step u1d461 formulated as hatwide u1d466.alt u1d461 m u1d461 u1d44b u1d461 the developer who has just produced the code change is requested to immediately inspect the code while it is stillfreshintheirmind andtodecidewhetherornotthischange reallycontainsanydefect.thisprocessproducesatraininglabel h u1d44b u1d461 immediately after u1d461 whereh represents the practical humanlabelingprocedure.meanwhile thewaitingtimelabeling procedureisstillusedtolabelanysoftwarechangesthathavebeen previously predicted as clean and that are now ready to be labeled due to the end of their waiting time.
when u1d44b u1d461is predicted as clean class0 formulatedas hatwide u1d466.alt u1d461 m u1d461 u1d44b u1d461 thereisnoneedfor developers to immediately inspect this change.
therefore only the waiting time procedure isused.
607esec fse december3 san francisco ca usa liyan song leandro lei minku congteng xinyao the immediate training example u1d44b u1d461 h u1d44b u1d461 that is possibly created by humla and any delayed training example s that were possiblycreatedbythewaitingtimemethodatteststep u1d461areallused to update the jit sdp model following their labelingchronological order.
the latest model available before a new software change needstobepredictedisdenotedas m u1d461 .itisworthnotingthat whennosoftwarechangehasbeenlabeledbeforeanewsoftware change arrives to be predicted the model m u1d461 remains the same as the one in the prior test step as m u1d461 i.e.
m u1d461 m u1d461 .
when a new software change arrives to be predicted the time step u1d461is incremented as u1d461 u1d461 such that the most up to date model usedfor its prediction isnowreferredto as m u1d461 .
the humla procedure makes the modeling of jit sdp closer tothereality.asmoreimmediatehumanlabeledexamplescould better capture recent concepts ofthe defect generation processin jit sdp thislabelingprocedurecouldbepotentiallybeneficialto predictiveperformanceespeciallywhenconceptdriftoccursand as long as the quality of human labeling is not poor.
the benefit of humlatopredictiveperformanceisinvestigatedinrq1.
.however two issues need to be considered when investigating humla.
thefirstisthatdevelopersmaymakemistakeswhenlabelingthe defect predicted examples.
therefore it is important to investigate theimpactofsuchhumanlabelnoiseonpredictiveperformanceof jit sdpwhenadoptinghumla.thisisdoneinrq1.2andisfurther explained in section .
.
.
the second issue is that developers mayconsidertheefforttoinspect andlabel allchangespredicted as defect inducing as too high.
to save resources and reduce human effort developers may want to inspect only part of not all defect predicted examples.therefore itisof particularinterestto investigate the extent to which the amount of human effort would affect predictive performance when adopting humla.
this is done inrq1.3andisfurther explainedinsection .
.
.
it is worth noting that since only defect predicted software changeswouldbeimmediatelyinspectedbyhumans thecurrent underlying data generating process would likely be represented mostlybyexamplesofthedefect inducingclass.however sincethe defect inducing class is usually the minority in jit sdp such distributionbiastowardsthisminorityclasswouldprobablynotbe detrimentaltopredictiveperformanceofjit sdpmodelscompared to the benefits gained by the more recent training examples produced by humla .
indeed our experimental studies in section show that humla can typically lead to significant improvement in predictive performance.
.
.
human label noise.
when adopting humla defect predicted examplesmaybe mislabeled ascleanbyhumans whomay fail to find the defect induced by the software change.
when developersdofindadefect predictedsoftwarechangetoinduceadefect itisassuredthatthechangeshouldreallybedefect inducing i.e.
such label would be unlikely to be noisy.
therefore mislabeling ofdefect predictedexamplesisone sided .thatbeingsaid a changethatistrulydefect inducingmaybemanuallymislabeled asclean butachangethatistrulycleanwouldbehighlyunlikely to be manually mislabeledas defect inducing.
therefore the practical human labeling process h u1d44b of a software change described by the feature vector u1d44band predicted as defect inducingbythemostup to datemodelcanbeformulatedash u1d44b if u1d466.alt if u1d466.alt withthe probability u1d6fc if u1d466.alt withthe probability u1d6fc where u1d466.altis the true label of u1d44b u1d6fc denotes a pre defined humanlabelnoisecorrespondingtotheprobabilitythat u1d44bwould be mislabeledby human.
this formulation of h will be used to analyze the impact of differentamountsof one sided human label noise u1d6fc .
.
.
on the predictive performance of jit sdpwhenadoptinghumlatoanswerrq1.2insection .
.
.
inthispaperhereafter werefertosuchformulationas humlaat u1d6fc humannoise andfor the sakeof simplicity humla at human noiseisadoptedas the defaultsettingunless otherwisespecified.
.
.
humaneffortofhumla.
inthissection weformulatehumla consideringthatsomesoftwarechangespredictedasdefect inducing wouldnotbeinspectedbydeveloperstosaveeffort.givenarandom variable following the uniform distribution u1d704 u1d448 we decide whether humans would inspect and label a software change u1d44bpredictedas defect inducingbasedonthe following u1d44b braceleftbigg1 if u1d704 u1d6fd if u1d704 u1d6fd where u1d6fd denotes a pre defined human labeling percentagecorrespondingtothepercentageofdefect predictedexamples that developersopt forinspectingover the total number of defectpredicted examples and is the indicator function deciding whether value or not value the developer opts for inspecting thischangetoproduceatrainingexamplewithhumanlabel h u1d44b asineq.
.larger u1d6fdallowsformoredefect predictedexamplesto be inspectedbydevelopers usually requiring more human effort.
wewillanalyzetheimpactofdifferentamountsofhumaneffort intermsofthelabelingpercentage u1d6fd .
.
onpredictiveperformanceofjit sdpforansweringrq1.3insection .
.
.in thispaperhereafter werefertoitas humlaat u1d6fd humaneffort and for the sake of simplicity humla at human effort is adopted as the default setting unless otherwise specified.
it is also worth notingthatjit sdpat0 humaneffortisequivalenttothewaiting time method.
.
eco humla this section proposes an effort conservative humanlabeling eco humla method for rq2 to save human effort in labeling defect predicted examples while maintaining predictive performanceofjit sdp.assoftwarechangestobelabeledcorrespondto thosethatareinspectedbypractitionersatcommittimeinanattempttofindpotentialdefectsatanearlystage itisalsoimportant for eco humla to prioritize the labeling of software changes that are more likely to contain defects.
givenasoftwarechange u1d44b jit sdppredictswhetherornotthis softwarechangewouldbedefect inducing class1 orclean class basedonthelatestmodel m .besidestheclasspredictionas hatwide u1d466.alt m u1d44b many predictive models can also provide prediction probabilities.
in the case of ensembles of models the prediction probabilities can be computed as the mean predicted class probabilities of the base learners in the ensemble .
we use u1d4501 u1d44b and u1d4500 u1d44b to denote the prediction probability that u1d44bbelongs to 608a practical human labeling methodforonline just in timeso f tware defectprediction esec fse december3 san francisco ca usa class and class respectively where u1d4500 u1d44b u1d4501 u1d44b 1and u1d4501 u1d44b u1d4500 u1d44b .a predictedlabel istypicallydeterminedas hatwide u1d466.alt braceleftbigg1if u1d4501 u1d44b u1d4500 u1d44b 0if u1d4501 u1d44b u1d4500 u1d44b .
based onthese notations we define the predictionconfidence of test example u1d44bbasedonthe jit sdpmodel m as u1d70c u1d44b u1d4501 u1d44b u1d4500 u1d44b where u1d70c and denotes the absolute value.
this metric can measure how much confidence the model has in predicting the software change u1d44b and larger u1d70cindicates higher confidence upon this prediction.
for jit sdp a defect predicted example with larger u1d70cmeans that this change has higher chances of inducing adefectandthusthedeveloperwouldbestronglyrecommended toinspectthischangeatcommittime producinganimmediately labeledexample.
given a random variable uniformly distributed as u1d704 u1d448 practitionerscan heuristically decide whetherto inspect andlabel a defect predicted example u1d44baccording to a probability equal to thepredictionconfidence u1d70c u1d44b .thiseco humlaprocesscanbe formulatedas u1d70c u1d44b braceleftbigg1 if u1d704 u1d70c u1d44b if u1d704 u1d70c u1d44b where istheindicatorfunctiondecidingwhether value1 or not value0 practitionersoptforinspectingthechangetoproduce an immediately labeledtrainingexample.itisworth notingthat similar to humla eco humla only deals with defect predicted examples.
there is no need for humans to immediately inspect clean predicted examples asthiswouldleadto a largeamountof effortfor inspecting changes that are unlikely to inducedefects.
based on this formulation the higher the confidence u1d70c the more strongly practitioners are encouraged to inspect and label the software change and the more likely practitioners are to really label it.
however as this procedure is stochastic there is still some chancethatpractitionerswould wouldnot labelagivenchange thatwaspredictedasdefect inducingwithlow high confidence.
eco humlacouldalsobeusedinadeterministicwaybysetting a fixed threshold to replace u1d70fin eq.
.
however we encourage the use of this stochastic process to avoid the strict assumption thatpractitionerswouldhavetolabelallchangesaboveacertain threshold and cannot label any of the changes below the threshold.
we illustrate how to conductthe eco humlaprocedure as follows.giventestexample u1d44b1and u1d44b2 supposethatthemodelproduces prediction probability u1d4500 u1d44b1 .68and u1d4501 u1d44b1 .32for u1d44b1and u1d4500 u1d44b2 .32and u1d4501 u1d44b2 .68for u1d44b2 individually.
as u1d4501 u1d44b1 u1d4500 u1d44b1 we have hatwide u1d466.alt1 0and it is unnecessary for humans to inspect u1d44b1.
as u1d4501 u1d44b2 u1d4500 u1d44b2 we have hatwide u1d466.alt2 1and its predictionconfidenceisfurthercomputedas u1d70c u1d44b2 .
.
.
.
this means that u1d44b2has the probability of to be inspected by human to immediately obtain its training label.
theprocedure of eco humlais consistent withthe real world processwherepractitionerswouldfavorinspectingthe mostconfident predictions as defect inducing to find as many defects as possible while saving inspection effort.
however it relies on the assumption that the mostconfident defect predictions correspond tosoftwarechangesthataremorelikelytobedefect inducing sothat the number of defects thatpractitioners would miss to find at committimeisnotlarge.itisalsounclearhowmuchthereduction ofeffort isobtainedthroughthis procedureandhow itwouldaffect the predictive performance of the resulting jit sdp models.
thesethreepointswillbeinvestigatedaspartoftheexperiments to answer rq2.
rq2.2andrq2.
respectively.
anotherpossiblesetupforeco humlacouldbetheopposite wherepractitionerswouldberecommendedtoprioritizeinspectingthose leastconfident predictions so thatthemostinformative training samples for the jit sdp model can be produced during the human labeling process.
this could possibly contribute themosttotheperformanceimprovementofthejit sdpmodel.
we have confirmed this conjecture through experiments which have shown that human labeling the least confident predictions did outperform the proposed eco humla in terms of achieving significantlybetterpredictiveperformanceandconservingmuch morehumaneffort.however theleastconfidentdefect inducing predictions may be more likely to correspond to software changes that are actually clean than the more confident defect inducing predictions.ifwerecommenddeveloperstoprioritizeinspecting and thus labeling these changes they would waste effort in inspecting changes that may be clean and miss several defects by not inspecting the changes that are more likely defect inducing.
therefore while such alternative setup makes sense from a model predictive performance perspective it would be unsuitable as a practical approach to jit sdp.
datasets thispaperuses14githubopensourceprojectsasinpreviouswork to investigate the proposed human labeling methods for jit sdp assummarized in table of thesupplementarymaterial.
twelvemetricshavebeenusedasinputfeaturesfollowingprevious work as explained in section of the supplementary material.
thecommit guru tool was used to collect the data with input features and labels of software changes.
the tool is based on the szz algorithm to decide actual labels of software changes which are defect inducing class or clean class .
as szz is knowntoleadtolabelnoise wehaveconducteda manualinspectionofarandomsampleofchangesofeachproject to investigate its data quality.
four experts with at least years of programming experience have been asked to work in pairs to label these changes.
each pair was asked to discuss each of the softwarechangestodecideontheirlabels leadingtotwosetsof human generatedlabels pair1andpair2 .followingexistingwork humanannotatorswereaskedtolabelsoftwarechanges asdefect fixingornon defect fixing insteadoflabelingsoftware changes as defect inducing or clean directly.
this is because it wouldbeextremelytime consuming ifevenpossibleatall fora softwaredeveloperwhohadnotworkedonagivenprojecttomanually check whether a software change is defect inducing or clean directly on this project based on e.g.
codes and or commit messages.fixesareusedbyszztoidentifydefect inducingsoftware changes.therefore ahighlevelofnoiseintheszzidentificationof changes as defect fixing and non defect fixing means a high level ofnoiseintheszzlabelsofdefect inducingandclean.notethat noise arising from git blame withincommit guru is not included inthismanualinspectionprocessandcouldleadtoadditionalnoise 609esec fse december3 san francisco ca usa liyan song leandro lei minku congteng xinyao table kappascores dataset szz szz average of pair vs pair vs pair2 and vs pair2 brackets .
.
.
.
broadleaf .
.
.
.
camel .
.
.
.
fabric8 .
.
.
.
jgroups .
.
.
.
nova .
.
.
.
tomcat .
.
.
.
corefx .
.
.
.
django .
.
.
.
rails .
.
.
.
rust .
.
.
.
tensorflow .
.
.
.
vscode .
.
.
.
wp calypso .
.
.
.
median .
.
.
.
.asampleof100changes 50defect fixingand50non defectfixing wasdrawnfromeachprojectandsortedinrandomorder to circumvent bias during the human annotation process.
if the commit message did not contain an issue id within it then only thecommitmessageitselfwascheckedtodeterminewhetherthe softwarechangeisdefect fixingornot.otherwise boththecommit messageandtheissuewereused.inparticular ifacommitmessage isaddressinganissueidentifiedbyagivenidcorrespondingtoa bug this changewaslabeledas adefect fixing change.
the kappa scores between szz and pair between szz and pair and between pair and pair are shown in table1.followinghalletal.
weinterpretkappascoreasfollows lessthanchanceagreement slightagreement fair agreement moderate agreement .
.
substantialagreement and almostperfectagreement.
we can see that the kappa scores and indicate at least moderate agreement for all datasets.
in four datasets broadleaf fabric8 tomcat and corefx the average of and indicates a substantial agreement and in other four django tensorflow vscode and wp calypso it indicates almost perfect agreement.
moreover themediankappascoresacrossdatasetsbetweenhuman annotatorsandszz averageof and andbetweenhuman annotators themselves both indicate a substantial agreement showing that the level of agreement between human annotators and szz is in line with the level of agreement between humans themselves.
this suggeststhatthelabels providedbyszz were in generalunlikely to be worse thanthe labels given byhumans.
kappascoresindicatetheleveloflabelnoiseassociatedtocorrectlydistinguishingdefect fixes fromnon defect fixes which are in turn used to identify defect inducing changes by szz.
however if agiven fixhas not yetbeen implemented szz would be unable to link this fix to a defect inducing change even if its ability to distinguish defect fixes from non defect fixes is perfect.
a previousstudy showedthat ifweusethefirst10kchangesofthe projects in our study there is at least an estimated confidence level that the fixes corresponding to these changes have already been reported.
therefore we use the first 10k software changes of eachprojectinthe experiments to increasedata quality.
experimentalsetup toinvestigatetheimpactofthelabelingprocessesthattakeinto accountthehumanlabelingconductedthroughinspectionofdefectpredicted software changes the predictive performance of jit sdp modelswithandwithoutimmediatehumanlabelingwillbecompared.ourlabelingapproachescanbeadoptedwithdifferentmachinelearningalgorithms.inourexperiments oversampling based datastreamingbaggingwith confidence odasc usinghoeffding trees are adopted whenever jit sdp models need to be created updated.thisisarecentonlinelearningalgorithmforjit sdp.
beinganonlinealgorithm itupdatesjit sdpmodelsusingeach training example individually upon arrival and then discards it withouttheneedforretrainingonpastexamples.odascischosen forbeingthestate of the artonlinejit sdpmodel.itdealswith label noise resulting from verification latency by estimating the confidence in the labels assigned to training examples .
it was shown to be more robust to noise than oob which in turn wasshownto be betterthanslidingwindowapproaches .
we use geometric meanof recall0andrecall g mean to evaluate predictive performance of jit sdp models with and without immediate human labeling.
different from accuracy or precision g meanisknowntoberobustagainstclassimbalance which is particularly important for studies suffering from class imbalance evolution such as jit sdp .
we have also adoptedmatthewscorrelationcoefficient mcc asithas become popular in the area of jit sdp .
we use u1d461 u1d45d todenotetruepositives thenumberofdefect inducingsoftware changes that are predicted correctly u1d453 u1d45bto denote false negatives the number of defect inducing software changes that are erroneouslypredictedasclean u1d461 u1d45btodenotetruenegatives thenumberof clean software changes thatare predictedcorrectly and u1d453 u1d45d to denote false positives the number of clean software changes that are erroneously predicted as defect inducing .
based on them g mean radicalbig u1d461 u1d45d u1d461 u1d45d u1d453 u1d45b u1d461 u1d45b u1d461 u1d45b u1d453 u1d45d .
as the false positive rate is definedas1 u1d461 u1d45b u1d461 u1d45b u1d453 u1d45d g meantakesintoaccountthetrade offbetweentruepositivesandfalsepositives largerg meanmeansbetter performance.
mcc u1d461 u1d45d u1d461 u1d45b u1d453 u1d45d u1d453 u1d45b u1d461 u1d45d u1d453 u1d45d u1d461 u1d45d u1d453 u1d45b u1d461 u1d45b u1d453 u1d45d u1d461 u1d45b u1d453 u1d45b takes all elements of the confusion matrix into consideration and thus provides high scores only if the predictions return good rates forall4entriesoftheconfusionmatrix .g meanandmccare computed in a sequential way based on a fading factor to track the changesinpredictiveperformanceovertimeasrecommendedin theonlinelearning scenario .fadingfactor u1d703 controls how much emphasis one would like to place on past evaluation examplescomparedtothenewoneandlarger smallervaluesfor u1d703places more emphasis on the past present model performance status.
the fading factor u1d703 .99is used in this paper following previousjit sdpstudies enablingagoodtrade offbetweenrapidlytracking performancechangesand preventingwild variations.whencomputingtheaveragepredictiveperformance an averageofthe sequentialperformance wasused.
weusedagridsearchtochoosethebestparametersettingfor eachproject.odaschasthreeparameters theensemblesize hoeffdingtrees thedecayfactorofclassimbalance .
.
.
.
andtheresamplingthreshold .
.
.
610a practical human labeling methodforonline just in timeso f tware defectprediction esec fse december3 san francisco ca usa we adopt the parameter settings that can achieve the best average g means across runs based on the first software changes inthedatastreamofeachproject.hoeffdingtreesusethedefault parametersprovidedinthepythonpackage scikit multiflow following previous jit sdpstudies .
to investigate to what extent the proposed eco humla can encourage defects to be found when saving human effort we need to formulate two additional metrics.
the first metric is human recall 1that is the ratio of defect inducing changes u1d466.alt that were predicted as defect inducing hatwide u1d466.alt and that developers were askedtolabel h u1d44b .givenahumanlabelingprocess thismetric can be formulatedas u1d4451h where denotesthenumber ofsoftwarechangessatisfyingthe inside condition s .
we can see that u1d4451his the ratio of defectinducing changes that we asked developers to inspect label.
a highervaluemeansthatdevelopersarebeingaskedtoinspectmore softwarechangesthatarereallydefect inducing potentiallyuncovering more defects.
the second metric is human false alarm that is theratioofcleansoftwarechanges u1d466.alt thatwerepredictedas defect inducingandthatdeveloperswereaskedtolabel.givena human labelingprocess h this metric can be formulatedas u1d439 u1d44e u1d459 u1d460 u1d452 u1d434 u1d459 u1d44e u1d45f u1d45ah .
wecanseethat u1d439 u1d44e u1d459 u1d460 u1d452 u1d434 u1d459 u1d44e u1d45f u1d45ahistheratioofcleanchangesthatwe asked human to inspect label.
a higher value means that more human inspection effortiswasted.
predictiveperformanceofonlinelearningmethodswiththebest parametersettingsisevaluatedbasedontherest500 000softwarechangesoftheproject.comparisonsbetweenjit sdpwithvs without the practical human labeling method are conducted based on the mean predictive performance across runs to account of odasc s stochasticity.
we report the results corresponding to the waiting time of days following previous related studies .
in particular thiswaitingtimewasfoundtoleadtojit sdpmodels with better predictive performance thanothervaluesas they offer a better trade off between one sided label noise and the ability to tackle concept drift .
friedman tests will be performedforstatisticalcomparisonsbetweenmorethantwojit sdp methods across datasets.
the null hypothesis h0 states that all methodsperformsimilar thealternativehypothesisstatesthatthey havestatisticallysignificantdifference.giventherejectionofh0 wewillfurtherperformpairwisecomparisonsusingtheconover posthoctests.two tailedpairwisewilcoxonsignedranktestsat significancelevel0.
willbeperformed wheneverstatistical comparisons between twomethodsacrossdatasets are needed.
to analyze human effort in addition to consideringthe human labelingpercentagementionedinsection .
.
wewillalsoanalyze its corresponding code churn which is a popular metric of human inspectioneffortinthedefectpredictionliterature .
the code churn of a software change is defined as u1d43f u1d434 u1d43f u1d437 where u1d43f u1d434is the number of lines added and u1d43f u1d437is the number of linesdeletedbythe software change.table rq1.
performance comparisons between humla at0 humannoiseand100 humaneffort thedefaultsetup and the waiting time method in terms of g mean and mcc.
datasetg mean mcc waiting time humla imp waiting time humla imp bracket .
.
.
.
.
.
broadleaf .
.
.
.
.
.
camel .
.
.
.
.
.
fabric8 .
.
.
.
.
.
jgroup .
.
.
.
.
.
nova .
.
.
.
.
.
tomcat .
.
.
.
.
.
corefx .
.
.
.
.
.
django .
.
.
.
.
.
rails .
.
.
.
.
.
rust .
.
.
.
.
.
tensorflow .
.
.
.
.
.
vscode .
.
.
.
.
.
wp calypso .
.
.
.
.
.
ave rank .
.
.
.
imp denotes the improvementratio of humla over the waiting time control method.symbols and denoteinsignificant small medium and large a12 respectively.presence absenceofthesign ina12meansthathumlawas worse betterthan the waiting time method.the a12effect size of performance differencesforeachdatasetwastypicallylarge.thelastrowreportstheaverageranks across datasets for the two methods which were found to be statistically significantly differentboth in terms of g mean and mcc.smallerranks arebetterranks.
experimentalresults .
rq1 jit sdpwith humla wecompleteouranswertorq1inthissectionbyinvestigatingthe impact of humla on predictive performance of jit sdp compared tothewaitingtimemethod andwithrespecttodifferenthuman label qualityandlevels of human effort.
.
.
rq1.
predictive performance.
table2shows the performance comparison between humla and the waiting time method.
performance tables using other metrics are reported in the supplementary material for space consideration.
we can see that humla leadstosignificantdifferenceinpredictiveperformanceinterms of both g mean and mcc across datasets.
two tailed pairwise wilcoxon signed rank tests found significant difference with u1d45dvalues0.0208and0.0016intermsofg meanandmcc respectively.
this means that a more practical labeling procedure would lead to significant differencesin performance comparedtothe lesspractical waiting time method.
thus it is important to evaluate jit sdp methodswithalabelingprocedure that iscloser to the reality.
we can also see from table 2that humla usually improves predictiveperformanceofjit sdpcomparedtothewaitingtime methodinmostdatasetsexceptforbracketwithanegativeimprovementratio .
corefxwith .
andtensorflowwith .
intermsofg mean andbracketwith .
corefxwith .
and rust with .
in terms of mcc respectively.
while such negative effects are of a small magnitude the benefit to predictive performanceofjit sdpcanbesubstantial theimprovementratios are .
.
in broadleaf .
.
in rails and .
.
in wp calypso in terms of g mean mcc .
this is in line with thestatistical test which found significant benefitto theperformance.suchimprovementsalsomeanthatexiting studiesmay be underestimating predictive performance of jit sdpmethods.
611esec fse december3 san francisco ca usa liyan song leandro lei minku congteng xinyao a averageranks in g mean.
b averageranks in mcc figure rq1.
performance of humla at different human noise.
bar plots report average ranks of methods across datasets.thewaitingtimemethodischosenasthecontrol method framed in red .
methods significantly better worse than thecontrolmethod are filled inlight orange green .
.
.
rq1.
humla at different human noise.
when labeling software changes predicted as defect inducing through humla humans may mislabel some software changes leading to noisy trainingexamples.differentpersonsandorganizationsmayhave different human annotation error rates.
determining the typical human annotation error rate associated to software engineers is not possible as the ground truth labels are unknown.
in particular neitherhumansnoralgorithmssuchasszzarecurrentlyableto providefullyreliablegroundtruthlabelsforthispurpose.therefore we provide a detailed analysis to investigate theimpactof various amounts of human annotation error rates which we refer to as human noiseinthis section for brevity.
figure2showsaveragefriedmanranksofhumlaintermsof g mean and mcc at different amounts of human noise against the waiting time method across datasets.
plots of the performance overtimeandtablesofoverallperformanceusingseveralmetrics are reported in the supplementary material for space considerations.friedmantestswiththesignificancelevel0.05rejecth0with u1d45d values8.61e 19and2.12e 17intermsofg meanandmcc respectively meaning that different human noise leads to significant difference in predictive performance of jit sdp.
the waiting time method isthen chosenasthecontrol methodto conductconover post hoc tests whose results are alsoillustratedinfigure .
we can see that humla at human noise can significantly improvepredictiveperformanceofjit sdpcomparedtothewaiting time method when the human label noise is no worse than in terms of g mean mcc such beneficial impact produced by humla would be significant.
therefore even when there is humanlabelnoise adoptingalabelingprocedureclosertoreality can have positive impact on predictive performance of jit sdp comparedtothewaitingtimemethod.inotherwords existingwork adopting waiting time may be underestimating the performance thatcanbeachievedinpracticeespeciallywhenadoptingmccasa performancemetric.inaddition humlacanobtainsimilarorbetter rankingthanthewaitingtimemethodsolongasthehumanlabel noiseisnohigherthan80 ing mean mcc .thismeans that producing labels earlier leads to similar or better predictive performance unless the amount of noise in the human labels is extremely high.
therefore delaying the production of labels for changes predictedas defect inducingisnot recommended.
.
.
rq1.
humlaatdifferenthumaneffort.
figure3shows performance comparisons between humla at different amounts of a averageranks acrossdatasets in g mean.
b g mean comparison.
c averageranks acrossdatasets in mcc.
d mcc comparison.
figure3 rq1.
predictiveperformanceofhumlaatdifferentamountsofhumaneffort.barplotsreportaverageranks of each method across datasets.
the waiting time method isequivalenttohumlaat0 humaneffortandischosenas thecontrolmethod framedinred .methodsthatperform significantlybetterthanthecontrolmethodarefilledinlight orange.nomethod was worsethanthe control.
radarplots show performance comparisons of humla at a particular humaneffort against the controlmethod.
human effort in terms of g mean and mcc respectively.
performancetablesusingothermetricsarereportedinthesupplementary material for space consideration.
friedman tests at the significance level0.05runacrossdatasetsrejecth0with u1d45d values1.576e 05and .159e in terms of g mean and mcc respectively.
therefore differentamounts ofhuman effort lead tosignificant differencein predictiveperformanceofjit sdp.giventherejectionofh0 the waiting time method humla at human effort is chosen as the controlmethodtoconductconoverpost hoctests whoseresults are illustratedinfigures a and3 c .
we can see from these figures that larger human effort is in general beneficial to the predictive performance.
in addition when practitioners randomly label defect predicted test examples humlasignificantlyimprovespredictiveperformanceof jit sdp compared to the waiting time method in terms of g mean mcc .
indeed so long as the amount of human effort is no less than60 intermsofg mean mcc humlawouldhavesignificant benefit to predictive performance compared to the control method.
moreover humla achieves similar ranking as the waiting time method when using only of the effort in terms of human labelingpercentage.
asthekeydifferencebetweenhumlaandthe waitingtimemethodistheearlierlabelingoftrainingexamples the positive impact of humla on predictive performance is due to the abilitytoupdatejit sdpmodelsearlier.thisinturnmayenable jit sdptoreacttoconceptdriftfaster evenwhentheamountof immediately labeled data is not large.
figures b and3 d show that humla at human effort can indeed usually attain performanceimprovementcomparedtothecontrolmethodinmost 612a practical human labeling methodforonline just in timeso f tware defectprediction esec fse december3 san francisco ca usa figure rq1.
relationship between the human labeling percentage and the cumulative code churn on random datasets for humla at varying amounts of human effort.
results of other datasets showed the same pattern and were omitted forspacereasons.
datasets whereasfor thedatasets wherethere isno improvement the magnitude of the differences in performance is very small .
forbracket .
forcorefxand .
fortensorflowin terms ofg mean and .
for corefxinterms ofmcc .
figure4shows the relationship between cumulative code churn and the human labeling percentage on random datasets.
plots of other datasets showed the same pattern and were reported in the supplementary material for space restrictions.
we can see that higherhumanlabelingpercentagealmostalwaysaccountsforlarger value of the cumulative code churn showing a good correlation between these two metrics.
therefore given a project for which practitioners may be interested in creating a jit sdp model reducingtheinspectionrateisreallylikelytocorrelatewithadecrease inchurnforthisproject.table 3containsthecodechurnvaluesfor all datasets.
we can see that for example humla at and human effort would reduce around up and code churns respectively being consistent to the human labelingpercentages.
these results are particularly relevant and of practical significanceastheyindicatethatwhenpractitionersallowforarelatively low cost of human effort in randomly labeling a small portion e.g.
of defect predicted test examples humla already begins to have beneficial impact when humla allows for a moderately large human effort such as it would perform significant better performance comparedto the waiting time method.
answertorq1 humlawouldnotonlymodelthepracticaljitsdp process tobe closer totherealitybutalsoleadstosignificant differenceinpredictiveperformancecomparedtothelesspractical waiting time method.
even when there is human label noise adopting humla can usually have positive impact on predictive performance indicating that existing work adopting the waiting time method is likely underestimating the predictive performance thatcanbeachievedinpracticeespeciallyintermsofmcc.humla allowingforhigheramountsofhumaneffortgenerallyattainbetter performance and when the allowance for human effort is beyond a moderately large value such as humla would produce significantlybetterperformance comparedto the waiting time method.
.
rq2 jit sdpwith eco humla thissectioncompletesouranswertorq2toevaluatetheproposed eco humlawithrespecttohowwellitcansavehumaneffortwhile maintainingpredictiveperformanceandavoidingalargenumber of defect inducing software changes to be missed by practitioners.
a averageranks acrossdatasets in g mean.
b g mean comparison.
c averageranks acrossdatasets in mcc.
d mcc comparison.
figure rq2.
performance comparisons between ecohumlavs humlaatdifferent amountsof humaneffort.bar plots report average ranks of each method across datasets.
humlaat100 humaneffortischosenasthecontrolmethod framedinred andmethodsthatperformsignificantlyinferior to the control method are filled in light green.
ecohumla is framed in orange to facilitate visualization.
radar plots showperformance comparisonsbetween eco humla andhumla at humaneffort foreachdataset.
.
.
rq2.
retained performance.
figure5shows performance comparisonsbetweeneco humlaandhumlaatdifferentamounts of human effort in terms of g mean and mcc.
performance values usingseveralmetricsarereportedinthesupplementarymaterial for space consideration.
friedman tests at significance level .
reject h0with u1d45d values .65e 05and .10e 10in terms of g mean andmcc respectively meaningthateco humlaandhumlawith different amounts of effort achieve significantly different performance.
given the rejection of h0 humla at human effort is chosen as the control method to conduct conover post hoc test whose results are illustratedinfigures a and5 c .
we can see from these figures that statistical tests found no significant difference between the performance of eco humla and humla at human effort across datasets in terms of both g mean and mcc.
this indicates the capability of eco humla inretaining predictiveperformancecomparedtohumlaat100 humaneffort despitelabelinglesssoftwarechanges.wecanalso seethat eco humla sranking isinbetweenhumlaat40 human effort in terms of both g mean and mcc.
further post hoc comparisonusingeco humla asthecontrolmethod cannotfind significantdifferencebetweenthesethreemethods indicatingthem to have similar performance when comparedacrossdatasets.
figures5 b and5 d show that eco humla s performance is usually below that achieved by humla at human effort in mostdatasetsasonewouldexpect buttheinferiorityratioisusually of small magnitude in terms of g mean the inferiority ratio is 613esec fse december3 san francisco ca usa liyan song leandro lei minku congteng xinyao table rq2.
savedhumaneffortof eco humla against humlaatdifferenthumaneffortintermsofhumanlabeling percentageandcumulative codechurn in kilo .
datasethuman effort in kilo ofhumla eco humla human effort in kilo ofhumla autohuman bracket .
.
.
.
.
.
.
.
.
.
.
.
broadleaf .
.
.
.
.
.
.
.
.
.
.
.
camel .
.
.
.
.
.
.
.
.
.
.
.
fabric8 .
.
.
.
.
.
.
.
.
.
.
.
jgroup .
.
.
.
.
.
.
.
.
.
.
.
nova .
.
.
.
.
.
.
.
.
.
.
.
tomcat .
.
.
.
.
.
.
.
.
.
.
.
corefx .
.
.
.
.
.
.
.
.
.
.
.
django .
.
.
.
.
.
.
.
.
.
.
.
rails .
.
.
.
.
.
.
.
.
.
.
.
rust .
.
.
.
.
.
.
.
.
.
.
.
tensorflow .
.
.
.
.
.
.
.
.
.
.
.
vscode .
.
.
.
.
.
.
.
.
.
.
.
wp calypso .
.
.
.
.
.
.
.
.
.
.
.
median .
.
.
.
.
.
.
.
.
.
.
.
forhumla at humaneffort the a12effect size of differences in saved human effort for eachdatasetwas always large than eco humla.
effect sizes for otherlevelsof human effort arein the supplementarymaterial.
below3.
inbroadleafandwp calypso intermsofmcc most of the inferiority ratios are around below except for .
in jgroup and .
in rails.
this is in line with the results of the abovementionedconovertests whichfoundnodifferencebetween eco humla andhumla at human effortacrossdatasets.
animportantquestionisthenhowmuchefforteco humlacan savecomparedtoothermethodsthatobtainedsimilarperformance.
this isinvestigatedinsection .
.
.
.
.
rq2.
saved human effort.
table3shows that the median humaneffortofeco humlaacrossdatasetsis372.1kintermsof the cumulative code churn.
compared to the median cumulative code churn766.2k ofhumlaat human effort eco humla cangenerallyconservearound50 humaneffortincheckingthe linesofcodebeingchanged demonstratingtheeffectivenessofecohumla in saving human effort.
we can also see that the median humanlabelingpercentageofeco humlaacrossdatasetsis51.
in betweenhumlaat50 humaneffort alsoshowingthat eco humla saves around halfofhuman effort.
combining with the observation in section .
.
we can conclude that eco humla achieved similar performance of humla at human effort while requiring only around half of the inspectioneffortintermsofhumanlabelingpercentageandcodechurn.
therefore we would recommend practitioners to inspect and label alldefect predictedsoftwarechangeswhenthehumancostisallowable astheinspection used tolabel defect predicted examples isthesameastheinspectionneededtofindandfixanydefectsthat thechangemayinduce.therefore inspectingthesechangesmay helppractitionerstofindmore defectsatanearlystage.however ifthecostofinspectingalldefect predictedsoftwarechangesistoo high inspecting only around half of the defect predicted software changesthrougheco humlawillnotleadtoworsepredictiveperformanceoftheresultingjit sdpmodels.inthissense itwould be acceptable to reduce labelingeffortthrougheco humla.
.
.
rq2.
finding defects.
based on the previous sections eco humla requires similar effortand leads tosimilar predictive performanceashumlaat50 humaneffort.however eco humla was designed to direct practitioners inspection effort towards software changes that are more likely to contain defects so that thetable4 rq2.
recall1andfalsealarmforhumansbetween eco humla vs humla at human effort that perform similarly to eco humla at the cost of similar amount of humaneffort as foundinrq2.
.
datasetrecall 1forhumans falsealarm forhumans eco humla humla eco humla humla bracket .
.
.
.
broadleaf .
.
.
.
camel .
.
.
.
fabric8 .
.
.
.
jgroup .
.
.
.
nova .
.
.
.
tomcat .
.
.
.
corefx .
.
.
.
django .
.
.
.
rails .
.
.
.
rust .
.
.
.
tensorflow .
.
.
.
vscode .
.
.
.
wp calypso .
.
.
.
ave rank .
.
.
.
symbols and denoteinsignificant small medium andlargea12 respectively.
presence absenceof the sign in a12means that humla at human effort was worse betterthan eco humla.
effect size of differences in performance for eachdatasetweretypicallylarge.
the lastrowreports the averageranks across datasets for the twomethods whichwerefoundto bestatistically significantly differentboth in terms of recall1 and false alarms.smallerranks arebetterranks.
reductionineffortdoesnotcomeatthecostofpractitionersmissing atoolargenumberofdefectsinthecode.so aquestionremains onwhethereco humlacanreally encouragealargernumberof defectsto be foundthanhumla at humaneffort rq2.
.
table4reportsthehumanrecall1thatisformulatedineq.
and the human false alarm that is formulated in eq.
between eco humlaandhumlaat50 humaneffort.wecanseethatin terms of the human recall practitioners can usually detect more defects with the improvement ratio of up to .
for django andaround26 fornova corefx andvscode insoftwarechanges when they opt for eco humla compared to humla at human effort and such superiority is statistically significant accordingto two tailedpairwisewilcoxonsignedranktestsatsignificancelevel .
u1d45d value .03e .
in the meantime practitioners can have lower better humanfalsealarm withtheimprovementratioof around for broadleaf jgroup rails and rust compared to humla at human effort and such superiority is significant according to two tailed pairwise wilcoxon signed rank tests at significance level .
u1d45d value .
.
such results are of practical significance as it means that although eco humla can get similar predictive performance at similar human cost compared to humla at human effort this targeted human labeling approach can helppractitionersinspectchangesthataremorelikelytrulydefectinducing encouragingthemto find defectsat an early stage.
answer to rq2 the proposed eco humla can save around humaneffortinterms ofboth thehumanlabelingpercentage and thecumulativecodechurnwhilestillretainingpredictiveperformanceofjit sdpcomparedtohumlaat100 humaneffort.moreover whenadoptingeco humla practitionerswouldbeencouraged to find more detects and achieve a lower false alarm rate than whenrandomlydecidingwhichchangestoinspectbasedonhumla at humaneffort.
614a practical human labeling methodforonline just in timeso f tware defectprediction esec fse december3 san francisco ca usa threats to validity internal validity.
one potential issue for humla is that developers maytakeawhiletodecideactuallabelsofdefect predictedexamples resulting in verification latency of the immediate human labeling procedure.
however such time delay would be negligiblecomparedtothatinthewaitingtimelabelingprocedure.also as such human delay is disregarded the humla labeling procedurewillnotproducetrainingexamplesexactlyinchronological order.
nevertheless it still leads to a more realistic labeling and modeltrainingproceduresthantheonesadoptedinexistingliteraturewhenthetruetimetakentolabeldefect inducingsoftware changes is unknown.
future work could further investigate the lengthandimpactofsuchtimedelay.similartopreviousworkthat has adoptedthe same datasets otherthreatsto internal validityincludevarioustypesofnoisearisingfromszz including noise stemming from the git blame command used incommit guru thatwerenotcapturedbyourmanualkappaanalysis in section .
the data were collected based on the original szzalgorithm whichwasshowntoleadtojit sdpmodelsof similarpredictiveperformancetomodelscreatedbasedonthemost recent szz algorithm which applies the largest number of noise filters in comparison to several other szz variants .
wealsoadoptedthewaitingtimethatwasshowntoleadtoagood trade offbetweenlabelnoiseandtheobsolescenceofthetrained models .tomitigatethreatsrelatedtotherandomnessof odasc our results are basedon100runsoneachdataset.
construct validity.
g mean recall and recall are unbiased metrics suitable for class imbalanced problems such as jit sdp.
we have also adopted mcc a popular metric in the sdp literature which uses all entries of the confusion matrix.
a fading factor is adopted to enable tracking the fluctuations in predictive performance over time as recommendedfor onlinelearning .
externalvalidity.
we haveinvestigated14open sourceprojects coveringawiderangeofdatacharacteristicsasexplainedinsection4.
however as with any study involving machine learning experimental results may not generalize well to other projects.
we investigate the proposed methods based on odasc with hoeffding trees whichhavebeenpreviouslyoptedforonlinejit sdp .
other online machine learning models could lead to different results.however itisworthnotingthatmachinelearningapproaches are in general expected to struggle more to adapt to a concept drift if there is no labeled data coming from the new underlying data distribution than if they had access to such labeled data.
our labeling procedures can thus act as enablers for learning approaches to adapttoconceptdriftmorepromptly possiblybenefitingpredictive performance of other learning algorithms.
when investigating the impactofhumaneffortwithin eco humla zerohumannoisewas assumed facilitatingafocusedanalysisofhumaneffortwithoutbeing affected by human noise.
in practice a non zero and unknown levelofhumannoisewouldbeassociatedwith eco humla which could leadtodifferentconclusions.following standardpracticein thejit sdpliterature wehaveusedthemainbranchofopensource repositories to collect software changes and their labels.
therefore only software changes that have been accepted in the main branch possibly after code review have been used in our evaluation.
the results maynot generalize to predictingrejectedchanges.
conclusions we have conducted the first study on how to consider the effect of adopting jit sdp during the software development process in the labeling procedure of software changes for online jit sdp leading tothehumla procedure.theimpactofhumlaon thepredictive performanceofjit sdpwasinvestigatedatdifferentlevelsofnoise and human effort.
we have also proposed eco humla to save human effort by targeting the inspection process towards software changes predictedas defect inducingwithhigher confidence.
our experiments showed that adopting a labeling procedure closer to reality leads to a significant impact on the predictive performanceofjit sdp withgenerallybetterperformancethanthe delayedlabelingmethodwithwaitingtimeevenwhenhumanlabelingcontainedacertainlevelofnoise.asanimplicationtoresearch this shows the importance of adopting labeling methods such as humla that are closer to what would be adopted in practice when conductingstudies to evaluate jit sdp models.
as an implication to practice it shows the importance of not delaying the inspection of software changes toachieve better performing jit sdpmodels.
we also showed that it is possible to save around of inspection effort through eco humla while maintaining predictive performancecomparedtohumlaat100 humaneffortandencouragingalargernumberofdefectstobefoundthanwhensavingeffort throughhumlaat50 humaneffort.asanimplicationtoresearch this shows thateffort aware strategies can be designed to work in anonlinemanner encouragingfurtherresearchononlineeffortawarejit sdp.researchersmayalsoanalysetheperformanceof theirmodelsunderdifferentlabeling effortsthrougheco humla.
asanimplicationtopractice theseresultsshowthatifpractitioners are unable to inspectall defect predicted software changes due to the inspection effort we recommend to target the inspection effort based on the confidence of the predictions through eco humla ratherthanrandomlydecidingwhichchangestoinspect.iftheycan affordthehighereffortoraredealingwithsafety criticalsystems itisstillrecommendedto use humla to find more defects.
futureworkincludesinvestigatinghumlaandeco humlawith other jit sdp models datasets and input features such as highlevel latent representations of a deep neural network proposing noveleffort awareonlinejit sdpapproachestofurtherimprove oneco humla analyzingtheverificationlatencyofthehumaninspection process investigating eco humla with jit sdp models forpredictingpre codereviewchanges investigatingtheimpact ofgit blame usedby szzin thelabel qualityoftheinvestigated datasets andinvestigatingtheimpactofvariouslevelsofhuman effortundervariouslevels of human noisefor eco humla.
data availabilitystatement a replication package is available in .
thesource code is availableunderagnugpl v3.
license.