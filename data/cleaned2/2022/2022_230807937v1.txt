automated testing and improvement of named entity recognition systems boxi yu boxiyu link.cuhk.edu.cn school of data science the chinese university of hong kong shenzhen cuhk shenzhen chinayiyan hu yiyanhu link.cuhk.edu.cn school of science and engineering the chinese university of hong kong shenzhen cuhk shenzhen chinaqiuyang mang qiuyangmang link.cuhk.edu.cn school of data science the chinese university of hong kong shenzhen cuhk shenzhen china wenhan hu wenhanhu link.cuhk.edu.cn school of data science the chinese university of hong kong shenzhen cuhk shenzhen chinapinjia he hepinjia cuhk.edu.cn school of data science the chinese university of hong kong shenzhen cuhk shenzhen china abstract named entity recognition ner systems have seen rapid progress in recent years due to the development of deep neural networks.
these systems are widely used in various natural language processing applications such as information extraction question answering and sentiment analysis.
however the complexity and intractability of deep neural networks can make ner systems unreliable in certain circumstances resulting in incorrect predictions.
for example ner systems may misidentify female names as chemicals or fail to recognize the names of minority groups leading to user dissatisfaction.
to tackle this problem we introduce tin a novel widely applicable approach for automatically testing and repairing various ner systems.
the key idea for automated testing is that the ner predictions of the same named entities under similar contexts should be identical.
the core idea for automated repairing is that similar named entities should have the same ner prediction under the same context.
we use tin to test two sota ner models and two commercial ner apis i.e.
azure ner and aws ner.
we manually verify of the suspicious issues reported by tin and find that are erroneous issues leading to high precision .
.
across four categories of ner errors omission over labeling incorrect category and range error.
for automated repairing tin achieves a high error reduction rate .
.
over the four systems under test which successfully repairs out of the reported ner errors.
keywords metamorphic testing named entity recognition software repairing ai software introduction named entity recognition ner systems have become popular in recent years and are widely used to enhance natural language processing nlp applications such as information retrieval machine pinjia he is the corresponding author.translation and text classification.
with the development of the internet mountains of data accumulate every day.
common crawl a nonprofit organization that crawls the web and freely provides its archives and datasets to the public has collected tb of data and .
billion pages by october .
as of december there are articles in the english wikipedia and it contains over billion words .
being able to identify the semantics of interest in unstructured texts ner systems play a significant role in several downstream applications.
for instance chatbots like chatgpt and google s bard employ ner to identify and categorize entities in user queries enhancing response accuracy while in finance ner algorithms sift through reports and online mentions extracting and classifying data for in depth profitability analysis and real time stock market trend monitoring.2news providers such as publishing houses can harness ner to sift through and categorize their abundant daily content.
by detecting crucial entities like people organizations and places in articles they can seamlessly tag and organize them optimizing content discovery for readers.
though achieving shining f1 score on multiple ner benchmarks e.g.
conll03 and ontonotesv5 current ner systems are far from perfect and errors produced by these systems largely dissatisfy the users and could lead to detrimental influence.
in ner systems errors pertain to either the incorrect delineation of named entities within a text corpus or the misclassification of such identified entities into inappropriate categories.
in table we present multiple instances of ner errors encompassing four categories omission over labeling incorrect category and range error.
for example in the first sentence the name paul is clearly identifiable as a person.
however the ner system fails to recognize any word as a named entity leading to an omission error.
the inaccuracies stemming from ner errors can reverberate across a broad spectrum of domains.
for example in the report by mishra et al.
some ner models are better at identifying white names across all datasets with higher confidence compared with other aug 2023esec fse november san francisco usa boxi yu yiyan hu qiuyang mang wenhan hu and pinjia he demographics such as black names.
in addition zhao et al.
found that some ner systems are prone to identifying female names as chemicals and most ner systems perform better on male related data than female related data.
although the reliability of ner systems is of great importance there is a dearth of general methods for automatically testing and repairing various ner systems since it is quite challenging.
first unlike traditional code based software ner systems are mainly based on deep neural networks with millions of parameters.
therefore many testing techniques that perform well on code based software may not be suitable for testing ner systems.
second it is laborious to manually construct the test oracle i.e.
large amounts of text data with labels indicating the presence of named entities such as people and organizations.
third ner systems have multiple standards for identifying and categorizing named entities in text which further compounds the difficulty of designing a general method for testing and repairing various ner systems.
traditional approaches for repairing ai based systems either adopt data augmentation or algorithm optimization.
thus these approaches need manual labeling of the data or modification of the model architecture which often incurs a high cost.
in addition these traditional methods need to access the model in a white box manner while a black box testing and repair approach would be much more general.
to address the challenges we propose tin the first approach for automatically testing and repairing ner systems which is applicable to ner systems with various standards.
we design three transformation schemes for test case generation including similar sentence generation structural transformation and random entity shuffle.
meanwhile we design the corresponding metamorphic relations for each of the transformation schemes.
then we construct the test inputs consisting of the original sentence the mutant sentence and their ner predictions.
the test input is reported as a suspicious issue if it does not satisfy the corresponding metamorphic relation.
after receiving the suspicious issues from the automated testing our location algorithm would be used to locate the suspicious entity that is prone to ner errors.
for each suspicious entity we use bert to generate similar named entities and a novel repairing algorithm to predict the correct ner category of the suspicious entity by leveraging the prediction of similar entities under the same context.
we apply tin to test two sota ner models from flair a widely used nlp library and two commercial apis from azure and aws.
the two sota models flair conll and flair ontonotes are trained with the ner standard of conll03 and ontonotesv5 respectively.
for the commercial apis we denote azure cognitive services for language as azure ner and aws sagemaker as aws ner for simplicity.
to verify the effectiveness of tin we manually inspect part of the results for testing and repairing.
tin successfully reports erroneous issues out of suspicious issues with a high precision ranging from .
to .
on the four ner systems under test.
the detected ner errors in the erroneous issues include omission mislabeling incorrect category and range error.
in terms of ner repairing tin improves predictions with a high rate ranging from .
.
to .
while only downgrading predictions in only a low ratio from .
to .
.
tin also exhibits a high error reduction rate .
.
on the four nersystems demonstrating its effectiveness in repairing ner systems and reducing ner errors.
all the source code and data in this work will be released for reuse.
this paper makes the following main contributions it introduces tin a novel widely applicable approach for automatically testing and repairing ner systems.
tin provides three transformation schemes for test case generation and the corresponding metamorphic relations for ner error detection.
tin contains a novel repairing algorithm that can effectively fix the reported ner errors.
it presents the evaluation of tin against sota models and commercial apis achieving a high precision of .
.
for reporting erroneous issues and a high error reduction rate of .
.
for repairing the ner errors.
preliminaries .
named entity recognition ner systems are essential tools in nlp because they allow machines to extract structured information from unstructured text.
ner systems are used in a variety of applications including information extraction machine translation and question answering.
for example a ner system might be used to extract the names of people organizations and locations from a news article.
this information can then be used to build a database of entities and their relationships which can be queried and analyzed to gain insights or to perform tasks such as information retrieval or summarization.
overall ner systems are significant because they enable machines to understand and make sense of large volumes of unstructured text which is an increasingly important task in today s digital world.
ner task has multiple standards e.g.
the ner dataset conll03 and ontonotesv5 have different ner standards.
in addition the four ner systems have different ner standards which only share the common categories of person per location loc andorganization org in table .
the various identification standards further exacerbate the difficulty of manual test oracle construction.
.
constituency parser a constituency parser is a natural language processing tool that is used to analyze and understand the syntactic structure of sentences.
it works by breaking down a sentence into its constituent parts known as constituents and arranging them in a tree like structure known as a phrase structure tree or a constituency tree.
the tree represents the syntactic relationships between the words in a sentence and the grammatical roles they play.
in the process of analyzing sentences and producing phrase structure trees constituency parsers use context free grammars cfgs .
cfgs are a set of rules that specify how words can be combined to form phrases and sentences.
the parser uses these rules to identify the constituents of a sentence and the relationships between them.
in tin we adopt the constituency parser implemented in stanford corenlp to parse the sentences.
in the constituency parser a non terminal node is a node in the phrase structure tree that represents a syntactic category such as a noun phrase np verb phrase vp and sentence s .
non terminal nodes are used to represent theautomated testing and improvement of named entity recognition systems esec fse november san francisco usa table example of the ner errors error type sentence predicted entities target entities omission sir paul s command of the stage is so casual that he makes it look easy flair ontonotes .
null over labeling elon musk is having a similar effect on the platform azure .
incorrect categorynorrie believes securing unesco status could offer new opportunities in sustainable tourism and branding of local produce while at the same time highlighting the environmental value of the peatland flair conll .
range error det supt rance said the investigation remained active aws .
table ner systems with different standards ner systems ner categoreis flair conll person organization location miscellaneous names flair ontonotes person organization location cardinal date event fac gpe language law money norp ordinal percent product quantity time work of art azure ner person organization location persontype event product skill address phonenumber email url ip datetime quantity aws ner person organization location commercial item date event other quantity title internal structure of the sentence and the relationships between its constituents.
a terminal node is a leaf node in the phrase structure tree that represents an individual word or punctuation mark in the sentence.
terminal nodes are used to represent the basic building blocks of the sentence and they do not have any children.
.
problem definition .
.
ner testing.
we denote the ner systems under test as n and the input sentence as s. tin adopts metamorphic testing to alleviate the test oracle problem.
n s represents the output of the ner systems which contains a list of ner predictions with the corresponding ner categories e.g.
person ororganization .
tin transforms the original sentence sto obtain the mutant sentence s through transformation s t s by utilizing the structure of the sentence sand the ner output n s .
we then design the corresponding metamorphic relation mr for the ner output pair n s n s and report the n s n s along with s s as a suspicious issue if the metamorphic relation is unsatisfied.
.
.
ner repairing.
ner repairing aims to repair the ner errors and gives the correct ner prediction.
we design a repairing system rto repair the ner predictions of the suspicious issues reported by the testing part of tin.
we repair both the ner prediction of the original sentence and the mutant sentence and obtain the repaired ner predictions r s andr s .
approach and implementation .
overview the overview framework of tin is shown in fig which consists of three main components which are test case generation detecting suspicious issues and automated repairing respectively.
the first part of tin is test case generation.
tin first feeds the original sentences into the ner systems and gets the ner prediction of the original sentences which are used to assist the automated generation of test cases.
after generating multiple mutant candidates we use sentence filters to filter out low quality sentences.
the filtered mutants are then used for automated testing.
the second part of tin is detecting suspicious issues.
we construct the test inputs which consist of the sentence and the nerpredictions of the original and mutant sentences.
we design the metamorphic relation for each of the transformation schemes and report the suspicious issues that may contain ner errors if the metamorphic relations are violated.
the third part of tin is automated repairing.
after we obtain the suspicious issues from the second part of tin we start to locate the suspicious entities that may lead to the ner errors.
we use bert to generate mutant named entities with similar semantics to replace the suspicious entities.
we then filter them through entity filtering to obtain the filtered mutants.
we feed the filtered mutants to the ner systems and obtain a list of ner predictions for repairing the suspicious entities.
in practice we develop a scoring system to leverage the entity predictions to repair the suspicious entities.
.
automatically generating test cases to automatically generate test cases we design three transformation schemes to generate mutant sentences from the original sentence.
the transformation schemes consist of similar sentence generation structural transformation and random entity shuffle.
we will introduce each of these transformation schemes and their implementations in the following.
.
.
similar sentence generation.
the core idea of generating similar sentences is to substitute the words or phrases in the sentences with the ones that have similar semantics.
in this procedure we avoid modifying named entities of the original sentences predicted by the ner systems.
we implement two methods for similar sentence generation including token level and phrase level similar sentence generation which are explained as below.
token level similar sentence generation after we obtain the ner prediction of the original sentence n s we mask the token that does not belong to a named entity in turn one word being masked each time and feed the masked sentence into bert to generate a list of candidate words.
we select the top k most predictive candidate words and use them to replace the original ones and generate mutant sentences.
during this process we only use verbs and adjectives as candidate tokens to avoid grammatically strange or incorrect sentences.
in addition we ensure the token toesec fse november san francisco usa boxi yu yiyan hu qiuyang mang wenhan hu and pinjia he original sentencener predictionsnosatisfied?
sentencemutationner systems mutant candidatessentencefilteringsuspiciouslocationner systems fixedpredictionentityfiltering filteredmutantsner systemsner predictionsnewpredictionoriginalpredictionsuspicious issuesmutant candidatessuspiciousentitiesfilteredmutants originalsentencemutantsentencenewpredictionoriginalpredictiontest inputsoriginalsentencemutantsentencemrcheckingner predictionsrepairingsystem test case generationdetecting suspicious issuesautomated repairing overall sentence transformationentitymutation figure overview of tin but nhs england says emergency care will continue to be provided.s cc np vp .
vbz sbar s np vpbut nhs england says emergency care will continue be provided.
np medical care insert delete figure phrase level similar sentence generation be replaced keeps the identical part of speech by using the nltk s pos tagging tool .
phrase level similar sentence generation besides tokenlevel replacement we design a phrase level sentence transformation scheme by utilizing the constituency parser.
specifically we only modify the noun phrase to avoid grammar errors.
as shown in fig.
we use a constituency parser to parse the original sentence and find np nodes where there are no other np nodes in their subtree i.e.
the np nodes with the smallest unit.
then we use sense2vec to generate similar noun phrases to replace the content in the np nodes we find in turn one np node being replaced each time .
as shown in fig.
the np node with emergency care has been replaced by the np node with medical care .
finally we obtain a list of mutant sentences each with one noun phrase replaced.
.
.
structural transformation.
the general idea of structural transformation is changing the syntactic structure of the original sentence as well as ensuring the new sentences have a similar context to the original sentence.
specifically we provide three transformation implementations of sentence syntactic structure to transform the declarative sentence into the interrogative sentence.
we first use a constituency parser to parse the original sentence and obtain a parse tree.
then we find the sentence node s that satisfies the cfgs grammar of s np vp and check the first child vp node of s that modifies the np node.
there are different ways to handle the first child vb node of the vp node and we use a specific transformation method for each case.
if the vb node is a be verb that serves as a normal verb and directly modifies a noun phrase e.g.
the sentence he is a student.
we will transform the sentence structure from subject verb obj ect to verb subject object and get a sentence is he a student?
.
please notify we use here to represent that the object is not necessary.
as shown in fig.
a we will move the vb node to the position of the first child of node s. as a result the declarative sentence twitter was the obvious solution is transformed into the interrogative form was twitter the obvious solution .
if the vb node is a normal verb that is not a be verb e.g.
the sentence with the transitive verb i eat a burger or the sentence with the intransitive verb he cried we will transform the sentence structure from subject verb object to aux subject ver b object .
as shown in fig.
b we will insert a vb node with an auxiliary verb at the position of the first child of node s. if the vb node is an auxiliary verb e.g.
the sentence he has faced floods we will transform the sentence structure from subject aux verb object to aux subject verb objautomated testing and improvement of named entity recognition systems esec fse november san francisco usa s np vp .
nptwitter was the obvious solution?vbsubject object verb s np vp .
vb nptwitter was the obvious solution.subject object verb twitter was the obvious solution.
was twitter the obvious solution?a s np vp .
vb npbelarus shares a border with russia as well as ukraine.subject object verb belarus shares a border with russia as well as ukraine .bnp vp .
vb npbelarus share a border with russia as well as ukrainesubject object verb ?vb does belarus share a border with russia as well as ukraine ?aux s s np vp .
vb vptravellers have.subject object verb c vb np faced hogmanay disruption on scotland s railways after friday s floods travellers have faced hogmanay disruption on scotland s railways after friday s floods.s np vp .
vb vptravellers vb np faced hogmanay disruption on scotland s railways after friday s floods have travellers faced hogmanay disruption on scotland s railways after friday s floods?have ?does aux aux subject verb object figure structural transformation ect .
as shown in fig.
c we will move the vb node to the position of the first child of node s. after the movement or insertion we replace the .
at the end of the sentence with ?
.
.
.
random entity shuffle.
we propose to use random entity shuffle to generate mutant sentences and test whether the ner predictions keep the same.
we first use the ner systems to obtain the ner predictions and replace the original named entities with the placeholders of their own type e.g.
org and per .
then we randomly shuffle the order of the named entities of the same category within the sentence and place the named entities into the corresponding placeholders to generate mutant sentences.
for example the sentence spotify apple music and deezer all said the track was their top performer of the year beating competition from ed sheeran drake and taylor swift.
will be modified to org org and org all said the track was their top performer of the year beating competition from per per and per .
after we random shuffled and filled named entities for each ner category respectively mutant sentences such as apple music spotify and deezer all said the track was their top performer of the year beating competition from ed sheeran taylor swift and drake.
will be generated.
.
filters to improve the quality of test cases to improve the quality of the test cases generated by tin we adopt a semantic filter to ensure the semantic similarity between the original sentence and mutant sentence and a syntactic filter to filter out the sentences that are prone to grammar errors.
.
.
semantic filter.
in automated testing we have two transformation methods for similar sentence generation including tokenlevel and phrase level substitution.
as the same named entity may have different ner categories under various contexts we need toensure that these two transformation methods only change the semantics of the sentence subtly.
in practice we use bert to obtain the context aware embedding of the word or phrase and compute the cosine similarity between the mutant and the original one.
to avoid ambiguity we denote bert mlm as the function we use to generate words by using the bert masked language model and bert embedding as the function that we use bert to compute the context aware embedding.
the context aware embedding hof the wordwin a sentence sis denoted as h bert embedding w s .
for a phrase that contains multiple words we use the average of the context aware embedding vectors in the phrase as its contextaware embedding.
given a context aware embedding vector hand another context aware embedding vector h the semantic similarity is defined as cossim h h ht h h h which is the cosine similarity between handh .
the semantic filter will filter out the mutant sentences where the semantic similarity cossim is less than a threshold sthreshold .
specifically we set sthreshold as .
for token level replacement and phrase level replacement based on experience which achieves good performance on four ner systems under test.
.
.
syntactic filter.
when we transform the original sentence to generate mutant sentences there may include grammar errors punctuation errors or rarely used expressions.
after the transformation of the sentences it is significant to filter out these test cases which rarely appear in the real world.
to this end we use the syntactic evaluator syneval implemented by aeon to compute the syntactic score of our mutant sentences which evaluates the naturalness through the perplexity of the pre trained language model.
specifically we calculate the difference in the syntactic score between the original sentence sand the mutant sentence s which is defined as syneval s syneval s .
we adopt the syntactic filter for all the transformation approaches and use a threshold synthreshold to filter out the low quality mutant sentences with greater than synthreshold .
we use grid search to find the best value between and .
balancing quantity and quality of test cases.
for flair conll which only contains four ner categories we set as .
for structural transformation and .
for other sentence transformation.
for the other three ner systems which have much more ner categories we set as for all transformation methods to achieve stricter filtering.
.
detecting suspicious issues after we generate the mutant sentences and use the filters to select the test cases with high quality we begin to construct the test inputs for tin.
the test inputs contain the original and mutant sentences and their predictions by the ner systems.
as shown in fig.
we examine the metamorphic relations of the test inputs.
we check if the first metamorphic relation mr1 is met for generating similar sentences.mr1requires n s e n s e e s.t.
e sande s esec fse november san francisco usa boxi yu yiyan hu qiuyang mang wenhan hu and pinjia he named entityprediction 1prediction 2appear in both sentencesuspicious entity or notusloclocyesnoamericannullmiscnonobbc news russianorgmiscyesyessentence some of those supporting theus loc had posed asindependent mediaoutlets and some had tried to pass off content from legitimate outlets such asbbc news russian org as their own.sentence some of those supporting theus loc had posed asamerican misc mediaoutlets and some had tried to pass off contentfrom legitimate outlets such asbbc news russian misc as their own.
figure suspicious entities location wheren s e andn s e represents the ner prediction of ein the original sentence sand the mutant sentence s .
in this case a token or phrase is changed in the mutant sentence s which may result in a new named entity that does not exist in the original sentence.
thus we would not check the named entities that do not exist in both sentences since the ner predictions of these named entities are not defined in both sentences.
however structural transformation and random entity shuffle do not involve new named entities which only change the order of the named entities or the syntactic structure of the sentence.
therefore we check whether the test inputs satisfy the second metamorphic relationmr2 defined as below n s n s .
if a test input does not satisfy the metamorphic relations tin will report the test input as a suspicious issue.
.
automatically repairing the named entity recognition errors after a suspicious issue is raised we will repair both sentences sands to obtain fixed ner predictions r s andr s .
for the original sentence s ifn s r s we will report a repairing attempt replacing n s withr s and for the mutant sentence s if n s r s we will report a repairing attempt replacing n s withr s .
the process of black boxed repairing consists of the following two parts i.e.
suspicious entity location and relabeling.
.
.
suspicious entity location.
a suspicious issue consists of two sentences and their ner predictions.
however we do not know which named entity contains the ner errors and needs to be repaired.
to ensure the quality of the fixed ner prediction we need to locate the suspicious entities that are prone to ner errors.
we compare the contents and the ner predictions between the original sentence sand the mutant sentence s to locate suspicious entities.
for each sentence an entity eis a suspicious entity if both of the following conditions are satisfied.
the entity exists in both sentences e sande s the ner prediction of this entity is different between the two sentences n s e n s e we then add all satisfied eto a list as the suspicious entities of both sentences.
for example as shown in fig.
we first iterate all the named entities predicted by the ner systems and obtain their ner categories.
the information on these named entities is shown in the table.
bbc news russian appears in both sentences and the predictions of its evaluation functionbbc news is an operational business division of the bbc.
suspicious entity bbc news loc news is an operational business division of the bbc.
bbc is an operational business division of the bbc.
bert masked language model cnn news fox news bbc newspaper bbc company cnn news is an operational business division of the bbc.
fox news is an operational business division of the bbc.
bbc newspaper is an operational business division of the bbc.ner system cnn news org .
fox news org .
bbc newspaper misc .3org .
misc .3relabeled named entity bbc news org .
select the ner type with the maximum score filtering sentence generationfigure suspicious entity relabel ner category are different between the two sentences thus it is reported as a suspicious entity located.
.
.
suspicious entity relabeling.
given a suspicious entity we would relabel the suspicious entity by our repairing algorithm in turn one suspicious entity being relabeled each time .
similar to the automated testing part we first generate and filter the mutant entities which have similar semantics to the suspicious entity.
then we feed the mutant entities into a scoring system to obtain the fixed ner prediction.
similar entity generation given an input sentence sand a suspicious entity es tin masks each word subword a fragment of a word that might not stand alone as a full word in the language tokenized by bert ofesin turn one word or subword being masked each time and feeds the masked sentences into bert mlm to generate top k mutant entities with predictive logit.
similar entity filter next to ensure high quality of the similar entities we adopt another threshold pthreshold and filter out mutant entities with a predictive logit plower thanpthreshold .
to ensure the mutant entities have consistent semantics with the suspicious entity we then compute the context aware embedding for the suspicious entity and the mutant entities.
for each mutant entityei we filter out eiif the predefined semantic similarity between the embedding of esand embedding of eiis lower than a thresholdsthreshold .
we also check the format consistency between the suspicious entity and the mutant entities and remove those with inconsistent formats.
for example the case of the first letter of the generated word must remain unchanged.
in practice we setpthreshold as .
andsthreshold as .
based on experience which outperforms well on the four ner systems.
scoring system after entity generation and filtering we replace the suspicious entity with the filtered mutant entities e e ... e min turn to generate mutant sentences s s ...s m which will be fed into the ner system to obtain the ner prediction of each mutant entity i mn s i e i .
specifically we add a new ner category named null to indicate that the words should not be recognized as a named entity.
for each ner category we add a score calculated by an evaluation function f p sim to it in the scoring system where pdenotes the predictive logit of the mutant entity andsimdenotes the semantic similarity.
for the masked word subword w mutante and mutant sentence s we define theautomated testing and improvement of named entity recognition systems esec fse november san francisco usa at the moment mr moodley uses youtube clips of test matches to collect footage but he hopes to deal with cricket south africato secure more videos.suspicious entities cricket south africa south africa relabeled prediction cricket south africa org .
south africa loc .503suspiicous entity relabeling fixed named entity prediction cricket south africa org .
cricket south africa south africa are conflicted in rangeloc .
org .911range conflict detectingdeprecate south africa figure repairing of range conflict evaluation function as below f p sim pexp k sim n s e null wis subword pexp k sim n s e null wis word pexp k sim n s e null wis subword pexp k sim n s e null wis word wherekis a constant to balance the weights between the predictive logit and the semantic similarity are constant coefficients between used for controlling the influence of the newly added ner category null and the subword tokenized by bert respectively.
in practice we set k .
.
and .5for all ner systems we adopt tin to repair.
finally the ner category with the maximum score in the scoring system will be selected as the relabeled ner prediction of the suspicious entity.
the overall process of the relabeling is detailed by algorithm .
we define a function relabel where the input consists of a suspicious entityesand the sentence s. the output of the function is the relabeled ner category prediction and a scorep score for the ner category.
we first define an empty dict score to store the score of each ner category during the process line .
then we enumerate each word subword waines line and mask waby a special mark to obtain a masked sentence smask line .
the masked language model function bert mlm generates a list of candidate wordscwwith the corresponding predictive logits pand returns the top k candidate words with the highest predictive logit line .
the embedding function bert embedding extracts the contextaware embedding of the suspicious entity and the mutant entities lines and .
we enumerate each word wband its predictive logitpincw line and filter them through the introduced filters format consistency predictive logit and semantic similarity lines to .
after that we generate the mutant entity emand the mutant sentence smwith the filtered wb which will be fed into the ner system to obtain the ner prediction of em including the newly added category null denoted as n sm em .
then we use the evaluation function f p sim to compute the score for the em and add the score to score line .
finally we select the ner category with the maximum score in the dict as the relabeled ner category prediction line and its score as p score line .there is an example using tin to relabel the suspicious entity bbc news locin the sentence bbc news is an operational business division of the bbc.
as shown in fig.
.
tin first masks each word subword in the bbc news in turn to generate mutant entities cnn news fox news bbc newspaper and bbc company through the bert.
then tin filters out the mutant entity bbc company and feeds the mutant sentences into the ner system to obtain the ner predictions cnn news org fox news org and bbc newspaper misc .
the evaluation function of tin will calculate the score for each filtered mutant entity and aggregate the score to the corresponding ner category org score .
misc score .
.
after that tin selects the ner category orgthat has the maximum score as the relabeled result of the bbc news .
repairing of range conflict as the relabeling process is independent for each suspicious entity there are potential range conflicts among the relabeled named entities which will be solved by tin using p score .
in practice we enumerate all pair ea eb of relabeled named entity within sand check whether there is a range conflict between eaandeb.
if there is a range conflict among two relabeled named entities tin will deprecate the relabeled named entity with lower p score betweeneaandeb.
for example as shown in fig there is a range conflict between two suspicious entities cricket south africa and south africa in the sentence.
after the relabeling process the p score of the former is0.911whereas the latter is .
by which tin will deprecate the relabeled named entity south africa .
finally we obtain a fixed ner prediction r s without range conflicts.
if the fixed ner prediction differs from the original n s the ner system is repaired using r s .
algorithm black box relabeling the suspicious entity data s a sentence input es the suspicious entity to relabel result prediction the relabeled ner type of the suspicious entity p score the score of the relabeled ner type.
1function relabel s es 2score dict 3hs bert embedding s es foreachwa esdo 5smask replace s wa 6cw bert mlm smask foreach wb p cwdo ifformat incosistent wa wb then continue ifp pthreshold then continue sm replace s wa wb em replace es wa wb hm bert embedding sm em sim cossim hs hm ifsim sthreshold then continue score f p sim end end 18prediction arg maxkey ner category score 19p score max score.values returnprediction p score 21end functionesec fse november san francisco usa boxi yu yiyan hu qiuyang mang wenhan hu and pinjia he table precision of tin on bbc news dataset ner systems token level replacement phrase level replacement structural transformation random entity shuffle tin overall flair conll .
.
.
.
.
flair ontonotes .
.
.
.
.
azure ner .
.
.
.
.
aws ner .
.
.
.
.
table precision comparison between tin and baseline on conll03 tested on flair conll tin seqattack methods token level replacement phrase level replacement structural transformation random entity shuffle tin overall bert attack clare precision .
.
.
.
.
.
.
evaluation rq1 how accurate is tin at finding erroneous issues?
rq2 what categories of named entity recognition errors can tin find?
rq3 how effective is tin at repairing ner errors?
.
experiment setup to verify the results of tin we manually inspect the results both for the automated testing and the automated repairing part of tin.
for the automated testing part of tin we collectively decide whether the issue contains ner errors and if yes what category of ner errors it contains.
for the automated repairing part of tin we collectively decide whether the suspicious entity has been repaired correctly.
it is noticeable that manual inspection is only used to verify the effectiveness of tin which is totally automated.
specifically we evaluated suspicious issues for ner testing and suspicious entities for ner repairing.
all experiments are conducted on a linux ubuntu .
.
lts workstation with 64gb memory and geforce rtx gpu.
.
dataset we use two datasets in this paper the bbc news dataset and conll03 dataset.
we collect the sentences on bbc news with multiple news categories to construct the bbc news dataset which contains sentences without ner labels.
we use this dataset to evaluate tin s performance to test and repair ner systems on general text data.
the conll03 dataset is a widely used ner dataset that contains the ground truth label.
as the baselines rely on ground truth label to generate test cases we compare tin and the baselines on the test dataset of conll03.
.
comparison tin is the first black box testing and repairing approach for general ner systems and there is a dearth of baselines for testing approaches.
alternatively we choose two popular adversarial attack approaches as our baseline methods bert attack and clare and compare tin with the baseline methods on the conll03 test dataset .
we use tin and the baselines to test the bert basener fine tuned on conll03 dataset and evaluate the precision of the reported suspicious issues.
.
evaluation metric as tin consists of automated testing and automated repairing for ner systems we use different evaluation metrics for these two parts.
for ner automated testing the output is a list of suspicious issues each containing original sentence sand its ner predictions n s mutant sentence s and its ner predictions n s .
we define the precision of ner automated testing as the percentage of erroneous issues in suspicious issues which have ner prediction errors in at least one sentence.
we use precision to evaluate how precise tin reports the suspicious issues.
explicitly for a suspicious issue pt we seterrort pt totrue ifn s orn s have ner prediction errors.
otherwise we will set errort pt tofalse .
given a list of suspicious issues the precision is calculated by precision pt pt1 error pt pt whereptis the suspicious issues returned by tin and pt is the number of suspicious issues from automated ner testing.
for automated ner repairing there are four possible outcomes tf ft ff tt .
we usetandfto represent whether the ner prediction is correct or incorrect and tf ft ff tt present the transition through the repairing e.g.
ftmeans the repairing process successfully fixes the ner systems by changing incorrect prediction to correct.
the number of incorrect ner predictions before repairing is numerror ft ffand the number of correct ner predictions before repairing is numcorrect tt tf.
three evaluation metrics are used to evaluate the effectiveness of ner repairing err2cor measures the probability of changing incorrect ner predictions to correct and is calculated as err2cor ft numerror.
cor2errmeasures the probability of changing correct ner predictions to incorrect and is calculated as cor2err tf numcorrect.
errorreduce measures the ability to reduce ner errors and is calculated as errorreduce ft tf numerror.
.
rq1 precision of finding erroneous issues we adopt tin to test four ner systems and report suspicious issues where the effectiveness lies in how precise the reported issues are.
the evaluation result on bbc news dataset is shown in table where tin achieves a high average precision ranging fromautomated testing and improvement of named entity recognition systems esec fse november san francisco usa .
to .
over the four ner systems under test.
among all the transformation methods on the four ner systems under test tin achieves a high precision from .
to .
.
on bbc news dataset tin successfully reports erroneous issues out of suspicious issues we sample.
the evaluation result on bbc news dataset has shown that tin achieve a good performance on the dataset without ground truth label.
we also compare tin with the baselines on conll03 where the result is shown in table we can observe that tin achieves much higher precision than the baselines ranging from .
to .
.
while bert attack and clare only achieve a low precision of .
and .
respectively.
therefore tin has a significant advantage over the baselines for reporting ner errors even though it does not need the ground truth label to generate the test cases.
rq1 answer tin achieves a high average precision on the bbc news dataset without ground truth label ranging from .
to .
over the four ner systems under test.
for the comparison between tin and the baselines on conll03 tin has achieved a much higher precision than the baselines.
.
rq2 categories of the reported ner errors tin is capable of finding ner errors of diverse kinds.
in our experimental results we mainly found categories of ner errors omission over labeling incorrect category and range error where the categories are concluded during our manual inspection.
we will show examples of the reported ner errors with respective to each category of ner error in the following.
omission an omission error indicates that the ner systems fail to recognize the named entity in a sentence.
for example in the first sentence of table flair conll fails to detect the named entity esa .
over labeling an over labeling error indicates that the ner systems label the words that do not belong to any ner category.
for example in the second sentence of table flair ontonotes mistakenly label the halfway as the ner category cardinal .
incorrect category an incorrect category error indicates that the ner systems incorrectly predict the ner category of the named entity.
for example in the third sentence of table azure ner predicts mekelle asperson which is indeed a location .
range error a range error indicates the ner systems only label part of the named entity or involve unnecessary elements besides the named entity.
for example in the fourth sentence of table aws ner erroneously predicts the two words project and atratum of the named entity project atratum a specific project name which belongs to other in aws ner categories as other and title which overlooks the whole meaning of the named entity and leads to the range error.
we calculate the distribution of the four ner categories on a random sample of ner errors found by tin where omission over labeling incorrect category and range error each accounts for .
.
.
.
.rq2 answer tin can successfully find four categories of ner errors including omission over labeling incorrect category and range error which occupies .
.
.
and .
on an evaluation of ner errors.
.
rq3 repairing performance of tin we evaluate a total of suspicious entities for assessing tin s ability to repair the ner errors.
the results of the automated repairing are shown in table where we can observe that a big ratio of the suspicious entities contains ner errors from .
to .
among the four ner systems under test.
we have found and ner errors for flair conll flair ontonotes aws ner and azure ner respectively.
by utilizing tin to repair these ner errors and of the ner errors have been repaired.
the err2corin table is used to show tin s ability to repair the ner errors where a high ratio from .
to .
of the ner errors have been repaired to be correct ner predictions among the four ner systems under test.
however tin may sometimes turn the correct ner prediction into an incorrect prediction.
the cor2errin table is used to assess the extent where tin would mislead the ner systems.
we can observe that only .
to .
of the correct ner predictions have been misled to the incorrect ner predictions which is much lower than the ratio that the ner errors are repaired.
errorreduce considers both the errors being repaired and the correct ner predictions being misled.
we can observe that there is a great ratio of error reduction after the automated repairing from .
to .
.
therefore we can see that the automated repairing effectively decreases the ner errors of the four ner systems under test.
as shown in table tin can repair four categories of ner errors it finds repairing of omission error in the first sentence of table tin locates the suspicious entity esa which is an abbreviation of the organization name environmental services association .
in the original prediction the ner system misses the ner prediction of esa where there is an error of omission.
tin successfully predicts the correct ner category orgof esa in the fixed prediction.
repairing of over labeling error in the second sentence of table tin locates the suspicious entity halfway which does not belong to any ner category under the rule of flair ontonotes.
in the original prediction the ner system recognizes the ner prediction of halfway ascardinal where there is an error of over labeling.
tin successfully deprecates the ner prediction of halfway in the fixesd prediction.
repairing of incorrect category in the third sentence of table tin locates the suspicious entity mekelle which is a special zone and capital of the tigray region of ethiopia.
in the original prediction the ner system recognizes the ner prediction of mekelle as incorrect ner category person where there is an error of incorrect category.
tin successfully predicts the correct ner category location of mekelle in the fixed prediction.
repairing of range error in the fourth sentence of table tin locates the suspicious entities project stratum and project stratum where project stratum is a broadband infrastructure project to extend access to superfast broadband services acrossesec fse november san francisco usa boxi yu yiyan hu qiuyang mang wenhan hu and pinjia he table example of four categories of ner errors and repairing of them by tin sentence suspicious entities original prediction fixed prediction ben johnson from the environmental services association esa told bbc news more and more people were putting devices containing these batteries in with household rubbish or mixing them with other recycling.
flair conll error category omission the halfway point affords us an opportunity to step back and then look at what our margins are and where we could be a little smarter to buy down risk and understand the spacecraft s performance for crewed flight on the very next mission.
flair ontonotes error category over labeling they say the only positive thing the federal authorities have done is to return electricity to mekelle.
azure ner error category incorrect category fibrus is delivering a similar scheme in northern ireland known as project stratum.
aws ner error category range error table improvement based on manual inspection ner systems tt tf ft ff numerror numcorrect err2cor cor 2err errorreduce flair connll .
.
.
.
.
.
.
.
.
flair ontonotes .
.
.
.
.
.
.
.
.
aws ner .
.
.
.
.
.
.
.
.
azure ner .
.
.
.
.
.
.
.
.
northern ireland.
in the original prediction the two elements of project stratum i.e.
project and stratum are recognized as other andtitle where there is an error of range.
tin successfully predicts the correct ner category other of project stratum and deprecates the ner predictions of project and stratum in the fixed prediction.
rq3 answer the evaluation results show the great ability of tin to repair the ner systems where err2coris much higher thancor2err and there is a high ratio of errorreduce from .
to .
on the four ner systems under test.
tin can repair four categories of ner errors it finds.
discussion .
false positives in the manual inspection of the experimental results for tin s testing part false positives refer to the suspicious issues that do not have ner errors.
in addition if only the mutant sentence in a suspicious issue contains the ner errors while it is unnatural i.e.
grammatically incorrect or difficult to understand we also regard the suspicious issue as false positive.
this is because ner errors detected on the unnatural mutant sentence contribute little to improving real world ner systems.
in practice we use a semantic filter and a syntactic filter to reduce the amounts of unnatural mutant sentences.
it is worth noting that the accuracy of ner systems does not impact the false positives of tin s testing part even though we use the ner prediction of the original sentence from ner systems to generate mutant sentences.
specifically if the original sentence s ner prediction is incorrect it may lead to unexpected sentencetransformation e.g.
replacing named entity in similar sentence generation.
however when tin reports suspicious issues in this case it would not result in false positives since the suspicious issues contain at least an original ner prediction error.
in the manual inspection of the experimental results for tin s repairing part false positives represent the case that tin turns the correct ner prediction to incorrect ner prediction which is pre defined as tfin section .
.
however tfonly ranges from .
to .
on the repairing results which is much lower compared with the true positive indicator ft. despite the presence of false positives tin can improve the performance of ner systems reducing errors from .
to .
.
.
threat to validity the main threats to external validity lie in the selection of the ner systems and the dataset for testing.
there are over hundreds of ner systems designed for multiple purposes and mountains of text data on the internet thus we can not evaluate tin on all of them.
to enrich the diversity of the test data we collect the bbc news dataset containing multiple news categories.
in addition we evaluate tin on two typical sota ner systems in flair and two famous commercial ner apis to verify its generalization ability.
threats to internal validity are factored into our experimental methodology and they may affect our results.
even though tin automatically tests and repairs the ner systems we need to manually inspect the results to assess the performance of tin which is potentially error prone.
to minimize this threat two people performed manual inspection separately and collectively decided whether tin succeeded in reporting erroneous issues and repairing ner errors.automated testing and improvement of named entity recognition systems esec fse november san francisco usa related work .
robustness of nlp systems deep neural networks have boosted the performance of multiple nlp tasks including sentiment analysis code analysis reading comprehension and machine translation .
however there are still many bugs during the usage of the sota nlp systems.
many researchers researched on the robustness of nlp systems which unveiled bugs proposed by the neural networks used for various nlp systems .
for example jia et al.
proposed an adversarial evaluation scheme for the stanford question answering dataset squad which found seventeen sota models performance is primarily undermined by the adversarial sentence inserted into the paragraph.
similar to these works tin focuses on the validation and improvement of the robutsness for ner systems.
although some of tin s mutation operators have already been utilized in existing nlp testing approaches e.g.
word replacement these established nlp testing tools cannot be seamlessly adapted for testing ner systems.
this is due to the need to avoid substituting named entities with non named entities and the absence of named entity labels in real world text.
however tin leverages the predictions of ner systems and performs mutations accordingly eliminating the dependence on named entity labels for test case generation.
tin approach effectively addresses the challenges posed by traditional methods.
furthermore the testing framework employed by tin can be adopted to test other nlp systems offering the advantage of finely controlling the components of the generated test cases.
.
automated improvement of nlp systems beyond the attack and test improving the performance of the nlp systems is the ultimate goal.
some researchers relabeled the reported bugs of the nlp systems and fine tuned the model on the relabeled dataset to fix the bugs.
though effective this method demands a significant amount of human resources.
other researchers developed automated repairing techniques to fix the machine translation bugs.
for example cat and transrepair are word replacement methods that provide automatic fixing of revealed bugs without model retraining.
inspired by cat and transrepair we design a repairing system to fix the ner errors found by tin which is the first black box repairing algorithm to repair the general ner systems.
.
robustness of ner systems although ner systems have gained significant progress and benefited from deep neural networks it is not a solved task yet.
some researchers use adversarial methods to attack the ner systems which undermines the performance of the ner systems under the perturbed datasets.
the adversarial attack method needs to access the parameters of the ner models thus it can not be conveniently adopted to test the commercial ner apis.
xu et al.
adopted two specialized metamorphic relations to test an industrial ner system called litigant.
this approach can only be used to test litigant because it relies on the characteristics e.g.
plaintiff and defendant roles in two companies and application contexts e.g.
the transformation between parent and sub company names via extension of litigant.
thus we did not include it in our experiments.
we proposed tin the first black box approach for testing and repairing general ner systems which can be easily adapted to test various ner systems with different ner categories.
.
metamorphic testing metamorphic testing is a technique for testing software systems where the input and output of a test case are related in some way such that if the input is modified in a specific way the output should also be modified in a specific way.
as an effective approach to address the test case generation problems and test oracle problems metamorphic testing is widely adopted to test a variety of systems including traditional software such as compilers and database systems and ai based systems such as machine translation and image captioning systems .
tin is the first metamorphic testing approach for validating general ner systems with different ner categories.
conclusion this paper presents tin the first approach that automatically tests and improves the general ner systems which is black box and widely applicable to various ner systems.
we have proposed three transformation schemes to generate test cases and two metamorphic relations to test the ner systems which achieve a high average precision from .
to .
for reporting erroneous issues over the four ner systems under test.
tin have successfully found a diversity of ner errors among the erroneous issues including omission mislabeling incorrect category and range error.
the automated repairing of tin can fix all four categories of ner errors and achieve a high rate of errorreduce from .
to .
on the four ner systems under test.
data availability codes and data of tin can be found at .