code search based on context aware code translation weisong sun weisongsun smail.nju.edu.cn state key laboratory for novel software technology nanjing university chinachunrong fang fangchunrong nju.edu.cn state key laboratory for novel software technology nanjing university chinayuchen chen yuc.chen outlook.com state key laboratory for novel software technology nanjing university china guanhong tao taog purdue.edu purdue university west lafayette indiana usatingxu han hantingxv .com school of information management nanjing university chinaquanjun zhang quanjun.zhang smail.nju.edu.cn state key laboratory for novel software technology nanjing university china abstract code search is a widely used technique by developers during software development.
it provides semantically similar implementationsfromalargecodecorpustodevelopersbasedontheirqueries.
existing techniques leverage deep learning models to construct embeddingrepresentationsforcodesnippetsandqueries respectively.featuressuchasabstractsyntactictrees controlflowgraphs etc.
are commonly employed for representing the semantics of code snippets.
however the same structure of these features does not necessarily denote the same semantics of code snippets and viceversa.inaddition thesetechniquesutilizemultipledifferent word mapping functions that map query words code tokens to embedding representations.
this causes diverged embeddings of thesameword tokeninqueriesandcodesnippets.weproposea novelcontext awarecodetranslationtechniquethattranslatescode snippets into natural language descriptions called translations .
the codetranslationis conductedon machineinstructions where the context information is collected by simulating the execution of instructions.
we further design a shared word mapping func tion using one single vocabulary for generating embeddings for both translations and queries.
we evaluate the effectiveness of our technique called trancs on the codesearchnet corpus with queries.experimentalresultsshowthattrancssignificantlyoutperformsstate of the arttechniquesby49.
to66.
interms ofmrr mean reciprocal rank .
ccs concepts software and its engineering search based software engineering.
corresponding author.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa association for computing machinery.
acm isbn ... .
code search deep learning code translation acm reference format weisongsun chunrongfang yuchenchen guanhongtao tingxuhan and quanjun zhang.
.
code search based on context aware code translation.
in 44th international conference on software engineering icse may pittsburgh p a usa.
acm new york ny usa pages.
introduction software development is usually a repetitive task where same orsimilarimplementationsexistinestablishedprojectsoronline forums.developerstendtosearchforthosehigh qualityimplementationsforreferenceorreuse soastoenhancetheproductivityandqualityoftheirdevelopment .existingstudies show thatdevelopersoftenspend19 oftheirtimeonfindingreusablecodeexamplesduringsoftwaredevelopment.codesearch cs is anactiveresearchfield whichaims atdesigningadvancedtechniquestosupportcoderetrievalservices.
given a query by the developer cs retrieves code snippets thatare related to the query from a large scale code corpus such as github andstackoverflow .figure1showsanexample.
thequery howtocalculatethefactorialofanumber infigure1 a isprovidedbythedeveloper whichisusuallyashortnaturallanguage sentence describing the functionality of the desired code snippet .
the method function in figure b is a possible code snippet that satisfies the developer s requirement.
g8 g16 g17 g24 g12 g10 g23 g0 g12 g13 g14 g27 g0 g28 g21 g23 g18 g13 g15 g10 g21 g12 g0 g2 g10 g14 g18 g0 g14 g17 g11 g24 g25 g15 g1 g0 g42 g7 g12 g13 g14 g27 g0 g28 g21 g23 g18 g13 g15 g10 g21 g12 g0 g08 g0 g5 g06 g00 g10 g14 g18 g0 g10 g0 g08 g0 g5 g06 g00 g28 g13 g15 g0 g2 g06 g0 g10 g0 g05 g08 g0 g14 g17 g11 g24 g25 g15 g06 g0 g10 g3 g3 g1 g0 g42 g02 g28 g21 g23 g18 g13 g15 g10 g21 g12 g0 g08 g0 g28 g21 g23 g18 g13 g15 g10 g21 g12 g0 g4 g0 g10 g06 g01 g41 g04 g15 g25 g18 g17 g15 g14 g0 g28 g21 g23 g18 g13 g15 g10 g21 g12 g06 g03 g41 g10 g13 g40 g0 g18 g13 g0 g23 g21 g12 g23 g17 g12 g21 g18 g25 g0 g18 g10 g25 g0 g28 g21 g23 g18 g13 g15 g10 g21 g12 g0 g13 g28 g0 g21 g0 g14 g17 g11 g24 g25 g15 g6 g2 g24 g1 g0 g07 g0 g20 g13 g26 g25 g0 g22 g14 g10 g16 g16 g25 g18 g0 g1 g1 g2 g21 g1 g0 g07 g0 g20 g17 g25 g15 g40 figure an example of query and code snippet existing cs techniques can be categorized into traditional methodsthat use keyword matching between queries and code snippets ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa weisong sun and chunrong fang et al.
suchasinformationretrieval basedcodesearch andquery reformulation basedcodesearch anddeeplearningmethods thatencodequeriesandcodesnippets into embedding representations capturing semantic information.
traditionalmethodssimplytreatqueriesandcodesnippetsasplain texts andretrievequery relatedcodesnippetsbyonlylookingat matchedkeywords.theyfailtocapturethesemanticsofbothquery sentencesandcodesnippets.deeplearning dl methodstransform input queries and code snippets into embedding representations.
specifically for a given query all the words in the query sentence arefirstrepresentedaswordembeddingsandthenfedtoadlmodeltoproduceaqueryembedding .foracodesnippet multiple aspects are extracted as features such as tokens abstract syntactic trees asts and control flow graphs cfgs .
these features are transformed into corresponding embeddings and processed by another dl model to produce a code embedding .
thecodesearchtaskishencetofindsimilarpairsbetweenquery embeddingsand codeembeddings.while dlmethods surpasstraditional methods in capturing the semantics of queries and code snippets theirperformancesarestilllimitedduetotheinsufficiency of encoding semantics and the embedding discrepancy betweenqueries and code snippets.
existing techniques miss either data dependenciesamongcodestatementslikemman orcontrol dependenciessuchasdeepcs carlcs cnn andtabcs .
furthermore the embedding representations of code snippets are largely different from those of query sentences written in naturallanguage causingsemanticmismatchduringthecodesearch task.
for example mman uses different word mapping functions thatmapawordortokentoanembeddingrepresentation to encode queries and tokens asts and cfgs in code snippets.for the widely used word lengthin both queries and code snippets the embedding representations are different in those word mappingfunctions leadingtopoorcodesearchperformanceaswe will discuss in section and experimentally show in section .
.
.
weproposeanovelcontext awarecodetranslationtechnique that translates code snippets into natural language descriptions called translations .
such a translation can bridge the representation discrepancy between code snippets in programming languages andqueries innaturallanguage .specifically weutilize a standard program compiler and a disassembler to generate the instruction sequence of a code snippet.
however the context information such as local variables data dependency etc.
are missed fromtheinstructionsequence.wehencesimulatestheexecutionof instructionstocollectthosedesiredcontexts.asetofpre defined translationrulesarethenusedtotranslatetheinstructionsequence andcontextsintotranslations.suchacodetranslationiscontextaware.
the translations of code snippets are similar to those de scriptions in queries in which they share a range of words.
wehence design a shared word mapping mechanism using one sin gle vocabulary for generating embeddings for both translations andqueries substantiallyreducingthesemanticdiscrepancyand improving the overall performance see results in section .
.
.
in summary we make the following contributions.
weproposeacontext awarecodetranslationtechniquethat transforms code snippets into natural language descriptions with preserved semantics.
we introduce a shared word mapping mechanism which bridges the discrepancy of embedding representations from code snippets and queries.
weimplementacodesearchprototypecalledtrancs.we evaluate it on the codesearchnet corpus with queries.experimentalresultsshowthattrancsimprovesthe top hit rate of code search by .
to .
compared to state of the art techniques.
in addition trancs achieves mrrof0.
outperformingdeepcs andmman by .
and .
respectively.
the source code of trancs and all the data used in this paper are released and can be downloaded from the website .
background .
machine instruction sincethecontext aware codetranslationtechniqueweproposeis performedatthemachineinstructionlevel wefirstintroducethe background about machine instructions.
a program runs by executing a sequence of machine instructions .
a machine instruction consists of an opcode specifying the operation to be performed followed by zero or more operands embodyingvaluestobeoperatedupon .forexample injava virtual machine istore 2 is a machine instruction where istore is an opcode whose operation is store intinto local variable and 2is an operand that represents the index of the local variable.
machine instructions have been widely used in software engineering activities such as malware detection api recommendation codeclonedetection programrepair andbinary code search .
machine instructionsare generated by disassemblingthebinaryfiles suchasthe .classfileinjava.therefore itis alsocalledbytecode orbytecodemnemonicopcode in some of the works mentioned above.
for ease of understanding the terminology instruction is used uniformly in this paper.
.
deep learning based code search g2 g14 g8 g9 g0 g5 g13 g10 g15 g15 g9 g18 g17 g4 g19 g9 g16 g20 g4 g19 g9 g16 g20 g0 g3 g13 g7 g14 g8 g9 g16 g2 g14 g8 g9 g0 g3 g13 g7 g14 g8 g9 g16 g2 g3 g5 g10 g12 g10 g11 g6 g16 g10 g18 g20 g1 g4 figure a general framework of dl based cs techniques asshowninfigure2 wecanobservethatdeeplearning dl based cs techniquesusually consist of threecomponents a query encoder a code encoder and a similarity measurement component.
thequeryencoderisanembeddingnetworkthatcanencodethe queryqgiven by the developer into a d dimensional embedding representation eq rd.
to train such a query encoder existing dl basedcstechniqueshavetriedvariousneuralnetworkarchitectures such as rnn lstm and cnn .
in dl based csstudies itisacommonpracticetousecodecommentsasqueries during the training phase of the encoder .
code commentsarenaturallanguage descriptionsused toexplain whatthe authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
code snippets want to do .
for example the first line of figure a is a comment for the code snippet sa.
therefore we do not strictly distinguish the meaning of the two terms comment and query and use the term comment during encoder training and queryatothertimes.thecodeencoderisalsoanembeddingnetwork that can encode ncode snippets in the code corpus sinto corresponding embedding representations es rn d. inexisting dl basedcstechniques thecodeencoderisusuallymuchmore complicatedthanthequeryencoder.forexample thecodeencoder of mman consists of three sub encoders that are built on the lstm tree lstm andggnn architectureswiththe goal of encoding different features of the code snippet e.g.
tokens asts andcfgs.thesimilaritymeasurementcomponentisused to measure the cosine similarity between eqand each es es.
the targetofdl basedcstechniquesistorankallcodesnippetsin s by thecosine similarity .
thehigher the similarity the higher relevance of the code snippet to the given query.
motivation inthissection westudythelimitationsofcommonlyusedrepresentationsofcodesnippetsaswellastherepresentationdiscrepancy between code snippets and comments in existing works .
g8 g6 g6 g0 g27 g25 g34 g27 g42 g34 g25 g41 g29 g0 g41 g32 g29 g0 g40 g42 g35 g0 g37 g30 g0 g25 g36 g0 g33 g36 g41 g0 g25 g39 g39 g25 g43 g7 g38 g42 g26 g34 g33 g27 g0 g33 g36 g41 g0 g27 g25 g34 g20 g39 g39 g25 g43 g22 g42 g35 g2 g33 g36 g41 g23 g24 g0 g25 g39 g39 g25 g43 g1 g0 g46 g00 g33 g36 g41 g0 g40 g42 g35 g0 g19 g0 g5 g17 g00 g33 g36 g41 g0 g33 g0 g19 g0 g5 g17 g02 g30 g37 g39 g0 g2 g17 g0 g33 g0 g18 g0 g25 g39 g39 g25 g43 g5 g34 g29 g36 g31 g41 g32 g17 g0 g33 g4 g4 g1 g0 g46 g01 g40 g42 g35 g0 g19 g0 g40 g42 g35 g0 g4 g0 g25 g39 g39 g25 g43 g23 g33 g24 g17 g04 g45 g15 g39 g29 g41 g42 g39 g36 g0 g40 g42 g35 g17 g16 g45 g6 g6 g0 g27 g25 g34 g27 g42 g34 g25 g41 g29 g0 g41 g32 g29 g0 g0 g40 g42 g35 g0 g37 g30 g0 g25 g36 g0 g33 g36 g41 g0 g25 g39 g39 g25 g43 g8 g38 g42 g26 g34 g33 g27 g0 g33 g36 g41 g0 g27 g25 g34 g20 g39 g39 g25 g43 g22 g42 g35 g2 g33 g36 g41 g23 g24 g0 g25 g39 g39 g25 g43 g1 g0 g46 g7 g33 g36 g41 g0 g39 g29 g40 g42 g34 g41 g0 g19 g0 g5 g17 g00 g33 g36 g41 g0 g33 g36 g28 g29 g44 g0 g19 g0 g5 g17 g00 g43 g32 g33 g34 g29 g2 g33 g36 g28 g29 g44 g0 g18 g0 g25 g39 g39 g25 g43 g5 g34 g29 g36 g31 g41 g32 g1 g0 g46 g02 g39 g29 g40 g42 g34 g41 g0 g19 g0 g39 g29 g40 g42 g34 g41 g0 g4 g0 g25 g39 g39 g25 g43 g23 g33 g36 g28 g29 g44 g24 g17 g01 g33 g36 g28 g29 g44 g4 g4 g17 g04 g45 g15 g39 g29 g41 g42 g39 g36 g0 g39 g29 g40 g42 g34 g41 g17 g45 g16 g2 g25 g1 g0 g21 g37 g28 g29 g0 g22 g36 g33 g38 g38 g29 g41 g0 g1 g2 g2 g26 g1 g0 g21 g37 g28 g29 g0 g22 g36 g33 g38 g38 g29 g41 g0 g1 g3 figure code snippets g14 g12 g21 g9 g26 g26 g12 g31 g10 g29 g22 g25 g12 g26 g12 g22 g27 g12 g26 g26 g12 g31 g13 g21 g24 g14 g20 g26 g15 g28 g29 g26 g23 g27 g29 g22 g16 g24 g26 g19 g8 g6 g19 g7 g12 g26 g26 g12 g31 g5 g21 g15 g23 g17 g28 g18 g27 g29 g22 g4 g8 g19 g12 g26 g26 g12 g31 g13 g21 g24 g14 g20 g19 g4 g4 g8 g6 g27 g29 g22 g14 g12 g21 g9 g26 g26 g12 g31 g10 g29 g22 g25 g12 g26 g12 g22 g27 g12 g26 g26 g12 g31 g13 g21 g24 g14 g20 g26 g15 g28 g29 g26 g23 g27 g29 g22 g30 g18 g19 g21 g15 g19 g8 g6 g19 g7 g12 g26 g26 g12 g31 g5 g21 g15 g23 g17 g28 g18 g27 g29 g22 g4 g8 g19 g12 g26 g26 g12 g31 g13 g21 g24 g14 g20 g19 g4 g4 g8 g6 g27 g29 g22 g2 g12 g1 g0 g1 g2 g32 g27 g0 g9 g10 g11 g2 g13 g1 g0 g1 g3 g32 g27 g0 g9 g10 g11 figure abstract syntactic trees figure shows two code snippets for calculating the sum of a given intarray.figure3 a usesa forstatementtoloopoverallthe elementsinthearray line5 andaddtheirvaluestovariable sum line6 .
figure b employs a whilestatement forthe sametask lines5 .semantically thetwocodesnippetshavetheexactsame meaning.
in figure we show the abstract syntax trees asts for the above two code snippets sa left figure and sb right figure g8 g14 g0 g30 g25 g27 g40 g37 g38 g33 g25 g34 g2 g36 g41 g35 g26 g29 g38 g1 g9 g14 g0 g30 g25 g27 g40 g37 g38 g33 g25 g34 g16 g7 g10 g14 g0 g27 g37 g41 g36 g40 g29 g38 g16 g7 g11 g14 g0 g30 g37 g38 g2 g27 g37 g41 g36 g40 g29 g38 g15 g16 g36 g41 g35 g26 g29 g38 g1 g29 g36 g28 g30 g37 g38 g13 g14 g0 g38 g29 g40 g41 g38 g36 g0 g30 g25 g27 g40 g37 g38 g33 g25 g34 g12 g14 g0 g30 g25 g27 g40 g37 g38 g33 g25 g34 g16 g30 g25 g27 g40 g37 g38 g33 g25 g34 g4 g27 g37 g41 g36 g40 g29 g38 g11 g14 g0 g27 g37 g41 g36 g40 g29 g38 g3 g3 g19 g25 g34 g39 g29 g22 g38 g41 g29 g8 g14 g0 g27 g25 g34 g17 g38 g38 g25 g42 g21 g41 g35 g2 g25 g38 g38 g25 g42 g1 g9 g14 g0 g39 g41 g35 g16 g7 g10 g14 g0 g33 g16 g7 g11 g14 g0 g30 g37 g38 g2 g33 g15 g16 g25 g38 g38 g25 g42 g6 g34 g29 g36 g31 g40 g32 g1 g29 g36 g28 g30 g37 g38 g13 g14 g0 g38 g29 g40 g41 g38 g36 g0 g39 g41 g35 g12 g14 g0 g39 g41 g35 g16 g39 g41 g35 g3 g25 g38 g38 g25 g42 g23 g33 g24 g11 g14 g0 g33 g3 g3 g19 g25 g34 g39 g29 g22 g38 g41 g29 g2 g25 g1 g0 g1 g2 g43 g39 g0 g18 g19 g20 g2 g26 g1 g0 g1 g1 g43 g39 g0 g18 g19 g20 figure control flow graphs respectively.
observethatthe sub treescircledin dottedlinesare different for the two code snippets.
such representations cause the inconsistency of code semantics leading to inferior results in code searchaswewillshowinsection5.
.
.controlflowgraph cfg is also commonly used for representing code snippets.
figure depictsthecfgsforthetwocodesnippets saands1 seefigure1 b insection1 .thetaskof s1istocalculatethefactorialofagiven number while saistocalculatethesumofagivenarray.thetwo code snippets have completely different goals.
however the cfgs shown in figure have the same graph structure which cannot differentiatethesemanticdifferencebetweenthetwocodesnippets.
this example delineates the insufficiency of utilizing cfgs for representing code semantics.
our experimental results in section .
.
showthatastate of the arttechniquemman leveragingasts and cfgs has a limited performance.
existingtechniquesleveragedeeplearningmodels i.e.
theencoders introduced in section .
for code search where code snippetsandcommentsneedtobetransformedintonumericalforms in order to train those models and produce desired outputs.
a common way is to build vocabularies for code snippets and comments and construct corresponding numerical representations e.g.
word embeddings .awordmappingfunctionisadictionarywiththekey ofatokenincodesnippetsorawordincomments fromvocabular ies andthe valueofafixed lengthreal valued vector.
deepcs builds four mapping functions for method names mn api se quences apis tokens and comments separately.
mman utilizesfourdifferentmappingfunctionsfortokens asts cfgs and comments respectively.
the embeddings in these mappingfunctions are randomly initialized and learned during the trainingprocessoftheencoder.suchalearningprocedureintroduces discrepantembedding representationsfor asamekey e.g.
a code token .
for instance asts are composed of code tokens whichshare a portion of same keys with the token vocabulary.
token namescanalsoappearincomments.forexample morethan50 ofkeysappearinbothcodesnippetsandcommentsvocabularies used by deepcs and mman.
inconsistent embeddings for same words tokens can lead to unsuitable matches between code snippets and comments causing poor performance of code search see section .
.
.our solution.
weproposeanovelcodesearchtechnique called trancs thatbetterpreservesthesemanticsofcodesnippetsand bridges the discrepancy between code snippets and comments.
differentfromexistingtechniquesthatleverageastsandcfgs we authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa weisong sun and chunrong fang et al.
g4 g13 g0 g30 g35 g33 g24 g0 g25 g28 g34 g0 g19 g29 g28 g33 g34 g17 g28 g34 g0 g2 g2 g5 g13 g0 g33 g34 g29 g32 g21 g0 g25 g28 g34 g0 g2 g25 g28 g34 g29 g0 g26 g29 g19 g17 g26 g0 g36 g17 g32 g25 g17 g18 g26 g21 g0 g19 g21 g16 g3 g18 g11 g19 g21 g15 g20 g2 g6 g13 g0 g30 g35 g33 g24 g0 g25 g28 g34 g0 g19 g29 g28 g33 g34 g17 g28 g34 g0 g2 g2 g7 g13 g0 g33 g34 g29 g32 g21 g0 g25 g28 g34 g0 g2 g25 g28 g34 g29 g0 g26 g29 g19 g17 g26 g0 g36 g17 g32 g25 g17 g18 g26 g21 g0 g14 g3 g14 g17 g10 g11 g23 g2 g8 g13 g0 g26 g29 g17 g20 g0 g25 g28 g34 g0 g22 g9 g15 g21 g11 g22 g32 g29 g27 g0 g26 g29 g19 g17 g26 g0 g36 g17 g32 g25 g17 g18 g26 g21 g0 g14 g3 g14 g17 g10 g11 g23 g2 g9 g13 g0 g26 g29 g17 g20 g32 g21 g22 g21 g32 g21 g28 g19 g21 g0 g9 g18 g18 g9 g24 g3 g9 g18 g18 g9 g24 g22 g32 g29 g27 g0 g26 g29 g19 g17 g26 g0 g36 g17 g32 g25 g17 g18 g26 g21 g0 g9 g18 g18 g9 g24 g3 g9 g18 g18 g9 g24 g2 g10 g13 g0 g23 g21 g34 g0 g26 g21 g28 g23 g34 g24 g0 g29 g22 g0 g17 g32 g32 g17 g37 g0 g9 g18 g18 g9 g24 g3 g9 g18 g18 g9 g24 g2 g11 g13 g0 g25 g22 g0 g17 g28 g20 g0 g29 g28 g26 g37 g0 g25 g22 g0 g25 g28 g34 g0 g22 g9 g15 g21 g11 g25 g33 g0 g23 g32 g21 g17 g34 g21 g32 g0 g29 g32 g0 g21 g31 g35 g17 g26 g0 g34 g29 g0 g25 g28 g34 g0 g15 g11 g17 g12 g20 g13 g34 g24 g21 g28 g0 g23 g29 g0 g34 g29 g0 g4 g4 g2 g5 g4 g13 g0 g26 g29 g17 g20 g0 g25 g28 g34 g0 g22 g9 g15 g21 g11 g8 g3 g1 g22 g32 g29 g27 g0 g26 g29 g19 g17 g26 g0 g36 g17 g32 g25 g17 g18 g26 g21 g0 g19 g21 g16 g3 g18 g11 g19 g21 g15 g20 g2 g5 g5 g13 g0 g26 g29 g17 g20 g32 g21 g22 g21 g32 g21 g28 g19 g21 g0 g9 g18 g18 g9 g24 g3 g9 g18 g18 g9 g24 g22 g32 g29 g27 g0 g26 g29 g19 g17 g26 g0 g36 g17 g32 g25 g17 g18 g26 g21 g0 g9 g18 g18 g9 g24 g3 g9 g18 g18 g9 g24 g2 g5 g6 g13 g0 g26 g29 g17 g20 g0 g25 g28 g34 g0 g22 g9 g15 g21 g11 g8 g4 g22 g32 g29 g27 g0 g26 g29 g19 g17 g26 g0 g36 g17 g32 g25 g17 g18 g26 g21 g0 g14 g3 g14 g17 g10 g11 g23 g2 g5 g7 g13 g0 g26 g29 g17 g20 g25 g28 g34 g0 g22 g9 g15 g21 g11 g8 g5 g22 g32 g29 g27 g0 g9 g18 g18 g9 g24 g3 g9 g18 g18 g9 g24 g15 g22 g9 g15 g21 g11 g8 g4 g16 g2 g5 g8 g13 g0 g25 g28 g34 g0 g32 g21 g33 g35 g26 g34 g0 g25 g33 g0 g25 g28 g34 g0 g22 g9 g15 g21 g11 g8 g3 g17 g20 g20 g0 g25 g28 g34 g0 g22 g9 g15 g21 g11 g8 g5 g14 g0 g30 g35 g33 g24 g0 g32 g21 g33 g35 g26 g34 g0 g25 g28 g34 g29 g0 g22 g9 g15 g21 g11 g8 g6 g2 g5 g9 g13 g0 g33 g34 g29 g32 g21 g0 g25 g28 g34 g0 g22 g9 g15 g21 g11 g8 g6 g25 g28 g34 g29 g0 g26 g29 g19 g17 g26 g0 g36 g17 g32 g25 g17 g18 g26 g21 g0 g19 g21 g16 g3 g18 g11 g19 g21 g15 g20 g2 g5 g10 g13 g0 g25 g28 g19 g32 g21 g27 g21 g28 g34 g0 g26 g29 g19 g17 g26 g0 g36 g17 g32 g25 g17 g18 g26 g21 g0 g14 g3 g14 g17 g10 g11 g23 g18 g37 g0 g19 g29 g28 g33 g34 g17 g28 g34 g0 g3 g2 g5 g12 g13 g0 g23 g29 g34 g29 g6 g2 g6 g6 g13 g0 g26 g29 g17 g20 g0 g25 g28 g34 g0 g22 g9 g15 g21 g11 g8 g7 g1 g22 g32 g29 g27 g0 g26 g29 g19 g17 g26 g0 g36 g17 g32 g25 g17 g18 g26 g21 g0 g19 g21 g16 g3 g18 g11 g19 g21 g15 g20 g2 g6 g7 g13 g0 g32 g21 g34 g35 g32 g28 g25 g28 g34 g22 g9 g15 g21 g11 g8 g7 g1 g22 g32 g29 g27 g0 g27 g21 g34 g24 g29 g20 g2 figure code translations of saandsb directly translate code snippets into natural language sentences.
specifically weutilizeastandardprogramcompilerandadisassembler to generate the instruction sequence of a code snippet.
such a sequence however lacksthecontextinformationsuchaslocalvariables data dependency etc.
we propose to simulate the execution ofinstructionstocollectthosedesiredcontexts.asetofpre defined translationrulesarethenusedtotranslatetheinstructionsequence and contexts into natural language sentences.
details can be found insection .figure 6showcases thetranslations ofthetwocode snippetssaandsbby trancs.
the different colors denote different variablenamesusedin sa blue and sb red .thenumbers wordsin bold e.g.
valueand22 denote the data and control dependencies amonginstructions.observethat thetranslationsof saandsbare the same except for local variable names.
the overall semanticsdescribed by the sentences in figure are the same.
the translations are similar to those descriptions in comments in which they sharearangeofwords.wehencedesignasharedwordmapping functionusingonesinglevocabularyforgeneratingembeddings forbothcodesnippetsandcomments substantiallyreducingthe semantic discrepancyand improving theoverall performance see results in section .
.
.
methodology .
overview figure7illustratestheoverviewofourtrancs.thetoppartshows thetrainingprocedureoftrancsandthebottompartshowsthe usageoftrancsforagivenquery.duringthetrainingprocedureof trancs two types of input data are leveraged comments and code snippets.thecommentsinfigure7arenaturallanguagedescriptions that appear above the code snippet e.g.
javadoc comments notinthecodebody.thesecommentsareinputtotrancsinpairs withthecorrespondingcodesnippetstotraincencoderandtencoder.forcomments trancstransformsthemintovectorrepresentations vcusingasharedwordmappingfunction.forcodesnippets theyaredifferentfromnaturallanguageexpressionssuchascomments.inthispaper weaimtobuildahomogeneousrepresentation g9 g29 g21 g22 g0 g15 g28 g24 g30 g30 g22 g33 g32 g9 g29 g27 g27 g22 g28 g33 g32 g11 g28 g32 g33 g31 g34 g20 g33 g24 g29 g28 g32 g5 g8 g16 g16 g31 g18 g28 g32 g26 g18 g33 g24 g29 g28 g32 g14 g34 g22 g31 g37 g5 g9 g2 g18 g1 g0 g9 g29 g28 g33 g22 g36 g33 g4 g18 g35 g18 g31 g22 g0 g9 g29 g21 g22 g0 g16 g31 g18 g28 g32 g26 g18 g33 g24 g29 g28 g2 g19 g1 g0 g12 g29 g21 g22 g26 g0 g16 g31 g18 g24 g28 g24 g28 g23 g11 g28 g30 g34 g33 g13 g34 g33 g30 g34 g33 g5 g10 g5 g6 g9 g1 g5 g7 g9 g1 g2 g16 g29 g30 g4 g25 g0 g9 g29 g21 g22 g0 g15 g28 g24 g30 g30 g22 g33 g32 g5 g9 g2 g4 g3 g1 g9 g20 g10 g14 g17 g14 g17 g13 g1 g18 g12 g1 g9 g20 g10 g17 g6 g8 g4 g8 g4 g9 g5 g8 g0 g24 g20 g29 g28 g32 g33 g17 g5 g6 g8 g0 g24 g32 g33 g29 g31 g22 g17 g7 g38 g17 g18 g19 g9 g10 g28 g20 g29 g21 g22 g31 g9 g10 g28 g20 g29 g21 g22 g31 g9 g21 g15 g2 g5 g3 g1 g7 g11 g19 g15 g18 g22 g16 g11 g17 g21 g1 g18 g12 g1 g9 g20 g10 g17 g6 g8 g16 g10 g28 g20 g29 g21 g22 g31 g18 g19 g3 g10 figure framework of trancs betweencommentsandcodesnippets whichcanbettercapturethe shared semantic information of these two types.
specifically we propose a context aware code translation which translates code snippets into natural language descriptions as shown in the dotted box detailsare discussed in section4.
.
the natural languagede scriptions translated from code snippets are also transformed into vector representations vtusing the same shared word mapping function.
trancs leverages the two vector representations vcand vtfor building two encoders i.e.
cencoder and tencoder that generateembeddingswithpreservedsemanticsforbothcomments and code snippets.
cencoder takes in the comment vector representations vcandproducesconcise embeddingrepresentations ec thatpreservessemanticinformationfromthecomments.tencoder generates embedding representations etfor code snippets.
details of training these two encoders are elaborated in section .
.
when trancsisdeployedforusage ittakesinaqueryfromthedeveloper and passes it to cencoder which produces an embedding eqfor the query.
trancs then compares the query embedding eqwith those code embeddings etfrom the training set.
a top k selection methodisleveragedforprovidingcodesnippetstothedeveloper which are semantically similar to the query.
.
context aware code translation the goal of context aware code translation is to translate code snippets into natural language descriptions according to the predefinedtranslationrules.asshowninthedottedboxoffigure7 this phase consists of two steps.
in step given code snippets trancsutilizesastandardcompileranddisassemblertogenerate their instruction sequences.
in step trancs applies the predefinedtranslationrulestotranslatetheinstructionsequencesintonatural languagedescriptions.
we discuss thetwo steps indetail in the following sections.
.
.
instruction generation.
in this step trancs takes in code snippets and produces their instruction sequences.
in practice for agivencodesnippet trancsfirst utilizesastandardprogramcompiler and disassembler to generate the disassembly representation of the code snippet.
for example trancs integrates javacversion .
.0 144 a compiler and javapversion .
.0 144 a disassembler to generate the disassembly representations for code snippets writteninthejavaprogramminglanguage.forthecodesnippets that can not be compiled the main reason is due to the lack of class methoddefinitionsaroundthem.weusejcoffee tomake authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
code search based on context aware code translation icse may pittsburgh pa usa themcompilable byaddingclass method definitionsaroundthem to complement the missing pieces.
then trancs parses the disassemblyrepresentationandextractstheinstructionsequence.for example figure a shows an instruction sequence which is generated by inputting the code snippet shown in figure a into trancs.
g6 g15 g0 g32 g26 g36 g35 g40 g41 g23 g6 g7 g15 g0 g32 g40 g41 g36 g39 g28 g23 g8 g8 g15 g0 g32 g26 g36 g35 g40 g41 g23 g6 g9 g15 g0 g32 g40 g41 g36 g39 g28 g23 g9 g10 g15 g0 g32 g33 g36 g24 g27 g23 g9 g11 g15 g0 g24 g33 g36 g24 g27 g23 g7 g12 g15 g0 g24 g39 g39 g24 g44 g33 g28 g35 g30 g41 g31 g13 g15 g0 g32 g29 g23 g32 g26 g34 g37 g30 g28 g8 g8 g7 g6 g15 g0 g32 g33 g36 g24 g27 g23 g8 g7 g7 g15 g0 g24 g33 g36 g24 g27 g23 g7 g7 g8 g15 g0 g32 g33 g36 g24 g27 g23 g9 g7 g9 g15 g0 g32 g24 g33 g36 g24 g27 g7 g10 g15 g0 g32 g24 g27 g27 g7 g11 g15 g0 g32 g40 g41 g36 g39 g28 g23 g8 g7 g12 g15 g0 g32 g32 g35 g26 g9 g4 g0 g7 g7 g14 g15 g0 g30 g36 g41 g36 g10 g8 g8 g15 g0 g32 g33 g36 g24 g27 g23 g8 g8 g9 g15 g0 g32 g39 g28 g41 g42 g39 g35 g2 g24 g1 g0 g17 g35 g40 g41 g39 g42 g26 g41 g32 g36 g35 g0 g19 g28 g38 g42 g28 g35 g26 g28 g6 g15 g0 g37 g42 g40 g31 g0 g32 g35 g41 g0 g26 g36 g35 g40 g41 g24 g35 g41 g0 g21 g37 g26 g22 g5 g7 g15 g0 g40 g41 g36 g39 g28 g0 g32 g35 g41 g0 g21 g37 g40 g22 g0 g32 g35 g41 g36 g0 g33 g36 g26 g24 g33 g0 g43 g24 g39 g32 g24 g25 g33 g28 g0 g21 g37 g43 g22 g5 g8 g15 g0 g37 g42 g40 g31 g0 g32 g35 g41 g0 g26 g36 g35 g40 g41 g24 g35 g41 g0 g21 g37 g26 g22 g5 g9 g15 g0 g40 g41 g36 g39 g28 g0 g32 g35 g41 g0 g21 g37 g40 g22 g32 g35 g41 g36 g0 g33 g36 g26 g24 g33 g0 g43 g24 g39 g32 g24 g25 g33 g28 g0 g21 g37 g43 g22 g5 g10 g15 g0 g33 g36 g24 g27 g0 g32 g35 g41 g0 g21 g37 g40 g22 g0 g29 g39 g36 g34 g0 g33 g36 g26 g24 g33 g0 g43 g24 g39 g32 g24 g25 g33 g28 g0 g21 g37 g43 g22 g5 g11 g15 g0 g33 g36 g24 g27 g39 g28 g29 g28 g39 g28 g35 g26 g28 g0 g21 g37 g40 g22 g29 g39 g36 g34 g0 g33 g36 g26 g24 g33 g0 g43 g24 g39 g32 g24 g25 g33 g28 g0 g21 g37 g43 g22 g5 g12 g15 g0 g30 g28 g41 g0 g33 g28 g35 g30 g41 g31 g0 g36 g29 g0 g24 g39 g39 g24 g44 g0 g21 g37 g40 g22 g5 g13 g15 g0 g32 g29 g0 g32 g35 g41 g0 g21 g37 g40 g22 g0 g32 g40 g0 g30 g39 g28 g24 g41 g28 g39 g0 g36 g39 g0 g28 g38 g42 g24 g33 g0 g41 g36 g0 g32 g35 g41 g0 g21 g37 g40 g22 g0 g41 g31 g28 g35 g0 g30 g36 g0 g41 g36 g0 g21 g37 g32 g22 g5 g7 g6 g15 g0 g33 g36 g24 g27 g0 g32 g35 g41 g0 g21 g37 g40 g22 g0 g29 g39 g36 g34 g0 g33 g36 g26 g24 g33 g0 g43 g24 g39 g32 g24 g25 g33 g28 g0 g21 g37 g43 g22 g5 g7 g7 g15 g0 g33 g36 g24 g27 g39 g28 g29 g28 g39 g28 g35 g26 g28 g0 g21 g37 g40 g22 g29 g39 g36 g34 g0 g33 g36 g26 g24 g33 g0 g43 g24 g39 g32 g24 g25 g33 g28 g0 g21 g37 g43 g22 g5 g7 g8 g15 g0 g33 g36 g24 g27 g0 g32 g35 g41 g0 g21 g37 g40 g22 g0 g29 g39 g36 g34 g0 g33 g36 g26 g24 g33 g0 g43 g24 g39 g32 g24 g25 g33 g28 g0 g21 g37 g43 g22 g5 g7 g9 g15 g0 g33 g36 g24 g27 g32 g35 g41 g0 g21 g37 g40 g22 g29 g39 g36 g34 g0 g24 g39 g39 g24 g44 g0 g21 g37 g40 g22 g5 g7 g10 g15 g0 g32 g35 g41 g0 g39 g28 g40 g42 g33 g41 g32 g40 g0 g32 g35 g41 g0 g21 g37 g40 g22 g0 g24 g27 g27 g0 g32 g35 g41 g0 g21 g37 g40 g22 g06 g37 g42 g40 g31 g32 g35 g41 g0 g39 g28 g40 g42 g33 g41 g5 g7 g11 g15 g0 g40 g41 g36 g39 g28 g0 g32 g35 g41 g0 g21 g37 g40 g22 g32 g35 g41 g36 g0 g33 g36 g26 g24 g33 g0 g43 g24 g39 g32 g24 g25 g33 g28 g0 g21 g37 g43 g22 g5 g7 g12 g15 g0 g32 g35 g26 g39 g28 g34 g28 g35 g41 g0 g33 g36 g26 g24 g33 g0 g43 g24 g39 g32 g24 g25 g33 g28 g0 g21 g37 g43 g22 g25 g44 g0 g26 g36 g35 g40 g41 g24 g35 g41 g0 g21 g37 g26 g22 g5 g7 g14 g15 g0 g30 g36 g41 g36 g21 g37 g32 g22 g5 g8 g8 g15 g0 g33 g36 g24 g27 g0 g32 g35 g41 g0 g21 g37 g40 g22 g29 g39 g36 g34 g0 g33 g36 g26 g24 g33 g0 g43 g24 g39 g32 g24 g25 g33 g28 g0 g21 g37 g43 g22 g5 g8 g9 g15 g0 g39 g28 g41 g42 g39 g35 g32 g35 g41 g21 g37 g40 g22 g29 g39 g36 g34 g0 g34 g28 g41 g31 g36 g27 g5 g2 g25 g1 g0 g20 g39 g24 g35 g40 g33 g24 g41 g32 g36 g35 g0 g18 g42 g33 g28 g40 figure an example of instruction sequence and translation rules.
and indicate filling in a constant and variable respectively.
indicates filling a value popped from the operand stack while indicates filling in an in struction index.
in addition to the instruction sequence trancs also extracts the local variable table from the disassembly representation which willbe usedin thesubsequent instructiontranslationprocess.
for example listing1showsanexampleofalocalvariabletable localvariabletable thatpresentsthelocalvariablesinvolvedinthe codesnippet indetail andisgenerated alongwith theinstruction sequenceinfigure8 a .detailsabouttheusageoflocalvariables are introduced in section .
.
.
localvariabletable start length slot name signature this lcalarraysum array i sum i i i listing an example of local variable table .
.
instruction translation.
in this step trancs takes in instructionsequencesandproducestheirnaturallanguagedescriptions.inthissection wefirstintroducethetranslationrulesusedintrancs then introduce the instruction context and finally present how trancs implements context aware instruction translation.
translation rules tr .
trusedintrancsismanuallyconstructed based on the instruction specification.
in practice to constructtr wecollectedalloperationsanddescriptionsofinstructions from themachine instruction specification such asjava virtual machine specification .
an operation is a short naturallanguagedescriptionofaninstruction.forexample theinstruction istore s operation is store intinto local variable.
from this operation we can know the behavior of istoreis to store an intvalue into a local variable.
a description is a long natural language description of an instruction which details theinteraction of the instruction on the local variables and operand stack.
for example istore s description is theindexisanunsignedbytethatmustbeanindexintothelocalvariable array of the current frame.
the valueon the top of the operand stack must be of type int.
it is popped from the operand stack and the value of the local variable at indexis set to value.
from this description we can know that istorefirst pops an int value from the operand stack and then stores the value into the index th position of the local variable array.
if we only use the operation as the translation of the instruction the translation will be inaccurate due to the loss of some important context.
if we only usethedescriptionasthetranslationofinstructions thetranslation will be too long.
however research in the field of natural language processing nlp remindsusthatcapturingthesemanticsoflong textsismoredifficultthanshorttexts .basedontheabove westrivetomaketheinstructiontranslationshortandrelatively accurate.
therefore we use the operation as the basis combing the context specified inthe description to manuallycollate atranslationforeachinstruction.suchatranslationdelicatelybalances shortness and accuracy.
for example the translation we collate for the instruction istoreas follows store int into local variable .
where and denote placeholders that specifies the position wherethecontextwillbefilled anddetailsaboutinstructioncontext arediscussedinsection context aware instruction translation .
for example figure b shows the result of trancs using tr to translate the instruction sequence in figure a .
instruction context.
the context of an instruction consists of constants local variables and data and control dependencies with other instructions.
constants and local variables are directly determined by operands.
as shown in figure a an opcode is followed by zero or more operands.
an operand can be a constant or an indexofalocalvariable oranindexofaninstruction.forexample in figure a the operand following the opcode iconstrepresents a constant while the operand following the opcode istore representstheindexofthelocalvariable sumshowninlisting1 control dependencies between instructions are explicitly passed through the indices of the instruction.
the indices are also directly specifiedbyoperands.forexample theoperand22followingthe opcode if icmpge representstheindexoftheinstruction iload 2 atline22infigure8 a .datadependenciesbetweeninstructions are implicitly passed through the operand stack.
as described in sectiontranslation rules tr withtheguidanceofthedescription wecanknowhoweachinstructioninteractswiththeoperand stack suchaspoppingorpushingdata.iftheinstruction iapops i.e.
uses the data that is pushed onto the operand stack by the instruction ib then we say that iais data dependent on ib.
for example figure a shows the changes of the operand stack as theopcodesequenceinfigure8interactswiththeoperandstack.
the values in the operand stack are the carriers that reflect data authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa weisong sun and chunrong fang et al.
dependencies between instructions.
figure b shows the data and control dependencies between the instructions in figure a .
in this figure nodes represent instructions the labels of nodes are instructions indices thesolidanddashededgesrepresentdataand control dependencies respectively.
g21 g36 g36 g21 g40 g40 g21 g31 g17 g25 g20 g5 g40 g21 g31 g17 g25 g21 g36 g36 g21 g40 g40 g21 g31 g17 g25 g40 g21 g31 g17 g25 g20 g8 g40 g21 g31 g17 g25 g20 g9 g8 g13 g29 g31 g34 g21 g24 g20 g7 g4 g4 g13 g29 g23 g34 g33 g37 g18 g20 g4 g5 g13 g29 g37 g18 g34 g36 g25 g20 g6 g4 g6 g13 g29 g23 g34 g33 g37 g18 g20 g4 g7 g13 g29 g37 g18 g34 g36 g25 g20 g7 g9 g13 g21 g31 g34 g21 g24 g20 g5 g10 g13 g21 g36 g36 g21 g40 g31 g25 g33 g27 g18 g28 g31 g25 g33 g27 g18 g28 g40 g21 g31 g17 g25 g11 g13 g29 g26 g20 g29 g23 g32 g35 g27 g25 g6 g6 g40 g21 g31 g17 g25 g20 g5 g5 g4 g13 g29 g31 g34 g21 g24 g20 g6 g5 g5 g13 g29 g31 g34 g21 g24 g20 g5 g5 g6 g13 g29 g31 g34 g21 g24 g20 g7 g40 g21 g31 g17 g25 g20 g6 g21 g36 g36 g21 g40 g40 g21 g31 g17 g25 g20 g5 g5 g7 g13 g29 g21 g31 g34 g21 g24 g40 g21 g31 g17 g25 g20 g7 g40 g21 g31 g17 g25 g20 g5 g5 g8 g13 g29 g21 g24 g24 g5 g9 g13 g29 g37 g18 g34 g36 g25 g20 g6 g5 g10 g13 g29 g29 g33 g23 g0 g7 g0 g5 g5 g12 g13 g27 g34 g18 g34 g0 g8 g6 g6 g13 g29 g31 g34 g21 g24 g20 g6 g6 g7 g13 g29 g36 g25 g18 g17 g36 g33 g1 g2 g4 g8 g2 g1 g3 g5 g6 g7 g2 g2 g2 g3 g2 g4 g2 g5 g2 g6 g2 g7 g2 g9 g3 g3 g3 g4 g2 g21 g1 g0 g14 g28 g21 g33 g27 g25 g37 g0 g34 g26 g0 g18 g28 g25 g0 g18 g35 g25 g36 g21 g33 g24 g0 g19 g18 g21 g23 g30 g2 g22 g1 g0 g17 g33 g37 g18 g36 g17 g23 g18 g29 g34 g33 g0 g15 g25 g35 g25 g33 g24 g25 g33 g23 g40 g0 g16 g36 g21 g35 g28 figure an example of the changes of the operand stackand instruction dependency graph context aware instruction translation.
the basic idea of context aware instruction translation is to simulate the execution of instructions by statically traversing the instruction sequence fromtoptodown.inthetraversalprocess wecollectthecontext of each instruction which will be used to update the tr based translations of the current or other related instructions.
inthe actualexecutionofinstructions aframeiscreated when thecorrespondingcodesnippetisinvoked .aframecontainsa localvariablearrayandalast in first outstack i.e.
operandstack .
thesizesofthelocalvariablearrayandtheoperandstackaredeterminedatcompile time.thelocalvariablearraystoresalllocal variables used in the instructions.
for example the local variables showninlisting1areusedintheinstructionsequenceshownin figure8 a .theindicesofthelocalvariablearraycorrespondsto that in localvariabletable shown in listing where the slot column presents indices of the local variables.
the names and indices of local variables are determined at compile time but their values are dynamically updated with the execution of the instructions.
the values in the operand stack are also dynamically updated with the execution of the instructions.
as mentioned earlier the contextofaninstructionincludesconstants localvariables dataand control dependencies.
among them constants local variables and controldependenciesarecloselyrelatedtoinstructions operands.
theycanbeeasilydeterminedbytheoperands forconstants or byretrievingtheinstructionsequence forcontroldependencies using the index specified by the operand.
however determining the values of local variables is a challenging task because they are dynamicallyupdatedwiththeexecutionoftheinstruction.analogously thedeterminationofdatadependenciesisachallenging task because they are implicitly passed through the operand stack.
the values in operand stack are also dynamically updated with the execution of the instruction.
therefore we need to know in advance how the instruction interacts with the local variable array e.g.
settingvalue ortheoperandstack e.g.
poppingorpushing data .
in practice we obtain such information from the descriptionofeachinstruction.thedescriptionofeachinstructionhasbeen introduced when we introduced the translation rules earlier.
with the guidance of the description we divide the instructions into the followingfourcategoriesaccordingtowhethertheyinteractwiththe local variable array or the operand stack.
category expressed as is.i nis the instruction only interacts with the operand stack.
iscan be subdivided into the following three types ipu.inthistype theinteractionistopushtheoperandonto the operand stack.
ipo.in this type the interaction is to pop values from the operand stack.
ipou.in this type the interaction is composed of popping values from the operand stack performing the operation andpushingtheresultoftheoperationtotheoperandstack.
category iv.i niv the instruction only interacts with the localvariablearray.theinteractionistoloadthevaluefromthe localvariablearray or storethenewvalueintoit.thistypeofinstructiondoesnotinteractwiththeoperandstack.forexample theinstruction iinc 1onlyinteractswiththelocalvariablespecified by the first operand not with the operand stack.
category3 isv.inisv theinstructioninteractswiththeoperand stackaswellasthelocalvariablearray.forexample theinstruction istore 2 first loads the integer value from the operand stack and then stores the value into a local variable.
category4 io.inio the instructionneitherinteractswith the operandstacknorwiththelocalvariablearray suchastheinstruction gotoandnop.
table shows the categories of instructions.
basedontheaboveclassification trancsusesalgorithm1to perform context aware instruction translation.
trancs takes aninstruction sequence i translation rules tr a local variable array v andthedepthoftheoperandstack d asinputs.
tr v anddhave been introduced earlier.
trancs first initializes an stack with a depth of dto store intermediate results produced during traversing i line .
trancs then traverses ifrom top to down lines2 .foreach i i trancsfirstgeneratesitstranslation tbased on tr line .
then trancs extracts the operands from i line .
the operands are used to update sandtin subsequent processes.
trancs determines i s category according to the predefined categories shown in table .
according to i s category trancs uses different processes to update sandt line .
forexample figure9 a showsanexampleofthechangesoftheoperand stack when trancs traverses the instruction sequenceshown in figure a from top to down.
after traversing all the instructionsin i thealgorithmfinishesandoutputs i stranslations t.forexample figure6showsthetranslationgeneratedbytrancs for the instruction sequence shown in figure a .
.
model training the goal of model training is to train two encoders which will be deployed to support code search service.
this phase consists of twostepsasshowninfigure7.instep giventranslationsand comments trancstransformsthemintovectorrepresentations vc andvtusing a shared word mapping function.
in step trancs leverages vcandvtto train cencoder and tencoder.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
code search based on context aware code translation icse may pittsburgh pa usa table the category of instructions .category is ivisvio type ipuipoipou instructionsaconst null anewarray iconst fconst bipush dconst d fconst f iconst i jsr jsr w lconst l ldc ldc w ldc2 w new sipushareturn if icmpge ireturn athrow dreturn freturn if acmp cond if icmp cond if cond ifnonnull ifnull invokedynamic invokeinterface invokespecial invokestatic invokevirtual ireturn ishl ishr lookupswitch lreturn monitorexit pop pop2 putfield putstatic tableswitchaaload arraylength baload caload d2f d2i d2l dadd daload dcmp op ddiv dmul dneg drem dsub dup dup x1 dup x2 dup2 dup2 x1 dup2 x2 f2d f2i f2l fadd faload fcmp op fdiv fmul fneg frem fsub getfield getstatic i2b i2c i2d i2f i2l i2s iadd iaload iand idiv imul ineg instanceof ior irem isub iushr ixor l2d l2f l2i ladd laload land lcmp ldiv lmul lneg lor lrem lshl lshr lsub lushr multianewarray lxor newarray saload swapiinc wideaastore aload aload n astore astore n bastore castore dastore dload dload n dstore dstore n fastore fload fload n fstore fstore n iastore iload iload n istore istore n lastore lload lload n lstore lstore n sastoregoto checkcast goto w nop ret return algorithm context aware instruction translation input an instruction sequence i translation rules tr a local variable array v the depth of the operand stack d. output instruction translation t s initialize an empty operand stack with a depth of d. for each iinido t generate the tr based translation of ibased on tr operands extract the operands from i ifi iputhen s pushoperands ontos t replace in twithoperands end if ifi ipothen values pop values from sbyoperands t replace in twithvalues end if ifi ipouthen values pop values from sbyoperands t replace in twithvalues new value do operation s pushnew valueontos end if ifi ivthen variable get variable from vbyoperands t replace in twithvariable end if ifi isvthen values pop values from sbyoperands t replace in twithvalues variable get variable from vbyoperands t replace in twithvariable end if iftcontains then t replace in twithoperands end if t t t end for outputt .
.
shared word mapping.
in trancs both translations and comments are natural language sentences.
sentence embedding isgeneratedbasedonwordembedding .wordembedding techniques can map words into fixed length vectors i.e.
embeddings so that similar words are close to each other in the vector space .
awordembeddingtechniquecanbeconsideredawordmapping function which can map a word wiinto a vector representation wi i.e.
wi wi .
as aforementioned both translations and comments are natural language sentences so we design a shared word mappingfunction.toimplementsucha webuildasharedvocabularythatincludestop nfrequentlyappearedwordsintranslations and comments.
we further transform the vector representations ofthewordsintoanembeddingmatrix e rn m wherenisthe size of the vocabulary mis the dimension of word embedding.
the embeddingmatrix e w1 ... wi tisinitializedrandomly and learned in the training process along with the two encoders.basedonthisembeddingmatrix trancscantransformstranslations and comments into the vector representations vcandvt.a simplewayofsentencevectorrepresentationsistoviewitasabag of words and add up all its word vector representations .
.
.
encodertraining.
inthissection wefirstintroducethearchitecture of cencoder and tencoder then present how to jointly train the two encoders.
encoderarchitecture.
asdescribedinsection4.
.
intrancs both translations and comments are natural language sentences.
therefore wecanusethesamesequenceembeddingnetworkto designcommentencoder cencoder andtranslationencoder ten coder insteadofdesigningdifferentembeddingnetworksforthem as the previous dl based cs techniques such as deepcs and mman .
in practice trancs applies the lstm architecture to designcencoderandtencoder.consideratranslation comment sentence s w1 wnscomprising a sequence of nswords trancs first uses the shared word mapping function to produce vectorrepresentations vs.then trancspasses vstotheencoder authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa weisong sun and chunrong fang et al.
i.e.
cencoder or tencoder that generates embeddings es.
the hidden state hs iof thei th word in sis calculated as follows hs i lstm hs i wi where wirepresents the vector of the word wiand comes from the embedding matrix e. in addition trancs uses attention mechanism proposed by bahdanauetal.
toalleviatethelong dependencyprobleminlong text sequences .
the attention weight for each word wiis calculated as follows s i exp f hs i us summationtext.1ns j 1exp f hs j us wheref denotes a linear layer usdenotes the context vector which is a high level representation of all words in s and denotes theinnerprojectof hs iandus.thecontextvector usisrandomly initializedandjointlylearnedduringtraining.then s sfinalembedding representation escan be calculated as follows es ns summationdisplay.
j 1 s i hs i joint training.
now we present how to jointly train the two encoders i.e.
cencoder and tencoder of trancs to transform both translations and comments into a unified vector space with a similarity coordination.
we follow a widely adopted assumption thatifatranslationandacommenthavesimilarsemantics their embeddingrepresentationsshouldbeclosetoeachother .
in other words given a code snippet swhose translation is tand a comment c we want it to predict a high similarity between tandc ifcis a correct comment of s and a little similarity otherwise.
in practice we first translate all code snippets into translations.
then we construct each training instance as a triple angbracketleftt c c angbracketright for eachtranslation tthereisapositivecomment c aground truth commentof s andanegativecomment c anincorrectcommentof s .theincorrectcomment c isselectedrandomlyfromthepoolof all correct comments.
when trained on the set of angbracketleftt c c angbracketrighttriples trancs predicts the cosine similarities of both angbracketleftt c angbracketrightand angbracketleftt c angbracketright pairs and minimizes the ranking loss l summationdisplay.
angbracketleftt c c angbracketright gmax cos t c cos t c where denotes the model parameters gdenotes the training dataset isasmallandfixedmarginconstraint t c andc are theembeddedvectors of t c andc respectively.
intuitively the ranking loss encourages the cosine similarity between a translationanditscorrectcommenttogoup andthecosinesimilarities between a translation and incorrect comments to go down.
.
deployment of trancs afterthetwoencoders i.e.
cencoderandtencoder aretrained wecandeploytrancsonlineforcodesearchservice.figure7 showsthedeploymentoftrancs.forasearchquery qgivenbythe developer trancs first uses the shared word mapping function to transformitintovectorrepresentation vq.trancsfurtherpasses vqinto cencoder to generate the embedding eq.
then trancsmeasuresthesimilaritybetween eqandeach et et.thesimilarity is calculated as follows sim q t cos eq et eq et bardbleq bardbl bardblet bardbl trancsranksall tbytheirsimilaritieswith q.thehigherthesimilarity thehigherrelevanceofthecodesnippetto q.finally trancs outputs the code snippets corresponding to the top ktranslations to the developer.
evaluation and analysis we conduct experiments to answer the following questions rq1.whatistheeffectivenessoftrancswhencomparedwith state of the arttechniques?
rq2.what is the contribution of key components in trancs i.e.
context aware code translation and shared word mapping?
rq3.what is the robustness of trancs when varying the query length and code length?
.
experimental setup .
.
dataset.
weevaluatetheperformanceofourtrancsona corpusofjavacodesnippets collectedfromthepubliccodesearch netcorpus .actually wehaveconsideredthedatasetreleasedby baselines i.e.
deepcs andmman .however the dataset ofdeepcsonlycontainsthecleanedjavacodesnippetswithoutthe raw data unable to generate the cfg for mman.
and the dataset of mman is not publicly accessible.
werandomlyshufflethedatasetandsplititintotwoparts i.e.
324samplesfortrainingand1 000samplesfortesting.itisworth mentioning adifference betweenour dataprocessing andthe one in .in theproposedapproachisverifiedonanotherisolated datasettoavoidthebias.sincetheevaluationdatasetdoesnothave thegroundtruth theymanuallylabelledthesearchedresults.as possible subjective bias exists in manual evaluation in this paper we also adopt the automatic evaluation.
figure a and b showthelengthdistributionsofcodesnippetsandcommentson the training set.
for a code snippet its length refers to the number of lines of the code snippet.
for a comment its length refers tothe number of words in the comment.
from figure a we can observethatthelinesofmostcodesnippetsarelocatedbetween20to40.thiswasalsoobservedinthequotein functionsshould hardlyeverbe20lineslong .fromfigure10 b itisnoticedthat almost all comments are less than in length.
this also confirms the challenge of capturing the correlation between short text with itscorrespondingcodesnippet.figure10 c and d showthelength distributionsofcodesnippetsandcommentsontestingdata.we canobservethat despiteshufflingrandomly thedistributionsofdatasizes i.e.
lengths inthetwodatasetsareconsistent sowe can conclude that the testing set is representative.
.
.
evaluation metrics.
in the evaluation we consider the comment of the code snippet as the query and the code snippet it self as the ground truth result of code search which is similarto but different from .
during the testing time we treat each comment in the testing samples as a query the codesnippet correspondingto thequeryas thecorrectresult and authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
code search based on context aware code translation icse may pittsburgh pa usa 1600200040006000800010000count a code snippets on training set0 00200040006000800010000count b comments on training set 160050100150200count c code snippets on testing set0 0050100150200count d comments on testing set figure length distributions the other code snippets as distractor results.
we adopt two automaticevaluationmetricsthatarewidelyusedincodesearch studies to measure the performance of trancs i.e.
successrateat k successrate k andmeanreciprocalrank mrr .
successrate k measures the percentage of queries for which the correct result exists in the top kranked results which is computed as follows successrate k q q summationdisplay.
i 1 frankqi k whereqdenotes a set of queries and q is the size of q denotesafunctionwhichreturns1iftheinputistrueandreturns0 otherwise frankqireferstotherankpositionofthecorrectresult forthei thqueryin q.successrate k isimportantbecauseabetter cs technique should allow developers to discover the expected code snippets by inspecting fewer returned results.
the higher the successrate k value the better the code search performance.
mrris the average of the reciprocal ranks of results of a set of queriesq .
the reciprocal rankof a query is the inverse of the rank of the correct result.
mrris computed as follows mrr q q summationdisplay.
i frankqi the higher the mrrvalue the better the code search performance.
meanwhile as developers prefer to find the expected code snippetswithshortinspection weonlytest successrate k andmrr on the top that is the maximum value of kis ranked list following deepcs and mman .
in other words when the rank ofqiis out of then frank qiis set to .
.
.
baselines.
inthispaper wecomparethefollowingbaselines deepcs .deepcsisoneoftherepresentativedl based cstechniques.deepcsusestwokindsofmodelarchitecture to design the code encoder to embed three aspects of the code snippet i.e.
two rnns for method names and apisequences and a multi layer perceptron mlp for tokens.
its query encoder also uses rnn architecture.
mman .mmanisoneofthestate of the artdl based cstechniques.mmanusesmultiplekindsofmodelarchitecturestodesign thecodeencoderto embedmultipleaspects ofthe codesnippet i.e.
one lstmfortoken atree lstm for ast and a ggnn for cfg.
its query encode uses lstm architecture.
.
.
implementation details.
to train our model we first shuffle the trainingdata and setthe mini batch sizeto .the size ofthe vocabulary is .
for each batch the code snippet is padded with a special token angbracketleftpad angbracketrightto the maximum length.
we set the word embedding size to .
for lstm unit we set the hidden size to .
the margin is set to .
.
we update the parameters via adamw optimizer with the learning rate .
.
to prevent over fitting we use dropout with .
.
in trancs the comment and the code snippet share the same embedding weights.
all models areimplementedusing thepytorch .
.1frameworkwithpython .
.
allexperiments are conductedon aserver equipped withone nvidia tesla v100 gpu with gb memory running on centos .
.
all the models in this paper are trained for epochs and we select the best model based on the lowest validation loss.
.
evaluation results in this section we present and analyze the experimental results to answer the research questions.
.
.
rq1 effectiveness of trancs.
table shows the overall performance of trancs and two baselines measured in terms of successrate k andmrr.
the columns sr sr 5andsr show the results of the average successrate k over all queries whenkis and respectively.
the column mrrshows the mrr valuesofthethreetechniques.fromthistable wecanobservethat forsr k the improvements of trancs to deepcs are .
.
and32.
when kis1 and10 respectively.theimprovementstommanare67.
.
and25.
respectively.for mrr the improvements trancs to deepcs and mman are .
and49.
respectively.wecandrawtheconclusionthatunder all experimental settings our trancs consistently achieves higher performanceintermsofbothtwometrics whichindicatesbetter code search performance.
table overall performance of trancs and baselines tech sr 1s r 5s r mrr deepcs .
.
.
.
mman .
.
.
.
trancs .
.
.
.
the codesearchnet corpus also provides realistic natural languagesqueriesandexpertannotationsforlikelyresults.each query result pair was labeled by a human expert indicating the relevance of the result for the query.
we also conduct experiments on99 queriesprovidedbythe codesearchnetcorpusfor thejava authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa weisong sun and chunrong fang et al.
programminglanguage.weusethesamemetric normalizeddiscounted cumulative gain ndcg to evaluate baselines and trancs.ourtrancsachievesndcgof0.
outperformingdeepcs .
and mman .
by and respectively.
table contribution of key components in trancs tech sr 1s r 5s r mrr tokecs .
.
.
.
trancs cct .
.
.
.
tokecs swm .
.
.
.
deepcs swm .
.
.
.
trancs cct swm .
.
.
.
.
.
rq2 contributionofkey components.
we experimentally verified the effectiveness of two key components of trancs i.e.
context awarecodetranslation cct andsharedwordmapping swm .
in table trancs cct and trancs cct swm are two specialversionsoftrancs amongwhichtheformerusestwodifferentwordmappingfunctionstotransforminstructiontranslations andcommentstovectorrepresentations whilethelatterusesswm.
in other words if it is only cct trancs uses two vocabularies.
in the case of cct swm trancs uses a shared vocabulary.
moreover numerous existing studies including deepcs and mman have shown that tokens of code snippets play a keyroleincodesearchtasks.therefore weassumethatthisisa scenariowherethecodesnippetisnottranslated andwedirectly pass the tokens of the code snippet into the model to train the codeencoder.theeffectivenessofthetoken basedcstechnique tokecs is shown in the second line of table .
to demonstrate theeffectivenessofswm wealsotriedtoapplyswmtotokecs deepcsandmman.toapplyswmtotokecs weuseaunified word mapping function to transform tokens and comments.
in deepcs theauthorusesfourwordmappingfunctionstotransform themn apis tokenandcommentsintovectorrepresentations.to apply swm to deepcs we first merge the four vocabularies into a shared vocabulary by extracting the union of them.
then we use a unified word mapping function to transform mn apis token and comments.
in mman the author not only uses lstm architecture to embed tokens but also uses tree lstm and ggnn to embed astandcfg whilethethreearchitecturescannotshareaword mappingfunction.therefore swmcannotbeappliedtomman.
theeffectivenessoftoke swm deepcs swm areshowninlines 5oftable3.fromthelines2 3oftable3 wecanobservethat forsr k theimprovementsoftrancs cct totokecsare42.
.
and .
when kis and respectively.
for mrr the improvement to tokecs is .
.
therefore we can conclude thatcctcontributestotrancs.for sr k theimprovementsof trancs cct swm totrancs cct are59.
.
and24.
.
formrr the improvement of trancs cct swm to trancs cct is .
.
therefore we can conclude that swm contributes to trancs.
besides we can also observe that swm also has slight improvements to tokecs and deepcs.
therefore we can draw the conclusionthatswmandcct whichpromoteeachother improve the performance of trancs jointly.
.
.
rq3 robustness of trancs.
to analyze the robustness of trancs we studied two parameters i.e.
code length and comment length thatmayhaveanimpactontheembeddingrepresentations of translations and comments.
figure shows the performance of trancs based on different evaluation metrics with varying parameters.fromfigure11 wecanobservethatinmostcases trancs maintainsastableperformanceeventhoughthecodesnippetlength orcommentlengthincreases whichcanbeattributedtocontextaware code translation and shared word mapping we proposed.
when the length of the code snippet exceeds a common range describedinsection5.
.
theperformanceoftrancsdecreases as the length increases.
it means that when the length of the code snippets or comments exceeds the common range as the lengthcontinues to increase it will be more difficult to capture their semantics.overall theresultsverifytherobustnessofourtrancs.
.
.
.
.
.
.0scoresr sr sr mrr a varying code snippet lengths0 .
.
.
.
.
.0scoresr sr sr mrr b varying comment lengths figure robustness of trancs case study g6 g39 g33 g28 g24 g0 g36 g40 g21 g34 g16 g30 g25 g31 g25 g32 g37 g17 g32 g18 g28 g36 g37 g2 g18 g28 g36 g37 g12 g17 g32 g37 g25 g26 g25 g35 g14 g0 g30 g28 g36 g37 g4 g0 g28 g32 g37 g0 g28 g4 g0 g28 g32 g37 g0 g29 g1 g0 g42 g7 g28 g32 g37 g0 g25 g30 g25 g31 g25 g32 g37 g0 g13 g0 g30 g28 g36 g37 g5 g26 g25 g37 g2 g28 g1 g11 g8 g30 g28 g36 g37 g5 g36 g25 g37 g2 g28 g4 g0 g30 g28 g36 g37 g5 g26 g25 g37 g2 g29 g1 g1 g11 g9 g30 g28 g36 g37 g5 g36 g25 g37 g2 g29 g4 g0 g25 g30 g25 g31 g25 g32 g37 g1 g11 g10 g41 g2 g22 g1 g0 g15 g33 g24 g25 g0 g20 g32 g28 g34 g34 g25 g37 g0 g1 g1 g2 g23 g1 g0 g15 g33 g24 g25 g0 g20 g32 g28 g34 g34 g25 g37 g0 g1 g2 g36 g40 g21 g34 g0 g37 g40 g33 g0 g25 g30 g25 g31 g25 g32 g37 g36 g0 g28 g32 g0 g37 g27 g25 g0 g30 g28 g36 g37 g2 g21 g1 g0 g19 g38 g25 g35 g40 g0 g1 g6 g39 g33 g28 g24 g0 g36 g40 g21 g34 g16 g30 g25 g31 g25 g32 g37 g17 g32 g18 g28 g36 g37 g2 g18 g28 g36 g37 g12 g17 g32 g37 g25 g26 g25 g35 g14 g0 g30 g28 g36 g37 g4 g0 g28 g32 g37 g0 g28 g4 g0 g28 g32 g37 g0 g29 g1 g0 g42 g7 g15 g33 g30 g30 g25 g23 g37 g28 g33 g32 g36 g5 g36 g40 g21 g34 g2 g30 g28 g36 g37 g4 g0 g28 g4 g0 g29 g1 g11 g8 g41 figure12 exampleoftwocodesnippetsimplementingthe same functionality this isa case to studythe performance of trancsin retrieving codewithimplantationdifference.figure12 b and c showtwo codesnippetsthatimplementthesamefunctionality i.e.
swapping authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
code search based on context aware code translation icse may pittsburgh pa usa twoelementsinthelist.thefirstone s1 implementsthefunctionality from scratch and the second one s2 directly calls the external apicollection.swap .
we use trancs to convert the two code snippets into corresponding translations which are very different meaningtrancscaneffectivelydifferentiatesemanticallysimilar code but differs in apis used.
this is because trancs reserves api information e.g.
name parameter when generating code translation.forexample asshowninfigure13 thetranslationsproduced by trancs reserve the information of the api collection.swap invoked by s2 including the parameters e.g.
list and the method nameswap.
g3 g8 g0 g19 g22 g9 g12 g0 g23 g13 g14 g13 g23 g13 g21 g11 g13 g0 g19 g16 g24 g25 g0 g14 g23 g22 g20 g0 g19 g22 g11 g9 g19 g0 g27 g9 g23 g16 g9 g10 g19 g13 g0 g7 g5 g11 g12 g2 g4 g8 g0 g19 g22 g9 g12 g0 g16 g21 g25 g0 g16 g0 g14 g23 g22 g20 g0 g19 g22 g11 g9 g19 g0 g27 g9 g23 g16 g9 g10 g19 g13 g0 g5 g2 g5 g8 g0 g19 g22 g9 g12 g0 g16 g21 g25 g0 g17 g0 g14 g23 g22 g20 g0 g19 g22 g11 g9 g19 g0 g27 g9 g23 g16 g9 g10 g19 g13 g0 g6 g2 g6 g8 g0 g16 g21 g27 g22 g18 g13 g0 g11 g19 g9 g24 g24 g0 g1 g9 g7 g7 g4 g3 g12 g5 g9 g8 g11 g24 g25 g9 g25 g16 g11 g0 g20 g13 g25 g15 g22 g12 g0 g11 g13 g2 g10 g2 g7 g8 g0 g23 g13 g25 g26 g23 g21 g0 g27 g22 g16 g12 g0 g14 g23 g22 g20 g0 g20 g13 g25 g15 g22 g12 g2 figure translations of the code snippet s2 threats to validity the metrics used in this paper are successrate k andmrrfor evaluating the effectiveness of trancs and existing techniques.
these are the same metrics adopted in mman .
we do not use another metric precision k that measures the percentage of relevant results in the top kreturned results for each query .
this is due to the constraint that the relevant results need to be labelled manually whichis empiricallylessfeasibleandcanintroducehuman biases.
we hence focus on the two metrics successrate k andmrrin the paper.
trancsiscurrentlyonlyevaluatedonjavaprogramsandmayrequiremodificationsforextendingtootherprogramminglanguages.
thecorecontributionoftrancsisthecontext awarecodetranslation technique.
to realize the context aware code translation trancsrequiresasetoftranslationrules suchastheoperations anddescriptionsofinstructions.inordertoextendtrancstoother programminglanguages correspondingtranslationrulesneedto be designed and provided.
we plan to evaluate the performance of trancs on these programming languages in future work.
related work earlycstechniqueswerebasedonirtechnology suchas .thesetechniquessimplyconsiderqueriesandcodesnippets asplaintextandthenusekeywordmatching.toalleviatetheproblemofkeywordmismatch andnoisykeywords many queryreformulation qr basedcstechniques have been proposed one after another.
for example the words fromwordnet orstackoverflow areusedtoexpanduser queries.however qr basedcstechniquesconsidereachwordindependently whileignoringthecontextoftheword.inaddition bothir basedandqr basedcstechniquesonlytreatthecodesnippetas plain text and cannot capture the deep semantics of the code snippet.tobettercapturethesemanticsofqueriesandcodesnippets deeplearning dl basedcstechniques have been proposed one after another.
gu et al.
first apply dl to the code search task.
they first encode both the query and aset of code snippets into corresponding embeddings using mlp orrnn andthenrankthecodesnippetsaccordingtothecosine similarityofembeddings.otherdl basedcstechniquesaresimilar to deepcs with onlya difference in choosingthe embedding architecture.forexample tocapturethesemanticsofotheraspects of the code snippet mman integrates multiple embedding networks i.e.
lstm tree lstm and ggnn to capture semantics of multiple aspects such as token ast and cfg.
codebert coacor andbaselines in codesearchnetchallenge only treatthecodesnippetasplaintext tokensequence whichmiss richerinformationsuchasapis ast andcfg etc.tbcnn is atree basedconvolutionalneuralnetworkthatencodestheastof the code snippet.
our baseline mman has encoded ast usingtree basedneuralnetworksandisinferiortoourtrancs.all these works have a similar idea that first transforms both codesnippets and queries into embedding representations into a uni fiedembeddingspacewithtwoencoders andthenmeasuresthecosine similarity of these embedding representations.
however trancs differs from previous work in two major dimensions trancsfirsttranslatesthecodesnippetintosemantic preserving naturallanguagedescriptions.inthiscase thegeneratedtranslationsandcommentsarehomogeneous.
basedoncodetranslation trancsnaturallyusesasharedwordmappingmechanism which can produce consistent embeddings for the same words therebybetter capturing the shared semantic information of translations and comments.
conclusion in this paper we propose a context aware code translation tech nique which can translate code snippets into natural language descriptionswithpreservedsemantics.inaddition weproposea shared word mapping mechanism to produce consistent embeddingsforthesamewords tokensincommentsandcodesnippets so as to capture the shared semantic information.
on the basisofcontext awarecodetranslationandsharedwordmapping we implementanovelcodesearchtechniquetrancs.weconductcom prehensiveexperimentstoevaluatetheeffectivenessoftrancs andexperimentalresultsshowthattrancsisaneffectivecstechnique and substantially outperforms the state of the art techniques.
infuturework wewillfurtherexplorethefollowingtwodimensions as shown in figure10 statistical results on large scale data sets show that most code snippets have no more than lines.
within this range trancs is robust and stable.
constructing representationsoflongcodesnippetsisstillanopenproblem andweleave it to future work.
lstm encoder is just a component of trancs which can be easily replaced with more advanced including pre trained models in .
we will explore more advanced models in future work.
acknowledgement the authors would like to thank the anonymous reviewers for insightful comments.
this work is supported partially by national natural science foundation of china .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa weisong sun and chunrong fang et al.