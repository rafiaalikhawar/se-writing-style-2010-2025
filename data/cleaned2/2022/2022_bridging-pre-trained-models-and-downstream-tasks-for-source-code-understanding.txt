bridging pre trained models and downstream tasks for source code understanding deze wang national university of defense technology china wangdeze14 nudt.edu.cnzhouyang jia national university of defense technology china jiazhouyang nudt.edu.cnshanshan li national university of defense technology china shanshanli nudt.edu.cn yue yu national university of defense technology china yuyue nudt.edu.cnyun xiong fudan university shanghai china yunx fudan.edu.cnwei dong national university of defense technology china wdong nudt.edu.cn xiangke liao national university of defense technology china xkliao nudt.edu.cn abstract with the great success of pre trained models the pretrain thenfinetune paradigm has been widely adopted on downstream tasks for source code understanding.
however compared to costly training a large scale model from scratch how to effectively adapt pretrained models to a new task has not been fully explored.
in this paper weproposeanapproachtobridgepre trainedmodelsand code related tasks.
we exploit semantic preserving transformation to enrich downstream datadiversity and help pre trained models learn semantic features invariant to these semantically equivalent transformations.further weintroducecurriculumlearningtoorganizethetransformeddatainaneasy to hardmannertofine tune existing pre trained models.
we apply our approach to a range of pre trained models and they significantly outperform the state of the art models on tasks for source code understanding such as algorithm classification code clone detection and code search.
our experiments even show thatwithoutheavypre trainingoncodedata naturallanguagepretrained model roberta fine tuned with our lightweight approach could outperform or rival existing code pre trained models finetuned on the above tasks such as codebert and graphcodebert.
thisfindingsuggeststhatthereisstillmuchroomforimprovement in code pre trained models.
zhouyang jia and shanshan li are the corresponding authors.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation onthe firstpage.copyrights forcomponentsof thisworkowned byothersthan the author s mustbehonored.abstractingwithcreditispermitted.tocopyotherwise or republish topostonserversortoredistributetolists requirespriorspecificpermission and or a fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn ... .
concepts computing methodologies supervised learning artificial intelligence .
keywords fine tuning dataaugmentation curriculumlearning test timeaugmentation acm reference format deze wang zhouyang jia shanshan li yue yu yun xiong wei dong and xiangke liao.
.
bridging pre trained models and downstream tasksforsourcecodeunderstanding.in 44thinternationalconferenceon software engineering icse may pittsburgh pa usa.
acm new york ny usa pages.
introduction large scale models such as bert roberta gpt t5 and bart have greatly contributed to the development of the field of natural language processing nlp and gradually form the pretrain then finetune paradigm.
the basic idea of thisparadigmis tofirstpre trainamodelon largegeneral purpose datasetsbyself supervisedtasks e.g.
maskingtokensintraining dataandaskingthemodeltoguessthemaskedtokens.thetrained model is then fine tuned on smaller and more specialized datasets eachdesignedtosupportaspecifictask.thesuccessofpre trained modelsinthenaturallanguagedomainhasalsospawnedaseries of pre trained models for programming language understanding andgeneration includingcodebert graphcodebert plbart andtheusageoft5tosupportcode relatedtasks improving the performance of a variety of source code understanding and generation tasks.
however pre training a large scale model from scratch is costly.
additionally along with an increasing number of pre trained models howtoeffectivelyadaptthesemodelsforanewtaskisnotfully exploited.
in this paper we try to take the first step to bridge large pre trained models and code related downstream tasks.
moreover despite the success of existing pre trained models for code related ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa deze wang zhouyang jia shanshan li yue yu yun xiong wei dong and xiangke liao tasks thesemodelshavetwopotentialissues.first thesemodels graftnlppre trainingtechniquestounderstandthesemanticsof sourcecode h owever the semanticsofprogramminglanguageand naturallanguageareessentiallydifferent andsemanticallyequivalentsourcecodemaybeinvarioussyntacticforms.thesecond issueisthatpre trainedmodelstypicallyhaveatleastmillionsof parameters so when a pre trained model is applied to downstream taskswithspecializeddatasets thereisariskofoverfittingbecause the model is over parameterized for the target dataset.
many studieshavealsofoundthatwhenthetestsetisdifferentfromtheactualsceneorthetestsetisslightlyperturbed variousmodelsforsource code would make mistakes .
toaddresstheaboveissues wedesignalightweightapproach on top of the existing pre trained language model fine tuning paradigm that satisfies extracting code semantic knowledge embedded in diverse syntactic forms and complementing it to pre trainedmodels reducingoverfittingtothetargetdatasetandbeingmorerobustintesting.inordertoincorporatesemanticknowledgeofthe programminglanguagesintomodels weemploydataaugmentation which is mainly used to enrich the training dataset and make it as diverse as possible.
there are many successful applications of dataaugmentationinthefieldofimageprocessing suchasrandom cropping flipping and dropout .
for code data this paperconsiders semantic preservingtransformation.anexample ofcodetransformationisshowninfig.
wherethesameprogram is transformed three times successively keeping the semantics unchanged.sincethesemanticsoftheoriginalprogramarepreserved it is logical that the model should have the same behavior as the original program for the program generated by the transformation techniques.
moreover it is cheap to leverage a source to source compiler toperformsemantic preservingtransformationson source code.
in this paper we build our approach on a series of large scale pre trained models including natural language pre trained model roberta and code pre trained models codebert and graphcodebert to bridge pre trained models with downstream tasks forsource code.
we first construct semantic preserving transformationsequencesandapplythemtooriginaltrainingsamples asin fig.
togeneratenewtrainingdataandintroducecodesemantic knowledge into models.
the transformation sequences make code transformationsmorecomplicatedandcouldguidemodelstobetterlearn the underlying semantics of the code.
these training data are then fed to pre trained models to fine tune the models.
finally we augmentthetestsetswiththesameaugmentationtechniquesas thetrainingsetstoobtainmultipletransformedtestsets.tofurther reduce overfitting from the training process we average the model performance on these test sets.
since our method averages the predictionsfromvarioustransformationversionsforanycodesnippet intestsets thefinalpredictionsarerobusttoanytransformation copy.
the transformed data significantly increase the data diversity however they can also be considered as adversarial examples comparedtotheoriginaldata .fig.1showstheoriginalprogram and programs after multiple code transformations.
as the number oftransformationsincreases newtokensandsyntacticformsare introduced andthedistributionoftransformeddatabecomesmore distinct from that of original data making it more difficult to learn.to solve this issue we introduce curriculum learning cl and present training examples in an easy to hard manner instead ofacompletelyrandomorderduringtraining.manystudieshave shown that it benefits the learning process not only for humans butalsoformachines .thekeychallengeofclishowto defineeasyandhardsamples andinthispaperweproposetwohy pothesesandexperimentallyverifythemtodeterminethelearning order.
inourexperiments basedonpre trainedmodelscodebertand graphcodebert ourmethodsignificantlysurpassesthestate ofthe art performance on algorithm classification code clone detection and code search tasks.
in the algorithm classification task our approach improves .
mean average percision map compared to the state of the art performance and in the code clone de tectiontask usingonly10 oftherandomlysampledtrainingdata codepre trainedmodelcodebertfine tunedwithourapproach outperforms the state of the art model graphcodebert normally fine tunedwithalltrainingdata.inthecodesearchtask ourmethodimprovesthestate of the artperformanceto0.720meanreciprocalrank mrr .moreimpressively totestwhetherourapproachintroduces additional semantic knowledge of source code for the model we apply our approach to natural language pre trained model roberta and find that it even outperforms codebert with .
map on algorithm classification task and roberta pre trained with code on code search task and has the same performance as codebertoncodeclonedetectiontask.thedata pre trainedmodels andimplementationofour approach are publiclyavailable at the link the main contributions of our paper are as follows we design a lightweight approach on top of the existing pre trained language model fine tuning paradigm to bridge pre trainedmodelsanddownstreamtasksforsourcecode.
to the best of our knowledge it is the first work in this direction.
we apply our method to pre trained models codebert and graphcodebert andtheaugmentedmodelsdramatically outperformthestate of the artperformanceonalgorithm classification code clone detection and code search tasks.
our study reveals that for code related tasks without the needforheavypre trainingoncodedata naturallanguage models e.g.
roberta easilyoutperformthesamemodels pre trained with code as well as the state of the art codepre trained models e.g.
codebert with the help of our approach.
preliminaries and hypotheses .
data augmentation dataaugmentation da isatechniquetocreatenewtrainingdata fromexistingtraining dataartificially.transformationswererandomly applied to increase the diversity of the training set.
data augmentation is often performed with image data where copiesof images in the training set are created with some image transformationtechniquesperformed suchaszooms flips shifts and more.
in fact data augmentation can also be applied to natural languageandcodedata.inthispaper ourpurposeofintroducingdata augmentation is to learn code semantics from semantic preserving authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
bridging pre trained models and downstream tasks for source code understanding icse may pittsburgh pa usa int maxn secmaxn for int count count size count if numbers maxn secmaxn maxn maxn numbers else if numbers secmaxn if numbers !
maxn secmaxn numbers cout maxn secmaxn endl int maxn secmaxn int count maxn secmaxn for int count count size count while count size if numbers maxn secmaxn maxn maxn numbers else if numbers secmaxn else if numbers secmaxn if numbers !
maxn numbers !
maxn secmaxn numbers count cout maxn secmaxn endl int count maxn secmaxn int nonsense temp while count size if numbers maxn secmaxn maxn maxn numbers else if numbers secmaxn numbers !
maxn secmaxn numbers count cout maxn secmaxn endl printf d d n maxn secmaxn int count maxn secmaxn int nonsense temp while count size if numbers maxn secmaxn maxn maxn numbers else if numbers secmaxn numbers !
maxn numbers !
maxn secmaxn numbers count cout maxn secmaxn endl original program transformation program transformation program transformation program figure an example of code transformation.
k transformation program represents the result of the original program after k transformations.all four programs implement the function to find the maximum and second largest values in the array.
transformation more specifically to assist models in extracting and learning features in a way that are invariant to semantically equivalent declarations apis control structures and so on.
inthispaper weexploitdataaugmentationnotonlyforthetrainingsetbutalsoforthetestset.theapplicationofdataaugmentationtothetestsetiscalledtest timeaugmentation tta .
specifically itcreatesmultipleaugmentedcopiesofeachsample in the test set has the model make a prediction for each and then returns an ensemble of those predictions.
the number of copies of thegivendataforwhichamodelmustmakeapredictionisoften small.inourexperiment werandomlysamplethreesamplesfor eachpieceofdatafromtheiraugmentedcopies taketheaveragere sultsastheresultoftheaugmentedperspectiveandaddtheresults on the original dataset as the final results.
.
curriculum learning the learning process of humans and animals generally follows the order of easy to difficult and cl draws on this idea.
bengio et al.
propose cl for the first time imitating the process of human learning andadvocatethatthemodelshouldstartlearningfrom easy samples and gradually expand to complex samples.
in recent years cl strategies have been widely used in various scenariossuch as computer vision and natural language processing.
it has shownpowerfulbenefitsinimprovingthegeneralizationabilityand accelerating convergence of various models .
at the same time it is also easy to use since it is a flexible plug and play submodule independent of original training algorithms.
there are two key points of cl one is the scoring function and the other is the pacing function.
the scoring function makes it possible to sort the training examples by difficulty and present to the network the easier samples first.
the pacing function determines the pace by which data is presented to the model.
the main challenge is how to obtain an effective scoring function without additional labelling of the data.
.
hypotheses we formulate two hypotheses about the scoring functions to determine the order of learning and conduct experiments to verify them.many studies have shown that deep models for source code are vulnerabletoadversarialexamples .slightperturbations to the input programs could cause the model to make false predictions.therefore itisnaturalforustoformulatethefirsthypothesis thatthe augmented data are more challenging to learn than the original data for general models.
we design an experiment toverifythishypothesisdirectly asshowninalgorithm1.itshows the pseudocode to verify the impact of code transformation bycomparing the performance of the model on a range of trainingset variants.
the training set variants are generated by iterating the transformation functions on the original training set.
line afterthemodelistrainedonthewholetrainingsetincludingall trainingsetvariants weevaluatethemodelondifferenttraining set variants.
we apply algorithm to the state of the art model codebert withbenchmarkdatasetpoj104 willbeexplainedin4.
.fig.
shows the performance of codebert for these training set vari ants.
the model performs best on the original training set.
the performancegetsprogressivelyworseasthenumberoftransformationsontheoriginaldatasetincreases whichverifiesthatdata augmentation would increase the difficulty of the training set and experimentally supports our hypothesis.
.
.
.
.
.
original trans trans trans transprecision r figure the performance of codebert on both original and augmented training sets for poj104 dataset.
astheaugmenteddataaremoredifficulttolearn itisnaturalto letthemodellearntheoriginaldatafirstandthentheaugmented data from easy to hard.
we detail our curriculum learning strategy based on this hypothesis in the next section.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa deze wang zhouyang jia shanshan li yue yu yun xiong wei dong and xiangke liao algorithm validation algorithm for hypothesis input trainingset d transformationfunctions t1 ... t k model m d a set of training set variants a set of experimental results d prime d fortransformation t ...kdo d prime tt d prime d prime end for train model mwith the whole training set fordatasetxin do calculate results on the model m m x m x end for return the second hypothesis we propose is to solve the multiclass classificationtask.imageclassification textclassificationlikenews andalgorithmclassificationareallclassicalmulticlassclassification tasks.
the task is quite difficult and a common simplification isto split the multiclass classification task into easily solvable subtasks withfewer classes.hence weformulate thehypothesis that forthemulticlassclassificationtask itismoreeffectiveto determine the learning order of the model from a class perspective.
based on this hypothesis the optimization goal of the model gradually transitions from a classification problem with few classes to a classification of multiple classes during the entire training process.
intuitively the task is much easier to solve under this settingcomparedtoastraightforwardsolution.wenextconduct an experiment to verify the hypothesis.
the difficulty of code data may be reflected in the length of the code the use of rare tokens the complexity of logic etc.
although these heuristics are reasonable for people they are not necessarily thecaseformodels.therefore unlikethepreviousvalidationexperimentthatusescodeaugmentationtechniquestodistinguishthe difficultyof thesamples artificially we letthemodel itselfgive an evaluationofthedataasthedifficultyscores asshowninalgorithm .
the purpose of algorithm is to get the average difficulty score of each class on the training set.
to get the difficulty score of each sampleonthetrainingset weapplytheleave one outstrategy i.e.
whenwecomputethedifficultyscoresforapartofthesamples we train the model with all the other data.
line then we compute the average difficulty scores on each class.
line to have a comparison with the learning order under the first hypothesis wealsoapplyalgorithm2tothestate of the artmodel codebert with poj104 dataset.
poj104 dataset contains many classes andthetaskofpoj104datasetistopredicttheclassfora given program.
we apply algorithm to both the original training set and the augmented training set.
we sort their average difficultyscores of each class according to the scores on the original training set as shown in fig.
.
.
.
.
.
.
.
.
.
0precision r class id results on original dataset results on augmented dataset figure visualization of average performance across training classes for poj104 dataset by codebert.
algorithm validation algorithm for hypothesis input training set d the entire dataset s modelm c a set of difficulty scores a set of average difficulty scores on classes split training set duniformly as di i ...n fori ...ndo calculate the difference set of diovers s di train model mwiths diand get model mi evaluate diwithmiand obtainthe experimental results of each sample as the difficulty score set ci c c ci end for train model mwith training set d forclassxinddo calculate average difficulty scores on class x c x c x end for return fromfig.3itcanbefoundthattheperformanceofthemodel on various classes varies greatly.
the experimental performance reflects the difficulty of classes the better the experimental performance thelowerthedifficulty andviceversa.also wefindthat the performance on the augmented dataset is almost always lower than that on the original dataset further validating our previous hypothesis.
at the same time fig.
shows that the performance of themodelontheaugmenteddataset althoughdecreasing isalways distributed around the performance of the same class on the originaldataset.therefore weconcludethatformulticlassclassification tasksorganizingthedatabyclasscanyielddatawithmorestable gradients than artificially differentiating the data by augmentation techniques.itmotivatesustoexposemodelstotheeasierclasses first and then gradually transition to the harder classes.
proposed approach inthissection wedescribethedetailsofourapproach.ourmethod is built on the fine tuning paradigm and adapts pre trained models to downstream tasks.
given pre trained models and datasets of downstreamtasks weexploitthepotentialofpre trainedmodels on these tasks by acting on the data only.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
bridging pre trained models and downstream tasks for source code understanding icse may pittsburgh pa usa training set augmented datasetordered datasetmodel resultsdeclaration api controloriginalaugmentation technique test time augmentation nti g166 figure overview of our proposed method.
.
approach overview fig.
presents an overview of our approach.
our approach mainly consists of three components.
augmentation for training data that transforms given programsintosemanticallyequivalentprogramsandbuild augmented dataset to make training data more diverse.
curriculum strategy that organizes augmented dataset into the ordered dataset in an easy to hard order.
the order is determined by scoring functions.
test time augmentation that yields transformed versions of programs for prediction.
the results are the fusion of results of original programs and transformed programs of different transformation types.
table code transformation techniques transformation familyc c java controlfor while if transformerfor while if else transformer apiinput output c cpp style transformerequal loc equal func add assign transformer declaration and otherunused decl brace return transformerstmt sort merge divide transformer .
augmentation for training data inordertohelpmodelslearncodefeaturesinawaythatareinvariant to semantically equivalent programs we construct semanticpreserving transformations for code data.
the lexical appearances andsyntacticalstructuresaredifferentbeforeandaftertransformations but the semantics of programs are identical.
various languages apply different transformation techniques duetospecificlanguagecharacteristics.inthispaper weusethe same transformation techniques for data in the same language whichdonotrelyonpriorknowledgefromtasksordatasets.there are two programming languages in our experiments.
for c c we modify the work from quiring et al.
.
for java we apply thespattool .weapply tentransformationsforc c and nine transformations for java.
the specific transformations areshown in table .
these techniques are grouped by the granu larity of their changes.
they change the control structure api and declaration respectively to help models extract and learn thecorresponding features while ensuring that the semantics remainunchanged.takingthetransformationsinfig.1asanexample the fortransformer is applied to transform the original program to the1 transformation programandconvertsthe forstructureto while.thistypeoftransformationenablesthemodeltounderstand variouscontrolstructures.from1 transformation programto transformation program unused declandbracetransformer are applied.
this type of transformations could also generate diverseandequivalentdeclarationstatementsbymerging splitting and swapping declaration statements helping the model to ignore theinterferenceofsyntacticformalsandfocusonsemantics.inthelasttransformationto3 transformation program theoutputapi coutis converted to printf.
the api transformation exploits the fact that thesame function can be implementedby different apis.
these transformation techniques would also work in combination to make the dataset more diverse.
.
curriculum strategy thekeychallengeofcurriculumlearningishowtodefineeasy difficult examples.inthispaper weproposetwodifficultyscoringfunctions based on the hypotheses presented in section .
.
augmentation based curriculum strategy.
the previous section hasintroduceddataaugmentationtechniquesforcodedata anditis cheap to generate diverse data through transformations.
however compared with original data the augmented data can be regarded asperturbationsoradversarialexamplesoforiginaldata and they should be more difficult to learn as verified in section .
.
therefore wedesignanaugmentation basedcurriculumstrategy.
we first train on only the original data and then gradually increasetheproportionoftheaugmenteddata ensuringthatthe modelisexposedtomoredataandthedifficultygraduallyincreases during the training process.
in particular it should be noted that in the process of learning the augmented data we do not strictly follow the order of transformation programsto multi transformation programs since wefind that some programshave far more transformed programvariantsthanothersandmultipletransformationscouldcause the data to be unbalanced.
therefore we sample an equal number ofaugmentedsamplesfromthetransformedprogramvariantsof eachsampleintheoriginaltrainingsetforlearning andthedatastatisticsareshownintable2.thismethodiseasytoimplement on general models and we illustrate its effects in the following experiments.
class based curriculum strategy.
especially for multiclass classificationtasks basedonthehypothesisverifiedinsection2.
we propose a class based curriculum strategy.
specifically theleave one outstrategyisemployedtoobtainthe difficulty scores on the entire training set and then the average difficultyscoreoneachclassiscalculated.thesamplesinthesameclasstaketheaverageclassdifficultyscoreastheirdifficultyscores.inthetrainingprocess thissettingallowsthemodeltolearneasierclassesfirst andthentomoredifficultclasses.obviously themodel needs toextract and learn morefeatures to deal withincreasingly difficult tasks.
once the scoring function is determined we still need to definethepaceatwhichwetransitionfromeasysamplestoharder authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa deze wang zhouyang jia shanshan li yue yu yun xiong wei dong and xiangke liao samples.withreferencetothework whenselectingandapplyingdifferentpacingfunctions weensurethatthemodelhasa numberofsamplestolearnwhenthetrainingiterationbegins and gradually gets in touch with difficult samples until all samples are available.weimplementarangeofpacingfunctionsaccordingto penhaet al.
and illustrate its effects in section .
.
.
test time augmentation we also apply augmentations to the test set.
these are the same astheaugmentationtechniquesappliedonthetrainingset.tta neither modifies trained models nor changes test distribution.
it performspredictionsonthesametestdataasthegeneraltestprocedure exceptthatthepredictionforeachtestinputistheaggregation of predictions on multiple transformed versions.
tofurthereliminateoverconfidentincorrectpredictionsdueto overfitting for each sample in the test set we sample three augmented copies from its transformed candidates.
sampling more samplesforpredictionmaymaketheresultsmorerobust butwould increase the predictiontime proportionally.
asshown in the right part of fig.
the final experimental performance is the sum ofresults on the original test set and results in the augmented perspective which are the average of the results on augmented copies.
as a result incorrect prediction on a single test case by the model is corrected by combining multiple perspectives to make a final prediction.
experiments in this section we conduct experiments to verify whether our methodiseffectiveindifferenttasks includingalgorithmclassification code clone detection and code search tasks.
table data statistics dataset original training set augmented training set poj104 codeclonebench codesearchnet .
data preparation inthissubsection wepresentbenchmarkdatasetsforthreetasks from codexglue poj104 bigclonebench and codesearchnet anddescribehowtosimplyadaptdataofvarious tasks to our approach.
poj104datasetiscollectedfromanonlinejudgeplatform which consistsof104programclassesandincludes500student written c c programsforeachclass.thetaskforpoj 104datasetisto retrieve other programs that solve the same problem as a given program.wesplitthedatasetaccordingtolabels.weuse64classes of programs for training classes of programs for testing and classesofprogramsforvalidation.fordataaugmentation tosuccessfullycompiletheprograms include statementsareprepended beforetheprograms.thisprocessdoesnotintroducedifferences since added statements are the same for all programs.
as some programs cannot be compiled we further use regular expressions tocorrectprogramswithsimplegrammaticalerrors andremovetherestwithseriousgrammaticalandsemanticproblems.atotalof 1710programswereremoved accountingforabout3 .
to guarantee the fairness of the experiments we also evaluate the baseline models on both the original dataset and the normalized dataset.
for test time augmentation the results of the original and augmented versions of the same program are merged to make a prediction.
bigclonebenchdatasetcontains25 000javaprojects cover10 functionalities and including true clone pairs and 000falseclonepairs.thedatasetprovidedbywang etal.
isfiltered by discarding code fragments without any tagged true or false clone pairs leaving it with java code fragments.
the dataset includes901 416pairsfortraining validationand testing respectively.
this dataset has been widely used for thecode clone detection task.
for code augmentation since the datais in the form of code pairs we replace any original program in clone pairs with augmented programs to form new pairs.
for testtime augmentation all versions of a code pair are considered to determine whether it is a clone pair.
codesearchnet contains about million functions from opensource code spanning six programming languages.
in this paper weusethedatasetinjava.givenanaturallanguagequeryasthe input the task is to find the most semantically related code from a collection of candidate programs.
according to the state of the art model graphcodebert we expand query candidates to thewholecodecorpus whichisclosertothereal lifescenario.the answer of each query is retrieved from the whole validation and testing code corpus instead of candidate programs.
for code augmentation in the training set since the data are pairs of naturallanguagequeriesandprogramminglanguagefragments wereplace originalprogramswithaugmentedprogramsandformnewpairs withtheirnaturallanguagequeries.when it is different from the previous two tasks.
since the test set is the set of natural language queries we apply code augmentation techniquestothecodebasecorrespondingtothesequeries.fora queryandeachcodeincodebase wecalculatesimilarityofthecodeanditsmultipletransformedversionstothequery respectively.we use the average of similarity for sorting and evaluation.
theoriginalandaugmenteddatastatisticsoftheabovetasksare shown in table and the augmented datasets contain the original data.
we release all data for verification and future development.
theoretically moreaugmenteddatacanbeobtained however more data to train would bring larger time overhead.
to trade off theexperimental performance and time overhead we use a limitedamount of augmented data and we apply curriculum learningstrategy where the model is trained from a smaller data size and the overhead is further reduced.
.
experimental setups to illustrate the effectiveness of our method on code related tasks we build our approach on code pre trained models codebert and graphcodebert.
to illustrate the applicability of our method we alsoevaluateourmethodonnaturallanguagepre trainedmodel roberta thathasnotbeenexposedtocodeatall.inreplicationexperiments wefollowthedescriptionintheiroriginalpapers and released code.
for parameter settings to ensure fairness we authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
bridging pre trained models and downstream tasks for source code understanding icse may pittsburgh pa usa keepallparametersconsistentwiththeirreleasedcodeincluding random seeds except for the warmup step and epoch.
the warmup stepparameteradaptstotheincreaseofthedataset anditsvalue is adjusted from the original dataset size to the augmented dataset size.alsoduetotheincreaseindatasizeandthe progressivecurriculum learning we increase the epoch and set it to and on poj104 bigclonebench and codesearchnet respectively.
we replicatecodebertandgraphcodebertwiththesameparameter settings.theresultsreportedintheoriginalpapersandourreplicated results are not much different and we present all the results.
for data augmentation we implement augmentaion techniques on the top of clang for c c .
with respect to pacing function the hyperparameters are set according to penha et al.
.
.
algorithm classification metrics and baselines.
we use precision and map as the evaluationmetricsofthealgorithmclassificationtask.precisionisdefined as the average precision score and map is the rank based mean of averageprecisionscore eachofwhichisevaluatedforretrieving most similar samples given a query.
we apply roberta and the state of the art model codebert as baseline methods.
roberta is apre trainedmodelonnaturallanguage.codebertisapre trained model on code data.
it combines masked language modeling with replaced token detection objective to pre train a transformer encoder.
table algorithm classification comparison model precision map roberta .
.
.
roberta da cl .
.
codebert .
.
.
codebert da cl .
.
results.we compare with and without our method da cl for these pre trained models.
table summarizes these results.
for baseline methods all experimental results are evaluated on our normalizeddataset exceptforresultsofmapinparentheses.these resultsarereportedintheoriginalpaperofbaselinemethodsand map is their only metric for algorithm classification task.
natural language pre trained model roberta fine tuned with our method achieves88.
onprecision .
onmap.ourmethodimproves itsperformancenoticeablyby5.
onprecision .
onmapand .
compared to the results reported in the original paper.
code pre trainedmodelcodebertfine tunedwithourmethod achieves .
precision and .
on map.
our method substantially improves8.
onprecision .
onmap and10.
compared totheoriginalresult.notably withourmethod robertamodel withoutbeingpre trainedoncodedataoutperformstheexisting state of the art model codebert fine tuned on this task by .
map.
.
code clone detection metrics and baselines.
we use precision recall and f1 score as the evaluation metrics of the code clone detection task.
in ourexperiments we compare a range of models including the state ofthe art model graphcodebert.
graphcodebert is a pre trained modelforcodewhichimprovescodebertbymodelingthedata flowedgesbetweencodetokens.cdlh learnsrepresentations of code fragments through ast based lstm.
astnn encodes astsubtreesforstatementsandfeedstheencodingsofallstatement treesintoanrnntolearnrepresentationforaprogram.fa astgmn leverages explicit control and data flow information and uses gnns over a flow augmented ast to learn representation for programs.
tbccd proposes a tree convolution based method to detect semantic clone that is using ast to capture structural informationandobtainlexicalinformationfromtheposition aware character embedding.
table code clone detection comparison model precision recall f1 cdlh .
.
.
astnn .
.
.
fa ast amn .
.
.
tbbcd .
.
.
roberta data .
.
.
.
roberta data da cl .
.
.
codebert data .
.
.
codebert data da cl .
.
.
graphcodebert .
.
.
results.table4showsresultsforcodeclonedetection.ourreproduced results are mostly consistent with results reported in originalpapers exceptforthef1scoreof0.964forroberta whichishigherthantheoriginalresultof0.
.weimplementourmethod onrobertaandcodebert.experimentsshowthatmodelswith our method consistently perform better than the original models.notably withourmethod robertaperformscomparablyto codebert and codebert outperforms the state of the art model graphcodebert.moreimportantly followingtheoriginalsettings ofcodebert codebertonlyrandomlysamples10 ofthedata fortrainingcomparedtographcodebert.eventhoughweexpandthedatausingdataaugmentationintheexperimentforcodebert thedatausedbycodebertarestillmuchlessthandataforgraphcodebert.
.
code search metrics andbaselines.
for codesearch task weuse mrras the evaluation metric.
mrr is the average of the reciprocal rank ofresults of a set of queries.
the reciprocal rank of a query is the inverse of the rank of the first hit result.
table5showstheresultsofdifferentapproachesonthecodesearchnet corpus.
the first four rows are reported by husain et al.
.nbow cnn birnn andselfattrepresentneuralbagof words 1d convolutional neural network bidirectional gru based recurrent neural network and multi head attention respectively.
results.table5showsresultsofdifferentapproachesforcode search.
roberta code is pre trained on programs from codesearchnet with masked language modeling while maintaining the authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa deze wang zhouyang jia shanshan li yue yu yun xiong wei dong and xiangke liao table code search comparison model mrr nbow .
cnn .
birnn .
selfatt .
roberta .
roberta code .
roberta da cl .
codebert .
codebert da cl .
graphcodebert .
.
graphcodebert da cl .
roberta architecture.
our reproduced result .
of graphcodebertisslightlydifferentlyfromtheoriginallyreportedresult0.
.
weimplementour methodonroberta codebert andthestateof the artmodelgraphcodebertforcodesearch.theresultsshow thatnaturallanguagepre trainedmodelrobertawithourmethodoutperformsroberta code whichisthesamemodelarchitecturepre trainedoncodedata.codebertwithourmethodoutperforms the original state of the art model graphcodebert.
the perfor mance of graphcodebert with our method reaches .
mrr surpassing the original result .
mrr.
.
summary on above tasks and their benchmark datasets our method substantiallyimprovestheperformanceofarangeofpre trainedmodels achievingthestate of the artperformanceonalltasks.forthenatu rallanguagepre trainedmodelwithnoexposuretocodeatall with thehelp ofourapproach it isabletomatch orevensurpassexisting code pre trained models normally fine tuned to corresponding tasks.
in the code search task roberta pre trained with naturallanguageandfine tunedwithourmethod surpassesthesame architecture pre trained with code data and fine tuned with the general method.
these all illustrate the strong bridging role of our method between pre trained models and code related downstream tasks by introducing semantic knowledge for downstream tasks into pre trained models.
forcode relatedtasks applyingourapproachtoapre trained modelatthefinetunestagewitharelativelysmallcostispreferable to pre training a more complicated model from scratch with huge resources.
it illustrates the superiority of our method but this isnottonegatetheworkofcodepre trainedmodelseither.infact our approach achieves better results when applied to a superior pre trained model.
probably the research of pre trained models for source code has much work to do in terms of data diversity and conjunction with downstream tasks.
analysis this section analyzes the effects of different parameters on the performance of tasks in our experiment.
.
ablation study thissectioninvestigateshowdataaugmentationandcurriculum learning affect the performance of models respectively.
the following subsectionsshow theseresults for algorithmclassification code clone detection and code search task.
table ablation study on algorithm classification model precision map codebert .
.
codebert da cl .
.91w o da training .
.79w o tta .
.
w o cl .
.
algorithm classification.
for algorithm classification task we conduct experiments without augmention on training set datraining test timeaugmentationorcurriculumlearning.theresultsareshownintable6.thefirstrowshowstheresultsofthe baselinemodel.thesecondrowpresentstheresultsofthebaseline model with our full method.
the third row removes augmentation onthetrainingset.thefourthrowpresentstheresultsofremoving test time augmentation.
the results of removing curriculumlearning strategy are shown in the last row.
as seen from the results removinganyofthecomponentsleadstoadropofthemodel performance andtheremovaloftest timeaugmentationleadsto a significant performance degradation indicating that all three componentsarenecessarytoimproveperformance andtest time augmentaion contributes the most to the improvements.
we be lieve that for clustering tasks similar to algorithm classification integratingmultipleperspectivesinadataaugmentationmanner during testing could be a huge boost to model performance.
table ablation study on code clone detection model precision recall f1 codebert .
.
.
codebert da cl .
.
.972w o tta .
.
.
w o cl .
.
.
w o da training cl .
.
.
codeclonedetection.
forcodeclonedetectiontask wealsoconduct experiments without augmention on training set test time augmentationorcurriculumlearning.unlikealgorithmclassification weapplyaugmentation basedcurriculumlearningforcode clone detection task.
the removal of augmentation on the training set means that the cl component also does not work and onlytest time augmentation component works.
the experimental re sults in table show that the combination of augmentation onthe training set and cl component has the largest performance improvement andtest timeaugmentationhasnosignificantperformance improvement but the model can still benefit from it.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
bridging pre trained models and downstream tasks for source code understanding icse may pittsburgh pa usa table ablation study on code search model mrr graphcodebert .
graphcodebert da cl .
w o tta .
w o cl .
w o da training cl .
code search.
with the same ablation experimental setups as for the code clone detection task we conduct experiments on the code searchtask.asshownintable8 weconcludethatallthreecomponentsarenecessaryfortheimprovements.thelastrowshows the resultusing only test timeaugmentation which isable to significantly exceed the original state of the art performance without training with additional augmentation data.
we speculate that test timeaugmentationis abletocombinemultiple augmentation copiesinthecoderetrievalprocesstomakejudgmentsandelim inate overconfident incorrect predictions on the original test set.
thepenultimaterowshowstheexperimentalresultofremoving clcomponent.inotherwords itisobtainedbythecombination of augmentation on the training set and test time augmentation actingonthemodel.comparedtotheresultofapplyingtest time augmentationcomponentonlyinthelastrow wefindthatmore augmenteddatausedfortrainingmayresultinnegativegains.one possiblereason isthat theaugmented dataintroduces morenoise causing the model to choose from more candidates for the same query during training.
these results further illustrate the necessity of curriculum learning on augmented data.
table effects of augmentation types on algorithm classification model precision map all .
.
w o declaration .
.88w o api .
.
w o control .
.
.
effects of augmentation type sincethispaperconsidersmultipleaugmentationtechniques inthissection we explore the effects of augmentation techniques at differ entgranularitiesontheexperimentalresults.webuildtransformeddatasetsofthesamesizeusingaugmentationtechniquesofdifferent granularitiesand traincodebert separatelyon thesedatasets for algorithmclassificationtask.resultsareshownintable9.thefirst rowshows theresultsusingallaugmentation techniques ofthree granularities while the second to fourth rows show the results withouttheaugmentationtechniquesforthedeclaration api or controlstucturegranularity respectively.fromtheresults itcan be seen that not using the augmentation techniques of declaration or api granularity leads to a decrease in results while not usingthe augmentation techniques of control sturcture leads to an increase.thisindicatesthattheaugmentationofdeclarationandapicontributemoretotheimprovements however thecontrolstructure augmentation introduces more noise than contribution.
we speculate that changing the control structure has a greater impact onthetokenorderandcontextrelativetotheothertwogranularitiesofaugmentationtechniques andpre trainedmodelsweuse are based on masked language modeling and are context sensitive.
these reasons make it more difficult for the models to learn the knowledge and features introduced in the process of changing the controlstructure.thisfindingalsoencouragesthecodepre trained modeltofurtherexploitstructuralinformationofsourcecodein order to better understand the program semantics.
table effects of pacing function on algorithm classification model precision map random baseline .
.
anti .
.
linear .
.
step .
.
geom progression .
.70root 2 .
.
root 5 .
.
root 10 .
.
.
effects of pacing function to understand how the model is impacted by the pace we go from easytohardexamples weevaluatetheeffectsofdifferentpacing functions on the experimental results as shown in table .
we conduct experiments onpoj104 dataset in thealgorithm classfication task.
the learning order is determined by the scoring function described in section .
.
the baseline model codebert is trained in a random order and the anti method orders training samples from hard to easy.
the other methods learn training samples from easy to hard with the difference that at each epoch a different proportion of the training data are fed to the model as determined by theirfunctions.webrieflyintroducedifferentpacingfunctionsand thedetailsaredescribedinpenha etal.
.thelinearfunction linearlyincreasesthepercentageoftrainingdatainputtothemodel.
stepfunctiondividestrainingdataintoseveralgroups andafter fixed epoches a group of training samples will be added for model training.
root nandgeom progression functions correspond to two extremecases.
root nfunctionfeeds themodelwith alarge numberof easysamples andthen slowlyincreases theproportion of hard samples while geom progression function does the opposite.inthe root nfunction nisthehyperparameter andthelarger the value of n the more training data are fed to the model at the beginning.
all these functions are fed with the same training data at the final stage of training.
intable10 wecanseethatfeedingdatafromeasytohardhas a certain performance improvement while the performance of inputtingtrainingsamplesfromhardtoeasyissignificantlyworse than the baseline in a random order.
these results illustrates the effectiveness of our curriculum learning strategy and scoring functions.
comparison of different pacing functions shows that linear authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa deze wang zhouyang jia shanshan li yue yu yun xiong wei dong and xiangke liao andstepfunctions achieve similar results as root 2 function.
the rootfunctions obviouslyoutperformthe geom progression function whichisconsistentwiththefindingsofsohrmann etal.
andpenha etal.
.thereasonsarethattherootfunctiongives themodelmoretimetolearnfromharderinstancesandisbetter than nocl in terms ofstatistical significance.
inour experiments we used root 10 function for algorithm classification task and sincewedidnotperformablationstudyonthedatasetsoftheother two tasks we use linearfunction by default.
the performance on these two tasks could probably be further improved with different pacing functions and we leave it for future work.
related work .
data augmentation data augmentation aims to increase the data diversity and thus thegeneralizationabilityofthemodelbyvarioustransformation techniques.
this approach is widely used in the computer vision domain .inrecentyears researchersapplydataaugmentation to code data as well .
a series of studies are motivated by the fact that existing models are vulnerable to adversarialexamples andtheydesignmethodstoexposethevulnerability of models and improve the robustness of models.
our aimistomakethemodelsmoregeneralizableandperformbetter on real data unlike the methods described above.
jain et al.
improveaccuracyincodesummarizationandtypeinferencetask basedon equivalent datatransformationsand unsupervisedauxiliary tasks.
nghi et al.
propose a self supervised contrastive learning framework for code retrieval and code summarizationtasks.
our aim is similar to these studies but we do not need to designtheobjectivefunctionormodelarchitecture.withoutthe needforcomplicatedmodeldesign ourapproachaccomplishesthesamegoalbyactingonthedataonly.wesimplyaugmentsthedata and feeds the augmented data into the model in an easy to hard manner.therefore ourlightweightmethodcanbeeasilyapplied over existing models and various downstream tasks.
.
curriculum learning learning educational material in order from easy to difficult is very common in the human learning process.
inspired by cognitivescience researchershavefoundthatmodeltrainingcan also benefit from a similar curriculum learning setting.
since then cl has been successfully applied to image classification machine translation answer generation and information retrieve .
the core of cl lies in the design of the scoring function that is how to define easy and hard samples.
a straightforward approach istostudythedatatocreatespecificheuristicrules.forexample bengioetal.
useimagescontaininglessvariedshapesaseasy examples to be learned first.
tay et al.
use paragraph length as an evaluation criterion for difficulty in the question answer task.
however these are highly dependent on the task dataset and cannot be generalized to general tasks.
guo et al.
examine the examples in their feature space and define difficulty by the distribution density which successfully distinguishesnoisy images.
xuetal.
generallydistinguisheasyexamplesfromdifficultones on natural language understanding tasks by reviewing the trainingset in a crossed way.
in this paper similar to xu et al.
we also utilizecrossvalidationtomeasuredatadifficultybymodelitself but wealsotaketheclassdistributionintoconsideration.weintuitively solve the multiclass classification problem from a class perspective by first transforming it into a classification of fewer easy classes andthengraduallyincreasingthenumberofdifficultclasses.atthesametime wecombinecurriculumlearninganddataaugmentation toovercometheproblemthataugmenteddataismoredifficultto learn.
we first learn the original data then gradually transitionto augmented data and experimentally illustrate and verify the effectiveness of the design.
threats to validity there are several threats to validity of our method.
duetotheuseoftest timeaugmentationinourmethod thiscomponentcannotbeeasilyappliedtocodegenerationtasks.
augmentationonthetrainingsetandcurriculumlearning arestillapplicable e.g.
jain etal.
haveachievedgood performance on the code summarization task using code augmentation.
thetransformationtechniquesweusearenotrepresentative of the whole.
due to the characteristics of various tasks and datasets sometransformationsmayleadtolargeimprovements and some may bing no improvements.
therefore we release the datasets for replication and reducing experimentalbias.ourapproachisdesignedtobealightweightcomponentthatgeneralizestomultipledownstreamtasks.for specific downstream tasks new augmentation techniques can also be applied to optimize the performance.
due to limited computed resource we did not explore theperformance of our approach for the code clone detectiontask on graphcodebert or conduct ablation stuies on all threetasksregardingthepacingfunctionandtransformation type.
in fact there should be room for improvement andinterestingconclusionstobeexplored.weshallgetbetter results by searching for more suitable pacing functions and transformation types for the other two tasks.
we leave it for future works.
conclusion in this paper we focus on bridging pre trained models and coderelateddownstreamtasksandproposealightweightapproachon the fine tuning paradigm which is easy to implement on top of variousmodels.webuildourapproachoncodepre trainedmodels ofcodebertandgraphcodebert andthesemodelssubstantially outperform original models and achieve the state of the art per formance on algorithm classification code clone detection and codesearch.moreover weapplyourmethodtonaturallanguage pre trainedmodelrobertaanditachievescomparableorbetter performancethanexistingstate of the artcodepre trainedmodels fine tuned on these tasks.
this finding reveals that there is stillmuch room for improvement in existing pre trained models for source code understanding.
this paper focuses on code discriminative tasks.
it is more challenging to apply our approach to code generation tasks.
however generation tasks are data hungry and may require more diverse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
bridging pre trained models and downstream tasks for source code understanding icse may pittsburgh pa usa data for learning such as code generation where multiple code candidates are expected to be generated.
in the future it wouldbe interesting to combine our approach and prompt based learning tofurtherexploitthepotentialofgenerativepre trained models on code generation tasks.