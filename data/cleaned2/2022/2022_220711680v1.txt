no more fine tuning?
an experimental evaluation of prompt tuning in code intelligence chaozheng wang harbin institute of technology shenzhen china wangchaozheng stu.hit.edu.cnyuanhang yang harbin institute of technology shenzhen china ysngkil gmail.comcuiyun gao harbin institute of technology shenzhen china gaocuiyun hit.edu.cn yun peng the chinese university of hong kong hong kong china ypeng cse.cuhk.edu.hkhongyu zhang the university of newcastle newcastle australia hongyu.zhang newcastle.edu.aumichael r. lyu the chinese university of hong kong hong kong china lyu cse.cuhk.edu.hk abstract pre trained models have been shown effective in many code intelligence tasks.
these models are pre trained on large scale unlabeled corpus and then fine tuned in downstream tasks.
however as the inputs to pre training and downstream tasks are in different forms it is hard to fully explore the knowledge of pre trained models.
besides the performance of fine tuning strongly relies on the amount of downstream data while in practice the scenarios with scarce data are common.
recent studies in the natural language processing nlp field show that prompt tuning a new paradigm for tuning alleviates the above issues and achieves promising results in various nlp tasks.
in prompt tuning the prompts inserted during tuning provide task specific knowledge which is especially beneficial for tasks with relatively scarce data.
in this paper we empirically evaluate the usage and effect of prompt tuning in code intelligence tasks.
we conduct prompt tuning on popular pre trained models codebert and codet5 and experiment with three code intelligence tasks including defect prediction code summarization and code translation.
our experimental results show that prompt tuning consistently outperforms fine tuning in all three tasks.
in addition prompt tuning shows great potential in low resource scenarios e.g.
improving the bleu scores of fine tuning by more than on average for code summarization.
our results suggest that instead of fine tuning we could adapt prompt tuning for code intelligence tasks to achieve better performance especially when lacking taskspecific data.
ccs concepts software and its engineering software development techniques corresponding author.
the author is also affiliated with peng cheng laboratory and guangdong provincial key laboratory of novel security intelligence technologies.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november singapore singapore association for computing machinery.
acm isbn .
.
.
.
code intelligence prompt tuning empirical study acm reference format chaozheng wang yuanhang yang cuiyun gao yun peng hongyu zhang and michael r. lyu.
.
no more fine tuning?
an experimental evaluation of prompt tuning in code intelligence.
in proceedings of the 30th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november singapore singapore.
acm new york ny usa pages.
https introduction code intelligence leverages machine learning especially deep learning dl techniques to mine knowledge from large scale code corpus and build intelligent models for improving the productivity of computer programming.
the state of the art dl based approaches to code intelligence exploit the pre training and finetuning paradigm in which language models are first pre trained on a large unlabeled text corpora and then finetuned on downstream tasks.
for instance feng et al.
propose codebert a pre trained language model for source code which leverages both texts and code in the pre training process.
to facilitate generation tasks for source code wang et al.
propose a pre trained sequenceto sequence model named codet5.
these pre trained source code models achieve significant improvement over previous approaches.
however there exist gaps between the pre training and finetuning process of these pre trained models.
as shown in figure a pre training models such as codebert and codet5 are generally pre trained using the masked language modeling mlm objective.
the input to mlm is a mixture of code snippets and natural language texts and the models are trained to predict randomlymasked input tokens.
however when models are fine tuned into the downstream tasks e.g.
defect detection the input involves only source code and the training objective changes to a classification problem.
as shown in figure b the pre trained model represents each input code snippet using a classification head cls head and fine tunes the cls head based on a task specific dataset.
the inconsistent inputs and objectives between pre training and finetuning render the knowledge of pre trained models hard to be fully explored leading to sub optimal results for downstream tasks.
besides the performance of fine tuning largely depends on the scale of downstream data .arxiv .11680v1 jul 2022esec fse november singapore singapore chaozheng wang yuanhang yang cuiyun gao yun peng hongyu zhang and michael r. lyu import json defconvert string jsonitem json .loads string return json dict string a b jsonitem convert string the code isdefmax a b if b return b else return a return the value.import json defconvert string jsonitem json .loads string return json dict string a b jsonitem convert string a pre trainingmlm headcls head b fine tuning c prompt tuningmlm headab vocab classset labelwords max minvocabc mlm head verbalizerpredicted class ... ...... figure illustration on the process of pre training fine tuning and prompt tuning on defect detection task.
and denote two special tokens in pre trained models.
recently prompt tuning is proposed to mitigate the above issues of fine tuning.
figure c illustrates the concept of prompt tuning.
instead of only involving source code as input prompt tuning firstly rewrites the input by adding a natural language prompt such as the code is at the end of the code snippet and then let the model predict the masked token .
there is also a verbalizer that maps the tokens predicted by the model to the class.
by adding a prompt and verbalizer prompt tuning reformulates the classification problem into an mlm problem aligning the objective with the pre training stage.
this alignment unleashes the hidden power stored in the pre trained models.
besides the inserted natural language prompt can involve task specific knowledge to facilitate the adaption to downstream tasks .
inspired by the success of prompt tuning in the nlp field we would like to investigate if prompt tuning is effective for code intelligence tasks which to our best knowledge still remains unexplored.
in this paper we conduct an experimental evaluation on the effectiveness of prompt tuning on three popular code intelligence tasks defect detection code translation and code summarization.
we mainly investigate the following three research questions rqs how effective is the prompt tuning in solving code intelligence tasks?
how capable is prompt tuning to handle data scarcity scenarios?
how different prompt templates affect the performance of prompt tuning?
to answer the first rq we apply prompt tuning to the three code intelligence tasks.
to answer the second rq we evaluate prompt tuning in data scarcity scenarios from two aspects including lowresource settings and cross domain settings.
to answer the third rq we comprehensively study the influence of different prompt templates and verbalizers on model performance.
based on the experiment results we find that prompt tuning brings non trivial improvement to the the performance of downstream code intelligence tasks including both classification and thecode is a hardprompt b vanilla softprompt c prefixsoft prompt ...nnaturallanguagetokenlearnablevirtualtokenimportjsondefconvert string jsonitem json.loads string returnjson dictstring a b jsonitem convert string figure illustration on the different types of prompt where and indicate the input slot and answer slot respectively.
both vanilla soft prompt b and prefix soft prompt c belong to soft prompt.
generation tasks.
furthermore prompt tuning can significantly outperform conventional fine tuning especially when the training data are scarce.
the major contributions of this paper are as follows to the best of our knowledge this paper serves as the first study on the performance of prompt tuning for code intelligence tasks.
we explore how different prompts can affect the performance of prompt tuning on code intelligence tasks.
we discuss the implications of our findings and suggest further research on the usage of prompt tuning.
background .
fine tuning fine tuning a pre trained model for downstream tasks is a prevalent paradigm in the nlp field.
fine tuning aims at exploiting the knowledge learned by pre trained models without learning from scratch and can be regarded as a way of applying transfer learning .
to adapt pre trained models into downstream tasks fine tuning trains the model in a supervised way.
specifically givenno more fine tuning?
an experimental evaluation of prompt tuning in code intelligence esec fse november singapore singapore a dataset which consists of task specific samples xand corresponding labelsy fine tuning aims to find a set of parameters for the pre trained model that arg min p y x .
.
prompt tuning the intuition of prompt tuning is to convert the training objective of downstream tasks into a similar form as the pre training stage i.e.
the mlm objective .
as shown in figure c prompt tuning aims at predicting masked tokens in the input.
it also modifies the model input by adding a natural language prompt enabling the input format identical to the pre training stage.
specifically prompt tuning employs a prompt template fprompt x to reconstruct the original input x producing new input x .
as illustrated in figure the prompt template can involve two types of reserved slots in i.e.
input slot and answer slot .
the input slot is reserved to be filled with original input text and the answer slot is to be filled by predicted labels such as defective .
for the example shown in figure prompt tuning outputs the final predicted class by a verbalizer .
the verbalizer denoted as v is an injective function which maps each predicted label word to a class in the target class set y v w y wherewindicates the label word set.
for the example in figure c the label word set wincludes for buggy code snippets and for the others.
the class set ycontains and for indicating defective and clean code respectively.
in the example the verbalizer maps the label with highest probability defective into the target class in the class set.
according to the flexibility of the inserted prompt prompt tuning techniques can be categorized into two types hard prompt and soft prompt.
we elaborated on the details of each prompt type in the following.
.
.
hard prompt.
the hard prompt is a technique that modifies the model input by adding fixed natural language instruction prompts .
it aims to elicit task specific knowledge learned during pre training for the tuning stage.
hard prompt is also known asdiscrete prompt since each token in the prompts is meaningful and understandable .
for instance in the defect detection task by appending the code is .
to the input code the task objective becomes predicting the label word at the answer slot such as defective or clean .
the designed prompt template for defect prediction task can be formulated as fprompt x thecodeis where denotes the input code.
although hard prompt has shown promising performance in previous work the template design and the verbalizer choices are challenging.
for example the prompt template fprompt x can also be designed as itis where the label words in the verbalizer involve bad and perfect .
.
.
soft prompt.
the soft prompt as the name implies is an alternative to hard prompt.
different from hard prompt the tokens in the soft prompt template are not fixed discrete words of a natural language.
instead these tokens are continuous vectors which can be learnt during the tuning stage.
they are alsocalled virtual tokens because they are not human interpretable.
soft prompt is proposed to alleviate the burden of manually selecting prompt template in hard prompt.
there are two kinds of soft prompt denoted as vanilla soft prompt andprefix soft prompt respectively.
vanilla soft prompt as depicted in figure b can be obtained by simply replacing the hard prompt token with a virtual one denoted as such as fprompt x the embedding of virtual tokens are optimized during tuning stage.
prefix soft prompt prepends several virtual tokens to the original input as shown in figure c .
it can generate comparable performance with the vanilla soft prompt and hard prompt.
fprompt x n wherenindicates the number of virtual tokens.
experimental evaluation .
research questions we aim at answering the following research questions through an extensive experimental evaluation rq1 how effective is the prompt tuning in solving code intelligence tasks?
rq2 how capable is prompt tuning to handle data scarcity scenarios?
rq3 how different prompt templates affect the performance of prompt tuning?
we design rq1 to verify our hypothesis that prompt tuning which aligns the training objectives with the pre training stage is more effective than fine tuning for the downstream code intelligence tasks.
rq2 aims at investigating whether prompt tuning embodies advantage in data scarcity scenarios including low resource and cross domain settings.
in rq3 we aim at exploring the impact of different prompt templates such as varying prompt types and selection of label words on the performance of downstream tasks.
table statistics of the datasets used in this paper.
tasks datasetstraining val.
test set set set defect detection defect ruby javascript code go summarization python java php code translation translation .
code intelligence tasks with prompt tuning to evaluate the prompt tuning technique on source code we adopt three downstream code intelligence tasks namely defect detection esec fse november singapore singapore chaozheng wang yuanhang yang cuiyun gao yun peng hongyu zhang and michael r. lyu code summarization and code translation.
we describe the detail of pre trained models and prompt template of each task in the following.
.
.
pre trained models.
we choose codebert and codet5 as the studied pre trained models since they are the most widely used model and state of the art model for source code respectively.
codebert is an encoder only model which is realized based on roberta .
codebert is pre trained on codesearchnet .
it is able to encode both source code and natural language text.
codebert has million parameters.
codet5 a variant of text to text transfer transformer is the state of the art model for code intelligence tasks.
it regards all the tasks as sequence to sequence paradigm with different task specific prefixes.
it can solve both code understanding and code generation tasks.
code t5 is pre trained on a larger dataset including codesearchnet and an additional c c language corpus collected by the authors.
codet5 is classified into two versions codet5 small and codet5 base according to their sizes.
the numbers of parameters in codet5 small and codet5 base are million and million respectively.
.
.
defect detection.
given a code snippet defect detection aims to identify whether it is defect prone such as memory leakage and dos attack.
the task is defined as a binary classification task in training codebert and a generation task in training codet5 .
forhard prompt as shown in figure c with prompt tuning models predict the probability distribution over the label words.
a verbalizervmaps the label word with highest probability to the predicted class.
one cloze style template fprompt with an input slot and an answer slot is designed as below fprompt x thecode is v where the left and right sides of indicate the predicted class and corresponding label words.
to study the impact of different prompts we also design other prompt templates including it is the code is the code is defective and a code .
forvanilla soft prompt for facilitating the comparison of hard prompt and vanilla soft prompt we simply replace the natural language tokens in the hard prompt templates with virtual tokens for generating vanilla soft prompts.
for example is the vanilla soft prompt version of it is .
forprefix soft prompt we design the prefix soft prompt by appending a learnable prefix prompt according to equation .
.
.
code summarization.
given a code snippet the code summarization task aims to generate a natural language comment to summarize the functionality of the code.
we only utilize codet5 in this task because codebert does not have a decoder to generate comments.forhard prompt we append the natural language instruction of the task to the input code so the template can be fprompt x generatecomment for where and denote the slot of programming language type input slot and the generated answer slot.
the natural language instruction generate comment for is manually predefined for adjusting the generation behavior of codet5.
we also design other prompt templates for experimentation including summarize .
note that there is no verbalizer for the generation task.
for the vanilla soft prompt andprefix soft prompt they are designed in the same way as the defect detection task.
for example we replace the natural language tokens in the hard prompt templates into virtual tokens for generating vanilla soft prompts.
the prefix soft prompts are defined according to equation .
.
.
code translation.
code translation aims to migrate legacy software from one programming language to another one.
the vanilla soft prompt andprefix soft prompt are designed similar to the above two tasks so we only describe about the hard prompt for the task.
for hard prompt we design the template by appending task specific instruction fprompt x translate to the template explains that the model is translating the input code in one programming language to the code in another programming language .
.
evaluation datasets to empirically evaluate the performance of prompt tuning for source code we choose the datasets for the three tasks from the popular codexglue benchmark1 .
.
.
defect detection.
the dataset is provided by zhou et al.
.
it contains 27k c code snippets from two open source projects qemu and ffmpeg and .
of the entries are defective.
.
.
code summarization.
we use the same dataset as the codet5 work .
the dataset is from codesearchnet which contains thousands of code snippet and natural language description pairs for six programming languages including python java javascript ruby go and php.
.
.
code translation.
the dataset is provided by lu et al.
and is collected from four public repositories including lucene poi jgit and antlr .
given a piece of java c code the task is to translate the code into the corresponding c java version.
.
evaluation metrics .
.
defect detection for the defect detection task following we use accuracy as the evaluation metric.
the metric is to measure the ability of model to identify insecure source code defined as acc d i yi yi d more fine tuning?
an experimental evaluation of prompt tuning in code intelligence esec fse november singapore singapore table hyperparameter settings hyperparameter value hyperparameter value optimizer adamw warm up steps learning rate 5e training batch size lr scheduler linear validation batch size beam size adam epsilon 1e max.
gradient norm .
wheredis the dataset and d denotes its size.
the symbol yiand yiindicate the ground truth label and predicted label respectively.
the1 x function returns if xis true and otherwise returns .
.
.
code summarization following previous work we use bilingual evaluation understudy bleu score to evaluate the quality of generated comments.
the idea of bleu is that the closer the generated text is to the result of ground truth text the higher the generation quality.
the metric is defined as below bp 1if c r e1 r cifc r bleu bp exp n n 1wnlogpn wherepnmeans the modified n gram precision and wnis the weight.
bprepresents the brevity penalty and candrindicate the lengths of generated comment and target comment length respectively.
in our experiments we choose smoothed bleu score i.e.
n for evaluating the generation tasks following previous work .
.
.
code translation to better measure the quality of generated code snippets besides bleu score another two metrics including accuracy and codebleu are used following .
the computation of accuracy is the same as equ.
which is the most strict metric.
codebleu parses the generated code and takes both the code structure and semantics into account for measuring the similarity between the generated code and the code in ground truth.
its computation consists of four components including n gram matching score bleu weighted n gram matching score weighted bleu syntactic ast matching score ast score and semantic data flow matching score df score codebleu bleu weighted bleu ast score df score where are weights for each component.
following they are all set as .
.
.
implementation details .
.
experimental setup.
all the pre trained models and corresponding tokenizer in our experimentation are loaded from the official repository huggingface2.
the overall framework is pytorch3.
our implementation of prompt is based on openprompt .
we use the generic training strategy and parameter settings following official implementation of codet5 with details shown in table .
specifically for the defect detection task we train codebert and codet5 for and epochs respectively.
for codet5 model we set the maximum source length and target length as and respectively.
for the code summarization task because codebert is an encoder only architecture model we focus on evaluating prompt tuning on codet5.
we train codet5 for epochs.
the maximum lengths of source text and target text are defined as and .
for the code translation tasks we train the codet5 models for epochs.
the maximum length of source text and target text are set as and respectively.
for parameter configuration in fine tuning we use the configurations provided by the original work which were already well adjusted.
for a fair comparison we use the same parameter configurations when implementing prompt tuning.
all the experiments are run on a server with nvidia tesla v100 and each one has 32gb graphic memory.
.
.
fine tuning baselines.
we fine tuned codebert and codet5 on the three code intelligence tasks.
specifically we fine tune codebert only for the defect detection and codet5 for all the three tasks.
for codebert we use the first output token the token as the sentence embedding and feed it into a feed forward network ffn to generate predictions.
for codet5 all the tasks are treated as generation tasks.
it takes either code or natural language sentences as input and generate target texts.
table classification accuracy on defect detection.
methods accuracy codebertfine tuning .
prompt tuning .
codet5 smallfine tuning .
prompt tuning .
codet5 basefine tuning .
prompt tuning .
experimental results .
rq1 effectiveness of prompt tuning in this section we study the effectiveness of prompt tuning by comparing with the standard tuning paradigm fine tuning on the three code intelligence tasks defect detection code summarization and code translation.
we present the best performance achieved by our experimented prompts.
full results can be found in our project repository4.
we also discuss the impact of different prompts in section .
.
defect detection.
table shows the comparison results for defect detection in which codebert and codet5 serve as pretrained models.
we can observe that prompt tuning always outperforms fine tuning across different pre trained models.
for example prompt tuning obtains an improvement of .
over fine tuning on codebert.
for codet5 small and codet5 base the improvements november singapore singapore chaozheng wang yuanhang yang cuiyun gao yun peng hongyu zhang and michael r. lyu table results bleu scores of the codet5 model on code summarization task.
methods ruby javascript go python java php overall codet5 smallfine tuning .
.
.
.
.
.
.
prompt tuning .
.
.
.
.
.
.
codet5 basefine tuning .
.
.
.
.
.
.
prompt tuning .
.
.
.
.
.
.
publicvirtualintsize lock mutex returnc.size a originalc codepublicintsize returnc.size c generatedjavacodebyfine tuningpublicintsize synchronized mutex returnc.size d generatedjavacodebyprompttuning overridepublicintsize synchronized mutex returnc.size b groundtruthjavacode figure an example for illustrating the quality of code snippets translated by fine tuning and prompt tuning in the code translation task respectively where the pre trained model is codet5 small.
are .
and .
respectively.
we also perform a statistical significance test t test on defect detection task and the results show that prompt tuning outperforms fine tuning at the significance level at .
p value .
.
the results indicate that prompt tuning is more effective than fine tuning for pre trained models with different architecture or different sizes on the defect detection task.
code summarization.
since codebert is an encoder only model we only involve codet5 as the pre trained model on the code summarization task.
table presents the bleu scores achieved by prompt tuning and fine tuning for different programming languages.
we can observe consistent improvement on overall performance as in the defect detection task compared with fine tuning prompt tuning obtains an improvement of .
and .
when using codet5 small and codet5 base as pre trained models respectively.
looking into specific programming language prompt tuning also always achieves better summarization performance than finetuning.
it shows the largest advancement for the code written in php with increase rate at .
and .
on codet5 small and codet5 base respectively.
moreover prompt tuning can perform statistically better than fine tuning at the significance level .
on code summarization with a p value .
.
the results indicate the effectiveness of prompt tuning in the code summarization task.
code translation.
for the task we only involve the pre trained codet5 model for evaluating the performance of prompt tuning.
the results of prompt tuning and fine tuning based on codet5 are depicted in table .
from the table we can see that prompt tuning outperforms fine tuning in both directions of translation.
comparing with fine tuning prompt tuning achieves an average improvement of .
.
and .
on both directions for bleu accuracy and codebleu respectively.
the improvement demonstrates the effectiveness of prompt tuning on this task.
tobetter illustrate how prompt tuning improves the quality of code translation we give an example in figure .
from the example we can see that using fine tuning methods codet5 small does not accurately translate the c code into the corresponding java version by missing an important synchronized lock statement synchronized mutex while it can output more accurately with prompt tuning.
we attribute this improvement to the learned prior knowledge carried by the prefix soft prompts.
through the powerful prior knowledge codet5 can quickly adapt to the translation of the code in c to java and pay more attention to language specific grammar details.
but fine tuning methods can only make the model learn the translation direction after multiple iterations of training the model may fail to focus on the important part such as lock in the input.
based on the performance of all the three tasks we find that prompt tuning is more effective than fine tuning.
another interesting observation is that the improvement of prompt tuning on codet5 small is .
.
and .
respectively which is higher than that on codet5 base with the increase rat at .
.
and .
respectively.
this may be attributed to that codet5base is a significantly larger model than codet5 small million v.s.
million parameters and prompt tuning parameters per token .
the observation suggests that prompt tuning shows more obvious improvement than fine tuning for smaller pre trained models.
finding prompt tuning is more effective than fine tuning on the code intelligence tasks with respect to different pre trained models and different programming languages.
besides the advantage of prompt tuning is more obvious for smaller pre trained models.
.
rq2 capability of prompt tuning in different data scarcity scenarios considering that the performance of fine tuning is known to strongly rely on the amount of downstream data while scenarios with scarce data in source code are common .
in this section we study how well prompt tuning can handle the data scarcity scenarios.
we focus on two kinds of data scarcity settings low resource scenario in which there are significantly few training instances and cross domain scenario in which the model is trained on a similar data sufficient domain and tested on target domain.
performance in low resource scenario.
we study the performance of prompt tuning in low resource setting on the classification task i.e.
defect detection and one generation task i.e.
code summarization.
we simulate this setting by randomly select a small subset of training instances also called shots in the original dataset.no more fine tuning?
an experimental evaluation of prompt tuning in code intelligence esec fse november singapore singapore table experimental results on code translation tasks java c and c java.
methodsc to java java to c bleu accuracy codebleu bleu accuracy codebleu codet5 smallfine tuning .
.
.
.
.
.
prompt tuning .
.
.
.
.
.
codet5 basefine tuning .
.
.
.
.
.
prompt tuning .
.
.
.
.
.
table classification accuracy on defect detection in low resource scenario.
denotes the model fails to converge due to extreme lack of training data.
method zero shot shots shots shots shots codebertfine tuning .
.
.
.
.
prompt tuning .
.
.
.
.
codet5 smallfine tuning .
.
.
prompt tuning .
.
.
codet5 basefine tuning .
.
.
prompt tuning .
.
.
a ruby d python b javascript c go e java f php codet5 smallfine tuningcodet5 smallprompttuningcodet5 basefine tuningcodet5 baseprompttuning figure results of fine tuning and prompt tuning on code summarization task in low resource scenarios.
the horizontal axis indicates the number of training instances while the vertical axis means the bleu score.
to avoid randomness in data selection we produce each subset five times with different seeds and run four times on each dataset.
the average results are reported.
for the defect detection task we choose and training shots per class to create five small training subsets.
table 6presents the accuracy achieved by prompt tuning and fine tuning regarding the five settings.
note that in zero shot settings no tuning data are involved.
given test data the fine tuning model directly generates target labels defective or clean while the prompt tuning model predicts the label words.
by comparing the results withesec fse november singapore singapore chaozheng wang yuanhang yang cuiyun gao yun peng hongyu zhang and michael r. lyu table experimental results bleu score on crosslanguage code summarization.
the models are trained on python or java datasets and tested on ruby javascript and go respectively.
training methods ruby javascript go codet5 small pythonfine tuning .
.
.
prompt tuning .
.
.
javafine tuning .
.
.
prompt tuning .
.
.
codet5 base pythonfine tuning .
.
.
prompt tuning .
.
.
javafine tuning .
.
.
prompt tuning .
.
.
table classification accuracy of comparing the performance of codebert model on defect detection task via different prompt templates.
the verbalizer is fixed as bad defective perfect clean .
the underlined texts are replaced by virtual tokens in the corresponding vanilla soft prompt.
hard prompt vanilla soft promptaccuracy hard soft the code is .
.
a code .
.
it is .
.
the code is .
.
table classification accuracy of different verbalizers on the defect detect task where the pre trained model is codebert.
the template is the code is .
verbalizer accuracy yes no .
bad perfect .
bad defective clean perfect .
bad defective insecure .
clean perfect secure bad defective insecure vulnerable63.
clean perfect secure invulnerable those in the full data setting in table we can find that the model performance shows severe drop.
for the codet5 model it even does not converge under the zero shot and shot settings due to the extreme lack of training data.
the low results are reasonable since pre trained models require task specific data for better adapting to downstream tasks.
however we observe that with prompt tuning the pre trained models achieve significantly better performance than the models using fine tuning.
on average prompt tuning outperforms fine tuning by .
.
and .
on codebert codet5 small and codet5 base respectively.
note that prompt tuning under zero shot setting even outperforms prompt tuning with shots and fine tuning with shots.
it indicates that the knowledge of pre trained model can be elicited by the prompt without tuning the parameters.
for the code summarization task we choose and training shots as subsets.
figure shows comparison on bleu scores of prompt tuning and fine tuning codet5 models on different programming languages.
we can find that although the model performance drops significantly on the subsets prompt tuning consistently outperforms fine tuning showing an average improvement at .
and .
for codet5 small and codet5base respectively.
we also observe that the improvement becomes less stark with the growth of training shots.
the results demonstrate that prompt tuning is more advantageous on few training data than fine tuning.
performance in cross domain scenario.
for some programming languages the training data are generally insufficient.
as shown in table the data sizes of languages such as java and python are greatly larger than those of languages including javascript and ruby.
cross domain learning is one popular solution i.e.
transferring the knowledge of similar domains with sufficient data to the target domains.
we use the code summarization task for evaluating the performance of prompt tuning under cross domain setting.
considering the adequacy of training data and domain similarity we perform training on the programming language java or python and evaluate on the language with fewer data such as ruby javascript and go.
table shows the cross domain bleu scores achieved by codet5.
we can observe that prompt tuning achieves better performance than fine tuning for most cross domain settings except for the adaption from python to javascript.
with prompt tuning the bleu scores of codet5 small and codet5 base are increased by .
and .
on average respectively.
finding prompt tuning is more effective in low resource scenarios than fine tuning.
the fewer training instances the larger the improvement achieved by prompt tuning.
besides prompt tuning also shows superior performance on the cross domain code intelligence task.
.
rq3 impact of different prompts in this rq we explore the impact of different prompts on the performance of prompt tuning.
we focus on the following three aspects hard prompt template hard prompt v.s.
vanilla soft prompt and length of prefix soft prompt.
.
.
different hard prompt templates.
there are two factors that can impact the performance of hard prompts including the template design and verbalizer.
due to the space limit we present the evaluation results on the classification task i.e.
defect detection and one generation task i.e.
the code summarization.
note that we have the same observation for the code translation task.
template design.
the natural language tokens in hard prompt templates are manually defined.
to study the impact of differentno more fine tuning?
an experimental evaluation of prompt tuning in code intelligence esec fse november singapore singapore table results bleu scores of prompt tuning with different prompt templates on the code summarization task.
there is no verbalizer for the prompts of generation tasks.
fprompt ruby javascript go python java php overall codet5 smallsummarize .
.
.
.
.
.
.
.
.
.
.
.
.
.
generate comments for .
.
.
.
.
.
.
.
.
.
.
.
.
.
codet5 basesummarize .
.
.
.
.
.
.
.
.
.
.
.
.
.
generate comments for .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
50100150200250c to javajava to c b codetranslation a codesummarization17.
.
.
.
.
.
.
.
.
.
.
.
.
50100150200250javapythonbleu 4bleu 4bleu 4bleu c defect detection6263646566 50100150200250codet5codebert6060.
.
.
1050100150200250accacc figure bleu score of comparing the performance of codet5 models on code summarization and code translation with different prefix lengths.
the horizontal axis indicates the length of prefix.
tokens in the template we conduct experiments with fixed verbalizers.
table and table show the results on the defect detection task and code summarization task respectively.
comparing the row in table we can find that the template design impacts the model performance.
for example when using the hard prompt the code is codebert outperforms the case when using a code by .
.
in addition by changing thecode is to thecodeis in which only the token order is different a drop in performance by .
is observed.
however comparing row and in table we can find that the model performance is less affected by the template design for the code summarization task.
this may be attributed to that only few prompt tokens in the templates can hardly provide helpful guidance for the large solution space in the code summarization task.
thus we achieve that the template design for hard prompt is more important for the classification task than the generation task.
different verbalizers we fix the hard prompt template as thecode is and analyze the impact of different verbalizers on the model performance.
specifically we choose task relevant label words for the verbalizers with the results on the defect prediction task shown in table .
we can observe that different verbalizers influence the performance of prompt tuning.
when choosing label words such as yes and no row rather than adjectives to fill the answer slot the result is .
lower than that of using adjectives in the verbalizer row .
it indicates that constructingverbalizer with correct grammar is helpful for the prediction.
comparing row we can also find that increasing the number of label words is not always beneficial for the model performance which may be because more label words could introduce bias to the prediction results.
when using two label words for indicating each class the model presents the highest performance.
.
.
hard prompt vs. vanilla soft prompt.
as introduced in section .
.
the vanilla soft prompt replaces the natural language tokens in hard prompt with virtual tokens.
the comparison results on the defect detection task are illustrated in table .
we experiment with different hard prompts shown in the first column with the corresponding vanilla soft prompts at the second column.
from the results listed as the last two columns we can find that hard prompts present better prediction accuracy than the corresponding vanilla soft prompts.
for the code summarization task the results are shown in table .
comparing the performance of hard prompts such as summarize and generate comments for with the corresponding vanilla soft version we can observe that the difference is marginal which may be due to the large generation space of the task.
thus we summarize that hard prompts may be more effective than the corresponding vanilla soft prompts for classification tasks and the advantage tends to be weakened for generation tasks.esec fse november singapore singapore chaozheng wang yuanhang yang cuiyun gao yun peng hongyu zhang and michael r. lyu turntabler .authorizeduser .update laptop original string def update laptop name assert valid values name w mac pc linux chrome iphone cake intel android api user.modify laptop name self.attributes laptop name true end language ruby code def update laptop name assert valid values name w mac pc linux chrome iphone cake intel android api user.modify laptop name self.attributes laptop name true end a ground truth comment updatesthe laptop currently being used b comment generated by fine tuning modifythe laptop.
c comment generated by prompt tuning updatethe laptop.
figure case study on the code summarization task where the pre trained model is codet5 small.
.
.
different lengths of prefix soft prompts.
we also study the impact of different lengths of prefix soft prompts.
we illustrate the performance under different prefix prompt lengths for the three tasks in figure .
as can be seen too short or long lengths of prefix prompts can degrade the model performance.
for all the tasks prompt tuning achieves the best or nearly best performance when the length of prefix prompt set to a value between and .
in our work the prefix lengths are set as and for defect detection code summarization and code translation tasks respectively.
finding prompt templates have large impact on the performance of prompt tuning.
it is crucial to construct prompt templates with suitable template design and verbalizers based on domain knowledge.
when using the prefix prompts the length of prompts has impact on the model performance.
discussion .
implications of findings implication on the utilization of pre trained models.
prompt tuning performs well in adapting pre trained models on code intelligence tasks.
we observe that prompt tuning can consistently outperform fine tuning in our experiments under full data settings data scarcity settings and cross domain settings.
the advantage of prompt tuning is especially outstanding in data scarcity settings which suggests that prompt tuning is a superior solution when there is a lack of task specific data.
implication on the utilization of prompts.
our experiments demonstrate that different templates and verbalizers influence the performance of the code intelligence tasks.
the templates that have the same semantics but different prompt tokens can lead to different performance results.
researchers could try different combinations of the words in their templates and evaluate the effectiveness through experiments.
besides although the vanilla soft prompt is helpful to reduce the manual cost of prompt template designing the best performance is achieved mostly by well designed hard prompt.
furthermore we find that the performance of prefix soft prompt varies with its length.
determining the best length of the prompt for a downstream task is difficult.
based on our experiments in general promising results can be achieved by soft prompt when the length is between and .
public virtual bool contains object o return indexof o !
public boolean contains object o return indexof o !
a original c code b ground truth java code public boolean contains object o return indexof o public boolean contains object o return indexof o !
c generated java code by fine tuning d generated java code by prompt tuningfigure case study on the code translation task where the pre trained model is codet5 small.
.
case study in this section we provide additional case studies to qualitatively compare prompt tuning with fine tuning.
the case in figure shows a ruby code snippet with comments generated by fine tuning and prompt tuning models.
from the case we can observe that the fine tuning model is mislead by the word modify in the code snippet and fails to capture the main functionality update .
quite the opposite the prompt tuning model accurately summarizes the code snippet.
we also give another case in code translation task in figure .
the original c code a is to check whether object ois contained.
the code translated by fine tuning model c only returns the index of o but does not compare it with where the code semantic changes.
however the prompt tuning model generates the identical java code d with the ground truth one b .
.
future directions based on the findings and implications we suggest two possible future directions for prompt tuning on source code.
first we suggest future research to consider more characteristics of source code like syntactic structures in the design of template and the choices of verbalizer.
experiment results demonstrate that domain knowledge plays an important role on the design of prompts.
as code structure information has been demonstrated on the design of regular dl models for code related tasks we believe that the domain knowledge carried by them can also help the design of prompts.
second through constructing cloze style prompt template the factual knowledge and biases contained in the pre trained models can be investigated .
researchers can focus on improving the interpretability and robustness of pre trained models and designing novel pre training tasks in the future.
.
threats to validity we have identified the following major threats to validity limited datasets.
the experiment results are based on a limited number datasets for each code intelligence task.
the selection of data and datasets may bring bias to the results.
to mitigate this issue we choose the most widely used datasets for each code related task modify the seeds and run the sampling multiple times.
we also plan to collect more datasets in the future to better evaluate the effectiveness of prompt tuning.
limited downstream tasks.
our experiments are conducted on three code intelligence tasks including one classification taskno more fine tuning?
an experimental evaluation of prompt tuning in code intelligence esec fse november singapore singapore and two generation tasks.
although these tasks are the representative ones in code intelligence there are many other tasks such as code search and bug fixing .
we believe that we could obtain similar observations on these tasks since they can all be formulated as either classification tasks or generation tasks for source code.
we will evaluate more tasks with prompt tuning in our future work.
suboptimal prompt design.
we demonstrate that prompt tuning can improve the performance of pre trained models.
however the prompts we use in this paper may not be the best ones.
it is challenging to design the best prompt templates and verbalizers which will be an interesting future work.
related work .
pre training on programming language code intelligence aims at learning the semantics of programs to facilitate various program comprehension tasks such as code search code summarization and bug detection .
recently inspired by the huge success of pre trained models in nlp a boom of pre training models on programming languages arises.
cubert and codebert are two pioneer works.
cubert utilizes the mlm pre training objective in bert to obtain better representation of source codes.
codebert is able to learn nl pl representation via replaced token detection task .
svyatkovskiy et al.
and kim et al.
train gpt on large scale programming languages for solving code completion task.
the work graphcodebert leverages data flow graph dfg in model pre training stage making model better understand the code structure.
apart from aforementioned encoder or decoder only models pre trained models that utilize both encoder and decoder are also proposed for programming languages.
for example ahmad et al.
propose plbart which is able to support both understanding and generation tasks.
the work utilizes text to text transfer transformer t5 framework to solve code related tasks.
wang et al.
modify the pre training and finetuning stages of t5 and propose codet5 .
.
prompt tuning the concept of prompt tuning is formed gradually.
in the work the authors find that the pre trained language models have ability to learn the factual knowledge due to the mask and predict pre training approach.
therefore pre trained language models can be regarded as a kind of knowledge base.
to measure the capability of pre trained models to capture factual information they propose a language model analysis dataset lama .
later jiang et al.
attempt to more accurately estimate the knowledge constrained in the language model .
they propose lpaqa to automatically discovery better prompt templates.
several works focus on exploring good templates.
yuan et al.
replace phases in the template via a thesaurus.
the work utilizes a neural prompt rewriter to improve the model performance.
aforementioned works explore the manual templates or hard templates meaning the words in the template are fixed and not learnable .
researchers also attempt to optimize the template in the training process soft prompt .
for example li et al.
add an additional learnable matrix in front of theinput embedding .
zhong et al.
propose to initialize these matrices by natural language tokens for more effective optimization .
recently a series of works also study prompts in pre training stage.
they find that the behavior of language models can be manipulated to predict desired outputs sometimes even require no task specific training.
in our work we adapt prompt tuning in code intelligence tasks to exploit knowledge about both natural language and programming languages captured by pre trained models.
conclusion in this paper we experimentally investigate the effectiveness of prompt tuning on three code intelligence tasks with two pre trained models.
our study shows that prompt tuning can outperform finetuning under full data settings data scarcity settings and crossdomain settings.
we summarize our findings and provide implications that can help researchers exploit prompt tuning effectively in their code intelligence tasks.
our source code and experimental data are publicly available at .