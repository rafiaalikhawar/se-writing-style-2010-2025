arewebuilding on therock?
on theimportanceof data preprocessing forcodesummarization linshi shilin iscas.ac.cn institute of software chinese academy of sciences beijing chinafangwenmu fangwen2020 iscas.ac.cn institute of software chinese academy of sciences beijing chinaxiao chen chenxiao2021 iscas.ac.cn institute of software chinese academy of sciences beijing china song wang wangsong eecs.yorku.ca lassonde school of engineering york university toronto canadajunjie wang junjie iscas.ac.cn institute of software chinese academy of sciences beijing chinaye yang yangye gmail.com schoolof systems andenterprises stevensinstitute of technology hoboken nj usa ge li lige pku.edu.cn key lab of high confidence software technology peking university beijing chinaxinxia xin.xia acm.org softwareengineeringapplication technology lab huawei chinaqing wang 2 pilcrow wq iscas.ac.cn institute of software chinese academy of sciences beijing china abstract codesummarization thetaskofgeneratingusefulcommentsgiven the code has longbeen ofinterest.
mostof theexistingcodesummarization modelsare trainedandvalidatedon widely usedcode comment benchmarkdatasets.
however little isknown aboutthe quality of the benchmark datasets built from real world projects.
arethebenchmarkdatasetsasgoodasexpected?tobridgethegap weconductasystematicresearchtoassessandimprovethequality of four benchmark datasets widely used for code summarization tasks.
first we propose an automated code comment cleaning tool that can accurately detect noisy data caused by inappropriate data preprocessing operations from existing benchmark datasets.
then weapplythetooltofurtherassessthedataqualityofthefourbenchmark datasets based on the detected noises.
finally we conduct comparative experiments to investigate the impact of noisy data on the performance of code summarization models.
the results showthatthesedatapreprocessingnoiseswidelyexistinallfour benchmarkdatasets and removingthese noisydataleads toa significant improvement on the performance ofcode summarization.
also with laboratory for internet software technologies institute of software cas 2alsowithuniversityof chinese academy of sciences 3bothauthors contributed equally tothisresearch also with science technology on integrated information system laboratory instituteof software cas pilcrowcorresponding author esec fse november 14 18 singapore singapore copyright held by the owner author s .
acm isbn978 .
believe that the findings and insights will enable a better understanding of data quality in code summarization tasks and pave the wayforrelevant research andpractice.
ccs concepts software and its engineering open source model generaland reference empirical studies .
keywords code summarization data quality empirical study acmreferenceformat lin shi fangwen mu xiao chen song wang junjie wang ye yang ge li xin xia and qing wang.
.
are we building on the rock?
on the importance of data preprocessingfor code summarization.in proceedings ofthe30thacmjointeuropeansoftwareengineeringconferenceandsymposiumonthe foundationsofsoftwareengineering esec fse november 14 18 singapore singapore.
acm new york ny usa 13pages.
introduction codesummarizationconcernstheproductionofanatural language description of source code that facilitates software development and maintenance by enabling developers to comprehend ideate anddocumentcodeeffectively.learning basedmodelshavebeen widely leveraged for the advantages in semantic modeling and understandingoflanguages.
similarto manyotherlearningtasks code summarization models require large scale and high quality training datasets.
to that end multiple benchmark datasets for code summarizationtasks have been constructed from real world projectrepositories e.g.
github andarepopularlyusedinmany codesummarizationstudies.forexample funcom wasreleased with over .1m code comment pairs from over 29k java projects in thiswork islicensedunderacreativecommonsattribution4.0international license.
esec fse november14 singapore singapore lin shi fangwenmu xiaochen song wang junjiewang ye yang geli xinxia andqingwang benchmark datasets origin clean dataclean dataorigin data controlled datatraining set test set models sec.
the code comment cleaning toolsec.
impacts on the performance of code summarizationsec.
quality assessment of benchmark datasets benchmark datasets clean sec.
taxonomy of noisy data non literal over splittingauto code duplication ......comment code ...... heuristic rules figure overview ofour researchmethodology.
thesourcererrepository.manycodesummarizationmodels such asre2com deepsumm andeditsum aretrainedand evaluatedtoberelativelyeffectiveonit.similarpopulardatasets includetlc csn andpcsd .
althoughthebenchmarkdatasetsareexpectedtobeofgoodquality noise is inevitable due to the differences in coding conventions and assumptions employed in modern programming languages andides aswellasadhocnatureofdevelopmentprocessesand practices .forexample sourcecodeingithubiscontributed by developers all around the world thus their comments are likely to contain multiple natural languages that can lead to increases in complexity regarding the understanding and maintenance of source code.
existing studies also have confirmed the existence of many different types of noise in various benchmark datasets such as auto generated code todo comments and incomplete comments despite their data cleaning efforts.
particularly steidl etal.
analyzedfiveopensourceprojects and reported that nearly one third of the comments do not promote systemunderstanding.
toinvestigatetheaforementionedconcernsofdataqualityfor code summarization we conducta systematic study to assess and improvethequalityoffourwidely usedbenchmarkdatasets i.e.
funcom tlc csn andpcsd.theresearchmethodologyoverview consists of four main steps as illustrated in figure .
first we propose a taxonomy of different types of data noises due to inappropriate or insufficient data preprocessing in code summarization derivedfromobservationsontheselectedfourbenchmark datasets.second webuildarule basedcleaningtool namedcat code comment cleaning tool for automatically scanning and detectingtheoccurrencesanddistributionofdatanoisesforagiven dataset basedontheproposedtaxonomy.themanualvalidation resultsshowthatthetoolcanaccuratelydetectnoisydata.third we conduct an evaluation study to assess the data quality of the four widely used benchmark datasets.
the results show that noisy dataextensivelyexistinthefourbenchmarkdatasets rangingfrom to .
finally we investigate the impacts of noises on three typical code summarization models i.e.nngen ncs and rencos by comparing their performance trained on the same datasetsbeforeandafterdatacleaning.theabovefourstepswill be elaborated in later sections i.e.
sec.
to sec.
respectively.
theresultsshowthat removingnoisydatahaveapositiveinfluenceonmodelsummarizationability.trainingthreemodelswith the filtered datasets improves the bleu by and respectively.the majorcontributionsofthis paper are as follows.
to the best of our knowledge it is the first to systematically study the patterns and impact of the noises in various code summarizationdatasets.
wedevelopanautomateddatacleaningtool namedcat for codesummarizationdatasets whichcanhelpdistillhigh quality code commentdata.
we perform a comprehensive assessment on data quality of benchmarkdatasests whichprovidespracticalinsightsforfuture code summarizationresearch.
we conduct a comparative analysis on the performance of code summarizationmodelstrainedontheoriginanddistilledbenchmark datasets our results demonstrate that removing noises yieldssignificant modelperformance improvement.
we release cat and the distilled benchmark datasets to the general public in order to facilitate the replication of our study andits extensive applicationinothercontexts.
intheremainderofthepaper section 2illustratesthepreliminaries.
section 3introduces the taxonomy of noisydata.
section presentsthecode commentcleaningtool.section 5demonstrates the quality assessment of benchmark datasets.
section 6shows the impact of noisy data on the performance of code summarization.
section7discussesresultsandthreatstovalidity.section 8surveys the relatedwork to our study.section 9concludes this paper.
preliminaries thissectionbrieflyintroducestheliteratureofcodesummarization as well as fourwidely usedbenchmarkdatasets.
.
codesummarization codesummarization aimsatgeneratingacommentfora givenblockofsourcecodethatcanhelpdevelopersbetterunderstandandmaintainsourcecode.theessentialtaskistotranslate thecodewritteninprogramminglanguagesintocommentswritten in naturallanguages.meanwhile comments maydescribe not only the functions but also the design intents program logic and functionalities of programs behind the source code.
the existing code summarization models can be categorized into three different types based on the techniques used i.e.
information retrieval ir based approaches neural machine translation nmt basedapproaches andhybrid approaches that combine ir andnmttechniques.
specifically ir based code summarization models use ir techniquestoextractkeywordsfromthesourcecodeandcomposethem intoterm basedsummarizationforagivencodesnippet .
for example edmund et al.
generated code summarization for agiven codesnippetbyretrievingthereplicatedcodesamples from the corpus with clone detection techniques.
recently with theboomingofdeeplearningtechniques manynmtbasedcode summarization approaches have been proposed which train the 108are we building onthe rock?
on the importance of datapreprocessingforcode summarization esec fse november14 singapore singapore neuralmodelsfromalarge scalecode commentcorpustoautomaticallygeneratesummaries .
forexample iyer etal.
treatedthecodesummarizationtaskas an end to endtranslationproblem andfirstintroducednmtinto codecommentgeneration.thehybridapproaches leveragetheadvantagesofirandnmttechniquesforimproving codesummarization.
for example zhang et al.
first retrieved topsimilarcodeinthetrainingdataforagivenpieceofcodeand theninputthemintoannmtmodelforsummarizationgeneration.
.
benchmark datasets asintroducedearlier thisstudyconductsvariousexperimentson fourwidely usedcodesummarizationdatasets including funcom tlc csn andpcsd .thedataformatofthesedatasets isprimarilyrepresentedusing code commentpairs wherethe codedataisatthegranularityof method level .eachdatasetappliesitsownoperationswhenextractingandpreprocessingtheraw data.table 1summarizestheinformation ofdescriptive metadata and associated studies where each dataset has been employed in existing literature.
more specifically funcomis a collection of .1m code comment pairsfrom29kprojects.foreachmethod itextracteditsjavadoc comment and treated the first sentence in the javadoc of each methodasitssummary.
tlchas87kcode commentpairscollected from more than 9k open source java projects created from to with at least stars.
it extracted the java methods and their corresponding javadoc comments.
these comments are considered ascodesummaries.
csncontainsabout2mmethodandcomment pairsmined frompubliclyavailableopen source non forkgithub repositories spanning six programming languages i.e.
go java javascript php python andruby.inthisstudy weconductthe experimentson the javaportion ofthe csndataset.pcsdcontains 105k pairs of python functions and their comments from open sourcerepositoriesingithub.specifically itusesdocstrings i.e.
thestringliteralsthatappearrightafterthedefinitionoffunctions as summariesfor pythonfunctions.
the taxonomyofnoisydata an essential and effective starting point is a systematic and robust categorization of data noises.
this section presents details on how thenoisydatataxonomyisbuilt andthedescriptionsandexamples for every categories.
.
taxonomy construction we employ an open cardsort processby involving nineparticipants.participantsincludetwophdstudents fourmasterstudents andthreeseniorresearchers.allofthemhavedoneeitherintensive table benchmarkdatasetsinformation name funcom tlc csn pcsd year source sourcerer github github github download language java java java python pairs train val test .
.5by project 1by function 1by project 2by function trained on models research work with software development or have been actively contributing to open source projects.
the sorting process is conducted in multiple rounds.
for each round we randomly sample code comment pairs without replacement from the four benchmark datasets pairs for each .
in the first round all participants labelthesamesampleddata withanintensivediscussionsessionto achieve conceptual coherence about noisy categories.
the average cohen skappais0.
whichindicatessubstantialagreement.then asharedpoolofcategoriesisutilizedandcarefullymaintained and eachparticipantcouldselectexistingcategoriesfromand oradd newcategorynamesintothesharedpool.thesortingprocessends when there isno new categoryaddedfor twoconsecutiverounds.
intotal weconducted10roundsandlabeled1 600pairs ofsource code and the corresponding comments pairs for each of the fourbenchmarkdatasets .thedetailedannotationresultscanbe foundinsection .
.
.
.
comment relatednoisy data partialsentence.
sinceitisacommonpracticetoplaceamethod s summaryatthefirstsentenceofitscomment mostresearchers use the first sentences of the code comments as the target summaries.
while we have observed that some inappropriate processingcanleadtopartialfirstsentencescollected.forexample funcom onlycollectsthefirstlinefromthefollowingjavadocasthecomment i.e.
returns the high value where the next line that should be part of the first sentence is missing.
this is primarily due to automaticsplitting using newlinecharacters such as n .
returns the high value for an item within a series.
comment funcom returns the high value verbosesentence.
whencollectingthefirstsentenceasthetarget comment some inappropriate processing will lead to verbose first sentences collected.
for example pcsdexcessively includes theargument description arguments course data into the functionality summary.
generate a csv file containing a summary of the xblock usage arguments course data comment pcsd generate a csv file containing a summary of the xblock usage arguments course data content tampering.
developers may use html tags for documentation auto generation or urls for external