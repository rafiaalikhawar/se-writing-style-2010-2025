explanation guided fairness testing through genetic algorithm ming fan mingfan mail.xjtu.edu.cn xi an jiaotong university chinawenying wei waving stu.xjtu.edu.cn xi an jiaotong university chinawuxia jin jinwuxia mail.xjtu.edu.cn xi an jiaotong university china zijiang yang zijiang xjtu.edu.cn xi an jiaotong university chinating liu tingliu mail.xjtu.edu.cn xi an jiaotong university china abstract the fairness characteristic is a critical attribute of trusted ai systems.
a plethora of research has proposed diverse methods for individual fairness testing.
however they are suffering from threemajorlimitations i.e.
lowefficiency loweffectiveness and model specificity.
this work proposes expga an explanationguided fairness testing approach through a genetic algorithm ga .
expga employs the explanation results generated by interpretable methods to collect high quality initial seeds which are prone to derivediscriminatorysamplesbyslightlymodifyingfeaturevalues.expgathenadoptsgatosearchdiscriminatorysamplecandidates byoptimizingafitness value.benefitingfromthiscombinationof explanationresultsandga expgaisbothefficientandeffectiveto detect discriminatory individuals.
moreover expga only requires prediction probabilities of the tested model resulting in a bettergeneralization capability to various models.
experiments on multiplereal worldbenchmarks includingtabularandtextdatasets show that expga presents higher efficiency and effectiveness than four state of the art approaches.
ccs concepts software and its engineering software creation and management.
keywords explanation result fairness testing genetic algorithm acm reference format ming fan wenying wei wuxia jin zijiang yang and ting liu.
.
explanation guidedfairnesstestingthroughgeneticalgorithm.in 44th international conference on software engineering icse may pittsburgh pa usa.
acm new york ny usa pages.
https corresponding author.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa association for computing machinery.
acm isbn ... .
introduction artificialintelligence ai systemshavebeenpenetratingvarious aspects of our lives.
except for the benefits brought by ai systems however an ai system would harm the fairness of our society if it makes biased decisions and thus presents a discriminatory nature.
recentstudies pointedoutthatsomefamiliaraisystems behavewithdeviationsingenderandskincolor.forexample inface recognition systems developed by ibm microsoft and face the detection accuracy for white skin men is much higher sometimes higher than that of black skin women.
fairness characteristic is critical for trusted ai systems and fairness testing has attracted much attention recently in both industrial e.g.
ibm s ai fairness google s ml fairnessgym andacademiccommunities .thegdpr a dataprivacylawacrosseurope claimsthat thepersonaldatashall be processed fairly in relationto the data subject .
definitions of fairness in existing work fall into two categories individual fairness i.e.
similarindividualsaregivensimilardecisions group fairness i.e.
equal decisions are made for different groups.
most research on software fairness including this work has focused on individual fairness .
individual fairness testing mutates the input samples to generate sufficient testing samples and checks whether these samples are individual discriminatory.
typical approaches include themis aequitas sg adf andmt nlp .
in general these fairness testing methods first construct a set of seed samples that are suspiciously discriminatory using random or clusteringtechniques.then theysearchformorediscriminatory samples around the seeds through different strategies such as symbolicexecutionandgradient basedmethods.afterthat they expand the original dataset by modifying the labels of detecteddiscriminatory samples and finally retrain to improve the model fairness.priorindividualfairnesstestingapproacheshaveachieved relatively good performance however theystillsuffer fromthree major limitations.low efficiency whenhandlingcomplexdeeplearningsystems sg isinefficientduetotheuseofsymbolicexecution.symbolic execution whichtypicallyrequiressmtsolvers iscomputationally expensive to generate particular test cases.low effectiveness themis aequitas and mtnlp adopt unguided or semi guided search strategies leading to a low success rate of the discriminatory sample generation.model specificity adf a lightweight approach requires gradientinformation.suchinformationisextractedfromthewhitebox dnn model but unavailable in black box models.
ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa ming fan and wenying wei et al.
label income 50klabel income 50k same samenative country sex...... marital status figure an example pair of discriminatory samples to address these limitations this work proposes expga a model agnostic fairness testing approach.
expga employs explanation results and the genetic algorithm ga thus detecting discriminatory samples both efficiently and effectively.
our expga first leverages the explanation results generated by interpretable methods to identify a set of seed samples.
seed samples are prone to derive discriminatory samples by slightlymodifying their feature values.
then using seed samples asthe input expga employs the ga to produce large amount of discriminatoryoffspring.inexpga theexplanationresultsfavor high quality seeds.
ga can guide the search process towardsdiscriminatory sample solutions by optimizing the fitness value.furthermore expga is able to handle black box models since it only requires prediction probabilities from the models.
to evaluate the efficiency and effectiveness of our expga we compare expga with four state of the art discriminatory testingapproaches includingaequitas sg adf andmt nlp.
using five diverse training models we conduct comprehensiveexperiments on three tabular dataset benchmarks handled by aequitas sg and adf and two text dataset benchmarks handledbymt nlp .thedataofourexperimentscanbefound on github1.
the results indicate that expga can detect discriminatory samplesmoreefficientlyandeffectivelythanthebaselineapproaches.
inparticular ontabulardatasets expgaconsumeslessthan0.2s toidentifyadiscriminatorysamplewithasuccessrateof49 on average outperformingaequitas sg andadfinbothefficiency and accuracy.
on text datasets expga is five times more efficient andtwicemoreeffectivethanmt nlp.inaddition theperformance ofexpgaismorestableinvariousmodelsthanthefourbaseline approaches indicating a better generalization capability of expga.
in summary our work makes the following contributions i we propose a model agnostic fairness testing approachcalled expga.
it can identify individual discriminatorysamples for both tabular data and text data by combining the local explanation and ga. ii we conduct extensive experiments on three tabular datasets andtwotextdatasetstoevaluateexpga.theresultsshow that expga outperforms four state of the art approaches in terms of both efficiency and effectiveness.
in the rest of this paper section introduces the background andtheproblemdefinition.section3illustratesexpgaandsection 4reports experimentalresults.section5 and6discussthe threats to validity and related work.
section draws conclusions.
background and problem definition .
individual discrimination there exists individual discrimination in a classifier if some protected attributes explicitly play important roles in the decisionmaking process .
the protected attributes are predefined in specific domains such as gender country and age.
formally given a classifier f x a xdenotes the input domain and a a1 a2 anrepresentsthefeaturespaceof x. each input sample x xis represented as a ndimensional feature vector i.e.
x a1 a2 ... an .
considering a set of protected attributes p aclassifier fpresentsindividualdiscriminationifthere existsapairofsamples xandx prime satisfyingthefollowingeq.
wherex a1 a2 ... an andx prime a prime a prime ... a prime n .
in this case x andx primeareapairofdiscriminatorysamplesin f.hereweusethe symbols and trianglerightequaltodenotethatonefeatureiscloselyrelatedand unrelated to a specific protected attribute respectively.
ai x ai p p ai a prime i aj x a j trianglerightequalp p a j a prime j f x f x prime fig.1presentsanexamplepairofdiscriminatorysamplesthat areclassifiedintodifferentclasses.theonlydifferencebetweenthe twosamplesistheirvaluesofsexfeature whichisrelatedtothe protectedattribute gender indicatingtheunfairnessoftheclassifier withregardto gender.notethattheapproachofignoringcertain protectedattributestomitigatemodelbiasisnoteffectivedueto the presence of redundant encoding in the training dataset .
.
locally interpretable method the drawbacks of invisible black box models raise a series of security issues many studies have proposed interpretable methods to measure the credibility of the prediction process.among them the locally interpretable methods are the most prominent.
they interpret the predictions of black box classifier models by providing potentially important features that dominate the decision results.
by approximating the decision boundaryof the classifier model the generated interpreter learned withinterpretable methods can explain the prediction specificallyfor a single sample.
concretely given a test sample x and its classification result y f x an interpreter gwill output explanation results e a sorted set of features ranking based on their importance to this decision result.
.
genetic algorithm the genetic algorithm ga is commonly used to search for approximate optimal solutions .
ga first randomly initializes a population with potential solutions to the targetproblem.
it then approximates the optimal solution iteratively throughthreebiologicallyinspiredoperators includingselection crossover and mutation.
specifically gaencodeseachsolutionintheinitialpopulation into a set of gene sequences and designs a fitness function tomeasure the qualities of current solutions.
in the selection step solutions with better fitness scores tend to be selected into the authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
explanation guided fairness testing through genetic algorithm icse may pittsburgh pa usa model datasetinitialization phase preprocessingexplanation generationconstruction of seed sample set ranknum selection crossover mutation optimization phasediscriminatory samples figure the overview of expga.
newpopulation.inthecrossoverstep twoparentsolutions gene fragmentsareexchangedtoproducechildrensolutions.afterthat a part of gene fragments of children solutions are mutated randomly probably breaking out of the local search space and enhancing the diversityofsolutions.garepeatsthewholeprocessuntila desirable solutionisfoundorthenumberofiterationsreachesthemaximum as configured.
.
problem definition for a given model if the input is identified as an individual discriminatory sample this model would be suffered from individual discrimination and may produce a biased decision.
in this software fairness testing work we assume that the dataset xfor training a model the corresponding feature space a and the protected attribute set pare provided.
the problem is that given a blackbox model dtrained using dataset x can we effectively and efficiently detect individual discriminatory samples for d?b y supplementingthediscoveredindividualdiscriminatorysamplesto x themodelfairnessisdesiredtohaveanimprovementthrough retraining.wehavenopriorknowledge i.e.
trainingalgorithms andparameters abouttheblack boxmodel makingexistingwhite boxbasedmethodsthatrequiremodelinnerinformationineffective.
methodology inthispaper weproposetheexpga alightweightmodel agnostic software fairness testing method.
fig.
illustrates the overallframework of expga.
the inputs are a dataset and its trained model.
in the initialization phase expga employs an interpretable method to search for seed samples in the entire feature domain.
seed samplesare more likely to derivediscriminatory samples by slightly modifyingfeature values thanother samples.
usingseeds asinputs the optimizationphase employsgatoefficientlygenerate a large number of discriminatory samples through the selection crossover and mutation operators.
to illustrate the generalizability of expga we present two diversescenariosthatexpgaisabletohandle theinputdatasetis a tabular form or a set of text paragraphs.
fig.
shows thetwo corresponding examples.
the first example is selected from thecensusincomeclassificationdataset asshowninfig.
a .
each sample includes features of marital status sex occupation sample sample no male lawyer 50h yes male teacher 36h ......indeed in my opinion the movie itself rates as one the alltime great experiences of silent cinema.
a creative artist of th e first rank robertson is a master of pace camera angles and montage.
he has also drawn brilliantly natural performances from all his players...... a an example in the census income classification dataset b an example in the sentiment analysis datasetlabel income 50k label income 50k label positive features marital status sexoccupation educationhours per week figure3 twoexamplesintabulardatasetandtextdatasetscenarios.
education hours per week etc.
the labels of sample and sample areincome 50kandincome 50k respectively.
another example is selected from the sentiment analysis dataset for movie comments i.e.
imdbdataset .asshowninfig.
b eachsampleinthis dataset is a text paragraph.
although the samples in the above two differentscenariosareheterogeneous theybothcanbeconsistently represented as feature vectors x a1 a2 ... an .
for tabular samples aidenotes the i th feature value for text samples ai denotes the i th word.
for convenience we will uniformly use aito denote the i th word of sample x. .
initializationphase theinitializationphasefirstidentifieswhetherasamplecontains sensitive words that are related to the given protected attributes p. then it learns an interpreter by employing the interpretable methods outputting the words that dominate decision results.
the samples whose sensitive word ranks high in these explanation results are selected as the seed samples.
concretely this phasecontains three steps i.e.
prepossessing explanation generation and construction of the seed sample set.
.
.
preprocessing.
this step will identify the sensitive words that are related to the protected attributes.
considering that the protectedattributeis gender wecaneasilyidentifythattheword sex infig.
a isdefinitelyclosetotheattributesincetheypresent a similar semantic meaning.
however in some cases like fig.
b it is difficult to literally decide the relationships between the words and protected attributes due to the diversity of the language.
inthis example the word master might be related to the gender information implicitly.
tosolvethisproblem wemanuallyconstructaknowledgegraph containing the relations between sensitive words and protected attributes.
at first we obtain the protected attributes following an existing work .
then we construct a set of general relations such as isa relatedto distinctfrom accordingto the conceptnet a widely used open and multilingual knowledgegraph.afterthat weexpandthegraphbyaddingsimilar wordswithawordembeddingtoolsuchas glove .foranew word if there exists a similar word in the graph of which the similarity surpasses the configured threshold i.e.
.
it will be authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa ming fan and wenying wei et al.
knowledge graphgender has countryhasreligionhas ...hasmaster father motherdistinctfrom isa christianity catholicism ...islamism isa america spanish ...african isa... agepersonhas has figure a partial prior knowledge graph.
algorithm initialization phase input x p classifier f interpreter g seed num output seed sample set seedset 1seedset 2foreachsamplexinxdo 3if seedset seed numthen label f x e g x f label foreachaiinxdo ifai p p pthen r ranknum ai e ifr then seedset seedset x 11returnseedset addedtothegraph.finally onceawordhasanaccessiblepathto the protected attribute in the graph it is regarded as a sensitive word that is related to the specific protected attribute.
note that the knowledge graph only needs to be constructed once.
fig.
depicts a sub graph of the knowledge graph we have constructed.thesub graphcontainsasubsetofprotectedattributes such asgender age country and religion.
an edge of this graph indicates a relation from a subject to an object.
for instance america isa a country.
based on this graph for the example shown in fig.
b we can identify that master is a person and it hasthegenderattribute.thus itisidentifiedasasensitiveword related to gender that is master gender.
.
.
explanation generation.
our assumption of using interpretable methods is that non sensitive words should have a synergistic influence dominating the classification of samples withoutdiscrimination.onthecontrary fordiscriminationsamples their sensitive words would rank relatively high in explanationresults.
as a result a slight perturbation of sensitive words ondiscrimination samples might dramatically change the predictedlabel.
furthermore we rely on the model agnostic interpretable methodssincetheyaregoodattraininganinterpreterforanopaque model thus benefiting the generalization of expga to different models such as cnn mlp and svm.
fig.5presentstheexplanationresultsforthetwoexamplesin fig.
.thewords male and master aremarkedwithredcolor sincetheyareidentifiedassensitivewordsduringtheprepossessing step.
the darker blue shading words play more essential roles in the classification process than the lighter shading words.sample sample no male lawyer 50h yes male teacher 36h a explanation result of the first example b explanation result of the second examplelabel income 50k label income 50k label positive ......indeed in my opinion the movie itself rates as one the alltime great experiences of silent cinema.
a creative artist of th e first rank robertson is a master of pace camera angles and montage.
he has also drawn brilliantly natural performances from all his players...... figure explanation results of the two examples.
.
.
constructionofseedsampleset.
thisstepconstructsaset of seed samples from which more discriminatory samples will be derived by the genetic algorithm.
we first rank the explanation resultebasedontheimportancescoreassignedbytheinterpretable method.thenwedesignafunction ranknum ai e toobtainthe rank number of a word aiine.
by iteratively obtaining the rank number of each sensitive word we select the highest number to checkwhetheritishigherthanathresholdvalue .ifhigher the sample will be added to the seed sample set.
controls the balance between the number and quality of the selected seed samples.
in fig.
given that all samples are selectedasseedsamplessincetheranknumbersofsensitivewords containedinthesesamplesare2.however if noneofthem wouldbeselected.itcanbeseenthatabigger wouldresultinmore seed samples however the rank number of sensitive words might be lower making the sensitive words less important.
therefore the quality of these samples would be poor.
on the contrary asmaller can result in high quality seed samples with a smaller number whichmightaffecttheefficiencyofthegenerationoffinal discriminatory samples.
algorithm1formalizestheinitializationimplementations.the inputsincludethedataset x protectedattributeset p aninterpreter gconstructed by leveraging the existing interpretable method and the target number of seed samples seed num .
the output is a set of seed samples denoted as seedset.
each sample is first fed to the classifierandinterpretertogenerateitsexplanationresults lines .thesample whichcontainsanysensitivewordwithasmaller ranking number than will be added to seedset lines .
.
optimizationphase the optimization phase leverages the biologically inspired ga to generate discriminatory samples taking advantage of its highconvergence speed and strong local searching ability.
here weuse algorithm to help understand the order of the threeoperators.
at first we will construct the initial population basedon the seedsetinput line .
then the complete process of the selection crossover and mutation lines will be repeated l times as set by users.
after finishing each iteration we leverage discrniminatorycheck to check whether a candidate sample x is atruediscriminatory sample.
if the output set dissetdoes not authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
explanation guided fairness testing through genetic algorithm icse may pittsburgh pa usa algorithm optimization phase input seedset l output discriminatory sample set disset 1curpopulation initpopulation seedset 2num 3whilenum ldo 4newpopulation selection curpopulation 5curpopulation crossover curpopulation 6curpopulation mutator curpopulation 7foreachsamplexincurpopulation do iftrue discrniminatorycheck x then disset disset x 10returndisset containthesample xwillbeaddedtotheset lines7 .thedetails of the above steps are introduced below.
.
.
constructionofinitialpopulation.
thisstepconstructsthe initialpopulationbasedonseedsamples.however itisdifferent to construct the initial populations for the tabular dataset and the text dataset.
for seed samples from a tabular dataset all of them will be added to the initial population since they have the same number of features.
for seed samples from a text dataset it is unfeasible to initialize the population by simply aggregating all seed samples since the lengths of paragraphs in different text samples are diverse.
asimple replacement of a word aiin one sample with that word a prime iin another sample in the next crossover step would lead to a new sentence with syntax errors or incorrect semantic.
in this work we construct an initial population correspondingly from each seed sample.
specifically for a given seed sample we first randomly select knon sensitive words from it.
correspondingly k newwordsthathavesimilarmeaningswiththose kselectedwords canbeobtainedthroughthesimilaritycomputationbytheword embeddingtool glove.afterthat wereplacetheoriginallyselected words with these new words producing knew samples derived fromtheseedsample.thesetofthese k 1sampleswillforman initial population.
our work sets k according to experimental experience.
.
.
selection.
this step selects high quality samples to generate the next population through optimizing a fitness function.
we designafitnessfunction fit x forasample x.fit x quantifiesthe extentofachangefromthepredictionprobabilityof x primetothatof x prime prime.x primeandx prime primeare derived from x. we assume that xwith a higher fitness score is more likely to be mutated into a discriminatory sample.fit x can be obtained with eq.
fit x prob x prime l prob x prime prime l wenowintroducehowtoderive x primeandx prime primefromx.giventhat x contains a sensitive word ai p we construct the tildewideai tildewideai that isapairofsensitivewordsbutwithoppositesemanticmeanings.
we then substitute tildewideaiand tildewideaifor the original ai producing two newsamples x primeandx prime prime respectively.
prob x prime l andprob x prime prime l correspondtothepredictionprobabilityof x primeandx prime primeforthespecific labell.algorithm selection of new population input curpopulation output newpopulation 1newpopulation 2sumfit summationtext.
x curpopulation fit x 3 x 4foreachsamplexincurpopulation do 5 x fit x sumfit x x x 7while newpopulation curpopulation do 8z select curpopulation x 9newpopulation newpopulation z 10returnnewpopulation algorithm crossover and mutation input curpopulation cr mr output curpopulation 1foreachsamplexincurpopulation do 2x prime randomselect curpopulation ai ... a j randomselect x cr 4frag x i j ai ... a j 5x x.replace frag x i j frag x prime i j 6x prime x prime.replace frag x prime i j frag x i j 7foreachsamplexincurpopulation do 8foreachaiinxdo ifai trianglerightequalpthen trianglerightequaldenotes that aiis not related to p a prime i calsimilarword ai mr x x.replace ai a prime i 13returncurpopulation for example given ai actor and p gender tildewideaiis the same as ai and tildewideaiwould be actress .
another example is that when ai master p gender and aihas a relationship with p we would set tildewideai male master and tildewideai female master based on the prior knowledge graph we have constructed in section .
.
.
algorithm shows the selection in terms of fit x .
the input is current population curpopulation and the output is a new population newpopulation afterselection.
sumfitdenotesthesum of all fitness scores of the samples in curpopulation .
x denotes the probability value of xthat might be selected into the new population.
x isasetofall x ofsamplesin curpopulation .
finally selecting based on the probability distribution indicated from x wecanconstructanewpopulationthathasthesame sample size as curpopulation .
.
.
crossover and mutation.
we conduct the crossover and mutation operators on the selected population to generate more varieties expanding the search space of discriminatory samples.
algorithm4illustratesthecrossover lines1 andthemutation lines7 .thetwoparameters crandmr denotethecrossover rate and mutation rate.
werandomlychoosepairsofsamplesasthecrossoverparents fromthecurrentpopulationwithaprobabilityof cr.forapairof samples xandx prime fragments ai ... aj and a prime i ... a prime j are extracted from xandx primerandomly.
the two samples will exchange their fragments with each other.
during the mutation each word of a sample in the current populationisreplacedwithitssimilarwordwithaprobabilityof mr. authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa ming fan and wenying wei et al.
algorithm checking of discriminator sample input x classifier f p output dis 1dis false 2if ai x ai p p pthen tildewideai tildewideai getpair ai 4x prime x.replace ai tildewideai 5x prime prime x.replace ai tildewideai 6iff x prime f x prime prime then dis true 8returndis sample sample no male lawyer 50h yes male teacher 36hcrossoverno male teacher 50h yes male lawyer 36hmutation doctor no 40h ......indeed in my opinion the movie itself rates as one the all time marvelous experiences of silent cinema .a creative artist of the first rank robertson is a master of pace camera angles and montage.
he has also drawn excellent natural performances from all his players ...... ......indeed in my opinion t h ef i l m itself rates as one the all time great experiences of silent theater .a creative artist of the first rank robertson is a master of pace camera angles and montage.
he has also drawn brilliantly natural performances from all his players ............indeed in my opinion the film itself rates as one the all time great experiences of silent theater .a creative artist of the first rank robertson is a master of pace camera angles and montage.
h e has also drawn brilliantly natural performances from all his players............indeed in my opinion the movie itself rates as one the all time marvelous experiences of silent cinema .a creative artist of the first rank robertson is a master of pace camera angles and montage.
he has also drawn excellent natural performances from all his players......sample sample crossover viewfeels famous participants mutation a crossover and mutation steps of tabular dataset samples b crossover and mutation steps of text dataset samples figure examples for the crossover and mutation steps.
note that the mutation excludes the sensitive words.
concretely categoricalwordsuchas no and teacher willbereplacedwith similarwordsinthesamecategorysuchas yes and lawyer text word such as great and brilliantly will be replaced with similar words using the tool glove numeric word such as and 36h can be replaced with numeric data like and 40h .
fig.
a shows the crossover and mutation for the tabular samplesandfig.
b forthetextsamples.infig.
a thecrossover fragmentsare lawyer insample1and teacher in sample .
then the words of teacher yes and 36h are mutated with doctor no and 40h .
in b the different words between the two samples are underlined.
the two sub sentencesintheorangeshadingareexchangedwitheachother.
afterthat somewordslike excellent willbemutatedwiththeir similar words like famous .
.
.
checkingofdiscriminatorysample.
aftereachiterationof theselection crossover andmutation wecheckwhetheragiven sample is a discriminatory one by identifying their label difference using algorithm .
specifically once a sample contains a sensitive wordai we leverage the function getpairto return tildewideai tildewideai .
then we replace the word aiwith the new words and obtain two new samples.
once the two new samples predicted labels aredifferent thegivensamplewillbecheckedasadiscriminator one.
evaluation in this section we evaluate the performance of expga on boththe tabular datasets and text datasets.
we first introduce our experimentalsetup section4.
.thenweconducttheevaluation by answering the three research questions.
rq1 whatistheperformanceofexpgainfindingdiscriminatory samples on tabular datasets?
section .
rq2 whatistheperformanceofexpgainfindingdiscriminatory samples on text datasets?
section .
rq3 towhatextentdothediscriminatorysamplesgeneratedby expga improve the model fairness through retraining?
section .
.
experimental setup .
.
datasets.
we conduct the evaluation on fivepopularpublic benchmark datasets including three tabular datasets and two text datasets.
their descriptions are listed below census income dataset labels whether the income of anadultisover 50kornot.thereare32 561samplesand features involving three protected attributes i.e.
gender age andrace.
german credit dataset is used to assess the credit level of applicants i.e.
good or bad based on their personal conditions.
there are samples depicted by diverse features.
the protected attributes are genderandage.
bank marketing dataset is related to the direct marketing campaign of a portuguese banking institution.
theobjectiveistopredictwhethertheclientwouldsubscribe toatermdepositbasedontheirinformation.thereare16 features and samples.
the protected attribute is age.
imdbdataset isalargemoviereviewdatasetforbinary sentimentclassification.theoriginaldatasetcontains50 samples where each sample contains words on average.
the protected attributes are genderandcountry.
sst dataset is also used to classify whether a movie comment is positive or negative.
the original dataset contains samples where each one has about words onaverage.theprotectedattributesarethesameasthose of the imdb dataset.
for convenience we use cencus credit and bank to denote the abbreviations of the three tabular datasets.
.
.
baselineapproaches.
onthetabulardatasets weselectthree state of the artapproaches i.e.
aequitas sg andadf asthe baseline approaches.
their descriptions are listed below aequitas it is a two phase method to search for individual discriminatory samples where the search is random in its global phase and is directed in its local phase.
aequitas proposes three schemes i.e.
random scheme semi directedscheme andfully directedscheme toguide thelocalstage.afully directedschemeperformsbest which is adopted in our comparative experiment.
sg sg is based on program symbolic execution and interpretable method.
instead of directly obtainingexplanation results for a single sample sg uses lime to generate local perturbation samples to builda decision tree and generates test inputs by solving path constraints.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
explanation guided fairness testing through genetic algorithm icse may pittsburgh pa usa table training parameters of different models.
dataset modeltraining parameters cencus svmrbf kernel credit rf100 trees bank mlpfive layer fully connected nn neurons each layer imdb cnnone convolution layer one pooling layer one fully connected nn layer sst lrl2 penalty adf it searchesforseed discriminatorysamplepairs along with the gradient direction and generates morediscriminatory samples locally guided by gradients in an opposite way.
inourexperiments weimplementaequitasandadfbasedon their public repositories published on github .
we implement sg based on the refactored version available in the adf repository.
the parameter settings in experiments are the same as the optimal settings given in their papers.
on the text datasets we use mt nlp as the baseline approach.
mt nlp producestestinputsaccordingtometamorphic testing mt .
mt nlp uses a knowledge graph to locatesensitivewordsinasentence.afterward itemploys two mutation schemes to generate perturbed sentences followedbyadiscriminatorycheckingonalltheseperturbed sentences.
given that our proposed approach and the baselines contain random factors when selecting seed samples we conduct the experiment30timesandusetheaveragevaluesontabulardatasets.notethatforthemt nlp sinceitssourcecodeisnotavailable we conduct the comparison using its paper results.
.
.
trainedmodelsandinterpreters.
toevaluatethegeneralizability of expga we conduct expga on various models which are also used in the baseline approaches.
as shown in table we train the svm random forest rf and mlp models on tabulardatasets and train the cnn and logistic regression lr models on text datasets.
here the traditional machine learning models i.e.
rf svm and lr models are trained via scikit learn library and the deep learning models such as mlp and cnn are trained via tensorflow .
table lists key parameters for training these models.theotherparametersabsentinthistableusethedefault values configured by scikit learn and tensorflow.
based on the trained models we leverage existing locally interpretable methods lime and shap to generate corresponding interpreters.
recent empirical studies o n differentinterpretablemethodspresentthatlimeperformsbeston tabular datasets and shap outperforms the others on text datasets.
consistently our experiments use lime for tabular datasets and shap for text datasets by default.
.
.
measurement metrics.
we use dss and sur metrics to measure the efficiency and effectiveness of discriminatory sample detectionbydifferentmethods.weassumethatadiscriminatorytable parameter settings in different datasets.
dataset c rm r cencus .
.05credit .
.05bank .
.05imdb .
.05sst .
.
sample detection method presents a better performance if it achieves a smaller value of dss and a larger value of sur.
dss denotes the average consumed time for generating a discriminatory sample dss time dsn wheretimeanddsndenotetheconsumedtimeandthenumber ofdetecteddiscriminatory samples respectively.surdenotes the success rate for generating discriminatory samples sur dsn tsn where tsn is the total number of generated testing samples.
it is worth noting that when calculating the number of generatedtestingsamples weonlyconsiderthesamplesthatarefedtothe classifiers to perform discriminatory checking.
.
.
parameters setting.
recall that expga involves three important parameters including for seed sample selection crfor the crossover and mrfor the mutation.
table lists these parameter values configured for each dataset.
to configure an appropriate value for each dataset we first randomly select samples from each dataset and generateexplanation results.
then we rank these samples in increasing orderaccordingtotheirsensitivewordrankingnumbers.
valueis setastherankingnumberofthe20thsample indicatingthatabout of the samplesare regarded as seed samples.
we configure cr andmraccording to our experimental experience.
ourexperimentsareconductedonaserverwithubuntu18.
operatingsystem intelxeon2.50ghzcpu nvidiartxgpu and 128gb system memory.
.
performanceon tabular datasets toanswerrq1 wecompareexpgawithaequitas sg andadf on three tabular datasets.
for the census and bank datasets we first randomly select samples from each as the input.
for the creditdatasetthatcontains600 samples weinputallthesamples into the methods.
then we insert a timer in each method to count the generated discriminatory samples every five minutes during aperiodofonehour.afterthat wemeasuretsn dsn dss and sur for each method.
table3 table4 andtable5presentperformancemeasurements of the four methods based on mlp rf and svm models respectively.
since the adf method requires gradient information which is unavailable in rf and svm models adf s results are absent in table and table .
four main observations are listed below.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa ming fan and wenying wei et al.
table the performance of detecting discriminatory samples for four approaches using mlp model.
datasetexpga aequitas sg adf tsn dsn dss sur tsn dsn dss sur tsn dsn dss sur tsn dsn dss sur censusgender .
.
.
.
.
.
.
.
age .
.
.
.
.
.
.
.
race .
.
.
.
.
.
.
.
creditgender .
.
.
.
.
.
.
.
age .
.
.
.
.
.
.
.
bank age .
.
.
.
.
.
.
.
table the performance of detecting discriminatory samples for three approaches using rf model.
datasetexpga aequitas sg tsn dsn dss sur tsn dsn dss sur tsn dsn dss sur censusgender .
.
.
.
.
.
age .
.
.
.
.
.
race .
.
.
.
.
.
creditgender .
.
.
.
.
age .
.
.
.
.
.
bank age .
.
.
.
.
.
table the performance of detecting discriminatory samples for three approaches using svm model.
datasetexpga aequitas sg tsn dsn dss sur tsn dsn dss sur tsn dsn dss sur censusgender .
.
.
.
age .
.
.
.
.
.
race .
.
.
.
creditgender .
.
.
.
.
.
age .
.
.
.
.
.
bank age .
.
.
.
.
.
forthetsn expgaoutperformsthebaselineapproaches significantly indicating that the size of search space via expga is much bigger than other approaches.
the main reason is the crossover and mutation operators employed in expga benefitingan efficientgenerationoflargenumbers of new testing samples and avoiding a localized search.
forthedsnanddss expgacangenerateabout4timesthe discriminatory samples compared with adf.
the advantage ofexpgaisprominentwhencomparedwithaequitasand sg.
for example in the census dataset with genderas the protected attribute expga requires only .03s indicated by dss to generate a discriminatory sample while aequitas and sg require 2s and .13s.
for the sur in most datasets expga presents the besteffectiveness except for a few cases where sur value by expgaisslightlysmallerthanthatofsg.forexample on thebankdataset thesurbyexpgais57.
whilesurbysgis68.
.wefigureoutthatthisexceptionalobservation is caused by the time period setting.
that is sur by expga has not achieved a stable value within a time of one hour while sg achieves its highest sur value.
after we extended the time to two hours the sur of expga surpasses sg.
we observe the stability of methods with changes ofmodels.
for all the three models expga can achieve a figure comparison of the diversity for four approaches.
relativelymorestableperformancewithasmallerdssand a bigger sur than other approaches.
the results indicate abettergeneralizabilityofexpgaondifferentmodels.for aequitas inthebankdatasetusingthesvmmodel the surisonly0.
muchlowerthanthose sometimes63.
inmlpandrfmodels.forsg inthecensusincomedataset using the svm model there exist dsn values equal to .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
explanation guided fairness testing through genetic algorithm icse may pittsburgh pa usa table the performance of expga using cnn model.
datasetexpga tsn dsn dss sur imdbgender .
.
country .
.
sstgender .
.
country .
.
wealsocomparethegatorandomexploration keepingseed exploration the same.
the results show that without ga all the sur values are not higher than and the dss values are times ofthoseusingga indicatingthatgaissignificantingenerating discriminatory samples.
the details can be found on our github.
insummary eventhoughaequitasisthefirstapproachthat proposes a two phase framework and inspires the following work itsrandomlyglobalsearchschemeleadstodetectioninefficiency.
sg has superior effectiveness than adf in general and sometimes outperforms expga.
however sg relies on the heavyweight symbolicexecutionthatrequiresasubstantialtimeconsumption when generating testing samples.
adf appears to be a good choice for white box deep learning models.
however the gradient information required by adf is often unavailable in most practical scenarios.
furthermore we measure the diversity of generated discriminatory samples for the above four approaches.
specifically we first randomly select discriminatory samples on the mlp modelforeachapproach.then wetranslatetheirfeaturesintotwodimensions using pca.
as illustrated in fig.
we can observe that thecoverageareaofexpgaislargerthanothers indicatingthat the discriminatory samples generated by expga present the most diversity among the four approaches.
finally consideringtherandomfactorsintheaboveapproaches weusestatisticalteststomeasuretheirsignificanceofdssinterms of two metrics i.e.
p value and effect size .
to calculate the p values weapplythenon parametricmann whitney wilcoxon tocomparethedssofexpgawiththoseofotherthreebaseline approaches.then tocalculatetheeffectsizemetric weemploythe non parametric vargha and delaney s hatwidea12statistics.
the results demonstratethatallthep valuesare0 andalltheeffectsizesare lowerthanthestandardthresholdvalue i.e.
.
indicatingthat the dss of expga is significantly smaller than the others.
answering rq compared with three baseline approaches our expga presents the best efficiency andeffectiveness in detecting discriminatory samples from tabulardatasets.onaverage expgarequireslessthan0.2s indicated by dss to detect a discriminatory sample with about49 indicatedbysur successrate.moreover the performanceofexpgaondiversemodelsismorestable than those of other approaches indicating that expgais model agnostic and can effectively handle black box models.table the performance of expga using lr model.
datasetexpga tsn dsn dss sur imdbgender .
.
country .
.
sstgender .
.
country .
.
.
performanceon text datasets toanswerrq2 werandomlyselect1 000samplesfromtheoriginal datasetsasourinput.notethatwesettheiterationparameterfor the text datasets las to stop our algorithm.
table and table list the discriminatory sample detection results by expga using cnn and lr models respectively.
from the results we can draw two main conclusions.
the results show that expga can effectively detect discriminatory samples.
however the performance measured by dssandsur ontextdatasets isworsethanthat ontabular datasetsshowninrq1.theperformancedifferenceisdue to that the word replacement in expga employs different strategiesfortextandtabulardatasets.ontabulardatasets a wordcanbereplacedbymanycandidates.ontextdatasets aword only can be finitely replaced by its synonyms to retain thesyntaxandsemanticcorrectnessofasentenceafterword substitution.asaresult dsncalculatedontextdatasetsis smaller than that on tabular datasets.
we observe that dss values on the imdb dataset are bigger than those on the sst dataset.
it indicates that the detection ontheformerdatasetrequiresagreatertimeconsumption than that on the latter.
the average length of samples inthe imdb dataset is ten times larger than that in the sstdataset.
the longer the samples the larger the detection space leading to a higher time cost in every iteration.
moreover wecomparetheperformanceofourapproachwith mt nlp based on cnn and lr models.
since the source code ofmt nlp is unavailable we directly use the performance resultsevaluated by the work of .
table presents the comparison results on imdb dataset with genderprotected attribute.
although expga shows smaller tsn than mt nlp its dss value is about fivetimessmallerthanmt nlp andthesurvalueisatleasttwice larger than mt nlp.
answering rq expga outperforms the mt nlp baseline withatleastfivetimestheefficiencyandtwice theeffectivenessinfindingdiscriminatorysamplesontext datasets.
.
fairness improvement through retraining to answer rq3 we employ a data augmented method used in aequitas and adf to investigate the model fairness improvement imposed by the discriminatory samples detected by expga.
following the data augmented method given a target model weexecuteexpgatogeneratediscriminatorysamplesfrom authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa ming fan and wenying wei et al.
table comparison results of detecting discriminatory samples between expga and mt nlp on imdb dataset.
dataset modelexpga mt nlp tsn dsn dss sur tsn dsn dss sur imdb gendercnn44 .
.
.
.
lr77 .
.
.
.
table comparison results of detecting discriminatory samples before and after retraining.
datasetsample addednormal sample testing accuracydiscriminatory sample testing percentagedss sur before after before after before after impr before after impr cencusgender .
.
.
.
.
.3x .
.
.
age .
.
.
.
.
.9x .
.
.
race .
.
.
.
.
.4x .
.
.
creditgender .
.
.
.
.
.0x .
.
.
age .
.
.
.
.
.4x .
.
.
bank age .
.
.
.
.
.5x .
.
.
imdbgender .
.
.
.
.
.
.8x .
.
.
country .
.
.
.
.
.8x .
.
.
sstgender .
.
.
.
.
.4x .
.
.
country .
.
.
.
.
.
120x .
.
.
adataset.aportionofthesediscriminatorysamplesareretained in the original dataset constructing a new dataset.
based on the updateddataset weretrainthemodelandre executeourexpga to test the model fairness.
to verify the fairness of retrained model we randomly select 000discriminatorysamplesastheground truth.sincerq2shows that the dsnon text datasets ismuch smaller than thatof tabular datasets weusehalfofthegenerateddiscriminatorysamplesfor retraining and the remaining for testing.
thedatasetcolumn and sample added column in table list the datasets with the protected attributes and the percentage of generateddiscriminatorysamplesthatremainedforconstructing new datasets in this experiment.
on tabular datasets original dataset discriminatorysamples arereservedforretraining.ontextdatasets thepercentagesarecalculatedbasedonthe size of generated discriminatory samples.
the columns in table show experiment results i.e.
the model accuracy and performance of expga before and after retrainingmodels.
normalsampletestingaccuracy indicatesthe model accuracy calculated against normal samples in the datasets anddiscriminatory sample testing percentage is the corresponding accuracy against the ground truth discriminatory samples.
dss and sur show the efficiency and effectiveness of expga.
impr columnsdenotethedss orsur improvementsof afterdss sur vs.beforedss sur .
these measurements show that themodelaccuraciesofdetectingnormalsamplesarenearly unchangedafterretrainingmodelsondatasetsaugmented with the discriminatory samples generated by expga.
this observationindicatesthat thedataaugmented methodhas littleinfluenceontheclassificationperformanceofnormal samples.
afterretraining onaverage morethan97 oftheoriginal discriminatorysamplesarenotbiasedtonewmodels indicating that model fairness has been enhanced significantly.
theimprofdsspresentsthatamuchlongertimeisrequired todetectadiscriminatorysamplebyexpgaforre trained models.
for example in sst dataset using countryas the protectedattribute aftersearchingaboutthreehours only discriminatory samples are detected.
the sur values drop byan average of up to .
in sstdataset indicatingthegreaterdifficultyforasuccessful generation of a discriminatory sample.
answering rq after retraining models via a data augmentedmethodusingdiscriminatorysamplesgenerated by our expga the model fairness can be considerably improved i.e.
morethan97 oftheoriginaldiscriminatory samples are not biased to new models.
furthermore the efficiencyandeffectivenessof generatingdiscriminatory samples on new models become much lower compared with the original models while keeping the testing accuracies on normal samples unaltered.
threats to validity .
threats to internal validity interpretable methods our expga relies on interpretable methods such as lime and shap to obtain explanation results for an initial seed selection.
in practice it is difficult for interpretable methods to guarantee perfect explanation results.consequently the probably missing or mismatching sensitive authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
explanation guided fairness testing through genetic algorithm icse may pittsburgh pa usa featuresmightaffectourseedsamplequality.tomitigatethisthreat our future work will explore more interpretable methods.
retraining to improve model fairness in rq3 we test the model fairness using a data augmented method by imposing discriminatorysamplesdetectedbyexpga.althoughthemodel fairness has been improved a lot we are not claiming that expga radically combs out the model fairness issue.
in the future a consideringofthefairnesscharacteristicduringdatapreparation andmodeltrainingshouldbeapromisingexplorationdirection .
.
threats to external validity limited experiment datasets we evaluate expga on five benchmark datasets that are used by baseline approaches.
the resultsdemonstratedthatexpgaoutperformsbaselinesondiverse benchmarks though we cannot conclude that expga would achieveagoodperformanceonotherdatasetswithotherprotected attributes.oncenewdatasetsareavailable wewillfurtherverify expga.construction of knowledge graph we construct a prior knowledge graph to identify the words related to protected attributes.
currently this graph only involves a limited number of protected attributes but sufficient for our experimental evaluation.
whenusingexpgatotestmodelfairnessregardingotherprotected attributes theknowledgegraphshouldbeextendedtoexpressmore domains.
related work fairnesstesting recently software fairness testing has received muchattention.galhotra etal.
pioneeredafairnesstestmethod namedthemis.themisdistinguishesthegroupdiscrimination andindividualdiscriminationforsoftwarefairnesstesting.however the strategy of randomly generating test cases in themis is inefficient.
following the definitions coined by themis udeshi etal.
proposedaequitas achievingabetterefficiencybya directionalsearchingscheme.thissearchschememakesitdifficultforaequitastocoversamplesindiversedistributions.agarwal et al.
proposedsgbasedonprogramsymbolicexecution.sguses limetogeneratelocalperturbationsamplestobuildadecisiontree and then analyzes each tree path to generate test inputs.
similar to sg our expga also leverages the interpretable method lime.
the difference is that expga uses explanation results to identify highqualityseedsamples.adf isadetectionmethodspecifically designed for dnn networks.
adf requires gradient informationfrom the model architecture thus restricting its practical usage.
chakraborty et.
al thought that data imbalance and improper data label are two main reasons for model biased thus they proposed fair smote to achieve group fairness.
fair smote first syntheticallygeneratesnewdatapointsforallthesubgroupsexcept the subgroup having the maximum number of data points to solve dataimbalanceintrainingdata.then itfindoutandremovebiasedsamplesintrainingdatatoeliminatebiasedlabels.itisworthnoting that our work focuses on individual fairness while fair smote focuses on group fairness.
therefore we do not compare our work with fair smote in this paper.insummary comparedwiththeseapproaches ourexpgaisa lightweightmodel agnosticindividualfairnesstestingmethod able to handle diverse and large scale scenarios.interpretable methods this work is also related to researches related to machine learning interpretable methods.
ribeiro et al.
proposed a fast and effective method lime that fits a local decision boundary with a simple linear regression model.
the weightofthelinearmodelrepresentstheimportanceoffeatures.
lundberg and lee proposed shap generates a set of shap values based on linear functions to represent the coefficients of eachfeature.unlikelime shapdefinesmultipleshapkernelsasaweightfunction.atthesametime inspiredbygametheory shap defines three properties to constrain the fitting process.
guidotti et al.
proposedarule basedblackboxmodelinterpretablemethod calledlore whichfirstusesgeneticalgorithmstogeneratetwo sample sets with completely different labels.
then it constructsa decision tree and extracts a set of rules from the tree as a localexplanation.
considering that a single linear function cannot fit thehighlynonlineardecisionboundarywell guo etal.
used multiple linear models to approximate the local decision boundary andchosetheonewiththehighestaccuracyasinterpretationresult.
conclusion thisworkproposesexpga anexplanation guidedmethodthrough thegaforsoftwarefairnesstesting.thenoveltyofexpgaisthe combinationofexplanationresultsandga thusdeterminingits highefficiencyandeffectivenessindiscriminatorysampledetection.
expga is also model agnostic and can handle black box models in diverse scenarios.
the evaluation experiments demonstrate that expgacandetectdiscriminatorysamplesmuchfasterwithahighersuccessratethanfourstate of the artmethods bothonthetextand tabular benchmarks.
augmented with the discriminatory samples generatedbyexpga thefairnessoftestedmodelshasasubstantial improvement through retraining.
acknowledgment this work was supported by national key r d program of china 2018yfb1004500 national natural science foundationof china u1766215 chinapostdoctoralsciencefoundation 2019tq0251 2020m673439 2020m683507 innovativeresearch group of the national natural science foundation of china ministryofeducationinnovationresearchteam irt 17r86 youth talent support plan of xi an association for science and technology .