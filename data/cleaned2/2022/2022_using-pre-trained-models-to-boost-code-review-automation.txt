using pre trained models to boost code review automation rosalia tufano seart software institute universit della svizzera italiana switzerlandsimone masiero seart software institute universit della svizzera italiana switzerlandantonio mastropaolo seart software institute universit della svizzera italiana switzerland luca pascarella seart software institute universit della svizzera italiana switzerlanddenys poshyvanyk semeru computer science department william and mary usagabriele bavota seart software institute universit della svizzera italiana switzerland abstract code review is a practice widely adopted in open source and industrialprojects.giventhenon negligiblecostofsuchaprocess researchers started investigating the possibility of automating specific code review tasks.
we recently proposed deep learning dl modelstargetingtheautomationoftwotasks thefirstmodeltakes as input a code submitted for review and implements in it changes likely to be recommended by a reviewer the second takes as input the submitted code and a reviewer comment posted in natural language and automatically implements the change required by thereviewer.
while the preliminary results we achieved are encour aging both models had been tested in rather simple code review scenarios substantially simplifying the targeted problem.
this was also due to the choices we made when designing both the tech nique and the experiments.
in this paper we build on top of that work by demonstrating that a pre trained text to text transfer transformer t5 model can outperform previous dl models for automatingcodereviewtasks.also weconductedourexperiments on a larger and more realistic and challenging dataset of code review activities.
ccs concepts softwareanditsengineering softwaremaintenancetools .
keywords code review empirical study machine learning on code introduction the benefits of code reviews have been widely recognized with severalstudiesprovidingevidenceofthehigherqualityofreviewed code .also codereviewshelpinpreventingbugsand foster knowledge transfer among developers .
however studiesoncodereviewsalsohighlightedanadditionalcostthatsuch aprocessentails empiricalevidencesuggeststhatlargesoftware projects can undergo hundreds of code reviews per month.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa association for computing machinery.
acm isbn ... .
500reviewspermonthin linux andindustrial e.g.
3kreviewspermonthinmicrosoft bing projects.asaresult developerscanspendmanyhours per week reviewing code .
given the non negligible cost of code review we recently proposed the automation of specific code reviewtasks thegoalisnottoreplacedevelopers buttohelpthem savetimeintwoscenarios.thefirstisthatofacontributor i.e.
the developersubmittingthecodeforreview whowantstoreceivea rapid feedback about the code they wrote before submitting itfor review.
the feedback is provided by a deep learning dl modeltrainedtotakeasinputthecodetosubmitforreview csand provide as output a revised version of cs i.e.
cr implementing code changes that are likely to be recommended by a reviewer.
the second scenario concerns the reviewer s involved in the process a dl model is trained to take as input i the code cs submittedforreview and ii acomment rnlwrittenbythereviewer in natural language to request a specific change on cs.
the output ofthemodelisarevisedversionof cs i.e.
cr implementingthe changesrecommendedin rnl.theideahereisthatthereviewercan use themodel to provide thecontributor with a concreteexample of the code changes that they would like to see implemented.
inourpreviouswork wetrainedandexperimentedwiththe dl models on a dataset composed of 17k triplets angbracketleftcs rnl cr angbracketright extractedfromcodereviewsperformedingithub andgerrit .
in particular the model recommending code changes to the contributor is an encoder decoder model with one encoder taking csas input and one decoder generating cr.
our evaluation shows thatthismodelcanrecommendachangeasareviewerwould single prediction to of the cases different predictions .
the model employed in the second scenario i.e.
the automated implementation of a comment recommended by the reviewer has instead two encoders taking as input csandrnl respectively and onedecodergenerating cr.thismodelcansuccessfullyimplement a change recommended by a reviewer in single prediction to different predictions of the cases.
despite the encouraging preliminary results our approach aswellastheconductedempiricalstudysuffersofseverallimitations we try to overcome in this paper.
first in we adopted code abstraction to reduce the vocabulary size and simplify the learningofthedlmodel themodeldidnotworkontherawcode butonanabstractedversionofitinwhich forexample variable identifiers were replaced with a special var idtoken where idis a progressive number e.g.
the second variable is represented by var 2 .the possibilityto goback toraw codewas guaranteedby a map linking abstracted to raw tokens in cs e.g.
var 1 i .
ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa rosalia tufano simone masiero antonio mastropaolo luca pascarella denys poshyvanyk and gabriele bavota whilesuchaproceduresimplifiesthelearningofthemodel it posesastronglimitationonthevarietyofcodereviewtasksthat can be supported by such a model.
indeed the abstraction process forces to exclude from the dataset of triplets angbracketleftcs rnl cr angbracketrightall those in whichcrintroduces identifiers or literals that were not present incs.
this is necessary because the abstraction map is built on cs and if a new variable var 2is introduced in crduring the review process suchavariablecannotbemappedbacktorawsourcecode making such an approach unusable in practice.
this means that the triplets angbracketleftcs rnl cr angbracketrightonwhich weevaluated ourapproach wererelativelysimplechangesimplementedduringcodereview not requiring the introduction of new identifiers or literals.
second tosimplifythelearning weonlyconsideredtriplets angbracketleftcs rnl cr angbracketrightin which both the code submitted for review cs and the revised code cr had no more than tokens .
again this reduced the complexity of the tackled problem.
basically the two above choices resulted in training and experimentingtheproposedmodelsonquitesimplecodereviewinstances only representative of a minority of the code transformations actually implemented during code reviews.
in this paper we build on top of our previous work e x perimentingwithdlmodelsforcodereviewautomationinmore realisticandchallengingscenarios.westartbytrainingtherecently proposed text to text transfer transformer t5 model o n adatasetsimilartotheoneusedin .however weadoptatokenizer i.e.
sentencepiece thatallowsustoworkwithraw source code without the need for code abstraction.
also we increase the maximum length of the considered code componentsfrom abstracted tokens to sentencepiece tokens i.e.
abstracted tokens .theabsenceofanabstractionmechanism and the increased upper bound for input output length allowed us to build a substantially larger dataset as compared to the one used in 168k instances vs.17k and more importantly to featureinsuchadatasetawidervarietyofcodetransformationsimplemented in the code review process including quite challenging instances such as those requiring the introduction of new identifiersandliterals accountingfor63 ofthenewdatasetwebuilt .
also we experimented with the automation of a third task related to the code review process given the code submitted for review cs generating a natural language comment rnlrequesting to the contributor code changes as a reviewer would do i.e.
simulating a reviewer commenting on the submitted code .
we also compare the t5 model with the encoder decoder model presentedinourpreviousworkontheoriginaldatasetusedin .
ourresultsshowthesuperiorperformanceoft5 whichrepresents a significant step forward in automating code review tasks.
to summarize the contributions of this work are i a novel approach for code review automation overcoming several limitations of the state of the art technique ii a comprehensive empirical evaluation of such an approach including a comparison with our previous technique iii the automation of a third task given the code submitted forreview automaticallygeneratingnaturallanguagecomments requesting changes as reviewers would do iv acodereviewdatasettotrainandtestdlmodelsinmore realistic scenarios as compared to the one used in v a comprehensive replication package .
t5 to automate code review wedescribethedlmodelweadopt theconstructionprocessofthe datasets needed for its training and the procedure used for hyperparameter search model training and generation of predictions.
.
text to text transfer transformer t5 the text to text transfer transformer or simply t5 is not merely a model.
raffel et al.
compare pre training objectives architectures unlabeleddatasets transferapproaches andotherfactorson dozens of language understanding tasks .
the result ofthis exploration isthe best combinationof architectures and training techniques namely t5.
t5 is based on the transformer architecture.theproposedimplementationdiffersonlyinsomedetails regardingthenormalizationlayerandthe embedding scheme from its original form.
raffel et al.proposed severalversionsoft5 differingfromeachotherintheirsize e.g.
number of layers and as a consequence training complexity.
in this work we adopt the smallversion of t5 consisting of headed attention 6layersinboththeencoderandthedecoder eachhaving a dimensionality of and the output dimensionality of 60m parameters .
themodelissubjectedtoafirsttraining pre training whose purpose is to provide it with a general knowledge useful to solve a setofrelatedtasks.suppose forexample thatwewanttotraina modelableto i translateenglishtogerman and ii summarize english text.
instead of starting by training the model for thesetwo tasks t5 can be pre trained in an unsupervised manner by usingthe denoisingobjective ormaskedlanguagemodeling the model is fed with sentences having of their tokens e.g.
words inenglishsentencesorcodetokensinjavastatements randomly masked and it is asked to predict them.
by learning how to predict themaskedtokens themodelcanacquiregeneralknowledgeabout the language of interest.
in our example we could pre train the model on english and german sentences.
once pre trained t5 is fine tuned on the downstream tasks in a supervisedfashion.eachtaskisformulatedina text to text format i.e.
boththeinputandtheoutputofthemodelarerepresented astext .forexample forthetranslationtaskadatasetcomposed ofpairsofenglishandgermansentencesallowstofine tunethe model.similarly thesummarizationtaskrequirestheinputenglishtext and a corresponding summary.
in the next sections we explain how we pre train and fine tune t5 to support code review tasks.
.
training data wedescribetheprocessusedtobuildthedatasetsneededforthe pre training section2.
.
andfine tuning section2.
.
oft5.partofthefine tuningdatasethasbeenusedforhyperparametersearch section .
and for testing the performance of t5 section .
.
.
pre training dataset.
giventhegoalofthepre trainingphase i.e.
providing the model with general knowledge about the languages of the downstream tasks we built a dataset allowing to train t5 on java and technical english.
indeed besides source code technical english is instrumental in a code review process in which reviewers post natural language comments about code.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
using pre trained models to boost code review automation icse may pittsburgh pa usa westartfromtwodatasetsfeaturinginstancesincludingboth sourcecodeandtechnicalenglish theofficialstackoverflowdump sod andcodesearchnet csn .stackoverflowisaq a websiteforprogrammers.thedatadumpweusedcollectsallthe questionsandrelativeanswersbetween2006and2020foratotal of roughly 51m posts where a post is a single question or answer .
a post includes english text as per the so guidelines and or code snippets.postsareusuallyaccompaniedbytagscharacterizingtheir topic e.g.
java android andcanberatedwith up down votes and forwhatconcernstheanswers theycanbemarkedasthe accepted answer from the question s author.
we extracted from the sod all the answers i having a java tag ii containingatleastone pre code htmltagtoensure the presence of at least one code snippet in the answer and iii having at least up votes and or being the accepted answer.
thesefilters are justified by the goal of our pre training.
indeed we want the model to acquire knowledge about technical english and java focusing on answers containing at least one code snippet increases the chances that their natural language text refers to an imple mentation task similarly to what happens in code review.
also the up votes accepted answer filter aims at discarding low quality instances containing for example wrong code solutions.
this is alsothereasonwhywefocusedonhigh qualityanswerslikelyto contain working solutions rather than on questions that even if up voted e.g.
because they are relevant for many users may contain wrong implementations.
from this step we obtained candidate instances from the sod.
oneachselectedanswer a weperformedthefollowingcleaning steps we remove emojis non latin characters control characters trailingspacesandmultiplewhitespaces.somespecialsymbolsarereplacedusinglatincharactershavingthesamemeaning e.g.
is replaced with .
moreover we replace any embedded link with a special tag link i withibeing an integer ranging from to n wherenisthenumberoflinksin a.finally weremovedall theinstanceshavinglessthantentokensormorethan512 .
this left us with valid instances.
thecsn javadatasetfeatures1.5muniquejavamethods someofwhichcontainingtheirjavadoc.wefilteredoutallthosein whichajavadocwasnotavailableoritdidnotcontainanyletter removing of them.
unlike the sod csn can contain instances in which the textual part i.e.
the method comment is notinenglish.topartiallyaddressthisissue weexcludepairsin which no latin characters were found.
while this does not exclude all non english comments at least identifies and removes those writteninspecificlanguages e.g.
russian chinese .we decided to accept some level of noise in the pre training dataset e.g.
comments written in french since i given the size of this dataset thislittleamountofnoiseshouldnotsubstantiallyaffect the model s performance and ii the pre training dataset is not usedastestsettoassesstheperformanceoftheapproach.aswe willexplainlater amorefine grainedcleaninghasbeenperformed for the fine tuning dataset that instead is used for performance evaluation.
on the remaining instances we performed the same cleaning steps described for the sod e.g.
remove emojis .
finally from each pair we obtain a single string concatenating the javadoc comment and the code retaining the ones having more than ten and less than tokens instances left .by putting together the instances collected from the sod and csnweobtainedthepre trainingdatasetconsistingof1 instances.
to perform the pre training we randomly mask in each instance of its tokens.
the masked tokens are replaced withsentinel tokens extra id i whereiis an increasing number rangingfrom0upto n wherenisthenumberoftokensmasked inagiveninstance.ifseveralcontiguoustokensaremaskedthey arereplacedbyasinglesentineltoken.these maskedinstances representtheinputofthemodelduringthepre training.thetarget i.e.
thestringthemodelisexpectedtogenerate isbuiltconcatenatingthesentineltokensandthetoken s theyaremasking.an extra sentinel token is added to indicate the end of the string.
our pre training dataset is publicly available .
.
.
fine tuning datasets.
to create the fine tuning dataset we mined java open source projects from github using the web applicationbydabic etal.
.usingthequeryinginterface we selected all java projects having at least pull requests prs ten contributors ten stars and not being forks.
the filters aim at i ensuring that enough code review material is contained in the projects i.e.
atleast50prs ii discardingpersonal toyprojects atleasttencontributorsandstars and iii reducingthechanceof mining duplicated code.
this resulted in a list of projects.
we alsominedthesixgerrit installationsusedin containing code review data about projects.
fromboththegithubandthegerritdatasetsweextracttriplets ms cnl mr wheremsis a method submitted for the review cnlisasinglereviewer scommentsuggestingcodechangesfor ms andmristherevisedversionof msimplementingthereviewer s recommendation cnl.
note that i we only looked for prs that are accepted at the end of the code review since we want to learn howtorecommendchangesthat attheend canleadtocodeconsidered good from a reviewer s perspective and ii a single pr in github and gerrit can result in several triplets for our dataset.
indeed we minethedifferentreviewroundsineachpr.forexample amethod mscan be submitted for review receiving a comment cnlasking for changes first round .
the revised version of msaddressing cnl isthenresubmitted mr resultinginthesecondreviewround possibly leading to additional comments and revisions of the method .
we stop when the code is formally accepted.
overall we mined valid triplets from github and gerrit usingthepipelinefrom thatwesummarizeinthefollowing see for additional details .
wetarget triplets in which a comment cnlhasbeenpostedbya reviewer onamethod ms.wecanidentify thesecasessincebothgithubandgerrit i provideinformation aboutthedeveloperssubmittingthecodeandpostingcomments in the review process and ii allow to retrieve the specific codeline s cnlrefersto i.e.
thecodein msthathasbeenhighlighted by the reviewer when posting the comment .
we exclude all the comments posted by the authors of the code e.g.
to reply to reviewers since they do not represent a review of thecode.thus thetripletsinourdataset have cnlbeingasingle commentpostedbyareviewer.also weexclude cnllinkedtoinline comments rather than code lines in ms since we target the fixing ofcode relatedissues.toconsideratripletasvalid cnlmustbethe onlycommentpostedbyarevieweron msinthatspecificreview round.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa rosalia tufano simone masiero antonio mastropaolo luca pascarella denys poshyvanyk and gabriele bavota in this way we can be confident that the revised version submitted later on by the author mr actually aimed at implementing cnl.
also mrmust differ from ms i.e.
a change must have been implemented in the code to address cnl .
from the technical point of view the parsing of the methods from the patches submitted for review has been done using the lizard library .
note that the removaloftripletsinwhich cnlincludemorethanonecomment has been done later in the processing pipeline we will get back to this point .
indeed before we had to clean comments possibly just representing noise.
as done for thepre training dataset we performed some cleaningsteps.wereplacedanylinkwiththenumberedtoken link i withibeinganintegerrangingfrom0to n wherenisthetotal numberof linksin cnl msandmr.if thesame linkappears indifferent parts e.g.
incnlandmr it is replaced with the same token.
we also removed any emoji and non ascii characters from the comments extraspacesandcontrolcharactersfromboththecomments andthemethods andinlinecommentsfromthemethods weare not interested in addressing issues related to internal comments .
afterthecleaningprocessweobtainedsometripletsinwhich cnlbecame an empty string or where msandmrbecame equal e.g.
they only differed for some spaces before the cleaning .
we removed these instances as well as those having cnl ms ormrlongerthan512tokens .weconsideredthesumof cnlandmsin terms of length because for one of the tasks i.e.
the automated implementation of a comment posted by a reviewer they will be concatenated to form the input for the model.
then weremovedfromourtripletsnon relevantcomments i.e.
commentsnotrecommendingcodechangesuggestions e.g.
looksgoodtome .in wemanuallycraftedasetofnatural languagepatternstospotnon relevantcomments e.g.
single word commentscontainingwordssuchas thanks nice etc.
.wehave extendedthissetsincewenoticedthatinourricherdatasetseveral non relevant comments were left by these patterns.
such analysis hasbeendonebyoneoftheauthorsbymanuallyinspectingallthe tripletshaving cnlconsistingoflessthansixwords.theupdated heuristics are available in our replication package .
wealsoexcludedtripletsincludingnon english cnlcomments through a pipeline composed by three language detector tools.
a preliminary classification has been performed using the pythonlibrarieslangdetect andpycld3 .ifbothofthesetools classify the comment as non english we relied on the google languagedetectionapiforafinaldecision.suchaprocesswasneeded since we noticed that the google api was the most accurate in detecting the language especially when the comments also featured codeconstructsinthem.inthisscenario thepythonlibrariesoften generated false negatives i.e.
classifying an english sentence as non english .
however we had a limited number of requests available for the google api.
thus we performed a pre filtering using the python libraries and when they both reported the comment as being not in english we double checked using the google api.
after this cleaning process we excluded all triplets featuring morethanonecommentin cnl .finally weremovedallthe duplicatesfromthefine tuningdataset .tobeconservative we identify as duplicates two triplets having the same ms thus even triplets having the same msbut different cnl mrhave been removed .the resulting dataset features triplets that have been used to build the three fine tuning datasets needed for the threetasks we aim at automating.
in the first task code to code the model takes as input mswith the goal of automatically generating its revised version mr implementing code changes that may be required in the code review process.
thus the fine tuning datasetis represented by pairs m s mr. inthesecondtask code comment to code themodeltakesas inputboth msandacomment cnlpostedbythereviewerandtargets thegenerationof mr therevisedversionof msimplementingthe code changes recommended in cnl.
themscode contains two special tags start end marking theportionofthecode cnlrefersto.thefine tuningdatasetofthis second task is represented by pairs ms cnl mr. finally inthethirdtask code to comment themodeltakesas inputmsand aims at generating a natural language comment cnl suggestingcode changesasa reviewerwould do.thefine tuning dataset is represented by pairs ms cnl.
table pre training and fine tuning datasets instances dataset train evaluation test pre training stack overflow codesearchnet fine tuning all three fine tuning datasets have been split into training evaluation and test.
table summarizes the number of instancesinthedatasets thepre trainingisonlyusedfortraining whilethefine tuningdatasetsareexploitedalsoforthehyperparametertuning evaluation andforassessingtheperformanceof the model test .
in table we only report information for a single fine tuning dataset rather than for the three previously described since all three fine tuning datasets contain the same number of instances.
indeed they are all derived from the same set of triplets.
.
training and hyperparameter search raffelet al.
showed the major role pre training plays on the performance of t5 models.
the importance of pre training has alsobeenconfirmed forothertransformer basedmodels inthe contextofcode relatedtaskssuchastestcasegeneration .to further study this aspect we decided to experiment with both a pre trained and a non pre trained model both of which have been subject to a hyperparameter tuning process.
since we adopted the smallversion of t5 presented by raffel et al.
we did not experiment with variations related to its architecture e.g.
changing the number of layers or the number ofhiddenunits .though asalsodonebymastropaolo etal.
we experimented with different learning rate configurations i costant learning rate c lr in which the learning rate value is fixedduringthetraining ii inversesquarerootlearningrate isrlr inwhichthe learningratevaluedecaysastheinverse square root of the training step iii slanted triangular learning rate stlr in which first the learning rate linearly increases and then it linearlydecaysreturningtothestartingvalue iv polynomialdecay learning rate pd lr in which the learning rate polynomially decays to a fixed value in a given number of steps.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
using pre trained models to boost code review automation icse may pittsburgh pa usa thehyperparametertuninghasbeendoneforthefine tuning phase only.
indeed even though we just focus on one hyperparameter such a process still remains quite expensive requiring thetrainingofeightdifferentt5models i.e.
pre trainedandnon pre trained each with four different learning rates .
for pre training we use the same configuration proposed by raffelet al.in .
wepre trainied the modelon thepre training dataset for 200k steps epochs .
starting from the pretrained model we fine tuned for 75k steps four different models each using one of the experimented learning rates.
since the goal ofthis procedure is to find thebest learning rate forthethreecodereviewtasks wefine tunedeachofthesemodels using a mixture of the three tasks a single model is trained to supportallthreetasksusingtheunionoftheirtrainingsets.this isoneofthecharacteristicsoft5 thepossibilitytotrainasingle model for multiple tasks.
the same approach has been used forthe non pre trained model in this case four t5 models one per learning rate have been directly fine tuned.
we assessed the performance of the eight models on the evaluation set of each task in terms of perfect predictions namely casesinwhichthegeneratedoutputwasidenticaltothetarget expected string.
table reports the achieved results.
as it can be seen no learning rate achieves the best results in all the tasks.
nevertheless st lr shows better overall performance and for this reason is the one we adopt in our experiments.
table hyperparameter tuning results task learining rate strategy c lr isr lr st lr pd lr pre trained code to code .
.
.
.
code comment to code .
.
.
.
code to comment .
.
.
.
non pre trained code to code .
.
.
.
code comment to code .
.
.
.
code to comment .
.
.
.
given the best configuration for both the pre trained and the non pre trained models we fine tuned them for a maximum of 300k steps using an early stop strategy.
this means that we saved a checkpointofthemodelevery10kstepscomputingitsperformance in terms of perfect predictions on the evaluation set and stopped the training if the performance of the model did not increase for three consecutive checkpoints to avoid overfitting .
.
generating predictions oncethemodelsaretrained theycanbeusedtogeneratepredictions.
as done in previous work we adopt a beam search strategy togeneratemultiplepredictionsgivenasingleinput.forexample in the case of the code to code task for a single msmethod provided as input multiple mrcandidates can be generated.
when we ask the model to generate kpredictions it generates the kmost probablesequencesoftokensgiventheinputsequence kisknown as thebeam size and we experiment with k .
foreachpredictiongeneratedbyt5 wealsoexploitedits score function to assess the model s confidence on the provided input.thevaluereturnedbythisfunctionrangesfromminusinfinity to and it is the log likelihood ln of the prediction.
thus if it is it means that the likelihood of the prediction is i.e.
the maximum confidence since ln while when it goes towards minus infinity the confidence tends to be .
in our empirical study section we assess the reliability of the confidence level as a proxy for the quality of the predictions.
study design thegoalof our evaluation is to empirically assess the performance of the t5 model in code review automation tasks.
the contextconsistsof i thedatasetswepresentedinsection2 and ii thedataset fromourpreviouswork .fromnowonwerefertoourpreviouslypresentedapproachasthe baseline.thestudyaimsattackling five research questions rqs .
rq1 towhatextentist5abletoautomaticallyrecommend code changesto developers asreviewers would do?
weprovide asinputtot5ajavamethod mssubmittedforreviewandassess theextenttowhichthemodelisabletoprovideasoutputarevised versionof ms mr implementingcodechangesthatwillbelikely requested during the code review process.
the idea here is that such a model could be used beforethe code is submitted for review as an automated check for the contributor.
rq2 towhatextentist5abletoautomaticallyimplement code changes recommendedby reviewers?
given a java method submittedforreview ms andanaturallanguagecomment cnl in whicharevieweraskstoimplementspecificcodechangesin ms weassesstheabilityoft5toautomaticallyrevise mstoaddress cnl thus obtaining a revised method mr .
thethirdrqfocusesonthenovelcodereview relatedtaskwe introduce in this paper rq3 towhatextentist5abletoautomaticallyrecommend changesinnaturallanguageasreviewerswoulddo?
inthisrq t5 is provided as input with a java method submitted for review ms and it is required to generate a natural language comment cnl requesting code changes as reviewers would do.
forrq1 rq3 weexperimentwithdifferentvariantsofthet5 model.
in particular we assess the quality of t5 predictions forall three tasks when i the model is pre trained or not and ii the predictions have different confidence levels.
thanks to these analyses we can answer our fourth rq rq4 what is the role played by the model pre training on the performance of t5?
how does the confidence of the pre dictions affects their quality?
as explained in section .
we performanablationstudyinwhicht5isfine tunedwithoutany pre training i.e.
by starting from random weights in the neural network .
thisallows to assessthe contribution ofthe pre training tothe performanceof themodel.
asfor theconfidenceof thepredictions weassesswhetheritcanbeusedasareliableproxyforthequalityofthepredictions i.e.
thehighertheconfidence thehigher the likelihood the prediction is correct .
if this is the case sucha finding would have implications for the usage of the t5 modelin practice a developer using the model could decide to receiverecommendations having confidence higher than t reducing the chances of receiving meaningless predictions.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa rosalia tufano simone masiero antonio mastropaolo luca pascarella denys poshyvanyk and gabriele bavota finally thelastrqcomparestheperformanceofthet5model with that of the approach we presented in rq5 what is the performance of t5 as compared to the state of the arttechnique?
weusetheimplementationanddatasets from our previous work to compare the performance of the t5 model with the baseline .
.
data collection and analysis toanswerthefirstfourresearchquestions weexperimentwiththe best configuration of both the pre trained and non pre trained t5 model on the test set of the fine tuning dataset reported in table .
rememberthatforeachofthethreetaskswesupport i.e.
the onesthatmaptorq rq2 andrq the16 779testsetinstances arethesametriplets ms cnl mr .theonlydifferenceisthat in rq1the model has been trained and is tested to take as input ms and produce mr i nr q2it takes as input msandcnland produces mr i nr q3it takes as input msand produces cnl.
by running the models on the test sets we report for each of the three tasks the percentage of perfect predictions namely the cases in which the output of the model is the expected one.
for example in the case of rq this means that the model was able givenmsasinput togenerateacomment cnlidenticaltotheone manually written by the reviewer who inspected ms. besides computing theperfect predictions in rq i.e.
the task in whichthe model isrequired to generatenatural language text wealsocomputethebleu bilingualevaluationunderstudy score of the predictions .
bleu assesses the quality of the automatically generated text.
the bleu score ranges between and with indicating in our case that the natural language comment generatedbythemodelisidenticaltotheonemanuallywrittenbythe reviewer.
we use the bleu variant that computes the overlap in terms of grams between the generated and the reference text.
in rq1and rq2 i.e.
in the tasks in which the model is required to generate code we adopt instead the codebleu a recently proposed similarity metric inspired by the bleu score but tailored to assess the quality of automatically generated code.
differently from bleu codebleu computes not only an ngrambasedsimilarity butitalsoconsidershowsimilartheabstract syntax tree and the data flow of the generated and the reference codeare.ren etal.
whoproposedthecodebleu showedthat theirmetricbettercorrelateswithdevelopers perceptionofcode similarity as compared to the bleu metric.
concerningrq wecomparetheresults i.e.
perfectpredictions bleu codebleu achieved by the t5 model with and withoutpre training.
we also statistically compare the two models i.e.
with withoutpre training usingthemcnemar stest andodds ratios ors ontheperfectpredictionstheycangenerate.asforthe confidenceofthepredictions wetakethebestperformingmodel i.e.
the one with pre training and split its predictions into ten bucketsbasedontheirconfidence cgoingfrom0.0to1.0atstepsof .
i.e.
thefirstintervalincludesallpredictionshavingaconfidence cwith c .
the last interval has .
c .
then we report for each interval the percentage of perfect predictions.
finally inrq wecomparet5withthebaseline onthetwo tasks automated in our previous work i.e.
the ones related to our rq1and rq .asmetricsforthecomparisons weusedthepercentageofperfect predictions and the codebleu of the predictions.
we compared the two techniques in several scenarios.
first we used the dataset from featuring triplets ms cnl mr .
by performing somechecksonthisdataset wenoticedthatafewinstances had comments cnl not written in english or containing invalid unicode characters that did not allow our tokenizer to work.
thus we excluded those instances from the training and the test sets shared by the authors.
the training set has then been used to i train the baseline and ii fine tune the t5 model without any pre training.inthisway wecancomparetheperformanceofthe two modelson the testset when trainedon exactly the samedata.
important to notice is that the baseline has been trained and testedonabstractedcode asdonein whilet5workeddirectlywith the raw source code.
ontopofthis wealsoreporttheperformanceofthepre trained t5modelwhenrunonthetestsetfrom .thispre trainedmodel has been fine tuned using the training dataset in .
clearly this analysisfavorst5sinceithasbeentrainedonmoredata i.e.
the pre training dataset .
however it pro vides additional hints into theroleplayedbythepre trainingandontheeffectivenessofthe t5 model in general.
besides reporting descriptive statistics we statisticallycomparethetwomodelsusingthemcnemar stest andoddsratios ors ontheperfectpredictionstheycangenerate.
sincemultiplecomparisonsareinvolved e.g.
comparingthepretrainedandthenonpre trainedmodeltothebaseline weadjust thep values using the holm s correction .
results discussion we start by answering rq1 rq3 section .
presenting the performance of t5 in the three tasks we aim at automating.
then we discuss the impact on the performance of the pre training and the reliabilityoftheconfidencelevelasaproxyforthequalityofthe predictions section .
.
finally we compare t5 with the baseline section .
.
.
rq rq3 performance of t5 fig.1reportstwographsforeachtask.thelinechartontopshows the percentage of perfect predictions y axis achieved by t5 for different beam sizes x axis the continuous line represents the pre trainedversionofthemodel whilethedashedlinethenonpretrainedone.theboxplotsatthebottomreportthecodebleufor thetwocode generationtasks i.e.
code to code andcode commentto code and the bleu score for the code to comment task in which text is generated.
lighter blue represents the pre trained model.
we start by commenting on the perfect predictions line charts .
at a first sight the performance of the model might seem quite low.forexample inthecaseof code to code atk i.e.
asingle prediction is proposed by t5 both the pre trained and the nonpre trained models achieve of perfect predictions and instances correctly predicted with and without pre training respectively .however sucharesultshouldbeconsideredinthe contextofwhatwasreportedbythestate of the arttechnique that onamuchsimplertestdataset achievedforthesametaskand same beam size .
of perfect predictions.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
using pre trained models to boost code review automation icse may pittsburgh pa usa beam size beam size beam sizecode bleu code bleu bleuperfect predictions perfect predictions perfect predictions code to code code comment to code code to comment t5 pre trained t5 non pre trained figure results t5 dataset large similar observations can be made for the code comment to code task where at k t5 can generate .
instances and .
perfect predictions when pre trained and not respectively.
for this task in our previous work we achieved onasimplerdataset12.
perfectpredictions.wedirectlycompare the two approaches in rq .
interestingly increasing the beam size from to does only result in marginal imp rovements for all tasks.
the largest improvement is obtained for the code comment to code where we move from .
k to .
k of perfect predictions for the pre trained model.
given the goal of our approach we believe that the most relevant performance are those achieved at k .
indeed providingseveralrecommendationstoinspecttoadeveloper might be counterproductive especially considering that the recommendations are entire methods in the case of the two code generation tasks.
movingtothe code to comment task t5strugglesinformulating natural language comments identical to the ones written by reviewers.thepre trainedmodel at k generates356correct comments .
against the .
of the non pre trained model.
these numbers only slightly increase at k with a maximum of .
perfect predictions achieved with pre training.
thetoppartoffig.2showstwoexamplesofperfectpredictions generated by the model for each task.
a dashed line separates the two examples within each task.
for the code to code task the first code in each example represents the input of the model while the seconditsoutput.wehighlightedinboldthepartsofcodechanged by the model and replaced irrelevant parts of the methods with to save space.
in the first code to code example t5 removes an unneeded instanceof check since filesystemdataset is a subclass of dataset.
instead the second example simplifies the checkingfortheexistenceofacluster providingameaningfulerror message.
this second case cannot be supported by the baseline since it requires the introduction of new code tokens that were not present in the input code.
remember that these being perfectpredictions theimplementedchangesareidenticaltothose performed by developers during code review.
forthecode comment to code task theinputprovidedbythe modelincludesthecommentwrittenbythereviewerandrequiringaspecificchangetothepartofcodehighlightedinorange.in thefirstexample thereviewersuggeststouseaspecificobjectto performthe nullcheckandt5correctlyimplementsthechange.thesecondoneisinterestingbecause despitethereviewerhighlighting return null astherelevantcodefortheircomment else isredundant themodelcorrectly understands thattheactionto take is the removal of the unneeded elsestatement.
finally for the code to comment task we report the code providedasinputtothemodel firstline withthecommentitgenerated asoutput secondline .inthefirstexample t5suggests asdone by therealreviewer toadd a nullcheck alsoshowing the code needed for its implementation.
this code is not just a template but it is suitable for the provided input code it refers to the supplier object .
in the second example t5 suggests to rename an identifier providing valid recommendations for the renaming.
lookingatthebottomoffig.
theresultsintermsofcodebleu show a median higher than .
for all beam sizes and for both code generation tasks.
however while we report these values for completenessandforbeingconsistentwithwhatdoneinsimilar works theysaylittleaboutthequalityofthepredictions andtheyaremostlyuseful forfutureworkthatwantstocompare withourapproach completedistributionsareavailableinourreplicationpackage .indeed itisdifficulttoproperlyinterpretthese values for two reasons.
first there is no accepted threshold above whichgoodperformancecanbeclaimed.second asalsodonein previousworksproposingmodelstakingasinputacodesnippet and providing as output the same code revised in some way e.g.
withafixedbug withasinglestatementadded orwith review related changes implemented we computed the codebleubetweenthepredictedandthetargetcode twomethodsin ourcase .however theinputprovidedtothemodelisalreadyquite similar to the target output which means that a model taking as input a method andnot implementing any change onit is likely to obtain high values of codebleu.
for this reason we mostly focus our discussion on perfectpredictions.
concerning the bleu score achievedinthe code to comment task themedianrangesaround .
see fig.
.
such a result is expected given the low percentage of perfect predictions achieved for this task.
goingbacktotheperfectpredictions theresultsreportedinthe linechartsinfig.1representalowerboundfortheperformance of our approach.
indeed we consider a prediction as perfect only ifitisidenticaltothereferenceone.forexample inthecaseofthe code to comment task the natural language comment generated by t5 is classified as correct only if it is equal to the reference one including punctuation.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa rosalia tufano simone masiero antonio mastropaolo luca pascarella denys poshyvanyk and gabriele bavota perfect predictions code to code public configbuilder readfrom view ?
view if view instanceof dataset view instanceof filesystemdataset filesystemdataset dataset filesystemdataset view public configbuilder readfrom view ?
view if view instanceof filesystemdataset filesystemdataset dataset filesystemdataset view public response getcustomizedstateaggregationconfig pathparam clusterid string clusterid helixzkclient zkclient gethelixzkclient if !zkutil.isclustersetup clusterid zkclient return notfound public response getcustomizedstateaggregationconfig pathparam clusterid string clusterid if !doesclusterexist clusterid return notfound string.format cluster s does not exist clusterid code comment to code private string getbillingfrequencydescription award award if objectutils.isnull award objectutils.isnull award.getbillingfrequency i suggest objectutils check for nulls private string getbillingfrequencydescription award award if award null award.getbillingfrequency null public t extends iremoteconnection.service t getservice if return return null public t extends iremoteconnection.service t getservice if return else return null else is redundant code to comment static e t validation e t valid supplier ?
extends t supplier return new valid supplier.get please add a check objects.requirenonnull supplier supplier is null public list getexecutebefore rules ann this.getclass .getannotation rules.
class if ann !
null rename ann to rules rulesannotation or something more descriptive.
alternative and valid predictions code comment to code public userdto adduser userdto userresource return userdto.createinstancewithprivatedata user inline this variable public userdto adduser userdto userresource userdto saveduser userdto.createinstancewithprivatedata user return saveduser code to comment extract the building of the responsemessage to it s own variable in eclipse select the text right click refactor extract local variable select code shift alt l .
this will make the code a bit more readable especially when you ll be passing in other things besides the responsemessage.
please make this one a variable as well public void handlesetdevicelifecyclestatusbychannelresponse responsemessage.newresponsemessagebuilder .
figure examples of perfect and alternative predictions however it is possible that a natural language comment generated by t5 is different but semantically equivalent to the one written by the developer e.g.
variable vshould be private vs changevvisibilitytoprivate .similarobservationsholdforthe two code generation tasks e.g.
a reviewer s comment could be addressed in different but semantically equivalent ways .
tohaveanideaonthenumberofvaluablepredictionspresent among those classified as wrong i.e.
the non perfect predictions threeauthorsmanuallyanalyzedasampleof100 wrong predictions for each task in total .
the analysis was done intwo meetings in which each instance was discussed by all threeauthors.
the goal was to classify each instance into one of three categories i semanticallyequivalent i.e.
thegeneratedcode commentisdifferentbutsemanticallyequivalenttothereference one ii alternative solution i.e.
the generated code comment is not semantically equivalent but valuable or iii wrong i.e.
the generatedcode commentisnotmeaningfulfortheprovidedinput .
since we also computed the confidence for each of the predictionsgenerated by t5 rather than randomly selecting the instances to inspect we decided to target for each task the top wrong predictions generated by the model in terms of confidence.
indeed thosecases areparticularlyinteresting sincetheyrepresentwrong predictions for which however the model is quite confident.table3 manualanalysisof100 wrong predictionspertask task semantically equivalent alternative solution wrong code to code code comment to code code to comment table3showstheresultsofourmanualanalysis.forthe codeto codewe observed that in most cases the model actually generateswrongpredictionsthatarenotinlinewiththechanges implementedbythedeveloper.therearefewexceptionstothese cases mostly related to small changes in which the model madea decision different from that one of the developer but still valid e.g.
extractingastringintoavariableandusingadifferentname for the extracted variable .
more interesting are the results for the other two tasks.
in the case of code comment to code we found that out of the100 wrong predictionsweinspectedwereactuallyvalidimplementationsofthechangerecommendedbythereviewer.one example is presented at the bottom of fig.
black background where we show the input provided to the model i.e.
the code in the first line and the reviewer s comment inline this variable and the output of the model right below.
t5 successfully addressed the reviewer s comment.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
using pre trained models to boost code review automation icse may pittsburgh pa usaperfect predictions confidence confidence confidencecode to co dec ode comment to co dec ode to comment figure perfect predictions by confidence of the model however thepredictionisdifferentfromthetargetimplementation sincethelatteralsoincludesanotherchangethatwasnot explicitlyrequiredinthecodereview.thiscaseisrepresentative ofall56instancesweclassifiedas alternativesolutions forthis taskand giventhegoalofthe code comment to code webelieve they represent good predictions.
finally also for the code to comment task we found a large number of wrong predictions that are actually valuable with of themeven being semantically equivalent i.e.
t5 formulated a comment asking the same changes required by the reviewer but using a different wording .
one example is reported at the very bottomoffig.
.whilethemodelonlyreceivedthecodeasinput wealsoshowtheoriginalreviewer scomment i.e.
pleasemake thisoneavariableaswell tomakeiteasiertoassesstherelevance of the comment generated by t5 i.e.
extract the building ... .
overall our analysis showed that the perfect predictions really representalowerboundfortheperformanceoft5 especiallyfor the two tasks in which natural language comments are involved.
.
rq pre training and confidence in fig.
we observed better performance for the pre trained model inthecode comment to code andinthe code to comment task while thenonpre trainedmodelperformedbetterinthe code to code task.
theresultsofthemcnemar stestonthepredictionsat k confirm suchfindings besidesthesignificantdifferenceconfirmedforall tasks p value .
theorsindicate85 and59 higherodds of obtaining a perfect prediction using the pre trained model in thecode comment to code or .
andinthe code to comment or .
task whileoddsare34 lowerinthe code to code task or .
.twoobservationsareworthtobemade.first overall the pre trained model seems to represent a more valuable solution.
second the lack of improvement in the code to code task can be explained by the pre training and fine tuning we performed.
in deed the code to code task only focuses on source code with no natural language in the input nor in the output.
the fine tuning stage focusedonsourcecode wasprobablysufficienttothemodel to learn about the code syntax and the possible transformationsto perform.
the additional pre training also including technicalenglish did not benefit the model for the code to code task.
the othertwotasks instead eitherincludenaturallanguageasinput code comment to code orrequireitsgenerationasoutput code tocomment obtaining a boost of performance from the pre training.
fig.3depictsthepercentageofperfectpredictions y axis within eachconfidenceinterval from0.
.1upto0.
.
x axis when using the pre trained model and k .
to better interpret the reported results the gray line represents the overall performance of themodelwhenconsideringallpredictions e.g.
.
ofperfect predictions for the code to code task .in all three tasks we observe a clear trend with the predictions in the highest confidence bucket .
.
ensuring substantially better performance than the overall trend.
when only considering the predictions in this bucket the percentage of perfect predictionsincreasesto .
for code to code fromanoverall4.
.
for code comment to code overall .
and .
for code to comment overall .
.
considering the complexity of the addressed tasks the jump in performance is substantial andindicates the usability of the confidence level as a proxy for the predictionquality.also whilethepercentageofperfectpredictions is quite limited with seven out of ten predictions being wrong inthebest casescenario .
for code comment to code itis worth considering what previously observed in our manual analysis with valuable predictionswhichareclassifiedas wrong in our quantitative analysis.
.
rq comparison with the baseline fig.
compares the performance achieved by the t5 model with those obtained by the baseline .
in the line charts the continuous lines represent the pre trained t5 the dashed lines non pre trained t5 and the dotted lines the baseline.twoimportantpointsareworthremembering first the results in fig.
have been computed on the test set used in .
indeed the performance in terms of perfect predictions are substantiallyhigherascomparedtothoseinfig.
seevaluesonthe y axis duetothesimplerinstancesfeaturedinthisdataset.second thebaselinehasbeentrainedandtestedon abstractedcode asin the original paper while t5 worked on raw source code.
whenk t5 achieves substantially better performance.
the results of the statistical test in table always show a significant differenceinfavoroft5 adjusted p value .
withorsranging from .
non pre trained t5 vs in thecode to code task to .
pre trained t5 vs in thecode comment to code task .
thepre trainedt5inthiscaseperformsbetterthanthenonpretrainedoneforbothtasks.thisislikelyduetothelimitedsizeofthe fine tuning dataset used in this comparison.
indeed to have a fair comparisonwith wefine tunedt5onthetrainingsetweused in and composed by .5k instances vs the 134k we had in ourfine tuningdatasetwhenansweringrq rq4 .thisisprobably notsufficienttoeffectivelytrainalargemodelsuchast5 andmakes the instances used in the pre training fundamental to further learn aboutthelanguage.still evenwithoutpre training t5outperforms the baseline when k .
for example in the code comment to code task the baseline achieves .
perfect predictions against the .
of the non pre trained t5 and the .
of the pre trained t5.
the baseline observes a stronger improvement with the increasing of k i.e.
the beam size as compared to t5 see fig.
.
we believe this is due to usage of the abstraction.
indeed when workingwithabstractedcodethe searchspace i.e.
thenumberof possiblesolutionsthatcanbegeneratedwiththegivenvocabulary ismuchmorelimitedsincethemodeldoesnotdealwithidentifiers andliterals.attemptingtenpredictionsinasmallersearchspace is more likely to result in correct predictions.
the results of the codebleuconfirmthetrendobservedwiththeperfectpredictions with the pre trained t5 being the best model.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa rosalia tufano simone masiero antonio mastropaolo luca pascarella denys poshyvanyk and gabriele bavota code to code beam size beam sizecode comment to codecode bleu perfect predictions t5 pre trained t5 non pre trained baseline figure t5 vs.baseline we also looked at the union of perfect predictions generated by thetwoapproachesonthetestsettoverifythecomplementarity ofthetechniques.onthe code to code code comment to code task weobservedthat15 ofperfectpredictionsaresharedbyboth approaches i.e.
both succeed are perfect predictions only for t5 and only for the baseline.
table rq mcnemar s test adj.
p value and or task test p value or code to codet5 pre trained vs .
.
t5 non pre trained vs .
.
t5 pre trained vst5 non pre trained .
.
code comment to codet5 pre trained vs .
.
t5 non pre trained vs .
.
t5 pre trained vst5 non pre trained .
.
threats to validity construct validity.
as explained in section we took care of cleaning the datasets used in our study by removing duplicates and noisy data points to the extent possible.
still we are awarethatproblematicinstancesmaybepresent especiallyinthenew large datasetwebuilt.thismanifests forexample innon english comments orinsomewrong links betweencommentsandimple mentation e.g.
weassumethat mrimplementedachangedescribed incnlwhile in fact it implemented another change .
internal validity.
we did not fully explore the role played by the t5 parameters on its performance.
indeed our hyperparameter tuning was limited to variations in the learning rate as done in previous work .
for the other parameters we relied on the best architecture identified by raffel et al.
.
we acknowledge that additional tuning can result in improved performance.
externalvalidity.
rq1 rq4havebeenansweredusingadataset beingoneorderofmagnitudelargerascomparedtoourprevious work on automating code review tasks .
however our findings are limited to java.
concerning rq 5in which we compare with the baseline we only used the dataset presented in .
this is due to the fact that our previous approach requires code abstraction and as previously explained cannot work on instances having new identifiers and literals inserted during the code review process.thenewdatasetusedinthispaperhasnotbeenbuiltwith such a constraint in mind and thus it is not suitable for direct comparison.
related work our work relates to three research areas i dl techniques to automatesoftware relatedtasks ii empiricalstudiesoncodereview and iii works providing recommendations on how to optimize thecode reviewprocess and orpresenting techniques topartially automateit.herewefocusonthethirdresearcharea whileforthe firsttwowepointthereadertothesystematicliteraturereviews bywatson etal.
deeplearninginsoftwareengineering and by davila and nunes modern code review .
optimizing automatingthecodereviewprocess.
bystudying tools and techniques supporting code review tymchuk et al.
concluded that popular code review platforms e.g.
gerrit code flow phabricator mostly offer the same basic functionalities with little support for automating tasks.
such a finding has been confirmedbypascarella etal.
.also inastudyperformedby lewiset al.
at google the authors show that while developers are excited by the idea of embracing automated solutions for code review they find current solutions not to be ready for daily use.
startingfromtheseobservations researchersstudiedpossibleoptimizationsof thereviewprocess baum et al.
investigate the effectoforderingsubmittedchangesinalternativewaysratherthaninalphabeticalorderthat asshownbybarnett etal.
andbaum and schneider is sub optimal.
baum et al.
concluded that smarterorderingisneededasthesizeofthepatchincreases and suggest to aggregate changed parts by relatedness.
di biaseet al.
studied the impact of the patch size on the review seffectiveness findingthatsmallerpatches whilenotin creasing the defects found affect how reviewers approach theirtask.
spadini et al.
compared the effectiveness of a standard code review process with test driven code review tdr i.e.
the reviewerinspectsthechangedtestcodebeforetheproductioncode.
they show that tdr does not boost the code review effectiveness.
several researchers suggest exploiting defect predictionmodelsduringcodereview.similarly balachandran and singhet al.
suggest the use of static analysis tools to automatically spot coding standard violations and common defects.
concerningtheautomationofspecificcodereviewtasks authors proposedtechniquestooptimizethereviewers assignment.forex ample al zubaidi etal.
inopensourceandchouchen etal.
in industrial contexts show how a multi objective search based approach can simplify the code review triaging process.
shiet al.
and chouchen et al.
look at the automation ofcodereviewfromasimilarperspective.shi etal.
presenta dl model taking as input the code submitted for review and the revisedcodeimplementingthechangesrecommendedbyreviewers and providing as output whether the change can be accepted ornot.
note that the change s required by the reviewer s are notconsidered by the model.
chouchen et al.
use instead a set ofqualitymetricsasfeaturesformachinelearningalgorithmstoclassify the quality of the code submitted for review.
recently hellendoorn et al.
focus on the prediction ofthe location of a possiblereviewer scomment showingthateventhissimpletaskis challenging to automate.
the above discussed techniques are complementary totheapproachwepresentedin and asaconsequence tothe models experimented in this work .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
using pre trained models to boost code review automation icse may pittsburgh pa usa while shi et al.
and chouchen et al.
assess the code underreviewthrougha booleananswer i.e.
accepted rejected or well written badly written we attempt the automation of code changesimplementedincodereview.also theapproachbyhellendoorn et al.could be combined with the automation of the codeto comment task we presented.
conclusion and future work ourpaperstartsbydiscussinglimitationsintheapproachwerecentlyproposedtoautomatecodereviewtasks .wehighlighted thattheusageofcodeabstractiondoesnotallowtosupportnontrivialcodereviewscenariosrequiringcodechangesresultingin theintroductionofnewidentifiers literals.hence weproposedthe usage of a pre trained t5 model relying on a sentencepiece tokenizer to overcome such a limitation and work directly on rawsourcecode.ourempiricalevaluation performedonamuch larger andrealistic code review dataset shows the improvements brought by the t5 model that represents a step forward as compared to the state of the art both in terms of applicability i.e.
scenarios in which it can be applied and performance.
still the levelofactualperformanceobservedmakesthesetechniquesfar frombeingdeployableinpractice callingformoreresearchincode review automation.
our future research agenda will be focused on designing improved solutions to boost the prediction accuracy of these tech niques e.g.
by combining different representations of code and orbyexploitingthemodel sconfidenceasapossiblefilterto select only high quality recommendations .
the code and data used in our study are publicly available .
acknowledgment this project has received funding from the european research council erc under the european union s horizon research andinnovationprogramme grantagreementno.
.w m has been supported in part by the nsf ccf and ccf2007246 grants.
any opinions findings and conclusions expressed hereinaretheauthors anddonotnecessarilyreflectthoseofthe sponsors.