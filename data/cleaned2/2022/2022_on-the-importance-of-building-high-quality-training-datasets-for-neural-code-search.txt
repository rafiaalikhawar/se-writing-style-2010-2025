on the importance of building high quality training datasets for neural code search zhensu sun zhensuuu gmail.com monash university melbourne victoria australiali li tongji.edu.cn tongji university shanghai chinayan liu yanliu.sse tongji.edu.cn tongji university shanghai china xiaoning du xiaoning.du monash.edu monash university melbourne victoria australiali li li.li monash.edu monash university melbourne victoria australia abstract theperformanceofneuralcodesearch issignificantlyinfluenced bythequalityofthetrainingdatafromwhichtheneuralmodels are derived.
a large corpus of high quality query and code pairs is demandedtoestablishaprecisemappingfromthenaturallanguage to the programming language.
due to the limited availability most widely used code search datasets are established with compromise such as using code comments as a replacement of queries.
our empiricalstudyonafamouscodesearchdatasetrevealsthatover one thirdofitsqueriescontainnoisesthatmakethemdeviatefrom naturaluserqueries.modelstrainedthroughnoisydataarefaced withsevereperformancedegradationwhenappliedinreal world scenarios.toimprovethedatasetqualityandmakethequeriesofits samplessemanticallyidenticaltorealuserqueriesiscriticalforthe practical usability of neural code search.
in this paper we propose adatacleaningframeworkconsistingoftwosubsequentfilters arule basedsyntacticfilterandamodel basedsemanticfilter.this is the first framework that applies semantic query cleaning to code search datasets.
experimentally we evaluated the effectivenessof our framework on two widely used code search models and three manually annotated code retrieval benchmarks.
training the populardeepcsmodelwiththefiltereddatasetfromourframework improves its performance by .
mrr and .
answer on average with the three validation benchmarks.
ccs concepts software and its engineering reusability.
keywords code search dataset data cleaning deep learning xiaoning du and li li are co corresponding authors.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa association for computing machinery.
acm isbn ... .
reference format zhensusun lili yanliu xiaoningdu andlili.
.ontheimportance of building high quality training datasets for neural code search.
in 44thinternationalconferenceonsoftwareengineering icse may21 pittsburgh pa usa.
acm new york ny usa pages.
https introduction a semantic code search engine is a vital software development assistant which significantly improves the development efficiency andquality.withadescriptionoftheintendedcodefunctionality innaturallanguage asearchenginecanretrievealistofsemantically best matched code snippets from its codebase.
recently deep learning dl has been widely applied in this area in view of its advantages in semantic modeling and understanding of languages.
inthetaskofcodesearch dlmodelslearnandrepresentthesemanticmappingsbetweenthenaturallanguageandtheprogramminglanguage from query code pairs.
like many other dl tasks code search models are data hungry andrequirelarge scaleandhigh qualitytrainingdatasets.nevertheless collecting a large set of query code pairs is challenging where the queries are supposed to be natural expressions from developers and the code to be a valid semantic match.
instead considering the scale and availability code comments are popularly usedasanalternativetothequeries manyofwhichdescribethe corefunctionalitiesandwiththecorrespondingcodeimplementationrightlyavailable.tobetterunderstandthequalityofdatasets henceconstructed weinvestigatedagithubdataset codesearchnet java which is popularly used in current code search research.
surprisingly we found a considerable amount of noiseand unnaturalness in the queries of its data samples which can hinderthetrainingofhigh qualitymodelsforpracticalusage.as showninfig.
one thirdofitsqueriescontaintextfeatures see table1forexamplesofdifferentfeatures thathardlyexistinactual userqueries.thefeaturesaresummarizedbasedonourobservationsofthedataset andmaynotbesufficient.commentsmayalso be used for other purposes such as copyright and to do instead of describingthecorefunctionalities thusshallnotbeseenasqueries.
the proportion of noise data can be higher than one third.
code search models trained with noisy queries will face severe performance degradation when dealing with actual user queries.
thegapbetweenthecollectedcomment codepairsandthenatural userqueriesviolatesthebasicassumptionoflearningalgorithms ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa zhensu sun li li yan liu xiaoning du and li li non english languages .
interrogation0.
punctuation10.
html tags16.
urls0.
short sentence .
parentheses .
javadoc tags .
ja p33.
.
others noise figure statistics of code comments used in codesearchnet java .
the feature definitions are presented insection .
.
that the training data and the evaluation data share a similar distribution.itisalsonoteworthythatevaluatingthemodelwithanoisycomment code benchmark can hardly reflect how useful the model would be in practice and even worse may bring non negligible biastothemodeldesign evaluationandapplication.manyotherresearchers also point out the misalignment between code comments and natural user queries and report it as a threat to the validityoftheirapproaches.asmentionedin improving the quality of the training data is still a research opportunity for machine learning including dl based code search models.
considering that there are still plenty of comments close to actual userqueries and naturally paired with high quality code snippets apromising solution is to filter out the noisy ones.
manual filter ing can produce the most accurate results but is hardly practical forlarge scaledatasets.automateddatacleaningmethodsareof demanding needs.
queriesforcoderetrievalpossessspecific syntactic andsemantic characteristics which can be utilized as key features to distinguishgenuine user queries from noise.
typical syntactic features include text attributes such as keywords sentence length and language type.
semantic features are related to the intention underneath the textexpression whichusuallydescribesthecomputationalfunc tionality of code snippets and might be influenced by the design conventionofcommonprogramapis.comparedwithsyntacticfeatures semantic features are more abstract implicit and hard to be matchedbysimplerules.recently someinitialeffortshaveemerged on query quality enhancement but primarily focusing on the regularization ofsyntactic features.simple filtering heuristicsare proposed based on the appearance of verb and noun phrases keywords uncommonly used in queries and constraints on the query length .
however theimprovementindataqualityislimited.asdeclared in thecollecteddatasetisstillnoisydespitetheirdatacleaning efforts.theproposedrulesarenotsufficienttocoverthevarious syntacticviolations letalonethesemanticmisalignment.forexample warning messages such as use of this property requires java widely exist in the code comments but few code queries would request thisway.
hence aremaining challengeis recognizing the codecommentsthataresyntacticallyvalidbutencodesemantics rarely seen in natural user queries.
to tackle this challenge we propose an automated and effective data cleaning framework that distills high quality queries fromtable examples of syntactic rules.
syntax feature rule action example html tags partly remove p parse line p parentheses partly remove todo send requests javadoc tags fully remove returns a link support urls fully remove see non englishlanguagesfully remove punctuation fully remove interrogation fully remove is this a name declaration?
short sentence fully remove deprecated generally collected code comments on a large scale.
the framework is orthogonal to the design of code search algorithms and could be integrated with any of them to improve the quality of the trainingdataset.basically itencompassestwosubsequentfilters arule basedsyntacticfilter andamodel basedsemanticfilter.the rule based filter includes a set of systematically designed heuristic rulesandweedsoutdatawithanomaloussyntacticfeatures e.g.
html tags and javadoc tags.
it is developed to cover a diverse range of syntactic violations and each member inside is validated to reduce the noises effectively.
it is also extensible to fulfill the specificrequirementsforthedatasetbasedontheapplications.the model based semantic filter further refines the dataset produced bytherule basedfilterandretainsthecommentsthataresemanticallyclosetothenaturalqueries.thefilterreliesonabootstrap query corpus a set of high quality queries which represents how semanticallythequeriesshouldlook.itlearnsthesemanticfeatures of the corpus such as the expression style and topic with a dl model and leverages it to identify samples with similar semantics.
the bootstrap query corpus could be constructed with any trusted sources of natural user queries and we formulate it with question titles from stackoverflow in this work.
these titles are an ideal approximationofnaturalqueriesandcouldbere usedbyrelated studies.then avariationalauto encoder istrainedwiththe bootstrap query corpus which maps the inputs into a latent space and attempts to reconstruct the original inputs solely based on the latent features.
the reconstruction loss reflects how far away an input is from the training data distribution i.e.
the distribution of queriesinthebootstrapquerycorpus.thelowerthereconstruction loss the more qualified an input is as a natural query.
we compute thereconstructionlossforeachcodecommentintherawdataset and cluster them into two groups.
the group of qualified queries is retained for training and the group of noises is discarded.
to evaluate the effectiveness of our data cleaning framework wecomparetheperformanceofcodesearchmodelstrainedwith datasets before and after the filtering.
one training dataset two neuralmodels andthreemanuallyannotatedvalidationdatasets areusedintheexperiments andourframeworkbringsasignificant performance improvement under all settings.
in particular the performanceofthepopulardeepcs modelisimprovedby .
mrrand21.
answer onaveragewiththethreevalidationdatasets.moreimportantly withlesstrainingdatausedafterthe filtering we also save the training time and computation resources.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
on the importance of building high quality training datasets for neural code search icse may pittsburgh pa usa further wecarryoutacomprehensiveablationstudytovalidate theusefulnessofeachfiltercomponentandeachruleandmanually inspect the quality of the rejected and retained data.
finally we releasetheimplementationofourframework nlqf andacleaned codesearchdataset cofic tofacilitatefutureresearch.thesource code and datasets are available at to the best of our knowledge this is the first systematic data cleaning framework for comment based code search datasets.
our main contributions include a two step data cleaning framework for code search datasets which bridges the gap between code comments and natural user queries both syntactically and semantically.
implementation of the framework as a python library for the code search task in academia and industry.
a comprehensive evaluation of our framework s effectiveness whichdemonstratessignificantmodelperformanceimprovement on three manually annotated validation benchmarks.
thefirstsystematicallydistilledgithubdatasetforneuralcode search containing over one million comment code pairs.
preliminaries we prepare readers with the primary sources for collecting querycodepairsandthevariationalauto encoder amajorbuildingblock of our framework.
.
data source a query for neural code search describes in natural language the functionality of the code snippets desired by users e.g.
convert string to json object .
the ideal data source for genuine codequeries is the production data from existing neural code search engines.
however these queries are not publicly accessible due to privacy and business sensitivity.
in academia researchers use texts withsimilarintentions e.g.
codecomments asareplacement.the primary alternative data sources for semantic code search research include github and stackoverflow.
.
.
github.
github is an open source community hosting more than million repositories.
it is the most popular platform for developers to share and manage their projects.
the large scale well maintained repositories on github are a treasury for code reuse during development thus naturally becoming the main re trieval source for code search tasks.
moreover mature projectsare usually accompanied by canonical development documents.
according to javadoc a code comment style guide the first sentence of doc comments should be a summary sentence.
therefore it is convenient to construct a code search dataset by collecting the code snippets paired with the first sentence of comments forming the comment code pairs.
javadoc generated comments have hence been widely used in practice for various software engineering purposes due to their large scale ease of obtaining and being close to actual use scenarios.
however developerswritecommentsfortheirsoftwareprojects withoutconsideringtheretrievalpurposes.notallthecomments properly map to queries.
as mentioned in section the codesearchnet collected from github contains plenty of anomalies that rarely exist in natural user queries.
it is not appropriate to includethesecommentsinthedataset andwecallformoreattentiontobe drawn to this problem.
.
.
stackoverflow.
stackoverflow servesasaq acommunity specialized for software developers.
it is a rich resource of software related questions and answers.
when asking about codes or apis for implementing a specific functionality users would proposeaquestiontitletoexpresstheirintention.thesearenatural userquerieswithvalidsyntaxandsemantics.hence researchers also collect the titles of stackoverflow questions paired with proper answers containing sample code snippets which also form thequery codepairs.othersalsoevaluatetheircodesearchmodels with queriesmanually selected from stackoverflow and additionalpublicevaluationbenchmarkscouldbefoundin .
compared with the github data source queries from stackoverflow have a significant advantage of being closer to natural user queries butthequalityofcodesamplesishardtoguarantee.hence the dataset collected from stackoverflow is still not as desired.
nevertheless thequerycorpusisvaluable.itisworthinvestigating whether and how it could be leveraged to improve the other query code datasets.
.
variational auto encoder variationalauto encoder vae isaneuralmodelthatlearns the distribution of a set of data.
a vae model consists of an en coder and a decoder.
the encoder learns to map an input data x into a prior distribution p z from which a latent variable zis sampled andthedecodermaps zbackto x areconstructionof x. it is expensive to calculate p z directly so vae introduces an approximate posterior q z x .
and are parameters of the prior and the approximate posterior.
thelossfunction evidencelowerbound elbo whichseeks to maximize the likelihood of reconstructing the original data and minimize the kullback leibler kl divergence between the actual and estimated posterior distributions is represented as l eq z x kl q z x p z whereklrepresents the kl divergence.
theoretically the distributionsofq z x andp z canbearbitrary.inpractice thegaussian distribution is mostly adopted.
the data cleaning framework this section introduces our automated and effective data cleaning frameworkforcodesearchdatasets mainlytofilteroutquery code pairswithinappropriatequeries.theframeworkconsistsoftwo subsequentfilters the rule basedsyntacticfilter andmodel basedsemanticfilter.anoverviewoftheframework whenappliedtoclean thecomment codepairscollectedfromgithub isshowninfig.
.
therawcomment codepairsarefirstlycleanedbytherule based filter where a ruleset is appliedto detect the existence of invalid querysyntax.next forthemodel basedfilter leveragingasmall bootstrap query corpus as the semantics reference a vae modelis trained to model its characteristics and further used to reject commentsviolatingthenaturalquerysemantics.herewecollect thebootstrapquerycorpus noneedofthepairedcodesnippets from stackoverflow and more detailscan befound insection .
.
such we take advantage of both the github and stackoverflow authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa zhensu sun li li yan liu xiaoning du and li li github stackoverflowheuristic rule variational auto encoderbootstrap query corpus em gmm clusteringraw commentcode pairsjavadoc tags html tags short sentence ... losstrain model based semantic filter comment code pairs query code pairsrule based syntactic filter figure an overview of our data cleaning framework.
data sources and produce a large set of high quality query code pairs.inthefollowing weelaborateonthedesignofthetwofilters.
.
the rule based syntactic filter codecommentscontainricherinformationthanjustdescriptions on code functionalities and manifest various syntactic features rarely existing in actual code search quires.
for example urls are usedforexternalreferences andhtmltagsareusedincomments for documentation autogeneration.
to reduce such deviations from naturalqueries wesampled codecomments fromcodesearchnet manually inspected and summarized noises in these 949instances.
we establish a black list of invalid syntax features toreject unqualified code comments.
if a comment matches any of thesefeatures weremovetheinvalidpartsiftheyaredetachable otherwise we abandon this comment code pair.
based on a comparative observation of code comments and user queries wedevelopasetofrulestopreciselyidentify synthetically inappropriate queries and leave the fine grained semantic check tothe model basedfilter.
tofacilitate themanagement wedefine three criteria that the ruleset must comply with any rule shoulddefineauniqueandspecificconstructionpattern therulesshould be conservative and limit the preclusion of valid queries within an acceptable range and any rule isnot asubrule ofother rules in theset.
asa plug inframework therulesetis extensible and any rules that meet these criteria can be appended to the set.
weintroducethesyntaxfeaturescoveredbyourrulesetinthefollowing andtheirexamplescanbefoundintable1.weempirically decide whether to keep the content enclosed by a feature structure or not and validate the decisions with experiments see our web site for more details and manual inspection see section .
.
fromtheresults ourdecisionshelpimprovethenaturalnessand bring greater improvement to the model performance.htmltags htmltagsareusedfordocumentationautogeneration in comments and should not appear in user queries.
however the content wrapped by the tags can still be informative.
therefore weremovethehtmltagsfromthecommentsbutkeepthe wrapped content.
parentheses parentheses in comments are for adding supplementary information and do not appear in user queries.
due to suchpurpose the removal of the content inside the parentheses doesnothavemuchinfluenceonthecompletenessofthecomments.we only retain the content outside of the parentheses.
javadoc tags javadoc tags starting with an sign are special tagsindicatinga javadoccomment.suchcommentsare onlyconsumed by the javadoc project for autogenerating well formatted documentation.consideringthatthespecialsyntaxofthetagsmay mislead code search models on natural language understanding we reject all comments containing javadoc tags.
urlsurlsincommentsprovideexternalreferencestorelevant code snippets but natural language queries do not contain any urls.
we reject all comments containing urls.non english languages non english expressions exist as developers from different countries may write comments in their first languages.however currentcodesearch modelsarenot designed to handle multi languages.
we reject all non english comments.punctuation sometimes punctuationsymbolsareusedforsection partitioninginthecomments.forexample developersusearow of equal signs or asterisks signs see examples in table to indicate a new section.
for effectiveness we reject comments containing no english letters in our implementation.interrogation basedonourobservation someofthecomments in the dataset are interrogative.
developers seem to use comments to communicate with their collaborators during the code reviewprocess.
there may be some sparse information about the code functionality butthequalityishardtocontrol.werejectcomments ending with a question mark.shortsentence thesentencelengthisacommonlyusedcriterion for comment filtering.
extremely short comments are not informativeenoughforcodesearchmodelstoestablishtheirmappingto thecorresponding codesnippets.werejectcommentscontaining no more than two words.
.
the model based semantic filter thissectionintroducesthemodel basedsemanticfilter whichtakes the initially cleaned comment code pairs from the rule based filter as input and further selects the pairs with comments semantically closetothequeriesinapre collectedbootstrapquerycorpus.we present the detailed design of the vae model and discuss how it is used for filtering.
.
.
the vae model.
the two main components of a vae model aretheencoderanddecoder whicharegenerallycomposedofdeepneuralnetworks.here weusegatedrecurrentunit gru for both the encoder and decoder in our vae model.
gru is a variant of recurrent neural network rnn which enables the modeltocaptureinformationfromsequentialtexts.fig.3illustratesan overviewofthedesignofthemodelstructure.detailsabouteach layer are as follows.
embedding given a query w0w1...w nof length n thei th token iswi.
the embedding layer is responsible for mapping each tokenintoanembeddingvector.itconsistsofanembeddingmatrix e row d whereowis the vocabulary size of the query language anddis the dimension of embedding vectors.
the matrix is initialized with random values and updated during training.
gruencoder wedesigntheencoderofvaeasabi directional gru.sequentially itdealswiththeinputtokens andpropagates theupstreamanddownstreamcontextthroughthehiddenstates authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
on the importance of building high quality training datasets for neural code search icse may pittsburgh pa usa convert int to string token embeddingconvert int string to cell cell cell cell gru encoder cell cell cell cell gru decoder convert int string to cell reconstructed tokenlatent variable zloss figure the structure of the variational auto encoder in themodel basedfilter.thedashedlinesdenotethepropaga tion of hidden states in neural cells.
respectively in the forward and backward directions as shownin eq.
and eq.
.
embmaps a token to its embedding vector.
finally wesumupthelasthiddenstatesofbothdirectionstoget the final hidden state as in eq.
and pass it to the next layer.
hi gru emb wi hi h i gru emb wi hi h h n h n latent variable based on the hidden state hfrom the encoder we estimate the parameters of a gaussian distribution with a fullyconnected layer which are the mean vector and variance vector 2.thelatentvariable zisrandomlysampledfromthisdistribution.
the equations are as follows 2 fc h z r e 2 wherefcis a fully connected layer and ris a random vector from the standard normal distribution.
grudecoder thelatentvariablerepresentsthekeyfeatures of the original input in a highly abstract and compact way.
the decoderworkstoreconstructtheinputsolelybasedonthelatent variable.
iteratively the decoder computes the hidden state siat each step iand reconstructstoken w prime i basedon the previousstate si orzat step and w prime i 1generated in the previous step.
the equations are as follows si braceleftbigg gru emb bos z i gru emb w prime i si i pi fc si w prime i argmax pi wherebosis a special token indicating the start of a sentence and pi rowrepresents the probability of i th token to be generated.
losswemeasurethelikelihoodofreconstructingtheoriginal inputwiththecross entropy ce loss.hence theelbolossintroduced in section .
can be computed as l nn summationdisplay.
i 1celoss wi w prime i kldivergence 2 performance best dividing point figure an illustration of the relation between the portionofretaineddataandtheperformanceofthecodesearchmodel trained with it.
wherecelossandkldivergence represents the calculation of the ce loss and kl divergence.
.
.
the filtering algorithm.
we train the vae model with a set ofhigh qualitycodesearchqueriescollectedfromnear realscenarios whichwecallthebootstrapquerycorpus.afterthetraining thevaemodelisabletorecognizewhetheraquerysemantically resembles those in the corpus.
we measure the reconstruction loss i.e.
theceloss ofaninputwhenfedtothevaemodel whichjustreflectshowwellitiswithinthetrainingsetdistribution.intuitively the loss value is the anomaly score gauging how far an input stays away from the queries in bootstrap query corpus.
comments with smaller losses are more likely to be query appropriate.
toselectcommentsresemblingqueriesinbootstrapquerycorpus we sort the comments based on their reconstruction losses inascendingorder andretainthetop rankedones.itistrickyto decide an appropriate dividing point for retaining the portion with better quality and discarding the remaining.
the less data we keep fromthetop thehigherthedataset quality.however asharpreduction in the data size hinders the performance of the trained codesearchmodel.fig.4showsatheoreticalmodelillustratingthe relation between the dividing point and the model performance.
astheamountofretaineddataincreases themodelperformance firstly increases and then decreases after reaching the peak.
there is a trade off between the quality and quantity of the dataset.
we leverage an unsupervised clustering algorithm em gmm expectation maximizationforgaussianmixturemodel to decide the partition automatically.
it is widely used to model themixed distributions of a dataset.
for our task em gmm dividesa set of comments into the qualified and the unqualified groups basedonthereconstructionloss.foreachgroup gmmfitsagaussian probability density function and mixes them together as the distribution of the whole dataset which can be represented as p x n x q q n x uq uq where isthemixturecoefficientforthequalifiedgroup q q and uq uq are the parameters for the gaussian probability density functions of the qualified and unqualified groups respectively.
finally theemalgorithm isappliedtoestimateasetofoptimal values for all the parameters.
note that to establish a high quality code search dataset all commentsareprocessedtogetherwiththeirpairedcodesnippets.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa zhensu sun li li yan liu xiaoning du and li li hence weobtainasetofcomment codepairsafterapplyingthis semantic filter where the comments are syntactically and semantically close to natural user queries.
experiment setup we introduce the research questions the basic experimental setup about the datasets and models and the evaluation metrics used throughouttheevaluation.theresearchquestionsweaimtoanswer include rq1 how effective is our data cleaning framework?
rq2 what is the impact of each filter component and each rule on the effectiveness of our framework?rq3 how is the dividing point determined by the clustering algorithm during the model based filtering?
.
datasets three types of datasets are involved in our evaluation including the training and validation datasets used to train and assess the performanceofcodesearchmodelsandthebootstrapquerycorpus used to develop our model based filter.
to make the best use ofexisting resources we focus on the java programming languagein this work for which there have been the most public datasetsand models in the field of neural code search.
theoretically ourframework is language independent and applicable to other programminglanguageswithaproperadaptationofthefilteringrules.
.
.
trainingdatasets.
weusethepopularcodesearchnet csn datasettotrainallthecodesearchmodels.csnisacollectionofdatasets and benchmarks for semantic code retrieval.
it extracts functionsandtheirpairedcommentsfromgithubrepositories.it coverssixprogramminglanguages andwetakethetrainingdataset for java which contains data points.
we took the first sentence of each comment.
in what follows we denote it as csn t. anotherwidelyuseddatasetindeepcs isnotincludedbecause theauthorsonlyreleasedtheprocesseddata butourframework cannot work without accessing the raw data.
.
.
validation datasets.
we utilize human annotated validation datasets to evaluate how well a code search model performs in a real world scenario and three widely used datasets are adopted.
itis noteworthy that the validation datasets are neverfiltered by our frameworkinordertoensurethefairnessofourexperiments.they are listed as follows csn vcsn also offers a validation benchmark for java containingquery codepairscollectedfrombingandstackoverflow.humanannotatorsarehiredtoratetherelevancebetweenthequeryandthecode.pairswithascoregreaterthan0aredeemedasrelevant andthereare434relevantpairsintotal.inthedataset each pair is accompanied by distractor code snippets.
it means givenaquery thecodesearchmodelneedstoretrievetheground truth among candidates.
cbcosbench cb isa validationdataset consistingof selectivequeriesfromstackoverflow.foreachquery theauthors preparedaroundtenparingcodesnippetsasitsgroundtruths including its best answer on stackoverflow and several other matched code snippets selected from github.
additionally there is a pool of distractor code snippets.
the model needsto search the ground truths from a mixture with the complete code pool given a query.
ncsedproposed in the ncsed dataset contains question queries manually collected from stackoverflow.
for each query therearearoundthreepairingcodesnippetsselectedfrom github.
the ground truths are mixed with other distractorcodesnippetscollectedfromgithub.thesearchmodel is required to retrieve the ground truths from the large corpus for a query.
the extremely large search space in ncsed and cb makes it extremely hard for code search models to achieve a good performance and the performances variations brought by data cleaning canalsobetoomarginaltocompare.withoutlossofgenerality foreachqueryinncsedandcb weconstruct999distractorsnippets following a similar fashion as csn v. .
.
bootstrapquerycorpus.
stackoverflowisanidealsourcefor collecting resemblers of actual user queries though the quality of thepairingcodesnippetsishardtoguarantee.itbecomesanoptimalchoicetoestablishthebootstrapquerycorpus.wesurveyedexistingstackoverflowdatasetsinthecodesearchfield andfoundthatthey were of severely limited size.
with their aim to collect high quality question code pairs numerous questions were discarded due tothe lack of qualified code answers.
hence to better facilitate the training of our vae model we determined to construct a questiononly corpus from stackoverflow instead of using existing ones.
accordingtoastudy thestackoverflowquestionscanbedivided into four types debug corrective need to know howto do it and seeking different solution .amongthem questions of the how to do it type are most relevant to queries for the codesearchtask.aimingtoselectthemostqualifiedresemblers werequirethequestiontitlesto1 startwith howto betaggedwith java and3 passtherule basedsyntacticfilterproposedinsection3.
exceptfor the interrogationrule .
intheend 779out of java related question titles were retained.
afterwards wetransformedthemintodeclarativesentencesbyremovingthe starting how to and thequestionmarks ifany thusformingthe bootstrap query corpus which is used to train the vae model later.
.
code search model two code search models deepcs and carlcs are used inourexperiments.theyaredesignedwithrepresentativearchitectures among most neural code search models.
deepcs is based onthe siamesearchitecture and carlcsisaninteraction based network .
the siamese architecture consists of two dl models to represent the query and code respectively with independent embeddingvectors andthesimilaritybetweenthesevectorsisusedtomeasuretherelevancebetweenqueryandcode.theinteraction basednetworkcomparesthequeryandcodedirectlybygenerating an interaction matrix to reflect their relevance.
whentrainingthemodelswithourtrainingdataset weadopted the recommended settings for all the hyper parameters except for thetrainingepochofthedeepcsmodel.inordertosavesometime and computation resources we set the maximum training epochof deepcs to instead of the recommended .
without lossof fairness the same setting has been used for training with the dataset either before or after the data cleaning.
this change should authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
on the importance of building high quality training datasets for neural code search icse may pittsburgh pa usa table the answered k and mrr scores of the deepcs and carlcs models trained over different datasets.
model test set query train set pairs train hours a a a mrr deepcscsn v 434csn t all 8h .
csn t controlled 4h .
csn t filtered 4h .
.
.
.
.
cb 52csn t all 8h .
csn t controlled 4h .
csn t filtered 4h .
.
.
.
.
ncsed 287csn t all 8h .
csn t controlled 4h .
csn t filtered 4h .
.
.
.
.
carlcscsn v 434csn t all 6h .
csn t controlled 3h .
csn t filtered 3h .
.
.
.
.
cb 52csn t all 6h .
csn t controlled 3h .
csn t filtered 3h .
.
.
.
.
ncsed 287csn t all 6h .
csn t controlled 3h .
csn t filtered 3h .
.
.
.
.
not affect the evaluation conclusion on the effectiveness of our framework whichfocusesmoreonwhetherthemodelperformance improves after removing the noises instead of its absolute level.
.
evaluation metrics twowidelyusedmetricsareadoptedinourexperimentstoevaluate the code retrieval performance.
answered k answered k abbrev.
a k is the number of queries answered by snippets in the top k results.
mean reciprocal rank mrr mrristheaverageofthereciprocal ranks of the ground truth in the result list.
results inthissection weshowtheexperimentalresultsandanswertheresearchquestions.measuresforbothevaluationmetricsarereported as the medium over five independent runs.
.
rq1 effectiveness this experiment evaluates the effectiveness of our data cleaning solution as a pre processing step when training neural code search models.
specifically one training dataset csn t two code search models deepcs and carlcs and three validation datasets csnv cb and ncsed are used in the evaluation.
thus we have six experimental settings in total.
during experiments a relatively smaller filtered training set will be derived from csn t after our framework is applied for the data cleaning.
to also benchmarktheperformancevariationbroughtbythesizeshrinking wefurtherderiveacontrolledtrainingsetbyrandomlyselecting from csn t an equivalent number of data as the filtered set.
we observethe modelperformance resultedfrom trainingwiththese three datasets respectively.
themodelperformanceismeasuredwith fourevaluationmetrics namely a a a andmrr andtheresultsareshownintable2.underallthesixexperimentalsettings ourdatacleaning framework demonstrates a positive influence on the model s searchingabilityandhelpsithitthebestscore.onaverageofthe threevalidationdatasets deepcstrainedoverthefiltereddataoutperformstheonetrainedoveroriginaldataby21.
a .
a .
a and .
mrr.
correspondingly the improvements of carlcs are .
a .
a .
a and38.
mrr.
regarding the mrr on the three validation datasets csn v cb and ncsed deepcs achieves .
.
and .
andcarlcsachieves0.
.056and0.
respectively.basically deepcs and carlcs are boosted to their new best records and carlcs sees agreater improvement.
note that thea score of carlcs over ncsed is increased by .
from to which is an extraordinary improvement.
overall with around half of the data quantity and half of the training time models trained over the filtered data achieve a significant improvementon the number of answeredqueries and the rank of ground truth in search results.
answer to rq1 our filtering framework produces a highquality query codedataset which shortensthe trainingtime by reducing the training data and effectively improves theperformance of the code search model under a real world application scenario.
.
rq2 the impact of each filter component and each rule weevaluatetheeffectivenessofeachfiltercomponentwithablation experiments and conduct manual inspection on the queries accepted rejected by each syntactic rule and the model based filter to study their precision in identifying noises.
each time one of the two filter components is muted for the ablation experiments.
we observe the model performance after authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa zhensu sun li li yan liu xiaoning du and li li table results of the ablation experiments on the filter components.
model test set query train set pairs a a a mrr deepcscsn v 434csn t all .
csn t filtered .
rule filter only .
.
.
.
.
model filter only .
.
.
.
.
cb 52csn t all .
csn t filtered .
rule filter only .
.
.
.
.
model filter only .
.
.
.
.
ncsed 287csn t all .
csn t filtered .
rule filter only .
.
.
.
.
model filter only .
.
.
.
.
carlcscsn v 434csn t all .
csn t filtered .
rule filter only .
.
.
.
.
model filter only .
.
.
.
.
cb 52csn t all .
csn t filtered .
rule filter only .
.
.
.
.
model filter only .
.
.
.
.
ncsed 287csn t all .
csn t filtered .
rule filter only .
.
.
.
.
model filter only .
.
.
.
.
trainingwithsuchderivedfiltereddatasetandcompareitwiththeir previous performance in section .
.
if the performance declines comparedwithwhenbothfiltersareenabled wecaninferapositive impact of the muted component on the framework effectiveness.
weevaluatetheperformanceofdeepcsandcarlcstrainedunder ablation and report the results in table .
the removal of any filter leads to worse performance scores.
without the model based filter the a a a and mrr scores of deepcs on the three validation sets reduce by .
.
.
and .
on average.
carlcsseesamuchmoreseverededuction andonaverage a a a and mrr decrease by .
.
.
and .
.
after removing the rule based filter the performance of deepcs averagely drops by .
a .
a .
a and .
mrr.meanwhile theaveragereductionpercentagesofcarlcs onallthevalidationsetsare32.
a .
a .
a and14.
mrr.itisnoteworthythatthea 1scoreofcarlcson ncseddropsfrom35to18whentherulefilterismuted indicatingthattherulesetplaysaveryinfluentialpartduringthedatacleaning.
for the manual inspection two annotators with over two years development experience are hired to rate how likely a sentence is tobeusedasacodesearchquery.theratingscorerangesfrom0to where0meansworstand2best.thereare11groupsofdatato annotate includingeightgroupsofcommentsrejectedbyeachrule the group of comments discarded by the model filer the original csn t dataset and the filtered dataset after the two filter cleaning.
thelasttwogroupsareforcomparisonpurposes.forrulesfocusing on detachable features i.e.
the parentheses and html tags we let the annotators judge how well the removed part can help with aqueryexpression.wesampleasubsetofdatafromitsfullsetfor each group.
the sample size ssof each group is computed by a statisticalformulawhichisextractedfrom ss z2 p p c2 z2 p p c2 population wherepopulation is the size of the entire dataset pis the standard deviation of the population cis the confidence interval margin of error zis the z score determined by the confidence level.
in this experiment we choose to work with a confidence level i.e.
.96z scoreaccordingto astandarddeviationof0.
.
is the maximum standard deviation and a safe choice given theexact figure unknown and a confidence interval of .
we also measure the agreement between the two annotators with cohen s kappa which is .
and within the range of fair to good.
for each data we finalize its score as the average of scores from the two annotators and display the statistics in table .
we report the number of data examined in each group the respective portion of data scored as or no less than and the group s average score inthelastfourcolumns.ingeneral thecommentsrejectedbyeither the rule based filter or the model based filter poorly resemble real userqueries with96.
and85.
ofthemreceivingascoreof0 andtheaveragescoresbeingaslowas0.04and0.
respectively.
still it comes at an acceptable cost of losing a small set of good qualitydata where3.
and14.
ofthediscardeddatabythetwofiltersscoreatleast1.eachoftheeightrulesrejectscodecommentsinaneffectiveway withfourofthemrejectingnon query likedataat100 precision.theprecisionoftheparenthesesruleisrelativelylow where11 ofthediscardeddataisofhighquality.inthefuture when deciding whether the content inside the parentheses should authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
on the importance of building high quality training datasets for neural code search icse may pittsburgh pa usa table results of the manual inspection.
type rule likeness score avg.
origin .
.
.
discarded by rule filerhtml tag .
.
.
parentheses .
.
.
javadoc tags .
.
.
urls .
.
.
non eng.
lan.
.
.
.
punctuation .
.
.
interrogation .
.
.
short sentence .
.
.
in total .
.
.
discarded by model filer .
.
.
retained .
.
.
be removed a more refined rule can be derived.
also the modelbasedsemanticfilterisaccompaniedbyalargersacrifice indicating it as a more challenging task.
overall through the two phase filtering the average likeness scoreincreasesfrom0.27to0.
.inparticular theportionofnonquery likedatadropsfrom79.
to59.
andtheportionofhighly query likedatascoringatleast1improvesfrom21 to40.
.there arestillmanycommentsinappropriatetobeseenascodesearch queries but our data cleaning framework makes a substantial contribution to alleviating the situation.
we call for more attention to be drawn to overcoming related challenges.
answertorq2 eachfilterandruleinourframeworkdemonstrates a positive contribution to the effectiveness.
the full settingboostsittothebestperformance.however thereremainmanyunqualifiedcommentsevenafterthefiltering and it calls for more attention to be paid from the community.
.
rq3 quality of dividing point determined in the model based filtering in themodel based filter we useem gmm to decidethe dividing pointbetweenthequalifiedandtheunqualifiedgroups.toassess thequalityofthedividingpoint weobservethemodelperformanceresultingfromalternativedividingpoints includingfixproportions andtheonedecidedbyk means anotherwidelyusedclustering algorithm.forthefixedproportions weseta25 stepandselect and top ranked comments respectively.
theresultsondeepcsandcarlcsarereportedintable5.for deepcs em gmmoutperformsk meansandthefixedproportionson all the validation sets.
compared with the second best partition em gmm still achieves higher average performances by .
a .
a .
a and3.
mrr.thesuperiorityisalso observed on carlcs at every metric and em gmm outperforms k means on average of csn v and ncsed by .
a .
a .
a and .
mrr.
em gmm ultimately retains data points accounting for .
of the original dataset which locates between .
thedividingpointsetbyk meansand75 .asdiscussedinsection3.
.
the relation between the data quantity and the model performance should be a convex function.
according to the property of theconvex function if there exists another optimal dividing point it would locate between .
and .
.
therefore em gmmsuccessfully identifies an optimal solution of the dividing point with an error less than .
calculated by .
.
.
answer to rq3 em gmm produces a better approximation ofthebestdividingpointforthedatasetsandisadequateto be used in the framework.
application thissectionpresentstheapplicationsofourfilteringframework including a proof of concept data cleaning toolbox and a highquality code search dataset.
.
nlqf natural language query filter wereleasetheimplementationofourfilteringframeworkasathirdpartypythonlibrary naturallanguagequeryfilter nlqf which is designed to systemically filter queries for neural code search models.
as alightweight library with convenient apis nlqfcan be easily integrated into the development pipeline of any codesearch model.
besides nlqfis extensible at several features to ensure its applicability in a wide range of contexts extensible ruleset the ruleset in nlqfis configurable which enablesuserstospecifytherulesbasedonthecharacteristicsoftheirowndata.besides nlqfacceptsuser definedfunctionsasapartof rule basedfiltering.onecaneasilyextendthefilterimplementation by creating the filtering function for any new rule.open sourcefilteringmodel nlqfrequiresatrainedvaemodel inthemodel basedfilter.wereleasethesourcecodefortrainingthe vaemodelusedinthispaper.followingtheinstructions userscan easily train a new model with their own bootstrap query corpus which may boost the filtering performance further.
tunabledividingproportion besidestherecommendedclustering method em gmm nlqfalso provides an interface accepting user defineddividingpoints.userscancreatetheirownmethodfor finding the dividing point and configure nlqfto adopt it easily.
.
cofic codebase paired with filtered comments we build and release a codebase paired with filteredcomments cofic for java programming language.
.
.
datasetbuilding.
wecollectthesourcecodeofjavarepositoriesfromgithubaccordingtothelistmaintainedbylibraries.io fromthesefiles weextractthemethodsandcorrespondingcomments using the scripts provided by codesearchnet .
in the end raw comment code pairs are obtained.
through the processing with nlqf there are data points left in the cleanedquery codedataset.detailedstatisticsofthedatasetduring filtering are reported in table .
.
.
datasetcomparison.
wecompare cofic onthequeryquality withseveralotherdatasetscurrentlyusedinneuralcodesearch research.
following the same manual inspection convention as authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa zhensu sun li li yan liu xiaoning du and li li table results of changing the em gmm to other methods.
model dividing point csn v cb ncsed a a a mrr a a a mrr a a a mrr deepcspercentile .
.
.
percentile .
.
.
kmeans .
.
.
.
em gmm .
.
.
.
percentile .
.
.
percentile .
.
.
carlcspercentile .
.
.
percentile .
.
.
kmeans .
.
.
.
em gmm .
.
.
.
percentile .
.
.
percentile .
.
.
table the statistics during the data filtering.
step rule discarded retained rule basedhtml tags parentheses javadoc tags urls non english languages punctuation interrogation short sentence model based table7 acomparisonbetweenthetrainingdatasetsforcode search tasks.
dataset source language likeness cofic github java .
m csn java github java .
k hu et al.
github java .
k staqc stackoverflowpython .
k sql .
k barone et al.
github python .
k insection5.
theannotatorsratethequeriessampledfromeach dataset reported in table .
again we measure the agreement level between the two annotators with cohen s kappa which is0.73andwithintherangeoffairtogood.amongallthedatasetscollected from github coficreceives the highest score on data quality but there is still a gap compared with the stackoverflow dataset staqc.
indeed the datasetscollected from stackoverflow havehigh qualityqueries buttheysufferfromtheunstablecode quality in answers .
with our filtering framework a github dataset with better quality is established.
besidestheuser study wealsoexperimentallycompare cofic with csn t. we train the deepcs and carlcs models with threedatasets csn t cofic and a controlled cofic same size as csnt .
the model trained with coficoutperforms other experimental settingsonthethreevalidationdatasets csv v cb andncsed .
the detailed results are reported in table .
threats to validity rule design though our experiments have evaluated the usefulness of each rule in the ruleset the rule based filter may still introduceafewfalsepositivesorfalsenegativesduetoitsdesign and implementation.
for example the widely used query quick sort can be filtered out by the rule short sentences.
besides some rules are tricky to be implemented exactly in line with our aim.for example non english letters in the comments are identified based on ascii encoding.
it may leave out several other languages also using english letters.
but no english sentences will be falsely filtered out.
overall it requires further exploration on balancing the trade off between precision and recall better.bootstrapquerycorpus thebootstrapquerycorpusinthiswork is built based on the questions on stackoverflow.
only titles starting with how to are collected into the corpus which limits the sentence pattern.
the vae model trained over this corpus may not have a good tolerance to other patterns.
besides stackoverflow titlesarealsonotfullyquery appropriate.althoughwefilterthe titles by rules there are still semantically irrelevant texts left.generalization limited by the accessibility of models and evaluation benchmarks for code search tasks we evaluate our solutiononly on java datasets.
in theory our approach is capable of any comment based code search dataset.
yet the generalization of our filtering framework in different programming languages has not beenexperimentallyverified.besides weonlyevaluateourfiltering framework on two code search models deepcs and carlcs which is also a threat to the generalizability of our approach.
related work codesearch dataset recentyearshavewitnessedagrowinginterest in the semantic search for code snippets .
dl models are appliedtoestablishlinksbetweennaturallanguageandprogramminglanguage.totrainthesemodels authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
on the importance of building high quality training datasets for neural code search icse may pittsburgh pa usa table the results of the experimental comparison between cofic and csn t. modeltest set query train set pairs a a a mrr deepcscsn v 434csn t .
cofic controlled .
.
.
.
.
cofic .
.
.
.
.
ncsed 287csn t .
cofic controlled .
.
.
.
.
cofic .
.
.
.
.
cb 52csn t .
cofic controlled .
.
.
.
.
cofic .
.
.
.
.
carlcscsn v 434csn t .
cofic controlled .
.
.
.
.
cofic .
.
.
.
.
ncsed 287csn t .
cofic controlled .
.
.
.
.
cofic .
.
.
.
.
cb 52csn t .
cofic controlled .
.
.
.
cofic .
.
.
.
code snippets paired with comments are collected from github .
according to a manual investigation there are categories of comments in source code most of which e.g.
todo license and exception are not appropriate to serve as queries.howev er tothebestofourknowledge thecommentsin codesearchdatasetshaveneverbeenfullycleaned.forexample barone et al.
remove empty or non alphanumeric lines from the docstrings.
codesearchnet filters each comment code pair with its comment length.
ling et al.
use heuristic rules e.g.
theexistenceofverbandnounphrases tofiltercomments.cambronero et al.
filterout queries that containspecific keywords.
these simple and scattered efforts are not enough to filter out the various noises especially the texts that are semantically unrelated to real queries.
liu et al.
also mention that improving the data quality is still a research opportunity for deep learning based code search models which well motivates our work.
there are two evaluation methods for neural code search research train test split and actual user query evaluation.
a lot of works split their datasets into train and test sets.
the queries of their test set contain the same defects as the train set so thattheresultsfailtoreflectthemodelperformanceinanactual environment.manuallyreviewedqueries canovercome thisproblembuttheyareusuallyonasmallscaleandcannotserve as the training dataset.unsupervisedanomalydetection commentscleaningisanapplication of the unsupervised anomaly detection algorithm as labeledcommentsarenon trivialtoobtain.unsupervisedanomaly detection algorithms identify the outliers solely based on the in trinsic properties of the data instances.
various techniques canbe applied such as principal component analysis generative adversarial network spatio temporal networks and lstm .amongthem auto encoder ae isthefundamental architecture for unsupervised anomaly detection .
it has been applied in many tasks.
for example zhang et al.
detect therumorsinsocialmediausingmulti layerae.castellinietal.
applyaetodetectfalsefollowersontwitter.luoandnagarajan useaetoidentifytheerroreventsofinterestsuchasequipment faults and undiscovered phenomena in wireless sensor networks.
the encoder of ae maps an input to a point in the latent space while vae maps an input to a region.
in this way vae can extract moreabstractsemanticfeatures.ithasbeenappliedtounsupervisedanomalydetectionwithpromisingevaluationscores .
conclusion weproposethefirstdatacleaningframeworkforcodesearchtasks which improves the quality and naturalness of the queries.
the framework leverages two subsequent filters the rule based syntactic filter and the model based semantic filter.
the rule based filteruses configurable heuristics rules to filter out comments with syntactic anomalies.
the model based filter aims to refine the dataset semantically.
it trainsa vae model over apre collected bootstrap querycorpus andexploitsittoselectcommentswithsmallerreconstructionlosses.experimentsshowthatourfilteringframework can significantly save computing resources and improve the model accuracy.
finally we release our framework as a python library nlqfand make public a high quality cleaned code search dataset cofic to facilitate relevant research in academia and industry.