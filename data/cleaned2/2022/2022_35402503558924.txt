mpbp verifyingrobustness ofneural networks with multi path bound propagation ye zheng zhengyeah foxmail.com shenzhenuniversity shenzhen chinajiaxiang liu jiaxiang0924 gmail.com shenzhenuniversity shenzhen chinaxiaomu shi xshi0811 gmail.com shenzhenuniversity shenzhen china abstract robustness of neural networks need be guaranteed in many safetycriticalscenarios suchasautonomousdrivingandcyber physical controlling.
in this paper we present mpbp a tool for verifying the robustness of neural networks.
mpbpis inspired by classical bound propagation methods for neural network verification and aims to improve the effectiveness by exploiting the notion ofpropagationpaths.specifically mpbpextends classicalbound propagation methods including forward bound propagation backward bound propagation and forward backward bound propagation with multiple propagation paths.
mpbpis based on the widely usedpytorchmachinelearningframework henceproviding efficient parallel verification on gpus and user friendly usage.
we evaluate mpbpon neural networks trained on standard datasets mnist cifar and tiny imagenet.
the results demonstrate the effectiveness advantage of mpbpbeyond two state ofthe art bound propagation tools lirpa and gpupoly with comparableefficiencytolirpaandsignificantlyhigherefficiencythan gpupoly.
a video demonstration that showcases the main features ofmpbpcan be found at .
source code is available at and .
ccsconcepts software and its engineering formal methods formal softwareverification .
keywords formalverification neuralnetworks boundpropagation acm reference format yezheng jiaxiangliu andxiaomushi.
.
mpbp verifyingrobustnessof neural networks with multi path bound propagation.
in proceedings of the 30th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november 14 singapore singapore.
acm new york ny usa 5pages.https corresponding author esec fse november 14 18 singapore singapore copyright heldby theowner author s .
acm isbn .
introduction deep neural networks dnns have been increasingly deployed in manyrealisticscenarios .nevertheless thesafetyof neural networks is difficult to be guaranteed.
especially there are many signs that dnns are not robust .
that is the output of a dnn may be unstable against small perturbations to the input.
for instance some dust on a stop traffic sign that is negligible for humans maylead to amisclassificationof the signinto an 80 km h trafficsignbyanauto drivingsystem whichisintolerant.
the lack of robustness hinders the deployment of dnns in such safety criticalscenarios.
atypicalrobustnessevaluationtechniqueistesting whichchecks whetheraperturbedinputinducesanincorrectoutput.alargebody of techniques are developed to test the robustness of dnns .
theyareeffectivetofindadversarialexamplesviolatingrobustness butcannotformally guaranteetherobustness.ontheotherhand formalverification e.g.
isabletoformally provetherobustnessofagivendnnagainstperturbedinputs thus providingaformalguarantee for safety criticalapplications.
bound propagation plays an important role in dnn verification.
it is a method for calculating provable lower and upper bounds of each neuron in a dnn according to the input region generatedbyperturbations.theprovableboundsofoutputneurons arethenusedtodeterminewhetheramisclassificationmayexist.if no therobustnessw.r.t.theinputregionisguaranteed.duetoits effectivenessandefficiency boundpropagationisoftenusedasa stand alone verification method but also can be utilized as an essential component in other verification techniques such as the branch and bound bab method .
it is the mainstream choice for bounding in bab methods which can be usedtodeterminethe nextbranching.
tightness is the most crucial measurement for the effectiveness of bound propagation methods.
calculating tighter bounds means thatamethodcandealwithmorerobustnessverificationproblems.
accordingtopropagationstrategies boundpropagation isfurther categorized into backward bound propagation bbp forward bound propagation fbp forward backward bound propagation fbbp andintervalboundpropagation ibp withdifferenttrade offsbetweentightnessandefficiency .
manyeffortshave beenmade to improve the tightness .
our recent work proposesthenotionof propagationpaths forbbp .withthisnotion existing bbpmethods canbe seen asemployingonly one singlepropagationpath whichis a specialcase of multi path bbp.theexperimentalresultsin showthatleveragingmultiple pathsinbbptightenstheoutputboundseffectively.furthermore the notion ofpropagation paths is suitable for parallelization.
thiswork islicensedunderacreativecommonsattribution4.0international license.
esec fse november14 18 singapore singapore ye zheng jiaxiang liu andxiaomu shi .pth .onnx modelsspecfication gid00049 gid00032 gid00045 gid00036 gid00414 gid00032 gid00031 reachable region gid00048 gid00041 gid00038 gid00041 gid00042 gid00050 gid00041 gid00020 gid00021 gid00016 gid00017 gid00429 gid00421 gid00020 gid00021 gid00016 gid00017 g3 g3 g3 g71 g14 0x gid00014 gid00043 gid00007 gid00003 gid00017 gid00014 gid00043 gid00003 gid00003 gid00017 gid00014 gid00043 gid00007 gid00003 gid00003 gid00017 gid00010 gid00003 gid00017 pytorch frameworkconstraint generatorverification methodsm g3465bp path configuration figure architecture andworkflow of mpbp inthispaper wepresentadnnverificationtool mpbp whichextendstheideaofmulti pathbbpto multi pathboundpropagation.
mpbpis implemented based on the widely used machine learning frameworkpytorch .itprovidesefficientparallelverification especiallyinthepresenceofgpus.specifically thetwomainfeatures ofmpbpare as follows it is a multi path bound propagation tool.
mpbpsupports fourboundpropagationtechniques multi pathbbp multipath fbp multi path fbbp and classical ibp.
they offer varioustrade offs betweenverificationeffectivenessandefficiency whichcanaccommodatedifferentapplicationscenarios.
it is a pytorch based verification tool.
with efficient parallel tensorcomputationsprovidedbypytorch boundpropagation through multiple paths spends almost the same time as thesingle pathboundpropagation.ontheotherhand for the users who are already familiar with pytorch we believe thatmpbpprovideseffortless andmore flexibleusage.
the mpbptool thearchitectureandworkflowof mpbparedepictedinfigure .
mpbpacceptsasinputsadnnmodel aspecification andapath configuration then outputswhetherthegiven modelsatisfiesthe expected specification verified orunknown .
the core of mpbp consists of four modules mpfbp mpbbp mpfbbpandibp which calculate the output bounds of the given dnn via respectively multi pathfbp multi pathbbp multi pathfbbp andtheclassical ibp techniques.
these modules inherit the different trade offs from theircorrespondingtechniques.userscanchooseoneofthemto fittheirrequirements.theoutputboundscomputedbythespecified module constitute the output reachable region.
the reachable region is then compared against the unsafe region the red square in figure which is constructed by the constraint generator with the input specification.
according to the comparison mpbpoutputs an answer verified indicating that the input specification e.g.
robustness issatisfiedbythe inputdnn or unknown .
.
input andoutput mpbpsupportsfeedforwardfully connectedneuralnetworks ffnns and convolutional neural networks cnns with the most commonlyusedreluactivationfunction.thenetworkcanbespecified in the pytorch model format .pthor the.onnxformat.
the input specification identifies the properties that users want to verify.
itconsists of an input x0to the dnn a perturbation threshold and a set of safe unsafe labels.
it specifies that all perturbed inputs which are in the neighborhood of x0w.r.t.lp norm distance are classified into safe labels equivalently not classified into unsafe labels bythednn.forexampleinfigure ifwewanttoverify whethera stop signimagepreservestheprediction stop even withperturbations under threshold we use theimageas x0and set the stop label as safe label hence others are unsafe .
mpbp supportsthree typesof lp normsto measurethedistancefor perturbation l l1 andl2 norms.finally inthe pathconfiguration userscanspecifythenumberofpropagationpathsemployedfor verification.morepathsbringtighterbounds.buttheimprovement is not permanent.
we suggest a default path number which is empirically chosenthroughsomepreliminary experiments.
afterboundcalculationwiththespecifiedmodule mpbpoutputs the reachable region of the given model w.r.t.
x0and .
meanwhile it also returns the verified unknown answer indicating whether theinput modelsatisfies theinput specification.for the unknown answer users may try to change the configurations path and or verification technique to perform amore accuratecalculation.
besidestheaforementionedinputs mpbpprovidesoptions bp toselecttheboundpropagationtechnique and verbose tochoose between differentkindsof outputinformation.
.
multi path boundpropagation indnnverification thegiveninput x0andtheperturbationthreshold constitutetheinputregion usuallyrepresentedbyintervals.
therefore thevalueofeachneuroninthenetworkrangeswithinan interval instead of being a single value.
a comprehensive introductiontoboundpropagationcanbefoundin .generallyspeaking backwardboundpropagationcomputestheneuronboundsviaa depth 2nestingloop.theouterloopiteratesthroughallneurons from the input to the output layer by layer.
for each neuron the innerloopcalculatesitslowerandupperboundfunctions byiterating conversely towardtheinputlayerusingallneuronboundsthat arealreadycomputed.thedirectionoftheinnerloopdefinesthe notion backward propagation .
the two bound functions are then concretized by the input intervals to obtain the concrete bounds of the neuron.
these bounds are again leveraged to compute the bound functionsfor neuronsinthe nextlayers inthe network.
classical bbp tools like eran andcrown maintain onlyasinglepairoflowerandupperboundfunctionsforeachneuron.thenotionofpropagationpathsregardsthespecifictechnique 1693mpbp verifying robustness of neural networkswithmulti pathbound propagation esec fse november14 18 singapore singapore forcomputingtheboundfunctionsasaspecificpath .itisthen natural to synergize different techniques to achieve better bounds.
multi pathbbpextendsclassicalbbpwithmultiplepropagation paths to obtain multiple pairs of lower and upper bound functions for each neuron.
multiple concrete bounds for a neuron can be derived from these bound functions.
since they are all provable bounds their intersection provides a tighter or at least identical provablebound.
the possibly betterboundwill benefit thebound computationfor neuronsinnextlayers.
figure2exemplifies two path bbp on a simple dnn.
assume thatallneuronsinthenetworkhavethebias .consideringtheneuronxi twolowerboundfunctions l1 x0 x0 andl2 x0 x0 areobtainedfromtwoover approximationtechniquesforbound propagation xi xi 1andxi respectively.
these two techniques constitute two propagation paths.
by concretizing l1 x0 x0 andl2 x0 x0 with input intervals we get two concrete lower bounds lb1andlb2ofxi .
takinglb max lb1 lb2 guarantees apossiblytighterlower bound of xi .
g16 g16111 1ix g16 2ix g16 2ix 1ix1 1ix g14 1relu ix1 10ix g14 g1161 10ix g14 g1161 1i ix x g14 g1161 1i i ix x x g14 g16 g16 g116 g16 path path 1x 2x1 il x x x g14 g116 il x x x g14 g116 figure anexample oftwo path bbpforneuron xi although the notion of propagation paths is proposed when considering bbp it is applicable as well when discussing other bound propagationmethodslike fbpandfbbp.
forwardboundpropagationisanalternativetobbp.unlikebbp fbpcalculatestheneuronboundsfromtheinputtotheoutputlayer by layer without performing backward propagation i.e.
the inner loop .
therefore it is extremely efficient with time complexity o n wherenis the number of neurons in the dnn.
as a consequence thebounds obtained by fbp are looserthan those by bbp.
comparedtobbp fbpsacrificestightnessforefficiency.butitis an important alternative in the applications like bab.
in the bab method boundpropagationisoftenthechoicehelpingtodecide the next branching.
it requires the bound propagation technique to be fast enough with acceptable tightness making fbp an excellent choice.
like bbp existing fbp techniques employ only a single propagationpath.weextendtheclassicalfbptomulti pathfbpin mpbpusing multiple propagation paths.
it guarantees tighter at leastidentical bounds thanthe classicalsingle path fbp.
forward backwardboundpropagationisacompromisebetween fbpandbbp.itcomputestheneuronbounds layerbylayer without backward propagation like fbp until the last layer but then performs backward propagation for the neurons in the last layer to obtain their bound functions.
fbbp bridges the tightness gap and efficiency gap between fbp and bbp.
we extend it to multipath fbbp which also bridges the tightness gap and efficiency gap between multi path fbpandmulti path bbp.note that in figure besides xi xi 1andxi it holds that xi xi 1for any .
we take the following strategy to prescribe the propagation paths in mpbp.
the first propagation path utilizes the heuristics in adopting a strategy to minimize local over approximation errors.
when the path number m besides the first path we take 0when computing bounds as the second propagation path.
for m the third propagation path takes .
whenm along with the abovefirstthreepaths wefurtheraddthe m sectionpoints of the interval as the s for the other m propagation paths.
for instance 4 2for the 4th path when m 4 and 5 3for the 4th and5th pathsrespectivelywhen m .
the computations by different propagation paths are easy to parallelize.theoretically themulti pathboundpropagationtechniques can improve tightness without sacrificing efficiency via parallelization comparedtotheirclassicalsingle pathcounterparts.
we implement the multi path bound propagation techniques in pytorch.byplacingdifferentpathsintoanewdimensionoftensors it enables the same time cost as the single path bound propagation.
experiments we evaluate mpbpagainst two state of the art bound propagation tools lirpa and gpupoly .
lirpa is the essential component performing bound propagation in crown the winner of the 2nd international verification of neural networks competition .
gpupoly is the gpu implementation of the main boundpropagationenginein eran anactivelyupdatedverificationtool.theevaluationisconductedinthreednns i anffnn trained by the mnist dataset which contains neurons ii acnntrainedbythecifar 10dataset withabout .
neurons and iii a cnn trained by the tiny imagenet dataset consisting ofabout .
105neurons whichis thelargestamong thethree.allexperimentsarerunonalinuxserverwithtwointelxeongold6132cpus anvidiateslav100gpuand256gb memory.
mpbpsetsthepathnumberto4 whiletheperturbation thresholds are allmeasuredw.r.t.
l norm.
effectiveness.
to evaluate effectiveness we compare mpbpwith lirpa andgpupoly on all three dnns with different bound propagation techniques.
the task is to verify whether the first correctly classifiedimagesineachdatasetarerobustagainstvarious perturbation thresholds.
the results are depicted in table .
it shows the numbers of robustness problems successfully verified bythetools.alargernumberindicatesthatthecorrespondingtool is more effective.
overall it is easily observed that with the same boundpropagationtechnique ourtool mpbpismoreeffectivethanthe other two toolsonall networksagainst every perturbationthreshold.
onthemnistffnn wecomparethemulti pathfbpandmultipath fbbp techniques in mpbpwith the classical fbp and fbbp techniquesinlirpa respectively.gpupolyisnotinthecomparison sinceitdoesnotsupportfbporfbbp.with multi path fbp we can see that mpbpverifies more problems than lirpa for each perturbationthresholdintable .thebestimprovementappears when .
mpbpsuccessfullyverifying7 more out of100 problemsthanlirpa.usingthemoreeffectivetechnique fbbp both tools are able to verify more problems.
still mpbp outperforms lirpa byleveragingmultiple pathsinfbbp.
1694esec fse november14 18 singapore singapore ye zheng jiaxiang liu andxiaomu shi table effectiveness evaluation numbers of verified problems are shown.larger numbermeansmore effective.
tools modelsandperturbationthresholds mnist ffnn .
.
.
.
fbpmpbp lirpa fbbpmpbp lirpa cifar cnn tiny imgnet cnn .
.
.
.
bbpmpbp lirpa gpupoly forthetightesttechniquebbp wecompare mpbpwithlirpa andgpupolyonthetwolargecnns.onthecifar 10cnn table demonstrates that gpupoly achieves the same results as lirpa andmpbpverifies more problems and respectively for both perturbation thresholds.
as for the largest network tiny imagenet cnn the inthetableindicatesthatgpupolycurrentlydoesnot supportthetinyimagenetdataset.lessproblemsareverifiedby bothmpbpandlirpaforthislargestnetwork.notwithstanding mpbprevealstheadvantagesbeyondlirpa verifying2and3more problems respectivelyfor the twoperturbation thresholds.
in summary the evaluation demonstrates that mpbpis more effectivethanlirpaandgpupoly thankstothemulti pathbound propagationtechniques.
efficiency.
toevaluateefficiency wechoosethemnistffnn andtheperturbationthreshold .
andtrytoverifythesame 100robustnessproblemsasabove.wecomparetheverificationtime spent by all three tools configured with each supported technique.
wedidnotevaluateonthetwocnnsbecausefbpandfbbpcan hardlyverifyany problems for theselarge networks.
figure3illustrates the verification time by each tool with differenttechniquestoverifyallthe100robustnessproblems.thex axis listsallthetoolswithdifferentconfiguredtechniques whichare groupedw.r.t.thetypesofboundpropagation drawnindifferent colors .
the y axis refers to the verification time in seconds spent by the tools.
the results demonstrate that each configuration of mpbpis slightly slower than the corresponding configuration of lirpa but the difference is negligible.
using bbp mpbpis even much faster than gpupoly which is also accelerated by gpus.
on the other hand we can see from the figure that multipath fbbp does bridge the efficiency gap between multi path fbp andmulti path bbp.
in summary the evaluation shows that mpbpis much more efficient thangpupoly whilecomparable to lirpa.
related work a growing attention has been paid to dnn verification.
earlier worksreducednnverificationproblemstoconstraintsolvingproblems e.g.smtproblems andlp milpproblems .
.
.
.
.
.
.
.
.
.
.
m g3465bp gid00014 gid00043 gid00007 gid00003 gid00017 lirpa fbp lirpa fbbp m g3465bp gid00014 gid00043 gid00007 gid00003 gid00003 gid00017 lirpa bbp m g3465bp gid00014 gid00043 gid00003 gid00003 gid00017 gpupolytime s fbp fbbp bbp figure efficiency comparisonofverification time thesemethodsaremostlysound i.e.nofalsenegative andcomplete i.e.
nofalsepositive .buttheysufferfromlimitedscalability due to the computational complexity.
bound propagation is a lightweight method in terms of complexity.
it is sound but incomplete because of the over approximation techniques that have beenused .itturnsouttobeeffectiveandefficient thus applicabletolarge scalenetworks.ourworkaimstoimprovethe effectivenessofboundpropagationwithoutsacrificingitsefficiency.
many efforts have been made to improve the tightness hence effectiveness of bound propagation.
a line of work focuses on designingnovelover approximationtechniquestobalancetightness with efficiency e.g.
.
with the notion of propagation paths theseworksdesignbetterpropagation paths whichcanbe easilyintegratedin mpbp.ontheotherhand crown leverages optimization techniques to achieve better over approximation for neurons in the dnns while the authors in and exploit lp or milp solving to calculate tighter bounds of neurons for propagation.
these works introduce constraint solving into bound propagation makingitmoreeffectivebutconsiderablylessefficient.
instead mpbpimprovestheeffectivenesswithmerelynegligible extratimecost.anotherimportantmethodtotightenthebounds is the bab method where bound propagation is usuallychosen as the bounding component .mpbpis orthogonal to bab methods andcan be usedinbabfor boundingas well.
conclusion thispaperpresents mpbp adnnverificationtoolthatleverages multi pathboundpropagationtechniques.
mpbpshowsitseffectivenessadvantagesovertwostate of the artboundpropagation tools withcompetitive efficiency.basedonpytorch mpbpoffers friendlyandflexibleusage.moreover modeltrainingandtesting canalso beconducted within mpbp.the training verifyingworkflowissupportedas well.