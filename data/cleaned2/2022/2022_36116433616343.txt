deepinfer deep typeinference from smartcontract bytecode kunsongzhao the hong kong polytechnic university china kunsong.zhao connect.polyu.hkzihaoli the hong kong polytechnic university china cszhli comp.polyu.edu.hkjianfeng li xi anjiaotong university china j f li.xjtu gmail.com he ye kthroyalinstitute of technology sweden heye kth.sexiapuluo the hong kong polytechnic university china csxluo comp.polyu.edu.hktingchen universityof electronic scienceand technology of china china brokendragon uestc.edu.cn abstract smartcontractsplayanincreasinglyimportantroleinethereum platform.
it provides various functions implementing numerous services whosebytecoderunsonethereumvirtualmachine.to use services by invoking corresponding functions the callers need to know the function signatures.
moreover such signatures providecrucial informationformany downstream applications e.g.
identifyingsmartcontracts fuzzing detectingvulnerabilities etc.
however it is challenging to infer function signatures from the bytecode due to a lack of type information.
existing work solving this problem depended heavily on limited databases or hardcoded heuristic patterns.
however these approaches are hard to be adaptedtosemanticdifferencesindistinctlanguagesandvarious compilerversionswhendevelopingsmartcontracts.inthispaper weproposeanovelframework deepinfer that f irstleveragesdeep learning techniques to automatically infer function signatures and returns.thenoveltiesof deepinfer are deepinfer liftsthe bytecode into the intermediate representation ir to preserve code semantics deepinfer extractsthetype relatedknowledge e.g.
critical data f lows constant values and control f low graphs from theirtorecoverfunctionsignaturesandreturns.weconductexperiments on solidity and vyper smart contracts and the results show that deepinfer performs faster and more accurate than existing tools while being immune to changes in different languages andvariouscompilerversions.
ccs concepts securityandprivacy software reverseengineering .
keywords smartcontract type inference deeplearning corresponding authors.
permissionto make digitalor hard copies of allorpart ofthis work for personalor classroom use is granted without fee provided that copies are not made or distributed forpro f itorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation onthe f irstpage.copyrights forcomponentsofthisworkownedbyothersthanthe author s mustbehonored.abstractingwithcreditispermitted.tocopyotherwise or republish topostonserversortoredistributetolists requirespriorspeci f icpermission and or a fee.
request permissions from permissions acm.org.
esec fse december san francisco ca usa copyright heldby the owner author s .
publicationrightslicensed to acm.
acm isbn ... .
format kunsong zhao zihao li jianfeng li he ye xiapu luo and ting chen.
.deepinfer deep type inference from smart contract bytecode.
in proceedings of the 31st acm joint european software engineering conference andsymposiumonthefoundationsofsoftwareengineering esec fse december san francisco ca usa.
acm new york ny usa 13pages.
introduction cryptocurrencieshaveshownaprevalenttrendinbothindustry andacademiainrecentyears.withtheemergenceofethereum oneofthelargestdecentralizedplatform programmablecryptocurrency services enter a new era .
smart contracts as the key component of ethereum enable developers and users releasecryptocurrencies deployapplications andutilizeservices ontheethereumblockchainwithouttheinterventionofthetrusted third parties .
a smart contract is implemented by high levellanguages e.g.
solidity and vyper thencompiled into the bytecode executed on ethereum virtual machine evm and f inallydeployedonethereum.thefunctionsofasmart contractwillberegisteredastheapplicationbinaryinterface abi making others easily invoke and implement their functionalities .
table an overview of existing studies for recovering function signaturesandreturnsinsmart contracts.
approachscalability accuracy techniqueslanguages compilers signature return ebd fsd eveem fsd sa gigahorse sa sigrec sa deepinfer dl fullpartial nosupport fsd ethereumfunction signaturedatabase sa staticanalysis dl deep learning theabiconsistsoffunctionsignaturesandreturns.wheninvoking a function users need to know the signatures because it de f ines the calling rules .
function signatures comprise a function id and a list of parameter types in which function id is derived from the f irst bytes of the keccak hash result of a function name and the corresponding list of parameter types in source code esec fse december3 san francisco ca usa kunsongzhao zihaoli jianfengli he ye xiapu luo andting chen .
moreover the function returns specify the format of return values of the function .
identifying function signatures and returnsisacrucialsteptoanalyzethebehaviorofafunctioninsmart contracts because one needs to f irst know how a function is called andwhat isreturnedbefore the evaluation.
for example previous studies adopted function signatures to recognize different types of smartcontracts andutilized functionarguments togeneratemoremutanttestcasestofacilitatethefuzzingtoolsfor detecting more vulnerabilities .
moreover checking returnsofafunctioncanavoidvulnerabilities suchastheunchecked callreturn valueweakness .
problem function signatures and returns are black boxes for users.
this is because nearly of the deployed contracts are closed source .
the arguments of function signatures and returns arerepresented asthe256 bit wordswithoutthe type informationand debuginformation inthebytecode whichmakes it hard to be recovered .
as shown in table some studies contributetorecoveringfunctionsignaturesinsmartcontracts.for example evm bytecode decompiler ebd searches function signaturesfromtheirethereumfunctionsignaturedatabase fsd .
eveem andgigahorse relyonthehard codedheuristicsto restorefunctionsignaturesinwhicheveemalsoincorporatesthe knowledge from the fsd.
chen et al.
proposed a static analysisbased tool sigrec that manually designed heuristic patterns accordingtotheaccessrulesfordifferentparametertypesinthe evmandemployedthesymbolicexecutiontechniquetorecover function signatures from the bytecode ofasmart contract.
unfortunately theseapproachesstillsufferfromthefollowing limitations.
limitation l1 the existing methods for the function signature recoveryheavilydepend oneither anincomplete database involvingafractionoffunctionsinthewild orrestrictedmatching patterns designed by human experts.
the limited database is hard to cover all the functions on ethereum where as the f ixed matching patterns will be invalid when the smart contracts are developedbynewprogramminglanguageswiththeevolutionof the ethereumecosystems.
limitation2 l2 despitetheexistingtooldesigningasetof arguments access rules that has shown the ability to recovering types of arguments its accuracy is still less than satisfactory especiallywhentheaccesspatternsencounterevenaslightchangedue to the compilerversionupgrades.
limitation l3 all the existing studies merely focus on recovering function signatures but the inference of the returns of a functionis ignored.
since theaim of smart contractsis to execute transactionsonethereumplatform boththesignaturesandreturns of a contract function are indispensable components for analyzing thefunctionalityandcheckingwhethersomedesiredoperations are performedcorrectly .
with deep learning techniques achieving promising results over traditionalpattern basedmethodsinmanysoftwareengineering tasks one can leverage these data driven techniques to learnimplicitknowledgetoinferfunctionsignaturesandreturns.
however it is challenging to design deep learning based inference models for this purpose.
challenge c1 the inference model needs to automatically learn how different types of arguments are operated in the evmbytecoderatherthanponderouslydependsonthefunctiondatabase or manually extracts a limited set of access patterns focusing on speci f ic languages.
this requires that the model actually grasps the differences in different types of arguments which are languageindependent.
challenge c2 as the compiler version upgrades the patternstoaccesstheargumentswillbechanged.thisrequiresthat themodelisscalable i.e.
itneedstobefreefromthecharacteristics ofvariouscompilerversions.
challenge3 c3 forfunctionsignatureinference wecanforce the model to understand the access patterns of different types of argumentsand thenusesuch knowledgetopredictthesignatures of other functions from a prepared type list.
unfortunately it is infeasibleforinferringsomecomplextypes e.g.
array becausewe cannot determine how the number of nesting layers or dimensions thereshouldbeandrestrictittoalimitedsetoftypesinadvance.
thatis itwillsufferfromanout of vocabulary oov issue .
this requiresthe modelto decidewhichtypes needto beinferred from a f inite set of types and which types need to dynamically determinethe nestingdepthordimensionsduringthe inference.
challenge c4 recovering returns is more difficult than inferring signatures because the return values are stored in the memoryandreturnedattheendoffunctionexecution .this requires the model to be able to understand the semantic of the whole function rather than a speci f ic part.
despite there also exist some studies for function signature inference in other scenarios noneofthemsupportstypeinferencefromthebytecode ofsmart contracts because they target the sourcecode.
in this paper we present deepinfer a novel deep learning based frameworktoautomaticallyrecoverfunctionsignaturesandreturns fromtheevmbytecodewithoutanyhumanintervention.tomake the model deal with various languages deepinfer f irst lifts the bytecodecompiledfromdifferentlanguagesintotheirinwhichthe language speci f icandcompiler speci f icoperationsarestripped c1 c2 .then itconductsade f inition useanalysistoextractcritical information e.g.
data f lowsandconstantvalues thatarehighly relevantfortheinferenceofsignaturesfromtheir .
.tomake themodelhavethepotentialtorecovervarioustypesofarguments we design a two stage inference framework in which the basic and complex types are handled individually .
.
speci f ically deepinfer recognizes the basic and complex types according to the knowledgeextractedandlearnedfromtheir.forthebasictypes deepinfer recoverstheactualtypebyselectingtheonewhohasthe maximum probability among thelist ofprede f ined types collected from the official documentations because of the limited number of types .
.sincethecomplextypescannotberestrictedina f inite set due to the above mentioned oov issue deepinfer employs a sequencegenerationmodeltodynamicallygeneratethepossible structure of such types according to the knowledge learnt from thecriticalinformation .
c3 .torecoverreturns deepinfer constructs the control f low graph cfg that captures the function semanticbymeansofthestructuredgraphrepresentationfromthe ir andemploysthegraphneural networktoexcavatethe implicit semanticknowledge aimingatpromotingthemodeltounderstand the functionality .
c4 .
weconductexperimentsonopen sourcesmartcontractswritten bysolidity and vyper.
we collectunique functionsfrom these 746deepinfer deep type inference from smartcontract bytecode esec fse december3 san francisco ca usa open sourcesmartcontractsandusethemtoevaluatetheaccuracy ofdeepinfer .theexperimentalresultsshowthat overall deepinfer obtainsthetop 5accuracyof0.980and0.937forsignatureinference inbothsolidityandvypersmartcontracts respectively.compared with the baselines in table deepinfer obtains an average accuracy improvement by .
for recovering function signatures.
moreover deepinfer achieves the top accuracy of .
and .
for recovering function returns respectively.
further experiments show that deepinfer performs more than times faster than baseline methods.
meanwhile deepinfer is not affected by different languagesandvariouscompilerversions.
insummary thispapermakesthefollowingmajorcontributions wedevelop deepinfer anovelframeworkthatextractsandlearns functionaccess relatedinformationfromtheliftedthree address code to recover function signatures.
moreover deepinfer is able to understandcode semantics for recoveringfunction returns.
we conduct comprehensive experiments on real world smart contractswrittenbysolidityandvyper andevaluatetheaccuracy ofdeepinfer .
the results show that deepinfer obtains an average accuracy improvement by .
compared to the existing tools forrecoveringfunctionsignatures.
deepinfer alsohastheunique potentialtorecoverfunction returnsand isimmunetochanges indifferentlanguagesandvariouscompilerversions.
deepinfer isthe f irstworkonrecoveringfunctionsignaturesand returnsfromevmbytecodebasedondeeplearninginference to thebestofourknowledge.onthecontrarytothestate of the art work ourprocessisdoneinafullyautomaticmannerwithout any human intervention.
background .
ethereum smartcontracts ethereum isasecond generationblockchain basedplatform thatprovidesmore f lexibledistributedcomputingabilitiesbyincorporatingsmartcontracts .therearetwotypesofaccountsin ethereum blockchain external owned account eoa that can be treatedasawalletkeepingassets e.g.
ether andsmartcontract whichiscreatedbyeithereoa orothersmart contracts .
smart contracts are executable programs that can run on the ethereum blockchain which implement various functionalities satisfying the requirements of end users.
it can be executed in a completely decentralized manner and does not depend on any trusted third parties .
smart contracts are written by high level programming languages e.g.
solidity and vyper and compiled intothe bytecode executingonthe evm.
.
functioninvocation in evm theeoaandcoaaccountscancallasmartcontractfunctionby sendinganinvocationmessagethatconsistsoftheaddressofthe smart contract where the invoked function is located and a special f iledcalldatawhichcarriesinformationaboutthefunctioninvoked and actual arguments .
thecall data f ield appears as a sequence ofbyteswhose f irst4bytesrefertothefunctionidandtherestis thearguments .for example to invoke the functionshown in listing1 thecall data f ield starts from the corresponding function id0xea7cabdd followedbytwospeci f ictypesofarguments tokenid aone dimensional dynamicarray and owner an address .there are two instructions read arguments from the call data f ieldto evm including calldataload andcalldatacopy .calldataload f irstloadsthetopelementinthestackofevmandusesitas the offset to locate data in the call data f ield.
it then loads bytes data starting from the offset from the call data .
eventually the loadeddataisputintothestackofevm.differentfrom calldataloadthatreadsa f ixed sizedataintothestack calldatacopy loads avariable sizedatafrom calldatatothememoryofevm .it f irstloadsthreeelementsfromthetopstack includingthememory locationforstoringthedata thelocationof calldataforloadingthe data andthelengthofdatatobeloaded.therearemanysupported typesinsolidityandvyperintheirofficialdocumentations whichcanbetreatedasbasictypesandcomplextypesaccordingto whetheronetypeisenumerable.thebasictypesincludeaddress string struct bool bytes u int u1d440 where u1d440 ... and bytes u1d441 where u1d441 ... .
the complex types contain variousarraysthatarederivedfromthebasictypesexceptforstruct butcanhavevariablenestingdepthandsizes suchasone multidimensional array with compile time f ixed or dynamic sizes and thenestedarray .readerscanseemoreaccessdifferences ofthesetypes inthe literature .
1function checkallowner uint256 tokenid address owner public view returns bool ...... 3for uinti i tokenid.
length i if owner !
zombietoowner return false 8return true listing a function sample with two arguments and one return whose function id is 0xea7cabdd .
.
motivating example listing1illustratesanexampleofafunctiondeployedinethereum.
the function consists of two arguments and one return value in whichthe f irstparameteristhedynamicarrayandthesecondoneis theaddress.thisfunctioncheckswhetheralltokensinthedynamic array tokenidbelongtoaparticular ownerandreturnstrueifso false otherwise.
there are different evm instructions to load these two typesofparameters.for thesecondparameter theevm f irst usesacalldataload instructionwhoseoperandisthestartpoint of this parameter in the call data f ield.
then it loads a byte data andusesan andinstructionformaskingtoa f ixed length i.e.
bytes .
the masked result is the actual parameter value that canbeusedbyfollowinginstructions.torecoverthetypeofthis parameter wecanmakethemodellearntheoperationinstructions involvedandconstantvaluesthatareusedformasking andthen selectthetypewiththemaximumprobabilityfroma f ixedtypelist because the basic types are limitedas mentionedabove.
however thetypeinference forthe f irst parameteris difficultbecause there is no dimension information of the array in the bytecode and thus is unknown during the compilation.
assume the actual parameters of tokenid in thecall data is and the arrangement of the data is displayed in fig.
.
the f irst bytes refer to the function id and the offsetrecords the start point of this array.
moreover numis the size of the actual elements in this array followed by their individual real data values i.e.
0xa 0xb and 0xc .
hence to access an array element e.g.
0xb the evm 747esec fse december3 san francisco ca usa kunsongzhao zihaoli jianfengli he ye xiapu luo andting chen bytes bytes bytes id offset num 0xa 0xb 0xc figure1 thedataarrangementofadynamicarrayuint256 .
uses two calldataload instructions to load the items offsetand numto identify the start point of this array followed by another calldataload instruction to obtain the actual value 0xb according to its address.
to infer such types of arguments deepinfer designs agenerativedeeplearningmodelwhichhasthepotentialtoautomatically generate nested structures by learning critical data f lows andinvolvedconstantvalues.thisisbecausetheformsofarrays arevarious suchasone multi dimensionalstatic dynamicarrays and nested arrays whose nested depths and dimensions cannot be limitedintoa f ixedset.
on the other hand recovering the type of the return value is difficult because they are stored in memory during evm running.
differentfromthetyperecoveryofsignaturesthatholdsexplicit operationinstructionstoloadandaccessthefunctionarguments return values are stored into or loaded from the memory.
this prevents the model from collecting obvious instruction operations.
to infer the return type we make the model to understand the functionality of the contract function and determine what is the type ofthe return value.
framework .
overview fig.2demonstratestheoverallframeworkof deepinfer thattakesas input the evm bytecode of smart contracts and f inally outputs the signatures and returns of each function in it.
speci f ically deepinfer f irst lifts the bytecode into ir in which the complex stack operationsarestrippedbuttheusefulinstructionoperationsarereserved in the form of readily comprehensible three address code.
then deepinfer recognizes all functions from the generated ir according to the function boundaries .
.
after that deepinfer extracts function access related information such as data f low features and constant values and constructs control f low graphs cfg .
this isbecausethefunctionaccess relatedinformationindicateshow each parameter is loaded and used in the evm for recovering function signatures whereas the cfg is related to understanding the functionalityforrecoveringfunctionreturns .
.basedonthese information deepinfer trains the deep learning models to recover thebasictypesofargumentsbybuildingtheclassi f icationmodel .
generatesthecomplextypesofargumentsbytrainingthe sequence generation model .
and infers the function returns byunderstanding the wholefunctionality .
.
.
lifting functionrecognition fortheinputevmbytecodeofasmartcontract deepinfer f irstuses gigahorse toparsethebytecodeintotheregister basedir i.e.
the three address code that consists of a clause opcode operand operand ... operand u1d45b result u1d45b in which the resultis the output of the instruction opcodewith the operands e.g.
operand .
ifavariableappearsin result weconsideritavariablede f initionoperation.moreover ifavariable appearsin operand we consider it is used.
note that each variable can be de f ined only once i.e.
it holds a unique value but can be used multiple times .
we use the ir rather than smart contract bytecode in the following steps ofdeepinfer because it simpli f ies stack operations to clauses whichmakesiteasytoanalyzeandextracttheaccesspatternsof function arguments.
more speci f ically deepinfer f irst recognizes the jump instructions to f ind the boundaries of basic blocks and then conducts the context sensitive and f low sensitive analysis to process the register based ir at the contract level.
besides it speculatesthe entrance and exitof each basic block to recognize the function boundaries and generates the register based ir at the function level.
each basic block is connected to one or more precursorandsuccessorbasicblocksexceptfortheentranceand exit.the precursorsrefer tothe ones thatmay be executed before the execution of a basic block.
the successors means the ones that may be executed after the execution of a basic block.
after generatingtheregister basedirforasmartcontract deepinfer uses aregularexpressiontoextractallthepublicfunctionsbymatching the functiondeclarations.next we introduce howdoes deepinfer extract criticalfeatures from such ir for modeltraining.
.
featureextraction after obtaining the three address code of each function deepinfer extractsfunctionaccess relatedfeaturesfromit.thekeyinsights herelieinthreeaspects.first inordertoinferthetypesofarguments deepinfer intensively learns how each parameter is loaded andusedbytheinstructions.thisisanecessarystepbecausedifferent types of parameters are operated by distinct opcodes.
for example two iszeroinstructionswill beusedwhen theloadedparameter is a boolwhereas only the instruction bytecan be used toassesseachbytedataforaparameterwiththetype byte32 .
thus deepinfer proactivelylearnssuchaccesspatternsautomaticallyaccordingtotheinstruction f lows.second sometypes e.g.
int arewithdifferentbitsizeswhicharerepresentedasthemask value de f ined by the constant values in bytecode.
for example the typeuint8requires bits of zeros to mask the left side of the data whereas another type uint248simply needs bit.
thus deepinferis carefully designed to collect such constant values from the de f initionofthem .
.
third unlike thearguments thatutilize different instructions to access distinct types of parameters the returnvaluesarestoredinthememoryandreturnedafterthefunctionisexecuted whichmeansthattherearenospeci f icinstruction operations.thus deepinfer needstounderstandthewholefunctionalitytoinferthereturnsratherthanlearntheaccesspatterns from the instruction f lows.
.
.
definition useanalysis.
theaimofde f inition useanalysisis totraceasetofrelationsbetweenvariablesinwhichonevariableis usedbyothers.asaforementioned aparameterisaccessedby usingeither calldataload orcalldatacopy instruction thus we do not need to analyze the de f initions and uses of all variables.
on the contrary we fully focus on analyzing the variables containing the de f initions or uses of each of the parameters in a function.
we canachievesuchvariableanalysisbyusingthede f inition use analysis based on the parsed ir.
thanks to such ir that assigns a unique variable name to represent the execution result of each 748deepinfer deep type inference from smartcontract bytecode esec fse december3 san francisco ca usa ...... begin block 0x14e ...... 0x14e v14e ... const ..... begin block 0x152 ...... 0x158 v158 calldataload v140 0x159 v159 ... const 0x16e v16e and v159 v158 ...... bytecode0x608080604052 1357600080fd5...lifting function recognition .
feature extraction .
signature returnfunction return recovery .
preprocessingfunction signature recovery .
.
recovery constant attentionsoftmax ed c1 c2 c3 ckec typesignature inferenceoutputcls ed c1 c2 c3 cks1out1 sos out2 eos s2 s3 stout3copygeneration encoder decoder...... ...input probability vocabulary probability ...signature generation ed hflm local weightsglobal encoder local encoder hijcdf embedding global weights cfg embeddingfunction returnssoftmax function return recoverycv embeddingmarked as array?
control flow graph recognition constant tracing definition use analysis figure the overallframeworkof deepinfer .
instruction deepinfer cansearchwhichvariablede f initionscontain the two instructions and treat these variables as the start points aimingatcollectingtheparameteraccess relatedpaths.sincethe clauses in the ir will use the assigned variables as the operands deepinfer can recursively look for the clauses whose operands depend on the variable de f initions of the two instructions.
as a result deepinfer canobtainasetofinstructionsequenceswhere each sequence starts from either calldataload orcalldatacopy instruction and we call each instruction sequence the critical data flow cdf inthe following.
.
.
constant tracing.
the goal of constant tracing is to collect a setofconstantvaluesthatarerelevanttotypesoffunctionarguments.
the analysis of de f initions and uses can generate a set of cdfstarting from calldataload orcalldatacopy .however only using cdfs to build the signature inference model is inadequate becausesometypesnotonlyusetype relatedaccessinstructions but depend on values to determine their sizes e.g.
uint8 or the dimensions e.g.
address .
such values are also de f ined in the ir with the keyword const.
one intuitive solution is to collect all the de f initions that contain const.
however not all the values de f ined byconstare related to the parameter access operations because some conditional jumps e.g.
jumpi or operations that consume constantvalues e.g.
calldataload canalsoinvolvesuchvalues.tosolvethisissue westartwitheachcdfandcollectthevariables thatappearintheoperands.foreachvariable werecursivelybacktrack to where it is de f ined until the initial clause whose opcodeis the keyword constis found.
thus we collect the corresponding operandinthisclauseastheconstantvalue cv .asaresult we canobtainasetofcvsforeachcdf.bytraversingallthecdfs we cancollectalltheconstantvaluesassociatedwiththearguments access operations.
.
.
control flow graph construction.
the above features can be used to determine the access patterns and the corresponding sizesordimensionswhilerecoveringfunctionsignatures.however while recovering function returns there is no related operation manifestationintheirbecauseallthereturnvaluesarestoredin the memory i.e.
there is a lack of explicit instructions for identifying speci f ic operations about types that contained in returns.
this requires f indingarepresentationthatnotonlycontainstheimplicit characteristicsofeachfunctionbutcanalsobelearnedbythedeep learning model.
as an alternative cfg abstractly represents the possible f lows of all basic blocks in a function using the structured graph representation.
it can re f lect how each statement is executed duringtheprogramrunning .besides thestructuredpresentation of a cfg also supports the use of the learnable model to dig for implicit features i.e.
its functionality .
these bene f its 749esec fse december3 san francisco ca usa kunsongzhao zihaoli jianfengli he ye xiapu luo andting chen encourage us to extract the cfg from the ir.
because the ir has beenclearlydividedintothebasicblocks .
wecanuseregular expressionmatchingtoretrievethesebasicblocksandtreateach basicblockasthenodeofacfg.besides wegeneratetheedgesofa cfgbymatchingtheprecursorsandsuccessorsofeachbasicblock.
as a result we can obtain the structured cfg for each function in whichnodesrepresentthebasicblocksandedgesrefertothejump relationships.
.
functionsignatureinference one aim of deepinfer is to recover function signatures by learning how different types of arguments are operated in the bytecode.
therearemanytypesofargumentsusedinthehigh levellanguages e.g.
solidityandvyper inwhichsomebasictypes e.g.
uint int string bool etc can be restricted into a f ix length set because they areenumerablebutothercomplextypes i.e.
array cannotbecause they can hold unlimited dimensions and arbitrary sizes .
thus wecannotsimplytreatthesignaturerecoveryasatraditional classi f icationtask.tosolvethisproblem weproposeatwo stage framework that determines different types using distinct strategies.
speci f ically we f irst mark types that are not enumerable with a generalsign i.e.
array andallthetypescanberestrictedintoa limited type list.
thus deepinfer trains a classi f ication model to determine the type of a parameter by selecting the one with the maximal prediction probability from this type list.
then for the typesthataremarkedasarray deepinfer trainsanothergenerative model to generate their actual types.
in this subsection we will introduce how to construct the classi f ication model to learn the parameteraccesspatternsandleavethedetailofconstructingthe generative modelinthe nextsubsection .
.
.
.
cdfembedding.
afterextractingasetofcdfsandcvsfrom theirofafunction deepinfer trainsamodeltolearnaccesspatterns ofdifferent typesofarguments.forthe collectedcdfs we retain theopcodesformodeltrainingbecauseopcodespreservethereal operationsduringthecodeexecution.sincetheobtainedopcodes arenotnumericandcannotbeusedastheinputoftyperecovery model deepinfer f irst trains a word2vec model based on these opcodes to produce the initial embedding vectors.
speci f ically it treatstheopcodesextractedfromacdfasasentenceandemploys the continuous bag of word cbow technique that predicts a word i.e.
opcode using its contextual words to generate an embedding mapping matrix w. sincetherearemultiplecdfsineachfunctionandallofthem together form the access patterns of arguments inspired by previous work deepinfer designs the hierarchical flow learning mechanism hflm to learn the access patterns among the opcodes in these cdfs.
speci f ically assume the set of opcode sequences extracted from the cdfs is u1d7031 u1d7032 ... u1d703 u1d45a where u1d703 u1d456 u1d703 u1d4561 u1d703 u1d4562 ... u1d703 u1d456 u1d45b is the u1d456 th sequence.
u1d45aand u1d45brepresent the numberofcdfs andthelengthofthecdf respectively.foreach opcode deepinfer f irst initializes it with the embedding matrix i.e.
u1d452 u1d456 u1d457 w u1d703 u1d456 u1d457.then foreach u1d703 u1d456 itusesthebi directionallstm toincorporatethecontextualoperationsfromtwodirectionsand produce the hiddenembedding vectors uni210e u1d456 u1d457 u1d43f u1d446 u1d447 u1d440 u1d452 u1d456 u1d457 u1d43f u1d446 u1d447 u1d440 u1d452 u1d456 u1d457 where uni210e u1d456 u1d457isthe hidden embeddingvector ofthe u1d457 thopcode in u1d703 u1d456 and refersto the concatenate operation.
local encoder .
after obtaining the vector of each opcode deepindernext generates the embedding vector of a cdf.
since there exist instructions that merely use the variable de f inedby previous instructions as the target address and the type related instructions will occur after such instructions simply summing or averaging operation over the opcodes will introduce some irrelevant noise information.tomakethemodelpaymoreattentiontotype related operations weemploytheattentionmechanismtocalculatelocal weights from these opcodes and aggregate them into the representation ofthe cdf u1d452 u1d456 summationdisplay.
u1d45e uni210e u1d456 u1d457 u1d452 u1d465 u1d45d parenleftbig u1d453 u1d459 parenleftbig uni210e u1d456 u1d457 parenrightbig parenrightbig summationdisplay.
u1d45e u1d452 u1d465 u1d45d parenleftbig u1d453 u1d459 parenleftbig uni210e u1d456 u1d457 parenrightbig parenrightbig where u1d452 u1d456istheembeddingvectorof u1d456 thcdf u1d452 u1d465 u1d45d referstothe exponential function u1d453 u1d459represents the linear layer followed by the relu activation function .
we call the above operations the local encoder as shown in fig.
because they focus on a single cdfinafunction.
globalencoder .accordingtotheaboveoperations wecanobtain the embedding vector for each cdf.
then we introduce how to generatetheoverallembeddingvectoratfunctionlevelbecauseour aimistorecoverthesignaturesforafunction.therearemultiple cdfscanbeextractedfromafunctionandtheyareaccessedinthe sameorderastheargumentsthatappearinthefunctiondeclaration.
thus we treat the cdfs as a sequence and use their embedding vectors i.e.
u1d452 u1d456 asthemodelinput.wealsousethebi directional lstm to updatethe embedding vectors of eachcdf u1d452 u1d456 u1d43f u1d446 u1d447 u1d440 u1d452 u1d456 u1d43f u1d446 u1d447 u1d440 u1d452 u1d456 where u1d452 u1d456is the hidden embedding vector of u1d456 th cdf generated by the model.
similarly notall cdfsplaythesameimportance inrecovering thetypeofoneparameterbecausetheevmwilluseoneormore different instructions to access distinct types of parameters.
hence deepinfer usestheattentionmechanismtocalculateglobalweights fromthe embeddingvectors ofall the cdfsandincorporate them into an overall representation forcing the model to concentrate on the parts of interested cdfs while recovering different types of parameters u1d438 u1d451 summationdisplay.
u1d45d u1d452 u1d456 u1d452 u1d465 u1d45d parenleftbig u1d453 u1d454 u1d452 u1d456 parenrightbig summationdisplay.
u1d45d u1d452 u1d465 u1d45d parenleftbig u1d453 u1d454 u1d452 u1d456 parenrightbig where u1d438 u1d451refers to the overall representation vectors of cdfs u1d453 u1d454 representsthelinearlayerfollowedbythereluactivationfunction.
by using theabove equations deepinfer can output the overall cdf embedding vector for each function.
next we will introduce howto dealwithconstant values.
.
.
cv embedding.
as for constant values since there is no orderrelationsbetweenthem deepinfer buildsalookuptable u1d44a u1d450 in which each constant is initialized randomly and updated during themodeltraining.assumeasetofconstantvaluescollectedinthe irofeachfunctionis u1d7191 u1d7192 ... u1d719 u1d458 inwhich u1d458isthenumber of unique constant values.
deepinfer embeds each constant into its initialembeddingvectorbyinquiringaboutthelookuptable i.e.
u1d450 u1d458 u1d44a u1d450 u1d719 u1d458.wewillexplainhowtheseconstantvaluesareaggregated 750deepinfer deep type inference from smartcontract bytecode esec fse december3 san francisco ca usa intothecdfembeddingvectorstoenhancethemodelinference abilitywhenbuildingtheclassi f icationmodelandthegenerative modelinthe following.
.
.
classificationmodel.
asmentionedabove wehaverestricted allthetypesintoalimitedlist.thus wecannaturallyregardthesignature inference as a classical multi classi f ication task.
speci f ically given the embedding vectors of cdfs u1d438 u1d451and cvs u1d4501 u1d4502 ... u1d450 u1d458 we expectthemodeltolearnnotonlytheaccessoperationsfromcdfs but also the possible sizes from cvs.
thus deepinfer f irst uses a linear function u1d453 u1d450to map each constant embedding u1d450 u1d456into the hiddenvector u1d450 u1d456andthenemploystheattentionmechanismtolearn topaymoreattentiontotheconstantvaluethatarerelevanttoa speci f ic type i.e.
constant attention u1d438 u1d450 summationdisplay.
u1d458 u1d452 u1d465 u1d45d u1d450 u1d456 summationdisplay.
u1d458 u1d452 u1d465 u1d45d u1d450 u1d456 u1d450 u1d456 where u1d438 u1d450istheoverallrepresentation ofthecvsineachfunction.
to learn the operation patterns and the constant information simultaneously deepinfer concatenatesthesetwoembeddingvectors to form the representation vector for eachfunction u1d438 u1d450 u1d459 u1d460 according to the representation u1d438 u1d450 u1d459 u1d460 deepinfer can infer the most possible type for the arguments by selecting the one that has the maximal prediction probability over the type list u1d45c u1d462 u1d461 u1d45d u1d462 u1d461 u1d450 u1d459 u1d460 u1d44e u1d45f u1d454 u1d45a u1d44e u1d465 u1d44a1 u1d438 u1d450 u1d459 u1d460 u1d44f1 where u1d45c u1d462 u1d461 u1d45d u1d462 u1d461 u1d450 u1d459 u1d460is the predicted type of a parameter u1d44a1and u1d44f1 represent the trainable weightmatrixandbias respectively.
recall that we mark all the types of arrays as the general signal array.ifdeepinfer predictsaparameterasthetypearray itsactual type still needs to be further determined.
we will introduce the details inthe nextsubsection.
.
functionsignaturegeneration differentfromenumerabletypes typesthataremarkedasarray canincludein f initenestedstructureandarbitrarysize whichmakes it impossible to maintain a limited type list holding all the possible situations.tosolvethisproblem deepinfer designsasequencegenerationmodeltodynamicallygeneratethepossiblearraytypeby learning the type related information extracted from the ir.
specifically deepinfer followsthearchitectureofsequencetosequence learningwhichconsistsofanencoderandadecodertoachievethis goalas showninfig.
.
encoder .differentfromtheclassicalsequencelearningencoder that takes the tokenized sequence asinputs and makes each token learnthecontextualtokeninformation deepinfer directlytreatsthe embedding vector of cdfs i.e.
u1d438 u1d451 and all the embedding vectors of cvs i.e.
u1d450 u1d458 as the input but doesn t require them to recognize the context because there is no order relationships between the inputs.deepinfer utilizes such an input form because we expect itcannotonlypredictthebasictypesofanarray e.g.
uint bool address etc accordingtoimplicitknowledgefromcdfsbutalso determine the dimensions according to the cvs.
thus deepinfer adopts the input embedding vectors x u1d4500 u1d4501 u1d4502 ... u1d450 u1d458 u1d4500 u1d438 u1d451 to guide the type generationinthe decoding procedure.decoder.deepinfer uses vanilla recurrent neural network rnn as the basic architecture of the decoder which reads an input tokenandcombinesitwithembeddingvectorsfromtheencoderto generate the target token.
assume the vocabulary of basic types is v u1d4631 ... u1d463 u1d45f where u1d45frepresentsthevocabularylength deepinfercan predict the basic type of an array from this vocabulary accordingtothe input information.
however thenested depthor dimension size of an array cannot be foreseen because such values needtobedetermineddynamicallyaccordingtotheinputs.that is it suffers from the out of vocabulary oov issue .
to deal with this situation deepinfer introduces the copy mechanism thatcan dynamically copy orgenerate the mostpossible token as the decoder output.
given the encoder output u1d4500 u1d4501 u1d4502 ... u1d450 u1d458 and the current i.e.
step u1d461 decoder input hidden status u1d460 u1d461 deepinfer employs the attentionmechanismto calculatethe currentcontextvector u1d438 u1d450 u1d461 u1d465 u1d458 summationdisplay.
u1d456 u1d6fc u1d456 u1d450 u1d456 u1d6fc u1d456 u1d460 u1d461 u1d44a u1d454 u1d450 u1d456 summationdisplay.
u1d456 u1d460 u1d461 u1d44a u1d454 u1d450 u1d456 where u1d6fc u1d456is the weight score and u1d44a u1d454is the trainable weight matrix.
then deepinfer predictstheprobabilityoftheoutputtokenat currentstepbyeitherselectingthemostpossibletokenfromthe vocabularyvorcopyingthe token from the inputsequence x u1d45c u1d462 u1d461 u1d461 u1d45d u1d454 u1d454 u1d452 u1d45b u1d452 u1d461 u1d45d u1d454 u1d450 u1d45c u1d45d u1d466.alt u1d461 wherethe u1d45c u1d462 u1d461 u1d461isthe f inalprobabilitydistributionatstep u1d461 u1d45d u1d454refers totheprobabilityatokenneedstobegenerated u1d454 u1d452 u1d45b u1d452 u1d461and u1d450 u1d45c u1d45d u1d466.alt u1d461 meanstheprobabilitydistributionsoverthevocabulary vorthe inputsequencex respectively u1d45d u1d454 u1d70e parenleftbig u1d44a u1d450 u1d461 u1d465 u1d438 u1d450 u1d461 u1d465 u1d44a u1d460 u1d460 u1d461 u1d44a u1d45d u1d460 u1d461 u1d44f u1d454 parenrightbig u1d454 u1d452 u1d45b u1d452 u1d461 u1d460 u1d45c u1d453 u1d461 u1d45a u1d44e u1d465 u1d44a u1d449 u1d44f u1d449 u1d450 u1d45c u1d45d u1d466.alt u1d461 summationdisplay.
u1d456 u1d465 u1d456 u1d466.alt u1d44e u1d461 u1d461 u1d461 u1d456 where u1d70eis an activation function u1d44a u1d450 u1d461 u1d465 u1d44a u1d460 u1d44a u1d45d u1d44f u1d454 u1d44a u1d449 and u1d44f u1d449 are the trainable parameters u1d466.altis a token and u1d465 u1d456 x u1d44e u1d461 u1d461 u1d461 u1d456refers to the attentive probability distributionover xat step u1d461.
thedecoderstartswiththetoken sos andoutputsallpossible tokensstepbystepuntilaspecialtoken eos appearedfollowing the process in previous work .
to make the model better learnthecorrectoutputtokensequence deepinfer introducesthe teacher forcing technique during the model training.
this techniqueemploysthescheduledsampling thatmakestheinput distribution between training and generation as similar as possible to forcethe outputtokens being subjectto truly array types.
.
functionreturn recovery asdescribedabove argumentsaccesshasexplicitinstructionoperations in the evm bytecode whereas the returns of a function is stored into the memory.
thus the model needs to understand thefunctionalityforinferringfunctionreturns.cfgre f lectshow each statement is executed during the program running andweexpectthemodeltounderstandreturn relatedknowledge fromthecfg.graphneuralnetwork gnn duetoitspowerful 751esec fse december3 san francisco ca usa kunsongzhao zihaoli jianfengli he ye xiapu luo andting chen structurallearningability hasbeenwidelyusedinmanycodecomprehension related tasks .
because of the graph structure representationofthecfg itisnaturalthatusingthegnnmodel to learn its implicit information.
in this work deepinfer adopts the graphattention network gat to learntheknowledge fromthe cfg because not all the execution paths are associated with the returnvaluesandgatcanautomaticallygivemorefocusonthe executionpathsrelatedto the functionreturns.
concretely assumethenodesetofacfgis u1d7141 u1d7142 ... u1d714 u1d707 where u1d707represents the number of basic blocks.
we extract the instruction sequencefromeach basic blockandusetheiropcodes to train another cbow model for producing a new embedding matrixw similar to .
.
.
we don t use the previous mapping matrixwbecause our aim here is tomake theembedding ofeach opcode capture its contextual operations at basic block level rather than the signature related cdfs.
for each basic block deepinfer usesw toinitialize theopcodesincluded in itand averages them to produce the node embedding vectors.
next deepinfer employs the gat to update the embedding vectors of each node by incorporating its neighborhood nodes with differentweights uni210e u1d707 summationdisplay.
u1d708 n u1d707 u1d711 u1d707 u1d44a u1d45f uni210e u1d708 where uni210e u1d707istheupdatedembeddingvectorofthe u1d707 thnodeandn u1d707 means the neighborhood nodes of the u1d707 th node.
u1d44a u1d45fis trainable parameters.
u1d711 u1d707isthe weightscore across n u1d707 u1d711 u1d707 exp parenleftbig leakyrelu parenleftbig u1d44a u1d45f parenrightbig parenrightbig summationtext.
u1d708 n u1d707exp parenleftbig leakyrelu parenleftbig u1d44a u1d45f parenrightbig parenrightbig where leakyrelu is the activation function and u1d44a u1d45fis a trainableparameter.
after the node embedding vectors are updated by multi layer gat deepinfer aggregatesall thenodeembedding vectorstoproduce the representation vectors u1d43b u1d43afor a cfg using the similar operation as eq.
and eq.
.
finally deepinfer uses a softmax layer to outputthe probability distribution.
evaluation .
experimentalsetup .
.
dataset.
to evaluate the accuracy of deepinfer we follow the previous work to instrument an ethereum node to obtain the runtime bytecode from deployed smart contracts.
as a result wecollect47 631bytecodeforsoliditysmartcontracts and bytecode for vyper smart contracts.
we merely keep theopen sourcesmartcontractsbecausetheground truthcanbe obtainedfromtheirsourcecode.weremovetheduplicatedsmart contracts and use the etherscan api to collect their groundtruth.
according to the statistic .
functions with equal or less thannineargumentsandreturns.followingtheprocessingprinciple in previous work we focus on the functions with no morethannineargumentsorreturns.asaresult weobtain292 unique functions for solidity contracts and unique functions for vyper contracts respectively.
for each language we randomly select80 ofthecollecteddataasthetrainingsetandtheremainder istreatedas the test set.
.
.
implementation.
we implement deepinfer in about lines of code using python .
the ir isparsed from the bytecode using the gigahorse tool .
we employ multiprocessing package with processesto parallelize the data processing pipeline.
since there are multiple arguments or returns for each function we f irst trainamodeltopredictthenumberofargumentsorreturnsand separately train the models for arguments or returns on each position.weiterativelytrainthemodelstorelievethedatade f iciency issueandenhancetheknowledgelearnedbythemodel .
forthemodelconstruction weadoptpytorchframework to implementourhflmcomponentinwhichtwobi directionallstm layersareused.weembedeachinputtokenintoa100 dimensional embeddingvectorandthesizeofhiddenlayerissetas200.tooptimize themodel parameters we selectthe adam algorithm as theoptimizertoupdatethegradient.duringthegenerationprocess weadoptbeamsearchtechnique withabeamwidthof10to producetheoutputsequence.tounderstandthefunctionalityfrom thecfg weusetwolayersofthegattolearnandupdatenode embedding vectors.
table evaluationresults of deepinfer compiler metricsignature return number typenumber type soliditytop .
.
.
.
top .
.
.
.
top .
.
.
.
vypertop .
.
.
.
top .
.
.
.
top .
.
.
.
.
rq1 howistheaccuracy of deepinfer ?
.
.
motivation.
the aimof deepinfer istorecoverthe function signatures and returns by learning the access patterns or understandingthefunctionalityfromevmbytecode respectively.this questionisdesignedtoexploretowhatextent deepinfer canrecover the function signatures orreturns.
.
.
approach.
toanswerthisquestion wetraindifferentmodels todealwithdistincttasks.for function signature inference deepinfertrainsamodeltopredictthenumberofargumentsandthen other models are trained to determine the type at each position over a limited type list .
.
for the types that are marked as array sincethein f initenestingdepthandarbitrarysizeofthem deepinfer trainsgenerativemodelstorecovertheactualtype .
.
forfunctionreturninference deepinfer trainsclassi f icationmodels that can understand the functionality to predict the number and types of returns .
.
we evaluate the accuracy of deepinfer with all the collected functions in open source solidity and vyper contracts respectively.
we report the average top u1d458accuracy of deepinfer which is the ratio of correct inference occurred in the most probable u1d458 u1d458 candidates to the total number of unique ground truths.
.
.
results.
table2illustratestheaccuracyof deepinfer for recoveringthenumberandtypesofsignaturesandreturns respectively.
when considering the top suggestion deepinfer achieves 752deepinfer deep type inference from smartcontract bytecode esec fse december3 san francisco ca usa the accuracy of .
.
.
and .
for the number and type inference of arguments and returns in solidity respectively.
it obtainsthe top accuracy of .
.
.
and .
invyper smart contracts respectively.
when taking the top suggestion intoaccount theaccuracygoesupto0.
.
.988and0.943for solidity and .
.
.
and .
for vyper respectively.
it showstheaverageaccuracyimprovementsinrecoveringsignatures and returns by .
and .
insolidity and vyper respectively.
we manually investigate the incorrect inference and summarize the causes offailures as follows.
1function register bytes domain address address external ...... 3addresses address listing aninaccurate recovery incase1 case deepinfer will confuse thetypes bytesandstringwhen there is no operation on the parameter itself in smart contracts.
as shownin listing despitedeepinfer correctly predict thesecond parameterasthetype addressbutitinaccurately predictsthe f irst parameter as stringinstead of bytes.
this is because both bytesand stringareloadedbythesameinstruction calldatacopy .theydiffer only in that the former can be accessed by the byteinstruction butthelattercannot.however inthiscase the f irstparameteris used as the index without any manipulation for itself i.e.
the byte instructionisabsent inthe corresponding ir.
1function getcontractversioncount bytes32 name external ... ...... 3return addressstoragehistory .
length listing aninaccurate recovery incase2 case deepinfer will confuse the types bytes32anduint256.
when loading the byte sequence it will be masked by zeros on the low order side.
instead loading the unsigned integer will result in the masking by zeros on the high order side.
however when loading the types bytes32oruint256 they have already reached the maximum length i.e.
bytes so no masking operation is needed i.e.
they have the same loading instructions.
the differences betweenthemarethattheformercanbeaccessedby byteinstruction whereasthelattercanbeusedforarithmeticoperations.asaresult asshowninlisting deepinfer incorrectly predictthe bytes32as theuint256 becausetherearenobyteaccessorarithmeticoperationsandthis parameterisonly usedas the index.
1function verify address tokens uint256 args external 2address deposittoken tokens 3address issuetoken tokens 4uint256 totalissueamount args 5uint256 interestrate args 6uint256 maturity args 7uint256 issuefee args 8uint256 minissueratio args ...... listing aninaccurate recovery incase3 case there are some missing constant values due to the compilation optimization of smart contracts which misleads the predictionof deepinfer .asshowninlisting theargumentsofthe function are one dimensional static array whose sizes needed to be inferred by incorporating the information from constant values.
however sincetheoptimization operationis activated duringthe compilation the constant values related to the size are missing.
as aresult deepinfer inaccuratelyinfersthe size ofthe array.1function getriskandvalue bytes32 result public returns uint80 uint128 2bytesmemory riskb slicefrombytes32 result 3bytesmemory valueb slicefrombytes32 result 4return uint80 touint128 riskb touint128 valueb listing aninaccurate recovery incase4 case deepinfer fails to infer some rare types.
as shown in listing5 very few functions adopt the type uint80as the return type.
this causes the model can simply learn extremely limited knowledge about this type during the model training process.
as a result deepinfer outputs an incorrectinference.
answertorq1 thetop 5accuracyof deepinfer is0.
for solidityand0.
for vyper.
.
rq2 howdoes deepinfer perform compared with existingtools?
.
.
motivation.
sincethere are someexisting tools thatdecompile bytecode into human readable pseudocode ordirectly recover function signatures from the bytecode this question is designed to explore whether deepinfer performs better than existing techniques.
.
.
approach.
wecompared deepinfer withthreestate of theart decompilers including ebd eveem and gigahorse andonesymbolicexecution basedstaticanalysistechnique sigrec in terms of signature recovery.
all the four baseline approachessupportthebytecodeofsmartcontractsasinput.ifone method can correctly infer the types of signatures or returns at aposition wetreatthisasasuccessfulprediction.wereportthe average accuracy of these baselines in solidity and vyper smart contracts respectively.
.
.
results.
table3presents the average results for deepinfer andbaselinemethods.fromthistable wecan f indthattheaverage top accuracy of deepinfer achieves average improvements by .
and .
compared with the baselines while recovering function signatures in solidity and vyper respectively.
despite deepinfer obtainingnearlythesametop 1accuracyassigrecwhen predicting the types of arguments in solidity it achieves an improvement by .
when considering the top suggested types.
besides the top accuracy of deepinfer achieves an improvement by .
compared with sigrec when dealing with vyper smart contracts.
this is because sigrec depends on the static heuristic rulesdesignedbyhumanexperts whichlimits itsabilitytoexpand to cover newly developed and compiled contractsinvolving new access patterns.instead deepinfer can automatically learn theaccess patterns of arguments from the bytecode without any human intervention.
we can see from this table that none of the baselines support recovering function returns.
instead deepinfer achieves the top accuracy of .
and .
while recovering function returns insolidityandvyper respectively.
753esec fse december3 san francisco ca usa kunsongzhao zihaoli jianfengli he ye xiapu luo andting chen table average results for deepinfer andbaselines compiler approach ebd eveem gigahorse sigrecdeepinfer top top top soliditysignature .
.
.
.
.
.
.
return .
.
.
vypersignature .
.
.
.
.
.
.
return .
.
.
answertorq2 thetop 1accuracyof deepinfer forrecovering functionsignatures achieves average improvements by .
and .
across the baselines in solidity and vyper respectively.
in addition only deepinfer can recover function returns with the top accuracy by .
for solidity and0.
for vyper.
.
rq3 howefficientis deepinfer ?
.
.
motivation.
asthestaticanalysistechniquesforfunctionsignaturerecovery e.g.
sigrec aretime expensiveduetotheprogram simulation executionand path exploration deepinfer depends onwell traineddeeplearningmodelstodirectlypredictfunction signatures.
this question is designed to evaluate how efficient is deepinfer whilerecoveringfunctionsignatures andreturns.
.
.
approach.
to explore what is the performance overhead of deepinfer we execute and compare it with other baselines.
we excludetheapproachebdbecauseitdependsonalimitedfsdand searches the function signatures from this database if exists which just consumes a negligible amount of time.
for deepinfer since the training process can be done offline we compare its predictive time cost.
we report the average time consumption for recovering signatures andreturns ofeachfunctioninseconds.
table average time consumption approach deepinfer u1d446sigrec gigahorse eveem speedup deepinfer u1d445 time s .
.
.
.
.25x .
deepinfer u1d446anddeepinfer u1d445refer to theinference for signatures andreturns respectively.
.
.
results.
table4elaborates the average time consumption of deepinfer andotherthreebaselinesforrecoveringfunctionsignatures.deepinfer onlyspends0.08secondsforrecoveringsignatures of each function which is on average over times faster than otherbaselines.
besides deepinfer spends .
seconds forrecoveringfunctionreturnsonaverage.thisisbecausedifferentfrom the static analysis based techniques that run on central processing units deepinfer adoptsdeeplearningastheinfrastructurewhich naturallysupportstheoperation accelerationofthegraphicsprocessing units.in addition deepinfer can be fullytrainedofflineand then usedto predict online.
answer to rq3 deepinfer is over times faster than the baselinesonaverage.
.
rq4 howdoesthecompiler version affect deepinfer ?
.
.
motivation.
the compiler versions are frequently upgraded which will introduce the new characteristics and operations toavoid potential vulnerabilities .
this question is designed to explore howthe changes of compilerversionsimpact deepinfer .
.
.
approach.
toevaluatetheimpactofvariouscompilerversions ondeepinfer we categoryall theopen source solidity smart contracts according to their main versions.
we train the models on the lower versions and test whether deepinfer still works on thehighversionthatisnotvisibleduringthemodeltraining.for example we trainthe models onthesmart contracts withthe versions .
.x to .
.xand assess its accuracy on the high version .
.x.
we select the smartcontractswhose versionislarge than0.
.x becausethecounterpartistoolittletosupportthemodeltraining less than .
we report the average accuracy of deepinfer on different compilerversions.
top top top .
.
.
.0accuracyv0.
.x v0.
.x v0.
.x v0.
.x a signaturetop top top .
.
.
.0accuracyv0.
.x v0.
.x v0.
.x v0.
.x b return figure evaluation results of deepinfer under various compiler versions.
.
.
results.
fig.3presentstheaverageaccuracyof deepinfer underdifferentcompilerversions.wecan f indthat deepinfer achieves nearly the same performance for recovering function signatures and returns across various compiler versions respectively.
despite thestate of the arttoolsigrec hasthe abilitytorecover thefunction signatures as shown in .
it will be completely disabled when thecompiler version islarger than .
.
.this isbecause the design of sigrec relies on the access patterns of compiler versions.
instead deepinfer is built upon the ir which strips the compilerrelated operations.
such design makes deepinfer insensitive to the changesofcompilerversions.italsodemonstratesthat deepinfer actuallylearnstheimplicitknowledgethatisrelevanttofunction signatures andreturns.
answer to rq4 deepinfer is immune to the changes in compilerversions.
discussion in this section we will discuss the limitations of deepinfer and potentialthreatsto validity encountered.
first deepinfer cannot recover the function signatures if the ir is not correctly parsed during the phase of lifting.
deepinfer adopts the state of the art lifting tool gigahorse to convert thebytecodeintotheregister basedir.ifgigahorsecrashes their cannotbeproperlylifted deepinfer willnotbeabletoextractthe validaccesspatterns resultinginafailedrecovery.similarsituation will also occur while recovering returns.
but we found that such situation is rare nearly .
which implies that deepinfer will hardly be affected.
we will try to extend such tool to support valid 754deepinfer deep type inference from smartcontract bytecode esec fse december3 san francisco ca usa lifting operation especially for the vyper smart contracts in the future.
second the accuracy of inferring some rare types is not as high asthatofinferringothertypes.thisisbecause deepinfer designs adeeplearning basedarchitecturewhich isinherentlylimited by the training data itself due to the data hungry characteristics of deep learning techniques .
one solution is to employ some data enhancementtechniquesthatpreservethesemanticsofthe bytecode andwe leave this as the immediate future work.
on the other hand there are some possible threats to validity duringourexperiments.thethreatstointernalvaliditylieinthe tuning of hyper parameters in the models.
to relieve this validity we f ine tune the batch size from to and the learning rate from1e 5to1e 3tomakethemodelbefullytrained.inaddition wesetthehiddensizeas200dimensionsandadoptsomedefault settings i.e.
stackingtwolstmorgatlayers forexperiments.
otherparametercombinationsmayalsoimprovetheaccuracyof deepinfer andwe leave this explorationas the future work.
related work .
signatureinference in smartcontracts abi guesser wasdeveloped to infer thetypes ofabi encoded data.
chen et al.
was the f irst to develop a static analysis based tool called sigrec to recover function signatures in smart contracts.speci f ically sigrec f irstdisassembledthebytecodeandthen proposedtype awaresymbolicexecutionthatexploredhowaparameterwasmanipulatedinevminstructions.itdesigneddifferent patternsfordifferenttypesaccordingtothespeci f ictype related operations in evm and symbolically executed evm instructions to recover parametertypes.
however abi guesser can onlydeal with very simple data formats which limits the ability in real world smart contracts.
in addition sincetheaccessingrulesusedbysigrecheavilyrelyon the expert knowledge itis hard to be extended.
thus we propose the f irstdeeplearningbasedmethodthatcanautomaticallylearn theaccessingrulesorunderstandfunctionalitiestorecoverfunctionsignaturesandreturnsfromthebytecodeofsmartcontracts withoutany manual efforts.
.
deeplearning forsignatureinference although very little work focusedon functionsignature inference in smart contract this topic been explored much more in other scenarios with the help of deep learning techniques.
chua et al.
was the f irst to introduce deep learning into function type recovery from c c binaries.
they regarded instructions as a sequence and employed the rnn model to learn the semantics to recover types.
however some important information related to function signatures was stripped off during the compilation process.
to deal with this limitation lin et al.
proposed resil that injected compiler optimization related domain information intotheinstructionstoassisttheinferenceoffunctionsignatures.
peietal.
pretrainedamodeltolearntheoperationalsemantics from asssembly instructions with generative state modeling and thenusedthemodeltoinferthetypesforc c binaries.lehmann et al.
tried to recover types from webassembly binaries.
they de f inedgrammarsofhigh leveltypelanguagesandemployedtheneural machine translation architecture to recover these high level types basedonthe dwarfdebugginginformation.
inadditiontotheabovestudiesrecoveringsignaturesfrombinaries there also exist work focused on signature recovery from sourcecode.maliketal.
developedalstm basedmodelcalled nl2type that utilized the code comments function names and parameternames fromthesourcecodetopredictthefunctiontypes injavascript.allamanisetal.
developedagraphneuralnetwork basedmethodequippedwithanoveltripletlossfunctionwhichcan learn the syntactic and semantic from source code to predict types in python functions.
mir et al.
f irst parsed python source code intoastsandthenextractedidentifers contextualinformationand visibletypehints.theyincorporatedcodesemanticslearntbyusing a rnn model from code contexts with type hints and employed knn algorithm to infer types.
peng et al.
proposed hityper thatcombineddeeplearningwithstaticanalysisfortypeprediction inpython.
they f irst constructedatypedependencygraph tdg fromsourcecodeandthenconductedforwardstatictypeinference along the tdg according to type dependencies.
for some variables that would impact the types of many others hityper trained a similarity based deep learning model to recommend possible types.
differentfromtheabovestudies wefocusonrecoveringfunction signaturesandreturnsfromthebytecodeofsmartcontractswithout any human intervention.
since the smart contracts are designed toperformespecialactionsontheblockchain e.g.
transactions theabove mentionedtoolscannotcapturesomedomain speci f ic operations such as calldataload andcalldatacopy .
thus it is necessarytodevelopanewtoolforthebytecodeofsmartcontracts.
to the best of our knowledge we are the f irst to design such an automaticdeeplearningmodelfor this purpose.
conclusion wepresent deepinfer anoveldeeplearning basedframeworkto automatically recover function signatures and returns from the bytecodeofsolidityandvypersmartcontractswithoutanyhuman intervention.
the experimental results demonstrate that deepinfer is more accurate and efficient than existing tools under different languages and distinct compiler versions.
in the future we plan to conduct a deep analysis on each component of the deepinfer framework and explore other alternative model components such astransformers andparametercombinationstoimprovethemodel performance.
in addition we will extend our tool to support smart contracts running onotherblockchains.
data availability ourexperimentalmaterialsareavailableat or com sepine deepinfer .