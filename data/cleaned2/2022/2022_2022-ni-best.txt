the best of both worlds integrating semantic features with expert features for defect prediction and localization chao ni school of software technology zhejiang university hangzhou zhejiang china chaoni zju.edu.cnwei wang college of computer science and technology zhejiang university hangzhou china wangw99 zju.edu.cnkaiwen yang college of computer science and technology zhejiang university hangzhou china kwyang zju.edu.cn xin xia software engineering application technology lab huawei hangzhou china xin.xia acm.orgkui liu software engineering application technology lab huawei hangzhou china brucekuiliu gmail.comdavid lo singapore management university singapore davidlo smu.edu.sg abstract to improve software quality just in time defect prediction jit dp identifying defect inducing commits and just in time defect localization jit dl identifying defect inducing code lines in commits have been widely studied by learning semantic features or expert features respectively and indeed achieved promising performance.
semantic features and expert features describe code change commits from different aspects however the best of the two features have not been fully explored together to boost the just in time defect prediction and localization in the literature yet.
additional jit dp identifies defects at the coarse commit level while as the consequent task of jit dp jit dl cannot achieve the accurate localization of defect inducing code lines in a commit without jit dp.
we hypothesize that the two jit tasks can be combined together to boost the accurate prediction and localization of defect inducing commits by integrating semantic features with expert features.
therefore we propose to build a unified model jit fine for the just in time defect prediction and localization by leveraging the best of semantic features and expert features.
to assess the feasibility of jit fine we first build a large scale line level manually labeled dataset jit defects4j .
then we make a comprehensive comparison with six state of the art baselines under various settings using ten performance measures grouped into two types effort agnostic and effort aware.
the experimental results indicate that jit fine can outperform all state of the art baselines on both jit dp and jitdl tasks in terms of ten performance measures with a substantial improvement i.e.
in terms of effort agnostic measures xin xia is the corresponding author.
wei wang and kaiwen yang contributed equally to this research.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november singapore singapore association for computing machinery.
acm isbn .
.
.
.
jit dp in terms of effort aware measures on jit dp and in terms of effort aware measures on jit dl .
ccs concepts software and its engineering software defect analysis .
keywords just in time defect prediction defect localization deep learning acm reference format chao ni wei wang kaiwen yang xin xia kui liu and david lo.
.
the best of both worlds integrating semantic features with expert features for defect prediction and localization.
in proceedings of the 30th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november singapore singapore.
acm new york ny usa pages.
https introduction software defects are unavoidable in software development and most of them are induced by the commits submitted in the evolution of software .
once software is released with defects the various walks of human like could be substantially affected with unacceptable consequences on finance1and human life .
additionally the cost of maintaining defect contained software will be increased sharply .
to address this challenge one potential feasible method is to identify and fix defects as early as possible.
to this end practitioners have been exploring various approaches of identifying defects at the coarse grained level or finegrained level .
meanwhile a few approaches are also proposed to timely and finely locate where the defects exist which can help developers better understand the code .
these fine grained approaches aim at assisting developers to identify the defect inducing commits lines submitted by developers before merging them which is also referred to as just intime jit techniques.
the promising results achieved by existing studies indicate that jit defect prediction and localization jit dp jit dl can provide effective hints for software participants to 672esec fse november singapore singapore chao ni wei wang kaiwen yang xin xia kui liu and david lo identify and locate software defects in time especially for the limited resource scenario.
in the literature the state of the art jit dp techniques mainly adopt machine learning technologies to build a prediction model that is used to predict the defect inducing commits by learning from the semantic features or the carefully curated expert features.
semantic features are mainly mined from the semantic information and syntactic structure hidden in the faulty source code to represent the semantic characteristics of defect prone commits .
the expert features are defined by experts according to their understanding on defect inducing commits with their professional knowledge and experience which are taken as the input of learning based classifiers to identify the defect inducing commits.
in the literature kamei et al.
firstly defined types of expert features cf.
section with five dimensions i.e.
diffusion size purpose and history of code changes as well as programmers experience which have been widely used as the expert features to detect the defect inducing code changes.
the state of the art techniques also mainly adopt machine learning technologies with token features to identify lines jit dl that are associated with the defective commits by two stage or two different approaches .
a diversity of state of the art research work has been proposed to boost the jit dp and jit dl tasks for developers and have made a great progress with promising results .
however the state of the art jit dp and jit dl techniques are still limited on the granularity and accuracy of defect prediction and localization.
for example 1the low quality datasets with tangled commits could affect the model training and further lead to the low accuracy of identified results because of the noise in both training data and testing data .2semantic features and expert features represent different characteristics of code changes from different dimensions which however are not explored together for defect prediction or localization yet.
we infer that the best of them could be exploited together for jit dp and jit dl tasks.
3jitdp and jit dl tasks are respectively studied as two independent techniques while they could be designed into a unified model from their purpose of identifying the defect inducing code lines in code change commits.
to sum up we intuitively hypothesize that justin time defect prediction and localization could be proceeded with a unified learning model by integrating the semantic features with expert features behind code changes.
in this paper based on our hypothesis we comprehensively explore the importance of combining semantic features with expert features for building a unified jit dp and jit dl model and present a novel approach named jit fine with the widely used deep learning technique codebert .
to the best of our knowledge jit fine is the first to build a unified model for jitdp and jit dl tasks.
the underlying intuition of our approach is to capture the distinguishing features of non defect inducing code changes and their contexts by integrating semantic features with expert features and consequently to identify the defective commits as well as locate the defect position at line level.
more specifically jit fine mainly consists of three steps extracting expert features and semantic features from the code changes and their contexts with pre trained models learning integrated features between semantic and expert features to distinguish commits from defect inducing ones to defect free ones and predictingwhether a commit is defect inducing and locating which code line induce the defect in a commit.
eventually this paper makes the main contributions as below dataset jit defects4j.
we build a large scale line level labeled dataset jit defects4j on the top of lltc4j a dataset collected by herbold et al.
for analyzing the tangled bug fixing commits to support the research of just in time defects and to fill the gap of the low quality dataset in the jit dp and jit dl community.
jit fine.
we implement a unified just in time defect prediction and localization technique jit fine with a unified learning model to mine the distinguishing features from the semantic features and expert features of code changes and their contexts.
the replication package of jit fine and the aforementioned dataset jit defects4j are publicly available.
just in time defect prediction and localization.
we comprehensively investigate the value of integrating the expert features and semantic features for just in time defect prediction and justin time defect localization.
the results indicate that jit fine outperforms the state of the art e.g.
a higher f1 measure by percentage points on defect prediction and a higher top accuracy by percentage points .
analyzing the effectiveness of combined features.
we analyze the effectiveness of semantic features and expert features on jitline and jit fine and the experimental results uncover that the combination of semantic and expert features presents obvious advantages on just in time defect prediction than the usage of single ones.
the rest of this paper is organized as follows.
section first introduces the background and motivation of our work.
following that section introduces the design of jit fine.
section describes the details of studied dataset.
section presents the experimental setting including compared baselines and considered performance measures.
section reports the experimental results.
following that some threats to validity are presented in section .
section describes related prior work.
finally we conclude our work and mention future plan in section .
background and motivation .
expert features and semantic features this section clarifies the notions of expert features and semantic features related to the code changes.
expert features are the descriptive features defined by experts according to their understanding of defect inducing commits with their professional knowledge and experience to measure the quality of code changes.
in the literature dozens of expert features have been defined at change level from various dimensions e.g.
diffusion size purpose history experience review programming language .
the features presented in table defined by kamei et al.
from five dimensions are widely adopted in the domain of just in time defect prediction and have been demonstrated for their effectiveness on such prediction tasks .
therefore we follow the state of the art work to adopt the expert features as an important part to mine the integrated features of code changes for just in time defect prediction and localization.
673the best of both worlds integrating semantic features with expert features for defect prediction and localization esec fse november singapore singapore table studied basic change level features.
name description dimension ns the number of modified subsystems diffusionnd the number of modified directories nf the number of modified files entropydistribution of modified code across each file la lines of code added size ld lines of code deleted lt lines of code in a file before the change fix whether or not the change is a defect fix purpose ndevthe number of developers that changed the modified files history agethe average time interval between the last and current change nucthe number of unique changes to the modified files exp developer experience experience rexp recent developer experience sexp developer experience on a subsystem semantic features represent the theoretical units of meaningholding components which are used for representing word meaning and play a crucial role in determining the lexical relationships between words in a language.
in the domain of software engineering semantic features capture the meaning of tokens in code as well as their contexts e.g.
semantic and syntactic structural information of code that have been widely used to represent the intrinsic characteristics of code mined with deep learning techniques like codebert .
codebert aims at learning contextual word embedding i.e.
the embedding of a word changes dynamically according to the context in which it appears and has been widely used in multiple natural language processing nlp tasks and software engineering se tasks .
in this work we extract semantic features of code changes by leveraging the popular pre trained model codebert and fine tune it on commits for adapting it to downstream tasks.
.
motivation in the literature just in time defect prediction and localization have been attracting more and more attention to boost just intime software quality analysis and indeed has achieved promising results for developers to automatically identify defects in code change commits from coarse grain granularity to fine grained granularity .
nevertheless just in time defect prediction and localization is still an open question because of various limitations e.g.
low quality dataset independently mined features and non unified models which motivates us to conduct this study to boost the development of just in time defect prediction and localization.
low quality datasets .
the state of the art jit dp and jit dl techniques are highly dependent on accurate information about code changes in the related datasets.
unfortunately these techniques suffer from the low quality dataset e.g.
tangled commits mislabeled by practitioners .
in practice a single commit could be tangled with several kinds of code changes e.g.
bug fix code refactoring testing new features and documentation.
in addition commits tangled with several bug fixes can heavily affect the accuracy of data extraction and labeling with the related algorithm szz and its variants which subsequently has negative impacts on jit dp and jit dl tasks since noise data could bias the learning results of corresponding models.
unfortunately almost all prior studies were proposed based on such a low quality dataset .
to address the challenge of the low quality dataset more and more large scale and accurate datasets are collected by researchers.
for jit dl task yan et al.
and pornprasit et al.
respectively built line level datasets on the top of szz labeled dataset.
for jit dp rosa et al.
used the natural language processing technique to identify bug fixing commits by utilizing developers information i.e.
developers explicitly referenced that the commits fixed bugs to build a large scale dataset with accurate bug fixing commits.
similarly zeng et al.
built a large scale dataset with the data labeling algorithm szz.
however these datasets still do not consider the side effects of tangled bugfixing commits i.e.
a single commit is tangled with several bug fixes .
to fill this gap we build a large scale high quality linelevel dataset jit defects4j to alleviate the effects of tangled commits on the basis of lltc4j which can be utilized in jit dp and jit dl tasks.
independently mined features .
semantic features reflect the intrinsic characteristics of code changes and their contexts which have been mined by practitioners to build the just in time defect prediction model with deep learning techniques e.g.
deepjit and cc2vec .
expert features are defined on the basis of the participants knowledge and experience that are the extrinsic understanding from human.
researchers proposed to utilize expert features e.g.
ealr cbs oneway churn and lapredict to conduct the just in time defect prediction task.
more recently pornprasit and tantithamthavorn proposed a just in time defect localization technique jitline by considering code token features ignoring token order as well as the deeper semantic features and a few expert features.
these research work has achieved promising results on just in time defect prediction and localization.
actually semantic features and expert features represent distinguishing features of code from the aspect of expert knowledge and intrinsic structure of code respectively.
however the state of the art research work did not explore the integration of semantic features and expert features for jit dp and jit dl tasks yet.can a combination of both kinds of features achieve higher performance?
with this question we intuitively hypothesize that leveraging the best of semantic and expert features could be used to improve the just in time defect prediction and localization.
eventually our work fills this gap by investigating the combination of semantic features and expert features to verify our intuitive hypothesis.
independently built models .
just in time defect prediction and localization have been studied in various ways to support the software quality assurance.
existing works either treat the two tasks independently or address the two tasks with twostage two different approaches .
for example yan et al.
proposed a two phase work for jit dp and jit dl.
specially they build two independent models for the two tasks respectively prediction model with expert features for jit dp and n gram model for jit dl.
besides pornprasit and tantithamthavorn proposed 674esec fse november singapore singapore chao ni wei wang kaiwen yang xin xia kui liu and david lo coderepository newly commitintegratedfeaturelearning p1 psoftmax expert features combined coderepresentationssemantic featuresfeature extractionofflinetraining trainabletrainableintegratedfeature learning featureextractiondefectprediction localizationprediction localizationmodel onlinepredicting path to file1101102103104105101102103104105 path to file2101102103104105101102103104105commit messagedescription of code changescodebert e e1e2...ene1......e2em semantic extractorexpert defined figure an overview of our approach jit fine of predicting and localizing defects.
the jitline model to address the jit dp task with a prediction model by combining expert features with code token features and to address the jit dl task with another model lime by calculating the contribution score of each token for its defect inducing interpretation.
moreover the existing jit dl techniques can identify the defect inducing code changes at the fine grained code line level but cannot obtain the high accuracy on identifying the exact defect inducing code lines in code changes without considering the identification of defect inducing commits e.g.
the most recent state of the art jitline can only achieve .
accuracy for the identified top most suspicious defect inducing code lines in code changes in our dataset .
can the jit dp and jit dl tasks be integrated into a unified model to improve their performance?
this work is thus conducted to investigate the feasibility of designing a unified model to proceed the jit dp and jit dl tasks.
jit fine just in time defect prediction and localization to investigate the feasibility of our intuitive hypothesis we propose a unified approach jit fine that integrates the semantic features with expert features for the just in time defect prediction and localization.
as illustrated in figure jit fine consists of three main steps feature extraction where expert features are extracted by adopting the widely used change level features defined by kamei et al.
and semantic features are extracted with the popular pre trained model codebert integrated feature learning is proceeded with a fully connected layer and defect prediction and localization is to predict the defect inducing commits and locate defective code lines with previously learned features.
details of jit fine are presented in the following subsections.
commitmessageaddedlinedeletedline tokenizationtoken1 tokenn token1tokenm token1tokenp.........e e1ene e1eme e1ep.........ct1tnt t1 tmt t1 tp.........linearlinearlinearsemanticembedding dotproductattentionconcatenatelinearadd normlinearadd normcodebertnxfigure the structure of semantic feature extractor.
.
feature extraction feature extraction aims at converting the statistical data and code tokens into numeric representation that can be adapted to the deep neural network models to capture the distinguishing characteristics for the later prediction and localization tasks.
in the community semantic feature extraction mainly relies on the pre trained model codebert that aims at learning contextual word embedding i.e.
the embedding of a word changes dynamically according to the context in which it appears and has been widely used in multiple natural language processing nlp tasks and software engineering se tasks .
therefore we also leverage codebert to extract the semantic features for code changes.
the structure of the semantic feature extractor in jit fine is shown in figure which takes as input three types of information e.g.
commit message added lines deleted lines .
commit message represents the description of the submitted commit added line and deleted line represent the lines added and deleted in the commit respectively.
for the classification task a special token 675the best of both worlds integrating semantic features with expert features for defect prediction and localization esec fse november singapore singapore is always added in front of each input instance to obtain the semantic representation of the whole sentence we follow this workaround and add the token before the commit message.
in addition we add the other two tokens i.e.
and before the added line and the deleted line to differentiate them.
then commit message added lines deleted lines are tokenized into a token sequence that is fed into the codebert model to generate the corresponding embedding vector that is referred to as the semantic vector of a code change commit.
codebert is designed for the general software engineering tasks we thus fine tune it on commits to adjust downstream tasks by following the workaround verified in the prior work .
for expert features we directly adopt the widely used change level metrics proposed by kamei et al.
to extract the expert features from code change commit by directly using the commitguru that is an online service of automatically extracting and labeling commit level datasets.
.
integrated feature learning with the semantic and expert features extracted in different ways jit fine further needs to learn their integrated features.
in the literature learning integrated features can be proceeded in different ways e.g.
building a model by simply combining expert features and semantic features or training semantic features jointly with expert features to build a model .
recently pan et al.
built a model by training semantic features jointly with expert features to automatically identify the information type in developer chatrooms with a promising result.
therefore we also leverage the straightforward way of jointly training the semantic features with the expert features.
note that the initial expert feature vector only contains numeric elements extracted with the feature factors while the semantic feature vector generated by codebert has numeric elements.
to avoid that the semantic feature could overwhelm the expert feature and to treat the two kinds of features equally we follow yang et al.
s work to obtain the high dimensional representation vectors for expert features with the deep learning technology a fully connected layer is used in this study .
the extended expert feature vector is denoted as vef and the semantic vector outputted by codebert is denoted asvsf.
jit fine concatenates the two vectors i.e.
vefandvsf to generate a new one denoted as vf for constructing the features of the code changes in a given commit and fine tune them together with another fully connective layer during the training phase.
.
defect prediction and localization with the learned integrated features the last step of jit fine is to train a defect prediction and localization model that will be applied to commits under review.
to this end the learned integrated features are first fed into one fully connected layer for the binary classification task.
the model is trained by iterating it on all training datasets monitoring the loss function and optimizing the weights of feature relationship by back propagation mechanism.
such progress is executed by a few epochs i.e.
at most times since we optimize our model by early stop strategy .
meanwhile based on the attention mechanism in codebert we can utilize the weights of each input code token to locate where the defect exists.for making the decision of a given commit we can not only obtain the corresponding label but also obtain its defect density which can be calculated as the ratio between the probability outputted by the model y c and the total modified lines of that commit loc c .
the defect density of a commit is a good metric since different commit needs different costs of applying quality assurance activities .
for localizing the defect inducing line in each commit we calculate the contribution i.e.
weights of each token in modified code to the final classification task and summary up all tokens contributions in one line.
finally we rank defective lines that are associated with a given commit based on the total contribution.
clean dataset construction as presented in section .
the jit dp and jit dl community suffer from the low quality dataset with tangled commits especially a commit tangled with several kinds of code changes e.g.
bug fix code refactoring testing new features and documentation.
therefore we build a clean line level dataset for both jit dp and jit dl on the basic of lltc4j to alleviate the impact of tangled commit on the evaluation of different approaches.
lltc4j line labelled tangled commits for java is manually built by herbold et al.
which only focus on bug fixing commits in java projects.
in particular they manually validated bugs from projects and these bugs are fixed by commits.
to decrease the high degree of uncertainty when determining whether a commit is tangled or not they first recruited participants with two criteria i.e.
majoring in computer science or a closely related subject and at least one year of programming experience in java .
then they assigned each commit to four different participants to label the data independently.
each commit was shown to four participants and the consensus of each line in a commit can be achieved if at least three participants agreed on the same label.
otherwise no consensus for the line was achieved.
following that they used the agreement among the participants to decrease the uncertainty involved in the labeling.
in particular each modified line in a commit can be labeled as one of the following types contributing to the bug fixing changing whitespace changing documentation changing test unrelated improvement not required for the bug fixing and no consensus .
lltc4j is a good starting point for collecting a high quality finegrained comprehensive dataset for just in time defect prediction and line level localization research.
however as aforementioned it only collects the bug fixing commits and labels the lines in each bugfixing commit.
therefore we extend this dataset from two sides extracting both clean commits and buggy commits extracting the line label in defect introducing commits.
just in time defect prediction is typically treated as a binary classification task the dataset should contain both clean instances and buggy instances for building a model.
besides to investigate the effectiveness of just in time defect localization model we need the ground truth label for each line in a commit.
therefore the former one enables us to construct a comprehensive dataset for just in time defect prediction research while the latter one enables us to construct a line level bug introducing or bug free dataset for just in time defect 676esec fse november singapore singapore chao ni wei wang kaiwen yang xin xia kui liu and david lo javaprojects participant labelingagreement on each line filteringbug fixinglinesidentifyingbicsbug fixing commitsnon bug fixing commitsidentifynon bfcsfilteringoutbics1clean and buggy commits 2line level labelun labeledlinesinbicsarecleanonesremainingcommitsarecleanonesbic bug inducingcommitbfc bug fixingcommit figure the process of data extension and line level labeling.
table statistics of studied datasets jit defects4j.
java project timeframecommit level line level bc cc ratio bugs all bl cl ratio bugs all ant ivy .
.
commons bcel .
.
commons beanutils .
.
commons codec .
.
commons collections .
.
commons compress .
.
commons configuration .
.
commons dbcp .
.
commons digester .
.
commons io .
.
commons jcs .
.
commons lang .
.
commons math .
.
commons net .
.
commons scxml .
.
commons validator .
.
commons vfs .
.
giraph .
.
gora .
.
opennlp .
.
parquet mr .
.
all .
.
bc refers to buggy commit cc refers to clean commit bl refers to buggy line and cl refers to clean line .
localization research.
the data extraction and extension process is illustrated in figure .
for identifying bug introducing commits we start from all bugfixing commits in the original dataset.
those commits have at least one agreed contributing to the bug fixing line which means the line is labeled by at least three participants with same label and can be selected as candidate bug fixing commits.
for each contributing to the bug fixing line in the candidate bug fixing commits we use git blame to find its corresponding bug introducing commit and also label the corresponding lines which are modified in bug fixing commit as buggy line.
such an operation can greatly reduce the scope of defect introducing candidates and accurately label the line of code.
the remaining commits i.e.
not classified as bugintroducing commits are treated as clean ones.we also need to collect the modification in hunks3of each commit.
we use pydriller to extract the corresponding code changes and commit messages.
for the purpose of high quality dataset we set a few criteria to filter some extremities according to the suggestion in keeping the code changes made to java files only and filtering those non functional code changes e.g.
comments ignoring large commits which change more than lines of code or change more than files since those commits are likely noise caused by routine maintenance e.g.
copyright updates ignoring changes that do not add any new lines since the szz algorithm has an assumption that defects are introduced by adding new lines.
notice that we remove five projects from the original dataset4 since there have insufficient participants to label the commits in 677the best of both worlds integrating semantic features with expert features for defect prediction and localization esec fse november singapore singapore these projects.
besides we also filter out another two projects with some issues where many commits cannot be found in their corresponding repositories downloaded from github.
finally java projects are used for this study and we name the extension of lltc4j as jit defects4j for easier reference.
the statistical information of the final dataset can be found in table .
as shown in table we analyze two statistical information of our studied dataset commit level and line level.
jit defects4j has a varying number of commits ranging from to and its bug ratio is .
.
.
it also has a varying number of lines of codes ranging from to and its bug ratio is .
.
.
apart from extracting the modified lines i.e.
added lines and deleted lines from commits we also extract the change level defect features presented in table from five dimensions i.e.
diffusion size purpose history and experience as proposed by kamei et al.
with commitguru which are widely used in just in time defect prediction scenario .
experimental settings in this section we first briefly introduce the studied baselines and present the two types of performance measures.
.
baselines to comprehensively evaluate the performance of jit fine with prior work in this paper we totally consider six state of the art approaches i.e.
lapredict deeper deepjit cc2vec yan et al.
s work and jitline .
as presented in table lapredict aims at building a defect prediction model by leveraging the information of lines of code added expert feature with the traditional logistic regression classifier.
deeper focuses on building a defect prediction model by learning expert features with the deep belief networks.
deepjit is to build a defect prediction model with convolutional neural networks by learning the semantic features from the commit message and corresponding code changes.
cc2vec relies on the hierarchical attention network to learn semantic features for the defect prediction.
these works only consider the expert features or semantic features to address the jit defect prediction task.
yan et al.
explore the expert features to build a defect prediction model with the logistic regression and building a defect localization model with n gram technique respectively.
while jitline leverages the combination of expert table six baselines used in the comparison.
baselinesfeatures model jit tasksvenueef sf tf tm dlm dp dl lapredict issta deeper qrs deepjit msr cc2vec icse yan et al.
tse jitline msr jit fine this work ef expert feature sf semantic feature tf token feature tm traditional machine learning model and dlm deep learning model.
means that the related technique address the jit dp and jit dl tasks with two independently ways.features and token features to build a defect prediction model with random forest classifier and leverages the token features to build a defect localization model with the lime model.
different from the existing work jit fine aims at building a unified model for defect prediction and localization by exploring the best of the semantic features and expert features.
.
evaluation measures to evaluate the effectiveness of jit fine we adopt two types of widely used performance measures effort agnostic performance measures and effort aware performance measures.
effort agnostic performance measures.
effort agnostic performance measures evaluate the prediction performance without considering cost which means sqa team has enough resources to inspect all potential defective lines of code.
this group considers two widely used performance measures f1score and auc .
there are four possible prediction results for a commit in the testing dataset a commit can be predicted as defective one when it is truly defective true positive tp it can be predicted as defective one when it is actually clean false positive fp it can be predicted as clean one when it is actually defective false negative fn or it can be predicted as clean one and it is truly clean true negative tn .
therefore based on the four possible results f1 score can be defined as follows f1 score is a harmonic mean of precision tp tp fpandrecall tp tp fn.
it is computed as f1 score precision recall precision recall.
it is often used as a summary measure to evaluate if an increase in precision outweighs a reduction in recall and vice versa.
auc represents the area under the receiver operating characteristic roc curve which is a 2d illustration of true positive rate tpr on the y axis versus false positive rate fpr on the x axis.
roc curve is generated by varying the classification threshold over all possible values which can separates clean and buggy predictions.
auc ranges from to and a good prediction model can obtain an auc value close to .
the roc analysis is robust especially for imbalanced class distributions and asymmetric misclassification costs.
it also represents the probability that a model ranks a randomly chosen defective instance higher than a randomly chosen clean one.
effort aware performance measures.
effort aware performance measures evaluate the prediction performance by considering the given cost threshold e.g.
a certain number of lines of code to inspect.
such type of performance measures are extremely important especially when sqa team has limited resources to inspect potential defective lines of code.
developers want to discover as many defects as possible by manually inspecting the top percentages of lines that are likely to be defective.
similar to prior work we use the of total lines of code as the proxy of inspection effort.
this group totally considers seven widely used performance measures which can be sub divided into two groups performance measures for identifying defect prone commits and performance measures for localizing defects in commits.
in this former group three performance measures i.e.
recall effort effort recall and p opt are considered while four performance 678esec fse november singapore singapore chao ni wei wang kaiwen yang xin xia kui liu and david lo measures i.e.
top n recall effort line effort recall line and ifaline are considered in the latter group.
group performance measures for identifying defect prone commits.
recall effort r e measures a proportion between the actual number of defect introducing commits found with given inspection effort and the total number of commits.
in this paper similar to prior work we use the line of code loc as the proxy of inspection effort i.e.
the loc of the whole project.
a high value of r e means more actual defect introducing commits are ranked at the top of list to be inspected.
therefore developers will find more actual defect introducing commits with less effort.
effort recall e r measures the amount of effort that developers have to spend when actual defect introducing commits in testing dataset are found.
a low value of e r means developers will find the actual defect introducing commits with less effort.
popt is on the basic of the concept of the alberg diagram which indicates the relationship between the recall obtained by a prediction model and the inspection effort for a specific prediction model.
to compute this measure two additional prediction models are required the optimal one and the worst one.
in the optimal model and the worst model commits are sorted in decreasing and ascending order by defect densities respectively.
a good prediction model is expected to perform better than the random one and approximate the optimal one.
for a given prediction model m the poptcan be calculated as area optimal area m area optimal area worst where area m represents the area under the curve corresponding to the modelm.
in this paper the defect density of commit is estimated asy m loc c .
group performance measures for localizing defects in commit.
top n accuracy measures the proportion of actual defective lines that are ranked in the top n n in our study ranking.
in general developers need to inspect all modified lines for a given commit.
however it is not a ideal situation especially for limited sqa resources.
therefore a high top n accuracy means more actual defective lines are ranked at the top.
recall effort line r e line measures the proportion of defective lines which can be found i.e.
correctly predicted with a given effort i.e.
the top loc of modified lines of a given defect introducing commit.
a high value of r e linemeans more actual defective lines can be ranked at the top.
effort recall line e r line measures the percentage of effort that developers spend to find the actual defective lines for a given defect introducing commit.
a low value of e r line means the developers can find the actual defective lines with a little amount of effort.
initial false alarm lines ifa line measures the number of clean lines before developers find the first actual defective line for a given commit.
a low value of ifa linemeans that developers can find the first actual defective line by inspecting a few number of clean lines.
experimental results to investigate the feasibility of jit fine with integrated features on the just in time defect prediction and localization our experiments focus on the following three research questions rq .
to what extent just in time defect prediction performance can jit fine achieve?
rq .
how do the integrated semantic and expert features affect the performance of jit defect prediction models?
rq .
can jit fine with the unified model be used to proceed just in time defect localization accurately?
.
just in time defect prediction objective various advanced approaches have been proposed for jit defect prediction which either uses expert change level features e.g.
change level features or uses semantic features e.g.
learned with deep learning techniques to build a prediction model in various settings i.e.
effort aware setting or effort agnostic setting .
jit fine integrates expert features and semantic features for the just in time defect prediction.
therefore we investigate to what extent the prediction performance can jit fine achieve with the integrated features.
experiment design we totally consider six state of the art baselines lapredict yan et al.
deepjit cc2vec deeper and jitline .
besides to comprehensively compare the performance among baselines and jit fine we consider five widely used performance measures from two types effort agnostic ones and effort aware ones.
considering the heavy impact of time as proposed by mcintosh and kamei we follow the same timeaware strategy to build the training data and testing data from the dataset as prior work does .
specifically for each project we first sort all commits by their timestamp in ascending.
then the top of commits in each project are treated as training data while the rest of commits in each project are treated as test data.
we also keep the distribution as same as the original ones in training and testing data.
finally the training data from each project are combined into the target training data so does the testing data.
as for cc2vec we also retrain the model without any information from the testing dataset according to pornprasit et al.
s suggestion.
results the evaluation results are reported in table .
the best performances are also highlighted in bold.
according to the results we find that our approach jit fine outperforms all baselines table defect prediction results of jit fine compared against six baselines.
methods f1 score auc r e e r popt lapredict .
.
.
.
.
yan et al.
.
.
.
.
.
deeper .
.
.
.
.
deepjit .
.
.
.
.
cc2vec .
.
.
.
.
jitline .
.
.
.
.
jit fine .
.
.
.
.
r e recall effort e r effort recall.
the outperforming results are highlighted in bold.
indicates the smaller the better indicates the larger the better the same as tables and .
679the best of both worlds integrating semantic features with expert features for defect prediction and localization esec fse november singapore singapore methods on all performance measures.
in particular as for effortagnostic performance measures jit fine obtains .
and .
in terms of f1 score and auc which improves baselines by and by in terms of f1 score and auc respectively.
as for effort aware performance measures jit fine obtains .
.
and .
in terms of recall effort effort recall and popt which improves baselines by and in terms of recall effort effort recall and p opt respectively.
besides by comparing the performance of lapredict with other baselines we find that building a simple supervised model cannot better capture the characteristics of defect inducing commits.
moreover we can also observe that the prediction models from the top one lapredict to the bottom one jit fine can achieve better performance as they use more and more information and build with a more and more complex model in most cases.
rq jit fine outperforms the state of the art baselines on the just in time defect prediction especially achieving the overwhelming results at f1 score.
it indicates that the unified model learning the integrated semantic and expert features can achieve better performance on just in time defect prediction than the independent models with single feature.
.
effectiveness of integrated features objective expert features and semantic features are extracted considering different aspects of code change commits.
the expert features carry the characteristics of commits based on expert professional knowledge and experience while semantic features present the intrinsic characteristics of code change from their semantic and syntactic structural contexts that are often pre trained on largescale source code datasets.
therefore in this research question we explore how the integrated semantic and expert features affect the effectiveness of just in time defect prediction.
experiment design we set three training scenarios i.e.
semantic features expert features and their integration to train jit fine for assessing the effectiveness of integrated features on boosting defect prediction.
the experimental dataset is set the same as the experiment of rq i.e.
for training and for testing .
moreover we also consider two types of performance measures i.e.
effortagnostic and effort aware for comprehensively studying the impact of each individual type of feature.
in addition jitline utilizes the two types of features i.e.
expert features and token features where the token features can be considered as a kind of coarse grained semantic features as jitline leverages the bag of words technology to extract the semantic representative information.
we thus consider jitline as a baseline for this research question.
table comparing results on defect prediction with different features.
methods setting f1 score auc r e e r popt jitlineef .
.
.
.
.
sft .
.
.
.
.
ef sft .
.
.
.
.
jit fineef .
.
.
.
.
sf .
.
.
.
.
ef sf .
.
.
.
.927results the comparison results are reported in table and the best performances are highlighted in bold for each approach on three different settings ef using expert features only sf using semantic features only and ef sf combining expert features and semantic features .
according to the results we can obtain the following observations both expert features and semantic features have their own advantages in building a prediction model.
semantic features seem to have a better understanding of code characteristics than expert features in the domain of defect prediction.
jit fine can better utilize the advantages from both expert features and semantic features to build a performance better prediction model on both effort agnostic and effort aware settings.
jitline performs better on effort agnostic settings while performing badly on effort aware settings when combining expert features and semantic features.
complex models such as deep learning models have a better ability to learn knowledge from input information than relatively simple traditional machine learning models if they have the identical input information when we compare jitline and jit fine on ef and sf settings independently.
rq semantic features and expert features present their own advantages in identifying defect inducing commits.
integrating the best of them can help jit fine and jitline achieve better performance on defect prediction than the model fed with single features.
.
just in time defect localization objective localizing the defect that exists in modified codes can help developers better understand these issues and help developers faster address defects with fewer efforts.
however there exists a few machine learning approaches for fine grained jit defect prediction at the line level .
different from jitline and yan et al s work jit fine is to build a unified model for addressing defect prediction and defect localization simultaneously.
in this research question we thus investigate whether jit fine with the unified model can be used to proceed just in time defect localization accurately.
experiment design to evaluate the effectiveness of jitline ngram and jit fine we firstly need to label the high quality linelevel ground truth dataset.
low quality datasets especially built on tangled datasets can heavily affect the evaluation of different approaches performance on defect localization.
therefore as introduced in section we start from the line level manually labeled dataset.
we clone all git repositories of the studied projects and use pydriller toolkit to identify the defect introducing commits since our dataset has manually validated the bug fixing commit and their specific bug fixing lines of code.
different from previous work our work has exact labels of lines of code in bug fixing commit i.e.
manually labeling and we can finely identify the buginducing lines of code using gitblame which extremely decreases the noise.
therefore according to prior work those deleted lines in defect fixing commits are labeled as defective ones otherwise they are labeled as clean ones.
those deleted lines are further used to identify the bug inducing commits.
as for localizing the defective lines jitline first uses lime technology to generate synthetic instances and builds a local sparse 680esec fse november singapore singapore chao ni wei wang kaiwen yang xin xia kui liu and david lo linear regression model to identify the importance of each feature i.e.
token in bag of words features to the model decision.
once the importance score of each token is computed jitline generates the ranking of defect prone lines by summarizing the importance score for all tokens that appear in that line.
as for yan s approach they directly build a source code language model and train it on clean source code lines i.e.
a clean model by using the n gram model.
different from the two approaches jit fine directly utilizes the weights of each token in our fine tuned codebert model to calculate its impact on classification.
that is jitline n gram locates defective lines of code after the defect prediction model building while jit fine locates defective lines of codes during the process of the defect prediction model building.
result the evaluation results are presented in table and the best performances are highlighted in bold.
according to the results we find that our approach jit fine performs best on all performance measures.
in particular jit fine achieves .
.
.
.
and .
in terms of top top recall effort line effort recall lineand ifaline respectively which improves jitline and n gram by and by and by and by and by and in terms of of top top recall effort line effort recall lineand ifaline respectively.
the results also indicate the advantages of a unified model for defect prediction and defect localization.
table defect localization results of jit fine compared against jitline and yan et al.
s work.
methodsaccuracy r el e rl ifal top top jitline .
.
.
.
.
yan et al.
.
.
.
.
.
jit fine .
.
.
.
.
r el recall effort line e rl effort recall line ifal ifaline.
rq jit fine achieves the better performance on localizing defects compared with state of the art approaches which indicates that building a unified model with the integrated features can benefit both defect prediction task and defect localization task.
threats to validity threats to internal validity mainly correspond to the potential mistakes in our implementation of our approach and other baselines.
to minimize such threat we not only implement these approaches by pair programming but also directly use the original source code from the github repositories shared by corresponding authors.
besides we use the same hyperparameters in the original papers.
the authors also carefully review the experimental scripts to ensure their correctness.
threats to external validity mainly correspond to the studied dataset.
even we have tested our model on the so far largest finegained evaluation in the literature to ensure a fair comparison with baselines the project diversity is also limited from three aspects.
the first one is the programming language used in studied projects which is developed java programming language.
however projectsdeveloped by other popular programming languages e.g.
c c and python have not been considered.
the second one is the studied projects are open source projects the performance of jit fine on commercial projects is unknown.
thus more diverse commit level datasets can be explored in future work.
the last one is that in reality not all bugs can be identified in the code repository.
though herbold et al.
manually labeled their dataset they still may miss a few bug fix commits.
threats to construct validity mainly correspond to the performance metrics in our evaluations.
to minimize such threat we consider a few types of metrics for different comparison tasks.
in particular we generally consider two kinds of performance metrics i.e.
effort agnostic and effort aware with tens of performance metrics i.e.
f1 score auc p opt .
related work jit defect prediction has attracted extensive attention of researchers in recent years since it can identify defect inducing commit at a fine grained level at check in time.
mockus and weiss firstly extracted historical information i.e.
the number of touched subsystems the number of modified files the number of added lines of code and the number of modification requests in commits to build a classifier to predict the risk of new commits.
kamei et al.
then proposed change level features and used them to build an effort aware jit prediction model.
the change level features are widely used in the following studies.
yang et al.
subsequently proposed two approaches.
in particular yang et al.
firstly used deep belief network dbn to extract higher level information from the initial change level features.
then yang et al.
combined decision tree and ensemble learning to build an ensemble learning model for jit defect prediction.
to further improve yang et al s model young et al.
proposed a new deep ensemble method by using arbitrary classifiers in the ensemble and optimizing the weights of the classifiers.
later liu et al.
proposed a new unsupervised approach named code churn and evaluated it in effort aware settings.
following that chen et al.
treated the effort aware jit defect prediction task as a multi objective optimization problem and consequently a set of effective features are selected to build the prediction model.
mcintosh et al.
investigated the impact of systems evolution on jit defect prediction models via a longitudinal case study of changes from the rapidly evolving qt and openstack systems.
they found that the interval between training periods and testing periods has side effects on the performance of jit models and jit models should be trained using six months or more of historical data.
besides wan et al.
discussed the drawbacks of existing defect prediction tools and highlighted future research directions through literature review and a survey of practitioners.
moreover cabral et al.
utilized a new sampling technology to address the issues of verification latency and class imbalance evolution in online jit defect prediction setting.
recently hoang et al.
proposed two new approaches which use a modern deep learning model to learn the representation of commit message and code changes.
apart from the above approaches researchers also conduct studies on fine grained prediction and focus more attention on where the defect exists.
pascarella et al.
proposed a fine grained jit 681the best of both worlds integrating semantic features with expert features for defect prediction and localization esec fse november singapore singapore defect prediction model based on handcrafted features to prioritize which changed files in a commit are the riskiest ones.
yan et al.
proposed a two phase approach for defect identification and defect localization.
in particular they firstly trained a prediction model on software metrics to identify which commits are the most risky ones then they trained the n gram model on textual features which is subsequently used to locate the riskiest lines.
in addition wattanakriengkrai et al.
stated that a machine learning approach can achieve better performance than the n gram approach.
however these work mainly focused on file level defect localization.
recently chanathip et al.
proposed a new approach jitline which is a machine learning based jit defect approach for predicting defect introducing commits and for localizing defective lines that are related to defective commit.
prior studies either focus on defect prediction or focus on defect localization using only expert features or only semantic features .
a few studies try to address the two tasks simultaneously with two sub approaches or with two sub phases .
different from prior work in this paper we focus on building a unified model using both expert features and semantic features for addressing both defect prediction and defect localization.
conclusion and future work in this paper we propose a unified approach jit fine which fully utilizes both expert features and contextual semantic features of modified source code to build a performance better model for justin time defect prediction and just in time defect localization simultaneously.
we also build a large scale line level labeled dataset jit defects4j for both jit dp and jit dl research.
to investigate the effectiveness of jit fine we make a comprehensive comparison with six baselines on ten performance measures.
the results empirically demonstrate the value of integrating the expert features and semantic features for two kinds of just in time software quality assurance.
our future work involves extending our evaluation by considering more open source and commercial projects developed in other programming languages e.g.
c c python etc.
.
we also plan to implement jit fine into a tool e.g.
a github plugin to assess its usefulness in practice.