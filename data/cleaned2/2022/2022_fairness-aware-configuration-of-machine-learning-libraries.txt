fairness aware configuration of machine learning libraries saeid tizpaz niari saeid utep.edu university of texas at el pasoashish kumar azk640 psu.edu pennsylvania state university gang tan gtan psu.edu pennsylvania state universityashutosh trivedi ashutosh.trivedi colorado.edu university of colorado boulder abstract thispaperinvestigatestheparameterspaceofmachinelearning ml algorithmsinaggravatingormitigatingfairnessbugs.datadrivensoftwareisincreasinglyappliedinsocial criticalapplications where ensuring fairness is of paramount importance.
the existing approaches focus on addressing fairness bugs by either modifying the input dataset or modifying the learning algorithms.
ontheotherhand theselectionofhyperparameters whichprovidefiner controls of ml algorithms may enable a less intrusive ap proachtoinfluencethefairness.
canhyperparametersamplifyor suppress discrimination present in the input dataset?
how can we helpprogrammersindetecting understanding andexploitingtherole of hyperparameters to improve the fairness?
we design three search based software testing algorithms to uncover the precision fairness frontier of the hyperparameter space.
we complement these algorithms with statistical debugging to explain the role of these parameters in improving fairness.
we implementtheproposedapproachesinthetoolparfait ml parameter fairnesstestingformllibraries andshowitseffectivenessand utility over five mature ml algorithms as used in six social critical applications.
in these applications our approach successfully identified hyperparameters that significantly improve vis a vis the state of the arttechniques thefairnesswithoutsacrificingprecision.
surprisingly for some algorithms e.g.
random forest ourapproach showed that certain configuration of hyperparameters e.g.
restrictingthesearchspaceofattributes canamplifybiases across applications.
upon further investigation we found intuitive explanations of these phenomena and the results corroborate similar observations from the literature.
acm reference format saeidtizpaz niari ashishkumar gangtan andashutoshtrivedi.
.
fairness aware configuration of machine learning libraries.
in 44th international conference on software engineering icse may pittsburgh pa usa.
acm new york ny usa pages.
.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation onthe firstpage.copyrights forcomponentsof thisworkowned byothersthan the author s mustbehonored.abstractingwithcreditispermitted.tocopyotherwise or republish topostonserversortoredistributetolists requirespriorspecificpermission and or a fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn ... .
introduction data drivensoftwareapplicationsareanintegralpartofmodern life impacting every aspect of societal structure ranging from educationandhealthcaretocriminaljusticeandfinance .since thesealgorithms learnfrom priorexperiences itisnot surprising that they encode historical and present biases due to displacement exclusion segregation andinjustice.theresultingsoftwaremay particularlydisadvantageminoritiesandprotectedgroups1andbe found non compliant with law such as the us civil rights act .
therefore helping programmers detect and mitigate fairness bugs insocial criticaldata drivensoftwaresystemsiscrucialtoensure inclusion in our modern increasingly digital society.
the software engineering se community has invested substantial efforts to improve the fairness of ml software .
fairnesshasbeentreatedasacriticalmeta propertiesthatrequires an analysis beyond functional correctness and measurements such aspredictionaccuracy .however themajorityofpreviouswork withinthesecommunityevaluatesfairnessonthe mlmodels after training while the programmer supports to improve fairness during the inference of models i.e.
training process is largely lacking.
the role of training processes in amplifying or suppressing vulnerabilities and bugs in the input dataset is well documented .
the training process typically involves tuning of hyperparameters variables that characterize the hypothesis space of ml models and define a trade off between complexity and performance.
some prominentexamplesofhyperparametersinclude l1vs.l2lossfunction in support vector machines the maximum depth of a decision tree and the number of layers neurons in deep neural networks.
hyperparameters arecrucially differentfrom theml modelparametersinthattheycannotbelearnedfromtheinputdatasetalone.
in this paper we investigate the impact of hyperparameters on ml fairness and propose a programmer support system to develop fair data driven software.
we pose the following research questions to what extent can hyperparameters influence the biases present in the input dataset?
can we assist ml library developers in identifying and explaining fairness bugs in the hyperparameter space?
can we help ml users to exploit the hyperparamters to imporive fairness?
wepresentparfait ml parameterfairnesstestingformllibraries asearch basedtestingandstatisticaldebuggingframework 1awallstreetjournalarticleshoweddeloitte alife insuranceriskassessmentsoftware can discriminate based on the protected health status of applicants .
fico a credit risk assessment software is found to predict higher risks for black nondefaulters than white asian ones.
compas risk assessment software in criminal justice is shown to predict higher risks for black defendants .
ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa saeid tizpaz niari ashish kumar gang tan and ashutosh trivedi thatsupportsmllibrarydevelopersanduserstodetect understand and exploit configurations of hyperparameters to improve ml fairness without impacting functionality.
we design and implement three dynamic search algorithms independently random black box evolutionary and gray boxevolutionary tofindconfigurationsthat simultaneously maximize and minimize group based fairness with aconstraintonthepredictionaccuracy.then weleveragestatistical learning methods to explain what hyperparameters distinguishlow biasmodelsfromhigh biasones.suchexplanatory modelsspecificallyaidmllibrarymaintainerstolocalizeafairness bug.
finally we show that parfait ml can effectively vis a vis thestate of the arttechniques aidmluserstofindaconfiguration that mitigates bias without degrading the prediction accuracy.
weevaluateourapproachonfivewell establishedmachinelearningalgorithmsoversixfairness sensitivetrainingtasks.ourresults show that for some algorithms there are hyperparameters that consistently impact fairness across different training tasks.
for example max feature parameterinrandomforestcanaggravatethe biasesforsomeofitsvaluessuchas log2 num.features beyond a specific training task.
these observations corroborate similar empirical observations made in the literature .
in summary the key contributions of this paper are thefirstapproachtosupport mllibrarymaintainers tounderstandthefairnessimplicationsofalgorithmicconfigurations three search based algorithms to approximate the pareto curve of hyperparameters against the fairness and accuracy astatisticaldebugging approachtolocalizeparametersthat systematically influence fairness in five popular and wellestablish ml algorithms over six fairness critical datasets amitigation approach to effectively find configurations that reduce the biases vis a vis the state of the art and an implementation of parfait ml parameter fairness testing for ml libraries and its experimental evaluation on multiple applications available at tizpaz parfait ml.
background fairness terminology and measures.
let us first recall some fairnessvocabulary.weconsider binaryclassification taskswhere aclasslabelis favorable ifitgivesabenefittoanindividualsuch as low credit risk for loan applications default low reoffend risk for parole assessments recidivism and high qualification score for job hiring.
each dataset consists of a number of attributes such as income employment status previous arrests sex and race and a set ofinstances that describe the value of attributes for each individual.
we assume that each attribute is labeled as protected ornon protected.
according toethical and legal requirements ml software should not discriminate on the basis of an individual s protected attributes such as sex race age disability colour creed national origin religion genetic information marital status and sexual orientation.
thereareseveralwell motivatedcharacterizationsoffairness.
fairness through unawareness ftu requires masking protected attributes during training.
however ftuis not effective sinceprotectedandnon protectedattributesoftencorrelate e.g.
zip code andrace and biases are introduced from non protected figure data driven software system developments attributes.
fairness through awareness fta i sa nindividualfairness notionthatrequiresthattwo individuals withsimilar non protected attributes are treated equally.
groupfairness requiresthestatisticsofmloutcomesfordifferentprotectedgroups tobesimilar .therearemultiplemetrics to measure group fairness in ml software.
among them equal opportunitydifference eod measuresthedifferencebetweenthe truepositiverates tpr oftwoprotectedgroups.similarly average odd difference aod is the average of differences between the true positiverates tpr andthe falsepositiverates fpr oftwoprotected groups .
these metrics can naturally be generalized tohandlesituationswhereprotectedattributesmayhavemorethan two values.
for instance if race is a protected attribute then the eod is the maximum eod among any two race groups.
this paper focuses on group fairness.
data driven software systems.
data drivensoftwareisdistinguished from common software in that they largely learn their decision logic from datasets.
consequently while the traditional softwaredevelopersexplicitlyencodedecisionlogicviacontroland data structures the ml programmers and users provide input data perform some pre processing choose ml algorithms and tune hyperparameters to enable data driven systems to infer a model that encodes the decision logic.
figure1showsthekeycomponentsofadata drivensystem.ata high level adata drivensystemconsistsofthreemajorcomponents inputdata alearning training process andalibraryframework.
themlusersoftenprovideinputdataandbuildanmlmodelusing a programming interface.
the interface interacts with the core ml library e.g.
scikit learn tensorflow etc andconstructsdifferent instances of learning algorithms usinghyperparameters.
then they feed the training data into the constructed learning objects to infer the parameters of an ml model.
as a part of the training process ml users query the ml model withthevalidationset toevaluatefunctionalmetricssuchaspredictionaccuracyandnon functionalmetricssuchaseodforgroup fairness.
at the heart of the learning process tuning hyperparametersisparticularlychallengingsincetheycannotbeestimatedfrom the input data and there is no analytical formula to calculate anappropriatevalue .wedistinguish algorithmparameters i.e.
hyperparameters suchastolerance ofoptimization insvms maximumfeaturestosearchinrandomforest andminimumsamples inleafnodesofdecisiontreesthatsetbeforetrainingfrom model parameters that are inferred automatically after training such as thesplitfeatureofdecisiontreenodes theweightsofneuronsin neural networks and coefficients of support vector machines.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fairness aware configuration of machine learning libraries icse may pittsburgh pa usa related work.
evaluating fairness of ml models.
themis presents a causal testing approach to measure group discrimination on the basis of protected attributes.
particularly they measure the difference between the fairness metric of two subgroups by counterfactual queries i.e.
they sample inputs where the protected attributesareaandcomparethefairnesstoacounterfactualscenario where the protected attributes are set to b. agarwal et al.
present a black box testing technique to detect individual discrimination two people with similar features other than protected ones receive different ml predictions.
they approximate ml models withdecisiontreesandusesymbolicexecutiontechniquesoverthe tree structure to find discriminatory instances.
udeshi et al.
present a two step technique that first uniformly and randomly searchtheinputdataspacetofindadiscriminatoryinstanceand then locally perturb those instances to further generate biased test cases.
adversarial discrimination finder is an adversarial training method to generate individual discrimination instances in deep neural networks.
these works focus on testing individual ml mod elsandimprovingtheirfairness.wefocusonmllibrariesandstudy how algorithm configurations impact fairness.
inprocessingmethodsforbiasreduction.
abodyofworkconsiders inprocessalgorithmstomitigatebiasesinmlpredictions.adversarialdebiasing isatechniquebasedonadversariallearningto infer a classifier that maximizes the prediction accuracy and simultaneouslyminimizesadversaries capabilitiestoguesstheprotected attributefromthemlpredictions.prejudiceremover addsa fairness aware regularization term to the learning objective and minimizestheaccuracyandfairnessloss.thislineofworkrequiresthemodificationoflearningalgorithmseitherinthelossfunctionor the parameter of ml models.
exponentiated gradient is a metalearning algorithm to mitigate biases.
the approach infers a family of classifiers to maximize prediction accuracy subject to fairnessconstraints.
since this approach assumes black box access to the learningalgorithms weevaluatetheeffectivenessof parfait ml inmitigating biases with this baseline see subsection .
.
combiningpre processingandinprocessingbiasreductions.
fairway usesatwostepmitigationapproach.whilepre processing thedatasetisdividedintoprivilegedandunprivilegedgroupswhere the respective ml models train independently from one another.
then they compare the prediction outcomes for the same instance tofindandremovediscriminatorysamples.giventhepre processed dataset the inprocess step uses a multi objective optimization flash to find an algorithm configuration that maximizes bothaccuracyandfairness.theworkfocusesonusinghyperparam eterstomitigatebiasesinasubsetofhyperparametersandalimited number of algorithms.
in particular they require a careful selectionofrelevanthyperparameters.ourapproach however doesnotrequire a manual selection of hyperparameters.
instead our experiments show that the evolutionary search is effective in identifying and exploiting fairness relevant hyperparameters automatically.
in addition ourapproach explainswhathyperparametersinfluence fairness.
such explanatory models can also pinpoint whether some configurations systematically influence fairness which can be usefulforfairwaytocarefullyselectasubsetofhyperparametersinitssearch.toshowtheeffectivenessof parfait mlinreducingbiases we compare our approach to fairway see subsection .
.
overview we use the example of random forest ensemble to overview howparfait mlassistsmldevelopersanduserstodiscover explain and mitigate fairness bugs by tuning the hyperparameters.
dataset.
adultcensusincome isabinaryclassificationdataset that predicts whether an individual has an income over ka year.
thedatasethas48 842instancesand14attributes.forthisoverview we start by considering sexas the protected attribute.
learningalgorithm.
randomforestensembleisametaestimator thatfitsanumberofdecisiontreesandusestheaveragedoutcomes of trees to predict labels.
the ensemble method includes parameters.
three parameters are boolean two are categorical six are integer and seven are real variables.
examples of these parameters are the maximum depth of the tree the number of estimator trees and the minimum impurity to split a node further.fairnessandaccuracycriterion.
werandomlydividethedataset into4 foldsanduse75 ofthedatasetasthetrainingsetand25 as the validation set.
we measure both accuracy and fairness metrics after training on ml models using the validation set.
we report the average odd difference aod as well as the equal opportunity difference eod which were introduced in background section .
our accuracy metric is standard the fraction of correct predictions.
test cases.
our approach has three options for generating test cases independently random black box mutations and gray box mutations.
in this section we use the black box mutations see rq2 in section .
.
we run the experiment times each for hours thedefaultnumberofrepetitionandtime out .weobtain an average of valid test cases over runs.
since the default parametersofrandomforestsachieveanaccuracyof84 avalid test case achieves similar or better accuracy.
to allow finding a fair configuration in cases where the default configurations are the most accuratemodel we tolerate accuracy degradations.the overall accuracy ofml models over the entirecorpus of test cases variesfrom83 to85 .
.eachtestcaseincludesthevaluationof algorithm parameters accuracy aod andeod.
magnitude of biases.
we report the magnitude of group biases in the hyperparameter space of random forests.
the minimum and maximum aodare .
.
and .
.
respectively.
the values are the average of runs with confidence interval reported in the parenthesis and higher values indicatestronger biases.
similarly the minimum and maximum eodare .
.
and32.
.
.theseresultsshowthatwithin .
accuracymargins therecanbeover13 and27 difference inaodandeod respectively.theseresultsindicatethatdifferent configurationsofrandomforestscanindeedamplifyorsuppress the biases from the input dataset.
the details of relevant experimentsfordifferentdatasetsandlearningalgorithmsarereported in rq1 section .
.
explanationof biases.
our next goal is to explain what configurations of hyperparameters influence group based fairness.
clustering.
we first partition generated test cases in the domain of fairness aod versus accuracy.
particularly we apply the spectral clusteringalgorithmwherethenumberofpartitionsaresettothree.
figure a shows the three clusters identified from the generated authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa saeid tizpaz niari ashish kumar gang tan and ashutosh trivedi figure2 a testcasesfor censuswithsexareclusteredintothree y axisisthe aodbiasandx axisistheaccuracy b thetree classifier explains that the max features and the min weight fraction leaf discriminate the three clusters c two clusters forcensuswithrace d the classifier for censuswithraceshows a similar explanation to censuswithsex.
test cases.
looking into the figure we see that green and orange clusters have similar accuracy however they have significantly differentbiases aod .additionally theblueclusterachievesbetter accuracy with a similar aodto the green cluster.
tree classifiers.
next we use cart tree classifiers to explain the differences between the clusters in terms of algorithm parameters as shown in figure b .
each node in the tree shows a splitparameter the number of samples reaching the node and sam ple distributions in different clusters.
first let us understand thedifferences between the green and orange clusters.
the decisiontree shows that the max feature parameter distinguishes these two clusters.
while the values auto and none do not restrict the number of features during training sqrt and log2 randomly choose a subset of features according to the square root and thebase logarithm of total features during training.
this explana tion validates findings from zhang and harman where they reported that restricting the number of features strengthens the biases in ml models.
another localized parameter is the minimum sampletostopthegrowthandfitleafmodels.thisdistinguishes theorangeclusterfromthetwootherclusters.intuitively underprivileged groups tend to have less representation in the dataset.
since random forests assign predictions to the majority class in the leaves theytendtofavorprivilegedgroupswhenathresholdon the minimum samples is set.
featuretransferability.
inanotherexperiment weconsiderthe race attribute as the protected using the censusdataset.
figure c shows the inputs generated for the race feature is clustered into twogroups.theexplanationtreeinfigure2 d showsthatthemax featureandminimumsamplesinleafsaretwoparametersinthe treeregressorthatexplainthedifferenceinthe aodbiases similar to the case when sexis the protected attribute.
dataset transferability.
we also study other datasets in ml fairness literature including the german credit data credit see section6.
.fortherandomforest ourfindingsarestableacross different datasets and protected attributes the minimum sample weightsofleafnodesandmaximumfeaturesarethemostimportant parameters to distinguish configurations with high and low biases.
theseresultscanhelpmldevelopersunderstandthefairnessimplications of different configuration options in their libraries.
mitigation technique.
as we discussed previously parfait ml isalsousefultosuppressbiasesbypickinglow biashyperparameters.
the details of experiments to show the mitigation aspectof parfait ml can be found in section .
.
for random forests parfait ml can mitigate the biases from an eodof .
to .
withevenbetteraccuracycomparedtothedefaultconfigurationas abaseline.therearecasesthatparfait mlalonecannotreduce biasesinastatisticallysignificantway.insuchcases wefoundthat combing parfait ml with existing approaches can significantly reduce biases under certain conditions see rq4 in section .
.
problem definition the primary performance criteria for data driven software is accuracy.
however the presence of fairness results in a multi objective optimizationproblem.weproposeasearch basedsolutiontoapproximatethecurveofpareto dominanthyperparameterconfigurationsandastatisticallearningmethodtosuccinctlyexplainwhat hyperparameter distinguish high fairness from low fairness.
the ml paradigm.
data driven software systems often deploy mature off the shelf ml libraries to learn various models from data.
we can abstractly view a learning problem as the problem of identifyingamapping m x yfromaset xofinputstoaset yof outputs by learning from a fixed dataset d xi yi n i 1so thatmgeneralizes well to previously unseen situations.
the application interfaces of these ml libraries expose configuration parameters characterizing the set hof hyperparameters that let the users define the hypothesis class for the learning tasks.
thesehypothesisclassesthemselvesaredefinedoverasetofmodel parameters hbased on the selected hyperparameter h h. the ml programs sift through the given dataset dto learn an optimal value hand thus compute the learning model mh d x yautomatically.
when dand are clear from the context we write mhfor the resulting model.
the fitness of a hyperparameter h his evaluated by computing the accuracy ratio of correctresults ofthe model mhon a validation dataset d .
we denote the accuracy of a model m overd asaccm.
the dataset d is typically distinct from d but assumed to be sampled from the same distribution.
hence the key design challenge for the data driven software engineering is a searchproblem foroptimalconfigurationofthehyperparameters maximizing the accuracy over d .
fairness notion.
to pose fairness requirements over the learning algorithms we assume the access to two predicates.
the predicate x overtheinputvariablescharacterizingtheprotected status of a data point x e.g.
race sex or age .
without loss of authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fairness aware configuration of machine learning libraries icse may pittsburgh pa usa generality weassumethereareonlytwoprotectedgroups agroup with x and a group with x .
we also assume that the predicate y over the output variables characterizes a favorable outcome e.g.
low reoffend risk with y .
givend andm x y we define true positive rate tpr and false positive rate fpr for the protect group i as tprm i barex barex x y d x i m x y barex barex barex barex x y d x i barex barex fprm i barex barex x y d x i m x y barex barex barex barex x y d x i barex barex.
we use two prevalent notions of fairness .
equal opportunity difference eod of magainstd between two groups is eodm barex barextprm tprm barex barex and average odd difference aod is aodm tprm tprm fprm fprm .
letf braceleftbig eodm aodm bracerightbigbe some fixed fairness criterion.
we notice that a high value of fimplies high bias low fairness and a low value implies low bias high fairness .
the key design challenge for social critical data driven softwaresystemsistosearchforfairness bias optimalconfiguration h hof hyperparameters maximizing the accuracy and minimizing the bias fof the resulting model mh.
a hyperparameter h his pareto fairness dominated by hif the model m provides better accuracy and lower bias i.e.
accmh accm andfmh fm .
we say that a hyperparameter h his paretofairness optimal if it is not fairness dominated by any other hyperparameters.similarly we candefine pareto bias domination his pareto bias dominatedby ifaccmh accm andfmh fm andpareto bias optimalhyperparamters.aparetosetisagraphical depiction of all of pareto optimal points.
since we are interested inhyperparametersthatleadtoeitherlowbias highfairness or high bias low fairness our goal is to compute pareto sets for both fairnessandbiasoptimalhyperparameters wecallthissetatwined pareto set.
our goal is to compute a convenient approximation ofthetwinedparetosetthatcanbeusedtoidentify explain and exploit the hyperparameter space to improve fairness.
definition .
hyperparameter discovery and debugging .
givenanmlalgorithmandadatasetwithprotectedandfavorable predicates the hyperparameter discovery problem is to approximatethetwinedparetoset bothfairness optimalandbias optimalpoints .givensuchapproximation thefairnessdebuggingproblem is to explain the difference between the hyperparameter characterizing the high and low fairness with acceptable accuracy.
approach we proposedynamicsearchalgorithms todiscoverhyperparametersthatcharacterizefairness optimalandbias optimalmodelsand statistical debugging to localize what hyperparameters distinguish fair models from biased ones.algorithm parfait ml detectingandexplaining fairness and bias in parameters of ml algorithms.
input algorithm a space of hyperparamters h default configuration hd training dataset xt yt test dataset xt yt protected attribute a type of search st the margin time out t num.
clusters k. output test cases i predicates .
1model path run a hd xt yt st 2pred infer model xt 3accuracy d fairness d metric pred yt a 4i.add hd accuracy d fairness d path 5cur time 6while time cur tdo 7ifst random then h uniformlyrandom h 9else ifst black box orst gray box then h prime choicew i.project h h mutate h prime 12model path run a h xt yt st 13pred infer model xt 14accuracy fairness metric pred yt a 15ifpromising h accuracy fairness path i then i.add h accuracy fairness path 17label spectralclust i.project accuracy fairness k 18 dtclassifier i.project h label hyperparameter discovery problem .thegridsearchisanexhaustive method to approximate the pareto curve however it suffers from the curse of dimensionality.
randomized search may alleviate this curse to some extent however given its blind nature and the large spaceof parameters it may fail to explore interesting regions.
evolutionary algorithms ea guided by the promising input seeds often explore extremeregions of the pareto space and are thus natural candidates for our search problem.
while multiobjectiveeaslookpromising theyarenotoriouslyslow .for example nsga ii hasquadraticcomplexitytopickthenext best candidate form the samples in the population.
instead we propose a single objective ea with accuracy constraints.
algorithm sketches our approach for detecting and explaining strengths of discriminationsin the configuration space of ml libraries.
generalsearchalgorithms.
therandomsearchalgorithmgenerates testinputsuniformlyandindependentlyfromthedomainofparametervariables.theblack boxsearchisanevolutionaryalgorithm that selects andmutates inputs from itspopulation.
the gray box evolutionarysearchusesthesamestrategyastheblack boxsearch but is also guided by the code coverage of libraries internals.
initial seeds.
our approach starts with the default configuration andrunsthelearningalgorithmoverthetrainingdatasettobuild a machine learning model line of algorithm .
if the searchalgorithm is gray box the running of algorithm also returns the pathcharacterizations.apathcharacterizationis xorofhashvalues obtainedfromprogramlinenumbersvisitedintherun.then we use the machine learning model and the validation set to infer the authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa saeid tizpaz niari ashish kumar gang tan and ashutosh trivedi predictions line .
we use the predictions their ground truths and protected attributes to measure the prediction accuracy and the group fairness metrics such as eodandaod line .
inputselections.
weaddthedefaultconfigurationanditsoutcometo the population line and iteratively search to find configurations that minimize and maximize biases given a threshold on the accuracy.in thedomainofconfigurationparameters lines7to8 .otherwise wepickaninputfromthepopulationbasedonaweightedsampling strategy that prefers more recent inputs given a location i the probabilistic weight of sample iis2 i n n wherenis the size of input population assuming a higher location is more recent.
then werandomlychooseaparameterandapplymutationoperations overitscurrentvaluetogenerateanewconfiguration lines9to .
we use standard mutation operations such as increasing decreasing value by a unit.
given the new configuration we perform thetrainingandinferencestepstomeasurethepredictionaccuracy and biases lines to .
search objective.
identifying promisingconfigurations isa critical stepinouralgorithm.weconsiderthecharacteristicsofthenew configurationandcomparethemtothetestcorpus line15 .wesay a configuration is promising if no existing configuration in the test corpuspareto fairness dominateorpareto bias dominate based oneodandaod the new configuration.
thus we add promising inputs to the test corpus line .
if the search type is gray box we also consider the path characterization.
if the path has not been visited before and the corresponding configuration manifests an accuracy equal to or better than the accuracy of the default configuration within .
margin we add the configuration to the population as well.
fairness debugging problem.
with the assumption that the hyperparameter space his given as a finite set of hyperparameter variables h1 h2 ... hm wewishtoexplainthedependence of theseindividual hyperparametervariables towardsfairness for all pareto optimal hyperparameters as approximated in the test corpusi.
our explanatory approach uses clustering in the domain of fairness vs. accuracy to discover kclasses of hyperparameter configurations in the test corpus i line .
then we use standard decisiontreeclassifierstogeneratesuccinctandinterpretablepredicates over the hyperparameter variables line .
the resulting k predicatesserveasanexplanatorymodeltounderstandbiasesin the configuration of learning algorithms.
experiment we first pose research questions.
then we elaborate on the case studies datasets protectedattributes ourtool andenvironment.
finally we carefully examine and answer research questions.
rq1whatisthemagnitudeofbiasesinthe hyperparameter space of ml algorithms?
rq2are mutation based and code coverage based evolutionary algorithms effective to find interesting configurations?
rq3isstatisticaldebuggingusefultoexplainthebiasesinthe hyperparameter spaceofmlalgorithms?aretheseparameters consistent across different fairness applications?rq4is our approach effective to mitigate biases as compared to the state of the art inprocess technique?
all subjects experimental results and our tool are available on our github repository .
subjects we consider ml algorithms from the literature logistic regression lr uses sigmoid functions to map input data toareal valueoutcomebetween0and1.weuseanimplementationfromscikit learn thathas15parametersincludingthree booleans three categoricals four integers four reals and onedictionary.exampleparametersarethenormofpenalization prime vs dual formulation and tolerance of optimization.
random forest rf is an ensemble method that fits a number of decision trees and uses the averaged outcomes for predictions.
we refer to overview section for further information.
support vector machine svm is a classifier that finds hyperplanestoseparateclassesandmaximizesmarginsbetweenthem.
thescikit learnimplementationhas12parametersincludingtwo booleans three categoricals three integers three reals and one dictionary .
examples are tolerance and regularization term.
decision tree dt learns decision logic from input data in the formofif then elsestatements.weuseanimplementationthathas parameters including three categoricals three integers six reals and one dictionary .
example parameters are the minimum samples in the node to split and maximum number of leaf nodes.
discriminantanalysis da fitsdatatoa gaussianpriorofclass labels.
then it uses the posterior distributions to predict the class of new data.
we use an implementation that has parameters includingtwobooleans onecategorical oneinteger fourreals two lists and one function .
wealsoconsiderfourdatasetswithdifferentprotectedattributes anddefinesixtrainingtasksasshownintable1 similartoprior work .
adult census income census german credit data credit bank marketing bank and compas software compas arebinaryclassificationtaskstopredictwhether an individual has income over 50k has a good credit history is likely to subscribe and has a low reoffending risk respectively.
.
technical details our tool has detection and explanation components.
the detection component is equipped with three search algoirthms random black box mutations and gray box coverage.thesearchalgorithms are described in approach section .
we implement these techniques in python where we use the xml parser library to define theparametervariablesandtheirdomainsandtracelibrary to instrument programs for the code coverage .
we implement theclustering usingspectralalgorithm and thetreeclassifier using the cart algorithm in scikit learn .
.
experimental setup we run all the experiments on a super computing machine with the linux red hat os and an intel haswell .
ghz cpu with cores each with gb of ram .
we use python .
and scikitlearnversion0 .
.
.wesetthetimeoutforoursearchalgorithm authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fairness aware configuration of machine learning libraries icse may pittsburgh pa usa table datasets used in our experiments.
dataset instances features protected groups outcome label group1 group2 label label adultcensus48 14sex male sex femalehighincome low incomeincome race white race non white compassoftware7 28sex male sex femaledidnot reoffend reoffendrace caucasian race non caucasian germancredit sex male sex female good credit bad credit bankmarketing age young age old subscriber non subscriber to hours for all experiments unless otherwise specified.
additionally each experiment has been repeated times to account for the randomness of search techniques.
we averaged the results andcalculatedthe95 confidenceintervalstoreportresults.the difference between two means is statistically significant if theirconfidence intervals do not overlap .
we split the dataset into trainingdata andvalidationdata .wetrainanmlmodel with a given learning algorithm its configuration and the training data.
finally we report theaccuracy and fairnessmetrics over the inferred ml model using the validation set.
any configurations thatachievehigheraccuracythanthedefaultconfiguration with margins are valid inputs.
.
magnitude of biases rq1 one crucial research question in this paper is to understand the magnitude of biases when tuning hyperparameters.
table shows the magnitude of biases observed for different learning algorithms over a specific dataset and protected attribute.
we consider the inputs from all search algorithms and report the average as well as confidenceintervals intheparenthesis ofdifferentmetrics.
the column num.inputsshows the number of valid test cases generated from the detection step.the column accuracy ran eshows the range of accuracies observed from all generated configura tions.
the column aodran eshows the range of aodbiases for allconfigurations aodtop minshowsthelowest aodbiasesforinputs withintop ofprediction accuracy aodtop maxshowsthe highest biases for inputs within the top of accuracy.
for the example ofdtwithcensusandrace aodran eshowsthe aodbiasesfor configurationswithin79 .
to84 .
accuracy whereas aodtop min showsthelowestbiaseswithin83 .
to84 .
accuracy.thecolumneodran e eodtop min andeodtop maxshowtherangeofbiases based on equal opportunity difference eod for all valid inputs thelowest eodbiasesforinputswithinthetop1 ofaccuracy and the highest biases for inputs within top of accuracy.
the results show that the configuration of hyperparameters indeed amplifies and suppresses ml biases.
within of top accuracy margins a fairness aware configuration can suppress the group biases to below for aod eod and a poor choice can amplify the biases up to for eodand up to for aod.
answer rq1 tuningofhyperparameterssignificantlyaffects fairness.within1 ofaccuracymargins afairness awareconfiguration can reduce the eod bias to below and a poor choice of configuration can amplify the eod bias to .
.
search algorithms rq2 inthissection wecomparetheresultsofthreesearchalgorithms to understand which method is more effective in finding config urations with low and high biases.
table shows the number of generatedvalid inputsper searchmethod the absolutedifference betweenthemaximum aodandtheminimum aod andtheabsolutedifferencebetweenthemaximum eodandtheminimum eod.
the results show that there are multiple statistically significant differenceamongthethreesearchstrategies.in4cases therandom strategy generates the lowest number of inputs.
in cases the evolutionary algorithms both black box andgray box outperforms the random stratgey in finding configurations that characterize significant eodandaodbiases.
the comparisonbetween black boxand gray boxevolutionary algorithmsshowsthatthereisnostatisticallysignificantdifference between them in generating configurations that lead to the lowest andhighestbiases.weconjecturethatcodecoverageindetecting biases is not particularly useful since the biases are not introduced as a result of mistakes in the code implementation rather they are results of unintentionally choosingpoor configurations of learning algorithms by ml users or allowing poor configurations of algorithms by ml library developers in fairness sensitive applications.
intable3 weobservethatthestatisticallysignificantdifferences arerelevanttothedecisiontree dt .forthealgorithm weprovidethetemporalprogressofthreesearchalgorithmsfordifferenttraining scenarios see supplementary material for the rest .
figure showsthemeanofmaximumbiases soldline andthe95 confidenceintervals filledcolors overthe4hourstestingcampaignsof each search strategy.
there is a statistically significant difference if white spaces are present between the confidence intervals.
answerrq2 our experiments showthat mutation based evolutionary algorithms are more effective in generating configurationsthatcharacterizelowandhighbiasconfigurations.we did not find a statistically significant difference to support using code coverage in fairness testing of learning libraries.
.
statistical learning for explanations rq3 we present a statistical learning approach to explain what configurations distinguish low bias models from high bias ones.
we use clusteringtofinddifferentclassesofbiasesandthecarttreeclassi fierstosynthesizepredicatefunctionsthatexplainwhatparameters arecommoninthesameclusterandwhatparametersdistinguish one cluster from another.
similar techniques have been used for softwareperformancedebugging .welimitthemaximumnumber of clusters to and prefer threeclusters over twoclusters if authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa saeid tizpaz niari ashish kumar gang tan and ashutosh trivedi table the magnitude of biases in the parameters of ml algorithms based on aod and eod.
algorithm dataset protected num.inputs accuracyran eaverage odds difference aod equalopportunity difference eod aodran e aodtop minaodtop max eodran e eodtop mineodtop max lrcensus sex .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
census race .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
credit sex .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
bank age .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
compas sex .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
compas race .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rfcensus sex .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
census race .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
credit sex .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
bank age .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
compas sex .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
compas race .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
svmcensus sex .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
census race .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
credit sex .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
bank age .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
compas sex .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
compas race .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
dtcensus sex .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
census race .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
german sex .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
bank age .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
compas sex .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
compas race .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
dacensus sex .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
census race .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
credit sex .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
bank age .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
compas sex .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
compas race .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table the performance of different search strategies to find biases in ml libraries discrepancies are highlighted by red .
algorithm dataset protectednum.inputs aod.max aod.min eod.max eod.min random black box gray box random black box gray box random black box gray box lrcensus sex .
.
.
.
.
.
.
.
.
.
.
.
census race .
.
.
.
.
.
.
.
.
.
.
.
credit sex .
.
.
.
.
.
.
.
.
.
.
.
bank age .
.
.
.
.
.
.
.
.
.
.
.
compas sex .
.
.
.
.
.
.
.
.
.
.
.
compas race .
.
.
.
.
.
.
.
.
.
.
.
rfcensus sex .
.
.
.
.
.
.
.
.
.
.
.
census race .
.
.
.
.
.
.
.
.
.
.
.
credit sex .
.
.
.
.
.
.
.
.
.
.
.
bank age .
.
.
.
.
.
.
.
.
.
.
.
compas sex .
.
.
.
.
.
.
.
.
.
.
.
compas race .
.
.
.
.
.
.
.
.
.
.
.
svmcensus sex .
.
.
.
.
.
.
.
.
.
.
.
.
census race .
.
.
.
.
.
.
.
.
.
.
.
credit sex .
.
.
.
.
.
.
.
.
.
.
.
bank age .
.
.
.
.
.
.
.
.
.
.
.
compas sex .
.
.
.
.
.
.
.
.
.
.
.
compas race .
.
.
.
.
.
.
.
.
.
.
.
.
.
dtcensus sex .
.
.
.
.
.
.
.
.
.
.
.
census race .
.
.
.
.
.
.
.
.
.
.
.
credit sex .
.
.
.
.
.
.
.
.
.
.
.
.
bank age .
.
.
.
.
.
.
.
.
.
.
.
compas sex .
.
.
.
.
.
.
.
.
.
.
.
compas race .
.
.
.
.
.
.
.
.
.
.
.
dacensus sex .
.
.
.
.
.
.
.
.
.
.
.
census race .
.
.
.
.
.
.
.
.
.
.
.
credit sex .
.
.
.
.
.
.
.
.
.
.
.
bank age .
.
.
.
.
.
.
.
.
.
.
.
compas sex .
.
.
.
.
.
.
.
.
.
.
.
compas race .
.
.
.
.
.
.
.
.
.
.
.
figure the temporal progress of search strategies for the decision tree algorithm over 4training tasks.
x axis is the timestamp of search from 0s to 14000s hrs and y axis is the group fairness metric aod .covera e refers to gray box method.
and only if the corresponding classifier achieves better accuracy.
wealsolimitthedepthofcartclassifiersto3inordertogeneratesuccinct decision trees.
we first present the explanatory models of different algorithms over each individual training scenario e.g authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fairness aware configuration of machine learning libraries icse may pittsburgh pa usa censusdataset with sex .
next we perform an aggregated analysis of learning algorithms over the different training datasets the random search algorithms and the repeated runs to extract what hyperparameters are frequently appearing in the explanatory models and thus suspicious of influencing fairness systematically.
individualtrainingscenario .weshowhowourstatisticallearning approach helps localize hyperparameters that influence the biasesforeachindividualtrainingtask.figure4showstheexplanatorymodels clusteringandcarttree foreachlearningalgorithm over the censusdataset with sex except for random forest that was presented in the overview section .
for example figure b showsthetrueevaluationof solver!
sag fit intercept .
solver newton cg forthehyperparametersoflogisticregression whichexplainstheorangecluster leadstostrongerbiases.all models are available in the supplementary material.
miningoveralltrainingscenarios .foreachlearningalgorithm our goal is to understand what hyperparameters influence the fairness in multiple training scenarios and establish whether some hyperparameters systematically influence fairness beyond a specific training task.
overall we analyze cart trees and report hyperparameters that appear as a node in the tree more than 50times overall and more than times uniquely in the datasets.
different values of these frequent hyperparameters are suspicious ofintroducingbiases acrossdatasets searchalgorithms anddifferent protected attributes.
in the following within the parenthesis rightafterthenameofahyperparameter wereport thenumber ofexplanatorymodels outof180 wherethehyperparameterappearsand thenumberofdatasets outof4 forwhichthereisanexperimentwhoseexplanatorymodelcontainsthehyperparameter.
a logistic regression lr the computation time for inferring clustersandtreeclassifiersis77 .
s intheworstcase.theaccuracy of classifiers is between .
and .
.
three frequent hyperparametersbasedonthepredicatesinclassifiersare solver tol and fit intercept .
our analysis shows that the solversa afrequently achieves low biases after tuning the toleranceparameterwhereasthesolver newton c oftenachieveslow biases if the intercept term is added to the decision function.
b random forest rf the computation time for inferring models is .
s in the worst case.
the accuracy is between .
and .
.
two frequent parameters are max features and min weight fraction leaf .theseparametersandtheir connections to fairness are explained in the overview section .
c supportvectormachine svm thecomputationtimeforinferringmodelsis79 .
s intheworstcase.theaccuracyisbetween .
and98 .
.theonly relatively frequentparameteris degree .
thisshows the high variationof parameter appearancesin the explanation model.
thus the configuration of svm might not systematically amplify or suppress biases the influence of configuration on biases largely depends on the specific training task.
d decisiontree dt thecomputationtimeforinferringmodels is .
s in the worst case.
the accuracy is between .
and .
.
the frequent parameters are min fraction leaf andmax features .similartorandomforest theminimum required samples in the leaves and the search space of dataset attributes during training impact fairness systematically.e discriminant analysis da the computationtime forinferring models is .
s in the worst case.
the accuracy is between .
and .
.
however if we allowed a higher depth for the classifier morethan3 itisabove90 inallcases.thefrequentparameter istol .however theexactconditionoverthetolerancein theexplanatorymodelsignificantlydependsonthetrainingtask and might not influence fairness systematically.
mllibrarymaintainerscanusetheseresultstounderstandthe fairness implications of their library configurations.
answer rq3 we found the statistical learning scalable and useful to explain and distinguish the configuration with low andhighbiases.ourglobalanalysisof 180explanatorymodels per learning algorithm reveals that some algorithms and their configurationscan systematically amplify or suppress biases.
.
bias mitigation algorithms rq4 we show how parfait ml can be used as a mitigation tool to aid ml users pick a configuration of algorithms with the lowest discrimination.in oftimeandpickaconfigurationofhyperparameterswiththelowest aodandeod.toshowtheeffectiveness wecompareparfait ml to the state of the art techniques .
we say approach outperformsapproach ifitachievesstatisticallysignificantlower biases within a similar or higher accuracy.
a exponentiatedgradient presentsabiasreductiontechnique thatmaximizesthepredictionaccuracysubjecttolinearconstraints onthegroupfairnessrequirements.theyuselagrangemethods and apply the exponentiated gradient search to find lagrange multipliers to balance accuracy and fairness.
in so they usemeta learning algorithms to learn a family of classifiers one ineach step of the algorithm with a fixed lagrange multiplier and assignaprobabilisticweighttoeachofthem.inthepredictionstage theapproach chooses oneclassifierfromthefamily ofclassifiers stochastically according to their weights.
we choose this approach forafewreasons theapproachisan inprocess algorithmanddoes not add or remove input data samples in the pre processing step normodifiespredictionlabelsinthepost processingstep they assumeblack boxaccesstomlalgorithms thustheydonotmodify the learning objective nor model parameters.
we note that their approach is sensitive to the fairness metric to construct the linear constraints anddoesnotsupportarbitrarymetrics.inparticular they support the eodmetric but not the aodmetric.
thus we focus on the eodmetric in this experiment.
in addition since the discriminant analysis algorithm does not support meta learning we exclude this algorithm from this experiment.
we consider the default configuration of algorithms without any fairness consideration the exponentiated gradient method parfait ml andexponentiatedgradientcombinedwithparfaitml.
we also set the execution time of parfait ml to minutes in accordance with the max execution time of gradient approach in ourenvironment.table4showstheresultsoftheseexperiments.
comparedtothedefault configuration in18casesoutof24experiments theexponentiatedgradientsignificantlyreducesthe eod biases.however in11casesoutof24experiments exponentiated authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa saeid tizpaz niari ashish kumar gang tan and ashutosh trivedi a logistic regression cluters.
b logisticregressionexplained.
c svm clusters.
d svm explained.
e decision tree clusters.
f decision tree explained.
g discriminant clusters.
h discriminant explained.
figure4 thetestinputsovercensusdatasetwithsexastheprotectedattributeare clusteredintotwogroupsinthedomain of fairness and accuracy explained to understand which parameters distinguish low and high fairness outcomes.
table parfait ml as a bias mitigation technique compared to exp.
gradient within mins.
algorithm dataset protecteddefault configuration exp.
gradient parfait ml parfait ml exp.
gradient accuracy eod accuracy eod accuracy eod accuracy eod lrcensus sex .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
census race .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
credit sex .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
bank age .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
compas sex .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
compas race .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rfcensus sex .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
census race .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
credit sex .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
bank age .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
compas sex .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
compas race .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
svmcensus sex .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
census race .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
credit sex .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
bank age .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
compas sex .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
compas race .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
dtcensus sex .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
census race .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
credit sex .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
bank age .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
compas sex .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
compas race .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
gradient degraded the prediction accuracy.
parfait ml reduces theeodbiasesin23caseswith12casesofaccuracyimprovements andonlyonecaseofaccuracydegradations.overall parfait ml significantlyoutperformsthegradientmethod discrepanciesare highlighted with red font in table .
combining parfait ml and exponentiated gradient performs better than each technique in isolation see rfwithcensusandsex given that the gradient techniquedoesnotincreasethestrengthofbiasesinisolation see lr withcreditandsex .
in such cases parfait ml alone results in lower biases and higher accuracy.
b fairway usesamutli objectiveoptimizationtechnique known as flash to tune hyperparameters and chooses a configuration that achieves less biases with a minimum accuracy loss.we use their implementatio and compare our search based technique to this method.
fairway generally supports integer and booleanhyperparameters.
therefore weuse a subset ofconfigu rations as specified and reported for logistic regression lr and decision tree dt .
for a fair comparison we calculate the executiontimeof fairwayforeachexperimentinourenvironment andlimittheexecutiontimeof parfait mlaccordingly.table5shows the comparison results.
overall there are discrepancies inaodand 5discrepanciesineod noted byredfontintable5 .
parfait ml outperforms fairway in cases whereas fairway outperformsparfait mlin2cases.wealsonotedthatthecurrent implementationsof flashcarefullyselected4hyperparameters forlranddt.whenweincludethreemorehyperparameterswith integer or boolean types e.g.
dual and fit intercept in lr we authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fairness aware configuration of machine learning libraries icse may pittsburgh pa usa table parfait ml in comparison to fairway flash .
alg.scenariotime flash parfait ml s accuracy aod eod accuracy aod eod lrcensus sex .
.
.
.
.
.
.
.
.
.
.
.
.
census race .
.
.
.
.
.
.
.
.
.
.
.
.
credit sex .
.
.
.
.
.
.
.
.
.
.
.
.
bank age .
.
.
.
.
.
.
.
.
.
.
.
.
compas sex .
.
.
.
.
.
.
.
.
.
.
.
.
compas race .
.
.
.
.
.
.
.
.
.
.
.
.
dtcensus sex .
.
.
.
.
.
.
.
.
.
.
.
.
census race .
.
.
.
.
.
.
.
.
.
.
.
.
credit sex .
.
.
.
.
.
.
.
.
.
.
.
.
bank age .
.
.
.
.
.
.
.
.
.
.
.
.
compas sex .
.
.
.
.
.
.
.
.
.
.
.
.
compas race .
.
.
.
.
.
.
.
.
.
.
.
.
observethattheperformanceof fairwaysignificantlydegraded.
forlralgorithm over compaswithsexscenario the prediction accuracy is decreased to .
.
while the aod and eod bias metrics are increased to .
.
and .
.
respectively.
since fairway is sensitive to the domain of variables parfait mlcancomplementitwiththeexplanatorymodeltocarefully choose hyperparameters to include in the fairway search.
answer rq4 parfait ml is effective to improve fairness by findinglow biasconfigurationsofhyperparameters.itoutperformsexponentiatedgradient andfairway inreducing aodandeodbiaseswithequalorbetteraccuracy.parfait ml can complement both approaches to improve fairness.
discussion limitation.
the input dataset is arguably the main source of discriminations in data driven software.
in this work we vary the configurationoflearningalgorithmsandfixtheinputdatasetsince ourapproachistosystematicallystudytheinfluenceofhyperparametersinfairness.whilewefoundthatconfigurationscanreduce biasesin variousalgorithms ourapproach alonecannot eliminate fairness bugs.
our approach also requires a diverse set of inputs generatedautomaticallyusingthesearchalgorithms.asadynamic analysis ourapproachsolelyreliesonheuristicstogenerateadiverse set of configurations and is not guaranteed to always find interesting hyperparameters in a given time limit.
in addition we only use two group fairness metrics aodandeod and the overall prediction accuracy.
one limitation is that these metrics do not consider the distribution of different groups.
in general coming up with a suitable fairness definition is an open challenge.
threattovalidity.toaddresstheinternalvalidityandensureour findingdoesnotleadtoinvalidconclusion wefollowestablished guideline wherewerepeateveryexperiment10times reportthe averagewith95 confidenceintervals ci andconsidernotonly the final result but also the temporal progresses.
we note that non overlappingciisaconservativestatisticalmethodtocompare results.
instead non parametric methods and effect sizes can be usedtoalleviatetheconservativenessofourcomparisons.inourex periments wedidnotfindsignificantimprovementsusingcoverage metrics.however thismightbearesult ofourspecificimplementations and or the feedback criteria.
to ensure that our results aregeneralizable and address external validity we perform our experiments on five learning algorithms from scikit learn library oversix fairness sensitive applications that have been widely used in the fairness literature.
however it is an open problem whether the library algorithms and applications are sufficiently representative to cover challenging fairness scenarios.
usagevision.parfait mlcomplementstheworkflowofstandard testing procedures against functionality and performance by enabling ml library maintainers to detect and debug fairness bugs.
parfait mlcombinessearch basedsoftwaretestingwithstatistical debugging to identify and explain hyperparameters that leadto high and low bias classifiers within acceptable accuracy.
like standardmlcodetesting parfait mlrequiresasetofreference fairness sensitivedatasets.iftheexplanatorymodelsareconsistent across these datasets parfait ml synthesizes this information to pinpointdataset agnosticfairnessbugs.ifsuchbugsarediscovered mllibrarymaintainerscaneitherexcludethoseoptionsorwarn users to avoid setting them for fairness sensitive applications.
conclusion softwaredevelopersincreasinglyemploymachinelearninglibrariesto design data driven social critical applications that demand a delicatebalancebetweenaccuracyandfairness.the programming task in designing such systems involves carefully selecting hyperparametersfortheselibraries oftenresolvedbyrules of thumb.weproposeasearch basedsoftwareengineeringapproachtoexploring the space of hyperparameters to approximate the twined paretocurves expressing both high and low fairness against accuracy.
hyperparameterconfigurationswithhighfairnesshelpsoftware engineersmitigatebias whileconfigurationswithlowfairnesshelp mldevelopersunderstandanddocumentpotentiallyunfaircombinationsofhyperparameters.therearemultipleexcitingfuture directions.
for example extending our methodology to support deep learning frameworks is an interesting future work.