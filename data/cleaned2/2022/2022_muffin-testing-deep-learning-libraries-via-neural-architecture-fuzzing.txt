muffin testing deep learning libraries via neural architecture fuzzing jiazhen gu school of computer science fudan university shanghai key lab.
of intelligent information processing shanghai chinaxuchuan luo school of computer science fudan university shanghai key lab.
of intelligent information processing shanghai china yangfan zhou school of computer science fudan university shanghai key lab.
of intelligent information processing shanghai chinaxin wang school of computer science fudan university shanghai key lab.
of intelligent information processing shanghai china abstract deeplearning dl techniquesareproveneffectiveinmanychallenging tasks and become widely adopted in practice.
however previous work has shown that dl libraries the basis of building and executing dl models contain bugs and can cause severe con sequences.
unfortunately existing testing approaches still cannot comprehensivelyexercisedllibraries.theyutilizeexistingtrainedmodelsandonlydetectbugsinmodelinferencephase.inthiswork we propose muffinto address these issues.
to this end muffin appliesaspecifically designedmodelfuzzingapproach whichallows it to generate diverse dl models to explore the target library insteadofrelyingonlyonexistingtrainedmodels.
muffinmakes differential testing feasible in the model training phase by tailoring asetofmetricstomeasuretheinconsistenciesbetweendifferent dl libraries.
in this way muffincan best exercise the library code to detect more bugs.
to evaluate the effectiveness of muffin w e conductexperimentsonthreewidely useddllibraries.theresultsdemonstratethat muffincandetect39newbugsinthelatestrelease versions of popular dl libraries including tensorflow cntk and theano.
ccs concepts software and its engineering software testing and debugging software libraries and repositories .
keywords deeplearningtesting librarytesting modelgeneration fuzzing yangfan zhou is the corresponding author.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa association for computing machinery.
acm isbn ... .
reference format jiazhen gu xuchuan luo yangfan zhou and xin wang.
.
muffin testingdeeplearninglibrariesvianeuralarchitecturefuzzing.in 44th international conference on software engineering icse may pittsburgh pa usa.
acm new york ny usa 13pages.
.
introduction deep learning dl techniques have been proven effective in many specific tasks such as image recognition video understanding andmachinetranslation .asaresult itbecomesatrend to include dl based functionality into traditional software design.
dlsystems i.e.
softwaresystemsbasedondltechniques have beenwidelyadoptedinvariousdomainsinpractice e.g.
self driving cars virtualassistants andsoftwareoperations .
however dl systems are shown to be lack of robustness and thus causereal worldaccidents.forinstance atesladriverwaskilled in self driving mode that failed to brake the car in and an uber autonomous driving car killed a pedestrian in .
errors defectsindlsystemscancausesevereconsequences and evenjeopardizehumanlives.therefore itisacriticaltasktotest dl systems before deploying them in real production scenarios.
unfortunately how to test a dl system still remains an open challenge to the software engineering community.
many recent approaches focus on testing its core component the dl model i.e.
adeepneuralnetworktrainedwithasetoftrainingdata.extensive workaimsatimprovingtherobustnessofdlmodelsviagenerating adequate test cases e.g.
adversarial inputs or corner cases .
manystudiesalsofocusondesigningcriteriatomeasurethetest adequacy .
however theexecutionofdlmodelsreliesontheirback end libraries i.e.
dllibraries .evenwithacorrectmodeldesign the outputscanbewrongiftheunderlyinglibrarycontainsbugs.specifically dllibrariesprovidehigh levelinterfacesoftheunderlying various computation implementations e.g.
matrix transformation gradient calculation and weight update over hardware infrastructure e.g.
cpu and gpu .
bugs in dl libraries can inevitably cause unexpectedoutputs orevenfatalfailureofdlsystems .but ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa jiazhen gu xuchuan luo yangfan zhou and xin wang one may tend to blame the dl model design instead of its underlyinglibrary whendebugging incurringmoredifficultytothe process.
hence it is critical to investigate how to test dl libraries.
recentefforts i.e.
cradle andlemon ondllibrary testing focus on the inference phase of dl models.
they adopt differential testing to detect bugs by comparing the inference results of existing already trained dl models with different dl libraries.
specifically cradle directly use such models as test inputs whilelemonfurthermutatessuchmodelsastestinputs.
however evenwiththeseapproaches bugsstillexistindllibraries as we have found in this work.
the key reason is that they relyon the inference phase of already trained models which cannot exercisethelibrarycodescomprehensively.suchalready trained models typically involve only a small set of dl library functions.moreover dl libraries also play an important role in the modeltraining phase e.g.
the library codes for back propagation .
these library codes also cannot be exercised as well.
but bugs in thesecodescancauseincorrecttrainingresults i.e.
wrongresulting models.
unfortunately solvingtheseconcernsisachallengingtask.first itishard ifnotinfeasible toobtaintremendous diversealreadytrained models to comprehensively exercise library codes.
mutations based on such existing models also cannot solve this problem as they inherit the model structures limiting the exploration of library functions.
moreover as test oracles are not available generally existingapproaches resorttodifferentialtesting based on comparing the model outputs with different dl libraries.
however such outputs are not existing in the training phase incurring a huge challenge to applying differential testing.
in this work we propose muffin a fuzzing based approach to testdllibrarieswithhighfunctionalitycoverage.insteadofrelying onalready trainedmodels muffinobtainsdiversetestinputs i.e.
models withanautomaticmodelgenerationalgorithm.itformulates model structure as a directed acyclic graph dag basedon which it builds a model layer by layer with an aim to achieve high functionality coverage of dl libraries.
to perform differential testing muffinreliesondatatraceanalysisinthetrainingphase.
in particular we divide the model training phase into three differentstages i.e.
forwardcalculation losscalculationandgradientcalculation and accordingly design a set of metrics on the data tracestomeasuretheconsistencyofresultsbydifferentdllibraries.
inconsistencies can thus indicate potential bugs.
we apply muffinto test release versions of three widelyused dl libraries i.e.
tensorflow cntk and theano .
muffindetects39newbugs including21crashbugs inthelatest release versions of these libraries.
extensive experiments based on 6populardatasetsshowthatcomparedwithexistingapproaches muffiniscapableofdetectingmoreinconsistencieswithinacomparable testing time.
furthermore we investigate the benefit ofour model generation method through comparing muffinwith layer by layertesting.theresultsshowthat muffiniscapableof detectingmoreinconsistenciesandcrashes.ourexperimentsprove the effectiveness of muffin.
muffincontributestothesoftwaretestingartinthefollowing three aspects figure the training phase of a dl model we propose muffin a dl library testing approach based on a novel dl model fuzzing method which can exercise dl library functionalities more comprehensively.
we make differential testing feasible in testing the model trainingphase byproposingadatatraceanalysismethodto detect inconsistencies between different test targets.
weimplementourideasasanopen sourceavailablesoftwaretoolmuffin whichcanfacilitatereal worlddllibrarytesting tasks as well as further follow up research.
weconductanextensivestudyon15versionsofthreewidely useddllibraries.theresultsshowthat muffincandetect39 new bugs which cannot be detected by previous methods.
therestofpaperisorganizedasfollows.section 2introduces backgroundknowledgeaboutdlmodelanddllibrary.section elaborates the design and implementation details of muffin.w e demonstratethe experimentalsetup insection andanalyze the results in section .
further discussion is provided in section .
we introduce related work in section 7and conclude the paper in section8.
background .
deep learning model dlmodelsaredesignedtoautomaticallydrawstatisticalrulesfrom training data .
a dl model typically consists of a number of neurons with a layered connected structure.
the neurons between layers are connected with links.
different links are associated with different weights which are obtained through training with input data.
each layer conducts a specific kind of transformation e.g.
convolution andpooling forthe inputdata with specific weights.
in particular the same layer can be adopted multiple times in a dl model which has different weight values on the links and thus produces diverse results.
essentially a developer would design the architecture of a dl model such as the types of layers how layers are connected and thelossfunction.thenthetrainingprocessofadlmodelistofind theappropriateweightvalues sothattheoutputscanbestproduce expected results.
the training phase typically consists of a huge amount of repeated training steps.
figure 1outlines the process of a single training step which can be divided into three stages forward calculation fc given a batch of training cases the model conducts specific calculations according to the layer types and get the corresponding outputs.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
muffin testing deep learning libraries via neural architecture fuzzing icse may pittsburgh pa usa figure the structure of dl libraries loss calculation lc the model calculates the value of the predefined loss function which measures the differences between the model outputs and the ground truth labels.
backward calculation bc according to the value of the loss function the model calculates the gradients of each neuronandupdatesthecorrespondingweightvaluesfrom the output layer to the input layer.
such training steps continue until the weights converge i.e.
the performance of the model cannot be further improved.
theweightvaluesdeterminehowthedlmodelprocessesthe inputtogenerateoutput.thus theperformanceofadlmodel i.e.
whetheritcanproducecorrectresults islargelydeterminedbythe weights.
since the weight values are obtained through the training process itiscriticaltodetectbugsinmodeltrainingphase.however existing work only focuses on detecting bugs in the inference phase .
.
deep learning library figure2shows the structure of dl libraries.
there are two tiers of libraries i.e.
high level and low level .
in general developers implementthesourceprogramswithhigh levellibraryapis which invokethealgorithmsimplementedinlow levellibraries.different low level libraries are based on different infrastructures e.g.
c pu gpuandtensorprocessingunit tpu thusmayhavedifferent implementationsforthesamealgorithmspecification.ontheother hand high level dl libraries can hide the differences between lowlevellibrariesandprovideaconsistentabstractiontofacilitatedlmodel development.
keras is one of the most popular high level dl libraries that has been widely used in various domains .
keras generally runs ontop of three low level libraries i.e.
tensorflow cntk and theano which cover most of the widely used libraries.
developersimplementsourceprogramsbycallingapisprovided bykeras whichinvoketheassignedbackendlow levellibraryto execute the computation.
specifically implementinga dlmodelusingkeras mainlycontains three parts loading the data defining the model architecture and training the model with the data.
it is worth noting that while the training process includes complicated calculations i.e.
fc lc and bc it can be simply implemented via calling the model.fit function provided by keras.
a high level library is relatively simple which glues the functions in a low level library that provide concrete complicated computation.
low level libraries are not bug free and they are alsonot easy to be tested due to their complication.
similar to existing work wefocusontestinglow levellibraries e.g.
tensorflow cntk and theano.
we adopt keras as the high level library.
our target is to test the dl library codes involved in the modeltraining phase with high functionality coverage.
specifically dl librariescontainmanyauxiliarycodesforvarioustaskssuchasprofiling and hardware adaptation rather than learning related ones.
likeexistingtoolstotestdllibraries wefocusonlyonlearningrelated apis.
in this paper we use functionality coverage as the coveragemetric whichreferstothepercentageoftheinvokedapis in all the pre defined learning related apis we considered.
.
challenges in order to perform comprehensive dl library testing e.g.
test thelibrarycodesinvolvedinmodeltraining therearetwomain challenges.first itisdifficulttoobtainasetofdlmodelsastesting inputs that cover most library apis.
a dl model has a layered connected structure which hinders the adoption of traditional test input generation approaches.
furthermore many apis in dl li braries have specific usage scenarios e.g.
convolution layer for image processing tasks recurrent layers for text processing and various activation functions e.g.
relu and leaky relu .
due to such complication it is non trivial to obtain a set of welltrained models to achieve high functionality coverage.
the second challenge is the test oracle in model training phase.
existing approaches utilize differential testing based on the model outputs with different dl libraries.
unfortunately as we have discussed dl models learn the weight values through training.
therefore the model outputs not exist in the training phase causing existing differential testing methods infeasible.
next we introduceour approach muffin which isdesigned to address the above two challenges.
approach .
overview inthiswork wepropose muffin anovelapproachtoperformcomprehensivedllibrarytesting i.e.
testthelibrarycodesrelatedto model training with high functionality coverage.
figure 3presents theoverviewof muffin whichisspecificallytailoredtosolvethe two design challenges.
to obtain diverse dl models we propose a fuzzing based model generation method.
in contrast to existing methods that adopt manually designed models the proposed model generation approachallows muffintoexercisethetargetlibrarywithtremendous diverse models.
specifically we divide the model architecture into two parts structure information i.e.
how layers are connected and layer information i.e.
what layer types are used .
throughformulating the structure information of a dl model as a dag muffinfirst generates dags as the structure information and then utilizes a greedy layer selection algorithm to generate the layer information.inthisway muffincangeneratediversedlmodels section3.
.
to conduct differential testing muffinperforms data trace analysisinthemodeltrainingphase.inparticular muffinprofilesthe data traces from different training stages i.e.
fc lc and bc .
itthen detects the inconsistencies of different libraries based on a authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa jiazhen gu xuchuan luo yangfan zhou and xin wang figure overview of muffin set of proposed metrics which measures the output variance of consecutive layers.
section .
.
.
model generation asdiscussedinsection .
adlmodelhasalayeredstructurewith connectionsbetweenlayers.inordertogenerateasetofdiversedl modelstoexplorelibrarycodes weneedtodecidewhattypesof layersareusedinamodel aswellashowtheselayersareconnected.
unfortunately simplyselectingaseriesoflayersandstackingthem together can easily cause model failure.
for example the add layerisusedtoaddalistofinputs.ifonlyoneinputisfedtothis layer the model generation would fail.
besides the inputs of add layershouldalsohavethesameshapetoavoidfailuregeneration.
therefore we design a top down generation algorithm which first generates the structure information i.e.
the topology of how layers areconnectedinthemodel followedbythegeneratingofthe layer information i.e.
specific layer types adopted in the model .
.
.
structure information generation.
givenasetofinputs adl model performs specific computation layer by layer so as to yield the outputs.
therefore the computation flow of a dl model can be abstractedasadag.specifically everyvertexinthedagrepresentsalayer andeveryedgebetweentwoverticesrepresentsalink betweenthecorrespondinglayersintheoriginalmodel.suchan abstraction method is also applied in current model representation.
for example tensorflow uses a dag to represent the computa tional graph of a dl model .
therefore we utilize a dag to represent the structure information of a dl model.
althoughitisnotdifficulttogenerateadag thecorresponding model structure may be too simple or too complicated which is rarely used in practice.
inspired by recent studies in neural architecture search nas that targets on automating the design of model architectures we summarize two model structure tem plates as shown in figure .
specifically figure a shows the chainstructurewithskips.chain structuredarchitectureisthesim plestexampleofthemodelstructuretopology.throughpermitting arbitrary skip connections between nodes this template can cover manycommonly useddlmodels e.g.
fully connectednetworks vgg and densenet .
on the other hand the cell based a chain structure with skips b cell based structure figure examples of dl model structure templates structure in figure b builds upon the observation that many specifically designedmodelarchitecturesconsistofrepetitionsof fixed structures e.g.
resnet .
each cell in the structure is a small dag that conducts a specific transformation e.g.
the computation cell in figure b contains computation layers while the reduction cell is used for downsampling.
it is worth noting that originally thesame cells e.g.
computationcells should have the same dag.
since our target is generating diverse structures insteadoffindingthearchitecturewiththebestperformance we remove this restriction in the proposed template i.e.
same cells may have different dags .
in addition we also guarantee that the generated dag has only one vertex whose in degree is as theinput layer one vertex whose out degree is as the output layer.
there is also no isolated vertex in the generated dag.
in this way muffingenerates a dag as the model structure information.
.
.
layer information generation.
giventhegeneratedstructure information weneedtorefinethelayerinformation i.e.
determine the specific layer type for each vertex in the dag.
as discussedbefore stacking layers without guidance can easily cause model failure.
specifically there are two types of restrictions when selectinglayers.thefirstrestrictionisthe input number restriction.in particular most layers e.g.
convolution are si single input layers while some layers e.g.
concatenation are mi multipleinput ones.ifmorethanoneinputisfedtoansilayer thislayer authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
muffin testing deep learning libraries via neural architecture fuzzing icse may pittsburgh pa usa canonlyprocessoneoftheinputs leadingtotheexistenceofinvalid connections between layers.
the corresponding dag thus isequivalenttothedagwithouttheinvalidconnections which lowers the dag diversity.
therefore we have to choose the proper layer according to the number of inputs.
considering that an edge in a dag represents the data flow direction we can determine the numberofinputsthatthecorrespondinglayertakesbasedonthe in degree of the vertex.
thesecondrestrictionisthe input output shape restriction.specifically milayersrequiretheinputstohavethesameshapeinspecific axis es soastoconductthetransformationproperly e.g.
theinputs of concatenation layershouldhavethesameshapeexceptforthe concatenationaxis.therefore beforefeedingtheinputstoanmi layer we adopt additional reshaping layers to reshape the inputs into the same shape.
in addition the input shape of the input layer i.e.
thevertexwith0in degree andtheoutputshapeoftheoutput layer i.e.
the vertex with out degree need to be properly set according to the training data and the task type e.g.
classification regression .
we still resort to the reshaping layer to reshape the outputsize whiletheinputshapeisdirectlysetbasedontheshape of input data.
furthermore in order to increase the diversity of the generated models intuitively weshouldgivealargerchancetothelayerthat is rarely used before.
based on this intuition we design a layer selection procedure based on fitness proportionate selection .
specifically for a specific layer l muffinrecords the number of times that lhas been selected to construct a model denoted as c. thenmuffincalculates s c 1as the score for l. based on s the probabilitythat lisselectedamongalllayertypescanbecalculated as follows p s summationtext.1r k 1sk whereristhetotalnumberofpossiblelayers.sincewedivide layertypesintotwocategories i.e.
siandmi score sandprobabilitypare calculated based on the layers belonging to the same category.inthisway muffingeneratesthelayerinformationvia selectingaspecificlayerforeachvertexinthedag.adlmodel can thus be constructed according to the generated structure information and layer information.
.
.
entire algorithm.
we formally describe our fuzzing based modelgenerationmethodinalgorithm .thisalgorithmtakes5 parameters where nmisthetotalnumberofmodelstogenerate serving as the terminating condition max candmax vare parameters to control the size of dag liandloshould be manually set according to the input data and target task.
lines iterativelygenerateasetofdlmodels.specifically lines3 13randomly chooseatemplateandgenerateadagasthestructureinformation.
lines17 18settheinputlayer.lines22 24selectsilayersforthe in degree vertices and lines select mi layers for the verticeswithmorethan1in degree.lines30 31settheoutputlayer.
finally lines construct a dl model mbased on the generated structure information and layer information then adding mto the result set m.algorithm model architecture generation input nm number of generated models max c maximum number of cells in a model max v maximum number of vertices in a dag li input shape lo output shape output m a set of generated models 1m 2whilesize m nmdo select a template and generate structure information si 3p random 4ifp .5then 5nv randomint max v 6si createchaindag nv 7else 8nc randomint max c 9g forifrom toncdo gi randomdag g g gi si createcelldag nc g 14li layer information generate model according to dag 16foreach nodejintopologicalsequence si do ifthe in degree of node jis 0then li li setinputlayer li j else pj getalldirectpredecessors j ifsize pj 1then si layer shape getshape pj li li setsilayer shape j updatesiscore else mi layer shape randomshape li li reshapinglayers pj shape li li setmilayer shape j updatemiscore ifthe out degree of node jis 0then li li setoutputlayer j lo 32m constructmodel si li 33m m m 34returnm .
inconsistency detection inordertodetectinconsistenciesandperformdifferentialtesting accordingly muffinrequiresproper metricsto measurethedifferences between the execution results of different libraries.
however the metrics proposed by the existing work are designed only for already trained models which calculate theinconsistency between the ground truth label and model outputs.
since our target is to test dl library in the training phase i.e.
without a trained model authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa jiazhen gu xuchuan luo yangfan zhou and xin wang such metrics cannot be directly applied.
instead we propose a new metric based on the variance of outputs in consecutive layers.
asdemonstratedinsection .
themodeltrainingphaseincludes repeated training steps and each training step can be divided into three stages fc lc and bc.
specifically in fc stage the model performs calculation from input layer to output layer.
in lc stage the model calculates the value of loss function.
in bc stage the model calculates gradients from the output layer to the input layer.
resorting to dynamic analysis we can collect the data traces of different libraries and compare the differences.
in particular we utilize the functional api mechanism provided by keras to collect dynamic traces.
more specifically we profile theresults producedbyeverylayerinfcstage thelossvalue in lcstage andthegradientvalueofeachlayerinbcstage.based on the dynamic trace muffingradually compares the values of layeroutputs theloss andthegradients soastodetectthesuspect behavior of specific layers.
however due to normal uncertain factors such as floating point deviation wecannotdeterminewhetheravaluedifferenceis causedbypotentialbugsornormalfactors.specifically thereare manysmalldeviations lessthan10 inlayeroutputs whichmay be gradually amplified or reduced e.g.
by pooling or activation functions.weconsiderthatnormalfactorswouldonlyleadtoslightlayeroutputdifference i.e.
ifthedifferencesoflayeroutputschange dramatically it indicates a suspicious behavior.
therefore instead of comparing the outputs from one single layer we consider the difference changesbetweentwoconsecutivelayers.onlywhenthe deviationisamplifiedwould muffinconsiderinconsistency.since outputsfromdifferentlayersmayhavedifferentshapes wefirstusethe following chebyshev distance i.e.
l distance to measure the difference of the outputs from the same layer.
d x y maxm xm ym in the above equation xandyare two tensors i.e.
output of a layeristypicallyahigh dimensionaltensor while xpandypare elements in xandy respectively.
chebyshev distance defines that thedistancebetweentwotensorsisthegreatestoftheirdifferences along any coordinate dimension.
in this way we can avoid theinfluence of different tensor shapes from different layers when measuring the differences.
we now describe the inconsistency detection procedure of muffin.
for brevity we denote nas the total number of layers li i astheithlayer oi jan liusinglibrary jandk respectively.
p i denotesthesetoflayersthataredirect predecessorsof liinthedag i.e.
eachlayerin p i isli sprevious layer.
infc stage muffincomparesthe differencesof theoutput tensorsfrom lianditspredecessors lp.ifthedifferenceof lpissmaller than whilethedifferenceof liislargerthanauser definedthresholdt thenmuffindetermine that an inconsistency is detected in li.
the inconsistency layers detected in fc stage can be formally defined as follows inc fc li i d oi j oi k t d op j op k p p i table versions of libraries under test idkerastensorflow theano cntk e12.
.
.
.
.
.
.
.
e22.
.
.
.
.
.
.
.
e32.
.
.
.
.
.
.
.
e42.
.
.
.
.
.
.
.
e52.
.
.
.
.
.
.
.
in lc stage the model calculates loss value based on the results from the output layer.
to avoid the transmission of errors muffin only performs inconsistency detection in lc stage when the difference of model outputs is smaller than .
it is worth noting that a smalldifferenceinmodeloutputsdoesnotmeanthatthereisno inconsistencyinmiddlelayers.largedifferencecouldbemasked duetotheexistenceofdownsamplinglayerssuchaspooling.since theresultofalossfunctionisanumber wedirectlycomparethe absolute difference as follows inc lc l loj lok t lgj lgk t d on j on k intheaboveequation ldenotesthelossfunction lojandlok are the output results of l lgjandlgkare the gradient results of l on jandon kare the model outputs using library jandk.
in bc stage the model calculates gradients to update weights from the output layer to the input layer.
similarly muffinonly conducts inconsistency detection if the difference in loss function issmallerthan .weformulatetheinconsistencydetectioninbc stage as follows inc bc li i d gi j gi k t d gs j gs k s s i wheres i denotes the set of layers that are direct successors of li gi jandgi kdenotethegradientresultof liusingdifferentlibraries.
especially the successor of the output layer is the loss function.
evaluation setup in the evaluation we evaluate the performance of muffinthrough answering the following research questions.
rq1 howdoes muffinperformindetectingbugsindllibraries?
rq2 canmuffinachievebetterperformancecomparedtoother methods?
rq3 howdothedifferent parametersettingsaffecttheperformance of muffin?
.
libraries and datasets .
.
libraries.
we use three widely used dl libraries i.e.
tensorflow theano and cntk as the back end low level libraries as ourtestingtargets andkerasasthefront endhigh levellibrary.to sufficiently illustrate the effectiveness of muffin we utilize a total of release versions of the three back end libraries and construct fiveexperimentalenvironmentsfordifferentialtesting i.e.
e1 e5 in table1.
in particular in e1 keras .
.
is the latest version that supports multiple back ends theano .
.
and cntk .
.
are the latest versions while tensorflow .
.
is the latest version that authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
muffin testing deep learning libraries via neural architecture fuzzing icse may pittsburgh pa usa supported by keras.
for the sake of brevity we use tf th and cktorepresenttensorflow theanoandcntkinthefollowing figures and tables.
.
.
datasets.
ourapproachisnotsensitivetodatasets i.e.
theoretically any data type can be used for testing.
in order to facilitate subsequent comparative experiments with existing approaches we selected widely used datasets in existing studies i.e.
mnist f mnist cifar imagenet sine wave and stock price.
specifically the first four are popular image classification datasets while thelasttwoaresequencedatasets.inparticular sine waveisthe sine function value sequence and stock price is the disneyland stock price sequence from to .
.
competitors in order to demonstrate the effectiveness and efficiency of muffin wecompare muffinwiththestate of the artapproach lemon .
lemonperformsdllibrarytestingthoughmutatingexistingmodelstogenerateahugeamountofnewtestinputs.followingtheeval uationsetupin weuse11existingmodels i.e.
alexnet lenet5 resnet50 mobilenetv1 inceptionv3 densenet121 vgg16 vgg19 xception lstm lstm as the seed models for mutation.
by comparing with lemon we evaluate whether muffin based on directedtestcases i.e.
model generation canoutperformlemon in exposing bugs.
since lemon cannot perform testing in the lc and bc stages we only compare muffinwith lemon through analyzing the number of inconsistencies and bugs detected in the fc stage.
in addition muffinis designed to perform comprehensive library testing so we also compare the functionality coverage achieved by muffinand lemon.
besides ourdag basedmodelgenerationisthecorecomponent ofmuffin.
thus it is also interesting to investigate the effectivenessofthiscomponent.tothisend weimplement muffin ut a simplified muffin versionmethodbasedonunittesting.
muffin ut differs from muffinonly in the model generation part.
specifically inmuffin ut a functional layer e.g.
conv2d is a to be tested unit.muffin ut createsmodelswithonlyonefunctionallayer and simple reshapinglayers to copewith input output i.e.
dimension transformationtomatchtheinput outputrequirementsofthetobe tested layer.
by comparing with muffin ut we show that unit testingisstillinadequatetotestdllibraries.
muffin bygenerating diverse models can expose bugs which are difficult to be detected by traditional approaches.
.
measurements .
.
number of inconsistencies.
sincetheproposedapproachconducts inconsistency detection in layer level during model training an inconsistency between two low level dl libraries means that they produce different calculation results given the same input underaspecificlayer.inordertoeliminateduplicatedinconsistencies caused by the same function we only count the inconsistenciesproduced by the same layer once.
in particular for muffinand muffin ut we compare the number of inconsistencies detectedin different training stages i.e.
fc lc and bc respectively.
for lemon weonlycounttheinconsistenciesdetectedinfcstage.although different inconsistencies may be the manifests of the same potentialbug morefailure triggeringtests i.e.
themodelandinputdata that trigger the inconsistency reflecting a fault in different ways provide more information for fault localization.
therefore the number of detected inconsistencies can reflect the effects of these methods to some extent.
.
.
number of detected bugs.
although we count the number of detected inconsistencies it is more important to measure thenumber of unique bugs revealed by muffin.
based on the voting mechanism of differential testing we can localize the buggy layer in the library.
to avoid false positives we further check the buggy layer manually.
specifically we save all intermediate layer outputs duringthetesting.whenaninconsistencyisreported twoauthors check the corresponding source codes in different libraries and comparetheresults.iftheiridenticallayerproducesdifferentresults and their implementationideas are different the third authorwill joinmanualinspectionsoastoconcludewhetherthereportistrue or false positive.
.
.
number of nan crash bugs.
besidesinconsistentcalculation results bugsindllibrariesmayleadtonan notanumber and crashesaswell .generatingdlmodelsthattriggernan crashes can also provide valuable information for identifying potentialbugs.
therefore we count the number of models with nan or crashesgeneratedbythreemethods.inparticular weonlycount the nan crash when at least one of the dl library can execute properly e.g.
tensorflowproducesnormalresultswhiletheano and cntk produce nan.
in addition in order to avoid duplication the nancaused by thesame layer and crashes withthe same error message are only counted once.
.
implementations intheexperiments weleteachmethodgenerateatotalof300models for each dataset.
for lemon we use its default parameters.
formuffin we set the maximum number of cells i.e.
max c t o andthemaximumnumberofnodes i.e.
max v to30.interms of inconsistency detection we set the threshold tto be .
and to be 1e .
thistvalue is relatively large so as to avoid many falsepositives asshownin section .
.inaddition muffindonot considersomelayerssuchas dropout and gaussiannoise soas to avoid introducing randomness and affecting the execution results.
all the experimentsare conducted on theintel r core tm i76700kcpu .00ghzmachinewith32gbofram ubuntu20.
.
lts and one nvidia gtx ti gpu.
theimplementationof muffinispubliclyavailableongithub1.
results and analysis .
effectiveness of bug detection we first investigate the effectiveness of muffinin terms of new bugsdetectedinthelatestversionsofdifferentlibraries i.e.
e1in table1.
after manual analysis muffindetects bugs in the latest versionoftheselibraries including12bugsinfcstage 2bugsin lc stage bugs in bc stage and nan bugs as shown in table .
inaddition muffinalsodetects21crashbugs mainlyfromtheano and cntk.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa jiazhen gu xuchuan luo yangfan zhou and xin wang table new bugs and crashes detected by muffin libraryfcbug lc bug bc bug nan crash tensorflow theano cntk total fc bug lc bug bc bug respectively refer to new bugs found in forward calculation loss calculation and backward calculation stages.
nan refers to bugs related to nan calculation.
3the number in parentheses means the bug exists in tensorflow2.
.
but has been fixed in the latest version.
other bugs are all exists in the latest version.
in particular for the bugs detected in tensorflow .
.
we manuallycheckwhetherthesebugscanbereproducedinthelatest version i.e.
tensorflow .
.
.
the results show that among these bugs 1bughasbeenfixedwhiletheother3bugsstillexist.after reporting these bugs to the issue repository bug has been confirmed by developers.
among the bugs detected in cntk will befixedinthefutureversion .wealsoprovidebugcaseanalysis according to different bug types.
fcbugs.
the12bugsdetectedinfcstageinvolvedifferentlayer types including averagepooling2d conv1d in theano and lstm depthwiseconv2d batchnormalization in cntk.
by taking the averagepooling2d bug in theano as an example.
this bugoccurswhensettingthelayerparameter paddingto same and pool sizetothesameastheshapeoftheinputtensor.byanalyzing theresults wefindthatinthiscase theanowouldchooseawrong pooling location resulting in large difference i.e.
more than whilet .
between the results from other libraries.
lc bugs.
by taking the binarycrossentropy bug in tensorflow as an example.
when passing parameter values output .
.
.
and target to the binarycrossentropy loss function theano and cntk return a value whiletensorflowreturns amongwhichthedifferenceofthefirstelementisnotnegligible.byreviewingthesourcecode wefindthattensorflow redundantly usesan epsilonparameter to clipinput values resulting in errors.
this bug has been confirmed by the developers of tensorflow.
bc bugs.
bytakingthe relu bugintheanoasanexample.
when0existsintheinputtensorof relu theanoback propagates a different gradients value compared with tensorflow and cntk.
this bug is caused by the wrong equal sign position of theano i.e.
relu z z z 0intheano while relu z z z 0inother libraries.
although such implementation does not affect the results in forward calculation the implementation in theano would let the gradient propagate to previous layers in backward calculation which should not happen .
this bug can only be detected in bc stage proving the effectiveness of muffin.
nan bugs.
bytakingatensorflowbugasanexample.given twonanvalue the globalmaxpooling layer returns inf leading to the inconsistency.
this bug has been fixed in the latest .
.
version.
regarding false positives muffinreports unique inconsistencies totally where one false positive is found.
the false positive occursinthe mean absolute percentage error lossfunction.this function returns mean which amplifies the deviation and cause the false alarm.
in addition muffindetects crash bugstable comparison of distinct voted layers method lib fc lc bc muffintf th ck lemontf th ck muffin uttf th ck 1the number in parentheses denotes the number of voted layers that only detected by the corresponding method.
totally.
among them four are due to unsupported models muffingenerates which can be treated as false positives.
but such falsepositiveshaveclearerrormessages thuscanbeautomatically detected so as to avoid false alarms.
to further illustrate the effectiveness of muffin we count the numberofdistinctvotedlayersdetectedbydifferentapproaches asshownintable .wecanobservethatallthe4layersdetected bylemoncanbedetectedby muffinandmuffin ut indicating thatmuffinandmuffin ut can cover the exploration scope of lemon.ontheotherhand muffinandmuffin ut havetheirown distinct voted layers that cannotbe detected by the other proving theeffectivenessofinconsistencydetectionapproach.theseresults indicate that the natural architecture fuzzing approach adopted in muffinis a good supplement to unit testing.
.
performance comparison inordertofurtherevaluatetheperformanceof muffin wecompare thenumberofinconsistencies nan andcrashdetectedbydifferentmethods.wepresenttheresultsunderenvironmente1asanexample2.
table4showsthe inconsistencies detected by three methods underdifferentdatasetsandenvironments.specifically inthelatest library versions i.e.
e1 muffinfinds a total of inconsistencies of which are found in the fc stage.
in comparison lemon can only find7 inconsistencies muchless than muffin.
similarresults can also be observed in other environments which prove the effectivenessof muffininlibrarytesting.themainreasonisthat muffin can explore more library functions through the model generation approach whilelemoncanonlymutateseedmodels andthuscan hardly cover the functions not being used in seed models.
besides lemonalsocannotexplorethelibrarycodesrelatedtolossandgradient calculation.
as a result lemon only achieves .
functionalitycoverage thepercentageoftheinvokedapisinall thepre defined learning relatedapisweconsidered while muffin can achieve .
functionality coverage.
the inconsistent apis thatcannot beidentifiedby lemoninclude depthwiseconv2d locallyconnected1d conv3d andvariouslossfunctions.itis worth noting that although muffinis not designed to achieve high linecoverage wesummarizeandreportthelinecoverageresults muffinachieves .
which is .
times of that achieved by lemon .
.
onthe otherhand comparedwith muffin ut muffindetects more inconsistencies in e1 which proves the performance of 2more experiment codes and results are available at authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
muffin testing deep learning libraries via neural architecture fuzzing icse may pittsburgh pa usa table comparison of inconsistency number idmethodlibpaircifar mnist f mnist imagenet sine wave stock price total fclc bc fclc bc fclc bc fclc bc fclc bc fclc bc fclc bc e1muffintf th400 tf ck th ck lemontf th1 tf ck th ck muffin u ttf th103 tf ck th ck fc lc bc respectively represent the number of inconsistencies detected in the three stages.
2for inconsistencies caused by the same kind of layer or loss function we only count once.
table comparison of nan crash number idmethodlibcifar mnist f mnist imagenet sine wave stock price total nangc ec nangc ec nangc ec nangc ec nangc ec nangc ec nangc ec e1muffintf201 th301 ck264 lemon muffin u ttf000 th003 ck014 nan represents the number of outputs with nan.
for nan caused by the same kind of layer we only count once.
gc and ec are respectively short for generation crash and execution crash .
the crash number have been deduplicated according to error messages.
3lemon does not record nan crash information for each backend so we only obtain the total nan crash number triggered by lemon.
muffin.itisalsoworthnotingthatthenumberofinconsistencies detected by muffinreduces in other environments.
the key reason is that the numbers of nan and crash triggered by muffinincrease in old library versions as shown in table .
taking nan and crash into consideration muffincan still trigger more exceptions i.e.
inconsistency nanandcrash than muffin ut.inparticular the layerfunctionswhere muffincandetectexceptionswhile muffinutcannotinclude averagepooling1d conv3dtranspose and categoricalcrossentropy .
furthermore wealsocomparetheexecutiontimeofthethree methodstogenerate50modelsandperformtestingunderdifferent datasets.
the execution time of muffinandmuffin ut consists of the model generation time and the three stage inconsistency detection time i.e.
fc lc and bc .
the execution time of lemon consists of model mutation time and inconsistency detection time only fc .
the results are shown in table .
in this table we can observe that except imagenet the execution time of muffinis the longest in most cases.
the reason is thatmuffinconducts additional model generation compared with muffin ut andinconsistencydetectioninadditionaltwostages compare with lemon .
considering that muffincan detect much moreinconsistencies wethinksuchoverhead i.e.
aroundtenminutes isacceptable.theseresultsalsodemonstratethattheproposed approach modelgenerationandinconsistencydetection donot bring huge overhead to muffin.
moreover when performing library testing with imagenet the executiontimeoflemonisgreatlyincreased.thereasonisthatthetable comparison of execution time min.
dataset method e1 e2 e3 e4 e5 cifar 10muffin .
.
.
.
.
lemon .
.
.
.
.
muffin ut .
.
.
.
.
mnistmuffin .
.
.
.
.
lemon .
.
.
.
.
muffin ut .
.
.
.
.
f mnistmuffin .
.
.
.
.
lemon .
.
.
.
.
muffin ut .
.
.
.
.
imagenetmuffin .
.
.
.
.
lemon .
.
.
.
.
muffin ut .
.
.
.
.
sine wavemuffin .
.
.
.
.
lemon .
.
.
.
.
muffin ut .
.
.
.
.
stock pricemuffin .
.
.
.
.
lemon .
.
.
.
.
muffin ut .
.
.
.
.
seedmodelsusedbylemonaremuchmorecomplicated compared tothoseunderotherdatasets.thisphenomenonrevealsthatthe executiontimeoflemonhighlydependsonthecomplexityofseedmodels.ontheotherhand muffindoesnotsufferfromthisproblem.
thegeneratedmodelcomplexityof muffincanbecontrolledvia setting proper values of max candmax v. under the same max c andmax vvalue the execution time of muffinis quite stable.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa jiazhen gu xuchuan luo yangfan zhou and xin wang .
.
.
.
t inconsistencies fc lc bc figure5 performanceof muffinunderdifferentthresholds compared with muffin ut muffinrequires additional dagbased model generation.
in addition the number of layers in the model also affects the inconsistency detection time.
the larger the model thelongerthedetectiontime.asaresult theexecutiontime ofmuffinis slightly longer than that of muffin ut.
finally it is worth noting muffindoes not consider the final modelperformance e.g.
precisionandrecall inspecifictaskswhen generating model architectures since it is not the objective of a testingtool.incontrast existingapproaches e.g.
mutatingexisting models typically generate limited model architectures but can obtain high performance models which however are not more capableindetectingbugs.instead muffinfocusesmoreonmodel quality in testing.
muffincan generate high quality models.
in executions 3libraries eachwith300models only77executions .
cause four unsupported crashes by the same reason .
moreover wehavealsoshownsuchmodelsaremorecapableinexposing bugs as discussed in section .
.
.
effect of different parameter settings muffinintroducesfourparameters i.e.
max candmax vtocontrol the size of model structure and threshold tand for inconsistency detection.sinceinallexperiments muffinachievessatisfyinglayer coverage i.e.
only one layer cannot be used with all datasets we considerthatthevaluesof max candmax varesetproperly.for the thresholds is a quite small value i.e.
e thus we only evaluate the number of inconsistencies detected by muffinwith differenttvalues.
figure5showsthenumberofinconsistenciesdetectedby muffin under different tvalues ranging from .
to .
.
we can observe thatwhen tissmall muffinismoresensitivetosmallvarianceof differences thus it detects more inconsistencies.
as the value of tincreases the number of inconsistencies decreases slowly and keeps stable when t .
thus the default tvalue in muffinis .
.
although bugs incurring small variances may be neglected suchbugscanberevealedunderotherinputvaluesor model architectures i.e.
variance larger than t .
discussion .
summary of evaluation asdiscussedbefore muffin ut isdesignedbasedontheideaofunit testing whichtestsaspecificlibraryfunctionatatime.theevaluationresultsshowthat muffincandetectmorelayerinconsistencies thanmuffin ut.themainreasonisthatmanylayerinconsistencies canonlybetriggeredbyspecificinputs.forinstance thegradientsinconsistencyof maxpooling1d layeronlyhappenswhenmultipleelementsintheinputtensorhavethesamemaximumvalue.
in order to trigger such inconsistencies using muffin ut we have to fuzzing the inputs.
since layers in dl libraries typically have hugeinputvalueranges e.g.
high dimensionaltensorinputswhere eachelementrangesfrom itisquitechallengingtofind specificinputsthatcantriggercornercases .ontheotherhand muffinperforms testing based on generated models.
due to the existence of different layer types we thus simulates the real calculationprocessandreducetheinputranges.asaresult thepossible corner cases i.e.
multiple maximum values can be triggered by muffin.consideringthatunittestingisnecessarybeforeversion release whilebugscanstillbedetectedinthelatestversions we believedllibrarytestingbasedonmodelgenerationisaneffective supplement to unit testing.
among the bugs detected by muffin some of them are actually causedbyunclearspecifications.forinstance thegradientcalculationbug of categorical hinge loss functionis actuallycaused by the different specification of calculating the gradients of max function.
specifically when there are multiple maximum elements tensorflow will divide the gradient with the number of maximum element whiletheanoandcntkdonothavethisoperation.simi larly for maxpooling1d layer whentherearemultiplemaximumelements tensorflowandcntkwouldonlyapplythegradientstooneofthemaximumelements whiletheanoapplythegradientsto all maximum elements.
due to unclear specifications different dl librarieshavedifferentimplementations.althoughinmostcases theresultsoftheseimplementationsareconsistent robustnessissues may be caused by the corner cases e.g.
easier to generateadversarial inputs .
therefore we call for the community to pay more attention on the unclear specification problems in dl libraries.
.
threats to validity we now discuss possible threats in this work and the methods we take to address such threats.
first of all we only evaluate the effectivenessof muffinonthreedllibraries i.e.
tensorflow theano and cntk.
these libraries can be called using the same front end library i.e.
keras which facilitate the implementation and per forming differential testing.
other libraries that do not support keras i.e.
pytorch currentlyarenotsupportedby muffin.ho wever the ideas of model generation and inconsistency detectionadopted in muffinare general.
for instance in order to test pytorch it requires to replace the keras apis used in muffinwith the corresponding pytorch apis.
to reduce this treat we evaluate muffinwithatotalof15differentreleaseversionsofdllibraries.
in addition we also use diverse models including the models generatedby muffin existingmodelson6realdatasets aswellastheir mutants generated by existing work to evaluate the inconsistency detection performance of our approach.
anotherthreatmainlyliesinrandomnessandthresholdsettings in our experiment.
to reduce the randomness we conduct fiveexperiments with different library versions i.e.
e1 e5 refer to table table2 in supplementary material .
in each experiment every method generates mutates the same number of models for commonly useddatasets andwerecordandcomparetheresults authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
muffin testing deep learning libraries via neural architecture fuzzing icse may pittsburgh pa usa andexecutiontime.forthresholdsettings e.g.
t max candmax v since in every experiment muffinachieves function usage we do not increase max candmax v. for threshold t as discussed in section .
we choose a quite large threshold.
slightly changing t e.g.
from .
to .
has little impact on the results.
.
future directions muffincanbepotentiallyimprovedinthefollowingtwoaspects.
first muffinonly covers library codes in the layer function granularitythroughtryingtogeneratemodelcoveringalltheprovided apis.however there maystillbealargeportionoflibrarycodes that cannot be covered e.g.
private methods branches .
in the future muffincan be extended to consider other coverage metrics e.g.
linecoverage branchcoverage andconductsmodelgeneration mutation to explore more library codes.
second muffinstill relies on differential testing to solve the test oracleproblem.however ifdifferentdllibrariesproducethesame wrong results muffincannot identify such bugs.
moreover in the evaluation we also notice that under certain circumstances the modelgenerated by muffinmaycause onelibrary tocrash while theothertwoproduceinconsistentresults.insuchcases itrequires huge human efforts to identify potential bugs.
to get rid of this limitation weintendtodesignmetamorphicrelationsbasedonthe properties of dl models and conducts metamorphic testing to test one library accordingly.
related work asdiscussedbefore cradle andlemon arethemost related work to ours that targets dl library testing both of which require existing dl models and only detect bugs in model inference phase.
different from them muffindetects dl library bugs in modeltrainingphaseviadag basedmodelgeneration.intheliterature thereisabodyofworkfocusingontestingmachinelearning ml libraries as well .
for instance dutta et al.
propose probfuzz to test probabilistic programming systems via generatingprogramsbasedonpre definedtemplates.dwarakanath et al.
adopt metamorphic testing to test image classification applications through mutating the training and testing data.
however these approaches cannot be directly adopted for dl librariestesting.
ontheotherhand thereisagreatdealofresearchesfocusing on the testing of dl models .
in particular many research efforts have been put on designing criteria to measure test adequacy .
for instance pei et al.
first propose neuron coverage as the criteria for testing dl models.
ma et al.
furtherdefinebothneuronandlayerlevelcoveragecriteriatohelpgaugingthetestingqualityofdlmodels.kim et al.
propose surprise coverage based on surprise adequacy which measures relative surprise of each input with respect to the training data.
du et al.
propose a set of similarity metrics and coverage criteria to analyze stateful dl systems such as recurrent neural networks rnns .moreover therearealotofstudiesintend to reveal defects in dl models via generating adversarial inputsor finding corner cases .
for instance tian et al.
implementdeeptestfordetectingerroneousbehaviorsofdl based self drivingcarsviaautomaticallygeneratingtestcasesbasedonimagetransformations.similarly zhang et al.
implementdeeproad which applies generative adversarial networks gans to test dl basedself driving cars.
besides there are alsomany researches focus on detecting different kinds of bugs in model structuresortrainingparametersettings .forinstance zhang et al.
propose debar a static analysis approach for detecting numericalbugsindlmodels.wardat et al.
proposeadynamic analysisbasedapproachtodetectnumericalerrorswhentraining dl models.
similarly zhang et al.
propose autotrainer a toolthatdetectsandauto repairscommonly seenmodeltraining problemssuchasvanishinggradient explodinggradientsandslow convergence.
different from them our work focuses on testing dl libraries rather than dl models or parameters.
ourworkisalsorelatedtodifferentialtesting aneffectivemethod that use similar programs as cross referencing oracles to detectbugs .
differential testing has been successful in uncovering bugs across various types of programs such as compilers javavirtualmachine jvm implementations webapplications and security related apis .
in recent years researchers also utilize differential testing in the area of dl test ing .
for instance pei et al.
propose deepxplore a differential testing framework to identify dl model defects via imagetransformation.guo et al.
proposedlfuzz adifferential fuzzingtestingframeworkthatexposesdlmodelerrorsthrough mutating inputs to maximize model output difference.
these approaches focus on testing dl models while muffinis designed for dl library testing with high coverage.
conclusion in this paper we propose a novel approach to test dl library codes viadirectmodelgenerationusinglibraryapis.inordertogenerate diversedlmodels weusedagtoformulatethemodelstructure and propose a dag based model generation algorithm.
in order to detectbugs wedividethemodeltrainingphaseintothreestages and design different measurements for each stage.
in this way our approach can detect library bugs related to model training which is not covered by previous studies.
we implement our approach as an open source tool called muffin.
to evaluate the performance ofmuffin we conduct a series of experiments based on release versionsofthreewidely useddllibraries.
muffindetects39new bugs in the latest versions of these libraries.
besides muffinoutperformsothermethodsintermsofthenumberofdetectedunique inconsistencies.