lessons from eight years of operational data from a continuous integration service an exploratory case study of circleci keheliya gallaba centre for software excellence huawei canada kingston canada keheliya.gallaba huawei.commaxime lamothe polytechnique montr al montr al canada maxime.lamothe polymtl.cashane mcintosh university of waterloo waterloo canada shane.mcintosh uwaterloo.ca abstract continuousintegration ci isapopularpracticethatenablesthe rapidpaceofmodernsoftwaredevelopment.cloud basedciserviceshavemadeciubiquitousbyrelievingsoftwareteamsofthe hassle of maintaining a ci infrastructure.
to improve these ci services prior research has focused on analyzing historical ci data to help service consumers.
however finding areas of improvement for ci service providers could also improve the experience for service consumers.tosearch for theseopportunities we conductan empiricalstudyof22.2millionbuildsspanning7 795open source projects that used circleci from to .
first we quantitatively analyze the builds i.e.
invocations of theciservice withpassingorfailingoutcomes.weobservethat the heavy and typical service consumer groups spend significantly different proportions of time on seven of the nine build actions e.g.
dependencyretrieval .ontheotherhand thecompilationand testing actions consistently consume a large proportion of build timeacrossconsumergroups median33 .second westudybuilds thatterminatepriortogeneratingapassorfailsignal.througha systematic manual analysis we find that availability issues configuration errors user cancellation and exceeding time limits are key reasons that lead to premature build termination.
ourobservationssuggestthat heavyserviceconsumerswould benefit most from build acceleration approaches that tackle long builddurations e.g.
skippingbuildsteps orhighthroughputrates e.g.
optimizingciservicejobqueues efficiencyincipipelines can be improved for most ci consumers by focusing on the compilationandtestingstages and avoidingmisconfigurationsand tacklingserviceavailabilityissuespresentthelargestopportunities for improving the robustness of ci services.
keywords automated builds build systems continuous integration permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa association for computing machinery.
acm isbn ... .
reference format keheliya gallaba maxime lamothe and shane mcintosh.
.
lessons fromeightyearsofoperationaldatafromacontinuousintegrationservice an exploratory case study of circleci.
in 44th international conference on software engineering icse may pittsburgh pa usa.
acm new york ny usa pages.
introduction continuous integration ci is a software development practice inwhichdevelopmentevents e.g.
thecreationofandupdatesto pull requests pushes to central code repositories trigger build test andreportingroutinesautomatically.themaingoalofciis to provide fast feedback to developers allowing them to verify whethertheirchangescleanlyintegratewithotherchanges.indeed the benefits of ci such as increases in developer productivity and improved software quality have been observed by the software development community .
both open source and proprietary softwareorganizationshavededicatedresourcesto maintaining and operating ci pipelines for this purpose.
dedicatedcloud basedciproviders suchascircleciandtravis ci offer ci services for software organizations.
these services provide consumers with the benefits of ci without the hassle of provisioning operating and maintaining ci infrastructure.
the broad adoption of ci services has facilitated research on ci.
researchershaveinterpretedtheoutcome andduration of the builds run by these ci providers from the perspective of the ci consumers discussing the challenges and benefits of adoptingci .however theciproviders perspectivehasremained largely unexplored.
focusing on build data from the perspective of ci service providers could uncover opportunities to holistically improvethecisolutionsthatswathsofsoftwareteamsrelyupon.forexample focusingoptimizationeffortonslowcistagescoulddrivedown service costs for the ci service providers and simultaneously provide fast feedback to their consumers.
tothatend weconductanexploratorycasestudyof circleci oneofthemostpopularciserviceprovidersforprojectshostedongithub.
our observations are likely to generalize to other popularcloud basedci cdproviders e.g.
travisci githubactions withsimilarfeatures e.g.
yaml basedconfiguration socialcodingplatform integration container based orchestration support job based parallelismforprojectsthattargetmultipleexecutionplatforms .
ouranalysisincludes22.2millionbuildsspanning7 795opensource projectsthatusedcircleciduringtheperiodof2012 .this data enables us to address the following research questions ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa keheliya gallaba maxime lamothe and shane mcintosh rq1 howdoestheusageofaciservicechangeovertime?
motivation studying the growth of circleci in terms of active consumers and productivity heuristics may help to identify specific areas that need attention to improve resource allocation and the overall user experience.results during the last eight years the circleci service has grown in terms of both monthly consumers and totalnumber of builds that are invoked every month.
however this growth has stagnated since mid .
at least ofprojects that were inactive on circleci during have startedusing another ciservice.
throughputand buildduration have been increasing up to minutes per build and 900buildspermonth fortheplatform sheaviestconsumers while median values have remained stable.
inequality in termsofbuildexecutionratesandresourceconsumptionhas steadily increased to gini coefficients of and .recommendations additional resource usage incurred by growth could be tackled by applying build acceleration and skipping approaches from the literature.
the high successrate shows that there is a large pool of builds that couldlikely be safely skipped.
automated repair techniques can be used to further reduce mttr.
rq2 how is time spent during signal generating builds?
motivation signal generatingbuildsterminatewitheither a pass or fail outcome.
by understanding the time consumption of different steps in the ci pipeline service providers canidentifyresourcebottlenecksandestimateoperational costs.
identifying stages that slow down the ci pipeline can allow researchers and developers to target the most impactful stages.results comparedtootherstagesintheciprocess alarger proportion median35 oftheciruntimeisspentonthe compilation and testing stages.recommendations focusingresearcheffortsonacceleratingtestingandcompilationstepsinthecipipelinewillyield the largest reductions to ci workload costs for ci providers and feedback delays for ci consumers.
the heaviest con sumers will benefit most from the allocation of additional networkbandwidthduringdependencyinstallationorthe deployment of local mirrors of dependency archives.
rq3 why are some builds unable to provide a signal?
motivation whenabuilddoesnotprovideasignal developers are not provided with feedback about the changes thattheyhavesubmitted.sincereceivingearlyfeedbackisa keyfeatureofci thesenon signal generatingbuildsarea concern.
studying why builds fail early without providing a signal can help service providers and researchers to developapproaches to mitigate such instances yielding more robust and available ci services.results mostnon signal generatingbuildsoccurduetouser cancellation .
availabilityissues .
configuration errors .
and exceeding time limits .
.recommendations promisingdirectionsforresearchinclude the analysis of why consumers cancel builds and the developmentofapproachestoimprovetraceabilityoforphaned builds and identify builds that are likely to time out.
core concepts in modern ci theconsumers of ci services may subscribe with multiple projects each with its own ci workflow .
a workflow is comprised of one or more jobs.
the result of executing a workflow is referred to as a build.
ci services typically support specifying workflows in a configuration file e.g.
.circleci config.yml for circleci .travis.yml for travis ci .
although this configuration fileis primarily usedto specifythe sequence ofcommands to be executed other options can be customized like parallelism the number of parallel instances of a job to run.
environment environment variables to be set for each build.
resource class the compute resources allocated to each job.
workflows can be configured to invoke builds based on developmentevents e.g.
whenapullrequestiscreatedorupdatedorwhen a push to a central repository is performed on a schedule e.g.
nightly or manually e.g.
programmatically via an api requestor on demand to retry a build without changing the code .
oncea build request is received depending on the subscription of theconsumer and the workflow configuration of the project a buildenvironment e.g.
a set of physical machines virtual machines and or containers is allocated to execute the build.
.
ci build outcomes ci builds are executed with the expectation that they will produce asignalindicatingwhetherchangestothecodebasearereadyto be integrated.
however in practice build outcomes are not always conclusive.
therefore we categorize build outcomes as either signal generating.
buildsthatexecuteuntilapassingorfailing outcomeisproduced.ifabuildpasses ciconsumersknow that the associated changes to the codebase have at least passed the baseline checks.
if a build fails ci consumers can diagnose the problems with their changes while design decisions are still fresh in their minds.
non signal generating.
buildsthatareterminatedbeforecompletion.abuildcouldprematurelyterminateduetoauser aborting the build configuration errors or infrastructure provisioning issues.
these builds do not provide consumers with a meaningful signal about their changes.
in an ideal ci pipeline all builds are signal generating.
if nonsignal generatingbuildsoccuroften serviceconsumerswilllose faith in a provider s ability to deliver a meaningful ci signal.
.
ci indicators ciproviders haveproposedindicators totrackperformance inci pipelines.
circleci1has recently proposed four such indicators build duration.
the time taken for a build to execute.
a long duration may force developers to switch contexts a costly actionfor knowledgeworkerslikesoftware engineers .
ashortdurationmayindicateinadequatetestingisincluded in the ci workflow.
mean time to recovery mttr .
the average length of the time interval between the end times of failing and subse quent passing builds.
a long mttr suggests that failures authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
lessons from eight years of operational data from a continuous integration service icse may pittsburgh pa usa are difficult to diagnose or resolve or a lack of build status monitoring.
a short mttr suggests that consumers are quicklyresolvingbuildfailureswithouttoomuchdisruption.
success rate.
the proportion of signal generating builds with passingoutcomes.theimportanceofthesuccessrateiscontextsensitive.thesuccessrateinthemainbranchwherea large team collaborates should be kept high to avoid impeding development progress.
on the other hand the success rateinadeveloper specificfeaturebranchmaybeirrelevant.
throughput.
thenumberofbuildsthatareperformedduringa given period of time.
throughput may vary across projects based on the development model and the team size.
this metric gives an indication of expected total server load and network bandwidth usage for the ci service provider.
these four indicators provide insight into the maturity of ci adoptionamongcircleciconsumers.individualsoftwareorganizations also may find these indicators useful to understand how their ci usage compares with others.
recently circleci startedproviding these metrics to their users on a per workflow basis within their web interface.
in this study we use these indicators to characterize the growth of ci usage over time.
study design in this section we provide our rationale for studying circleci section3.
anddescribeourdataextraction section3.
anddata analysis approaches section .
.
.
subject ci provider withthepopularityofciasasoftwaredevelopmentpractice many cloud based providers offer ci services.
a forrester market report2 identifiedfiveleadersinthecloud nativeciareabycomparingtheircurrentproductofferings strategy andmarketpresence namely googlecloudbuild awscodebuild azuredevopsservice gitlab and circleci.
amongthese arealeaders according togithubmarketplacestatistics 3circlecihasthelargestnumber of installs 748k .
therefore considering its popularity and that ci builddataforalargenumberofitsusersisopenlyavailablefora spanofeightyears wechoosetofocusouranalysisoncircleci.
as a leading ci platform circleci has served over one million developers during its nine years of operation.
.
data extraction and filtering figure1providesanoverviewofourdataextractionandfiltering approach.
we describe each step below.
to arrive at reliable conclusions representing the workload of a typicalciserviceprovider itisimportantthatweaccessallpubliclyavailablebuilddataforprojectsthatuse circleci.westart byqueryingforprojectsthatusecircleciinthepublicgithub dataset on google bigquery 5one of the largest publicly availabledatasetsofsoftwarerepositories.forthispurpose wecheck continuous integration tools q3 e res148217 sort 3apopularity desc projects that have a .circleci config.yml configuration file in their version control system see de1 of figure .
this query identifies projects with a circleci configuration.
havingacircleciconfigurationfileinitsversioncontrolsystem isanecessarybutnotsufficientconditiontoconcludethataproject uses the circleci service.
even with a ci configuration file it is possiblethattheciservicewasneveractivatedorthatnocibuilds wererunforaparticularproject.therefore wequerythecircleci api for projects that have run at least one ci build in their lifetime see de2 of figure .
a corpus of projects survive this filter.
next viathecircleciapi weretrievethemetadataofthebuilds thatareassociatedwiththesurvivingprojects.atotalof23 build records were retrieved see de3 of figure .
we remove buildsthathavemissingvaluesformandatoryfields i.e.
platform vcs url build num because we as external observers cannot determine why these fields are incomplete.
we also remove builds thatwerestartedafterdecember31st seede4offigure1 .
wechosethiscutoffdatetoallowonlycompletedcalendaryears intoourdatasetforanalysis.weusethe22 413uniquebuilds spanning projects for further analysis.
.
data analysis figure1providesanoverviewofourdataanalysisapproach.we describe each step below.
we first extract the outcome of each build and label each one as signal generatingornon signal generating.welabelbuildswith an outcome of successorfailedas signal generating builds becausethesebuildsprovideaconclusivesignaltotheuser reporting whethertheirproposedchangescanbeintegratedintothemainline of development or not.
all builds with other outcomes are categorizedasnon signal generatingbecausetheywereprematurely terminated without providing a signal.
next we identify projects that are heavy ci consumers.
we considerprojectsthatconsumealargeproportionofthebuildtimeasheavyconsumersanddetermineathresholdforthisconsumption.
toidentifytheseheavyciconsumers wefirstcalculatethetotal monthlybuildtimeconsumptionforallprojects.then weconsider amonthlybuildtimeconsumptionthresholdvalue abovewhich projectsareconsideredheavyciconsumers.wethenmodifythis thresholdvaluetoverifyitsimpactonthesizeoftheheavyciuser samplethatweobtain.weconsiderathresholdvalueacceptable iffluctuationsinthethresholdvaluepresentlittlevarianceinthe number of heavy ci consumers.
weuseallbuildactivitydatatoanswerrq1.toanswerrq2and rq3 weusesignal generatingandnon signal generatingbuilds respectively.
study results in this section we present the results of our study with respect to ourresearchquestions.foreachresearchquestion wedescribethe approach used to address it and the results that we observe.
rq1 how does the usage of a ci service change over time?
rq1 approach.
for each month throughout the studied period from2012to2020 weplotthenumberofprojectsthatusecircleci authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa keheliya gallaba maxime lamothe and shane mcintosh stage data extraction and filteringraw build activity data filter incomplete buildsretrieve build data projects .3m ci builds7 projects .2m ci builds rq3 non signal generating buildsprojects with circleci con guration select projects with circleci con g projectspublic github project data .9m projects stage data analysisgoogle bigquerycon rm activity on circleci projectscircleci api de1 de2 de3de4compute build noise metricsrq1 ci usage over time rq2 signal generating builds cleaned build activity data split by signal generation statusprojects with registered build activity figure an overview of the approach we followed for data analysis 12active projects per monthbuilds per monthactive projects per month builds per month figure the growth of circleci usage during the period of .
the number of projects that used circleciineachmonthisshowninpurple.thenumberof circlecibuilds of these projects in each month is shown in yellow.both lines are loess smoothed curves with gray shaded ar eas indicating the confidence interval.
and the number of total builds that are run on circleci.
then we plotthegrowthof circleci susagebycomputingtheproject level valuesof a buildduration b meantimeto recovery mttr c success rate and d throughput see section .
.
rq1 results.
figure2showsthegrowthof circleciusageduring thestudiedperiod intermsofthenumberofprojectsthatareusing circleci showninblue andthenumberoftotalbuildsthatare executed on circleci shown in red .
observation the number of builds per month across all studied projects grew over the years reached a peak of builds during the month of april and then declined.
competitive pricing and new features offered by other service providers such as gitlab ci and github actions github s own automation service may havecontributedtoauserexodusfromcircleci.forinstance compared to the free minutes of build time per month in circleci githubactionsprovides20freeparallelbuildsandunlimitedbuild minutes for every open source project.
there are other community and technical factors at play.
for example widder et al.
observedthatprojectswithmorepullrequeststendtobelesslikely to abandon a ci service.
this suggest that the projects derive value fromcibyusingittoevaluatepullrequests.similarly widder et v2 beta release v2 public release 12builds per monthplatform version figure number of builds on circleci platforms and during the time period.
al.found that projects with longer build durations are less likely to abandon ci suggesting that projects with more complex builds are better able to adapt the ci service to fit their needs.
to explore whether other ci service providers are attracting users away from circleci we investigate the ci usage of projects thathavestoppedusingcircleci.forthispurpose wefocuson projectsthathavenotexecutedanycibuildson circleciduring the last year of our analysis.
we find that of projects match this criteria.
then we query the github api to determine if these projects have reported the result of a build from anyotherciserviceduringtheyear2020oriftheyhaveconfigured github actions to execute any ci workflows.
observation at least of projects that were inactive oncircleci duringthe year 2020have startedusing anotherci service.wefoundthatprojectsthatbecameinactiveoncircleci migratedtootherciproviderssuchastravisci githubactions appveyor scrutinizer andsemaphoreci .
the inactive projects on circleci had configured at least different ci services to report results back to github.
some of these projects were configured to use more than one ci service therefore the sum of percentages exceeds .
figure shows the number of builds on the two different circleci platforms during the time period.
the second versionof thecircleciplatform6providedusers withadditional authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
lessons from eight years of operational data from a continuous integration service icse may pittsburgh pa usa 12build duration in seconds a build duration1 hour1 day1 week1 month1 y ear 12mttr in log scale b mean time to recovery mttr 12success rate c success rate0300600900 12monthly throughput d throughput figure the evolution of four ci indicators during the period of in circleci.
the median value of each metric across the subject systems is shown by the black line.
the confidence interval is shown by the gray band.
capabilitieslikenativesupportfordockerimages flexibleresourceallocation customizableimages andsshaccess.thereleaseofthisupdatedplatformmayhavecontributedtotherapidgrowthduringthe2016 2018period.thecircleci2.0platformwasavailableasaclosedbetainnovember2016andwaslatermadepubliclyavailable for all circleci consumers in july .
figure shows that these dates coincide with sharp changes in the circleci build activity.
observation while median project level indicator values have remained stable indicators like throughput and build duration have beenincreasingamongtheplatform sheaviestconsumers.
figure4 shows the evolution of the four project level indicators over the studiedperiod.medianvaluesarecomputedtoaggregateindividual buildstothelevelofaproject.theblacklinesshowtheproviderlevel medians and gray bands show confidence intervals of each metric across the projects.
figure4 a showsthatthemedianbuilddurationstayedbelow seconds throughout our data.however of studied projects tookatleast25minutestobuild.similarly astudyconductedby circleci7between august and august found of studied workflows ran to completion in under four minutes while5 oftheworkflowstookmorethan35minutes.
theselarge projectsmightbenefitfromtheciaccelerationtechniquesproposed in prior research .
figure b shows a fluctuating median mttr before december of because of the low number of observations.
however after this themedianmttrstayedintherangeof12 87mins.similarly the circleci study7reported that of the workflows recovered in minutes.
for the slowest of projects the mttr was atleast one week.
as shown in prior research these projects could be takinga long time torecover from build failuresbecause the software teams are not taking the ci signal seriously.
onlya small proportion of software teams may be focusing on fixingbroken builds.
for the benefit of software teams who are truly strugglingtorepairbuilds researchinautomaticallyrepairingbuildbreakage providingdeveloperassistanceforbuildbreakage resolution andmorebroadlyautomatedprogramrepair can be incorporated into ci services to fix build breakages quickly thereby reducing the mttr.
thelargeconfidenceintervalinfigure4 c showsthatthesuccess rate varies widely between projects.
although the median success rate fluctuates before december of the rate gradually authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa keheliya gallaba maxime lamothe and shane mcintosh 12gini coefficient throughput total build time figure5 ginicoefficientforthroughputandtotalbuildtime during2012 .inequalityofciconsumersisincreasing.
increases later in the studied period and ranges between ofallbuilds.similarly thecirclecistudy7reportedthatmedian success rates were for the default branch and for the nondefaultbranch.thisdemonstratesthat increasingly inthemajority of ci builds the newly introduced changes are not causing any buildfailures.ifthesechangesthatarenotcausinganybuildfailurescanbeidentifiedinadvance theexecutionofsuchcibuilds can be completely skipped saving time and compute resources.
abdalkareem et al.
have proposed a machine learning approach to identify such commits that can be skipped.
jin and servant haveproposedtoreducethehighcostofcibyrunningfewerbuilds while running as many failing builds as early as possible.
similarly ananthanarayanan etal.
haveproposedtouseaprobabilistic model poweredbylogisticregressiontoselectbuildsthataremostlikely to succeed and speculatively execute them in parallel.
these approaches to skip builds can be used by ci providers to prioritize builds that can uncover faults early without wasting computing resources on the growing proportion of passing builds.
figure4 d showsthatthemedianthroughputremainedunder 30throughoutthestudiedperiod roughlyonebuildperday .however for of the studied projects the throughput grew rapidlyand reached a peak of builds per month.
confirming our ob servations the circleci study 7reported that of workflows were invoked fewer than one time per day .
while in the top workflows were invoked over times per day.
observation inequality of build execution and resource consumption has steadily increased over time.
to further investigate the imbalanceofciusageacrossusersduringthestudiedperiod we compute the gini coefficient a popular measure of inequality.
we calculatethe gini coefficient of thethroughput and total build time.theginicoefficientofthethroughputestimatesinequality inthenumberofbuildsbeingexecuted whiletheginicoefficient of total build time provides a finer grained perspective.
we do not considerprojectswithnobuildsforthecalculationofginiindexin agivenmonth.therefore thereportedvaluesshouldbeinterpreted as a lower bound on the true inequality.figure shows the evolution of the gini coefficients of throughputandtotalbuildtimeasloess smoothedcurveswithgrayshadedareasindicatingthe95 confidenceinterval.increasinglyhighgini coefficients indicate that there is a smaller number of projects that generatealarger proportionofbuildsand consumeagreaterproportion of the build time per month.
since these heavy consumers runcibuildsfrequently andputaheavyburdenonaciprovider s resources itisimportantforciproviderslikecirclecitoinvest in approaches that optimize the workloads of their heaviest consumers.
although the demand for ci has rapidly grown over the years ci providers have managed to provide a consistentservice for the regular consumers.
however to cater to theheaviest consumers who account for a growing proportion of the build activity and resources ci providers may benefit fromresearchbreakthroughsintheareasofbuildacceleration and automated build and program repair.
rq2 how is time spent during signal generating builds?
rq2 approach.
to answer rq2 we focus our analysis on signalgeneratingbuilds.thecircleciapiresponseprovidesstartand endtimesforeachstepinagivenbuild.furthermore eachstephas anactiontypewhichis machine infrastructure checkout dependencies compile test database deployment orteardown.
we use the action type and runtime of each build step to compute the percentageofruntimespentexecutingeachactiontypeinabuild.then weapplythescott knotteffectsizedifference esd test to clustertheactiontypesofbuildstepsintostatisticallydistinctranksbasedontheproportionofthebuildtimespentoneachactiontype.
moreover we investigate whether the time spent during the signal generating builds of heavy consumers is different from typicalconsumers.
afteridentification ofheavy consumers we apply the mann whitney u test and cliff s delta statistical instruments.weusethosetestsbecausetheyallowforcomparison of the runtime distributions of build steps in heavy and typical consumer categories without an assumption of normality.
in this rq note that we are investigating how time is spent performingdifferentactionsduringbuildsandnottheamountof timespentoneachaction.therefore wefocusonrelativevalues instead of absolute values with regard to durations.rq2 results.
thetopthreerowsoftable1showthedistribution of signal generating builds used to answer rq2.
observation compilingsourcecodeandrunningteststakeupthe greatestproportionofthebuildruntime.
figure6showstheruntime percentage for each action in signal generating builds.
the median runtimepercentagesofthecompilationandtestingstagesare33.
and32.
respectively.thenextlargestactiontypeisdownloading dependencies with a median of .
and is a statistically distinct ranklowerthancompilationandtesting.focusingeffortstoreduce the timetaken forcompilationand testingstages duringthe build will provide the most value for ci providers.
anattentivereadermaythink thattheresultsofrq2areinfluencedbyaggregatingbuildsfromdifferentprojects.forexample basedontheresultsofrq1 itislikelythatfigure6isinfluencedby authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
lessons from eight years of operational data from a continuous integration service icse may pittsburgh pa usa table distribution of build outcome.
the global percentage of each category is shown in brackets.
outcome signal generating success .
failed .
sub total .
non signal generating canceled .
infrastructure fail .
timedout .
no tests .
null .
sub total .
compiletest dependencies deployment infrastructuremachine teardown checkout database run timeaction type figure runtime percentage of each action type in signalgenerating builds.
the numbered rows indicate the distinctranks discovered by the scott knott esd ranker.
projects that generate many builds many action types.
because we chose to focus on the overall impact of each action type on the ci service providers we believe that the influence of aggregate builddata is relevant for the service providers.
however as we observe in rq1 the heaviest consumers exhibit different behaviour than others.
figure shows the distribution of monthlybuildtimeconsumptionacrossallstudiedprojects.guidedbythis weselect1 oftotalmonthlybuildtimeconsumptionasthethresholdforidentifyingprojectsthatheavilyuseci.basedonthis threshold we identify of projects as heavy ci consumers.
we conservatively selected this threshold based on the distributionofconsumptiondatawhichfollowsalognormaldistribution anderson darlingnormalitytest forlog transformedvalues .
.
inthe log transformed distribution our thresholdis .
standard deviations away from the mean and therefore projects above the threshold are certainly outliers typically sds away from the mean .
tocheck iftheselected thresholdforidentifying heavyciconsumersissuitable wechangethethresholdvalueandseehowmany projects survive.
a more lenient threshold of .
only categorizes fivemore ofthe7 795projects asheavyconsumers.a morestrict threshold of .
removes only four more projects from the group0100200300 .
.
.
median build time consumption in log scale frequency figure distribution of median monthly build time consumption.
dashed black line at marks the threshold forselecting heavy ci consumers.
checkoutcompiledatabasedependenciesdeploymentinfrastructuremachineteardowntest run timeaction typeproject ci usage heavy non heavy figure runtime percentage of each action type in signalgenerating builds in heavy ci consumers vs others.
ofheavyconsumers.thisindicatesthatthechosenthresholdvalue will not heavily impact the sample size of heavy ci consumers.
furthermore notethat the groupof heaviest consumers n identifiedinthisrqandthe27projectswithhighestvaluesforeach indicatorinrq1maynotbethesamesetofprojects.whilethere isconsiderableoverlapbetweenthegroupofheaviestconsumers n identified in this rq and the projects with the highest throughput 15projects ofrq1 thereisnooverlapbetween theheaviestconsumersandthe27projectswiththelargestbuild durations mttrs andsuccessratesofrq1.wedonotbelievethis isaconcernbecausethelargestoutliersofthesemeasuresdonot represent the same concept.
observation for all action types exceptdeployment and compile there were statistically significant differences between heavy user builds and other builds in terms of the runtime percentage.
figure compares the runtime percentage of each action type in the signalgenerating builds of heavy and typical consumer groups.
we apply authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa keheliya gallaba maxime lamothe and shane mcintosh the mann whitney u test to the runtime percentage of each action type for the builds of the two user groups.
the results show thattherearesignificantdifferencesintheusagepatternsandresource consumption of heavy ci consumers compared to typical consumers p .
for all action types except deployment andcompile.
the deployment andcompileaction types had insufficient evidence i.e.
one and zero observations respectively among the builds of heavy consumers to reach any conclusions.
basedon cliff s delta effect sizes are large for infrastructure checkout database andteardown action types.the effect size ismedium for thedependencies action type while for the machineandtestaction types the effect sizes are small.
as heavy consumers account for an increasing portion of resource consumption catering to their needs will have a substantial effectonciprovidersaswell.forexample heavyconsumersare spending more time downloading dependencies during the ci processcomparedtotypicalconsumers.therefore ifacceleratingthose buildsisapriority theciservicecanallocateadditionalnetwork bandwidth during the dependency installation of the heavy consumers.
cachingthe build environment aftera ci buildis run and reusingthatenvironmentinsubsequentbuildscouldhelptoreduce the bandwidth demand as proposed in prior work because it will eliminate the need to download dependencies for some builds.
approachestomaketestingandcompilingfasterwillbenefit a large proportion of ci consumers.
the heaviest consumers andciprovidersasaconsequence willbenefitmostfrom approaches to optimize dependency installation.
rq3 why are some builds unable to provide a signal?
rq3 approach.
first we use an open coding approach on a randomized sample to study the reasons why builds failed to generate a signal.
next we evaluate the identified set of reasons i.e.
codes by developing scripts to label example builds automatically.
we alsomanuallycodea sampleofthescript classifiedbuildstoevaluate correctness.
finally we group codes according to common themes.below wedescribeoursampling discovery validation and grouping procedures in more detail.
sampling .
the bottom six rows of table show the outcomes of .
million non signal generating builds.
since coding all of these builds is impractical we select a random sample for coding.
ourgoalistodiscoverascompleteofasetofreasonsforwhy signals could not be generated for builds as possible.
therefore we strive to achieve saturation with our codes.
similar to prior work wecoderandomlyselectednon signal generatingbuilds until no new codes are discovered for consecutive builds.
we aim to achieve saturation separately for each outcome type provided by circleci of non signal generating builds.
we reach saturationaftercoding53 50and116buildsfor canceled infrastructure fail timedout no tests and nulloutcome types respectively.discovery .
code discovery was performed by all three authors duringremotecodingsessions.duringthecodingsessions theauthorstable reasons why build signals could not be generated.
category availability issues .
orphaned builds .
github unreachable .
infrastructure fail .
configuration errors .
missing or outdated configuration file .
unsupported xcode version .
github missing circleci ssh key .
user cancellation .
timed out .
other .
jointlyanalyzedcircleciapiresponses buildlogs andgitcommits of the sampled non signal generating builds to identify the circumstances that led to the particular outcome type.validation .toensurethattheidentificationofthecodesisconsistentandrepeatable wesynthesizeourmanualcodingbehaviour intoclassificationscripts.werunthesescriptsonasampleof100 non signal generatingbuilds.thesampleisalsomanuallycodedby one author and the inter rater agreement cohen s is computed to measure the agreement between automatic and manual labels.grouping .
weapply open cardsorting to constructa taxonomy of codes to help us to discover latent themes in our detailed coded data.wepresentthesethemestohelpguidedecisionstoimprove futureresearchandpracticeinthecontextofciservices.thiscard sorting activity was performed collaboratively by all authors.rq3 results.
table provides an overview of the coded reasons for why builds were unable to provide signals.
in the validation phase weobserveaperfectinter rateragreement cohen s between automatic and manual labels demonstrating a consistent and repeatable coding.
below we describe the discovered codes in detail according to the four themes that we discovered.
observation at least builds failed to generate a signal duetoavailabilityissuesoftheciserviceandthesupportingservices.
a ci service that is available and reliable is essential to providing prompt feedback to consumers.
however we observe multiple instances where the ci service or the supporting services were unavailable.weobservethat10 993buildswhichwereunableto provideasignalhadan infrastructure fail outcome.according to circleci forum discussions 8builds can terminate with the infrastructure fail outcome due to faults in circleci internalinfrastructureorotherservicesthatarecontactedduringthe builds e.g.
github aws .
although builds that terminate due to infrastructure failures are restarted automatically it wastes time and resources.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
lessons from eight years of operational data from a continuous integration service icse may pittsburgh pa usa of the builds with a nulloutcome builds were not executedandnoexplanationwasprovided.furtherinvestigationrevealedthattherewerenoreferencestothesebuildrequestsfromthe githubside.therefore wecodesuchbuildsasorphanedbuilds.an additional builds with null outcomes had not started because fetching information from github was unsuccessful.
moreover incidentreports9inthecirclecistatusdashboardconfirmedthe occurencesofoccasionalserviceoutagesthatpreventedbuildsfrom executing.
to put these rates of failure in context let us presume that ci providerslikecircleciarestrivingtoachieveindustry standard levelsofavailabilitylikefivenines .
.theseavailabilitylev elsareoftenachievedbycloudservices .intotal atleast371 .
failedtogenerateasignalduetoavailabilityissuesofthe ciserviceandthesupportingservices.thissuggeststhatcircleci fallsalittlebitshortofthisstandard .
.
but it is a goal that is not too far out of reach.
since we do not have access to the internals of circleci service we cannot pinpoint the specific architectural shortcomings that lead to availability issues.
however sincepriorresearchhasidentifiedthatinvestingeffort indisasterrecovery fastfailuredetection andeliminatingsingle points of failure help to improve availability in cloud services we recommend ci services in general to direct their attention to such best practices.
observation the vast majority of cancelled builds were cancelled by user request.
we observe that of the non signalgeneratingbuilds 954of1 wereabruptlyterminated withabuildoutcomeof cancelled .ourcodingrevealstworeasons for cancellation user requests or automatically by circleci if the buildisdeterminedtoberedundant.ifthe auto cancelredundant buildsfeature is enabled circleci cancels any queued or running buildswhenanewerbuildistriggeredonthatsamebranch.wefind thatonly64 077ofthecancelledbuildswerecancelledautomatically.
thevastmajority werecancelledbyuserrequest.since nofurtherreasonisprovided futureuserstudiesmayhelptobetter understand why so many builds are being cancelled by consumers.
observation 917buildstimedoutbeforecompletion.
ifthereis nooutputfromanyofthecommandsduringbuildexecutionfortenminutes circleciterminatesthebuild.weobservethatbuildstime out at all stages of the ci process e.g.
setting up the environment installing dependencies testing .
currently consumers can extend the timeout in the ci configuration if they expect a build stepto continue longer than ten minutes without output.
however programmatically determining if a program will terminate is aform of undecidableproblemknownas the halting problem .
therefore usingonlythesourcecodechangestobebuilt ciservice cannot determine if a build will eventually terminate if given more timeorwillhangforever.althoughterminationanalysisresearch has proposed automatic tools to determine whether some builds will eventually terminate implementing such solutions in a ci servicetosupportamultitudeofprogramminglanguagesandbuild tools is infeasible.
instead ci services can use recent past buildoutcomes to speculate that a build will time out similar to how operatesbasedonthehypothesisthatfailing builds in ci happen consecutively after another build failure.
observation at least builds failed to generate a signal due to misconfigurations.
there are three codes that are associated with misconfigurations that lead to signals not being generated.
at least31 238buildswith nulloutcomesdidnotcompletebecause the ci process was not properly configured e.g.
builds with missing or outdated circleci configuration files builds specifyinganunsupportedxcodeversion 23buildswheregithub was missing a circleci ssh key .
adoption of tools such as ciodor andhansel gretel proposedinpriorresearchto identifyandfixconfigurationerrorsmayreducefutureoccurrences of the builds terminating due to misconfiguration.
furthermore someconfigurationissuesareresolvedovertime by the service provider.
for example in platform version .
of circleci if a command was not defined for the testing phase ofthe ci process the build terminated with the special outcome of no tests .
to prevent this behaviour from interfering with their workflow circleci consumers had to include an explicit no op command in the testing phase of their ci configuration.
we found that36 184buildsterminatedwithabuildoutcomeof no tests .in version .
of the circleci platform this requirement was relaxed such that a test section was not required for build job execution.
availability issues configuration errors user cancellation andexceedingtimelimitsarekeyreasonsthatleadtononsignal generatingbuilds.approachestoincreasetheavailabilityandimprovetherobustnessofciconfigurationwill likelyyieldthelargestreductionsinnon signal generating builds.
threats to validity this section describes the threats to the validity of our study.
.
construct validity we use the mapping of commands and action types provided by circleciapitodeterminethecistageforeachcommandandthencomputethetimedistributionforeachsignal generatingbuild.the accuracy of this mapping depends on circleci s labelling.
to mitigate this threat we manually inspected a sample of commands and their assigned action types for consistency.
someciconsumersmayhaveconfiguredcibuildsthatfinish quicklyandreturnasuccessfulbuildoutcomeinsecondswithout executing any useful tests.
in such situations metrics such as build duration and success rate will not provide value as indicators of theeffectivenessofci.therefore wedonotpromotethesemetrics asgoalsforsoftwareteams.instead weusethemasindicatorsof service usage from the ci providers perspective.
.
internal validity we manually analyze api responses build logs and source code changestocharacterizereasonsthatcausebuildstoterminatewith outprovidingasignal.todiscoverascompleteofasetofreasonsas is possible we set out to achieve saturation in our samples of manuallyanalyzedexamplesofnon signal generatingbuilds.although wesetwhatwebelievetobeaconservativesaturationcriterion authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa keheliya gallaba maxime lamothe and shane mcintosh consecutivelylabelledexampleswithoutdiscoveringanewlabel ourapproachisnotexhaustive.theremaybeotherreasonsthat causedthebuildstoabruptlyterminate whichhavenotyetbeenuncovered.nonetheless wewereabletodiscoverasetofreasonsthatcanbe a linkedtoactionableimplicationsforciserviceproviders and b automatically discovered enabling further characterization and the assessment of mitigation strategies.
.
external validity althoughwestudiedalargesampleofprojects andbuilds .
million spanning a long period eight years our study focusesonthecontextofasingleciprovider.asaconsequenceofthechoiceoftheresearchmethod weaimtoprovidedeeperinsightson a single case and does not aim at statistical generalizability.
on the otherhand modernciprovidersshareasimilarconfigurationinter face simpleyaml baseddsls andlargelyoverlappingfeaturesets e.g.
social coding platform integration container based orchestration job based parallelism support for projects that target multiple execution platforms and multiple programming languages .
therefore we suspect that similar conclusions would be drawn from otherserviceproviders.nevertheless replicationstudiesinother contexts might prove fruitful.
related work in this section we situate our work in the context of the literature ontheanalysisofcidata thechallengesassociatedwithci and the service provider perspective in other devops contexts.
.
analysis of ci datasets the analysis of large collections of historical ci data is not uncommon in the literature.
for example the travis ci service has been the target of several research efforts.
beller et al.
s travistorrent provides a curated set of data about .
million builds from the travis ci service.
their initial analysis indicates that testing is thesingledominantreasonforbuildstofail .durieuxetal.
have curated an even larger set of million travis ci jobs.
using independentcollectionsof traviscidata hiltonetal.
studied how developers use ci and rausch et al.
performed a targeted analysisofbuildfailuresinthecontextofjavaopensourcesystems.
similarly zhang et al.
have studied .
million ci builds to identify the ten most common compiler error types.
complementingtheworkonbuildcategorizationsbasedonfailures inourwork we categorize the build outcome based on whether a signal was provided to the consumer at the end of the build.
priorworkhasdemonstratedthatthesecorpora madeoflarge collections of operational data are not free of noise.
gallaba etal.
found that on average there is at least one build with an inaccurate outcome in every eleven builds that they analyze.zolfagharinia et al.
described the build inflation problem in an analysis of million builds from the perl ecosystem.
felidr et al.
identified four characteristics of projects that use ci technology without adopting ci principles.
since our focus is to provide the ci provider s perspective of the overall usage of the ci service we do not remove noisy builds from our analysis.
a line of work on anti patterns has emerged from the studies of ci data.
gallaba et al.
formulate four anti patterns thatimpact ci specifications and propose hansel gretel to detect and repairthem.
vassallo et al.
identifiedfour additional antipatternsthathinderthebenefitsassociatedwithci andpropose ci odor to detect them.
zampetti et al.
studied q a forums to identify ci smells.
since slow build durations hinder thepace of development ghaleb et al.
characterize builds with longdurations.whilethesepriorworksadopttheciconsumers perspective we focus on the ci providers perspective to facilitate all users amidst the presence of anti patterns.
collectionsofbuilddatahavealsobeenusedtostudythelinksbetweentheadoptionofciandotherprojectcharacteristics.vasilescu et al.
observe an increase in developer reported bugs after theintroductionofci suggestingthatcihelpsdevelopersdiscover more defects.
zhao et al.
report that the introduction of ci technology is associated with a higher rate of successful pull re quests however pull requests tend to take longer to arrive at an outcome.
instead offocusing on individual projectcharacteristics we focus on the overall state of the ci service over time.
.
challenges in ci researchers have also studied the challenges faced when software teams are adopting ci.
by conducting a qualitative study hiltonet al.
found that when using ci developers face trade offs betweenspeedand certainty better accessand security and more configurationoptionsandeaseofuse.pintoetal.
surveyed158 ci users about the benefits and problems of ci systems and found thatdevelopersareunsureaboutwhatconstitutesasuccessfulbuild due to reasons such as flaky tests or misconfigured ci jobs.
widderetal.
identifyinformationoverload organizationalpainpoints configuration slowfeedback andtestingdeficienciesasthemaincipainpoints.usingtheciprovider sperspective ourworkconfirms thatslowfeedbackduetolong runningbuildsandconfiguration issuescausingnon signal generatingbuildsarecommoninpractice strengthening the conclusions from prior work.
.
devops service providers perspective therehavebeenseveralstudiesonthedesignandoperationoflargescale devops services.
schermann and leitner propose using geneticalgorithmsforschedulingexperimentswhencontinuous deployment is used by software organizations.
similarly g nalp et al.
presentrondo atoolsuiteforcontinuousdeploymentof service orientedapplications whichaimsforadeterministicand idempotent deployment process.
goingbeyondresearchexperiments practitionersalsoreporton state of the artdevopsplatformsdevelopedatlarge scalesoftwareorganizations.esfahanietal.
describehowmicrosoft sinternal distributedbuildservicewasdesignedtospeedupthe ciworkflow oftheirexistingprojects.guptaetal.
reportonhowalargescaleonlineexperimentationplatformwasdesignedatmicrosoftto provide scalable and trustworthy results for internal users in their controlledexperiments.anexperiencereportbysavoretal.
presents observations from following the continuous deployment processofcloud basedsoftwareatfacebookandoanda.similarly rossietal.
describehowcontinuousdeploymentispracticed during mobile software development at facebook.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
lessons from eight years of operational data from a continuous integration service icse may pittsburgh pa usa whilepriorworkfocusesondesigningandmaintainingdevops infrastructureforinternalusers exposingtheservicetoexternal usersmayalsopresentuniquechallenges.wethereforelookinto the challenges in providing ci services for external users.
conclusions lessons learned through an empirical study of .
million builds from the popular circleci service we set out to better understand the challenges thatciserviceprovidersfaceandtheopportunitiesthatarepresent.
we make ten observations see section from which we conclude thatpriorresearchinnovationsarewell suitedtoaddresscurrent limitations in ci services the rapidly growing build throughput and build durations ofheavyconsumers observation3 suggestthatbuildaccelerationapproachesareneededtostemthisrisingtide.
large softwareorganizationslikemicrosoftandgooglehaveproduced internalsolutionstoacceleratebuilds however theiradoption inothersettingspresentschallenges.approacheslikekotinos strive to make accelerated builds accessible to other organizations.
thehighrateofsuccessfulbuildssuggeststhatciprovidershavea growingpoolofcandidatebuildswithwhichtoapplytechniques proposed in the literature to skip builds.
for example candidate approaches aim to save resources and time by skipping the executionofbuildsthatareunlikelytofail.priorwork estimates that of successful builds could be skipped equating to .
millionbuilds inoursetting leading toconsiderablesavings for aproviderlikecircleci.furthermore ciproviderscanconsider offering suggestions to developers to fix build errors by using auto matedrepairtechniqueslikebuildmedic hirebuild and deepdelta therebyreducingthemttr.vassalloetal.
have shown that suggesting public solutions to build breakages which can be found online reduces the time to fix breakages by an average of .
hypothesis tools and techniques for accelerating builds will help manage the rapid growth in build throughput and build durations.
focusing optimization effort on compilation and testingstages will likely provide the most benefit.
most of the build timeisspentinthecompilationandtestingstagesforalargeproportionof serviceconsumers observation .therefore approaches to optimize compilation and testing steps effectively reducing their run time or skipping such steps altogether are well suited to drive downservicecostsand improve thr oughput.forexample research in the facebook context has shown that using one such strategy predictive test selection can reduce the total infrastructure costoftestingcodechangesbyafactoroftwo.yettherearechallenges that make adopting such approaches difficult in the global contextofaciprovider.forexample theplethoraofbuildtools lan guagetoolchains andtestingframeworksmakestool andlanguagespecific approaches unlikely to yield optimal results.
any provider side solution will need to operate in the heterogeneous deployment environment in which ci services operate.hypothesis language agnosticsolutionsinciservicesto optimize compilation and testing stages will be most beneficial to reduce build durations.
providing more transparency regarding orphaned buildsmayimprovetheuserexperience.
orphanedbuildsconstitutea largeproportionofthebuildsthatareaffectedbyavailabilityissues observation .
these builds do not provide any details about the internalfailuresthatcausedthedisruptionoftheserviceandarenotlinkedfromgithub.tousers orphanedbuildsareentirelyopaque servicefailures whichmayimpacttheirperceptionoftheservice provider.therefore providingdetailedreportingandtraceability for orphaned builds will improve the user experience helping to retain and attract consumers for the ci service.
hypothesis providingvisibilityaboutorphanedbuildswill improve customer retention and growth.
serviceproviderscanuserecentpastbuildoutcomestoiden tify builds that are likely to timeout.
we find builds timedoutwithoutgeneratingasignal observation9 .timedout buildstakethefullallocatedtimereservedtoperformabuildanddonotprovideachangestatussignal.therefore althoughthesebuilds accountforasmallpercentageofallbuilds fromtheperspective ofbothciprovidersandconsumers timedoutbuildsareawaste of themaximum amountof resources andtime.
however due toits similarity to the halting problem identifying builds that will time out is not trivial.
instead similar to smartbuildskip ci serviceproviders canuse heuristics e.g.
manytime out buildsin ciarefollowedbymoreconsecutivetime outbuilds.
toskipbuilds that are likely to time out saving resources.
hypothesis heuristicssuchasfrequent time outbuildsin therecentpastcanbeusedtoidentifyandskipbuildsthat are likely to time out.
userresearchisneededtobetterunderstandwhybuildsarebeing cancelled by users.
since one of the most common reasons for abruptly terminating ci builds is cancellation by the user observation8 futureresearchisneededtocharacterizethisbehaviour.
for example while cancellation may indicate that a build wasunintentionallytriggered e.g.
publishingaprbeforeitwas ready there may be other cases that more careful user experience engineering could help to mitigate.
hypothesis userresearchwillhelptounderstandreasons for user cancelled builds.
in order to aid in future replication of our results we make our data and scripts publicly available online.