predicting software performance with divide and learn jingzhi gong loughborough university leicestershire united kingdom j.gong lboro.ac.uktao chen university of birmingham birmingham united kingdom t.chen bham.ac.uk abstract predicting the performance of highly configurable software systems is the foundation for performance testing and quality assurance.
to that end recent work has been relying on machine deep learning to model software performance.
however a crucial yet unaddressed challenge is how to cater for the sparsity inherited from the configuration landscape the influence of configuration options features and the distribution of data samples are highly sparse.
in this paper we propose an approach based on the concept of divide and learn dubbed dal.
the basic idea is that to handle sample sparsity we divide the samples from the configuration landscape into distant divisions for each of which we build a regularized deep neural network as the local model to deal with the feature sparsity.
a newly given configuration would then be assigned to the right model of division for the final prediction.
experiment results from eight real world systems and five sets of training data reveal that compared with the state of the art approaches dalperforms no worse than the best counterpart on out of cases within which cases are significantly better with up to .
improvement on accuracy requires fewer samples to reach the same better accuracy and producing acceptable training overhead.
practically dalalso considerably improves different global models when using them as the underlying local models which further strengthens its flexibility.
to promote open science all the data code and supplementary figures of this work can be accessed at our repository ccs concepts software and its engineering software performance .
keywords configurable system machine learning deep learning performance prediction performance learning configuration learning acm reference format jingzhi gong and tao chen.
.
predicting software performance with divide and learn.
in proceedings of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering esec fse december san francisco ca usa.
acm new york ny usa pages.
tao chen is the corresponding author permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for third party components of this work must be honored.
for all other uses contact the owner author s .
esec fse december san francisco ca usa copyright held by the owner author s .
acm isbn .
introduction what will be the implication on runtime if we deploy that configuration?
the above is a question we often hear from our industrial partners.
indeed software performance such as latency runtime and energy consumption is one of the most critical concerns of software systems that come with a daunting number of configuration options e.g.
x264 a video encoder allows one to adjust options to influence its runtime.
to satisfy the performance requirements it is essential for software engineers to understand what performance can be obtained under a given configuration before the deployment.
this not only enables better decisions on configuration tuning but also reduces the efforts of configuration testing .
to achieve the above one way is to directly profile the software system for all possible configurations when needed.
this however is impractical because the number of possible configurations may be too high .
for example hipacc a compiler for image processing has more than possible configurations.
even when such a number is small the profiling of a single configuration can still be rather expensive wang et al.
report that it could take weeks of running time to benchmark and profile even a simple system.
therefore an accurate performance model that can predict the expected performance of a newly given configuration is of high demand.
with the increasing complexity of modern software the number of configurable options continues to expand and the interactions between options become more complicated leading to significant difficulty in predicting the performance accurately .
recently machine learning models have been becoming the promising method for this regression problem as they are capable of modeling the complex interplay between a large number of variables by observing patterns from data .
however since machine learning modeling is data driven the characteristics and properties of the measured data for configurable software systems pose non trivial challenges to the learning primarily because it is known that the configuration landscapes of the systems do not follow a smooth shape .
for example adjusting between different cache strategies can drastically influence the performance but they are often represented as a single digit change on the landscape .
this leads to the notion of sparsity in two aspects only a small number of configuration options can significantly influence the performance hence there is a clear feature sparsity involved .
the samples from the configuration landscape tend to form different divisions with diverse values of performance and configuration options especially when the training data isarxiv .06651v4 feb 2024esec fse december san francisco ca usa jingzhi gong and tao chen limited due to expensive measurement a typical case of sample sparsity .
this is particularly true when not all configurations are valid .
existing work has been primarily focusing on addressing feature sparsity through using tree liked model via feature selection or deep learning .
however the sample sparsity has almost been ignored which can still be a major obstacle to the effectiveness of machine learning based performance model.
to address the above gap in this paper we propose dal an approach to model software performance via the concept of divideand learn .
the basic idea is that to handle sample sparsity we divide the samples configurations and their performance into different divisions each of which is learned by a local model.
in this way the highly sparse samples can be split into different locally smooth regions of data samples and hence their patterns and feature sparsity can be better captured.
in a nutshell our main contributions are we formulate on top of the regression of performance a new classification problem without explicit labels.
we extend classification and regression tree cart as a clustering algorithm to divide the samples into different divisions with similar characteristics for each of which we build a local regularized deep neural network rdnn .
newly given configurations would be assigned into a division inferred by a random forest classifier which is trained using the pseudo labeled data from the cart.
the rdnn model of the assigned division would be used for the final prediction thereafter.
under eight systems with diverse performance attributes scale and domains as well as five different training sizes we evaluate dalagainst four state of the art approaches and with different underlying local models.
the experiment results are encouraging compared with the best state of the art approach we demonstrate that dal achieves no worse accuracy on out of cases with of them being significantly better.
the improvements can be up to .
against the best counterpart uses fewer samples to reach the same better accuracy.
incurs acceptable training time considering the improvements in accuracy.
interestingly we also reveal that dalcan considerably improve the accuracy of an arbitrarily given model when it serves as the local model for each division compared with using the model alone as a global model which is used to learn the entire training dataset .
however the original dalwith rdnn as the local model still produces the most accurate results.
dal s error tends to correlate quadratically with its only parameterdthat sets the number of divisions.
therefore a middle value between and the bound set by cart can reach a good balance between handling sample sparsity and providing sufficient training data for the local models e.g.
d 1ord or divisions in this work.this paper is organized as follows section introduces the problem formulation and the notions of sparsity in software performance learning.
section delineates the tailored problem formulation and our detailed designs of dal.
section presents the research questions and the experiment design followed by the analysis of results in section .
the reasons why dalworks its strengths limitations and threats to validity are discussed in section .
section and present the related work conclude the paper and elaborate data availability respectively.
background and motivation in this section we introduce the background and the key observations that motivate this work.
.
problem formulation in the software engineering community the question introduced at the beginning of this paper has been most commonly addressed by using various machine learning models or at least partially in a data driven manner that relies on observing the software s actual behaviors and builds a statistical model to predict the performance without heavy human intervention .
formally modeling the performance of software with nconfiguration options is a regression problem that builds p f s p r whereby sdenotes the training samples of configuration performance pairs such that x s.xis a configuration and x x1 x2 xn where each configuration option xiis either binary or categorical numerical.
the corresponding performance is denoted as p. the goal of machine learning based modeling is to learn a regression function fusing all training data samples such that for newly given configurations the predicted performance is as close to the actual performance as possible.
.
sparsity in software performance learning it has been known that the configuration space for software systems is generally rugged and sparse with respect to the configuration options feature sparsity which refers to the fact that only a small number of configuration options are prominent to the performance.
we discover that even with the key options that are the most influential to the performance the samples still do not exhibit a smooth distribution over the configuration landscape.
instead they tend to be spread sparsely those with similar characteristics can form arbitrarily different divisions which tend to be rather distant from each other.
this is a typical case of high sample sparsity and it is often ignored in existing work for software performance learning.
in figure we show examples of the configuration samples measured from four real world software systems.
clearly we see that they all exhibit a consistent pattern1 the samples tend to form different divisions with two properties property configurations in the same division share closer performance values with smoother changes but those inbetween divisions exhibit drastically different performance and can change more sharply.
1similar pattern has been registered on all systems studied in this work.predicting software performance with divide and learn esec fse december san francisco ca usa a bdb j b x264 c hsmgp d vp8 figure projection of configurations in the landscape with respect to the performance and two most important options the divisions are circled .
property configurations in the same division can have a closer value on at least one key option than those from the different divisions.
in this regard the values of performance and key configuration options determine the characteristics of samples.
in general such a high sample sparsity is caused by two reasons the inherited consequence of high feature sparsity and the fact that not all configurations are valid because of the constraints e.g.
an option can be used only if another option has been turned on thereby there are many empty areas in the configuration landscape.
when using machine learning models to learn concepts from the above configuration data the model needs to handle the complex interactions between the configuration options with high feature sparsity while capture the diverse characteristics of configuration samples over all divisions caused by the high sample sparsity e.g.
in figure where samples in different divisions have diverged performance ranges.
for the former challenge there have been some approaches proposed to target such such as deepperf andperf al .
however very little work has aimed to address the latter which can be the main obstacle for a model to learn and generalize the data for predicting the performance of the newlygiven configuration.
this is because those highly sparse samples increase the risk for models to overfit the training data for instance by memorizing and biasing values in certain respective divisions especially considering that we can often have limited samples from the configuration landscape due to the expensive measurement of configurable systems.
the above is the main motivation of this work for which we ask how can we improve the accuracy of predicting software performance under such a high sample sparsity?
division1division2divisionnclassifiernew config.
performance cart rdnnrdnn rdnndata samples .........dividing training flowpredicting flowdividingtraining predicting figure the architecture of dal.
divide and learn for performance prediction drawing on our observations of the configuration data we propose dal an approach that enables better prediction of the software performance via divide and learn .
to mitigate the sample sparsity issue the key idea of dalis that since different divisions of configurations show drastically diverse characteristics i.e.
rather different performance values with distant values of key configuration options we seek to independently learn a local model for each of those divisions that contain locally smooth samples thereby the learning can be more focused on the particular characteristics exhibited from the divisions and handle the feature sparsity.
yet this requires us to formulate on top of the original regression problem of predicting the performance value a new classification problem without explicit labels.
as such we modify the original problem formulation equation as below d g s di d p f di p r overall we aim to achieve three goals goal dividing the data samples into diverse yet more focused divisions d building function g and goal training a dedicated local model for each division di building function f while goal assigning a newly coming configuration into the right model for prediction using functions gandf .
figure illustrates the overall architecture of dal in which there are three core phases namely dividing training and predicting .
a pseudo code can also be found in algorithm .
.
dividing the very first phase in dalis to appropriately divide the data into more focused divisions while so by considering both the configuration options and performance values.
to that end the key question we seek to address is how to effectively cluster the performance data with similar sample characteristics goal ?
indeed for dividing the data samples it makes sense to consider various unsupervised clustering algorithms such as k mean esec fse december san francisco ca usa jingzhi gong and tao chen algorithm pseudo code of dal input the expected depth dextracted from cart a new configuration cto be predicted output the predicted performance of c 1ifm then dividing phase.
3s randomly sample a set of configurations and their performance 4t traincart s 5d whiled ddo ifd dthen d extract all the leaf divisions of samples from tat the d th depth else d extract all divisions of samples from tat thed th depth 11d d training phase.
fordi ddo m trainregularizeddnn di predicting phase.
16iffhas not been trained then 17u removing performance data and labeling the configurations based on their divisions in d 18u smote u 19f trainrandomforest u 20di predict f c 21m get the model from mthat corresponds to the predicted division di 22return predict m c birch or dbscan .
however we found that they are ill suited for our problem because the distance metrics are highly system dependent.
for example depending on the number of configuration options and whether they are binary numeric options it is difficult to combine the configuration options and performance value with appropriate discrimination and clustering algorithms are often non interpretable.
as a result in dal we extend classification and regression tree cart as the clustering algorithm lines in algorithm since it is simple with interpretable analyzable structure it ranks the important options as part of training good for dealing with the feature sparsity issue and it does not suffer the issues above .
as illustrated in figure cart is originally a supervised and binary tree structured model which recursively splits some if not all configuration options and the corresponding data samples based on tuned thresholds.
a split would result in two divisions each of which can be further split.
in this work we at first train the cart on the available samples of configurations and performance values during which we use the most common mean performance of all samples for each division dias the prediction ydi di yj diyj in whichyjis a performance value.
for example figure shows a projected example in which the configuration that satisfies rtquality true and threads would lead to an inferred runtime of seconds which is calculated over all the samples involved using equation .
samples 18rtquality?
samples samples runtime 32s samples runtime 122s samples runtime 154s threads ?d 1d 2falsetruefalsetruertqualityperformancethreadsperformancethreadsdivision1division2division2rtqualitydivision1division3divisionsplitfigure projection of cart for vp8 showing the possible divisions with different colors under alternative depth d. by choosing ranking options that serve as the splits and tuning their thresholds in dal we seek to minimize the following overall loss function during the cart training l dl yj dl yj ydl dr yj dr yj ydr wheredlanddrdenote the left and right division from a split respectively.
this ensures that the divisions would contain data samples with similar performance values property while they are formed with respect to the similar values of the key configuration options as determined by the splits thresholds at the finest granularity property i.e.
the more important options would appear on the higher level of the tree with excessive splitting.
however here we do not use cart to generalize prediction directly on new data once it is trained as it has been shown that the splits and simple average of performance values in the division alone can still fail to handle the complex interactions between the options leading to insufficient accuracy .
further with our loss function in equation cart is prone to be overfitting2especially for software quality data .
this exacerbates the issue of sample sparsity under a small amount of data samples which is not uncommon for configurable software systems .
instead what we are interested in are the branch and or leaf divisions made therein with respect to the training data which enable us to use further dedicated and more focused local models for better generalizing to the new data lines in algorithm .
as such the final prediction is no longer a simple average while we do not care about the cart overfitting itself as long as it fits the training data well.
this is similar to the case of unsupervised clustering for which the clustering is guided by implicit labels via the loss function at equation .
specifically in dalwe extract the data samples according to the divisions made by the dth depth of the cart including all the leaf divisions with depth smaller than d. an example can be seen from figure where dis a controllable parameter to be given.
in this way daldivides the data into a range of divisions d each of which will be captured by a local model learned thereafter.
note that when the number of data 2overfitting means a learned model fits well with the training data but works poorly on new data.predicting software performance with divide and learn esec fse december san francisco ca usa samples in the division is less than the minimum amount required by a model we merge the two divisions of the same parent node.
as a concrete example from figure we see that there are two depths when d 1there would be two divisions one branch and one leaf with and samples respectively similarly when d there would be three leaf divisions two of each have samples and one is the division with samples from d 1as it is a leaf.
in this case cart has detected that the rtquality is a more important binary option to impact the performance and hence it should be considered at a higher level in the tree.
note that for numeric options e.g.
threads the threshold of splitting threads is also tuned as part of the training process of cart.
.
training given the divisions produced by the dividing phase we train a local model for the samples from each division identified as part of goal lines in algorithm .
theoretically we can pair them with any model.
however as we will show in section .
the stateof the art regularized deep neural network rdnn namely deepperf published at icse is the most effective one under dalas it handles feature sparsity well for configurable software.
indeed ha and zhang showed that rdnn is more effective than the others even with small data samples when predicting software performance in our study we also evaluate the same systems with small training sample sizes as used in their work .
therefore in dalwe choose rdnn as the underlying local model by default.
in this work we adopt exactly the same structure and training procedure as those used by ha and zhang hence we kindly refer interested readers to their work for the training details .
since the local models of the divisions are independent we utilize parallel training as part of dal.
.
predicting when a new configuration arrives for prediction dalchooses a model of division trained previously to infer its performance.
therefore the question is how to assign the new configuration to the right model goal ?
a naive solution is to directly feed the configuration into the cart from the dividing phase and check which divisions it associates with.
yet since the performance of the new configuration is unforeseen from the cart s training data this solution requires cart to generalize accurately which as mentioned can easily lead to poor results because cart is overfitting prone when directly working on new data .
instead by using the divided samples from the dividing phase which serves as pseudo labeled data we train a random forest a widely used classifier and is resilient to overfitting to generalize the decision boundary and predict which division that the new configuration should be better assigned to lines in algorithm .
again in this way we are less concerned about the overfitting issue of cart as long as it matches the patterns of training data well.
this now becomes a typical classification problem but there are only pseudo labels to be used in the training.
using the example from figure again if d 1then the configurations in the sample set would have a label division1 similarly those in the sample set would result in a label division2 .however one issue we experienced is that even with d the sample size of the two divisions can be rather imbalanced which severely harms the quality of the classifier trained.
for example when training bdb c with samples the first split in cart can lead to two divisions with and samples respectively.
therefore before training the classifier we use synthetic minority oversampling technique smote to pre process the pseudo label data hence the division s with much less data minority can be more repeatedly sampled.
finally the classifier predicts a division whose local model would infer the performance of the new configuration.
.
trade off with the number of divisions since more divisions mean that the sample space is separated into more loosely related regions for dealing with the sample sparsity one may expect that the accuracy will be improved or at least stay similar thereby we should use the maximum possible dfrom cart in the dividing phase .
this however only exists in the utopia case where there is an infinite set of configuration data samples.
in essence with the design of dal the depthdwill manage two conflicting goals that influence its accuracy greater ability to handle sample sparsity by separating the distant samples into divisions each of which is learned by an isolated local model and a larger amount of data samples in each division for the local model to be able to generalize.
clearly a greater dmay benefit goal but it will inevitably damage goal since it is possible for cart to generate divisions with imbalanced sample sizes.
as a result we see das a value that controls the trade off between the two goals and neither a too small nor too large dwould be ideal as the former would lose the ability to deal with sample sparsity while the latter would leave too little data for a local model to learn hence produce negative noises to the overall prediction.
similar to the fact that we cannot theoretically justify how much data is sufficient for a model to learn the concept it is also difficult to prove how many divisions are sufficient for handling the sample sparsity in performance modeling.
however in section .
we will empirically demonstrate that there is a upward quadratic correlation between dvalue and the error incurred by daldue to the conflict between the above two goals.
experiment setup here we delineate the settings of our evaluation.
in this work dalis implemented based on tensorflow and scikit learn .
all experiments were carried out on a machine with intel core i7 2ghz cpu and 16gb ram.
.
research questions in this work we comprehensively assess dalby answering the following research questions rq rq1 how accurate is dalcompared with the state of theart approaches for software performance prediction?
rq2 can dalbenefit different models when they are used locally therein for predicting software performance?
rq3 what is the sensitivity of dal s accuracy to d?esec fse december san francisco ca usa jingzhi gong and tao chen table details of the subject systems.
b n denotes the number of binary numerical options and c denotes the number of valid configurations full sample size .
system b n performance description c used by apache maximum load web server bdb c latency ms database c bdb j latency ms database java x264 runtime ms video encoder hsmgp runtime ms compiler hipacc31 runtime ms compiler vp8 runtime ms video encoder lrzip runtime ms compression tool rq4 what is the model building time for dal?
we ask rq1 to assess the effectiveness of dalunder different sample sizes against the state of the art.
since the default rdnn in dalis replaceable we study rq2 to examine how the concept of divide and learn can benefit any given local model and whether using rdnn as the underlying local model is the best option.
in rq3 we examine how the depth of division d can impact the performance of dal.
finally we examine the overall overhead of dalinrq4 .
.
subject systems we use the same datasets of all valid configurations from real world systems as widely used in the literature .
to reduce noise we remove those that contain missing measurements or invalid configurations.
as shown in table we consider eight configurable software systems with diverse domains scales and performance concerns.
some of those contain only binary configuration options e.g.
x264 while the others involve mixed options binary and numeric e.g.
hsmgp which can be more difficult to model .
the configuration data of all the systems are collected by prior studies using the standard benchmarks with repeated measurement .
for example the configurations of apache a popular web server are benchmarked using the tools autobench andhttperf where workloads are generated and increased until reaching the point before the server crashes and then the maximum load is marked as the performance value .
the process repeats a few times for each configuration to ensure reliability.
to ensure generalizability of the results for each system we follow the protocol used by existing work to obtain five sets of training sample size in the evaluation binary systems we randomly sample n 2n 3n 4n and 5nconfigurations and their measurements where nis the number of configuration options .
mixed systems we leverage the sizes suggested by splcon queror a state of the art approach depending on the amount of budget.
the results have been illustrated in table .
all the remaining samples in the dataset are used for testing.table the training sample sizes used.
ndenotes the number of configuration options in a binary system.
system size size size size size apache n 2n 3n 4n 5n bdb c n 2n 3n 4n 5n bdb jn 2n 3n 4n 5n x264n 2n 3n 4n 5n hsmgp hipacc261 vp8 lrzip .
metric and statistical validation .
.
accuracy.
for all the experiments mean relative error mre is used as the evaluation metric for prediction accuracy since it provides an intuitive indication of the error and has been widely used in the domain of software performance prediction .
formally the mre is computed as mre k k t at pt at wherebyatandptdenote thetth actual and predicted performance respectively.
to mitigate bias all experiments are repeated for runs via bootstrapping without replacement.
note that excluding replacement is a common strategy for the performance learning of configuration as it is rare for a model to learn from the same configuration sample more than once .
.
.
statistical test.
since our evaluation commonly involves comparing more than two approaches we apply scott knott test to evaluate their statistical significance on the difference of mre over runs as recommended by mittas and angelis .
in a nutshell scott knott sorts the list of treatments the approaches that model the system by their median values of the mre.
next it splits the list into two sub lists with the largest expected difference .
for example suppose that we compare a b andc a possible split could be a b c with the rank r of and respectively.
this means that in the statistical sense aandbperform similarly but they are significantly better than c. formally scott knott test aims to find the best split by maximizing the difference in the expected mean before and after each split l1 l l1 l l2 l l2 l whereby l1 and l2 are the sizes of two sub lists l1andl2 from listlwith a size l .l1 l2 andldenote their mean mre.
during the splitting we apply a statistical hypothesis test hto check ifl1andl2are significantly different.
this is done by using bootstrapping and a12 .
if that is the case scott knott recurses on the splits.
in other words we divide the approaches into different sub lists if both bootstrap sampling and effect size test suggest that a split is statistically significant with a confidence level of and with a good effect a12 .
.
the sub lists are then ranked based on their mean mre.predicting software performance with divide and learn esec fse december san francisco ca usa table the median and interquartile range of mre denoted as med iqr for daland the state of the art approaches for all the subject systems and training sizes over runs.
for each case green cells mean dalhas the best median mre or red cells otherwise.
the one s with the best rank r from the scott knott test is highlighted in bold.
approachapache bdb c bdb j x264 hsmgp hipaccvp8 lrzip r med iqr r med iqr r med iqr r med iqr r med iqr r med iqr r med iqr r med iqr deepperf .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
decart .
.
.
.
.
.
.
.
n a n a n a n a n a n a n a n a perf al .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
splconqueror .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
size dal .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
deepperf .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
decart .
.
.
.
.
.
.
.
n a n a n a n a n a n a n a n a perf al .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
splconqueror .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
size dal .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
deepperf .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
decart .
.
.
.
.
.
.
.
n a n a n a n a n a n a n a n a perf al .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
splconqueror .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
size dal .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
deepperf .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
decart .
.
.
.
.
.
.
.
n a n a n a n a n a n a n a n a perf al .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
splconqueror .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
size dal .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
deepperf .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
decart .
.
.
.
.
.
.
.
n a n a n a n a n a n a n a n a perf al .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
splconqueror .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
size dal .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
evaluation .
comparing with the state of the art .
.
method.
to understand how dalperforms compared with the state of the art we assess its accuracy against both the standard approaches that rely on statistical learning i.e.
splconqueror linear regression and sampling methods and decart an improved cart together with recent deep learning based ones i.e.
deepperf a single global rdnn and perf al an adversarial learning method .
all approaches can be used for any type of system except for decart which works on binary systems only.
following the setting used by ha and zhang splconqueror3 anddecart use their own sampling method while dal deepperf andperf al rely on random sampling.
since there are systems and sample sizes each we obtain cases to compare in total.
we use the implementations published by their authors with the same parameter settings.
for dal we setd 1ord 2depending on the systems which tends to be the most appropriate value based on the result under a small portion of training data see section .
.
we use the systems training sizes and statistical tests as described in section .
all experiments are repeated for runs.
.
.
results.
the results have been illustrated in table from which we see that dalremarkably achieves the best accuracy on out of cases.
in particular dalconsiderably improves the accuracy e.g.
by up to .
better than the second best one on size ofvp8.
the above still holds when looking into the results of the statistical test dalis the only approach that is ranked first for out of the cases.
for the cases where daldoes not achieve the best median mre it is equally ranked as the first for two of them.
these conclude that dalis in cases similar to cases or 3since splconqueror supports multiple sampling methods we use the one or combination for the mixed system that leads to the best mre.significantly better cases than the best state of the art for each specific case which could be a different approach .
for cases with different training sample sizes we see that dalperforms generally more inferior than the others when the size is too limited i.e.
size andsize for the binary systems.
this is expected as when there are too few samples each local model would have a limited chance to observe the right pattern after the splitting hence blurring its effectiveness in handling sample sparsity.
however in the other cases especially for mixed systems that have more data even for size dalneeds far fewer samples to achieve the same accuracy as the best state of the art.
for example on lrzip dalonly needs samples size to achieve an error less than while deepperf requires samples size to do so.
another observation is that the improvements of dalis much more obvious in mixed systems than those for binary systems.
this is because the binary systems have fewer training samples as they have a smaller configuration space.
therefore the data learned by each local model is more restricted.
the issue of sample sparsity is more severe on mixed systems as their configuration landscape is more complex and comes with finer granularity.
as a result we anticipate that the benefit of dalcan be amplified with more complex systems and or more training data.
to summarize we can answer rq1 as rq1 dalperforms similar or significantly better than the best state of the art approach in out of cases with up to .
improvements.
it also needs fewer samples to achieve the same accuracy and the benefits can be amplified with complex systems more training samples.
.
dalunder different local models .
.
method.
since the idea of divide and learn can be applicable to a wide range of underlying local models of the divisionsesec fse december san francisco ca usa jingzhi gong and tao chen table the scott knott ranks r on the mre of dalunder different local models and their global counterparts.
the green cells denote the best rank.
raw mre results can be accessed at system dal rdnn dalrf rf dalcart cart dallr lr dalsvr svr dalkrr krr dalknnknn apachesize size size size size bdb csize size size size size bdb jsize size size size size x264size size size size size hsmgpsize size size size size hipaccsize size size size size vp8size size size size size lrzipsize size size size size average .
.
.
.
.
.
.
.
.
.
.
.
.
.
identified we seek to understand how well dalperform with different local models against their global model counterparts i.e.
using them directly to learn the entire training dataset .
to that end we run experiments on a set of global models available in scikit learn and widely used in software engineering tasks to make predictions directly such as cart random forest rf linear regression lr support vector regression svr kernel ridge regression krr and k nearest neighbours knn .
we used the same settings as those for rq1 and all models hyperparameters are tuned in training.
for the simplicity of exposition we report on the ranks rproduced by the scott knott test.
.
.
result.
from table we can obtain the following key observations firstly when examining each pair of the counterparts i.e.
dalxandx dalcan indeed improve the accuracy of the local model via the concept of divide and learn .
in particular for simple but commonly ineffective models like lr dalcan improve them to a considerable extent.
yet we see that daldoes not often lead to a significantly different result when working with cart against using cart directly.
this is as expected since using different cart models for the divisions identified by a cart makes little difference to applying a single cart that predicts directly.
interestingly we also see that our model performs better than the traditional ensemble learning dalcart a cart based divideand learn model performs generally better than rf which uses cart as the local model and combines them via bagging.secondly the default of dal which uses the rdnn as the local model still performs significantly better than the others.
this aligns with the findings from existing work that the rdnn handles the feature sparsity better.
indeed deep learning models are known to be data hungry but our results surprisingly show that they can also work well for a limited amount of configuration samples.
the key behind such is the use of regularization which stresses additional penalties on the more important weights options.
this has helped to relieve the need for a large amount of data during training while better fitting with the sparse features in configuration data.
a similar conclusion has also been drawn from previous studies .
therefore for rq2 we say rq2 thanks to the concept of divide and learn dalis able to significantly improve a range of global models when using them as the underlying local model.
.
sensitivity to the depth d .
.
method.
to understand rq3 we examine different dvalues.
since the number of divisions and hence the possible depth is sample size dependent for each system we use of the full dataset for training and the remaining for testing.
this has allowed us to achieve up to d 4with divisions as the maximum possible bound.
for different dvalues we report on the median mre togetherpredicting software performance with divide and learn esec fse december san francisco ca usa figure the median mre s its iqr area and the average smallest training size of the divisions achieved by dalunder different depths dvalues number of divisions over all systems and runs.
r 1means rank in the scott knott test on mre.
the best ranked dis marked as .
with the results of scott knott test for runs.
we also report the smallest sample size from the divisions averaging over runs.
.
.
results.
from figure we see that the correlation between the error of dalanddvalue is close to quadratic dalreaches its best mre with d 1ord .
at the same time the size of training data for a local model decreases as the number of divisions increases.
since dcontrols the trade off between the ability to handle sample sparsity and ensuring sufficient data samples to train all local models d 1ord 2tends to be the sweet points that reach a balance for the systems studied.
after the point of d ord the mre will worsen as the local models training size often drops dramatically.
this is a clear sign that from that point the side effect of having too less samples to train a local model has started to surpass the benefit that could have been brought by dealing with sample sparsity using more local models.
whend which means only one division and hence dalis reduced to deepperf that ignores sample sparsity the resulted mre is the worst on out of systems the same applied to the case when d .
this suggests that neither too small d e.g.
d 0with only one division nor too larger d e.g.
d 4with up to divisions i.e.
too many divisions are ideal which matches our theoretical analysis in section .
.
therefore we conclude that rq3 the error of dalhas a upward quadratic correlation tod.
in this work d 1ord to divisions reaches a good balance between handling sample sparsity and providing sufficient training data for the local models.
.
overhead of model building .
.
method.
to study rq4 we examine the overall time required and the breakdown of overhead for dalin various phases.
as sometable the overhead ranges across all systems and sizes.
approach overhead min restriction and prerequisite deepperf to none decart .
to .
does not work on mixed systems perf al .
to none splconqueror 4to 3needs to select sampling method s dal to needs to set the depth d dal dividing 4to .
none dal training to none dal predicting .
to none baselines we also illustrate the model building time required by the approaches compared in rq1 .
.
.
result.
from table dalincurs an overall overhead from to minutes.
yet from the breakdown we note that the majority of the overhead comes from the training phase that trains the local models.
this is expected as daluses rdnn by default.
specifically the overhead of dalcompared with deepperf to minutes is encouraging as it tends to be faster in the worstcase scenario while achieving up to .
better accuracy.
this is because each local model has less data to train and the parallel training indeed speeds up the process.
in contrast to perf al a few seconds to one minute dalappears to be rather slow as the former does not use hyperparameter tuning but fixed parameter values .
yet as we have shown for rq1 dalachieves up to a few magnitudes of accuracy improvement.
although splconqueror anddecart have an overhead of less than a minute again their accuracy is much more inferior.
further splconqueror requires a good selection of the sampling method s which can largely incur additional overhead while decart does not work on mixed systems.
finally we have shown in rq3 that dal s mre is quadratically sensitive tod upward hence its value should be neither too small nor too large e.g.
d 1ord 2in this work.esec fse december san francisco ca usa jingzhi gong and tao chen a prediction by dal b prediction by deepperf figure example run of the actual and predicted performance by daland deepperf forvp8.
in summary we say that rq4 dalhas competitive model building time to deepperf and higher overhead than the other state of the art approaches but this can be acceptable considering its improvement in accuracy.
discussion .
why does dalwork?
to provide a more detailed understanding of why dalperforms better than state of the art in figure we showcase the most common run of the predicted performance by dalanddeepperf against actual performance.
clearly we note that the sample sparsity is rather obvious where there are two distant divisions.
deepperf as an approach that relies on a single and global rdnn has been severely affected by such highly sparse samples we see that the model tries to cover points in both divisions but fails to do so as it tends to overfit the points in one or the other.
this is why in figure 5b its prediction on some configurations that should lead to low runtime tend to have much higher values e.g.
when rtquality andthreads while some of those that should have high runtime may be predicted with much lower values e.g.
when rtquality andthreads .dal in contrast handles such a sample sparsity well as it contains different local models that particularly cater to each division identified hence leading to high accuracy figure 5a .
.
strengths and limitations the first strength of dalis that the concept of divide and learn paired with the rdnn can handle both sample sparsity and feature sparsity well.
as from section .
for rq1 this has led to better accuracy and better utilization of the sample data than the state ofthe art approaches.
the second strength is that as from section .
for rq2 dalcan improve different local models compared with when they are used alone as a global model.
while we set rdnn as the default for the best accuracy one can also easily replace it with others such as lr for faster training and better interoperability.
this enables great flexibility with dalto make trade offs on different concerns of the practical scenarios.
a limitation of dalis that it takes a longer time to build the model than some state of the art approaches.
on a machine withcpu 2ghz and 16gb ram dalneeds between and minutes for systems with up to options and more than samples.
.
whyd is highly effective?
we have shown that the setting of dindalshould be neither too small nor too large the key intention behind the dis to reach a good balance between handling the sample sparsity and providing sufficient data for the local models to generalize.
this is especially true when the cart might produce divisions with imbalanced sample sizes e.g.
we observed cases where there is a division with around samples while one other has merely less than .
our experimental results show that such sweet points tend to be d ord 2for the cases studied in this work.
however the notion of too small and too large should be interpreted cautiously depending on the systems and data size.
that is although in this study setting d 1ord 2appears to be appropriate they might become too small settings when the data size increases considerably and or the system naturally exhibits well balanced divisions of configuration samples in the landscapes.
yet the pattern of quadratic correlation between dand the error of dalshould remain unchanged.
.
using dalin practice like many other data driven approaches using dalis straightforward and free of assumptions about the software systems data and environments.
we would recommend setting d 1ord 2by default especially when the data sample size is similar to those we studied in this work.
of course it is always possible to fine tune the dvalue by training dalwith alternative settings under the configuration samples available.
given the quadratic correlation between d and the error it is possible to design a simple heuristic for this e.g.
we compare the accuracy of daltrained with d iandd i starting from d 1and finally selecting the maximum dvaluek such that dalwithk 1is less accurate than dalwithk.
.
threats to validity internal threats.
internal threats to validity are related to the parameters used.
in this work we set the same setting as used in state of the art studies .
we have also shown the sensitivity of daltodand reveal that there exists a generally best setting.
we repeat the experiments for runs and use scott knott test for multiple comparisons.
construct threats.
threats to construct validity may lie in the metric used.
in this study mre is chosen for two reasons it is a relative metric and hence is insensitive to the scale of the performance mre has been recommended for performance prediction by many latest studies .
external threats.
external validity could be raised from the subject systems and training samples used.
to mitigate such we evaluate eight commonly used subject systems selected from the latest studies.
we have also examined different training sample sizes as determined by splconqueror a typical method.
yet we agree that using more subject systems and data sizes may be fruitful especially for examining the sensitivity of dwhich may lead to a different conclusion when there is a much larger set of training configuration samples than we consider in this study.predicting software performance with divide and learn esec fse december san francisco ca usa related work we now discuss the related work in light of dal.
analytical model.
predicting software performance can be done by analyzing the code structure and architecture of the systems .
for example marco and inverardi apply queuing network to model the latency of requests processed by the software.
velez et al.
use local measurements and dynamic taint analysis to build a model that can predict performance for part of the code.
however analytical models require full understanding and access to the software s internal states which may not always be possible feasible.
dalis not limited to those scenarios as it is a data driven approach.
statistical learning based model.
data driven learning has relied on various statistical models such as linear regressions tree liked model and fourier learning models etc.
among others splconqueror utilizes linear regression combined with different sampling methods and a stepwise feature selection to capture the interactions between configuration options.
decart is an improved cart with an efficient sampling method .
however recent work reveals that those approaches do not work well with small datasets which is rather common for configurable software systems due to their expensive measurements.
this is a consequence of not fully handling the sparsity in configuration data.
further they come with various restrictions e.g.
decart does not work on mixed systems while splconqueror needs an extensive selection of the right sampling method s .
in contrast we showed that dalproduces significantly more accurate results while does not limit to those restrictions.
ensemble model.
models can be combined in a shared manner to predict software performance.
for example chen and bahsoon propose an ensemble approach paired with feature selection for mitigating feature sparsity to model software performance.
other classic ensemble learning models such as bagging and boosting e.g.
rf can also be equally adopted.
indeed at a glance our daldoes seem similar to the ensemble model as they all maintain a pool of local models.
however the key difference is that the classic ensemble models will inevitably share information between the local models at one or more of the following levels at the training level e.g.
the local models in boosting learn the same samples but with a different focus the bucket of models i.e.
what chen and bahsoon did builds local models on the same data and uses the best upon prediction.
at the model prediction level e.g.
bagging aggregates the results of local models upon prediction.
dal in contrast has no information sharing throughout the learning as the samples are split and so does the prediction of the local models.
this has enabled it to better isolate the samples and cope with their inherited sparsity e.g.
recall from rq2 the overall accuracy of dalcart is better than rf they both use cart as the local models but learn with and without sharing information .
deep learning based model.
a variety of studies apply neural network with multiple layers and or ensemble learning to predict software performance .deepperf is a state of the art dnn model with l1regularization to mitigate feature sparsity for any configurable systems and it can be more accurate than many other existing approaches.
the most recentlyproposed perf al relied on adversarial learning which consists of a generative network to predict the performance and a discriminator network to distinguish the predictions and the actual labels.
nevertheless existing deep learning approaches capture only the feature sparsity while ignoring the sample sparsity causing serve risks of overfitting even with regularization in place.
compared with those we have shown that by capturing sample sparsity dalis able to improve the accuracy considerably with better efficiency and acceptable overhead.
hybrid model.
the analytical models can be combined with data driven ones to form a hybrid model .
among others didona et al.
use linear regression and knn to learn certain components of a queuing network.
conversely weber et al.
propose to learn the performance of systems based on the parsed source codes from the system to the function level.
we see dalas being complementary to those hybrid models due to its flexibility in selecting the local model when needed the local models can be replaced with hybrid ones making itself a hybrid variant.
in case the internal structure of the system is unknown dalcan also work in its default as a purely data driven approach.
conclusion this paper proposes dal an approach that effectively handles the sparsity issues in configurable software performance prediction.
by formulating a classification problem with pseudo labels on top of the original regression problem dalextracts the branches leaves from a cart which divides the samples of configuration into distant divisions and trains a dedicated local rdnn for each division thereafter.
prediction of the new configuration is then made by the rdnn of division inferred by a random forest classifier.
as such the division of samples and the trained local model handles the sample sparsity while the rdnn deals with the feature sparsity.
we evaluate dalon eight real world systems that are of diverse domains and scales together with five sets of training data.
the results show that dalis effective as it is competitive to the best state of the art approach on out of cases in which of them are significantly better with up to .
mre improvement efficient since it often requires fewer samples to reach the same better accuracy compared with the state of the art flexible given that it considerably improves various global models when they are used as the local model therein robust because given the quadratic correlation a middle dvalue s between and the bound set by cart can be robust and leads to the best accuracy across the cases e.g.
d 1ord 2under the sample sizes in this work.
mitigating the issues caused by sparsity is only one step towards better performance prediction hence the possible future work based ondalis vast including multi task prediction of performance under different environments and merging diverse local models e.g.
a mix of rdnn and lr as part of the divide and learn concept.
consolidating dalwith an adaptive dis also within our agenda.
data availability data code and supplementary figures of this work can be found at our repository december san francisco ca usa jingzhi gong and tao chen