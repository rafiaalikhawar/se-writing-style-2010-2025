a universal data augmentation approach for fault localization huan xie school of big data software engineering chongqing university chongqing china huanxie cqu.edu.cnyan lei school of big data software engineering chongqing university chongqing china yanlei cqu.edu.cnmeng yan school of big data software engineering chongqing university chongqing china mengy cqu.edu.cn yue yu national university of defense technology changsha china yuyue nudt.edu.cnxin xia software engineering application technology lab huawei china xin.xia acm.orgxiaoguang mao national university of defense technology changsha china xgmao nudt.edu.cn abstract data is the fuel to models and it is still applicable in fault localization fl .
many existing elaborate fl techniques take the code coverage matrix and failure vector as inputs expecting the techniques could find the correlation between program entities and failures.
however the input data is high dimensional and extremely imbalanced since the real world programs are large in size and the number of failing test cases is much less than that of passing test cases which are posing severe threats to the effectiveness of fl techniques.
to overcome the limitations we propose aeneas a universal data augmentation approach that gener ates synthesized failing test cases from reduced fe aturespace for more precise fault localization.
specifically to improve the effectiveness of data augmentation aeneas applies a revised principal component analysis pca first to generate reduced feature space for more concise representation of the original coverage matrix which could also gain efficiency for data synthesis.
then aeneas handles the imbalanced data issue through generating synthesized failing test cases from the reduced feature space through conditional variational autoencoder cvae .
to evaluate the effectiveness of aeneas we conduct large scale experiments on versions of programs from manybugs sir and defects4j by six state of the art fl techniques.
the experimental results clearly show that aeneas is statistically more effective than baselines e.g.
our approach can improve the six original methods by on average under the top accuracy.
ccs concepts software and its engineering software testing and debugging.
yan lei is the corresponding author.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa association for computing machinery.
acm isbn .
.
.
.
fault localization imbalanced data data augmentation acm reference format huan xie yan lei meng yan yue yu xin xia and xiaoguang mao.
.
a universal data augmentation approach for fault localization.
in 44th international conference on software engineering icse may pittsburgh pa usa.
acm new york ny usa pages.
.
introduction .
preliminary software debugging is a painstaking but important job for developers which requires a significant level of time and energy.
to reduce the cost various fault localization fl techniques have been proposed during the past several decades.
fl techniques provide automated ways to assist developers in locating buggy lines that may cause unexpected outputs.
!
!
!
!
!
!
statements failures !
m tests test suite program suspiciousness evaluation raw data input rank list output dlfl sfl input figure the overall workflow of fl.
figure.
shows the traditional workflow of fl.
the raw data which includes the coverage matrix and failure vector are collected from runtime information when the program is executing the test cases in the test suite.
in the code coverage matrix the rows and columns correspond to the test cases and source code statements respectively.
each cell i.e.
denoted asxij whereimeansi th test case jis thej th statement is assigned with the value of if the j th statement is executed in the i th test case1 and with the value of otherwise.
for each test case there is a testing result which is marked asyiin figure where imeansi th test case.
the value of will be assigned if the program functions correctly and with 1we can also assign a decimal number to the values of the elements of a vector to represent different weights e.g.
.
ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa huan xie yan lei meng yan yue yu xin xia and xiaoguang mao the value of otherwise.
as we obtained the raw data they serve as the universal input for many fl techniques such as the most popular ones of spectrum based fault localization sfl anddeep learning based fault localization dlfl .
sfl as a classical localization approach devises ingenious formulas that could assign each line of code a suspiciousness value.
many formulas of sfl originate in the statistics coefficient or probability related to the raw data .
different from the intuitive sfl dlfl utilizes the learning ability of deep learning to construct a localization model for each program expecting the model could reflect the complex nonlinear relationship between the statements and test results.
after the training process ends a set of virtual test cases in which each test case only covers one single statement will be fed into the model .
the output of the model is considered to be the suspiciousness of the corresponding covered statement.
actually the effectiveness of the model is closely related to the sample data i.e.
theraw data .
after we obtain the suspiciousness of each statement from either sfl or dlfl we can thus rank the statements in descending order of their suspiciousness.
.
motivation from the pipelines of sfl and dlfl we can observe that the effectiveness of sfl and dlfl heavily relies on the statistical information and quality of the input raw data.
nevertheless the quality of raw data poses severe threats due to the class imbalanced test suite in real world programs.
as presented in table we list the number of total test cases and that of failing test cases for all faulty versions of a subject program in column tt and column ft respectively.
the column ft tt calculates the ratio of the number of failing test cases over that of all test cases.
the values in column ft tt indicate that the failing test cases only take up a small proportion of all test cases i.e.
less than for most programs .
in other words we face a between class imbalance problem rooted in the nature of real world test suite.
furthermore previous studies have found that a class balanced test suite is useful for fault localization .
table statistics of test cases and statements in representative benchmarks.
program tt ft ft tt ts bs bs ts gzip .
.
libtiff .
.
python .
.
space .
.
chart .
.
closure .
.
math .
.
lang .
.
time .
.
mokito .
.
tt totaltest cases ft failing test cases ts total executable statements bs buggy s tatements.
however to the best of our knowledge existing sfl and dlfl techniques just take raw data as input and rarely take the imbalanced data problem into account which may consequently cause the ineffectiveness or inefficiency of their approaches.
in addition generating test cases directly from inputs is nearly impossible dueto the complexity of the program.
thus there is an urgent need to tackle the class imbalanced problem from a different perspective.
recently the utilization of deep learning techniques in fl indicates that there are distinguished features i.e.
specific statements that can make a distinction more effectively between passing test cases and failing ones than traditional sfl.
motivated by the existing dlfl techniques we can take the coverage matrix and failure vector as samples and labels in machine learning domain respectively.
in other words each test case corresponds to a sample and each sample is composed of features i.e.
statements .
in machine learning data augmentation is a commonly used solution to the problem of limited data .
oversampling augmentation is one of the data augmentation approaches which creates synthetic instances by generative networks and adds them into the original data.
inspired by the data augmentation approaches we intend to apply the generative networks to the raw data expecting the synthetic data could help to improve the effectiveness of sfl and dlfl.
when considering the usage of the generative networks we found another important problem originated from the program size.
in common scenarios real world programs are large in size but only several lines of codes or even one statement is responsible for program failure.
the low ratios between the number of buggy statements and the number of total executable ones in table i.e.
the column bs ts confirm the issue.
mapping this situation to feature space the dimension of the data is high and there is only a relatively small number of distinguished features.
the previous study has shown that high dimensional data in the input space is usually not good for learning due to the curse of dimensionality.
.
contributions to deal with the above high dimensionality and class imbalanced problems for better data augmentation we propose aeneas a universal data augmentation approach that gener ates synthesized failing t est cases from reduced fe aturespace for improving the universal fl input i.e.
theraw data .
specifically aeneas consists of two stages for data augmentation namely dimensionality reduction and data synthesis.
stage dimensionality reduction.
a common way to solve the high dimensional problem is feature selection which reduces the dimensionality by selecting a subset of features from the input feature set .
the dimensionality reduction aims at obtaining reduced feature space for a better representation of the original coverage matrix while gaining effectiveness and efficiency.
to this end aeneas adopts a revised principal component analysis pca for feature selection as the first stage.
stage data synthesis.
as for the challenge of imbalanced data a different way of handling this problem is that we can generate synthesized failing test cases.
concretely each synthesized failing test is an n dimensional vector with positive label i.e.
value of where the value of nis the number of statements.
thus aeneas adopts a widely used generative model conditional variational autoencoder cvae to generate the specific class of data i.e.
failing test cases until the class is balanced.
to the best of our knowledge our study is the first ever to process the raw data from the feature perspective in fault localization.
we authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a universal data augmentation approach for fault localization icse may pittsburgh pa usa implement our approach and intend to make it universal to all fl techniques that take the raw data as input.
in summary this paper makes the following contributions .
a new perspective.
we provide a novel insight that coverage matrix and failure vector in fl are equivalent to samples and labels respectively in machine learning.
thus we can apply dimensionality reduction feature selection and other machine learning algorithms to the coverage matrix.
.
a universal approach.
we propose a universal data augmentation approach which can be useful for better localization effectiveness.
we tackle the imbalanced data problem with an idea that generates synthesized failing test cases by using cvae.
along with this we alleviate the high dimensional issue by adopting a revised pca technique.
.
large scale experiments.
we evaluate the effectiveness ofaeneas across versions of real world programs and six representative fl approaches e.g.
dstar and cnn fl and the results reflect the improvement of aeneas over original fl techniques and data sampling approaches.
the remainder of the paper is organized as follows.
section introduces the preliminaries of our approach.
section illustrates our methodology in detail.
section evaluates the effectiveness of our approach while section and discuss the limitations and related work respectively.
finally section concludes the paper.
background .
conditional variational autoencoder in section we find that the test suite is imbalanced in realworld programs.
thus in this paper we mainly focus on handling the imbalanced data problem.
many studies deal with the class imbalanced problem by using different kinds of basic data augmentation methods such as flip rotation scale crop and translation .
later researchers take advantages of generative models e.g.
generative adversarial networks variational autoencoders and convolutional neural networks for data augmentation .
xian et al.
use a popular generative model named conditional generative adversarial network gan to generate features for zero shot learning.
xian et al.
propose a framework that combines gan with another powerful generative model named variational autoencoder vae to learn and generate the convolutional neural network features for any shot learning.
these techniques essentially strengthen the features of the small number of classes.
motivated by these works we are confident of generating synthesized failing test cases from the coverage matrix.
!exp input hidden layer encoder input mse loss hidden layer output decoder kld loss vzlatent variable v condition vector condition vector figure the difference between vae and cvae.
variational autoencoder vae is one of the most popular generative models which aims at modeling the underlying probabilitydistribution of data so that it could sample new data from the distribution.
the structure of vae network includes two parts namely encoder and decoder as presented in figure the contents of the red dotted rectangle are not included in terms of vae .
the encoder part encodes input data into latent variables and the decoder part generates data from the latent variables.
generally the decoder part could be used independently of the encoder part as long as the model is well trained and we can generate new samples by feeding the decoder a random noise vector.
however the labels of newly generated samples are unknown if we just use a random noise vector as input.
in our approach the data we need is the synthesized failing test cases but vae model could not determine the labels of the new samples.
conditional variational autoencoder cvae is an extension of vae which can control the process of data generation.
as shown in figure cvae combines the input data and latent variables with labels i.e.
conditional vector vwrapped by red dotted rectangle in figure in the training process and then the decoder parts can generate data according to given labels.
in other words the label information is used both in the encoder module for supervision and in the decoder module for guiding the generation.
.
principal component analysis principal component analysis pca is a technique for exploratory data analysis dimensionality reduction of large datasets and predictive models construction .
since pca is used in exploratory data analysis we can do data visualization on coverage matrix from a feature s perspective.
concretely we take the coverage matrix as the input of pca algorithm and set the number of principal components to two.
finally we have the data with two dimensions and we can visualize our data in the coordinate system.
figure shows the visualized plots of some subject programs we used in experiments.
there are nine plots in figure and each plot shows two dimensions i.e.
two principal components of the coverage matrix after applying pca.
note that the two dimensions do not correspond to any original statements since new dimensions are obtained by using the linear combination of the original dimensions.
in figure the passing test cases are denoted as blue dots while the orange dots represent the failing test cases.
we can observe that the orange dots are well separated from blue dots in these cases which demonstrates that within the coverage matrix there are distinguished features that can make a distinction between failing test cases and passing test cases.
from the above exploratory experiment we notice that pca is a powerful method that can present intuitive figures of raw data.
on the other hand the reduction process of pca is to create new variables that do notcorrespond to any original features.
thus pca can not be directly used for fl since we need to locate the specific statements i.e.
features but this information is notpreserved by pca.
a revised pca which utilizes the core idea of pca for feature selection can fill this gap and the detail of steps are described in section .
.
methodology .
overview given a faulty program pwithnstatements it is executed by a test suitetwithmtest cases which contains at least one failing authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa huan xie yan lei meng yan yue yu xin xia and xiaoguang mao figure data visualization for coverage matrix after applying pca.
test case.
according to the runtime information by executing all the test cases we can obtain the code coverage matrix xwith the size ofm n and coverage vector x i represents the coverage information of i th test cases i ... m .
by observing the behavior of the program for each test case e.g.
comparing the expected output and the actual one we could deduce the value of failure vector y wherey i means the label of i th test cases.
the code coverage matrix xand the failure vector yare the inputs of our approach.
our approach includes two stages namely dimensionality reduction and data synthesis in figure .
the first stage is dimensionality reduction which takes the coverage matrix xas input and outputs reduced coverage matrix xreduce whose size is m k k n after filtering out the potential irrelevant statements.
the following data synthesis stage will construct a generative model for the program.
in order to model the underlying probability distribution of data xreduce and condition vector v i.e.
one hot vector of the corresponding label are used for the training process.
the cvae model can capture the key latent variables and the decoder can sample specific data by a random noise rand given condition vector v. the generation process ends until the class is balanced i.e.
the number of failing test cases is equal to that of passing test cases.
next we will depict the above steps in detail.
.
dimensionality reduction using revised pca we formally present the steps of dimensionality reduction in figure in this subsection.
algorithm describes the process of feature selection by revised pca in detail.
besides the coverage matrix x another two compulsory parameters i.e.
the number of largest eigenvalues mand the number of principal components k are needed.
the algorithm starts with calculating the covariance matrix ofxand solves all the eigenvalues and eigenvectors of the covariance matrix lines .
then we select the eigenvectors corresponding to the first mlargest eigenvalues line i.e.
weobtain an msize matrixvconsists ofmeigenvectors each eigenvector hasnelements.
next we compute the contribution values denoted as c of each feature component by summing melements in every row of v lines .
according to the contribution value listc we sortcin descending order and store indexes of them into variableicontrimax line .
now the compulsory data are ready we can easily select kstatements according to icontrimax and add the selected column of xintoxreduce lines .
finally the algorithm returns xreduce with the size m k line .
algorithm dimensionality reduction using revised pca input coverage matrix with the size of m n x number of largest eigenvalues m number of principal components k output reduced coverage matrix with size of m k xreduce covx covariance matrix of original samples eigenvec eigenvectors of covx eigenval eigenvalues of covx v select the eigenvec corresponding to the first mlargest eigenval fori i n i do calculate contribution value ci m p vpi end for icontrimax argmax c initializexreduce as none fori i k i do selectedstatement select theicontrimax th column of x add xreduce selectedstatement end for returnxreduce the number of principal components i.e.
k is automatically determined by comparing the number of executed statements with all the faulty historical versions in our datasets.
the key idea is if a faulty version has a larger number of executed statements the reduction ratio would be higher since there are usually several faulty statements no matter what the size of executed statements is.
in detail we divide all the faulty versions into ten equally sized groups in descending order of the number of executed statements.
for the ten groups we set the reduction proportion into ten values of percentages from group with to group with in increments to determine k. .
synthesizing failing test cases using cvae the data synthesis stage has two main steps i.e.
a training step and a generating step as shown in figure .
in the training step there are two structures called neural networks nn encoder and nn decoder wrapped by the dotted rectangle in figure because they utilize the learning ability of neural networks.
for the training step as shown in the top half of figure of the data synthesis stage the cvae model takes the xreduce from the filter stage as training samples.
given any index i i ... m the cvae model first converts the corresponding label y i into authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a universal data augmentation approach for fault localization icse may pittsburgh pa usa test suite tprogram p raw data feature selection training data execution input model update parameters generator generated data sfl dlfl dimensionality reduction data synthesis training generating figure the overview architecture of aeneas.
m !exp input c lc l nn encoder input mse loss cvae generator c lc l...output nn decoder !
!
!
!
!
!
!
!
!
1fl kld loss !
!
!
!
!
!
statements failures !
cv ae model !
!
!
!
!
!
!
vzlatent variable v vrandom vector r condition vector vsynthesis failing tests until data is balanced input batch size condition vector condition vector training data !
!
figure data synthesis stage of aeneas.
one hot vector v i and attaches it to the respective coverage vectorxreduce i .
specifically passing test case i.e.
value of and failing test case i.e.
value of will result in one hot vectors and respectively.
the coverage vector xreduce i and the corresponding one hot vector v i as a whole are the inputs of our model.
followed by the combinational vector there are two convolutional layers cl in figure .
convolutional layer with the filter can extract features from given data and many studies have fully explored the ability of feature extraction of convolutional neural networks .
since the test cases are independent of each other we take one dimensional convolution layers which the dimension of filter in each convolutional layer is one.
then the second convolutional layer of nn encoder generates the mean value mand standard deviation whichmcontrols the value of latent variables and is used to compute the weights of the noise vector.
the noise vector tcomes from randomly sampled from the gaussian distribution.
the encoder finally generates latent variableszby equation .
z m e t to avoid the value of weights e infinitely close to i.e.
is negative infinity the cvae model sets a loss function named kld loss that could be calculated by equation .
kldloss m2 et the nn encoder outputs latent variables zandzis the input of the nn decoder.
nn decoder also considers the condition vector v that we mentioned above.
for each training sample the nn decoder takesz i andv i as inputs and outputs the vector xreconstruct i that has the same dimension of xreduce i through two convolutional layers that are followed by active layers using relu .
note that both nn encoder and nn decoder could have more than two convolutional layers when considering different program size.
by calculating the mean squared error mse of xreduce i and xreconstruct i by equation we get another loss of the training process.
mseloss k j xreduce i xreconstruct i k the goal of the training step is to minimize the sum of kldloss andmseloss .
we use stochastic gradient descent to update network parameters with batch size h which means for every step of the training process a h kmatrix with the number of hcorresponding one hot vectors are fed into the network.
in addition we set the drop rate as to prevent from overfitting .
the network is trained iteratively and the training process will learn a trained model which can sample new data through the underlying probability distribution.
the generating step as shown in the bottom half of figure takes advantage of the nn decoder that is separated from the welltrained cvae model.
specifically we can generate synthesized failing test cases x reduceby feeding the nn decoder a random authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa huan xie yan lei meng yan yue yu xin xia and xiaoguang mao noise vector rfrom gaussian distribution and condition vector v. the conditional vector vis set to because the class we need is positive.
the number of synthesized failing test cases remains to be fixed in this step.
many studies have found that a class balanced test suite is useful for fault localization and algorithms with balanced data should generally surpass these with imbalanced data in performance .
therefore we synthesize failing test cases until the test suite is class balanced.
finally aeneas takesxreduce andx reduceas a whole for the next fl techniques e.g.
sfl and dlfl to address their imbalanced data problem.
.
an illustrative example figure shows an artificial example of a bug.
the bug occurs at line in which the number in the if statement should be instead.
to locate the buggy line there exists a category named sfl we mentioned in section .
here we use gp02 for our motivating example.
the cells below each statement indicate whether the statement is covered by the execution of a test case or not for executed and for not executed and the rightmost cells represent whether the execution of a test case is failing or not for passing and for failing .
we can observe that the original test suite has six test cases in which the test t1andt6are two failing test cases.
aeneas will generate two more synthesized failing test cases i.e.
t7andt8 marked with pink in figure .
the rows that start with gp02 feature selection and gp02 aeneas contain the results of original method the method using feature selection and our approach respectively.
with the original test suite the ranking of suspicious faulty statements of gp02 is s7 s8 s9 s12 s10 s11 s14 s15 s16 s1 s2 s3 s13 s4 s5 s6 .
the first line marked with blue in figure shows the results of feature selection using the revised pca the sign of means the corresponding statement is filtered out.
with the reduced feature space the ranking of suspicious faulty statements is s7 s9 s12 s10 s11 s1 s2 s3 s13 s4 s6 s5 s8 s14 s15 s16 .
with the new test suite generated by aeneas the ranking of suspicious faulty statements of gp02 is s8 s9 s12 s7 s1 s13 s3 s2 s16 s14 s15 s10 s11 s5 s6 s4 .
we can observe that gp02 ranks the faulty statements s3as the 12th place and after dimensionality reduction the rank is 8th place.
aeneas ranks the faulty statement s3as the 4th place showing better localization results.
experiment to evaluate the statistical effectiveness of our approach we have integrated aeneas into a pipeline that could rank the statements by their suspiciousness.
the code implemented in python is publicly available at github2.
our experiment was conducted on a bit linux server with intel r xeon cpus and 128g ram.
the operating system is ubuntu .
.
.
.
datasets we evaluate aeneas on real world programs with all real faults.
as presented in table we list some statistics of subject programs.
2the github repository for this study subject programs program versions loc k test description gzip data compression libtiff image processing python general purpose language space adl interpreter chart java chart library closur e closure compiler math apache commons math lang apache commons lang time standard date and time library mokito mocking framework for java total the space is collected from sir3 the gzip libtiff andpython are collected from manybugs4 while the other programs are from defects4j5.
these programs are commonly used for fault localization.
note that it is time consuming to collect inputs since the programs of defects4j are large in size so we reuse the coverage matrix collected by pearson et al.
.
.
evaluation metrics for the evaluation process we use the following widely used metrics number of top k it is the number of buggy versions with at least one faulty statement that is within the first k position of rank list by a fl technique.
in previous study most respondents view a fault localization as successful only if it can localize bug in the top positions from practical perspective .
following the prior work we assign k with the value of and for our evaluation.
mean average rank mar for a faulty version average rank is the mean rank of all faulty statements in rank list.
mar is the mean average values for the project that includes several faulty versions.
mean first rank mfr it first computes the rank that any of the statements is located first for a faulty version.
then compute the mean value of the ranks for the project.
relative improvement rimp it is to compare the total number of statements that need to be examined to find all faults using aeneas versus the number that need to be examined by using other fault localization approaches.
wilcoxon signed rank tests wsr the metrics above just compare the values by given a specific definition.
to evaluate the statistical significance of our results we adopt wilcoxon signed rank tests following prior studies .
.
research questions and results .
.
rq1.
how does aeneas perform in localizing real faults compared with original state of the art fl techniques?
there are two major popular types of fl spectrum based fault localizaiton sfl and deep learning based fault localization dlfl .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a universal data augmentation approach for fault localization icse may pittsburgh pa usa program bug information s1 read a b c s8 d2 c s15 else output d2 s2 d1 d2 d3 s9 if a s16 output d3 s3 if b s10 a a c s4 d1 b s11 else a a b s5 d2 c s12 d3 a s6 d3 a s13 if c s7 else d1 b s14 output d1 t7 and t8 are new failing tests generated by aeneas.
then there are passing test cases and failing ones.
s3 is faulty.
correct form if b test a b c s1 s2 s3 s4 s5 s6 s7 s8 s9 s10 s11 s12 s13 s14 s15 s16 result t1 t2 t3 t4 t5 t6 gp02 susp .
.
.
.
.
.
.
.
.
.
.
.
rank feature selection result rank t7 .
.
.
.
.
.
.
.
.
.
.
t8 .
.
.
.
.
.
.
.
.
.
.
gp02 aeneas susp .
.
.
.
.
.
.
.
.
.
.
rank figure an example of our approach.
to evaluate the effectiveness of aeneas we use the six state ofthe art fl techniques and compare aeneas with them.
the six techniques are listed as follows dstar6 ochiai and barinel are three traditional sfl methods.
for more details about them please refer to .
mlp fl cnn fl and rnn fl are three of the dlfl techniques which utilize the multi layers neural network convolutional neural network and recurrent neural network for localization.
the source code of mlp fl cnn fl and rnn fl are not public available so we first acquire the source code of cnn fl from the authors and then implement mlp fl and rnn fl based on the source code of cnn fl.
we choose mlp fl cnn fl and rnn fl as our baseline for the following two reasons.
first our work focuses on statementlevel fl whereas many recent dlfl baselines e.g.
deepfl fluccs and trapt belong to a different topic i.e.
methodlevel fl.
second since our approach only relies on the raw data of test cases we select baselines that only take the raw data of test cases as inputs without relying on complex source code structure e.g.
ast analysis which might increase the difficulty to use it in practice when the size of project is large.
thus we do not include them e.g.
deeprl4fl as baselines.
in such baselines the data augmentation approach would be another topic which should consider other structures e.g.
data flow graph and ast to generate samples.
as a reminder in our evaluation when several statements have the same suspiciousness value we adopt the statement order based 6the in dstar formula is usually assigned to .strategy that we sort the statements in ascending order according to the line number.
table the results of top top top mar and mfr by comparison of original method and aeneas.
metric scenario dstar ochiai barinel rnn fl mlp fl cnn fl mfrorigin .
.
.
.
.
.
aeneas .
.
.
.
.
.
marorigin .
.
.
.
.
.
aeneas .
.
.
.
.
.
top 1origin aeneas top 3origin aeneas top 5origin aeneas as presented in table we list the data under top top top5 mar and mfr metrics.
we can observe that aeneas outperforms the original baseline at most cases among six fl approaches.
for instance when examining the top top and top metrics of mlp fl the number of bugs that aeneas can locate is and respectively which means aeneas improves recall at top top and top by and in comparison with original mlp fl.
especially the improvement observed in the experiments through applying aeneas on dlfl is more than that on sfl.
there are two possible reasons for this.
firstly the bugs in defects4j take the most proportion of all bugs and ranks on many faulty versions in defects4j are already at the top by using dstar ochiai and barinel.
secondly aeneas reformulates the data imbalance problem as data augmentation from the feature perspective in machine learning authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa huan xie yan lei meng yan yue yu xin xia and xiaoguang mao domain which may have more positive influence on deep learningbased approaches.
note that the experiment results of original dlfl approaches underperform that of the traditional sfl approaches.
the work has shown that traditional fl approaches e.g.
ochiai perform extremely well on the widely used defects4j leading to a benchmark overfitting problem.
in fact for other datasets dlfl actually performs better .
despite the effectiveness of original dlfl is less than that of original sfl in our experiment we also take into account various kinds of dlfl approaches considering that the aim of our work is to propose a universal data augmentation approach.
when considering the mar andmfr metrics the rank of aeneas is lower than that of baselines for all six fl techniques which indicates that we can always locate buggy line first and find all buggy lines with the least effort.
take barinel as an example the original method will examine .
lines on average to find the first bug in all fault versions i.e.
mfr while our approach only checks .
lines of code .
.
.
of the original method.
to evaluate the effectiveness of aeneas we adopt the aforementioned metrics and the results demonstrate our approach outperforms the baselines.
however these metrics fail to get a statistical comparison of our method.
therefore we adopt the paired wilcoxon signed rank tests wsr which is a more rigorous and scientific metric at the statistical level.
given independent samples xi yi i from a bivariate distribution i.e.
paired samples wsr computes the difference di xi yiand gives the p values.
one assumption of the test is that the differences are symmetric.
the two sided test has the null hypothesis that the median of the differences is zero against the alternative that it is different from zero.
the one sided test has the null hypothesis that the median is positive against the alternative that it is negative or vice versa.
if thep value that calculated by wsr is greater than the given significant level it can be concluded that we can reject the null hypothesis.
table statistical comparison of aeneas and the original methods.
comparsion greater less two sided conclusion dstar a eneas v.s.
dstar .26e .53e better ochiai aeneas v.s.
ochiai .96e .91e better barinel aeneas v.s.
barinel .77e .15e better rnn fl aeneas v.s.
rnn fl .92e .78e better mlp fl aeneas v.s.
mlp fl .90e .81e better cnn fl aeneas v.s.
cnn fl .49e .97e better in our experiment we use both two sided and one sided to check at the level of .
.
for all faulty versions in the dataset we use the list of the rank that locates the first bug by using aeneas as the list ofx whileyiis the element in the list of ranks using original baselines.
table shows the statistical results of wsr in which the columns greater less and two sided are the p values.
the column greater gives the assumption that the median of xi yiis greater than zero but the p values in column greater is greater than which denotes the hypothesis that ranks using aeneas is greater than the ones using original baselines is rejected.
that is to say the number of lines of code to be examined by using aeneas is less than that by using original methods.
from the perspective ofthe statistics we can observe that aeneas obtains better results in six fl techniques.
we also analyze the improvement in terms of mar loc value for a faulty version where the loc means executable lines of code for the faulty version.
figure presents the distributions of mar loc values of six fl techniques for all faulty versions.
for all six fl techniques we can observe that aeneas outperforms the original methods especially dlfl methods.
figure boxplot of mar loc values of aeneas and original methods.
summary for rq1 in rq1 we explore the effectiveness of aeneas over original method.
the results show that our method statistically outperforms original method especially dlfl techniques which indicates that the neural network model is greatly limited by the imbalanced input data.
.
.
rq2.
ho w effective is aeneas compared with the data resampling and undersampling approaches?
in addition to the origin methods we also compare our approach with resampling technique and undersampling technique which aim at obtaining class balanced data by replicating minority samples and removing the majority samples respectively.
for more details about oversampling technique and undersampling technique please refer to gao et al.
and wang et al.
respectively.
as seen in table aeneas also improves over the resampling and undersampling baselines.
from the table we can clearly find thataeneas can achieve very promising overall fault localization results than undersampling and resampling techniques.
for instance aeneas is able to locate and faults within top top and top metrics when using dstar.
apart from the top k metrics the mar and the mfr of aeneas are also the best among the studied approaches.
although the metrics above can show detailed values they miss the statistical information.
we also use wilcoxon signed rank test to evaluate the effectiveness of aeneas over that of the other data sampling techniques.
table shows the statistical results of aeneas over data undersampling and resampling techniques in three sfl techniques and three dlfl techniques.
the conclusion column gives the conclusion according to p value.
for example in comparison to the resampling technique of ochiai the p value of authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a universal data augmentation approach for fault localization icse may pittsburgh pa usa table the results of top top top mar and mfr by comparison of sampling methods and aeneas.
metric scenario dstar ochiai barinel rnn fl mlp fl cnn fl mfrre .
.
.
.
.
.
under .
.
.
.
.
.
aeneas .
.
.
.
.
.
marre .
.
.
.
.
.
under .
.
.
.
.
.
aeneas .
.
.
.
.
.
top 1re under aeneas top 3re under aeneas top 5re under aeneas re means resampling and under means undersampling table statistical comparison of aeneas and the data sampling approaches.
method comparison greater less two sided conclusion dstarundersampling .05e .10e better resampling .37e .74e better ochiaiundersampling .01e .01e better resampling .64e .13e better barinelundersampling .85e .71e better resampling .87e .74e better rnn flundersampling .43e .86e better resampling .32e .66e better mlp flundersampling .81e .36e better resampling .77e .54e better cnn flundersampling .23e .46e better resampling .42e .84e better greater less and two sided are .64e and .13e respectively.
according to the definition of wsr it means that the mfr value ofaeneas is less than that of the resampling technique leading to abetter result.
from the table we can observe that aeneas outperforms others data sampling techniques in almost all cases.
figure visualizes the distributions of mar loc values of six fl techniques for all faulty versions in three scenarios i.e.
the resampling technique the undersampling technique and our approach .
for all six fl techniques we can observe that aeneas outperforms other scenarios.
especially in the undersampling technique the improvement is more significant compared to resampling technique.
summary for rq2 in rq2 we make comparison between aeneas and other data sampling techniques i.e.
data resampling and data undersampling.
based on all experimental results we can find that resampling technique is much better than undersampling technique since data undersampling technique will drop a lot of useful information.
further we can observe that aeneas is more effective over both the data resampling and undersampling techniques.
figure boxplot of mar loc values of fl techniques in scenarios.
.
.
rq3.
is each stage of aeneas necessary for the capability of aeneas?
since aeneas is a two staged i.e.
dimensionality reduction stage and data synthesis stage data augmentation approach it is natural to explore the capability of each stage.
thus we conduct ablation experiments to explore the contributions of each stage.
in order to answer this rq we implement the approach with only dimensionality reduction stage and that with only data synthesis stage and we named them as aeneasreduction andaeneassynthesis respectively.
we use rimp as our metric to demonstrate the contributions of each stage of aeneas.
more specifically we calculate the rimp values ofaeneas aeneasreduction method and aeneassynthesis method over original method.
.
.
.
.
.
.
dstarochiaibarinelrnn fl mlp fl cnn fl synthesis reduction aeneas figure the comparison of aeneas aeneasreduction method and aeneassynthesis method over original method under rimp metric.
figure shows the results of the comparison of the three scenarios over original method under rimp metric.
as we can see in this figure the improvement of only one stage is relatively small than our two stage method aeneas.
take ochiai as example the rimp value of aeneasreduction is .
that of aeneassynthesis is .
while that of aeneas is .
.
the results show that the dimensional reduction stage and data synthesis stage both contribute to aeneas.
the reason may be that the reduced feature space from feature selection can make a better representation than raw data.
therefore the generative model can obtain well trained parameters that can help to generate more robust negative samples which are beneficial to locate bugs.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa huan xie yan lei meng yan yue yu xin xia and xiaoguang mao summary for rq3 in rq3 we explore the capability of each stage ofaeneas our experimental results show that the dimensional reduction stage and data synthesis stage both contribute to our proposed method.
discussion .
reasons for non improvement already nearly balanced data.
we observe that if the test suite of a program is already balanced or nearly balanced aeneas will have limited effect on the program.
chart 257in defects4j is an illustrative example of such case.
this faulty version has four bugs that all belong to nullcheck category.
correspondingly there exist four triggering test cases i.e.
testdrawwithnullmeanvertical testdrawwithnulldeviationvertical testdrawwithnullmeanhorizontal andtestdrawwithnulldeviationhorizontal .
the code coverage matrix collected by pearson et al.
consists of only nine rows five for passing test cases and four for failing test cases.
thus the raw data is nearly balanced in this case.
finally the original dstar method just gives the rank of and both the resampling technique and aeneas can not improve the rank greatly.
there are eight about versions that are already nearly balanced in our experiments.
for the eight versions the improvement is less than under the mfr metric on average.
multiple faults.
we also take chart as the example.
for each triggering test case in chart the program failed due to the different root causes.
this indicates the features that are responsible for failures are distributed.
we conduct an extensive experiment on this faulty version.
concretely we isolate each failing test case and then we get five passing test cases and one failing test case as input.
the rank of aeneas in such case is revealing a considerable improvement.
we further check the original test cases from defects4j benchmark and defects4j reported test cases for chart in total.
in other words the number of lines in the code coverage matrix collected by pearson et al.
is inconsistent with that of original test cases.
the reason is that they only apply fault relevant classes to reduce cpu costs.
applying only fault relevant classes indeed saves cpu costs nevertheless it drops a huge amount of useful information that may contribute to the effectiveness of fault localization.
.
the imbalanced levels distribution of subject programs in this subsection we intend to explore the relationship between the extent of improvement and different levels of imbalance.
heet al.
reported the ratios of and between majority class and minority class could be viewed as imbalanced data.
since the number of test cases of our subject programs is no more than we set three imbalanced levels of subject programs in the order of their appearance in the sequence upper level indicates more balanced data.
ratio that is greater equal than .
.
ratio that is less than .
and greater equal than .
.
ratio that is less than .
and greater equal than .
.
.src.patch .
.
.
.
.
.
ratio .
r0.
r .
.
r .
figure the distribution of imbalanced levels for each subject program.
for each subject program we count the percentage of each level as shown in figure .
in figure the bar for each subject program indicates the distribution of different imbalanced levels.
we count the number of passing test cases and that of failing test cases for a faulty version of a subject program first.
then we calculate the imbalanced ratios of each faulty version and count the number of ratios in listed intervals.
finally we could get the percentage of each level for a subject program.
take math as an example there are bugs and the numbers of ratios of three imbalanced levels are .
.
and .
respectively.
the programs in figure are sorted from left to right in descending order according to imbalanced level .
in order to explore the relationship between the different levels of the imbalance and the extent of improvement.
we define a metric named improvement ratio impr and the impr is defined as follows.
impr maroriginal mar aeneas maroriginal a higher value of impr shows better improvement of our approach.
we calculate the values of impr based on rnn fl method and theimpr value increases as the proportion of imbalanced level decreases.
in other words our method is more effective in more imbalanced data.
.
threats to validity.
integrity and validity of raw data.
we collect the raw data of defects4j by using the results of pearson et al.
for convenience instead of executing test cases to collect coverage matrix and failure vector.
but this may introduce the problems of data integrity and validity.
as we mentioned before pearson et al.
applied each fault localization technique only to the fault relevant classes which directly result in the data integrity problem compared to applying all test cases.
we should further spend more cpu costs to collect more complete data before we conduct the experiment.
another problem is data validity.
pearson et al.
used an improved version of gzoltar that relies heavily on the configurations and the local environment.
even at the same computer with the same configurations the coverage matrices of two independent runs may differ from each other.
a more stable version of gzoltar should be used for collecting raw data.
implementation of baselines and our method.
our implementation of baselines and aeneas may potentially include bugs.
we first implement the dstar ochiai and barinel according to the formulas and then test those methods by hand made test cases for their authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a universal data augmentation approach for fault localization icse may pittsburgh pa usa correctness.
as for rnn fl mlp fl and cnn fl we first acquire the source code of cnn fl from the authors and then implement mlp fl and rnn fl based on the source code of cnn fl.
however neural networks require many parameters for construction such as the units of the hidden layer the number of the layers batch size learning rate and optimizer.
that is the details of rnn fl mlp fl and cnn fl may differ from the original paper.
apart from the implementation of baselines we implement our pipeline of dimensional reduction test cases synthesis and fault localization as different modules which may also contain bugs.
to mitigate those threats our six team members check our code implementation rigorously and make all relevant code publicly available.
generalizability of the results.
our comparison was only conducted on real faults of defects4j sir and manybugs dataset.
those datasets are widely used in fault localization research.
in recent years defects4j is a popular dataset for both fault localization and automated program repair.
however in the early years the fault localization studies frequently used siemens suite and other c programs with small size .
thus it is unknown that whether our approach is also effective or not on these programs and other programs with imbalanced test cases.
further experiments on other datasets should be carried out to migrate this threat.
related work spectrum based fault localization.
in recent years fault localization fl has been intensively studied.
spectrum based fl sfl is one of the typical localization methods.
sfl such as tarantula dstar ochiai barinel jaccard locates bugs from statistical analysis under the assumption that a program entity e.g.
statement method or class executed by more failing test cases and not executed by passing test cases is more suspicious.
followed by this sfl is usually lightweight straightforward and effective.
deep learning based fault localization.
deep neural networks have shown to be promising in many research areas due to their powerful learning ability.
researchers have reformulated the fl as learning the relationship between program entities and failures.
bp neural network based fl mlp fl and cnn fl directly use raw data as training data.
deeprl4fl devises an enhanced coverage matrix and adopts a test case ordering technique to fully explore the learning ability of the convolutional neural network.
other fl techniques such as deepfl fluccs and trapt combine more information for more precise localization.
ablfl takes more comprehensive features i.e.
statistical information e.g.
number of strings number of integers and number of operators semantic information e.g.
code complexity and textual similarity and dynamic information e.g.
coverage matrix stack trace and dynamic program slice into account for localization.
generally one method with more information tends to be more effective and less efficient than that with less information.
however the effectiveness also relies on the structure of neural networks and the quality of input data.
other advanced fault localization techniques.
in recent years there are more advanced fl techniques that leverage various useful approaches.
here are some examples.
automated program repair apr has been an essential research topic and fl is a crucial start in apr pipeline.
lou et al.
improved fl at method level by leveraging the patch execution information of apr techniques asfeedback.
in this work they combine causal inference techniques and machine learning to estimate the failure causing effect of statements.
for more information about different types of fault localization techniques please refer to wong et al.
.
conclusion in this paper we propose aeneas a universal data augmentation approach that aims at improving the effectiveness of fault localization approaches.
aeneas is a novel approach to handle the problems of high dimensional and extremely imbalanced data by feature selection and data synthesis respectively.
our key ideas include treating coverage matrix and failure vector as samples and labels reformulating the test cases generation problem as a data augmentation problem tackling the high dimensional and classimbalanced problem by feature selection and data synthesis more concretely we use a revised pca technique for feature selection and a promising generative model cvae for data synthesis.
in the cvae model we use convolutional layer as its component due to its effectiveness at extracting features.
we have implemented our method and integrated it into the fl pipeline.
further we evaluated aeneas on the parts of manybugs sir and defects4j and the experimental results show that our method statistically outperforms the baselines and other data sampling techniques.
in future work we intend to use more subject programs and replace cvae by more powerful generative models.