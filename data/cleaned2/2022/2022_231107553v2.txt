an extensive study on adversarial attack against pre trained models of code xiaohu du huazhong university of science and technology china xhdu hust.edu.cnming wen huazhong university of science and technology china mwenaa hust.edu.cnzichao wei huazhong university of science and technology china u201911736 hust.edu.cn shangwen wang national university of defense technology china wangshangwen13 nudt.edu.cnhai jin huazhong university of science and technology china hjin hust.edu.cn abstract transformer based pre trained models of code ptmc have been widely utilized and have achieved state of the art performance in many mission critical applications.
however they can be vulnerable to adversarial attacks through identifier substitution or coding style transformation which can significantly degrade accuracy and may further incur security concerns.
although several approaches have been proposed to generate adversarial examples for ptmc the effectiveness and efficiency of such approaches especially on different code intelligence tasks has not been well understood.
to bridge this gap this study systematically analyzes five state of the art adversarial attack approaches from three perspectives effectiveness efficiency and the quality of generated examples.
the results show that none of the five approaches balances all these perspectives.
particularly approaches with a high attack success rate tend to be time consuming the adversarial code they generate often lack naturalness and vice versa.
to address this limitation we explore the impact of perturbing identifiers under different contexts and find that identifier substitution within forandifstatements is the most effective.
based on these findings we propose a new approach that prioritizes different types of statements for various tasks and further utilizes beam search to generate adversarial examples.
evaluation results show that it outperforms the state of the art alert in terms of both effectiveness and efficiency while preserving the naturalness of the generated adversarial examples.
national engineering research center for big data technology and system services computing technology and system lab hust wuhan china hubei engineering research center on big data security hubei key laboratory of distributed system security school of cyber science and engineering hust wuhan china corresponding author cluster and grid computing lab school of computer science and technology hust wuhan china permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse december san francisco ca usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
concepts software and its engineering software testing and debugging computing methodologies neural networks .
keywords adversarial attack pre trained model deep learning acm reference format xiaohu du ming wen zichao wei shangwen wang and hai jin.
.
an extensive study on adversarial attack against pre trained models of code.
inproceedings of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering esec fse december san francisco ca usa.
acm new york ny usa pages.
introduction given the rapid development of deep learning dl many researchers have applied dl techniques in various programming language processing tasks with promising results achieved over recent years and such a trend continuously rises.
the recently proposed transformer architecture which mainly employs the self attention mechanism has shown promising results on dealing with the long range dependency problem which is a critical challenge for traditional sequence models such as the recurrent neural network.
therefore a number of state of the art dl models are designed based on such an architecture one category of which is the pre trained models of code ptmcs such as codebert and codegpt .
via utilizing the pre training techniques domain knowledge in the large scale publicly available code repositories can be learned by ptmcs which can be further leveraged on downstream tasks such as vulnerability detection clone detection and code summarization .
unfortunately recent studies have shown that similar to conventional deep learning models in the domains of computer vision and natural language processing ptmcs can also generate totally different results given two semantically identical input programs one of which a.k.a.
the adversarial example is generated by performing certain semantic preserving transformations to the other .
this is devastating considering that ptmcs have been deployed to a wide range of mission critical applications such as vulnerability detection .
specifically an attacker may easily generate an adversarial example that retains the vulnerability while misleading the ptmc to label it as non vulnerable .arxiv .07553v2 nov 2023esec fse december san francisco ca usa xiaohu du ming wen zichao wei shangwen wang and hai jin one potential way to alleviate such a threat is adversarial retraining where the models under attack are continuously trained with the generated adversarial examples to enhance the robustness .
therefore over the years a number of adversarial attack approaches have been proposed aiming to automatically generate adversarial examples .
existing adversarial attack approaches differ in terms of various design aspects.
first at a high level the semantic preserving transformation can be performed at both the token level e.g.
by identifier substitution and the statement level e.g.
by adding dead code .
second even if certain approaches are designed at the same granularity e.g.
identifier substitution there also exists multiple choices when determining how and what identifiers to be replaced by random selection or via pre defined heuristics .
such design aspects may significantly affect the effectiveness of the attack approaches.
for instance yang et al.
showed that certain pre defined heuristics could outperform random selection on generating new identifier names.
although huge efforts have been made towards advancing adversarial attacks targeting for ptmcs the performance of existing techniques has not been systematically evaluated and compared.
little is known about their advantages and disadvantages.
there is thus an urgent need for a comprehensive empirical study comparing and analyzing the effectiveness of the state of the art sota adversarial attacks targeting ptmcs.
in particular how is the effectiveness and efficiency of the sota approaches with respect to various ptmcs?
how well do different approaches generalize across various code intelligence tasks?
most importantly how is the quality of the adversarial examples generated by different approaches?
if the quality is extremely low the practical usefulness can be compromised since they can be easily perceived by developers.
additionally it is unclear how the context of code perturbations affects the effectiveness of current attack approaches.
understanding such problems is important to guide future researches in this field.
to fill this gap in this study we perform an extensive study on existing sota adversarial attack approaches against ptmcs.
specifically we utilize five sota adversarial attack approaches to attack three widely used ptmcs e.g.
codebert codegpt and plbart .
our evaluation is performed on three well studied code intelligence tasks including one generation task i.e.
code summarization and two understanding tasks i.e.
vulnerability detection and code clone detection .
through extensive evaluations and comparisons our study makes several interesting findings ptmcs can be easily attacked under all the three tasks and they are relatively less robust on the generation tasks compared with understanding tasks there is a trade off between the effectiveness and efficiency for the adversarial attacks the attack approach with the highest success rate usually queries ptmcs for the most times the quality of adversarial examples is heavily influenced by the identifier substitution strategy.
identifiers predicted with context aware information produce the highest quality examples that are very similar to the original code followed by a cosine similarity based substitution strategy.
on the other hand random substitution leads to the lowest quality adversarial examples and replacing identifiers under different types of statements exhibits diverse chances to generate adversarial examples successfully whilesuch chances differ significantly with respect to the generation and understanding tasks.
based on our findings we design an efficient yet effective attack approach called beamattack for code adversarial attack.
beamattack separates identifiers into several groups based on the statements where they are extracted.
it then iteratively selects identifiers in a prioritized manner selecting those that are most likely to result in successful attacks as summarized by our empirical evaluation.
beamattack reduces the attacking costs by dividing identifiers into smaller sub groups and prioritizing them based on the likelihood of successful attacks rather than searching the entire identifier space.
it can also reduce the risk of getting stuck in local optima as opposed to searching each individual identifier similar to wir .
the results on a total of six datasets demonstrate that our approach achieves higher attack success rates with less queries than alert while can preserve the naturalness of the generated adversarial examples i.e.
the generated examples bear a high resemblance to the code written by humans .
to summarize we make the following major contributions originality.
to our best knowledge we perform the first extensive study on existing sota adversarial attacks approaches towards ptmcs under well studied code intelligence tasks.
extensive study.
we systematically compare five state of theart adversarial attack approaches from three perspectives effectiveness efficiency and the quality of generated examples.
our evaluation reveals the strengths and weaknesses of existing approaches highlights useful insights thus paving the way for future researches in this field.
improvement.
based on our empirical findings we exploit the differences among diverse program contexts with respect to the chances of successfully generating adversarial examples and design a simple yet effective attack approach.
our approach has demonstrated promising results via extensive evaluation.
open science.
we have released all the artefacts of our study including the source code and experiment results which available at background .
pre trained models of code ptmc can learn universal language representations on the large corpus and can avoid training a new model from scratch .
the pre training paradigm usually consists of two stages pre training and fine tuning.
in the pre training stage it captures generic language knowledge by employing self supervised learning on a large unlabeled corpus.
in the fine tuning stage the trained model can be fine tuned for different downstream tasks.
ptmc can be divided into three categories based on their architectures encoder only decoder only and encoder decoder models .
encoder only pretrained models can support both the understanding and generation tasks and the most widely used ones are codebert and graphcodebert .
decoder only models are good at generation tasks like code completion while the adopted unidirectional architectures are less effective on understanding tasks such as clone detection .
codegpt is a well known model based on transformer belonging to this category.
encoder decoder models are proposedan extensive study on adversarial attack against pre trained models of code esec fse december san francisco ca usa aiming to tackle both the understanding and generation tasks and plbart as well as codet5 are typical ones of such models.
.
adversarial attack on code .
.
code processing tasks.
following prior works we briefly introduce three typical code processing tasks which involve the understanding task and generation task.
clone detection.
it is a program understanding task aiming to detect whether two source code snippets are identical or similar.
vulnerability detection.
it is another program understanding task whose purpose is to determine if a given code snippet contains vulnerabilities or not.
code summarization.
it is a generation task which aims to generate natural language texts that describe the functionality of a given code snippet.
.
.
definitions.
we give two definitions respectively for the understanding task and generation task.
for the understanding task a classifierf x y is expected to predict the ground truth label ytruth y for a given code snippet x x. the goal of adversarial attack is to add slight perturbations on xto generate adversarial examplesxadvthat can mislead f. specifically an adversarial code example should satisfy the following three requirements adversarial example should mislead the model f xadv f x ytruth .
adversarial perturbations should ensure the code is still syntactically correct.
that is perturbations should conform to the syntax rules of the programming language.
for example for the c language the identifiers can only contain letters numbers and underscores.
xadvshould be semantically equivalent to x i.e.
have exactly the same functionalities and generate the same results given a same input .
for the generation task we take the code summarization as an example.
the model f x y aims to maximize p ytruth x where a given code snippet x xand the ground truth summary ytruth y. since the output yof summarization models contains many possibilities we cannot employ the first requirement in the understanding task to directly determine whether the attack is successful.
the existing work utilizes the decrease on the bleu score to evaluate the performance of attack approaches.
in this paper we follow the existing study to consider an attack successful when the bleu score between adversarial summary and the reference summary is which indicates that the adversarial summary does not match the reference summary at all.
similarly adversarial examples in generation tasks should also meet the requirements and as defined in the understanding tasks.
study design in this section we introduce the design of our empirical study including the selected pre trained models adversarial attack approaches and benchmark datasets.
we then introduce our designed research questions and the corresponding experimental settings.
.
subjects and datasets .
.
target models.
section .
presents the sota ptmcs to date.
for each category of ptmcs we choose one model for evaluation as the previous report indicates that they achieve very close performance.
meanwhile there is no ptmc model that can achieve the optimum performance across different tasks and datasets e.g.
for encoder only model codebert is better than graphcodebert invulnerability detection while vice versa in clone detection.
similarly for the encoder decoder model codet5 outperforms plbart in vulnerability detection while vice versa in clone detection .
therefore we select the most popular and widely used one indicated by the number of citations for each category.
in particular we select codebert codegpt and plbart in our study from each category.
.
.
adversarial attack approaches.
table summarizes the stateof the art adversarial attack approaches published in the major conferences and journals.
the approaches selected in this study are all black box approaches since white box attacks often require to access the information of model structures and parameters which might not be easily obtained in practice and thus attackers typically can only access the provided apis to query the model and white box attacks tend to be model specific in that different models employ different structures and thus an attack approach against a specific model cannot generalize well to other ones.
however in this study we aim to evaluate the selected attack approach against different models under various applications.
among the listed blackbox attacks we exclude the approach proposed by nguyen et al.
because it performs fake api insertion at the class level while the datasets selected to evaluate the three tasks in this study are all at the function level.
among the remaining eight black box attacks four different attacks can be directly reproduced and are thus selected as our study subjects.
as for the remaining four approaches they perform semantic preserving transformations at the statement level e.g.
by inserting dead code or transforming forloop into while loop .
because such approaches usually contain common and similar transformation strategies we summarize widely used strategies and integrate them into one approach.
we briefly introduce the selected five approaches below.
mhm .
mhm performs iterative identifier substitution based onmetropolis hastings m h sampling .
this attack has two main hyperparameters the maximum number of iterations and the number of candidate identifiers per iteration.
the larger the value the higher chance the attack will be successful.
unfortunately it will be less efficient at the same time.
we set these two parameters to and respectively following the original paper .
accent .
accent first selects k candidates for each identifier based on the cosine distance and then selects the best identifier and candidate based on the change in scores before and after substituting the identifiers.
this approach has two main hyperparameters the number of candidate identifiers kand the number of the identifiers max that can be replaced.
for a fair comparison with other attacks we set k to and cancel the parameter max which means we do not limit the number of replaced identifiers.
wir random .
wir random utilizes word importance rank wir to determine the substitution sequence of identifiers which ranks each identifier according to the difference in the probabilities generated by the model before and after renaming the identifier to unk .
then wir random sequentially replaces the sorted identifiers by randomly selecting candidates.
for fair comparison with mhm we also limit the number of candidate identifiers to .
alert .
alert utilizes context aware identifier prediction for substitution.
in particular in terms of the identifier selection strategy alert adopts two methods the greedy algorithm and the genetic algorithm.
we set the relevant hyperparameters followingesec fse december san francisco ca usa xiaohu du ming wen zichao wei shangwen wang and hai jin table summary of existing adversarial attack approaches in ascending order by the publication year approach venuewhite blacktask perturbation damp oopsla whitefunctionality classification code completionrandom substitution dead code insertion mhm aaai black functionality classification random substitution srikant et al.
iclr white functionality classificationrandom substitution dead code insertion rabin et al.
ist 21white blackmethod name predictionrandom substitution style transformation pour et al.
icst blackmethod name prediction code captioning code search code summarizationrandom substitution style transformation nguyen et al.
ase black api recommender fake apis insertion averloc saner black code summarizationrandom substitution style transformation accent tosem black code summarization based on cosine distance alert icse blackauthorship attribution clone detection vulnerability detectioncontext prediction ropgen icse black authorship attribution style transformation carrot tosem whitefunctionality classification clone detection vulnerability detectionrandom substitution wir random issta blackvulnerability detection code summarizationrandom substitution the original paper including the number of candidate identifiers i.e.
and the maximum number of iterations the larger one between numiand where numidenotes the number of identifiers in the code .
styletransfer .
the idea of styletransfer is to perform certain transformations that do not alter the semantics of the program.
in this attack we select some common transformation strategies from existing studies including randomly adding a logstatement replacing while andforloops with each other exchanging two independent statements reordering a binary condition exchanging switch toif randomly adding a try catch block randomly adding a piece of dead code switching the value of a boolean variable and propagating this change.
then we apply transformations to generate ncandidate examples and use them to attack the model.
nis set to in this study to avoid the huge overhead in the attack process.
.
.
datasets.
to ensure the comprehensiveness of our understanding towards the performance of existing attacks we study three tasks vulnerability detection clone detection for understanding tasks and code summarization for generation tasks.
we select representative benchmarks to evaluate them.
for clone detection bigclonebench is a widely used clone detection benchmark that contains four main types of intra project and inter project clones.
to better evaluate the adversarial attacks we adopt the filtered dataset proposed by yang et al.
.
their filtering strategies include removing unlabeled data balancing the two labels clones and non clones and making the data at a computationally friendly scale.
as a result our dataset includes examples for training and examples for validation and testing respectively.
for vulnerability detection the open web application security project owasp benchmark1is a java test suite designed to evaluate vulnerability detection tools and it is widely used in vulnerability detection tasks .
we adopt version .
of this benchmark which contains more data and is suitable for training models.
as a result the dataset includes examples for training and for validation and testing respectively.
for code summarization codesearchnet is a widely used dataset which includes data from six programming languages.
we follow existing works and use the filtered java sub datasets for code summarization which results in examples for training for validation and for testing.
.
evaluation metrics we adopt the following metrics for evaluation.
accuracy.
it is the proportion of correctly predicted instances in the test set which is used in the task of vulnerability detection.
precision recall and f1 score.
these three metrics are used for evaluating clone detection.
precision p is the proportion of cloned pairs correctly predicted as cloned to all pairs predicted as cloned.
recall r is the proportion of cloned pairs correctly predicted as cloned to all known real cloned pairs.
f1 is the harmonic mean of precision and recall and it is calculated as f1 p r p r .
bleu .
bleu is widely used to evaluate the textual similarity between the text generated in generative systems and the groundtruth.
bleu is a variant of bleu where the indicates that four consecutive words gram are used as the matching unit.
we fine tune ptmcs following existing works and table lists the reproduced results.
the results are consistent with the previously reported ones in the original paper which indicates that the models in our experiments have been adequately fine tuned.
table evaluation results on pre trained models of code task vd cd cs metrics acc precision recall f1 bleu codebert .
.
.
.
.
codegpt .
.
.
.
.
plbart .
.
.
.
.
vd vulnerability detection cd clone detection cs code summarization .
research questions the goal of this study is to systematically evaluate and compare the performance of the sota adversarial attack approaches against various ptmc under different pl tasks including their effectiveness and efficiency.
more importantly we are also curious to know the code qualities of the generated adversarial examples since it is reported that the quality of the generated examples is of significant importance .
to our best knowledge it is also the first largescale investigation towards the quality of the adversarial examples.
besides we also investigate whether the context of perturbed identifiers will affect the performance of existing adversarial attack approaches.
we introduce our target rqs in detail as follows rq1 attacking performance how do existing adversarial attack approaches perform against different ptmcs under various tasks?
in this rq we attempt to thoroughly compare the sota adversarial attack approaches based on two criteria .
c1 effectiveness.
we compare the effectiveness of adversarial attacks according to the attack success rate asr which is the percentage of code snippets on which an attack approach can successfully generate adversarial examples given a code dataset.
a higher asr indicates a more effective attack.
c2 efficiency.
we compare the efficiency of adversarial attacks according to two metrics average model queries amq .
amq denotes the number of queries to the attacked model during thean extensive study on adversarial attack against pre trained models of code esec fse december san francisco ca usa generation of adversarial examples which is positively related to the attack running time.
too many model queries will be abnormal and suspicious for the attacked party.
average running time art .
art is an overall metric of the efficiency of the attack approach.
it is not only related to the number of model queries but also to the perturbation strategy.
for example the genetic algorithm is more time consuming than the greedy algorithm .
rq2 adversarial code quality what is the quality of adversarial examples generated by adversarial attacks?
naturalness is crucial in adversarial example generation as highlighted by accent people will easily argue that if the replaced identifiers are significantly different from the original ones the summary should be different.
therefore they use cosine similarity to constrain adversarial examples.
according to the existing works on the evaluation of perturbation towards text and code there are two main aspects concerning the quality of the generated examples.
first the number of tokens that are replaced should be as small as possible.
second the adversarial tokens need to be as similar as the original ones in terms of their semantics.
in this study we evaluate the former with identifier change rate icr and token change rate tcr and the latter with average code similarity acs and average edit distance aed following existing studies .
the calculation of these four metrics are as follows.
for kadversarial examples if there are in total miidentifiers in the ithcode snippet and niidentifiers have been changed in the adversarial examples then icr is evaluated as k 1ni k 1mi beyond the identifiers the source code may contain other code tokens such as keywords operators etc.
tcr is the ratio of the changed tokens in the adversarial example to the total number of tokens in the entire code.
we use the cosine similarity to reflect the code similarity before and after the perturbations are performed.
in particular acs is computed based on the embeddings that vectorized from the source code by codebert aed reflects the character level token differences which is the number of times a token needs to be edited at the character level in order to transform into another.
in general a high quality adversarial example should preserve lower icr aed and tcr while the acs should be higher.
rq3 context of perturbed identifiers how do the contexts of the perturbed identifiers affect the adversarial attacking performance?
existing adversarial perturbations tend to treat all identifiers equally which leads to a large search space and might also compromise the attacking efficiency.
to reduce the search overhead we aim to explore the impact of the contexts of different identifiers on the attacking results in this rq.
in particular we regard the statements where the identifiers reside as contexts and investigate whether perturbing identifiers residing at different contexts will affect the attacking effectiveness.
in this study we select the top five statements that are commonly used in code for investigation which are return if throw try and forstatements.
in addition to these types of statements we also investigate the impact of merely modifying method names and the parameters to verify whether the models are vulnerable to such changes.
we refer to them as method in this study .
we use asr to observe the impact.
particularly we choose two attacks with the highest asr which are mhm and wir random.
finally we use codebert as the target model because it is the most studied ptmc to date.
.
settings of attacks we use the trained models as introduced in section .
.
as the attack targets and adapt the original code of the five attack approaches in this study.
in particular we only make limited modifications on the code specifically focusing on the data loading and a few parameters e.g.
the candidate identifiers as mentioned in section .
.
to serve for the need of processing our selected datasets.
we use all the test set as the target instances i.e.
in total for attacks on vulnerability detection and clone detection.
for the code summarization task we randomly select examples from the test set as instances used for attacks to align with the number of the target instances used in the other two tasks.
meanwhile it is beneficial for our study to explore the differences between the robustness of models for different tasks under the same scale of adversarial attacks.
when evaluating the attack approaches based on identifier substitution we skip source programs without identifiers.
besides we also skip the instances that are classified incorrectly by the model to mitigate the effect of model performance.
such settings are commonly used in adversarial attacks .
although we exclude a small proportion of instances our study is large scale.
in particular we perform attacks on more than target programs with over million queries to various ptmc models.
empirical results in this section we present the results of our empirical studies.
.
attack performance rq1 .
.
effectiveness.
we perform experiments on the five attack approaches and measure their attack success rate asr and the results are shown in table .
generally all the three target models can be easily attacked under the three different tasks.
in particular mhm can achieve the highest asr i.e.
.
averaged over all the experiments followed by wir random i.e.
.
.
based on the results we make the following observations.
first random substitution is more effective than the other perturbation strategies .
specifically both mhm and wir random adopt the strategy of random substitution while alert perturbs identifiers based on context aware prediction.
consequently mhm and wir random outperform alert by .
and .
respectively.
meanwhile such outperformance can be observed for all the three tasks which reflects that random substitution is the most effective strategy to mislead pre trained models.
on the contrary styletransfer is less effective.
we conjecture the behind reason is that existing trained clone detection models are more robust to various code transformation strategies.
for example the cloning method summarized by walker et al.
includes adding deleting code snippets and reordering statements which is very similar to the strategies as adopted by styletransfer.
therefore the clone detection model can learn sufficient code transformation features on such code clone pairs thus being robust to styletransfer.
second the models are more robust against adversarial attacks under the understanding tasks than the generation tasks .
in particular we observe that the asr of the five attacks on the code summarization model is higher than that of the clone detection and vulnerability detection.
the average asrs of clone detection and vulnerability detection are .
and .
much lower than that of code summarization which is .
.
among them mhmesec fse december san francisco ca usa xiaohu du ming wen zichao wei shangwen wang and hai jin table attack success rate on pre trained models of code attack approach mhm accent alert wir styletransfer avg cdcodebert .
.
.
.
.
.
codegpt .
.
.
.
.
.
plbart .
.
.
.
.
.
vdcodebert .
.
.
.
.
.
codegpt .
.
.
.
.
.
plbart .
.
.
.
.
.
cscodebert .
.
.
.
.
.
codegpt .
.
.
.
.
.
plbart .
.
.
.
.
.
average number .
.
.
.
.
cd clone detection vd vulnerability detection cs code summarization achieves an asr over on the three code summarization models on average which shows that these models can be easily attacked under the task of code summarization and output completely irrelevant summaries compared to their original outputs.
third among the different pre trained models codegpt is more resistant to various attacks.
codegpt achieves the lowest asr in of the experiments five attacks for three tasks .
the average asr over the three tasks of the five attacks on codegpt is .
as shown in the last column of table which is lower than that of codebert by .
and plbart by .
respectively.
finding pre trained models with excellent performance can be easily misled by various adversarial attacks.
in particular random strategies are more effective models for the generation tasks are less robust compared to understanding tasks and codegpt is in general more resistant to various attacks.
.
.
efficiency.
table shows the results with respect to amq and art.
via analyzing these two metrics in conjunction with asr we make the following observations.
first the efficiency of different attacks varies greatly.
the average amq of alert and mhm is .
and .
respectively while that of wir random and accent is only .
and .
.
such differences are caused by the characteristics of the attack approaches themselves.
in particular mhm employs a large number of iterations while styletransfer only transfers the target code for a limited number of times to maintain the naturalness of the code.
besides alert uses the genetic algorithm with multiple iterations which tends to repeatedly replace the same identifier while wir random and accent only replace identifiers sequentially according to their importance calculated by the algorithm and they will not repeat replacing identifiers.
for the same attack the efficiency varies on different tasks as well.
specifically the average amq of the five attack approaches on the three ptmcs is .
and .
on clone detection and vulnerability detection but this value is .
on code summarization.
further analysis reveals that it is caused by the low robustness of code summarization models finding .
the high asr of the attack approaches on code summarization models means that attacks can terminate early without performing all iterations or visiting all replaceable variables.
second the number of model queries is positively correlated with the attack successful rate in general.
figure depicts the correlation between amq and asr.
as it reveals attacks with a higher asr often require a larger number of amq.
for example the mhm with the highest asr has an average amq of .
across all models while the corresponding values of accent and styletransfer with a cd b vd c cs d cd e vd f cs figure the correlation between amq art and asr a lower asr are .
and .
respectively.
however the running time art is not necessarily positively correlated with asr.
specifically although styletransfer queries the models for less times than mhm .
vs .
on average it takes much longer time for styletransfer to process the queries than mhm .
mins vs .
.
as a result the metric art is not positively correlated with asr as shown in figure .
it is because an attack approach often contains additional time consumption besides querying the model.
for instance styletransfer usually spends a lot of time on code transformation.
a cd b vd c cs d cd e vd f cs figure the correlation between amq art and the number of identifiers in the target program a cd b vd c cs figure the correlation between asr and the number of identifiers in the target program third the efficiency of various attacks is affected by the total number of identifiers that can be extracted from the program.
as shown in figure amq and art always increase with the number of identifiers and this trend holds for all the three tasks.
such a trend arises from the fact that the number of replaceable identifiers plays the fundamental role in attacks.
specifically both alert and wir utilize the greedy algorithm to iterate over all identifiers.
therefore in the worst case where the attack fails its theoretical number of queries is the product of the number of identifiers and thean extensive study on adversarial attack against pre trained models of code esec fse december san francisco ca usa table average model queries amq and average running time art on attacking codebert codegpt and plbart attack approachaverage model queries amq average running time art min mhm accent alert wir random styletransfer mhm accent alert wir random styletransfer clone detectioncodebert .
.
.
.
.
.
.
.
.
.
codegpt .
.
.
.
.
.
.
.
.
.
plbart .
.
.
.
.
.
.
.
.
.
vulnerability detectioncodebert .
.
.
.
.
.
.
.
.
.
codegpt .
.
.
.
.
.
.
.
.
.
plbart .
.
.
.
.
.
.
.
.
.
code summarizationcodebert .
.
.
.
.
.
.
.
.
.
codegpt .
.
.
.
.
.
.
.
.
.
plbart .
.
.
.
.
.
.
.
.
.
average number .
.
.
.
.
.
.
.
.
.
number of potential candidates.
we further explore the correlation between the number of identifiers in the target program and the attack successful rate and the results in figure show that more identifiers can in general lead to higher attack successful rates.
finding there is a trade off between the effectiveness and efficiency for adversarial attacks.
attacking with higher successful rates often requires a larger number of model queries.
besides the efficiency of attack is also affected by the number of identifiers in the target program.
.
adversarial code quality rq2 the above rq demonstrates the effectiveness and efficiency of adversarial attacks against different ptmcs under various tasks.
however with the recent focus on the naturalness of the generated adversarial examples a question naturally arises which attack generates adversarial examples of higher qualities?
via analyzing the results of icr tcr acs and aed are shown in figure we make the following observations.
first none of the attacks can achieve the optimal performance on the three tasks in terms of naturalness.
specifically alert achieves the best in terms of icr acs and aed on average while accent is the optimal on average against tcr.
the results are also different on various tasks.
for instance alert outperforms accent on average against icr acs and aed on clone detection and vulnerability detection but vice versa on code summarization.
in general accent and alert outperform mhm and wir random in terms of the averaged acs and aed on all tasks since they both consider the naturalness of adversarial examples when replacing identifiers.
second effective attacks in general generate less natural adversarial examples.
the adversarial examples generated by mhm and wir random with the highest asr have the lowest acs to the original code and the largest icr tcr and aed indicating that their adversarial code quality is generally lower.
this raises the question towards the usefulness of mhm and wir random in practice since an existing study points out that the adversarial example should not only cheat the model but also be natural to human judges .
conversely the adversarial codes generated by alert and accent perform best on both acs and aed indicating that these adversarial code are more similar to the original code.
such results also confirm that the adversarial code are more natural than random replacement as claimed by alert and accent.
a potential reason for the high attack successful rate of mhm is that less natural perturbations may lead to out of distribution ood examples.
a icr codebert b icr codegpt c icr plbart d tcr codebert e tcr codegpt f tcr plbart g acs codebert h acs codegpt i acs plbart j aed codebert k aed codegpt l aed plbart figure comparison of icr tcr acs and aed on attacking codebert codegpt and plbart.
the lower icr aed and tcr with the higher acs indicates better performance.
such examples could easily lead to the success of adversarial attacks because the models may not perform well on data with different distributions .
third the adversarial examples generated by attacking codegpt are less similar to the original program than the other pre trained models.
specifically when the target model is codegpt for all attacks the average acs is .
which is lower than .
and .
of codebert and plbart and the average aed is .
which is higher than .
and .
of codebert and plbart.
such differences are all significant as revealed by the mann whitney u test p value .
.
this result also confirms finding that is codegpt is more resistant to various attacks.
since asr is negatively correlated with naturalness the attack algorithm hasesec fse december san francisco ca usa xiaohu du ming wen zichao wei shangwen wang and hai jin table attack success rate for replacing identifiers under different contexts positionclone detection vulnerability detection code summarization totalmhm wir randomtotalmhm wir randomtotalmhm wir random n n n n n n n n n n n n method .
.
.
.
.
.
.
.
.
.
.
.
return .
.
.
.
.
.
.
.
.
.
.
.
if .
.
.
.
.
.
.
.
.
.
.
.
throw .
.
.
.
.
.
.
.
.
.
.
.
try .
.
.
.
.
.
.
.
.
.
.
.
for .
.
.
.
.
.
.
.
.
.
.
.
to choose the suboptimal word with a longer distance among the candidate identifiers to achieve the purpose of misleading codegpt.
finding there is a trade off between the effectiveness and naturalness for adversarial attacks.
specifically effective attacks generate less natural adversarial examples.
in general substitution strategies such as context aware identifier prediction and replacement based on cosine similarity can generate examples of higher qualities than that of random substitution.
.
contexts of perturbed identifiers rq3 the exploration of the above two rqs demonstrates that there are two factors affecting the effectiveness of adversarial attack the search algorithm and the identifier substitution strategy.
however they both concentrate on how to change the programs while another important perspective for adversarial attack is determining what identifiers should be changed.
in this rq we investigate if perturbing identifiers under different contexts can cast significant impact on the attacking effectiveness.
via analyzing the results as shown in table we make the following observations.
a large number of instances can be successfully attacked even if only one identifier is replaced.
specifically we limit the replaced identifiers to and respectively.
the results show that existing techniques can still attack various models successfully and the asr increases as such a threshold increases.
for different attack approaches mhm is consistent with previous experiments in that asr is higher than wir random in all cases.
in this paper we explore the impact of identifiers in different statements on the models performance.
next we take mhm as an example to make such explorations since the asr of mhm and wir random on different statements share similar trend.
to ease for presentation we refer to the different statements by their name such as for.
the results show that the attacking effectiveness is sensitive to the identifiers under various types of statements and such sensitivity diverges across various tasks .
specifically in clone detection replacing identifiers in for if and tryis more likely to result in a successful attack.
for instance when n the asr of these three statements all exceed and the highest is .
of for.
the perturbations tomethod throw and return achieve relatively low asrs less than the lowest of which is .
of return .
in vulnerability detection the first three most effective statements are if method andfor.
when n the asr of these three statements are close to and the highest is .
of if.throw has the lowest impact asr .
.
in code summarization method dominates the attack effectiveness with an extremely high asr of .
which is significantly higher than those of the other statements.
specifically the asrs of the remaining types of statements are all below .among them throw has the lowest impact on the code summarization model with its asr reaching .
.
we further perform a case study to analyze why prioritizing statements can significantly affect the performance of adversarial attacks see section .
.
finding the context of the identifiers e.g.
where the identifiers reside can affect the attacking effectiveness significantly which suggests that the perturbation strategies should consider the context of identifiers aiming for more effective attacks.
new approach our empirical investigation reveals two main challenges for adversarial attack against ptmc which are the trade off between effectiveness and efficiency finding as well as that between effectiveness and naturalness finding .
both the two challenges may compromise the practical usefulness of existing adversarial attacks.
aiming to alleviate the second challenge the state of the art approach alert adopts a context aware identifier substitution strategy to improve adversarial code naturalness.
however our experiment reveals that both the effectiveness and efficiency of alert are still limited.
for instance it only achieves an average asr of .
on the three tasks.
our tool aims to enhance both the effectiveness and efficiency while guaranteeing the naturalness and the novelty of which is mainly embodied in the following two aspects.
first finding shows that perturbations on different types of statements can achieve varying success rates on existing attack techniques.
as such we propose to incorporate such prior knowledge to prioritize identifier selection thus enhancing the effectiveness and efficiency of the attack.
this attack strategy which incorporates code features is different from all the previous works including the sota alert.
second finding reveals that the effectiveness and efficiency of existing attacks are still limited.
the reasons are as follows.
mhm mainly uses a random method to replace identifiers one by one along the sequence of identifiers that can be replaced while this strategy requires a large number of queries to mislead the model.
meanwhile alert accent and wir random replace identifiers sequentially.
once the top candidates fall into local optimal solutions it is difficult for them to find the global optimal solution since they do not repeatedly process the replaced identifiers.
to alleviate such problems we propose to use beam search to focus on all the identifiers in a statement which can simultaneously search from multiple sequences and replace multiple identifiers.
.
approach design algorithm shows the workflow of beamattack.
it first obtains the set of identifiers in different statements sand the number of statement types t line .
the priority of different statements isan extensive study on adversarial attack against pre trained models of code esec fse december san francisco ca usa summarized by our prior knowledge as shown in table .
for instance the prioritized statement types for the clone detection task is for if try method throw return and others .
beamattack then performs beam search over different types of statements sequentially to generate new examples.
for the first iteration the replaced codes are the source program p line .
for subsequent iterations the replaced code are the kperturbed codes returned by function beamsearch section .
.
in the previous iteration.
the sequence to be replaced which is denoted as seq is initialized to all the identifiers in the entire statement line .
we apply beamsearch multiple times until the last category of statements is searched or an adversarial example is successfully generated.
note that we record the replaced identifiers after each beamsearch line .
finally beamattack performs the last beamsearch with the recorded replaced identifier as seq line .
this step is to alleviate the limitation that beam search will not process those identifiers that have already been replaced.
since an identifier can only be replaced by a unique candidate in the adversarial example some suboptimal candidates are discarded during the search and they may become optimal after subsequent identifiers are replaced.
.
.
beamsearch.
the maximum iteration in the search is set to be the product of seq s length and the weight of the statement type line .
in particular we set the weight of the most important statement type to and the other weights are set proportionally according to the prior knowledge in table .
in each iteration in beamsearch we apply perturb section .
.
on all the identifiers from the current type of statements.
after that beamsearch selects the k best ones in the current generation and the previous generation to serve for the next iteration.
note that the search process will stop if the current iteration fails to generate new qualified candidates.
since the number of identifiers is proportional to the maximum number of iterations of beamsearch it will directly affect the efficiency of beamattack.
as shown in figure the number of identifiers differs for the three tasks.
to balance the attack success rate and efficiency we set k to and in clone detection vulnerability detection and code summarization respectively.
.
.
perturb.
perturb first uses codebert to generate the most similar candidates for an identifier following the four attacks investigated in our study.
this similarity is based on the cosine similarity between the embeddings from codebert mlm following alert which is trained with the objective of masked language modeling.
these candidates are generated in real time for each perturb .
since some identifiers are changed during the attack the top candidates for the identifier to be replaced will also change accordingly.
then perturb chooses the identifier in the candidate list that reduces the current probability of true label the most for replacement to guarantee the naturalness.
if the drop in probability changes the model s predicted label or makes the code summary completely independent of the ground truth i.e.
bleu we consider the attack successful and output the adversarial example and the replaced identifier.
otherwise perturb returns the perturbed code original identifier replaced identifier and the probability for the corresponding replacement.
.
evaluation .
.
evaluation datasets.
to evaluate our method thoroughly we first evaluate it on the dataset as listed in section .
.
.
furthermore algorithm the main workflow of beamattack input source program p beam sizek statement weight sw output adversarial example adv 1rv replaced variable 2s t getstatements p s is the set of identifiers in different statements t is the number of statement types 3p0 p s 4fort tdo 5seqt s 6max iter length seqt sw pt beamsearch pt max iter 7rv.append pt .replacedvariable 8beamsearch codet rv length rv function beamsearch pt codet seqt max iter while not exceedmax iter do 10pcopy pt forcodet iinptdo forseqt jinptdo pt i perturb codet i seqt j 14pt selection pt pt k ifpt pcopythen returnpt returnpt to demonstrate its generalizability we perform additional evaluations on three other datasets.
for clone detection we use the filtered google code jam gcj dataset consisting of examples for training and for validation and testing respectively.
for vulnerability detection we use the juliet test suite2 which is another widely used open source security benchmark besides owasp.
in particular we utilize the java sub dataset and exclude instances with identical function bodies and finally obtain the training validation and testing sets consisting of and examples respectively.
for code summarization we use tl codesum tlc which is widely used as a benchmark .
similar to juliet we filter out duplicate examples and obtain the training validation and testing sets consisting of and examples respectively.
we also try to reproduce the results on these datasets and the results reflect that our fine tuned models can also achieve similar performance.
.
.
evaluation results.
we evaluate beamattack and alert on sets of experiments pre trained models tasks datasets as they both use the context aware identifier prediction as the substitution strategy to guarantee the naturalness of adversarial examples.
the evaluation results are summarized in table and we can observe that beamattack consistently achieves higher attack success rates on all experiments.
on average beamattack outperforms alert by .
in terms of asr showing that it is more effective in achieving successful attacks.
with respect to the attack efficiency beamattack outperforms alert on experiments.
in addition the average amq of beamattack is .
which is .
less than that of alert .
on average .
this indicates that our method which relies on the statement importance to search for adversarial examples is more efficient.
since beamattack replaces identifiers based on context aware identifier prediction the adversarial examples generated by it are of higher qualities with lower perturbation rates.
specifically the average icr of beamattack is .
which is lower than that of alert .
on average .
meanwhile the average icr and tcr of beamattack on the december san francisco ca usa xiaohu du ming wen zichao wei shangwen wang and hai jin table performance comparison between beamattack and alert attack resultsclone detection bcb vulnerability detection owasp code summarization csn asr amq art icr tcr acs aed asr amq art icr tcr acs aed asr amq art icr tcr acs aed codebertbeamattack .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
alert .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
codegptbeamattack .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
alert .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
plbartbeamattack .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
alert .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
attack results clone detection gcj vulnerability detection juliet code summarization tlc codebertbeamattack .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
alert .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
codegptbeamattack .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
alert .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
plbartbeamattack .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
alert .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
note bold numbers indicate the better performance for the given metric.
the cell with lightgray background denotes the outperformance is significant p .
.
1publicstaticvoidmain stringargs scannerscanner newscanner new filereader infile intt scanner.nextint filewriterfw newfilewriter outfile for intt t t t intr scanner.nextint intc scanner.nextint intw scanner.nextint fw.write string.format d s t solve r c w 14privatestaticintsolve intr intc intw intres c w r w c w returnres 1publicstaticvoidmain string args bufferedreaderbr newbufferedreader newinputstreamreader system.in stringstr br.readline while str br.readline !
null string temp str.split intr integer.parseint temp intc integer.parseint temp intw integer.parseint temp intans if c w ans c w w r else ans c w w r 1cookie cookies request.getcookies 2stringparam null 3booleanfoundit false 4if cookies!
null for cookiecookie cookies if cookie.getname .equals foo param cookie.getvalue foundit true if !foundit param else param 13stringbar newtest .dosomething param 14java.io.fileoutputstreamfos newjava.io.
fileoutputstream org.owasp.benchmark.
helpers.utils.testfiledir bar false a clone detection the first version of index from dataset gcj 1publicstaticvoidmain stringargs scannerscanner newscanner new filereader infile intt scanner.nextint filewriterfw newfilewriter outfile for intt t t t intr scanner.nextint intc scanner.nextint intw scanner.nextint fw.write string.format d s t solve r c w 14privatestaticintsolve intr intc intw intres c w r w c w returnres 1publicstaticvoidmain string args bufferedreaderbr newbufferedreader newinputstreamreader system.in stringstr br.readline while str br.readline !
null string temp str.split intr integer.parseint temp intc integer.parseint temp intw integer.parseint temp intans if c w ans c w w r else ans c w w r 1cookie cookies request.getcookies 2stringparam null 3booleanfoundit false 4if cookies!
null for cookiecookie cookies if cookie.getname .equals foo param cookie.getvalue foundit true if !foundit param else param 13stringbar newtest .dosomething param 14java.io.fileoutputstreamfos newjava.io.
fileoutputstream org.owasp.benchmark.
helpers.utils.testfiledir bar false b clone detection the cloned version of index from dataset gcj 1publicstaticvoidmain stringargs scannerscanner newscanner new filereader infile intt scanner.nextint filewriterfw newfilewriter outfile for intt t t t intr scanner.nextint intc scanner.nextint intw scanner.nextint fw.write string.format d s t solve r c w 14privatestaticintsolve intr intc intw intres c w r w c w returnres 1publicstaticvoidmain string args bufferedreaderbr newbufferedreader newinputstreamreader system.in stringstr br.readline while str br.readline !
null string temp str.split intr integer.parseint temp intc integer.parseint temp intw integer.parseint temp intans if c w ans c w w r else ans c w w r 1cookie cookies request.getcookies 2stringparam null 3booleanfoundit false 4if cookies!
null for cookiecookie cookies if cookie.getname .equals foo param cookie.getvalue foundit true if !foundit param else param 13stringbar newtest .dosomething param 14java.io.fileoutputstreamfos newjava.io.
fileoutputstream org.owasp.benchmark.
helpers.utils.testfiledir bar false c vulnerability detection index from dataset owasp figure case study on clone detection and vulnerability detection experiments are .
and .
which are lower than alert .
and .
respectively .
the average acs of beamattack is .
which outperforms alert .
.
beamattack only performs worse than alert on aed .
vs. .
.
to verify whether the performance differences are statistically significant we apply the one sided mann whitney u tests to each experiment.
significant differences p value .
are marked with lightgray background in table .
the results show that our method outperforms alert on .
of the cases i.e.
experiments metrics .
among these cases accounting for .
demonstrate significant differences strongly verifying that our method not only surpasses alert on most evaluation metrics but also achieves significant outperformance.
although alert performs relatively better in the remaining cases we note the difference between beamattack and alert is significant for only cases.
discussion .
case study in this section we perform an additional case study to qualitatively compare beamattack with alert to understand their distinctions and why prioritizing statements can significantly affect the performance of adversarial attacks.
we illustrate based on figure on clone detection and vulnerability detection.
the cloned code corresponding to figure 5a transforms forloop into while and refactors thesolve function as shown in figure 5b.
figure 5c displays a code snippet from the owasp dataset that contains a path traversal vulnerability on lines .
the vulnerability exists due to the lack of input validation for the variable barwhich receives data from the cookie i.e.
param .
such validation is often performed by usingtheifstatement which highlights the importance of ifin influencing the predictions for vulnerability detection models.
these two cases can reflect our finding in rq3 that the statements have the most significant impact on adversarial attacks against clone detection and vulnerability detection are for andif respectively.
specifically during the attack process the first set of identifiers extracted by beamattack are and respectively since it prioritizes those statements based on the learned prior knowledge.
one successful attack substitution follows w j c k fw ww and cookies cooks param ram bar ban i vi foundit foundait dosomething runnothing respectively.
it can be seen that successful attacks can be achieved by only replacing part of the identifiers in the forandifstatements.
in contrast the sequences replaced by alert are c fw w res r infile outfile t t scanner and bar param response foundit fos i respectively.
we note that alert does not accurately replace the identifiers required to achieve the attack.
for example it replaces irrelevant identifiers infile andoutfile in clone detection and also misses the identifier cookies that is highly relevant to the vulnerability.
the above analysis explains the distinction between alert and beamattack as well as why alert is less effective.
in addition to the difference in prioritizing identifier substitution it is worth noting that we also utilize beam search to attack which allows more sequences for identifier substitutions within the same statement.
for instance in figure 5c alert replaces identifiers strictly according to a fixed order meaning only param can come after bar.
in contrast after replacing bar beamattack can choose foundit param orias the next replaceable identifier which mitigates the risk of getting stuck in local solutions.an extensive study on adversarial attack against pre trained models of code esec fse december san francisco ca usa .
implications for model robustness we find that existing ptmcs are susceptible to adversarial attacks finding which presents a considerable challenge to their robustness.
therefore we strongly advocate that researchers need to place equal if not greater emphasis on improving model robustness while striving to improve model performance.
practical strategies such as adversarial training or data augmentation can be employed to enhance robustness.
these methods can equip models with the resilience required to counter adversarial perturbations and improve their overall reliability.
for adversarial attack approach considering the trend of large language models llms towards being closed source and chargeable attacks through massive queries may become extremely costly or even infeasible.
therefore a further exploration of the relationship between code identifiers and model prediction results is necessary to derive a more accurate sequence for identifier replacement.
additionally we can leverage the sota language generation techniques such as chatgpt to replace identifiers and enhance the naturalness of adversarial examples.
finally considering the effectiveness of statement prioritization future methods should focus on a more in depth analysis of code structure such as employing comprehensive semantic analysis to devise more effective attacks.
threats to validity internal validity parameter settings such as the number of iterations can lead to different results.
we adopt the following strategy to mitigate this threat.
when the parameters can be set uniformly we set the parameters consistently with the five attacks such as the number of candidates for the identifier.
when the parameters are specific to an attack approach we follow the settings in the original paper exactly to achieve fair comparisons.
another internal validity threat is the potential bugs in our implementation.
to reduce such threats we have carefully checked our implementations and also open sourced all the materials and code to the community for further checks.
external validity external validity is threatened by the generalizability of tasks datasets and models.
for tasks the selected ones have been extensively studied in existing works on adversarial attacks .
for datasets we not only use the codexglue benchmark studied in many original papers on ptmcs and adversarial attacks but also include a new dataset the owasp benchmark to evaluate the general applicability of the attack approaches.
for the target models we mitigate this threat by selecting the most popular ptmcs with relatively high performance.
related work black box attack approaches have been extensively discussed in section .
.
and we introduce other white box attack approaches in this section.
specifically yefet et al.
propose damp which utilizes the gradient information of the target model to find replacement identifiers in the opposite direction of the gradient descent.
meanwhile they use one hot vector to encode code aiming at obtaining candidates by perturbing the vector and then mapping them back to tokens.
however such an approach is unable to constrain the candidates so that it may obtain irrelevant identifiers similar to random substitutions.
srikant et al.
turn the adversarial attack into an optimization problem and identify two aspects inthe adversarial attack which parts of the program to transform and what transformations to use.
they correspond to the search strategy and replacement strategy respectively as we mentioned above.
then they use projected gradient descent pgd based joint optimization jo solver to obtain the optimal transform location and transform method.
zhang et al.
propose carrot which incorporates gradient information into transform operations to guide the search process more effectively.
although retrieving gradients during transform operations may take more time it can effectively reduce search iterations.
the above white box attacks are not very practical as the latest sota models such as chatgpt are increasingly becoming closed source.
these models are typically deployed remotely and offer services through api interfaces making it difficult to access their internal structure and parameters.
there is no comprehensive evaluation towards adversarial attack on ptmcs currently and the study most similar to ours is that of zeng et al.
.
however they mainly focus on evaluating the effectiveness of ptmcs while the adversarial attack approaches are not fully studied.
specifically they evaluate several attack approaches adapted from the field of natural language processing nlp and focus on comparison in terms of asr.
in contrast our study specifically focuses on attacks designed for se applications and we have additionally evaluated attack efficiency and the quality of the generated adversarial examples.
moreover the attack approach they proposed simply combines wir and random replacement without incorporating the unique characteristics of programming languages and code intelligence tasks which is a common shortfall for most existing studies on adversarial code attacks.
in this work for the first time we generate adversarial examples by perturbing source code based on the contextual information of identifiers.
conclusion this study thoroughly evaluates the performance efficiency and robustness of adversarial attacks on ptmcs.
results show that ptmcs are easily susceptible to adversarial perturbations with varying levels of robustness among different tasks.
the code summarization model is found to be the most vulnerable.
additionally high performing attack approaches often come with significant computational overhead.
the importance of different statements is also analyzed revealing varying levels of sensitivity among different context identifiers to counterattacks.
based on such findings we propose a new approach beamattack which improves the effectiveness of attacks by .
and efficiency by .
compared to the existing approach alert using the same identifier substitution strategy.
data availability the data source code and the results of this paper are available at