training data debugging for the fairness of machine learning software yanhui li state key laboratory for novel software technology nanjing university china yanhuili nju.edu.cnlinghan meng state key laboratory for novel software technology nanjing university china menglinghan smail.nju.edu.cnlin chen state key laboratory for novel software technology nanjing university china lchen nju.edu.cn li yu state key laboratory for novel software technology nanjing university china yuli smail.nju.edu.cndi wu momenta suzhou china wudi momenta.aiyuming zhou state key laboratory for novel software technology nanjing university china zhouyuming nju.edu.cn baowen xu state key laboratory for novel software technology nanjing university china bwxu nju.edu.cn abstract withthewidespreadapplicationofmachinelearning ml software especiallyinhigh risktasks theconcernabouttheirunfairnesshas beenraisedtowardsbothdevelopersandusersofmlsoftware.the unfairness of ml software indicates the software behavior affected bythesensitivefeatures e.g.
sex whichleadstobiasedandillegaldecisionsandhasbecomeaworthyproblemforthewholesoftware engineering community.
according to the data driven programming paradigm of ml software weconsidertherootcauseoftheunfairnessasbiasedfeatures in training data.
inspired by software debugging we propose anovelmethod linear regressionbased training datadebugging ltdd to debugfeaturevaluesintrainingdata i.e.
a identify which features and which parts of them are biased and b exclude thebiasedpartsofsuchfeaturestorecoverasmuchvaluableand unbiased information as possible to build fair ml software.
we conductanextensivestudyonninedatasetsandthreeclassifiers to evaluate the effect of our method ltdd compared with fourbaseline methods.
experimental results show that a ltdd can betterimprovethefairnessofmlsoftwarewithlessorcomparable damagetotheperformance and b ltddismoreactionablefor fairness improvement in realistic scenarios.
linghan meng is the co first author.
lin chen is the corresponding author.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa association for computing machinery.
acm isbn ... .
debugging fairness ml software training data acm reference format yanhuili linghanmeng linchen liyu diwu yumingzhou andbaowen xu.
.trainingdatadebuggingforthefairnessofmachinelearning software.in 44thinternationalconferenceonsoftwareengineering icse may pittsburgh pa usa.
acm new york ny usa pages.
introduction machinelearning ml softwarehaspenetratedmanyaspectsofour dailylives whoseresultsareemployedinhigh stakeapplications to make decisions or predictions.
for example people have applied ml software to identify credit risks to prove loan applications topredictheartdisease andeventoestimatereoffending probabilities .alongwiththewidespreadusageofmlsoftware theconcernaboutitsfairnesshasalsobeenraisedtowardsdevelopers users and regulators of ml software.
the unfairness of ml softwareusuallyindicatesthediscriminatorytreatmentofdifferent groups divided by sensitive features e.g.
sex.
according to biased decisions made by ml software one group e.g.
male may haveaparticularadvantage i.e.
withmoreopportunitytoobtain favorable decisions1 called privileged over the other group e.g.
female called unprivileged.
under such circumstances the biased treatment not only undermines the objectivity of ml software but also violates some anti discrimination laws .
the fairness of ml software has been considered a worthy software engineering se research problem wherefairness has cometobeseenasacorenon functionalqualityproperty of mlsoftware.brunandmeliou reportedanewkindofsoftware defect fairness defect which is related to software behavior in 1toillustrate forloanapplications approval isthefavorabledecision and rejection is the unfavorable decision.
ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa yanhui li linghan meng lin chen li yu di wu yuming zhou and baowen xu a biased manner and needs to be tackled.
chakraborty et al.
arguedthat whendiscoveringunfairproblemswithmlsoftware it is the job of software engineers to reduce discrimination.
biswas and rajan conducted an empirical evaluation of fairness on real world ml software under a comprehensive set of fairness indicators.theyobservedthatperformanceoptimizationtechniques might lead to unfairness.
followingthedata drivenprogrammingparadigm mlsoftwareobtainsitsdecisionlogicfromtrainingdata .thebehavior of ml software to a large extent is determined by the quality of training data i.e.
biased training data may lead to the trained ml software with statistical discrimination.
researchers have tried toexplaintheeffectoftrainingdataonthefairnessofmlsoftware from the following angles the size of feature sets .
zhang and harman compared the results of ml software with different sizes of feature sets and reported that enlarging the size of feature sets in training data would increase ml software fairness.
biasedlabelsandimbalanceddistributions.chakraborty etal.
assumed that label information might contain bias and proposed a novel method fairway to remove training sampleswithbiasedlabels.recently they postulatedroot causes of unfairness as a combination of biased labels and imbalancedinternaldistributionsandproposedfair smote to remove biased labels and rebalance internal distributions.
thispaperdeviatesfrompreviousstudies asitadoptsadifferent angle the root cause of the unfairness could be biased features in trainingdata.here wedefinebiasedfeaturesas featuresthatare highlyrelatedtothesensitivefeatureintrainingdata.inthedevelopmentprocessof mlsoftware developershavenoticed thatbiased featuresmightleadtounfairbehaviorsofmlsoftware.asreported inamazonfairnessissues whendevelopersconstructedamazon s same day delivery service they observed that some features in the training data are highly related to the race the sensitive feature that is already excluded in the training process of customers e.g.
the zipcode feature could obviously deduce whether the deliveryaddressisinwhiteornon whitecommunities.asaresult even though thedelivery serviceruns withoutobtaining the raceinformation itstillprefersthedeliveryaddresswiththezipcodeinwhite communities and causes unfairness between white and non white customers.
this kind of fairness issue about biased features has beenconsideredfairnessbugs whichasksfornovelmethods todebug formallyfindandresolve biase dfeatures.however to ourknowledge therearenopreviousstudiesaboutmlsoftware debugging techniques focusing on biased features.
toobtainunbiasedfeatures wetryto debugfeaturevaluesin training data i.e.
a identify which features and which parts of themarebiased and b excludethebiasedpartsofsuchfeatures torecoverasmuchusefulandunbiasedinformationaspossible.to achieve this goal we propose a novel method linear regression basedtraining datadebugging ltdd whichemploysthelinear regression model to identify the biased features see details insection3 .onthewhole ourmethodcomprisesthefollowing three steps a adopting wald test witht distribution to check whether the features contain significant bias b estimating the biased parts of training features as explained variances i nt h eoriginal feature values by the sensitive feature and c removing thebiasedpartsoftrainingfeaturesbysubtractingtheestimated values from original feature values to construct fair ml software.
to evaluate our method ltdd we conduct an extensive empirical study on nine tasks with common sensitive features sex race and age under three widely used ml classifiers logisticregression naivebayes andsvm.toassesstheperformanceofltdd weintroducefourmethodsasthebaselines two state of the art fairness methodsfair smote at fse 2021and fairway atfse andtwowidelystudiedpre processing methods reweighing and disparate impact remover .
our experimentalresultssupporttheclaimthatltddperformswell comparedwiththebaselines ltddcanlargelyimprovethefairnessofmlsoftware withlessorcomparabledamagetoitsperformance.
our study makes the following two contributions strategy.thispaperproposesanovelfairnessmethod linearregression based training data debugging ltdd as anefficient strategy to alleviate ml software s unfairness by identifyingbiasedfeaturesandremovingtheirbiasedparts in training data.
study.thispapercontainsanextensiveempiricalstudyon nine tasks under three ml classifiers.
the results of ourexperiment show that ltdd can largely improve the fairness of ml software with less or comparable damage to its performance.
the rest of this paper is structured as follows.
we introduce backgroundinsection2andpresentadetaileddescriptionofour method ltdd in section .
section presents the experimental settings includingstudieddatasets baselinemethods prediction settings and used classifiers.
sections and give the experimental results and important further discussions.
finally section describes threats to validity and section concludes our paper.
background .
preliminaries we introduce some concepts and symbols about the fairness of ml software here which facilitate the readers to understand the following parts.
mlsoftware.forbinaryclassificationtasks2 mlsoftware sml would be considered as a function mapping the domain feature vectors x rdinto class labels y i.e.
sml rd .
normally for a new input x w eu s e yto denote the actual label while y sml x indicates the predicted label by ml software.
sensitive feature.
the sensitive feature is a feature xsin the domainfeaturevector x whichdividessamplesintotwocategories called privileged and unprivileged in which the benefits received differconsiderably.forexample sexisoneofthecommonsensitive features.asobservedin themalegroupisoftenregarded astheprivilegedgroupwithmorefavorablelabelspredictedbyml software.
groupfairness.mlsoftware hasgroupfairness if thesensitive feature doesnot affect thegroup probability ofdecision outcomes 2here we focus on the ml software for binary classification tasks.
our definition can be easily generalized to multiple classification tasks.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
training data debugging for the fairness of machine learning software icse may pittsburgh pa usa i.e.
privilegedgroupsandunprivilegedgroupshave almost equalprobabilitiesofbeingpredictedasfavorablelabels.groupfairness is widely studied and guaranteed by legal regulations on fairness .
in this study we focus on group fairness.
.
related work we introduce the related work from three aspects fairness testing and evaluation bias removal algorithm and data debugging.
.
.
fairness testing and evaluation.
fairness testing hopes to find instances of bias in the data set and evaluate the fairness of themodel spredictionresults.aequitas findsinstancesof discriminationinthedatasetandgeneratesmoreinstancestoretrainfurther.itappliestwosearchstrategiesglobalsearchbasedon randomtechnologyandlocalsearchconductedbydisturbingthe instancesfoundinglobalsearch.affectedbyaequitas aggarwal etal.
proposedablack boxtechniqueforfairnesstestingbased on global and local search combined with symbolic execution and local interpretable models.
zhang et al.
proposed a white box testing technique to find and generate test cases which aims at complex dnn models and introduces gradient based methods into global and local search.
.
.
bias removal algorithm.
bias removal algorithms can be divided into three categories pre processing in processing and post processing.
a thepre processingalgorithmaffectsthetrainingofthemodel by modifying the data to achieve fairness.
as the pre processingalgorithm is the most related work to this study we detailedly summarize pre processing approaches in the following three folds datapoint modification which deletes generates or assigns weights to datapoints of training data feature selection which leverages feature construction to choosemoresuitablefeaturesthatleadtobothhighaccuracy and fairness feature modification which modifies features as a debias ing method.
disparate impact remover dir is the most relatedfeaturemodificationmethod andhasalsobeen employed as one of four baselines see section .
.
dir focuses on the predictability of sensitive features from nonsensitive features non sensitive sensitive which is an opposite strategy of our method to evaluate the bias.
comparedwithrelatedwork ourmethodemployslinearregression to estimate the biased association of non sensitive features fromsensitivefeatures i.e.
sensitive non sensitive seesection3 which is a novel angle to assess and remove the bias.
b the in processing method is to modify the model to make the prediction result of the model fairer.
zhang et al.
p r o posedamethodtoimprovethemodelthroughnegativefeedback.
it combines an adversarial model to obtain the difference between sensitive features and non sensitive features to reduce bias.
c the post processing method achieves prediction fairness by modifying the prediction results of the model.
hardt et al.
proposedapost processingmethodbasedonequalopportunitydif ference whichachievesthegoaloffairnessbytransferringthecost of bad classification from unprivileged groups to decision makers.
.
.
data debugging.
thegoalofdatadebuggingistolocateand modify the data that causes program errors.
chad and ronald introducedprogramchippingtosimplifyinputsthatcauseprogram errors which can automatically delete or cut off certain parts ofthe inputs.
to debug the input data lukas et al.
introduced analgorithmcalledddmax whichmaximizesthesubsetofinputthat the program can still process to repair the input data.
they arguedthatddmaxisthefirstgeneraltechnologytorepairfaulty inputsautomatically.wuetal.
proposedacomplaint driven datadebuggingsystemnamedrainwhichallowsuserstocomplain about the intermediate or final output of the query and returns the smallestsetoftrainingsubsetthatcansolvethebugafterdeletion.
our approach inthissection wepresentadetaileddescriptionofourapproach.
first we show a motivation example.
next we employ linear regression to model biased features.
finally we present our method linear regression based training data debugging ltdd .
.
motivation example insection1 wequotedareport aboutamazonfairnessissues to show that when biased features appear in the training data ml software might discriminate between privileged and unprivileged groups.
here we propose a more detailed motivation exampleto show why and how features are biased i.e.
why features areconsidered related to sensitive features and how they cause mlsoftware unfairness.
this example is also helpful to demonstrate the following steps in sections .
and .
.
figure1 a presentsasnippettakenfromthecompasdataset whichisusedtoevaluatethepossibilityofacriminaldefendant reoffending.
the snippet contains six samples with the sensitive feature race andthenon sensitivefeature age .hereweconsider thesnippet3asasmallbutrepresentativemotivationexampleto showthetrainingdatasetwithbiasedfeatures.ascanbeseenin figure a we have the following observations biasbetweenracegroupsisobservedintrainingdata.thereareobviouslydifferentratesoffavorablelabels no reoffend between privileged white and unprivileged non white groups e.g.
allthreewhitepeoplearelabeledasno reoffend the favorable decision .
in contrast only one out of three non white is labeled as no reoffend.
why age containsracebias?thereisahighassociation between race and age the age valuesoftheprivileged group threewhitepeople arerelativelymuch largerthan that of the unprivileged group three non white people .
how age causesml software biased even without knowing race information?
since larger age values imply white people the race bias may be introduced by the age values when training ml software.
.
linear regression modeling as discussed above some features in the training data might be biased leadingtounfairnessinmlsoftware.toobtainunbiased 3for brevity we focus on the six samples here and discuss the biased association betweenthesensitivefeature race andthenon sensitivefeature age inthissnippet.
in section .
we will extend the scope to the whole set of compas dataset.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa yanhui li linghan meng lin chen li yu di wu yuming zhou and baowen xu groupdomain features label sensitive non sensitive race age ... privilegedwhite ...no reoffend white ...no reoffend white ...no reoffend unprivilegednon white ...no reoffend non white ... reoffend non white ... reoffend a association between age and race.employ linear regression equation age a b race to estimates a and b a b a b a b a b a b a b b calculate bias partsremove biased parts from age age a b race ageu a g e age c remove biased parts figure an example of biased features age which are highly related to sensitive features race .
features wehavetodebugfeaturevaluesintrainingdata i.e.
a identify which features and which parts of them are biased and b removethebiasedpartstorecoverasmuchhelpfulandunbiased information as possible.
the main idea of our training data debugging is to apply the linear regression equation to analyze the association between non sensitive features and sensitive features which is a simple but effective method applied in many se research areas such as defect prediction and software effort estimation .
specifically we buildalinearregressionmodelforthenon sensitivefeatures xnon the sensitive features xs xn a b xs where the symbols a b and denote the population regression intercept slope and residual respectively.
we employ wald test witht distribution to check whether the null hypothesis that the slope of the linear regression model is zero holds.
when the pvalues .
weignorethecurrentnon sensitivefeatureandmove tothenext otherwise weconsiderthat xncontainssignificant bias and conduct the following steps to estimate and exclude bias.
by calculating on values of xnandxsin training data of ml software we obtain the estimates aand bforaandb respectively.
basedontheaboveestimates wehavethefollowingequationto measure the association between xnandxs xn a b xs where xncouldbeconsideredasthepredictedvaluegeneratedfrom equation .
intuitively xnis the explained variance in the original non sensitivefeatures xnbythesensitive feature xs.weconsider xnas thebiasedpart ofxn remove xnfromxn and denote the remaining unbiased part as xun xu n xn xn fig.
b andfig.
c showthedetailofcalculatingthesample estimates and the remaining values based on the six samples in fig.
a .
in detail after translating categorical variables white nonwhite into numerical variables we have the following linear regression equations bracketleftbig36 bracketrightbigt a b bracketleftbig1 bracketrightbigt we find that p value .
and obtain the estimates a and b .finally weobtaintheunbiasedagevalue ageubysubtracting the predicted values age race ageu age age.a s can be seen in fig.
c the distributions of age values underprivileged and unprivileged groups become independent of race afterremovingthebiasedparts i.e.
therevised age valuesintwo groups are the same.
.
our method based on the linear regression model we propose our algorithm linear regression based training data debugging ltdd shown in algorithm .
given the training dataset str angbracketleftx1 y1 angbracketright angbracketleftxn yn angbracketright where foranyj j n xj xj ... xj d isad dimensionvectorto denotedfeaturevalues xj disassumedtobethesensitivefeature valueandtheothervalues xj xj d 1arenon sensitivefeature values and yj our method ltdd comprises the following three steps identify the biased features and estimate the biased parts of them.for each non sensitive feature xi we evaluate the association between the sensitive features xdandxiin the training dataset.
it is worth noting that since the associationbetweensomenon sensitivefeaturesandthesensitive feature may be trivial we employ wald test line with t distribution to check whether the null hypothesis that the slope bof the linear regression model is zero holds.
specifically weintroducethe p valueofwaldtesttoavoid unnecessaryremoving steps i.e.
consider p value .
line as a precondition.
if p value .
holds we calculate the estimates aiand biof the linear regression model line which are sorted in eaandeb line .
exclude the bias parts from training samples.
in this step for any training sample angbracketleftxj yj angbracketright we conduct the following two operators to eliminate bias deleting the sensitive feature line and revising the non sensitive feature values by removing the association line .
after that we build a fair ml software smlbased on the revised and unbiased training dataset line i.e.
sml rd .
apply the same revision on the testing samples.
as we revised thedimensionanddistributionoftrainingdata weapplythe samerevisiononthetestingsample xtetofitmlsoftware line i.e.
delete the sensitive feature and revise the other attributes by the estimates ea andeb calculated on the training samples.
finally we use smlto predict the label of xte line .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
training data debugging for the fairness of machine learning software icse may pittsburgh pa usa algorithm linear regression based traning data debugging ltdd str xte input the training dataset str angbracketleftx1 y1 angbracketright angbracketleftxn yn angbracketright where xj xj ... xj d is ad dimension vector to denote thedattribute values xj dis the sensitive feature value and the other xj xj d 1are non sensitive feature values yj and the testing sample xte xte ... xte d .
output a ml software smland the predicted label sml xte forxte.
1initialize d dimension array ea withea which is used to store the estimation result of intercept ai 2initialize d dimension array eb witheb which is used to sort the estimation result of slope bi 3construct the column vector vdof the sensitive feature values from str vd x1 d xn d t 4fori d do 5construct the column vector viof the current non sensitive feature values vi x1 i xn i t 6apply the linear regression model on vi vi ai bi vd 7conduct wald test with t distribution to get the p value 8ifp value .05then estimate aiand biforaiandbi insert aiand biintoeaandeb ea ai eb bi 11for angbracketleftxj yj angbracketright s trdo 12remove the sensitive feature from xj xj xj 13fori d do remove the biased part based on the estimation xj i xj i e a e b xj d 15train ml software smlfrom the revised d dimension training data 16remove the sensitive feature from xte xte xte 17fori d do 18apply the same revision on the testing sample xte xte i xte i e a e b xte d 19returnsmlandsml xte experiment setups in this section we will introduce the experiment setups.
code and data are publicly available online and reusable see section .
.
.
studied datasets weemployninedatasetstoevaluatetheperformanceofourmethod allofwhichhavebeenwidelyusedinprevioussoftwarefairness studies .
adult .thisdatasetcontains48 842sampleswith14features.
the goal of the data set is to determine whether a person s annual income can be larger than 50k.
this dataset has two sensitive features sex and race.
compas .
compas is the abbreviation of correctional offender management profiling for alternative sanctions which is a commercialalgorithmforevaluatingthepossibilityofacriminal defendant committing a crime again.
the dataset contains the variablesusedbythecompasalgorithmtoscorethedefendantandtable the datasets used in this experiment dataset feature size sensitive feature adult sex race compas sex race default sex german sex heart age bank age student sex meps15 race meps16 race thejudgmentresultswithintwoyears.thereareover7000rowsin this dataset with two sensitive features sex and race.
defaultofcreditcardclients defaultforshort .thepurpose ofthisdatasetistodeterminewhethercustomerswilldefaulton payment through customers information.
it contains30 rows and features including one sensitive feature sex.
german credit german for short .
this data set contains rows and features where each person is divided into good or bad credit risk according to feature values.
the sensitive feature in this dataset is sex.
heart disease heart for short .
this dataset judges whether the patient has heart disease based on the collected information.
the data set includes features where age is the sensitive feature in this dataset.
bankmarketing bankforshort .thegoalofthisdatasetis to predict whether the client will subscribe to a term deposit.
it contains pieces of data and its sensitive feature is age.
studentperformance studentforshort .thisdatasetanalyzes studentperformanceinsecondaryeducation.itcontains1044pieces of data with the sensitive feature sex.
medicalexpenditurepanelsurvey mepsforshort .itcollects data about the health services used by respondents the cost and frequencyofservices anddemographics.weusethesurveydata for the calendar years and named meps15 and meps16 respectively.theycontainmorethan15 000rows andthesensitive feature is race.
since there are two sensitive features sex and race in two datasets adult and compas we convert the datasets into scenarios by considering one sensitive feature in one scenario i.e.
theadult compas datasetappearstwiceasadult sexand adult race compas sex compas race .
.
baseline methods as described in section our method ltdd is a pre processing method i.e.
debuggingtrainingdatabeforetheconstructionofml software to reduce the bias.
we introduce four fairness methods as the baselines two state of the art methods fairway at fse andfair smoteatfse andtwowidelyusedpre processing methodsreweighinganddisparateimpactremover dir forcomparison.
fair smote fair smote is a pre processing method that usesthemodifiedsmotemethodtomakethedistributionof authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa yanhui li linghan meng lin chen li yu di wu yuming zhou and baowen xu datasettrain set test setdivide into85 trained modelremove association and train evaluationremove associationtest100 times figure prediction settings for our experiment.
sensitive features in the data set consistent and then deletes biased data through situation testing.
fairway fairwayisahybridalgorithmthatcombines pre processingandin processingmethods.ittrainsthemodels separately based on samples with different sensitive features to remove biased data points.
then it uses the flash technique for multi objective optimization including model performance indicators and fairness indicators.
reweighing reweighing is a pre processing method thatcalculatesaweightvalueforeachdatapointbasedon theexpectedprobabilityandtheobservedprobability tohelp the unprivileged class have a greater chance of obtaining favorable prediction results.
disparateimpactremover dir thismethodisapreprocessingtechnology whoseprimarygoalistoeliminate disparateimpactandincreasefairnessbetweengroupsby modifying feature values except for the sensitive features.
our code for implementing ltdd and allbaselines is based on python3.
.
wherereweighinganddirareimplementedusing aif360 version .
.
.
.
prediction settings and classifiers following the training and testing process in fairway we randomly divide thestudied datasetinto twoparts is usedto preprocessandtrain4 andtheremaining15 isusedasthetestset.
since our studied methods have randomness as shown in figure we repeat our experiment times to reduce this influence.
like the previous researches on fairness testing we introduce logisticregressionmodel asamainclassifiertotestfairness in this experiment.
to test the effect of our method under other classifiers wealsoemploynaivebayesandsvm whicharealso widelyusedclassifiersinseresearch .whenweimplement allclassifiers weemploythedefaultsettingsofthesklearnmodule.
.
fairness and performance metrics asreportedin theincreasingofsoftwarefairnessmaydamage theperformance whichiscalledtheaccuracy fairnesstradeoff.our experiment focuses on both fairness and performance metrics to check the relationship between fairness and performance changed after applying fairness methods to ml software.
4infairway arefortrainingand15 areforvalidation.forothermethods without validation we employ the total for training.fairnessmetrics.
disparateimpact di andstatisticalparity difference spd arethemainindicatorsweusetomeasurefairness.giventhestudiedmlsoftware sml thesample x thelabel y the predicted y sml x ofx andthesensitivefeature xs wedefine disparate impact and statistical parity difference as follows disparate impact di .
it indicates the ratio of the probabilitiesoffavorableresultsobtainedbytheunprivileged xs and privileged xs classes.
di p y xs p y xs statisticalparitydifference spd .itissimilartodi butit represents the difference between the unprivileged and privileged classes to obtain favorable results.
spd p y xs p y xs whenreportingdi wecalculateitsabsolutedistancefrom1 i.e.
di .
while for spd we report the absolute value result spd .
we take di and spd as the main fairness metrics due to the following reasons.
a di and spd are designed to indicate thedifference between the decision distributions of privileged and unprivilegedindividuals whichisspecificallyprohibitedbyantidiscriminationlaws b basedontheircalculation diandspd areindependentofthelabelinformationaboutthedecisions i.e.
theyneedonlytheprediction y sml x ratherthanthelabel y whichissuspectedofcontainingbias .besides insection6.
we will present the results under other fairness metrics including aod and eod .
performancemetrics weintroducethreeperformancemetrics to measure the performance of the classifiers.
accuracy acc for short .
it measures the ratio of correct predictions on total data acc tp tn total wheretpdenotesthetruepositivesamples tndenotesthetrue negative samples and totaldenotes the total samples.
recall.
recall represents the probability of being predicted as positive samples in samples that are actually positive recall tp tp fn wherefndenotes the false negative samples.
false alarm.
it is also called the false positive rate false alarm fp fp tn wherefpdenotes the false positive samples.
whencalculatingtheseaboveindicators includingbothfairness andperformance weemploypythonaif360module which integrates all these indicators.
.
analysis method toverifywhetherthedifferencebetweenthemetricvaluesobtained by our method and the baselines is statistically significant we use the wilcoxon rank sum test .
if the difference between the two setsofresultsissignificant thenthe p valueobtainedbythetest should be less than .
.
then tomeasuretheeffectsizeofthetwosetsofresults weuse cliff sdelta .ifthevalueof islessthan0.
thedifference between the two sets is negligible if it is greater than .
but authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
training data debugging for the fairness of machine learning software icse may pittsburgh pa usa figure comparison results of our approach with the original ml software under five indicators.
for di and spd more negative differences mean more fairness improved.
lessthan0.
thenthedifferenceisrelativelysmall ifitisgreater than0.330butlessthan0.
thenthereisamediumdifference if it is greater than .
then the difference between the two is considered large.
according to p value and value we mark w t l for twosetsofcomparedresults.
w meansourresultwins wherethe corresponding p valueislessthan0.
andthe valueisgreater than .
.
l means our results lose where the p value is less than .
and the value is smaller than .
.
otherwise the situation is t which means the two sets of results are tied.
experiment results in this section we present the results of our three rqs along with their motivations approaches and findings.
rq1 how well does our algorithm improve the fairness of ml software?
motivationandapproach.
wefirstneedtocheckwhetherour methodcaneliminatethediscriminationthatexistsintrainingdata.
meanwhile wehopethatourmethodcandamagetheperformance of ml software as little as possible.
the main classifier used in this rqisthelogisticregressionmodel.toeliminatetheinfluenceof randomness we repeat the experiment times and report the averagevalues.wecomparetheresultsofourmethodandbaselines under five indicators a di and spd as fairness indicators and b acc recall and false alarm as performance indicators.
results.
infigure3 wereportthechangeofourmethodsapplying to original ml software.
the blue and orange bars in the figurerepresent the changes in di and spd respectively.
based on the calculation of di and spd the smaller the value of di and spd the fairer the prediction result of the model.
therefore the more negative the difference observed the more ourmethodimproves thefairnessofthe standardclassifier.itcan be seen that our method ltdd significantly improves the fairness of the standard classifier in most scenarios.
especially on compas dataset ltdd reduces the value of di by .
.
besides the other three bars represent the changes of acc recall andfalsealarm.ascanbeseen ourmethodhasminimal impactontheperformanceofmlsoftwareinmostscenarios.evenunder the two data sets compas and german our method has surprisingly improved the recall of the original ml software.
answer to rq1 our method can greatly improve the fairness of the original ml software and slightly damage its performance.
rq2 how well does ltdd perform compared with the state of the art fairness algorithms?
motivation and approach.
we compare our method ltdd with the state of the art algorithms to see if our method can perform betterthanthesealgorithms.wehopethatourmethodcanachievethat comparedwithbaselines a itcanbetterimprovethefairnessof ml software b the negative impact of our algorithm on perfor mancewouldbelessorcomparable.similartorq1 thisrqapplies the logistic regression model as the main classifier and repeats the experiment times to eliminate the influence of randomness.
results for fairness.
the comparison results under fairness metricsareshownintable2.thegraybackgroundinthetableindicates thatourmethodhaswontheresultofthebaseline thewilcoxon p valueislessthan0.
andthecliff sdelta isgreaterthan0.
.
the black background indicates that our method loses to the baseline the wilcoxon p value is less than .
and the cliff s delta islessthan .
.thewhitebackgroundindicatesatie.itcan be seen that our method is superior to other baselines in most cases in terms of average values less values means more fairness .
ourmethodhaswonthebaselinesin37outof55casesunder diand38outof55casesunderspd.
comparedwiththebest baseline fair smote ltdd has comparable performance under di 3winsand3losses whileitperformsbetterunderspd 5wins and losses .
we conclude that our method significantly improves the fairness of ml software and surpasses the baselines in most cases.
resultsforperformance.
welistthecomparisonresultsunder performance indicators in table .
to compare the effects of different methods on the performance loss we compare our method and baselineswiththeresultsofthe originalclassifierswithoutapplyingfairnessalgorithms.generally allmethodshavelossestothe performanceoftheoriginalclassifiers.intermsofacc recall and authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa yanhui li linghan meng lin chen li yu di wu yuming zhou and baowen xu table fairness comparison of our method and baselines with logistical regression.
lessvalues means more fairness.
indicators methodsdatasets w t l adult raceadult sexcompas racecompas sexgerman default heart bank meps15 meps16 student di original .
.
.
.
.
.
.
.
.
.
.
reweighing .
.
.
.
.
.
.
.
.
.
.
dir .
.
.
.
.
.
.
.
.
.
.
fairway .
nan .
.
.
.
.
.
.
.
.
fair smote .
.
.
.
.
.
.
.
.
.
.
ltdd .
.
.
.
.
.
.
.
.
.
.
spd original .
.
.
.
.
.
.
.
.
.
.
reweighing .
.
.
.
.
.
.
.
.
.
.
dir .
.
.
.
.
.
.
.
.
.
.
fairway .
.
.
.
.
.
.
.
.
.
.
fair smote .
.
.
.
.
.
.
.
.
.
.
ltdd .
.
.
.
.
.
.
.
.
.
.
the background color indicates the comparison result between ltddandbaseline methods .
the gray background indicates that our method wins the baseline that is to say the p value is less than .05andthe isgreaterthan0.
theblackbackgroundindicatesthatourmethodlosestothebaseline thatistosay the p valueislessthan0.05andthe valueislessthan .
awhitebackground indicates a tie.
the classifier predicts all results for the privileged class as .
table performance comparison of our method and baselines with logistical regression.
indicators methodsdatasets w t l adult raceadult sexcompas racecompas sexgerman default heart bank meps15 meps16 student accoriginal .
.
.
.
.
.
.
.
.
.
.
reweighing .
.
.
.
.
.
.
.
.
.
.
dir .
.
.
.
.
.
.
.
.
.
.
fairway .
.
.
.
.
.
.
.
.
.
.
fair smote .
.
.
.
.
.
.
.
.
.
.
ltdd .
.
.
.
.
.
.
.
.
.
.
recalloriginal .
.
.
.
.
.
.
.
.
.
.
reweighing .
.
.
.
.
.
.
.
.
.
.
dir .
.
.
.
.
.
.
.
.
.
.
fairway .
.
.
.
.
.
.
.
.
.
.
fair smote .
.
.
.
.
.
.
.
.
.
.
ltdd .
.
.
.
.
.
.
.
.
.
.
false alarmoriginal .
.
.
.
.
.
.
.
.
.
.
reweighing .
.
.
.
.
.
.
.
.
.
.
dir .
.
.
.
.
.
.
.
.
.
.
fairway .
.
.
.
.
.
.
.
.
.
.
fair smote .
.
.
.
.
.
.
.
.
.
.
ltdd .
.
.
.
.
.
.
.
.
.
.
the background color indicates the comparison result between fairness methods and theoriginal method.
the gray background indicates that the fairness method wins the original method that is to say thep value is less than .
and the is greater than .
the black background indicates that the fairness method loses to the original method that is to say the p value is less than .
and the is less than .
a white background indicates a tie.
false alarm our method significantly damages the performance of the original model in out of cases which is less than out of 4baselines.besides ourmethodhasonly2caseswithsignificant damage under false alarm which is the best among all fairness methods.
it can be said that our method maintains the original performanceinmostcases i.e.
thenegativeimpactofouralgorithm on performance is less or comparable.
answertorq2 ourmethodperformsbetterthanbaselinesin the improvement of fairness indicators with the performance damage less than or comparable to baselines.
rq2a how well does our method compare with the state ofthe art fairness algorithms under different classifiers?table fairness comparison between our method and baselines with three classifiers.
vs. reweighing vs. dir vs. fair smote vs. original di spd di spd di spd di spd win tie loss because fairway s model optimization part is designed for logistic regression models we do not compare with fairway here.
motivationandapproach.
toverifythegeneralizationofltdd we also conduct our experiments under two other classifiers nb and svm.
similarly we run the experiment times and compare it with baselines under fairness and performance indicators.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
training data debugging for the fairness of machine learning software icse may pittsburgh pa usa table5 comparisonofperformanceindicatorsbetweenoriginalclassifiersandfairnessmethodsin11scenariosfor3classifiers methods reweighing vs. original dir vs. original fair smote vs. original ltdd vs. original indicators acc recallfalse alarmacc recallfalse alarmacc recallfalse alarmacc recallfalse alarm win tie loss resultsforfairness.
weshowthecomparisonoffairnessmetrics intable4.wecountthenumberofwin tie andlosscasescompared withthebaselinesandsummarizetheresultsofthethreeclassifiers together.
when the number of wins is greater than or equal to the number of ties plus losses we mark this column with the gray background whichmeansthatourmethodissignificantlybetter thanthebaseline.itcanbeseenthatin7of8scenarios ourmethod is significantly better than baselines.
when compared with fairsmote our results on di are similar but our number of wins is still slightly higher than fair smote.
resultsforperformance.
welistthecomparisonresultsofthe fairnessindicatorsintable5.wecomparethefourmethodswith theresultsoftheoriginalclassifierandcountthenumberofw t l. it can be seen that our method has the least number of losses in termsofaccandfalsealarmcomparedwiththeoriginalclassifier there are and cases that produce losses respectively.
answer to rq2a with three different classifiers our method stilldefeatsbaselines inmostcasesunderfairnessindicators and also has the least loss under acc and false alarm.
rq3 is our method actionable for fairness improvement in realistic scenarios?
motivation and approach.
in previous rqs we have compared the prediction results of our method and baselines and the results haveshownthatourmethodcangeneratefairerpredictions.inthis rq we introduce a new indicator the rate of favorable decisions t o measuretheactionabilityoffairnessmethods.ourmeasurement is basedon the followingpoints a in realistic scenarios the rate offavorabledecisionsislimitedduetothefinitesocialresources e.g.
for the loan application the rate of approval is restricted b beforeapplyingfairnessmethods originalmlsoftwarehasmade favorable and unfavorable decisions among which the original favorableratecouldbeconsideredasabenchmark c afterapplying fairness methods ml softwaremight changethe favorablerate.
if thefavorablerateessentiallyincreases itwouldalsoraisetheneed for social resources even beyond the boundaries of available social resources reducingactionability.inthisrq wewillcompareour method with fair smote under the new indicator.
results.
the result of the comparison is shown in figure where we employ two endpoints to indicate the favorable rates of the privileged and non privileged classes and connect them to drawa line.
the short horizontal stroke indicates the whole favorable rate.
the figure s red blue and green points represent the original value the result of fair smote and that of ltdd respectively.
itcanbeseenthatinmost 10outof11 cases thewaythatltdd improvesfairnessistoincreasethefavorablerateofnon privilegedclasses and to reduce the favorable rate of privileged classes sothat the whole favorable rate is very closeto the original value.
thismeansthatourmethoddoesnotneedmoresocialresourcesto achieveinmostcases.onthecontrary fair smotetriestoincreasethefavorablerateofprivilegedandnon privilegedclasses resultinginasignificantincreaseofthewholefavorablerate.forexample ontheadultdataset thewholefavorablerateoffair smoteincreases about .
times.
this means that the implementation of fair smote needs much more social resources.
answer to rq3 our method improves the fairness indicators while ensuring that the whole favorable rate is close to the original value and does not need more social resources.
discussion wefurtherdiscusstheaimsandresultsofourmethodltddinthe following six points.
.
distribution of biased features as described in sections .
and .
we judge whether features are biased according to the p value generated by wald test of linear regression model.
here we calculate the distribution of biased features with p value .
.
figure5showstheaverageproportionofbiasedfeaturesin11 scenarios where the blue bars indicate the proportion.
we can see thatin8outofthe11scenes thepercentsofbiasedfeaturesareover .
in the compas data set when the race isused as a sensitive feature all other features are considered biased which means that thehighassociationwiththesensitivefeatureisuniversalinthe data set.
besides we have also observed that the proportions of biasedfeaturesinthethreedatasets bank student andgerman arethelowestthree.accordingtotheresultsofrq1 thefairness optimization effect of these three data sets is also the worst among alldatasets.wearguethattheproportionofbiasedfeaturesmay be related to the fairness improvement effect of ltdd.
.
a case study on the compas dataset this section will use an example to show how our method eliminatesbiasedpartsfrombiasedfeatures narrowingthedifferencein these biased features between privileged and unprivileged classes.
asshowninfigure6 thisisthedistributionofthe age feature for privileged white and unprivileged non white categories on thecompasdataset.theorangedottedlinerepresentsthemedian value the purple triangle represents the average value and the green diamonds indicate the outliers.
figure a is the original distribution of the age feature.
it can be seen that the mean and median values of age of white authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa yanhui li linghan meng lin chen li yu di wu yuming zhou and baowen xu figure comparison of our approach with fair smote on favorable rates.
the favorable rates of privileged and non privileged classes are represented by two endpoints while the whole favorable rate is represented by the short horizontal stroke.
figure distribution of biased non sensitive features.
a before b after figure an exampleto show howltdd revises biasedfeatures on compas dataset.
figures a and b show the original and revised age distribution by normalizing into of non white and white on compas dataset people are obviously higher than that of non white people.
af ter removing the association between the age feature and the race feature thedistributionsof age betweenthetwogroups are shown in figure b .
it can be seen that both sets of values have been shifted which makes thedistribution between the twotable6 comparisonofaodandeod betweenourmethod and fair smote in scenarios for classifiers modelslsr nb svm meanw t l meanw t l meanw t l aodfair smote .
.
.
3ltdd .
.
.
eodfair smote .
.
.
5ltdd .
.
.
groups closer.
from the mean and median we can see that after removingbiasedparts ourmethod narrowsthedistributiondifference between the privileged group and the unprivileged group.
.3performanceunderotherfairnessindicators as observed in previous studies there may be conflicts between different fairness indicators.
to verify the performance of ourmethodunderotherfairnessindicators wecompareourmethod withthestate of the artmethodfair smotemethodunderother two indicators aod and eod with three classifiers.
thedifferencebetweentheresultsofourmethodandfair smote is shown in table .
as can be seen from the table our methodis slightlybetterthan fair smotein most scenarios.the average value of the two indicators of our method in the three classifiers is lower lower means more fairness than fair smote.
except for the eodindicatorundersvm thenumberofwinsforourmethodis greaterthan thenumberoflosses.
combinedwiththeconclusion in rq2 it can be seen that our method has a greater improvement under other fairness indicators compared to fair smote.
.
differences between di and spd intuitively diandspdaretwosimilarfairnessmetrics.inthissub section wewillconductadditionalanalysistoshowthedifferences between di and spd.
a we give a case study to show the differences between di and spd.
given two groups a band their favorable rates under two fairnessmethods f1 f2 r1 a .32andr1 b .2forf1 andr2 a .
andr2 b .
forf2.
according to the calculation of di and spd we authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
training data debugging for the fairness of machine learning software icse may pittsburgh pa usa obtain the following results di f1 r1 a r1 b .
di f2 r2 a r2 b .
spd f1 r1 a r1 b .
spd f2 r1 a r1 b .
wewoulddrawdifferentconclusionsfromthecomparisonbetween f1andf2underdiandspd fordi f2performsbetterthan f1 more close to means better for spd f1performs better than f2 more close to means better .
b we compute the pearson correlation coefficient between the values of di and spd in studied scenarios for five methods.
we observe that the coefficient is very small in some scenarios e.g.
.
for the german dataset which means the values of di and spddonothavehighcorrelations.hence oneofthemcannotbe directly substituted for the other.
.
performance with other association approaches?
this paper employs the linear regression model to obtain the association between features.
it is unclear how other association approacheswork.inthissection weconductanexperimentwith anotherassociationapproach polynomialregression whichisan extension of simple linear regression xn a b1 xs b2 x2 s bd xd s wherexsandxndenote sensitive and non sensitive features.
to makethefittingresulthavealargedifferencefromthelinearregres sion wesetthedegree d ofthepolynomialto20 linearregression couldbeconsideredasaspecificpolynomialregressionwhen d .
we compare the results of linear regression and polynomial regressionunderfairnessindicators.weobservethatinmostcases theresultsbetweenthetwomethodsarecomparable andtheresults of linear regression are significantly better worse in scenarios.
.
insight of our method our method ltdd explores a simple but effective model linear regression toidentify estimate andremovethehighassociation between non sensitive features and sensitive features as the biased parts of the non sensitive features.
the results of our experiments support the claim that ltdd can largely improve the fairness of mlsoftware withlessorcomparabledamagetoitsperformance.
our study may lead to three insights for the following studies in the fairness of ml software a the distribution and association of feature values in training datawouldbehelpfultoestimateandimprovethefairnessofml software.
we encourage following researchers to employ more effective methods to model the feature distribution and association.
b although ml software is much different from traditional software theresearchmethodsintraditionalsearealsoapplicable to the related research topic of ml software and can achieve good performance.weencouragefollowingresearcherstoapplymore traditional se methods and ideas to the research of ml software.
c for fairness research our study provides a new practical perspective we employ the rate of favorable decisions to measurethe actionability of fairness methods which indicates the actual costofsocialresourcesforfairnessmethods.wehopethatfuture researchers can take into account the actual cost while focusing on fairness.
threats to validity in this section we discuss the threats to validity of our approach.
first of all the choice of data set may be a threat.
we employ 9datasetswidelyusedinfairnesstestingtoevaluateourmethod.
however thesedatasetsmaynotbesufficient becausethedistribution and characteristics of different data sets are not the same.
we will introduce more datasets to test our method in the future.
second the choice of classifiers may be a threat.
in the experiment we mainly use the logistic regression model for fairness testing whichisusedinmanyfairnessstudies buttheinfluenceofthemethodondifferentlearnersmaybedifferent.althoughwealso testtheperformanceundertheothertwolearners therearestillmore complex learners worth experimenting with.
in the future wewilltesthowourmethodperformsincomplexneuralnetworks.
finally thefairness indicators we use may also be a threat.
previous studies have pointed out that there may be contradictions betweendifferentfairnessindicators.therearesomanyfairness indicators proposed at present and it is impossible to satisfy all fairnessindicators .therefore theindicatorsshouldbeselected reasonablyaccordingtotheactualscenario.inthepaper wemainly discusstheresultsunderdiandspd andchecktheeffectofour method under other indicators in the discussion.
conclusion the widespread usage of machine learning software has raised concerns about its fairness.
more and more se researchers havepaid attention to the fairness of ml software.
the training set is consideredtobeoneofthereasonsforthebiasofmlsoftware.this paper adopts a new perspective the root cause of the unfairness could be biased features in training data.
toobtainfeaturesunbiased wetrytodebugfeaturevaluesin training data identify which features and which parts of them are biasedandexcludethebiasedpartsofsuchfeatures.toachievethisgoal weproposeanovelmethod linear regressionbased training datadebugging ltdd which employs the linear regression modeltoidentifythebiasedfeatures.weintroduceninedatasetstoevaluateourmethodandcompareourmethodswithfourbaselines.
the results show that our method performs better in improving thefairnessindicators anddamagestheperformanceslightly.in thefuture wehopetoconductresearchonmoreextensivedatasets to explore different characteristics of the datasets.
.
replication package we provide all datasets and source code used to conduct this study at