evacrc evaluating codereview comments lanxinyang statekey laboratory of novel softwaretechnology software institute nanjing university nanjing china lanxin.yang smail.nju.edu.cnjinwei xu statekey laboratory of novel softwaretechnology software institute nanjing university nanjing china jinwei xu smail.nju.edu.cnyifan zhang statekey laboratory of novel softwaretechnology software institute nanjing university nanjing china yifan.zhang smail.nju.edu.cn he zhang statekey laboratory of novel softwaretechnology software institute nanjing university nanjing china hezhang nju.edu.cnalbertobacchelli departmentof informatics universityof zurich zurich switzerland bacchelli i f i.uzh.ch abstract in code reviews developers examine code changes authored by peers and provide feedback through comments.
despite the importance of these comments no accepted approach currently exists forassessingtheirquality.therefore thisstudyhastwomainobjectives todeviseaconceptualmodelforanexplainableevaluation of review comment quality and to develop models for theautomatedevaluationofcommentsaccordingtotheconceptual model.
to do so we conduct mixed method studies and propose a newapproach evacrc evaluatingcodereviewcomments .to achievethe f irstgoal wecollectandsynthesizequalityattributes of review comments by triangulating data from both authoritative documentationoncodereview standardsandacademicliterature.
we then validate these attributes using real world instances.
finally we establish mappings between quality attributes and grades by inquiring domain experts thus de f ining our f inal explainable conceptualmodel.toachievethesecondgoal evacrcleverages multi labellearning.toevaluateandre f ineevacrc weconduct anindustrialcasestudywithaglobalictenterprise.theresults indicate that evacrc can effectively evaluate review comments whileofferingreasons for the grades.
data andmaterials ccs concepts software and its engineering software development process management empirical software validation .
keywords code review reviewcomments qualityevaluation permissionto make digitalor hard copies of allorpart ofthis work for personalor classroom use is granted without fee provided that copies are not made or distributed forpro f itorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation onthe f irstpage.copyrights forcomponentsofthisworkownedbyothersthanthe author s mustbehonored.abstractingwithcreditispermitted.tocopyotherwise or republish topostonserversortoredistributetolists requirespriorspeci f icpermission and or a fee.
request permissions from permissions acm.org.
esec fse december san francisco ca usa copyright heldby the owner author s .
publicationrightslicensed to acm.
acm isbn ... .
format lanxin yang jinwei xu yifan zhang he zhang and alberto bacchelli.
.
evacrc evaluating code review comments.
in proceedings of the 31stacmjointeuropeansoftwareengineeringconferenceand symposium on the foundations of software engineering esec fse december san francisco ca usa.
acm new york ny usa 13pages.https introduction codereviewwasdesignedprimarilytodetectqualityissuesinsoftwaredevelopment thenwasfoundusefulinidentifyingcode weaknesses and seeking opportunities for improvement in design testing and security e.g.
.
apart from supporting quality assurance code review bene f its other aspects such as team awareness knowledgetransfer andtoolimprovement .
compared with other quality assurance practices e.g.
testing codereviewmorelargelyreliesonorganizationalcultureanddevelopers expertise experience participation and engagement .
moreover code review sometimes may become wasteful or even harmful for projects and teams due to its confusion time and effort consuming low efficiency and effectiveness andothernegative side effects .
overthepastfewyears standards guidelines andexperience havebeensharedtoimprovecodereview e.g.
ieeecomputersociety google and gitlab .
most of these resources underlinethekeyroleofreviewfeedback assharedthroughreview comments.
review comments serve as a communication tool for reviewerstosharetheirassessmentofcodechanges.thus these comments are the key guide for corrections andimprovements.
despite the importance of review comments however only few studiesfocusontheirevaluation.twostudies haveinvestigateddevelopers perceptionoftheusefulnessofreviewcomments anddevelopedmodelsforautomatingtheevaluationprocess.however these approaches output a binary label i.e.
useful or not withoutprovidinganyadditionalinformationtojustifyitorsuggest improvements.
this binary assessment could inadvertently overlook potentially useful review comments potentially leading todeveloperscontestingtheresultsorevenresistingtheevaluation.
existingpredictivemodelspredominantlydependon reviewcontext features e.g.
change trigger determining if any changes occur 275esec fse december3 san francisco ca usa lanxinyang jinweixu yifanzhang he zhang andalberto bacchelli within a line of the highlighted comment in subsequent iterations andgratitude assessingifthecodeauthorrespondswithexpressions like thank you .
they also utilize experience features e.g.
code authorship countingthecommitsadeveloperhas madeona f ileand external library experience gaugingadeveloper sfamiliaritywithexternallibrariesfromareviewed f ile .
asigni f icantlimitationofthesefeaturesistheiravailabilityonly afterthe completion of the review thus falling short in offering instantaneous feedbackto the authorofthe comments.
wecontendthattheevaluationofreviewcommentsshouldgo beyond assigning a grade.
instead the evaluation should guide corrections or enhancements.
herein we propose an approach evacrc evaluatingcodereviewcomments tosupporteffectiveevaluationofreviewcomments.evacrccombinesaconceptualmodelandanautomatedmodel.theformeraimstoofferan explainable conceptualmodel forreview comments while thelatter aims to automate their evaluation.
following the guidelines formixed methodresearchdesign wedevisetwostudiesfor implementingandevaluatingevacrc respectively.
inthe f irststudy toimplementtheconceptualmodelofevacrc we adopt data triangulation including authoritative documents academic literature and real world examples to collect synthesize andvalidatequalityattributes.ourresultingconceptualmodel consists of four review context independent quality attributes that characterizereviewcomments i.e.
emotion question evaluation and suggestion andtheirmappingstooneoffourqualitygrades i.e.
excellent good acceptable and poor .
as opposed to prior approaches e.g.
which output only a binary grade i.e.
useful or not evacrc is designed to output one overall quality grade together with four speci f ic quality attributes that explain it.forinstance giventhereviewcomment getabsolutepath is not allowed in security scanning use getcanonicalpath instead.
evacrc evaluates it as acceptable with the quality attributes valuesbeing emotion positive question no evaluation yes and suggestion yes respectively.
according to the conceptual model evacrc generates the overall quality grade for each reviewcommentbyfollowingtheprede f inedmappings cf.table asevaluatedandapprovedbydevelopersatalargeinformationand communicationtechnology ict enterprise and thefourspeci f ic qualityattributesarelearned fromgenuinereviewcomments.we formulate the learning task as multi label text classi f ication.
since manual classi f ication is time consuming and bias prone we use natural languageprocessing and machine learning to develop text classi f iers to form an automatedmodelofevacrc.
in the second study to evaluate and improve evacrc we carry out an industrial case study at a large ict enterprise.
we f irst continue to implement the conceptual model by establishing the enterprise speci f icmappingsbetweenqualityattributesandgrades.
we then collect and annotate review comments and build sixmulti labeltextclassi f ierstoseekthemosteffectiveforforming the automated model of evacrc.
finally we interview software practitionersatthisictenterprisetounderstandtheirperceptionof evacrc.
the results from the case study indicate that evacrc can effectivelyevaluatereviewcomments providingreviewersnotonly evaluation results quality grades that are more acceptable than a binary evaluation but detailed explanations quality attributes for timely correction andimprovement.
related work this section reviewsthe literature that isrelevantto our work.
usefulnessofreviewcomments.
theonlyaspectconcerning the qualityofreviewcomments investigatedsofaris usefulness .
bosu et al .
interviewed seven developers at microsoft discussing with them a total of almost comments in terms of usefulness.
developers de f ined as useful review comments that indicate functional issues identify validation issues alternate scenarios and transfer knowledge.
whereas developers ratedas somewhatuseful reviewcommentsthatindicate nitpickingissues indentation comments style identi f ier naming issues and alternatives.
finally developersratedas notuseful commentsthatmerelypresent questions appreciation and suggestionsfor future improvement.
hasanetal .
interviewedsamsung sdevelopersandfound thattheyde f ineas useful reviewcommentsthatidentifyatleast oneofthefollowing defects missinginputvalidations opportunities for making code efficient and optimized readability weakness logical mistakes redundant code corner cases opportunities for integrating code deprecated functions designweakness and codingstandardviolations.areview commentis notuseful whenitregards visualrepresentation issues that can be also identi f ied by static analysis tool false positive misinterpretation discussiononanalreadyresolved issue or solutionapproachwithwhichtheauthordisagrees.on some types of comments there was no consensus praise clari f icationquestions andsuggestionsonimprovingdocumentation.
automated evaluation of review comments.
bosu et al .
created an automated approach to classify review comments as either useful alsoincludingtheir somewhatuseful category or not useful .their approachwas developedby annotating989examples and devising a decision tree with hand crafted rules.
their approach heavily relies on the impact of comments particularly whether it triggered a subsequent change or whether it was labeled as wont f ix .
their model reached mean precision recall and accuracyof89 and83 respectively.
unlikebosuetal .
rahmanetal .
onlyleveraged change triggering to label a dataset of the usefulness of review comments.
rahmanetal.computedfeaturesconcerningboththe textualreview comments e.g.
readingease andcodeelement ratio and the reviewerexperience e.g.
codeauthorship andexternal libraryexperience fordevelopingpredictionmodels.theauthorscollected annotated examples and developed three learning based classi f iers na vebayes logisticregression andrandomforestsfor binary evaluation.randomforests achievedthe bestperformance withameanprecisionof65.
meanrecallof65.
meanf1score of65.
andmean accuracyof .
.
in additionto adopting the features presented in bosu et al .
andrahmanetal .
hasanetal .
devisedmorefeaturesfrom the perspectives of textual review comments e.g.
word count reviewcontexts e.g.
authorresponded andreplysentiment and reviewer experience e.g.
reviewing experience .
they collected annotated examples and trained six classi f iers decision tree random forests support vector machine multi layer perceptron xgboost andlogisticregression.thebest performingclassi f ieris random forestswithamean accuracyof .
.
276evacrc evaluating code reviewcomments esec fse december3 san francisco ca usa overall thestudieshavecreatedmodelstoautomaticallypredict theusefulnessofreviewcommentswithoutprovidinganyexplanation.however theunexplainedresultsmayleadtodevelopers disagreements with the results or even resistance to the evaluation itself.
in addition the prediction models rely on a large number of complex hand crafted features some of which can only be assessed after the comments have been read and acted upon by other developers e.g.
whether the comment triggered a change or whether it has been labeled as wont f ix .
therefore the proposed approaches havelimitedtonouseinpractice forexample togiveimmediate feedback to the author of a review comment about what they have written.
furthermore the heavy feature engineering work e.g.
collecting and calculating complex features from multi source heterogeneousdata selectingsuitablefeatures andscalingfeatures asks for agreatamount oftime andeffort.
studyoverview westartbypresentingreal worldexamplesofreviewcomments cf.
table1 whichweusetomotivatewhyitisimportantandchallenging to evaluate review comments.
then we present an overview of our proposedapproach followedbyour researchquestions.
table real world examples ofcodereview comments idcomment c1 delete c2 magicnumber c3 whydo you want this?
c4 whyagain hardcoding???
c5 ithink it shouldbe logger.info c6 shouldit be public or protected?
c7 change the logginglevel to debug.
c8 downloadtips mdownloadtips c9 getabsolutepath is not allowed in security scanning use getcanonicalpath instead.
c10 pleasediscussboxnamewith xyz1111111 wehaveaboxnamingrule and useit in tracing data.
c11 check whether these scripts already executed or need to execute in .
.
.103and placethe scriptsaccordingly.
c12 do these values refer to speci f ic values?
if you don t need to use minusseparated statements like full time remove the minuses and quotes.
otherwise keep just the minus separated statements and remove quotes from the others.
.
evaluatingreviewcomments challenges several challenges arisewhen evaluatingreviewcomments.
challenge1 disagreementsbyreviewers.
pastresearch e.g.
has recognized that there isa signi f icant variance inthe qualitymaturity amongreviewcomments.howcanthese differences be characterized and graded?
there are no standard criteria and approachesfor evaluatingreviewcomments.
in our industrial case study cf.
section one manager we interviewedexplainedthatitischallengingtogenerallyevaluate theusefulness ofacomment especiallyonabinaryscale assheput it it is hard to say that a review comment is absolutely not useful in practice ...even one interrogation mark can serve as a reminder to developers.
however we are seeking to develop guidelines and standardsforperformingcodereviewsaswellasevaluationcriteria.
otherwise code review becomes wasteful.
other developers argued that using only a usefulness criterion is more likely to result in biases and carelessness.
to de f ine the quality of a review comment some developers suggested taking into account the type of defectsit detects e.g.
comments detecting security or critical issues are to be consideredhigh quality .
finally providing negative feedback to reviewers about their commentsmayresultindisagreementsandnegativefeelings especially if no explanation is provided.
negative evaluations may impact reviewers productivity in either code reviews or and other software activities possibly leading them to leave project teams in extreme cases.
challenge2 modelingreviewcomments.
reviewcomments often include jargons abbreviations and misspellings.
table presentsreal worldexamplestoillustratethelinguistic diversityand complexity ofreviewcomments lexicon naturallanguage programminglanguage andspecialsymbolsaremixed c5 c6 c8 c9 syntax declarative c5 imperative c7 generalinterrogative c3 andselectiveinterrogative c6 and pragmatics indication c2 question c4 c11 suggestion c1 c12 andassignment c10 .
in addition review comments transfer reviewers emotions e.g.
c4 whichhaveimpactsondevelopers feelingsaswellasonthe qualityofthe projectthey are workingon .
moreover review comments transfer complex functional semanticsofthelanguage e.g.
causality contrast exempli f ication clari f ication similarity and hypothesis and offer multiple practical functions e.g.
correctness decision management and interaction .whenitcomestotheprimarygoalofcodereview i.e.
f inding defects1 review comments indicate multiple types suchasevolvabilityorfunctionaldefects andcompilererrors .
gunawardena et al .
identi f ied types of defects thatfellinto15groupsincodereview.thediversityandcomplexity ofreviewcomments make themhardto comprehendandmodel.
.
evaluatingreviewcomments goals we strive to achieve the following goalsinour studies.
goal explainable evaluation.
we aim to establish an explainable conceptual model.
the term explainable indicates that the conceptualmodelshouldoutputnotonlythe f inalqualitygrade as previousstudiesdid butalsothecausesforthatgrade.weplace emphasis on letting developers know the causes behind evaluations to help them learn and improve.
we also strive to ensure that the conceptual model is independent of the review context e.g.
author reviewership review process outcome .
let us consider areviewcommentthatismerelyaquestionmark.inonereview it may be evaluated as useful because it triggers code changes whereas inanotherreview itisevaluatedas notuseful because it triggers no code changes.
such an evaluation only based on the effect of the comment offers little help to improve.
instead the evaluationshouldofferexplanationsthusleadingtoimprovements.
goal automated evaluation.
automating the process of evaluation is essential due to the time and effort consuming nature of the manual evaluation of comments.
we aim to develop a model thatautomatically learnsthe high levelrepresentationofreview comments to tackle their linguistic complexity.
we aim to avoid heavyhand craftedfeatureengineering e.g.
featureextractionand 1in this paper the term defect is used as a general concept that depicts any conditiondeviatingfromexpectationsbasedonprede f inedstandards orfromdevelopers experiences.the defect couldbe bug vulnerability error etc.
inaspeci f iccontext.
277esec fse december3 san francisco ca usa lanxinyang jinweixu yifanzhang he zhang andalberto bacchelli selection fortrainingautomatedmodels.therefore weinvestigate naturallanguageprocessing anddeeplearningfor automation.
we expect the conceptual model quality attributes and mappings ratherthantheautomatedmodelstoprovideanexplanation oftheevaluation results as researching thelatter e.g.
explaining how the neural network based models understand semantics from genuine reviewcomments isbeyondthe scope ofthis study.
.
evaluatingreviewcomments approach ourapproach evacrc isasynergyoftwosub goals explainable evaluation andautomated evaluation .
the ultimate goal g to associatetwosub goals ofthisstudydependsonthefunction f to learnqualityattributes qfromreviewcomments andthemapping z to determine quality grades with attributes .
once the attributes aredeterminedandtheirmappingstogradesareestablished the remaining task is to seek appropriate models and strategies for learning the objective function f. our goalcan be formulated asa supervisedmulti label textclassi f ication problem.
figure1showsanoverviewofresearchdesignsofthismixedmethodresearch.inthe f irststudy weimplementthedesigns as presented above of evacrc in the second study we evaluate evacrc as well as seekimprovements.
.
research questions our study isstructuredaround three researchquestions rqs .
rq1 whatqualityattributesareappropriatefortheconceptualmodel?
motivation we propose rq1 to guide the collection synthesis and validation of quality attributes to form the core components of the conceptualmodel.
methods we address rq1 by using data triangulation and thematic synthesis .
as shown in figure the top of the implementation part ourtriangulateddatasourcesincludeauthoritative documents academic literature and real world examples.
triangulationisthehigh levelmethodweused whilethematicsynthesis isusedto synthesize qualityattributes speci f ically.
rq2 how effective are text classi f iers in predicting rq2.
thequalityattributes and rq2.
thequalitygrades?
motivation aslittleisknownabouthowwelltextclassi f iersperform we propose rq2 to guide the comparisons.
method to address rq2 we conduct experiments the bottom of theimplementation part in figure in which we use the review commentscollectedfromanictenterpriseasthedataset weset sixmulti labeltextclassi f iersand evaluatethemusingfourmetrics for answeringrq2.
andthree metrics for answeringrq2.
.
rq3 how dopractitioners perceive evacrc?
motivation we investigate how practitioners perceive the effectiveness of evacrc as a further evaluation and to guide future improvements.
method to address rq3 we conduct focus group interviews theevaluation part in figure with practitioners from the ict enterprisethatprovidesuswiththeexperimentaldataset.sothat we could receive insightful comments andrecommendations.
studyi implementing evacrc we describe our mixed methodstudy for implementingevacrc.
.
conceptual model .
.
q ualitya t tributes.
ouroriginalintentionwastocollectqualityattributesfromfagan sarticle andieeecomputersociety s standards which are the main authoritative documents concerning code review.
unfortunately these did not directly meet our goal as expected.
therefore we followed the guidelines toadoptdatatriangulation i.e.
a combinationofmethodologies in the study of the same phenomenon to gather different types of evidence to collect quality attributes and cross validate them.
data triangulation increases the con f idence of research data creates innovative and multi perspective ways of understanding a problem andreveals unique f indings .
with data triangulation we added data sources i.e.
academic literature andreal worldexamples andtailored our data collectionmethodsbasedonthebest f itwiththegoal .theresearch methodper data sourceiselaboratedas follows.
source authoritative documents.
we investigated designandcodeinspectionstoreduceerrorsinprogramdevelopment michaelfagan and ieeestandardforsoftwarereviews and audits ieeecomputer society .
the formeris theseminalstudythat formalizes codeinspection a.k.a.
classical code review while the latter presents standards forsoftwarereviewsandaudits.eventhoughseveraltypesofreviewsexist e.g.
managementreviews technicalreviews andwalkthroughs they are all required to review source code i.e.
code review is fundamental .
from these documents we found that the core objective of code review is reducing defects this is the primary qualityattribute reducing defects .
source2 academicliterature.
the f irstadditionaldatasource is academic literature.
since academic literature has been peerreviewed therigorandcredibilityofitscontentscanbeguaranteed tosomeextent.although codereviewcomments arethesubject of our research our initial searches with google scholar ieee xplore digital library and acm digital library indicated that limitedliteraturerefersspeci f icallytoreviewcomments.therefore togatherourqualityattributes webroadenedoursearchscopeand analyzedliterature whose subjectis code review .
we employed rapid literature review a form of evidence collectionandsynthesisthatsimpli f iescomponentsofasystematic literature reviewtoquicklyacquireknowledge .toretrieve the primary studies to analyze we gathered the papers from the twomostup to date codereview relatedsystematicreviews i.e.
.
both reviews were published in in the journal of systems and software .
davila and nunes collected primary studiesfrom2005to2019.wangetal .
collected112primary studies published between and .
by combiningthe two sets we were left with182primary studiesto analyze.
sinceonlyafewstudiesdirectlypresentqualityattributes we extractedtwoitems bene f its andchallenges fromeachprimary study if they exist to synthesize quality attributes.
both items are associated with code review working for investigating what bene f its can be fully achieved and what challenges can be to some extentmitigatedbyevaluatingandimprovingthequalityofreview 278evacrc evaluating code reviewcomments esec fse december3 san francisco ca usa evaluating code review comments2 authoritative documents systematic reviews primary studies benefits challenges quality attributes grades automation natura language processing text classifier automated modeldata triangulation thematic synthesis task goals impleme ntation the proposed approach evacrc outputs evaluation multi label learning bertq 20k real world review comments quality attributes 16mapping rulesexplainability conceptual model experimentexperiment study1 study2focus group interview reports feedback figure anoverview ofmixed method researchdesigns with severalresults comments .
at the same time with reference to the authoritative documents the core objective of code review is extracted to further guide the attribute synthesis.
moreover we used another new datasource i.e.
real worldexamples tovalidateandimprovethe qualityattributesoutputinthisstudy.theresearchmethodsand procedures are as follows.
we synthesizedquality attributesfrom the two setsof items by usingthematicsynthesis .accordingtocruzesanddyb a general thematic synthesis requires steps to extract data u1d4461 code data u1d4462 translate codes into themes u1d4463 create amodelofhigher orderthemes u1d4464 and assessthesynthesis trustworthiness u1d4465 .
in u1d4461 weassigned the sampleset to two researcherswith prior experience with analyzing qualitative data.
their goal was to extractbene f its and challenges by reading each primary study s abstract introduction andanyothersections if needed.
becausethe abstract and introduction may outline the code review related bene f its and challenges while the others focus on the speci f ic subject andtechnicalissues e.g.
improvingtheaccuracyofrecommending reviewers .
bothresearchers startedby analyzing studies together to coordinate on the task then independently analyzed the remaining studies.
in u1d4462 thosetworesearchersperformed opencoding withexisting keywords in the text.
for instance given the primary study stating contemporary code review is a widespread practice used bysoftware engineerstomaintainhighsoftwarequalityand share project knowledge the researchers extracted software quality assurance and knowledgeexchange asbene f its andforthesentence however conducting proper code review takes time and developers often have limited time for review the researchers extracted time consuming aschallenge.
we did not measure the inter coder reliabilityatthisstageduetothelackofprede f ined completeitems beforeperformingcoding.instead theresearchersveri f iedtogether theextractionresultsandmadeanycorrectionsafterreachingan agreement.
the extraction results were also randomly checked by another researcher to further mitigate biases.
through this step wesummarized22bene f itsand35challenges.themostfrequentlymentioned bene f itsare defectdetection f ix u1d4351 software code qualityassurance improvement u1d4352 knowledge transfer u1d4353 increasing team awareness u1d4354 and exploringalternative solutions u1d4355 etc.the most frequentlymentioned challenges include f inding suitable reviewers u1d4361 time cost effort consuming u1d4362 understandingcodechanges u1d4363 fairness bias con f lict u1d4364 defectescaping u1d4365 increasingworkload u1d4366 lackofsupporttools u1d4367 and expertise needed u1d4368 etc.
thecompletedataitemsareavailableintheonline replication package .
in u1d4463 we excluded the bene f itsandchallenges that cannot be analyzed from the review comments e.g.
u1d4361 u1d4362 .
for instance weremoved u1d4362becauseevacrcisexpected tobereviewcontextindependent i.e.
the only data source input is textual review comments .
although the time effort and cost may impact the quality ofreviewcomments theywouldalso changewiththeknowledge andexperienceofreviewersandthecomplexityofcodechanges reviewed which are difficult to measure and can be affected by biases .
then we performed axial coding to group items with relationships suchasopposition e.g.
defect f inding anddefectescaping defect f inding subordination e.g.
defect f inding andcodequality assurance codequalityassurance andcausality e.g.
bias con f lict andteam awareness emotion .
in this step we brought all the items bene f itsandchallenges together.
in u1d4464 weperformed selectivecoding tooutputcorethemes quality attributes i.e.
qualityassurance andemotion.
in u1d4465 wevalidatedandimprovedthetwoqualityattributeswith real world reviewcomments.
the procedures are as follows.
source real world examples.
we analyzed review comments from commercial projects from domestic and overseas teams atoneglobalictenterprise 20k theexperimentaldatasetwas collectedfromthem andfrom f iveitenterprises 1k and oss projects eclipseandqt 3k toincreasethevalidity actionability and generalizability of the quality attributes.
having analyzed a large number of real world review comments we found reviewers frequentlyexpressconfusion e.g.
raisingquestions incomments.
in general there are two major types of questions to understand motivations and implementations of code e.g.
why printstacktrace is there?
and to express uncertainty for defects e.g.
is it a mandatory parameter?
.
although such review comments may not trigger code changes immediately they help to remindauthorsandotherreviewers ifexisted ofseekingfurtherdefects and transferprojectcontext codingknowledge and experiencebetween authorsand reviewers .
ina nutshell if there is any confusion in code review it should be raised in time to prevent possible risks by the misunderstanding.
by raising questions reviewerscanreducemisjudgmenttosomeextent orthey wouldresultinlow qualitycodeaswellasauthors negativefeelings.moreover reviewersandauthorshelpeachotherbyasking 279esec fse december3 san francisco ca usa lanxinyang jinweixu yifanzhang he zhang andalberto bacchelli usefulquestions.therefore weuse question que tocharacterize such reviewcomments.
moreover wefoundthatreviewcommentsexpresstwostyles when working for quality assurance point out defects e.g.
this f ield is not required and offer suggestions e.g.
delete this line.. this is for debugging .
therefore we break quality assurance synthesized at the second stage into evaluation eva and suggestion sug todescribethetwostylesofreview comments respectively.
review comments should be evaluated byassociatingfourattributestogethertoavoidpotential bias.for instance ifadopting evaluation only somereviewersmayroughly indicate incorrect here but neither explain the reason behind nor providesuggestionsforcorrection.inthiscase reviewcomments offer very little help to code authors.
table 2shows the ultimate qualityattributesinourconceptual model.notethatthequalityattributes by answering descriptive questions differ in a dichotomy u1d441 u1d45c u1d44c u1d452 u1d460 and for emotion negative is annotated with whilst neutral and positive are allannotatedwith1.
rq1 in retrospect by employing data triangulation four quality attributes of review comments emerge emotion question evaluation and suggestion .
the quality grade of areviewcommentcanbegeneratedbyassociatingtheoutputs ofits fourattributes cf.table .
.
.
a t tribute mappings.
given that binary evaluations e.g.
useful or not often lead to disagreements we establish four quality tiersforreviewcommentsinourconceptualmodel excellent iv good iii acceptable ii and poor i .additionally wemanually annotate quality attributes instead of comment grades to train automated models as it is easiertoreach agreement onattributes than on grades thus reducing inconsistency in annotation.
acknowledging that there can be possible pairings between the four quality attributes and the four grades and recognizing that eachsoftwareorganizationmayhaveuniquecriteriaforevaluating review comments we do not enforce strict mapping rules in our model.
rather we provide a sample mapping see table from an industrialcasestudy as areference.
.
automatedmodel .
.
technicalpreliminary.
asweareaimingtopredictfourattributes labels from a single review comment we can consider this issue as a multi label learning problem also known as multi label classi f ication.
multi label learning is a sub f ield of supervised learning.
itdiffers from binaryclassi f ication where each example corresponds to one of two labels and multi class classi f ication where each example is associated with only one label from multiple but mutually exclusive classes.
multi label learning allows an example to be linked with multiple labels.
each label representsadifferentclassi f icationtaskthatcan becorrelatedwith others.thisapproachoffersadvantagessuchasenhancingdataand computationalefficiency andreducing the riskofover f itting .
.
.
baselineclassifiers.
weevaluatedsixpopulartextclassi f iers aspotentialcandidatesfortheautomatedmodelinourevacrcsystem.theseclassi f iersincluderandomforests rf textcnn textrcnn dpcnn transformer and bert .among these rf is a traditional machine learning model based on pre designed features while the rest are deep learning models that simplifytheprocessbyavoidinglaboriousfeatureengineeringtasks suchasfeatureextractionandselection .rfwasincluded becauseithasproveneffectiveinpriorresearch .though thesemodelswereprimarilydesignedforsingle labeltextclassi f ication we adjusted their architecture speci f ically their output unitsandlossfunctions tosuitourmulti labeltextclassi f ication needs.
for the rf model we f irst used word2vec to convert a textual review comment into a vector effectively replacing feature engineering and then fed it into the rf model.
during the training phase for each classi f ier we f inely tuned their hyperparameters such as inputlength to adapt themto our speci f ic task.
.
.
evaluation metrics.
in line with the evacrc system s approachofde f iningthemaincomponentofreviewcommentevaluation as a multi label text classi f ication task we used standard multi label learning metrics to assess classi f iers hamming loss hl subset0 1loss 01l macro f1 mf andmacro auc ma .
hamming loss measures the proportion of labels that are incorrectly predicted while subset loss is stricter requiring all labelsforanexampletobepredictedcorrectly.macro f1andmacroauc calculate the f1 score and auc score independently for each class then average the results.
this process ensures all classes are treated equally and helps evaluate how well classi f iers handle class imbalance problems.
furthermore as predicting a single label can be viewed as a binary classi f ication problem we used precision p recall r f1 score f andauc a toevaluatetheclassi f ierson this aspect.
in terms of evaluation metrics lower values of hl and 01lsuggestbetterclassi f ierperformance whilehighervaluesfor other metrics indicate improved performance.
to avoid bias we conducted5 fold cross validation for eachclassi f ier.
studyii evaluating evacrc this section reports on an industrial case study that empirically evaluates evacrc includingexperiments andinterviews.
.
experimentaldata .
.
datacollection.
giventhesigni f icantvariabilityinpullrequest comments e.g.
sentence length and element complexity and the absence of such comments in many project s code f iles we focused solely on inline review comments as done in previous studies .asour aim istocreate automatedmodels with strong generalization capabilities we did not restrict other project characteristics such as the number of authorsandreviewers.
.
.
data annotation.
according to the con f iguration of our conceptualmodel eachreviewcommentisassigned f ivelabels.four of these labels represent the quality attributes which are manually annotated while the f ifth label indicates the quality grade automaticallydeterminedbasedonprede f inedmappings.table illustratestheenterprise speci f icmappings.theinitialversionof thesemappingswasdraftedbythreeauthorsfollowingdiscussions with practitioners from various software organizations.
all the authorscollaboratedtocreateanintermediateversion andthe f inal version wascon f irmed with professionals from theict enterprise.
280evacrc evaluating code reviewcomments esec fse december3 san francisco ca usa table descriptions ofqualityattributes descriptivequestion rationale emodoesit comment in a kind way?
codereviewshouldevaluate codechanges ratherthan authors.
quedoesit askquestions oraskfor con f irmation?
anything unclear in code review should be con f irmed to prevent them from becoming risks.
also knowledge transfer in codereviewshouldbetwo way to bene f ittwosides.
evadoesit presentevaluationsorspecifycodeweaknesses?
codereviewshould f irstdetect and identify defects.
sugdoes it provide suggestions for correction or prevention?
codereviewisexpected to help f ix defects improvequality and improvedevelopers qualityconcerns.
table exemplar mappings qualityattributes grades m1 m2 m3 m4 m5 m6 m7 m8 m9 m10 m11 m12 m13 m14 m15 m16 emo0 que0 eva0 sug0 iiiiii ii iii ii iii ii iii iiiv iv iv ii iv wemanuallyannotatedandcarefullycross checked17 000review comments section .
.
.
figure2shows the distributions of the experimental dataset from which we observe that neither qualityattributes norgrades are imbalanced.
acceptable good excellent poor suggestion evaluation question emotion no yes figure2 distributionsoftheexperimentaldataset left attributes right grades .
experimentalresults andanalysis .
.
answer to rq2.
a t tribute s perspective .
table4summarizestheperformanceofthesixtextclassi f iersinpredictingquality attributes with the best performances highlighted in bold.
our initial focus is on the precision and recall of each classi f ier.
for thepredictionof emotion textcnnstandsoutwiththehighest precision at .
but its recall is relatively low at .
.
dpcnn and transformerhaveevenlowerrecallrates at0.19and0.14respectively indicating a majority of the negative emotion examples were inaccurately predicted.
in our task we consider the negative emotion as the positive class as our objective is to identify it.
bert althoughitachievesaprecisionof0.
hasahigherrecallof .
whichplacesit f irstintermsofrecallamongthesixclassi f iers.
whenpredicting question rfemergesasthebestwiththehighest precision .
closelyfollowedbytransformer .
.bert with a precision of .
ranks third but exhibits the highest recall .
.
in terms of predicting evaluation bert s precision .
is lower than that of transformer .
but bert maintains second place.
a similar pattern occurs in the prediction of suggestion bert is secondinterms ofprecision .
but itleads for recall .
.
owingtoitsperformanceinrecall bertsigni f icantlyoutperformstheothermodelsintermsoff1 score.itregistersscoresof .
.
.
and .
for predicting emotion question evaluation and suggestion respectively.
additionally because of its strong performance in maintaininga hightrue positive ratewhile minimizingfalsepositives bertalsooutperformstheothertext classi f iersintermsofauc.furthermore wefoundthatbert along with the other text classi f iers generally performs better when predicting question and suggestion comparedtotheotherattributes.
.
.
answer to rq2.
grade s perspective .
table5presents a summary of the performance of six text classi f iers from the quality grade sperspective.we f irstfocusonpredicting excellent bert leadsalltheevaluationmetrics .11forhammingloss .37for0 loss and0.79formacro f1 .followedbyrfwhoseperformances are .
for hamming loss .
for loss and .
for macro f1.
transformer performs worse than any others whose performances are .
for hamming loss .
for loss and .
for macrof1.
due to the extremely high loss transformer is unable to correctlypredict excellent .bertcontinues to be awinnerwhen predicting good speci f ically .06forhammingloss .20for0 loss and0.63formacro f1.thesuboptimalclassi f ieristextcnn withahamminglossat0.
a0 1lossat0.
andamacro f1at .
.thewinnerchangeswhenpredicting acceptable .speci f ically transformer that performs worst in predicting excellent wins the lead over twoevaluation metricsthis time .
forhamming loss and .
for loss .
the macro f1 of transformer is .
just behindtextcnn .
andbert .
.
finally wefocusonclassi f iers performanceinpredicting poor as it may suffer from most disagreements in practice.
as shown in figure2 thenumbersof excellent and poor reviewcomments are signi f icantly rare.
that is there is a serious class imbalance problem.
most classi f iers do not work in this case.
for instance dpcnn s and transformer s loss is as high as .
and .
respectively.
bert is highly effective however.
bert s hamming lossand0 1lossareaslowas0.04and0.
respectively making it remarkably outperform other classi f iers.
the closest classi f ier isrfwhosehamminglossand0 1lossare0.14and0.
respectively.
bert also wins rf in terms of macro f1 .
.
.
to sum up bert in most cases is the most effective classi f ier from the perspective ofeitherquality attributes orgrades.
rq2 in retrospect the experiments with realworld examples show that the bert based multi label text classi f iercaneffectivelypredictthequalityattributesofreview comments signi f icantly outperforming other classi f iers on multiple metrics.
we selectitfor the automatedmodel.
.
practitioners feedback .
.
methodology.
we conducted focus group interviews with practitioners to collect a variety of feedback of evacrc.
the methodology ofthe interviewsisas follows.
step1 design preparation.
ourprotocol forthe focusgroup interviewconsists of f ive aspects.
objectives we aim to evaluate andimprove evacrc.
questions three questionsshould be answeredbyinterviewees.
q1 howusefulofevacrcforimprovingcodereviews?
ranging from 1to 5grades q2 whatare the shortcomingsof evacrc?
q3 howto addressthe shortcomingsof evacrc?
281esec fse december3 san francisco ca usa lanxinyang jinweixu yifanzhang he zhang andalberto bacchelli table performancesummary ofeachtextclassi f ierfromtheattributeperspective rf textcnn textrcnn dpcnn transformer bert p r f a p r f a p r f a p r f a p r f a p r f a emo .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
que .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
eva .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
sug .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table performancesummary ofeachtextclassi f ierfromthegradeperspective rf textcnn textrcnn dpcnn transformer bert hl01lmf hl01lmfhl01lmfhl01lmfhl01lmfhl01lmf iv0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
iii0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ii0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
i0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
macro auc cannot be calculated if onlytrue or falseclassexamples exist.
participants five interviewers and eightinterviewees participated inthein personfocusgroupinterview.theinterviewersinclude one research professor two doctoral researchers and two master s students fromthe same academic institute.
the interviewees include two executives from the management division and six representatives of the two projects studied two developers and one leader from each .
there were two steps for selecting the interviewees.
first we invited ten executives to participate in the interviews twoofwhich had theavailability to meetus.then the twoexecutivesinvitedfurthersixinterviewparticipantsbecause theyhavemorethan10yearsofexperienceindevelopmentand codereviews theystillparticipateindailycodereviews they areacknowledgedasreviewexpertsbythisenterprise andmore importantly they are in charge of two important projects whose review comments are selected for the live demonstration.
with multiple roles of participants we expect to receive relevant and insightful feedbackinthe interview.
auxiliarymaterials threetypesofdocumentswerepreparedfor the interviews.
d1 technique report.
it elaborates on background and related work evaluation designs conceptual model and automatedmodel experimentalsettings etc.
evaluation results and analysis automated model performance error analysis etc.
and operationmanuals datapreparation automatedmodeltraining anddeployment etc.
.
d2 sourcecodefiles.theyarecorecode f ilesfor datapreprocessing datasetshuffleandsplit etc.
bertandother automatedmodels construction training andtesting statistics andanalysis correlation analysis etc.
.
d3 evaluationexamples.theyaretextualreviewcommentsconsisting of two parts experimental dataset review comments two projects whose leaders should participateintheinterviews 914reviewcomments .eachreview commentisassociatedwithfourspeci f icqualityattributes andone overallqualitygrade.
consents recording and disclosing the interview should be permitted without disclosing sensitive enterprise information or personal information e.g.
names andevaluation results for exact projects.
step execution.
two weeks before the interview we delivered thetechniquereporttothemanagementdivisionandprojectleader representatives at the ict enterprise.
during the group interview we f irst introduced the background of evacrc the designs and implementations of evacrc the designs and results of experiments and the evaluation results of twointernalprojectsthatdifferfromtheexperimentalprojects.then weran evacrcfora livedemonstration.the textualreviewcomments input kernel codeof evacrc bert based textclassi f ier qualitygrade output1 andqualityattributes output2 regarding each review comment were randomly checked by interviewees.
note that we displayed only the source code of evacrc to the interviewees.
more precisely thesourcecode ofthe bert based multi labeltextclassi f ierwhichconstitutestheautomatedmodel of evacrc.
the two reasons are evaluation evacrc can be evaluated more rigorously if the source code is disclosed.
as the techniquereporthadbeendeliveredbefore the f irstkeypartofthe interview is to evaluate the detailed implementation of evacrc i.e.
source code .
replication before the formalinterview the executives indicate that evacrc might be used at this enterprise if a evacrc performs well and b the source code i.e.
third party libraries used of evacrc are license allowed.
therefore we did not provide evacrc with a user interface but disclosed the source code only to make interviewees check third party libraries and understand our model s architecture dependencies and input and outputformatsfor replication.
having f inished the introduction and live demonstration we f irst presented feedback to any comments or questions raised by theintervieweesimmediatelyandthenaskedtheintervieweesto answertheprede f inedthreequestions.next theexecutivesstarted the discussion about how to apply evacrc from the aspects of datapreparation models selection training deployment output displays etc.
basedonthe existing platforms at the enterprise.
the interview lasts around two hours and a half.
after that we conducted several rounds of follow up discussions with the executives and project representatives regarding improving and applyingevacrc at this enterprise.
step analysis.
we received a wide range of feedback from participants based on their personal expertise experience story and observations in the interview.
then we usedthe narrative analysis method to qualitatively interpret interviewees satisfaction comments concerns and suggestions for improving and applying evacrc.atthisstage weperformedinductivecoding .finally we concluded the interview by forming and checking the minutes.
.
.
results.
theintroductiontoevacrcwasverywellreceived.
for the live demonstration the interviewees were surprised by the ability to predict emotion and were interested in mispredicted examples even though the majority of outputs were consistent withtheirperception.forinstance chancesofnullpointer!!!
too 282evacrc evaluating code reviewcomments esec fse december3 san francisco ca usa manyspellingmistakes.
possition ?
?
werecorrectlypredictedto bewithnegative emotion .
unnecessarycastingto http responseservlet use method jquery.isnumeric instead of two method call werecorrectlypredictedtobe evaluation and suggestion respectively.
on theother hand indent error was incorrectly predicted tobe suggestion addtheexceptiontothelog.howtodebug f ithere is an error was incorrectly predicted to be question .
then we discussed possible causes of errors e.g.
low quality dataset due to inconsistentstandardsforannotatingreviewcomments without correction for model outputs as well as correction and prevention strategies e.g.
annotation checklists .
all interviewees recognized the value i.e.
improving code review through evaluating code review comments and effectiveness i.e.
predicting quality attributes and grades of code review comments of evacrc.
moreover evacrc is designed to mitigate developers notorious resistance to evaluations by providing explanations.
therefore they all gave the highest usefulness rating.
the main shortcomingsof evacrc are the inability todifferentiate quality grades of very long review comments because they may bethoughtfulandaregradedtobe excellent almost thelack of con f idence for the predicted quality attributes the lack of error i.e.
anydeviationfromexpectations correctionmechanisms.
the f irst shortcoming limits the usage scenario of evacrc i.e.
inline review comments are ideal.
for the second shortcoming we could make the automated output not only the predicted labels but their inference probabilities.
for the last we could develop hand craftedcorrectionrules.pleaserefertosection .
.
ouput correction improvement for details.
someintervieweesraisedquestionsaboutdataannotationand validation automatedmodelselection andtraining etc.
andmost importantly the application of evacrc.
we report on them in section6 whichconstitutes guidelines for applyingevacrc.
rq3 in retrospect the practitioners feedback received fromanictenterpriseindicatesthatsoftwarepractitioners haverecognizedthevalueandeffectivenessofevacrc.moreover theyhaveprovidedinsightfulcommentsandsuggestions for its improvement andapplicationinpractice.
discussion inthissection wedelveintolessonslearnedandsuggestionsfor bothresearchers whodevelopevacrc andpractitioners whouse evacrc.thelessonsareprimarilyderivedfromouractionsand observationsasauthors whilethesuggestionsarelargelyinformed byfeedbackfrompractitioners someofwhichhavenotbeenacted upon yet.
we have organized this information primarily for clarity of presentation.
it is important to note that the lessons learned could alsoserve as recommendations for future actions.
.
lessonslearned .
.
annotating review comments.
in our study we have been able to gather a set of lessons that assist in annotation such as identifying the presence of speci f ic keywords code elements or analyzingthewordcountinareviewcomment.additionally there are existing tools e.g.
to assist annotation.
however thesetools may not always perform as anticipated due to the inherent limitationsandexceptions to hand craftedrules .
for example while we commonly interpret a ?
question mark as a clear indication of a question it could also be part of a ternary if else operator b ?
x y .
in such scenarios the process becomes challenging as we are required to continually revise heuristicrules modifyregularexpressions andexpandkeyword dictionaries.therefore cautionand f lexibilityarerequiredwhen using automated tools and heuristics for annotation taking into account the complexityandvariability of code reviewcomments.
emotion.
review comments with positive words such as good great smart cool awesome likely indicate a positive emotion while those containing negative words such as bad poor awful terrible shit idiot fool suggest negative emotion .
additionally the presence of multiple punctuation marks like ...
?
??
!
!!
may hint at anger satire orcriticism.
question.
when raising questions review comments generally containinterrogativekeywordssuchas what who which where when if whether how many how long how often etc.
and consistofthegeneralquestionordisjunctivequestions or more simply containtheinterrogativepunctuation ?
.ontheother side review comments that ask for clari f ication or con f irmation generallycontainkeywordssuchas check examine ensure con f irm remember makesure .
evaluation.
when evaluating code changes review comments generally contain adjective keywords that indicate opinionsor judgments e.g.
redundant useless invalid incorrect insecure obsolete dangerous illegal unquali f ied enough .
contain adverbs of degree e.g.
too much hardly nearly fairly rather almost already .
suggestion.
when offeringsuggestions reviewcomments generally contain verb keywords that directly indicate what to do or not to do or how to do e.g.
add remove delete change replace format etc.
contain suggestive keywords e.g.
suggest recommend how about what about why not had better should avoid forbid use no don t let s .
table examples ofannotating codereview comments c1 c2 c3 c4 c5 c6 c7 c8 c9 c10 c11 c12 emo1 que0 eva0 sug1 iii iii ii ii iii ii iii iii iv iii iii iv pleaserefer to table 1for completereview comments.
.
.
ensuring label consistency.
we have learned the following prior annotating.
the bene f its of prior annotating are to warm up annotatorsas earlyas possibleand to timelyidentify anomalies and violations then adjust quality attributes as well asannotationchecklistsifneeded andfurtherreachaconsistent understanding among annotators.
lock checklists.
once reaching a consensus it is recommended totemporarilylockannotationchecklistsincaseoffurtherextra debates.however periodicrevisitswouldbenecessarysincethe dataset mayhave already shifted.
pair annotating.
the functions of pair annotating are similar to pair programming or pair reviewing which helps to stay focused and make timely corrections.
despite there being annotation 283esec fse december3 san francisco ca usa lanxinyang jinweixu yifanzhang he zhang andalberto bacchelli checklists we strongly recommend pair annotating for handling borderlineexamplesto ensure consistency.
human in the loop.
human in the loop leveragesmachines and human knowledge to develop intelligent models.
with humanin the loop learning we f irst feed a few manually annotated examplesintomodelsfortraining checkandcorrectthepredictivelabels andthenfeedintonewexamples.byrepeatingseveraliterations itislikely to convergeto consistent andcorrectlabels.
consistency measure.
consistency measures the rate of agreement between multiple annotation outputs.
we recommend the metrics such as kappa coefficient to quantitatively measure the consistency rate.
by adopting the above measures we expect to constructthe ground truth dataset for training models.
.
.
checking example consistency.
example inconsistency a.k.a.
dataset shift occurs when the distribution of datasets differs betweentrainingandtestset makingpredictionmodelsfail.it isdifferentfromlabelinconsistencyasweassumethatlabelsare completely correct andconsistent inthiscase.the followingare recommendedtechniques for detecting example inconsistency.
descriptivestatistics.
descriptivestatisticsprovidethebasicinformation of examples e.g.
comment words affiliations that summarize the dataset.
together with graphic analysis e.g.
histogram andpie chart they form the representations ofdatasets.
hypothesistesting.
statisticalhypothesistestingworksforassessing the dataset by using sample examples.
for instance we test whether the two groups of examples are equally distributed by leveragingkolmogorov smirnov test to detectdataset shift.
erroranalysis.
detectingexampleinconsistencybyutilizingreliable models.
once the well performed model does not work as expected itistimetolookintotheexamples distributionaswellas the labels correction.
we call it a model based detection technique to distinguish the data based detection techniques discussed above.
.
.
analyzing label correlation.
through an analysis of the correlation between attribute labels we can better understand reviewers commentinghabits enabling usto improve evacrc.figure shows the correlation heatmap for examples with a total of 000binaryclasslabels.the f igureineachblockrepresentsthe kendall rank correlation coefficient u1d70fof two variables attribute labels .notethatallstatisticalhypothesistests nullhypothesis no correlationbetweentwovariables werecon f irmedbycheckingthe p value .
andthereforewedonotpresentdetailedp values here.ingeneral if u1d70fislessthan0.
thentwovariableshavefewcorrelations if u1d70fis between .
and .
then two variables have a few correlations otherwise they have at leastmoderatecorrelations.
suggestion evaluation question .
emotion .
.
i i suggestion evaluation .
i question emotion .
.
.
.
.
.
.
figure correlation analysis ofreview comment labels figure3showsthat whenservingsuggestions reviewersgenerallydonotevaluatecodechangesorraisequestions.reviewersexpresspositiveemotionsinmostcases.insummary allattribute labels have no strong correlations which con f irms the rationale behindevacrc sharingcommoninputrepresentationsbutregardingeachpredictiontaskasindependent.duetotheseobservations we did not compare evacrc that is a algorithm adaptation style approach to adapt a single label classi f ication algorithm to the multi label task by adjusting its cost function to problem transformation style approaches to transform multi label taskstosingle labeltasks single classormulti class e.g.
classi f ier chain to regard each label as a part of a conditioned chain of single classclassi f icationtasks andlabelpowerset toregard each label combination as a separate class with one multi class classi f ication task .
further discussionon algorithm adaptation and problem transformation are beyondthe scope of this study.
.
recommendation to researchers .
.
the conceptual model.
the following are recommendations regarding twocomponentsof the conceptualmodel.
attributes.
although there might be a number of attributes for evaluating review comments e.g.
change triggering they are at riskofdisagreementsfromcodereviewers.wehaveofferedfour attributes and experience in annotating.
since they have been validated withdata triangulation they are rigorous credible and can be directly transferredto other software organizations.
we recommendtailoringattributesfromtheperspectiveofpragmaticsrather thanmorphologyandsyntax cf.section .
ifnecessary.sinceour conceptual model consists of attributes and mappings the more complex ofattributes the more complex mappings willbe.
mappings.
we have established four to four mappings four attributes and four grades .
the case mappings cf.
table can be customizedandtailoredbasedonthesoftwareorganizations needs e.g.
three quality grades.
again we donot recommendthebinary evaluation as itbears the risksof disagreements from developers.
.
.
the automated model.
the following are recommendations regarding data model andcorrection.
datapreparation.
duetoinsufficientreviewcommentsforseveral projects atthisict enterprise wehave notseparatedprojects for trainingandtesting.onceaprojectaccumulatesenoughcomments wesuggestsamplinganequalnumberfromeachfortrainingand testing e.g.
400fortrainingand100fortesting .itisalsocrucial that a project has diverse comments from multiple reviewers to ensure consistency across projects without this the automated modelmayhave reducedaccuracyandlimitedgeneralizability.
model selection training.
we examined six leading text classi f iersandfoundberttobeexceptionallyeffective.sincebert s debut numerousvariantslikexlnet andcodebert have emerged.
exploring these bert based and newer models could enhance our automated model in the future.
additionally training strategies like pretraining and adversarial training can further boost modelaccuracyandrobustness.
outputcorrection improvement.
theevaluationlargelydepends on the automated model and set mappings.
while models aid the process they are not perfect so correction strategies are essential.
for example comments with negative emotions can only 284evacrc evaluating code reviewcomments esec fse december3 san francisco ca usa achieve acceptable andwithoutsuggestions theycannot be excellent .
even though we have not used linguistic characteristics in ourmodel theycanenhancepredictions commentswithover20 words are deemed acceptable at the minimum.
beyond rule based corrections examining mislabeled examples canhighlight needed adjustments.
once recti f ied these examples should be retrained in the model until accurately predicted.
when it comes to improving the trustworthiness of outputs we recommend outputting not only eachquality attributebut also its con f idenceusing predicted probability e.g.
suggestion yes .
.
.
recommendations to practitioners fororganizations.
evaluationsshouldbeconductedinawaythat minimizes pushback.
for instance display evaluation results in f lexible formats e.g.
online for quality attributes reminder offline for quality grades evaluation use relaxed formats to convey results e.g.
emojis offer clear explanations and best practices based on set mappings e.g.
guiding reviewers on how toavoida poor gradeorachievean excellent one conduct comprehensive evaluations that go beyond just review comments e.g.
reviewparticipationandcoverage andthenumberandseverity of defects detected in review and in post review activities e.g.
testing make code changes and review comments public rather than restricting access to speci f ic projects or reviewers.
in essence evaluationsshouldaimatenhancingfuturecodeandreviewquality whilealsoincreasingdevelopers awarenessofqualityissues.ifnot executedcorrectly evaluations can be counterproductive.
for developers.
in addition to considering the recommendations rationales providedintable reviewersshouldprioritizeproducing high quality code over only striving for high quality review comments.
while experienced developers might f ind it redundant to elaborate on minor code defects such as naming conventions newcomers bene f it from detailed explanations and suggestions for recti f ication.authorsshouldensureclarityintheircodecomments changedescriptions remindersforareasofuncertainty andselfreviewpriortosubmittingforareview.additionally itisimportant for authorsto respondpromptly toany questionsorclari f ications soughtbyreviewers.onlywiththesepracticescanthecodereview processbe both effective andefficient.
threats to validity construct validity.
the possible threat to construct validity may resultfrom themeasurementof reviewcomment quality .there is no standard measurement for this problem so far.
to this end we employed data triangulation to collect synthesize and validate quality attributes aiming to develop a valid measurement of review comment quality .
these attributes have been empirically con f irmed to be able to cover almost all review comments while providing clear differences cf.
figure and have been con f irmed bythe developers at alarge ict enterprise.
internal validity.
in the second study we strive to ensure that the experimental results effect should only be affected by the classi f iers cause .weidenti f iedthreepossiblethreatstointernal validity.
the f irst concerns example collection and pre processing.
we collected examples from the projects with multiple participants and long term development periods and formatted examples byremoving project aware marks e.g.
defect and severity to reduce theirimpactsonclassi f iers.thesecondisaboutexampleannotation.
wehavetakenmanymeasurestoimproveannotationreliability cf.
section6.
.1and section .
.
.
the third relates to text classi f iers input representation.
we used deep features that are automatically learned by models rather than hand crafted features that are manuallyengineeredbyresearchers.becausewecanhardlydistinguish thecause and effectrelationshipbetweendataandclassi f ierperformancesif we mixtwotypes of features.
externalvalidity.
wehaveevaluatedevacrcatalargeictenterprise whichmaybeunderthreattoexternalvalidity.wehave made efforts to mitigate possible impacts.
when developing the conceptualmodel we employed data triangulationto collect synthesize and validate quality attributes.
they are expected to be language e.g.
english german chinese project e.g.
commercial oss andorganization independent.theexamplemappings re f lecttheinterestsoftheenterprisebutcouldbeadjusted.when performingexperiments fordevelopingtheautomatedmodel we selectedreviewcommentsfromawiderangeofprojects e.g.
domestic and overseas innersource and private .
the bert based automatedmodelcouldbeupdatedtoadapttonewdatasources.
overall theindustrialcasestudyshowsthevalueandeffectiveness of evacrc.
more importantly evacrc provides a paradigm for evaluatingreviewcommentsin whichboth theconceptualmodel andautomated modelsaretailorable.sucha paradigm isexpected to be generalized to other contexts with little effort when referring to our experience andrecommendations.
conclusions inourpursuitofcreatinganexplainable automatedsystemforevaluating review comments weactively gatheredempiricalevidence throughdatatriangulation.thisledustoformulateaconceptual model.
furthermore we utilized multi label learning to develop text classi f iers culminating in an automated model powered by bert.acasestudyataglobalictenterprisevalidatedtheefficacy of bothour conceptualand automatedmodels.
our work ssigni f icanceextendsbeyondintroducingtheconceptualandautomated evacrc models for code review evaluations.
crucially we also offersoftwareorganizationsapragmatic customizableapproachand share tangible experiences related to ensuring code review quality throughthe evaluation of reviewcomments.
data availability thereplicationpackage datafortriangulation experiments and interviews isavailable in .