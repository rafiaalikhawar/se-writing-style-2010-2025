efficient text to code retrieval with cascaded fast and slow transformer models akhilesh deepak gotmare salesforce ai research singapore akhilesh.gotmare salesforce.comjunnan li salesforce ai research singapore junnan.li salesforce.com shafiq joty salesforce ai research united states sjoty salesforce.comsteven c.h.
hoi salesforce ai research singapore shoi salesforce.com abstract the goal of semantic code search or text to code search is to retrieve a semantically relevant code snippet from an existing code database using a natural language query.
when constructing a practical semantic code search system existing approaches fail to provide an optimal balance between retrieval speed and the relevance of the retrieved results.
we propose an efficient and effective text to code search framework with cascaded fast and slow models in which a fast transformer encoder model is learned to optimize a scalable index for fast retrieval followed by learning a slow classificationbased re ranking model to improve the accuracy of the top k results from the fast retrieval.
to further reduce the high memory cost of deploying two separate models in practice we propose to jointly train the fast and slow model based on a single transformer encoder with shared parameters.
empirically our cascaded method is not only efficient and scalable but also achieves state of the art results with an average mean reciprocal ranking mrr score of .
across programming languages on the codesearchnet benchmark as opposed to the prior state of the art result of .
mrr.
our codebase can be found at this link.
ccs concepts computing methodologies information extraction information systems learning to rank language models .
keywords developer productivity code retrieval text to code search transformer models cascaded retrieval schemes top k retrieval acm reference format akhilesh deepak gotmare junnan li shafiq joty and steven c.h.
hoi.
.
efficient text to code retrieval with cascaded fast and slow transformer corresponding author akhilesh.gotmare salesforce.com.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse december san francisco ca usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
in proceedings of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering esec fse december san francisco ca usa.
acm new york ny usa pages.
introduction building automatic tools that can enhance software developer productivity has recently garnered a lot of attention in the deep learning and software engineering research communities.
code retrieval systems can make developers more productive by enabling them to search and reuse from the enormous volume of open source repositories available online thus speeding up the software development lifecycle.
code search systems can be particularly of great value for organizations with internal proprietary code.
indexing source code data internally for search can prevent redundancy and boost programmer productivity.
a study by xu et al .
surveys developers to understand the effectiveness of code generation and code retrieval systems.
their results indicate that the two systems serve complementary roles and developers prefer retrieval modules over generation when working with complex functionalities thus advocating the need for better code search systems.
beyond their direct utility in improving developer productivity code search solutions have also been leveraged to improve the performance of code generation systems when used as sub components thus adding to the significance of research on improving text to code retrieval.
our primary focus in this work is to improve the performance of text to code search solutions as evaluated by these two aspects the speed of retrieval and the relevance of the retrieved results to the input query.
we propose to bring this improvement by cascading two approaches with complementary strengths fast retrieval systems that sacrifice relevance but offer high retrieval speed and slow retrieval systems that sacrifice speed but return results with higher relevance.
we find inspiration for this multi stage approach from recent progress in the text to image retrieval domain .
researchers have shown impressive results on the traditional document retrieval problem text to text search using transformer based models .
this progress has in turn guided a lot of research in the text to code retrieval domain .
parallel to the progress in natural language processing nlp language models lms pre trained on code like codebert codegpt instructgpt codex plbart and codet5 esec fse december san francisco ca usa akhilesh deepak gotmare junnan li shafiq joty and steven c.h.
hoi have now been proposed for understanding and generation tasks involving programming languages.
however there has been very limited research on studying the similarities between the two problem settings of text to image and text to code retrieval despite their common theme of aligning data from two different modalities.
we believe further improvements in text to code search can be achieved using the two stage paradigm that has been shown to be effective for text to image search.
one could question the pertinence of text to code search given the current state of research on code generation using transformer based large language models llms .
chen et al .
s 12b parameter codex li et al .
s 41b parameter alphacode nijkamp et al .
s 16b parameter codegen and austin et al .
s 137b parameter lm use large scale autoregressive language models to demonstrate impressive capabilities of generating multiple lines of code from natural language descriptions well beyond what previous generation models like gpt c could accomplish.
would developers need text to code search when llms trained on code can generate correct looking programs for a natural language prompt?
we argue that text to code retrieval would still be a valuable offering for developers for the following reasons the impressive performance of code generation systems is often predicated on being able to draw many samples from the model and machine check them for correctness.
this setup will often not be the case in practice .
code generation models also entail security implications possibility of producing vulnerable or misaligned code making their adoption tricky.
besides some recent studies have found limitations with popular benchmarks like humaneval that have been relied on to measure the correctness of model generated programs suggesting that the synthesized program correctness scores of code llms have been overestimated.
given this current landscape code retrieval systems can serve as attractive alternatives when building tools to assist developers.
with efficient implementations code search for a single query can typically be much faster for most practical index sizes than generating code with large scale lms.
as opposed to code generation code retrieval offers interpretability and the possibility of a much greater control over the quality of the result as the index entries can be verified beforehand.
another benefit with code search systems is the ability to leverage additional data post training as this simply requires extending the index by encoding new instances.
moreover a code generation system can be augmented with a code retrieval system to improve the generation ability .
for semantic code search deep learning based approaches involve encoding query and code independently into dense vector representations in the same semantic space.
retrieval is then performed using the representational similarity based on cosine or euclidean distances of these dense vectors.
this framework is often referred with different terms like representation embedding based retrieval dense retrieval two tower or fast dual encoder approach in different contexts.
an orthogonal approach involves encoding the query and the code jointly and training semantic code search systems as binary classifiers that predict whether a code answers a given query referred to as monobert style or as slow classifier .
with this approach the model processes the query paired with each candidate code sequence meaning the text and code snippets are concatenated at the nl querycandidate code snippets implement bubble sort ...def insertion sort items for i in range len items ....def mergesort arr l r if l r m l r l sort first ... transformer encoder1100...k nearest neighbor lookuppl index constructed offline 22nl querytop k code snippets implement bubble sortdef mergesort arr l r if l r m l r l sort first ... ...... .
.
.053transformer classfiersearch result transformer encoderfigure cascode s inference stage the query xiand the code snippets are first encoded independently by the transformer encoders.
the top k candidates based on nearest neighbor lookup are then passed to the classifier which jointly processes the query with each of the filtered candidates to predict the probability of their semantics matching.
input stage of the neural network.
intuitively this approach helps to sharpen the cross information between query and code and is a better alternative for capturing matching relationships between the two modalities natural language nl and programming language pl than the simple similarity metric between the fastencoder based sequence representations.
while this latter approach can be 389efficient text to code retrieval with cascaded fast and slow transformer models esec fse december san francisco ca usa nl querycandidate code snippets implement bubble sort ...def insertion sort items for i in range len items ....def mergesort arr l r if l r m l r l sort first ... transformer encoder... .
.
.
.
.
.
.
.02transformer classfiersearch result nl querycandidate code snippets implement bubble sort ...def insertion sort items for i in range len items ....def mergesort arr l r if l r m l r l sort first ... transformer encoder...k nearest neighbor lookuppl index constructed offline transformer encodersearch result figure illustration of the the inference stage of fast encoder left and slow classifier right based semantic code search approaches.
with the encoder based approach we independently compute representations of the natural language nl query and candidate code sequences.
the code snippet with representation nearest to the query vector is then returned as the search result.
with the classifier based approach we jointly process the query with each code sequence to predict the probability of the code matching the query description.
the code sequence corresponding to the highest classifier confidence score is then returned as the search result.
promising for code retrieval previous works have mostly leveraged it for tasks like text to code generation or binary classification in the form of text code matching .
directly adapting this approach to code search tasks would be impractical due to the large number of candidates to be considered for each query.
inference with this setup would require each candidate to be combined with the query and passed through the classifier.
we depict the complementary nature of these approaches in figure when using a transformer encoder based model for retrieval and classification.
in order to leverage the potential of such nuanced classifier models for the task of retrieval we propose a cascaded scheme cascode where we process only a limited number of candidates with the classifier model.
this limiting can be achieved by employing the representation based fastencoder approach and picking its top few candidate choices for processing by the second classifier stage.
our cascaded approach leads to state of the art performance on the codesearchnet benchmark with an overall mean reciprocal ranking mrr score of .
significantly improving over previous results best reported mrr score of .744from guo et al .
.
we propose a variant of the cascaded scheme with shared parameters where a single transformer model can serve in both the modes encoding in the representation based retrieval stage and classification in the second stage.
this shared variant can be achieved by multi task training using the sum of the two objectives corresponding to these two distinct task settings.
cascode s shared variant substantially reduces the memory requirements while offering retrieval performance that is comparable to the separate variant withan mrr score of .
.
we also show improvements with our cascode approach for the advtest python dataset popularised by the codexglue benchmark to assess the generalization abilities of code retrieval models when the function and variable names of a program are normalised and thus unrelated to its semantics.
figure illustrates the trade off involved between inference speed and retrieval performance mrr for different algorithmic choices where we have the fast encoder model on one extreme and the slow classifier model on the other.
with cascode we offer performance comparable to the optimal scores attained by the classifier model while requiring substantially lesser inference time thus making it computationally feasible.
our key contributions in this paper are the following.
we first show that the performance of existing dense retrieval models codebert and graphcodebert trained with contrastive learning can be significantly improved when trained with larger batch size these serve as stronger baselines for code retrieval.
to further push retrieval performance we propose the cascaded code search scheme cascode that performs code retrieval in two stages and we analyze the trade off of the inference speed and retrieval performance.
we show that the transformer models in the two stages of cascode can be shared by training in a multi task manner which significantly reduces the memory requirements.
with cascode we report state of the art text to code retrieval performance on public benchmarks of codesearchnet and the normalised advtest python dataset from codexglue 390esec fse december san francisco ca usa akhilesh deepak gotmare junnan li shafiq joty and steven c.h.
hoi figure overview of the speed versus performance mrr metric higher is better trade off of current code search approaches.
with cascode we are able to achieve performance comparable to the optimal classifier based approach top right while requiring substantially lesser inference time.
areas of the circles here are proportional to model sizes.
for reference fast encoders require 125m parameters.
related work our work is heavily inspired by recent progress in neural search and ranking for natural language where pre trained transformer language models have been extensively used.
karpukhin et al .
finetune bert based encoders to build the passage retrieval component of their open domain question answering qa system where the goal is to develop systems capable of answering questions without any topic restriction.
efficient passage retrieval to select candidate contexts is a critical step in such pipelines.
xiong et al .
show improvements in transformer based dense retrieval of text by using globally retrieved hard negatives when finetuning the encoders resulting in effective performance on web search and qa.
chang et al .
propose novel pre training objectives to train transformer models that specialize at embedding based large scale text retrieval.
lin et al .
provide an exhaustive survey on the use of pretrained language models for text ranking and study the trade offs involved in the different alternatives.
in the single stage fashion a common approach is representation based ranking where bertbased models bi encoders or fastencoders are trained to independently encode the query and documents and inference involves dot product based similarity search for retrieval .
another single stage approach is monobert slow classifier where query document pairs are passed jointly to a bert encoder and the model predicts whether the input document is relevant to thequery or not.
the monobert approach is computationally more expensive but also tends to be more accurate than the bi encoder approach.
however with the bi encoder approach we can index all the document representations offline.
thus at inference time we simply need to encode the query making it a very attractive retrieval setup.
achieving this inference speedup by caching representations is not possible in the monobert setting as it jointly processes the query and document strings.
as an alternative to these two frameworks khattab and zaharia propose colbert which performs late interaction between a query and document after their independent encoding.
this leads to performance that is comparable to the monobert approach but is less computationally expensive during inference.
however colbert requires storing per token representations of all the document candidates as inputs to the late interaction and this can demand impractically high storage.
the limitations of monobert when handling a large number of candidate documents inspire the need for multi phase retrieval where the first phase can retrieve candidate documents with the cost effective bi encoder approach dot product retrieval followed by the second stage where only the top candidates from the first stage are processed by a more expensive monobert model.
for code retrieval we experimentally show that these two models can share a majority of their parameters.
thus a single encoder backbone can serve in the two stages first as the bi encoder for fast retrieval and then as the more powerful monobert.
early work on neural approaches to code search leveraged unsupervised word embeddings to represent code snippets as textual documents.
subsequently supervised approaches using lstm architectures showed improvements by also leveraging data augmentation strategies to transform code snippets while preserving their semantics .
later with the advent of the transformer architecture in natural language processing several works employed transformer models for code retrieval tasks and reported significant gains in performance over previous approaches.
across a majority of these recent works codesearchnet by husain et al .
has emerged as a standard benchmark for calibrating code search performance.
researchers have attempted to modify the pre training of transformer models for the code domain by embedding the structural information associated with programs in different forms.
this has led to a string of code pre trained models like codebert which introduced novel pre training tasks for bimodal datasets containing text and code graphcodebert which is pre trained on code using tasks that embed structural information from the abstract syntax trees asts of code inputs and syncobert a syntax aware encoder architecture.
in a related line of work lu et al .
propose a benchmark nl code search webquery where natural language code search is framed as the problem of analysing a query code pair to predict whether the code answers the query or not.
more recently guo et al .
released unixcoder.
unike codebert s encoder only pre training that uses the masked language modeling mlm and replaced token detection rtd objectives only unixcoder is pre trained with a set of tasks like mlm unidirectional language modeling span denoising cross modal contrastive learning and cross modal generation and has shown to be a competitive alternative for several code understanding and generation tasks.
391efficient text to code retrieval with cascaded fast and slow transformer models esec fse december san francisco ca usa in contrast to the research theme of finding optimal pre training strategies for code we focus on the adaptation or fine tuning stages of pre trained models.
similar to the pre training stage this finetuning stage also offers different training choices which have been underexplored so far.
one could adapt a pre trained model in the fast encoder style or through the slow classifier style for retrieval.
cascode proposes to combine these two approaches to achieve optimal retrieval performance speed and relevance with any given pre trained code understanding model.
cascading transformer models for text to code retrieval cascode in this section we describe our proposed cascode approach including details of training and inference phases of the fast encoder stage .
the slow classifier stage .
our cascading scheme .
and the shared variant of cascode .
.
.
stage i fast encoders for the first stage of fast bi encoders we use the contrastive learning framework similar to the fine tuning by guo et al .
who leverage pairs of natural language and source code sequences to train text to code retrieval models.
the representations of natural language nl and programming language pl sequences that match in semantics a positive pair from the bimodal dataset are pulled together while representations of negative pairs randomly paired nl and pl sequences are pushed apart.
the infonce loss a form of contrastive loss function used for this approach can be defined as follows linfonce nn i logexp f xi tf yi j bexp f xi tf yj wheref xi is the dense representation for the nl input xi and yiis the corresponding semantically equivalent pl sequence.
n is the number of training examples in the bimodal dataset is a temperature hyper parameter to control the sharpness of the model s output probability distribution and bdenotes the current training minibatch.
while the above approach applies for any model architecture guo et al .
employ graphcodebert and codebert for f in their experiments.
we refer to this approach as fast as it benefits from caching of candidate encodings before query time.
during inference we are given a set of candidate code snippets c y1 y2 ...y c which are encoded offline into an index f yj j c .
for a test nl query xi we then compute f xi and return the code snippet fromccorresponding to the nearest neighbor as per the cosine similarity distance metric in the index.
during inference we are only required to perform the forward pass associated with f xi and the nearest neighbor lookup in the pl index as the pl index itself can be constructed offline.
this makes the approach very suitable for practical scenarios where the number of candidate code snippets c could be very large.
interestingly a single encoder either codebert and graphcodebert can be used to process the two modalities of text f xi and code f yi .
this could be attributed to the nl pl pre training of these models.
given this observation with the two code pretrained models in all our experiments we process the nl and pl inputs in the same manner agnostic to their modality.
.
stage ii slow classifiers although the above retrieval approach is efficient for practical scenarios the independent encodings of the query and the code make it less effective as these do not allow for self attention style interactions between nl and pl tokens.
similar to the monobert approach we could instead encode the query and the code candidate jointly within a single transformer encoder and perform binary classification for ranking.
in particular the model could take as input the concatenation of nl and pl sequences and predict whether the two match in semantics.
the training batches for this binary classification setup can again be constructed using the bimodal dataset positive pairs denoting semantic matches and the negative pairs mismatch can be constructed artificially.
given a set of npaired nl pl semantically equivalent sequences xi yi n i the cross entropy objective function for this training scheme would be lce nn i j ilogp xi yi log p xi yj wherep xi yj represents the probability that the nl sequence xisemantically matches the pl sequence yj as predicted by the classifier.
with a minibatch bof positive pairs xi yi i b we can randomly pick yj j b j i from the pl sequences in the minibatch and pair it with xito serve as a negative pair.
when using a transformer encoder based classifier the interactions between the nl and pl tokens in the self attention layers can help in improving the precision of this approach over the previous independent encoding one.
during inference we can pair the nl sequence xiwith each of theyjfromcand rank the candidates as per the classifier s confidence scores of the pair being a match.
this involves cforward passes each on a joint nl pl sequence thus longer inputs than the previous approach making this approach computationally infeasible when dealing with large retrieval sets.
we refer to this approach as the one using slow classifier for retrieval.
.
cascading of the two stages with a cascaded scheme that we call cascode we can unify the strengths of the two approaches the speed of the fast encoders with the precision of the slow classifiers .
to build cascode we first independently train the two stages discussed above the fast encoder stage using the infonce objective and the slow classifier stage using the cross entropy loss.
while these two approaches are alternatives to each other we employ them in a sequential manner to perform retrieval.
figure shows the overall framework of our approach.
this hybrid strategy combines the strengths of the two approaches in the following manner given a query at test time inference stage the first stage of fast encoders provides a similarity score based on the cosine distance between the query and candidate encodings for each candidate from the set cof code snippets.
in practice the size of the retrieval set c can often be very large and varies from to52660 for the codesearchnet datasets we study in our experiments.
the top kcandidates based on the similarity scores from the first stage are then passed to the second stage of slow classifiers where each of them is paired with the nl input query xiand fed to the model.
for a given pair this 392esec fse december san francisco ca usa akhilesh deepak gotmare junnan li shafiq joty and steven c.h.
hoi second stage classifier will return the probability of the nl and pl components of the input matching in semantics.
using these as confidence scores the rankings of the kcandidates are refined.
the resulting scheme is preferable for k c as this would add a minor computational overhead on top of what is required by thefast encoder based retrieval.
the second stage of refinement can then improve retrieval performance provided that the value of k is set such that the recall of the fast encoder is reasonably high.
k would be a critical hyper parameter in this scheme as setting a very lowkwould lead to high likelihood of missing the correct snippet in the set of inputs passed to the second stage slow classifier while a very highkwould make the scheme infeasible for retrieval.
as we discuss later in section cascode with a kas small as 10already offers significant gains in retrieval performance over the baselines with marginal gains as we increment kto100and beyond.
.
making cascode memory efficient in order to minimize the memory overhead incurred by the two stage model we propose to share the weights of the transformer layers of the fast encoders and the slow classifiers by training a model with the joint sum objective lshared linfonce l ce.
thus a single transformer model is trained to perform both encoding based retrieval and classification based retrieval in this multi task learning setting.
while the number of parameters in this shared variant would be nearly half of the separate non shared case the computational cost at inference would be the same.
note that we would need some exclusive parameters for the classifier model specifically the classification head a linear layer on top of the encoder hidden states output.
thus in this shared parameter variant of cascode the transformer model consuming the three kinds of inputs nl only and pl only for the fast encoder stage and nl pl for the slow classifier stage is identical except for the classification head in the second stage.
experiments .
setup and fast retrieval baseline we use the codesearchnet corpus from husain et al .
that includes six programming languages ruby javascript go python java and php.
our pre processing and train val test splits are identical to the setting from guo et al .
who filter low quality queries and expand the retrieval set to make the code search task more challenging and realistic.
figure shows examples of bimodal pairs from the resulting dataset and the statistics of the dataset after preprocessing are provided in table .
our primary evaluation metric is mean reciprocal ranking mrr computed as1 ntest ntest i ri where theriis the rank assigned to the correct code snippet for thei th queryxi from the set of retrieval candidates c. we report mrr on the scale of some works eg.
use the scale.
our fast encoder baseline is based on the codebert model from feng et al .
that is pre trained on programming languages.
in order to have a strong baseline we use a newer codebert checkpoint that we pre train using masked language modeling and replaced token detection tasks for longer after we found that the codebert checkpoint from feng et al .
was not trained till convergence.
when starting from our new checkpoint we find that the codebert baseline if fine tuned with a larger batch size largest possible that we can fit on a100 gpus and for a larger number of epochs is able to perform substantially better than the results reported before.
we report the baselines from guo et al .
in table along with the results for our replication of two of these baselines.
previous studies have emphasized this effect larger batch sizes are known to typically work well when training with the infonce loss in a contrastive learning framework due to more negative samples from the batch .
we also finetune graphcodebert as a structure aware model pre trained on programming languages.
graphcodebert leverages data flow graphs during pre training to incorporate structural information into its representations.
however for the code search task we report that graphcodebert does not offer any significant improvements in performance over codebert when both variants are trained with a large batch size.
as codebert performs competitively and has a relatively simpler architecture equivalent to roberta base model with layers dimensional hidden states and attention heads we chose it as the fast encoder baseline for the remainder of our experiments.
for finetuning on code search we begin with the baseline implementation of graphcodebert codebert tree master graphcodebert and adapt their setup to also implement the codebert model.
for the cascaded schemes many of our training design decisions are therefore the same as graphcodebert.
we use a100 gpus each with 40gb ram to train our baselines and cascode variants.
during training we set the batch size to a value that occupies as much available gpu ram as possible which is for the codebert and graphcodebert baseline finetuning with the infonce loss.
mrr scores on the test set for the codebert baseline fastencoder along with several other baselines including sparse methods like bm25 implemented using pyserini fine tuned cnn birnn multi head attention models are shown in table .
interestingly bm25 outperforms all other methods on the python dataset this could be attributed to the simplicity of python and its similarity with natural language .
for the codebert baseline and the cascode variants that we have proposed along with mrr we also report recall k for k that indicates the hit rate ratio of instances where we find the correct output in the top k results .
we encourage future work on code search to report these additional metrics as these are important in evaluating the utility of a retrieval system and are commonly reported in similar work in text retrieval and text based image or video retrieval .
as alluded to in section for designing the cascaded scheme we need to pick akthat is large enough to provide reasonably high recall and small enough for the second stage to be reasonably fast.
to guide our choice of k we show in figure the recall k k varied over the horizontal axis for the different programming languages with the fast encoder models over the validation set.
for our experiments we pick k 10and100where the recall for all datasets is over and90 respectively.
note that cascode is a general framework and several different models can be employed in the two stages.
we pick fine tuned codebert for the fast encoder phase of cascode as it is a simpler architecture than graphcodebert or unixcoder and gives strong performance on its own when evaluated in the first stage only.
393efficient text to code retrieval with cascaded fast and slow transformer models esec fse december san francisco ca usa table data statistics of the filtered codesearchnet corpus for go java javascript php python and ruby programming languages.
for each query in the dev and test sets the answer is retrieved from the set of candidate codes last row .
ruby javascript go python java php training examples dev queries testing queries candidate codes table examples of bimodal pairs natural language docstring with corresponding code sequence from codesearchnet python .
docstring prompt the user to continue or not code snippet def continue prompt message answer f a l s e message message n y e s o r no t o c o n t i n u e while answer not in yes no answer prompt message e v e n t l o o p e v e n t l o o p i fanswer yes break i fanswer no break return answer docstring sends a message to the framework scheduler.
code snippet def message s e l f d a t a l o g g i n g .
i n f o d r i v e r s e n d s framework mes sag e .format d a t a return s e l f .
d r i v e r .
sendframeworkmessage d a t a .
results with cascode to build the model for the second phase of cascode separate on top of the codebert based fast encoders we train the slow classifiers independently but evaluate them by cascading with the first phase.
for this second stage model we finetune the codebert pre trained checkpoint detailed above with a classification head on top a linear layer on top of the hidden states output using the codesearchnet dataset.
on the validation set we study the performance of this finetuned classifier for retrieval and report the mrr scores in figure for different values of k wherekis the number of top candidates passed from the first fast encoder stage to the second.
interestingly the retrieval performance of this joint classifier does not improve significantly beyond certain values of k. for example increasing kfrom 10to100only marginally improves the mrr for ruby javascript and java while for other languages there is no significant improvement beyond k .
in cascode s separate variant we pair the fast encoder with this second stage classifier model and the mrr scores for this approach and the relevant baslines are provided in table .
with our cascaded approach we observe significant improvements over the fast encoder baselines the overall mrr averaged over the six programming languges for cascode separate is .
whereas the fast encoder baseline codebert reaches .
.
the improvements with cascode are noticeably greater over the baseline for ruby javascript python figure recall at different values of k over the validation set of codesearchnet when using a finetuned codebert encoder fast for text code retrieval.
figure mean reciprocal ranking mrr at different values of k over the validation set of codesearchnet when using a finetuned codebert slow binary classifier match or not for text code retrieval.
note that with an increase in the number of top candidates passed to the second stage the inference time would also increase however we do not observe substantial gains in mrr beyond top k of .
and java.
we report modest improvements on the go dataset where thefast encoder baseline is already quite strong .
mrr .
we also train fastand slow models with shared parameters denoted by cascode shared .
the training objective for this model is the sum of the binary cross entropy loss lceand the infonce losslinfonce as described in section .
the shared variant of cascode attains an overall mrr score of .
which is comparable 394esec fse december san francisco ca usa akhilesh deepak gotmare junnan li shafiq joty and steven c.h.
hoi table mean reciprocal ranking mrr scores of different methods on the codesearch task on programming languages from the codesearchnet corpus test set .
the first row indicates performance with the bm25 scoring using bag of words sparse representations.
the next set consists of four finetuning based baseline methods nbow bag of words cnn convolutional neural network birnn bidirectional recurrent neural network and multi head attention followed by the second set of models that are pre trained then finetuned for code search roberta pre trained on text by liu et al .
roberta code roberta pre trained only on code codebert pre trained on code text pairs by feng et al .
graphcodebert pre trained using structure aware tasks by guo et al .
codet5 base encoder decoder transformer model by wang et al .
pretrained for code understanding and generation tasks unixcoder unified cross modal pre trained model for code by guo et al.
.
syncobert pre trained using syntax aware tasks by wang et al .
.
cascode separate our cascaded retrieval scheme using two independent transformer encoder models first in the fast dual encoder stage and later in the slow classifier monobert style stage.
cascode ours shared a single encoder model is used in both the stages of cascode using model parameter sharing.
in the last four rows we report the results with the shared and separate variants of our cascode scheme.
model method ruby javascript go python java php overall bm25 .
.
.
.
.
.
.
as reported by guo et al.
nbow .
.
.
.
.
.
.
cnn .
.
.
.
.
.
.
birnn .
.
.
.
.
.
.
selfatt .
.
.
.
.
.
.
roberta .
.
.
.
.
.
.
roberta code .
.
.
.
.
.
.
codebert .
.
.
.
.
.
.
graphcodebert .
.
.
.
.
.
.
codet5 base .
unixcoder .
as reported by wang et al.
syncobert .
.
.
.
.
.
.
replicated with a larger training batch size codebert .
.
.
.
.
.
.
graphcodebert .
.
.
.
.
.
.
ours k cascode shared .
.
.
.
.
.
.
cascode separate .
.
.
.
.
.
.
ours k cascode shared .
.
.
.
.
.
.
cascode separate .
.
.
.
.
.
.
to the separate variant.
this slight difference can be attributed to the limited model capacity in the shared case as the same set of transformer layers serve in the encoder and classifier models.
we also evaluate the mrr scores for the cascode shared model when used in the fast encoder stage only and the test set mrr scores were .
.
.
.
.
.
for ruby javascript go python java and php respectively with the overall mrr being .
.
thus the cascaded model that was trained in a multi task manner with a joint objective gives competitive retrieval performance even when used only in its first stage.
the improvements in the mrr scores of both cascode variants shared and separate over the codebert fast encoder baseline are statistically significant for all 6programming languages with p .
as per the one tailed student s t test recommended forretrieval by urbano et al .
for bothk 10andk .
we also report the recall k metric for cascode separate and shared variants in figure .
for all six programming languages we observe improvements over the fast encoder baseline with our cascaded scheme.
similar to our observation from table the shared variant of cascode is slightly worse than the separate one.
cascode training details for training the joint nl pl classifier of cascode separate we are able to use a batch size of .
this batch size is lower than the fast encoder finetuning batchsize because we are required to process joint nl pl sequences f which will be much longer in length than a nl only or pl only sequence.
for cascode s shared variant we need to further reduce the training batch size to as we are required to store activations from multiple forward passes for a given bimodal 395efficient text to code retrieval with cascaded fast and slow transformer models esec fse december san francisco ca usa pair nl only f xi pl onlyf yi and joint nl pl f .
all models are trained for epochs.
for all our experiments we use a learning rate of 2e and use the adam optimizer to update model parameters.
for both the cascode variants when performing evaluation on the development set for early stopping we usek 100candidates from the fast encoder stage.
when running inference on a single a100 gpu on a single query batchsize of the codebert style fast encoder occupies mb of gpu ram.
this is also true for the slow binary classifiers monobert style and the shared cascode variants.
the inference stage memory requirement gets roughly doubled with cascode s separate variant where there is no sharing of weights between the fast encoder and slow classifier stages.
this is expected because the separate variant stores two different encoder models identical architecture except the classification head but different weights on the gpu ram.
.
retrieval speed comparison having established the improvements in retrieval performance with cascode we proceed to analyze the trade off between inference speed and performance for the different methods discussed.
for each variant we record the time duration averaged over 100instances required to process obtain a relevant code snippet from the retrieval set an nl query from the held out set.
we use the ruby dataset of codesearchnet for this analysis which contains candidate code snippets for each nl query.
we conduct this study on a single nvidia a100 gpu.
table shows the results.
for the fast encoder approach using infonce finetuned codebert we first incur some computational cost to encode all the candidate code snippets and construct the pl index .76seconds for ruby s retrieval set .
this computation is common to all approaches except the slow binary joint classifier one.
since this computation can be performed offline before the model is deployed to serve user queries we do not include this cost in our results in table .
with the pl index constructed beforehand we report the time required to encode a user nl query and perform nearest neighbor lookup on the pl index with the encoding in the first row of table .
this computation is again performed by all the cascode variants and thus acts as the lower bound on time taken by cascode for retrieval.
for the analysis to be as close to real world scenarios as possible we do not batch the queries which can provide further speed ups specially on gpus and encode them one by one.
batching them would require assuming that we have the nl queries beforehand while in practice we would be receiving them on the fly from users when deployed.
with the slow classifier approach we would pair a given query with each of the candidates and thus this would lead to the slowest inference of all the variants.
for all variants of cascode the inference duration listed in table includes the time taken by thefast encoder based retrieval first stage along with the second stage.
for cascode s second stage we can pass the kcombinations query concatenated with each of the top kcandidate from the fast stage in a batched manner.
the shared variant while requiring half the parameters incurs the same computational cost when used in the cascaded fashion.
we note from table that at a minor drop in the mrr score lowering cascode s kfrom 100to10can lead to almost 3x faster inference.
.
advtest normalized set evaluation previous works have employed a normalised variant of the codesearchnet python dataset called advtest to evaluate text tocode retrieval models.
the function and variable names appearing in the code snippets in the test and development sets of this python dataset are normalized func for function names arg i for the ith variable name .
an example of this normalization is shown in table .
this dataset was processed and released by to test the understanding abilities of code search systems as part of the codexglue benchmark.
we follow their lead in evaluating our proposed cascode on the advtest benchmark to study its retrieval effectiveness in this challenging setting.
when experimenting with the advtest dataset our focus is to is to compare the code understanding and retrieval abilities of different approaches in this more rigorous evaluation setting than the regular codesearchnet dataset as the normalization scheme should prevent the models from over relying on the natural language semantics english components of the candidate programs.
for instance the snippet def bubble sort python program here in the regular setting would be easier to retrieve than the candidate def func python program here in the normalized setting for the query implement bubble sort .
in the normalized setting the model would have to rely on understanding the program semantics instead of variable or function names which tend to be closer to natural language or plain english.
note that the advtest dataset is not adversarially constructed and it does not involve any gradient based methods adversarial attacks to perturb inputs beyond a simple normalisation function.
we speculate that our models would inherit the same vulnerabilities to adversarial input perturbations as their nlp counterparts.
evaluating the robustness of code search models to such sophisticated adversarial attacks is beyond the current scope of our work.
the advtest dataset contains training examples validation set examples and test set examples.
each example is a bimodal pair of natural language docstrings and corresponding code snippets.
during test time all the code snippets are treated as candidates for a given test query.
the code retrieval results achieved by different approaches on this dataset are shown in table .
results in the first two rows are reported from where roberta and codebert are fine tuned batch size of with the infonce loss discussed before in the fast encoder framework.
in our re implementation of the stronger baseline of codebert we increase the training batch size to .
this leads to an improved test mrr score of .
.
for cascode s separate variant we finetune the slow classifier stage with the binary cross entropy loss.
this second stage model is initialized from the codebert pre trained checkpoint and trained on the pairs.
for each positive pair we can create a synthetic negative one by pairing a docstring with a random code snippet.
for cascode s shared variant we train the two stages jointly by tying the weights of the two encoder similar to previous experiments from section .
.
the finetuning loss is the sum of the infonce loss and binary ce loss computed using the same minibatch.
from table we see that when using cascode with k candidates we observe a substantial improvement the shared variant scores mrr of .
and the separate one .
.
retrieval 396esec fse december san francisco ca usa akhilesh deepak gotmare junnan li shafiq joty and steven c.h.
hoi figure recall k with the fast encoder and cascode shared and separate methods on the test set queries of codesearchnet dataset.
table inference speed comparison for different variants of the proposed methods.
the number of parameters corresponding to the classifier head are separated with a sign in the second column.
inference duration is averaged for 100queries from the ruby subset of codesearchnet using a single a100 gpu.
constructing the pl index offline requires .76seconds for the ruby dataset and is not included in the durations listed here.
mrr scores are reported on the entire test set.
throughput of the retrieval model measured in queries processed per second is listed in the last column.
model params inference time secs mrr queries s fast encoders codebert style 125m .
.
.
slow binary classifiers monobert style 125m .5m .
.
.
cascode separate k 250m .5m .
.
.
cascode shared k 125m .5m .
.
.
cascode separate k 250m .5m .
.
.
cascode shared k 125m .5m .
.
.
performance can be further improved to mrr score of .
with the shared variant and .
with the separate if we increase the number of candidates kto100.
in figure we show an example from this test set where for a given query the first stage of fast encoder equivalent to the re implemented codebert baseline assigns a rank riof to the matching code snippet and then the slow classifier refines the ranking to .
in cases when cascode fails at retrieving the correct code snippet as the top search result our qualitative analysis suggests that the resulting code snippet is often closely related in semantics and functionality.
the gap in performance for deep learning models between the original unaltered codesearchnet test set and the advtest one is nonetheless still an open problem that suggests our current models over rely on the function and variable naming as done by human programmers and less on the inherent structure of the code in representing source code.
table lists that unixcoder and codet5 base when used in a single stage fast encoders perform competitively on the advtest benchmark.
unixcoder s performance with an mrrof0.413is significantly better than codebert s mrr of .
but worse than cascode s mrr of .
.
we expect additional improvements to cascode s performance on advtest by fine tuning a model like unixcoder in its two stages.
subsequent to our work several submissions to the codexglue advtest leaderboard seem to have made this improvement.
to the best of our knowledge the details of these approaches have not yet been released preventing any further analysis or comparison.
conclusion and future work we propose cascode a cascaded text to code retrieval scheme consisting of transformer encoder and joint binary classifier stages which achieves state of the art performance on the codesearchnet benchmark with significant improvements over previous results.
we also propose a shared parameter variant of cascode where a single transformer encoder can operate in the two different stages 397efficient text to code retrieval with cascaded fast and slow transformer models esec fse december san francisco ca usa code snippet def d a y s t a r t u t s e l f ut s e t t i m e z o n e t o t h e one o f g t f s o l d t z s e l f .
s e t c u r r e n t p r o c e s s t i m e z o n e ut time .
mktime time .
l o c a l t i m e ut s e t p r o c e s s t i m e z o n e o l d t z return ut normalized code snippet def func arg 0 arg 1 arg 2 arg 0 .
s e t c u r r e n t p r o c e s s t i m e z o n e arg 1 time .
mktime time .
l o c a l t i m e arg 1 s e t p r o c e s s t i m e z o n e arg 2 return arg 1 figure an example of the normalization performed for constructing the advtest dataset by lu et al.
.
table results on the advtest set of codesearchnet.
model method test mrr roberta .
codebert original implementation .
codebert our re implemention w a larger bsz .
codet5 base .
unixcoder .
cascode shared k .
cascode separate k .
cascode shared k .
cascode separate k .
when trained in a multi task fashion.
with almost half of the parameter size and memory cost cascode s shared variant offers comparable performance to the non shared separate variant.
despite showing promising results there are still some areas for improving our method.
one limitation of our current cascaded scheme is that the computation spent in generating representations in the first stage of fast encoders is not fully leveraged in the second stage.
currently we process raw token level inputs in the second stage but ideally the representations from the first stage should be useful for the classification stage too .
our initial attempts along this direction did not turn fruitful and future work could address this aspect.
to improve the inference speed of the two stage retrieval future work could explore methods like quantization and model distillation of the transformer models e.g.
employing the onnx runtime .
another limitation warranting further investigation is associated with the training of the shared variant of cascode.
here training with the multitask learning framework joint objective of infonce and binary cross entropy leads to a model that performs slightly worse than the separate variant individually finetuned models .
we tried augmenting the capabilities of this model with solutions like using independent cls tokens for the three modes the model has to operate in nl only pl only nl pl concatenation and adjusting the relative weight of the two losses involved but failed to obtain any improvement over the separateinput nl query creates a base django project correct code snippet retrieved by cascode s second stage def func arg 0 i fos .
path .
e x i s t s arg 0 .
py arg 1 os .
path .
j o i n arg 0 .
app dir arg 0 .
project name i fos .
path .
e x i s t s arg 1 i farg 0 .
f o r c e l o g g i n g .
warn removing e x i s t i n g p r o j e c t s h u t i l .
rmtree arg 1 else l o g g i n g .
warn found e x i s t i n g p r o j e c t not c r e a t i n g use f o r c e t o o v e r w r i t e return l o g g i n g .
i n f o c r e a t i n g p r o j e c t arg 2 s u b p r o c e s s .
popen cd s t a r t p r o j e c t dev n u l l .format arg 0 .
app dir arg 0 .
v e d i r os .
sep arg 0 .
project name os .
sep bin os .
sep django admin .
py arg 0 .
project name s h e l l true os .
w a i t p i d arg 2 .
pid else l o g g i n g .
e r r o r unable t o f i n d python i n t e r p r e t e r i n v i r t u a l e n v return top code snippet retrieved by cascode s first stage def func arg 0 bunch defaults arg 0 .
p r o j e c t r o o t g e t p r o j e c t r o o t i f not arg 0 .
p r o j e c t r o o t r a i s e runtimeerror no t a s k s module i s imported cannot d e t e r m i n e p r o j e c t r o o t t h i s a ss u m es an i m p o r t a b l e s e t u p .
py i farg 0 .
p r o j e c t r o o t not in s y s .
path s y s .
path .
append arg 0 .
p r o j e c t r o o t try from s e t u p import arg 6 except i m p o r t e r r o r from s e t u p import s e t u p a r g s as arg 6 arg 0 .
p r o j e c t bunch arg 6 return arg 0 figure an example from the test set of the advtest normalized variant codesearchnet py dataset with retrieved queries from cascode s two stages.
for the nl query creates a base django project cascode correctly retrieves the corresponding code snippet as the top result.
the fast encoder baseline first stage of cascode presents this snippet as the 3rd result this is then re ranked to the top by cascode s second stage.
variant.
lastly similar to related work in nlp designing innovative pre training schemes to specifically improve code search performance is also a promising direction for future work.
data availability we provide our implementation source code and instructions to access datasets as supplementary material to replicate the experiments at this figshare url.
398esec fse december san francisco ca usa akhilesh deepak gotmare junnan li shafiq joty and steven c.h.
hoi