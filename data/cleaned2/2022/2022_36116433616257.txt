fix fairness don t ruin accuracy performance awarefairness repairusingautoml giangnguyen gnguyen iastate.edu iowa stateuniversity usasumonbiswas sumonb cs.cmu.edu carnegiemellonuniversity usahridesh rajan hridesh iastate.edu iowa stateuniversity usa abstract machinelearning ml isincreasinglybeingusedincriticaldecisionmaking software but incidents have raised questions about the fairness of ml predictions.
to address this issue new tools and methodsareneededtomitigatebiasinml basedsoftware.previous studieshaveproposedbiasmitigationalgorithmsthatonlywork in specific situations and often result in a loss of accuracy.
our proposed solutionis a novelapproach that utilizesautomatedmachinelearning automl techniquestomitigatebias.ourapproach includes two key innovations a novel optimization function and a fairness awaresearchspace.byimprovingthedefaultoptimization function ofautomland incorporating fairness objectives we are able to mitigate bias with little to no loss of accuracy.
additionally we propose a fairness aware search space pruning method for automl to reduce computational cost and repair time.
our approach builtonthestate of the art auto sklearn tool isdesignedtoreduce bias in real world scenarios.
in ordertodemonstratethe effectiveness of our approach we evaluated our approach on four fairness problemsand16differentmlmodels andourresultsshowasignificantimprovementoverthebaselineandexistingbiasmitigation techniques.ourapproach fair automl successfullyrepaired60 out of buggy cases while existing bias mitigation techniques only repairedup to44 outof64 cases.
ccsconcepts softwareanditsengineering search basedsoftwareengineering computing methodologies machine learning .
keywords software fairness bias mitigation fairness accuracy trade off machinelearningsoftware automatedmachine learning acm reference format giangnguyen sumonbiswas andhrideshrajan.
.fixfairness don t ruin accuracy performance aware fairness repair using automl.
in proceedings of the 31st acm joint european software engineering conference andsymposiumonthefoundationsofsoftwareengineering esec fse december san francisco ca usa.
acm new york ny usa 13pages.
esec fse december san francisco ca usa copyright heldby theowner author s .
acm isbn .
introduction recent advancementsinmachine learninghave ledto remarkable success insolving complex decision makingproblemssuch asjob recommendations hiring employees social services and education .
however ml softwarecanexhibitdiscriminationduetounfairnessbugsinthe models .thesebugscanresultinskeweddecisionstowards certain groups of people based on protected attributes such as race age orsex .
to address this issue the software engineering se communityhasinvestedindevelopingtestingandverificationstrategies to detect unfairness in software systems .
additionally the machine learning literature contains a wealth of research on defining different fairness criteria for ml models and mitigating bias .
various bias mitigation methods have been proposed to build fairer models.
some approachesmitigatedatabiasbyadaptingthetrainingdata some modify ml models during the training process to mitigate bias andothersaimtoincreasefairnessbychangingthe outcome ofpredictions .
despitetheseefforts currentbiasmitigationtechniquesoften come at the costofdecreasedaccuracy .
theireffectiveness variesbasedondatasets fairnessmetrics orthechoiceofprotected attributes .hortetal .proposedfairea anovel approachtoevaluatetheeffectivenessofbiasmitigationtechniques whichfoundthatnearlyhalfoftheevaluatedcasesreceivedpoor effectiveness.
moreover evaluations by chen et al.also showed that in of cases bias mitigation methods reduced both ml performance andfairness .
recent works have shown that parameter tuning cansuccessfullyfixfairnessbugswithoutsacrificingaccuracy.by finding the best set of parameters parameter tuning can minimize theerrorbetweenthepredictedvaluesandthetruevaluestoreduce bias.
this helps to ensure that the model is not overly simplified or toocomplex whichcanleadtounderfitting highbias oroverfitting lowaccuracy respectively.
by tuning the parameters we can find therightbalancebetweenbiasandaccuracy whichleadstoamodel that generalizes well to different dataor fairness metric.
however itischallengingtoidentifywhichparametersettingachievesthe best fairness accuracytrade off .
recent advancements in automl technology have made it possible forbothexpertsand non expertsto harnessthe powerofmachinelearning.automlprovestobeaneffectiveoptionfordiscoveringoptimalparametersettings however currently thereisalackoffocusonreducingbiaswithintheautomltechniques.thus weposethefollowingresearchquestions isitpossible toutilizeautomlforthepurposeofreducingbias?isautomleffective thiswork islicensedunderacreativecommonsattribution4.0international license.
esec fse december3 san francisco ca usa giangnguyen sumonbiswas andhrideshrajan in mitigating bias?
does automl outperform existing bias reduction methods?
is automl more adaptable than existing bias mitigation techniques?
weintroduce fair automl anoveltechniquethatutilizesautoml to fix fairness bugs in machine learning models.
unlike existing bias mitigationtechniques fair automl addresses their limitationsbyenablingefficientandfairness awarebayesiansearchto repairunfairmodels makingiteffectiveforawiderangeofdatasets models andfairnessmetrics.thekeyideabehind fair automl isto use automl to explore as many configurations as possible in order tofindtheoptimalfixforabuggymodel.particularly fair automl enhancesthepotentialofautomlforfixingfairnessbugsintwo novel techniques by generatinga new optimizationfunction that guides automl to fix fairness bugs without sacrificing accuracy and by defining a new search space based on the specific input to accelerate the bug fixing process.
together these contributions enablefair automl toeffectivelyfixfairnessbugsacrossvarious datasetsandfairnessmetrics.wehaveimplemented fair automl ontopof auto sklearn thestate of the artautomlframework.
fair automl aims toeffectivelyaddressthe limitationsofexisting bias mitigation techniques by utilizing automl to efficiently repairunfairmodelsacrossvarious datasets models andfairness metrics.
we conduct an extensive evaluation of fair automl using 4widelyuseddatasetsinthefairnessliterature and16 buggymodelscollectedfromarecentstudy .theresultsdemonstratetheeffectivenessofourapproach as fair automl successfully repairs out of buggy cases surpassing the performance of existing bias mitigation techniques which were only able to fix up to outof64 bugsinthe same settings andtraining time.
our main contributionsare the following wehaveproposedanovelapproachtofixunfairnessbugs andretain accuracyat the same time.
we have proposed methods to generate the optimization functionautomaticallybasedonaninputtomakeautoml fixing fairnessbugsmore efficiently.
we have pruned the search space automatically based on an inputto fixfairnessbugsfasterusing automl.
we have implemented our approach in a sota automl auto sklearn .the artifactisavailablehere .
the paper is organized as follows 2describes the background 3presents a motivation 4indicates the problem definition shows the fair automl approaches 6presents the our evaluation 7discussesthelimitationsandfuturedirectionsof fair automl 8discusses the threats to validity of fair automl 9concludes and 10describes the artifact.
background we begin by providing an overview of the background and related researchinthe field ofsoftware fairness.
.
preliminaries .
.
ml so f tware.
given an input dataset u1d437split into a training dataset u1d437 u1d461 u1d45f u1d44e u1d456 u1d45bandavalidationdataset u1d437 u1d463 u1d44e u1d459 amlsoftwaresystem canbeabstractlyviewingasmappingproblem u1d440 u1d706 u1d450 u1d465 u1d466.altfrominputs u1d465tooutputs u1d466.altbylearningfrom u1d437 u1d461 u1d45f u1d44e u1d456 u1d45b.mldevelopersaimsto search for a hyperparameter configuration u1d706 and complementarycomponents u1d450 for model u1d440to obtain optimal fairness accuracy on u1d437 u1d463 u1d44e u1d459.
the complementary components can be ml algorithms combinedwithaclassifier i.e.
pre processing algorithms.
.
.
automl.
given the search spaces and u1d436for hyperparameters and complementary components automl aims to find u1d706 and u1d450 to obtain the lowestvalueof the costfunction equation u1d440 argmin u1d706 u1d450 u1d436 u1d436 u1d45c u1d460 u1d461 u1d440 u1d706 u1d450 u1d437 u1d463 u1d44e u1d459 u1d706 u1d450 argmin u1d706 u1d450 u1d43f u1d45c u1d460 u1d460 u1d440 u1d706 u1d450 u1d437 u1d461 u1d45f u1d44e u1d456 u1d45b .
.
measures.
we consider a problem where each individual in the population has a true label in u1d466.alt .
we assume a protected attribute u1d467 such as race sex age where one label is privileged denoted and the other is unprivileged denoted .
the predictions are u1d466.alt that need to be not only accurate with respectto u1d466.altbut alsofair withrespectto the protectedattribute u1d467.
accuracy measure.
accuracy is given by the ratio of the number ofcorrectpredictions bythe total number of predictions.
accuracy truepositive truenegative total fairness measure.
we use four ways to define group fairness metrics whichare widely usedinfairnessliterature thedisparate impact di is the proportion of the unprivileged group with the favorable label divided by the proportion of the privilegedgroup withthe favorable label .
u1d437 u1d43c u1d443 u1d45f u1d443 u1d45f thestatistical parity difference spd quantifies the disparity betweenthefavorablelabel sprobabilityfortheunprivilegedgroup andthe favorable label s probability for the privilegedgroup .
u1d446 u1d443 u1d437 u1d443 u1d45f u1d443 u1d45f theequal opportunity difference eod measures the disparity betweenthetrue positiverateoftheunprivilegedgroupandthe privilegedgroup.
u1d447 u1d443 u1d445 u1d462 u1d443 u1d45f u1d447 u1d443 u1d445 u1d45d u1d443 u1d45f u1d438 u1d442 u1d437 u1d447 u1d443 u1d445 u1d462 u1d447 u1d443 u1d445 u1d45d the average absolute odds difference aod is the mean of the differenceoftrue positive rate and false positive rate among the unprivilegedgroup andprivilegedgroup .
u1d439 u1d443 u1d445 u1d462 u1d443 u1d439 u1d443 u1d445 u1d45d u1d443 u1d434 u1d442 u1d437 u1d439 u1d443 u1d445 u1d462 u1d439 u1d443 u1d445 u1d45d u1d447 u1d443 u1d445 u1d462 u1d447 u1d443 u1d445 u1d45d to use all the metrics in the same setting di has been plotted in the absolute value of the log scale and spd eod aod have been plottedinabsolutevalue .thus thebiasscoreofamodelis measuredfrom withlower scores indicating more fairness.
.
related work .
.
biasmitigation.
seandmlresearchershasdevelopedvariousbiasmitigationmethodstoincreasefairnessinmlsoftware dividedintothree categories pre processing approachesreducebiasbypre processingthe training data.
for instance fair smote addresses data bias by removingbiasedlabelsandbalancingthedistributionofpositive andnegativeexamplesforeachsensitiveattribute.
reweighing 503fix fairness don t ruinaccuracy performance aware fairness repairusingautoml esec fse december3 san francisco ca usa decreases bias by assigning different weights to different groups based on the degree of favoritism of a group.
disparate impact remover is a pre processing bias mitigation technique that aims to reduce biasbyediting feature values.
in processing approachesreducebiasbymodifyingmlmodels during the training process i.e.
parfait ml present a searchbased solution to balance fairness and accuracy by tuning hyperparametertoapproximatethetwinedparetocurves.
maat isan ensembleapproachaimedatimprovingthefairness performance trade off in ml software.
instead of combining models with the samelearningobjectivesastraditionalensemblemethods maat merges models that are optimizedfor differentgoals.
post processing approaches change the outcome of prediction toreducebias.thistechniqueunfavorsprivilegedgroups instances and favors those of unprivileged groups lying around the decision boundary.forexample equalizedodds reducesthevalueof eod by modifying the output labels.
fax ai eliminates direct discrimination in machine learning models by limiting the use of certain features thereby preventing them from serving as surrogates for protected attributes.
reject option classification prioritizes instances from the privileged group over those from the unprivilegedgroupthataresituatedonthedecisionboundarywith high uncertainty.
previouseffortshavemadesignificantprogressinreducingbias however they come at the cost of decreased accuracy and their resultscanvarydependingonthedatasetsandfairnessmetrics.our proposal fair automl aimsto strikeabalance betweenaccuracy and bias reduction and demonstrate generalizability across various datasets andmetrics.
.
.
search space pruning.
search space pruning involves reducing the size or complexity of the search space in optimization or machine learning tasks.
pruning techniques are employed to acceleratetheoptimizationprocessofautomlbyeliminatingunpromisingorredundantoptions thusfocusingcomputationalresources on more promising areasofthesearch space.
for example feurer et al.
introduce auto sklearn .
a novel approach aimed at enhancing the performance of auto sklearn.
this advancement involvesconstrainingthesearchspacetoexclusivelycompriseiterativealgorithms whileeliminatingfeaturepreprocessing.this strategic adjustment streamlines the implementation of successive halving asitreducesthecomplexitytoasinglefidelitytype the numberofiterations.otherwise theincorporationofdatasetsubsetsasanalternativefidelitywouldrequireadditionalconsideration.
another innovative contribution comes from cambronero et al.
whointroducesams .thismethodcapitalizesonthewealthof sourcecoderepositoriestostreamlinethesearchspaceforautoml.
notably ams harnesses the power of unspecified complementary andfunctionallyrelatedapicomponents.byleveragingthesecomponents thesearchspaceforautomlisprunedeffectively.diverging from prior research efforts fair automl distinguishes itself by leveraging data characteristics to effectively trim down the search space.notably existingtechniquesinsearchspacepruningprimarilytargetaccuracyenhancementwithinautoml.incontrast our innovativepruningmethodologywithin fair automl isuniquely directedtowardsrepairing unfairmodels.
inverted trade off regionswin win region good trade off region bad trade off regionlose lose region baseline fairnessaccuracy original model model with mutation degree figure baselinefairness accuracy trade off .
.
automl extension.
automl aims to automate the process of building a high performing ml model but it has limitations.
it can be costly time consuming to train and produces complex models that are difficult to understand.
to address these limitations softwareengineeringresearchershavedevelopedmethodsto enhanceautomlperformance suchas ams andmanas .
amsutilizes source code repositories to create a new search space forautoml while manasmineshand developedmodelstofinda betterstartingpointforautoml.thegoalofthesemethodsisto improveautomltomaximizetheaccuracy.differentfromthese methods fair automl built on top of auto sklearn is the first to focusonrepairing unfairmodels.
motivation the widespread use of machine learning in software development has brought attention to the issue of fairness in ml models.
although various bias mitigation techniques have been developed toaddressthisissue theyhavelimitations.thesetechniquessuffer from a poor balance between fairness and accuracy and are not applicable to a wide range of datasets metrics and models .togainadeeperunderstandingoftheselimitations we evaluate six different bias mitigation techniques using four fairness metrics four datasets and six model types.
the evaluation criteria are borrowed from fairea and are presented in table .
faireaisdesignedtoassessthetrade offbetweenfairnessand accuracy of bias mitigation techniques.
the methodology of fairea is demonstrated in figure where the fairness and accuracy of a bias mitigation technique on a dataset are displayed in a twodimensional coordinate system.
the baseline is established by connecting the fairness accuracy points of the original model and the mitigationmodelsonthedataset.
faireaevaluatestheperformance ofthemitigationtechniquebyalteringtheoriginalmodelpredictionsandreplacingarandomsubsetofthepredictionswithother labels.themutationdegreerangesfrom10 to100 withastepsizeof10 .thebaselineclassifiesthefairness accuracytrade off ofa biasmitigationtechniqueinto fiveregions lose lose trade off lose bad trade off bad inverted trade off inv good trade off good and win win trade off win .
a technique reducing both accuracy and fairness would fall into the lose lose trade off region.
if the trade off is worse than the baseline it would fall into the bad trade offregion.ifthetrade offisbetterthanthebaseline itwould fallintothegoodtrade offregion.ifabiasmitigationmethodsimultaneouslydecreasesbothbiasandaccuracy itwouldfallintothe 504esec fse december3 san francisco ca usa giangnguyen sumonbiswas andhrideshrajan table mean proportions ofmitigationcasesthat that fall into eachmitigationregion criteria losebadinvgoodwin metricdi spd eod aod datasetadult bank german titanic mean bad badtrade offregion lose lose losetrade offregion inv inverted trade off region good good trade offregion win win win trade offregion.
inverted trade off region.
if the technique improves both accuracy andfairness itwouldfall intothe win wintrade offregion.
theresultsoftheregionclassificationofsixbiasmitigationtechniques reweighing disparateimpactremover parfaitml equalized odds fax ai reject option classification areshownintable .theevaluationwasconducted on64buggycasesusingdifferentcriteriasuchasfairnessmetrics anddatasets.thecaseisidentifiedasbuggywhenitfallsbelowthe faireabaseline.themeanpercentageofeachtechniquefallinginto thecorrespondingregionsislistedineachcell.themeanresults provideageneraloverviewofthecurrentstateofbiasmitigation techniques.furtherdetails ontheperformanceofeachindividual biasmitigationtechniquecanbefoundintable 3ofourevaluation.
table1illustratesthatthemajorityofexistingbiasmitigation techniques have a poor fairness accuracy trade off across different datasets fairness metrics and classification models.
specifically ofthe casesshowthatthesetechniquesperformworsethan the original model with of the cases resulting in a poor tradeoff and resulting in a decrease in accuracy and an increase inbias.
additionally table 1shows thattheperformanceofthese techniques varies depending on the input as demonstrated by the different resultsobtainedwhen usingdifferent datasetsorfainess metrics .
for example the bias mitigation techniques had a high performance in62 of the cases using the adult dataset forgoodtrade offregionand7 forwin wintrade offregion but only achieved40 goodeffectiveness inthe bank dataset.
hortetal.
havedemonstratedthatthroughproperparameter tuning it is possible to address fairness issues in machine learning models without sacrificing accuracy.
however determining the optimalfairness accuracytrade offcanbeachallenge.although automlcanbeeffectiveinfindingthebestparametersettings it does not specifically address bias reduction.
this motivates the development of fair automl a novel approach that utilizes bayesian optimizationto tuneparametersandaddressfairnessissueswithout hindering accuracy.
fair automl is evaluated for its generality across different fairness metrics and datasets and unlike other bias mitigationmethods itcan be appliedto any dataset ormetric.
thisworkfocusesonimprovingfairnessquantitativelyofbuggy models instead of targetinga specifictype ofdatasets and models.
our method is general since we utilize the power of automl to try as many configurations as possible to obtain the optimal fix therefore ourmethodcanworkonvarioustypesofdatasetsandmetrics.
the rest of this work describes our approach fair automl thataddresses the limitations of both existing bias mitigation methods and automl.
as a demonstration fair automl achieved good performance in of the buggy cases in the adult dataset while of the mitigation cases showed a good fairness accuracy tradeoff and theremaining exhibited animprovement inaccuracy withoutsacrificingbiasreduction.
problem definition thisworkaimstoutilizeautomltoaddressissuesofunfairnessin mlsoftwarebyfindinganewsetofconfigurationsforthemodel that achieves optimal fairness accuracy trade off.
because fairness is an additional consideration beyond accuracy the problem becomesamulti objectiveoptimizationproblem requiringanewcost functionthatcanoptimizebothfairnessandaccuracysimultaneously.
to achieve this we use a technique called weighted sum scalarization equation whichallowsustoweightheimportanceofdifferentobjectivesandcreateasinglescalarcostfunction.
u1d434 u1d45b summationdisplay.
u1d456 u1d450 u1d456 u1d6fd u1d456 where u1d6fd u1d456denotesthe relative weightof importanceof u1d450 u1d456 u1d45b summationdisplay.
u1d456 u1d6fd u1d456 in this work we use a cost function or objective function that isaweighted sumscalarizationoftwodecisioncriteria biasand accuracy.thiscostfunction asshowninequation assignweights tobiasandaccuracyinthecostfunctionallowustoadjustthetradeoff between the twocriteriaaccording to the specific problems u1d436 u1d45c u1d460 u1d461 u1d440 u1d706 u1d450 d u1d467 u1d6fd u1d453 u1d6fd u1d44e we analyze the output of the buggy ml software including bias and accuracy to create a suitable cost function for each input.
by analyzing the output we are able to automatically estimate the weights of the cost function in order to balance fairness and accuracy for a specific problem.
to the best of our knowledge this is the first work that applies output analysis of the software to automlto repairunfairml models.
however using automlcan be costly and time consuming.
to addressthisissue weproposeanovelmethodthatautomatically create new search spaces and u1d436 based on different inputs to accelerate the bug fixing process of automl.
these new search spacesaresmallerinsizecomparedtotheoriginalones and u1d436 u1d436 .
particularly as shown in equation fair automl takes as input a ml model and a dataset with a protected attribute u1d467 andaimstofind u1d706 and u1d450 inthesmallersearchspace inorder to minimizethe costvalue.
u1d440 argmin u1d706 u1d450 u1d436 u1d436 u1d45c u1d460 u1d461 u1d440 u1d706 u1d450 u1d437 u1d463 u1d44e u1d459 u1d467 thetechniqueofsearchspacepruningin fair automl utilizesdata characteristicstoenhancebug fixingefficiency.byshrinkingthe search spaces based on input analysis fair automl can find better solutions more quickly.
a set of predefined modifications to the ml model are pre built and used as a new search space for new input datasets reducing the time needed to fix buggy models.
our approach is based on previous works in automl but updated and modified to tackle bias issues.
to the best of our knowledge 505fix fairness don t ruinaccuracy performance aware fairness repairusingautoml esec fse december3 san francisco ca usa offline online input datasetmatching inputgood fixes f a unfair ml models fixed ml models efficient fair automl original search spacefair automl fairness datasets ml models with default parameters modified optimization function automl automatically estimate new search spaceinput in offline phasefair automl input in online phase1 the most similar inputthe most suitable fix figure anoverview of fair automl approach we are the first to propose a search space pruning technique for fairness aware automl.
fair automl this section describes a detailed description of key componentsof fair automl figure2 thedynamicoptimizationfunction steps andthe searchspacepruning steps .
.1dynamic optimization for bias elimination we strive to eliminate bias in unfair models by utilizing equation as the objective functionand determiningthe optimalvalue of u1d6fdto minimizethecostfunction.inthissection weproposeanapproach toautomaticallyestimatetheoptimalvalueof u1d6fdforaspecificdataset andatargetedmodel.thismethodensuresefficientcorrectionof fairnessissueswhilemaintaininghigh predictive accuracy.
.
.
upperboundofthecostfunction.
toestimatetheoptimal valueof u1d6fd thefirststepistodeterminetheupperboundofthecost function.
this can be done by using a pseudo model which is the mutation degree model as shown in the figure .
in other words the pseudo model always achieves the accuracy on any binary classification problem as follows u1d44e0 u1d45a u1d44e u1d465 u1d443 u1d44c u1d443 u1d44c given an input the pseudo model achieves an accuracy of u1d44e0 and a bias value of u1d4530on that input.
we define the cost function u1d436 u1d45c u1d460 u1d461 ofthebuggy ml modelwithaccuracy u1d44eand biasvalue u1d453on theinput.asautomltriesdifferenthyperparameterconfigurations tofix themodel thevalues of u1d44eand u1d453may changeovertime.the upper bound ofthe costfunctionisdefinedas equations 8and9 u1d436 u1d45c u1d460 u1d461 u1d440 u1d706 u1d450 d u1d467 u1d6fd u1d4530 u1d6fd u1d44e0 u1d6fd u1d453 u1d6fd u1d44e u1d6fd u1d4530 u1d6fd u1d44e0 the upper bound of the cost function is defined with the goal of repairing a buggy model so that its performance falls within a good win win trade off region of fairness and accuracy.
in other words the accuracy of the repaired model must be higher than the accuracyofthe pseudo model.the repairedmodelmustbe better than the pseudo model in terms of the cost function s value.
since the pseudo model has zero bias u1d4530 the upper bound of thecostfunction isdefinedas follows equation u1d6fd u1d453 u1d6fd u1d44e u1d6fd u1d44e0 .
.
lower bound of u1d6fd.in this work we desire to optimize the valueof u1d6fdinordertominimizebiasasmuchaspossible.thecost function used by fair automl is designed to balance accuracy and fairness and increasing u1d6fdwill place more emphasis on reducing bias.
however simply setting u1d6fdto its highest possible value is notaviableoption asitmayleadtolowpredictiveaccuracyand overfitting.wecannotacceptmodelswithpoorpredictiveaccuracy regardless of their low bias .
to overcome this challenge weaimtofindthelowerboundof u1d6fd whichcanbedonebasedon the upper bound ofthe costfunction.
from equation we get u1d6fd u1d44e u1d44e0 u1d44e u1d44e0 u1d453 however if the value of u1d6fdis smaller than u1d44e u1d44e0 u1d44e u1d44e0 u1d453 the optimizationfunction u1d436 u1d45c u1d460 u1d461willalwaysmeetitsupperboundcondition.if thevalueof u1d6fdalwayssatisfiestheupperboundconditionofthecost functionregardlessofaccuracyandfairness wecanobtainabetter optimization function by either increasing accuracy or decreasing bias.
in this case we cannot guide automl to produce a lower bias.
therefore to guide automl produces an output with improved fairness we setalower bound for u1d6fdas equation u1d6fd u1d44e u1d44e0 u1d44e u1d44e0 u1d453 theintuitionbeingthatourmethodaimstoincreasethechance for automl to achieve better fairness.
however by setting u1d6fd u1d44e u1d44e0 u1d44e u1d44e0 u1d453and u1d44e u1d44e0 we aim to find a model which has better accuracythanthepseudo model anyvalueofbias f cansatisfy upper bound condition of the cost function which lower chance to obtain fairer models of automl.
to increase this chance we set u1d6fd u1d44e u1d44e0 u1d44e u1d44e0 u1d453and u1d44e u1d44e0.
in this case automl need to find better modelsthathaslowerbiastosatisfyequation .inotherwords this lower bound condition indirectly forces bayesian optimization to searchfor lower biasmodels.
.
.
u1d6fdestimation.
the final step is estimating the value of u1d6fd based on its lower bound condition.
suppose that the buggy model achievesanaccuracyof u1d44e1andabiasvalueof u1d4531onthatinput.from the begining we have u1d44e u1d44e1and u1d453 u1d4531.
in that time the lower bound of u1d6fdis u1d43f u1d44e1 u1d44e0 u1d44e1 u1d44e0 u1d4531 sowe have u1d6fd u1d43f u1d458 u1d458 wepresentagreedyalgorithmforestimatingthevalueof u1d6fd which is detailed in algorithm .
given a dataset u1d437with a protected attribute u1d467and a buggy model u1d440 line we start by measuring thelowerboundof u1d6fd.next werun fair automl ontheinputunder time constraint twith a value of u1d6fdset to u1d44e1 u1d44e0 u1d44e1 u1d44e0 u1d4531 line .
as the algorithm searches whenever fair automl finds a candidate model that meets the condition u1d436 u1d45c u1d460 u1d461 u1d436 u1d45c u1d460 u1d4610 lines the value of u1d6fdis slightly increased by u1d6fc line .
if after n tries fair automl cannotfindamodelthatsatisfiesthecondition the final value of u1d6fdis set to u1d6fd u1d6fd u1d6fcfor the remaining search time to prevent overfitting from an excessively high value of u1d6fd lines .
the algorithm returns the bestmodelfound line .
506esec fse december3 san francisco ca usa giangnguyen sumonbiswas andhrideshrajan algorithm1 greedy weightidentifier input a dataset u1d437with protected attribute u1d467 buggy model u1d440 hyperparameteredby u1d706 theincrementvalue u1d6fc thesearching time u1d461andthe threshold n u1d6fd u1d44e1 u1d44e0 u1d44e1 u1d44e0 u1d4531 u1d436 u1d45c u1d460 u1d461 u1d440 u1d706 u1d450 d u1d467 u1d6fd u1d453 u1d6fd u1d44e u1d436 u1d45c u1d460 u1d4610 u1d440 u1d706 u1d450 d u1d467 u1d6fd u1d44e0 count checker false while u1d461do u1d440 u1d706 u1d450 argmin u1d706 u1d436 u1d45c u1d460 u1d461 u1d440 u1d706 u1d450 d u1d467 count count if u1d436 u1d45c u1d460 u1d461 u1d440 u1d706 u1d450 d u1d467 u1d436 u1d45c u1d460 u1d4610 u1d440 u1d706 u1d450 d u1d467 then ifchecker false then u1d6fd u1d6fd u1d6fc count if u1d450 u1d45c u1d462 u1d45b u1d461 u1d441andchecker false then u1d6fd u1d6fd u1d6fc checker true return u1d440 u1d706 .
search spacepruning forefficient bias elimination weproposeasolutiontospeedupthebayesianoptimizationprocessinfair automl byimplementingsearchspacepruning.this technique takes advantageofdata characteristics toautomatically reducethesizeofthesearchspaceinautoml thusimprovingits efficiency.ourapproachincludestwophases theofflinephaseand theonlinephase.theofflinephasetrainsasetofinputsmultiple timestogatheracollectionofhyperparametersandcomplementary componentsforeachinput formingapre builtsearchspace.inthe onlinephase whenanewinputisencountered itismatchedagainst theinputsstoredinourdatabasetofindamatchingpre builtsearch space which is then utilized to repair the buggy model.
this approacheffectivelyreplacestheoriginalsearchspaceof fair automl makingthebayesianoptimizationprocessmuchfaster.searchspace pruning hasalready been successfully appliedbefore however this is the first application of data characteristics to prune the searchspacefor fairness aware automl.
.
.
offlinephase.
thisphaseconstructsasetofsearchspacesfor fair automl based on different inputs.
it is important to note that the input format in the offline phase must match that of the online phase which includes a dataset with a protected attribute and a ml model.
this ensures that the pre built search spaces created in the offline phasecan be effectivelyutilizedinthe onlinephase.
input.in the offline phase we collect a set of inputs to build search spaces for fair automl .
the inputs are obtained as follows.
firstly weminemachinelearningdatasetsfrom openml considering onlythe active datasets thathave been verifiedto work properly.
secondly to ensure that themined datasets are relevant to the fairness problem we only collect datasets that contain at least one of the following attributes age sex race .
in total we collected fairness datasets.
thirdly for each mined dataset we use all available protected attributes.
for example when dealing with datasets that contain multiple protected attributes such asalgorithm2 databasebuilding input a dataset u1d437with protected attribute u1d467 a model u1d440with defaulthyperparameters u1d706.running time t. d dev database space count whilecount ndo count count whiletdo u1d440 u1d706 argmin u1d436 u1d45c u1d460 u1d461 u1d440 u1d706 d u1d467 u1d451 u1d451 u1d440 u1d706 kbestpipelines top k d mbestcomponents top m kbestpipelines formodel kbestpipelines do forpara modeldo space space forpara spacedo ifparaisnumerical then no outliers fori space do if u1d456 u1d460 u1d45d u1d44e u1d450 u1d452 u1d451 u1d452 u1d463 u1d70e u1d460 u1d45d u1d44e u1d450 u1d452 then no outliers no outliers space space database space mbestcomponents returndatabase theadultdataset that includes sexandraceas protected attributes we treat them as distinct inputs for the dataset.
finally we use the defaultvaluesforthehyperparametersoftheinputmlmodelin theofflinephase aswedonotknowthespecificvaluesthatwillbe usedinthe onlinephase.
database building.
to build a pre defined search space database we use thealgorithm outlined in algorithm 2to obtain a pre built searchspaceforeachcollectedinputinordertofixthebuggymodel.
thisprocessinvolvestrainingafairnessdatasetwithaspecificprotectedattributeandmlmodelmultipletimesusing fair automl collectingthetop u1d458bestpipelinesfound andextractingparameters from these pipelines.
in particular we use fair automl to train the fairness dataset with a specific protected attribute and a ml model for u1d45biterations line .
we then gatherthe top u1d458best pipelines including a classifier and complementary components found by fair automl accordingtotheoptimizationfunction svalue line .thisresults in u1d458 u1d45btotalpipelines.fromthesepipelines we extract and store the m most frequently used complementary components in the database line .
for each classifier parameter we alsostoreitsvalue lines14 .thisresultsink nvaluesbeing stored for each hyperparameter.
if a hyperparameter is categorical anditsvaluesaresampledfromasetofdifferentvalues westoreall its unique values in the database.
if a hyperparameter is numerical and its values are sampled from a uniform distribution we remove anyoutliersandstoretherangeofvaluesfromtheminimumtothe maximuminthedatabase lines17 .afterthisprocess wehave collected the pre built search space for the input lines .
we believethattwosimilarinputsmayhavesimilarbuggymodelsand 507fix fairness don t ruinaccuracy performance aware fairness repairusingautoml esec fse december3 san francisco ca usa algorithm3 inputmatching input a input dataset u1d437with the protected attribute u1d467 the numberofdatapoints u1d45d thenumberoffeatures u1d453 lowerbound u1d43f abuggy model u1d440 andadatabase.
dist for u1d451 u1d456indatabase do dist u1d453 u1d456 u1d453 u1d45d u1d456 u1d45d similardataset min dist key dist.get dist for u1d467 u1d456insimilardataset do dist u1d43f u1d456 u1d43f similarattribute min dist key dist.get similarmodel m withdefaultparameter returnsimilardataset similarattribute similarmodel fixes sothepre builtsearchspaceisbuiltbasedonthebestmodelsfoundby fair automl fromsimilar inputs makingitareliable solution for fixing buggy models.
.
.
online phase.
this phase utilizes a pre built search space from the database to fix a buggy model for a given dataset by replacing the originalsearchspacewiththe pre builtone.
searchspacepruning.
ourapproachofsearchspacepruningin fair automl improves the bug fixing performance by reducing the size of the hyperparameter tuning space.
algorithm 3is used to matchtheinputdataset protectedattribute andmlmodel tothe mostsimilarinputinthedatabase.firstly datacharacteristicssuch asthenumberofdatapointsandfeaturesareusedtomatchthenew dataset with the most similar one in the database .
l1 distance is computed between the new dataset and each mined dataset in the space of data characteristics to determine the closest match.
we consider that the most similar dataset to the new dataset is the nearest one line .
secondly we compute the lower bound u1d43f u1d44e1 u1d44e0 u1d44e1 u1d44e0 u1d4531of u1d6fdof the new input.
we then estimate the lower bound of u1d6fdof all the protected attributes of the matched dataset andselecttheattributewhoselowerboundisclosestto u1d43f line6 .
lastly twosimilarinputsmustusethesamemlalgorithm line10 .
thematchingprocessiscarriedoutintheorderofdatasetmatching protectedattributematching andmlalgorithmmatching.theprebuiltsearchspaceofthesimilarinputisthenusedasthenewsearch spacefor the newinput.
evaluation in this section we describe the design of the experiments to evaluatetheefficientof fair automl .wefirstposeresearchquestions and discuss the experimental details.
then we answer research questions regarding the efficiency and adaptability of fair automl .
rq1 isfair automl effectiveinfixingfairnessbugs?
to answer this question we quantify the number of fairness bugs thatfair automl is able to repair compared to existing methods allowing us to assess the capability of an automl system in fixing fairnessissues.
rq2 isfair automl more adaptable than existing bias mitigationtechniques?
theadaptabilityofabiasmitigationtechniqueindicatesitsperformanceacrossadiverserangeofdatasets metrics.
so we analyze the effectiveness of fair automl andexistingbiasmitigationtechniquesondifferentdataset metricsto assessthe adaptability ofan automlsystemonfixfairnessbugs.
rq3 aredynamicoptimizationfunctionandsearchspace pruning effective in fixing fairness bugs?
to answer this question we assess the performance of auto sklearn both with and without the dynamic optimization function and search space pruning to demonstratethe impact of eachproposedapproach.
.
experiment .
.
benchmarks.
weevaluatedourmethodusingreal worldfairnessbugssourcedfromarecentempiricalstudy withourbenchmarkconsistingof16modelscollectedfromkagglecoveringfive distincttypes xgboost xgb randomforest rf logisticregression lrg gradientboosting gbc supportvectormachine svc .
we use four popular datasets for our evaluation theadult census race comprised of observations and12featuresthatcapturethefinancialinformationofindividuals fromthe1994u.s.census.theobjectiveistopredictwhetheran individualearns an annual incomegreater than50k.
thebank marketing age has data points with featuresincludinginformationondirectmarketingcampaignsof a portuguese banking institution.
the classification task aims to identifywhether the clientwillsubscribe to aterm deposit.
thegerman credit sex has observations with featurescontainingcreditinformationtopredictgoodorbadcredit.
thetitanic sex has data points with features containingindividualinformationoftitanicpassengers.thedatasetis usedto predict whosurvivedthe titanic shipwreck.
.
.
evaluated learning techniques.
we examined the performanceof fair automl andothersupervised learning methodsaddressingdiscriminationinbinaryclassificationincludingallthree types ofbiasmitigationtechniques andauto ml techniques.
biasmitigationmethods.
weinvestigateallthreetypesofbias mitigationmethods pre processing in processing post processing.
weselectwidely studiedbiasmitigationmethodsforeachcategory thepre processing includesreweighing r disparate impact remover dir .
thein processing includesparfait ml pml .
thepost processing includesequalized odds eo fax ai fax rejectoption classification roc .
auto sklearn.
weexploretheefficiencyof auto sklearn as on mitigating bias in unfair model.
although auto sklearn does notseektodecreasebias wecompareitsperformancewith fairautomltodemonstratetheefficientofourtechniquesinguiding auto ml to repairfairnessbugs.
fair automl.
we create versions of fair automl in this evaluationrepresenting for fair automl withdifferentcostfunctions t1uses u1d6fd u1d437 u1d43c u1d6fd u1d44e u1d450 u1d450 u1d462 u1d45f u1d44e u1d450 u1d466.alt as a cost function.
t2uses u1d6fd u1d446 u1d443 u1d437 u1d6fd u1d44e u1d450 u1d450 u1d462 u1d45f u1d44e u1d450 u1d466.alt asacostfunction.
t3uses u1d6fd u1d438 u1d442 u1d437 u1d6fd u1d44e u1d450 u1d450 u1d462 u1d45f u1d44e u1d450 u1d466.alt asacostfunction.
t4uses u1d6fd u1d434 u1d442 u1d437 u1d6fd u1d44e u1d450 u1d450 u1d462 u1d45f u1d44e u1d450 u1d466.alt asacostfunction.
.
.
experimentalconfiguration.
experimentswereconducted using python .
on intel skylake processors.
fair automl leverages the capabilities of auto sklearn taking advantage of 508esec fse december3 san francisco ca usa giangnguyen sumonbiswas andhrideshrajan table trade offassessmentresults of fair automl auto sklearn andmitigationtechniques modelmetrict1t2t3t4asrdirpmleofaxrocmodelmetrict1t2t3t4asrdirpmleofaxroc acc0.
.
.
.
.
.
.
.
.
.
.
acc .
.
.
.
.
.
.
.
.
.
.
di0.
.
.
.
.
.337invinv0.
.
.
di0.
.
.
.
.
.
.326lose0.375bad0.
spd0.
.
.
.
.
.055invinv0.047inv0.
spd0.
.
.
.
.
.
.079bad0.
.
.
eod0.019inv0.
.
.008invinvinv0.041invlose eod0.
.
.
.049lose0.
.
.
.
.
.059rf aod0.028inv0.
.
.
.
.001inv0.
.007badlrg aod0.
.
.
.
.
.
.
.
.
.
.
acc .
.
.
.
.
.
.
.
.
.
.
acc .
.
.
.
.
.
.
.
.
.
.
di0.
.
.
.
.
.378loselose0.330inv0.
di0.
.
.
.147inv0.
.027lose0.
.
.
spd0.
.
.
.
.
.058loselose0.051inv0.
spdbad0.
.
.029inv0.058badlose0.
.
.
eod0.
.
.
.030loseloselose0.
.044invlose eodbad0.
.
.024invloseloselose0.
.010loseadultcensusxgb aod0.
.
.
.
.
.009lose0.
.
.020badgbc aod0.
.
.
.
.
.
.031lose0.
.041bad acc .
.
.
.
.
.
.
.
.
.
.
acc .
.
.
.
.
.
.
.
.
.
.
di0.
.
.
.
.
.210losebadbad0.129bad di0.
.
.
.097lose0.312badbadbad0.016bad spd0.035bad0.
.027lose0.065bad0.031bad0.021bad spd0.
.
.
.021lose0.053badbadbad0.002bad eodlosebadloseloseloseloselose0.029bad0.001bad eodloselose0.
.012loseinvinv0.054badinvinvrf aod0.
.
.
.
.016bad0.
.046bad0.031badxgb2 aod0.
.
.
.027loseinvinv0.040bad0.014bad acc .
.
.
.
.
.
.
.
.
.
.
acc .
.
.
.
.
.
.
.
.
.
.
di0.
.
.
.092lose0.
.174badbad0.
.
di0.
.
.
.014lose0.332badbadbadinvbad spd0.
.
.
.020lose0.042bad0.029bad0.013bad spdlose0.028invinvlose0.052badbadbadinvbad eod0.021inv0.
.018loseloselose0.064badinvlose eodloseloseinvinvloseinvlosebadbadinvinvbank marketingxgb1 aod0.
.
.
.046loselose0.
.054bad0.
.043gbc aod0.
.
.
.025lose0.017losebadbad0.012bad acc .
.
.
.
.
.
.
.
.
.
.
acc .
.
.
.
.
.
.
.
.
.
.
dibad0.076lose0.060lose0.
.
.
.095badbad di0.
.
.
.
.035badbadbadbad0.111bad spdbad0.052lose0.039lose0.
.
.
.068badbad spd0.
.
.
.
.021badbadbadbad0.078bad eod0.
.
.
.
.
.
.
.
.
.023bad eod0.
.
.
.
.
.
.039bad0.
.092badrf aodlosebadloseloseloseloselose0.
.044losebadsvc aod0.
.034bad0.027loseloselosebad0.
.016bad acc0.
.
.
.
.
.
.
.
.
.
.
acc0.
.
.
.
.
.
.
.
.
.
.
di0.
.
.
.101badbadbad0.051bad0.035lose di0.
.
.
.
.
.
.
.
.
.135bad spd0.
.
.
.069bad0.
.
.038bad0.025bad spd0.
.
.
.
.
.
.
.
.
.098bad eod0.
.
.
.
.
.
.
.
.
.055bad eod0.
.
.
.
.
.
.
.
.
.002badgerman creditxgb aod0.
.
.
.037losebadbad0.
.064badbadknn aod0.
.011inv0.034loseinvinv0.
.
.035bad acc .
.
.
.
.
.
.
.
.
.
.
acc .
.
.
.
.
.
.
.
.
.
.
di1.
.
.
.849lose0.
.160lose2.
.
.
di0.
.
.
.
.
.
.038bad1.769bad1.
spd0.
.
.
.545losebadbadlosebad0.
.
spd0.
.
.
.447invbadbad0.285bad0.
.
eod0.
.
.
.446loseloseloselose0.
.045bad eodbadbadbadbad0.058loselose0.280bad0.
.426rf aod0.
.
.
.
.062bad0.
.
.
.336badgbc aod0.
.
.
.
.176bad0.
.
.
.306bad acc .
.
.
.
.
.
.
.
.
.
.
acc .
.
.
.
.
.
.
.
.
.
.
di0.
.
.
.
.
.
.
.643badbad2.
di1.
.
.
.663lose0.
.
.
.811inv2.
spd0.
.
.
.
.063bad0.
.115bad0.
.
spd0.
.
.
.491losebad0.
.
.567inv0.
eodbad0.
.
.021loselose0.
.171bad0.
.
eodbad0.
.
.389loseloselose0.
.473inv0.423titanic lrg aod0.
.
.
.
.101bad0.
.214bad0.420badxgb aod0.
.
.
.524losebad0.
.
.
.062bad eachcell shows the accuracy bias differencebetween the originaland repaired models.
foraccuracy accuracydifference newaccuracy old accuracy.
forbias di spd eod aod bias difference old bias new bias.
thus a positive value indicates an improvement in bias accuracy in the repaired model compared to the original and vice versa.
for bias if a method falls into either the good region regular numbers or the win win region bold numbers the bias difference value will be provided.
if it falls into any other region the region type will be indicated.
the values highlighted in bluedenote the most effective bug fixing method.
the data from this table is divided and analyzed in depth in tables .
table proportionof fair automl auto sklearn andmitigationtechniques that fallinto eachmitigationregion fairness metric dataset di spd eod aod adultcensus bank marketing germancredit titanic method lose bad inv good win lose bad inv good win lose bad inv good win lose bad inv good win lose bad inv good win lose bad inv good win lose bad inv good win lose bad inv good win t10 t20 t36 t40 avg2 as38 r0 dir13 pml25 eo0 fax0 roc6 avg7 the proportions in this table are determined based on the data presented in table proportion for fairness metric buggy cases of a metric fall into a region buggy cases of that metric proportionfor dataset buggy cases fall of adatasetintoaregion buggy cases of that dataset.
itsautomaticoptimizationofthebestmlmodelforagivendataset.
wetailored auto sklearn tobetterfitourmethodintwoways its search space was restricted to the type ofthe faulty classifier for example if the faulty classifier is random forest auto sklearn willonly optimize the hyperparameters and identify complementary components for that specific classifier.
the faulty model was setasthedefaultmodelfor auto sklearn .thesemodificationsare features of auto sklearn that we utilized.
509fix fairness don t ruinaccuracy performance aware fairness repairusingautoml esec fse december3 san francisco ca usa methodologyconfiguration .weselectedanincrementvalue of u1d6fcfor u1d6fdof0.
to balancethe timebetween u1d6fdsearch andmodel fixingprocesses.theusercanoptforamoreaccuratevalueof u1d6fd by decreasing theincrement value and using a longersearch time.
to conduct search space pruning we ran fair automl times n with a hour search time t to gather the best ml pipelines .
from each run we collected the top pipelines k resulting in 100modelsperinput.thispre builtsearchspaceincludesasetof hyperparametersandthetop3mostfrequentlyusedcomplementarycomponents m .wehaveexploredotherparametersettings but thesehave proven to provideoptimal results.
evaluation configuration .we evaluate each tool on each buggy scenario times using a randomre split ofthedatabased on a train test split ratio .
the runtime for each run of fairautomlandauto sklearn isapproximatelyonehour .the meanperformanceofeachmethodiscalculatedastheaverageof the runs which is a commonly used practice in the fairness literature .
ourevaluation targets fixing 16buggy models for 4fairnessmetrics resultinginatotalof64 buggy cases.
.
effectiveness rq1 we evaluate the effectiveness of fair automl by comparing it with auto sklearn and existing bias mitigation techniques based on faireabaseline.
the comparisons are based on the following rules rule1 amodelisconsideredsuccessfullyrepairedwhen its post mitigation mean accuracy and fairness falls into win win goodtrade offregions.
rule2 amodelthatfallsinthewin winregionisalways betterthanone fallingintoany otherregion.
rule3 iftwomodelsareinthesametrade offregion the one withlower biasispreferred.
our comparison rules for bug fixing performance were established based on fairea and our evaluations.
firstly we define a successful bug fix as a fixed model that falls within the win win orgoodtrade offregions astheseregionsdemonstrateimproved fairness accuracy trade offs compared to the baseline in fairea.
secondly whencomparingsuccessfullyfixedmodelsindifferent trade offregions win winversusgood we consider thewin win modelstobesuperior as theyoffer improvedfairnessandaccuracy.
lastly for models that fall within the same trade off region the one with lower bias is deemed to be better as our goal is to fix unfair models.
our evaluations then consider two aspects of the bug fixingperformance thenumberofsuccessfulbugfixesandthe number oftimes abiasmitigationmethodoutperforms others.
.
.
is fair automl effective in fixing fairness bugs?
the results presented in table 4show that fair automl was effective in resolving out of fairness bugs while auto sklearn only fixed out of and bias mitigation techniques resolved up to out of .
this indicates that auto sklearn alone was not effective in reducing bias however our methods were successful in enhancing automl to repair fairness bugs.
moreover fair automl was able to repair more cases than other bias mitigationtechniques whichoftenresultedinloweraccuracyforlowertable fair automl fa vs bias mitigationmethods in fixing fairness bugs faasrdirpmleofaxroc bugs fixed best models theresultsinthistablearederivedfromthedatapresentedintable .therow bugs fixedindicates the number of cases where the technique falls into either the win win orgood trade offregion.
the row bestmodels represents the number of instances whereabiasmitigation technique outperforms allothermethods.
bias.
this highlights the effectiveness of our approaches in guiding automl towards repairing models for better trade off between fairnessandaccuracycomparedto the fairea baseline.
.
.
does fair automl outperform bias reduction techniques?
fair automl demonstrated superior performance in fixing fairness bugs compared to other bias mitigation techniques.
the results presented in table 4indicate that out of buggy cases were fixed byfair automl auto sklearn or bias mitigation techniques.
among the repaired buggy cases fair automl outperformed other techniques times .
on the other hand auto sklearn outperformedfair automl andbiasmitigationtechniquesonly4times andbiasmitigationtechniquesoutperformedothertechniques times at most .
this highlights that fair automl is often more effective inimproving fairness and accuracy simultaneously orreducing more biasthanotherbiasmitigationtechniques.
.
adaptability rq2 toassessthe adaptabilityof fair automl we measure the proportionsofeachevaluatedtoolsthatfallintoeachfairness accuracy trade off region in different categories fairness metric and dataset .tofurtherevaluatetheadaptabilityof fair automl insteadofusingourpreparedmodelsanddatasets weusedthebenchmark ofparfait mltoevaluate fair automl .particularly we evaluatefair automl andparfait mlonthreedifferentmlmodels decisiontree logisticregression randomforest ontwodatasets adultcensus andcompas table 5andfigure .
.
.
isfair automlmoreadaptablethanexistingbiasmitigation techniquesandauto sklearn?
table3showsfair automl demonstratesexceptionalrepaircapabilitiesacrossvariousdatasetsand fairnessmetrics withahighrateofsuccessinfixingbuggymodels.
forexample inthe adult census bank marketing germancredit andtitanicdatasets fair automl t4 repaired100 and ofthemodels respectively.similarly inthedi spd eod and aod fairness metrics fair automl t4 achieved repair rates of and .
on the other hand bias mitigation methodsoftenshowinconsistentresults.forinstance equalizedodds repairedallbuggycasesin adultcensus butnonein bankmarketing .
in fact our methods effectively guides automl in hyperparameter tuningtoreducebias leadingtosuperiorrepairperformanceacross differentdatasets andmetrics.
.
.
isfair automleffectiveinfixingfairnessbugsonotherbias mitigation methods benchmark?
based on evaluation of parfaitml we only use accuracy and eod as evaluation metrics for this evaluation.
to make a fair comparison with parfait ml weutilizetheversionof fair automl thatincorporateseodand accuracy as its cost function t3 .
the results are displayed in table5 showcasing the accuracy and bias eod achieved by both 510esec fse december3 san francisco ca usa giangnguyen sumonbiswas andhrideshrajan table accuracy and fairness achieved by fair automl and pafait ml on pafait ml s benchmark datadecisiontree logisticregression randomforest t3 pml t3 pml t3 pml acceodacceodacceodacceodacceodacceod adult0.
.
.
.
.
.
.
.
.
.
.
.
compas .
.
.
.
.
.
.
.
.
.
.
.
figure accuracyandfairness achieved by fair automl green circle andpafait ml orangecircle with decision tree left andlogisticregression right on adult dataset pafait ml sbenchmark .
the blue line showsthefairea baseline andred linesdefine thetrade off regions.
fair automl t3 and parfait ml in parfait ml sbenchmark.
the table showcases the actual results of the repaired models rather thanthedifferenceinaccuracy fairnessbetweentheoriginaland repaired models.
upon inspection the results for the compas datasetfor both fair automl andparfait ml are similar.
however for the adult dataset some differences arise.
for instance with the random forest classifier fair automl performs better than parfait mlinbothaccuracyandeod.withthelogisticregression classifier fair automl achievedahigheraccuracybuthigherbias comparedtoparfait ml.nevertheless fair automl fallsintothe win win trade off region while parfait ml only falls into good trade offregion figure .withthedecisiontreeclassifier both fair automl andparfait mlfall into the win wintrade off region figure3 however parfait ml performed better since it has lower bias.
these results highlights the generalization capability of fairautomlto repairvariousdatasets andml models.
.
ablation study rq3 wecreateanablationstudytoobservetheefficiencyofthedynamic optimizationfunctionandthesearchspacepruningseparately.the ablationstudy compares the performance ofthe following tools auto sklearn as represents automl.
fair automl version fav1 represents automl dynamic optimization function.
fair automl version fav2 represents automl dynamic optimization function searchspacepruning.
to evaluate the efficiency of the dynamic optimization function wecomparethe performanceof fav1withauto sklearn .wecomparefav1withfav2toobservetheefficiencyofthesearchspace pruning approach.
the complete result is shown in table .
notice thatweuse fair automl tooptimizedifferentfairnessmetrics thus we only consider the metric that each tool tries to optimize.
for instance theresultsofrandomforestonadultdatasetinthetable 6showsthatachievedscoresof0.096fordi .014forspd .024for eod and .
for aod.
this result means that t1 achieves .
for di t2 achieves .
for spd t3 achieves .
for eod t4 achieves0.035foraod.theevaluationonlyconsiderscaseswheretable trade offassessmentresults of auto sklearn fav1 andfav2 metric model asfav1fav2model asfav1fav2 di .
.
.
.
.
.
spd .
.
.
.
.
.
eod .
.
.
lose0.
.
aodrf .
.
.035lrg .
.
.
di .
.
.
invbad0.
spd .
.
.
invbad0.
eod lose0.
.
inv0.
.013adultcensus aodxgb .
.
.053gbc .
inv0.
di .
.
.
loseinv0.
spd lose0.
bad loseinv0.
eod loseinvlose loselose0.
aodrf .
.
.032xgb2 lose0.
.
di loseinv0.
lose0.
.
spd lose0.
.
loselose0.
eod loseinv0.
loseloseinvbank marketing aodxgb1 loselose0.046gbc lose0.
.
di losebadbad .
.
.
spd losebad0.
.
.
.
eod .033bad0.
.
.
.
aodrf loseloselosesvc lose0.
.
di badbad0.
.
inv0.
spd badlose0.
.
.
.
eod .036lose0.
.
.
.085german credit aodxgb loselose0.037knn loseinv0.
di lose1.
.
.
.
.
spd lose0.
.
inv0.
.
eod lose0.
.
.
.
bad aodrf .
.
.601gbc .
.
.
di .
.
.
lose1.
.
spd .
.
.
lose0.
.
eod lose0.
.
lose0.
.285titanic aodlrg .
.
.179xgb lose0.
.
the data in table 6iscreated in the same waysas table .
foreachmethod in the good trade offregionand win win region bold number a trade offmeasurement valueisgiven for otherregions the regiontypeis displayed.the values in blue orange and blackindicate the top1 top2 top3 bugfixing tools respectively.
thetoolssuccessfullyrepairthebug.thesamerulesdescribedin rq1 isappliedinthis evaluation.
.
.
aredynamicoptimizationfunctionandsearchspacepruning effectiveinfixingfairnessbugs?
fromtable ourresultsshowthat the dynamic optimization function approach in fair automl helps fix buggy models more efficiently.
comparing the performance in fixing fairness bugs fav1outperforms auto sklearn times whileauto sklearn outperforms fav1only7times.thesearchspace pruningapproachin fair automl alsocontributestomoreefficient bug fixing as fav2outperforms both fav1andauto sklearn and 55timesrespectively while fav1andauto sklearn onlyoutperform fav214 and4times respectively.
discussion in this work we bring particular attention to the fairness accuracy tradeoff while mitigating bias in ml models.
many works in the area only optimize fairness metrics by sacrificing accuracy and do not consider the tradeoff rigorously.
however as shown by recent work trivialmutationmethodscanalsoachievefairnessifaccuracy is compromised in different magnitudes.
therefore a rigorous evaluation method is necessary to demonstrate that the tradeoff is beneficial.anotherlimitationofexistingtoolsisnotgeneralizing over different ml classifiers e.g.
lrg gbc rf xgb multiple fairnessmetrics anddatasetcharacteristics.
tothatend weleveraged the recent progress of automl in the context and achieved better tradeoff than sota methods.
we believe that our approach 511fix fairness don t ruinaccuracy performance aware fairness repairusingautoml esec fse december3 san francisco ca usa is versatile and can be applied to various ml problems.
particularly thedynamicoptimizationfunctionapproachremainsversatile across various datasets and models.
furthermore the search space pruning approach is refined through pre constructed database and amatchingmechanism thatcapitalizesondiversedatasetsstored inrepositoriessuch as openmlorkaggle.
we implemented fair automl on top of auto sklearn to ensure itswideapplicabilityonmlalgorithms.state of the artbiasmitigationtechniquesalsoprimarilyuseclassicmlalgorithms that are supported by auto sklearn .
these models are more suitable than the dl models since the fairness critical tasks in prior works commonly use tabular datasets.
should one desire to explore alternative model types not directly supported byauto sklearn they can adopt the general ml model adoption of auto sklearn .
our approach also outlines several opportunities towards leveraging automl and search based software engineering to ensure fairnessinnewmlmodelsthatarebecomingavailable.first the greedyweightidentifieralgorithm sperformancemightsufferfor complex models due to computational costs algorithm .
second searchspacepruningquantitativelyestimatesthesimilarity ofdatasetsbasedondatacharacteristics.thus ifwedonothave a dataset similar enough to the input dataset automl may not perform well.
to address this we plan to regularly update our database with new datasets.
lastly constructing suitable search spaces particularly for resource intensive methods like deep learning couldentailsignificantcomputationalexpenses.furtherworks are needed to maximize the versatility and effectiveness of our approach over novel fairness critical tasks.
one key direction is to combinefair automl withotherbiasmitigationtechniques such as integrating fair automl smodel with pre processingbiasmitigationmethodsto enhanceoverallpipeline fairness.
additionally integrating fair automl with ensemble learning could improve both performance and fairness by capturing a broader range of biases and patterns.
these directions could significantly amplify the impact of this work making fair automl a potent tool for promotingfairness and equityin machinelearning acrossvarious domains.
threats to validity constructvalidity.
thechoiceofevaluationmetricsandexisting mitigationtechniquesmayposeathreattoourresults.wemitigate this threat by employing a diverse range of metrics and mitigationmethods.first wehaveusedaccuracyandfourmostrecent andwidely usedfairnessmetricstoevaluate fair automl andthe state of the art.
these metrics have been commonly applied in the software engineering community .
second we demonstratethesuperiorityof fair automl overstate of the art methods in different categories pre processing in processing and post processing which are most advanced techniques from the se andmlcommunities.forevaluatingfairnessandapplyingthese mitigation algorithms except parfait ml we have used aif toolkit.
for evaluating parfait ml we have used its original implementation.
we create a baseline using the original fairea implementation enabling us to conduct a comprehensive comparisonbetweenourapproachandexistedmitigationmethods.inthefuture weintendtoexploresupplementaryperformancemetricsand extendouranalysistoincorporateadditionalmitigationtechniques for amore comprehensive evaluation.
externalvalidity.
toensureanequitablecomparisonwithcuttingedge bias mitigation techniques we leverage a diverse array of real world models datasets and evaluation scenarios.
particularly we utilize a practical benchmark comprising real world models thoughtfullycuratedbypriorresearch .then thesemeticulously chosenmodelsundergoevaluationusingfourextensivelystudied datasets in the fairness literature .
we conducted experimentsunderidenticalsetupsandsubsequentlyvalidatedour findings .
in addition to assessing fair automl against alternative methods within our established settings and benchmarks we subject fair automl to evaluation using the parfait ml benchmark aleading edge biasmitigationframework.
internal validity.
implementing fair automl on top of autosklearnmay introducea threat to itsactual bias mitigation performance.in other words the favorableoutcomesachievedby fairautomlcould be attributed to its integration with auto sklearn .
to address this threat we evaluated auto sklearn on various benchmarks comparingitsperformancewith fair automl andwithout auto sklearn our proposed approaches to gauge the effectiveness offair automl .
conclusion we present fair automl an innovative system that enhances existingautomlframeworkstoresolvefairnessbugs.thecoreconcept offair automl istooptimizethehyperparametersoffaultymodels to resolve fairness issues.
this system offers two novel technical contributions a dynamic optimization function and a search space pruningapproach.thedynamicoptimizationfunctiondynamically generates an optimization function based on the input enabling automltosimultaneouslyoptimizebothfairnessandaccuracy.the searchspacepruningapproachreducesthesizeofthesearchspace basedontheinput resultinginfasterandmoreefficientbugrepair.
our experiments show that fair automl outperforms auto sklearn and conventional bias mitigationtechniques witha higherrate of bug repair and a better fairness accuracy trade off.
in the future we plan to expandthe capabilitiesof fair automl to include deep learningproblems beyondthe scope of the currentstudy.
data availability toincreasetransparencyandencouragereproducibility wehave made our artifact publicly available.
all the source code and evaluationdata withdetaileddescriptionscan be foundhere .