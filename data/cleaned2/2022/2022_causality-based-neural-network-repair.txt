causality based neural network repair bing sun singapore management universityjun sun singapore management university long h. pham singapore management universityjie shi huawei singapore abstract neural networks have had discernible achievements in a wide range of applications.
the wide spread adoption also raises the concern of their dependability and reliability.
similar to traditional decisionmaking programs neural networks can have defects that need to be repaired.
the defects may cause unsafe behaviors raise security concerns or unjust societal impacts.
in this work we address the problem of repairing a neural network for desirable properties such as fairness and the absence of backdoor.
the goal is to construct a neural network that satisfies the property by minimally adjusting the given neural network s parameters i.e.
weights .
specifically we propose care causality based repair a causality based neural network repair technique that performs causality based fault localization to identify the guilty neurons and optimizes the parameters of the identified neurons to reduce the misbehavior.
we have empirically evaluated care on various tasks such as backdoor removal neural network repair for fairness and safety properties.
our experiment results show that care is able to repair all neural networks efficiently and effectively.
for fairness repair tasks care successfully improves fairness by .
on average.
for backdoor removal tasks care reduces the attack success rate from over to less than .
for safety property repair tasks care reduces the property violation rate to less than .
results also show that thanks to the causality based fault localization care s repair focuses on the misbehavior and preserves the accuracy of the neural networks.
acm reference format bing sun jun sun long h. pham and jie shi.
.
causality based neural network repair.
in 44th international conference on software engineering icse may pittsburgh pa usa.
acm new york ny usa pages.
introduction neural networks have had discernible achievements in a wide range of applications ranging from medical diagnosis facial recognition fraud detection and self driving .
while neural networks are demonstrating excellent performance there has been a growing concern on whether they are reliable and dependable.
similar to traditional decision making programs neural networks inevitably have defects and need to be repaired at times.
neural permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
are usually inherently black boxes and do not provide explanations on how and why decisions are made in certain ways.
as a result these defects are more hidden compared to traditional decision making programs .
it is thus crucial to develop systematic ways to make sure defects in a neural network are repaired and desirable properties are satisfied.
similar to the activity known as debugging for traditional software programs there is often a need to modify a neural network to fix certain aspects of its behavior whilst maintaining other functionalities .
existing efforts to repair the unexpected behavior of neural networks often focus on retraining with additional data .
although retraining is natural and often effective retraining a neural network model could be difficult and expensive for real world applications .
more importantly unlike debugging traditional software program where we can be reasonably certain that the bug is eliminated after the fix there is no guarantee that the retrained model eliminates the unwanted behavior.
therefore techniques for modifying an existing model without retraining will be preferable in certain scenarios.
that is we sometimes would like to apply a small modification on an existing neural network to remove unexpected behaviors whilst retaining most of its well trained behaviors.
neural networks may misbehave in different ways.
given a model trained primarily for accuracy it could be discriminative i.e.
the prediction is more favourable to certain groups thus violating fairness property.
in this situation small adjustment can be applied to the trained network without retraining i.e.
to improve its fairness whilst maintaining the model s accuracy.
in another scenario malicious hidden functionalities backdoor could be easily embedded into a neural network if the training data is poisoned .
such a backdoor produces unexpected behavior if a specific trigger is added to an input whilst presenting normal behavior on clean inputs.
in this scenario one would like to repair the neural network by removing the backdoor while maintaining the correct behavior on clean inputs.
a further scenario is that a trained model could violate safety critical properties on certain inputs e.g.
the output is not within the expected range .
in this case we would like to repair the network by making adjustments to its parameters so that the repaired model satisfies the specified property.
for both traditional software programs and neural networks debugging and repairing are essentially reasoning over causality.
for traditional software programs the notion of causality is natural and well defined e.g.
based on control and data dependency.
however the same is not true for neural networks i.e.
a wrong decision could easily be the collective result of all neurons in the network and yet considering that all of them are responsible and should be modified is unlikely to be helpful.
although causality analysis has been applied to interpret machine learning models and verification existing causality analysis typically focuses ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa bing sun jun sun long h. pham and jie shi on the causal relation from the input to the model prediction and not the hidden neurons.
hence it is still not clear how to apply existing definitions of causality for repairing neural networks.
in other words how to repair a neural network to remedy defects based on causality is still largely an open problem.
in this work we introduce care causality based repair a general automatic repair algorithm for neural networks.
instead of retraining care applies minor modifications to a given model s parameters i.e.
weights to repair known defects whilst maintaining the model s accuracy.
the defects are defined based on desirable properties that the given neural network fails to satisfy.
care is a search based automated program repair technique that first performs fault localization based on causal attribution of each neuron i.e.
it locates those neurons that have the most contribution to the model s defects.
secondly care performs automatic repair by adjusting model parameters related to the identified neurons until the resultant model satisfies the specified property whilst maintaining accuracy .
we summarize our contributinos as follows.
we propose and implement care as a general automatic repair algorithm that applies causality based fault localization and pso optimization to all layers of neural networks.
we demonstrate the effectiveness of care in the context of three different tasks fairness improvement backdoor removal and safety property violation repair.
we empirically evaluate care on multiple neural networks including feed forward neural networks ffnns and convolutional neural networks cnns trained on multiple benchmark datasets.
the results indicate that care improves the models performance over the specified properties significantly and care outperforms existing approaches proposed for neural network repair.
the remainder of this paper is organized as follows.
in section we review relevant background and define our problem.
in section we present each step of our approach in detail.
in section we evaluate our approach through multiple experiments to answer multiple research questions.
we review related work in section and conclude in section .
preliminary in this section we review relevant background and define our research problem.
.
neural network properties there are many desirable properties we would like a trained neural network to satisfy besides meeting its accuracy requirement.
in this work we assume to be one of the following three kinds of properties and show that care can handle all of them.
fairness fairness is a desirable property that potentially has significant societal impact .
due to the fact that machine learning models are data driven and the training data could be biased or imbalanced models trained based on such data could be discriminative .
in this work we define independence based fairness following as follows.definition .
independence based fairness .
letnbe a neural network and be a positive real value constant.
we write yas the prediction of non a set of input features xandlas the prediction set.
we further write f xas a feature encoding some protected characteristics such as gender age and race.
nsatisfies independence based fairness with respect to if and only if l l fi fj f suchthati j p y l f fi p y l f fj intuitively definition .
states that nis fair as long as the probability difference of a favourable prediction for instances with different values of protected feature is within the threshold .
absence of backdoor with the wide adoption of neural networks in critical decision making systems sharing and adopting trained models become very popular.
on the other hand this gives attackers new opportunities.
backdoor attack is one of the neural network attacks that often cause significant threats to the system.
backdoor is a hidden pattern trained into a neural network which produces unexpected behavior if and only if a specific trigger is added to an input .
in classification tasks the backdoor misclassifies an arbitrary inputs to the same target label when the associated trigger is applied to the input.
hence another desirable property of a neural network would be backdoor free where the backdoor attack success rate is kept below a certain threshold.
definition .
backdoor attack success rate .
letnbe a backdoored neural network tbe the target label xbe a set of adversarial inputs with the trigger and n x be the prediction on x. we say attack success rate sr is sr t p n x t x x intuitively attack success rate is the percentage of adversarial inputs classified into the target label.
next we define backdoor free neural network.
definition .
backdoor free neural network .
letnbe a backdoored neural network and be a positive real value constant.
n satisfies backdoor free property with respect to andt ifsr t .
safety for neural networks applied on safety critical systems such as the airborne collision avoidance system all safety properties must be satisfied strictly.
however due to reasons like limited training data and insufficient training those critical properties could be violated.
hence another desirable behavior of a neural network is to satisfy all safety properties or the violation rate is kept below a certain threshold.
definition .
safety property violation rate .
letnbe a neural network xbe a set of inputs and n x be the prediction on x. let be the critical safety property that nis expected to satisfy.
we say the property violation rate vr is vr p n x x x intuitively violation success rate is the percentage of inputs that violate the property .
next we define safety property violationfree neural network.
definition .
safety property violation free neural network .
letnbe a neural network and be a positive real value authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
causality based neural network repair icse may pittsburgh pa usa constant.nis safety property violation free with respect to and ifvr .
example .
.
we train a feed forward neural network non census income dataset refer to details of the dataset in section .
to predict whether an individual s income exceeds 50k per year.
the neural network is configured to have hidden layers.
we use this network as the running example in this paper.
in this example we focus on a fairness property w.r.t.
protected feature gender i.e.
the model is considered unfair if the probability difference of a favourable prediction for females and males is greater than .
note that this is for illustration purpose only and such a threshold is probably too strict for real world applications.
given this neural network the problem is to find a repaired network mwith minimal adjustment to n s weight parameters so that msatisfies the fairness property whilst maintaining its accuracy.
.
our problem we are now ready to define our problem.
definition .
neural network repair problem .
letnbe a neural network sbe the input space and be one of the abovementioned desirable properties.
the neural network repair problem is to construct a neural network msuch thatmsatisfies in the input spacesand the semantic distance between nandmis minimized.
we define semantic distance between two neural networks n andmas follows.
definition .
semantic distance .
letn x be the prediction of neural network non inputxwherex s the semantic distance betweenn andm is defined as p n x m x intuitively the semantic distance between two neural networks is the probability that the predictions of the two models are different on the same input.
in this work we use the model accuracy difference of nandmas a measure of their semantic distance.
our approach in this section we present the details of our approach.
an overview of our framework is as shown in figure and algorithm shows the details of our approach.
the first step is to verify whether the given neural network nsatisfy property .
if is satisfied care terminates immediately.
otherwise care proceeds to the next step.
it performs causality based fault localization on all hidden neurons ofnand identifies those neurons that have the most contribution to n s unwanted behavior.
the third step is to optimize these neurons weights and generate a repaired network msuch thatmsatisfies the property and is semantically close to n. .
property verification in this step we verify neural network nagainst given property .
ifnsatisfies care returns nunchanged and terminates.
otherwise care proceeds to repair the neural network.
property verification is not the focus of this work and we adopt existing neural network verification techniques.
recently there have been multiple tools and approaches for solving the above mentionedstart verification satisfied?fault localizationoutput results repairstop yes no figure an overview of our framework algorithm care n s 1verify property onn 2perform accuracy check on n 3if is verified then return verified 5forall hidden neuron xinndo calculateacey do x 7sortace for allx 8candidate neurons top 9do 10candidatem pso searching verify property oncandidatem if is verified then break iffailed to find a better location in last consecutive search then break 16while iteration 17m candidatem 18perform accuracy check on m 19ifacc n acc m threshold then return significant accuracy drop!
21verify property on m 22if is verified then returnm 24else return not able to repair the network!
neural network verification problems.
we omit the details on how different properties are verified and refer the readers to .
in our implementation care s fairness verification algorithm is based on backdoor success rate verification is based on the method proposed in and safety property verification is based on the approach proposed in .
example .
.
in our running example care verifies nagainst the fairness property .
the resultant fairness score i.e.
the probability difference of a favourable prediction for females and males is .
.
therefore nfails to satisfy the fairness property which requires the fairness score to be within .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa bing sun jun sun long h. pham and jie shi .
causality analysis in this step we perform causality based fault localization.
the goal is to identify a relatively small set of neurons to repair.
note that in this work we restrict ourselves to repair by adjusting the parameters of existing neurons only i.e.
without introducing or pruning neurons.
the number of parameters in a neural network is often huge and even relatively small neural network may consist of thousands of weight and bias parameters.
therefore attempting to optimize all neural weights would be costly and hard to scale.
care thus adopts a fault localization method to identify neurons that are likely to be responsible for the undesirable behaviors.
this method is based on causality analysis carried out over all hidden neurons of n. an alternative method for fault localization is gradient based method which is to measure how much perturbing a particular neuron would affect the output .
care is designed to capture the causal influence of a hidden neuron on the performance on the given property.
in contrast gradient based method draws conclusion based on statistical correlations which is not ideal for model repair since our aim is to identify neurons that cause the defect .
next we describe our causality based fault localization in detail.
in recent years causality has gained increasing attention in interpreting machine learning models .
multiple approaches have been designed to explain the importance of the components in a machine learning model when making a decision based on causal attributions.
compared with traditional methods causal approaches identify causes and effects of a model s components and thus facilitates reasoning over its decisions.
in the following we review some concepts which are necessary for the causality analysis in this work.
definition .
structural causal models scm .a structural causal model scm is a tuple m x u f pu where x is a finite set of endogenous variables u denotes a finite set of exogenous variables f is a set of functions f1 f2 ... fn where each function represents a causal mechanism such that xi x xi fi pa xi ui wherepa xi is a subset of x xi ui uandpuis a probability distribution over u. scm serves as a framework for representing what we know about the world articulating what we want to know and connecting the two together in a solid semantics .
it plays an important role in causality analysis and is commonly applied in many studies .
figure shows an example causal graph to study the efficiency of a medication on a disease where the nodes represent variables and the edges represent cause effect relations intuitively.
in this graph age is considered as an exogenous variable i.e.
confounder patient s heart rate cholesterol level and whether the medication is applied or not are endogenous variables i.e.
whether the medication is applied or not is often considered as the treatment .
the outcome is the recovery rate of a patient.
as illustrated in the graph age affects the patient s health conditions such as heart rate and level of cholesterol.
furthermore the need or feasibility of applying this medicine on patients is affected by age i.e.
young people may not necessarily take the medicine and patients above years old are too risky to take the medicine.
patient s health condition and the application of medication affect the recovery rate.
furthermore agecholesterolheart rate medicationreco very rate figure an example causal graph age can affect the recovery rate directly since younger patient often recover faster than the elderly.
in this work we model neural networks as scms to analyze the causal relationship between hidden neurons and model s predictions.
to debug and repair a neural network we would like to measure the contribution of each neuron to the network misbehavior which is referred as the causal effect.
definition .
average causal effect .
the average causal effect ace of a binary random variable x e.g.
treatment on another random variable y e.g.
outcome is defined as follows.
ace e e wheredo operator denotes the corresponding interventional distribution defined by scm.
intuitively ace is used to measure the causal effect of xonyby performing intervention on x. there are many other causal effect metrics in the literature such as average treatment effect ate i.e.
ate e wherey represents the potential outcome if a treatment is applied exogenously and y represents the corresponding potential outcome without treatment.
ate measures the effect of the treatment at the whole population level conditional average treatment effect i.e.
cate e y y x0 which is the average treatment effect conditioned on a particular subgroup of x i.e.
x x0 and effect of treatment on the treated i.e.
ett e e which measures the probability of ywould beyhad xbeenx1counterfactually given that in the actual world x x0 .
in this work we focus on the ace metric since we are interested in measuring the causal effect of the hidden neurons on certain output value.
.
fault localization in the following we present details on how causality analysis is used for fault localization in neural networks.
firstly the neural networknis modeled as an scm.
as proposed in neural networks can be interpreted as scms systematically.
in particular feed forward neural networks ffnns and convolutional neural networks cnns can be represented as directed acyclic graphs with authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
causality based neural network repair icse may pittsburgh pa usa o1 onexogenous input hidden ouput figure ffnn as an scm edges from an earlier i.e.
closer to the input layer layer to the next layer until the output layer.
the following is a proposition.
proposition .
.
ann layer ffnn or cnn n x1 x2 ... xn where xirepresents the set of neurons at layer i can be interpreted by scm m u pu wherex1represents neurons at input layer and xnrepresents neurons at output layer.
corresponding to every xi firepresents the set of causal functions for neurons at layeri.urepresents a set of exogenous random variables that act as causal factors for input neurons x1andpuis a probability distribution overu.
proof.
in the scenario of ffnn the proof of proposition .
follows that provided in .
in the scenario of cnn similar to ffnn neurons at each layer can be written as functions of neurons at its previous layer i.e.
i xij xi xij fij xi wherexijrepresents thejthneuron at layer i. neurons at input layer x1 can be assumed to be functions of independent noise variables usuch that x1j x1anduj u x1j f1j uj .
thus a cnn can be equivalently expressed by a scm m u pu .
figure depicts the scm of a layer ffnn.
the dotted circles represent exogenous random variables which act as causal factors for the input neurons.
in this work we assume that neurons at the input layer are not causally related to each other but can be jointly caused by a latent confounder.
next we define the attribution problem i.e.
what is the causal influence of a particular hidden neuron on model defect.
recall that in definition .
we define the ace of a binary variable xon output variable y. however we cannot apply the definition directly for two reasons.
first the domain of neural networks neurons is mostly continuous not binary valued.
second we are interested in finding causal effects on the model defect rather than certain output variable.
hence we propose the following definition.
definition .
causal attribution .
we denote the measure of the undesirable behavior of given neural network nas y. the causal attribution of a hidden neuron xton sdefectyis acey do x e next we calculate the interventional expectation of ygiven intervention do x and we thus have the following.
e yyp y do x dy intuitively causal attribution measures the effect of neuron xbeing value ony.
we evaluate equation by sampling inputs according to their distribution whilst keeping the hidden neuron x and computing the average model undesirable behavior y. in this work we calculate yaccording to the desirable property .
letnt xip be the prediction value of class ton inputxipwe have for fairness repair we measure model unfairness yby taking the difference of the prediction value on favourable classtw.r.t.
samples that only differ by the sensitive feature.
letxipandx ipbe a pair of discriminatory instances that only differ by the sensitive feature .
we have yfair nt xip nt x ip and we calculate aceyf air do x as the causal attribution of neuron xw.r.t.
fairness property.
for backdoor removal we measure yby calculating the prediction value on target class t i.e ybd nt xip .
thus we aim to calculate aceybd do x for all hidden neurons.
note xipcan be clean inputs or adversarial inputs.
for safety property violation repair we measure yby calculating the prediction value on desirable class t i.e.
ysafe nt xip .
we calculate aceysaf e do x as the causal attribution for hidden neuron x. for other properties care is able to calculate the causal attribution of each neuron on yas long as the corresponding yis specified.
thus by calculating the causal attribution of each hidden neuron ony we are able to identify neurons that are most likely the cause to the unexpected behavior.
afterwards the identified neurons are used as candidates for model repair in our next step.
the time spent on this step depends on the total number of neurons nto analyze in the given neural network.
the time complexity for causality based fault localization is thus o n .
example .
.
in our running example care conducts causalitybased fault localization on all hidden neurons of nto identify neurons that are the most responsible to n sprediction bias.
care generates .4k samples to analyze all neurons and the total time taken is 120s.
it is observed that a few neurons such as the 11thneuron at 3rdhidden layer and the 3rdneuron at 5thhidden layer have a contribution which is outstanding .
this is good news to us as it implies that it is possible to improve the fairness by modifying the weights of only a few neurons.
care then selects the top of total number of neurons i.e.
in total for this network to be used as repair candidates.
.
network repair next we present how care performs neural network repair.
similar to traditional software programs neural networks can go wrong and incorrect behaviors should be fixed.
while existing efforts mainly focus on how to retrain the neural network to repair the unexpected behaviors there are a few recent works that address this problem by modifying the parameters of the network.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa bing sun jun sun long h. pham and jie shi1 1 1 24 1 33 1 45 2 7 2 11 2 18 2 25 3 7 3 113 12 4 3 5 300.51normalized causal attribution figure causal attribution of neurons in the example in the following we first briefly explain why these approaches do not apply in our setting.
goldberger et al.
propose a verificationbased method to adjust the neural weights of the output layer only.
their approach has limited effectiveness in repairing neural networks as demonstrated in .
their heuristics i.e.
focusing on neurons in the output layer is not justified from our point of view as the neurons causing the problematic behaviors may not be in the output layer.
in an approach is proposed to repair a model through inductive synthesis.
their approach has scalability issues as a large model size leads to a large search space.
while nnrepair provides a constraint based approach to fix the logic of a neural network at an intermediate layer or output layer its fault localization and repair applies to a single layer only but the actual fault could be across multiple layers.
thus its repair performance may not be ideal and we provide a comparison in section .
.
in our approach we propose to address the repair problem through optimization i.e.
based on the particle swarm optimisation pso algorithm.
besides pso there are many other optimization algorithms.
genetic optimization algorithms genetic algorithm for example usually do not handle complexity in an efficient way since the large number of elements undergoing mutation causes a considerable increase in the search space.
in comparison pso requires smaller number of parameters and thus has lower number of iterations.
another type of optimization is stochastic optimization algorithm.
markov chain monte carlo mcmc as an representative uses a sampling technique for global optimization.
however mcmc often encounter either slow convergence or biased sampling .
hence in this work we select intelligence based algorithm pso.
the idea is to search for small adjustments in weight parameters of the neurons identified in the previous step such that the specified property is satisfied.
pso simulates intelligent collective behavior of animals such as schools of fish and flocks of birds.
it is known to be particularly effective for optimization in continuous domains .
in pso multiple particles are placed in the search space.
at each time step each particle updates its location xiand velocity viaccording to an objective function.
that is the velocity is updated based on the current velocity vi the previous best location found locally piand the previous best location found globally pg.
its location is updated based on the current location and velocity.
we writer c to denote a random number uniformly sampledfrom the range of .
formally the pso update equation is as follows .
vi vi r c1 pi xi r c2 pg xi xi xi vi where c1andc2represent inertia weight cognitive parameter and social parameter respectively.
in pso the fitness function is used to determine the best location.
in care the weights of the identified neurons are the subject for optimization and thus are represented by the location of the particles in pso.
the initial location of each particle is set to their original weight and the initial velocity is set to zero.
as defined in section our problem is to repair a given neural networknagainst property while minimizing the accuracy drop.
therefore two essential components need to be considered in the optimization process model performance on property and model accuracy.
in care we measure the model performance based on the property specified.
formally the fitness function of pso is defined as follows.
fitness ub accuracy where ub undesirable behavior is a quantitative measure on the degree of violating property constant parameter determines the relative importance of the accuracy.
for fairness improvement task we randomly sample a large set of instances andubis measured by the percentage of individual discriminatory instances within the set i.e.
let n x be the prediction of xand x x be a pair of discriminatory instances in the sample set ub p n x n x for backdoor removal task we add backdoor trigger if it is known to all samples in testing set and measureub sr t following definition .
for safety task we randomly sample a large set of instances and measure ub vr following definition .
.
intuitively our optimization target is to make the repaired model satisfy given property while maintaining the accuracy.
note that for other properties care is able to conduct repair as long as a performance indicator is specified based on the given property properly.
example .
.
in our running example care applies pso to generate a repair network mthat will improve the fairness performance ofn.
the weights of the identified neurons in the previous step are the subjects for the optimization.
we set to0.8so that pso can optimize for fairness without sacrificing model accuracy too much.
pso terminates at 17thiteration where no better location is found in the last consecutive iterations.
by adjusting neural weights accordingly mis generated.
care performs fairness and accuracy check on mand the maximum probability difference is .007with accuracy of0.
original accuracy is .
.
thus care successfully repairs nwith a fairness improvement of .
and the model accuracy is mildly affected.
implementation and evaluation in the following we conduct a set of experiments to evaluate care.
we demonstrate the technique in the context of three different tasks neural network fairness improvement neural network backdoor removal and neural network safety property violation authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
causality based neural network repair icse may pittsburgh pa usa repair.
all experiments are conducted on a machine with dualcore intel core i5 .9ghz cpu and 8gb system memory.
to reduce the effect of randomness all experimental results are the average of five runs if randomness is involved.
.
experiment setup in the following care is evaluated over multiple datasets by answering mutpile research questions rqs .
the details of the datasets are summarized as follows for fairness repair care is evaluated over three datasets that are commonly used in machine learning model fairness testing census income this dataset consists of training instances containing features and is used to predict whether an individual income exceeds 50k per year.
among all attributes gender age and race are three protected characteristics.
the labels are if an individual income exceeds 50k per year or not.
german credit this dataset consists of instances with features and is used to assess an individual s credit.
here age and gender are the protected features.
the labels are whether a person s credit is good or not.
bank marketing this dataset consists of instances and there are features.
among them age is the protected feature.
the labels are whether the client will subscribe a term deposit or not.
we train three feed forward neural networks following standard practice and run care to repair the unfair behavior of each model against the corresponding protected features.
for backdoor removal care is evaluated over three datasets german traffic sign benchmark dataset gtsrb this dataset consists of .2k training instances and .6k testing instances of colored images.
the task is to recognize different traffic signs.
we train a cnn network consists of convolutional layers and dense layers.
mnist this dataset consists of 70k instances of handwritten digits as gray scale images.
we train a standard 4layer cnn network to recognize the digits .
fashion mnist this data set consists of 70k instances of fashion categories e.g.
dress coat and etc.
each sample is a 28x28 grayscale image.
we train a cnn network consists of convolutional layers and dense layers.
for safety violation repairing care is evaluated over three acas xu networks.
acas xu contains an array of dnns that produces horizontal maneuver advisories of unmanned version of airborne collision avoidance system x. as discovered in some dnns violates certain safety properties e.g.
dnn n2 violates the safety property 8. we apply care on sub networks n2 n3 3andn1 9against properties 8 2and 7respectively aiming to repair the misbehavior.
table shows the details of the trained networks used in our experiment.
in pso we follow the general recommendation in and set parameter .
c1 c2 .41and number of particles to .
the maximum number of iteration is set to .
to further reduce the searching time we stop the search as soon as the property is satisfied or we fail to find a better location in the last consecutivetable neutral networks used in our experiments model dataset architecture neuron accuracy nn 1census layer ffnn .
nn 2credit layer ffnn .
nn 3bank layer ffnn .
nn 4gtsrb conv dense cnn .
nn 5mnist conv dense cnn .
nn 6fashion conv dense cnn .
nn 7acasn2 layer ffnn nn 8acasn3 layer ffnn nn 9acasn1 layer ffnn iterations.
note that care performs accuracy check after pso finds anm.
if the accuracy drop is significant i.e.
bigger than a threshold of3 care returns error message and a larger value of shall be set in such scenarios.
.
research questions and answers in the following we report our experiment results and answer multiple research questions.
rq1 is care successful in neural network repair?
to answer this question we systematically apply care to the above mentioned neural networks.
forfairness repair tasks to better evaluate the performance of care we set a strict fairness requirement i.e.
to make sure all models fail the fairness property and care performs fairness repair on all of them note that some of models fail fairness even if is set to be a realistic value of .
table summarizes the results where the columns show model the protected feature the unfairness maximum probability difference before and after repair and model accuracy before and after repair.
note that in this experiment input features are assumed to be independent and normal distribution is followed in our sampling process.
the number of neurons to repair is set to be no more than of total number of neurons in the given network.
as illustrated in table among all cases nn 2andnn 3show alarming fairness concern i.e.
with unfairness above .
care successfully repairs all neural networks with an average fairness improvement of .
and maximum of .
.
in terms of model accuracy either the accuracy is unchanged nn or has only a slight drop nn 1andnn .
we further compare the performance of care with the state ofthe art work for this task.
the method proposed in relies on learning a markov chain from original network and performing sensitivity analysis on it.
then optimization is applied to find small adjustments to weight parameters of sensitive neurons for better fairness.
we run experiments on nn nn 2andnn 3and the performance comparison is shown in table dtmc represents the method proposed in .
care is able to improve fairness by61.
on average and the model accuracy drops by .
while dtmc only improves fairness by .
at a higher cost of .
.
forbackdoor removal tasks we train neural networks nn nn andnn 6following the attack methodology proposed in badnets .
we randomly chose a target label for each network and vary the authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa bing sun jun sun long h. pham and jie shi ratio of adversarial inputs in training to achieve a high attack success rate of while maintaining high accuracy.
the trigger is a white square located at the bottom right of the image with size around of the entire image .
in the experiment we assume the trigger is unknown as it is often the case in real application and testing dataset is available.
we adopt the technique proposed in to reverse engineer the trigger.
as discovered in the reverse engineered trigger is not perfect in terms of visual similarity.
however care is able to remove the backdoor effectively as shown below.
since nn nn 5andnn 6are cnns and convolutional layers tend to extract local features such as edges textures objects and scenes we apply care on dense layers only.
we measure the attack success rate sr by adding the reverse engineered trigger to all images in the testing set.
as shown in table for all the three models care is able to mitigate the backdoor attack by reducing the attack sr from over to less than while the accuracy of the repaired networks either maintains the same nn reduced by nn or even improved nn .
furthermore we compare care with the state of the art neural network repair technique proposed in named nnrepair.
nnrepair leverages activation map to perform fault localization of a buggy neural network and fix undesirable behaviors using constraint solving.
we conduct experiments over care and nnrepair on two cnn models experiment subjects of nnrepair trained on mnist and cifar10 datasets.
the average performance comparison is shown in figure .
care is able to reduces the sr by .
with accuracy drop of .
on average.
in contrast nnrepair only reduce sr by .
and model accuracy drops by .
.
forsafety property repair tasks we use n2 n3 3andn1 9of acas xu networks as our nn nn 8andnn .
these networks take dimentional inputs of sensor readings and output possible maneuver advises.
katz et al.
and long et al.
demonstrate thatn2 9violates the safety property 8 n3 3violates property 2 andn1 9violates property 7. therefore we apply care on these networks to improve their performance on the corresponding property.
in this experiment for each network we randomly sample 10k counterexamples to the property as the evaluation set and 10k instances that are correctly classified by the original neural network as the drawdown set.
we measure the violation rate vr on each set to evaluate the performance.
as shown in table for all the three networks care successfully brings down the violation rate from .0to in evaluation set while the performance in drawdown set is not affected.
to further evaluate the performance of care on safety property repair tasks we compare care with the state of the art approach proposed in named prdnn.
prdnn introduces decoupled neural network architecture and solves neural network repair problem as a linear programming problem.
we apply prdnn on the above mentioned three acas xu networks.
prdnn fails to repair nn program hangs and the average result of nn 9andnn is shown in table .
both tools are able improve the performance effectively while care outer performs prdnn by .
at a lower cost.
thus to answer rq1 we say care is able to repair given buggy neural network successfully for various properties while maintaining high model accuracy.
furthermore care outer performstable fairness repair results model p. feat.fairness score accuracy time s before after before after loc tot nn race .
.
.
.
nn age .
.
.
.
nn gender .
.
.
.
nn age .
.
.
.
nn gender .
.
.
.
nn age .
.
.
.
existing works for different tasks.
rq2 what is the effect of parameter i.e.
the trade off between the degree of property violation and accuracy?
recall that the value of controls the trade off between fairness and accuracy.
to understand the effect of s value we perform another set of experiments.
figure illustrates the result on all neural networks where the first plot shows the performance improvement of repaired network compared to original network and the second plot shows the cost measured by model accuracy drop.
as increases from .1to0.
the importance of model accuracy over repair objective in pso fitness function increases.
as shown in the plots for nn the model accuracy is quite sensitive with respect to the value of i.e.
when .2model accuracy drops by .
from .88to0.
.
although a smaller results in better fairness we select a larger to keep high model accuracy.
as shown in the plots for nn model accuracy is stable over different values.
similar to the previous case smaller results in better fairness.
therefore we select a small for more effective fairness repair.
as for nn a large improves model accuracy significantly.
when is greater than .
the model accuracy is even higher than the original network.
that is because pso tries to optimize for model accuracy as well.
however the effectiveness of fairness improvement drops i.e.
model unfairness is reduced to .
for .1but at .9the unfairness is as high as .
.
for nn nn 5andnn 6costs for all values are small .
this is because fixing the backdoor itself will improve model accuracy as well.
for nn 4andnn 5the performance improvement drop in sr is quite stable over different .
fornn the performance improvement drops as increases hence we select small to let the optimization focus on backdoor removal.
as for nn nn 8andnn the performance improvement is stable over different s values.
for nn the cost drops from .
to0.
when increases from .1to .
.
while for nn 8andnn value of does not affect the cost much.
hence we select small value fornn 7andnn 8and select .1fornn .
as shown by the experiment results the value of balances the trade off between performance improvement and cost.
a smaller often results in more effective property repair but model accuracy may be affected.
in our experiments we set the value of as shown in table .
thus to answer rq2 we say that does have a significant impact on the repairing result.
in practice the users can decide its value based on the importance of the property.
in the future we aim to study systematic ways of identifying the best value automatically.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
causality based neural network repair icse may pittsburgh pa usa table backdoor removal results modelattack sr accuracy time s before after before after loc tot nn .
.
.
nn .
.
.
nn .
.
.
.
table safety property repair results modelcounterexample vr drawndown vr time s before after before after loc tot nn .
.
.
.
nn .
.
.
.
nn .
.
.
.
table comparison with existing works tech.fairness backdoor safety imp.
cost imp.
cost imp.
cost care .
.
.
.
.
dtmc .
.
nnrepair .
.
prdnn .
.
table parameter setting modelnn 1nn 1nn 1nn 2nn 2nn p. feat.
race age gender age gender age .
.
.
.
.
.
modelnn 4nn 5nn 6nn 7nn 8nn .
.
.
.
.
.
rq3 how effective is the causality based fault localization?
this question asks whether our causality based fault localization is useful.
we answer this question by comparing the repair effectiveness through optimizing the weight parameters that are selected in four different ways i.e.
selected based on our approach selected randomly selected based gradients and include all the parameters.
the results are shown in table .
firstly we perform network repair with randomly selected neurons to optimize.
we follow the same configuration of care as the one we used in rq1 i.e.
with the same value and the number of neurons to fix.
the performance improvement and cost of the repair is shown in random1 columns in table .
adjusting randomly selected parameters results in an average performance improvement of47.
.
while care improves the performance by .
on all tasks.
for fairness repair tasks the improvement is significant with randomly selected neurons.
however the model accuracy drops sharply.
the overall accuracy drop is .
on average with the worst case of .
fornn 2w.r.t.
protected feature gender.
for backdoor0.
.
.
.
.
.
.
.
.
.
.
.
.
performance improvement nn nn nn nn nn nn nn nn nn .
.
.
.
.
.
.
.
.
cost nn nn nn nn nn nn nn nn nn figure effect of parameter removal tasks the performance improvement is .
on average and model accuracy is affected.
especially for nn accuracy is reduced by .
after the repair.
in terms of safety repair tasks the repair is not so effective.
for nn 7the performance improvement is less than and in average the improvement is only .
although the model accuracy is not affected much.
therefore with randomly selected parameters the repair is not effective and the performance is improved at the cost of disrupting correct behavior of the original model.
on the other side as described in section our fault localization is based on the causal influence of each neuron on the targeted misbehavior without altering other behaviors.
secondly we optimize all weight parameters of the given neural network i.e.
no fault localization.
in this setting the search space in the pso algorithm increases significantly compared to the case of repairing of parameters in care .
therefore we limit the time taken by pso so that time allowed to spend in this step is the same as that taken by care with fault localization.
the results are shown in columns all.
for fairness repair tasks the fairness is improved by .
on average.
but the model accuracy drops significantly i.e.
the accuracy drops by .
on average and .
in the worst case.
for backdoor removal the performance improvement is only .
on average while care reduces the sr by .
.
for safety repair tasks the performance improvement is .
on average and care manages to reduce the vr in evaluation set by .
.
although the cost is below for these two tasks the repair is not that effective.
in all of the cases without fault localization pso is not able to converge within the time limit and as a result neither performance improvement nor cost is optimized when pso terminates.
in the literature gradient is a commonly used technique that guides the search in program fault localization program repair and fairness testing .
therefore we conduct experiments authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa bing sun jun sun long h. pham and jie shi modelperformance improvement cost care gradient random all care gradient random all nn 1race .
.
.
.
.
.
.
.
nn 1age .
.
.
.
.
.
.
.
nn 1gender .
.
.
.
.
.
.
.
nn 2age .
.
.
.
.
.
.
.
nn 2gender .
.
.
.
.
.
.
.
nn 3age .
.
.
.
.
.
.
.
nn .
.
.
.
.
.
.
.
nn .
.
.
.
.
.
.
.
nn .
.
.
.
.
.
.
.
nn .
.
.
.
.
.
.
.
nn .
.
.
.
.
.
.
.
nn .
.
.
.
.
.
.
.
table fault localization effectiveness to compare the performance of care with gradient guided fault localization.
that is instead of calculating causal attribution of the hidden neurons the fault localization step is based on y vwhere yrepresents the favourable output value and vrepresents hidden neuron value.
we use the gradient based method to identify the same amount of responsible neurons and perform optimization with the same setting as care.
the results of gradient based localization method is illustrated in columns gradient of table .
compared with care for fairness repair tasks the gradient based method is able to find a repair that satisfies fairness criteria but fails to maintain model accuracy.
overall fairness improves by .
but average accuracy drops by .
which is not desirable.
for backdoor removal the gradient based method is not able to find an effective repair where the overall performance improvement is below .
especially for nn the sr is still as high as .
after the repair.
for safety repair gradient based method is able to find a repair for nn 8andnn 9with performance improvement above while care archives backdoor removal.
for nn gradient based method is not useful at all where vr in evaluation set is as high as after the repair.
hence the performance of gradient based method is not stable and often not effective.
thus to answer rq3 we say our causality based fault localization is effective in identifying candidates for parameter optimization.
it guides care to focus on fixing undesirable behavior of the model while keeping the correct behavior unaffected.
related work this work is broadly related to works on neural network verification repair and causal interpretation.
neural network verification.
there have been an impressive line of methods proposed recently for neural network verification.
these includes solving the problem using abstraction techniques smt sovler milp and lp symbolic execution and many others .
there have also been attempts to verify neural network fairness properties including and based on numerical integration based on non convexoptimization and based on probabilistic model checking.
unlike these works our approach focus on neural network repair instead and we leverage some existing verification methods in our work for property verification.
machine learning model repair.
there have been multiple attempts on repairing machine learning models to remove undesirable behaviors.
in kauschke et al.
suggest a method to learn a model to determine error regions of the given model which allows users to patch the given model.
in sotoudeh et al.
propose a method to correct a neural network by applying small changes to model weights based on smt formulation of the problem.
in goldberger et al.
leverage recent advances in neural network verification and presented an approach to repair a network with minimal change in its weights.
this approach aims to find minimal layerwise fixes for a given point wise specification and the performance is restricted by the underlying verification method used.
nnrepair performs constraint based repair on neural networks to fix undesirable behaviors but only applies to a single layer.
sotoudeh et al.
proposed a method in to solve a neural network repair problem as a linear programming problem.
unlike our approach both nnrepair and the method proposed in perform layer wise repair but the actual fault in a buggy network could be across multiple layers.
furthermore based on our experiment results our approach is more effective in neural network repair against different properties.
machine learning causal interpretation.
causality analysis has been applied to generate explanations for machine learning algorithms.
existing works focus on causal interpretable models that can explain why machine learning models make certain decision.
narendra et al.
model dnns as scms and estimate the causal effect of each component of the model on the output.
in chattophadhyay et al.
proposed an scalable causal approach to estimate individual causal effect of each feature on the model output.
causal inference has been applied in machine learning model fairness studies.
kusner et al.
proposed an approach to measure the fairness of a machine learning model based on counterfactuals where a fair model should authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
causality based neural network repair icse may pittsburgh pa usa have the same prediction for both actual sample and counterfactual sample.
in zhang et al.
propose a causal explanation based metric to quantitatively measure the fairness of an algorithm.
our work utilizes scms and do calculus to measure the causal attribution of hidden neurons on model undesirable behaviors.
the results are used as a guideline for fault localization.
conclusion we present care a causality based technique for repairing neural networks for various properties.
care performs fault localization on a given neural network model and utilizes pso to adjust the weight parameters of the identified neurons.
care generates repaired networks with improved performance over specified property while maintaining the model s accuracy.
care is evaluated empirically with multiple neural networks trained on benchmark datasets and experiment results show that care is able to repair buggy model efficiently and effectively with minimally disruption on existing correct behavior.
data availability a prove of concept poc realization of this work care is implemented on top of socrates as a causality based neural network repair engine.
the source code of care is publicly available at .