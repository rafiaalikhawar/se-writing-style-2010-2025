theearlybirdcatches the bug on exploitingearlylayersof encoder models for moreefficient codeclassification anastasiia grishina anastasiia simula.no simula researchlaboratory oslo norwaymaxhort maxh simula.no simula researchlaboratory oslo norwayleonmoonen leon.moonen computer.org simula researchlaboratory bi norwegian business school oslo norway abstract the use of modern natural language processing nlp techniques has shown to be beneficial for software engineering tasks such as vulnerability detection and type inference.
however training deepnlpmodelsrequiressignificantcomputationalresources.this paperexplorestechniquesthataimatachievingthebestusageof resources andavailable information in thesemodels.
we propose a generic approach earlybird to build composite representations of code from the early layers of a pre trained transformer model.
we empirically investigate the viability of this approach onthe codebert model by comparingthe performance of strategies for creating composite representations with the standardpractice ofonly usingthe last encoderlayer.
ourevaluationonfourdatasetsshowsthatseveralearlylayer combinations yield better performance on defect detection and some combinations improve multi class classification.
more specifically weobtaina 2averageimprovementofdetectionaccuracy on devign with only out of layers of codebert and a .3x speed up of fine tuning.
these findings show that early layers can beusedtoobtainbetterresultsusingthesameresources aswellas toreduce resourceusage duringfine tuningandinference.
ccsconcepts softwareand its engineering computing methodologies neural networks natural language processing keywords sustainability model optimization transformer code classification vulnerability detection ai4code ai4se ml4se acm reference format anastasiia grishina max hort and leon moonen.
.
the earlybird catchesthebug onexploitingearlylayersofencodermodelsformore efficient code classification.
in proceedings of the 31st acm joint european software engineering conference and symposium on the foundations of softwareengineering esec fse december3 sanfrancisco ca usa.
acm newyork ny usa 13pages.
esec fse december san francisco ca usa copyright heldby theowner author s .
acm isbn .
introduction automationofsoftwareengineering se taskssupportsdevelopers increationandmaintenanceofsourcecode.recently deeplearning dl models have been trained on large open source code corpora and used to perform code analysis tasks .
motivated by the naturalness hypothesis stating that code and natural language share statistical similarities researchers and tool vendors have started training deep nlp models on code and fine tuning them on se tasks .
amongst others such models have been appliedtotypeinference codeclonedetection programrepair anddefectprediction .innlp based approaches setasksarefrequentlytranslatedtocodeclassification problems.
for example detection of software vulnerabilities is a binary classification problem bug type inference is a multi class classificationsetting andtypeinferenceisamulti labelmulti class classification task in case a type is predicted for each variable in the program.
most modern nlp models build on the transformer architecture .
this architecture uses attention mechanism and consists ofanencoderthatconvertsaninputsequencetoa representation throughaseriesoflayers followedby decoderlayersthatconvert this representation to an output sequence.
although effective in terms of learning capabilities the transformer design results in multi layer models that need large amounts of data for training from scratch.
a well known disadvantage of these models is the high resource usage that is required for training due to both model and data sizes.
while a number of pre trained models have been published recently fine tuningthesemodelsforspecifictasks still requires additionalcomputational resources .
thispaperexplorestechniquesthataimatoptimizingtheuseof resourcesandinformationavailableinmodelsduringfine tuning.
in particular we consider open white box models for which the weightsfromeachlayercanbeextracted.wefocusonencoder only models astheyarecommonlyusedforseclassificationtasks in particular thetransformer basedencoders.thestandardpractice inencodermodelsistoobtaintherepresentationoftheinputsequence from the last layer of the model while information from earlier layers is usually discarded .
i.e.
while the early layers are used to computethe values of the last layer they are generally not considered as individual representations of the input in the way that the last layer is.
to exemplify the amount of discardedinformation atinference whenfine tuninga layered encoder suchascodebert forbugdetection ofthecode embeddingsare ignored.1however it hasbeenshownfornatural 1that is theweightsfrom out of layers areignored for classification.
thiswork islicensedunderacreativecommonsattribution4.0international license.
esec fse december3 san francisco ca usa anastasiiagrishina maxhort andleonmoonen language that the early layers of an encoder capture lower level syntacticalfeaturesbetterthanthelaterlayers which can benefitdownstream tasks.
inspiredbythelineofresearchthatexploitsearlylayersofmodels weproposeearlybird 2anovelandgenericapproachforbuildingcompositerepresentationsfromtheearlylayersofapre trained encodermodel.earlybirdaimstoleverageallavailableinformationinexistingpre trainedencodermodelsduringfine tuningto either improve results or achieve competitive results at reduced resource usage during code classification.
we empirically evaluate earlybirdoncodebert apopularpre trainedencodermodel forcode andfourbenchmarkdatasetsthatcoverthreecommonse tasks defectdetectionwiththedevignandrevealdatasets bug type inference with the data from yasunaga et al.
and exceptiontypeclassification .theevaluationcomparesthe baselinerepresentation that uses the last encoder layer with results obtained via earlybird.
we both fine tune the full size encoder and its pruned version with only several early layers present in the model.
the latter scenario analyzes the trade off between only using apartialmodelandthe performance impact onse tasks.
contributions inthispaper wemakethefollowingcontributions we propose earlybird an approach for creating composite representations of code using the early layers of a transformer based encoder model.
the goal is to achieve better code classification performanceatequalresourceusageorcomparableperformance at lower resourceusage.
weconductathoroughempiricalevaluationoftheproposedapproach.weshowtheeffectofusingcompositeearlybirdrepresentations while fine tuning the original size codebert model on fourreal worldcodeclassificationdatasets.werunearlybirdwith 10differentrandominitializationsofnon fixedtrainableparametersandmarktheearlybirdrepresentationsthatyieldstatistically significant improvement over the baseline.
we investigate resource usage and performance of pruned models.
we analyze the trade off between removing the later layers of a modelandthe impact this has onclassification performance.
mainfindings withearlybird weachieveperformanceimprovementsoverthebaselinecoderepresentationwiththemajorityof representations obtained from single early layers on the defect detectiontaskandselectedcombinationsonbugtypeandexceptiontypeclassification.moreover outofthereduced sizemodels withprunedlater layers we obtaina 2average accuracy improvementondevignwith3.3xspeed upoffine tuning aswellas .
accuracyimprovement with3.7xspeed uponaveragefor reveal.
the remainder of the paper is organized as follows.
we present related work in section 2and provide background details of the studyinsection .themethodologyisdescribedinsection 4which is followed by experimental setup in section .
we present and discuss results insection 6andconclude withsection .
related work here we give an overview of language models for se tasks and recent encoder models specifically as well as different approaches to use early layers ofencoder models.
2early layer based improvementorreduction of resourcesused2.
transformers in softwareengineering the availability of open source code and increased hardware capabilities popularized training and usage of deep learning including nlpandlargelanguagemodels llms forsetasks.todate deep nlpmodelshavealreadybeenappliedinatleast18setasks .
pre trainedlanguagemodelsavailableforfine tuningonsetasks largelybuildonthetransformerarchitecture sequence to sequence models andtheattentionmechanism .onewidelyused benchmarktotestdifferentdeeplearningarchitecturesonsetasks is codexglue .
the benchmark provides data source code for modelevaluation andaleader boardrankingmodelperformance ondifferenttasks .
setaskscanbetranslatedtoinputsequenceclassificationand generationofcodeor text.examplesofgenerative tasksinse are codecompletion coderepair generation ofdocumentation from code and vice versa and translation between different programming languages.
such tasks are frequently approached with neural machinetranslationmodels.fulltransformermodelsfortranslation fromaprogramminglanguage pl toanaturallanguage nl or pl pltasksincludeplbart pymt5 tfix codet5 break it fix it .alternatively generativemodelscaninclude the decoder only part of the transformer as in gpt type models.
in this case the decoder both represents the input sequence and transforms it into theoutput sequence.decoder based modelsfor code include for example codex andcodegpt .
inthetasksthatrequirecodeordocumentationrepresentation and their subsequent classification the encoder only architectures are used more frequently than in translation tasks.
examples of code classification problems are code clone detection detection of general bugs such as the presence of swapped operands wrong variablenames syntaxerrors orsecurityvulnerabilities.anumber of encoder models for code applied a widely used bi directional encoder bert topre trainitoncode withsomemodifications oftheinput.inthisway thecodebert graphcodebert cubert and polyglotcodebert models were created.
indetail the12 layerroberta basedcodebertmodelwaspretrainedonnl pltasksinmultipleplsandutilizedonlythetextual features of code.
note that roberta is a type of bert model with optimized hyper parameters and pre training procedures .
togetherwiththedecoder onlycodegptmodel theencoder only codebert model was used as a baseline in codexglue.
graphcodebertutilizesbothtextualandstructuralpropertiesofcodeto encodeitsrepresentations.polyglotcodebertistheapproachthat improves fine tuning ofthe codebertmodel ona multi lingual dataset for a target task even if the target task tests only one pl.
this paper focuses on the fine tuning strategies which in contrast to polyglotcodebert do not increase the resource usage for fine tuning.cubertisa24 layerpre trainedtransformer based encoder testedon a numberof codeclassificationtasks including exceptiontypeclassification.wetesttheperformanceoftheproposed earlybird composite representations on defect detection including the use of one of codexglue benchmarks as well as on error and exception type classification tasks.
however the goal of this paper is to achieve improvement over the baseline model whenitisfine tuned withcompositecoderepresentations.wedo notaimtocompareresultswithothermodels butratherpropose 896theearlybirdcatchesthe bug on exploiting early layersof encodermodels... esec fse december3 san francisco ca usa anapproachthatisapplicabletotransformer basedencodersfor source code and show its performance gains compared to the same modelusage withoutthe proposedapproach.
.
use ofearlyencoderlayers a number of studies explored different approaches to use information from early layers of dl models for sequence representation suchasprobingsinglelayers pruningandvariablelearningrates.
onewaytoleverageinformationfromearlymodellayersistogive different priority to layers while fine tuning the models .
for example the layer wise learning rate decay llrd strategy and re initialization of late encoder layers yielded improvement overthestandardfine tuningofbertonnlptasks .thellrd strategy was initially developed to tune the later encoder layers with larger learning rate.
in this way the later layers can be better adapted to a downstream task under consideration because the later layers are assumed to learn complex task specific features of input sequences .
moreover peters et al.
showed that theperformanceoffine tuningimprovesiftheencoderlayersare updatedduringfine tuningincomparisonwithtrainingonlythe classifier ontop offixed frozen encoder layers.
pruning later layers of transformer models is another way to consider only early layers for fine tuning .
sajjad et al.
investigatedhowtheperformanceoftransformermodels onnlpisaffectedwhenreducingtheirsizebypruninglayers.they consideredsixpruningstrategies includingdroppingfromdifferent directions alternated layerdropping ordroppinglayersbasedon importance for four pre trained models bert roberta xlnet albert .bypruningmodellayers sajjadetal.
were able to reduce the number of parameters to of the initialparametersetwhilemaintainingahighlevelofperformance.
while the performance on downstream tasks varies in their study the lower layers are critical for maintaining performance when fine tuning for downstream tasks.
in other words dropping earlierlayersisdetrimentaltoperformance.overall pruninglayers reducesmodelsizeandinturnreducesfine tuningandinference time.inlinewiththeworkofsajjadetal.
weextendourexperimentswiththepruningoflaterlayersandkeepingearlierlayers present inthe model see rq2 insection .
theuseofinformationfromsingleearlylayers inanumberof earlybirdexperimentsisalsoinspiredbypetersetal.
.intheir study peters et al.
present an empirical evidence that language models learn syntax and part of speech information on earlier layers of a neural network while more complex information such as semantics and co reference relationships are captured better by deeper later layers.
in another study karmakar and robbes probed pre trained models of code including codebert on tasks of understanding syntactic information structure complexity code length andsemanticinformation .whilekarmakarandrobbes probedfrozen earlylayersof differentmodelsforcodeinasingle strategy weuse12differentstrategiesforcombiningunfrozenearly layersduringfine tuningandfocusonthetasksofbugdetection or bug type classification.
similarly hern ndez l pez et al.
probed different layers of five pre trained models including codebert andgraphcodebert andfoundthatmostsyntactic information is encoded in the middle layers.
the novelty of ourstudy with respect to karmakar and robbes is that we combine earlylayersinadditiontoextractingeachofthem whilekarmakar and robbes extracted early layer representations and used them withoutcomposing newrepresentations.
encoders forcode classification in this section we presentthebackgroundontransformermodels and different uses of the encoder decoder or full transformer architecture as well as its encoder only and decoder only variants.
because our study focuses on encoder only open source models availableforfine tuning thedistinctionbetweentransformertypes isnecessary for understanding the methodology.
insequence to sequencegenerationscenarios thetransformer modelconsistsofamulti layerencoderthatrepresentstheinput sequence and a decoder that generates the output sequence based on the sequence representation from the encoder and the available output generated at previous steps .
for source code classification tasks the transformer is frequently reduced to only its encoder followed by a classification head a component added to theencodertocategorizetherepresentationintodifferentclasses.
dropping the decoder for classification is motivated by resource efficiency because the decoder is conceptually only needed for token generation from the input sequence.
during classification of aninput theencoder representsthesequenceandpassesittothe classificationhead.basedonthisdesign anumberofpre trained encodershavebeenpublishedinrecentyears suchasbertand roberta which were pre trained on natural language and similar models pre trained oncode or a combinationof code and natural language .thegoalofpre traininginthe pre trainandfinetunescenario is to capture language patterns in general so that they can serve as a basis for domain specific downstream tasks.
pre trainedmodelscanbefine tunedondifferentdownstreamtasks innlpandse.
processingtheinputsequencefor classificationconsistsofseveralsteps tokenization initial embedding encoding the sequence with an encoder and passing the sequence representation through aclassificationhead .tokenizationsplitstheinputsequence adds specialtokens matches thetokens totheirid sinthevocabulary of tokens and unifies the resulting token length for samples in a dataset.
embedding transforms the one dimensional token id toaninitialmulti dimensionalstaticvectorrepresentationofthe token andisusually apart of thepre trainedencoder model.this representation is updated using the attention mechanism of the encoder.
because of attention the representation of the input is influencedbyalltokens inthe sequence soitiscontextualized.
codebertisaroberta basedmodelwith12encoderlayerspretrainedon6programminglanguages python java javascript php ruby and go as wellas text to code tasks .
pre trainingwas done on the masked language modeling mlm and replaced token detection rtd tasks.thesetasksrespectivelytrainthemodelto derive what token is masked in mlm and in rtd predict whether any token in an original sequence is swapped with a different token that should not be in the sequence.
codebert outputs a bidirectional encoder representation of the input sequence which means that the model considers context from pre pending and subsequent wordsto represent eachtoken inthe inputsequence.
897esec fse december3 san francisco ca usa anastasiiagrishina maxhort andleonmoonen a pre trained model is usually released with a pre trained tokenizer.
the pre trained tokenizer ensures that token id s correspondtothoseprocessedduringpre training.thetokenizeralso addsspecialtokens suchasa clstokenatthestartofeachinput sequence padtokensto unifylengths ofinputsequences andthe eostoken to signify the end of the input string and the start of padding sequence .
all tokens are transformed by the model ineachencoderlayer.outofalltokens the clstokenrepresentationfromthelastlayer whichisupdatedbyallencoderlayers is typicallyusedas arepresentation for the full sequence.
the standard practice of using the clstoken from the last encoderlayerismotivatedbythepre trainingprocedure.forexample in mlm the model predicts the masked token based on the cls tokenrepresentationfromthe12thlayerofbertandcodebert.
however the choice of token to represent the full sequence in finetuning can be different.
for example in plbart a transformer modelforcodewithbothanencoderandadecoder the eostokenis usedforrepresentingtheinputsequence.inthispaper wepropose differentwaystorepresenttheinputsequenceanduseinformation from early layers ofthe modelinan effective way.
methodology in this paper the architecture of the code classification model consists of five parts a tokenizer an embedding layer an encoderwithseverallayers asetofoperationstocombinesequencerepresentationsfromencoderlayerswithearlybird and aclassificationhead.theoutputofeachstepisusedasinputinto the next step.
an overview of the architecture is shown in figure anddescribedbelow.themaindifferencebetweenthisarchitecture and the classification architecture discussed in section 3is step the standardarchitecture only consists ofsteps and .
steps use a pre trained tokenizer embedder and encoder.
earlybirdisformulatedinagenericwayandcanbeappliedtoany encoder but for our experiments we fix the codebert model and tokenizer.instep wecombineinformationfromallthelayers or from only some of the early layers of the encoder as opposed tothebaselinethatusesthelastlayeroftheencoder.finally the classificationheadinstep consistsofonedropoutlayerandone linearlayer withsoftmax.
the encodermodel represents eachtoken of an input sequence withavectorof size u1d43b alsoknownashiddensize.for eachinput sequenceoflength u1d446 andahiddensize u1d43b weobtainamatrixofsize u1d446 u1d43bforeachof u1d43flayersofthebasemodelasshowninfigure .
input source code sample i tokenizer encoder layers classification head l combination layers hh1 rs scls token i1 ... token in eos pad ... pad embedding layer hs figure modelarchitecture forcodeclassification.forexample thecodebertarchitectureisfixedwith12encoder layers i.e.
u1d43f 12forthatmodel.alltheinformationavailablein the encoder for one input sequence is stored in a tensor of size u1d43f u1d446 u1d43b.theearlybirdcombinationsmustproduceonevector vec u1d445 ofsize u1d43bthatrepresentsthe input asshown infigure .keeping the output code representation of size u1d43bis required to provide a faircomparisonofearlybirdcompositerepresentationswiththe standardcoderepresentationobtainedfromthelastlayer.inthis way the dimension of the classification head is the same for all combinations of early layers and has minimal possible influence duringfine tuning.
asastrategyforsystematicallyinvestigatingcompositerepresentations we create a grid search over three typical operations tocombineoutputsofneuralnetworklayers maximumpooling max pool weighted sum and slicing and two dimensions to apply the operations over tokens and or layers.
for the tokens dimension weeitheruseallofthetokensfromaspecificlayeror onlytheclstoken.amonglayers weeithersliceonelayer sumor takemaximumvaluesoveralllayers.
thechoiceofconsideringevery token of a layer is motivated by the fact the transformer based modelsexhibitvaryingdegreesofattentionfordifferenttypesoftokens whichindicatesthatsolelyusingthe clstokenmightnot be the best choice for tasks .
we also experiment with different sizesofthemodel.thecombinationstrategiesthatusealllayersof thepre trainedmodelaredividedintotwocategories thestrategies thatuseclstokensfromtheencoderlayers thestrategiesthatuse more tokens thanjust clsfrom encoder layers.
whenweslicethe clstokenandapplyeachoftheoperations over layers we obtain the following cls tokencombinations i baseline clstoken from the last layer i.e.
layer no.
u1d43f ii clstoken from one layer3no.
u1d459 u1d459 ... u1d43f iii max poolover clstokens from alllayers u1d459 u1d43f u1d459 iv weightedsum over clstokens from alllayers u1d459 u1d43f u1d459 .
thesecondsetofcombinationsusesrepresentationsofallthe tokens intokenizedinputsequences includingthe clstoken.
we firstapplymaxpoolingoperationtoeitheralltokensoralllayers and use the rest of operations.
then we apply weighted sum as the firstoperation followedbymax poolorslicing of alayer v max pooltokens from one layer no.
u1d459 u1d459 ... u1d43f vi maxpooloveralllayersforeachtokenintheinputsequence max poolover tokens vii maxpooloveralllayersforeachtokenintheinputsequence weightedsum over tokens viii max pool over all tokens for each layer no.
u1d459 u1d459 ... u1d43f weightedsum over layers ix weightedsumovertokensfromonelayerno.
u1d459 u1d459 ... u1d43f x weighted sum over tokens for each one layer no.
u1d459 u1d459 ... u1d43f weightedsum over alllayers xi weighted sum over all layers for each token in the input sequence weightedsum over alltokens.
notethatweightsintheweightedsumsarelearnableparameters.
however theaddednumberoflearnableparametersforfine tuning 3weuseeachlayer u1d459inthecombinationsseparatelyifwedenote u1d459 u1d459 ... u1d43f and specifythe set of layers u1d459 u1d43f u1d459 1if several layersareused at once.
898theearlybirdcatchesthe bug on exploiting early layersof encodermodels... esec fse december3 san francisco ca usa layer ... layer l... layer lr cls token i1 ... token in eos pad ... pad a baseline clstokenoflayerl i .layer ... layer l... layer lr cls token i1 ... token in eos pad ... pad b fullmodel clsoflayer l l ii .layer ... layer lr cls token i1 ... token in eos pad ... pad c prunedmodel cls of last layer l xii .
layer ... layer l... layer l... ...max pooling iii weighted sum iv r extracted cls tokenscls token i1 ... token in eos pad ... pad d combinations of cls tokens fromalllayers iii iv .layer ... layer l... layer lmax pooling v or weighted sum ix over tokens for layer l layer lrcls token i1 ... token in eos pad ... pad e combinations of alltokens froma singlelayer v ix .
max pooling vi vii or weighted sum xi over layers for each tokenmax pooling vi or weighted sum vii xi over tokens r layer 1layer llayer l... ...... ...... ...... ...... ... combined information across layerscls token i1 ... token in eos pad ... pad f combinations of all tokens combined across layers using max pooling or weightedsum vi vii xi .layer ... layer l... layer lmax pooling viii or weighted sum x over tokens for each layer l... ...weighted sum viii x r combined information across tokenscls token i1 ... token in eos pad ... pad g combinationsofalltokenscombinedforeachsinglelayerfirst usingmaxpooling or weightedsum viii x .
figure2 combinationsofearlyencoderlayersthatleadtocoderepresentationvector vec u1d445foreachtokenizedinputsequence.
thelatin numbering in brackets corresponds tothecombinations describedin section .
observe that thepresentation order hasbeen designed to preserve spaceby grouping similar combinationsinthesamesubfigure.
constitutes .
4of the number of learnable parameters in the baselineconfiguration.forthisreason wementionthatthemodels withcombinations ii x have thesamemodelsize whilebearingin mindthe overheadoflearnableweightsinthe weightedsums.
in addition to experiments with token combinations we also investigate performance of the model with first u1d459 u1d43flayers and the baselinetoken combination describedas follows xii clstokenfromthelastlayerofthemodelwith u1d459 u1d43fencoder layers.
note that the baseline combination i with the usage of the cls token from layer u1d43fcorrespondsto ii and xii if u1d459 u1d43f.
4weighted sum over tokens adds u1d446 512learnable weights.
because the weights of the sum are shared across the layers the maximum number of added weights is u1d43f u1d43b 524out of 124m learnable weights in the base model.
combinations without weighted sums do not add extra learnable parameters to the base model.
weighted sum over layersadds learnable u1d43f 12weightsfor codebert.the combinations are presented in figure .
similar combinationsarepresentedclosetoeachotherorarecombinedinthesame imageiftheyonlyhaveminordifferencesandsharethemajorparts.
forexample in figure 2c weillustratecombinations iii and iv becausebothofthemuse clstokensfromalllayerscombinedusing max pooling or weighted sum.
the roman numbers which indicate combinationtypesarepreservedeitherinthedescriptionsbelow thefiguresorinthefiguresthemselves buttheorderischanged.
we mention combination number corresponding to the description inthecurrentsection suchasbaselinecombination i infigure 2a or combination ii for clstoken from one early layer in figure 2b.
we highlight what parts of encoder layer outputs are used for each combination with color.
white cells correspond to the tokens that are not used in early layer combinations.
the goal of all combinationsistoobtainavectorrepresentation vec u1d445foreachinputcode 899esec fse december3 san francisco ca usa anastasiiagrishina maxhort andleonmoonen sample.
for example in figure 2a we consider the last layer u1d43fand extractonly the clstoken markedas vec u1d445.
anotherremarkontheearlybirdcombinationsconcernsthe usage of all tokens or only code tokens.
code tokens are those that correspondto tokenizedinputwords or sub words and are shown in figure 2astoken u1d4561 ... token u1d456 u1d441for an input sequence u1d456of size u1d456 u1d441.for each combination that uses more than just a clstoken i.e.
combinations v xi we experiment with code tokens only as well as with all tokens including cls eos andpad.
the motivation to check code tokens exclusively stems from the hypothesis that information inspecialtokens mayintroduce noiseintoresults.
experimentalsetup in this section we describe the datasets used for empirical evaluationandimplementationdetailsoffine tuningwiththeproposed earlybird approach.
we investigate binary and multi task code classification scenarios to explore generalisability ofour results.
.
datasetsforsourcecodeclassification wefine tuneandtestthecodebertmodelusingtheearlybird approach on four datasets.
the datasets span three tasks defect detection errortypeclassificationandexceptiontypeclassification with and classes respectively.
they also contain data intwoprogramminglanguages c andpython.inaddition the chosen datasets have similar train subset sizes.
in this way we aim toreducetheeffectofthemodel sexposuretodifferentamounts of training data during fine tuning.
statistics of the datasets are providedintable .we reportthesize ofthetrain validation test splits.
in addition we compute the average number of tokens in the input sequences upon tokenization with the pre trained codebert tokenizer.
because the maximum input sequence size for the codebert model is limited to u1d446 the number of tokens is indicative of how much information the model gets access to or howmuchinformation iscut off incaseoflonginputs.
devign this dataset contains functions in c c from two opensourceprojectslabelledasvulnerableornon vulnerable .we reuse the train validation test split from the codexglue defect detection benchmark.5the dataset is balanced the ratio of nonvulnerable functionsis54 .
reveal similarly to devign reveal is a vulnerability detection dataset of c c functions .
the dataset is not balanced it contains90 non vulnerablecodesnippets.boththedevignandreveal datasets contain real world vulnerable and non vulnerable functionsfrom open sourceprojects.
break it fix it bifi thedatasetcontainsfunction levelcode snippets in python with syntax errors .
we use the original buggyfunctionsandformulateataskofclassifyingthecodeinto threeclasses unbalancedparentheseswith43 ofthetotalnumber of code examples in bifi indentation error with code samples invalidsyntaxcontaining26 samples.thetrain testsplitprovided inthedatasetisreused andthevalidationsetisextractedas10 oftraining data.
exceptiontype thedatasetconsistsofshortfunctionsinpython withaninserted hole tokeninplaceofoneexceptionincode.
statistics offine tuningdatasets.
dataset classesavg tokens code samples train valid test devign reveal bifi exceptiontype the task is to predict one of masked exception types for each input function and is unbalanced.
the dataset was initially created from the eth py150 open corpus7as described in the original paper .
we reuse the train validation test split provided by the authors.
.
implementation thearchitectureisbasedonthecodebert8tokenizerandencoder model.themodeldefinesthemaximumsequencelength hidden size and has layers so u1d446 u1d43b u1d43f .hyperparameters in the experiments are set to u1d435 learning rate is u1d452 anddropoutprobabilityis .
.ifthetokenizedinputsample is longer than u1d446 we prune the tokens in the end to make the inputfitintothemodel.werunfine tuningwithadamoptimizer andtestingforeachcombination10timeswithdifferentseedsfor10 epochsandreporttheperformanceforthebestepochonaverage over runs.
the best epoch is defined by measuring accuracy on a validation set.
we use python .
and cuda .
and run experiments onone nvidiavolta a100gpu.
.
evaluationmetrics to present the impact of early layer combinations we compare the accuracy on the test set for all datasets because it allows us to compare our results with other benchmarks.
in addition we reportweightedf1 scoredenotedasf1 w foradetailedanalysis of selectedcombinationsto account for class imbalance.
to obtain theweightedf1 score theregularf1 scoreiscalculatedforeach labelandtheirweightedmeanistaken.theweightsareequalto the number ofsamples inaclass.
wealsoreportresultsofthewilcoxonsigned ranktestonthe corresponding metrics for the combinations that show improvementoverthebaseline .thewilcoxontestisanon parametric testsuitableforthesettinginwhichdifferentmodelvariantsare testedonthesametestset becauseitisapairedtest.thewilcoxon testchecksthenullhypothesiswhethertworelatedpairedsamples comefromthesamedistribution.werejectthenullhypothesisif p valueislessthan u1d6fc .
.incaseweobtainimprovementofa metricoverthebaselinewithanearlybirdcombinationandthe nullhypothesisisrejected weconcludethatthecombinationperformsbetterandtheresultisstatisticallysignificant.forthepruned models we compute vargha and delaney s u1d43412non parametric effectsizemeasureoftheperformancechangeforaccuracyandf1 w 900theearlybirdcatchesthe bug on exploiting early layersof encodermodels... esec fse december3 san francisco ca usa withthresholdsof0.
.64and0.56forlarge mediumandsmall effectsizes .
.
research questions during our empirical evaluation of composite earlybird code representations we addressthe following researchquestions rq1.compositecoderepresentationswithsamemodelsize what is the effect of using combinations ii xi of early layers with thesamemodelsizeincomparisontothebaselineapproachofusing onlytheclstokenfromthelastlayer i.e.
combination i forcode representation on model performance in the code classification scenario?
the goal is to find out whether any of the earlybird combinationtypesworkconsistentlybetterfordifferentdatasets andtasks.
rq2.prunedmodels whatistheeffectofreducingthenumberof pre trained encoder layers in combinations xii on resource usage andmodelperformanceoncodeclassificationtasks?asopposedto rq1 inwhichweconsiderthecombinationsthatdonotreducethe modelsize this research question is devotedto investigationof the trade off between using less resources with reduced size models andperformance variation interms ofclassification metrics.
for both research questions we evaluate the composite representationsonbinaryandmulti taskcodeclassificationscenariosto explore generalisability oftheresults obtained for the binary case.
we investigate if and what combinations result in better performance averagedover10runswithdifferentseeds.forcombinations that improve the baseline on average we also explore if the results are statisticallysignificant according to the wilcoxon test.
results and discussion .
earlybird with fixed size models to answer rq1 we explore one layer combinations multi layer combinations andestimatethestatisticalsignificanceoftheperformanceimprovement.
.
.
combinationsof tokens in single selected early layers.
figure3shows a heatmap of the difference of the mean accuracy obtainedwitheachcombinationthatusesonlyoneselectedearly layercomparedtothebaseline.inaddition weshowthevalueof the difference in mean accuracy for each combination type and layer number.
note that the scale is logarithmic and in the most extreme case spans the interval from ca.
to .
negative values are shown in black and positive values are shown in white.
differences that are statistically significant according to the wilcoxon test are marked with a star next to the value.
combinations that correspond to the baseline are marked with bsln and have zerodifference bydefinition.theresultsfortheweightedf1 score show a similar pattern as those for the mean accuracy.
they are visualizedinthe same wayinfigure .
thefirstrowsinfigures 3aand3bcorrespondtothecombinations ii cls token layer u1d459.
with this combination type average improvement over the baseline is achieved with the majority of earlylayers.specifically weobtainaccuracyimprovementsranging from .
to .
for devign in out of layers and accuracy improvements from .
to .
for reveal in out of layers.
layer ii cls token layer l v max pool all tokens v max pool code tokens ix w sum all tokens ix w sum code tokens .
.
.
.
.
.
.
.
.
.
.
bsln .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.6devign a devign accuracy.
layer ii cls token layer l v max pool all tokens v max pool code tokens ix w sum all tokens ix w sum code tokens0.
.
.
.
.
.
.
.
.
.
.
bsln .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.9reveal b reveal accuracy.
layer ii cls token layer l v max pool all tokens v max pool code tokens ix w sum all tokens ix w sum code tokens .
.
.
.
.
.
.
.
.
.
.
bsln .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.3bifi c bifi accuracy.
layer ii cls token layer l v max pool all tokens v max pool code tokens ix w sum all tokens ix w sum code tokens .
.
.
.
.
.
.
.
.
.
.
bsln .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0exception type d exception type accuracy.
figure difference of mean accuracy between earlybird and baseline bsln performance.
the star indicates a statisticallysignificantdifferencew.r.t.
the baseline.
thedynamicofthemetricchangeoverselectedlayernumbersis different for devign and reveal.
in detail the average performance ofcombination ii isbestwithlayer3ondevign a .0accuracy improvement andwithlayer1forreveal a .8accuracyimprovement .
the best improvement in terms of f1 w matches with layer 3for devignandwithlayer 2for reveal as showninfigure .
max pooling over all available tokens from a selected layer in combination v alsoachievesperformanceimprovementoverthe baseline as shown in rows and of figures 3a 3b.
in general layers4 11yieldhigheraccuracyandlayers2 11higherf1 w with max pooling for devign than the baseline.
for reveal all layers exceptlayer11resultinbetteraverageaccuracyandlayers2 901esec fse december3 san francisco ca usa anastasiiagrishina maxhort andleonmoonen layer ii cls token layer l v max pool all tokens v max pool code tokens ix w sum all tokens ix w sum code tokens .
.
.
.
.
.
.
.
.
.
.
bsln .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.8devign a devign f1 w .
layer ii cls token layer l v max pool all tokens v max pool code tokens ix w sum all tokens ix w sum code tokens .
.
.
.
.
.
.
.
.
.
.
bsln .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
reveal b reveal f1 w .
layer ii cls token layer l v max pool all tokens v max pool code tokens ix w sum all tokens ix w sum code tokens .
.
.
.
.
.
.
.
.
.
.
bsln .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.3bifi c bifi f1 w .
layer ii cls token layer l v max pool all tokens v max pool code tokens ix w sum all tokens ix w sum code tokens .
.
.
.
.
.
.
.
.
.
.
bsln .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.9exception type d exception type f1 w .
figure4 differenceofmeanweightedf1 scores f1 w betweenearlybirdandbaseline bsln .thestar indicatesa statistically significantdifferencew.r.t.
thebaseline.
have higher average f1 w .
max pooling over all tokens including special tokens achieves the best statistically significant average improvement ofaccuracyof .9ofallcombinationsfor reveal.
the weighted sum of all tokens or code tokens exclusively in combination ix doesnotimprovethebaselineperformance.we assumethatfine tuningfor10epochsisnotenoughforthistype ofcombination becausethe lossatepoch10 onbothtraining and validationsplitsishigherforcombinations ix thanforcombinations with max pooling.
since the goal of this study is to use the same or less resources for fine tuning we have not fine tuned this combination for more than10 epochs.whilecombinations ii and v perform betterfor themajority of layers onthe defect detection task multi class classification for bugor exception type prediction does not benefit from the combinations to the same extent as the binary task.
only max pooling of tokens of the last encoder layer achieves better performance than the baseline for bifi .
accuracy .
weighted f1 score improvements andexceptiontype a .2accuracy .1weighted f1 score improvements datasets.
theimpactofusingalltokensorcodetokensexclusivelydepends on the dataset.
the difference between performance of single layer combinations with max pooling of all tokens and only code tokens constitute0.
.1accuracyorf1 w .forthemulti classtasks the average results improve with the use of each later layer in the model.
we obtain performance improvement with the max pooling combination v whileotherone layercombinationsdonotperform betterthanthe baseline.
the best performing results on devign and exception type classificationdatasetsarestatisticallysignificantaccordingtothe wilcoxon test.
for reveal the second best result is statistically significant.
we havenot obtainedstatistically significantimprovements for bifi.
we explain it by the fact that the baseline metric is already high i.e.
.
accuracy.
achieving improvement is usually more challenging when the baselineperforms at this level.
inessence thecombinationsthatinvolve clstokenscorresponding to the single layer ii as well as the max pooling combinations v perform better on average for defect detection datasets devignandreveal.however onlythemaxpoolingcombination v of tokens from the last encoder layer outperforms the baseline onaverageformulti classdatasetsbifiandexceptiontype.the weightedsum oftokensfromaselectedlayer ix performsworse thanthebaselineiffine tunedforthesamenumberofepochsforall tasks.
multi class classification tasks require the information from the last layer for better performance in our experiments while the binary task of defect detection allows us to use early layers and improve the performance over the baseline.
.
.
multi layercombinations.
theaverageperformancedifferencewiththebaselineofcombinationsthatutilizeearlylayersis shown as heatmaps in figures 5and6.
we include the value of the average performance difference and add a star to the number if the difference is statistically significant.
again negative values are showninblack andpositive valuesare showninwhite.
whenweuseallinformationfromtheavailablelayers theimprovement over the baseline is less than what was observed in section6.
.
where one specific layer has been used.
in detail out ofcombinationsthatinvolve clstokensfromallearlylayers no combination performs better than the baseline for reveal bifi orexceptiontypedatasets.however thebestimprovement .
accuracy out of experiments with all layers is obtained on devign withtheweightedsumof clstokensinthecombination iv which islessthanthemaximumimprovementwiththecombinationsfrom one selected early layer in section .
.
.
the improvement of f1 w is shown in figure .
we obtained slightly better improvements of f1 w for devign no f1 w improvement for the unbalanced revealdataset.
theaveragef1 w differencewiththebaselinefor multi classtasksare the same as accuracydifference.
902theearlybirdcatchesthe bug on exploiting early layersof encodermodels... esec fse december3 san francisco ca usa d iii max pool cls tokens iv w sum cls tokens vi max pool layers max pool all tokens vi max pool layers max pool code tokens vii max pool layers w sum all tokens vii max pool layers w sum code tokens viii max pool all tokens w sum layers viii max pool code tokens w sum layers xi w sum layers w sum all tokens xi w sum layers w sum code tokens x w sum all tokens w sum layers x w sum code tokens w sum layers .
.
.
.
.
.
.
.
.
.
.
.
r .
.
.
.
.
.
.
.
.
.
.
.
b0.
.
.
.
.
.
.
.
.
.
.
.
e .
.
.
.
.
.
.
.
.
.
.
.
datasets figure difference of average accuracy between earlybird andbaselineperformanceond devign r reveal b bifi e exception type .
the star indicates a statistically significantdifferencew.r.t.
thebaseline.
if we consider the combinations that involve all tokens the combination vi withtwomaxpoolingoperationsoutperformsthe baselinefordevign reveal andbifiwithaccuracyimprovement between .
and .
.
no combination that involves all layers outperforms the baseline on average for exception type dataset.
combinations that involve one max pooling and one weighted sum of all tokens perform worse or neutral in comparison with thebaseline.thecombinationswithonlyweightedsumsperform worse thanthe baselineonaverage.
answer to rq1.
earlybird achieves statistically significant accuracy and f1 score improvements for defect detection datasets byusingsingle layercombinationsthatinvolvethe clstoken or max pooling over all tokens.
for bug type and exception type classification maxpoolingofthetokensfromthelastencoder layer has improved the performance.
weighted sum of tokens does not improve performance over the baseline.
.
prunedmodels this section is devoted to the combinations of early layers that are initialized with the first u1d459 u1d43fearly layers from the pre trained modelandfine tunedas u1d459 layermodels combinations xii .we startbycomparingtheperformanceofusingthe clstokenfrom layer u1d459of the full size model i.e.
combination ii and using the clstoken from layer u1d459of the model that has u1d459layers in total combination xii .
figure 7presents average accuracy obtained with thesetwo combinations dependingon the used layer aswell as the baseline combination of using clsfrom the last layer u1d43f 12of codebert.
on average the pruned models with reduced size perform on par with the full size model for defect detection on the balanced devign dataset and for bug type and exception typeclassification.however theperformanceofthetwoanalogicald iii max pool cls tokens iv w sum cls tokens vi max pool layers max pool all tokens vi max pool layers max pool code tokens vii max pool layers w sum all tokens vii max pool layers w sum code tokens viii max pool all tokens w sum layers viii max pool code tokens w sum layers xi w sum layers w sum all tokens xi w sum layers w sum code tokens x w sum all tokens w sum layers x w sum code tokens w sum layers0.
.
.
.
.
.
.
.
.
.
.
.
r .
.
.
.
.
.
.
.
.
.
.
.
b0.
.
.
.
.
.
.
.
.
.
.
.
e .
.
.
.
.
.
.
.
.
.
.
.
datasets figure6 differenceofmeanweightedf1 scorebetweencombinationsandbaseline.datasetsareabbreviatedtod devign r reveal b bifi e exception type .
the star indicates astatistically significantdifferencew.r.t.
the baseline.
combinations diverges for the unbalanced defect detection dataset reveal inlayers 4and6 .
most importantly the results show that reducing the model size andusingthe clstokenfromthelastlayerofthereducedmodel performs on par with the baseline for the defect detection task.
the best improvement with the reduced model is achieved with the layer encoder for devign and the layer encoder for reveal.
thisresultshowsthatitispossibletobothreduceresourcesand improve the model s performance during fine tuning on the defect detection taskwithboth abalancedandunbalanceddataset.
to explore the trade off between resource usage and performance degradation for bug type and exception type identification 58606264accuracydevign .
.
.
.
.
.5reveal used layer 687480869298accuracybifi used layer 45505560657075exception type ii full size model xii reduced model i baseline figure7 modelperformancewithasubsetof u1d459 u1d43flayers xii vs.models with alllayers ii clstoken fromlayer u1d459.
903esec fse december3 san francisco ca usa anastasiiagrishina maxhort andleonmoonen table2 comparisonofreducedsizemodelswiththebaseline.wereportmetricperformanceforthebaselineanddifferencewith the baseline for reduced models average time for one epoch fine tuning time in mm ss format speed up and performance variationobtainedwithmodelswith u1d459layers.statisticallysignificantimprovementsaremarkedinbold statisticallyinsignificant performancelossesaremarkedwith and u1d43412effectsizesforaccuracyandf1 w ifany areindicatedbycellcolor respectively large medium and small.the best metricimprovementwith highestspeed up factorsare underlined.
devign reveal bifi exceptiontype u1d459time speed up acc f1 w time speed up acc f1 w time speed up acc f1 w time speed up acc f1 w .0x .
.
.0x .
.
.0x .
.
.0x .
.
.
.1x .
.
.0x .
.
.2x .
.
.1x .
.
.
.2x .
.
.1x .
.
.4x .
.
.2x .
.
.
.3x .
.
.2x .
.
.5x .
.
.3x .
.
.
.5x .
.
.3x .
.
.7x .
.
.5x .
.
.
.6x .
.
.5x .
.
.8x .
.
.7x .
.
.
.8x .
.
.6x .
.
.1x .
.
.9x .
.
.
.2x .
.
.9x .
.
.5x .
.
.3x .
.
.
.6x .
.
.2x .
.
.0x .
.
.7x .
.
.
.3x .
.
.8x .
.
.8x .
.
.4x .
.
.
.4x .
.
.7x .
.
.5x .
.
.7x .
.
.
.8x .
.
.7x .
.
.0x .
.
.4x .
.
weshowtheaveragespeed upofonefine tuningepochandtheperformancelosscomparedtothebaselineforbifiandexceptiontype datasets in table .
we also report the corresponding values for devignandreveal forwhichbothgainsandlossesofperformance are indicated.
the speed up is reported as a scaling factor of the baseline time.
the metric difference is shown as gain or loss of the weightedf1 scoreandaccuracycomparedtothebaselineperformance.
statistically significant improvements are reported in bold while statistically insignificant losses are marked with a star .
the u1d43412effectsizesareindicatedbythreeshadesofblueasthecell color with the darkest shade indicating a large effect u1d43412 .
themiddleshadeindicating a medium effect u1d43412 .
and the lightest shade indicating a small effect u1d43412 .
.
we also underlineanddiscussselectedresultsthatimprovethemetricvaluesand reduce resourceusage.
themajorityofcombinations xii withprunedmodelsoutperformthebaselinefordevignandreveal.furthermore modelswith layers show statistically significant improvements of both metricsondevign withthe3 layermodelachieving 2accuracy improvementwitha3.
timesaveragespeed upoffine tuningwith the same hardware and software.
not only does the layer model improve the accuracy over codebert baseline to .
but also outperformsseveralothermodelstestedondevignandreported on the codexglue benchmark .
in particular our pruned 3layer codebert model outperforms the full transformer model plbart and code2vec code representations pre trained on abstractsyntaxtreesandcodetokensinajointmanner .however ourprunedmodeldoesnotoutperformthebestperforming modelreportedoncodexglue cotext whichachieves66.62accuracy .
modelswith1and11layersachievestatisticallysignificantaccuracyimprovementsforreveal.however the1 layermodelreduces thef1 w score.theuseoflayer11doesnotimpactthespeedof fine tuning while the layer model yields the .7x acceleration of the baseline fine tuning speed.
the lack of speed up with layermodelcanbeexplainedbythe factthat the numberof trainableparametersdoesnotdecreaselinearlywiththeremovaloflaterlayers sincetheadditionalembeddinglayerandclassificationheadremain unchanged.the2 layermodelresultsinthebestimprovementof f1 w which is statistically significant.
the layer model improves accuracy on reveal as well.
for devign and reveal statistically significant improvements have large effectsize.
forbifi weobtainstatisticallyinsignificantdecreaseoff1 w andaccuracyaccordingtothewilcoxontestwhichbringsabout .2x speed up of the fine tuning with the layer model.
if we decreasethe numberof layersto the performance on bifi stays withinthe baselinemetric limit butwegainupto1.7xaverage speed upofone epochfine tuning.incaseofusingmodelswith1 10layers weobserveastatisticallysignificantchangeofdistribution anddecreaseofmetric values.
for the unbalanced exception type dataset the performance dropsfasterandthespeed upislessprominentthanforbifi.the changeofmean valuesof the metrics for allmodelsisstatistically significant.indetail themetricsdecreaseby .0absolutemetric valueat11layerswith1.1xfine tuningspeed upandby .8with layers with .2x speed up.
we explain the sharper decline of the combinations performance by the lower baseline metric values .
accuracy .
weighted f1 score than in the case of bifi .7accuracyandweightedf1 score .forbifi statistically insignificantdeteriorationhavesmalleffectsize.however forboth bifi and exception type datasets we observe deterioration of performanceoflarge effectsize withprunedmodels.
we conclude that for the bifi dataset with high performing baselineand3classes theperformancelossatremovingeachlayer islessthanfortheexceptiontypeclassificationdatasetwithlower baseline performance and classes.
the resource usage which is correlated with time spent on tuning decreases faster for bifi than for exception type.
this is partially explained by a larger classification head for the exception type dataset because this dataset has classes as opposed to only classes in bifi.
in other 904theearlybirdcatchesthe bug on exploiting early layersof encodermodels... esec fse december3 san francisco ca usa words weobservethatthebifidatasethasastrongbaselinethat is hard to outperform with pruning.
by contrast the complexity of theexceptiontypedatasetcaninfluencetheresultsintheopposite way thebaselineperformanceisalreadynotverystrong andit proves hardto further improve onitwithearly layers only.
answer to rq2.
we obtain performance improvements over the baseline as well as fine tuning speed ups for both defect detection datasets by using the clstoken from the last layer of pruned models.
for multi class classification performance decreases upon pruning each layer from the end of the model.
thedecrease issharper forthe datasetwith 20exception types thanfor the taskwith3bugtypes.
.
threatsto validity themainthreattoexternalvalidityisthattheresultsareempirical and may not generalize to all code classification settings including other programming languages tasks and encoder based models for code.
we have tested earlybird combinations on code in c for defect detection and python for bug type and exception type classification in this study.
the choice of the codebert as the encoder model and its internal structure affects the results.
for instance an encoder model that takes smaller input sequences can performworse onthe same datasets becauselargerpartsof input codesequenceshavetobeprunedinthiscase.theexternalvalidity can be improvedbytestingonmore datasets andencoder models.
the threats to internal validity concern the dependency of models on initializations of trainable parameters and the choice of methods.
classification head and weighted sums with trainable parametersinourexperimentsdependontheinitializationofthe parameters and can lead the model to arrive at different local minima during fine tuning.
to reduce the effect of different random initializations we have fine tuned and tested all earlybird combinations10 times withdifferentrandom seeds.
in addition we used the wilcoxon test to verify whether the achievedimprovementsarestatisticallysignificant.however the wilcoxon test only estimates whether measurements of baseline valuesandearlybirdcombinationsaredrawnfromdifferentdistributions.thereportedtimesspentonfine tuningandcorresponding speed upshavethepurposeofillustratingthereductioninresource usage and will depend on the hardware used.
even when using factors of speed up for pruned models there is a chance that these numbers willbe differentonotherhardware configurations.
we implemented the algorithms and statistical procedures in python with the help of widely used libraries such as pytorch numpyandscipy.however wecannotguaranteetheabsenceof implementationerrorswhichmayhave affectedour evaluation.
concludingremarks inthispaper wehaveproposedearlybird anapproachtocombine early layers of encoder models for code and tested different earlylayer combinations on the software engineering tasks of defect detection bug type and exception type classification.
our study ismotivatedbythehypothesisthatearlylayerscontainvaluable information that is discarded by the standard practice of representing the code with the clstoken from the last encoder layer.earlybirdprovideswaysto improvethe performanceofexisting modelswiththesameresourceutilization aswellasforresource usage reduction while obtaining comparable results to the baseline.
results usingearlybird weobtainstatisticallysignificantimprovements over the baseline for the majority of the combinations that involve a single encoder layer on defect detection and with selectedearlybird combinationson bugtype andexception type classification.
max pooling of tokens from selected single layers yields performance improvements for all datasets.
both the classificationperformanceandtheaveragefine tuningtimeforoneepoch areimprovedbypruningthepre trainedmodeltoitsearlylayers and using the clstoken from the last layer of the pruned model.
for defect detection this results in a .
increase in accuracy and a .3x fine tuning speed up on devign and up to .
accuracy improvement with a .7x speed up on reveal.
pruned models do not leadto multi class classificationperformance gains but they do show a fine tuning speed up and the associated decrease in resourceconsumption.
theresultsshow thatprunedmodels withreducedsizeeither workbetterorcanresultinareductionofresourceusageduring fine tuningwithdifferentlevelsofperformancevariation which indicates thepotentialofearlybirdinresource restricted scenarios of deploying defect detection and but type classification in production environments.
for example earlybird achieves a .1x speed upfor bifi whilereducing accuracyfrom .7to .
.
future work the study can be extended by investigating the generalization to other encoder models.
we are in the process of studyingtheperformanceofearlybirdwithtwonewencodermodels starencoder andcontrabert c .anotherdirection for futureresearch iswhetherthe typesoflayercombinationand pruning aswehaveinvestigatedinthispaperforencoderarchitectures arealsoeffectivetechniquesfordecoderandencoder decoder architectures.
moreover it would be of interest to experiment with othercodeclassificationtasks suchasgeneralbugdetectionandthe predictionofvulnerabilitytypes.thelattercouldbeinvestigated usingthecwetypesfromthecommonweaknessenumerationas labeledinthe cvefixesdataset .