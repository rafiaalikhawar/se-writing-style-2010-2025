log based anomaly detection with deep learning how far are we?
van hoang le the university of newcastle nsw australia vanhoang.le uon.edu.auhongyu zhang the university of newcastle nsw australia hongyu.zhang newcastle.edu.au abstract software intensivesystemsproducelogsfortroubleshootingpurposes.recently manydeeplearningmodelshavebeenproposed to automatically detect system anomalies based on log data.
these modelstypicallyclaimveryhighdetectionaccuracy.forexample mostmodelsreportanf measuregreaterthan0.9onthecommonlyusedhdfsdataset.toachieveaprofoundunderstandingofhowfar wearefromsolvingtheproblemoflog basedanomalydetection in this paper we conduct an in depth analysis of five state of the art deeplearning basedmodelsfordetectingsystemanomaliesonfour public log datasets.
our experiments focus on several aspects of model evaluation including training data selection data grouping class distribution data noise and early detection ability.
our results point out that all these aspects have significant impact on the evaluation andthatallthestudiedmodelsdonotalwaysworkwell.
theproblemoflog basedanomalydetectionhasnotbeensolved yet.
based on our findings we also suggest possible future work.
ccs concepts software and its engineering maintaining software.
keywords anomaly detection log analysis log parsing deep learning acm reference format van hoang le and hongyu zhang.
.
log based anomaly detection with deep learning how far are we?.
in 44th international conference on software engineering icse may pittsburgh pa usa.
acm new york ny usa 12pages.
introduction highavailabilityandreliabilityareessentialforlarge scalesoftwareintensivesystems.asthesesystemsprovidevariousservicestoa large number of users a small problem in the system could leadto user dissatisfaction and even significant financial loss.
anomalydetectionis therefore importantforthequalityassuranceof complex software intensive systems.
hongyu zhang is the corresponding author.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa association for computing machinery.
acm isbn ... .
printingconsolelogs.alargeandcomplexsystemcouldproduce amassiveamountoflogs whichcanbeusedfortroubleshooting purposes.forexample thecloudcomputingsystemsofalibabainc.
produce about gigabytes around million lines of tracinglogsperhour .logdataisusuallyunstructuredtextmessages which can help engineers understand the system s internal statusandfacilitatemonitoring administering andtroubleshooting of the system .
log messages can be parsed into log events whicharetemplates constantpart ofthemessages.figure 1shows an example of raw log messages and the corresponding log events obtained after parsing.
log message .
.
.
served block blk to .
.
.
.
.
.
served block blk 3166960787499091856 to .
.
.
.
.
.
got exception while serving blk to .
.
.
log event served block to served block to got exception while serving to parsing figure an example of log messages and log events over the years many data driven methods have been proposed to automatically detect system anomalies by analyzing log data .forexample heetal.
evaluatedsixpopular machinelearning ml algorithmsforlog basedanomalydetection.
theseml basedmethodssharesomelimitationsofinflexiblefeatures inefficiency andweakadaptability .inordertoaddress these issues deep learning dl has been adopted and produced promising results.du etal.
proposedto uselong short term memory lstm to model the sequential patterns of normal sessions then identify anomalies as those violated the patterns.
meng etal.
trainedanlstmmodeltodetectsequentialandquantitativeanomaliesusinglogcountvectorsasinputs.theyalsoproposed template2vectoconsiderthesynonymsandantonymsofthewords inlogtemplates.otherstudies representlogtemplatesas semanticvectorstohandletheinstabilityoflogdata.generally the existing dl based log anomaly detection methods show promising results on commonly used datasets and claim their superiority over traditional ml based approaches.
for instance deeplog and loganomaly all reported a very good performance on commonly usedhdfsandbgldatasets withf measurevalues greater than .
.
however we notice that several important aspects are overlooked by the existing work.
these aspects are associated withexperimental datasets evaluation metrics and experimental settings.inthiswork wewouldliketodivedeepintotheproblemandanswer arelog basedanomalydetectionmethodswithdeeplearning as good as they claimed?
what are the major factors that could affect their performance?
ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa van hoang le and hongyu zhang toanswertheabovequestions weconductasystematicevaluation of five representative deep learning models for log based anomaly detection including deeplog loganomaly plelog logrobust andcnn onfourdatasets including hdfs bgl spirit and thunderbird under controlled experimental settings.
we first conduct an analysis of training data selection and grouping techniques.
then we explore the impact of different characteristics of datasets including data noise and class distribution on model performance.
finally we analyze the ability ofthemodelsintheearlydetectionofanomalies.throughextensiveexperiments weobtainthefollowingmajorfindingsaboutthe current deep learning models for log based anomaly detection the training data selection strategies random or chronologi cal havesignificantimpactonthesemi supervisedlog basedanomaly detection models.
randomly selecting training datacould cause the data leakage problem and unreasonably high detection accuracy.
different log data grouping methods have substantial influence on the performance of the models.
models tend to lose their accuracy when dealing with shorter log sequences.
theeffectivenessofthemodelsissignificantlyaffectedbythe highlyimbalancedclassdistribution.commonly usedmetrics including precision recall and f measure are not comprehensiveenoughforevaluatingalog basedanomalydetectionmodel with highly imbalanced data.
a small amount of data noise including mislabeled logs and log parsingerrorscandowngradeanomalydetectionperformance.
comparedtosemi supervisedmethods supervisedmodelsare moresensitivetomislabeledlogsinthetrainingdata.models capableofunderstandingthesemanticmeaningoflogdatacould reduce the impact of log parsing errors.
differentmodelshavedifferentabilitiesintheearlydetection of system anomalies.
some models can detect anomalies earlier than others.
in summary the major contributions of this work are as follows we conduct an extensive evaluation of five representative deep learning models for log based anomaly detection.
we conclude that the existing models are not evaluated comprehensivelyanddonotgeneralizewellindifferentexperimental settings.
basedontheevaluationresults wepointouttheadvantagesand disadvantagesofexistingmodels andsuggestfutureresearch work for log based anomaly detection.
log based anomaly detection with deep learning .
representative models in recentyears manydeep learning based modelshave beenproposed to analyze log data and detect anomalies .someofthesemodelsusesupervisedlearningtechniques such as logrobust and cnn while others employ semisupervisedapproaches suchasdeeplog loganomaly or unsupervised approaches .
some recent representative models are as follows deeplog.duetal.
proposedtoutilizeanlstmmodelto learn the system s normal executions by predicting the next log event given preceding events.
it detects anomalies by determining whetherornotanincominglogeventviolatesthepredictionresults of the lstm model.
their experimental results show that deeplog can achieve an f measure of .
on the hdfs dataset.
loganomaly.mengetal.
proposedloganomaly which useslogcountvectorsasinputstotrainanlstmmodel.theyalso proposed template2vec asynonymsandantonymsbasedmethod to represent log templates as semantic vectors to match new log events with existing templates.
like deeplog a forecasting based detectionmodelisdesignedtopredictthenextlogevent andiftheexaminedlogeventviolatesthepredictionresults itwillbemarked as an anomaly.
loganomaly can achieve f measures of .
and .
on hdfs and bgl datasets respectively.
plelog.yangetal.
addressedtheissueofinsufficientlabels viaprobabilisticlabelestimationanddesignedanattention based gruneuralnetworktodetectanomalies.thegru baseddetection model is built to classify log sequences into two classes normal orabnormal.
their experimental results indicate that plelog outperforms existing semi supervised methods and achieves high performance on hdfs and bgl datasets i.e.
.
and .
respectively .
logrobust.zhangetal.
incorporatedapre trainedword2vec model namelyfasttext andcombineditwithtf idfweights to learn the representation vectors of log templates.
then these vectors were input to an attention based bi lstm model to detect anomalies.theexperimentalresultsshowthatlogrobustcanaddresstheinstabilityoflogeventsandachievef measuresof0.
on the original hdfs dataset and .
.
on synthetic datasets.
cnn.l ue ta l .
applied a convolutional neural network cnn for log based anomaly detection.
logs are grouped into sessions thentransformedintoatrainablematrix.acnnmodel istrainedusingthismatrixasinputstoclassifyalogsequenceinto normal or abnormal.
the cnn model can achieve an f measure of .
on the hdfs dataset.
in this study we systematically evaluate the above five models.
deeplog and loganomaly adopt a forecasting based approach i.e.
detecting anomalies by predicting the next log event given preceding log sequences while plelog logrobust and cnn are classification based models i.e.
detecting anomalies by classifying log sequences .
apart from these models there are some other deeplearning basedapproaches.forexample logsy utilizes the transformer network to detect anomalies from log data.
autoencoder has been employedin to detectlog anomalies in an unsupervised manner along with isolation forest.
swisslog proposes to use a dictionary based log parser and an attentionbasedbi lstmnetworktodetectanomaliesfordiversefaults.as theirsourcecodeisnotpubliclyavailable wedonotexperimentally evaluate these models in this study.
.
the common workflow the common overall framework of dl models for log based anomalydetectionisshowninfigure .generally theframeworkconsists of four steps log parsing log grouping log representation anomaly detection through dl models.
.
.
log parsing.
logsare semi structuredtexts whichcontain variousfieldssuchastimestampandseverity.tofavordownstream authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
log based anomaly detection with deep learning how far are we?
icse may pittsburgh pa usa .
log parsing ... i. info dfs.datablockscanner verification succeeded for blk 4980916519894289 ...... i. verification succeeded for ... .
log grouping log sequencefixed windows sliding windows session windows3.
log representation sequential vectors quantitative vectors semantic vectors .
deep learning models rnn transformer cnnfeature extraction anomaly?
figure2 log basedanomalydetectionworkflowwithdeep learning the common workflow tasks log parsing is applied to automatically convert each log message intoa specificevent template constantpart associatedwith parameters i.e.
variable part .
for example the log template verification succeeded for can be extracted from the log message verificationsucceededforblk 4980916519894289 infigure .
here denotes the position of a parameter.
therearemanylogparsingtechniques basedonfrequentpatternmining clustering andheuristics .
the heuristics based approaches make use of the characteristicsoflogsandhavebeenfoundtoperform better thanother techniques in terms of accuracy and time efficiency .
.
.
log grouping.
themainpurposeofthisstepistoseparate logsintovariousgroups whereeachgrouprepresentsafinitechunk of logs .
these groups are called log sequences from which features are extracted and fed into anomaly detection models.
as introduced in three types of windows are applied see figure for log grouping including fixed window.
logs are grouped byfixedwindowsaccordingtotheiroccurrences.theoccurrenceis defined by the timestamp of log messages or by the order of its appearance in the dataset.
each window has a fixed size i.e.
window size which means the time span or the number of logs.
sliding window.slidingwindowsconsistoftwoattributes windowsizeand stepsize.thewindowsizecanbethetimespanorthenumberof logsinalogsequence whilethestepsizeistheforwardingdistance.
sessionwindow.
differentfromfixed slidingwindows session windows are based on the identifier of logs.
identifiers are used to grouplogsinthesameexecutionpath.forexample hdfslogsuse block idto record the execution path.
.
.
log representation.
afterloggrouping logsarerepresented in different formats required by dl models.
existing dl based anomalydetectionmodelsconvertlogsintothreemaintypes sequentialvectors quantitativevectors and semanticvectors.
sequentialvectorsreflecttheorderoflogeventsinawindow.forexample deeplog assigns each log event with an index then generates a sequential vector for each log window.
quantitative vectors are similar to log count vectors which are used to hold the occurrence of each log event in a log window.
loganomaly leverages both sequential and quantitative vectors to detect anomalies.
different from them semantic vectors are acquired from languagemodelstorepresent thesemanticmeaningoflogevents.
eachlogwindowisconvertedintoasetofsemanticvectorsforthe detectionmodels.forinstance logrobust adoptsapre trained fasttext model to compute the semantic vectors of log events.
.
.
deep learning models.
afterthelogrepresentationphase theextractedfeaturesarefedtodeeplearningmodelsfortheanomalydetectiontask.avarietyofdltechniqueshavebeenappliedto log based anomaly detection rnn.
recurrent neural networks rnns including its variants long short term memory lstm and gated recurrent units grus are neural networksdesignedtohandlethesequentialinputswitharbitrary length.
bi directional rnn is used to represent the sequential text inbothdirections i.e.
forwardandbackward .rnnanditsvariantshavebeenappliedinmanystudiesonlog basedanomalydetection.
specifically deeplog and loganomaly use lstm to predict the next log event.
logrobust applies an attention based bi lstmnetworktohandletheinstabilityoflogs whileplelog adoptsgrustobuildaclassificationmodel.
cnn.aconvolutionalneuralnetwork cnn usesconvolutionoperationtoextractmeaningful local patterns of input.
in a cnn model originally designed for sentence classification is applied to log based anomalydetection.
transformer.thetransformernetworkhas madesignificantprogressinneuralmachinetranslationandrelated pretraining tasks in recent years.
it has been applied in for log based anomaly detection.
study design .
motivation recent studies have shown that deep learning based approachescanachievehighaccuracy e.g.
f measurehigherthan on commonly used datasets e.g.
the hdfs dataset .
these results seem to suggest that the problem of anomaly detection can besolvedalmostperfectlythroughdeeplearning.toexplorethe actualeffectivenessofexistingdlmodelsforlog basedanomaly detection modelsfor short in the rest of the paper we would like to evaluate the models from the following aspects associated with the common workflow of the models .
.
the selection of training data.
the results of anomaly detection could be affected by the selection of training data.
in somestudies suchasdeeplog logrobust plelog and cnn the training and test data are selected based on the timestamp of logs.
we call it chronological strategy.
other studies apply fixed sliding windows to group log events into log sequences then shuffle all logs sequences before splitting them into training and testing sets.
we call it randomstrategy.
the random strategy allows models to see more log events and achieve high accuracy e.g.
higher than f measure for allmodels .
however this strategy may lead to the data leakage problem in the trainingphase.thatis itispossiblethatthetrainingsetcontains parts of future data and the testing set contains parts of past data authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa van hoang le and hongyu zhang thus making it not suitable for real world scenarios where we only usehistorical logsto builda detectionmodel.the effectivenessof themodelswithdifferenttrainingdataselectionstrategies random or chronological should be investigated.
.
.
the grouping of log data.
thelogdatacanbegroupedinto sequencesbysession sliding orfixedwindows.choosingaproper windowsizeischallenging.forexample ifthewindowsizeissmall themodelsfacedifficultyincapturingthoseanomaliesthatspan multiple sequences.
on the other hand if the window size is large logsequencesmightincludemultipleanomaliesandconfusethe detection scheme .
in this work we evaluate many window sizes includingthewindowsizesfrom20to200logmessagesas wellasthewindowsizesof0.5hourand1hour andalsosession window .
.
.
the imbalanced class distribution.
in literature much research work has shown that in a large software system thedistributionoffaultsisskewed thatasmallnumberofmodules accounts fora large proportionof the faults.
inour work wefind thatthedistributionofanomaliesisalsoskewed.theanomalous logsequencesusuallyaccountfortheminorityofthedataset which can be only from .
to of a dataset as shown in section .
.
the highly imbalanced data imposes challenges for anomaly detection.ingeneral itisdifficultforamachinelearningtechniqueto identify a small number of anomalies from a large amount of logs.
the imbalance between normal and abnormal classes could cause the model to perform poorly.
thehighlyimbalancedclassdistributionalsohasimplications for evaluation metrics.
the performance of log based anomalydetectionmodelsisusuallymeasuredbyprecision recall andfmeasure .however previousstudies pointed out that prediction results may not always be satisfactory in the presenceofimbalanceddatadistribution.inthiswork wewould like to explore if the commonly used metrics can effectively evaluatetheeffectivenessofalog basedanomalydetectionmodelunder the scenario of imbalanced class distribution.
.
.
the quality of data.
fortheevaluationoflog basedanomaly detection models labeled data is required.
the commonly used publicdatasets suchashdfsandbgl aremanuallyinspectedand labeled by engineers.
data noise false positives negatives may be introduced during the manual labeling process.
although the data noises only occupy a small portion of logs they could downgrade the performance of existing models.
the noise can be from theerrors in the preprocessing phase i.e.
log parsing .
the logging statementscouldalsofrequentlychangeduringsoftwareevolution resulting in new log events that were not seen in the training phase .zhangetal.
foundthat30.
oflogsarechanged in the latest version based on their empirical study on microsoft onlineservicesystems.leetal.
foundthatlogparsingerrors can lead to many incorrect log events thus downgrading anomaly detection performance.
therefore we would like to investigate the effectiveness of the models with different degrees of data noise.
.
.
early detection ability.
system anomalies can affect the normal operations of upper layer software applications and significantly affect users experience.
if no actions are taken more severe problems or even service interruptions may occur.
therefore it isimportantthattheanomaliesarecapturedearlier sothatmoremitigation actions could be taken.
an effective anomaly detection model should be able to identify the early signals of system anomalies detect the anomalies as early as possible and meanwhile achieve high detection accuracy.
this is especially essential for the online detection scenario where anomalies are detected on the fly.
because of the above concerns we argue that the capabilities of deep learning based techniques for anomaly detection shouldbere evaluated.inthiswork wedesignexperimentstomeasurethe impact of these factors on five representative dl models for log based anomaly detection.
.
evaluated models in this study we evaluate the five representative models described in section .
namely deeplog loganomaly plelog logrobust and cnn.
thesemodels have their source codepublicly available andwecanconfirmthecorrectnessofsourcecodesbyreproducing results presented in their original paper.
specifically we adopt the public implementations of deeplog and loganomaly.
forloganomaly thetemplate2vecmodelistrainedwithdomainspecificantonymsandsynonymsaddingbyoperators.sincethis informationisunavailable weuseapre trainedfasttextword2vec mode tocomputethesemanticvectorsoflogtemplates.the templatevectoriscalculatedastheweightedaverageofthevectors of the template s words.
for logrobust we adopt the implementation provided by its authors and convert it into a pytorch basedimplementation.forplelog weleverageitspublicimplementation .formodelswhosehyperparametersettingsarereported in their paper we use the same hyperparameter values.
otherwise we tune their hyperparameters empirically.
.
datasets toevaluatethestudiedmodelsforlog basedanomalydetection we selectfourpublicdatasets namelyhdfs bgl thunderbird and spirit.
the details of each dataset are as follows hdfs hadoop distributed file system dataset is produced from more than amazon ec2 nodes.
in total the hdfs dataset consists of log messages.
these log messages formdifferentlogwindowsaccordingtotheir block id reflecting aprogramexecutioninthehdfssystem.thereare16 838blocks of logs .
in this dataset indicating system anomalies.
bgl blue gene l dataset is a supercomputing system log dataset collected bylawrence livermore national labs llnl .thedatasetcontains4 963logmessages.eachmessage in the bgl dataset was manually labeled as either normal oranomalous.
there are log messages .
that were labeled as anomalous.
spiritdatasetisanaggregationofsystemlogdatafromthespirit supercomputing system at sandia national labs .
there are more than million log messages labeled as anomalous onthe spirit dataset.
in this paper we use a small set containing thefirst5millionloglinesoftheoriginalspiritdataset which contains abnormal log messages .
.
thunderbird datasetisanopendatasetoflogscollectedfroma thunderbird supercomputer at sandia national labs snl .
thelogdatacontainsnormalandabnormalmessageswhichare manuallyidentified.thunderbirdisalargedatasetofmorethan million log messages.
we leverage million continuous authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
log based anomaly detection with deep learning how far are we?
icse may pittsburgh pa usa log lines for computation time purposes which contain abnormal log messages .
.
table1summarizes the statistics of datasets used in our experiments.
.
research questions thegoalofthisstudyistoanalyzetheperformanceoftherepresentative deep learning models for log based anomaly detection models.
we design the following research questions in accordance with the evaluation aspects described in section .
.
rq1 how do the existing approaches perform with different training data selection strategies?
we want to evaluate whetherornotthestudiedmodelsareabletoachievegoodaccuracywithdifferenttrainingdataselectionstrategies.tothisend weconductexperimentswithtwodifferentstrategiesfortraining dataselection randomselection foreachdataset wefirstsort logs by timestamps and then apply the fixed window grouping technique to generate log sequences.
next these log sequences are shuffled and split into training testing sets with the ratio of .
chronological selection for each dataset we utilize the first of raw logs that appear in chronological order for training and the remaining for testing.
next we apply the fixed window groupingtechniquetogeneratelogsequences.wedonotshuffle the generated log sequences in this strategy.
therefore we can guaranteethatonlyhistoricallogsareusedinthetrainingphase and there are no future logs used in this phase.
in this rq we experiment on the bgl thunderbird and spirit datasets.
the window size for fixed window grouping is set to hour.asthehdfsdatasetdoesnotcontaintimestampinformation the chronological selection cannot be applied to hdfs and thus it is not used in this rq.
rq2 how do theexisting models perform with different data grouping methods?
to evaluate the impact of different data groupingmethodsontheperformanceofanomalydetectionmodels wechoosethefollowingthreedatagroupingmethods including fixed windowgroupingwiththewindowsizeof1hour asused in rq1 and .
hour fixed window grouping with the window sizevaryingfrom20to200logmessages and sessionwindow grouping.forthefirstcase weusebgl spirit andthunderbird datasets.
for the case of session windows we use block idand node idto group logs on hdfs and bgl datasets respectively.
rq3 can the existing approaches work with different class distributions?
as shown in section .
our subject datasets representhighlyimbalancedclassdistributionswithanomalyratios that can be only .
i.e.
on the thunderbird dataset .
to perform a more systematic evaluation we simulate different imbalancedscenariosbyrandomlyremovingthenormal abnormallog sequencesfromthesubjectdatasets.forareal worldproduction system the number of anomalies is much less than the number of normal events .
therefore we vary the imbalance ratio from0.
to15 whichindicatesthepercentageofanomaliesin thedataset.inthisway wecreatesixsyntheticdatasetswiththe imbalance ratio of .
.
and .
rq4 canexistingapproachesworkwithdifferentdegrees of data noise?
to evaluate the impact of mislabeled logs on theperformance of the studied models we randomly add some anomalies from to into the training data for semi supervised methods.forsupervisedmethods werandomlychangethelabelof a specific portion from to to simulate the mislabeled logs.
inthisway wecreatefivesyntheticdatasetswiththemislabeled proportion of and .
moreover to measure the impact of noises from log parsing errors we experiment with four commonlyusedlogparsers includingdrain spell ael and iplom .
rq5 how early can the existing models detect anomalies inonlinedetection?
asdescribedinsection .
.
amodelshould notonlydetectanomaliespreciselybutalsoshouldbeabletodetectanomaliesasearlyaspossiblesothatmoremitigationactionscould be taken.
therefore in this rq we evaluate the studied models on four datasets to investigate their ability in early detection of anom alies.
to this end we record the number of examined log messages before each model raises an anomaly alert for a log sequence in the online detection setting.
.
experimental setup in our experiments we preprocess the log data and conduct dlbased anomaly detection as follows log parsing.
to extract log templates from log data we use the log parser drain with the default parameter settings .
thelogdataisdenotedby l l1 l2 ... l i ... l nl andcontains nl entries i.e.
lines of log messages.
each log message liis parsed into a log template e li which is denoted eifor short.
log grouping.
we apply session window to group logs inthehdfsdatasetusing block id.eachsessionislabeledusing groundtruth.forotherdatasets bgl spirit andthunderbird weusethefixedwindowstrategytogrouplogdatainto nschunks i.e.
log sequences denoted as s s1 s2 ... s u ... s ns wheresu ei ei ... e j is a set of log templates.
according to the ground truth labeled by domain engineers a log sequence is abnormalifitcontainsananomalouslogmessageaccordingtotheground truth labeled by domain engineers .
fdenotes the size of each log sequence.inthisstudy wevarythevalueof ffrom20 200log messages to .
and hour depending on each research question.
logrepresentation.
logsequencesarenowconvertedinto numerical vectors which can be input to a dl model.
deeplog transformslog sequences intosequential vectorsbyassigning each log event with an index.
loganomaly leverages both sequential vectorsandquantitativevectorstotrainthemodel.theoccurrence of each log event is counted and forms the quantitative vectors whichrepresentthesystemexecutionbehaviors .plelogextracts the semantic vectors of log templates by using a pre trained glovemodel .similarly logrobustandcnnalsoconvertlogsequences into semantic vectors using a pre trained word2vec model.
we adopt the pre trained fasttext model to compute the semantic vectors for logrobust and cnn.
deeplearningmodel.
inthisstep dependingonthemethod a dl model is trained using the corresponding feature vectors generatedfromthepreviousphase.deeplogandloganomalyhave twolstmlayerswith128neurons.logrobustcontainsatwo layerbi lstmwith128neuronsandanattentionlayer.plelogutilizesa one layer gru network.
cnn has three convolutional layers with different filters and a max pooling layer for feature extraction.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa van hoang le and hongyu zhang table the statistics of datasets used in the experiments dataset log events grouping log sequences avg.
seq.
lengthtraining data testing data log sequences anomaly log sequences anomaly hdfs session random .
.
.
bgl hour random .
.
.
1hour chron.
.
.
.
100logs chron.
.
.
session random .
.
.
spirit hour random .
.
.
1hour chron.
.
.
.
100logs chron.
.
.
thunderbir d hour random .
.
.
1hour chron.
.
.
.
100logs chron.
.
.
note chron.
denotes the chronological strategy.
toavoidbiasfromrandomness weperformeachexperimentfive times and report the average results.
we conduct our experiments onawindowsserver2012r2withintelxeone5 2609cpu 128gb ram and an nvidia tesla k40c.
.
evaluation metrics tomeasuretheeffectivenessofmodelsinanomalydetection we usetheprecision recall specificity andf1 scoremetrics which are defined as follows precision the percentage of correctly detected abnormal log sequences amongst all detected abnormal log sequences by the model.prec tp tp fp.
recall the percentage of log sequences that are correctly identified as anomalies over all real anomalies.
rec tp tp fn.
specificity the percentage of log sequences that are correctly identified as normal over all real normal sequences.
spec tn tn fp.
f measure the harmonic mean of precision andrecall.
f1 prec rec prec rec.
tp truepositive isthenumberofabnormallogsequencestheare correctlydetected bythemodel.
fp falsepositive isthenumber ofnormallogsequencesthatarewronglyidentifiedasanomalies.
fn false negative is the number of abnormal log sequences that are not detected by the model.
results and findings .
rq1 performance with different training data selection strategies?
forrq1 weapplyfixedwindowgroupingwiththesizeof1hour togeneratelogsequencesonbgl spirit andthunderbirddatasets seetable .thetrainingdataisselectedbyrandomorchronological selection.
the experimental results are shown in table .
we find that for semi supervised models i.e.
loganomaly deeplog andplelog theresultswithrandomselectionaremuch betterthanthosewithchronologicalselection.forexample deeplog achieves an f measure of .
with the random selection of training data on the bgl dataset.
when training and testing sets areseparated by the time order i.e.
chronological selection the fmeasuredropsto0.
.thereasonisthatwithrandomselection themodelscanseefuturelogeventsinthetrainingphase i.e.
data leakage thereforetheycanmakemoreaccuratepredictions.besides deeplog and loganomaly train the models using the index oflogevents i.e.
sequentialandquantitativevectors andignore thesemanticmeaning oflogsduringthe trainingphase.deeplog marksanynewlogeventsasanomaliesandproducesmanyfalse alarms.
loganomaly can simply match some unseen log events withthoseappearinginthetrainingphase butitisnotadequate compared to those models that are trained through the semantic understanding of logs.
thesupervisedmodels i.e.
logrobustandcnn performmuch betterthanthesemi supervisedmodelsonbothstrategiessincethe models are trained with a large amount of normal and abnormal data.forexample thesetwomodelsachievearound0.94f measure onthethunderbirddatasetwiththechronologicalsetting while othersperformpoorly.anotherreasonfortheseresultsistheadvantages of semantic vectors used by these models which can identify thesemanticallysimilarlogeventsandalsodistinguishdifferentlogevents .still wecanseethatingeneraltheresultswithrandom selection are better than those with chronological selection.
our experimental results confirm that models perform better withrandomselection.thedataleakageproblemisareasonforthe goodperformanceofsomedl basedloganomalydetectionmodels e.g.
loganomaly which uses the random strategy in their evaluation .
due to this problem we suggest that chronologicalselection should be applied to evaluate the effectiveness of the modelsinreal worldscenarios.hence forotherrqs wewillapply the chronological selection to group log messages.
summary.
thetrainingdataselectionstrategieshavesignificantimpactonthesemi supervisedlog basedanomalydetection models.
although the random selection strategy leads to better results than the chronological selection strategy it could cause thedataleakageproblemandfailtoevaluatetheeffectiveness of the models in real world scenarios.
.
rq2 how do the existing models perform with different data grouping methods?
in rq1 we use the fixed window size of hour logs.
in this rq wetrainthemodelsonthreedatasetsusingchronologicalselection authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
log based anomaly detection with deep learning how far are we?
icse may pittsburgh pa usa table comparison of model performance with random selection and chronological selection of training data modelbgl random chronological selection spirit random chronological selection thunderbird random chronological selection prec rec spec f1 prec rec spec f1 prec rec spec f1 deeplog .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
loganomaly .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
plelog .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
logrobust .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
cnn .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
with fixed window grouping of various sizes i.e.
log messages log messages log messages and .
hour logs .
table shows the results.
table results of models with fixed window grouping of different sizes modelbgl spirit thunderbird 20l 100l 200l .5h 20l 100l 200l .5h 20l 100l 200l .5h deeplogp .
.
.
.
.
.
.
.
.
.
.
.
r .
.
.
.
.
.
.
.
.
.
.
.
s .
.
.
.
.
.
.
.
.
.
.
.
f1 .
.
.
.
.
.
.
.
.
.
.
.
loganomalyp0.
.
.
.
.
.
.
.
.
.
.
p .
.
.
.
.
.
.
.
.
.
.
.
s .
.
.
.
.
.
.
.
.
.
.
.
f1 .
.
.
.
.
.
.
.
.
.
.
.
plelogp0.
.
.
.
.
.
.
.
.
.
.
.
r .
.
.
.
.
.
.
.
.
.
.
.
s .
.
.
.
.
.
.
.
.
.
.
.
f1 .
.
.
.
.
.
.
.
.
.
.
.
logrobustp0.
.
.
.
.
.
.
.
.
.
.
.
r .
.
.
.
.
.
.
.
.
.
.
.
s .
.
.
.
.
.
.
.
.
.
.
.
f1 .
.
.
.
.
.
.
.
.
.
.
.
cnnp0.
.
.
.
.
.
.
.
.
.
.
.
r .
.
.
.
.
.
.
.
.
.
.
.
s .
.
.
.
.
.
.
.
.
.
.
.
f1 .
.
.
.
.
.
.
.
.
.
.
.
20l logs 100l logs 200l logs .5h .
hour logs.
the results show that different window sizes lead to different performance of detection models.
we can observe that on the bgl dataset comparedtotheresultsof1 hour logssettingintable the performance of the models mostly drops.
for example on the bgldataset the decrease of f1 measure ranges from .
plelog to .
loganomaly whenthe100 log messagessettingisused.
onthespiritdataset deeplogandloganomalyachievethebest results when using the window size of log messages while others perform the best with .
hour logs setting.
similar results can be found on the thunderbird dataset where the detection performance is different across different window sizes.
wenextevaluatetheperformanceofmodelsonsessionwindow grouping.logscanbegroupedbasedontheidentifiers i.e.
node id andblock idforbglandhdfsdatasets respectively torepresent theexecutionpathofatask.weperformanomalydetectionafter each session ends .
we do not evaluate this rq on other datasets i.e.
spirit and thunderbird because they do not have theidentifierinformation suchas block id intheirlogmessages.
table4shows the results.
it is obvious that the results using session windows on the bgl dataset are better than those using fixed windows.
moreover compared to the results on the bgl dataset in table we can findtable4 resultsofmodelswithsessiongroupingonbgland hdfs datasets modelbgl hdfs prec rec spec f1 prec rec spec f1 deeplog .
.
.
.
.
.
.
.
loganomaly .
.
.
.
.
.
.
.
plelog .
.
.
.
.
.
.
.
logrobust .
.
.
.
.
.
.
.
cnn .
.
.
.
.
.
.
.
thattheperformanceisimprovedusingthesessionwindow.the reason could be that the log events in an execution path exhibitmany relations which can be captured and utilized for anomalydetection.onthehdfsdataset allmodelsalsoachieve good performance all f measure values are higher than .
.
summary.
thedatagroupingmethodscouldhavesignificant impact on the log based anomaly detection models.
with fixed windowgrouping modelstendtoperformunsteadilywhendealing with different window sizes.
grouping by session windows could lead to better results.
.
rq3 how do the existing approaches perform with different class distributions?
table2showstheresultsonthesubjectdatasets whichhavedifferent ratios of anomalies.
for example we can see that on the spiritdataset deeplogandlog anomalyachievehighprecision recall and f measure all higher than .
with random selection.
however thespecificityresultsarelow lessthan0.
whichreveal that the models actually perform poorly they classify a lot of normallogsasanomaliesinthisscenario causingmanyfalsealarms.
this also happens on the thunderbird dataset when deeplog and loganomalymarkmostlyallofthenormallogsasanomalies specificity .
in this rq to perform a more systematic evaluation we simulate different imbalanced scenarios with the percentage of anomalies increased from .
to .
we use the results on hdfs and bgl datasets to explain our findings as shown in table .
from table we find that when the percentage of anomalies increases the performance of the models is better which is indicated by the increase of all four metrics.
for example the scores of logrobust on bgl dataset are improved by .
.
.
and .
in precision recall specificity and f measure respectively.
the result shows that it is more difficult to detect anomalies when the dataset is highly imbalanced.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa van hoang le and hongyu zhang table results with different class distributions modelhdfs bgl .
.
.
.
deeplogp .
.
.
.
.
.
.
.
.
.
.
.
r .
.
.
.
.
.
.
.
.
.
.
.
s .
.
.
.
.
.
.
.
.
.
.
.
f1 .
.
.
.
.
.
.
.
.
.
.
.
loganomalyp0.
.
.
.
.
.
.
.
.
.
.
.
r .
.
.
.
.
.
.
.
.
.
.
.
s .
.
.
.
.
.
.
.
.
.
.
.
f1 .
.
.
.
.
.
.
.
.
.
.
.
plelogp0.
.
.
.
.
.
.
.
.
.
.
.
r .
.
.
.
.
.
.
.
.
.
.
.
s .
.
.
.
.
.
.
.
.
.
.
.
f1 .
.
.
.
.
.
.
.
.
.
.
.
logrobustp0.
.
.
.
.
.
.
.
.
.
.
.
r .
.
.
.
.
.
.
.
.
.
.
.
s .
.
.
.
.
.
.
.
.
.
.
.
f1 .
.
.
.
.
.
.
.
.
.
.
.
cnnp0.
.
.
.
.
.
.
.
.
.
.
.
r .
.
.
.
.
.
.
.
.
.
.
.
s .
.
.
.
.
.
.
.
.
.
.
.
f1 .
.
.
.
.
.
.
.
.
.
.
.
previousstudiesonsoftwaredefectpredictionmodelsshowthat predictionresultsmaynotalwaysbesatisfactoryinthepresenceof imbalanceddatadistribution .basically ahighprobabilityofdetection i.e.
true positive rate and a low probability of false alarm i.e.
false positive rate do not necessarily lead to high precision due to the imbalanced class distributions.
our results confirm that finding.
as a consequence the commonly used evaluation metrics precision recall and f measure are not capable of evaluating models in some imbalanced data scenarios and may lead to imprecise evaluation.
therefore we propose to use an additional metric specificity toevaluatelog basedanomalydetectionmodelsmore comprehensively.
specificity which is the percentage of log sequences that are correctly identified normal over all real normalsequences canmeasuretheprobabilityoffalsealarms.high specificitymeansthatmodelscanperformwithalowfalse positive false alarms rate.
summary.
highly imbalanced data with a small percentage of anomalies impedes model performance.
using different metrics may lead to different conclusions about the model performance.
some commonly used metrics including precision recall and f measure arenotcomprehensiveenoughforevaluatinglogbased anomaly detection with highly imbalanced data.
more evaluation metrics such as specificity should be used for a thorough evaluation.
.
rq4 can existing methods work with different degrees of data noise?
.
.
the impact of mislabeled logs.
we evaluate the studied models with different degrees of mislabeled logs.
in this experiment we inject a specific portion of mislabeled logs into the training data while the testingsets remain the same.
specifically for semisupervisedmethods whichonlyusenormallogsfortraining we put back some anomalies to the training sets.
for supervised methods we change the label of some anomalies in the training setsto normal.
we experiment on all four datasets and find that theperformance of models can greatly decrease if training data containsmislabeledsamples.weshowtheresultsonhdfsandbgl datasets in figure 3to demonstrate our finding.
figure results with different ratios of mislabeled logs we can find that on the hdfs dataset even with just of mislabeledlogs thef measuresofthestudiedmodelsdeclinesignificantly.
for example logrobust and cnn drop .
and .
withonly1 ofnoise respectively.whenthenoiseratioreaches the f measures of logrobust and cnn drop to only .
and .
.theresultsconfirmthatevenwiththeadvantageofsemantic understanding thesedlmodelscanlosetheirperformancewith onlyasmallproportionofmislabeledlogs.itisalsotrueforplelog whichleveragesthesemanticmeaningoflogsaswell.anotherreason for this remarkable reduction is that there are many duplicate log sequences in the hdfs dataset thus labeling any duplicatenormal sequences as anomalies could have a large impact.
inter estingly the f measure of loganomaly only drops slightly with ofmislabeledlogs from0.915to0.
.whenthemislabeled ratio is and loganomaly can achieve better f measurescompared to other models.
the reason is that loganomaly uses quantitativevectorstoextractquantitativerelationshipsholding inlogsalongwithsequentialvectors thus allowingthemodelto predict the possibilities of the next event more precisely.
summary.
asmallamountofmislabeledlogscanquicklydowngrade the performance of anomaly detection.
supervised modelsaremoresensitivetomislabeledlogs.modelsadoptingthe forecasting based approach deeplog and loganomaly perform better with the presence of mislabeled logs.
.
.
the impact of log parsing errors.
wealsoevaluatetheimpact ofdatanoiseintroducedbylogparsingerrors.logparsingerrors canleadtoextralogeventsandwronglogtemplates .weexperimentwithfourcommonly usedlogparsers namelydrain spell ael andiplom onallfourdatasets.wefind that the performance of models is highly influenced by log parsers.
to demonstrate our findings we show the performance of models with different log parsers on bgl and spirit datasets in table .
we can observe that the performance of studied models varies a lot with different log parsers.
for example deeplog achieves an f measureof0.755andaspecificityof0.545withtheiplom parseronspiritdataset.whenexperimentingwithdrain these authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
log based anomaly detection with deep learning how far are we?
icse may pittsburgh pa usa table results with different log parsers modelbgl spirit drain spell ael iplom drain spell ael iplom deeplogp .
.
.
.
.
.
.
.
r .
.
.
.
.
.
.
.
s .
.
.
.
.
.
.
f1 .
.
.
.
.
.
.
.
loganomalyp0.
.
.
.
.
.
.
.
r .
.
.
.
.
.
.
.
s .
.
.
.
.
.
.
f1 .
.
.
.
.
.
.
.
plelogp0.
.
.
.
.
.
.
.
r .
.
.
.
.
.
.
.
s .
.
.
.
.
.
.
.
f1 .
.
.
.
.
.
.
.
logrobustp0.
.
.
.
.
.
.
.
r .
.
.
.
.
.
.
.
s .
.
.
.
.
.
.
.
f1 .
.
.
.
.
.
.
.
cnnp0.
.
.
.
.
.
.
.
r .
.
.
.
.
.
.
.
s .
.
.
.
.
.
.
.
f1 .
.
.
.
.
.
.
.
values drop to .
and .
respectively although drain is one ofthemostaccuratelogparsersaccordingtoarecentbenchmark study .
the results also show that logrobust and cnn can handle log parsing noise better than other models.
this is probably because of their use of semantic vectors.
moreover we findthat different log parsing errors have distinctive impact on thedetection models.
the results on bgl and spirit datasets show that deeplog and loganomaly perform better with spell and iplom than with drain.
this is because drain often produces many extra log events that hinder the performance of deeplogand loganomaly which use forecasting methods to predict the nextlogevent .incontrast othermodelscanbetterhandleextra log events but may fail when log parsers produce errors due to semantic misunderstanding .
summary.
thedatanoisefromlogparsingerrorshasimpacton the performance of models.
extra log events can quickly downgradetheperformanceofforecasting basedmodels.methods using semantic vectors can better handle log parsing errors.
.
rq5 how early can the existing models detect anomalies in online detection?
to answer rq5 we evaluate the five studied modelsusing a fixedwindow grouping of different sizes from logs to hour logs .
werecordthenumberofexaminedlogmessagesbeforeeachmodel can detect an anomaly in an online detection mode i.e.
detecting anomaliesonthefly .figure 4usestheresultswith100 logsand .
hour logs settings to explain our findings.
we find that deeplog can detect anomalies the earliest with an average of .
and .
log messages with logs and .5hour logssetting respectively.loganomaly whichisalsobased on predicting the next log events detects anomaly a bit later since itrequirestocapturebothsequentialandquantitativerelationships a data grouping with logs b data grouping with .
hour logs figure the number of examined log messages before each model can detect an anomaly inordertomarkalogsequenceasabnormal.incontrast theuse of semantic vectors could make other models including logro bust cnn and plelog detect anomalies much later as shownin figure .
these classification based models require to capture features of anonymous behavior to detect anomalies thus tendto raise alarms closer to the time the anomaly happens.
figure shows a case study on an anomalous log sequence with the sizeof logs on bgl dataset.
five models are applied to identify whetherthelogsequenceisabnormalornot.wefindthatdeeplog raises analarm afterexamining 11logs followed byloganomaly logrobust andcnn whileplelogisthelastmodelthatraisesan alarm after log messages arrived .
as efficiency is critical in online anomaly detection we also evaluatethestudiedmodelsbyrecordingthetimespentonboth thetrainingandtestingphases.onbgldataset deeplogspends .
and .
minutes on training and testing.
logrobust and cnn whichusehigher dimensioninputsandmorecomplexnetworks consume31.7and28.3minutesontrainingandtesting respectively.
in the testing phase logrobust and cnn only take .
minutes.
loganomaly whichusestwolstmnetworkstolearnsequential and quantitative features spends .
and .
minutes on trainingandtesting.plelog whichcontainsaclusteringmoduleand a gru module is the most time consuming model.
specifically plelog spends .
minutes and .
minutes to train the clustering and gru models respectively.
in the testing phase plelog authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa van hoang le and hongyu zhang consumes .
minutes to process the bgl dataset.
the results showthatplelogmaybeinappropriateforonlinedetectiondue to its incapability of early detection and heavy time cost.
beginning enddeeplogloganomaly logrobustcnnplelog figure timeline for detecting an anomaly in the bgl dataset summary.
different models have different abilities in the early detection of system anomalies.
forecasting based models deeplogandloganomaly candetectanomaliesearlier than classification based models plelog logrobust and cnn .
discussion .
the advantages and disadvantages of the studied methods based on our findings we can conclude that all the studied models donotalwaysworkaswellastheyclaimedintheirpapers.different scenarios have different impacts on the performance of anomaly detection models.we pointout the advantagesand disadvantages of each model as follows which are also summarized in table .
deeplog.
the main advantage of deeplog is that it does not require any abnormal logs to build a detection model using sequential vectors thus reducing the effort for model construction.
it can also detect anomalies earlier than other models.
however due to thischaracteristic deeplogperforms poorlyon more complex datasets with a large number of log events see section .
.
besides deeplogisgreatlyimpactedbythelogparsingerrorssinceit only leverages the index of log templates and ignores the semantic meaning of log templates see section .
.
.
loganomaly.
loganomaly uses sequential and quantitative vectors to train a detection model which can help reduce the impact of data noise caused by mislabeled logs.
like deeplog loganomaly can detect anomalies early and deal with a largeamount of data since it only trains with normal logs.
the main benefit of loganomaly is in the phase of matching similar log templates using semantic vectors.
this feature allows loganomaly to improvetheaccuracybymatchingnewlogtemplateswithanexisting one in the training logs instead of marking them as anomalies as deeplog does.
however like deeplog loganomaly trains the model using the index of log event sequential and quantitative vectors andcannotlearnthesemanticmeaningoflogtemplates so it is highly affected by the log parsing errors see section .
.
.
moreover loganomalycannotperformwellonlargedatasetswith numerous log events see section .1and4.
.
plelog.
the main advantage of plelog is that it can learn knowledge about historical anomalies via probabilistic label estimation.
plelog adopts a clusteringmethod i.e.
hdbscan toprobabilisticallyestimatethelabelsofunlabeledlogsequences.
thisapproachallowsplelogtoworkwithonlynormallogs.besides theuseofsemanticvectorsandattention basedgrunetworkmakes plelog perform more effectively.
however plelog is timeconsuming since it requires time to train the clustering model see section4.
.
moreover plelog cannot cope well with the noise in training data see section .
.
and it does not perform well on the early detection task.
logrobust.logrobustleveragessemanticvectorsoflogtemplatestogetherwithanattention basedbi lstmmodel.logrobust canworkwellwiththenoisefromlogparsingerrorsbyutilizingtheattention basedbi lstmmodel whichhastheabilitytocapture the contextual information of log sequences.
however as the main characteristic of supervised models logrobust requires both normal and abnormal data in the training phase which would cost much manual labeling effort.
another drawback of logrobust is thatitcanbegreatlyaffectedbythenoisefrommislabeledlogs see section4.
.
.
cnn.theconvolutionalneuralnetworkcanminemorerelationshipsinlog contextbyleveragingmultiple filters.theconvolution operation allows cnn to capture not only the correlation between log templates but also the correlation inside the semantic embeddingoflogtemplates .cnncanachievehighaccuracyon many datasets using a supervised approach.
however like logrobust cnn requires a large amount of labeled data which is mostly unavailableinpractice.cnnalsolosesitsaccuracywhendealing withhighlyimbalanceddataanddatanoise seesections .3and .
.
.
.
future research work basedonourfindings weidentifythefollowingresearchchallenges and also suggest possible solutions a variety of datasets.
our findings suggest that more datasets should be used for a more comprehensive evaluation of log based anomalydetectionmodels.agoodresultononedatasetdoesnot necessarily reflect good performance on other datasets due to a variety of data characteristics e.g.
class distributions noise etc.
.
limited labeled data.
although supervised learning based methods i.e.
logrobust and cnn can achieve higher accuracy than theunsupervisedcounterparts itistime consumingandtedious to manually label the anomalies due to the volume and velocity of log data.
semi supervised learning based models i.e.
deeplog loganomaly and plelog can deal with a large amount of data as they only require normal data.
however the accuracy achieved by existing methods is rather low in practice as shown in section .
.
improving the accuracy of semi supervised models or designing unsupervised models is a challenging but essential future work.
early detection.
our findings show that different models have differentabilitiesinearlydetectionofsystemanomalies.theanomalies should be predicted as early as possible to allow enough time for any preparatory or preventive actions in an online detection scenario.
more work is needed to build effective models that allow sufficientleadtime i.e.
reducingthenumberoflogmessagesbeing examined and meanwhile achieve high prediction accuracy.
evolvingsystems.
ourfindingsshowthatlogparsingerrorshave impact on log based anomaly detection.
log parsing errors canbe introduced by the change of logging statements during software evolution .
as real world software systems constantly evolve newlogeventsalwaysappear .therefore existing log based anomaly detection models will either fail to work due to authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
log based anomaly detection with deep learning how far are we?
icse may pittsburgh pa usa table a comparison of different log based anomaly detection approaches with deep learning model pros cons data requirements deeplog relatively simple only require normal data.
good at detecting anomalies early detection.perform poorly on complex datasets.
heavilyimpacted by the log parsing errors.
normal labeled andsimple data.
loganomaly only require normal data.
can match new logtemplates with existing ones in the training data.
perform poorly on complex datasets.
heavilyimpacted by the log parsing errors.
normal labeled andsimple data.
plelog only require normal and unlabeled data.
goodperformance compared to semi supervised methods.can learn the semantic meaning of log templates.cannot work well with the noise in training dataand in the early detection task.
complex designand time consuming.partially labeled data.
logrobust good performance on many datasets.
can capturethe contextual information of log sequences tohandle log parsing errors and the instability of logs.require a large amount of labeled data.
heavilyimpacted by mislabeled logs.
fully labeled data.
cnn good performance on many datasets.
can handle log parsing errors.require a large amount of labeled data.
heavilyimpacted by mislabeled logs.
fully labeled data.
theincompatibilitywithnewlogsorresultinlowperformancedue to the incorrect classification.
therefore models should be able to learnthesemanticmeaningofthewholelogmessagestohandle the instability of logs of evolving systems.
relations among log events.
as discussed in section .
session window grouping gathers logs in a specific execution path which could possess more relations among log events than the fixed window grouping method.
existing methods convert logs intosequences whichcapturesequentialrelationshipsamonglog messages.
a possible research direction is to explore more relation shipsbetweenlogs suchasthelogicalrelationshipsandinteractive relationships among logging components to capture a variety of anomalous behavior.
.
threats to validity duringourstudy wehaveidentifiedthefollowingmajorthreats to the validity.
limitedmodels.inthiswork weonlyexperimentallyevaluate five representative models that have publicly available source code.
inthe future wewill aimtore implement thedl modelsthatdid not release their source code based on the descriptions in their papers and then perform a larger scale evaluation.
reimplementation.
we mainly adopt the public implementations of studied models.
for logrobust as its original imple mentation is based on keras we convert it into a pytorch basedimplementation so that we have a unified framework for all log based anomaly detection tools.
in our version we use the same hyperparametersthatareprovidedbytheauthorsoflogrobust.for loganomalyandcnn weusethefasttextmodeltoreplacethe missing semantic embedding components from public implementations.
to reduce this threat we experiment on the same settings anddatasetsfromtheoriginalpaperandconfirmthatourresults are similar with the reported values.
limited datasets.
our experiments are conducted on four publiclogdatasets.althoughtheyarewidelyusedinexistingstudies on log based anomaly detection they may not represent all characteristicsoflogdata.toovercomethisthreat wecreatesynthetic datasetsforevaluatingmodelswithdifferentdatacharacteristics e.g.
differentclassdistributionsanddifferentlabelingnoise .inthe future wewillexperimentonmoredatasets includingindustrial datasets to cover more real world scenarios.data quality.
our experiments are conducted based on four datasetsthataremanuallyinspectedandlabeledbyengineers.however ourexperiment ondata noisesshows thatasmall portionof mislabeledlogs coulddowngrade theperformanceof anomalydetection models.
to reduce this threat we experiment with four publicdatasets.wealsocreatesyntheticdatasetsbyinjectingaspe cificportionofmislabeledlogs.inourfuturework wewillexplore methods for measuring and improving the quality of datasets.
conclusion we have conducted an in depth analysis of recent deep learning models for log based anomaly detection.
we have investigated several aspects of model evaluation training data selection strategies different characteristics of datasets and early detection capability.
ourresultspointoutthatalltheseaspectshavelargeimpactontheevaluationresultsandtheperformanceofthemodelsisoftennotasgood as expected.
our findings show that the problem of log based anomaly detection has not been solved yet.
we also suggest some possible future work.
we hope that the results and findings of our studycanbeofgreathelpforpractitionersandresearchersworking on this interesting area.
our source code and detailed experimental data are available at the datasets including the synthetic data that is used for evaluating modelswithdifferentdatacharacteristics canalsoserveasabenchmark for evaluating future log based anomaly detection models.