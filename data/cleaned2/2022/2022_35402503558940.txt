whatimproves developer productivityatgoogle?
codequality lancheng google unitedstates lancheng google.comemersonmurphy hill google unitedstates emersonm google.commarkcanning google unitedstates argusdusty google.com ciera jaspan google unitedstates ciera google.comcollingreen google unitedstates colling google.comandrea knight google unitedstates aknight google.com nanzhang google unitedstates nanzh google.comelizabeth kammer google unitedstates eakammer google.com abstract understanding what affects software developer productivity can help organizations choose wise investments in their technical and social environment.
but the research literature either focuses on whatcorrelateswithdeveloperproductivityinecologicallyvalid settingsorfocusesonwhatcausesdeveloperproductivityinhighly constrainedsettings.inthispaper webridgethe gapbystudying software developers at google through two analyses.
in the first analysis weusepaneldatawith39productivityfactors findingthat code quality technical debt infrastructure tools and support team communication goals and priorities and organizational change andprocessareallcausallylinkedtoself reporteddeveloperproductivity.
in the second analysis we use a lagged panel analysis to strengthenourcausalclaims.wefindthatincreasesinperceived code quality tend to be followed by increased perceived developer productivity butnotviceversa providingthestrongestevidence todate that code quality affects individualdeveloperproductivity.
ccsconcepts softwareanditsengineering softwarecreationandmanagement software development process management general and reference empirical studies .
keywords developerproductivity code quality causation paneldata acm referenceformat lan cheng emerson murphy hill mark canning ciera jaspan collin green andrea knight nan zhang and elizabeth kammer.
.
what improvesdeveloperproductivityatgoogle?codequality.in proceedings esec fse november 14 18 singapore singapore copyright held bytheowner author s .
acm isbn978 .
14 18 singapore singapore.
acm new york ny usa 12pages.
introduction organizationswanttomaximizesoftwareengineeringproductivity so that they can make the best software in the shortest amount oftime.whilesoftwareengineeringproductivity isessentialfor numerousenterprisesandorganizationsinmostdomains and canbeexaminedthroughmultiplelenses understandingthe productivity of individual software developers can be especially fruitful because it has the potentialto be improved through many actions e.g.fromtoolingtoprocesschanges andbymanystakeholders fromindividualdeveloperstoexecutives .however itis difficulttoknowwhichactions willtrulyimproveproductivityin an ecologically valid setting thatis in a way thataccurately characterizesproductivityinarealisticsoftwaredevelopmentcontext.
this motivates ourresearch question rq whatcausesimprovementstodeveloperproductivity in practice?
a wide spectrum of prior research provides some answers to this question but with significant caveats.
for example at one end of the research spectrum ko and myers controlled experiment showed that a novel debugging tool called whyline helped java developers fix bugs twice as fast as those using traditional debugging techniques .
while this evidence is compelling organizational leadersarefacedwithmanyopenquestionsaboutapplyingthese findingsinpractice suchaswhetherthedebuggingtasksperformed in that study are representative of the debugging tasks performed by developers in their organizations.
at the other end of the spectrum murphy hillandcolleaguessurveyeddevelopersacrossthree companies findingthatjobenthusiasmconsistentlycorrelatedwith highself ratedproductivity .butyetagain anorganizational leaderwouldhaveopenquestionsabouthowtoapplytheseresults thiswork islicensedunderacreativecommonsattribution4.0international license.
esec fse november14 18 singapore singapore l.cheng e. murphy hill m. canning c.jaspan c.green a.knight n.zhang e. kammer table hypotheticalcross sectionalsurvey response data.
productivity rating codequalityrating arujsomewhat productive medium quality rusla extremelyproductive extremelyhigh quality table more hypothetical survey responses collected monthsafterthedataintable .plusses andminuses indicatethedirection ofthechange sincethepriorsurvey.
productivity rating codequalityrating arujhighly productive high quality rusla somewhat productive high quality such as whether some unmeasured third variable causes bothhigh productivityandhigh jobenthusiasm.
more broadly theseexamples illustrate thefundamental limitations of prior approaches to understanding developer productivity.
ononehand softwareengineeringresearchthatusescontrolledexperiments canhelpshowwithahigh degreeofcertaintythatsome practices and tools increase productivity yet such experiments are by definition highly controlled leaving organizations to wonder whether the results obtained in the controlled environment will alsoapplyintheirmorerealistic messyenvironment.ontheother hand researchthatusesfieldstudies oftenwithcrosssectional datafromsurveysortelemetry canproducecontextuallyvalidobservations about productivity but drawing causal inferences from fieldstudiesthatrivalthosedrawnfromexperimentsischallenging.
our studybuilds onthe existingliterature aboutdeveloper productivity contributing the first study to draw strong causal conclusions in an ecologically valid context about what affects individual developer productivity.
motivation the paper s main technical contribution the ability to draw stronger causal inferences about productivity drivers than in prior work isenabledbytheuseofthe paneldataanalysis technique .
in this section we motivate the technique with a running example.
much of the prior work on developer productivity section reliesoncross sectionaldata .toillustratethelimitationsofcrosssectionaldata letusintroduceanexample.considerasurveythatasks aboutrespondents productivityandthequalityoftheircodebase.
thesurveyisdistributedatalargecompany andtwodevelopers respond aruj and rusla.
let s assume their responses are representative of thedeveloper population.
their survey responses are shownintable .
from this survey we see that productivity correlates with code quality.butwecannotconfidentlysaythathighcodequalitycauses high developer productivity due in part to the following confoundingexplanations time invariant effects.
these are effects that have the same influenceover time.for example if rusla went to college and aruj did not from cross sectional data we cannot distinguish between the effect of college and the effect of code qualityonproductivity.
respondent independenttimeeffects.
theseareeffects that influence all respondents uniformly such as seasonal effects or company wide initiatives.
for example prior to the survey perhaps all engineers were given their annual bonus artificiallyraising everyone sproductivity.
non differentiated response effects.
these are effects whererespondentswillgivethesameorsimilarresponsesto everysurveyquestion sometimesknownasstraightlining.
for example perhaps aruj tends to choose the middle optiontoeveryquestionandruslatendstoanswerthehighest optionfor every question.
we use panel analysis to address these confounds enabling strongercausalinferencethanwhatcanbeobtainedfromcrosssectionaldata .thepowerofpaneldataisthatitusesdatacollected at multiple points in time from the same individuals examining howmeasurements changeover time.
toillustratehowpaneldataenablesstrongercausalinference letusreturntotherunningexample.supposewerunthesurvey again three months later andobtain the data shownintable .
oneinterestingobservationisthatifweanalyzetable 2inisolation we notice that there snot acorrelationbetween productivity and codequality bothrespondentsreportthesamecodequality regardlessoftheirproductivity.butmoreimportantly lookingat thechangesin responsesfrom table 1andtable we see productivitychangesarenowcorrelated aruj sincreasingproductivity correlates with increasing code quality and rusla s decreasing productivitycorrelates withdecreasing code quality.
panel analysis rules out the three confounding explanations present inthe cross sectionalanalysis in the cross sectional analysis we could not determine if rusla s high productivity was driven by her college education.
but in this panel analysis we can rule out that explanation becausecollegeisatimeinvariantexposure it theoreticallywouldhavethesameeffectonherproductivity in the first survey as in the second survey.
this ability to rule out other potential causes that are time invariant exists whether or not the researcher can observe those potential causes.whilewithcross sectionalanalysis researchersmay be able to control for these potential causes using control variables researchers have to anticipate and measure those controlvariablesduringanalysis.thisisunnecessarywith panel analysis because time invariant factors are eliminated bydesign.
in the cross sectional analysis we could not determine if productivitywasdrivenbyarecentannualbonus.thisexplanation is ruled out in the panel analysis.
if the annual bonushadaneffect thechangeinproductivityscoresacross both participants wouldbe uniform.
in the cross sectional analysis we could not determine if respondents were just choosing similar answers to every question.
this explanation is also ruled out with panel analysis.
if respondents were choosing similar answers there would be no change in productivity scores and thus we wouldnot see acorrelation among the changes.
1303what improvesdeveloperproductivityatgoogle?
code quality esec fse november14 18 singapore singapore the ability of panel analyses to draw relatively strong causal inferences makes it a quasi experimental method combining some ofthe advantagesofexperiments withthoseoffield studies .
related work toanswerresearchquestionssimilartoours severalresearchers previously investigated what factors correlate with developer productivity.petersen ssystematicmappingliteraturereviewdescribes seven studies that quantify factors that predict software developer productivity factors largely drawn from the cocomo ii software cost driver model .
for instance in a study of projects from companies maxwell and colleagues found that certain toolsandprogrammingpracticescorrelatedwithprojectproductivity as measured by the number of lines of written code per month .
more broadly a recent study explored what factors correlate withindividualdevelopers self reportedproductivityat threecompanies .incontrasttoourstudy thesepriorstudies report correlations withrelativelyweakcausalclaims.
otherresearchershavebeenabletomakestrongercausalclaims aboutprogrammerproductivitybyrunningcontrolledexperiments.
for instance when tosun and colleagues asked professionals to completeasimpleprogrammingtask eitherusingtest drivendevelopment treatmentgroup oriterativetest lastdevelopment control group theyfoundthattreatmentgroupparticipantsweresignificantly more productive than control group participants where productivity was measured as the number of tests passed in a fixed amount oftime .
suchrandomized controlled experiments are considereda goldstandard becausetheycanmakeverystrong causalclaims .
the challengewithsuch studies isthat theyare expensive to run with high ecological validity.
consequently such studies typically use students as participants rather than professionals e.g.
usesmallproblemsandprogramsrather than more realistic ones e.g.
and can only vary one or two productivity factors per experiment e.g.
.
while the studypresentedhere cannotmakeasstrongcausalclaimsas experiments the present field study has higher ecological validity thanexperimental studies.
toaddressthesechallenges softwareengineeringresearchers have been using causal inference techniques in field studies where stronger causal claims can be made than in studies with simple correlations.
the core of such studies is analyses that leverage time series data rather than cross sectional data.
for instance wang and colleagues use granger s causality test to infer that women spullrequestscauseincreasesinthosewomen sfollower counts .
as another example using the bayesian causalimpact framework martinandcolleaguesshowthat33 ofappreleases caused statistically significant changes to app user ratings .
thesepapersusedfine grainedtimeseriesdata whichisnotpossibleforthetypeofdatadescribedinthispaper andtoourknowledge has not been appliedto studiesofdeveloper productivity.
panel analysis another causal inference technique has been usedbypriorsoftwareengineeringresearchers.qiuandcolleagues used github panel data to show that social capital impacts the prolongedengagementofcontributorstoopensource .islam and colleagues used panel data to show that distributed version controlsystems reducetheprivatecostsforparticipantsinanossprojectandthusincreasesthenumberofparticipants butdecreases theaveragelevelofcontributionbyindividualparticipants .
likethesepapers weusepaneldatatomakecausalinferences but inour case the inferences are aboutdeveloper productivity.
panel analysis methods towards answering our research question we next describe our data sources dependent variables independent variables panel data andmodelingdesign.
.
data sources the data of this study comes from two sources google engineers logs data and a company wide survey at google.
neither source wasbuiltspecificallyfortheresearch wedescribehere andsowe considerthis opportunisticresearch.
.
.
logs data.
we collected a rich data set from engineers logs from internal tools such as a distributed file system that records developers edits a build system and a code review tool.
this data contains fine grained histories of developers work enabling us to make accuratemeasurements ofactual workingbehavior such as the time developers spend actively writing code.
the data helps uscharacterizedevelopers workpractices suchaswhatkindsof developmenttasks theyare longthosetaskstake and how long they are waiting for builds and tests to complete.
details onthesetools howdataisaggregatedintometrics measurement validation andethicalconsiderationsofdatacollectioncanbefound elsewhere .wedescribetheexactmetricsweuseinsection .
.
.
.
engsat.
theengineeringsatisfaction engsat surveyisa longitudinal program to understand the needs of google engineers evaluate the effectiveness of tools process and organization improvements and provide feedback to teams that serve google engineers.
every three months the survey is sent out to one third of eligible engineers in one of five core engineering job roles havebeenatgoogle sparentcompanyforatleast6months and below the director level.
the same engineers are re surveyed every three quarters and a random sample of one third of new engineers is added each quarter so that all engineers are invited.
the survey questions cover a range of topics from productivity to tool satisfaction to team communication.
respondents are asked todescribetheirexperienceinthe3monthperiodpriortotaking the survey.
before beginning the survey respondents are informed howthe data isusedandthat participationisvoluntary.
while engsat response rates are typically between and response bias does not appear to be a significant threat.
we know this because we analyzed two questions for response bias one on productivity and one on overall engineering experience satisfaction.
we found that engsat tends to have lower response rates for technical leads and managers those who have been at googleforalongerperiod andforengineersfromthesanfrancisco bayarea wheregoogleisheadquartered.toestimatetheimpact ofnon responsebiasonametricderivedfromengsatresponses wecompareabias correctedversionofthemetrictoitsuncorrected versionandcheckforthedifference.thebias correctedmetricis calculated by reweighting engsat responses with the propensity score a similarity measure of responding to engsat which is 1304esec fse november14 18 singapore singapore l.cheng e. murphy hill m. canning c.jaspan c.green a.knight n.zhang e. kammer figure1 quantitativefeaturesthatpredictedself ratedproductivity.
estimatedbasedonfactorssuchasdevelopertenure worklocation andcodinglanguageandtools.wefindthataftercorrectingforthis non response bias using propensity score matching the percent of engineers who responded favorably did not change significantly foreitherquestion.forinstance adjustingfornon responsebias productivity decreases relatively by .
which is too small to reach statistical significance at the level.
these results were consistent acrossthe three rounds ofengsatthat we analyzed.
.
dependentvariable productivity weuseself ratedproductivityfromoursurveyasourdependent variable.
whilesubjective and objective measuresofproductivity each have advantages and disadvantages we chose a subjective measure of productivity here both because it is straightforward tomeasureinsurveyformandbecauseitisusedbroadlyinprior research .
theengsatsurveyasksthequestion overall inthepastthree months how productive have you felt at work at google alphabet?
respondentscanchoose notatallproductive slightlyproductive moderately productive very productive or extremely productive .wecodedthisvariablefrom1 notatallproductive to extremelyproductive .
prior software engineering research has shown that subjective productivity correlates with objective measures of productivity as a wayto establish convergentvalidityof questionbasedproductivitymetrics thatis howtheyrelatetoothermeasures of the same construct we sought to do the same by correlating our subjective productivity measure below with several objective measuresof productivity.
ratherthan using a linear correlationasusedinpriorwork wewereopentothepossibility that relationships were non linear and thus we selected a random forestas aclassifier.
first we created a simple random forest to predict a binary version of self rated productivity where we coded extremely productive and veryproductive as productive and the othervalues as notproductive.wethenpredictedthisbinarymeasureofself rated productivity using six quantitative productivitymetrics measured overathreemonthperiod.twoofthemeasurescapturetheamount outputproducedover the fixedperiod totalnumber ofchangelists.
thisrepresents thenumberof changelists cls thatanengineermerged aftercodereview intogoogle smain code repository.
total linesof code.
acrossallcls anengineermerged the totalnumber oflinesofcode changed.twomeasurescapturetheamountoftimeittakesanengineerto produce one unitofoutput achangelist median active coding time.
across every cl merged the median time an engineer spent actively writing code per cl .
medianwall clockcodingtime.
themedianwall clocktime an engineer spent writing code per cl that is the time elapsed between when the engineer starts writing code and when they request the code be reviewed.
the remainingtwomeasures capturednon productive activities that is how much time an engineer spends waiting per unit of output achangelist medianwall clockreviewtime.
themedianwall clocktime an engineer spentwaiting for code reviewper cl.
median wall clock merge time.
the median wall clock time an engineer waited between approval for merging and actuallymergingper cl.
we gathered the above data over consecutive quarters from 2018q1to2019q2.foreachquarter welinkedanengineer ssubjectivemeasureofproductivitytotheabovefivequantitativemeasures.
sinceengineersareinvitedtotakeoursurveyonceevery3quarters a single engineer may be represented at most twice in this data set.
in total we had1958 engineer data pointsfor our model.
afterrandomlyselecting10 ofthedataforvalidation themodel had precision and recall suggesting a substantial relationshipbetweenquantitativeandqualitativeproductivitymeasures.
lookingattheimportanceofeachquantitativemetricinclassifying developersinthe model figure we see that medianactive coding time was the most predictive quantitative feature.
this alignswithmeyerand colleagues findingthatmicrosoftengineers viewcoding as theirmostproductive activity .
.
independentvariables to predict the dependent variable we started with independent variables reducedto39afteramulticolinearitycheck section .
availablefromthesurveyandlogsdata.sincesurveyrespondents are asked to report on their experiences from the three months prior to the survey we collected log data for the corresponding three month period.
while many metrics could be analyzed we selected metrics that were relatively straightforward to collect and thatappearedplausiblyrelatedtoindividualproductivity basedon consultationwithinternalsubjectmatterexpertswithingooglethat were experiencedwithbuilding anddeployingdeveloper metrics.
below wegroupindependentvariablesintosixcategories describe each variable and link them to prior work.
we give each variableashortname inparentheses tomakereferencingthem easier in the remainder of the paper.
full survey questions and response scalesare available inthe appendix.
.
.
code quality technical debt.
the first category of potential drivers of productivity are those relating to code quality and technicaldebt.basedonexperience demarcoandlisterclaimthat software quality generally speaking is a means to higher productivity .inanexperiment schankinandcolleaguesfound that participants found errors faster when descriptive identifier names were used .
studying small industrial programs 1305what improvesdeveloperproductivityatgoogle?
code quality esec fse november14 18 singapore singapore written in the 1980s gill and kemerer found that code complexity correlates withsoftware maintenance productivity .
based oninterviewsandsurveyswithprofessionalsoftwaredevelopers besker and colleagues found that technical debt correlates negativelywithdeveloper productivity .
we measured code quality and technical debt with subjective factors from our survey code quality satisfaction sat.
withproject code quality sat.
withdependency code quality code technical debt projecttechdebt dependencytechnical debt dependency techdebt technical debthindrance techdebthindrance .
.
infrastructuretools support.
the next categoryofpotentialdriversofproductivityareissuesrelatingtotoolsandinfrastructure.
prior work showed that using the best tools and practices was the strongest correlate of individual productivity at google though not a significant correlate at two other companies .
storey and colleagues also found that microsoft developers processes andtoolscorrelatedwithindividualproductivity .
this category had6objective and12 subjective measures tools infrastructure and service satisfaction sat.
with infra tools toolsand infrastructurechoice choicesofinfra tools toolsandinfrastructureinnovativeness innovationofinfra tools toolsand infrastructureease easeofinfra tools tools and infrastructure frustration frustration of infra tools developer stackchange changeoftoolstack internaldocumentation support doc.
support internaldocumentation hindrance doc.
hindrance build test cycle hindrance build test cyclehindrance buildlatencysatisfaction sat.
withbuildlatency 50thand90thpercentileofbuildduration p50buildtime p90 buildtime oflong builds per week oflongbuilds 50th and 90th percentile of test duration p50 test time p90 test time oflong testsper week oflongtests learninghindrance learninghindrance migration hindrance migration hindrance .
.
teamcommunication.
thenextcategoryofdriversofproductivity are issues relating to team communication.
in a survey of knowledge workers hernaus and mikuli found that socialjob characteristics e.g.
groupcooperation correlatedwith contextual jobperformance .morespecifically insoftwareengineering chatzoglouandmacaulayinterviewedsoftwaredevelopers finding that most believed that communication among team members was very important to project success .
studying communication networksquantitatively kidaneandgloorfoundthatintheeclipse project ahigherfrequencyofcommunicationbetweendeveloper correlatedpositivelywithperformance andcreativity .tomeasureteamcommunicationinourstudy weexamined9 objective measures and1subjective measure 50th and 90th percentile of rounds of code review p50 code reviewrounds p90 code reviewrounds 50th and 90th percentile of total wait time of code review p50 code reviewwaittime p90 code reviewwaittime 50th and 90th percentile of code reviewers organizational distances from author p50 review org distance p90 review org distance 50thand90thpercentileofcodereviewers physicaldistances from author p50 review physical distance p90 review physicaldistance physical distance from direct manager distance from manager code review hindrance slowcode review .
.
goalsand priorities.
prior researchsuggeststhatchanging goals and priorities correlate with software engineering outcomes.
surveying365softwaredevelopers thestandishgroupfoundthat changing requirements was a common stated reason for project failure .
meyer and colleagues found that one of the top most commonlymentionedreasonsforaproductiveworkdaywashaving clear goalsandrequirements .
we measure this category with1subjective measure priorityshift priorityshift .
.
interruptions.
meyerandcolleaguesfoundthattwoofthe top five most commonly mentioned reasons for a productive workdayby379softwaredeveloperswashavingnomeetingsandfew interruptions .
similarly a prior survey of google engineers showed that lack of interruptions and efficient meetings correlated withpersonal productivity as diduse of personal judgment .
we measure this category with3objective measures 50th and 90th percentile of total time spent on incoming meetings per week p50meeting time p90 meeting time total time spent on any meetings per week total meeting time .
.
organizationalandprocessfactors.
finally outsideofsoftwareengineering organizationalandprocessfactorscorrelatewith avarietyofworkoutcomes.forexample accordingtohealthcare industry managers reorganizations can result in workers sense of powerlessness inadequacy andburnout .althoughnotwellstudied in software engineering based on personal experience demarco and lister and armour point to bureaucracy and reorganizations as leading to poor software engineering outcomes.
this category had2subjective and3objective measures process hindrance complicatedprocesses organizationalhindrance team orgchange number of times when engineers direct manager changes but colleaguesdonot change reorgdirectmanagerchange number of times when both an engineer s direct manager and colleagues change simultaneously non reorg direct manager change number of different primary teams the engineer has primary team change 1306esec fse november14 18 singapore singapore l.cheng e. murphy hill m. canning c.jaspan c.green a.knight n.zhang e. kammer .
from variablesto paneldata sincethesurveywassentouttothesamecohortofengineersevery three quarters we have accumulated a panel data set with two observations in different points of time for each engineer.
after joiningeachengineer ssurveydatawiththeirlogsdata wehave complete panel data for engineers.
.
modeling using the panel data set we applied a quasi experiment method of panel data analysis to analyze the relationship between engineers perceived overall productivity and the independent variables.
in this paper we use a fixed effect model to analyze panel data at the developer level.
the modelis yit i t dit it where yitis the dependent variable y self rated productivity for developer iat time t. iis unobserved engineer time invariant effects such as education andskills.
tistheengineer independenttimeeffect suchascompanywide policychanges andseasonalities at time t. dit d1 it d2 it ... dn it are observed productivity factors for developer iat time t. are the causal effects of productivity factorsditat time t. itisthe errorterm at time t. to estimate the fixed effect model we differenced equation between the twoperiodsandhave yit t dit it where t 0 1t.the prefixdenotesthechangefromone time period to the next.
t is a categorical variable representing panels in different time periods if we have more than one panel.
notethat after differencing iiscancelled out and tcanbe explicitlycontrolledbytransformingittoaseriesofdummyvariables.
therefore factors in iand tdonot confound the results.
wethenestimatedequation usingfeasiblegeneralizedleast squares fgls we chose fgls to overcome heteroskedasticity serialcorrelationbetweenresiduals andforefficiencycomparedto ordinaryleastsquareestimators.theparametersofinterestare the terms.
the hypothesis we are testing is that 0for alldit.
exceptfor binary variablesand percentagevariables we transform ditintolog dit .
the benefit of taking a natural log is to allow ustointerpretestimatesofregressioncoefficients terms asan elasticity whereapercentchangeinadependentvariablecanbe interpreted as a percent change in an independent variable.
this allows for both a uniform and intuitive interpretation of the effects acrossboth logs basedandsurvey baseddependent variables.
toliberallycapturecausalrelationshipsbetweenproductivity we use a p value cutoff of .
to define statistically significant results.
if the reader prefers a more stringent cutoff or using a false discovery correction we facilitate this byreportingp values.
analysis code was written in r by the first author using the packages glmnet randomforest binom car and plm.
all code was peer reviewedusing google sstandardcode reviewprocess .
.
multicollinearity tocheckformulticollinearityamongtheindependentvariables we calculatedvarianceinflationfactor vif scoresonthesemetrics.
wefoundsomebuildlatencymetricswerehighlycorrelatedand thus may cause a multicollinearity problem.
after consulting with expertsinourbuildsystem weremovedthreebuildlatencymetrics thathadavifscoreabove3 p50buildtime p50testtime and of longtests athresholdrecommendedbyhairandcolleagues .
the final listof39 metrics allhave vif scores below3.
.
threatsto validity like all empirical studies ours is imperfect.
in this section we describe threats to the validity of our study broken down into content construct internal andexternal validity threats.
.
.
content.
although our study examines a variety of facets of productivity itdoesnotexamineeverysingleaspectofproductivity oroffactors that mayinfluence productivity.
withrespecttoproductivityitself wemeasureitwithasingle surveyquestion.ononehand thequestionitselfiswordedbroadly and our validation section .
shows that it correlates with other objectivemeasuresofproductivity.ontheotherhand asevidenced bythefactthatthecorrelationwasimperfect itislikelythatour question did not capture some aspects of developer productivity.
as one example our question was only focused on productivity of an individual developer yet productivity is often conceptualized from ateam group orcompany perspective .
likewise our setof productivity factors like code qualityand buildspeed areincomplete largelybecauseweusedconveniently available and subjectively selected metrics and because we reused an existing long running survey.
in comparison prior work which usedacustom builtcross sectionalsurvey foundthattwoofthe strongest correlates with individual productivity were job enthusiasmandteammates supportfornewideas .neitherofthesetwo productivityfactorswereexploredinthepresentsurvey demonstrating that our productivityfactors are incomplete.
.
.
construct.
our engsat survey measures a variety of theoreticalconcepts andthequestionscontainedinitcontainarangeof construct validity.
for instance while we have demonstrated some amountofconvergentvalidityofourproductivityquestion respondentstothequestionmayhaveinterpretedtheword productivity differently somemayhaveinterpretedittoreferonlytothequick completionofworkitems whileothersmighttakeamoreexpansiveviewtoincludeaspectssuchasquality.whilewehavetriedto limit the impact of different interpretations of engsat questions by pilotingvariations gatheringinterpretivefeedback andrefining wording iteratively such issuesare unavoidablethreats.
another specific threat to construct validity is inconsistent and ambiguous question wording.
for instance while respondents are advisedatthebeginningofthesurveythattheyshouldreporton experiences over the last months some questions but not all reinforcethisscopingbybeginningwith inthelastthreemonths... .
asanotherexampleofinconsistency whilemostquestionsaskonly about experiences whichour models use topredict productivity three questions ask about the relationship between experience and perceived productivity such as how much has technical debt... 1307what improvesdeveloperproductivityatgoogle?
code quality esec fse november14 18 singapore singapore table metrics relationshipwith self rated productivity.
metric effectsize p value codequality technical debt sat.
withprojectcode quality .
.
sat.
withdependency code quality .
.
projecttechdebt .
.
dependency techdebt .
.
techdebthindrance .
.
infrastructure tools support sat.
withinfra tools .
.
choicesofinfra tools .
.
innovationofinfra tools .
.
easeofinfra tools .
.
frustration ofinfra tools .
.
changeoftoolstack .
.
doc.
support .
.
doc.
hindrance .
.
buildandtest cyclehindrance .
.
sat.
withbuildlatency .
.
p90 buildtime .
.
p90 test time .
.
oflongbuilds .
.
learninghindrance .
.
migrationhindrance .
.
teamcommunication p50 code reviewrounds .
.
p90 code reviewrounds .
.
p50 code reviewwaittime .
.
p90 code reviewwaittime .
.
p50 revieworgdistance .
.
p90 revieworgdistance .
.
p50 reviewphysical distance .
.
p90 reviewphysical distance .
.
distancefrom manager .
.
slowcode review .
.
goals priorities priority shift .
.
interruptions p50 meeting time .
.
p90 meeting time .
.
totalmeeting time .
.
organizationalchangeandprocess complicatedprocesses .
.
team andorgchange .
.
reorgdirectmanagerchanges .
.
non reorgdirectmanagerchange .
.
primary team change .
.
hindered your productivity?
.
as an example of ambiguity several questionsaskaboutengineers experienceswiththeprojectthey work on but respondents interpret for themselves what a project isand if they work onmultiple projects whichone to report on.
.
.
internal.
asweargueinthispaper ouruseofpanelanalysis helpsdrawstrongercausalinferencesthanthosethatcanbedrawnfrom cross sectional data.
however the most significant caveat to our ability to draw causal inferences is time variant effects.
in contrasttotimeinvarianteffects e.g.
prioreducationanddemographics time variant effects may vary over the study period.
for instance inourrunningexample ifarujlostamentorandrusla gainedamentorbetweenthetwosurveys ouranalysiscouldnot rule out mentorship as a cause of increased productivity or code quality.
thus our analysis assumes that effects on individual engineersaretimeinvariant.violationsofthisassumptionthreaten the internal validity ofour study.
another internal threat to the validity of our study is participants who chose not to answer some or all questions in the survey.
whileouranalysisofnon responsebias section .
.
showedthat two survey questions were robust to non response among several dimensions like level and tenure non response is still a threat.
for one respondents and non respondents might differ systematically onsomeunmeasuredordimension suchashowfrequentlythey get feedback from peers.
likewise respondents who choose not to answer a question will be wholly excluded from our analysis yetsuchparticipantsmightdiffersystematicallyfromthosewho answeredevery question.
anotherthreattointernalvalidityisthatweanalyzeddatafor only two panels per engineer.
more panels per engineer would increasethe robustnessof our results.
.
.
external.
asthetitleofthispapersuggests ourstudywas conductedonlyatgoogleandgeneralizabilityofourresultsbeyond that context is limited.
google is a large us headquartered multinational and software centric company where engineers work on largely server and mobile code with uniform development tooling andinamonolithicrepository.likewise duringthestudyperiod google developers mostly worked from open offices before the global covid19 pandemic when many developers shifted to remoteorhybridwork.whileresultswouldvaryifthisstudywere replicatedinotherorganizations contextsthatresembleoursare mostlikely to yieldsimilar results.
panel analysis results .
factors causally linkedto productivity paneldataanalysissuggestedthat16outofthe39metricshavea statisticallysignificantcausalrelationshipwithperceivedoverall productivity as listed in table .
the overall adjusted r squared value for the model was .
.
in table the effect size should be read as a percent change in the dependent variable is associated with that percent change in the independent variable.
for instance for code quality a change in project code quality from very dissatisfied to verysatisfied toquality isassociatedwitha10.
increaseinself reportedproductivity.tosummarize table for code quality we found that perceived productivity is causally related to satisfaction with project code quality but not causally related to satisfaction with code quality in dependencies.
for technical debt we found perceived productivityiscausallyrelatedtoperceivedtechnicaldebt both within projectsandintheir dependencies.
1308esec fse november14 18 singapore singapore l.cheng e. murphy hill m. canning c.jaspan c.green a.knight n.zhang e. kammer for infrastructure several factors closely related to internal infrastructureandtoolsshowedasignificantcausalrelationshipwithperceivedproductivity engineers who reported their tools and infrastructure were not innovative were more likely to report lower productivity.
engineerswhoreportedthenumberofchoiceswereeither toofewortoomanywerelikelytoreportlowerproductivity.wefurthertestedwhetheroneofthetwo toofew or toomany mattersbutnottheother byreplacingthis variablewithtwobinaryvariables onerepresentingthe caseof toofew choicesandtheotherrepresentingthe case of too many choices.
the results suggest that both casesare causallyrelatedto perceivedproductivity.
engineers who reported that the pace of changes in the developertoolstackwastoofastortooslowwerelikely toreportlowerproductivity.
similarly wetestedthe two cases too fast or too slow separately by replacing this variable withtwobinary variables one representing thecaseof toofast andtheotherrepresentingthecase of too slow .
results suggested both cases matter for perceivedproductivity.
engineerswhowerehinderedbylearninganewplatform framework technology or infrastructure were likely to report lower productivity.
engineerswhohadlongerbuildtimesorreportedbeing hindered by their build test cycle were more likely to report lower productivity.
forteamcommunication ametricrelatedtocodereviewwas significantly causally related with perceived productivity.
engineers who had more rounds of reviews per code review orreportedbeinghinderedbyslowcodereviewprocesses were likely to report lower productivity.
forgoalsandpriorities engineershinderedbyshiftingproject priorities were likely to report lower productivity.
organizationalfactorswerelinkedtoperceivedproductivity engineerswhohadmorechangesofdirectmanagerswere more likely to report lower productivity.
engineerswhoreportedbeinghinderedforteamandorganizational reasons or by complicated processes were more likely to report lower productivity.
.
quadrant chart tovisualizethesefactorsintermsoftheirrelativeeffectsizeand statistical significance we plot them in a quadrant chart figure .
thechartexcludesfactorswhosep valueisgreaterthan0.
.the factorshavevariousscalesfromsatisfactionscoretotimeduration sotomaketheireffectsizecomparable westandardizedmetricsby subtracting each data point by its mean and dividing it by its standard deviation.
the x axis is the absolute value of the standardized effectsize.the y axis isp values.
thetopfivefactorsintermsofrelativeeffectsizearesatisfaction with project code quality hindrance of shifting priorities technical debt in projects innovation of infrastructure and tools and overall satisfactionwithinfrastructure andtools.
lagged panel analysis methods thepaneldataanalysisweconductedsofarsuggestssatisfaction withcodequalitywithinprojectsisthestrongestproductivityfactor among the we studied based on standardized effect size and p value.
however because the observed changes in factors coincided duringthesametimeperiod suchconventionalpaneldataanalysis cantellwhichfactorsarecausallyrelatedtooverallproductivity but itdoes not tellusthe direction of the causality.
so doesbettercodequalitycauseincreasingproductivity ordoes increasingproductivitycauseimprovedcodequality?bothlinkages aretheoreticallyplausible ononehand codequalitymightincrease productivitybecausehighercodequalitymaymakeiteasierand faster to add new features on the other hand high productivity might increase quality code because engineers have free time to spend onqualityimprovement.
toverifythedirectionofthecausalrelationshipbetweenproject code quality and productivity we conducted another panel data analysis using lagged panel data.
in this analysis we focus only onthecausalrelationshipbetweencodequalityandproductivity.
althoughsuchananalysisispossibleforotherfactors itisnonethelesslaborious aswe shallseeshortly.thus wefocusourlagged analysisononlythesetwovariables whichhadthestrongestcausal relationship inour prior analysis.
inshort weverifiedthedirectionofthelinkagebetweenproject code quality and productivity by checking if the change in one factorisassociatedwiththechangeintheotherfactorinthefollowing period.
the idea is that if project code quality affects productivity we expect to see that changes in project code quality during time t are associated with changes in productivity during time t. sinceself reportedproductivityisnotavailablefortwoconsecutive quarters sinceeachrespondent issampledonlyonceeverythree quarters we switch to logs based metrics to measure productivity.
complementingourprioranalysisbasedonself ratingswithalogsbasedonehastheadditionalbenefitofincreasingtherobustnessof our results.
moreformally wetestedtwocompetinghypotheses hypothesis qap quality affects productivity and paq productivity affects quality .hypothesisqapisthatthechangesinprojectcodequality during time t are associated withchanges in productivity during timet.thisimpliesimprovementsinprojectcodequalityleadto betterproductivity.hypothesispaqisthatchangesinproductivity intimet 1areassociatedwithchangesinprojectcodequalityin timet.thisimpliesbetterproductivityleadstoanimprovement inprojectcode quality.
hypothesis qap changes in code quality during time t are correlated with changes in productivity during time t. the statistical modelis pit qit it where qit 1isthe change in code quality attime t and pitis thefollowingchangeinlogs basedproductivitymetricsattimet.
given the available data we use the difference between q3 andq22019tomeasure qit 1andthedifferencebetweenq32018 andq3 to measure pit.
1309what improvesdeveloperproductivityatgoogle?
code quality esec fse november14 18 singapore singapore figure quadrantofproductivity factorsineffectsize andstatistical significance hypothesispaq changesinproductivityintimet 1arecorrelatedwithchangesincodequalityintimet.thestatisticalmodel is qit pit it where pit 1isthechangeinlogs basedproductivityattimet and qitisthefollowingchangeincodequalityattimet.given the availability of data we use the difference between q3 and q22019tomeasure pit 1andthedifferencebetweenq32018and q1 to measure qit.
forthisanalysis wehadfulllaggedpaneldatafor3389engineers.
lagged panel analysis results our results support hypothesis qap but not hypothesis paq.
we found thata increase ofsatisfactionrating with project code quality i.e.goingfromaratingof verydissatisfied to verysatisfied attimet 1wasassociatedwitha10 decreaseofmedian active coding time per cl a decrease of median wall clock time from creating to mailing a cl and a decrease of median wall clock time from submitting to deploying a cl at time t. on theotherhand wedidnotfindanyevidencetosupporthypothesis paq changes in satisfaction with project code quality in time t were not associated with any of the productivity metrics in time t .
see appendix for a table containing this data and descriptions of each variable.
therefore returning to our research question we conclude thatchanges in satisfaction with project code quality cause changes inperceivedoverallproductivity.
discussion our findings provide practical guidance for organizations trying to improve individual developer productivity by providing a list of amenablefactorsthatarecausallylinkedtoproductivity.specifically our panel analysis shows that these factors are code quality technical debt infrastructure tools and support team communication goalsandpriorities andorganizationalchangeandprocess.
our quadrant chart shown in figure which we originally createdforanexecutivestakeholderaudiencewithingoogle allows practitioners to choose highly impactful productivity factors to act on.
factors at the top of the chart are those with high statistical significance and low standard error so practitioners can read those as the most consistent productivity factors.
factors on the right are the ones with the largest standardized effect size so these supply the biggest bang for the buck .
taken together the factors intheupperrightquadrantaretheonesmostpromisingtoimprove productivity at google.
for instance giving teams time to improve codequality reducetechnicaldebt andstabilizeprioritieswould be good candidate initiatives for improving individual developer productivity.
1310esec fse november14 18 singapore singapore l.cheng e. murphy hill m. canning c.jaspan c.green a.knight n.zhang e. kammer we found that several factors did not have a statistically significantrelationship withperceivedproductivity notably for documentation perceived productivity was not causally linked to reported poor or missing documentation doc.
hindrance orthefrequencyofdocumentationmeetingneeds doc.
support .
this is surprising given that github s surveyof5 500developersfoundthat incompleteorconfusingdocumentation wasthemostcommonlyencountered probleminopensource .github sfindingsareconsistentwithfindingsatmicrosoft andatgoogle engsat respondentsoftenreport poorormissingdocumentation as one of the top three hindrances to their own productivity.
however the results in this paper suggest that there is no causal relationship between developer productivity and documentation despite developers reports that it is importanttothem.onewaytoexplainthisfindingisthat documentation may not impact productivity but it may yet have other positive benefits such as to create inclusive communities .
for meetings we found that perceived productivity was notcausallylinkedtotimespentoneitherincomingmeetings p50meetingtime p90meetingtime oralltypesofmeetings totalmeetingtime .thisisalsosurprising giventhat prior research found in a survey of microsoft engineers that meetings were the most unproductive activity for engineers .
thecontradictoryresultscould be explainedby differencesbetweenthestudies ourpanelanalysisenables causalreasoning vscorrelational moreengineerswererepresented in our dataset vs and we used objective meeting data from engineers calendars vs. self reports .
for physical and organizational distances perceived productivitywasnotcausallylinkedtophysicaldistancefromdirect manager distance from manager or physical p50 review physicaldistance p90reviewphysicaldistance ororganizational distances from code reviewers p50 review org distance p90revieworgdistance .thisisincontrasttoramasubbuand colleagues cross sectionalstudy whichfoundthat asfirms distributetheirsoftwaredevelopmentacrosslongerdistance and time zones they benefit from improved project level productivity .
as with the prior differences explanatoryfactorsmayincludedifferencesinorganizationanda methodology individualproductivity versusorganizational productivity single company versus multiple companies andpanel versuscross sectionalanalysis.
aswementioned athreattotheseresultsisthethreatofreverse causality thestatisticsdonottelluswhethereachfactorcauses productivity changes or vice versa.
we mitigated this threat for code quality using lagged panel analysis providing compelling evidence that high code quality increases individual developers productivity.
withingoogle ourresultshavedrivenorganizationalchange aroundcodequality andtechnical debt as a way to improve developer productivity sinceitscreationinmay2019 aversionofthisreporthas beenviewedbymorethan1000uniquegoogleemployees withmore than500comments.
engsatresultshelpedmotivatetwocodequalityconferences forgoogleengineerswith4 000internalattendeesandmore than15 000viewsof live andon demandtalks.
the research motivated the creation of two initiatives a technicaldebtmaturitymodel akintothecapabilitymaturitymodel andtechnicaldebtmanagementframework tohelpteamsimprovetechnicaldebtassessmentand management.
severalteamsandorganizationssetobjectivesandkeyresults okrs to improve technical debt in their workgroups.
googleintroduced thehealthys anawardwhereteams submitatwopageexplanationofacodequalityimprovement initiativethey veperformed.using anacademicreviewing model outside engineers evaluated the impact of nearly submissions across the company.
accomplishments include morethanamillionlinesofcodedeleted.inasurveysentto awardrecipients of173respondents mostrespondentsreported that they mentioned the award in the self evaluation portionoftheirperformanceevaluation andthatthere wasatleastaslightimprovementinhowcodehealthwork isviewedbytheirteam andmanagement .
although difficult to ascribe specifically to this research and the above initiatives that it has influenced engsat has revealed several encouraging trends between when the report was released internallyinthesecondquarterof2019andthefirstquarterof2021 the proportionofengineersfeeling notatallhindered bytechnical debt has increased by .
the proportion of engineers feeling satisfiedwithcodequalityhasincreasedbyabout22 .theproportion of engineers feeling highly productive at work has increased by about18 .
conclusion prior researchhas madesignificant progressinimprovingourunderstanding of what correlates with developer productivity.
in this paper we ve advanced that research by leveraging time series data to run panel analyses enabling stronger causal inference than was possible in prior studies.
our panel analysis suggests that code quality technical debt infrastructure tools and support team communication goals and priorities and organizational change and process are causally linked to developer productivity at google.
furthermore ourlagged panelanalysisprovides evidencethatimprovements in code quality cause improvements in individual productivity.
while our analysis is imperfect in particular it is only one company and uses limited measurements it nonetheless can help engineering organizations make informed decisions about improvingindividualdeveloper productivity.
acknowledgment thankstogoogleemployeesforcontributingtheirengsatandlogs datatothisstudy aswellastheteamsresponsibleforbuildingtheinfrastructureweleverageinthispaper.thanksinparticulartoadam brown michael brundage yuangfang cai alison chang sarah d angelo daniel dressler ben holtz matt jorde kurt kluever justin purl gina roldan alvaro sanchez canudas jason schwarz simone styr fredwiesinger andanonymous reviewers.
1311what improvesdeveloperproductivityatgoogle?
code quality esec fse november14 18 singapore singapore