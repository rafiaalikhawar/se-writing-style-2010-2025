improving ml based information retrieval software with user driven functional testing and defect class analysis junjie zhu apple inc. cupertino ca usa jason.zhu apple.comteng long apple inc. cupertino ca usa teng.long apple.com wei wang apple inc. cupertino ca usa wwang52 apple.comatif memon apple inc. cupertino ca usa atif.memon apple.com abstract machine learning ml has become the cornerstone of information retrieval ir software as it can drive better user experience by leveraging information rich data and complex models.
however evaluating the emergent behavior of ml based ir software can be challenging with traditional software testing approaches when developers modify the software they cannot often extract useful information from individual test instances rather they seek to holistically verify whether and where their modifications caused significant regressions or improvements at scale.
in this paper we introduce not only such a holistic approach to evaluate the systemlevel behavior of the software but also the concept of a defect class which represents a partition of the input space on which the mlbased software does measurably worse for an existing feature or on which the ml task is more challenging for a new feature.
we leverage large volumes of functional test cases automatically obtained to derive these defect classes and propose new ways to improve the ir software from an end user s perspective.
applying our approach on a real production search autocomplete system that contains a query interpretation ml component we demonstrate that our holistic metrics successfully identified two regressions and one improvement where all were independently verified with retrospective a b experiments the automatically obtained defect classes provided actionable insights during early stage ml development and we also detected defect classes at the finer sub component level for which there were significant regressions which we blocked prior to different releases.
ccs concepts software and its engineering software testing and debugging information systems test collections relevance assessment .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november singapore singapore association for computing machinery.
acm isbn .
.
.
.
machine learning testing information retrieval system testing relevance search autocomplete search query interpretation acm reference format junjie zhu teng long wei wang and atif memon.
.
improving mlbased information retrieval software with user driven functional testing and defect class analysis.
in proceedings of the 30th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november singapore singapore.
acm new york ny usa pages.
introduction modern industrial scale information retrieval ir software such as search engines and recommendation systems rely heavily on machine learning ml to learn the intention and preference of different users based on their queries .
compared to traditional ir methods that used few features e.g.
term frequency and inverse document frequency modern ir systems have evolved thanks to richer features e.g.
search history geographic location and reviews increasing amount of training data and better ml techniques e.g.
deep learning methods .
however this technological leap has made system testing of such ml based ir ml ir software challenging for a number of reasons.
first ml ir developers are less concerned with failures on an individual query or test input rather they want to know if their modification has holistically improved or regressed any aspects of the software.
in traditional ir software testing a modification could cause a conventional lookup function foo i to fail its release due to a single test input iif the scenario is a blocking issue also known as a p 0failure within the software industry .
on the other hand if foowere a ml ir software function then the individual failure for the input iwould be acceptable if the modification caused footo perform better on a large number of other inputs.
take a location aware restaurant recommender system as an example of a ml ir software and assume that over of its customer base are young vegetarians.
if remote steakhouses start to rank lower due to a code modification or ml model retraining we might see recommendation improvement for many vegetarians despite a very localized regression where only a small fraction of the omnivore users start seeing fewer number of steakhouses nearby.
second ml ir developers wish to gather information rich testing results for specific classes of inputs that influence the decision esec fse november singapore singapore junjie zhu teng long wei wang and atif memon space of the ml components .
suppose the developers of the aforementioned recommendation software learn that the ml is only improving recommendations for vegetarian users at the cost of worsen recommendations for the omnivore users.
compared to just an overall performance score such refined information can guide the developers to investigate biases in the training data adapt the ml to improve upon a specific input subspace defined by age and dietary preferences in this case and eventually improve the ml to make smarter recommendations.
making these subclasses of inputs tractable can help advance ml development and maintenance especially as the software s decision space is continuously evolving with changes in the real data e.g.
shifts in user taste and restaurant quality.
third to comprehensively understand the behavior of their software the ml ir developers need a large number of test cases to cover the diverse decision space of the software e.g.
tests to cover all types of restaurants all age groups of customers all dietary preferences.
not only are such large test suites difficult to create they are even harder to maintain.
a conventional system test case is of the form input expected output where input may mimic an end user e.g.
a query input indian food and expected output determines if the software is functioning correctly e.g.
an output list of indian restaurants ranked by distance.
each test brings the software into a certain internal state applies the test input and checks the output.
a large number of test cases may become obsolete as the ml software frequently improves its output over time by learning from new training data.
consequently hard coded expected output in the test cases may turn stale with respect to the software s new improved output.
in this paper we develop a system testing approach that addresses these challenges.
our approach automates test authoring with appropriate expected outputs i.e.
test oracles by leveraging the scale of production data which reflect both positive and negative user feedback.
moreover we rely the notions of holistic measures which quantify overall functional behavior of a software to help developers make pass fail decisions on their modifications and defect classes which capture patterns in partitioned input spaces that can help developers reason about improvements and regressions.
crucially our approach factors in three different levels of evaluation that are typically treated separately user level model level and engineering level .
we have implemented the approach and evaluated it on a real search autocomplete search ac ml ir system that supports beyond millions of queries per day.
consequently the results in this paper are organized to illustrate how we can cover all three levels of testing comprehensively.
user level testing mainly relies on online evaluation based on live production traffic .
such testing including a b testing has become the gold standard for industrial scale ir systems due to the possibility of fielding a large number of users in an end to end fashion as well as the business impact that is associated with user engagement.
yet online measures do not directly expose bugs at the engineering or model levels so fixing these bugs is expensive not only because of wasted online traffic but also because of the additional engineering hours needed to understand the root cause retrain the ml models and deploy the system.our contribution at this level is that we were able to completely automatically reject a number of software modifications of which we present two along with another modification which was accepted all these outcomes were confirmed via independent online a b testing.
by relying on the holistic measure of the rate at which the system returns no results or recalls the ones desired by the user i.e.
test oracles automatically derived from production queries we were able to distinguish improvements and regressions that were consistent with the reaction by a large number of users.
model level testing ensures that individual ml models achieve performance gains with increasingly complex model architecture.
the models are typically evaluated offline not using live production traffic via common ml metrics such as the precision recall and thef1score .
these metrics however do not necessarily capture holistic and emergent behavior of the ir systems that can consist of multiple ml models non ml tables hard coded for certain inputs and code logic that puts these together.
at this level we conducted deeper analysis of the system based on the functional coverage space that captures the ml behavior of the search ac system.
we were able to identify many defect classes among which we present three examples that not only covered a broad class of inputs but also helped define new opportunities for ml model improvement.
engineering level testing emphasizes correct software implementation and is typically performed by traditional approaches such as unit tests and simple integration tests .
validating these aspects of the software correctness is undeniably necessary but as we illustrated via the restaurant recommendation system example ml based software needs significantly more attention at the system level because that is where its behavior truly emerges.
at this level we re purposed the test oracles derived automatically from production data to directly evaluate the multi task query interpretation component within the search ac system.
by such sub component testing we identified implementation artifacts of which we present two in this paper to show how the approach can drive the triaging of new regressions and bug fixes.
the main contribution of our work is two fold.
first we offer a practical methodology to perform system testing for industrial scale ir systems which complements popular online testing approaches by providing early improvement and regression signals.
to our knowledge there is limited research on such ml based ir system testing despite the wide spread usage of search engines and recommendation systems in industry.
second our methodology can be generalized to other ml based systems as it is designed for systems with large input spaces that contain complex functional dimensions.
as such our work contributes a solution to a class of similar applications within the broader ml testing literature .
following this section we model our holistic measure define defect classes and describe how we automatically obtain test cases.
in section we present our search ac application that we use in our experiment and in section we provide our hypothesis and results.
we outline a methodology for generic ml based systems in section after all the case studies highlight related work in section and finally conclude in section with a discussion of future work.
1292improving ml based information retrieval software esec fse november singapore singapore model in this section we describe the common model used for defining the metrics and designing the procedures that address different research hypotheses in section .
thus this section focuses on generic approaches to holistically measure ml ir software using functional testing as well as strategies to identify defect classes to both detect new regressions and drive improvements.
without loss of generality one simplified example we consider in this section is a search ac system which completes partial user queries e.g.
the string hote may auto complete to hotel.
this ac also supposed to leverage ml sub components to understand the query intent based on the query semantics and user context e.g.
geographic location to improve the query completion task e.g.
if a user is in the country angola then hote may auto complete to the city name hote as opposed to hotel.
the specific subject application for which we present the results that address our hypotheses will be described with full details in section .
.
holistic functional testing a functional test returns the pass fail outcome of an individual input by comparing the actual output and the expectation known as the test oracle .
take the search ac system as an example we may expect the oracle hotel to be ranked among the top completion suggestions with input hote for a given context.
if this test fails on only the input hote due to a slight ranking change after an ml update it does not necessarily mean ac has regressed especially if its ml improves the ranking on many other query inputs.
thus an individual test case for an ml software cannot fully generalize a systematic behavior unless there is sufficient evidence supported by many other similar inputs.
here we present a holistic measure that can easily build upon such existing functional metrics and their aggregated pass fail outcomes to determine if a failure is systematic.
let ax a0 a1 ... be a set of boolean values that represent failure or passing outcomes on the test suite indexed by xfor a specific function of the system under test sut which we refer to as sut a. we define a holistic measure to aggregate these individual outcomes via a weighted average ha x i xwiai where i xwi 1andwi 0for alli.
the weights capture the relative importance of individual test cases.
in practice the weight can default to wi x where x is the size of the test set then eq.
becomes the arithmetic mean of the pass fail outcomes also known as the pass rate .
the two constraints on the each weight valuewiensure that the holistic measure equals its maximum value of only when all tests are passing.
another way to construct the weights is to rely on the value space of one or more specific test dimensions with a mapping function f such thatwi f vi whereviis the value in these dimensions.
for instance if we consider vito be the specific query locale e.g.
place where the query originated as a dimension for the searchac system one can systematically assign higher weights to the inputs whose locales correspond to important markets already in production and lower weights to those that correspond to emerging markets under examination.when we compare the outcomes for sut awith those that result from a modified version which we refer to as sut b with bx b0 b1 ... we can directly evaluate the difference between the two suts on the same input set xwith the holistic measure in eq.
as follows da b x hb x ha x i xwi bi ai .
from this equation we can deduce that the overall function for sutbhas improved with respect to sut a if this difference is positive or it has regressed if the quantity is negative.
the second equality in eq.
that refactors the summation implies that one can still meaningfully evaluate individual cases that improved or regressed based on how they contributed to the overall holistic measure of a function.
for a given test case i the difference between the two outcomes bi ai equals if the case has improved from sut ato sut b if the case has regressed and if there is no difference.
we will show in section .
three examples of software modifications two of which we blocked and one passed using this model.
because randomness of the individual pass fail decisions can arise from ml fuzziness and infrastructure flakiness the holistic measure is effectively an observed statistic based on the specific test input set.
to make it more robust to such randomness one also has the option to compare the difference in eq.
with thresholds to create decision rules for improvement regression.
.
defect class analysis we also partition the input set based on functional coverage dimensions to enable efficient triaging .
the dimensional analyses can help us identify specific defect classes e.g.
the class of omnivore customers who received lower steakhouse rankings in section to understand the root cause of a holistic regression.
take the general case where the test suite xcan be partitioned as x0 x1... xm we can then compute the metric in eq.
for each partition to obtain sub metrics ha x0 ... ha xm andda b x0 ... da b xm given sut aand sut b. suppose that one of the input classes xj has a very low value of ha xj or negative value of da b xj then we refer toxjas a defect class.
the analyses of defect classes are not only useful for identifying relative regressions but also helpful for understanding how to improve the current ml based software with prioritization.
we can sortha x0 ... ha xm and prioritize the defect classes that are much worse than the overall average ha x .
addressing these worst performing defect classes would likely yield more noticeable improvements.
with any new modifications introduced say via sut b developers can then rely on the difference metric da b xj to track if the change presents any sign of improvement.
let us revisit the previous ac example where we partition the space of inputs based on the locale dimension.
we can measure if the model improvement in a new locale with inputs xjcomes as the cost of worsened performance in another locale with inputs xkby inspecting the directionality of da b xj andda b xk and further quantifying the trade offs.
in section .
we will show three examples of 1one way to determine such thresholds is to compare two identical suts on the same input set and establish what differences are likely insignificant.
1293esec fse november singapore singapore junjie zhu teng long wei wang and atif memon defect classes category queries with prepositions and locations queries with unique places and users with distant viewports all of which helped the developers of our software under test to make improvements.
.
automated test oracle our methodology for ml ir software requires sufficiently large input size x to ensure that the statistics in eq.
and eq.
do not deviate too far from the true underlying software behavior in production due to the law of large numbers .
when we want to additionally cover specific functional dimensions each partition e.g.
x0 x1... xm of the input space also requires sufficient number of inputs so that a sub partition can provide abundant evidence of a specific defect class.
thus both the holistic measures and the defect class analyses heavily depend on practical solutions to generate a large number of test cases automatically.
test automation relies on resource and data availability and more importantly deriving accurate expectations which is known as the test oracle problem .
to detect improvement or regressions in ml ir software we solve the oracle problem with production data that already captures some real user feedback based on the existing baseline version in production.
for instance if many users have tapped and interacted with the result hotel returned from ac when typing hote outside of angola we can trust the suggestions hotel as a reasonable oracle for each of the user scenarios within the context of angola the suggestion hote is a better oracle if most end users have tapped and interacted with this result.
the data captured by the test oracle is highly application specific so appropriate test oracles that offer insights into the core functions are the key to providing actionable feedback for development.
the benefit of using production data for ml based software is multi fold.
first it does not require case by case inspection and authoring by test authors.
instead it only requires automated logging of the relevant query input context and user selected output.
next it is well suited for software that can have fuzzy or complex outputs such as ac outputs with ranked suggestions.
the expected output from an ac can be elusive because the result is data dependent and can alter with slight variation of the data dependencies.
nevertheless if a suggestion that a user previously liked or selected starts disappearing or getting much lower ranks the user is likely to have a worse experience with the software.
if many users experience the same behavior then the holistic measure will reflect the degree of this regression systematically.
last but not least these measures can offer meaningful business insights if they reflect the user impact of the software such as common search metrics like user clicks.
in section .
we will show how we can concretely leverage user interactions to automatically derive test oracles for a specific search ac system.
.
stratified sampling the test inputs from production data we consider are often observational in that we do not have control over the user traffic.
therefore it is often necessary to balance the size of the inputs for different categories for critical dimensions so that the associated functionality can be evaluated fairly .
for instance suppose that an ac system applies three different ml models for short medium and long input queries which are distinguished by and character cutoffs.
here the input query length would be a coverage dimension with categories.
if short queries are heavily represented e.g.
in production traffic then this imbalance may cause insufficient evaluation of the two models that rely on longer inputs.
in other cases where there is only a single ml it is still useful to partition the input space based on one or more coverage dimensions to understand if there are any ml biases that distinguish different outputs.
to optimize the input test size we use the sampling strategy in algorithm to stratify such biases in production and construct x with a bottom up approach.
the algorithm utilizes parallelization and built in functions under common map reduce frameworks .
the function computesampleratio determines the sampling ratio for a each category in the coverage dimension based on n the desired number of counts per category.
if the counts of a specific category is less than n then the sampling fraction is set to so that all existing samples will be used.
the function samplebykey implements a default sampler without replacement via poisson sampling which does not generate exact sample sizes of nbut is highly performant.
a notable property of this algorithm is that one can iteratively stratify another dimension on a given stratified output so that the final output can contain fair representations of multiple dimensions.
we will apply algorithm based on specific input dimensions of the search ac system and its ml sub component in section .
algorithm distributed stratified sampling input parallelized input data input with attribute attr and target size n. output stratified data output with approximately n samples for each of category.
1function computesampleratio countmap n fractionmap emptyhashmap foreach entry countmap do fraction min n entry.getvalue fractionmap.put entry.getkey fraction end return fractionmap 8end function 9pairs input.map x x.get attr x 10countmap pairs.countbypair 11fractionmap computesampleratio countmap n 12sampledpairs pairs.samplebykey fractionmap 13output sampledpairs.map x x.
2 subject application we now revisit search ac and demonstrate how the elements in section apply to this heavily used application.
such search ac systems are commonly available and the interested reader may 1294improving ml based information retrieval software esec fse november singapore singapore easily find details of their design and implementation.
4in this section we first abstract a generic workflow of the search ac system from the end user s perspective and explain notions of search and ac conversions which are the prerequisites for solving our test oracle problem.
next we highlight specific functions of a geo aware ac application that relies on its query interpretation qi sub module to perform multiple ml tasks.
finally we illustrate how we derive test oracles for the ac and qi suts and the corresponding input sets.
.
search and ac conversions ac performs query completion in relevance search and enhances the user experience by displaying results given only partial inputs.
figure a provides an overview of a typical user session.
ac is invoked while the user is typing the query as shown on the right branch of figure a .
for each character typed a list of suggestions is displayed in real time.
the user can directly tap on a desired result among the ac suggestions.
if the user selects any specific ac suggestion and interacts with it we refer to this session as an ac conversion session.
broadly speaking a conversion refers to any desired user interaction and is often a key metric in most relevance search applications .
instead of selecting any of the ac recommendations the user can at any time switch to the click search workflow shown on the left branch of figure a .
this typically requires a user to type a full query and click search may return multiple results.
if the user selects one of the results and follows up with positive interactions we refer to this session as a search conversion session.5such clicks or conversions are a common industry standard used for evaluating and optimizing search engines .
ac conversion sessions are often desired over search conversion sessions because the former means the user got to their desired result faster with fewer input characters.
however compared to click search ac has to resolve the prefix to predict the full query and often needs to meet more stringent latency requirements .
for instance a three character prefix hot can be interpreted in numerous ways by ac and return different acceptable rankings of multiple results.
.
a geo aware search ac system to handle the inherent ambiguity in what one should expect from ac many ac software have gradually specialized towards custom applications and user contexts.
here we consider a geo aware search ac application that allows users to look up specific geographic geo results or place of interest poi results.
meaningful interactions with a specific click search or ac result can include continued exploration with the result such as setting it as a destination to navigate to.
hence the general concepts of search and ac conversion sessions concretely apply to this application.
this system also relies on ml to utilize the user context to understand the user intent.
the contexts include user preference 5in some cases ac may complete the query and trigger click search but the user still needs to select results presented by click search.
query complete?nnn search conversion session click search buttontrigger click searchcontains desired result?select search result and interactytype query charactercontains desired result?tap ac result and interactytrigger ac ac conversion session failed session a search ac sessionystart b search ac systemqiml models and dependenciesadditional dependenciesclick searchadditional dependenciesacfigure a workflow and b abstraction of the search ac system.
settings the user location and the viewport which is the rectangular geographic region of interest to the user .
well understood user intents can intuitively narrow down the search space and present more accurate and relevant results to the user.
to achieve this both the click search and ac can employ core ml models that interpret the meaning of the query based on the user context as well as the way the query is typed.
we refer to the modules that host such ml models as qi sub modules shown in figure b .
let s revisit the earlier example of hot .
the geo aware ac application can rely on popular search queries to infer that the user is searching for a hotel .
it can also utilize the viewport to determine if the user is looking for a hotel near their location or remotely.
next ac can either prompt the user to trigger a clicksearch result with the query hotel or present specific hotel pois in the viewport.
in the latter case the user may directly navigate to the poi or book a room from a link without having to view other results shown in click search .
consider another user query new york hot where the user provides more text.
qi can directly infer from the language pattern that the user might be searching for a hotel in new york city and return hotels within or near the specific city.
alternatively the user might intend to search for new york hotdogs in which case they would likely select pois that serve hotdogs a food keyword in new york .
thus ac can rely on qi to perform multiple tasks as follows.
query segmentation segmenting the query as well as inferring potential completions of the last entity e.g.
breaking the example query new york hot into two entities including segement new york for the first entity and hotel and hotdog as possible completed segments for the second entity.
query tagging labeling the query segements e.g.
tagging the first entity new york as a geo type and second as poi category for hotel and poi keyword for segment hotdog .
entity recognition fully resolving what each tagged segment specifically means e.g.
returning both the entity new york 1295esec fse november singapore singapore junjie zhu teng long wei wang and atif memon state or the entity new york city for possible interpretations of new york and the unique entity identifiers for hotel and hotdog respectively.
for query tagging and entity recognition qi can utilize indices which contain the mapping from segments to the entities and tags or the reverse map of different ways users refer to the same entity e.g.
the city of new york can be referred as nyc new york big apple etc.
in this example qi outputs four distinct interpretations as each entity yields two different interpretations to ac.
ac then relies on its own dependencies to retrieve specific results e.g.
hotel or restaurant pois before pruning and ranking the suggestions presented to the user.
.
test oracles for ac and qi next we provide details of how we adopt production data to derive test oracles for both the geo aware ac and its qi sub component following the strategy in section .
.
each uses a different sot shown in table in this paper to focus on different aspects of the context aware ac functions.
table relevant specification for subject applications.
unique places category brand preposition geo sut sot test oracles tested query classes acsearchconversion sessionsexpected place of interest epoi 1 5 2 5 3 5 1 4 5 2 4 5 3 4 5 5 1 5 2 5 3. qiacconversion sessionsexpected segmentation tags and entity ids1 1 5 2 5 3 5 1 4 5 2 4 5 3 4 5 5 1 5 2 5 3. .
.
ac end to end tests.
when considering the test oracles for ac we cross reference search conversion sessions to highlight ways to improve ac with click search as a reference .6this approach builds upon the insight that a user who applied click search to obtain search conversion potentially did not find satisfactory results in ac when typing the query.
for such a user session we chose the selected poi in click search as the expected poi epoi i.e.
the test oracle for ac.
as the input to click search is the full query we can truncate it to only include a prefix of the full query as the input for the ac sut and replay realistic user scenarios with preserved context.
the truncation mechanism is application dependent and to test our geo aware ac we mainly considered a minimum number of input characters to trigger ac we only allowed characters7 excluding numbers and special characters to be included for the last entity in the query.
such inputs enabled us to detect boundary conditions that could highlight new improvements if the geo aware ac leveraged the context despite the ambiguity in the input alone.
6we also use ac conversion sessions to ensure queries that previously generated ac conversions can continue to present the desired result to the user but we omit the discussion and the results from this paper as this approach is used for catching regressions similar to those found by qi testing here.
7for the specific geo aware ac system requiring at least characters moderately limits the number of results that match the query prefix while we can still expect the system to interpret the query under a reasonable level of ambiguity.
.
.
qi component tests.
among the experiments in this paper we mainly use the ones pertaining to qi to demonstrate how we detect regressions and triage the root causes that impact the endto end ac system.
thus we only focus on ac conversion sessions as the sot for qi tests.
note that qi is responsible for query understanding so its oracles include expected segmentation tags and entity ids which each correspond to its three main functions as opposed to the epoi oracle used for ac .
these three variables are logged as intermediate outputs when an ac conversion is generated.
not only do we have these multi task test oracles we also have the relevant inputs to qi such as the query and relevant contexts which are part of the production data.
then we can rely on specific assertions to determine if the interpretation generated for an ac conversion query is still returned when we compare a modified sut vs. the one in production.
.
input space partition for ac and qi here we describe the common key coverage dimension for both the geo aware ac and its qi sub component8 the query class based on user intents inferred by the ml.
each of these classes are represented by the query tag patterns that can be generated by qi but each classified tag pattern triggers a different code logic in the end to end ac system.
generally there are single entity queries including unique places e.g.
the hollywood sign category e.g.
hotels and brand e.g.
starbucks and others not shown in this paper that cover the entire intent space for ac and qi.
there are also multi entity queries which are ordered combinations of single entities such as location followed by a category e.g.
new york hotel denoted as 5 2 where 5represents a geo location entity and ones with prepositions e.g.
starbucks in new york denoted as 3 4 5 where 4represents a proposition entity and additional combinations supported by the geo aware ac.
in this paper we limit the scope of query classes to those shown in table for the two suts.
note that all the ac query classes contain both poi tagged by 2or3 and geo tagged by entities.
we refer to such inputs as mixed intent queries mi queries .
as opposed to single entity queries these queries have specific epois test oracles associated with the poi component but are constrained by the geo component and thus often require special treatment by the ml or dedicated ml modules.
the query classes for qi are a superset of those for ac as we will showcase how we use qi to detect regressions outside of mixed intent queries.
for both qi and ac experiments the inputs for each sut were sampled based on production data stratified by the query classes described in table according to algorithm .
hypothesis and results following the defect class model and holistic metrics defined in section and the subject application of the geo aware searchac system described in section we present our hypotheses and experiment results as follows.
8additional dimensions specific to functionalities for ac and qi are described in specific experiments in section .
1296improving ml based information retrieval software esec fse november singapore singapore .
hypothesis our strategy captures holistic improvements and regressions of ml driven software with large scale inputs metrics we first wanted to evaluate if our holistic measures can determine if a new software change leads to improvement or regression on specific functional metrics.
to demonstrate with our geo aware ac the application described in section .
we considered two different metrics i the suggestion rate which is the fraction of tests that return at least one suggestion and ii the recall rate which is the fraction of tests whose suggestions contain the search conversion epois the oracle defined in section .
.
we tested the software on a targeted input set of mi queries mixed intent queries one of the more complex query classes described in section .
as they are inherently complicated to evaluate and hence lend themselves to automated analysis.
procedure we considered three real mi feature modifications mod mod mod and compared them to their respective baselines using a targeted test set of over mi queries.
the queries were obtained by our stratified approach in section .
such that each of the mi patterns contain over test cases.
for each modification to be compared with its baseline we calculated the suggestion rate difference and the recall rate difference respectively based on the formula in eq.
.
meanwhile each modification was independently evaluated by online user traffic in a b experiments.
if a modification generated more ac conversions compared to the baseline it would be accepted and merged into the new baseline otherwise it would be rejected.
results table shows the differences between the modified version sut b and its baseline version sut a for each metric suggestion rate difference and recall rate difference along with the a b experiment outcome.
the fractions are converted to percentages for display purpose and the sign of the values correspond to negative and positive outcomes.
the modification that contains negative values in our targeted test suite coincides with the outcome of the independent a b experiments.
table relative changes for different modifications sugg.
rate differencerecall rate differencea b exp.
outcome mod .
.
rejected mod .
.
rejected mod .
.
accepted discussion both mod and mod were rejected modifications which attempted to improve the recall rate on mi queries.
both suffered in the experiments as more users were seeing no results in a b experiments.
the root causes agreed with the negative suggestion rate difference determined from our test suite.
interestingly when we observed the ml improvements in mod via the increased recall rate on some queries we were also able to catch emergent timeout issues that led to no results on other queries which ended up having a stronger impact on the user and led to experiment to be rejected.
for mod we observed alignment in the positive outcomes between our tests and the experiments even though the scale of the improvement does not directly correspond to the increase in ac conversion rate measured via online experiments.
.
hypothesis defect class analysis identifies systematic issues instead of individual failures for holistic improvement metrics we investigated the current production baseline to determine if there are other areas that could be improved for ac mi queries.
here we consider not only the recall rate but also the ranking behavior.
given a test set xjon a specific class of inputs let qj be the number of tests that recall any pois and rk jbe the number of tests that recall the search conversion epoi among the top k suggestions.
the ranking precision is defined as rk j qjifqj and ifqj the precision is set to .
for the following analyses we setk 6as it corresponds to the number of suggestions presented to the user which does not require additional scrolling actions in the ac system we studied.
in addition to the coverage dimension of intent types described in section .
we also include a second coverage dimension a distance dimension measured by the average travel time from the viewport centers to the epois.
we selected this dimension because this distance strongly influences the ranking due to a mix of rule based and ml based logic in the sut.
the distance dimension is split into sub categories based on min min hr hr or day driving time as thresholds to reflect the spectrum of user scenarios.
procedure taking the same target set of mi queries we ran all the test cases against the production version of the software to perform defect class analysis described in section .
we partitioned the inputs into subsets based on the intent patterns shown in table for the ac sut to compute the recall rate and ranking precision for each subset based on eq.
and compared which pass rates were systematically lower than the rest for each metric.
for each of the subsets we further partitioned the queries based on the distance dimension and repeated the computation of the same metrics for each sub partition of inputs.
results we visualize the query patterns in the full mi input space with a 3grid in figure where each row corresponds to an epoi subtype unique places category or brand and each column corresponds to a specific query composition location modifiers in the suffix location suffix after preposition preposition or in the prefix location prefix .
the values in the heatmap were initially recall rates but we scaled and centered the values around to highlight the contrast across different query patterns to distinguish the ones below average and the ones above average.
after normalization we also use the color spectrum from red to green to indicate the relative score from worst to best in figure and subsequent heatmaps.
in figure each of the cells were further divided into subcategories based on the distance dimension.
each value in the heatmap corresponds to the ranking precision which are then scaled and centered like figure .
discussion based on the results we identified defect classes that were relatively worse than average.
category queries with prepositions and locations this class has the lowest score in the center cell of figure .
it indicated that prepositions can often confuse ac when it is followed by incomplete location modifiers because ones like on in can also 1297esec fse november singapore singapore junjie zhu teng long wei wang and atif memon figure the heatmap displays relative recall rates for the different query patterns among mi queries for a production sut.
rows correspond to different poi subtypes and columns correspond to different query compositions.
values are centered at and scaled to magnitude .
red represents overall worse cases and green represents overall better cases.
figure this heatmap contains the identical query patterns on the same mi queries in figure but further subdivides each cell into distance categories.
the values in this heatmap are the relative ranking precision instead of the recall rate but each value is centered scaled and colored according to figure the spectrum from red to green also indicates the relative score from worst to best.
be seen as abbreviations of entities that are part of an address and ones like at by are often followed by specific types of queries.
queries with unique places these correspond to any mi queries with unique places shown the top row in figure such as hollywood sign california disney land in florida and washington park bellevue .
with the complete query a user was able to directly see and select the result from click search but the ac failed to recall the result.
our metrics quantified the gap between unique places and other query classes category and brand and suggested that there is still potential for the system to leverage the uniqueness of these entities to disambiguate the location modifiers.
users with distant viewports for the cases where the distance was more than a hour drive highlighted by the blue bounding boxes in figure we observed that pois in the viewport were returned and ranked much higher frequently.
this means that the system has a higher tendency to favor the viewport over the georelated text hints in the mi query.
consequently the epoi closer to the user location was not ranked high enough compared to other pois.
this is a more complex behavior where the user intent can be ambiguous because they can either prefer something close by or in a remote view port and the ranker needs to discern the intent based on additional query or contextual features.
in comparison our previous test suites which relied on ad hoc test scenarios and individual pass fail use cases were unable to identify these defect classes because the tests lacked holistic measures for proper comparisons across sub groups and they did not contain the scale of inputs to obtain sufficient number of cases for each defect class.
.
hypothesis our strategy applies to sub component testing and can drive triaging of new regressions metrics we also considered how testing subcomponents that perform specialized tasks can accelerate the triage process with our holistic approach.
the qi subcomponent relies on its own ml models and performs simultaneous query tagging completion and entity recognition defined in section .
.
thus we designed the following three recall based metrics based on the three corresponding qi specific oracles described in section .
tagging whether there is an interpretation whose entity tags corresponding to the generic classification of the segmented query equal those in the oracle.
completion whether there is an interpretation whose segments where each can contain query rewrites and last one can contain any necessary completion equal those in the oracle.
resolution whether there is an interpretation whose entity ids corresponding to a specific entity in the database e.g.
a specific brand or category equal those in the oracle.
procedure to guarantee the functionality of qi and focus on catching regressions we partitioned and stratified the inputs according to the derived intent patterns from ac conversion data shown in table for the qi sut .
the resulting data covered all supported intent patterns with at least queries and had been running in all testing environments comparing every new sut with the current sut in production.
for each intent pattern we required all three metrics computed based on the holistic difference in eq.
to be greater than instead of for each evaluation.
we allow for this margin to account for minor variations in the production data such as changing business names that can lead to a small number false positive failures.
results over the course of a few months after it was developed the qi test suite caught multiple regressions and blocked them from being released to production.
here we highlight two real scenarios that were identified and we only show the query classes that are relevant to the regressions we demonstrate .
each row corresponds to a different metric and each column corresponds 1298improving ml based information retrieval software esec fse november singapore singapore figure the heatmaps display two modifications a and b respectively.
each modification is compared with its baseline counterpart in production over qi specific functional metrics rows and query classes columns .
the values of each cell correspond to the percentage difference between the modification and the baseline based on eq.
.
to a different intent pattern.
the highlighted values indicate the relative regressions red and improvements green .
discussion first we identified a candidate generation artifact corresponding to figure a which improved mi performance but caused regressions on simpler queries especially category queries.
the intended improvement aimed at increasing the category and brand recalls of specific businesses but ended up creating an increasing number of noisy entity ids for the simple queries.
this artifact overrode the ones that match the oracles especially for category tag queries resulting in difference in the resolution metric.
additionally those that yielded an extreme number of entity ids ended up not returning any interpretations at all captured by in both tag and completion difference metrics .
second corresponding to figure b we caught a data change that removed noisy tokens e.g.
those with misspells such that qi could reduce the occurrence of unintuitive query completions.
indeed we noticed completion improvements in two classes of queries category tag and location category tag 5 2 .
however many of the entity ids and tags which were originally mapped to the removed tokens could no longer be retrieved resulting in a loss in resolution and tagging in the same query class.
this blocked the particular release sign off and led to a fix.
general methodology even though we used the search ac application as our running subject we have identified opportunities to apply similar methodologies to other applications of ml based software.
one of our recent works offers an example of an ml based spelling correction software where the test oracles can be automatically derived from production data.
specific to the spell correction functionality the test dimensions were captured by edit distancebased input typo spaces and the functional evaluation relied onrecall and ranking measures.
we note that these measures are special cases of the holistic metrics described in our current work and hence the defect class analysis applied to the search ac application should also be able to systematically identify defects including the ones reported in the original paper.
taken together both subject applications prescribe a general methodology that applies to testing the emergent behavior of generic ml software.
the methodology consists of the following steps define the functional behavior of the suts including inputoutput relations that capture ml behavior.
determine the source of truth sot that can determine the expected outputs i.e.
the test oracle .
specify coverage dimensions that reflect core functions and sample test cases according to the dimensions.
evaluate the sut based on the overall holistic metric to determine the degree of improvement or regression.
repeat the evaluation of the metrics on sub dimensional inputs and determine if there are defect classes.
it is crucial in step to distinguish the black box software behavior from the internal ml tasks so that the tests reflect the integrative influence of the ml or multiple mls .
for instance consider a speech recognition software that performs three functions speaker identity recognition language detection and transcript generation.
the black box input we consider could be an audio recording along with user configurations and the output could be the transcribed text along with interpretations.
on the contrary an internal ml may only rely on cleaner pre processed data for its task and the output from the ml needs to be post processed based on other configurations.
solving the oracle problem in step requires an sot that contains the content needed for functional assertions.
for the speech recognition software this means knowing the expected outputs associated with each individual function such as how the user is addressed the desired displayed language and the output text with appropriate highlighting.
the testers can take the user feedback as the sot to extract these outputs validated by the user either from explicit rating by the user or from positive subsequent interactions captured by the software.
there are generally two sources of oracles in house evaluation and crowdsourcing which mirrors the process of data labeling for ml.
this example and our proposed solution in section .
both fall under crowdsourcing from production data a type of semi automated labeling .
however if production data is not available one can still derive the oracle from the existing sot dataset being served e.g.
all entries in a knowledge graph since they can be converted into some form of expectation at the software output.
specifying coverage dimensions in step is a common procedure in traditional software testing which ensures sufficient use cases to cover all software functions.
this notion applies to our framework as well.
however because the space of inputs e.g.
text of audio inputs cannot be fully enumerated we resort to stratified sampling on each key dimension in section .
to ensure sufficient representation within each dimension.
the reasoning follows from label balancing used for ml training .
for instance to ensure all the languages supported by the speech recognition software are 1299esec fse november singapore singapore junjie zhu teng long wei wang and atif memon covered the tester can define language as an attribute of each test case whose value is derived from the user preference or from the output language with positive feedback and then balance the test cases along this dimension according to algorithm .
once test cases are generated steps and allow the tester to evaluate the software systematically they can rely on the holistic metric in section .
to evaluate a modification compared to a baseline to see if the change incurs an improvement or regression not only for each of the functional metrics but also for each subspace within each dimension for defect class analysis as explained in section .
.
with the speech recognition software for example one can rely on the specific metrics to answer different questions about a modification e.g.
whether there is correlated improvements in all three functionalities whether there is a trade off between language detection and others whether a regression in language detection is specific to only a subset of languages.
taken together these signals can allow the testers and developers to understand how to improve the software and prioritize the modifications systematically for timely evaluation .
related work our work builds upon a rich body of literature involving automating software testing at scale and addressing challenges of ml software testing.
here we discuss prior work on testing ir and ml systems.
evaluation of search and similar ir systems e.g.
other variations of search ac systems has been an active research area in automated software testing.
several works have proposed user driven black box testing approaches to solve the test generation and oracle problems that we also faced for their applications zhou et al.
adopted metamorphic testing to create tests for web search engines olteanu et al used crowdsourcing to identify categorize and understand problematic search completion suggestions.
in the work of shokouhi et al the researchers designed a time sensitive query autocomplete service and evaluated it against query sets with daily weekly monthly and annual cycles.
while these works focus on specific classes of inputs that are difficult to evaluate in search systems our case studies take a global view of ml ir systems based on our holistic metrics to determine how functional regressions need to be fixed and provide patterns of defects to prioritize triaging.
our results for ac and qi show that our strategy can adapt to complex ml ir systems by considering several regression metrics in a multi dimensional input space.
assuring the functionality and quality of ml based systems has also been proven complex and challenging by previous research.
zhang et al.
surveyed a comprehensive landscape of ml testing summarizing topics in robustness privacy efficiency and fairness.
lewis et al.
studied faults caused by mismatches among data science software engineering and operations in ml enabled systems.
santhanam et al.
and devanbu et al.
discussed the challenges of assessing deep learning systems.
other reviews have emphasized specific aspects of evaluating ml systems such as defect detection in ml programs or implementation quality .
practitioners in industry have also echoed the level of complexity of testing ml systems.
lwakatare et al.
reviewed challenges and solutions of testing large scale ml systems.
amershi et al.
conducted a study over the development workflows of multipleml based systems in microsoft and emphasized the importance of continuously evaluating the both the ml and non ml parts of ml based systems end to end.
to detect faults in neural machine translation systems zheng et al.
derived properties between input and output which can be checked systematically and applied it to test wechat a messenger app with over one billion monthly active users .
chen et al.
focused on continuous triaging of large scale online service systems.
washizaki et al.
surveyed good bad software engineering design patterns for ml techniques in practice.
chen et al.
studied the challenges in the deployment process of ml systems with a focus on deep learning based software.
even though these approaches do not prescribe a generic system testing workflow like ours they pose the multitude of existing industrial challenges including scalability applicability and acceptability and inspired us to propose our practical strategy.
conclusion as ml ir systems get increasingly complex consisting of multiple ml models and non ml decision making code they continue to challenge conventional software testing.
we presented a new approach to test such systems thereby addressing these challenges by leveraging functional test cases to inform ml ir developers holistically about the impact regression improvement of their latest code and data modifications measuring system regression at the level of a defect class a partition of the input space on which the ml ir software does measurably worse for an existing new feature and drilling down deeper to sub components to understand the causes of regressions.
our deployment on a production searchac system demonstrated the effectiveness of our new approach by successfully detecting multiple regressions and identifying defect classes at the system as well as sub system level to ease debugging.
meanwhile defining holistic metrics from individual functional test failures together with identification of defect classes is a powerful and general approach for continuous iterative improvement of any ml system.
in the near future we plan to study how defect classes evolve in ac and qi over time as new features are added systems are re designed and new usage patterns start emerging.
in the medium term we will apply the same approach to other ml systems including ones for which we do not have access to production data and hence we will need to develop test cases using other generative models.
in the long term we plan to build frameworks to generalize this approach understand its strengths and weaknesses and implement general tools that others can leverage to test their ml systems.
acknowledgment we would like to thank alex braunstein for constant support of our work emily kowalczyk for feedback on the manuscript and archana bhattarai for sharing insights into our subject application.