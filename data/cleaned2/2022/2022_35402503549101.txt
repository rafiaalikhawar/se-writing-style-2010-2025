less training more repairing please revisiting automated program repair via zero shot learning chunqiu steven xia university of illinois urbana champaign chunqiu2 illinois.edulingming zhang university of illinois urbana champaign lingming illinois.edu abstract due to the promising future of automated program repair apr researchers have proposed various apr techniques including heuristicbased template based and constraint based techniques.
among such classic apr techniques template based techniques have been widely recognized as state of the art.
however such template based techniques require predefined templates to perform repair and their effectiveness is thus limited.
to this end researchers have leveraged the recent advances in deep learning to further improve apr.
such learning based techniques typically view apr as a neural machine translation problem using the buggy fixed code snippets as the source target languages for translation.
in this way such techniques heavily rely on large numbers of high quality bug fixing commits which can be extremely costly challenging to construct and may limit their edit variety and context representation.
in this paper we aim to revisit the learning based apr problem and propose alpharepair the first cloze style orinfilling style apr approach to directly leveraging large pre trained code models for apr without any fine tuning retraining on historical bug fixes.
our main insight is instead of modeling what a repair edit should look like i.e.
a nmt task we can directly predict what the correct code is based on the context information i.e.
a cloze or text infilling task .
although our approach is general and can be built on various pre trained code models we have implemented alpharepair as a practical multilingual apr tool based on the recent codebert model.
our evaluation of alpharepair on the widely used defects4j benchmark shows for the first time that learning based apr without any history bug fixes can already outperform state of the art apr techniques .
we also studied the impact of different design choices and show that alpharepair performs even better on a newer version of defects4j .
with .3x more fixes than best performing baseline indicating that alpharepair can potentially avoid the dataset overfitting issue of existing techniques.
additionally we demonstrate the multilingual repair ability of alpharepair by evaluating on the quixbugs dataset where alpharepair achieved the state of the art results on both java and python versions.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november singapore singapore association for computing machinery.
acm isbn .
.
.
.
concepts computer systems organization neural networks software and its engineering software testing and debugging .
keywords automated program repair deep learning zero shot learning acm reference format chunqiu steven xia and lingming zhang.
.
less training more repairing please revisiting automated program repair via zero shot learning.
inproceedings of the 30th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november singapore singapore.
acm new york ny usa pages.
introduction software systems are all pervasive in everyday life from monitoring financial transactions controlling transportation systems to aiding healthcare tools .
software bugs in these systems can affect people around the globe and cost billions in financial losses .
to fix such bugs developers often need to invest significant manual effort e.g.
it is estimated that developers spend to of their time debugging software systems .
to reduce manual debugging efforts automated program repair apr techniques have been proposed to automatically generate patches to fix the bugs .
a popular approach for apr is generate and validate g v .
to start off fault localization is often used to reduce the search space by computing the suspicious program locations that likely caused the bug.
using these potential buggy locations the g v techniques will generate a list of candidate patches.
each candidate patch is compiled and validated against the test suite.
patches that successfully pass all tests are called plausible patches .
however tests often cannot cover all possible behaviors of the program hence a plausible patch might still fail under other inputs.
therefore plausible patches are further inspected by developers to determine the final correct patches that correctly fix the underlying bug.
depending on how patches are generated traditional apr techniques can be categorized into heuristic based constraintbased and template based .
among all traditional techniques template based apr techniques which leverage pre defined fix patterns to transform buggy code snippets into correct ones have been widely recognized as state of the art .
these fix patterns target specific types of bugs e.g.
null pointer exception and patterns in the source code and are often crafted by human experts.
while effective there is an upper limit to the number of candidate patches that such pre defined templates can generate.
therefore to increase the expressiveness of the edits esec fse november singapore singapore chunqiu steven xia and lingming zhang researchers have recently utilized machine learning ml or deep learning dl techniques for patch generation .
learning based apr techniques often leverage the recent advances in dl to train a neural network that transforms the original buggy program into the fixed version.
these techniques typically view the problem of program repair as a neural machine translation nmt problem and use nmt models from the field of natural language processing nlp where the model input is a language sequence and the output is a translated sequence in another language.
researchers have used nmt models for program repair where instead of translating natural languages the models aim to turn a buggy code into the fixed version.
these nmt models are typically made up of an encoder anddecoder pair where the encoder captures the buggy code elements with its context while the decoder takes in the encoded representation and generates a fixed version.
to facilitate apr such models must be trained using pairs of buggy and patched code.
despite being an effective apr approach existing learning based techniques face the following issues quality of training data.
current learning based apr tools require training or fine tuning of the models by using historical bug fixes i.e.
pairs of buggy and patch code.
this data is usually obtained by scraping open source projects to find specific commits that are about bug fixes.
however this relies on various handcrafted heuristics.
for example to find the bug fixing commits keywords such as bug fix patch solve issue problem are often used to filter the commits .
individual bug fixing commits can also include unrelated edits such as refactoring or new feature implementation .
as a result the extracted data can contain various irrelevant commits and unrelated code changes within bug fixing commits adding noise to the training dataset.
quantity of training data.
compared to large amount of opensource code snippets that are available in the wild the amount of bug fixes is limited.
to reduce the effect of the aforementioned issue of a commit containing irrelevant changes from bug fixes learning based apr tools usually limit the commits in their dataset to ones with few lines of changes further limiting the amount of training data.
by training on such limited historical fixes current learning based tools might restrict the edit variety of their approach only on what is in their training data.
context representation.
to provide a correct fix to a buggy code snippet the context before and after are crucial in providing useful syntactic semantic information.
current learning based apr tools first pass the context including the buggy code elements into an encoder as plain texts or structured representations .
the encoded context is then used directly or combined with a separate encoding of the buggy code snippet as input to the decoder.
however this process is unnatural since it is challenging for the models to distinguish the patch location within the context or effectively merge the separate bug context encodings.
as a result such techniques may miss intricate relations between a patch and its context such as the proximity of each code element that provides important syntax semantic information.
our work.
we present alpharepair the first cloze style or infilling style apr approach that uses large pre trained code models under a zero shot learning setting to directly generatepatches i.e.
without any additional training or fine tuning on bugfixing datasets.
different from all existing learning based apr techniques our main insight is that instead of modeling what a repair edit should look like we can directly model predict what the correct code is based on the context information like a cloze or text infilling task .
in this way our cloze style apr can avoid all above limitations of the existing techniques it completely frees apr from historical bug fixes it can simply get trained on all possible open source projects in the wild for massive training it is directly pre trained to model patches based on the surrounding context and thus can effectively encode the intricate relations between patches and their contexts.
furthermore while it is non trivial to adapt prior apr techniques for a new language due to a huge amount of code data engineering work for preparing historical bug fixes under our cloze style apr extending apr to a new language can be as simple as mining a new code corpus in the new language!
while our cloze style apr is generalizable to various pre trained models in this paper we implement alpharepair with one recent pre trained code model codebert .
unlike current learningbased apr tools which use limited numbers of bug fixes as training data codebert is directly pre trained using millions of code snippets from open source projects allowing it to provide a variety of edit types to fix different bugs.
we directly leverage the initial training objective of masked language model mlm predicting recovering randomly masked out tokens used in codebert to perform zero shot learning for apr.
we first prepare model inputs where each potentially buggy line is replaced with a mask line.
we then query codebert to fill the mask line with replacement tokens to produce candidate patches for a buggy program input.
this allows us to directly perform zero shot learning with no fine tuning since we use the same tasks as defined in mlm instead of predicting random mask tokens we generate predictions by masking out only the buggy code snippet.
note that to improve the patch search space the mask lines are designed to systematically reuse parts of the buggy line.
furthermore the bidirectional nature of codebert and mlm also allows us to naturally capture both the contexts before and after the buggy location for effective patch generation.
contribution.
this paper makes the following contributions direction dimension this paper opens a new dimension for cloze style apr to directly query large pre trained code models under a zero shot learning setting.
compared with existing learning based apr techniques our approach does not need any additional fine tuning retraining using historical bug fixes and can be easily adopted for multiple programming languages.
additionally we demonstrate the efficacy of directly applying large pre trained models for generating code fixes for real world systems which previously were mainly tested on generating simple small programs .
technique while our idea is general and can leverage various existing pre trained code models we have built alpharepair as a practical apr tool based on the recent codebert model.
we leveraged the original training objective of codebert using specific inputs of mask lines for direct program repair.
we used a variety of different mask line templates and further propose probabilistic patch ranking to boost apr.
960less training more repairing please esec fse november singapore singapore encoder context bug line bug line context context fix line fix line context decoder buggy code encoded representation fixed code decoder i want to build a repair decoder only gpttool encoder i mask to build a mask i want to build a repair encoder only bert decoder encoder i want to build a repair to repair i a build want encoder decoder t5a nmt repair overview b large language model overview figure nmt and large language model overview extensive study we have compared alpharepair with stateof the art java apr tools both traditional and learningbased on defects4j .
the results show that alpharepair can outperform all existing tools on the widely studied defects4j .
improving the number of fixed bugs from to and fixing unique bugs that no prior work can fix.
more surprisingly alpharepair even fixes .3x more bugs than the best baseline on the newly included bugs in defects4j .
demonstrating that alpharepair can avoid the datasetoverfitting issue of existing apr techniques.
moreover we have also studied alpharepair on both the java and python versions of the widely studied quixbugs dataset .
the results not only confirm that alpharepair can outperform all existing apr techniques in both java and python but also demonstrate the multilingual capability of alpharepair.
background .
learning based apr deep learning dl is a powerful learning mechanism to learn from large amounts of data used in many domains of machine learning.
researchers have leveraged dl techniques for apr by viewing the problem of program repair as a neural machine translation nmt task of turning a buggy code snippet into a patched version.
these learning based apr tools are typically built using the encoder decoder architecture.
figure 1a shows the workflow of these learning based repair tools.
the encoder takes in the buggy line and its surrounding context as input and outputs an encoded representation.
the decoder uses the encoded representation to generate a new code line to replace the buggy line.
sequencer is a sequence to sequence network built using a long short term memory lstm encoder and decoder network for program repair.
dlfix encodes the input code as an abstract syntax tree then uses a tree based recurrent neural network as a part of the encoder and decoder network to generate patches.
coconut proposes a new nmt architecture which encodes the context and buggy line separately.
it also leverages multi stage attention to increase the amount of important information passed on from the encoder to the decoder.
in order to improve the syntactic correctness before training on code fix pairs cure first pre trains the nmt model on a large corpus of developer code.
furthermore cure uses a static checking strategy to only generatepatches with valid identifiers.
more recently recoder modifies the nmt model by using a syntax guided decoder designed to generate syntactically correct edits on an input buggy code snippet.
recent study has shown that learning based apr tools can achieve the state of the art in apr .
however they are still limited by their need for pairs of buggy and fixed versions as training data.
these buggy and fixed code pairs are challenging to obtain as handcrafted heuristics are used to identify bug fixing commits and individual commits can contain other code changes apart from bug fixing adding noise to the dataset.
additionally learning based apr tools learn bug patterns and corresponding patch fixes from the training data in order to automatically generate patches for unseen buggy code.
this means it is hard for learningbased apr tools to generalize to fix patterns that are not present in their training dataset.
furthermore it can be tricky for current learning based apr tools to encode the context surrounding buggy code elements causing the generated patches to miss important syntax and semantic information.
interestingly although pre trained code models have also been adopted for apr recently they are still leveraged to learn from a large number of historical bug fixes thus still suffering from the above limitations of existing learning based techniques.
in this paper we address these issues by using large pre trained code model trained on massive amount of open source code directly for cloze style apr without the need to train or fine tune on any smaller dataset of buggy and fixed code.
.
large pre trained code models recent popularity in natural language processing nlp has led to development of large pre trained models that use large amounts of data.
to make use of the massive unlabeled training data pretrained models apply self supervised objectives e.g.
masked language model mlm where some training data is artificially masked and the training objective is to predict recover the real data.
a common component of large pre trained language models is a transformer .
it contains an encoder made up of multiple differentiable self attention layers in order to learn representation and also a decoder used to generate output.
figure 1b shows the three categories of large pre trained language models.
gpt is a large generative model which uses only the decoder component to predict the next token output given all previous tokens.
this type of decoder is autoregressive where a sequence is generated by iteratively inputting all previous tokens in order to generate the next one.
bert is another type of large pre trained model which contains only the encoder component.
bert is designed to learn a representation of the data and is trained using the mlm objective.
a small percentage of tokens in the training data will be replaced by a mask token where the goal is to train bert to predict the true value of the mask token.
to combine the usage of both encoder and decoder encoder decoder models have also been used to build large pre trained language models.
models such as t5 are designed for sequence to sequence tasks where the training objective aims to recover the correct output sequence given the original input english to french corrupted to uncorrupted etc .
these large pretrained language models can be fine tuned for downstream nlp 961esec fse november singapore singapore chunqiu steven xia and lingming zhang tasks such as text summarization text classification as well as question and response text generation .
researchers have extended encoder decoder and encoder decoder models to build large pre trained models for various programming language tasks.
codegpt adopts the original gpt architecture and trains a generative model from scratch using python and java functions.
codex is a gpt based code model created by fine tuning a larger gpt model for generating python functions based on natural language descriptions.
codebert and graphcodebert are bert based models for programming tasks and are trained using the mlm training objective.
graphcodebert additionally encodes simple data flow information to aid in code representation.
codetrans codet5 and plbart are unified encoder decoder models which uses denoising sequence to sequence training objectives to pre train both the encoder and decoder for various coding tasks.
in this paper we directly use large pre trained models for clozestyle apr via zero shot learning.
while our cloze style apr idea is general and can be achieved using all above pre trained models we demonstrate its potential by using the simple codebert model.
codebert is trained using the mlm objective which can be used to generate replacement code for buggy code snippets.
also codebert is bidirectional in nature allowing it to capture both contexts before and after for patch generation.
approach in this section we introduce cloze style apr a new direction for learning based apr that directly learns from the large number of code snippets available in the wild to generate patches.
different from all prior learning based apr techniques instead of viewing apr as a task of turning a buggy code snippet into a fixed code snippet we can directly learn what the correct code should look like given its surrounding context.
we view the problem as a cloze task where we replace the buggy snippet with blanks masks and query pre trained models to fill the blanks with the correct code snippet.
this problem setup does not require access to any dataset containing pairs of buggy and patched code versions and our approach can directly make use of the existing large pre trained models to automatically generate code repairs.
although our approach is general in this work we re purpose the recent codebert model for program repair.
the training objective for codebert uses masked language model mlm where given an input sequence of tokens x x1 x2 ...xn a random set of tokens in xis replaced with mask tokens to generate a new mask sequence xmasked .
the training goal is to output the original tokens in xmasked which have been masked out i.e.
recover xgivenxmasked .
given predictor pwhich outputs the probability of a token the mlm loss function can be formulated as lmlm i masked log p xi xmasked the mlm training objective allows us to directly use codebert for program repair where instead of randomly masking out tokens we mask out all tokens which are part of the buggy code snippet.
we then use codebert under a zero shot learning setting for program repair where we recover the correct tokens in place of the maskbuggy tokens.
as a result alpharepair does not require any additional retraining or fine tuning stage on bug fixing datasets since the mlm training is done as part of the pre training tasks in codebert.
while our basic idea is applicable for apr at different levels following state of the art learning based apr tools we focus on single line patches in this work.
figure provides an overview of our approach step section .
we first take in a buggy project and separate the surrounding context and the buggy line according to fault localization information.
we encode both the context before and after into token representations.
additionally we also encode the buggy line as a comment in the natural language input for codebert.
step section .
using the buggy line we generate multiple mask lines using templates replace entire line replace starting ending part of line etc .
each mask line replaces the buggy line and is tokenized together with the surrounding context as inputs to codebert.
step section .
we iteratively query codebert to generate candidate patches using mask lines.
each patch replaces the mask line with a generated code line.
step section .
we use codebert again to provide patch ranking by computing the score of the generated patch using the joint probability of the generated tokens.
step section .
we compile each candidate patch and validate it against the test suite.
finally we output a list of plausible patches for developers to examine.
.
input processing to generate the inputs for alpharepair first we extract the buggy line and surrounding context from the buggy project.
we use the codebert tokenizer which is built using byte level byte pair encoding bbpe a technique to reduce the size of the vocabulary by breaking uncommon long words into subwords that are found commonly in the corpus .
bbpe has been used in various models and shown to mitigate out of vocabulary issues .
figure provides an example input of a buggy program.
we first define our tokenization structure a list of tokens as inputs for codebert.
we tokenize both the context before and after and sandwich the mask line placeholders for what codebert will predict described in section .
between them.
for program repair the buggy line itself is also important to generate a patch.
however it does not make sense to include it as a part of the context since we aim to generate code to replace it.
to capture the encoding for the buggy line we make use of the bimodal nature of codebert where it can take in both programming language and also natural language comments .
we transform the original buggy line into a comment by surrounding it with the block comment characters comment .
recall equation which describes the basic mlm loss function where xmasked is a mask sequence.
xmasked in codebert concatenates both natural language and code tokens such that xmasked wmasked cmasked wherewmasked andcmasked are mask sequences of natural language and code token sequences.
the original mlm loss function 962less training more repairing please esec fse november singapore singapore pre trained codebert model buggy project context before context before buggy line context after context after context before context before mask line context after context after context before context before mask line context after context after context before context before mask line context after context after context before context before mask line context after context after context before context before mask line context after context after context before context before candidate fix context after context after buggy line w context processed masked input candidate patches ranked candidate patches ranked plausible patches 5context before context before mask line context after context after context before context before mask line context after context after context before context before candidate fix context after context after context before context before mask line context after context after context before context before mask line context after context after context before context before plausible fix context after context after figure alpharepair overview context before buggy line context after if dataset !
null tokenization structure comment buggy line context before context after mask line max tokens public legenditemcollection getlegenditems legenditemcollection result new legenditemcollection if this.plot null return result int index this.plot.getindexof this categorydataset dataset this.plot.getdataset index if dataset !
null return result int seriescount dataset.getrowcount if plot.getrowrenderingorder .equals sortorder.ascending for int i i seriescount i if isseriesvisibleinlegend i legenditem item getlegenditem index i figure example input for alpharepair is now lbimodal mlm i masked log p xi wmasked cmasked this way codebert learns both modalities of function representation natural language and function code .
alpharepair makes use of this additional understanding and transforms the buggy line into a comment.
together the comment buggy line context before mask line and context after are tokenized as input for codebert.
to maximize the context we can encode we start from the buggy line and increase the context size lines away from the buggy code until we reach the maximum codebert input token size of .
.
mask generation in order to generate patches we replace the buggy line with a mask line .
mask line is defined as a line with one or more mask tokens mask .
we use codebert to fill each mask token with a replacement code token and together the filled mask line becomes a generated patch line.
figure shows the strategies we use to generate a mask line complete partial and template mask.
complete mask.
the simplest strategy is to replace the entire buggy line with a line containing only mask tokens.
we refer to this as line replacement since we ask codebert to generate a new line to replace the buggy line.
we also generate mask lines where we add mask tokens before after the buggy line.
these represent bug fixes where a new line is inserted before after the buggy location.
if fntype !
null mask mask .... mask mask mask mask .... mask mask if fntype !
null mask mask .... mask mask return founddigit !hasexp mask mask ... mask !hasexp return mask mask ... mask primitivevalues.put double.class mask mask ... mask double.class primitivevalues.put mask mask .. mask primitivevalues.put double.class mask primitivevalues.put double.class mask if endindex if endindex mask .. mask if endindex mask line replace line after partial after partial before method replace parameter replace single replace add parameter more cond replace operator complete mask partial mask template mask template mask if mask mask ... mask expression replace line before figure different strategies to generate mask lines partial mask.
another strategy of generating mask lines is by reusing partial code from the buggy line.
we first separate the buggy line into its individual tokens and then keep the last first token and replace all other tokens before after with the mask tokens.
we then repeat this process but append more tokens from the original buggy line to generate all the possible versions of partial mask lines.
for example in the partial before strategy in figure we start with generating a mask line of return mask mask ... mask by keeping the first token of return .
then we generate another mask line by keeping the first two tokens return founddigit to generate return founddigit mask mask ... mask .
in total we generate l number of mask lines where lis the number of tokens in the buggy line for both partial after and before generation method.
this approach is motivated by patches where the fix will reuse parts of the buggy line.
by prepending and appending the mask line with a part of the buggy line we can reduce the number of tokens codebert needs to generate for a correct fix.
furthermore the partial buggy code acts like initial starting point for codebert to start generating tokens by providing important context.
963esec fse november singapore singapore chunqiu steven xia and lingming zhang template mask.
we implemented several template based mask line generation strategies targeting conditional and method invocation statements as they are two of the most common bug patterns .
additionally several traditional apr tools focus solely on fixing conditional statement related bugs showing the importance of targeting common bug patterns.
unlike the previous strategies template mask can only be generated for specific buggy lines.
the first set of templates are designed to target buggy method invocations.
method replacement will replace the method call name with mask tokens.
this represents asking codebert to generate a replacement method call using the same parameters as before.
we also use several parameter based changes replacing the entire inputs with mask tokens replacing one parameter with mask tokens and adding additional parameter s more than one parameters can be added since we vary the number of mask tokens therefore codebert can add multiple parameters .
we also designed template mask lines for conditional statements in the form of a boolean expression.
we generate mask lines that replace the entire boolean expression or add additional and or expressions by appending the statement with mask tokens.
additionally we also identify common operators etc and replace them directly in the buggy line with mask tokens.
these template based mask line generations are inspired by common fixes for many bugs and also previous apr tools that utilize preset templates to fix bugs .
these simple generated templates serve a similar functionality to the partial masks in providing more starting code for codebert to generate potential patches.
for a fix that only needs to modify a small part of the buggy code codebert only needs to generate a small number of tokens using mask lines from the template masks.
by including a larger portion of the buggy code we reduce the search space codebert has to consider.
for each generated mask line we increase the number of generated tokens from until the total number of tokens in that line becomes l wherelis the number of tokens in the original buggy line.
for example if we use the partial after strategy on a buggy line with lof and we keep the first original tokens we will vary the number of masked tokens from to .
this process is done for each masking strategy that we apply except for the replace operator strategy where we only replace common operators with a single mask token .
we apply the input processing step section .
for each mask line to obtain a list of processed inputs for codebert.
.
patch generation in order to generate a patch that replaces the original buggy code we use codebert to generate code token for every mask token in our input.
to this end we leverage the original training objective of mask replacement used in codebert.
codebert is trained by predicting the correct token value that has been replaced with a mask token given its surrounding context.
for each mask token codebert outputs a probability for each token in its vocabulary to replace the mask token.
typically during training a small percentage of tokens is masked out and the model will attempt to predict the actual token that has been replaced.
if endindex startindex mask mask if endindex mask mask mask temp joint score if endindex endindex mask mask if endindex emptyrange mask mask if endindex endindex mask if endindex startindex mask if endindex startindex mask initial input mask first round second round if endindex mask endindex if endindex mask if endindex mask startindex re ranking first token codebert query if endindex startindex mask endindex if endindex startindex mask if endindex endindex mask startindex re ranking second token codebert query if endindex startindex mask if endindex startindex mask if endindex endindex mask re ranking final token codebert queryjoint score .
.
.
a patch generation b patch re ranking codebert query codebert query if endindex startindex endindex if endindex endindex startindex if endindex startindex final output codebert query .
.
.
.
.
.
.
.
.
temp joint score temp joint score figure patch generation and re ranking example the task we have is similar to the original training objective of codebert where we also preprocessed the inputs such that a small set of tokens has been masked out.
however a key difference between our input and the masked training data is our mask tokens are grouped together .
a distinguished feature of codebert and bert family of models is the bidirectional nature where each token representation is predicated not only on context before but also context after.
in order to generate replacement tokens for the mask tokens codebert looks at the tokens before and after the mask location.
this works well for the training objective since the mask tokens are spread out where each token has sufficient tokens before after to give context.
however for our input data the mask tokens are together.
to generate an output for a mask token in the middle the immediate context before after are all mask tokens.
in order to facilitate token generation for grouped mask tokens we iteratively output tokens by replacing mask tokens with previously generated tokens.
figure 5a shows an example of how this process is done.
we start with the initial input mask line of if endindex mask mask mask and use codebert to determine the top n most probable replacement tokens for the first mask token.
n is the beam width and is an adjustable parameter for our approach.
in this example n is meaning we take the top most likely token along with its conditional probability.
in the next iteration we query codebert again by replacing the first mask token with the top replacement tokens .startindex .endindex .emptyrange to find the top token pairs with the highest joint conditional probability .startindex .startindex .endindex .
we call 964less training more repairing please esec fse november singapore singapore this joint conditional probability value temp joint score .
givenn as the mask token length t t1 ...tp mask p ...mask n as the mask line with ptokens generated so far p n letc t t be the codebert conditional probability of generating twhen all tokens intafter and including thave been masked out the temp joint score is defined as temp joint score t pp i 1log c t ti we note here that the temp joint score does not represent the actual probability of the generated token once the complete line has been generated all mask tokens have been replaced .
this is because codebert uses both context before and after when determining the likelihood of a replacement token the probability value does not account for the mask tokens to be generated in future.
when computing the temp joint score of a token sequence t1 ...tp tp mask p ...mask n in the mask line t codebert sees the values of the tokens before tp t1 ...tp however all tokens after are masked out mask p ...mask n .
that said the temp joint score is conditioned on the mask tokens whose values have not yet been decided and the conditional probability does not account for the future concrete values of those mask tokens .
thus we use this probability value as a proxy only temporarily and further re assign the likelihood after described in section .
for more precise patch ranking.
in each iteration we use the temp joint score to keep only the top highest score generated token sequences.
we repeat this process until we finish generating tokens for all mask tokens in our input.
by generating tokens sequentially we guarantee that when generating any mask tokens codebert has at least one side of the immediate context.
this helps with generating more syntactically correct candidate patches since codebert can use the previous immediate context to inform what the best next tokens should be.
this process is similar to beam search commonly used in code or natural language generation tasks .
one difference is that traditional code generation can accurately calculate the likelihood of a sequence as the average of the log conditional probability of the generated tokens.
for our approach the naive average is only an approximation of the likelihood since the probability outputs for tokens in the beginning of the mask line do not include future generated tokens.
to address this issue we further re rank each candidate patch by re querying codebert to obtain an accurate likelihood value as shown in section .
.
.
patch re ranking the re ranking procedure makes use of the codebert model again.
the key idea is to provide an accurate score i.e.
likelihood for each patch after it is fully generated for more effective patch ranking.
we start with the complete patch with all the generated tokens.
we then mask out only one of the tokens and query codebert to obtain the conditional probability of that token.
we apply the same process for all other previous mask token locations and compute the joint score which is an average of the individual token probabilities.
givenngenerated tokens in a sequence t t1 t2 ...tn letc t t be the codebert conditional probability when masking out only tokentin the sequence t the joint score is defined as joint score t nn i 1log c t ti the joint score can now be understood as the conditional probability of the generated sequence i.e.
given both contexts before and after what is the likelihood of the generated patch according to codebert?
.
this is done for all patches generated across all mask generation strategies complete partial and template mask .
we divide it by nto account for the token length difference since different mask lines have different numbers of mask tokens.
figure 5b shows an example of the re ranking process.
we use the patches from the patch generation example and for each of them mask out the first token and obtain the probability value from codebert.
we repeat this process for the other two tokens and finally we end up with joint scores for all patches.
we use the joint scores to provide a ranking for each patch.
by re querying codebert to obtain the joint score we can provide more accurate patch ranking that allows for prioritization when only a subset of generated patches can be validated.
.
patch validation for each candidate patch we generate we apply the corresponding changes to the buggy file.
we compile each patch and filter out any patches that fail to compile.
we then run the test suite against each compiled patch to find plausible patches that pass all the tests.
experimental design .
research questions in this paper we study the following research questions rq1 how does alpharepair compare against state of theart apr tools?
rq2 how do different configurations impact the performance of alpharepair?
rq3 what is the generalizability of alpharepair for additional projects and multiple programming languages?
we demonstrate the effectiveness of alpharepair by comparing against both state of the art traditional and learning based apr tools with perfect fault localization the exact fix location of the bug is provided and not perfect fault localization use the suspicious locations generated by fault localization as inputs.
note that the former is the preferred or only comparison setting for all recent learning based techniques since it eliminates the impact of other factors such as fault localization and can show the pure potential of different patch generation strategies .
therefore this paper also uses perfect fault localization by default unless specifically mentioned.
we also show the contribution for each of our design components by conducting an ablation study.
finally we evaluate the generalizability of alpharepair to additional projects in defects4j .
and quixbugs.
additionally we also evaluate multilingual repair capability of alpharepair by testing on the python version of quixbugs.
.
implementation alpharepair is implemented in python with pytorch implementation of the codebert model.
we directly reuse the model 965esec fse november singapore singapore chunqiu steven xia and lingming zhang parameters of the pre trained codebert model.
for perfect fault localization patch generation we use a beam width of and generate at most patches which is comparable to other baselines .
for not perfect fault localization patch generation we use a beam width of and consider the top most suspicious lines same as the recent recoder tool .
we use ochiai fault localization same as previous approaches .
for patch validation we use the uniapr tool .
all patches generated are validated and we evaluate alpharepair on an core workstation with intel i7 10700kf comet lake cpu .80ghz and 16gb ram running ubuntu .
.
lts and openjdk java bit server version .
.0 312 with nvidia geforce rtx ti gpu.
for all our experiments we set a time out of hour end to end time limit for fixing one bug consistent with previouslearning based tools .
.
subject systems for evaluation we use the widely used benchmark of defects4j .
defects4j is a collection of reproducible bugs from open source projects in java.
we first use defects4j version .
to answer research questions and .
defects4j .
contains bugs after removing depreciated bugs across different java projects.
to address research question we use defects4j .
which adds bugs on top of defects4j .
.
since alpharepair is designed for single line bug fixing we evaluate only on the single line bugs present in the new bugs in defects4j .
this setup is similar to previous single line apr tools .
we also use quixbugs that contains small classic algorithms with single line bugs used to evaluate many apr tools .
quixbugs contains both python and java versions of the same buggy programs.
we evaluate alpharepair on both python and java versions to demonstrate the multilingual ability of our tool.
.
compared techniques we compare alpharepair against state of the art baselines containing both learning based and also traditional apr tools.
for learning based apr we choose recently published tools evaluated on java or python bug datasets recoder java deepdebug python cure java coconut java and python dlfix java and sequencer java .
these tools use nmt models to generate patches given the buggy line and surrounding context.
following the most recent recoder work we also compare against state of the art traditional single hunk apr tools tbar prapr avatar simfix fixminer capgen jaid sketchfix nopol jgenprog jmutrepair and jkali .
in total our baseline comparisons comprise of different apr tools.
following prior work we use patch correctness results gathered from previous papers for defects4j .
evaluation and remove depreciated bugs.
we use the recently updated results of recoder by the authors instead of the outdated results in the original paper .
for many tools we can only obtain either perfect fault localization or not perfect fault localization therefore we only compare against baselines where the evaluation is under the same localization setting.
for defects4j .
evaluation we directly run the two best performing baselines of alpharepair others cure recoder tbar alpharepair coconut dlfix recoder cure a with learning based apr tools b with all tools figure correct patch venn diagrams for defects4j .
mask type template mask more condition mask line bug id mask type complete mask line replace mask line bug id closure 77a b figure example bug fixes in defects4j .
tbar template based and recoder learning based with perfect fault localization under the same setting as our tool and report the results.
for quixbugs evaluation we compare against several learning based apr tools since they have shown to perform the best on quixbugs for both java and python.
all baseline results on quixbugs are taken from previous papers experiments .
for evaluating our technique against state of the art tools we use the standard metrics of both plausible patches that just pass the entire test suite of a project and correct patches that are syntactically or semantically equivalent to the developer patches.
following the common practice for apr the correct patches are determined by manually inspecting each plausible patch for semantic equivalency.
result analysis .
rq1 comparison against state of the art .
.
perfect fault localization.
we first compare alpharepair with state of the art learning based and traditional apr tools under the preferred perfect fault localization setting.
table shows the performance of alpharepair along with other baselines that also use perfect fault localization.
alpharepair can successfully generate correct fixes for bugs which outperforms all previous baselines including both traditional and learning based apr techniques.
to show the effectiveness of alpharepair further we evaluate the number of unique bugs that only alpharepair can fix.
we first compare against learning based apr tools.
figure 6a shows the unique fixes of alpharepair and other learning based tools we 966less training more repairing please esec fse november singapore singapore table baseline comparisons with perfect fault localization project alpharepair recoder tbar cure coconut prapr dlfix sequencer chart closure lang math mockito time total correct plausible table baseline comparisons w o perfect fault localization tool correct plaus.
tool correct plaus.
alpharepair capgen recoder jaid avatar sketchfix dlfix nopol tbar jgenprog prapr jmutrepair simfix jkali fixminer exclude sequencer since it has unique bug fixes .
we observe that alpharepair is able to fix the most number of unique bugs of .
figure 6b shows the unique fixes of alpharepair the best performing baselines and all other apr tools combined others in figure 6b .
we observe that alpharepair is able to fix the most number of unique bugs of .
this also demonstrates that alpharepair can be used together with other techniques to further increase the number of correct patches that can be generated.
we provide a few examples of unique bugs that only alpharepair can fix.
figure 7a shows a bug with a missing length check on the array obtained from i.getarguments .
this is a difficult bug for both traditional and learning based tools to fix since the array used is not a variable but is obtained from a method call.
traditional apr tools such as template based tools can detect that the method call returns an array however it would be infeasible to add a length check for every such case as the search space would be too huge to traverse.
learning based tools rely on bug fixing changes for training data.
while there could be many inserted length check fixes this specific example inserting a length check on a return value from a method call can be rare to find in the dataset.
alpharepair can fix this bug since the usage of the codebert model does not require any bug fix code pairs and learns directly from large amount of open source data where many of them contain similar code where the length checks can be placed on different expressions not just simple array variables.
furthermore alpharepair also captures the context after and identifies the usage of kin accessing i.getarguments to insert the correct length check.
figure 7b shows another bug that only alpharepair can fix.
the correct fix is to insert an additional case statement to handle the missing case.
this is a difficult bug to fix since it does not just slightly mutate any existing code line but a completely new line needs to be added to handle a specific case in the program execution when cis .
alpharepair can generate the correct fix for this bug by identifying its surrounding context.
a case statement makes sense to insert here given the context of switch block and othertable component contribution component correct patch plausible patch complete mask partial begin partial end template comment buggy line total case statements.
codebert is able to generate the appropriate case since other case statements use similar identifier formats n r .
the outcome of the case also follows similarity to nearby context by adding block sb.append break .
traditional apr tools cannot fix this bug since it requires adding a new semantic line into the program which is beyond the ability of traditional apr tools built for modifying existing lines or inserting simple statements try catch null pointer checker etc .
learning based apr tools also struggle with generating the correct patch for this bug since the added line does not fit a common edit pattern found in the training dataset.
by observing the surrounding context and using previously seen examples repeating case statements in other projects alpharepair can generate the correct fix for this bug.
these examples combined with the new state of the art results achieved show that alpharepair opens up a new promising direction for apr.
.
.
not perfect fault localization.
we also compare against stateof the art tools without perfect fault localization.
table shows the performance of alpharepair with other techniques also evaluated under this setting.
alpharepair is able to produce correct patches which outperforms previous state of the art tools.
additionally alpharepair is able to correctly fix unique bugs the highest among all studied techniques that cannot be fixed by any other technique.
for not perfect fault localization since we do not have access to the ground truth location of the bug alpharepair generates patches for multiple suspicious lines.
to account for this we lower the beam width of alpharepair for this setting in order to generate fewer patches per suspicious line.
in this experiment we show that even with the reduced number of patches generated per suspicious line alpharepair can still achieve state of the art results.
.
rq2 ablation study to study the contribution of adding different components in the design of alpharepair we conduct an ablation study.
table contains the result with each row representing one component and the increase in number of correct plausible patches alpharepair can produce.
to show how each mask generation strategy section .
967esec fse november singapore singapore chunqiu steven xia and lingming zhang figure patch ranking of correct fixes with and without using patch re ranking lower is better improves the number of bugs fixed we start with the most basic strategy and iteratively add more complex masking strategies.
to begin with we only use complete mask where the entire buggy line is replaced with all mask tokens.
this is the case where we give codebert the entire freedom to generate any variety of edits.
however this is often not desirable as the search space grows exponentially with the number of mask tokens and it becomes hard for codebert to obtain a correct fix.
we observe that we only achieve correct patches when solely using this mask generation strategy.
we obtain increases in correct patches generated as we start to use more mask generation strategies.
the highest increase in performance is the usage of template mask lines which add an additional new fixes.
compared to complete mask template mask only masks out certain parts of the buggy line parameter boolean expression function calls .
this allows codebert to fill out only a small number of mask tokens which limits the search space and allows alpharepair to quickly find the correct patch.
in addition we also see an increase in performance when we add the encoding for the commented version of the buggy line as input to codebert.
the buggy line itself is important for patch generation since it contains important information such as specific variables used and the type of line.
this demonstrates that alpharepair is able to make use of the buggy line to help guide the generation of valid fixes.
combining all components in alpharepair we are able to achieve the final number of correct patches generated.
after the patch generation process alpharepair re queries codebert again to generate more accurate ranking of each patch.
to evaluate the effectiveness of our patch ranking strategy we compare the order of the correct patches with and without re ranking.
figure shows the patch ranking of all correct patches generated with and without re ranking dotted line represents the average patch ranking for each strategy .
we observe that on average the correct patch is ranked 612th without using the re ranking strategy.
when using re ranking the correct patch on average is ranked 418th .
reduction .
furthermore out of correct patches are ranked higher after re ranking compared to before.
as mentioned in section .
the temp joint score no re ranking is not an accurate representation of the actual likelihood of the generated tokens since it is conditioned on mask tokens where the concrete values are not yet determined.
by re ranking the patches generated we make sure that the joint score is calculated without any mask tokens providing an accurate likelihood calculation.
this demonstrates that the patch re ranking process in alpharepair can effectively order the patches and prioritize patches that are ranked higher in case that only a subset of generated patches can be validated.table baseline comparisons on defects4j .
projects alpharepair recoder tbar cli codec collections compress csv gson jacksoncore jacksondatabind jacksonxml jsoup jxpath total correct plausible html.attributes .put attribute else if stringutil.in name base basefont bgsound command link meta noframes style title else if stringutil.in name base basefont bgsound script command link meta noframes style title return tb.process t inhead else if name.equals body mask type template mask add parameter mask line else if stringutil.in name base basefont bgsound mask ... mask command link meta noframes style title bug id jsoup figure example bug fix in defects4j .
table baseline comparisons on quixbugs tool alpharepair cure deepdebug recoder coconut java python .
rq3 generalizability of alpharepair .
.
defects4j .
.
to demonstrate the generalizability on additional projects and bugs and confirm that alpharepair is not simply overfitting to bugs in defects4j .
we evaluate alpharepair on the single line bugs in defects4j .
dataset.
table shows the results compared against other baselines on defects4j .
.
we observe alpharepair is able to achieve the highest number of correct patches of .3x more than top baseline .
defects4j .
contains a harder set of projects for apr with different variety of fixes compare to defects4j .
.
we observe that while template based tools such as tbar was able to generate a high amount of correct patches for defects4j .
the number of correct patches it can generate for defects4j .
is limited.
learning based tools such as recoder will also suffer from 968less training more repairing please esec fse november singapore singapore moving to a harder evaluation dataset since the edits are learnt from training datasets which might not be present in defects4j .
.
in contrast alpharepair does not use any fine tuning on specific bug datasets which makes it less prone to suffer from generalizability issues of traditional template based or learning based tools.
figure shows an example of a bug from defects4j .
dataset that only alpharepair can fix.
in this example the code checks if the variable name is one of the string literals.
the bug is caused by missing a string literal of script .
this bug is particularly hard to fix for both traditional and learning based apr tools.
for traditional tools such as template based ones designing this specific pattern can be hard as it requires insertion of a seemingly arbitrary literal of script .
for learning based tools it faces the similar problem in lack of example bug fix pairs where the fix is to insert this particular string literal.
in order to generate a correct fix of this bug one must understand the semantic meaning of the code.
upon further inspection the string literals in this conditional statement are all html tags.
alpharepair can generate the valid html string literal of script by understanding that the surrounding context deals with html documents and tags.
additionally we observe other patches generated by alpharepair for this bug include other valid html tags such as head html font etc.
the specific example and improvement in repair effectiveness over the baselines demonstrate the generalizability of alpharepair.
.
.
quixbugs.
we show the multilingual repair capability of alpharepair by evaluating on the quixbugs dataset which contains both java and python versions of buggy programs.
table shows the results against state of the art java and python apr tools.
we observe that alpharepair is able to achieve the highest number of correct patches in both java and python and .
we also observe that alpharepair is the only tool out of the baselines that can be directly used for multilingual repair coconut trains separate models .
traditional learning based tools require access to bug fixing datasets which are often only in one programming language restricting the ability for them to be used in a multilingual setting.
unlike traditional learning based apr tools codebert is jointly trained on java python go php javascript and ruby code snippets this allows alpharepair to be directly used for multilingual repair tasks with minimal modifications.
threats to validity internal one internal threat to validity comes from our manual analysis on the correctness of the patches.
to this end the authors carefully looked through all plausible patches and had detailed discussions in order to determine if a patch is correct.
we have also released all correct patches for public evaluation along with the code to reproduce our experiments .
another internal threat is the direct usage of the codebert model.
the evaluation benchmark of defects4j could overlap with the training data used in codebert which consists of over million code functions.
to address this we calculated the number of fixed functions in defects4j that are in the codebert training dataset.
overall there are out of .
defects4j .
bugs and out of .
defects4j .
bugs that are present in the original training data.
out of the and bugs that alpharepair can correctly fix in defects4j .
and .
and .
and .
bugs have their corresponding developer patch in the codebert training data.
for the bugs we manually perturb the buggy code change variable names add empty while if statements etc and use the perturbed version for repair.
we observe that alpharepair is still able to generate the correct fixes for all bugs.
we believe this adequately shows that alpharepair is not simply overfitting to patches that are present in the original codebert training dataset.
furthermore the overall comparison results if we were to exclude the overlapping bug fixes would still improve on state of the art baselines vs on best baseline in defects4j .
and vs on best baseline in defects4j .
.
note quixbugs dataset is not part of the codebert training data.
future work to address this even more is to retrain the entire codebert model by taking out all patched functions in original data and then re evaluate alpharepair.
additionally another internal threat is the experimental setup causing potential differences in results.
for example a longer timeout threshold or faster machine can lead to more bug fixes.
to this end we adopt an ordinary machine configuration detailed in section .
and follow prior learning based apr tools by setting a hour end to end timeout for fixing each bug.
furthermore we follow the common practice in apr by directly taking bug fix results from previous studies instead of directly running the apr tools.
to completely address this threat one would need rerun the results from all the selected baselines apr tools on the same machine with the same time out threshold.
external the main external threat to validity comes from the evaluation benchmarks we chose.
our claims on the performance of alpharepair may not translate to other datasets.
to address this threat we evaluate the generalizability of alpharepair on a newer dataset defects4j .
.
we also evaluate our claim on the generalization to other programming languages by studying alpharepair on both the python and java versions of quixbugs.
conclusion we propose and implement alpharepair the first cloze style apr technique that leverages large pre trained code model directly for repair under a zero shot learning setting.
this opens a new dimension for multilingual learning based apr that does not require any fine tuning on repair datasets.
we build alpharepair using codebert and design inputs to make use of the pre training objective of codebert to directly generate fix lines from the surrounding context.
we evaluate alpharepair on popular java benchmarks of defects4j and quixbugs to show that alpharepair achieves new state of the art with the highest improvement being .3x more bugs fixed than best baseline in defects4j .
.
we further demonstrate the multilingual ability of alpharepair on the python version of quixbugs where we achieved similar results compared to java.