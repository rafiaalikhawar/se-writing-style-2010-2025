arclin automated api mention resolution for unformatted texts yintong huo the chinese university of hong kong hong kong china ythuo cse.cuhk.edu.hkyuxin su school of software engineering sun yat sen university zhuhai china suyx35 mail.sysu.edu.cn hongming zhang the hong kong university of science and technology hong kong china hzhangal cse.ust.hkmichael r. lyu the chinese university of hong kong hong kong china lyu cse.cuhk.edu.hk abstract onlinetechnicalforums e.g.
stackoverflow arepopularplatforms for developers to discuss technical problems such as how to use aspecificapplicationprogramminginterface api howtosolve the programming tasks or how to fix bugs in their code.
these discussionscanoftenprovideauxiliaryknowledgeofhowtouse the software that is not covered by the official documents.
the automaticextractionofsuchknowledgemaysupportasetofdownstreamtaskslikeapisearchingorindexing.however unlikeofficial documentation written by experts discussions in open forums aremade by regular developers who write in short and informal texts including spelling errors or abbreviations.
there are three major challengesfortheaccurateapisrecognitionandlinkingmentioned apis from unstructured natural language documents to an entry in the api repository distinguishing api mentions from common words identifyingapi mentionswithoutafully qualifiedname and disambiguatingapimentionswithsimilarmethodnames butinadifferentlibrary.inthispaper totacklethesechallenges we propose an arclin tool which can effectively distinguish and linkapiswithoutusinghumanannotations.specifically wefirst design an api recognizer to automatically extract api mentionsfrom natural language sentences by a conditional random field crf on the top of a bi directional long short term memory bilstm module then we apply a context aware scoring mechanism to computethe mention entry similarityfor eachentry in anapi repository.
compared to previous approaches with heuristic rules ourproposedtoolwithoutmanualinspectionoutperformsby8 inahigh qualitydatasetpy mention whichcontains558mentionsand sentences from five popular python libraries.
to our bestknowledge arclinisthefirstapproachtoachievefullautomation corresponding author suyx35 mail.sysu.edu.cn .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa association for computing machinery.
acm isbn ... .
collected labels.
keywords api api disambiguation text mining acm reference format yintonghuo yuxinsu hongmingzhang andmichaelr.lyu.
.arclin automatedapimentionresolutionforunformattedtexts.in 44th international conference on software engineering icse may pittsburgh pa usa.
acm new york ny usa 12pages.https introduction applicationprogramminginterface api isanessentialcomponent for programming.
developers use apis to interact with a programminglanguageorasoftwarelibrary.
however as alibrarycontains thousandsofapis e.g.
pytorchv1.8hasover2 400apis andthere are hundreds of popular libraries in a language it is impossible for developerstobefamiliarwithallapis.therefore developersare used to discussing programming related questions in the online technical forum when they face troubles in programming tasks.
oneofthe mostpopularforums stackoverflow containsover20 million questions and million users1.
it motivates researchers toexplorehowtoidentifyknowledgeinopenforumstoassistdevelopersinmanyaspects suchasapirecommendation api misuse detection and document augmentation .
thefoundationoftheabovetasksisrecognizingandidentifying api mentions from an unstructured natural language.
conventionally researcherstriedtouserule basedmethodstosolvethetask.
forexample bacchellietal .
treudeandrobillard identified apielementsintextsbyasetofregularexpressions.huangetal .
choseahyperlinkineachstackoverflowpostandusedregular expressions to detect api entities.
they also analyzed whether the text in html code tag can match the api names in the api repositories.lietal .
detectedapisbycheckingwhetherthe tokenofasentencecanmatchorpartiallymatchthenameofan api by conducting minor modifications.
ren et al .
kept api mentions only in html code elements.
however theserule basedmethodsdonotconsidertheshortand informal nature of forum discussions falling short in mining apis 1the data dump is retrieved in september 1st .
ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa yintong huo yuxin su hongming zhang and michael r. lyu in certain scenario.
typically a forum may contain a large number of unprofessional developers with different technical backgrounds who share the knowledge and information in their own writing styles.asaresult theapi mentionscould bein differentformats.
for example previous study concluded from stackoverflow poststhat47 oftheapielementsarenotincludedwiththehtml code tag.suchinconsistencycausesdifferentkindsofambiguity when we recognize and identify apis.
in this paper we categorize these ambiguities into the following three types.
thefirstoneis common wordambiguity referringtotheambiguitybetweencommonwordsandapimentions .traditionally apinameiscomposedofpunctuations brackets anduppercase letters however sometimesdevelopersonlywritetheapi smethod name in their answers causing the difficulty of distinguishing it fromcommonwords.thefirstgroupinthetable 1illustratesexamplesofthisproblem.eveniftwosentencesareallmentioningthe wordview thefirstsentenceuse viewtorefertotheapi torch.view whereas the second one use viewas a common verb.
regular expressionsfailindiscriminatingsuchapimentionswithcommon words.previouswork revealedthat35.
ofthetoken apply in stackoverflow posts tagged with pandas actually referred to an api mention.
thesecondoneis morphologicalambiguity whichisbecausedevelopers rarely write down the full api name that can be perfectly matchedwithanapinameinthelibrary.researchonstackoverflow concludes that morphological mentions which include abbreviations synonyms andmisspellings arequiteoftenininformal discussions.
fourexamples in the secondgroup of table 1demonstratethemorphologicalvariations.inthefirstthreesentences the apinumpy.reshape wasmentionedbyreplacing numpywithits abbreviation np omittinglibraryname andusingthecustomized variable name respectively.
the fourth sentence talks about the torch.nn.conv2d andtorch.nn.conv3d apis butincludesneitherthe library module class name nor the correct case i.e.
use conv2d instead of conv2d .
the third type is reference ambiguity which happens if the api lists contain various third party libraries.
the third group in table1provides two instances of this problem.
even if both pytorch library and tensorflow library contain the api method flatten we could characterize what the mentions refer to based on their sentence contexts i.e.
the first sentence mentions keras module whereas the second one mentions pytorch .
it is often the case that developers do not explicitly point out the specific library in their mentions but such information can be derived from other words in the context.
dueto theabove ambiguities traditional informationretrieval techniques cannot be effectively employed.
dagenais and robillard appliedasetoffilteringheuristicstotacklethesecondchallenge buttheyfailedinresolvingcommonwordsambiguityduetothe shortcoming of regular expressions.
the above challenges become more difficult if we apply the api mining task into unformattedsentences.
such free text does not contain any code tags so detecting api in this scenario is even harder.
however it is a nonnegligibleproblem since inotherscenarios e.g.
emails wecannot use html tags.
to make our research applicable for a broaderapplication we focus on mining apis from free text.
althoughthe most recent work claimed to distinguish api mentionsfromcommonwords theystored code tagsandcodesnippetsin pre code tagsfromstackoverflowposts insteadofmining fromfreetexts.thus theirapproachcannotbeextendedtogeneral scenarios.
inthispaper toovercometheaforementionedambiguitychallenges we propose a new api mining approach named arclin apirecognition and contextual linking which recognizes and identifies api mentions from natural language descriptions to a set of apis without any human annotated labels or handcrafted rules.
ourmodelismadeupofanapirecognizerthatfindsapimentionsinfreetexts andacontextualapilinkerthatlinksapimentionstothecorrectapitheyreferto.specifically ourapirecognizerextensively deals with the first common word ambiguity by considering the contextinformation insentence level aroundan apimention.
for the words that are predicted to be an api mention a librarypredictor inside the api linker predicts the related library to the sentence restricting arclin to link apis in the predicted library which resolves the reference ambiguity.
the similarity function in the api linker compares api mention with every entry in the api repository considering both spelling similarity and lexical similarity sominormorphologicalchangeswillnotaffectthelinking result.
to the best of our knowledge arclin is the first approach that can automatically cope with these challenges above.
considering the numerous number of apis in the real world it isimpracticaltoaskannotatorstolabelsuchalargescaleofdata.
to avoid this labor intensive process we design arclin to be free fromanyhumanannotationinthetrainingprocessbyexploiting naturallabelsinthetrainingset.unlikehuman labeleddata the automatedlabelsmaycontainerrors butourapilinkerinthenext step provides a strict selection to address this problem.
to evaluate theeffectivenessofarclin weannotateatestset whichcontains 948sentenceswith563mentionsunderfivepopularthird party libraries.
on average arclin achieves .
.
and .
inprecision recallandf1score respectively.thepromisingresultsindicate that even though our approach does not need any human annotatedlabels itoutperformsthecurrentstate of the artbaseline trained with labeled data.
to sum up the main contributions of this paper are threefold to our best knowledge we are the first to design an unlabeled approach focusing on api recognizing and linking in unformatted text corpora.
webuildanapicontextuallinker makingthemodelautomaticallylinkapimentionsto anapirepository takingthe sentence context into account.
theexperimentresultsshowarclincandiscovertraceabilitylinksbetweenapisandtherepositorymoreaccurately comparingwithstate of the artbaselinemodels.thecode and dataset are released2.
problem statement in this section we first introduce the main concepts used in this paperinsection .1andthenprovideaformaldefinitionofthetask in section .
.
2please find the resources in authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
arclin automated api mention resolution for unformatted texts icse may pittsburgh pa usa table1 threemainchallengesforapimininginunformattedtexts bluewordsreferstoapimentionsand redwordsrefers to common words.
question id sentence api so far i managed to use viewonce in my first very simple project... torch.view you can use .numpy to viewthe internal data... none simply reshaping the by np.reshape data would not work.
numpy.reshape the.reshape method of ndarray returns the reshaped array.
numpy.reshape i have tried a.reshape in for the numpy array but nothing is producing what i want.
numpy.reshape i would like to understand the difference between conv2dandconv3din pytorch.
torch.nn.conv2d torch.nn.conv3d i want to use the keras layer flatten or reshape at the end of my model... tensorflow.keras.layers.flatten but in pytorch flatten is an operation on the tensor.
torch.tensor.flatten figure a screenshot of one stackoverflow post.
.
terminology api mining is the task of recognizing api mentions from free texts and linking the recognized api mentions to the corresponding api repositories.
figure 1is a screenshot of a stackoverflow post3 w e use this screenshot to illustrate the concepts used in this paper.
here anapicouldbeaclassname amethodname oranattribute of a class.
the term free text also called unformatted text refers to thetextwithoutanyhtmltags e.g.
code .orangedashbox in figure 1shows an example of code usage.
an api mention in textsisatokenappearinginthefreetextthatreferstoaspecificapi intherepository.blueboxinthefigureshowstwoapimentions i.e.
nn.linear andnn.module .anapirepository isacollection 3the entire page is in 4nn.modulelist in orange box is also an api mention after removing the code tag.of all entire qualified api names.
entire qualified name is the exact api sname shown in its officialwebsite e.g.
torch.nn.linear torch.nn.module .
each api s name in this repository is called an entry.
an entry is composed of sub fields splitted by .
which are called entities the entities of nn.linear andtorch.nn.linear are shown in nearby red circles.
.
task description given a natural language sentence sin free text and an api repositoryd d1 d2 ... d n wheredireferstoanentryintherepository theapi miningtaskistolink apimentionstoanentryinthe api repository.
thetask involves two phases recognizing api mentions in the sentence linking api mentions to the correspondingentriesintheapirepository.inpractice wefirsttokenize sinto a token list then i n w e determine whether token irefers to the element dj d. approach in this section we introduce our approach including data preparation anapirecognizer andacontextualapilinker.figure 2shows theoverallframeworkofarclin.tobeginwith sentencesarefed intotheapirecognizertouncoverapimentions.specifically acontext encoder is applied to acquire contextual embeddings of tokens byabidirectionallong shorttermmemory lstm network then these representations are decoded via a conditional random field crf .
the tokenswhich decoded asapi mentions are sent tothe apilinker.next anapilinkerisdesignedfordiscoveringthemost possiblematchedentryintherepository.todoso wefirstgenerate a series of candidates by heuristic rules then a library predictornarrows down the candidates by specifying a library.
after that we use anintegrated scoringfunction to rank mention entry pairs.finally thecandidatewiththehighestsimilarityabovethe threshold will be chosen as a link.
.
data preparation .
.
text corpus.
given some libraries we crawl all questions taggedwithatleastoneofthegivenlibrariesfromanonlinetechni calforum.besidesquestionsandanswers zhangetal .
revealed thatthemajorityofcommentswerealsoinformativeastheyprovided a supplementary view to the answer.
therefore for each question answeringthread wecrawlthequestion allanswers and their comments.
we discard code snippets in pre code but authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa yintong huo yuxin su hongming zhang and michael r. lyu figure the framework of arclin.
keep contents in code when it appears in a natural language sentence.
stackoverflow users highlight api mentions in a natural language sentence by code tags.
however tabassum et al .
shows that of the code mentions are not indicated with this tag.
if we only rely on the tags to do the mention detection we will miss a large number of mentions.
moreover it is observed that contentsincodetagscanbenoisy manynon codeelements e.g.
variablesname keypoints orusername arealsohighlightedby codetags .toaddressthisnoisinessissue aswellastogeneralizetootherscenariosthatcannotuse code tags e.g.
emails we removeall code tagsinsentencesandthemarkdownmarkersto makethis taskmoresimilarto therealapplication.
aftercollecting the data we tokenize all sentences with the nltk sentence parser.
as a result we obtain a set of parsed sentences in free text.
.
.
repository construction.
foreachgivenlibrary wecrawlall apiswiththeirentirequalifiednamesfromofficialdocumentations.
for instance the apis in the pytorch library include methods e.g.
torch.tensor.dim functions e.g.
torch.nn.functional.avg pool1d classes e.g.
torch.nn.adaptiveavgpool1d and attributes such as torch.backends.cudnn.enabled.
the api repository is made up of all crawled apis names.
.
.
tokenizer.
weadaptasoftware specifictokenizerusedinye et al.
and ye et al .
which preserves the integrity of an api mention.
current popular tokenizers such as spacy stanford parser and nltk all parse numpy.shape into a token listof butthedeployedsoftware specific tokenizer will treat numpy.shape as a single token.
.
.
inverse document frequency idf .
idf is a way to measure theimportanceofawordinacorpus.aword sidfisdispropor tionate to the word s frequency.
given the assumption that if a word frequently occurs in a document it may contain relativelyless information the formula for computing idf for a word wis shown in equation idf w log documents number document with w inthispaper wecomputetwotypesofidf idf tokenandidf entity.
we useidf tokento measure the token s importance in a corpus whereweregardeachsentenceasadocument.for idf entity w e computetheentity simportanceintherepository.weconsidereach entry is a document and its entities are words.
for example the document numpy.reshape hastwowords numpy and reshape .
intuitively since all numpy apis contain the entity numpy its idf value is relatively low.
.
recognizer the first step of mining apis from the sentences is extracting api mentions without specifying which apis they refer to.
at this step weproposeanautomaticapimentionrecognizerthatprefersrecall over precision to cover as many api mentions as possible.
we first introduceanautomaticapproachtominenatural butnoisy labels thenelaborateonarchitecturesoftherecognizerinthefollowing subsections.
.
.
automatic labeling.
traditionalmachinelearningapproaches label a large scale training set to train classification models for the task.
however considering that there are enormous apis even inoneprogramminglanguage itisinfeasibletoobtainsufficient human labeled data for all of them.
there is a need to devise an algorithm that escapes from any human annotation.
previousstudies showthatpriorexternalknowledge i.e.
api repository was critical for good performance in identifying namedentityinasentence.motivatedbythis weusethefollowingcriteria i.e.
domainknowledge toautomaticallyannotatepotential api mentions authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
arclin automated api mention resolution for unformatted texts icse may pittsburgh pa usa ifatokenisexactlytheentirequalifiedname i.e.
sameas anentryinapirepository weregardthetokenasanapi mention.
inspiredfromthatusersusuallyuse attheendofatoken torepresentanapimethodnameorfunctionname wetreat the token as an api mention if the token contains .
users also use .
e.g.
numpy.shape o r x.shape when they mention an api thus we consider the token is an api mention if it contains .
.
to distinguish such mentions from emoticon or punctuation we require the token to consist of more than three characters.
moreover toaddressthecommon wordpolysemyproblemintroducedinsection weemployadataaugmentationtechnique for each sentence with at least one api mention being detected.
specifically we randomly replace the originally detected api mention with a new one only containing the last part of the name e.g.
x.viewwill be replaced with view .
this data augmentation process forces the recognizer to learn contextual information of an api mention.
the self labeling process inevitably introduces some noisy labels.
for instance even if the token python2.
consists .
it is not an api mention besides that a missing space between two sentences e.g.
... plot ellipses on a single graph.if you do ... will generate the wrong label for the token graph.if .
however theproposedcontextuallinkerisabletomitigatethesenoisylabels.
after automatic labeling we feed the self labeled data as well as the augmented ones into our context encoder.
.
.
context encoder.
contextencoderisresponsibleforacquiringcontextualwordembeddingsinasentence.thelongshort term memory lstm network has shown promising results in sequential labeling tasks due to its strong ability to capture longdistancecontextinformation.thememoryunitinlstmenables it to generate the representation based on both the short distance and long distance context.
in this work we design an lstm networkto achieve thegoal.
abi directional lstm bi lstm is specificallyusedforpreservingbothpastandfutureinformation within a sentence.
thearchitectureoftheconstructedencoderisshowninfigure where two granular level features are considered.
by so thebi lstm encodersimultaneouslygrasps word level semantics and character level details.
firstly word embedding techniques are used to extract word level semantics.
word embedding represents words as distributed vectors in a low dimensional space so that words with similar semantic or syntactic meaning tend to be close in their vector space.
assuming that words present in a similar contexthavesimilarmeanings thecommonapproachskip gram word2vec learnswordembeddingsbypredictingsurrounding words given the central word.
similar to previous research we train domain specific word embeddings by skip gram on a domain corpus.
secondly as previous study has shown that character level representation is crucial to extract morphological evidence we use this feature to alleviate the second morphological problem mentionedinsection .besides sincedeveloperswriteapimentions with customized variable names under different scenarios deployingcharacter levelembeddingallowsustocopewithunseenwords namedtheout of vocabulary oov problem.inparticular weelicitcharacter levelfeaturesfromthearchitectureshowninblue dotted rectanglesinfigure whichincorporatesonemaxpoolinglayer after a convolutional neural network cnn is applied.
.
.
tag decoder.
given contextual word representations in a sentence thetagdecoderisusedtodeterminewhethertheword is an api mention or just a common word.
inspired by previoussequence labeling works in the natural language processing domain we adopt a conditional random field crf to conductthe tag decoder on top of the text encoder i.e.
bi lstm layer .
byaccuratelyobtainingstructuraldependenciesamongadjacent words in a sentence the crf module jointly predicts the tag of each word sequentially instead of predicting tags independently.
inordertobalancebetweenapimentioncoverageandprecision in predictions we select top ppaths with the highest confidence scoreastheresultofthecrflayer.ifthetokenin k k p paths is predicted as an api mention we treat the token as an api mention and feed it to the contextual linker.
.
contextual linker once we obtain the api mentions in the text arclin links the correctapimentionstoanentryintherepository.thecoreidea behindthislinkerisaseriesofdisambiguationmethods.specifically wefirstlyselectentriesascandidatesintherepository thenrank thesimilarityscoreofevery mention entry pairwiththehelp of the mention s context information.
although the predicted api mentionsmaycontainerrors thewrongmentionwillbehardto find an entry with a high similarity score.
from this aspect the noise introduced by the last step will not affect the final results.
.
.
candidate selection.
to reduce the time complexity of comparingallentriesintherepositorywiththeapimention wenarrow the scope by listing a set of candidates.
inspired by the fact that even though humans can make errors in spelling words such misspelling is hardly seen at the beginning of the word.
so do the developers.
given a mention we directly compare its last part i.e.
lastentity andthelastpartoftheentriesintherepository.ifthe firsttwocharactersofthelastentitiesarecase insensitivematching we add the entry to a candidate list.
.
.
library predictor.
as the third challenge aforementioned similar api entries in different libraries bring difficulties to disambiguate the mention.
an intuitive way is to take sentence level semanticsintoconsideration.tocapturerichcontextualinformationfromsentences wefirsttrainthemostpopularlanguagemodelbert withalltrainingsentencesforeachlibrary.then onefully connectedlayerfollowedbyasoft maxoutputlayerisfine tuned to predict the library of input sentences based on the semantic embedding produced by bert.
.
.
similarity computation.
given an api mention mand its candidates e we calculate the similarity score between the api mention and each candidate.
finally we rank all candidates based ontheirsimilarityandselectthemostrelevantcandidateabovethe threshold.
basically we compute similarity based on bag similarity.
given two bags of entities m eibeing split by .
from the api authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa yintong huo yuxin su hongming zhang and michael r. lyu mentionmand a candidate ei e respectively we compute the similarity from two aspects lexical similarity and entity similarity.
lexicalsimilarity.
thisstepismotivatedbythefactthatsometimesdevelopersmakespellingerrorsinsentences especiallywhen the mentioned api name is long.
however even if we make a typo insomewords itslexicalmeaning i.e.
wordrepresentationlearned from corpus will not change.
inspired by huang et al .
we use the equation 2to calculate lexical based similarity between mention entities mand entities of one candidate ei.
sim l m ei summationtext.
w msim w e i idf token w summationtext.
w midf token w whereidf token w represents the idf value of token win the trainingdata.
sim w e i referstothemaximumlexicalsimilarity score between the element w mand elements in set ei.w e calculate lexical similarity for pairs of entities by another word embeddingmodelfasttext .unlikeword2vec fasttextisthe embeddingmodelthatincorporatesn gramfeaturesofatoken soit solves the oov problem.
inversely we also compute the similarity sim l ei m by exchanging mandeiin equation .
in the end theoveralllexicalsimilarityisformulatedthroughanarithmetic mean operation sim l m e i sim l m ei sim l ei m .
entitysimilarity.
jaccardsimilaritycoefficient iswidely used in gauging how similar the two sets are.
given two bags of entitiesm ei we formulate our weighted jaccard similarity as equation sim j m e i summationtext.
w m ei idf entity w summationtext.
w eiidf entity w whereidf entity w represents the idf value of entity win the api repository.
idf provides a standard to measure the salience ofatoken.a higheridfvaluerepresentsthatitappearsmorefrequently carryinglowerinformationentropy.forinstance inour repository tokenssuchas nn torch numpycontainalowidfvalue sinceit isalmostpresent ineveryentry buttokens suchas adaptivemaxpool1d andbinary cross entropy deserve more attention thusahighidfvalue.intuitively insteadofclassormodulenames we always use method names to clarify the mentioned api which contains a higher idf value.
such discriminative tokens contribute significantlytothis sim jfunction whilemissingamatchin nnjust makes a minor effect on the entity similarity score.
overall similarity.
tosumup thescoringfunctionforcalculating similarity is composed of a lexical similarity function and an entitysimilarityfunction.givenanapimention mandanentry ei the overall similarity is calculated by equation sim m e i sim l m e i sim j m e i wheresim l m e i andsim j m e i are defined above.
toexclude theapimentionsthatarewrongpredictionsintroducedfromthe recognizer and the api mentions that refer to an api out of our repository weeliminatethecandidates ei ewithlower sim m e i valuethanthesimilaritythreshold s.finally werankallremaining candidates and choose the ej ewith the highest sim m e j as output.table statistics of api repository and py mention set.
library version api mention sentence pytorch .
.
tensorflow .
.
pandas .
.
numpy .
matplotlib .
.
sum table statistics of training set.
library sentence autolabel augmentation pytorch tensorflow pandas numpy matplotlib sum experimental setup in this section we introduce the experimental setup details including data collection implementation details and evaluation metrics.
.
data collection .
.
text preparation.
inthispaper wefocusonfivewidely used third partylibrariesinpython pytorch pandas tensorflow numpy matplotlib.wecrawlallquestionstaggedwithatleastoneofthe abovelibrariesusingscrapyinstackoverflow.for eachquestionanswer thread we collect questions all answers and their comments.detailsofthedatapreprocessingmethodaredescribedin section3.
.
.
.
.
api repository.
weconstructanapirepositorycontaining allapiinfivechosenthird partylibrarieswiththeirentirequalified names.
we use scrapy to crawl all apis from their official websites.
informationsuchastheversionofeachlibrary thenumberofapisineachlibraryislistedintable .consideringthatparentheses arenotthesigntodifferapisfromeachother weremoveall at the end of the api entire qualified names e.g.
store numpy.einsum instead of numpy.einsum .
.
.
dataset.
consideringalltextscrawledfromtextpreparation are too large to cope with we randomly sample sentences foreachofthelibrariesandtreatthemasunlabeledtrainingdata.af terapplyingself labelinganddataaugmentation weobtain125 sentences for training the recognizer.
the distributions of training data automaticallyapilabels andaugmentationresultsareshown in table3.
for the testing data we randomly select sentences from eachlibrary withoutoverlappingwiththetrainingdata andask expertstoannotatethem.toensureannotationquality twoinvited experts both have more than four years of experience in pythondevelopment and are all familiar with five libraries.
consideringthat a long sentence is more likely to contain api mentions weselect the testing data sentences longer than ten tokens.
during authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
arclin automated api mention resolution for unformatted texts icse may pittsburgh pa usa the annotation given the whole api repository experts are asked to annotate whether each token in a sentence is referring to an api in the repository or not.
if yes they need to write down the entirequalifiednameofanapimention.wealsoaskannotatorsto throw away the sentence if they are not confident at what it refers to.inthisway wecollect2 830sentenceswith558apimentions from five libraries in total where their distributions along with the repository s distribution are shown in table .
typical examples are below if you don t want to export please uncomment plt.show and remove ... i veusuallygottengoodperformanceoutofnumpy s einsum function and i like ... here is a way to do it using stack orunbind .
here black italic fonts indicates api mentions and blue italic fontsin brackets are the linked apis in the repository with entire qualified names .
.
implementation details inthedatapreprocessingperiod wetrainaskip gramword2vec model based on our corpus with gensim .
we also train a fasttext word embedding model with gensim for computing mention entry pair lexical based similarity.
the embedding size for word2vec and fasttext models are set to .
two models are trained for ten epochs5.idf tokenandidf entityare trained on the training data.
for the api recognition part we use an open source natural sequencelabelingtoolfrom astheimplementationandtrain therecognizerontheaugmenteddata.thecharacterembedding size is set to and the layer number of bi lstm is set to one.
we traintherecognizerwiththelearningrateas0.001forfiveiterations.
we choose five paths with the highest confidence score in the crf layer andtreatatokenasanapimentionifandonlyifitispredicted so in at least two out of five paths top p k .
for the api linker we train our library predictor with transformer for ten iterationswiththelearningrateof0.
.thedefaultthreshold s for the scoring function is .
unless we specify them with other values.
.
baselines to the best of our knowledge there is no existing work focusing onextractingapilinksfromunformattedtexts.wecompareour method with the following baselines apireal is the most relevant work to ours but they mine apis from stackoverflow posts and the other two baselines are rule based.
.
.
apireal.
ye et al.
proposed the model named apireal which predicted api recognition and linking in a stackoverflow post.apirealcontainstwostagessimilartoours arecognizerto extract api mentions and a linker to link api to the repository.
in therecognizer theymanuallylabeledthetrainingdatatolearnapi mentions by feeding human crafted features into a crf model.
in the linker they utilized external information such as the question 5word2vec and fasttext models converge before ten epochs.title contentsinthecodeblock code tags and urlsinapost to predict what an api mention links to.
whenimplementingthisbaseline we counterfeit afilecrawled from stackoverflow in the same input format where each line is a sentencefromourtestset.inthisway apirealwilltreatourfile as a post from stackoverflow and continuously processes them.
moreover asthedatabaseofapirealincludesthreeoffivelibraries comparingtoours i.e.
pandas numpy matplotlib wecompute precision recall and f1 scores on the three libraries.
.
.
rulebase pure.
we also include a pure rule based approach as the baseline.
specifically we check whether each token in the sentenceisthesameasanentryintheapirepository.thisbaseline provides us with insights into the quality of written api mentions in stackoverflow.
.
.
rulebase knowl.
we also include a rule based approach withpriorknowledgeasabaseline.here priorknowledgerefersto the common writing behaviors for api mentions in stackoverflow.
specifically wereplace np with numpy pd with pandas tf with tensorflow for each token respectively.
.
evaluation metrics for fair comparison we use precision recall and f1scores to evaluatearclin sperformanceinourtestset whichisalsousedby all previous works .
specifically precision means what percentage of api linking predictions are correct recall means what percentage of the real api mentions are covered and f1 is the harmonic mean of precision and recall.
experimental results inthissection wediscusstheperformanceofarclinmodelby divingintothreeresearchquestionsfromsection .1tosection .
howeffectiveisarclin?
wecomparearclintothree baselinesintheproposedtestset.theresultshowsthatarclin outperformsbaselinesbylargemargins eventhoughitisfreefrom any labor intensive annotations and handcrafted rules.
how effective are the components of arclin?
the devised framework is made up of an api recognizer and an api linker.
the latter one includes a library predictor and a scoring functionbalancebetweenthelexicalsimilarityandentitysimilarity.
toevaluatethecontributionofeachcomponent wediscardeach element at one time and implementthe remaining part in our test set.
details of analysis are provided along with the experiment results.
what is the generalization ability of arclin?
considering the large number of libraries in the real world we are interestedinhowarclinperformsinminingapisinsideanunseenlibrary.toexploreitsgeneralizationability wetrainthemodel in one library and test it in another library.
.
rq1 how effective is arclin?
arclin aims to automatically extract api mentions from free text sentences and link them to an entry in the repository.
thus to prove its effectiveness we evaluate arclin in sentences selected fromstackoverflowposts.wefeedtestsentencesintothearclin model and examine whether it could mine correct apis.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa yintong huo yuxin su hongming zhang and michael r. lyu table experimental results.
approach precision recall f1 rulebase pure .
.
.
rulebase knowl .
.
.
apireal .
.
.
w o rules .
.
.
arclin ensemble .
.
.
pytorch .
.
.
tensorflow .
.
.
pandas .
.
.
numpy .
.
.
matplotlib .
.
.
to answer this question we compare arclin with a current state of the artbaselinenamedapireal apurelyrule based approachrulebased pure andarule basedapproachincorporating priorknowledgenamedrulebased knowl.experimentalresultsare shownin table .
apartfrom theoverallperformance ofarclin inour wholetestset in arclinensemble we alsoexaminethe performance in every single library shown in the following five rows.theresultsindicateourarclinsignificantlyoutperforms allotherbaselines.fromtheresults weseethatourarclinmodel canachieve78.
.
and76.
inprecision recall andf1 score respectively.
it is worthy to notice that rulebase pure only retrieves .
of all api mentions reflecting that developers rarely write the entire qualifiednamewhentheymentionsomeapis.thisisalsooneof the motivations of this work.
the rulebase knowl model provides better performance with the help of prior knowledge.
however if we want to extend the model to a large number of libraries itis implausible for researchers to enumerate all possible abbrevi ations for each library.
although the model gives an acceptable performance itcanhardlybeusedextensively.anotherbaseline apirealreachesthef1scoreof0.
whichislowerthantheperformanceintheirdataset.weattributetheunfavorableperformance toseveralreasons theconstraintsofhandcraftedpatternsin resolvingcustomizedvariables.asillustratinginthesecondmorphologicalchallenge theunprofessionaldevelopersusuallywrite downapimentionswithcustomizedvariables a.reshape oraliases np.reshape .
since apireal leverages a collection of pre defined rules to solve the problem e.g.
npfornumpy the customized variablesoruncommonaliasesoutsidethescopeleadtomistakes.
the impact of such handcrafted rules is quantitated in the w o rule lineintable .
difficultyinminingapisinfreetexts.apireal leverages code tagsinitsrecognizer thus oncesomeonewrites down api mentions in such tags apireals can easily extract them.
but our dataset does not contain such signs to help the recognizer find out api mentions.
insufficient information.
apireal utilizesinformationfromsourcestackoverflowposts suchas urls question titles and code snippets.
however mining apis from sentences in our task requires the model to capture a richer semantic meaning.
arclin reaches the highest performance among the three baselines.
we conclude the reasons as follows arclin owns the figure p r curve of different hyper parameters kands.
table effectiveness of components in arclin.
precision recall f1 arclin ensemble .
.
.
w o recognizer .
.
.
w o lib pred .
.
.
w o lexical sim .
.
.
w o entity sim .
.
.
recognizerthatkeepsallpossibleapimentionsbyselectingthetopfivepathsincrfandconductvoting.inthisway arclinwillnot misstoomanyapimentions arclin slibraryindicatorprovidesscopeforlibraryselection preventingitfrom linkingtothe entryfromwronglibraries arclin sscoringfunctionbalances the lexical similarity and spelling similarity so small variations of an api s name will not affect its final prediction.
inaddition tothe goodperformance anotheradvantage ofarclin is its flexibility.
figure 3provides a precision recall curve to showhowtheperformanceisaffectedbythehyperparameters k ands.
each curve crf kin the figure represents a token will be considered as an api mention if k k out of five paths predict it so.
each point in a curve is arclin s performance under a similarity threshold s s .
a higher scoring threshold means a matched mention entry requires a higher similarity.
generally ahigherprecisionoccurssimultaneouslywithalower recallrate.arclinisabletoachieve100 precisionunderalow recall rate.
therefore we can customize the threshold under differentscenarios.thefigurealsoshowsthatwecannotachieve100 recalleventheprecisiongetsdowntozero.weascribethesituationintothefollowingreason comparedwithcharacter disorder word disorderistoocomplexforarclintodealwith.forexample when torch.nn.bcewithlogitsloss is written as bcelosswithlogits e v e ni f arclinnarrowsdowncandidatesintothecorrectlibrary itishard forarclintoconductapilinkingwitheachother.toconclude afterevaluatingarclininourtestsettheexperimentresultshows that it outperforms baselines by large margins even though it is free from any labor intensive annotations and handcrafted rules.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
arclin automated api mention resolution for unformatted texts icse may pittsburgh pa usa table the generalization ability of arclin.
p r and f1 refers to precision recall and f1 score respectively.
trainingtesting pytorch tensorflow pandas numpy matplotlib prf prf prf prf prf pytorch .
.
.
.
.
.
.
.
.
.
.
.
tensorflow .
.
.
.
.
.
.
.
.
.
.
.
pandas .
.
.
.
.
.
.
.
.
.
.
.
numpy .
.
.
.
.
.
.
.
.
.
.
.
matplotlib .
.
.
.
.
.
.
.
.
.
.
.
.
rq2 how effective are the components of arclin?
arcliniscomprisedofan apirecognizer andanapilinkerwitha librarypredictor andascoringfunctionbalancebetweenthe lexical similarity andentitysimilarity.toinvestigatethecontributionof each module we discard each component at a time implement the modelinourtestset andanalyzeitsperformance.experimental results of this ablation study are shown in table where each rowbelowarclin ensemble representstheresultofamissing component.
in the w o lexical sim and w o entity sim setting we set the scoring threshold to .
.
generally the missing module negativelyaffectsthemodel sperformancemoreorless.wewill discuss the effects in the following paragraphs respectively.
.
.
no recognizer.
in this setting the model tries to link an entry in the api repository for each token.
the precision perfor manceisdramaticallydecreasingbecausethemajorityoftokensin a sentence are not api mentions but they can still be linkedto an api in the repository because of their high similarity.
forinstance the common word wherehas a high similarity with the apinumpy.where becausebothofthemcontain where within the token but it is not an api mention.
arclin made lots of such mistakes causing low precision.
.
.
no library predictor.
in this setting the model tries to generatecandidatesfromallfivelibraries neglectingthesentencecontext information.
the failure occurs when different libraries have a method with similar names.
for instance pytorch has the method torch.stack while numpy also contains the method numpy.stack if a developer only writes stack as the api mention the model cannot disambiguate the token.
.
.
no lexical similarity.
withoutthelexicalsimilarity thescoringfunctionfullyreliesontheentitysimilarity.a mention entry pairwillbelinkedifandonlyifsomeentitieswithinthemareexactly the same.
this approach provides a high precision rate since it issimilar toan advanced rule basedalgorithm.
however itcannot deal with spelling errors.
for example np.zeros will be linked withnumpy.zeros because both of them has the entity zeros butnumpy.zeros cannotmatchedwith np.zero eveniftheapi mention contains only one missing character.
.
.
no entity similarity.
in this case the lexical similarity is determinativetothescoringfunction.thisfunctionworksfineinmost cases butfallingshortwhenanapimentionreferstoalongapiname.
for instance the api mention tf.layers.batch normalization hasahighersimilarityscorewith tf.keras.layers.batchnormalization ratherthan tf.compat.v1.layers.batch normalization .fromalexical perspective batch normalization is not far away from batchnormalization sothefinalscoringfunctionwilleasilybeaffectedby other factors i.e.
missing module name in this example .
.
rq3 what is the generalization ability of arclin?
considering the large number of libraries even for one program ming language we are interested in the generalization ability of arclin.
a promising api mining model should have the ability to mine apis without training on the library specific corpus.
toanswerthequestion wetraintherecognizerandlinkerinone librarycorpus thenthemodelattemptstorecognizeandidentify apis of another library in our test set.
in this setting the model neverseesthenewlibrarybefore sothelibrarypredictorisremoved from arclin.
table6shows the generalization ability of each pair of libraries.
the experimental results show arclin gains the generalization abilityto someextent.the experimentsfurtherindicates thatthe transferred model evaluated with matplotlib achieves a higherperformance.forinstance themodelthathasbeentrainedfrom numpy isabletocorrectlyrecognize81.
matplotlibapis accordingtothetable .weascribethereasontothedistinctivenessof api snameinmatplotlib.specifically apinamesinmatplotlib e.g.
matplotlib.pyplot.pcolormesh are rather different from apis in scientificcomputinglibraries e.g.
numpy.zeros ortorch.zeros so the model is free from mistakenly linking to apis in other libraries.
generally the transferred models contain a better recall rate ratherthanprecision andwediscussthereasonasfollows.without library predictor arclin may link api mentions to the wrong library if they contain similar method names.
for example given a sentence i have trouble with concatenating a list of tensors using pytorch s stack.
where stack here is labeled as torch.stack in ground truth during the testing phase.
if we train the model inpandas and evaluate its generalization ability in numpy library arclinwilllink stack totheapi numpy.stack .insummary the experimentresultsdemonstratetheeffectivenessandrobustnessof generalization ability.
such library transferred experiment mimics the real world scenario of applying arclin to mine apis from unseen libraries.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa yintong huo yuxin su hongming zhang and michael r. lyu table case study.
approach rule k apireal arclin api mention recog link recog link figure.add suplot ax.set major locator ticker.multiplelocator plt.show case study in this section we dive into three cases to specify why arclin outperforms apireal and rule k i.e.
rulebase knowl where ground truth is shown in blue italic font.
the experimental results offourapimentions inblue arepresentedintable 7withrespect to the two phases i.e.
api recognizer and api linker6 .
one api mention is successfully identified and resolved if and only if the link phase gives the correct answer check .
isthereamoreconvenientalternativeto figure.add suplot if i have multiple figures ... youmaytrytickingthemajoraxisusing ax.set major locator calledwith ticker.
multiplelocator .
if you don t want to export please uncomment plt.show and remove ... compared arclin with rule k and apireal we categorize the characteristicsforthreeapproaches.firstly rule kcanonlyresolve the api mention with the qualified name or a collection of specific abbreviations depending on the handcrafted rules.
for instance if we add the common writing behavior that a developer usuallycallsmatplotlib.pyplot by its alias plt rule k will try to replace the alias with its original name for each token then find if thenew token matches a fully qualified api name in the repository.
secondly we observe that apireal is more flexible than rule based matching algorithm by uncovering some api mentions by therecognizer e.g.
ax.set major locator andticker.multiplelocator allowing it to address the first common word ambiguity challenge.
nevertheless its api linker is not perfect to resolve the ambiguity introduced by morphological mentions mainly comes from the customizedname suchas axorticker.apirealdetectsaliasesby handcrafted patterns e.g.
pdforpandas thus the alias that is notcoveredbyruleswillbeinappropriatelycopedwith.lastbut notleast thecasesdemonstratetheeffectivenessofarclin.the carefully devised api recognizer enables it to detect api mentions in unformattedtext.
besides theapi linker withentity similarity forcesthemodeltopayattentiontotheinformativeentities e.g.
set major locator andthelexicalsimilarityallowsittoaddressthe misspelling in api mentions.
therefore arclin can even resolve thefigure.add suplot tomatplotlib.figure.figure.add subplot even if the mention leaves out the letter b .
6api recognizer is denoted as recog and api linker is denoted as link for space limitation.
threat to validity in this section we discuss three potential threats to the validity of arclinandprovideoursolutionstoalleviatethesethreats.the first one is the potential bias brought by manual annotation of the data.
we evaluate arclin the py mention dataset which is annotatedbytwodifferentannotators.toovercomethehumanbiasandensurethedataquality wenotonlyemploydomainexpertsinsteadofcrowd sourcingworkers butalsothrowawaythesentenceswith uncertainty.annotationexamplesandguidelinesareprovidedat first.asaresult theannotatorsfullyunderstandwhattheyneed to do and keep confidence in their annotation.
thesecondoneis thelimitedrecallrate.as showninfigure the recall cannot achieve regardless of the threshold.
in other words arclin cannot cover all ground truth labels.
we owe this recalllimitation tothe reasonsofobserved worddisorder inmentions.arclincomputessimilaritybasedonlexical levelandentitylevel but it fails in comparing mention entry pairs in word disorder.
for example if we use bcelosswithlogits to represent torch.nn.bcewithlogitsloss thesimilarityscorefromarclinis close totorch.nn.bceloss therefore the final output gets perturbation by other factors.
to alleviate the issue an n gram based similarity can be used to extend our arclin model.
thethirdthreatisstyleconstraint.currently weevaluatearclinwithfivepythonlibrariesandachievepromisingperformance but if we migrate the model to other programming languages theinconsistencyof functioncallingformatwill introducethisthreat.
for instance in c language we use double colon to call a static function or declare the namespace identification.
besides to call a function in a class one may use from a pointer or use the node .
from a c entity.
arclin uses .
to split the api s entirequalifiednameintoabagofpackageentitiesforsimilarity computation.ifweimplementarclininanotherlanguage e.g.
c it is necessary to implement new split marks.
related works apirecognition.
if we want to link an api to some other source thefirststepistorecognizeapis.inthispaper weusearecognizertorecognizeapisinfreetextsentences.dagenaisandrobillard adopted partial program analysis ppa to parse java snippets and thenextractscode liketermsininformaldiscussions.thedifference between theirs and ours is our paper targets extracting apis from naturallanguagesentences buttheabovestudieswereaboutextractingapisfromcodeblocks writteninfreetexts .bacchellietal .
employed arule based approach to extractapi mentions from e mails by designing different regular expressions applicable to different languages.treude and robillard suggested different regular expressions for question and body to extract api mentions from stackoverflow posts.
rigby and robillard used island grammars to identify code elements from free text with the help of compoundcamelcasedtermswhileignoringthecommon wordambiguity.
the most relevant research to us is apireal but their approachwasapplicabletorecognizingapisfromstackoverflow postswith code tags whichwasmucheasierthanoursetting.
besides insteadoflinkingsuchfinegranularityapis researchers alsoexploredlinkingbetweentextualdocumentsandcodeartifacts for maintenance.
some works antoniol et al .
chen marcus authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
arclin automated api mention resolution for unformatted texts icse may pittsburgh pa usa and maletic marcus et al .
used information retrieval ir techniquesorleveragelatentsemanticindexing lsi torecover traceabilitylinksbetweenelementsinnaturallanguagedocumentation and source code in software systems.
these studies were different from this paper since they performed a coarse granularity linking.
api linking.
linking can refer to linking code artifacts to documents or linking apis from free texts to its entire qualified namein therepository.regarding tolinkingcode artifactstodocuments bacchellietal .
usedtwostring matchinformationretrievaltechniques i.e.
vectorspacemodelandlsi tolinkdetected apis from e mails to source code artifacts.
the latter category is whatwehavedoneinthispaper themainideaofmatchingapis with their entire qualified name is how to conduct the disambiguation.
dagenais and robillard suggested a set of filtering heuristics to disambiguate the api mentions.
ye et al .
disambiguated apimentionsinastackoverflowpostbyutilizinginformationin code blocks question titles and the location where urlspoints to.
the first paper did not address the common word polysemy whilethesecondresearchmitigatedthemorphologicalchallenge by labor intensive rules which was different from ours.
mining technical forums.
nowadays manyresearchersdevote themselves to mining knowledge from technical forums e.g.
stackoverflow tofacilitatedevelopersintheirprogrammingissues.
forexample apopularscenarioisapirecommendation thesepaperssuggestedalistof apiclassesforanaturallanguage query by mining stackoverflow posts.
specifically given a natural languagequery huangetal .
firstlysearchedthemostrelevant 50questionsandextractingapisfromposts.then itrankedallcandidate apis by considering the query title similarity and title apis similarity.
li et al .
proposed another application that explores api caveat in such a technical forum and presented a system to help developers to tackle the problem of negative usage of apis.
it isnoticedthatmanyworksstudiedinstackoverflowtalksabout apis our workserves asafoundation ofthis workforfacilitating themtorecognizeandidentifytheapiswithouttheentirequalified name.
conclusion in this paper we propose a novel framework arclin for recognizingapimentionsfromfreetextandlinkingtoanapirepository.
arcliniscomposedoftwocomponents anapirecognizerand an api linker.
the api recognizer extracts api mentions from free textsandtheapilinkerdisambiguatestheapimentionsbyalibrary predictor to address reference ambiguity and a scoring function incorporating lexical similarity and entity similarity.
after training the model in an unlabeled stackoverflow corpus we implement arclin in a human annotated dataset named py mention the experimental results demonstrate that it significantly outperforms allbaselines.moreover theexperimentaboutgeneralizationability demonstrates that arclin can extract apis from a new library even though arclin is trained from another libraries.
acknowledgement theworkwassupportedbytheguangdongkeyresearchprogram no.2020b010165002 andtheresearchgrantscouncilofthehong kong special administrative region china cuhk .