perfsig extracting performance bug signatures via multi modality causal analysis jingzhu he shanghaitech university shanghai china hejzh1 shanghaitech.edu.cnyuhang lin north carolina state university raleigh nc usa ylin34 ncsu.eduxiaohui gu north carolina state university raleigh nc usa xgu ncsu.edu chin chia michael yeh visa research palo alto ca usa miyeh visa.comzhongfang zhuang visa research palo alto ca usa zzhuang visa.com abstract diagnosing a performance bug triggered in production cloud environments is notoriously challenging.
extracting performance bug signatures can help cloud operators quickly pinpoint the problem and avoid repeating manual efforts for diagnosing similar performance bugs.
in this paper we present perfsig a multi modality performance bug signature extraction tool which can identify principal anomaly patterns and root cause functions for performance bugs.
perfsig performs fine grained anomaly detection over various machine data such as system metrics system logs and function call traces.
we then conduct causal analysis across different machine data using information theory method to pinpoint the root cause function of a performance bug.
perfsig generates bug signatures as the combination of the identified anomaly patterns and root cause functions.
we have implemented a prototype of perfsig and conducted evaluation using real world performance bugs in six commonly used cloud systems.
our experimental results show that perfsig captures various kinds of fine grained anomaly patterns from different machine data and successfully identifies the root cause functions through multi modality causal analysis for out of tested performance bugs.
keywords debugging bug signatures software reliability performance acm reference format jingzhu he yuhang lin xiaohui gu chin chia michael yeh and zhongfang zhuang.
.
perfsig extracting performance bug signatures via multi modality causal analysis.
in 44th international conference on software engineering may pittsburgh pa. acm new york ny usa pages.
introduction cloud systems are becoming increasingly complex which dramatically increase the occurrence chance of various software bugs.
in this work is licensed under a creative commons attribution international .
license.
icse may pittsburgh pa usa copyright held by the owner author s .
acm isbn .
whichcause cloud systems to get stuck in a hang state or experience performanceslowdown.performancebugstriggeredinproductioncloud environmentsarenotoriouslydifficulttodiagnoseandfixduetothe lack of diagnostic information.
when a performance bug occurs in productioncloudenvironments systemoperatorsanddevelopers often need to put a lot of manual efforts to diagnose and fix theproblem under time pressure.
for example it took more than hoursforamazontorecoveritsmembershipserviceoutagecaused by a performance bug .
the bug was triggered by a limit on the allowablethreadcount thatis theserverhungwhenthenumber of server threads exceeded its pre defined limit.
during our empirical bug study using popularbug repositories such as jira and bugzilla we observe that many performance bugs repeatedly occur in different versions of open source systems which causes the community to perform redundant debugging over thesame bug.moreover micro servicesusing containers make the bug replication easier than ever the same bug occurs inmultiplecontainersthatarecreatedfromthesamecontainerimage.
tothisend webelievecreatingsignaturesfordifferentperformance bugscanhelpsystemoperatorsquicklyidentifyrecurrentbugsandexpeditedebuggingprocess.aperformancebugsignatureuniquelycharacterizesaperformancebuginbothsymptoms i.e.
anomalous resource usages and or abnormal log sequences and root cause functions.
previousworkonperformancebugdetectionanddiagnosis e.g.
hastwomajorlimitationswhenapplying to the production cloud environment.
first previous work e.g.
mainly focuses on depicting performance bugs via analysis over single data type such as system metrics system calls system logs or performance counters.
however a performance bugmaymanifestasanomaliesindifferentdatatypes.forexample an infinite loop bug maycause a persistently highcpu usage while a timeout bug can cause abnormal log sequences.
thus it is likelythatwemayfailtoextractbugsignaturesforsomeperformancebugsifweonlyfocusonanalyzingonedatatype.moreover extracting anomalies alone often cannot uniquely characterize a performance bug because different performance bugs may exhibit similar anomaly patterns in one data modality.
for example different infinite loop bugs can all show increased cpu consumption.
thus it is necessary to perform multi modality analysis to extract representativesignaturesfordifferentperformancebugs.second ieee acm 44th international conference on software engineering icse icse may pittsburgh pa usa jingzhu he et al.
rpc class public static t protocolproxy t waitforprotocolproxy ... throws ioexception return waitforprotocolproxy ... ... return waitforprotocolproxy ... getrpctimeout conf ... public static int getrpctimeout configuration conf return conf.getint com monconfigurationkeys .ipc client rpc timeout key commonconfigurationkeys .ipc client rpc timeout default figure1thecodesnippetofthehadoop 11252bug.thebuggy code is invoked at the datanode.
!
!
!
!
!
!
!
!
!
!
!
figure2logsgeneratedbyhadoop 11252bug.thelogsareproduced at the namenode.
the existingtools arenot application agnostic.
theexisting tools often require domain knowledge extracted from the sourcecodeorbinarycode.however suchinformationisnoteasily accessibleinproductionsystems.therefore itisessentialtodesign a light weight performance bug signature extraction tool without requiring domain knowledge.
.
a motivating example weusehadoop bugtoillustratehowaperformance bug happens and how it manifests in different machine data types.
therootcauseofhadoop 11252bugisthatthedatanodedoesnot properly timeout the connection with the namenode.
timeout is a commonlyusedfailovermechanismtoclosethebrokenconnection.
asshowninfigure1 therootcausefunctionis waitforprotocolp roxyfunctionwhichpasses0timeoutvalue 0meansnevertimeout tothetimeoutconfigurationincorrectlyatthedatanodeside.when the namenode experiences some unexpected problems such as network outage thedatanode hangs on waitingfor the response fromtheremoteserverwithoutproducinganyerrorinformation.as showninfigure2 weobservethatthelogentriesthataretypically produced by server stopping are missing at the namenode side.
even if developers can discover the missing log anomaly at the namenode side it is still difficult for them to pinpoint the root cause function which is actually located at the datanode side.
.
contribution in this paper we present perfsig an automatic performance bug signature extraction tool which performs multi modality analysis acrossdifferentmachinedataincludingsystemmetrics systemlogs and function call traces.
when a performance alert or service level objective slo violationisdetected perfsigistriggeredtoanalyze a time window of recent machine data.
perfsig first employs signalprocessingtechniquesandunsupervisedmachinelearningmethods to identify fine grained anomaly patterns in various machine data.
for example for system metrics such as cpu usage time series we employfastfouriertransform fft andtimeseriesdiscordmining toidentifyanomalypatternssuchasfluctuationpatternchanges persistentincrease andcycleperiodchanges.forsystemlogs we identify abnormal log sequences such as missing log entries in a certain common sequence or overly long time span for certain sequences.next perfsigperformscausalanalysisbetweenabnormal metric log patterns and function call traces using information theorymethodmutualinformation mi .ourcausalanalysis reveals the granger causality i.e.
dependencies between the anomaliesdetectedindifferentmonitoringdata e.g.
systemmetrics system logs function call traces .
the goal is to identify the root cause function which is the top contributor to the metric or log anomaly.
perfsig outputs the performance bug signature as the combinationofthedetectedanomalypatternandthepinpointed root cause function.
specifically this paper makes the following contributions.
wepresentanewmulti modalityperformancebugsignature extractionframeworkwhichcanpreciselydepictaperformance bug using both fine grained anomaly patterns and root cause functions.
wedescribeasetoffine grainedanomalydetectionmethods to capture specific manifestation of a performance bug in system metrics or logs.
weintroduceaninformationtheorybasedcausalanalysis approach to pinpointing root cause functions by discovering the causal relationship between function call traces and anomaly patterns of system metrics or logs.
we have implemented a prototype of perfsig and evaluated it over real world bugs that are discovered in six commonly used cloud systems.
the results show that perfsig can produce precise signatures for out of performance bugs.
therestofthepaperisorganizedasfollows.section2discusses thesystemdesigndetails.section3presentstheexperimentalevaluation.section4discussesthethreatstovalidity.section5discusses the related work.
section concludes the paper.
section presents the data availability.
system design inthissection wedescribethesystemdesignoftheperfsigsystem indetails.wefirstgiveanoverviewofthesystem.next wepresent howweidentifyvariousfine grainedanomalypatternsinsystem metricandsystemlogdata respectively.finally wediscusshow weperform causalanalysisbetweensystemmetric log anomalies andfunctioncalltracestoidentifytherootcausefunctionwhich contributes to the anomaly.
.
approach overview perfsig adopts a two phase approach to extracting signatures for a performance bug shown by figure .
when a performance alert is generated or a service level objective slo violation is detected perfsigistriggeredtoextractperformancebugsignatureson the fly by analyzing a recent time window of system metrics system logs 1670perfsig extracting performance bug signatures via multi modality causal analysis icse may pittsburgh pa usa figure the architecture overview of perfsig.
andfunctioncalltraces.duringthefirstphase phasei perfsigemploysvarioussignalprocessingandmachinelearningtechniquesto extract fine grained anomaly patterns to capture the manifestation of the performance bug in either system metrics or system logs.
specifically for system metrics perfsig uses time series analysis schemes to extract principal features such as fluctuation pattern changes persistent increases and cycle period changes section .
.
for system logs we leverage classification and frequent sequenceminingtoextractanomalouslogpatternssuchasmissing certainlogentriesoroverlylongtimespanofcertainlogsequences section2.
.duringthesecondphase phaseii perfsigusesinformation theory approach to performing causal analysis between functioncalltracesanddetectedabnormalsystemmetricorsystem log patterns to pinpoint root cause functions section .
.
performancebugsoftenmanifestasabnormalresourceconsumption and orsystemlogoutputs i.e.
performancebugsymptoms which are typically caused by the root cause functions e.g.
infinite loops missing timeout costly operations .
therefore we leverage the granger causality between anomaly patterns and function time spananomaliestoidentifytherootcausefunctions.combiningthe phaseiandphaseiiresults perfsigoutputstheperformancebug signature as the combination of the fine grained anomaly pattern and the pinpointed root cause function.
.
system metric anomaly pattern detection manyperformancebugscanmanifestaschangesinsystemmetrics such as cpu utilization memory utilization and network traffic.
however differentperformancebugscanexhibitdifferentanomaly patterns.forexample figure4showsdifferentcpuusageabnormal patterns for three real performance bugs.
to extract distinctive signatures for different performance bugs we need to not only detect anomalies but also extract fine grained anomaly patterns.
weobservethatthesystemmetricssuchascpuconsumption are inherently fluctuating.
in order to extract principal anomaly patterns wefirstleveragelowpassfilterstoremoverandomnoises from the raw system metric time series.
the low pass filter performs data denoising by filtering out high frequency signals in original system metric time series.
the rationale is that random fluctuations usually manifest as the high frequency signals.
we transformthetimeseriestothesignalsinfrequencydomainanddrop high frequency signals.
note that we use relational values instead of absolute values which avoids setting manual thresholds.
if we choose a too large filtering percentage we filter out toomanysignalswhichmightincludeanomalies.ifwechoosea too small filtering percentage we cannot filter out noises.
in our experiment we filter out top high frequency signals.
after that we transform the signals in the frequency domain back to the time series.
we conduct extensive experiments to compare the time series patterns before and after performing low pass filters.
theresultsshowthattheanomalypatternsbecomemoresalient after filtering.
for example figure 4b and figure 9a show the same cpu usage patterns before and after the filtering respectively.
we can see the anomaly pattern is much clearer in figure 9a.
next we employ signal processing methods over denoised time series to extract principal anomaly patterns.
fluctuationpatternchanges.
fordynamicdata intensivecomputing systems such as hadoop cpu utilization usually has periodical large fluctuations during normal run.
it is because the application workload contains different types of interleaving jobs.
for example figure 4a shows the cpu utilization s fluctuation change when hadoop bug occurs.
during the normal run thefirsthalfofthefigure weobservelargefluctuations.afterthe bug is triggered the second half of the figure the system hangs inside an infinite loop which fully consumes one cpu core and thencpuutilizationstaysatasteadyvalue.weobservethatmany hang bugs in dynamic data intensive systems often manifest as fluctuation pattern changes which refer to the cases when the system usage changes from normal fluctuating patterns caused by dynamic workloads during normal runs tonearly non fluctuating patternscausedbythehangbugsduringbuggyruns.tocapture this anomaly pattern perfsig calculates the standard deviations of a moving window in the system metric time series and identify the time when the moving window standard deviation experiences significant changes e.g.
dropping from a large value to a small value .
persistentincreases.
besidessoftwarehangbugs slowdown bugsareanothercommoncategoryofperformancebugs.weobserve slowdown bugs caused by code inefficiency often consumes alargeamountofcomputationalresources e.g.
cpu duringthe abnormalperiod.forexample figure4bshowsthecpuincrease 1671icse may pittsburgh pa usa jingzhu he et al.
a hadoop 15415bug sfluctuationpatternchange.
the bug starts at .
b hadoop 6133bug spersistentincrease.thebug starts at and ends at .
c cassandra 7330bug scyclicpatternchange.the bug starts at .
figure three commonly seen system metric anomaly patterns.
caused by the hadoop bug.
perfsig detects such abnormal pattern using time series discord .
a sliding window is applied onthefilteredsystemmetrictimeserieswherethenearestneighbor distance between the previous sliding windows and the current sliding window is computed.
perfsig detects the persistent increase pattern when the nearest neighbor distance shows significant increases.
cyclicpatternchanges.
manyproductionserversystemsexhibitcyclicresourceconsumptionpatterns.itisbecauseproduction server resource usage patterns are typically drivenby production workloadpatterns.whentheproductionworkloadexhibitsregular patterns the corresponding system usage patterns show cyclic patterns.
we observe that when a performance bug is triggered the workload changes leading to the cyclic resource usage pattern changes.
figure 4c shows the cpu cyclic pattern change caused bythecassandra 7330bug.duringnormalrun cpushowsacycleofnineseconds whileduringbuggyrun cpudoesnotshow cyclic pattern.
to detect such cyclic pattern changes we employ fast fourier transform fft algorithm on a sliding window of systemmetrictimeseriestoextractthedominatingfrequencieswhose magnitude values are in the top rank list.
we detect the cyclic pattern change when the top frequency values experience changes.
.
system log anomaly pattern detection we now describe how perfsig extracts anomaly patterns from system logs.
much existing work focuses on detecting abnormal error logs that only appear in a buggy run.
however we observe that when a performance bug is triggered the system usually does not produce any error log message.
instead some log entries included inthenormalrunaremissingduringthebuggyrun whicharequite common among software hang bugs.
in other cases like slowdown bugs somelogsequencescouldexhibitlongertimespan i.e.
the timedurationfromthestarttimeofthefirstlogentryinasequence totheendtimeofthelastlogentryinasequence duringthebuggy run when compared with normal run.
previousworkinreconstructingexecutionpathfromthesystem logs contains three limitations focusing on sequential task execution is combined with domain knowledge extracted from binarycode and3 isnotgenericforallthesystems .compared with the existing work perfsig considers concurrent task executionandonlytakesthelogentriesandvectortimestampas the input.
perfsig does not require any application specific knowledge.todiscoverthelogsequencesfrominterleavinglogentries perfsigfirstclassifiesthelogentriesgeneratedbydifferenttasks.
figure the log entry classification framework.
after the task classification is done we further separate the log entriesbasedonthetimegaps.thenweperformfrequentsequence mining to extract the log sequences.
semantics based grouping.
after we collect logs from distributedhosts perfsigclassifieslogsgeneratedbydifferenttasks.
the idea is that logs generated by the same task have similar semanticmeanings.toachievethisgoal weusethewordembedding vector to represent each word s contextual meaning.
after that we use the average of the word embedding vectors of the words torepresenteachlogentry whichiscommonforgenerating representation for natural language sentences .
intuitively log entrieswhichhavesimilarmeaningshavesimilarwordembedding vectors.
therefore we can apply clustering algorithm to grouping the similar log entries together.
figure summarizes the classification procedures.
specifically perfsig pre processes the logs to splitthelogentriesintowords extractswordembeddingvectors foreachword buildslogentryrepresentationbyaggregatingthe word embeddings associated with each log entry and classifies the entries generated by different tasks with the self organizing map som algorithm .
first we pre process each log entry to extract the words for learningwordembeddingvectors.thefirststepistosplitthelog entries based on brackets and parentheses because content inside a pairofbrackets andparentheses oftenrepresents onecommand oroperation e.g.
databasequerycommand.afterthat wesplitthe logentrybasedoncomma fullstop colon andsemi colon.then we further separate them according to the spaces between words.
oncewehaveextractedwordsfromthelogentries wetreateach entry as a sentence and learns the word embedding representation foreachword.themajoradvantageofusingwordembeddingis that it considers words semantic meaning in the contexts.
it is basedonthehypothesisthatwordsoccurredinthesimilarcontexts tend to be semantically similar.
we choose to use word embedding asthefeaturevectorbecauselogentriesgeneratedbythesametask typicallyuseshortsentencewithsimilarterms.weuseword2vec with continuous bag of words cbow to learn the word embedding vector to represent each word s meaning.
each word embeddingisinitializedasa n dimensionalvector.ineachiteration 1672perfsig extracting performance bug signatures via multi modality causal analysis icse may pittsburgh pa usa !
!
!
!
!
!
figure6thelogentries embeddingvectorsformstwoclusters one for each task.
wetraineachword sembeddingusingthesumof msurrounding context words embedding vectors.
we update each word s embedding vector until the convergence is reached.
in our experiment n is set to and mis set to five.
weconstructeachlogentry sembeddingvectorbytakingthe average of all the words embedding vectors in the log entry.
after that we apply the som clustering algorithm to clustering log entriesthatbelongtodifferenttasks.comparedwiththetraditional distance based methods som model has better performance to cluster high dimensional vectors.
figure6showsanexampleofclassificationresults.thelogentriesaresplitintotwoclustersandeachclusterrepresentsonetype oftask.forexample logclusterarepresentshadoopsystemestablishes ipc connection between different processes.
log cluster b represents hadoop system runs the map and reduce computational jobs.
frequentsequence mining after task classification we successfullyseparateinterleavinglogsintologentryclusters representingdifferenttasks.thegoalofthisstepistoextractfrequent systemlogsequenceswitheachlogcluster whichoftenrepresenta setofexecutionsuchasclient serverconnectionorthreadscommunication.performancebugsmanifestasmissing logentriesinthe logsequenceorthelogsequencehasabnormaltimespan.forexample figure shows a complete log sequence.
when hadoop bughappens thelastthreelogentriesaremissing.ourideaisto only extract the constants log keys from the log entries.
then we split thelog entriesbased on thetime gapsand extract frequently occurred log sequences.
the log entries contain variables like socket reader number and portnumber whicharedifferentineachipcconnection.weextract the constants log keys by adopting simple regular expressions.
afterwesplitthelogentriesintowords wekeepthewordsthat only contain alphabetic letters and replace other words with .
then we concatenate the words with a space as the log key.
for eachlogentrycluster weseparateitbasedonthetimegaps.the rationale is based on the observation that the same log sequence occursafterlongtimegapcomparedwithitstimespan.wecalculate thetimegapsbetweeneachtwoconsecutivelogentriesandderives theaveragevalueandstandarddeviationofeachcluster.ifthetime gapbetweentwoconsecutivelogentriesexceedstheaveragevalue .
standard deviation we separate the log cluster.
after we get separated log clusters and replace each log entry with the log key we perform frequent sequence mining to !
!
!
!
figure logs generated by hdfs bug.
extract log sequences from normal run data.
we use prefixspan toperformfrequentsequenceminingbecauseitismoreefficient compared with other methods.
after we extract top frequent logsequencesfromnormalrun weusethemtodetectanomalies duringbuggyrun.specifically wedetecttwoanomalypatterns i.e.
missing logs and abnormal log sequence time span.
missing log entries anomaly pattern during the anomaly detection phase we first perform semantics based grouping.
in eachcluster weextractlogkeysandcheckwhetherthelogkeys belongtoanyextractedlogsequence.iftheanswerisyes wecheck whetherotherlogkeysofthelogsequencealsoappearinthelog cluster.
for example figure shows how we detect hadoop bug.
during normal run we extract the complete log sequence from starting the socket reader to stopping the server responder.
during buggy run we perform semantic based grouping to cluster logentriesgeneratedfromipcconnectiontaskstogether.forthe extracted the log sequence we find the log keys to start the server but we cannot find the log keys to stop the server.
excessive time span anomaly pattern similar to missing log patterndetection weperformsemantics basedgroupingandlog key extraction.
if all the log keys of one particular log sequence appearinonecluster persigextractsthelogsequencetimespan whichstartsfromthefirstlogkey soccurringtimetothelastlog key s occurring time.
if the time span is excessive long persig reports the abnormal log sequence time span pattern.
for example figure shows how we detect hdfs bug.
during normal run weextractthelogsequenceandthelogsequencespanssixseconds.
duringbuggyrun thelogsequencespans97secondsandpersig reports the abnormal log sequence time span pattern.
.
root cause analysis via multi modality causal analysis afterweidentifytheanomalypatterns weperformcausalanalysis to localize the root cause function.
.
.
causal analysis between system metrics and function call traces.after we identify three anomaly patterns of the system metric time series we retrieve a window of the filtered time series which contains the anomaly patterns.
suppose the bug happens betweentime then weretrievethe systemmetricbetween wherewistheparametertocontrolthewindow size.thenweretrieveallthefunctioncalltraceswhichoccursin 1673icse may pittsburgh pa usa jingzhu he et al.
i s ab3c06709e 3bbab4 b e d dfsinputstream bytearrayread r yarnchild p n path ... job 1621397508582 0005 job.split ... i 37dada 28edec1ea0 s 5cbe565 f9c5a3c8b b e d conf.getclassbyname r test p figure8theextractedfunctioncalltraces.
b representsfunction sstarttimestamp e representsfunction sendtimestamp and d representsthefunctionname.allthetraceshavethestart time no earlier than and end time no later than .
thetimewindow i.e .forexample figure9ashows the cpu utilization time series.
it contains the cpu spike occurred between00 54and00 .weset wtooneminute therefore we includethe normalrun data from00 to 54and data from to because we want to include the sharp cpuchangesat00 54and00 .includingnormalrundata can increase the accuracy of causal analysis.
if we only consider the buggy run data the frequently used functions also occur in the buggy run and may infer a high causality score.
however if we consider both normal run and buggy run the frequent used functionsalwaysappear whichdecreasesthecausalrelationship betweenthesystemmetricsandthem.figure8showstheretrieved function call trace snippet.
all the function call traces are in the chronologicalorder.weextractallthefunctioncalltracesoccurred between00 54and00 i.e theirstarttimesarenoearlier than and end time no later than .
we extract three kinds of information i.e beginning timestamp ending timestamp andfunctionname toformulatethefunctioncalltraceintotime series.
next weformulatethefunctioncalltracesintothetimeseries.
we extract the aggregated time span of each function between two consecutivetimepointsofsystemmetrictimeseries.ifcpuutilization are sampled at t1 t2 ... tn then the function time series can beformulatedas t t1 t2 t t2 t3 ... t tn tn wheret ti ti represents the function s aggregated time span during ti ti period.forexample figure9bshowsthe conf.getclassbyname functiontimeseries.duringthetime thebug happens and conf.getclassbyname is invoked for all the one second period.therefore conf.getclassbyname time serieshas the value of milliseconds at the time point .
if mfunctions are invoked during the whole time window then there are m function time series.
in this way the function time series is aligned withsystemmetrictimeseries andwecanapplycausalanalysis algorithm on them.
we extract the time span as the function call s feature because performance issues such as hang or slowdown usually manifest as the changes in function time spans.
after we get all the function time series we normalize each functiontimeseriesandthesystemmetrictimeseries.afternormalization thesumofallthedatapointsinonetimeseriesisequal to one.weadopt information theoreticmethod i.e.
mutual information mi toinferthecausalrelationshipbetweeneach function stimeseriesandcputimeseries.becauseperformance bugsoftenmanifestasabnormalsystemusagesand orabnormal a thecpuutilizationtimeseriesafterfiltering.thebugstartsat00 54and endsat00 .weretrieveoneminutenormalrundatabeforeandafterbug is triggered.
b thefunctioncalltimeseriesinhadoop 6133bug.
conf.getclassbyname is therootcausefunctionand dfsinputstream bytearrayread isannon rootcause function.
figure hadoop bug s time series.
systemlogoutputs weleveragethegrangercausalitybetweensystemanomaliesandfunctiontimespananomaliestoidentifyroot cause functions.
mutual information is one of the entropy based methods which are important methods to perform exploratory causal analysis for time series data .
compared with linear correlationmethodssuchaspearsonandspearmancoefficient mutualinformationcapturesnon linearcausalrelationshipbetween two time series.
mutual information measures how much knowing one of the two time series reduces uncertainty about the other.
mutualinformationnotonlyconsidertheabsolutevalueofeach data point but also consider the data distribution across the whole time series.
therefore we can filter out the frequently invoked functions.
it is because the frequently invoked functions have long time span during both buggy run and normal run.
considering the data distribution they cannot have a larger mutual information than those functions which are only frequently invoked during buggyrun.incomparison thefrequentinvokedfunctionscanhave highcorrelationscoreswithsystemmetrictimeseriesbecausethey have long time spans during buggy run.
mutual information between two time series xandyis defined as mi x y summationdisplay.
x yp x y log p x y p x p y wherep x andp y representtheprobabilitiesof xandyoccurred atthesamplingpoint.
p x y representstheprobabilitythat xand yoccur at the same time.
we calculate the mutual information between each function time series and the system metric time series.
we rank all the candidate functions based on the mutual information scores.
we 1674perfsig extracting performance bug signatures via multi modality causal analysis icse may pittsburgh pa usa a the log sequence time series.
the bug is triggered at .
b thefunctioncalltimeseries.
rpc.waitforprotocolproxy istherootcause function and clientprotocol.create is an non root cause function.
figure hadoop bug s time series.
then determine the function that contributes to the anomaly most as the one with the largest mutual information.
.
.
causalanalysisbetweensystemlogsandfunctioncalltraces.
weperformcausalanalysisonfunctiontimeseriesandlogsequence timeseriesto localize therootcausefunctionthat contributesto theloganomaly.eventhoughsystemlogscontainrichinformation e.g.
the class name that generates the logs it is still essential to perform causal analysis because the function that generates the log sequence is usually not the root cause function.
afterweidentifythemissinglogorlongerexecutionpatterns we retrieve the log sequence s information i.e.
beginning time and end time during both normal run and buggy run.
the start timeisthefirstlogentry sinvokingtimeandtheendtimeisthe last log entry s invoking time.
for example in figure the log sequence start time is and end time is .
if missing logpatternhappens weregarditstimespanasinfinitybecausethe log sequenceneverends.
besidesthat weretrieve allthe function call trace within the same time window i.e during the period that the log sequences are produced.
we formulate the log sequences and the function call traces into time series.
we sample them every one second and collect the aggregatedtimespanduringtheonesecondinterval.itissimilar to how we formulate the function time series in section .
.
.
note thatthefunctiontimeseriesstillneedstobealignedwiththelog sequencetimeseries.figure10ashowsthelogsequencetimeseries and figure 10b shows the function call time series.
the root cause function has similar shape with the log sequence.
after generatinglog sequence and functiontime series we calculate the mutual information between each function s time series and the log sequence to identify the root cause function similar to section .
.
.
experimental evaluation in this section we first present the evaluation methodology followed by the experimental results.
next we present several real bugexamplesincludingonenegativecasestudywhereperfsigfails to extract a signature for the bug.
.
evaluation methodology cloudsystems westudied20realperformancebugsfromsixcommonlyusedopen sourcecloudsystems hadoopcommonlibrary hadoopmapreducebigdataprocessingframework hadoophdfs filesystem hadoopyarnresourcemanagementservice hbasedatabasesystem andcassandradatabasesystem.fourhadoopsystems aresetupindistributedmodestoevaluateperfsig seffectiveness over distributed system performance bugs.
benchmarks we use the hang stuck block log cpu performance and slowdown keywordstosearchforperformance bugs.
we manually examine each bug to determine whether it is a real performance bug triggered in production environments and whether it is reproducible in deterministic ways.
to the best of our efforts we successfully reproduced bugs in six cloud systems.
table shows our collected bug benchmark.
setup all the experiments wereconducted in our lab machine withaninteli7 4790octa core3.6ghzcpu 16gbmemory running bit ubuntu v16.
with kernel v4.
.
.
.
implementation functioncalltracing thefunctioncalltracesarecollectedusing googledapperframework.dapperhasvariousimplementations ondifferentproductionsystems.forexample animplementation ofdapper htrace isintegratedintohadoopsinceversion2.
.
.
another implementation of dapper zipkin is integrated into hadoop hbase and cassandra.
those implementations collect tracesforerror pronefunctionsincloudsystems.wecanconfigure theparametersfordappertracingintheconfigurationfilesdirectly and deploy the production systems to trace the function calls.
anomalypatternanalysis weimplementtheanomalypattern detection in python .
.
we use scikit learn package to implementfftanalysisandlogclassification.weusetheword2veccbow implemented in gensim for embedding learning.
hyperparameter in word embedding learning we use the word2vec cbow algorithm with the embedding vector size of and the window size of words.
the som is set to be 5x5 grid map and the weight vector length in som is set to be the same as the embedding vector size.
.
alternative approaches performance bug signature is quite new.
previous tools only addressed partial problems.
perfsig first provides a comprehensive end to endsolutiontoextractbothsymptomsandrootcausesas thebugsignatures.tocomparewithperfsig weimplementseveral alternative approaches to perform log analysis and causal analysis.
.
.
logs anomaly pattern detection.
for system log anomaly pattern detection we implement four alternative approaches to compare with perfsig.
1675icse may pittsburgh pa usa jingzhu he et al.
table bug benchmark.
bug id represents it is a distributed system performance bug.
bug id version symptom description cassandra .
.
hang the corrupted inputstream returns error code causing an infinite loop.
cassandra .
.
hang improper exception handling skips loop index forwarding api causing an infinite loop.
hadoop .
.
hang the rpc connection timeout is missing leading to system hanging.
hadoop .
.
hang misconfigured parameter indirectly affects loop index causing an infinite loop.
hadoop .
.
slowdown the atomiclong operations cause contention with multiple threads.
hadoop .
.
slowdown extra calls cause 80x performance slowdown.
hadoop .
.
hang skipping after eof returns error code affecting loop stride.
hadoop .
.
alpha1 slowdown ipc connection timeout is hard coded causing a much longer failure recovery time.
mapreduce .
.
alpha hang timeout is missing when jobtracker calls a url causing system hanging.
mapreduce .
.
alpha hang task status updates cause hung task never timeout.
mapreduce .
.
hang nodemanager hangs on shutdown due to struggling deletionservice threads.
mapreduce .
.
hang misconfigured variable causes loop stride to be set to .
mapreduce .
.
hang skipping on a corrupted inputstream returns error code affecting loop stride.
hdfs .
.
alpha hang timeout is missing for image transfer operations causing system hanging.
hdfs .
.
alpha slowdown timeout value on image transfer operation is large.
hdfs .
.
hang dfs input streams do not timeout.
hbase .
.
slowdown timeout is missing for terminating replication endpoint.
yarn .
.
alpha hang skipping on a corrupted filereader returns error code affecting loop stride.
yarn .
.
hang yarnclient endlessly polls the state of an asynchronized application.
yarn .
.
hang skipping on a corrupted aggregated log file returns error code affecting loop stride.
dbscan embedding we usethe same embedding representation.
however instead of the som clustering algorithm we use dbscan togroupthelogentriesgeneratedbydifferenttasks.
dbscan is a popular non parametric density based clustering algorithm.
it automatically determines the number of clusters when trained.
som tfidf we use term frequency inverse document frequency tfidf to extract features from each log entry.
term frequency tf captures the frequent words in a log entry.
inverse documentfrequency idf isusedtoweightdownthefrequently used meaningless words such as the an and on .
the som algorithm is used to classify the tfidf vectors associated with different log entries into different task groups.
dbscan tfidf we use tfidf to represent each log entry and dbscan to classify the log entries into different task groups.
topic lda latent dirichlet allocation lda is a popular technique to analyze the topics of natural language documents.
when log entries are fed into the lda algorithm the algorithm estimatestheprobabilityofeachlogentrybelongingtodifferent topics.
we choose the topic with the largest probability as its topic.
thenweclassifythelogentrieswiththesametopicintoonetask group.
.
.
causal analysis.
we implement two alternative approaches forcausalanalysisforcomparisonwithperfsig.bothalternative approaches use correlation coefficients to predict causality.
specifically we test the pearson correlation coefficient and spearman s rank correlation coefficient .
pearson correlation coefficient measuresthe correlation using normalized co variances of absolutevalues.spearman srankcorrelationcoefficientmeasures the correlation using normalized co variances of ranked values.
a larger causal score means the two time series are strongly correlated.
.
results table shows the results of signature extraction for bugs which manifest as system metric anomalies.
perfsig can identify threedifferent anomaly patterns i.e fluctuation pattern changes persistent increases and cyclic pattern changes from all the tested bugs.
moreover perfsig is capable of detecting the true root cause functions for all the tested bugs.
wealsoevaluatedthreedifferentcausalanalysismethods i.e.
mi pearson and spearman under two different settings with lowpassfilterandwithoutlow passfilter.overall thelow passfilter improves the quality of causal analysis result no matter which causal analysis method is used.
it is because smoothing the time series reduces irrelevant fluctuations brought by dynamic workloads and makes the anomaly pattern more salient.
for example in hadoop bug the hflushfunction ranks the highest when filtering is not employed.
the hflushfunction is cpu intensive whichismis detectedastherootcausefunctionduetoalowcausal score.inourexperiments weobservethatsettinga50 filteringthresholdproducesthesameresults sowechoose50 asour default filtering threshold.
whencomparingthethreedifferentcausalanalysismethods we canseethattheproposedmischemeoutperformsthealternative pearson and spearman methods.
the experimental results show that good causal analysis techniques needs to consider data distribution across the whole time series.
moreover entropy based method such as mi is better than co variance based methods.
table shows the results of bug signature extraction results for bugs which manifest as log anomalies.
specifically we compare perfsig i.e.
som embedding withfourotheralternativedesigns dbscan embedding som tfidf dbscan tfidf andtopic lda.
overall perfsig is capable of detecting the anomaly pattern for eight out of nine bugs which show log anomalies.
first wecompareperfsigwithdbscan embeddingwherethe workload classification method som is replaced with dbscan.
perfsig outperforms dbscan embedding because the word embeddingvector is100 dimensional.dbscanhas poorperformanceon high dimensional vectors due to the curse of dimensionality .
if we replace the embedding representation with the tfidf representation we can see perfsig outperforms both alternative approacheswithtfidf i.e som tfidfanddbscan tfidf.itisbecausetfidfonlyconsidersindividualwordoccurrenceandfailsto 1676perfsig extracting performance bug signatures via multi modality causal analysis icse may pittsburgh pa usa table2signatureextractionresultsforbugswhichhavesystemmetricanomalies.pearsonfispearsonmethodwithfilter.similarly spearmanfis spearman method with filter.
for each method we present the root cause function s rank using causal analysis.
bug id anomaly pattern root cause functionnumber of candidate functionsperfsig pearsonfspearmanfmipearson spearman hadoop 8614fluctuation pattern changeioutils skipfully hadoop 15415fluctuation pattern changeioutils copybytes yarn 163fluctuation pattern changecontainerlogspage printlogs yarn 2905fluctuation pattern changeaggregatedlogsblock readcontainerlogs91 yarn 1630fluctuation pattern changeyarnclientimpl submitapplication mapreduce 7089fluctuation pattern changereadmapper mapreduce 6990fluctuation pattern changetasklog reader hadoop persistent increase fsdataoutputstream write hadoop persistent increase conf getclassbyname cassandra 9881cyclic pattern changescrubber scrub cassandra 7330cyclic pattern changestreamreader drain table3signatureextractionresultsforbugswhichhavesystemloganomalies.
representstheanomalypatternandtherootcausefunction cannot be identified.
for each method we present the root cause function s rank using causal analysis.
bug id signature anomaly pattern root cause function number of candidate functionsperfsigdbscanembeddingsom tfidfdbscan tfidftopic lda hadoop 11252infinite log sequence timespan due to missing closing server log rpc.waitforprotocolproxy91 mapreduce 5066infinite log sequence timespan due to missing closing server log jobendnotifier.localrunnernotification81 mapreduce 4089infinite log sequence timespan due to missing stopping service log pingchecker.run79 mapreduce 3862infinite log sequence timespan due to missing stopping service log deletionservice.delete81 hdfs hdfs 1490infinite log sequence timespan due to missing closing server log transferfsimage.getfileclient93 hadoop 9106abnormal log sequence timespan client.call hbase 17341abnormal log sequence timespan replicationsource.terminate hdfs 4301abnormal log sequence timespan transferfsimage.getfileclient capturethesemanticsofeachword.theembeddingrepresentation converselycapturesthesemanticinformationbymodelingthecontextual surrounding words.
as system logs are written in a human readable format for developers to diagnose problems of the system the advantage of embedding representation over tfidf is clear and also is observed from the experimental results.
we observe semantic grouping works well because the system log semantics are much simpler than natural language textual data.
som and dbscan clustering have similar performance on tfidf representation becausenumberoftfidfdimensionsisnotlarge.logentries typically are short sentences with limited key words.
next we replace the classification framework with topic extraction model i.e.
topic lda which is another kind of semantic analysis.
we observe a much worse performance compared with the other methods.topic ldahas poor performance becausethe log entries are usually very short.
according to a study on textdocumentsfrommicro bloggingplatformtwitter theperformanceoftopic ldadegradeswhentheinputtextdocuments are short.
according to it is hard for topic lda to extract semantics from short documents as topic lda cannot obtain sufficient statistics from short documents.
perfsig considers all functions invoked around the performance alert detection time as candidate root cause functions and ranks all candidaterootcausefunctionsbasedonthemiscores.asshownin table2andtable3 for80 bugs thenumberofcandidatefunctions exceeds .
in our experiments the true root cause functions have the highest mi scores higher than the second rank candidate functions on average in out of bugs.
1677icse may pittsburgh pa usa jingzhu he et al.
table4theruntimeoverheadanddiagnosistime.hadooprepresentsthefourhadoopsystem i.e.
hadoopcommon hadoop mapreduce hadoop hdfs and hadoop yarn.
system workloadtracing overheadmetric anomaly detection timelog anomaly detection timecausal analysis time hadoop calculation0.
.
.01s0.
.26s .
.16s hbasedatabase query0.
.
.01s0.
.01s0.
.002s cassandradatabase query1.
.
.01s0.
.03s .
.01s reflectionutils class public static t t newinstance ... setconf result conf public static void setconf ... setjobconf theobject conf private static void setjobconf ... class ?
jobconfclass conf.
getclassbyname org.apache.hadoop.mapred.jobconf configuration class public class ?
getclassbyname ... throws classnotfoundexception return class.forname name true classloader duplicated costly operations to search for the same configuration class figure the code snippet of the hadoop bug.
.
overhead and diagnosis time table4showstheruntimeoverheadanddiagnosistime.theruntimetracingoverheadisbelow2 .thediagnosistimeareallless than one second.
note that although log anomaly detection uses thedeeplearningmodel i.e.
wordembeddingmodel thediagnosis time is still very short because log entries are typically short sentences with repeated words.
.
case studies wehavedescribedhadoop 11252bug srootcauseinsection1.
.
perfsigidentifiesthelogsequencewhichstartsfromthefirstlog entryofstartingservertothelastlogentryofstoppingserveras in figure .
perfsig identifies the bug as the missing log pattern.
perfsig extracts the log sequence time series during normal run and regard the log sequence s time span as infinity due to missing logduringbuggyrun.perfsigperformscausalanalysisonallthe function call time series and the log sequence time series.
the time series are shown in figure .
perfsig determines the root cause functionas rpc.waitforprotoproxy becauseithasthelargestmi value.
hadoop bug is caused by duplicated costly operations.
as showninfigure11 whenweuse reflectionutils.newinstance function to initialize a new reflection instance at line setconf functionisinvokedatline117tosetthejobconfiguration.
setconf callssetjobconf functionatline64 thencallsthe conf.getclass !
figure logs generated by hdfs bug.
bynamefunctionatline81tofindaparticularconfigurationclass.
however thejdkfunction class.forname invokedby conf.getcl assbyname iscostly.whentherearemultiplethreadswhichinitializetheinstances class.forname isfrequentlyinvokedtosearch for the same configuration class which is unnecessary.
when the bug is triggered we observe 80x slowdown in the system.
perfsig identifies the bug anomaly pattern as the persistent increases because class.forname consumes a lot of cpu resources as shown in figure 9a.
perfsig then performs causal analysis on the cpu utilization time series and all the function time series.
as showninfigure9b persigdeterminestherootcausefunctionas conf.getclassbyname because it has the largest mi value.
negative case study hdfs bug is caused by missing timeout for dfsclient newconnectedpeer .
when this bug happens theresourcemanagerhangsonwaitingresponsefromthe hdfs cluster.
we observe that the logs of shutting down hdfs clusterismissingasshowninfigure12.perfsigfailstoclassifythe logentriestothesamecluster.forexample thefirsttwologentries are grouped in cluster a which represents datanode s tasks.
since the log entries of the log sequence are classified to different tasks we cannot extract the right log sequence from the log clusters.
threats to validity benchmark bias for the experimental evaluation we have reproduced20performancebugs whichareallthebugswecanreproduce within a time limit.
up to the submission we have exhaustively searchedthebugreportsinsixcommoncloudsystemsfromjira selected all the true performance bugs and tried our best to reproduce them.
parameter bias the choice of several hyperparameters in the designcanintroducebiasonthesystemefficacy suchasthethresholdofthelowpassfilter.inourexperiment weadjustthehyperparameters several times and choose the best one.
the experimental resultsshowthatourdesignislesssensitivetothehyperparameters than the alternative approaches.
related work in this section we discuss the existing work in the literature.
single modality data analysis previous work has worked on bug diagnosis by performing single modality data analysis.
cohenetal.
leveragedtree augmented naivebayesmodels and clustering methods to extract signatures from system metrics.
perfscope perfcompass andtscope diagnosedperformancebugsbyperformingunsupervisedmachinelearningon systemcalltraces.stitch andlprof reconstructedthedomainknowledgeandsystemmodelfromthelogs.cloudseer reconstructedtheexecutionworkflowentirelyfrominterleaving 1678perfsig extracting performance bug signatures via multi modality causal analysis icse may pittsburgh pa usa openstacklogs.plelog performedsemi supervisedlearning combininghdbscanclusteringandprobabilisticlabelestimationto detect log anomalies.
logfaultflagger extracted vectors based on tf idf method and applied the knn classifier to identifying abnormal logs.
deeplog applied a deep neural network model i.e.
long short term memory lstm to detect anomalies from the system logs.
the mystery machine analyzed logs from internet service to diagnose facebook request latency.
csight modeled the system behavior in the form of cfsm from system logs.kabinnaetal.
appliedmachinelearning basedmethods to determine the change risk of system logs based on certain metrics such as file ownership and log density.
li et al.
adopted the lda method to extract topics from source codes automatically and studied the likelihood of topics to be logged.
lou et al.
constructed program workflow from event traces to understand system behaviors and verify system executions.
pbi and rept leveragedhardwaretraces i.e.
performancecountersandintel processortrace tounderstandthesoftwarebugs.perfsigperforms multi modality analysis to address the limitation that performance bugsmanifestindifferentdatatypescomparedwiththeexisting work.
performance bug diagnosis and fixing previous work has proposeddetectionandfixingsolutionsforperformancebugs.hang doctor detectedsofthangsatruntimetoaddressthelimitations of offline detection.
perfchecker and hangwiz automatically detected soft hang bugs by searching the application code for known blocking apis.
yang et al.
and he et al.
p r e sented comprehensive empirical studies and detection solutions for two kinds of performance bugs i.e.
database backed web application performance bugs and configuration related performance bugs.perfdebug appliedadataprovenance basedtechnique to diagnose performance issues in applications that exhibit computation skew.
dscope and hangfix adopted pattern driven approachestodiagnosingandfixingsoftwarehangbugsincloud systems.persigcomplementstheexistingworkinprovidingamultimodality signature extraction framework to depict performance bugs in a comprehensive fashion.
causal analysis causal analysis attracts much attention in softwaredebuggingrecently.forexample unival transformed branchandlooppredicatesintovariablesandappliedstatisticcausal analysis to infer the faulty component.
reptrace performed causality analysis on system call traces to identify the execution dependencies.
compared with the existing work perfsig performs multi modality causal analysis among different types of data to extract the root cause functions of performance bugs taking a further step in applying causal analysis in software debugging.
causalanalysisontimeseriesdatatypicallyfocusedonidentifying granger causal relationship among different time series .mccracken presentedacomprehensivereviewto performexploratorycausalanalysis.thecausalanalysistechniques couldbedividedintotwocategories i.e.
regression basedandinformationtheory basedmethods.fortheregression basedmethod arnold et al.
adopted linear lasso regression for identifying granger causal relationship among time series.
tank et al.
explored the possibility of detecting non linear granger causal relationship among time series by training deep learning models i.e.
multi layer perceptrons and recurrent neural networks with sparsity constrain.
for the techniques using information theory the granger causalrelationshipwasdetectedbyentropy basedmeasures .
existing work leveraged different kinds of measures likedirectedinformationtheory ortransferentropy .
perfsigmakesthefirststeptoapplytheinformationtheorymethod mutual information to performance bug signature extraction.
conclusion inthispaper wepresentperfsig anautomaticperformancebugsignatureextractiontool.perfsigcananalyzevariouskindsofmachine data including system metric system logs and function call traces to identify principal anomaly patterns and root cause functions as unique signature patterns for representing performance bugs.
we have implemented a prototype of perfsig and conducted extensive evaluationsusing20realworldperformancebugsonsixcommonly used cloud systems.
our results show that perfsig can successfully extract unique signatures for out of tested performance bugs.
perfsig imposes low overhead to the cloud system which makes it practical for production environments.
data availability thedata andtheimplementation ofperfsigare publiclyavailable at