multilingual training for software engineering toufique ahmed university of california davis davis california usa tfahmed ucdavis.edupremkumar devanbu university of california davis davis california usa ptdevanbu ucdavis.edu abstract well trainedmachine learningmodels whichleveragelargeamounts of open source software data have now become an interesting approach to automating many software engineering tasks.
several se tasks have all been subject to this approach with performance graduallyimprovingoverthepastseveralyearswithbettermodels andtrainingmethods.more andmorediverse clean labeled datais betterfortraining butconstructinggood qualitydatasetsistimeconsuming andchallenging.
ways ofaugmenting the volumeand diversityofclean labeleddatagenerallyhavewideapplicability.for some languages e.g.
ruby labeled data is less abundant in others e.g.
javascript theavailabledatamaybemorefocusedonsomeapplicationdomains andthuslessdiverse.asawayaroundsuchdata bottlenecks wepresentevidencesuggestingthathuman written code in different languages which performs the same function israthersimilar andparticularlypreservingofidentifiernaming patterns we further present evidence suggesting that identifiers areaveryimportantelementoftrainingdataforsoftwareengineeringtasks.weleveragethisratherfortuitousphenomenontofind evidence that available multilingual training data across different languages canbeusedtoamplifyperformance.westudythisfor different tasks code summarization code retrieval and function naming.
we note that this data augmenting approach is broadly compatiblewithdifferenttasks languages andmachine learning models.
ccs concepts software and its engineering software notations and tools computing methodologies machine learning.
keywords codesummarization codesearch methodnameprediction deep learning acm reference format toufique ahmed and premkumar devanbu.
.
multilingual training for software engineering.
in 44th international conference on software engineering icse may21 pittsburgh pa usa.
acm newyork ny usa pages.
this work is licensed under a creative commons attribution international .
license.
icse may pittsburgh pa usa copyright held by the owner author s .
acm isbn .
introduction researchers in the nlp area have reported that multilingual training is beneficial for low resource language .
several papersshowthatmultilingual trainedmodelsshowbetterperformance and are more practical to deploy .
however this is observed in two situations for low resource languages and when the languages are related.
we find that programs in different languagessolvingthesameproblemusemoresimilaridentifiers furthermore different languages sometimes have similar keywords andoperators.highcapacitydeeplearning models arecapableof learninginterlingua sharedsemanticrepresentationbetweenlanguages .moreover withtaskslikesummarization ormethod naming we are dealing with a simplified many to one setting translating multiple source languages to a single target language which is believed to be easier than multi way task .
we beginbyintroducingthecodesummarizationtask whichweuse to motivate multilingual training.
developersoftenrelyheavilyoncomments togainaquick even if approximate understanding of the specification and design ofcode they are working on.
an actual example of a comment is showninfigure1.suchcommentshelpadevelopergainaquick mental preview of whatthe proximate code does and howit might goaboutit thishelpsthedeveloperknowwhattolookforinthe code.
knowing that such comments are useful to others or evenlatertooneself incentivizesdeveloperstocreatecommentsthatexplain the code however the resulting redundancy viz.
code that does something and some nearby englishtext that describes justwhat thecodedoes with thesameconcept expressedintwo languages results in a bit of extra work for the original coder.
thisextrawork ofcreatingalignedcommentsexplainingthecode canbefruitfullyviewed asataskrelatedto naturallanguage translation nlt e.g.
translating english to german .
the mature powerful technology of nlt becomes applicable for comment synthesis mlapproachesdevelopedfortheformercanbeusedfor the latter.
an effective comment synthesizer could help developers bysavingthemthetroubleofwritingcomments andperhapseven be used on demand in the ide to create descriptions of selected bits of code.
comment synthesis is now an active research area including manyprojectssuchascodenn deepcom astattgru codebert rencos secnn plbart cotext prophetnet x ncs code2seq re2com andmany more .
all these approaches rely on datasets of aligned code comment pairs.
typically these datasets are then used to train complex deep learning models to model a probabilistic distribution of the form p comments code onecansamplefromthese usuallygenerative modelstocreatecandidatecommentsforagivenapieceofcode.
givena datasetofcode comment pairsina specificlanguage e.g.
ieee acm 44th international conference on software engineering icse icse may pittsburgh pa usa toufique ahmed and premkumar devanbu java or python or php or ruby one can train models to translate code inthatlanguage to comments.
the quality of the translation will depend largely upon the inductive power of the model and quality and diversity of the code comment dataset.
returns the text con tent of this node and its descendants.
public string gettextcontent stringbuilder sb newstringbuilder g etchildnodescount appendtextcontent sb return sb.tostring figure example for code comment generation task oflate giventhepowerofgpus andthecapacityofthemodels the limitations largely arise from dataset quality and diversity especiallyinlanguagesforwhichlimited orratherspecializeddata is available.
for instance codexglue dataset consists of six languages i.e.
ruby java javascript go php python .
mostlanguages havewell over 000training examples covering awide set of application domains.
some languages particularly ruby and javascript have far fewer examples and cover a narrower range of applicationdomains.asaresult state of the artmodelsperform less well for these two languages.
this is a well known problem fornaturallanguagetranslation whiletrainingdataforlanguage pairslike english frenchisabundant resourcesmaybelacking for less used languages like quechua orbadaga.
in such cases a common technique is adapt ml models to learn useful statistics fromabundantdatainother perhapsrelatedlanguages .this workswellwhenlanguagesoftenhavesimilargrammars andshare common word etymologies.
weproposeananalogousapproachtoimprovethediversityand qualityoftrainingdataforsoftware engineeringtasks exploiting an interesting property of source code that human beings write.
it s generally agreed that variable names help code comprehension .developersknowthis andtypicallychoosedescriptive variable names reflective of code logic and purpose regardless of the language they are coding in.
thus one could expect that developerscoding thesamefunctionality usingsimilaralgorithms even in different languages will use similar variable names .
this suggests that machine learning approaches could sometimes leverage corpora in different programming languages.
this paper a shows that this expectation actually has a sound empirical basis and then b demonstrates that this approach in fact works not just for code summarization but also for several other tasks.
we make the following contributions.
using the rosettacode dataset we provide evidence that programssolvingthesameproblemindifferentlanguages are more likely to use the same or similar identifier names.
weshowevidencesuggestingthatcross languagetraining e.g.
trainonpython testonruby cansometimesleadto better performance than same language training.
westudy therelativevalueofidentifiers andsyntax using ablation and find that identifier names may matter more.
we show that pooled multilingual training data improves performanceonseveraltasks butespeciallyforlanguageslacking in diverse and abundant data.
we top a leaderboard for code comment synthesis1.
weshowthatmultilingualtraininghelpsfortwoothertasks code retrieval andmethod name prediction .
finally we evaluate a few different design choices for multilingual training and discuss threats to our findings.
overall this paper a shows that multilingual training is yet anotherusefultechniqueinthegeneralarsenalofmlapproaches to exploit the naturalness of code b shows why it is useful and c shows how to take good advantage of it.
note technicaldetailsfollow butprecisely whatwestudyhereis multilingual training in the fine tuning stage of foundation models .foundationmodelsforcode likecodebert graphcodebert already use multilingual data for pre training .
while pre training is self supervised and is done with unlabeled corpora task specific fine tuningis usually supervised usingclean hardwon labeled data multilingual pooling can be useful here.
background motivation we now present some motivating evidence suggesting the value of multilingualtrainingdatafordeep learningapplicationstosoftware tasks.
we begin the argument focused on code summarization.
deep learning models have been widely applied to code summarization with papers reporting substantial gains in performance over recent years .
we focus hereon what information in the code ml models leverage for summarization while we use summarization to motivate the approach we evaluate later on different tasks .
does every token in the program under consideration matter for the code summarization task?
or are the function and variable names usedintheprogramsmostimportant?sinceidentifierscarrymuch information about the program this may be a reasonable assumption.
consideringthecontentwords2intheexampleinfigure1there arefourmajorterms i.e.
returns textcontent node anddescendants used in the summary.
the first directly occur as tokens or subtokens in the code.
though the word descendants is missing in the program high capacity neural models like bert can learntostatisticallyconnect e.g.
descendant withtheidentifier subtoken child .
this suggests that perhaps comments are recoverable primarily from identifiers.
if this is so and identifiers matter more for comments than the exact syntax of the programming language thatmayactuallybeverygoodnewsindeed.ifdevelopers choose identifiers in the same way across different languages viz.
problem dependent rather than language dependent perhaps we canimprove thediversityandqualityofdatasetbypoolingtraining set across may languages.
pooled data sets may allow us to finetune using multilingual data and improve performance especially forlow resourcelanguages e.g.
rubyandjavascriptfrom codexglue .
since this is a core theoretical background for our 1this claim is based on publicly available evidence.
please check content words in linguistics are words that carry meaning as contrasted with functionwords suchasprepositions pronouns andconjunctions whichdenotegrammaticalrelationships.see consider function words to be keywords operators and punctuations and content words to be identifiers functions variables types etc 1444multilingual training for software engineering icse may pittsburgh pa usa work we start off with two basic research questions to empirically gauge the possibility and promise of multilingual fine tuning.
rq1what role do identifiers play in for code summarization?
rq2do programs that solve the same problem in different languages tend to use similar identifier names?
.
rq1 role played by identifiers we first examine the importance of identifiers for code summarization specifically we compare the relative value of identifier tokens and other tokens.
we use the codexglue dataset and pre trained codebert embeddings for the task .
we begin with a brief backgrounder on codebert bert .
codebertusesthepre training fine tuningstrategyofbert roberta etc .
this approach begins with a self supervised pre training step to learn textual patterns from a large unlabeled corpus using just the content in the next step fine tuning task specific labeleddataisusedtoprovidetask relatedsupervised training.thisapproachisknowntoachievestate of the artperformanceinbothnaturallanguageprocessing andsoftware related tasks .
we study the effect of identifiers in several steps.
for the pretraining step we start with the available codebert model which is pre trained on a large multilingual corpus of code.
for the finetuning step for this task we use the codexglue benchmark dataset seetable forlanguages and datasetsizes we startwith the original set of code comment pairs and apply two different treatments to create overall three different fine tuning training datasets basecaseleavingcodeasis atreatmenttoemphasize identifiers and a treatment to de emphasize them.
first toemphasizeidentifiersweabstractouttheprogram skeywords separators and operators by replacing those with three generic tokens i.e.
key sep and opt thus forcing the model during fine tuning to rely more on the identifiers for the task.
next to assesstheimportanceofkeywords separators andoperators we abstract out the identifiers with a generic token id .
we fine tune the model separately after each of these abstraction steps thus yielding3fine tunedmodels thebaseline keyword abstracted and identifier abstracted.wecomparetheresults smoothedbleu across all three.
ifafine tunedmodel sperformanceisrelativelyunaffectedby anabstraction onemayinferthatthemodelrelieslessontheabstractedtokens.weperformtheseexperimentswithtwolanguages withlow resource i.e.
rubyandjavascript seetable4 andtwo languages with high resource i.e.
java and python .
we train validate and test with the same dataset in each case.
for each test instance wehaveonevaluefromthecompleteprogramandanother one fromeach of the twoabstracted versions.
wecompared these values using two distinct pair wise wilcoxon tests alternative hypothesis ah complete program identifier de emphasis ah complete program identifier emphasis.
we also perform the same test with the keyword abstracted and identifier abstracted versions ah identifier emphasis identifier de emphasis .
thedata suggeststhatabstractingthekeyword separator andoperatorhasasmallerimpactontheperformance the bleu 4scoresarerathersimilar with effectsize rangingfrom0.
to0.
tothosefromtheunabstractedcode.ontheotherhand datasetcomplete programabstracting keyword operator separatorabstracting identifiers bleu bleu 4effect sizep value adjusted bleu 4effect sizep value adjusted ruby .
.
.
.
.
.
.
javascript .
.
.
.
.
.
.
java .
.
.
.
.
.
python .
.
.
.
.
.
table role played by identifiers whende emphasizingidentifiers theperformancedropsmore with effect sizes 5x 100x larger.
we find similar results while comparing the emphasizing and de emphasizing identifiers versions omitted for brevity .
languagetraining ruby javascript java go php python testingruby .
.
.
.
.
.
javascript .
.
.
.
.
.
java .
.
.
.
.
.
go .
.
.
.
.
.
php .
.
.
.
.
.
python .
.
.
.
.
.
table intra and inter language training and testing theresultsintable1suggeststhatsyntaxislessrelevantthat identifiernames.inallthepriorworks thetrainingandtestingwere doneinthesamelanguage.sincesyntaxislessimportant couldwe trainandtestwithdifferentlanguages?thecodexgluedataset enables just such an experiment.
using six different languages we apply a codebert model fine tuned in each language to a testsetinanotherlanguage.table2showsthatforhigh resource languages i.e.
java go php andpython weachievethebestresult diagonal when training and test data are from the same language.
however the performance does not degrade to a very large extent when trained with one language and tested on a different one.
surprisinglyweobservethatforrubyandjavascript weactually achievehigherperformancewhiletrainedwithjava php andpython than the language itself .
that indicates that code summarization isnotcompletelydependentonsyntax perhapsitreliesmoreon identifier similarity which we shall explore next finding .
code summarization sometimes appears to train quite well with data sets from other languages even if the syntax is different.
.
rq2 identifier similarity across languages here we evaluate rq2 given a problem do developers choose similar descriptive identifiers regardless of the programming language?
based on the findings in the previous section if identifiers were indeed used in similar ways perhaps code comment pairs from any programming language could help train a code summarization model for any other language .
as an example figure presentsthatallthe indexof functionsimplementedinjava php and javascript use very similar identifiers needle and haystack .
1445icse may pittsburgh pa usa toufique ahmed and premkumar devanbu quantitatively evaluating this hypothesis requires multiple implementations of the same problem in different programminglanguages where we couldcompare identifier names.
luckily rosettacodeprovidesjustsuchadataset.
rosettacode currentlyconsists of tasks draft tasks and includes languages3.w e collectthemineddata4andstudythesamesixlanguages i.e.
ruby javascript java go php and python in the codexglue dataset.
we get cross language pairs from six languages and measure identifier similarity between pairs of programs which solve the same problem in each language e.g.
programs for graph diameter problem in java and ruby .
for baselining we also compare witharandompair solvingdifferentproblems forthesametwo languages e.g.graph diameter in java and sha hashing in ruby .
fortunately wefoundsufficientsamplesizesforallourlanguage pairsinrosettacode .forexample forjava pythonwefind544 matchedprogrampairssolvingthesameprobleminbothlanguages.
we then take the java programs and randomly pair them with other python programs.
therefore we have two groups of programs i.e.
sameprogramimplementedindifferentlanguages anddifferentprogramsimplementedindifferentlanguages and we check the similarity level between the two groups.
note that size unrestricted random pairing may yield misleading results.
suppose we have a java python program matched pair with java subtokens and python subtokens.
now if we replace the matched python program with a random bigger program e.g.
subtokens we may have more chance of finding matched identifiers.therefore whilechoosingtherandomprogram wetryto ensureithasasimilarlengthtotheprogramitisreplacinginthe pair.
we randomly select a program having the subtoken counts withina5 lengthrange e.g.
42subtokensfora40subtoken program oftheremovedone.fortunately in .
cases wegetat leastoneexamplewithinthe5 range.ontheremaininginstances we select the program with the nearest subtoken count.
we measure identifier similarity thus remove all keywords operators and separators from the programs.
break all camelcase and snake case identifiers and keep only one copy of each sub token.
discard too small programs with less than sub tokens.
calculate the mean szymkiewicz simpson coefficient overlap coefficient for both groups i.e.
same program pair and random pair of programs.
repeatthisprocessacrossall15languagepairs forallprogram pairs.
table shows the common program pairs have additionalidentifier overlapcompared to random program pairs.
we compare the matched and random pair overlaps using the nonparametricwilcoxonsigned ranktest ah randomhaslessoverlap thanmatched .weobservethatthenullhypothesisisrejected and szymkiewicz simpsonoverlapcoefficient5issignificantlyhigher 3last accessed august 5this is a measure of similarity like the jaccard index we use it here since sometimes the sizes of the programs are quite different.
it s calculated as x y min x y .for the common program pairs in all the cases.
that indicates programssolvingthesameproblem evenindifferentlanguages are much more likely to use the same or similar identifier names.
language pair ofcommon programsoverlap coefficient effect sizep value adjusted for random programsfor common programsincreased in java python .
.
.
.
.
java ruby .
.
.
.
.
java javascript .
.
.
.
.
java go .
.
.
.
.
java php .
.
.
.
.
python ruby .
.
.
.
.
python javascript .
.
.
.
.
python go .
.
.
.
.
python php .
.
.
.
.
ruby javascript .
.
.
.
.
ruby go .
.
.
.
.
ruby php .
.
.
.
.
javascript go .
.
.
.
.
javascript php .
.
.
.
.
go php .
.
.
.
.
overall .
.
.
.
table cross language identifier similarity when functionality is preserved we also calculate each pair s jaccard index similarity coefficient and find more similarity between common pairs than random ones thus giving essentially the same result.
however we prefer to report the detailed result using the overlap coefficient because jaccard index can be affected by the differing verbosity of languages.
for example on average java python and rubyprogramsin rosettacode have29.
.
and17.63identifier subtokens.
java has higher subtokens compared to python andrubybecauseoftheimportstatements packagenamingetc.
therefore jaccard index between java and python will be lower thanthatofpythonandrubyeveniftheprogramsuseverysimilar identifiers.
finding .
for a given problem developers are likely to choose similar identifiers even if coding in different languages.
in this section we have presented evidence suggesting that a identifiers are important for code summarization that b crosslanguagetrainingispromising andalsothatc identifierstendto be used in similar ways across languages.
taken together these findings present a strong argument to try multilingual fine tuning for se tasks.
note that it is already well established that multilingual pre training is helpful and most bert style se pre trained modelsare multilingual .however pre training data areunsupervisedandeasytocollect.preparingcleandataforthe supervised fine tuning phase requires more time and attention.
in thispaper ouraimistoprovethatmultilingualtrainingisnotonly effectiveinpre trainingstagebutalsoinfine tuningstageforse models whichisalreadyfoundtobebeneficialfor naturallanguage models .
benchmark datasets and tasks we evaluate the benefits of multilingual training in the context of several tasks and associated datasets.
in this section we discuss the models and tasks used for our experiments.
1446multilingual training for software engineering icse may pittsburgh pa usa public static int indexof bytebuf needle bytebuf ha ystack todo maybe use boyer moore for efficiency.
intattempts haystack.r eadablebytes ne edle.readablebytes for inti i attempts i if equals needle needle.readerindex haystack haystack.
readerindex i needle.readablebytes return haystack.rea derindex i return a java public static function indexof string haystack string needle int offset int pos self strpos haystack needle o ffset return is int pos ?
pos b php function indexof haystack needle if typeof hays tack st ring return haystack.indexof needle for let i j l haystack.length n needle.length i l i if haystack needle j if j n return i j else j return c javascript figure2 usageofsimilaridentifiers e.g.
needle haystack in indexof function in different programming languages .
the models forourstudyofmultilingualtraining weadoptthebert or foundationmodel paradigm.foundationmodels have two stages i unsupervised pre training with corpora at vast scale andii fine tuningwithasmallervolumeofsuperviseddataforthe actual task.
foundation models currently hold state of the art performanceforagreatmanynlptasks.bert stylemodelshave also been adapted for code pre trained on a huge multilingual corpora and made available codebert and graphcodebert arebothfreelyavailable bothsourcecodeandpre trainedmodel parameters.
while these models for code have thus far generally been fine tuned monolingually they provide an excellent platform fortrainingexperimentslikeours tomeasurethegainsofmultilingualfine tuning.codebert graphcodebertuseamulti layer bidirectional transformer based architecture and it is exactly assameastheroberta with125mparameters weexplain them further below.
pre training thecodebert dataset hastwoparts amatchedpairspartwith2.1mpairsoffunctionandassociatedcomment nlplpairs and6.4msampleswithjustcode.thecodeincludesseveral programming languages.
it was created by hussain et al.
.
codebertmodelispre trainedwithtwoobjectives i.e.
masked language modelingand replacedtoken detection on bothparts.mask language modeling mlm is a widely applied and effective training objective where a certain number of tokensaremaskedout andthemodelisaskedtofindthosetokens.
forcodeberttraining feng etal.applythisfirstobjectiveonlyto bimodaldata .thesecondobjective replacedtokendetection rtd isabinaryclassificationproblemthatisappliedtoboth unimodalandbimodaldata.twodatagenerators i.e.
nlandpl generate plausible alternatives for a set of randomly masked positions andadiscriminatoristrainedtodeterminewhetherawordis the original one or not.
we note that codebert pre training is all aboutrepresentation learning bylearningtoperformthetaskwell the model learns a good way to encodethe text which is helpful during the next fine tuning stage.
thepre training took about hoursonamachinewith16nvidiav100cards andwouldhave takenusverymuchlonger soweweregratefultobeabletojust download the estimated parameters.
pre training graphcodebert graphcodebertaugmentssourcecode with data flow during pre training.
it uses a simple data flow graph dfg encoding a where the value comes from relation betweenvariables .thedfgnodesarevariableoccurrences edges arevalueflow.graphcodebertpretraininglearnsajointrepresentation of the dfg structure dfg alignment with source code and thesource code tokensequences.
graphcodebert is therefore pre trained with three training objectives i.e.
edge prediction node alignment and mlm on .3m functions pl nl pairs from codesearchnet dataset.
for details see .
the pre training fine tuning approach relies on very high capacitymodels andarepre trainedoveralarge multilingualcorpus.
thus evenbeforefine tuning themodelsalreadyknowalotabout each language.
thus fine tuning on many languages should not negatively impact what the model knows about any one language.
thuswefindthatmultilingualfine tuningimprovesonmonolingual fine tuning in most cases.
we believe our proposed approach would still consider the context surrounding the individual programming language even after multilingual training because these models have sufficient capacity to do so.
we now describe our tasks in each we describe the task the dataset and the multilingual fine tuning approach if applicable .
.
code summarization the task asdescribedearlier thegoalistogenerateanlsummary given code in some pl.
the dataset thereareseveraldifferentcodesummarizationdatasets we chose codexglue6 for two main reasons codexglueiscarefullyde duplicated .priordatasets like tl codesum have duplicates in training testing andvalidationpartitions.duplicationcaninflatemeasured performance .
we need a multilingual dataset to prove the effectiveness of multilingualfine tuning.noneoftheexistingdatasets is multilingual.
table presents the number of training testing and validation instances for each language.
in codexglue.
6codesearchnet datasetisastandardbenchmark whichhasbeenincorporated into codexglue 1447icse may pittsburgh pa usa toufique ahmed and premkumar devanbu programming languagetraining dev testcandidate codes ruby javascript java go php python candidate codes are only used for code retrieval task table codexglue dataset model fine tuning fenget al.use a transformer based encodedecoder architecture for the code summarization task .
the encoder is all ready well trained in the pre training stage for finetuning theencoderisprimedwithweightsfrompre training.now the transformer model is given the input code token sequence and asked to generate the comment as in the neural machine translation nmt problem.wefine tuneusing the codexglue paired samples.
during fine tuning the decoder is trained autoregressively using next token cross entropy loss.
feng et al.use smoothbleu fortheevaluationsofthemodels.subsequently we replace the pre trained codebert with pre trained graphcodebert in the encoder while evaluating the effectiveness of multilingual fine tuning with graphcodebert.
why baseline with codebert for code summarization?
fenget al.
comparecodebertwithotherpopularencoder decoderbased e.g.
lstm transformer roberta models codebert handily beats all of them .
thus codebert is a good baseline to measure the value of multilingual finetuning.
codebert also doesverywellonpriordatasets usingsmoothedsentencebleu we found that codebert reaches .
on tl codesum and .
on funcom .
tl codesum has high degree of duplicates wefoundthatfuncomalsodoes butjustinthecomments.
codexgluehasverylittleduplication whichmakesitmorechallenging and also more reliable.
note that graphcodebert does not report anyperformanceonthecodesummarizationtask andsowehad to measure it.
.
code search the task given a natural language query find the semantically closest code sample from a large set of candidates.
vector based information retrieval methods can be used here along withbertstyleencoders.codebertwasshowntoperformquitewell the best publishedperformance isreported by graphcodebert codebert augmented with graph representations .
we study the valueofmultilingualfine tuningforbothcodebertandgraphcodebert pre trainingofbothmodelswasdiscussedearlierin section .
.
the dataset guoetal.adaptthesamecodesearchnet dataset withsome additionaldatafor candidatecodes .note thatitis basically the same dataset we used for code summarization except the candidate codes.
7as reported in measurement approaches vary across papers and these numbersmaydifferfrompriorresults weusesmoothedsentencebleu 4everywhere in our paper.model fine tuning weuseguo etal.
sgraphcodebertmodel which at the time of submission is the best performing model with code and parameters available and so is fine tunable.
the finetuning data is code pl matched with nl comments from codexglue.
the pre trained graphcodebert embedding vector is calculatedforeachplandnlpart.duringfine tuning guo etal.
takeaminibatch of say n nlqueryvector alongwith n correct answers pl answer vectors.
n2dot products are calculated the embeddingvectorsarethenfull stacktrainedtogive normalized dotproductforthematches and forthemis matches.forthe actualretrieval graphcodebertcalculatesthevectorembedding of a given query and simply retrieves candidates ranked by the dot product distance from the query vector.
.
method name prediction the task as introduced by allamanis et al.
as the extreme summarization problem thetaskistopredictthefunctionnamegiven the body.
the dataset we adaptthe codexglue datasetby extractingthe function name and asking the model to find the name given the functionbody.following thefunctionnamesarebrokeninto subtokensusingbpe we veusedbpetokenizationforalltasks .
this problem then becomes very similar to code summarization.
model fine tuning previously code2seq and code2vec have worked on this problem.
all prior works use a monolingual datasets which are not suitable for our experiment.
we use thesamemodelweusedforsummarization exceptwenowlearn tosequentiallygeneratethemethodname subtokenbysubtoken.
weusef1 scorefortheevaluation.forexample thefunctionname createlocal is broken into two sub tokens i.e.
create and local andthemodelpredictsonly create .hence theprecision recall and f1 score are .
.
and .
respectively.
results inthissection weevaluatemultilingualfine tuningforthebaselines for the tasks enumerated above.
.
code summarization we apply multilingual fine tuning on the codexglue dataset.
we firstreplicatethesummarizationtaskby monolingually finetuningtheavailablepre trainedcodebertmodelforsixlanguages8.
we replicate the fine tuning stage for reasons we want to account for any hardware or environmental bias e.g.
wehaveadifferentsetofgpusthantheoriginal paper.wefine tunewithnvidiatitanrtx whilefeng et al.
use nvidia tesla v100 .
weuseapairwisetwo samplestatisticaltest asdescribed in it ismoreprecise thanjustcomparingtest set summary statistics to gauge differences.
this requires a performance measurement for each test sample which the repository did not include.
8we use the publicly available codebert implementation and dataset com microsoft codexglue tree main code text code to text 1448multilingual training for software engineering icse may pittsburgh pa usa our bleu numbers for monolingual training were close to reported numbers with some differences but we do obtain the same overall score .
table leftmost columns .
we use the same per language test sets to compare monolingual and multilingual fine tuning.
the validation set however is a singlemultilingualonecombiningallthemonolingualvalidation sets.
table shows that multilingual fine tuning improves performance even for high resource languages with more than 100k traininginstances .withcodebert multilingualfine tuninggains .
.
overmonolingualfine tuning foralllanguages yieldinga6.
overallimprovement .
weightedimprovement .
with themore advanced graphcodebert wesee smaller gains although the relative gains span a wide range.
weuseaone sided ah monolingual multilingual pairwise wilcoxon signed rank test thus avoiding the corpus level measurementpitfallsnotedin .nullhypothesisisrejectedforall sixlanguages forcodebert.for graphcodebert it s rejected overall and for every language except for javascript where the p value is .
all after b h correction .
thusourmeasurementindicatesthatmultilingualfine tuning providesastatistically significantimprovementovermonolingual training.
we find rather low effect sizes using cliff s delta .
while we report the effect size for the sake of completeness this is notamajorconcern wenotethat allgainsarestatisticallyhighly significant .
we also emphasize that even the minor improvements providedherebymultilingualtraining whichisbroadlycompatible witharangeofsettings constitutearelevantandpotentiallywidely useful result.
roy et al have previously noted that small gains in bleu may not be perceptible to humans as increased text quality nevertheless we note that natural language translation whichisnowwidelyused attainedhighperformancelevelsbased on decades of incremental progress this result and others below provideevidencethat multilingualtrainingcouldbe animportant stepintheprogresstowardsmoreusefulautomatedtools.finally we notethat bleu 4gains are higherfor low resource language e.g.
.
forruby andlowerforhigh resourcelanguages e.g.
.
for python as expected.
comparing multi lingual codebert with other models code summarization is widely studied there are many models for this task our specific focus here is to understand if multilingual fine tuning provides benefits using a high quality token sequencemodel and dataset.sowefocuscomparisonsonthepaperswhichreportperformanceoncodexgluedataset anduseatoken sequenceinductive bias comparingagainstallmodelsisbeyondthescopeofthispaper.
we compare multi lingual codebert polyglotcodebert and graphcodebert polyglotgraphcodebert with other models that have been published in peer reviewed venues among them four apply pre training strategies .
we achieve the best overall performance outperforming all the models and for four specific languages i.e.
ruby java go and php .
there is one other system cotext which claims in an unpublished non peer reviewed report betterperformancethan 9the codebert paper simply averages the bleu across languages to report the overall number our weighted average weights each bleu by the number of samples in that language.usforjustpython butisworseoverall.
wewillincludeitfor comparison once it is published in a peer reviewed venue.
thistablealsoprovidesevidencesupportingtheeffectivenessof multilingual fine tuning.
.
code search we study the gains from multilingual fine tuning using two pretrained models i.e.
codebert graphcodebert .
we multilinguallyfine tunebothmodelsusingthepubliclyavailablecode dataset10.
as we did for code summarization we re trained the baselinemodels togetperformancenumbersforeachcaseinthe testset toenablepairwisetwo sampletesting .weusethesame testsetsforbothmonolingualandmultilingualtrainingtoevaluate ourapproach.duringthetraining graphcodebertusesamatrix ofdimension query candidate codes .wecouldnotusethefull merged validation set as we did for the code summarization task because that makes the query and candidate code sets too large the resulting matrix could not fit on our gpu server.
we used a down sampledvalidationsetcomprisingsixmonolingualvalidation setswith 10k query and 50kcandidate codes each.however wedidnotfaceanyissuewhiletestingbecausewedidnotmerge the test sets.
wereportboththepublishedvalues andourreplication weneed the replication to measure pairwise gains.
though codebert and graphcodebert both work on sequence of code tokens graphcodebert creates a rudimentary data flow graph once it s told the programming language.
table shows that multilingual fine tuning improves the mean reciprocalrankforalllanguagesexceptgowithcodebert.theimprovementforruby javascript andjavaarestatisticallysignificant.
we found similar results for graphcodebert exhibiting improvement for ruby javascript java and python but with graphcodebertbothgoandphpshowedperformancedeclines.however overall both showed statistically signficant improvements p .
but the improvement for graphcodebert .
is lower than codebert .
.
finally we note that our numbers for codebert differ from the performance reported for on the codexglue leaderboard.
this is because codexglue benchmark uses onlypython andisbasedonarestrictedsettingwhereidentifier names are left out.
codexglue team argues that this abstraction enables them to stress test the generalization ability of a model however hereweconsideranunmodifiedsettingwheresomeone gives an natural language query and wishes to find natural code with variable names intact.
.
method name prediction asfortheprevioustwotasks wetrymultilingualfine tuningfor methodnamepredictionforcodebert.here too wefindevidence supportingtheconclusionthatmultilingualtrainingprovidesimprovementforallthelanguages .non parametricpairwise improvementsaresignificantforruby javascript andjava.wealso noteobserve relativelygreater effectsizefor rubyand javascript.
note that we achieve highest improvement for javascript because 1449icse may pittsburgh pa usa toufique ahmed and premkumar devanbu languagecodebert reported codebert re trained polyglotcodebert improvementeffect sizep value adjusted graphcodebert polyglotgraphcodebert improvementeffect sizep value adjusted ruby .
.
.
.
.
.
.
.
.
.
.
js .
.
.
.
.
.
.
.
.
.
.
java .
.
.
.
.
.
.
.
.
.
.
go .
.
.
.
.
.
.
.
.
.
.
php .
.
.
.
.
.
.
.
.
.
.
python .
.
.
.
.
.
.
.
.
.
.
overall .
.
.
.
.
.
.
.
.
.
.001overall weighted not reported19.
.
.
.
.
.
evaluation criteria followed by codexglue and codebert table effectiveness of multi lingual fine tuning for code summarization task.
note that p values are b h corrected many functions therein are anonymous lambdas since these functions have no names they are not useful and this diminishes available the javascript training set relative to other tasks lambdas stillhavesummaries andcanbeusedforothertasks .therefore multilingual fine tuning increases the dataset diversity and boosts javascript method name prediction performance.
.
two illustrative examples we used the same dataset for all tasks for illustration we show two test instances where all the tasks show improved performance from multilingual fine tuning.
in code summarization task the monolingualfine tuning scores bleu inexample .
codebertproducesasemanticallywrongcommentwheremultilingual fine tuning generates the semantically correct solution.
note thatthe bleu is84 for thesecond example becauseof the missing period in the gold standard bleu is case insensitive .
multilingual fine tuning also helps the code search problem by increasing the mrr from .
rank to .
rank .
we also observe performance improvement from the method name prediction task.
the gold standard consists of two sub tokens i.e.
set and values and mono lingual fine tuning generates three i.e.
set array andvalue one ofthem isexact match.
onthe otherhand multilingualfine tuningremovestheextra array subtokenand produces two subtokens i.e.
set and value resulting in the f score .
.weobserveasimilarresultinexample2.notethatlikebleu our method name prediction metric is also case insensitive.
finding3 .multilingualfine tuningislikelytoincreasediversity andhelpthemodelsperformbetterthanthosetrainedwithsmaller mono lingual datasets especially for low resource languages irrespective of the task.
models overall ruby javascript go python java php polyglotgraphcodebert .
.
.
.
.
.
.
polyglotcodebert .
.
.
.
.
.
.
prophetnet x .
.
.
.
.
.
.
plbart .
.
.
.
.
.
.
graphcodebert .
.
.
.
.
.
.
codebert .
.
.
.
.
.
.
roberta .
.
.
.
.
.
.
transformer .
.
.
.
.
.
.
seq2seq .
.
.
.
.
.
.
table comparison to existing models on codexglue dataset5 interpreting results and threats inthissectionweconsiderseveralissuesthatarerelevanttotheobserved performance of multilingual training such as model choice datasetduplication performancemetrics generalization anddifferent training strategies for the models.
.
does multilingual fine tuning help with other models?
thereareseveralmodels includingcotext prophetnet x and plbart which report higher performance than codebert model for the code summarization task.
the models for allthesetaskswerefine tunedusingmonolingualdatasets sowe might expect that multilingual fine tuning should improve performance.
these experiments would require a substantial investment ofcomputeenergyandisleftforfuturework.wefocusedoncodebert and also graphcodebert on some tasks .
we did some preliminaryexperimentswithmultilingualfine tuningonplbart.
inourpreliminarystudy didseethesamegainsforlow resource language ruby gain .
however we found a .
overall loss whichisinconsistentwithwhatweobservewith polyglotcodebert .
overallimprovement polyglotgraphcodebert .
overall improvement .
more study is needed.
finding .
multilingual fine tuning could benefit a broad range of models.wefindgainsforcodebertandgraphcodebert but more data is required for other models.
.
threats risk of data duplication?
dataduplicationcanleadtopoor qualityestimatesofperformance especiallywhendataisduplicatedacrosstraining test evenduplicationjustwithintestdatariskshighervarianceintheestimates.
allamanis finds that performance metrics are highly inflated when test data has duplicates and advocates de duplicating datasets for more robust results .
shiet al.also discusses the impact of duplication in code summarization task .
sadly there is a large amount of copied code on github inattentively combining different datasets harvested from github can lead to undesirable levels of duplication in the merged dataset.
fortnuately codexglueisatuallya carefullyde duplicated dataset performance estimates therein are thus more robust.
combining multilingual data is unlikely to introduce the same kind of exact duplication in the dataset because of syntax differences there is a 1450multilingual training for software engineering icse may pittsburgh pa usa languagecodebert publishe d codebert re trained polyglotcodebert improvementeffect sizep value adjusted graphcodebert published graphcodebert re trained polyglotgraphcodebert improvementeffect sizep value adjusted ruby .
.
.
.
.
.
.
.
.
.
.
.
javascript .
.
.
.
.
.
.
.
.
.
.
.
java .
.
.
.
.
.
.
.
.
.
.
.
go .
.
.
.
.
.
.
.
.
.
php .
.
.
.
.
.
.
.
.
.
.
.
python .
.
.
.
.
.
.
.
.
.
.
.
overall .
.
.
.
.
.
.
.
.
.
.
.001overall weighted not reported0.
.
.
not reported0.
.
.
evaluation criteria followed by graphcodebert table effectiveness of multi lingual fine tuning for code search task.
note that p values are bh corrected languagecodebert polyglotcodebert f score improvementeffect sizep value adjusted precision recall f score precision recall f score ruby .
.
.
.
.
.
.
.
.
javascript .
.
.
.
.
.
.
.
.
java .
.
.
.
.
.
.
.
.
go .
.
.
.
.
.
.
.
.
php .
.
.
.
.
.
.
.
.
python .
.
.
.
.
.
.
.
.
overall .
.
.
.
.
.
.
.
.001overall weighted .
.
.
.
.
.
.
table effectiveness of multi lingual fine tuning for method naming task.
note that p values are adjusted using benjamini hochberg possibilityofcross languageclones thestudyofthisisleftfor future work.
finding .
combining multilingual datasets is unlikely to cause exact duplication because of syntax differences.
more study is needed to study the effect of cross language clones.
.
threats other metrics?
followingcodexgluebenchmarkrecommendation weevaluate the code summarization task with smooth sentence bleu throughout this paper.
however other recognized metricsareare available e.g.
rouge l meteor .
prior works provide a careful analysis of the metrics baselines evaluations for code summarization task.
table shows rouge l and meteor data we find that multilingual fine tuning increases the overallperformanceby4.
and5.
inrouge landmeteor respectively.aswithbleu wefindthatmultilingualfine tuning shows similar performance gains with these metrics.
we find .
.
improvement in rouge l and .
.
gains in meteor exceptforphp wereweseea0.
decline notstatisticallysignificant .wealsoseethatpythonshowsthesmallestimprovement not as strongly statistically significant.
these metrics also indicatestronggainsfrommultilingualtrainingforlow resourceand narrow domain languages i.e.
ruby and javascript .
finding .
we observe performance improvement in all code summarization metrics with multilingual fine tuning.
.
monolingual minibatches?
or multilingual?
whiletrainingdeepneuralnetworkswithstochasticgradientdescent gradients multivariate derivatives of loss w.r.tlearnable parameters areestimatedover mini batches ratherthancalculatinglossgradientsovertheentiretrainingset theseestimatesareexample setthe values from an array public void setvalues array arr we omit intermediate lines to fit in the paper original code here code summarization models comments bleu gold set the values from an array na codebert sets the values of the array .
polyglotcodebert set the values from an array .
code search models mrr graphcodebert .
polyglotgraphcodebert .
method name prediction models method name sub tokens f score gold setvalues set values na codebert setarrayvalue set array value .
polyglotcodebert setvalue set value .
example registers set injection point .
public void registerpetitesetinjectionpoint final string beanname final string property we omit intermediate lines to fit in the paper original code here code summarization models comments bleu gold registers set injection point .
na codebert register a set of set injectionpoint .
polyglotcodebert register a set injection point .
code search models mrr graphcodebert .
polyglotgraphcodebert .
method name prediction models method name sub tokens f score gold registerpetitesetinjectionpoint register pet ite set in jection point na codebert addpropertyinjectionpoint add property in jection point .
polyglotcodebert setpropertyinjectionpoint set property in jection point .
registerpetitesetinjectionpoint setvalues tokens are abstracted for method name prediction task table examples exhibiting the effectiveness of multilingual training usedtoadjusttheweightsinthenetwork.betterchoicesofminibatches could improve convergence behavior.
with multilingual training a natural question arises is it better to sequentially interpersemonolingual mini batches e.g.
firstajavaminibatch then rubyminibatchandsoon beforegoingbacktojava?
orshould we make each minibatch persemultilingual?
in the previous experiments we had randomly sort the dataset hence ourmini batchesarealsomultilingual.sowedeliberately triedsequentiallymonolingualminibatchingduringmultilingual 1451icse may pittsburgh pa usa toufique ahmed and premkumar devanbu languagerouge l meteor polyglotcodebert improve.
effect sizep value adjusted polyglotcodebert improve.
effect sizep value adjusted ruby .
.
.
.
.
.
.
.
javascript .
.
.
.
.
.
.
.
java .
.
.
.
.
.
.
.
go .
.
.
.
.
.
.
.
php .
.
.65e .
.
.
.
.
python .
.
.
.
.
.
.
.
overall .
.
.
.
.
.
.
.001overall weighted .
.
.
.
improvement reported over codebert table10 performanceimprovementinrouge landmeteorfor code summarization task fine tuning.wefindthatsequentiallymonolingualminibatchtraining appears to somewhat degrade performance we observe the overallperformancegoesdownby1.
.however thechangeis not statistically significant for any language.
we omit the actual numericaldetails forspacereasons sincewedidn tfindanystrong results in either direction.
finding7 .wedon tfindanycleardifferencebetweenmultilingual mini batches and interspersed monolingual mini batches.
.
multilingual model as pre trained model our findings provide evidence supporting the claim that a multilingualfine tunedmodeliseffectiveforcodesummarizationtask whichoutperformsallthemodelstrainedwithmonolingualdatasets.
couldthisthisimprovedmultilingualmodelfurtherbenefitfrom a secondary monolingual fine tuning exercise where it receives specializedfine tuningforeachlanguageseparately?toevaluate this intriguing and promising idea we load the model with the weightsfrommultilingualfine tuning andfine tuneit again for each individual language.
table shows that we found some minorperformanceimprovementforjavascriptandpython.however the performance goes down for other languages.
we do not find evidence that a secondary monolingual fine tuning is helpful furtherwork isneededto understand why and perhapsdevelopother ways this idea might yield further improvement.
language polyglotcodebertpolyglotcodebert as pre trainingimprovementeffect sizep value adjusted ruby .
.
.
.
.
js .
.
.
.
.
java .
.
.
.
.
go .
.
.
.
.
php .
.
.
.
.
python .
.
.
.
.
overall .
.
.
.
.005overall weighted .
.
.
table multilingual model as pre trained model finding8 .wedon tfindevidencethatapplyingasecondary monolingual fine tuning provides benefits for all languages.
related work code summarization code summarization has recently been a hot topic.morethan30papershavebeenpublishedinthelastfiveyearsthatfollowsomeformofencoder decoderarchitecture .several works discuss the evaluations metrics andbaselining.
royet al.show that metric improvements of less than points do not guarantee systematic improvements in summarization and are not reliable as proxies of human evaluation .
we find more than points of improvement for ruby and almost points of improvement for javascript.
we observe less than points in other languages.
it should also be noted that we don tuse the corpuslevelmetricswhichroy etal.showisproblematic weusepairwise comparisons on the test sets.
finally we note that progress in both code nlp occurs in small steps over decades and innovations especially ones that could cumulate with others such as ours can beanimportantpartofresearchcommunity slong termpursuitof practically relevant performance improvements.
pre trained models are proven to be more effectivethanpriormodels.differentpre trainedmodelsaretrained withthedifferentpre trainedobjectiveseventhoughfine tuning steps are almost similar for all the models.
as discussed earlier in section .
codebert is an encoder model pre trained with mlmandrepacetokendetectionobjectives.unlike codebert plbart is an encoder decoder model which is trained as a denoisingauto encoder.thoughallthemodelsarepre trainedwith different training objectives there is one thing common among all the models using transformers as core architecture.
parvezetal.veryrecentlypresentanapproachthataugments training data using relevant code or summaries retrieved from a database e.g.
github stack overflow .
they apply this approachonmonolingualjavaandpythondatasetsfromcodexglue and claim gains over polyglotcodebert polyglotgraphcodebertforcodesummarization.
primafacie multilingualfine tuning is complementary to their approach this needs to be studied.
code retrieval and method name prediction code retrieval is also getting attention recently.
there are multiple datasets for this task.
codexglueintroducesamonolingualpythondataset takeninitially from codesearchnet abstracting the function names and variables.guo etal.modifythemultilingualcodesearchnetdataset and achieve state of the art performance on this task.
however using multilingual training we show that both codebert and graphcodebert can be improved.
there is one other very recent paper clsebert whichreports inanunpublished non peerreviewedreport betterperformancethanusinalllanguagesexcept ruby.
we could not show the effectiveness of multilingual training on clsebert because the authors have not released the code implementation yet.
notethat like code summarization wefocus only on the work using codesearchnet multilingual dataset.
codesearchnetdatasetcanbeeasilyadaptedtomethodname prediction task.
several earlier works address method name prediction inajava onlysuchascode2seq allamanis .theyall useaconventionalsingle stagemachine learningapproach nopretraining fine tuning .ourgoalhereistosimplydemonstratethat multilingualfine tuningimprovesuponmonolingualfine tuning for the method naming task so we demonstrate using codebert.
our numbers are roughly comparable with previously reported results butwecannotmakeaprecisecomparisonbecauseofdifferencesinsubtokenization andbecauseourdatasetsaremultilingual 1452multilingual training for software engineering icse may pittsburgh pa usa whereas previous work has largely been monolingual.
we are simply arguing here our data suggests that multilingual fine tuning is broadly beneficial in different tasks.
it would certainly be interesting to use same domain data for fine tuning.forexample summarizingmethodsinandroidapps might work better if trained on android app corpora however curated domain specific datasets for each domain are needed and maynotalwaysbepossible dependingonthedomain.however cross language data is already available and we show that it does indeed help improve performance!
the effect of domain specific corpora is left for future work.
conclusion webeganthispaperwiththreesynergisticobservations first when solvingthe sameproblem evenindifferentprogramminglanguages programmers are more likely to use similar identifiers than when solvingdifferentproblems .
second identifiers appear to be relatively much more important than syntax markers when training machine learningmodelstoperformcodesummarization.
third we findthatquiteoftenamodeltrainedinoneprogramminglanguage achieves surprisingly good performance on a test set in a different language sometimes even surpassing a model trained on the same languageasthetestset!takentogether thesefindingssuggestthat pooling dataacross languages thus creating multilingual training sets could improve performance for any language particularly perhaps languages with limited resources as has been found in natural language processing .
we test this theory usingtwobert stylemodels codebert andgraphcodebert with encouraging results.
foundation models are currently achieving best in class performance for a wide range of tasks in both natural language and code.
the models work in stages first pre training to learn statistics oflanguage orcode constructionfrom verylarge scale corporainaself supervisedfashion andthenusingsmallerlabeled datasets to fine tune for specific tasks.
we adopt the multilingual codexgluedataset andthepre trainedcodebertandgraphcodebertmodels andstudythevalueofmultilingualfine tuning foravarietyoftasks.wefindevidencesuggestingthatmultilingual fine tuning is broadly beneficial in many settings.
our findings suggest that multilingual training could provide added value in broad set of settings and merits further study.