unite an adapter for transforming analysis tools to web services via oslc ond ej va ek ivasicek fit.vutbr.cz brno university of technology honeywell international brno czech republicjan fiedor ifiedor fit.vutbr.cz brno university of technology honeywell international brno czech republictom kratochv la tomas.kratochvila honeywell.com honeywell international brno czech republic bohuslav k ena krena fit.vutbr.cz brno university of technology brno czech republicale smr ka smrcka fit.vutbr.cz brno university of technology brno czech republictom vojnar vojnar fit.vutbr.cz brno university of technology brno czech republic abstract this paper describes unite a new tool intended as an adapter for transforming non interactive command line analysis tools to oslc compliant web services.
unite aims to make such tools easier to adopt and more convenient to use by allowing them to be accessible both locally and remotely in a unified way and to be easily integrated into various development environments.
open services for lifecycle collaboration oslc is an open standard for tool integration and was chosen for this task due to its robustness extensibility support of data from various domains and its growing popularity.
the work is motivated by allowing existing analysis tools to be more widely used with a strong emphasis on widening their industrial usage.
we have implemented unite and used it with multiple existing static as well as dynamic analysis and verification tools and then successfully deployed it internationally in the industry to automate verification tasks for development teams in honeywell.
we discuss honeywell s experience with using unite and with oslc in general.
moreover we also provide the unite client unic for eclipse to allow users to easily run various analysis tools directly from the eclipse ide.
ccs concepts software and its engineering development frameworks and environments software verification and validation applied computing service oriented architectures .
keywords transformation to web services oslc oslc automation software analysis tool integration eclipse lyo acm reference format ond ej va ek jan fiedor tom kratochv la bohuslav k ena ale smr ka and tom vojnar.
.
unite an adapter for transforming analysis permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november singapore singapore association for computing machinery.
acm isbn .
.
.
.
to web services via oslc.
in proceedings of the 30th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november singapore singapore.
acm new york ny usa pages.
.
introduction with the ever growing complexity of mission critical software automated analysis and verification became an integral part of the software development process.
yet many analysis tools especially new ones struggle to find their applications in practice despite the analyses they offer being of high interest.
according to our experience from the industry the reasons for this are often of a quite practical nature their setup and configuration can be difficult even for experienced developers they can have expensive licenses they may require a lot of computational resources not available locally the analyses they perform often run for a long time their integration into existing development ecosystems can be complicated etc.
to help resolve such problems and to contribute to a wider especially industrial usage of analysis and verification tools we introduce unite auniversal analysis adapter based on the oslc standard that can be used to easily transform command line analysis tools into oslc compliant web services1.
by so one can offload these tools onto servers where they can be installed configured and maintained by specialists possibly authors of the tools who can make them available over the internet but without requiring them to be experts on web services the tools can use the resources available on the servers can run their analyses and can be easily integrated with other tools and environments via the standardized oslc interface.
the only task left for the users is then to provide the inputs call a service and wait for the results.
we tested unite with publicly available static and dynamic analysis tools such as valgrind facebook infer anaconda perun and theta as well as with inhouse industrial solutions such as hilite .
all of the tools were used through unite successfully resulting in a subsequent pilot deployment of unite in honeywell for a number of tools.
furthermore unite is currently being tested by several partners within the arrowhead tools eu ecsel project.
based on practical experience with unite in honeywell discussed later in this paper we believe 1unite is open source and publicly available at .esec fse november singapore singapore o. va ek j. fiedor t. kratochv la b. k ena a. smr ka and t. vojnar that our approach can save both time and costs while widening the use of analysis tools which can lead to more errors being detected earlier in the development process.
for users who are not familiar with oslc or do not have an oslc compliant client at their disposal we further provide the unite client unic for eclipse that allows tools hosted through unite as web services to be easily used from the eclipse ide.
to ease the adoption of unite and to demonstrate its capabilities we also provide a virtual machine with several analysis tools and step by step usage instructions including videos .
plan of the paper.
below we first briefly introduce the oslc standard and the fundamental idea of unite .
then we describe the architecture of unite and illustrate how it can be used to transform an analysis tool into an oslc compliant web service.
further we demonstrate how to execute analysis through unite using its rest api directly as well as a more user friendly approach using the unite client unic for eclipse.
then we discuss existing applications of unite with various analysis tools describe unite s deployment in honeywell and share honeywell s experiences with using unite .
finally we discuss related work and future work.
open services for lifecycle collaboration oslc oslc is an open standard for integrating tools across the entire software development lifecycle.
tools are integrated through self describing restful apis use standard http for communication and use serialized resource representation like rdf xml or json.
the oslc standard consists of a number of specifications for different domains such as requirements management change management or automation suitable for automated usage of development tools .
these specifications define resource shapes for the resources that make up the interfaces of tools in each domain.
resource shapes can be imagined as classes i.e.
type definitions and resources as their corresponding objects i.e.
instances .
for example test cases are a resource in the domain covered by the quality management specification.
all specifications are built on top of the oslc core specification which defines common resource shapes and communication principles http rest and other technologies .
oslc participants are servers and clients producing and or consuming resources from their domain.
as visible already from the above a strong point of oslc is that it is built upon standards both for communication rest http as well as data representation rdf xml json with a strong emphasis on linked data.
it is robust and extensible meaning new resource shapes can be added yet adding them does not prevent oslc participants not knowing these resource shapes from exchanging and sharing the data.
it supports data from various domains providing a unified representation of data from the whole development process requirements models artifacts tests etc.
which in turn enables interoperability.
its emphasis on linked data allows the data to be linked with meta models ontologies and other formal specifications allowing precise description of their semantics.
all of the above made oslc very popular in both industry and academia and makes oslc compliant tools very sought after as of late.
what is unite ?
since the fundamental idea of unite might be hard to understand for those not familiar with rest or oslc we now provide a nontechnical explanation of what unite is which is intended to help form a clearer picture.
imagine you have just learned about a new analysis tool.
the tool has a command line interface and you want to use it to analyse a program.
in order to do that you will likely need to go through the following steps download and install the analysis tool on your computer.
transfer the program to be analysed to your computer and compile it if needed.
execute the analysis tool on it using the command line by executing a command with the tool s parameters .
wait for the analysis to finish and then look at the standard outputs or files produced by the analysis.
to be able to perform the above steps you needed to install and run the analysis tool on your personal computer which can be difficult or even impossible due to the constraints of your hardware or software e.g.
operating system libraries other needed tools insufficient memory licensing etc.
.
if you then wanted your colleagues to start using the new tool as well then they would also need to go through the whole setup process themselves.
of course one could also think of installing the tool on a server.
then however there is a need for each user to access the tool remotely.
so manually typically using ssh is not a very user friendly way and can lead to problems with transferring the program to be analysed to the server running and monitoring a remote analysis task and transferring the results back.
the installation could also be made available from some file server to the local computer but then one can again hit incompatibilities with running it on the local computer or a lack of local resources.
now let us see how the analysis tool can be made available using unite by transforming it to an oslc compliant web service.
unite runs as a server executes analysis tools based on requests from local or remote clients and then makes outputs of those executions available to clients as analysis results.
the steps can be divided between two actors a provider and a consumer .
the providers host their analysis tools as web services on their servers while the consumers use the web services to run analyses on their programs.
a provider can be the author of the analysis tool or an administrator of a server within an organization interested in running the tool.
a consumer can then be anyone who has access to the service.
each of the actors needs to perform the following steps the providers directly manage their servers a they download and install the analysis tool on a server b install unite and configure it for the analysis tool and c start unite to make the tool available as a web service.
the consumers interact with unite as a web service a they send a request to transfer the program to be analysed to the server and to optionally compile it b send a request to run an analysis on the program using a chosen analysis tool c wait for the analysis to finish by polling its state and then get the standard or file outputs from the server.unite an adapter for transforming analysis tools to web services via oslc esec fse november singapore singapore possible clients eclipse plug in unic unite on a server analysis compilation exec suts manage .
.
get sut rest client or swagger ui exec on analysis tools2.
.
oslc http communication any client for oslc automation1.
.
results .
execute analysis1.
transfer sut .
.
results figure an overview of unite s architecture.
the left side shows different ways to interact with unite .
the right side shows unite running on a server made up of two components.
the compilation sub adapter manages suts and the analysis sub adapter executes analysis tools on them.
this way the analysis tool may run on a remote server managed by someone else than you assuming you are the consumer in this example possibly an expert on the tool maybe its author.
this means you do not need to go through the setup process or be limited by the hardware or software of your personal computer.
you only need to know the address of the web service and be able to interact with it.
the same applies to all your colleagues who might also be interested in the new analysis tool.
on the other hand the author of the analysis tool or the administrator installing the tool on a server does not have to be an expert in developing web services.
note that interacting with the service may be done through its oslc compliant restful api which is mainly meant for other developers who want to integrate the service into their own ecosystems for example to execute analyses from their test management tool or from a version control system like git whenever a new version is committed .
for regular users we provide unic a plugin for the eclipse ide which abstracts users from the rest api and oslc and all they need to do is to click a context menu to execute an analysis in a much more user friendly way.
the standardized interaction is what sets unite apart from other solutions.
while it is possible to write a script which uses e.g.
ssh to interact with a tool on a remote server it will be specific to this tool and every tool would need its own script for this purpose.
with unite any oslc or rest client knows how to interact with any tool.
unite defines what the requests and results look like how to choose the tool to execute how to specify the parameters if the tool has any and everything remains the same regardless of the tool we need to interact with.
architecture overview unite is a standalone tool chain of two servers referred to as subadapters as shown in figure and further explained throughout this section.
the solution was divided into two sub adapters to separate two different steps needed for analysing a system under test sut on a remote server and to allow each of the sub adapters to be reusable in other use cases and to be replaceable e.g.
by parts of an existing tool chain.
both sub adapters are also connected with a database to enable resource queries and persistence.
automation resultautomation requestautomation planproducedby executesreportsonfigure the oslc automation domain contains three main resources.
automation plans represent units of automation offered by a server and define their input parameters.
automation requests are what clients create to request execution of one of the available automation plans.
automation results are then produced by the server and contain outputs of the execution.
the first simpler sub adapter is the compilation sub adapter .
clients interact with it to transfer sut files to the server and optionally compile them.
after transferring sut files are stored in a local folder on the server and compiled if requested by executing a compilation command.
sut folders then work as workspaces for the execution of analysis tools i.e.
analyses are executed in the sut folders to preserve files produced for consecutive analyses .
the second more complex sub adapter is the analysis sub adapter .
clients interact with it to analyse a previously created sut using a chosen analysis tool.
we consider the adapter universal as it can be configured for any non interactive command line analysis tool see section for more details by essentially representing its command line interface as input parameters contained in an oslc request.
the analysis sub adapter communicates with the compilation sub adapter to get information about sut resources to be analysed such as the folders they are located in or their execution commands.
the analysis is performed by asynchronously executing the analysis tool in the obtained sut folder using the server s local shell powershell or cmd on windows bash on linux .
concurrent analysis requests will result in concurrent analysis tool executions.
a fifo queuing system can be enabled for analysis tools that should not run in multiple instances concurrently.
both sub adapters use a restful api as their interface modelled w.r.t.
the oslc automation specification .
these interfaces were generated from graphical models of unite s architecture in the eclipse lyo designer and use three main resources shown in figure .
unite then implements these interfaces to provide its functionality.
users interact with unite using the interfaces of one of the sub adapters depending on the current interaction .
in order to do that they can use any oslc automation client a simple rest client or unite s integrated swagger ui .
we also provide the unite client unic for eclipse see section a plug in for the eclipse ide that allows analysis to be executed directly from the ide using context menus as a more user friendly way to use unite .
note that unite s applicability is broader despite that it focuses mainly on analysis tools in terms of its features such as facilitating the typical workflow of getting an sut compiling it and then executing analyses on it .
indeed unite can be configured to execute any tool not just analysis tools that has a command line interface and produces its outputs to the standard output or by creating files.
the sole limitations we are aware of are interactive tools that require non deterministic user inputs during their execution suchesec fse november singapore singapore o. va ek j. fiedor t. kratochv la b. k ena a. smr ka and t. vojnar as interactive theorem provers.
as unite is implemented in java it should be usable on any os although we have so far explicitly tested it on windows and linux only.
it is important to note that due to its nature unite should be deployed in docker in a virtual machine or in a trusted environment as it allows clients to execute any sut directly on the server.
an sut could potentially be malicious and tamper with the server.
configuring unite to get a web service from an analysis tool this section demonstrates how to configure unite for a new analysis tool.
the configuration is a step that needs to be performed by the analysis tool provider in order to make it available as a web service on a server using unite .
we use anaconda a dynamic analysis framework as an example analysis tool.
configuring unite for a new analysis tool is performed by creating two files in its configuration directory an automation plan defined using an rdf file called anaconda.rdf in our example and a definition of technical parameters given as a java properties file called anaconda.properties in our example.
configuration files can be modified or added anytime during unite s deployment.
unite only needs to be restarted in order for the new configuration to take effect.
further optional configuration includes defining output filters which are used for processing outputs of the analysis tool.
all these configuration possibilities are described later in this section.
automation plans.
the first above mentioned configuration file the rdf file needs to contain a definition of an automation plan resource.
automation plans represent units of automation offered by oslc automation servers.
their main properties are parameter definitions that define input parameters which can be submitted by clients when requesting execution of an available automation plan.
unite uses automation plans to represent individual available analysis tools2 and their parameter definitions typically correspond to command line parameters of a given analysis tool.
clients can then browse available automation plans to discover which analysis tools are available and what their input parameters look like.
each automation plan needs to contain a unique identifier which is used as a part of its uri.
then a number of parameters can be defined based on the command line parameters of the analysis tool.
the defined parameters can mimic the tool s command line parameters to give clients full control of the analysis tool or they can be simplified to abstract clients from the details of the tool s interface.
each parameter definition needs to have a name and a commandline position which instructs unite how to order their values when constructing the string to execute the analysis tool more in section .
other possible properties include default values a list of allowed values occurrence restrictions or a value prefix for named command line parameters.
when a client requests execution of an automation plan unite automatically checks that all the submitted input parameters are correct and that no required ones are missing.
the simplest automation plan can contain just a single parameter which is used for clients to supply all the tool s parameters in a single string that is then placed on the command line and parsed as it would be when using the command line directly.
2there may be multiple automation plans for one tool e.g.
with different default parameters to represent individual pre configured analyses.
automationplan identifier anaconda identifier parameterdefinition name analyser name cmdlinepos cmdlinepos occurs res exactly one allowedvalue atomrace allowedvalue allowedvalue fasttrack allowedvalue parameterdefinition parameterdefinition name sutlaunchcommand name cmdlinepos cmdlinepos occurs res exactly one defaultvalue true defaultvalue parameterdefinition parameterdefinition name sutinputs name cmdlinepos cmdlinepos occurs res zero or one parameterdefinition automationplan a a simplified automation plan defined for anaconda inanaconda.rdf .
namespaces urls and some properties were omitted for space and readability reasons.
the automation plan defines three parameters.
the first parameter lines is compulsory and is used to pick one of the available analysers.
the second parameter lines is a special compulsory parameter which instructs unite to place the sut launch command at the specified command line position.
the third parameter lines is an optional parameter used to specify inputs for the executed sut.
1toollaunchcommand anaconda run.sh toolspecificargs no color 3oneinstanceonly false b technical properties defined in anaconda.properties .
there are three properties.
the first one tells unite how to execute anaconda .
the second one holds parameters to be always placed on the command line such as removing colored outputs which is currently not applicable foranaconda the line is commented out .
the third one tells unite whether multiple instances of the tool can run at the same time or whether to enable queuing.
figure an example configuration for anaconda figure 3a shows a simplified automation plan for anaconda and its command line interface which looks like this run.sh analyser sut launch command sut inputs there are some parameter definitions with reserved names and special functionality recognized by unite such as sutlaunchcommand that instructs the adapter to look up the sut launch command as a property of an sut resource and insert it to the specified command line position.
also a number of common parameter definitions are added to all automation plans by unite automatically.
these include parameters like an execution timeout or output processing options.
the most important common parameter added to all automation plans is the sut parameter that is compulsory and is used to reference the sut resource to be analysed.
technical properties.
the second above mentioned configuration file configures technical properties of the analysis tool that need not or should not be visible to clients.
most importantly it specifiesunite an adapter for transforming analysis tools to web services via oslc esec fse november singapore singapore run.sh help run.sh analyser sut launch command sut inputs read output.loganaconda command line get automation plan create automation request get automation resultunite oslc automation figure from a command line execution of anaconda to executing it via unite .
the path to the executable of the analysis tool.
an example of such a file is shown in figure 3b along with a description of its contents.
output filters.
further optional configuration allows custom output filters to be defined using a plug in system.
these allow outputs of each analysis tool to be processed in a tailored way.
a filter is a java class implementing a method which receives a collection of contributions parts of oslc automation results which represent individual outputs of the analysis tool such as standard outputs and or produced files.
the filter can then process the contributions in any way e.g.
search for error reports in the standard output and output a collection of untouched modified or entirely new contributions.
output filters are an optional feature and unite includes built in filters for use by clients who do not need custom ones.
for more details on output filters refer to unite s wiki .
running analysis using unite s api once unite is configured for an analysis tool by a provider on a server it can be used by consumers using their clients to execute analyses on their suts.
as shown in figure there are multiple clients that can be used to communicate with unite due to its interface being a rest api.
in this section we explain how to execute analysis with unite by interacting with its rest api directly i.e.
by sending and receiving http requests post get etc.
and responses with rdf data content.
this is most useful for understanding how unite works especially for developers who want to integrate analysis execution using unite into their products.
the typical workflow when a client wants to execute an analysis using unite consists of two steps with three sub steps each transfer the source files of your sut to the unite server by interacting with the compilation sub adapter a retrieve the automation plan for sut creation by sending a get request to its uri.
inspect the automation plan to learn about input parameters defined in it.
b create an automation request with input parameters e.g.
the source of sut files based on the retrieved automation plan by sending a post request to the appropriate uri.
this will start the sut creation process.
c poll the state of your automation request until it finishes.
then retrieve the produced automation result by sending a get request to its uri.
the result will contain the uri of your newly created sut resource.
execute analysis on the previously created sut by interacting with the analysis sub adapter a retrieve the automation plan which corresponds to your desired analysis tool by sending a get request to its uri.
inspect the automation plan to learn about input parameters defined in it.
b create an automation request with input parameters e.g.
tool parameters based on the retrieved automation plan by sending a post request to the appropriate uri.
this will start the analysis execution.
c poll the state of your automation request until it finishes.
then retrieve the produced automation result by sending a get request to its uri.
the result will contain outputs of your analysis.
step is performed by interacting with the compilation subadapter and step by interacting with the analysis sub adapter.
however both sub adapters use an oslc automation interface which makes both steps almost the same up to different automation plans input parameters and contents of results .
as discussed in section an oslc automation interface contains a number ofautomation plans i.e.
available units of automation .
a client needs to pick the one they want to execute and get it to see its parameter definitions.
then they can create an automation request by sending a post request to the appropriate creation factory referencing the selected automation plan while specifying input parameter values each input parameter corresponds to a parameter definition .
this will cause the destination sub adapter to execute the requested automation plan asynchronously and to produce an automation result .
either the automation result or the automation request then has to be polled3 i.e.
sending multiple get requests by the client to monitor their state property.
an in progress request can be canceled by updating or deleting it.
once complete the automation result will contain outputs of the executed automation plan including a verdict property which signifies whether the execution finished successfully or whether there were any errors e.g.
a non zero exit code out of memory crash etc.
.
all requests and results are stored in a sparql triplestore to allow queries using the oslc query syntax and optional persistence4.
for a better understanding of how oslc automation works see figure which shows how executing analysis using anaconda on the command line translates to it through unite .
we now provide more details on creating automation requests to the two sub adapters of unite .
note that typical requests can be pre prepared and reused either as is or just with some small changes for a given specific analysis run .
requests to the compilation sub adapter.
the compilation subadapter has a single automation plan for creating sut resources only.
as some sut resources may be executable the parameters of this automation plan include not only the source files of the sut but also the compilation and launch commands needed to compile and execute it.
supported file sources include a git repository a general url a base64 encoded string or a path to a file already located 3multiple automation requests can run and can be polled concurrently.
4sparql is well suited for rdf resources and is supported by oslc tooling .esec fse november singapore singapore o. va ek j. fiedor t. kratochv la b. k ena a. smr ka and t. vojnar on the server e.g.
transferred by the client manually by other means .
after an automation request is created by a client the compilation sub adapter will create a local folder as a workspace for the sut automatically fetch its source files from the specified source and optionally execute a compilation command on the sut.
the automation result will then contain a newly created sut resource identified by a uri along with outputs of the compilation and transfer processes.
see figure 5a for a simplified example of an automation request used for creating an sut.
requests to the analysis sub adapter.
the analysis sub adapter can have multiple automation plans corresponding to individual available analysis tools and or to various pre configured analyses for a single tool.
when creating an automation request the client needs to pick the right automation plan according to their desired analysis tool.
the parameters of the selected automation plan will include a uri of the previously created sut resource to be analysed input parameters for the analysis tool w.r.t.
its configuration from section and parameters for controlling additional features ofunite .
the analysis sub adapter will then take the values of the submitted input parameters corresponding to command line parameters of the analysis tool and build a single string to execute using the provided values of the parameters their defined command line positions value prefixes and default values.
the string will be executed asynchronously using the local shell.
the automation result can contain all outputs of the analysis as contributions which can include standard outputs any files produced by the analysis or contributions created by an output filter.
see figures 5b and 5c for simplified examples of an analysis execution request and of the result produced for that request.
if the parameter values from figure 5b are positioned according to the configuration from figure 3a one gets the following command to be executed anaconda run.sh atomrace .
hello helloworld using the eclipse ide to run analysis in the previous section we described how any rest client can be used to interact with unite .
naturally using a rest client requires the user to write and send an oslc compliant request to unite in order to successfully communicate with it.
having an oslc client alleviates a lot of the work and many companies already have their own internal oslc clients they could use to integrate unite into their oslc ecosystem.
yet many users do not have such a client at their disposal and remain stuck with only a rest client to use.
to address this issue and simplify the usage of unite we provide the unite client unic for eclipse.
users can install this client from an eclipse update site as any other eclipse plugin.
unic can be configured to remotely execute any tool transformed by unite .
this is thanks to the unic s extensible architecture in which users can define jobsmade up of built in tasks or their own custom tasks .
after the configuration users can invoke the tool directly from the menu of the eclipse ide feed it with input data and fetch the results back into eclipse potentially transforming them into eclipse markers for a more user friendly representation.
as an example we provide a unic configuration for the facebook infer static analyser available from the same update site asunic .
the user needs to add the verifit update site and install the infer on but server ete task feature from it.
unic and all1 automationrequest !
post to compilation execautomationplan res .. uri sutcreation inputparameter name srcurl name val http sut .
zip val inputparameter inputparameter name unpackzip name val true val inputparameter inputparameter name buildcommand name val make val inputparameter inputparameter name launchcommand name val .
hello val inputparameter automationrequest a a simplified automation request used for creating an sut.
the request contains a link to the automation plan to execute line and four input parameters.
the first one line specifies where to fetch the sut source files from in this case to download them from a url.
the second one line specifies that the downloaded sut files are in a zip archive which needs to be unpacked before compilation.
the last two lines specify the suts build and launch commands.
automationrequest !
post to analysis execautomationplan res .. uri anaconda inputparameter name analyser name val atomrace val inputparameter inputparameter name sut name val .. uri of the sut val inputparameter inputparameter name sutinputs name val helloworld val inputparameter automationrequest b a simplified automation request used for executing analysis on an sut using anaconda .
it contains a link to the automation plan to execute line which is the one for anaconda from figure 3a and three input parameters.
the parameters specify the type of analysis to perform line the sut to analyse line whose value must be a link to the sut resource created using the request from figure 5a and the input data given to the sut line .
automationresult producedbyautorequest res .. uri requestn reportsonautoplan res .. uri anaconda state res complete verdict res passed contribution title stdout title val helloworld ... val contribution contribution title stderr title val no dataraces ... val contribution contribution title statusmsg title val executing anaconda run .sh atomrace .
hello helloworld 12in dir path to sut directory 13completed successfully val automationresult c a simplified automation result produced for the request from figure 5b.
it contains links to the associated automation plan and request lines and properties which signify that the analysis finished without issues line .
then there are three contributions containing the standard outputs produced by anaconda lines and unite s reports on the execution process lines .
figure examples of requests sent to unite and of the result produced for one of them.
namespaces urls and some properties were omitted for space and readability reasons.unite an adapter for transforming analysis tools to web services via oslc esec fse november singapore singapore figure execution of facebook infer and visualization of its results in the eclipse ide.
of its dependencies will be installed automatically with this feature.
after the installation the analyse with facebook infer but server entry will show up in a popup menu under the tasks category as shown in the bottom left part of figure .
asinfer requires a build command in order to analyse a project the user must first specify it in the project s properties unite section sut page.
after that the user can right click on an eclipse project select the analyse with facebook infer but server task under the tasks category and let unic handle everything.
when the analysis is finished unic will fetch the results and transform them into eclipse markers adding one marker for each error found byinfer as shown in the problems view in the bottom right part of figure .
these markers are linked with specific lines in concrete source code files where the error was found and the user can jump to these locations from the problems view.
the code editors also show these errors as editor markers with their description in a hover text as can be seen in the center of figure .
theunic configuration for infer suffices with built in tasks which are part of unic and its libraries.
any tool developer using unite to transform their tool into an oslc compliant web service can adapt this configuration to allow unic to execute their tool and provide this configuration to the users through an update site.
case studies we now discuss several instances of using unite with different tools which confirm that unite is indeed usable with a broad spectrum of non interactive command line analysis tools.
in our experimental evaluation we considered a number of static as well as dynamic analysis and verification tools.
the tested tools included valgrind the facebook infer static analyser grep as a representative of a general unix utility also perceivable as the simplest static analyser the dynamic concurrency analyser anaconda used as an illustrative example in sections and the dynamic performance analyser perun the model checking framework theta and honeywell s proprietary test vector generation tool hilite .
anaconda andvalgrind as dynamic analysers have the same and most typical usage workflow as they first need the sut to be compiled as shown in figure 5a and then the analysis can be executed as shown in figure 5b which immediately produces the final results of their analyses as standard outputs that can be retrieved by clients as shown in figure 5c .
perun differs from the other dynamic analysers due to its tight integration with the git versioning system indeed perun aims at detecting performance regressions between program versions .
further perun needs to run multiple subsequent analyses on the same compiled sut introducing a need foresec fse november singapore singapore o. va ek j. fiedor t. kratochv la b. k ena a. smr ka and t. vojnar running multiple analyses in the same workspace i.e.
with files produced by one analysis persisting for the following analyses.
infer as a static analyser does not need the sut to be built it suffices with its source code and its compilation command5.
this can be achieved by simply disabling compilation during sut creation.
infer s analysis then produces standard outputs but also a directory which contains detailed reports and logs.
similarly theta only needs the sut to be preprocessed not compiled which can be treated as a special case of compilation with specific arguments for the compiler.
theta is then executed on the preprocessed source files producing standard outputs or files e.g.
containing counter examples produced by modelchecking .
all produced files and directories can be included on request in analysis results offered by unite .
grep demonstrates that unite is usable with common noninteractive unix utilities.
compilation would be disabled for grep just like it was for infer .
one could even use a pipeline of unix utilities or a script instead of grep in order to have a more complex functionality executed by unite .
finally hilite takes requirements and models as its inputs and generates test vectors.
as such hilite s inputs are not an sut in the sense of being a set of source files to compile and execute but general files.
unite can handle such files in the same way as a normal sut with disabled compilation.
note that the compilation step can be used to perform some form of preprocessing on the inputs for example.
hilite also needs some environmental variables to be set during execution which can be facilitated using one of the common input parameters provided by unite for all configured analysis tools.
despite all these differences unite allowed us to make all of the tools available as oslc compliant web services and to utilize them remotely6.
moreover unite is already being used by several academic as well as industrial parties other than the authors.
industrial experience unite is currently deployed with a number of tools in honeywell on both windows and linux platforms.
the most important ones arehilite and its versions.
other tools which are being experimented with include symbiotic divine and nusmv .
honeywell also has prior experience with oslc from using a number of other verification tools using their own custom oslc servers.
in this section we share honeywell s experience with using unite as well as with oslc in general.
most of the experience comes from using unite with hilite which is a qualified tool that generates test vectors from matlab models and from high level and low level requirements.
moreover hilite is also applied in connection with honeywell s usage of its hard real time variant of clips which is a tool for building expert systems originally developed by nasa .
clips rules are converted to requirements and then processed by hilite to 5infer may determine which files to analyse from the build process of the sut for which it needs to know how to build it even when not using the resulting executable.
6interested readers are welcome to check our claims through a unite virtual machine which allows one to see almost all the functionality claimed in this paper using a rest client postman with step by step instructions or by watching a video.
the exceptions missing in the vm are hilite proprietary and theta only tested after vm creation .
the video by itself outside of the vm is available at .generate test vectors.
unite was deployed and still is with various versions of hilite on honeywell servers for around months.
approximately engineers from brno in the czech republic further referred to as the brno team are using two different clients to run hilite through unite .
around of them are new users who do not have any prior experience with hilite and would have likely not used it otherwise.
in addition teams from asia further referred to as the asia teams are using a different client to also run hilite through unite .
their experiences were gathered using various interviews and data collected from servers and are discussed throughout the rest of this section.
experience from brno.
the brno team uses two different clients.
the first one is a web ui client made specifically for accessing hilite as an oslc compliant service.
the second one is forreq a desktop gui client which remotely executes a range of tools and aggregates their outputs.
forreq was previously unable to execute hilite due to hilite being limited to windows and there was no existing solution for running remote verification tools on windows prior to unite .
the new integration is also more robust compared to the previous integrations of forreq with other tools which were only sending remote commands to a server and any changes in how the tool should be executed forced changes in the integration code of all the clients.
on top of that the files to analyse needed to be transferred to the server by other means.
unite provides an oslc interface for both transferring files and performing analyses which unifies the communication.
in addition changes to how the hilite tool is executed do not mandate changes to the oslc interface simply updating the tool s automation plan suffices making it very robust.
using unite effectively removed all issues with new hilite versions breaking the integration code and solved problems of using multiple hilite versions concurrently by defining configuration and automation plans for each version.
the brno team has further estimated their average time savings.
they saved approximately three minute long matlab installations per month and approximately fourteen minute long installations of hilite and its other dependencies.
this came at a cost of having to install and configure unite with hilite once per server.
using the web ui client comes at no cost as it does not need any installation on the user side and using forreq essentially comes at no extra cost as well because the team members already use it for other purposes.
in practical experience unite can be installed in less than minutes and requires only maven and java to be available for the installation.
after the installation configuring unite for an analysis tool in the easiest way i.e.
handling all parameters of the tool as a single string can be done in a few minutes.
this process can be made even faster when using docker7 since potential issues with installing java and maven can be avoided.
the execution overhead of unite is very small.
processing an analysis request before executing the analysis tool itself takes a few seconds at most as does producing an analysis result after the analysis tool finishes its execution.
the analysis tool itself runs without any overhead.
overall the overhead is negligible compared to execution times of typical analysis tools which can take hours to finish their analyses.
transferring large suts to the server could delay the analyses yet each sut can be transferred only once 7dockerfiles and compose files for unite are available in its gitlab repository .unite an adapter for transforming analysis tools to web services via oslc esec fse november singapore singapore and then reused for multiple analyses.
unite s performance under heavy load i.e.
a large number of concurrent requests was not a major concern during its design due to the fact that a large number of analysis tools being executed concurrently will typically overload the server regardless of unite s performance.
unite can handle concurrent analysis requests and polling of request states without issues as long as the executed analyses do not overload the server.
unite does feature a fifo queuing system for analysis request executions which can be enabled to prevent such server overload.
having verification tools as oslc compliant web services enabled gathering of data throughout the development process.
it is now possible to track who uses which tool how often how long the execution takes how many errors were detected and so on.
it is also possible to compare execution times and outputs of different versions of the same verification tool on the same inputs to detect and fix performance degradations or newly introduced bugs.
for example hilite is typically executed thousands of times a month using unite and on average over of verified requirements need to be reworked to pass the verification.
experience from asia.
a number of teams in india are currently using a command line client to invoke hilite remotely in ci cd pipelines.
due to all the issues discussed throughout this paper such as installation and compatibility hilite s usage prior to unite was limited to only a few engineers from the india teams who were using it directly on their personal computers.
introducing unite allowed hilite to be used in ci cd pipelines by removing the need to run ci cd agents and hilite on the same server which was often impossible due to licensing compatibility and other issues.
having hilite integrated into ci cd pipelines allows users who are not even aware of this tool to still utilize its functionality and have their matlab models analysed.
this significantly widened hilite s user base in the teams from india.
however the disadvantage of this approach is that the analyses are done only when the pipeline is triggered which occurs when changes to the matlab models are published by the users.
naturally many users want to analyse their modified models before they publish them.
moreover they may also want to try different versions of hilite and or different configurations of its analyses which is difficult with pre configured pipelines.
therefore the india teams are currently starting to use unic to allow their users to perform analyses using various versions of hilite and different configurations of its analyses directly from their ides.
furthermore the simplicity of configuring unic for multiple versions of hilite with multiple configurations of its analyses which can be done remotely from eclipse update sites also sparked the interest of several teams in china.
the china teams cannot access hilite directly because of export control restrictions therefore members of the india teams were required to perform the analyses on their behalf.
with hilite being accessible as a service and with unic s ease of use the china teams can now do on demand analyses themselves by simply picking their desired version and configuration of hilite from a menu in their ide and waiting for the results instead of having to rely on the india teams.
an important advantage of using unite was saving effort time and costs.
most of the costs savings come from reduced licensingcosts and operator costs.
having only a few central installations of e.g.
matlab on servers reduces licensing costs compared to each team member having their own local license.
while having one shared matlab server is already common practice an operator is often required to process analysis requests on the server.
an operator is a team member whose job is to wait for tasks from the rest of the team and to then run these tasks on the shared server to provide outputs of the tools such as generated test cases in case of hilite back to the rest of the team.
having an operator abstracts the rest of the team from manually going through the process of using a tool on a remote server.
with unite each team member can run their own tasks easily using one of the available clients effectively eliminating the need for a dedicated operator.
as is evident from the described experience the most important advantage of using unite is its impact on widening the user base thanks to making analysis tools easier to adopt and use.
in practice out of the total of engineers from the brno team were new hilite users the number of users in the india teams increased substantially from just a few previous users8 and teams from china were unable to use hilite themselves before at all.
all these new users have not used hilite before and would have most likely not used it at all if it was not for unite .
this is indeed the main goal ofunite and its main selling point.
having more people actively use verification tools can be very beneficial to the overall development process as they can detect defects which would otherwise hopefully be detected only in the testing phase much later.
indeed the more people actively use a tool the more errors can potentially be detected by it making it more useful.
the more useful a tool is the more new users are likely to be interested in using it which leads to more users more defects detected and more usefulness of the tool.
detecting defects as early as possible in the development process of a product is crucial because the later an error is discovered the more expensive it is to fix it.
according to the study conducted within the avsi savi project around of errors in large systems are introduced as early as in the specification of system requirements yet over of those errors are only discovered during integration testing much later in the development process.
the cost of fixing an error in the integration testing phase is around times higher than in the initial requirements specification phase and grows much higher in later stages.
in honeywell for example generating test cases with hilite based on clips rules as was mentioned at the start of this section is performed exclusively through unite .
this enables early detection of defects in requirements specifications and expert systems reducing the time and the cost of reworking them.
honeywell s overall experience with unite and verification tools as oslc compliant web services is the following unite indeed is in practice easy to configure for new analysis tools makes them easier to adopt and use and allows multiple versions of them to be available at the same time.
practical experience confirmed that converting analysis tools to web services can have a significant positive effect on widening the user base of a tool.
8unfortunately we cannot provide a precise number here since we cannot easily compute how many users access hilite through the ci cd pipelines in india.esec fse november singapore singapore o. va ek j. fiedor t. kratochv la b. k ena a. smr ka and t. vojnar moreover having analysis tools available on servers as web services saves time on tool installations and reduces licensing costs and operator costs.
web services provided by unite are highly reliable and a verification tool can now work for months without issues.
unite is still being actively developed therefore there is still room for improvement in terms of tool maturity such as better troubleshooting options and in terms of missing features such as user management or better security.
finally there is currently no automated system in place which would allow clients to discover available servers and therefore each client needs to be manually configured to know the appropriate server address.
note that this issue will be fixed very soon see future work in section .
finally we discuss honeywell s experience with using oslc in general.
using oslc is beneficial because it is an open source distributed and very rich integration technology.
it is also by design flexible and easily extensible with new resources or entire specifications.
this means it has potential to become widely adopted as it is well suitable for a wide range of use cases.
however as is common with protocols that use serialized xml data oslc communication transfers a lot of information consuming bandwidth and making it not efficient for file management and repositories or for heavy event triggered traffic.
this however is not much of an issue in the case of unite as there is not such a high volume of analysis requests and their contents are fairly small.
an exception is transferring large suts or large analysis tool outputs which can cause issues in poor network conditions.
related work we are not aware of any existing solutions for universally transforming any analysis tool or at least a certain class of tools to an oslc compliant web service other than unite .
the closest seem to be two other implementations of the oslc automation specification that were used in the amass project for remote integration of tools into the chess toolkit .
the first implementation was developed by fbk es to provide adapters for their tools .
we were unable to find any publications about these adapters but were able to inspect their source codes and documentation at eata along with amass deliverables.
these adapters seem to serve the same role as unite andunic but only for fbk tools and not for universal use of a wide range of tools by others.
according to their documentation the adapters can be extended with new tools however so requires modifications of the source code both on the server side and client side.
in comparison adding new analysis tools tounite andunic can be done through configuration files with no changes to the source codes.
the second implementation used in the amass project was a verification server for linux developed by honeywell which was also briefly mentioned in section .
unite currently coexists with this server and is set to replace it in the near future.
the server can be extended with new tools but so requires changes to the source code for every tool and it is not publicly available.
in comparison unite is configurable without modification of the source code is portable not limited to linux and implements the oslc specification more completely e.g.
includes a service provider catalogue resource queries etc.
.there are of course other projects whose aim covers transformation of tools into web services.
out of those to the best of our knowledge the one that is perhaps the closest though not really too close to unite is jeti .
jeti is an integration platform used to combine multiple remote tools so that they can work together to provide a more complex combined functionality.
jeti does not use oslc and focuses at orchestration of the tools which need to be used within the platform.
unite is most similar to jeti s tool adapters which are a small part of jeti and seem to offer a basic functionality only.
unite allows the tools to be integrated into any oslc compliant platforms while giving clients detailed information about the tool execution process and full control over it.
there are other implementations of adapters for the oslc automation specification in general.
these are focused on a specific tool or only on a couple of tools.
for example the lyo project contains a jenkinsplugin although it is a limited sample implementation according to its website and has not been updated for a few years.
for a list of any other oslc compliant products such as the ibm jazz platform and atlassian products refer to .
conclusion and future work we introduced unite a universal analysis adapter based on the oslc standard.
it allows one to add oslc compliant interfaces to non interactive command line analysis tools transforming them to remotely accessible web services.
based on the practical experience with using unite in honeywell we are confident that unite can significantly ease the adoption and usage of various analysis tools can substantially widen their user base and can bring a number of benefits ranging from savings of time and costs to better chances of early detection and correction of defects in products.
as for future work we are currently finishing integration of both unite andunic with the eclipse arrowhead framework which can facilitate automatic discovery of available analysis tools instead of manual client configuration.
further unite s implementation currently lacks user management features.
this means that all clients have access to all requests all suts and all results.
these features will have to be addressed when the need arises.
then we plan on creating a publicly available web client not an eclipse ide plugin like unic which would allow users who do not have experience with rest apis to more easily execute analysis using unite directly.
we are also planning instantiations of unite for further tools to show developers it is indeed easy with the closest targets being the past time ltl based runtime verification tool spectra and further experiments with the symbolic executionbased analyser symbiotic .
furthermore we are planning to create a master version of unite which would aggregate multiple unite instances across different servers to allow advanced distributed computation techniques such as load balancing.
finally we intend to upgrade the core architecture of unite to the architecture currently used in unic to make it even more extensible and to support more complex configuration scenarios.