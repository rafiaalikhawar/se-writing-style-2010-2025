fadatest fast and adaptive performance regression testing of dynamic binary translation systems jin wu harbin institute of technology chinajian dong harbin institute of technology chinaruili fang university of georgia usa wen zhang university of georgia usawenwen wang university of georgia usadecheng zuo harbin institute of technology china abstract dynamic binarytranslation dbt is thecornerstone of many important applications.
in practice however it is quite difficult to maintain the performance efficiency of a dbt system due to its inherentcomplexity.althoughperformanceregressiontestingisaneffectiveapproachtodetectpotentialperformanceregressionissues itisnoteasytoapplyperformanceregressiontestingtodbtsystems because of the natural differences between dbt systems and common software systems and the limited availability of effective testprograms.inthispaper wepresent fadatest whichdevises severalnoveltechniquestoaddressthesechallenges.specifically fadatest automatically generates adaptable test programs from existingrealbenchmarkprogramsofdbtsystemsaccordingtothe runtime characteristics of the benchmarks.
the test programs can thenbeusedtoachievehighly efficientandadaptiveperformance regression testing of dbt systems.
we have implemented a prototypeoffadatest .experimentalresultsshowthat fadatest can successfullyuncoverthesameperformanceregressionissuesacross the evaluated versions of two popular dbt systems qemu and valgrind as the original benchmark programs.
moreover the testing efficiency is improved significantly on two different hardwareplatforms powered by x86 and aarch64 respectively.
ccs concepts software and its engineering source code generation software performance simulator interpreter.
keywords performance regression testing dbt test program generation acm reference format jinwu jiandong ruilifang wenzhang wenwenwang anddecheng zuo.
.
fadatest fastandadaptiveperformanceregressiontesting ofdynamicbinarytranslationsystems.in 44thinternationalconferenceon software engineering icse may pittsburgh pa usa.
acm new york ny usa 13pages.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation onthe firstpage.copyrights forcomponentsof thisworkowned byothersthan the author s mustbehonored.abstractingwithcreditispermitted.tocopyotherwise or republish topostonserversortoredistributetolists requirespriorspecificpermission and or a fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn ... .
introduction dynamic binary translation dbt is a key enabling technology for many important applications such as whole program analysis hardwaresimulation heterogeneouscomputation offloading and software fuzz testing .
in essence a dbtsystemdynamicallytranslatesexecutablebinarycodefrom aguestinstructionsetarchitecture isa toa hostisa whichcan bedifferentfromorsameastheguestisa.byexecutingthegeneratedhost codeon aphysical hostmachine thedbt systemcan eitheremulatethesemanticsoftheguestapplicationorenhance its functionality e.g.
execution tracing for performance analysis.
despitethevitalimportanceofdbt itisstillquitechallengingto developandmaintainan efficientdbtsysteminpractice.typically thetranslationprocessinadbtsystemmapsaguestinstruction into one or more host instructions which together emulate the semantics of the guest instruction.
given the semantic gaps between differentisas thetranslatedhostbinarycodeoftensuffersfrom significant code explosion.
for example an x86 instruction may be translated into dozens of aarch64 instructions due to the differencesbetweenthetwoisas.therefore ingeneral theperformance of a guest application running with dbt is remarkably worsethan itsnativeperformance.besides tosupportcodetranslationsacross variousisasinonesystem dbtdevelopersusuallyhavetocreatea hugecodebase.evenworse tokeeppacewiththerapidlyevolving hardware architectures existing dbt systemsneed to beupdated frequently to support emerging machine instructions.
these fac tors inevitably render the inherent difficulty of maintaining the performance efficiency of a dbt system.
togiveanexample qemu isawell knowndbtsystemand has been widely used in many research projects and real world products .ithasaround2.8mlinesofsourcecode.everyday anaverageof10commitsareaddedtothecodebasebydifferent developers embodying source lines revised in each commiton average.
as a consequence a tiny and seemingly innocuous modificationtothelargecodebasemayintroduceanunexpected impactontheperformanceofthesystem.indeed wehaveobserved multiple performance regression issues between two successive releaseversions whichencloseanaverageof 2188codecommits.
this apparently makes it quite difficult and time consuming to debug and fix the performance issues.
therefore in this paper we advocate that it is necessary and imperative to conduct performance regression testing during the daily developmentofadbtsystem.thiswillallowdbtdevelopersto notice unexpected performance regressions at a very early stage ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa jin wu jian dong ruili fang wen zhang wenwen wang and decheng zuo andthusreducethetremendousengineeringeffortneededtofixthe problems.
performance regression testing has been demonstrated tobeapracticalapproachtodetectpotentialperformanceinconsistencies between two different versions of a software system.
it has beenextensivelyadoptedduringsoftwaredevelopmentcyclesin commercialcompanies and alargeamount ofresearch work has been devoted to studying and enhancing performance regression testing .
however unfortunately performance regression testing of dbt systems faces several unique challenges.
first the performance of a dbt system is usually evaluated using industry standard and classicalbenchmarksuites suchas speccpu2017 andparsec .duetotheaforementionedperformanceoverheadincurred bydbt ittakesan extremelylong time evenwithmostrecentdbt optimizationsapplied tocompletethetestingof anentirebenchmarksuite.forinstance qemuneedsmorethan three hours to finish the execution of a singlebenchmark program in spec cpu .
note that although these benchmark suites are shipped with input data sets in different scales the smaller data sets are primarily used for correctness verification rather than performance testing.
second different from most software systems a dbtsystemtakesasinput executablebinarycode insteadofregular program data.
this makes it extraordinarily difficult to generate effectivetestinputsforperformanceregressiontesting aseachtest inputneedstobeabinarycodeinguestisacompiledfromatest program.simplyusingartificialtestprogramswouldnothelpbecausetheywill notbeabletoexposethesameperformanceissuesas real benchmark programs.
last but not least dbt systems usually runondiverseplatforms rangingfromservers todesktops mobile and embedded devices.
given the dramatically different computing power ofthese platforms it isobviously unreasonable touse a set offixedtestprogramstoconductperformanceregressiontesting ofadbtsystemonalloftheseplatforms becausethismayleadto either inaccurate testing results on high performance platforms or poor testing efficiency on low power platforms.
to address the above challenges and make dbt performance regression testing possible and practical we propose fadatest in thispaper.
fadatest aimstorealize fastandadaptiveperformance regression testing of dbt systems.
to this end fadatest first intelligently captures runtime characteristics of real benchmarkprograms through dynamic program profiling.
based on the collected information fadatest then automatically generates adaptabletestprogramstopreservethecharacteristicsofthebenchmark programs.
the generated test programs are able to accurately simulate the behaviors and performance results of original benchmark programs whiletheexecutiontimesofthetestprogramsaresignificantly shorter.
therefore the test programs can replace the original benchmark programs to achieve efficient performance regression testing of dbt systems.
furthermore the test size of each testprogramgeneratedby fadatest canbeeasilyscaledup down on demand to fit the target hardware platform of a dbt system.
we have implemented a research prototype of fadatest .t o evaluate the effectiveness of fadatest we employ two major performancebenchmarksuitesofdbtsystems speccpu2017and parsec.
specifically we utilize both the original benchmark programs and the generated test programs to test the performance of twowidely useddbtsystems qemuandvalgrind acrossdifferentreleaseversions.experimentalresults showthattheperformance resultsofthegeneratedtestprogramsstronglymatchwiththoseof the original benchmark programs.
in other words all performance regression issues in the evaluated versions that can be detected by the original benchmark programs are also successfully discovered by the test programs generated by fadatest .
more importantly the testing efficiency is enhanced significantly compared to the original benchmark programs.
this demonstrates the capability of fadatest to conduct efficient performance regression testing of dbt systems.
in addition the evaluation results on a low power aarch64 platform show that the adaptability of the generated test programs allows fadatest to achieve the high testing efficiency on this hardware platform without loss of testing accuracy.
in summary this paper makes the following contributions we present fadatest whichintegrates noveltechniques to realizeefficientandadaptiveperformanceregressiontestingofdbtsystems.
fadatest automaticallygeneratestestprogramsforperformanceregressiontestingaccordingtothe dynamic characteristicsof real benchmarks.
weimplementaresearchprototypeof fadatest .theprototype supports two mainstream isas on the market x86 andaarch64.ourimplementationalsoovercomesseveral technical obstacles caused by executing special hardware instructions in the generated test programs.
weconductcomprehensiveexperimentstoevaluatetheeffectivenessof fadatest .theevaluationresultsshowthat fadatest can successfully reveal performance regression issuesofdbtsystems.comparedtotheoriginalbenchmark programs the testing efficiency is significantly improved.
background and motivation how dbt works?
ingeneral a dbtsystemtranslatesguestbinary code at the basic block or block granularity.
each block contains a sequence of guest instructions with at most one branchinstruction at the end of the block.
that is a block has only one entryandoneexitandtheexecutionofablockissequential.the translated host binary code is saved to a software code cache to mitigate the translation overhead as a block may be executed multipletimesinthesameexecution.oncethetranslationofablock is completed the execution flow is transferred to the code cache so that the translated host binary code can be executed.
therefore the execution of a dbt system is typically interleaved by the code translation and the execution of the translated host binary code until all dynamically discovered blocks are translated.
highperformanceoverheadofdbt.
due to the significant semanticgapsbetweendifferentisas thetranslatedhostbinarycode oftensuffersfromsubstantialcodeexplosion whichundoubtedly leadstoheavyperformanceoverhead.togiveanexample figure showsthenormalizedexecutiontimesofqemu version5.
.
and valgrind version .
.
with benchmark programs in parsec ontwodifferenthardwareplatforms x86 64andaarch64.more detailsabouttheconfigurationofthetwoplatformscanbefound in section .
the performance baseline is the execution time of the corresponding benchmarks running natively on the x86 platform.
as we can see from the figure qemu introduces an average of and performance slowdown for the x86 and aarch64 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fadatest icse may pittsburgh pa usa blackscholesbodytrackcannealdedup fluidanimatefreqmine streamclusterswaptionsvipsgmeannormalized execution timex86 native aarch64 native valgrind x86 64qemu x86 qemu aarch64 figure1 normalizedexecutiontimesofqemuandvalgrind onx86 64andaarch64platforms.thebaselineisthenativebenchmark execution time on the x86 platform.
platforms respectively.similarly valgrindintroducesanaverage of28 performanceslowdownonthex86 64platform.wefailed to collect the data of valgrind on the aarch64 platform due to the unbearably long execution time.
this result demonstrates the slow executionofprogramsrunningatopadbtsystem.fromthisfigure wecanfurtherconcludethattheperformanceofthebenchmarksis much worse on the aarch64 platform i.e.
qemu aarch64 .
this is mainly because the computing power of the aarch64 processoris much lower than the x86 processor on our platforms.
on the other side this shows the difficulty to achieve a similar testing efficiencywhenusingthesameprogramstotesttheperformance of a dbt system on different hardware platforms.
performanceregressions ofdbt.
given the inherent complexity of modern isas developers have to manually create a huge code base to support various guest and host instructions in a dbt system.forexample oneofthesourcefilesrelatedtotranslating aarch64instructionsinqemucontainsaround15klinesofsource code.besides toimprovethetranslationquality aswellassupport emerginghardwareinstructions suchassingleinstructionmultiple data simd instructions frequentchangestothesourcecodeare very commonduring the development and maintenanceprocess.
this unavoidably leads to performance variance across different versions of a dbt system.
for example figure 2illustrates the performance results of different release versions of qemu withthe swaptions benchmark in parsec under different numbers of threads.
as shown in the figure the performance of qemu is not consistent across different versions e.g.
the performance is decreased significantly from .
.
to .
.
.
though the performance is increased later on in .
.
it is not clear whether the previous performancedegradationisfixedornot.therefore inordertomonitor and maintain the performance efficiency of a dbt system it is necessary and urgent to conduct performance regression testing during the daily development of the dbt system.
fadatest in general there are several principles for designing a practical performance regression testing approach for dbt systems high testing efficiency .
given the heavy performance overhead incurred by dbt systems the testing time should .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0normalized execution time1 thread thread4 thread thread16 thread figure normalized execution times of different versionsof qemu with the swaptions benchmark in parsec under different numbers of threads.
the baseline is the executiontime of qemu version .
.
of each thread configuration.
be sufficiently short.
this allows the testing approach tobe used during the daily development of a dbt system to rapidly discover performance regression issues.
high testing accuracy .
the testing approach is expected to have a strong capability to accurately uncover potentialperformanceregressionsofadbtsystem.thismeans representativedbtperformancebenchmarksuitescanbe replaced to avoid extremely long testing times.
high testing adaptability .
since a dbt system may run on a broad range of hardware platforms it is necessary and requiredthatthetestingapproachcanbeadaptedseamlesslytotheactualtargetplatformofthedbtsystemwithoutloss of the high testing efficiency and accuracy.
low manual effort .
it is typically not acceptable if the testingapproach needs toputadditional engineeringeffort ondbtdevelopers.moreover thetestingapproachshould beveryeasytouseandhelpfulsothatdbtdeveloperswould like to adopt it in their development cycles.
the key to realize practical performance regression testing of dbtsystemsistogenerateshortyeteffectivetestprograms whichcanbeusedtotestdbtperformanceregularly.ana veapproachistorandomlysampletheexecutionofaselectedbenchmarkprogram to collect a list of basic blocks and then assemble them together to form atest program.though thisapproach canpotentially shorten the testing time it does not comply with the above principles.
for example itishardtodetermineanappropriatesamplingratesothatthegeneratedtestprogramcanachieveacceptabletestingefficiencyandaccuracy.also thetestprogramsgeneratedusingthisapproach arefixedandthushardtobescaledup downtofitdifferenthardware platforms.
in contrast the design of fadatest is compliance with these principles.
it combines several novel techniques to realizefast accurate andadaptiveperformanceregressiontestingof dbtsystems.next wepresentahigh leveloverviewof fadatest followed by a detailed description of each component.
.
system overview fadatest takes several steps to generate a test program from a seed dbt performance benchmark program.
the seed benchmark programmaycomefromanexistingbenchmarksuite e.g.
spec authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa jin wu jian dong ruili fang wen zhang wenwen wang and decheng zuo 6hhg hqfkpdun 3urjudp hfxwlrq 3urilolqj7hvw 3urjudp 6fdolqj 8s 6fdolqj rzq ljk 3huirupdqfh rpsxwhu pehgghg rz 3rzhu hylfh orfn rwqhvv7kuhdg fwlylwlhv lvwulexwlrq 3dwwhuq qdo vlv7hvw 3urjudp hqhudwlrq vwhp vwhp 7hvwlqj 7hvwlqj figure system overview of fadatest.
cpu or parsec or provided by dbt developers with specificperformancetestingpurposes.figure 3showsthehigh level workflow of fadatest.
as shown in the figure fadatest first collects runtime characteristics of the seed benchmark program through online execution profiling.
this enables fadatest to understand the dynamic behavior of the seed program.
next fadatest performs an in depth analysis on the gathered information to determine the distribution patterns of the program.
this allows fadatest to generate adistributionpatterntosummarizehowthebasicblocksaredis tributed across different threads and how the number of threads influencessuchadistribution.withthisdistributionpattern the finalstepof fadatest istogeneratethetestprogram.morespecifically fadatest includesallblocksthatmayaffecttheperformance ofthedbtsysteminthetestprogram.besides thetestprogram isparameterizedintwodimensions.first thedynamicexecution counters of blocks can be adjusted to increase decrease the testing time.second thenumberofthreadscanbechangedtotestspecificthreadsettingsofthedbtsystem.bothofthemcanbeeasilytuned through the options of the test program provided to users.
this way the generated test programs can test dbt performance on different hardware platforms as shown in the figure.
.
program execution profiling in theory we can extract a test program statically from a seed benchmarkprogram ateithersourcecodelevelorbinarycodelevel.however thisapproachmayleadto inaccurate testingresultsdueto the lack of the important dynamic execution characteristics of the seedbenchmarkprogram.toavoidthisproblem thefirststepof fadatest is to collect the execution profile of the seed benchmark program.
to this end fadatest executes the seed program with thestandardinputdatasetprovidedbythebenchmarksuite.for example fadatest uses thereference input to collect the runtime characteristics of benchmark programs in spec cpu .
note that spec cpu also provides the testandtraininputs which are smaller than reference but they are generally not intended for performancetestingandthusnotconsideredby fadatest .during the execution of the seed program fadatest collects two major types of profiling information block hotness andthread activities.
qxpbwkuhdgv qxpbwkuhdgv qxpbwkuhdgv figure4 theactualnumberofthreadscreatedbythebench mark bodytrack in parsec is different from the number of threads specified by the input parameter num threads.m main thread w worker thread i i o thread.
block hotness profiling.
here the hotness of a basic block meansthe timestheblockis executeddynamicallyunderthe providedinputdataset.thereasonwhy fadatest collectsthehotness information at the basic block level is because basic block is the translationunitofdbtsystems.a hotbasicblocktypicallyimplies that the block is executed frequently and thus more likely to account for a higherproportion of the entire execution time of the dbt system compared to a coldblock.fadatest creates a hotness counterfor every basic block translated by the dbt system and increasesthecounterbyoneeachtimewhenthetranslatedhostbi narycodeoftheblockisexecuted.notethatthehotnesscounteris thread private which means each thread has its own block hotness data.
this allows fadatest to perform a thread aware distribution pattern analysis in the following step.
thread activity profiling.
thread activity information includes thespecifictimepointsatwhichathreadiscreatedandterminated.insteadofusingtheabsolutetime fadatest employsrelativetimes whicheliminateanypotentialinaccuraciescausedbyuncertainty factors suchasthreadsynchronizationandscheduling.specifically when a thread is created terminated fadatest marks down the correspondingexecutionpointofitsparentthread i.e.
thenumber of basic blocks that have been executed in the parent thread.
with this thread activity information the test program generated by fadatest is able to exhibit a similar thread behavior to the seed program.
this is important as the generated test program maybe used to test dbt performance with various thread settings whichcannotbeprofiled completely inadvance.forexample atest program generated based on the execution profiles of and threads may be used to test the performance of threads or more.
.
distribution pattern analysis the purpose of the distribution pattern analysis is to determine howtheexecutionofabasicblockisdistributedamongdifferent threads so that the generated test program can faithfully simulate the thread behaviors of the seed benchmark program.
in particular the distribution pattern analysis aims to answer the following two questions.
first how is the execution of a basic block changed whenthenumberofthreadsisscaledup down?second whatisthe exact distribution of a basic block among different threads under a specific number of threads?
to answer these questions fadatest conductsacomprehensivedistributionpatternanalysisbasedon the profiled block hotness and thread activity information.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fadatest icse may pittsburgh pa usa algorithm thread grouping analysis input hotnesslist the list of block hotness arrays where each array corresponds to a block and is indexed by thread id output resultgroups the result thread groups 1numthreads getnumthreads hotnesslist 2resultgroups ... numthreads 3foreachhotness array hainhotnesslist do 4haszero false 5hasnonezero false 6foreachgroupginresultgroups do forthread id tidingdo ifha 0then haszero true else hasnonezero true end end ifhaszero andhasnonezero then g1 g2 splitgroup g resultgroups.remove g resultgroups.append g1 g2 end 19end 20end 21returnresultgroups threadgroupinganalysis.
generally amulti threadedprogram exports to users an adjustable parameter e.g.
num threads which allows users to specify the number of threads for the execution oftheprogram.intuitively thevalueof num threadsisthesame astheactualnumberofthreadscreatedbytheprogram.but our observations on the multi threaded benchmark programs from parsecshowthattheactualnumberofthecreatedthreadsisprobablygreaterthan the specified value of num threads.
for example bodytrack isaparsecbenchmark.whenwerunthisbenchmark with a specified number of threads i.e.
num threads the actual number of the created threads is .
similarly if the specified number of threads is and the actual number of the created threads is and respectively.
figure 4illustrates this behavior.
furtherinvestigationshowsthatthisbenchmarkalwayscreatesa main thread an i o thread and num threadsworker threads.
in other words the main thread and the i o thread do notscale when the value of num threadsincreases.
therefore the first step of the distributionpatternanalysisin fadatest istounderstandhowthe threadsof theseed benchmarkprogram aregrouped andhowthe number of threads in each group scales.
to this end fadatest analyzes the profiled basic block hotness datatoidentifythethreadgroupingpattern.thisisinspiredbythe key observation that the threads in the same group should execute same similar basic blocks.
by searching for group private blocks which are executed by one group but not others fadatest can rapidly identify thread groups in an execution profile.
fadatest represents the thread grouping pattern using a two dimensional array 1 1 2 2 ... n n where i num threads iis the actual number of threads in the group i i n.table thread grouping patterns identified by fadatest for seed benchmark programs in parsec.
thread grouping pattern blackscholes bodytrack canneal dedup fluidanimate freqmine streamcluster swaptions vips algorithm 1describeshow fadatest identifiesthreadgroups.
initially thereisonlyonethreadgroup.theanalysisattemptsto split the initial group into multiple groups by scanning the hotness data of all basic blocks to identify group private blocks.
a group is split into two groups if at least one thread in the group executessome blocks that are not executed by any other thread s .
by ap plying this algorithm to multiple execution profiles of the same seedbenchmarkprogramwithdifferentnumbersofthreads e.g.
and fadatest canobtainmultiple threadgroupingresults.
withthesegroupingresults fadatest canfurtherinferthethread groupingpatternbyanalyzingtherelationshipbetweenthetotal number of threads in each group and the value of num threads.
recall the bodytrack benchmark in figure the thread groupingpatternidentifiedby fadatest is .we further list the thread grouping patterns identified by fadatest for seed benchmark programs from parsec in table .
the tableshowsthateverybenchmarkhasmorethanonethreadgroup.
streamcluster evenhassevengroups.thisexplainsthenecessity of the thread grouping analysis.
indeed without this information it will be extremely hard if not impossible to correctly scale up thenumberofthreadsinthegeneratedtestprogramandmatchthe thread behaviors of the seed benchmark program.
block hotness pattern analysis.
withthethread groupinginformation fadatest next analyzes the distribution patterns of basic block hotness.
fadatest achieves this through two steps.
first fadatest identifies the block hotness patterns at the thread group level i.e.
the relationship between the total execution counters of a basic block inside a group and the number of threads in the group.
second fadatest figures out the distribution patterns of basic block hotness among threads inside a group.
note that fadatest analyzes the hotness distribution pattern for eachbasic block as different blocks often exhibit different behaviors.
given a thread group gwithmthreads fadatest creates a simple yet accurate model to compute the total execution count of a block in g m where and are the model parameters and can be calculated based on the method of least squares approximation using the profiled execution counts of the block in gwith differentthreadnumbers.supposetheactualexecutioncountofthe blockisc1andthecountcomputedbythemodelis c2.theerror rateofthemodelisdeterminedby c1 c2 c1.ourexperienceswith standardmulti threadeddbtperformancebenchmarkprograms show that within an error rate less than this model is able to authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa jin wu jian dong ruili fang wen zhang wenwen wang and decheng zuo 16executed times k by each thread 16by each thread figure5 hotnessdistributionpatternsoftwodifferentbasic blocks of bodytrack in the same thread group.
the x axis is the number of threads in the group.
predict the execution counts of more than basic blocks.
for the remaining blocks they often have very small execution counts andthuscanbeexcludedfromthegeneratedtestprogram.tosummarize fadatest canaccuratelycomputethehotnesscountofa basic block for each thread group through this model.
next fadatest analyzesthedistributionpatternofablockinside a group.
it is worth pointing out that this pattern typically doesnotchangewhenthenumberofthreadsinthegroupchanges.
fadatest uses a vector to denote the pattern 1 2 ... m wheremis the number of threads in the group and summationtext.1m i 1 i .
to simplify the design fadatest heuristically treats a distribution pattern 1 2 ... m as an evenly distributed pattern i.e.
1 2 ... m m i f i ... m i m m .
.
this allowsfadatest toaccuratelycapturethepatternformorethan basic blocks.
figure5showstwodifferentdistributionpatternsanalyzedby fadatest for two basic blocks of the same thread group in the bodytrack benchmark.
as shown in the figure the total execution countofthefirstblockdoesnotchangeasthenumberofthreadsin the group increases while the total execution count of the second block increases.
but for both blocks the total execution counts are evenly distributed among different threads in the group.
.
test program generation fadatest adopts the c language for the test program as it is an efficientsystemlanguageanditisquitestraightforwardtointegrate theassemblycodeofbasicblocksintothetestprogram.toallow users to easily scale up down the execution time of the test program fadatest generates each test program with a scaling factor whichisusedtocalculatetheactualexecutioncounterofabasic blockbbfor each thread t actualexcutioncounter bb t hotnesscounter bb t here the hotness counter of bbfortis calculated using the distribution pattern derived from the previous analysis.
the value of can be tuned each time when the test program is used to test dbt performance on a new hardware platform.
in case the actual execution countof abasic blockbecomeszero i.e.
is higher than its hotness counter the block will be skipped in the testing.
for a multi threaded test program a simple execution mechanismistolaunchallthreadsatthebeginningoftheprogram.however thismayintroducepotentialinaccuratetestingresults.thereasonisthattheexecutionofamulti threadedprogramusuallyalgorithm test program generation input hotnesslist the list of block hotness arrays threadgrouplist the specification of thread groups threadactivitylist the specification of thread activities output tcode the generated source code of the test program 1tcode null 2foreachthread group id tgidinthreadgrouplist do 3tcode tcode gencodethreadfunchead tgid 4foreachbasic block bbinhotnesslist do tcode tcode genpayloadbb bb tgid 6end 7tcode tcode gencodethreadfunctail tgid 8end 9tcode tcode gencodectrlfunchead 10foreachthread group id tgidinthreadactivitylist do 11tcode tcode gencodethreadactivity tgid 12end 13tcode tcode gencodectrlfunctail tgid 14returntcode interleavesmulti threadedexecutionphaseswithsingle threaded executionphases.hence thetotalexecutiontimeoftheprogramshouldcombinetheexecutiontimesofallphases.inotherwords the test program generated by fadatest should preserve both multi threadedphasesandsingle threadedphasesintheseedbenchmark program.
therefore fadatest integrates the profiled thread activity information into the generated test program.
in particular the relative execution points of thread creation and termination events are the same as those in the seed benchmark program.
this way the multi threaded test program generated by fadatest can achieve more accurate performance testing results.
algorithm 2shows the process of generating a test program in fadatest .
an example of the generated test program is illustrated in figure .
the major components of the test program include several functions init run thread 0 fini andmain.
themain function serves as the driver of the test program.
the initand finifunctions allocate and deallocate memory resources required to run the test program respectively.
fadatest analyzes the instructionsineachbasicblocktodeterminetheexactmemoryresourcerequiredbytheblock seesection 4formoredetails .the run thread 0 function is the test function which contains the assemblycodeofbasicblocks.here istheindexofthethread group.
that means fadatest generates a test function for each thread group rather than each thread.
this design choice allows fadatest to limit the size of the test program and reduce the additionalpressureonthecodecacheofthetargetdbtsystem asall executed blocks will be translated and stored in the code cache.
implementation wehaveimplementedaprototypeof fadatest withthesupport oftwomainstreamisasonthemarket x86 64andaarch64.the execution profiler is implemented at the binary code level using thedbtsystemqemu .notethat fadatest isnottiedtoany specific dbt system and in fact the generated test programs canbe usedtotest variousdbt systems.thedistribution pattern authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fadatest icse may pittsburgh pa usa include ... define ... 3structtimeval tv begin tv end unsigned char ptr mem base 4voidrun thread 0 unsigned int percent ... other declarations 6voidinit void 7ptr mem base alignment16 malloc sizeof uint64 t 8ptr double float malloc sizeof double 9tv begin calloc sizeof struct timeval ... more initialization 12voidrun thread 0 unsigned int percent executes basic blocks according to hotness and scaling factor for long i i percent i asm volatile movq r15 n t prepares memory movq r10 n t repeats times per iteration loop 0 0 movl 0x2f r15 edx n t cmpl ebx edx n t jne jmp hit 0 0 n t branches to a provided target test r15 r15 n t jmp hit 0 0 dec r10 n t test r10 r10 n t jnz loop 0 0 n t m ptr mem base rdx r10 r15 ... more basic blocks ... more threads 30voidfini void ... releases memory 31intmain int argc void argv 32init runs threads according to thread activity profiling 34run thread func 0 executes first of main thread.
for int i i n thread i runs threads 36pthread create tid null void run thread func 1 percent for int i i n thread i 40pthread join tid null 42run thread func 0 executes remaining of main thread.
43fini return figure a test program generated by fadatest.
the code is simplified for demonstration.
analyzerandthetestprogramgeneratorareprimarilydevelopedinpython .4kloc .themajorobstacleweencounteredduringthe implementationwas howtolegitimately executetheinstructions in the basic blocks included in the generated test programs.
we next elaborate our solutions for each type of instructions.
memory access instructions.
to correctly execute a memory access instruction we needto passan accessiblememory address to the instruction.
in general the memory address accessed by an instruction is encoded through four elements and calculated based ontheformula base index scale offset.here baseisrequired whileindexandoffsetareoptional.
scaleisaconstantof1 or .
besides baseis typically provided through a register.
thus our implementationallocates amemoryregionin advanceandpasses appropriatevaluestorelatedregisterssothatthecalculatedaddress fallsintotheregion.notethatmemoryaccessinstructionsinthe sameblockcanshareonememoryregiontoreducetheoverhead caused by memory allocation and data preparation.
direct branch instructions.
since the generated test program does not recover the control flow of the seed program we need to update the target of a branch instruction.
otherwise the ex ecution of the branch instruction may experience an error if the branchtargetisunreachable.hence werevisethetargetofabranch instruction to the instruction immediately following the branch instruction.
furthermore several instructions are inserted betweenthebranchandthetargetincasethebranchisaconditionalbranch.indirectbranchinstructions.
ourimplementationalsoneedsto take care of indirect branches.
this is achieved by first placing a reachable code address into a memory location and then replacing the operand of an indirect branch with the address of the location.
this allows the indirect branch to be executed correctly.
call ret instructions.
in our implementation call ret instructions are not executed directly.
instead a call instruction is de composed into two instructions.
the first one saves the returnaddress and the second one branches to the call target which isreplaced with a reachable code address.
like most dbt systems ret instructions are handled in the same way as indirect branches.
floating point instructions.
inappropriate floating point operations may produce floating point exceptions e.g.
division by zero.
giventhataprogramwillbeterminatedonceafloating pointexception is triggered we need to avoid such operations.
to this end our implementation analyzes all floating point instructions in a basic block and carefully prepares the right floating point data for them so that no exception will be triggered.simdinstructions.
similar to floating point instructions our implementation feeds simd instructions with appropriate floatingpointvaluestoavoidpotentialfloating pointexceptions.inaddition simdinstructionsgenerallyaccessmultiplememoryitemssimultaneously.
this requires our implementation to pay special attention to calculate the actual size of the required memory and provide aligned addresses for simd instructions.
systemcalls.
ifablockcontainsasystemcall ourimplementation firstly analyzes the block to determine whether the system call numberisdefined.incasetheinstructionthatdefinesthesystem callnumberisnotfoundinthisblock anadditionalinstructionwill be inserted before the system call instruction to define the system call number.
we use the getpidsystem call because it does not require a parameter and also has no impact on the system state of thehostplatform.forsimplicity fadatest doesnotensurethatthe systemcallparametersarepreparedcorrectly.thisisreasonable because a system call may fail even in a normal execution.isa specific instructions.
some isa specific instructions may have special semantics and access implicit operands.
for example the x86 cpuidinstruction takes as input the value in raxand returnsthecpufeaturesto rax rbx andrcx.ourimplementation ensuresthecorrectexecutionofsuchaninstructioninabasicblock by analyzing the instructions before it and inserting additional instructions to initialize the input value if necessary.
experimental results inthis section we evaluate fadatest .
theevaluationaims toanswer the following research questions i how efficient are the test programs generated by fadatest when used to test dbt performance?
ii can the test programs generated by fadatest uncover the same performance regression issues of a dbt system as the originalseedbenchmarkprograms?iii howefficientisthetestprogramgenerationprocess?thedataandsourcecodeareavailable at experimentalsetup.
inourevaluation weuse fadatest togenerate test programs from benchmark programs in two standard authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa jin wu jian dong ruili fang wen zhang wenwen wang and decheng zuo blackscholesbodytrackcannealdedup fluidanimatefreqmine streamclusterswaptionsvipsgmeantesting performance speedupqemu aarch64 fadatest qemu x86 fadatest valgrind x86 fadatest qemu x86 rdmsmpl figure testing performance speedup achieved by the test programs generated by fadatest and random sam pling rdmsmpl .
the performance baseline is the testingtimes of original benchmarks.
higher is better.
benchmarksuites speccpu2017andparsec version3.
whichhavebeenextensivelyusedtotesttheperformanceofdbtsystems.foreachspeccpu2017benchmark fadatest onlyneedstorunit oncetocollecttheexecutionprofile asspeccpu2017benchmarksaresingle threaded.forparsecbenchmarks fadatest runsthem with and threads to collect multiple execution profiles for distributionpatternanalysis butteststhemwith8threadstoshow the capability of fadatest to scale up the number of threads.
the generatedtestprogramsareemployedtotesttheperformanceof two representative dbt systems qemu and valgrind with and historical versions respectively.
apart from comparing with the testing results of the original benchmarkprograms wealsoimplementarandomsamplingmechanism and compare it comprehensively with fadatest .
however ifthesamplingfrequencyistoohigh itwillsignificantlyprolong thesampling process.moreover itneeds extrastorageto savethe sampledbasicblocks.hence weadoptasamplingintervalof basicblocksandlimittheentiresamplingprocessto10hoursfor a benchmark program.
even with such a long time we still can onlycollectblocksforthreeparsecbenchmarks.thisalsodemonstrates the impracticabilityof the random sampling approach.
to demonstrate the adaptability of the generated test programs the evaluation covers two hardware platforms powered by x86 andaarch64 respectively.thex86 64platformisequippedwith an intel i9 cpu at .1ghz and 32gb main memory while the aarch64platformhasarockchiprk3399cpuat2.0ghzwith4gb memory.onthex86 64platform qemutakesasinputaarch64 guestbinaries whileontheaarch64platform itrunsx86 64guest binaries.
however qemu fails to run x86 binaries of original spec cpu benchmarks on the aarch64 platform.
also there is no data for valgrind on aarch64 due to the significantly long executiontimeofvalgrind.ittakeslessthan3minutestomanually tunethescalingfactor foratestprogramononeplatform.the twoplatformsareoccupiedexclusivelybyourevaluationtoreduce any potential impactsof random factors.
inaddition we run each test times and use the average value of them as the final result.
.
testing efficiency figure7showsthetestingperformancespeedupachievedbythe test programs generated by fadatest when used for testing the performance of qemu and valgrind on the two hardware platforms.
the performance baseline is the testing time of the original benchmark programs.
the results of randomsampling are also includedforreference.sincetherearemultipleversionsofqemuand valgrind we measure the testing performance speedup of every version for each benchmark and use the geometric mean as the final speedup result of the benchmark.
as shown in the figure the testing performance is significantly improved by the test programs generatedby fadatest comparedtotheoriginalbenchmarks with an average speedup of for qemu aarch64 for qemux86 and96 forvalgrind x86 .itisnotasurprisethatrandom sampling can also achieve testing performance speedup as it skips theexecutionofmanybasicblocks.onaverage itonlytakesaround 35secondstocompletethetestingofatestprogramgeneratedby fadatest .this showsthecapability of fadatest tooffer ahigh testing efficiency for dbt performance testing.
.
testing effectiveness qemu.figure8shows the performance testing results of differentversionsofqemuonthex86 64platformusingeachoriginal benchmarkprograminspeccpu2017andparsecandthecorrespondingtestprogramgeneratedby fadatest .thetestingtimes are normalized to the version .
.
so both of the two lines start from1.duetothespacelimitation weomitthetestingresultsof qemu aarch64 which are very similar to qemu x86 .
from figure we can make two observations.
first the performance of qemu x86 changes frequently across different versions.
for some benchmarks such as perlbench andleela there is a clear performance regression from the version .
.
to the version4.
.
.forsomeotherbenchmarks e.g.
canneal andvips the performanceregressionstartsearlier i.e.
fromtheversion2.
.1to the version .
.
.
this phenomenon again suggests that it is highly necessary to conduct performance regression testing for dbt systems.moreover acomprehensivetestsuiteisrequiredtoexpose theperformanceregressionsasearlyaspossible.second thetesting results of the test programs generated by fadatest highly match withthoseoftheoriginalbenchmarkprograms.inparticular the performance changes uncovered by testing the original benchmark programs can also be captured by testing the generated test programs.thisshowstheeffectivenessofthetestprogramsgenerated by fadatest for dbt performance regression testing.
valgrind.
figure9shows the performance testing results of valgrind using the original benchmark programs and the corresponding test programs generated by fadatest .
similarly the testing times of the version .
.
are used as the performance baseline.
as shown in the figure valgrind also suffers from performance regressions e.g.
from the version .
.
to the version .
.
for perlbench .furthermore the testingresults ofthetest programs generated by fadatest perfectly match with those of the original benchmark programs.
this further demonstrates the effectiveness andtheportabilityof fadatest fordbtperformanceregression testing across different dbt systems.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fadatest icse may pittsburgh pa usa .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0blackscholes .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0bodytrack .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0canneal .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0dedup .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0fluidanimate .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0freqmine .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0streamcluster .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0swaptions .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0vips original fadatest .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0perlbench .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0gcc .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0mcf .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0omnetpp .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0xalancbmk .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0normalized execution timex264 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0deepsjeng .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0leela .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0exchange2 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0xz figure testing effectiveness of the test programs generated by fadatest with different versions of qemu aarch64.
.
.
.
.
.
.
.
.
.
.
.
.0normalized execution timeperlbench .
.
.
.
.
.
.
.
.
.
.
.0omnetpp original fadatest .
.
.
.
.
.
.
.
.
.
.
.0dedup figure testing effectiveness of the test programs generated by fadatest with different versions of valgrind.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0normalized execution timecanneal .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0dedup original fadatest rdmsmpl .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0freqmine figure test effectiveness of the test programs gen erated by fadatest and the random sampling mecha nism rdmsmpl with different versions of qemu.
comparingwithrandomsampling.
wenextcomparethetesting results of the test programs generated by fadatest and those generatedusingtherandomsamplingapproach.wealsoinclude thetestingresultsoforiginalbenchmarksasreference.figure shows the results of some benchmarks for qemu.
from the figure we can clearly see that random sampling cannot achieve testing resultsthatalignwiththose oforiginalbenchmarkprograms.for example randomsamplingfailstouncovertheperformancefluctuation after the version .
.
for the benchmark canneal.
the100101102103104105 blackscholesbodytrackcannealdedup fluidanimatefreqmine streamclusterswaptionsvips averageabsolute execution time s online aarch64 offline aarch64online x86 offline x86 figure absolute execution times in seconds of the two stages of the test program generation process in fadatest online profiling and offline analysis and generation.
reason is that the test programs generated by random samplingcannot preserve the program specific runtime characteristics of originalbenchmarkprograms.incontrast fadatest generatestest programsbasedontheprofiledblockhotnessandthreadactivity information which enables it to achieve a testing effectiveness similar to original benchmark programs.
.
test program generation process study performanceoverhead.
therearetwomajorstagesin fadatest togenerateatestprogramfromabenchmarkprogram onlineprofil ingandofflineanalysisandgeneration.figure 11showstheabsolute authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa jin wu jian dong ruili fang wen zhang wenwen wang and decheng zuo table sizes of log files generated by fadatest in bytes .
a64 aarch64 x64 x86 .
benchmark a64 x64 benchmark a64 x64 blackscholes 327k 510k freqmine 437k 638k bodytrack .9m .0m streamcluster .9m .9m canneal 578k 826k swaptions 377k 592k dedup .3m .8m vips .8m .8m fluidanimate 510k 759k average .5m .2m table3 comparisonsofthetestprogramgenerationprocess between fadatest and random sampling rdmsmpl .
canneal dedup freqmine online fadatest 882s 116s 2188s profiling rdmsmpl 5157s 1453s 33465s offline fadatest .6s .63s .95s analysis rdmsmpl .79s .94s .41s log sizefadatest 578kb .3mb 437kb rdmsmpl 262mb 78mb .8gb times required by the two stages for each benchmark and the two isas.weomittheresultsofspeccpu2017benchmarksduetothe space limitation.
for online profiling the time is closely correlated totheoriginalexecutiontimeofeachbenchmarkprogram.onaverage the execution time of online profiling in fadatest compared totheoriginalbenchmarkexecutiontime isaround2 .
.wethink this performance overhead is reasonable and also acceptable as the test programs only need to be generated once.
compared to the online profiling stage the offline analysis and generation stage takesmuchlesstime.asshowninthefigure forallbenchmarks the generation time is less than seconds.
this demonstrates the highefficiencyoftheanalysisandgenerationprocessin fadatest .
storagecost.
we also study the cost of storing the log files generated byfadatest during the online profiling stage.
table 2shows the detailed sizes of the log files generated for each benchmarkand the two isas.
as shown in the table for all benchmarks the size is less than 3mb.
on average it only costs around .5mb for a benchmark and one isa.
we believe this storage cost is acceptable in practice given that the capacity of a typical storage device is up to several gigabytes even on embedded devices.
comparing with random sampling.
we further compare the test program generation process of fadatest with the random sampling approach.
table 3shows the results.
as shown in the table ittakesamuchlongertimeforrandomsamplingtofinishthe generation process.
this is because the sampled blocks need to be savedduringthesamplingprocess.moreover thesizesofthelog files generated by random sampling are much larger.
these results show the impracticability of the random sampling approach.
case study in this section we show two representative case studies on qemu to demonstrate the practicability and necessity of conducting performance regression testing through fadatest during the daily development of a dbt system.
.
.
.
.
1700absolute testing time s commit from old to newcommit 888ea96a figure performance regression by a single commit.
a performance regression caused by a performance bug.
in thiscasestudy wefirstobservedaperformanceregressionissueof theperlbench benchmarkunder the reference inputbetween two commits eabb7b91 and 757e725b which include1764othercommitsbetweenthem.thatmeans tofigureout whichcommit s leadtotheregression itisnecessarytoconduct comprehensive performancetesting on individualcommits.
given thelargenumberofthecommits itwillbeverytime consuming toconductsuchtestingusingtheoriginalbenchmarkbecauseof the long execution time of qemu.
note that we didn t observe a performancedifferenceifthe testinputofperlbench isusedforthe testing.
also it may take more than hours to test the commits with thetraininput.
in contrast with the fadatest generated test program wecompletetheperformancetestingofthe1764commits in around hours.
figure 12shows the testing result.
fromthefigure wecanclearlyseethatthisperformanceregression issue is caused by a single commit i.e.
888ea96a .
furtherinvestigationshowsthatthiscommitessentiallyremoved the manuallyenforced always inline attribute because the developeroveroptimisticallyassumesthat compilershaveimproved... andweareprobablybetterofftrustingthecompilerratherthantry ingtoforceitshand.
unfortunately however thecompilerisnotassmartashumandeveloperstomakeanoptimaldecisiononwhether a function should be inlined or not.
on the other hand inlining is an important compiler optimization that can enable many other optimizations e.g.
deadcodeeliminationandconstantpropagation.
that is why this commit leads to a performance regression.
a performance regression caused by extended functionalities.this case study centers on the performance regression of qemu for the perlbench benchmark between the two release versions .
.
and3.
.
asshowninfigure .
thereare5655commitsbetweenthesetwoversions.byconducting performancetestingusingthetestprogramgeneratedby fadatest two commits that caused the regression are identified in around 42hours f7b78602 and c47eaf9f .the first commit added additional code to check whether a translation blockisvalidfordifferentclusters.sincethecodewasaddedonthe criticalpathofthetranslationprocess itintroducedasubstantial performance slowdown.
the second commit mainly added the supportofarmpointerauthenticationinstructions whichweresimplyignoredbeforethiscommit.asaresult itwouldinevitablydecrease the performance if the guest application has such instructions.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fadatest icse may pittsburgh pa usa summary.
the above case studies clearly demonstrate the benefit ofthehightestingefficiencyofferedby fadatest onidentifying problematiccommitsthatcauseperformanceregressions.atthe same time it shows the necessity of conducting performance re gression testing during the daily development of a dbt system.
here wewouldliketoemphasizethatthehightestingefficiency offadatest also provides a performance centric view for developers oneach commit.
this isimportant even though thecommit istoaddanewfeaturetothedbtsystemandexpectssomeper formance slowdown as the testing will tell developers the exactperformanceslowdownandallowsthemtoimmediatelyremedy the performance loss if necessary.
related work performance regressiontesting.
a considerable amount of research work has been conducted to enhance both effectivenessand efficiency of performance regression testing.
some research work leverages automated workload generation to produce proper input data sets for performance regression testing.
someresearch work creates selective strategies to attain better testing efficiency .
bagherzadeh et al.
analyze the performance resultsofsystemcallsinhistoricversionsofthelinuxkernel .
wise presents an automated test generation techniques to find performancebugs .theresearchworkin proposestoutilize performance counters to detect performance regressions.
toobtainafasttestingreport someresearchworkstudiestestcaseprioritization approaches .
perfscope improves the testing efficiency by conducting performance risk analysis for prioritization.
to test the performance of multi threaded programs speedgun generates multiple threads from a single threaded application .
perfimpact sends the same input to two releases and automatically mines the corresponding execution traces to rank the impacts of code changes .
forepost detects performance issuesbyadoptingmachinelearningtechniquestoselectinputdata to cover computationally intensive program paths .
though performance regression testing has been explored comprehensively in common software systems it is still very challengingtodirectlyapplyexistingperformanceregressiontechniques to dbt systems.
there are several reasons.
first different from commonsoftwaresystems dbtsystemstakeasinputexecutable binaries which make it quite difficult to automatically and flexibly generate representative inputs to conduct performance regression testingofdbtsystems.second duetothelowefficiencyofdbt systems it takes extremely long time to complete the testing of standard benchmark programs on dbt systems.
third the diverse hardware platforms of a dbt system make it hard to use the same benchmarkprogramstotesttheperformanceofthedbtsystemon different hardwareplatforms especially whenthe platformshave significantlydifferentcomputingpower.toovercomethesechallenges fadatest generates test programs from real benchmark programsbasedontheexecutioncharacteristicsofthebenchmarks.
the generated test programs can achieve efficient and adaptive performance regression testing of dbt systems.
guestprogramgenerationfordbtsystems.
perfdbt attemptstogenerateguestprogramstotesttheperformanceofadbt system.
however perfdbt has several fundamental limitationswhen using the generated guest programs for dbt performance regressiontesting.first itisdesignedspecificallyforx86 64platforms and thus cannot be applied to dbt systems running on a different hardware platform.
second multi threaded benchmarkprograms are not supported by perfdbt.
as a consequence the multi threadingperformanceofthetargetdbtsystemcannotbe tested.
third perfdbt assigns too many memory objects in the generated guest programs which may introduce a negative impact on the performance testing results.
in contrast fadatest does not havetheselimitations.itisdesignedspecificallyforefficientand adaptive performance regression testing of dbt systems.
codegenerationforhardwaresimulators.
to generate input programs for simulators simpoint samples instructions during the execution of a program .
the sampling policy is created based on the phase information of the original program.
though the selected instructions can be used to test architecture simulators theyarenotsuitablefordbtsystems.thisisbecausedbtsystems aretypicallydevelopedtorunreal worldapplications insteadof smallpiecesofinstructions.moreover simpointdoesnotsupport multi threaded programs.
that means the instructions extractedby simpoint cannot recover the thread behaviors of the original programs.comparedtotheinstructionsextractedbysimpoint the test programs generated by fadatest are more appropriate for testing the performance of dbt systems as they can preserve the performance characteristics of original programs.
conclusion although dbt is a key enabling technology developing and maintaining a real world dbt system is not easy.
an important method toconstantlymaintaintheperformanceefficiencyofadbtsystem is to frequently conduct performance regression testing.
however applying performance regression testing to dbt systems suffersfrom several practical challenges such as the natural differencesbetween dbt systems and regular software systems the limited availabilityoftestprograms andvarioushardwareplatformstargetedby adbtsystem.this paperpresents fadatest toaddress these challenges.
fadatest employs several novel techniques to generate adaptable test programs which can be used to achieve efficientandadaptiveperformanceregressiontestingofdbtsystems.
we also implement a prototype of fadatest and use it to generatetestprogramsfrombenchmarkprogramsinspeccpu2017 and parsec.
experimental results show that the test programs can uncover the same performance regression issues across differentversionsofqemuandvalgrindastheoriginalbenchmark programsandthetestingefficiencyisimprovedsignificantly.we anticipatethat fadatest willbeintegratedintothedevelopment cycles of existing dbt systems by automatically launching the performance regression testing before each code commit.