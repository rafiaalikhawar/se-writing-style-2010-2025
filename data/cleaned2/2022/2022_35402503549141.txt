lighting up supervised learning in user review based code localization dataset and benchmark xinwen hu xinwen.hu.nju gmail.com state key laboratory for novel software and technology nanjing university nanjing jiangsu chinayu guo guoyu smail.nju.edu.cn state key laboratory for novel software and technology nanjing university nanjing jiangsu chinajianjie lu mf21320105 smail.nju.edu.cn state key laboratory for novel software and technology nanjing university nanjing jiangsu china zheling zhu mf21320238 smail.nju.edu.cn state key laboratory for novel software and technology nanjing university nanjing jiangsu chinachuanyi li lcy nju.edu.cn state key laboratory for novel software and technology nanjing university nanjing jiangsu chinajidong ge gjd nju.edu.cn state key laboratory for novel software and technology nanjing university nanjing jiangsu china liguo huang lghuang lyle.smu.edu department of computer science southern methodist university dallas texas usabin luo luobin nju.edu.cn state key laboratory for novel software and technology nanjing university nanjing jiangsu china abstract as user reviews urs of mobile apps are proven to provide valuable feedback for maintaining and evolving applications how to make full use of urs more efficiently in the release cycle of mobile apps has become a widely concerned and researched topic in the software engineering se community.
in order to speed up the completion of coding work related to urs to shorten the release cycle as much as possible the task of user review based code localization is proposed and studied in depth.
however due to the lack of large scale ground truth dataset i.e.
truly related ur code pairs existing methods are all unsupervised learning based.
in order to light up supervised learning approaches which are driven by large labeled datasets for review2code and to compare their performances with unsupervised learning based methods we first introduce a large scale human labeled ur code ground truth dataset including the annotation process and statistical analysis.
then a benchmark consisting of two sota unsupervised learning based and four supervised learning based review2code methods is constructed based on this dataset.
we believe that this both authors contributed equally to this research.
corresponding author.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november singapore singapore association for computing machinery.
acm isbn .
.
.
.
can provide a basis for in depth exploration of the supervised learning based review2code solutions.
ccs concepts software and its engineering software maintenance tools computing methodologies artificial intelligence .
keywords user review code localization supervised learning android acm reference format xinwen hu yu guo jianjie lu zheling zhu chuanyi li jidong ge liguo huang and bin luo.
.
lighting up supervised learning in user reviewbased code localization dataset and benchmark.
in proceedings of the 30th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november singapore singapore.
acm new york ny usa pages.
https introduction since the lunch of appstores in mobile operating platforms such as android ios and winphone there have been accumulated over a million mobile apps.
not only can users download or rate e.g.
five star scale apps but also they can comment on apps based on their experience such as what features they like the most or what features they need.
these comments are called user reviews and are generally presented as free form text.
there are four categories of user reviews namely problem discovery pd i.e.
denotes user s report of app s problems such as system errors or crashes feature request fr i.e.
denotes user s requests of implementing new or enhancing existing functions information seeking is i.e.
esec fse november singapore singapore xinwen hu yu guo jianjie lu zheling zhu chuanyi li jidong ge liguo huang and bin luo a b testinguser review emotion scores topic ... code activitiestest activities cd cideployment phaserelease user requirements system requirements cd ci analysis of user feedback ...release planninga b figure release cycle for app maintenance and development.
denotes user s query of information related to the app and information giving ig i.e.
denotes user s notification of update about an aspect related to the app .
it has been proven that pd and fr provide valuable feedback for guiding developers to engage in software maintenance and release .
considering the fast growth rate of user reviews e.g.
popular apps receive more than user reviews per day on average and the high proportion of pd and fr e.g.
.
of mobile app s user reviews are fr of user reviews of twitter are pd and fr how to make full use of user reviews more efficiently in the release cycle of mobile app has become a widely concerned and researched topic in the software engineering se community.
figure shows the mobile application release cycle widely followed by the industry in the context of devops i.e.
continuous integration deployment delivery and analysis from which the importance of user review can be seen .
the collection and analysis of user review can be regarded as the beginning of the next version release directly providing valuable information for at least three important tasks namely release planning testing and development.
specifically for release planning the severity of the related problems and the necessity cost of new functions mentioned in the urs provide important clues for the app s maintenance team to decide the focus of the next development cycle for example what problems need to be solved which features need to be optimized or which new features to be added in the next release.
for testing the description of errors or crash scenarios in urs provides key information for efficiently designing effective test cases which greatly speeds up the efficiency of solving related problems.
ultimately both feature mentioned in release planning and problems verified in testing need to be solved or implemented through coding.
however it is not enough to just realize that urs are valuable to the release cycle.
what is more important is how to quickly complete the coding activities related to urs to shorten the release cycle as much as possible.
fortunately automatically locating the related code according to urs i.e.
user review based code or change localization for convenience we name it review2code in this paper has proved to be an effective means to speed up bug fixes and new function development.concretely review2code is to locate the related source code in file level or method level according to the user reviews of mobile apps collected from appstores.
however there are still few relevant studies on review2code compared with bug localization even the importance of user reviews and review2code are fully recognized.
the reasons are two fold.
first other than natural language descriptions user reviews do not contain any additional information such as program execution records or test cases described in code form which are contained in bug descriptions.
it is widely recognized that correlating natural language with code is harder than correlating code with code.
second bugs or issues are generally collected from open source software repositories such as github where they will be associated with the relevant changed code after they are settled.
this means that sufficient ground truth datasets for bug localization can be easily obtained from open source software repositories.
however the ground truth dataset i.e.
true related ur code method level file level pairs of review2code needs to be manually annotated by researchers which is very expensive to obtain.
nevertheless researchers enthusiasm for review2code has not been blocked and five specific methods for locating code for user review have been proposed.
as shown in the top half of table they are all designed for android apps.
their input is a ur described in natural language and the output is a code file related to the ur i.e.
for solving problems or implementing features mentioned in the ur these code files should be modified .
although the dataset they use includes data from multiple apps only the ground truth used to evaluate the effect of the method is manually annotated generally less than data instances as shown in the last column of table .
due to the small size of ground truth the core of these methods is clustering or heuristic similarity measurement methods that do not require a large scale ground truth.
in other words they are all unsupervised learning based.
however given that supervised learning methods especially deep learning are widely used in se tasks and achieve state of the art performances nowadays the next question naturally comes to mind is if ground truth is available can the supervised learning approaches achieve better results than the unsupervised learning ones on review2code?
besides the supervised learning has two advantages over the unsupervised learning as newly proposed urs are handled more ur code pairs could be generated and added to the initial training set constructed from the one time effort of annotating to train better review2code models i.e.
models based on supervised learning will continue to improve while the others cannot the performance of supervised learning based models on new applications can be greatly improved by transferring existing models and annotating a small amount of data for new applications while unsupervised learning based ones cannot.
motivated by the idea of lighting up supervised learning for review2code and comparing supervised learning based approaches with the sotas of existing unsupervised learning based ones we make the following contributions in this paper .
manually label the first large scale dataset supporting supervised learning based review2code.
each data instance is a ur code pair.
the dataset totally consists of pairs of ur method collected from reviews of popular android apps.
in total there are code files and methods in the dataset.
534lighting up supervised learning in user review based code localization dataset and benchmark esec fse november singapore singapore table the basic information of existing review2code methods and potential supervised learning based models.
name input granularity dataset evaluation data urr review file apps unknown changeadvisor review file apps ur clusters reviewsolver review file apps urs where2change review file apps ur clusters rising review file apps urs unif query method codesearchnet cat description method conala codebert query method codesearchnet graphcodebert query method codesearchnet changeadvisor and where2change treat clusters of urs as the smallest unit to locate code.
they only label relevant code for each cluster during evaluation.
.
construct a new benchmark consisting of both unsupervised learning based and supervised learning based approaches for review2code based on the proposed dataset.
the unsupervised learningbased methods are where2change and rising and the supervised learning based ones are borrowed from another se task ie code search searching code snippets for the given natural language description which is similar to review2code as shown in the second half in table .
all of these methods will be described in detail in section .
.
evaluate and discuss the benchmark results through conducting in depth quantitative and qualitative analysis to motivate future researches on review2code.
the rest of the paper is organized as follows.
section introduces related work including user review analysis bug localization and code search.
section briefly describes existing and candidate review2code approaches as well as the criteria for selecting benchmark systems.
section illustrates the constructing process of the dataset in detail and a brief analysis.
section and section introduce and discuss the benchmark respectively.
threats to validity are in section and section concludes the paper.
related work .
user review analysis with the development of machine learning and deep learning researchers have proposed a variety of effective processing methods for mining and categorizing user reviews of mobile applications in recent years.
some authors designed classifiers of user reviews based on their sentiment e.g.
either expectance or anger and specific topics e.g.
either problem discovery or feature request .
gu and kim presented sur miner a framework that makes full use of the monotonous structure and semantics of user reviews to understand users preferences.
di sorbo et al.
proposed surf summarizer of user reviews feedback which has the ability to determine the fine grained topics discussed in user reviews e.g.
ui improvements or licensing issues and generate an interactive structured and condensed agenda of recommended software changes.
scalabrino et al.
devised clap 1all data and code can be found here listener for release planning an approach to categorizing user reviews and clustering them which provides inspiration for feature requests and bug fixes in the next release.
compared with surf clap could provide the priority of user reviews and divide them into specific categories.
a number of studies applying the topic model technique to extract features of user reviews inspire us as well.
mining the deep semantic information of user reviews would definitely benefit the work of locating code snippets based on user reviews.
for example meaningless user reviews can be filtered with user review classification before linking to code.
in this paper i.e.
section .
we adopt an existing bert based ur classification model to filter useless urs before annotating.
.
bug localization bug localization refers to locating the potential buggy files for a given bug report which is advantageous to help software developers to fix the bug effectively.
the approaches of bug localization are mainly divided into two categories based on information retrieval ir based and based on deep learning dl based .
ir technique is widely used for automatically extracting important information from resources.
vsm vector space model is popularly used for vectoring user reviews and source code .
based on the topic model some studies measure the similarity between topic distributions from source code snippets and bug reports .
however the mismatch between bug reports and source code snippets limits the performance of ir .
to address this limitation many researchers take api documents into account .
some others extract crucial features of source code snippets or explore deep relationships between bug reports and them.
in recent work dl techniques have been applicable to learn the characteristics of bug reports and source code snippets.
cnn is utilized for extracting the structural information of source code .
li et al devised deeprl4fl which treats the bug fault localization as an image pattern recognition problem through novel code coverage representation learning and data dependencies representation learning for program statements.
besides knowledge graph embeddings and attention mechanisms abstract syntax trees and transfer learning and adversarial learning are also utilized in machine learning based bl approaches.
different from bug reports that are usually generated by software developers user reviews are largely raised by ordinary users of apps.
moreover a lot of additional information e.g.
either program execution records or test cases described in code form is included in bug reports while user reviews are free form texts.
despite these differences bl approaches inspire review2code to consider different representations of user reviews and source code and enhance information to bridge the gap between them .
.
code search code search is to find code snippets from a database or repository that are closely relevant to the query in natural languages and is vital for applying fixes and extending software functionalities.
existing code search works in natural language processing can be 535esec fse november singapore singapore xinwen hu yu guo jianjie lu zheling zhu chuanyi li jidong ge liguo huang and bin luo classified into two mainstreams ir based information retrieval methods and dl based deep learning ones.
the two kinds of code search methods differ in their respective styles of matching queries and code snippets.
an ir based method usually extracts a set of keywords from a query and then searches for the keywords in code repositories .
comparatively a dl based method takes some deep learning techniques especially embedding algorithms to map raw data including queries and code snippets into a high dimensional space and match them .
lu et al.
studied the relationship between words and source code snippets.
raghothaman et al.
utilized related api to enrich the information of source code.
vinayakarao et al.
established the relation of programming concepts with syntactic forms.
sachdev et al.
proposed ncs to learn the embeddings of code based on unsupervised technology.
cambronero et al.
developed unif which is a supervised extension of ncs.
gu et al.
introduced codenn with three encoded individual channels to learn code representations.
haldar et al.
adopted a multiperspective crosslingual neural framework for code text matching and compared four models ct cat mp mp cat to verify the advantages of the framework in capturing similarities between two sequences.
with the popularity of pre training models such as bert and gpt many researchers began to pay attention to their application in code search and achieved gratifying results.
similar to review2code the task of code search allows users to raise natural language queries and returns a ranked list of related code snippets.
however user reviews referring to subjective feelings of users about using the application are non technical and potentially imprecise but the query of code search most closely matches a developer s intent and contains more code level semantic information.
there exist many large scale and high quality code search datasets because code annotations the input of which are uncomplicated to be extracted whereas we haven t found any datasets available for our work.
considering that the purpose and input of code search are similar we can build our own dataset and then utilize the code search model to evaluate the performance of the supervised model on review2code.
review2code systems before introducing the dataset construction it is necessary to present the candidate systems which support review2code.
as described in the introduction section table enumerates all existing unsupervised learning based and several potential supervised learning based approaches of review2code.
here we first make an in depth introduction for them.
for unsupervised learning based methods the core is to measure the similarity between a natural language description and a code snippet.
it requires that the dataset include at least two parts user reviews and candidate code.
the general process of these methods is as follows.
preprocess user reviews and code and build vector representation for them.
then calculate similarities between user reviews and candidate code snippets according to the predefined measurement and take the top kcode snippets with the highest similarity as the predicted results.
however different methods may apply additional information such as commits or issue reports to further bridge the gap between user reviews related code snippetspublic void switchtheme int type ... ... existing unsupervised preprocessing ur code pairs collectinghow do i switch to the dark them?user reviewreview2code codesearch supervised information enhancement representation searching by similarity measuringpreprocessing model training predicting and rankingfigure two types of review2code systems.
and source code which are proven to contribute to improving the predicting performance.
due to the lack of large scale labeled data i.e.
truly associated ur code pairs there are no supervised learning based models dedicated to judging whether there is an association between a givenurand acode .
however if there are enough labeled relevant or irrelevant ur code pairs a code retrieval method based on deep learning such as graphcodebert codebert unif and cat can be used to train the model that calculates the similarity between user reviews and code snippets.
when using the model user reviews and candidate code snippet pairs are input to the model one by one the correlation probability is calculated and sorted and the top kcode snippets are the prediction results.
figure demonstrates the comparisons between the above two schemes.
the inputs are user reviews and code snippets and the outputs are code lists associated with the reviews.
next we will first introduce various specific methods of the two schemes in detail w.r.t.
figure then determine the systems to be benchmarked and illustrate the reasons as well as present the general process of benchmark construction.
.
unsupervised systems existing review2code methods are urr changeadvisor reviewsolver where2change rising .
we first introduce them from perspectives of preprocessing vector representation information enhancement similarity calculation etc.
urr classifies user reviews into predefined categories and defines a set of structure categories for source code files.
then the model utilizes lucene core to perform a search and returns the top scoring source files.
a boosting score will be added during the search if the user review and source file have a matching taxonomy category.
changeadvisor clusters user reviews by hdp hierarchical dirichlet processes the goal of which is to group together user reviews expressing similar requirements.
then it measures the similarity between source code snippets and the clusters with cosine similarity.
the output is a list in descending order according to the similarity score.
536lighting up supervised learning in user review based code localization dataset and benchmark esec fse november singapore singapore reviewsolver utilizes a parse tree to capture the semantic information of user reviews and creates apg android property graph of the app.
then it measures the similarity between phrases from the tree and components from the graph.
finally it returns the top nfunction errors app specific errors general errors of the source code most relevant to the given user review.
where2change is similar to changeadvisor in clustering and similarity calculation.
however before similarity calculation it additionally measures the similarity between user review clusters and issue reports.
when the similarity score is higher than the threshold the issue report will be linked to the cluster for enriching user reviews.
rising is mainly composed of two steps clustering and similarity calculation which is similar to changeadvisor and where2change.
in addition it exploits commit messages as a medium to fill the lexicon gap between user reviews and source code snippets and uses a heuristic based method to infer parameter kin k means .
.
supervised candidates as illustrated in section .
the tasks of review2code and code search are very similar where input is described in natural language and the output is a code list associated with the input.
the difference lies in the definition of the relationship between inputs and outputs.
code search describes the functions of existing code snippets in natural language while review2code describes the functions to be developed or the problems existing in the current functions in natural language.
however in terms of models review2code can reuse the models of code search and train on real ur code data.
the main existing code search models are unif embedding unification is a supervised extension of the ncs neural code search technique which uses word embedding matrices to learn the mapping from tokens into vectors.
the model computes a simple average to combine the query token embeddings into a single vector which is present in ncs as well.
to combine each bag of code token vectors it utilized an attention mechanism to measure a weighted average.
finally it returns a list ranked by similarity score between the two single vectors.
cat is an extension of ct a baseline code and text model based on ast abstract syntax tree which can capture the structural characteristics of the code.
the model performs bilstm and max pool to obtain the output vectors of each input natural language code and ast and then concatenated the output vectors of code and ast to a single vector representing source code.
finally it returns a ranked list according to the similarity between vectors representing the source code and query.
codebert is the first bimodal pre trained model for multiple programming languages pl and natural languages nl .
it uses bimodal data including nl pl pairs and a larger scale of unimodal data to pre train the model so that it can well learn the general purpose representations of nl and pl.
the code snippet and the natural language are input to the model together.
then a softmax layer is connected to the representation of to calculate relevance scores between them to rank candidate code snippets.
graphcodebert is the first pre trained model that leverages the semantic structure of code to learn code representation.
by using data flow in the pre training stage it considers the internal structure of code and provides key code semantic information for code understanding.
the code snippet and the natural language are respectively input to the model to obtain their vector representations.
then the inner product of the two vectors is calculated as relevance scores between them to rank candidate code snippets.
.
selected systems we used the following criteria to choose review2code methods for our benchmark support java language .
the ultimate goal of our work is to provide valuable modification suggestions through review2code for the development and maintenance of mobile applications in android platform which are programmed in java language.
so the methods selected should support java language.
has publicly available source code or can be fully reimplemented .
if a method has neither publicly available source code nor complete and accurate illustration for re implementation there may be deviations between the reproduced version and the actual version which will affect the performance of the method and is disadvantageous to our evaluation.
therefore we exclude methods that lack key information or required data for implementation.
be representative i.e.
achieving sota performance from anyone s perspective or new enough .
in order to make the comparisons more convincing we select sota of existing unsupervised learning based review2code methods and newly published within three years supervised learning based candidates.
taking the above factors into consideration we finally selected six methods for our benchmark namely where2change rising unif cat codebert and graphcodebert.
upon deciding the benchmarking systems we reproduce them referring to available source code if any and prepare datasets according to the requirements of different methods.
while the methods and dataset are prepared we conduct experiments and finally discuss the results.
dataset construction the existing unsupervised methods do not need manually labeled data except to evaluate the performance .
in order to build a method based on supervised learning there must be sufficient relevant ur code concretely ur method pairs as the ground truth for training.
this section details the process of constructing the dataset for lighting up supervised learning for review2code.
as shown in figure the procedure consists of raw data collecting annotating preparation annotating and dataset statistical analysis.
.
data collecting the collection of raw data includes selecting the candidate apps and obtaining their user reviews issues pulls and source code.
the details are as follows 537esec fse november singapore singapore xinwen hu yu guo jianjie lu zheling zhu chuanyi li jidong ge liguo huang and bin luo ground truth collectingdata collecting preparation annotating statistical analysis app selecting user reviews issues pulls source codedata preprocessingannotator selecting ur issue pull code linkingapp knowledge learningannotating system annotating training first round annotating cross checkingtrue related ur code pairs coarse grained statistics fine grained statistics figure procedure of dataset construction.
table initial statistics of the selected five apps.
name type stars reviews files methods issues pulls k mail comm.
.2k antennapod vide.
.8k cgeo trav.
.1k anki educ.
.2k termux life .4k app selecting.
we select apps from the google appstore and available open source non fork github repositories2.
we first identify all software projects of different categories e.g.
business communication video lifestyle travel education etc.
then sort them bypopularity denoted by the stars and select the top most popular ones.
at the same time we filter out those with less than issues and reviews and those having no license or unclear license.
finally apps from different categories are selected namely k mail antennapod cgeo anki and termux .
raw data collecting.
first we collect user reviews of the selected android apps from google s appstore i.e.
google play .
then we get their source code from their corresponding opensource repositories on github.
we find that issues and pulls on github not only contain natural language descriptions but also are associated with relevant source code in method level i.e.
are linking to changes in the code .
on the one hand issues and pulls are raised by users on github i.e.
usually software programmers when finding bugs or proposing improvement feature suggestions which means they share the same valuable information with problem discovery and feature request in user reviews.
on the other hand after the issues or pulls are solved code changes that result from them are also linked to them.
considering these it will be able to speed up annotating related code for user review if issues and pulls related to user reviews can be recommended at first since the similarity between nl and nl is easier to be measured than that between nl and code.
therefore we also crawl the issues and pulls of these apps from github for assisting dataset annotating.
statistics of initially collected data are shown in table .
.
preparation the preparation includes selecting qualified annotators organizing annotators to learn knowledge of the selected apps preprocessing the raw data and constructing the data format as conveniently as possible for the annotators.
2found here selecting.
we selected annotators with more software requirements engineering experience from applicants has obtained a bachelor s degree in software engineering who met the following criteria have participated in the development i.e.
coding of at least two software ensure the ability of reading and understanding code have participated in requirements engineering of at least two software projects ensure understanding ur have raised issue or pull request for any open source project on github ensure understanding issue and pull request and have data annotating experience.
knowledge learning.
first annotators are assigned as much as possible to annotate apps of the type they are familiar with as well as considering distributing annotating tasks measured by the number of urs equally among annotators.
then they are organized to learn relevant knowledge of the app assigned to them including project introduction and background user manual if any feature list if any architecture concrete code files etc.
preprocessing.
foruser reviews the preprocessing is to filter out meaningless ones using the bert model trained in for classifying user reviews into three types namely problem discovery pd feature request fr and meaningless including information seeking and giving .
pd and fr in the collected user reviews are reserved for further annotating3.
for issues and pull requests those having no natural language descriptions or associated code files are filtered out.
besides tags e.g.
version tag error tag in the nl descriptions are also removed.
for code we use the ast parser4 to parse each code file we crawled down to extract names of each method.
besides start line position end line position and entire code snippet of each method related to each issue pull are also extracted.
relation between code files and methods contained by them are also recorded for further usage.
eventually the numbers of reserved artifacts of k mail antennapod cgeo anki termux are reviews issues and pull requests .
data linking.
to annotate truly related ur code pairs annotators should go through all code of the app for each ur theoretically.
since the annotators have already learned necessary knowledge of the app they can locate candidate code files faster than really going through all files.
in order to further speed up the annotating procedure we utilize issues pulls for highlighting candidate methods in code files for annotators.
concretely we first use bm255to automatically link each ur with their potentially related issues pulls then recommend codes that are naturally related to these issues pulls as candidates for annotators.
recall that in preprocessing we have extracted the modification range of related code files for each issue pull as well as the method name and line range of each method so we could easily link each issue pull to 3even if the meaningless ur is predicted to be pd or fr it will not affect the ultimate annotated dataset quality.
because that we will manually judge whether each reserved ur is meaningless or not during annotating.
if ur of pd fr is predicted as meaningless it will reduce the number of urs to be annotated which may lead to missing some valuable ur code pairs but will not affect the quality of annotation either.
this will be further stated in threats to validity.
5in order to apply bm25 we conduct preprocessing steps for both ur and issue pull following including filtering neither english nor numeric characters contractions expansion e.g it stoit is lemmatization e.g didtodo uppercase conversion and remove stop words.
538lighting up supervised learning in user review based code localization dataset and benchmark esec fse november singapore singapore figure screenshot of the review2code annotating system.
related methods.
to this end urs are linked to potentially related issues pulls and issues pulls are linked to related methods.
.
annotating for facilitating the annotating procedure we develop a simple but sufficient annotating tool.
the annotating is conducted after adequate annotation training for annotators.
after the first round annotating each pair of ur code is verified by another annotator during the second round data checking.
annotating system.
figure is the screenshot of the annotating tool.
there are three parts.
the left part displays the ur to be annotated and the candidate issues pulls that are linked to ur.
more issues pulls can be divided via turning pages.
the middle part is the directory tree of the app s source code where the leaf nodes are method names.
methods linked to the selected issue pull in the left part are highlighted in red.
for any method highlighted or not if the annotator believes it is related to the current ur he she should check the checkbox in front of that method.
the right part presents the code file containing the clicked method.
if the annotator believes that the current ur is meaningless he she should click the meaningless button.
upon all related methods are checked theconfirm button should be clicked.
the previous andnext buttons are for switching to the previous and next ur respectively.
while the current ur is changed all corresponding contexts will switch as well.
annotating training.
four hours were spent on training annotators to familiarize them with the system and the concept of ur method pair.
for each app the first ten user reviews are used as annotating examples for illustrating under what conditions the methods should be checked or not.
then the annotators are tested through annotating five user reviews and then corresponding methods which are randomly selected for them.
until they reach a consensus with i.e.
understand and agree with the ground truth of the test data they cannot start to annotate.
annotating.
during the first round of annotating each user review is annotated by two annotators.
both annotated results truly related ur method pairs of each ur annotated by two different annotators will be saved for the next checking round.
at this stage each annotator is assigned about user reviews on average and annotators are joining.
it cost us days on average to finish the first round annotation please note that the time costis also influenced by the scale of code.
different apps cost different time costs.
we report the average time cost of the five apps .
that is about man months for annotating about user reviews .
checking.
while checking previously annotated results of each ur except those labeled as meaningless by any annotator during the first round will be checked by another annotator who does not participate in the first round annotating.
for convenience we also develop a simple tool almost the same as the one shown in figure to facilitate the checking procedure.
the system will automatically select the checkbox of the previously annotated method.
moreover the method annotated by one or two annotators in the previous round will be highlighted in different colors.
if the checker finds that the results of the ur annotated by the previous two annotators are completely different he needs to discuss with them to reach an agreement for that ur.
the checking time cost is much less than the first round annotating and it is about man months annotators finished within days on average .
.
statistical analysis after the above annotation process we finally construct the review2code dataset supporting the supervised learning approaches.
the raw data of review2code dataset consists of android mobile applications with java methods and java files.
table summarizes the coarse grained labeling results.
remarkably ur file is automatically built according to the marked ur method .
overall excluding automatically filtered during preprocessing i.e.
automatically labeled as meaningless user reviews have relevant code files and methods while user reviews have no relevant code files or methods i.e.
manually labeled as meaningless .
in total the review2code dataset contains pairs of ur method and pairs of ur file .
compared with other deep learning datasets the proposed dataset is affordable to train deep learning models for the review2code task.
to gain a deeper understanding of the review2code dataset a simple but informative statistical analysis is conducted.
figure shows the distribution of the number of reviews associated with different numbers of methods and files in each project.
it is easy to get user reviews that have relevant code snippets are the most in all five apps regardless of method level or file level and separately average account for .
and .
in all projects.
through statistical calculation .
methods and .
files are averages associated with one user review for each app.
it proves that user reviews are informative and often involve many feature requests and problem discoveries and proves that review2code is valuable but difficult.
experimental setups next experimental setups of the benchmark are introduced including settings evaluating metrics and research questions rq as well as methodologies for answering rqs .
.
settings for all approaches we only conduct method level predicting i.e.
the output of each of the six selected methods is the list of methods.
to derive the predicted list of files we substitute the method 539esec fse november singapore singapore xinwen hu yu guo jianjie lu zheling zhu chuanyi li jidong ge liguo huang and bin luo table statistics of the annotated review2code dataset.
namereviewsmethods filespairs relevant irrelevant ur.
m. ur.
f. k mail antennapod cgeo anki termux currently the k mail project is now rewriting with kotlin which leads to the reduction of the number of java files in the latest version of k mail on github i.e.
less than .
k mail antennapod cgeo anki termuxnumber of reviewsnumber ranges of related methods a distributions of ur w.r.t.
of related methods k mail antennapod cgeo anki termuxnumber of reviewsnumber ranges of related files b distributions of ur w.r.t.
of related files figure distributions of urs w.r.t.
related methods files.
with the file containing it and repeated files are skipped6.
we split the dataset into train valid test proportions in the experiments.
to ensure the comparison of experimental results statistically significant each experiment is conducted for times and the average value is reported.
hardware settings .
for unsupervised learning based methods we ran on a computer with core cpu with intel r xeon r gold cpu .50ghz and 250g sever memory.
for supervised learning based models we conducted experiments on two computers of the same settings and each has four nvidia tesla v100 32gb memory gpus.
software settings .
the environment settings are as follows python version is .
.
and java version is .
.
pytorch version is .
.
and hugging face transformer version is .
.
.
6while calculating file level evaluating metrics the top kfiles should be firstly inferred from the recommended methods.hyper parameters .
recall that we re implement where2change according to its paper and set the number of user reviews clusters to .
for rising we adopt the setting of hyper parameters the same as the initial implementation.
for supervised models we mainly adjust the settings of epoch learning rate and batch size.
according to the order of unif cat codebert and graphcodebert epoch is learning rate is 1e 1e 1e 2e and batch size is respectively.
for the others we follow all the hyper parameters on the official code.
all of our implementations of different methods as well as details of hyper parameters and running environment will be made publicly available.
.
evaluation metrics to evaluate the performance of the models we employe the top k and mean reciprocal rank mrr as metrics both of which are used in where2change and rising .
their definitions are top kis the proportion of the reviews whose retrieved top k code snippets contains at least one in the ground truth top k ur ur uriscorrect ur top k ur where ur is the total number of ur and the iscorrect ur top k function returns if at least one of top kcode snippets actually is relevant to the review ur and returns otherwise.
mrr is the multiplicative inverse of the rank of first correctly retrieved code snippet mrr ur ur i ranki whererankiis the rank of the first correctly retrieved code snippet.
.
research questions we organize the benchmark through four research questions.
rq1 in the experimental setting under which conditions supervised learning based models beat the unsupervised learning based methods?
method we test the performance of different models on each app according to the train on single tos scheme which means that the model is trained on each app training set respectively and tested on its test set.
for the supervised models we train each model if it is a pre training model then fine tuning with the training set adjust the parameters with the validation set and evaluate with the test set.
for unsupervised methods verify the performance directly on the test set.
rq2 does the performance of supervised learning based models on different mobile apps influenced by the corresponding data scale?
how?
method data scale includes the number of ur method pairs methods or files and their ratio.
we rank the result of models on different apps according to this index to verify whether the performance is related to the corresponding order of the data scale.
rq3 can supervised learning based review2code models transfer knowledge across different apps?
method besides the tos we adopt another two training schemes for answering this question and they are train on all toa means that the model is trained on all training data of all the five 540lighting up supervised learning in user review based code localization dataset and benchmark esec fse november singapore singapore table the file level and method level results of apps by models on tostraining scheme.
app name modelmethod level file level top top top top top mrr top top top top top mrr k mailwhere2change .
.
.
.
.
.
.
.
.
.
.
.
rising .
.
.
.
.
.
.
.
.
.
.
.
unif .
.
.
.
.
.
.
.
.
.
.
.
cat .
.
.
.
.
.
.
.
.
.
.
.
codebert .
.
.
.
.
.
.
.
.
.
.
.
graphcodebert .
.
.
.
.
.
.
.
.
.
.
.
antennapodwhere2change .
.
.
.
.
.
.
.
.
.
.
.
rising .
.
.
.
.
.
.
.
.
.
.
.
unif .
.
.
.
.
.
.
.
.
.
.
.
cat .
.
.
.
.
.
.
.
.
.
.
.
codebert .
.
.
.
.
.
.
.
.
.
.
.
graphcodebert .
.
.
.
.
.
.
.
.
.
.
.
cgeowhere2change .
.
.
.
.
.
.
.
.
.
.
.
rising .
.
.
.
.
.
.
.
.
.
.
.
unif .
.
.
.
.
.
.
.
.
.
.
.
cat .
.
.
.
.
.
.
.
.
.
.
.
codebert .
.
.
.
.
.
.
.
.
.
.
.
graphcodebert .
.
.
.
.
.
.
.
.
.
.
.
ankiwhere2change .
.
.
.
.
.
.
.
.
.
.
.
rising .
.
.
.
.
.
.
.
.
.
.
.
unif .
.
.
.
.
.
.
.
.
.
.
.
cat .
.
.
.
.
.
.
.
.
.
.
.
codebert .
.
.
.
.
.
.
.
.
.
.
.
graphcodebert .
.
.
.
.
.
.
.
.
.
.
.
termuxwhere2change .
.
.
.
.
.
.
.
.
.
.
.
rising .
.
.
.
.
.
.
.
.
.
.
.
unif .
.
.
.
.
.
.
.
.
.
.
.
cat .
.
.
.
.
.
.
.
.
.
.
.
codebert .
.
.
.
.
.
.
.
.
.
.
.
graphcodebert .
.
.
.
.
.
.
.
.
.
.
.
apps and tested separately on each app s testing set.
train on part top means that the model is trained on four apps and tested on the other one.
rq4 would utilizing comments in source code improve the performance of supervised learning based review2code models?
method the supervised learning based models with higher performance are used to check the usefulness of code comments in review2code.
discussion by answering rqs answering rq1 unsupervised learning vs. supervised learning.
the performance of different methods on each app following the tos scheme is shown in table .
it shows that the supervised model graphcodebert performs best on all apps followed by codebert.
however by conducting a case study we surprisingly find that there are cases in which unsupervised methods beat the supervised ones.
taking figure as an example if the retrieved method belongs to gt it is highlighted in yellow the result of rising is comparative to those of graphcodebert and codebert and obviously outperforms unif and cat.
the potential reason forgraphcodebert and codebert outperforming the others is that they have learned the knowledge of natural language and code via pre training.
in addition graphcodebert introduces the control flow graph which provides key code semantic information and enhances structural information for code understanding.
this is the reason that graphcodebert beats codebert.
the reasons for the cases that rising performs betterthan cat and unif lie in that rising bridges the lexicon gap by combining commit messages and code snippets.
to this end it is meaningful to light up supervised learning for user review based code localization considering that graphcodebert outperforms rising to be the new state of the art review2code approach.
answering rq2 impacts of apps on supervised learning.
through experimental analysis we find the number of ur method pairs methods or files has no obvious relationship with the performance of models.
however as shown in figure the mrr value of each model increases along with the rise of the ratio of the number of ur method pairs and the number of methods or files.
the reasons are two fold.
on the one hand the larger the number of ur method pairs the more sufficient the model is trained.
on the other hand the smaller the number of methods or files the greater the probability that the model will find the correct method or file.
however there is an outlier codebert on anki has a better filelevel mrr than that it achieves on k mail .
we carefully study the output results of codebert on anki and discover that codebert incorrectly predicts many methods which are just right in the files containing the correct methods.
recall that the top kfiles are inferred by predicted methods as well as the ground truth files are generated from ground truth methods it is more likely to lead the misjudgment at the file level for apps containing more methods on average in one single code file.
unsurprisingly anki really 541esec fse november singapore singapore xinwen hu yu guo jianjie lu zheling zhu chuanyi li jidong ge liguo huang and bin luo for some reason it won t install packages.
i tried everything but nothing .
input user review ground truth buildenvironment onserviceconnected setupifneeded onstartcommand createtermsessionreview2code where2change getterminaltranscri... terminaliopreferen ... extrakeysview termuxservice termuxactivity rising determinezipurl buildenvironment onstartcommand oncontextitemselected onserviceconnectedunif setupstoragesymlinks determinezipurl actionserviceexecute uncommandservice onserviceconnected cat reloadfromproperties sendkey reload oncontextitemselected buildenvironmentcodebert replacealiasesdeterminezipurlbuildenvironment createtermsession onserviceconnected graphcodebert determinezipurlbuildenvironment onserviceconnectedsetupifneeded onstartcommand figure a case study top5 methods of different approaches.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
cgeo antennapod anki k mail termuxmrr v alue apps sorted in ascending order of ur method fileratio b mrr file level codebert graphcodebert .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
cgeo antennapod anki k mail termuxmrr v alue apps sorted in ascending order of ur method method ratio a mrr method level codebert graphcodebert figure statistics of mrr related to the scale of apps.
has the largest average number of methods per code file .
methods file among all apps.
answering rq3 transferability of supervised learning based review2code models.
firstly we conducted experiments following the toatraining scheme.
what we expect is that the model can learn semantic information about reviews and code better through more training data.
however according to table we are surprised to find that this way does not improve the performance of models compared with the results of tos.
then we conducted the topexperiment by excluding the training data which comes from the same app as the test dataset.
as table demonstrates the experimental results of topare significantly lower than tos.
.
.
.
.
.
.
cgeo antennapod anki k mail termuxmrr v alue apps sorted in ascending order of ur method method ratiomrr method level codebert codebert comment graphcodebert graphcodebert commentfigure method level mrr of codebert graphcodebert w o and w comments in code.
according to these we suppose that the performance of supervised review2code models mainly relies on the training data within specific relevant domain knowledge.
expanding dataset from applications of different types adds noise to the original training dataset.
to verify this we conducted a statistic of the proportion of the repeated words between different apps code tokens which is no more than .
fully proving the significant difference between apps.
however these results cannot deny the transferability of supervised learning based review2code models.
we are looking forward to expanding the review2code dataset to further study these models transferability.
answering rq4 impacts of natural language comments in code.
figure compares the results before and after adding comments to graphcodebert and codebert models.
it proves that comments have a certain impact on the performance of both models.
however whether it has a positive or negative influence is rather mixed.
generally the performance of codebert decreases after adding comments while that of graphcodebert is not significantly affected.
two special points deserve mention the performance of each model on anki always gets worse and the performance of each model on termux always gets better.
to try to interpret this we further calculate the average overlap of words between user review and comments in code of each ur method pair for these two apps but there is no big difference between anki .
words and termux .
words .
we leave this as a challenge for future work.
threats to validity threats to the validity of our work come from three aspects annotating deviation .
since the annotators are not the actual developers of the apps the overall processing still has a bias which has been reduced as much as possible through adequate training manual interventions and cross checking.
besides missing links between ur and code will lead to fewer positive samples and even false negative samples which will lead to a decrease in model performance.
scale of the dataset .
more apps and more annotated ur method pairs would benefit both the discussion on the benchmark and the researchers who are interested in supervised learning based review2code.
we will continue to annotate more data to enrich review2code dataset.
542lighting up supervised learning in user review based code localization dataset and benchmark esec fse november singapore singapore table the file level and method level model average results of apps on different training scheme.
training scheme modelmethod level file level top top top top top mrr top top top top top mrr toswhere2change .
.
.
.
.
.
.
.
.
.
.
.
rising .
.
.
.
.
.
.
.
.
.
.
.
unif .
.
.
.
.
.
.
.
.
.
.
.
cat .
.
.
.
.
.
.
.
.
.
.
.
codebert .
.
.
.
.
.
.
.
.
.
.
.
graphcodebert .
.
.
.
.
.
.
.
.
.
.
.
toaunif .
.
.
.
.
.
.
.
.
.
.
.
cat .
.
.
.
.
.
.
.
.
.
.
.
codebert .
.
.
.
.
.
.
.
.
.
.
.
graphcodebert .
.
.
.
.
.
.
.
.
.
.
.
topunif .
.
.
.
.
.
.
.
.
.
.
.
cat .
.
.
.
.
.
.
.
.
.
.
.
codebert .
.
.
.
.
.
.
.
.
.
.
.
graphcodebert .
.
.
.
.
.
.
.
.
.
.
.
selecting of the benchmarking systems .
currently only popular code search approaches are selected as representatives of supervised learning based review2code approaches.
we will use more supervised learning methods adopted by se community for review2code in the future.
conclusion and future work in this paper we try to light up supervised learning based methods for review2code by constructing a novel dataset and benchmark.
first the procedure for constructing the dataset is described in detail which includes app selecting raw data collecting annotator selecting app knowledge learning data preprocessing urissue pull linking and issue pull code linking annotating system implementing annotator training data annotating and checking.
the ultimate dataset consists of android apps and pairs of truly related ur method .
then the benchmark having two unsupervised learning based methods where2change and rising and four supervised learning based models unif cat codebert and graphcodebert is constructed.
finally four research questions on the overall performances of different methods impacts of project scales model transferability and usefulness of comments in code are proposed and answered for discussing the benchmark results.
which deserves mention is that graphcodebert outperforms the state of the art unsupervised learning based review2code methods.
this also proves to us the value of lighting up supervised learning based review2code.
as to future work we will continue to enrich the dataset and dedicated on proposing better review2code approach.