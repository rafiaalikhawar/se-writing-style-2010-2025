autotransform automated code transformation to support mo dern code review process patanamon thongtanunam patanamon.t unimelb.edu.au the university of melbourne australiachanathip pornprasit2 chanathip.pornprasit monash.edu monash university australiachakkrit tantithamthavorn chakkrit monash.edu monash university australia abstract codereviewiseffective buthuman intensive e.g.
developersneed tomanuallymodifysourcecodeuntilitisapproved .recently prior work proposed a neuralmachine translation nmt approach to automaticallytransformsourcecodetotheversionthatisreviewed and approved i.e.
the after version .
yet its performance is still suboptimal when the after version has new identifiers or literals e.g.
renamedvariables orhasmanycodetokens.toaddress these limitations we propose a utotransform which leverages abyte pairencoding bpe approachtohandlenewtokensanda transformer basednmtarchitecturetohandlelongsequences.we evaluate our approach based on changed methods with and withoutnewtokensforbothsmallandmediumsizes.theresults showthat whengeneratingone candidateforthe afterversion i.e.
beam width our autotransform can correctly transform changed methods which is higher than the prior work highlighting the substantial improvement of our approach for code transformationinthecontextofcodereview.thisworkcontributes towardsautomatedcodetransformationforcodereviews which could help developers reduce their effort in modifying source code during the code review process.
acm reference format patanamonthongtanunam chanathippornprasit andchakkrittantithamthavorn.
.autotransform automatedcodetransformationtosupport modern code review process.
in 44th international conference on software engineering icse may pittsburgh pa usa.
acm new york ny usa pages.
introduction code review is one of the important quality assurance practices ina softwaredevelopmentprocess.
oneofthemain goalsofcode review is to ensure that the quality of newly developed code meets astandardbeforeintegratingintothemainsoftwarerepository .hence inthecodereviewprocess newsourcecodewrittenby a code author has to be examined and revised until reviewers i.e.
developersotherthanacodeauthor agreethatthissourcecodeisof acorresponding author.
2the first and second authors contributed equally to this research.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acm mustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa association for computing machinery.
acm isbn ... .
studies also showed that during the code review process source codeisrevisednotonlytoremovefunctionaldefects butalsotoimprovedesignquality maintainability and readability .
despite the benefits of code review the code review process is still human intensive where developers have to manually review and revise code.
to support reviewing activities prior studies proposed approaches to save the reviewers effort e.g.
review prioritization .however therevisingactivitiesstillrequire manualeffortfromcodeauthors.indeed givenalargenumberof reviews e.g.
3kreviewspermonthatmicrosoftbing prior studies found that it is challenging for code authors to revise their code without introducing new defects while switching contexts and keeping track of other reviews .
thus automated code transformationwouldbebeneficialtoaugmentcodeauthorsandto savetheireffort byautomaticallyapplyingthecommonrevisions duringcodereviewsinthepasttothenewly writtencode while allowing code authors to focus on revising more complex code.
tothebestofourknowledge theworkoftufano etal.
is the mostrecentwork thatproposed anapproach toautomatically transformcodetotheversionthatisreviewedandapproved.inthe priorwork theyleveragedcodeabstractionandarecurrentneural network rnn architecture.
they have shown that their nmt approachcancorrectlytransformcodebyapplyingawiderange ofmeaningfulcodechangesincludingrefactoringandbug fixing.
whiletheresultsofthepriorwork highlightedthepotentialof using nmt to help code authors automatically modify code during thecodereviewprocess theapplicabilityoftheirapproachisstill limited.
specifically their code abstraction hinders the tufano et al.approachintransformingsourcecodewhennewidentifiersor literalsappearintheversionthathasbeenapproved.inaddition duetothenatureofthernnarchitecture thetufano etal.approach may be suboptimal when the size of source code is longer i.e.
source code has many tokens .
in this paper we propose a new framework called autotransform whichleveragesbyte pairencoding bpe subwordtokenization andtransformer toaddressthelimitationsoftheprior approach .weevaluatedour autotransform basedontwo typesofchangedmethods thechangedmethods without newlyintroducedidentifiers literals i.e.
changedmethodsw onewtokens and the changed methods withnewly introduced identifiers literals i.e.
thechangedmethodsw newtokens .through a case study of changed methods extracted from three gerrit code review repositories of android google and ovirt we addressed the following research questions ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may 21 29 pittsburgh pa usa patanamon thongtanunam chanathip pornprasit and chakkrit tantithamthavorn rq1 can a utotransform transform code better than tufanoet al.approach?
results.
whengeneratingonecandidateoftheafterversionfor 750changedmethodsinthetestingdatasets our autotransformcan correctly transform source code for changed methodswhichis567 higherthanthetufano etal.approach.specifically forthechangedmethodsw newtokens our autotransformcancorrectlytransform1 060methods whilethetufano et al.approachcouldnotcorrectlytransformanyofthechangedmethods w new tokens.
for the changed methods w o new tokens our autotransform can correctly transform changed methods while thetufano et al.approach cancorrectly transform only changed methods.
rq2 what are the contributions of autotransform s components?
results.
usingsubwordtokenizationbybpeenablesour autotransform to achieve perfect predictions higher than using codeabstractionlikethetufano etal.approach.furthermore using the transformer architecture can increase perfect predictions at leastby17 comparedtousingthernnarchitecture.inparticular wefoundthatthepercentageimprovementinperfectpredictions is much higher for the medium methods i.e.
.
significance contributions.
the results of our work highlightthesubstantialimprovementofourapproachforcodetransformation in the context of code review.
more specifically our autotransform cantransformawiderrangeofchangedmethods i.e.
methodswithnewtokensandmethodswithlongsequences than the state of the art approach .
the proposed approach results and insights presented in this paper contribute towards automated code transformation for code reviews which could help developersreduce theireffort inmodifyingsourcecodeand expedite the code review process.
novelty.
this paper is the first to present autotransform i.e.
a transformer based nmt approach to transform source code from the version before the implementation of code changes to the version that is reviewed and eventually merged.
the use of byte pair encoding bpe subword tokenization toaddressthelimitationoftransformingchangedmethods that have newly introduced identifiers literals.
an empirical evaluation of our autotransform and the tufanoet al.approach based on a large scale dataset with two types of changed methods.
anablationstudytoquantifythecontributionsofthetwo components i.e.
bpe and transformer in our proposed approach.
openscience.
tofacilitatefuturework thedatasetsof147 extractedchangedmethods scriptsofour autotransform and experimental results e.g.
raw predictions are available online .
paperorganization.
section2presentsanddiscussesthelimitations of the state of the art approach .
section presents our autotransform.section4describesourcasestudydesign.section presents the results while section discusses the results.
section discusses related work.
section discusses possible threats to the validity.
section draws the conclusions.
the state of the art code transformation for code reviews recently tufano etal.
proposedaneuralmachinetranslation nmt approach to automatically transform the source code of the before version of a method i.e.
the version before the implementation of code changes to its after version i.e.
the version after the code changes are reviewed and merged .
broadly speaking tufanoet al.
performed code abstraction to reduce vocabulary size by replacing actual identifier literals with reusable ids and built an nmt model based on a recurrent neural network rnn architecture totransformthetokensequenceofthebefore version to the token sequence of the after version.
their approach wasevaluatedbasedonthechangedmethodsthatwereextracted from three gerrit code review repositories namely android google andovirt .webrieflydescribetheirapproachbelow.
step code abstraction since nmt models are likely to be inaccurateandslowwhendealingwithalargevocabularysize tufanoet al.proposed to abstract code tokens into reusable ids to reduce the vocabulary size.
more specifically for both before and after versions mb ma of each changed method tufano et al.replaced identifiers i.e.
type method and variable names and literals i.e.
int double char stringvalues withareusableid.a reusableidmeansthatanidisallowedtobereusedacrossdifferent changed methods e.g.
the first variable appearing in a changed methodwillbealwaysreplacedwith var 1 .attheendofthisstep from the original source code of mb ma theabstracted code sequences amb ama wereobtained.inaddition foreachchanged method a map m amb was also generated which will be used to map the ids back to the actual identifiers literals.
step build an nmt model to build an nmt model tufano et al.usedarecurrentneuralnetwork rnn encoder decoderarchitecture with the attention mechanism .
given the abstracted code sequences amb ama from step an rnn encoderwilllearnthesesequencesbyestimatingtheconditional probability p y1 ... yt x1 ... xt wherex1 ... xtis the sequence ofambandy1 ... yt is the sequence of amawhile the sequence lengthstandt may differ.
a bi directional rnn encoder was used to learn the abstracted code sequence of the before version ambfrom left to right and right to left when creating sequence representations.anrnndecoderwasthenusedtoestimatethe probability for each token yiin the abstracted code sequence of the after version amabased on the recurrent state si the previous tokensy1..i andacontextvector ci.thevector ciistheattention vector which is computed as a weighted average of the hidden states allowing the rnn model to pay attention to particular parts ofamb when predicting token yiforama.
step3 generatepredictions giventheabstractedcodesequence of the before version of an unseen method i.e.
a testing instance amt b thernnmodelinthetufano etal.approachgeneratedthe abstractedcode sequenceof theafterversion amta.a beamsearch strategywasusedtoobtain ksequencecandidatesfor amta.since the generated output sequences are the abstracted code sequences tufanoet al.replaced the ids found in amtaback to the actual identifier literals based on the map m amt b .
limitations.
althoughthestudyoftufano etal.shedslightthat theirnmtapproachcantransformsourcecodewithmeaningful authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
autotransform automated code transformation to support modern code review process icse may 21 29 pittsburgh pa usa before version after version reusable ids byte pair encoding bpe generated sequence from rnn generated sequence from transformerdtufano et al.
approach code abstraction rnn our autotransform subword tokenization transformer 237public void onsuccess final type 1 result method 1 var 1 var 2 line var 3 244original code publicvoidonsuccess finalcom.google.gwtjsonrpc.client.voidresult result createcommenteditor suggestrow column line sidepanel abstracted code233 235original code publicvoidonsuccess finalcom.google.gwtjsonrpc.client.voidresult result createcommenteditor suggestrow column line side 246abstracted code public void onsuccess final type 1 result method 1 var 1 var 2 line var 4 240public void on success final com.google.
gwtjsonrpc.
client.
voidresult result create comment editor suggest row col umn line side 249public void on success final com.google.
gwtjsonrpc.
client.
voidresult result create comment editor suggest row col umn line side panel figure a motivating example for an unknown identifier literal for the newly introduced abstracted token limitation .
t able the number of changed methods with and without new tokens in the after version.
dataset method size w o new tokens w new tokens android small me dium google small me dium ovirt small me dium the detail of data preparation is provided in section .
.
code changes e.g.
bug fix refactoring their approach still has the following limitations.
limitation unknownidentifiers literalsforthenewtokens appearing in the after version.
we hypothesize that the tufanoet al.approach may not be able to correctly transform code whennew tokensappearin the afterversion butdid notappear in thebeforeversion.forsuchacase thecodeabstractionapproach withreusableidsoftufano etal.cannotmaptheidofanewtoken back to the actual identifiers or actual literals.
indeed itispossiblethatdevelopersintroducednewtokensthat didnotappearinthebeforeversion.figure1illustratesamotivating exampleforlimitation1.asshownintheexample the sidevariable is changed to sidepanel .
thus the sidepanel variable is a new token appearing inthe after version.
bythe code abstraction with reusableidoftufano etal.
anewidwillbeassignedto sidepanel i.e.
var 4 sidepanel .
note that this limitation is different from the out of vocabulary oov problem since the tufano et al.approachmaystillbeabletopredictthecorrectid i.e.
var 4 for the after version amainstead of assigning a special tokens e.g.
unk .
however this var 4cannot be realistically mapped back to the actual identifier i.e.
sidepanel since it does not appear in the before version mbnor its mapping m amb .
moreover whenweanalyzethechangedmethodsinthedatasets oftufano etal.
wefindthatasmuchas80 ofthechanged methodhavenewtokensappearingintheafterversion seetable1 .
however tufano etal.excludethesechangedmethodswithnew tokens whileusingonlytheremaining16 ofthechanged methodswithoutnewtokensfortheir experiment .thus the tufanoet al.approach may be applicable to only a small set of changed methods limiting its applicability in real world contexts.limitation suboptimalperformancewhenthesequences b ecomelonger.
priorstudieshaveraisedaconcernthattheperformanceofthernnarchitecturewillbesuboptimalwhensequences become longer .
although tufano et al.leveraged the attention mechanism to handle changed methods that have long sequences a recent study by ding et al.
noted that such attention mechanism for the rnn architecture still has difficulties in remembering long term dependencies between tokens in a sequence.
in particular the attention mechanism only computes attention weightsbasedonthefinalhiddenstatesofthernnarchitecture instead of using any given intermediate states from the encoder causingthernnmodeltoforgettokensseenlongago.thus we hypothesize that the performance of the tufano et al.approach which is based on the rnn architecture may be suboptimal for the changed methods with long sequences.
autotransform inthissection wepresentour autotransform aneuralmachine translation nmt approachthatcantransformcode whennew tokens appear in the after version and when code sequences becomelonger.
autotransform leveragesabyte pair encoding bpe approach to handle new tokens that may appear in the after version and leverages a novel nmt architecture called transformer to address the suboptimal performance of the rnn architecture used in the tufano et al.approach.
overview.
figure2providesanoverviewofour autotransformapproach whichconsistsoftwostages trainingandinference.
duringthetrainingstage our autotransform performstwomain steps.
in step we perform subword tokenization on the original sourcecodeofthebeforeandafterversionsofachangedmethod mb ma usingbpe whichproducessubwordsequences smb sma .
instep weusethesesubwordsequences smb sma totraina transformer model.
the inference stage is for transforming the before version mt b of given methods i.e.
testing data to the after version mta .
to do so we perform subword tokenization on the before version in step 1bto produce a subword sequences smt b. then instep weuse the transformernmt model togenerate a prediction for the source code of the after version mta.
below we provide details for each step in our approach.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may 21 29 pittsburgh pa usa patanamon thongtanunam chanathip pornprasit and chakkrit tantithamthavorn smb generate merge operations c o co co l col .
.
.mb ma original source code of the befor e and after versions in training dataa list of merge operations smbsma subword sequences of the befor e and after versions mb original source code of the befor e version in testing datasmb a subword sequence of the befor e versionapply merge operationssubword tokenization by bpetraining inferencesmabefore version input after version t arget embedding positional encodingmulti head self attentionfeed forwardembedding vectors apply merge operationsencoder layer x n eencoder block embedding positional encoding embedding vectorsmasked multi head self attentionfeed forwarddecoder layer x n ddecoder block multi head self attentionlearning code transformation by transformer smb before version input vocab position ......... nv1 v2 v3 v4 word probabilitiesgenerate target sequence generated sequenceconvert subwords to codema prediction a transformer modelgenerating predictions12 31a 1b 1b2a2b attention vectors 2c2d 3a 3b3c sma figure an overview of our a ut otransform.
.
subword tokenization to reduce the vocabulary size and handle new tokens that may appearintheafterversion weperformsubwordtokenizationusing byte pair encoding bpe .
bpe is a tokenization approach that splits a word i.e.
a code token into a sequence of frequently occurring subwords.
for example a code token columnin figure will be split into a sequence of col andumn where is a subwordseparator.whenusingbpe thevocabularywillmainly contain subwords which are more frequently occurring than the whole code tokens.
prior studies also have shown that bpe can effectivelyaddressthelargevocabularysizethanothertokenization approaches e.g.
camel case splitting .
moreover bpe will enable our approach to address the new token by allowing the nmt model to generate a new code token based on a combination of subwords existing across all methods in the training data.
for example the code token sidepanel which is introduced in the after version seefigure1 canbeconstructedbasedonacombinationof side andpanelif these two subwords exist in the training data.
thesubwordtokenizationconsistsoftwomainsteps seefigure .step 1aisforgeneratingmergeoperationswhichareusedto determine how a code token should be split.
to generate merge operations bpe will first split all code tokens into characters sequences.
then bpe generates a merge operation by identifying the most frequent symbol pair e.g.
the pair of two consecutive characters that should be merged into a new symbol.
for example giventhat c o isthemostfrequentsymbolpairinthecorpus the merge operation will be c o co .
then bpe replaces all oftheco occurrenceof c o with co withoutremoving c or o which may still appear alone .
after the previous merge operation is applied bpe generates a new merge operation based on the frequency of current symbol pairs e.g.
co l col .
this step is repeated until it reaches a given number of merge operations.
to ensure the consistency of subword tokenization between the before and after versions of a changed method we use jointbpe to generate merge operations based on the union of code tokens from both before and after versions.
note that we generated merge operations based on the training data.
in step 1b we apply merge operations to split code tokens into subwords.
to do so bpe will first split all code tokens into sequences of characters.
then the generated merge operations are appliedtothebeforeandafterversionsinthetrainingdata.notethatthelowerthenumberofmergeoperationsapplied thesmaller the size of vocabulary is.
we also apply the same list of merge operationstothebeforeversioninthetestingdata.notethatwe didnotsplitjavakeywordssincetheyarecommonlyusedacross changed methods.
.
learning code transformation to learn code transformation we train a transformer based nmt modelusingthesubwordsequences smb sma ofthebeforeand after versions.
unlike the rnn architecture the transformer architecture entirely relies on an attention mechanism without using thesequence alignedrnnsorconvolutionnetworks which allows the transformer model to better pay attention to any set of tokens across arbitrarily long distances.
transformer uses a selfattention functiontocomputeattentionweightsbasedonalltokens in a sequence where attention weights indicate how each token isrelevanttoallothertokensinthesequence.thisself attention function enables the transformer model to capture the contextual relationshipbetweenalltokensinthewholesequenceinsteadofrelying on the limited number of the final hidden states like the rnn architecture.
in addition instead of using a single self attention thetransformerarchitectureemploysamulti headself attention which calculates attention weights htimes where his the number of heads based on the different parts of input data.
thetransformerarchitectureconsistsoftwomajorcomponents an encoder block which encodes a subword sequence into a vector representation andadecoderblockwhichdecodestherepresentation into another subword sequence.
the transformer performs thefollowingfourmainsteps seefigure2 .first instep 2a given a subword sequence of the before version smb s1 ... st the transformer embeds the tokens into vectors and uses positional encoding to add information about the token position of the sequenceintotheembeddingvectors.second theembeddingvectors are then fed into the encoder block 2b which is composed of a stack of multiple encoder layers where neis the number of encoderlayers .eachlayerconsistsoftwosub layers amulti head self attention and a fully connected feed forward network ffn which computes attention weights and generates attention vectors.
the attention vectors will be used to inform the decoder block about which tokens that should be paid attention.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
autotransform automated code transformation to support modern code review process icse may 21 29 pittsburgh pa usa the subword sequence of the after version sma u1 ... ut is used as an input of the decoder block.
similarly in step 2c the subword sequence of smais embeded into vectors with the position information.
then the embedding vectors are fed into the decoder block 2dto generate an encoded representation of the target sequence.
the decoder block is also composed of a stack of multipledecoderlayers where ndisthenumberofdecoderlayers .
eachdecoderlayeralsoconsistsofmulti headself attentionand ffn sub layers.
however before the embedding vectors are fed intothemulti headself attentionsub layer themaskedmulti head self attention layer is used to ensure that only previous tokens i.e.
u1 ... ui areusedindecodingforacurrenttoken ui.after that the multi head self attention with subsequent ffn generates an encoded representation of the target sequence.
the encoded representation are converted into word probabilities and the final output sequence using the linear and softmax functions.
finally thelossfunctionwillcomparethisoutputsequencewiththetarget sequence to estimate an error.
.
generating predictions theinferencestageaimstogeneratetheafterversionforagiven methodbasedonthebeforeversion mt binthetestingdata.starting from the beforeversion mt b we performsubword tokenization by applying the list of the merge operations generated in the training stage to produce a subword sequence smt b for the before version instep1b.then instep weuseourtransformermodeltogenerate the target sequence i.e.
the after version mta.
in particular in step3a given the subword sequence smt b the transformer model will estimate a probability of each subword in the vocabulary at each position in the output sequence.
then based on the generated subword probabilities in step 3b we use the beam search approach to generate the target sequence.
broadlyspeaking beamsearchgeneratesthetargetsequenceby selectingthebestsubwordforeachpositionintheoutputsequence basedonthegeneratedsubwordprobabilities.inparticular beam searchselectsasubwordforacurrentpositionbasedontheselected subwords in the previous positions.
the beam width kis used to specifythenumberofpreviouspositionsthatshouldbeconsidered andthenumberof sequencecandidatesthatwillbegenerated.
in other words the selection of a subword for a position iis based on the conditional probabilities of selected subwords in the positions i ktoi .finally thebeamsearchgeneratesthebest ksequence candidates for the target sequence.
the sequence candidates generated in step 3bare the subword sequences.hence instep 3c weconvertthesesubwordsequences back to the code sequences.
this step is simply performed by concatenating the subwords ending with with the subsequent subword.
for example the subwords are converted back to column.
case study design inthissection wepresentthemotivationofourresearchquestions data preparation and experimental setup for our case study.
.
research questions to evaluate our autotransform we formulate the following two research questions.
rq1 can a utotransform transform code better than tufanoet al.approach?
motivation.
inthiswork weproposedour autotransform to address the limitations of the state of the art approach .
in particular the goal of our autotransform is to allow an nmt model to better transform code when new tokens appear in theafterversion and whenthecodesequencebecomelonger.
hence we set out this rq to evaluate our autotransform based on the two aforementioned aspects.
rq2 what are the contributions of autotransform s components?
motivation.
toaddress thelimitationsof thestate of the artapproach we used two different techniques in our autotransform i.e.
bpe to reduce the vocabulary size and handle newtokensappearingintheafterversionand transformer to learn codetransformation.
in this rq we setout to empirically evaluate the contribution of each technique to the performance of ourautotransform comparedagainstthetechniquesusedinthe tufanoet al.approach.
.
datasets in this work we obtain the datasets from the work of tufano et al.
.
the datasets consist of changed methods which wereextractedfrom58 728reviewsacrossthreegerritcodereview repositories namely android google and ovirt .
for each changed method the datasets consist of the source code of thebeforeandafterversions i.e.
mb ma .forourexperiment we performdatapreparationontheseobtaineddatasetstoclassifyand selectthechangedmethods.notethatwedidnotusethefiltered datasetsthatwereusedintheexperimentoftufano etal.
since theirfiltereddatasetsdonotincludethechangedmethodsofwhich the after version contains a new token which is considered in this work .table1providesanoverviewofthestudieddatasetsafter the data preparation which we describe in details below.
data preparation.
we first classify the changed methods into two types of changed methods the changed methods of which theafterversion doesnotcontainnewidentifiers literals additionallyfromthebeforeversion i.e.
changedmethods without new tokens and thechangedmethodsofwhichtheafterversion containsnewidentifiers literalsadditionallyfromthebeforeversion i.e.
changed methods withnew tokens .
the changed methods withoutnew tokenswere usedto fairlyevaluate our autotransformagainst the tufano et al.approach under the same condition as in the prior experiment while the changed methods with newtokenswereusedtoevaluatewhetherour autotransform cantransformsourcecodewhenanewtokenappearsintheafter version i.e.
evaluating the hypothesis discussed in limitation .
forthechangedmethods withoutnewtokens weusedthesame selection approach as in the prior work .
more specifically the changed methods withoutnew tokens are those methods of which theafterversion macontainsonly javakeywords top 300frequent identifiers literals and identifiers and literals that are already available in the before version mb.
the remaining changed authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may 21 29 pittsburgh pa usa patanamon thongtanunam chanathip pornprasit and chakkrit tantithamthavorn table2 vocabularysizefortheoriginal subwordtokenized and abstracted methods in the training datasets.
dataset subword tokenized method size change type original bpe2k bpe5k abs android w o new tokens small w new tokens google w o new tokens small w new tokens ovirtw o new tokens small w o new tokens android w o new tokens me dium w new tokens google w o new tokens me dium w new tokens ovirtw o new tokens me dium w o new tokens the w and w o new tokens change types are mutually exclusive sets.
methodsofwhichtheafterversioncontainstokensnotlistedinthe aforementionedtokencategoriesareclassifiedasthechangedmethodswithnewtokens.notethatweexcludethechangedmethods that were completely added or deleted i.e.
mb orma and thechangedmethodsofwhichbeforeandafterversionsappearthe same i.e.
mb ma since nmt models would not be able to learn any code transformation patterns from these methods.
in addition weremovetheduplicatechangedmethods i.e.
themethodswhose mb mb are exactly same to ensure that none of the duplicate methods in testing will appear in training.
afterselectingandclassifyingthechangedmethods weseparatethedatasetsintotwomethodsizes i.e.
smallandmediumto evaluate the hypothesis discussed in limitation see section .
thesmallmethods are those methods of which before and after versions have a sequence length no longer than tokens.
the medium methods are those methods of which before and after versions have a sequence length between tokens.
similar to prior work we disregard the changed methods that have a sequence longer than tokens because large methods have a long tail distribution of sequence lengths with a high variance which might be problematic when training an nmt model.
in total we conduct an experiment based on datasets i.e.
repositories android google ovirt method sizes small medium 2changetypes w oandw newtokens .eachofthe datasets is then partitioned into training validation and testing .
.
experimental setup this section provides setup details for our experiment.
source code pre processing.
before we perform subword tokenization for our autotransform and code abstraction for tufanoet al.approach we perform word level tokenization to converttheformattedcodeintoasequenceofcodetokens.todo so foreachversionofeachchangedmethod wesimplyseparate code lines identifiers literals java keywords and java reserved characters e.g.
byaspace.wedonotconvertcodetokens tolowercasesbecausetheprogramminglanguage i.e.
java ofthe studied datasets is case sensitive.
we also do not split code tokenstable hyper parameter settings for the transformer and rnn models.
hyper parameter transformer model rnn model encoder layers ne de coder layers nd cell types n a gru lstm cells n a emb edding size n a attention size n a attention heads h n a hidden size hs n a total settings withcompoundwords e.g.
camel case sinceour autotransform already performs subword tokenization and such compound word splitting is not performed in tufano et al.approach.
for ourautotransform we use the implementation of sennrichetal.
toperformsubwordtokenizationusingbyte pair encoding bpe .inthiswork weexperimentwithtwoencoding sizes i.e.
thenumberofmergeoperations bpe2k and5 bpe5k whichsubstantiallyreducethevocabularysize atleast approximately by see table .
for tufano et al.approach we usesrc2abs toabstract tokenswithreusableids abs andgenerateamap mforeachchangedmethod.similartopriorwork we do not abstract java keywords and the top frequent identifier literals .toensurethattheidentifier literalappearingin bothversionshasthesameid weuseapairmodeof src2absfor thetrainingandvalidationdata whileasinglemodeisusedforthe before version in the testing data.
nmtmodels hyper parametersettings.
tobuildatransformer model in our autotransform we use the implementation of thetensor2tensor library .
to build an rnn model in tufanoet al.approach we use the implementation of the seq2seq library which is also used in the prior work .
to ensure a fair comparison we use similar combinations of hyper parameters whereapplicable forbothtransformerandrnnmodels seetable .
therefore we experiment with eight hyper parameter settings for our transformer model in autotransform.
for the rnn models we use ten hyper parameter settings which are originally used in the experiment of the prior work .
when training the models for both approaches we set the maximumnumberofepochssimilarasinthepriorwork .1toavoid the overfitting of our models to the training data we select the modelcheckpoint i.e.
themodelthatwastraineduntilaparticular number of epochs that achieves the lowest loss value computed based on the validation data not the testing data .
evaluation measure.
to evaluate the performance of our autotransform and tufano et al.approach we measure the number of methods for which an approach achieves a perfect prediction i.e.
thegeneratedafterversion exactlymatchestheground truth i.e.
the actual after version .
note that we convert the generated after version i.e.
thesubwordsequenceofour autotransform and 1note that epochs are calculated based on the size of training data batch size and train steps which are calculated differently for the tensor2tensor andseq2seq libraries.
the details are provided in the supplementary materials .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
autotransform automated code transformation to support modern code review process icse may 21 29 pittsburgh pa usa t able perfect predictions pp of our a utotransform and tufano et al.approach approach for the small and medium changed method with and without new tokens in the after version.
the percentage value in the parenthesis indicates the percentage improvement of our autotransform.
beam width beam width beam width dataset autotransform t ufanoet al.autotransform t ufanoet al.autotransform t ufanoet al.
method size change type test pp pp pp pp pp pp android w o new tokens small w new tokens google w o new tokens small w new tokens ovirt w o new tokens small w new tokens android w o new tokens me dium w new tokens google w o new tokens me dium w new tokens ovirt w o new tokens me dium w new tokens totalw o new tokens w new tokens both theabstractedcodesequenceofthetufano etal.approach backto the code sequence before matching it with the ground truth.
in our experiment we use three different beam widths i.e.
k whengenerating theafterversion.thus ifoneofthe k sequencecandidatesexactlymatchestheground truth weconsider that the nmt approach achieves a perfect prediction i.e.
the code iscorrectlytransformed.wedonotuseothermetrics e.g.
bleu which measures the overlap or similarity between the generated and ground truth sequences since similarity cannot imply that the generatedsequencesareviableforcodeimplementation.ding et al.also argue that bleu should not be used to evaluate the code transformationsincethesequencesthatare similar i.e.
fewcode tokens are different between the two sequences may have largelydifferent intentions or semantics .
results rq1 can autotransform transform code better than tufano et al.approach?
approach.
to address our rq1 we evaluate how well our autotransform can transform the source code of the before version mbtotheafterversion maofgivenmethods inthetestingdata compareagainsttheapproachoftufano etal.
.therefore we build an nmt model for each of the datasets i.e.
small and medium methods with and without new tokens across three repositories see table1 .for thisrq we usethemaximum numberof merge operations of i.e.
bpe2k in our autotransform.
in total wetrain96transformermodelsforour autotransform i.e.
datasets hyper parameter settings and rnn models fortufano etal.approach i.e.
12datasets 10hyper parameter settings .then foreachapproachandforeachdataset weselect the model with the best hyper parameter setting that achieves the lowest loss value computed based on the validation data.
finally we measure perfect predictions based on the testing data.
to quantifythemagnitudeofimprovementforour autotransform wecompute a percentage improvement of perfect predictions using a calculation of ppour pptufano p ptufano.
results.
table shows the results of perfect predictions of our autotransform and tufano et al.approach of changed methodsacrossthe12datasets.theresultsarebasedonthreebeam widths k wherekisthenumberofsequencecandidates for a given method.
whenconsideringbothchangetypes i.e.
w oandw new tokens our autotransform achieves a perfect prediction for1 413methodswhichis567 higherthantheperfectpredictionsachievedbythetufano etal.approach.
table4shows that when a beam width is and both change types are considered ourautotransform achieves a perfect prediction for methods which accounted for for google medium for ovirt medium of the methods in testing data.
on the otherhand tufano etal.approachachievesaperfectpredictionfor 86methodswhichaccountedforonly0.
forgoogle medium forovirtsmall .intotalacrossthe12datasets ourautotransform cancorrectlytransform1 413methods which is567 higherthantheperfectpredictionsachievedbytufano et al.approach.
evenwhenweincreasethebeamwidthto5and10 our autotransform canachievehigherperfectpredictions i.e.
445for googlemedium 949forovirtmedium atbeamwidth and7 445forgooglemedium 949forovirtmedium at beamwidth .theperfectpredictionsareimprovedby511 for thebeamwidthof5 and490 forthebeamwidthof10 whencomparedtotheperfectpredictionsachievedbytufano etal.approach.
theseresultsindicatethatthenumberofmethodsforwhichour autotransform canachieve aperfect predictionissubstantially higher than tufano et al.approach.
for the changed methods withnew tokens appearing in theafterversion our autotransform achieveaperfectpredictionfor18 415methods.
at beam width is table shows authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may 21 29 pittsburgh pa usa patanamon thongtanunam chanathip pornprasit and chakkrit tantithamthavorn that ourautotransform achieves a perfect prediction for methods which accounted for of the changed methods withnew tokens in the testing data.
in total our autotransform achievesaperfectpredictionfor1 060methods.
similarly our autotransform achieveshigherperfectpredictions when the beam width is increased to and i.e.
at the beam width of and at the beam width of .
on the other hand table shows that tufano et al.approachcouldnotachieveaperfectpredictionforanyofthe changedmethodswithnewtokensappearingintheafterversion.
this is because the ids of the new tokens cannot be mapped back the actual identifiers literals as they did not exist in the before version see limitation in section .
these results highlight that ourautotransform cantransformsourcecodeevenwhennew tokens appear in the after version.
forthechangedmethods without newtokensintheafter version our a utotransform achieves a perfect prediction for methods while tufano et al.approach achieves a perfect prediction for methods.
at the beam width of table shows that our autotransform achieves a perfect predictionfor11 111methodswhichaccountedfor5 of the changed methods withoutnew tokens in the testing data.on theotherhand tufano etal.approachachieves aperfect predictionfor9 86methodswhichaccountedfor3 .
for the small methods our autotransform achieves more perfect predictions in the android dataset but fewer in the google andovirtdatasets.
wewillfurtherdiscuss thisresultinsection6.
nevertheless it is worth noting that for the medium methods our autotransform achieves perfect predictions higher than the perfect predictions achieved by tufano et al.approach.
the results are similar when the beam width is and .
these results suggest that when a sequence becomes longer ourautotransform transforms code better than the tufano et al.approach highlightingthatour autotransform canaddress limitation discussed in section .
rq2 what are the contributions of autotransform s components?
approach.
toaddressourrq2 weexamineperfectpredictions when a component in our autotransform is varied.
specifically we examine the percentage difference of perfect predictions when subword tokenization is changed to code abstraction bpe abs andwhenthenmtarchitectureischangedfromtransformerto rnn transformer rnn .
we also investigate the case when the maximum number of merge operations is changed from to bpe2k bpe5k i.e.
the vocabulary size increases.
thus weevaluatetheperfectpredictionsoffouradditionalcombinations bpe5k transformer abs transformer bpe2k rnn bpe5k rnn.
we build an nmt model using each combination foreachofthe12datasets.intotal forrq2 wefurtherbuild192 transformer models i.e.
transformer based combinations datasets 8hyper parametersettings and240rnnmodels i.e.
rnn based combinations datasets hyper parametersettings .
similar to rq1 we select the model with the best hyperparametersettingbasedonthevalidationdata andmeasureperfect predictions based on the testing data.
results.
figure shows the perfect predictions of our autotransform bpe2k transformer tufano etal.approach abs rnn and the four additional combinations.
using subword tokenization by bpe can increase perfect predictions at least by compared to the code abstractionwithreusableids.
figure3showsthatatbeamwidthof1 the perfect predictions of our autotransform bpe2k transformer is284 50forandroidsmall 22forovirtmedium higherthanabs transformer.consideringthecaseswhenthernn architecture is used with bpe i.e.
bpe2k rnn and bpe5k rnn the perfect predictions are also higher than tufano et al.approach abs rnn .
for example for the small methods figure shows thatbpe2k rnnachievesperfectpredictions29 higher than tufano et al.approach.
figure also shows similar results when the beam width was increased to and .
these results indicate that regardless of the nmt architecture subwordtokenizationbybpelargelycontributestotheperformance in transforming code of our autotransform.
when using a different number of merge operations bpe2k bpe5k wefindthatperfectpredictionswereslightlydifferent.for example forthesmallmethods atbeamwidthof1 thepercentage difference of perfect prediction between our autotransform and bpe5k transformer is .
figure also shows that the results are similar for the rnn based approaches i.e.
bpe2k rnnandbpe5k rnn .theseresultssuggestthatthe number of merge operations has an impact but relatively small on the performance of our autotransform.
using the transformer architecture can increase perfect predictions at least by compared to the rnn architecture.figure3showsthatatthebeamwidthof1andforsmallmethods ourautotransform achievesperfectpredictions17 for ovirt 18for google higher than bpe2k rnn.
it is alsoworthnotingthatthepercentagedifferenceismuchhigherfor themediummethods i.e.
12forgoogle forandroid .figure3alsoshowsalargedifferenceofperfectpredictions between our autotransform and bpe2k rnn when the beam width is increased to and .
the results are also similarwhencomparingtheperfectpredictionsbetweenbpe5k transformer and bpe5k rnn i.e.
the transfomer models tend to achieve higher perfect predictions than the rnn models.
these resultssuggestthatthetransformerarchitecturealsocontributes to the performance of our autotransform in transforming code especially for the methods with a relatively long sequence like medium methods.
discussion in this section we discuss our autotransform in several aspects including its advantage performance and practicality.
advantage whydoesoura utotransform workforthechanged methods with new tokens?
table shows that in total our autotransform can correctly transform the methods that have new tokens appearing in the after version.
we further analyze these methodstobetterunderstandthecharacteristicsofthenewcode authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
autotransform automated code transformation to support modern code review process icse may 21 29 pittsburgh pa usa192 62android m google m ovirt m android s google s ovirt s beam beam beam beam beam beam beam beam beam 10beam beam beam beam beam beam beam beam beam pefect predictionsautotransform bpe2k transformer bpe5k transformer bpe2k rnn bpe5k rnn abs transformer tufano et al abs rnn figure the perfect prediction of our a ut otransform when a component is varied.
the y axis shows the total number of perfect predictions of changed methods withandwithout new tokens.
tokensappearingintheafterversion.wefindthat43 ofthe new code tokens are the identifiers literals that already exist in the trainingdata i.e.
knowncodetokens suggestingthatour autotransform can reuse the code tokens existing across all methods thatautotransform havelearnt.ontheotherhand thetufano et al.approach cannot generate these new code tokens because their approach is restricted by the code abstraction with reusable ids to useonlytheidentifiers literalsthatexistinthebeforeversion or that are the top frequent identifiers literals.
furthermore the other of the new code tokens appearing in the after version are new identifiers literals that do not exist in the training data suggesting that our autotransform can generate these new code tokens based on a combination of knownsubwordsinthetrainingdata.weobservethatthesenew code tokens are related to changing a java package library e.g.
org.junit.assert org.hamcrest.matcherassert changing identifier e.g.
getlog type name getlogtypename d dt.mid oreven adding new statements e.g.
instantiating a new object .
limitation whydoesour autotransform achievefewerperfect predictions than tufano et al.
approach for small methods without new tokens inthe google and ovirt datasets?
table showsthat for thesmallmethodswithoutnewtokensintheafterversion our autotransform achievesfewerperfectpredictionsthanthetufano et al.approachinthegoogleandovirtdatasets.tobetterunderstand the methods for which our autotransform cannot achieve a perfect prediction we manually examine of the methods google and54ofthe86methods ovirt forwhichour autotransform cannot achievea perfectprediction but thetufano et al.approach could.
we find that there are only out of and out of methods that are incorrectly predicted by our autotransform.
on the other hand for the remaining out of9 and out of methods we find that our autotransform almost achieves a perfect prediction i.e.
the generated sequence is very similar tothe ground truth with minor errors.
broadly speaking we observe thatformostofthesemethods ourapproachtransformssomecode tokens that should remain unchanged in the after version.
one possible reason is that these code tokens are a rare token and bpe splitsthemintomanysubwords i.e.
oneraretokenbecomesalong subwordsequence.then duetothelargesearchspaceofsubwords our approach may unnecessarily generate a new combination of subwords.forexample weobservethat fixturestool.data center is split into subwords i.e.
fixturestool.
da ... te r .
then our approach inserts a subword a resulting in a new tokenfixturestool.a data center .basedonthisobservation anapproachtoreducethelengthofsubwordsequences e.g.
fine tuning the number of merge operations may improve the performance.
hyper parametersensitivity howsensitivethehyper parameter settingisinour autotransform?
deeplearningmodelsareknown forbeingsensitivetohyper parametersettings.whileweselectthe besthyper parametersettingbasedonthevalidationdataforour experiment we are interested in examining the impact of hyperparameter settings on the performance of our autotransform.
hence we analyze the perfect predictions of our autotransform when each of the eight hyper parameter settings in table is used.
we find that a setting of ne nd h hs allows ourautotransform achieves the highest perfect predictions for 7outof12datasets whileasettingof ne nd h hs allows our autotransform to achieve the highest perfect predictions for out of datasets.
nevertheless we observe that the perfect predictions is decreased by only percentage points when using the other hyper parameter settings instead of the best setting.
performance how long does our autotransform take to train and infer?
model training and inference time can be one of the important factors when considering the adoption of our approach in practice.
hence we measure the training and inference time for authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may 21 29 pittsburgh pa usa patanamon thongtanunam chanathip pornprasit and chakkrit tantithamthavorn the transformer models in our autotransform.
we find that the training time for each transformer model in our autotransform usedinrq1israngingfrom30minutesto2hoursdependingon the size of the datasets and the number of epochs.
the average inference time of autotransform per method is ranging from to60millisecondsforgeneratingonesequencecandidate i.e.
beam width .similarly whengenerating5and10sequencecandidates per method i.e.
beam width the average inference time per input sequence is ranging from to milliseconds.
note thatthetrainingandinferencetimeisbasedonacomputerwith an nvidia geforce rtx graphic card.
practicality to what extent autotransform can support the modern code review process?
our rq1 has shown that autotransformcancorrectlytransform1 413methods whichissubstantially betterthanthepriorwork.thisresulthighlightsagreatpotentialof autotransform toaugment codeauthorsby recommendingthe commonrevisionsthatoccurredduringthecodereviewsinthepast to apply to the newly written code without waiting for reviewers feedback.
nevertheless at this stage of the work autotransform may be suitable for code changes of software components that toleratefalsepositivesas autotransform willprovidearecommendation for every changed method which is subject to produce many false positives compared to humans.
indeed prior work reported that developers may decide to not use a supporting tool if itsfalsepositiverateisabove25 .furthermore similartotheprior work theapplicabilityof autotransform isstilllimitedto smallandmediummethodsizes.thus tobroadenthepracticalityof autotransform future work should aim to develop an approach thatismoreselectivetoachievehigheraccuracy e.g.
below25 of false positive rate for any method sizes including large methods .
related work automatedcodereview.
codereviewiseffective butstillhumanintensiveandtime consuming .thus recentworkleveraged machine learning techniques to support various activities throughout the code review process for example reviewer recommendation reviewtaskprioritizationbased oncodechangecharacteristics anddefect proneness .
several studies also proposed approaches to support reviewerswhenreadingandexaminingcode .althoughtheseapproachescanreducethemanualeffortofreviewers codeauthorsstillneedtomanuallymodifythesourcecodeuntil itisapprovedbyreviewers.yet fewstudiesfocusondeveloping anapproachtoautomaticallytransformsourcecodetohelpcode authors reduce their effort in the code review context.
tothebestofourknowledge onlytworecentapproaches are proposed to automatically transform the source code to the version that is approved by reviewers i.e.
the after version .
however both recent approachesuse code abstraction abs rnn abs transformer toreducethevocabularysize whereour results show that code abstraction hinders the nmt approaches in correctly transforming code if the after version has a new token e.g.
renamed variables .
thus these recent approaches still have a limitedusagetoautomaticallytransformcodeinthecodereview process.differentfrompriorwork weleveragebpetoaddressthis limitation of prior work.
importantly our rq2 shows that usingbpe bpe transformer bpe rnn achieves perfect predictions at least by higher than using the code abstraction with reusable ids abs transformer abs rnn highlighting the important contribution of this paper to the automated code transformation for code review.
neuralmachinetranslation nmt insoftwareengineering.nmt approaches have been developed to support various software engineering tasks which can be categorized into four types of transformation text text e.g.
language translation of code documentation query reformulation text code e.g.
codesearch code text e.g.
code summarization commit messagegeneration and code code e.g.
automated program repair programminglanguagetranslation codecompletion .although automated program repair apr approaches and our autotransformshareasimilarconceptofusingnmtforacode codetask aprapproaches onlyaimtoautomaticallytransform buggy code to clean code for bug fixing purposes which may not be related to other types of code changes in code review.
different fromapr our autotransform aimstoautomaticallytransform source code that is changed during the code review process e.g.
refactoring to improve readability maintainability and design quality .
threats to validity constructvalidity.
thesourcecodegranularityusedinthiswork is at the method level.
the results may be varied if the changed source code is extracted at a different granularity e.g.
changed lines .however priorworkpointedoutthatannmtmodelrequires codecontextandusingonlychangedlinesmayleadthenmtmodel tosufferfromtheout of vocabularyproblem eventhoughbpe is used .
hence we believe that training an nmt model at a method level would provide a reasonable range of code context than changed lines.
wedefinethemethodsizebasedonthenumberoftokens i.e.
50tokensforsmalland51 100tokensformedium .thissize definition may not reflect the actual method sizes in practices e.g.
amethodwith100tokensmayactuallyhavefewlinesofcode.the performance of autotransform may differ if other definitions of method size are used.
nevertheless for the sake of fair comparison inourexperiment weopttousethesamedefinitionofmethodsize as used in the prior work .
moreover from the aspect of the nmtalgorithm transforminglongsequencesarenottrivialasit requires higher memory and computation power .
internal validity.
we experiment with only two settings of merge operations i.e.
bpe2k and bpe5k and hyper parameter settingswhicharebasedonacombinationsoffourhyper parameters.
theresultsmaybevariedifthenumberofmergeoperationsand the hyper parameter settings of both approaches are optimized.
however findinganoptimalsettingcanbeverycomputationally expensive given a large search space of the number of merge operationsandallavailablehyper parameters.inaddition thegoal of our work is not to find the best setting but to fairly compare the performanceof ourapproachwith theprior approachbased on similar settings as used in the prior work .
nevertheless our analyses in sections and have shown that the number of merge authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
autotransform automated code transformation to support modern code review process icse may 21 29 pittsburgh pa usa operationsandhyper parametersettingshaveasmallimpacton the performance of our approach.
externalvalidity.
weevaluateour autotransform basedon thechangedmethodsthatwereextractedfromthreegerritcode repositories.inaddition weonlyexperimentedwithjavaprograms.
our results may not be generalized to other code review repositories or other programming languages.
however our approach is based on the techniques i.e.
bpe and transformer that are language independent.
in addition we provided a replication package to facilitate future work to replicate our approach on different repositories and programming languages.
conclusion priorwork proposedannmtapproachtoautomaticallytransformagivenmethodfromthebeforeversion i.e.
theversionbefore theimplementationofcodechanges totheafterversion i.e.
the versionthatisreviewedandmerged .yet itsperformanceisstill suboptiomal when the after version has new identifiers or literals orwhenthesequencebecomeslonger.hence inthispaper weproposeautotransform which leverages bpe to handle new tokens and a transformer to handle long sequences.
throughanempiricalevaluationbasedon14 750changedmethodsthatareextractedfromthreegerritcodereviewrepositories wefindthat our autotransform cancorrectlytransform1 methods of which the after version has new tokens while the prior work can not correctly transform any of these methods and for thechangedmethodsofwhichtheafterversiondonothavenew tokens our autotransform can correctly transform methods which is higher than the prior work.
furthermore our ablation study also shows that bpe and transformer substantially contribute to the performance improvement of our autotransform whencomparedtothecomponentsusedinthepriorwork.
these results highlight that our autotransform effectively addressthelimitationsofthepriorwork allowingthenmtapproach tobeappliedtoawiderrangeofchangedmethods i.e.
methods with new tokens and methods with long sequences .
the proposed approachandtheresultsofthispapercontributetowardautomated codetransformationforcodereviews whichcouldhelpdevelopers to reduce their effort on modifying source code during the code review process.
ackowledgement patanamon thongtanunam was supported by the australian research council s discovery early career researcher award decra funding scheme de210101091 .
chakkrit tantithamthavorn was supported by the australian research council s discovery earlycareerresearcheraward decra fundingscheme de200100941 .