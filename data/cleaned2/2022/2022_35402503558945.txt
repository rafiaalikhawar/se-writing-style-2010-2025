understanding why we cannot model how long a code review will take an industrial case study lawrence chen meta platforms inc. menlo park ca usa lawrencechen fb.competer c. rigby meta platforms inc. new york ny usa pcr fb.comnachiappan nagappan meta platforms inc. bellevue wa usa nnachi fb.com abstract code review is an effective practice for finding defects but because it is manually intensive it can slow down the continuous integration of changes.
our goal was to understand the factors that influenced the time a change i.e.a diff at meta would spend in review.
a developer survey showed that diff reviews start to feel slow after they have been waiting for around hour review.
we built a review time predictor model to identify potential factors that may be causing reviews to take longer which we could use to predict when would be the best time to nudge reviewers or to identify diff related factors that we may need to address.
the strongest feature of the time spent in review model we built was the day of the week because diffs submitted near the weekend may have to wait for monday for review.
after removing time on weekends the remaining features including size of diff and the number of meetings the reviewers have did not provide substantial predictive power thereby not being able to predict how long a code review would take.
we contributed to the effort to reduce stale diffs by suggesting that diffs be nudged near the start of the workday and that diffs published near the weekend be nudged sooner on friday to avoid waiting the entire weekend.
we use a nudging threshold rather than a model because we showed that hours in review cannot be accurately modelled.
the nudgebot has been rolled to over 30k developers at meta.
ccs concepts software and its engineering software development process management .
keywords code review statistical modelling rigby is an associate professor at concordia university in montreal qc canada.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november singapore singapore copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
reference format lawrence chen peter c. rigby and nachiappan nagappan.
.
understanding why we cannot model how long a code review will take an industrial case study.
in proceedings of the 30th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november singapore singapore.
acm new york ny usa pages.
introduction best engineering practices allow developers to take risk and celebrate failures as they allow companies to avoid wasting resources on something that does not and cannot be made to work.
it is important to report these results to the broader software engineering community and in this paper we report our attempt to model how long a code review will take and why we were unable to create a successful model.
before we discuss the model why is it important to model the amount of time something will be under review?
code review has become increasingly incremental with fagan inspection taking weeks to months in the late s to the present day where code reviews take on the order of hours to a day .
however code review is a manual process that can limit the productivity and the pace at which code can be continuously integrated and delivered.
our goal is to model the review interval using traditional mining software repository measures such as churn as well as new measures such as how many meetings a reviewer has and the amount of time the files under review have taken to review in the past.
the original practical goal for our hours in review model was to predict how long a diff will be in review and then to nudge reviewers of a diff that has taken longer than was predicted i.e.the diff review is stale .
this model is related to the nudge system deployed at microsoft to nudge developers on overdue pull request.
however there are significant differences microsoft reports a mean absolute error of .
hours.
given that of reviews at meta take less than hours a model with this level of inaccuracy would be unusable at meta.
the goal of this paper is to report our attempt to investigate a review time prediction model in a new industrial context.
the overall research question for this work is how long will this diff be in review?
we create a random forest model that includes multiple factors such as expected review time for the directories under review the workload of the developers reviewing the files and the size of the change.
we found that the strongest predictor was the day of week with diffs published nearer to the weekend waiting until monday for review.
removing weekend hours the remaining features had poor predictive power they reduced the median absolute error by only minutes over the baseline model.
esec fse november singapore singapore lawrence chen peter c. rigby and nachiappan nagappan this indicated that a prediction model does not work at meta.
our work guided the development of a threshold for nudging developers with a bot that has been rolled out to all developers at meta.
background and methodology meta is a large online services company that works across the spectrum in the communications retail entertainment industries.
meta has tens of thousands of employees with offices spread across the globe north america europe middle east and asia .
meta has its own dedicated infrastructure teams where the tools used are a mix of commercial and in house developed systems.
we study reviews from all the projects using phabricator including facebook and whatsapp.
code review at meta follows a similar structure to the contemporary practices used at google and microsoft so we only briefly summarize the practice here.
code review is conducted in thephabricator tool.
a developer makes a change called a diff selects potential reviews and teams of reviews and publishes it for review.
reviewers provide feedback and comments which are addressed by the author.
there may be multiple rounds of feedback and changes ultimately the diff will be accepted or rejected.
meta uses a single truck development model so if the diff is accepted it will be integrated for further testing and then ultimately shipped into production.
for this paper our data set includes all diff review sessions between june and august days covering over 150k reviews.
devex survey.
the motivation for modeling hours in review came from the developer experience devex survey that is conducted each half at meta.
in the second quarter of around .9k developers responded to the devex survey.
here we focus on the question related to hours in review how satisfied are you with the time it takes to get your diffs reviewed by peers?
from the survey alone we learned that .
of developers are satisfied with the time their diffs spend in review i.e.
.
are dissatisfied.
while the vast majority of developers are satisfied when we anonymously associate the time it takes for a developers slowest diffs to be reviewed we found that developers that had longer review times were less satisfied and less likely to respond that they were satisfied.
in discussions with the phabricator team that maintains the diff review tool we decided to model the slowest of diffs which on the meta dataset roughly corresponds to diffs that have been in review for or more hours.
more details can be found in shan et al.
.
.
model features we created a model to predict how long a diff will be in review using the features described in table .
we suspected information about the files contained in the diff under review would influence the time a diff is in review.
for example larger changes may take longer to review.
we included the repo name for each project to factor in project level differences in the time in review the historical median time taken by diffs that modified the files in the directories to understand if review time varies by directory and the length of words in the diff summary to understand whether the size of the explanatory text for the change affects time in review.
we differentiated between code generated by human developers andautomated refactoring scripts i.e.rule based changes .
we also included the day of week and time of day to understand whether reviews submitted later in the week or in the day effect time in review the diff author s timezone and the number of reviewers directly added to the diff as well as the number of reviewers added through review groups.
finally we factored in the average number of meetings that the assigned reviewers have scheduled over the next hours as an indicator of meeting workload.
.
model and evaluation method to evaluate the model of time in review the timeinreview model we compared the predicted review session duration with the actual review session time for each diff.
we narrowed our data to diffs with review session times between hours and hours roughly between p75 and p95 of review time.
this helped us reduce the skew of the distribution a large proportion of diff review times are under hours and they were not the target diffs of our overdue diff nudging initiative.
we built a random forest model using the features described in table .
our model was trained on 120k review sessions and tested on 30k review sessions between june and august days .
to evaluate the importance of each feature we use the feature importance package from scikit learn.1this determines the decrease in the score when each feature is removed and indicates its importance.
for comparison we also built a baseline model that predicted that every review would take the median time in review of our review session data set to compare our new random forest with a trivial modeling approach.
this trivial baseline represents a unsophisticated method of predicting review time without any consideration of diff or reviewer features.
our outcome measure is the absolute error in hours with the following metrics mean median mae p75 and max.
we contrast this with our baseline to evaluate how much of an improvement our more sophisticated model has compared to a trivial model.
results can we predict how long a diff will be in review?
in table we see that the timeinreview model accurately predicts the length of reviews with a mae of .
hours compared to the baseline model that always predicts the median delay and has a mae of .
hours an improvement in mae of .
hours.
in table we see the permutation importance of each feature.
we discuss each by feature importance the most important feature is the day of the week.
intuitively this makes sense as diffs submitted for review on fridays will often have to wait the entire weekend for a review.
the hour of day is intuitively important with diffs submitted later in the day waiting until the following morning for review.
geography as measured by the timezone in which the author resides influences review time.
a developer who makes a change in london will have to wait for a teammate in california to wake up before the review can happen.
the average number of meetings the reviewers have scheduled in the next hours impacts time in review.
meetings take time away from time available for reviewing and other development activities.
1315understanding why we cannot model how long a code review will take an industrial case study esec fse november singapore singapore table the features for the timeinreview model feature description list of file features lines added lines removed number of files modified number of different file extentions line count ratio total line count of files in the diff repository name the name of the project repo that the diff modifies one hot encoding median directory review timethe median time that the files in the directory in the diff take to review in the past summary count words excluding code blockthe summary of the code change without any code markdown.
rule based change is this diff created using a rule?
an example if a human wants to rename a widely used class across the entire code base they would apply a tool based rule to implement the diff.
author is bot is the code author a bot or developer?
review iteration how many times has the diff been updated during the review?
day of week the day of week when the author published the diff for review hour of day the hour of day when the author published the diff for review author timezone the author s timezone.
captures geographical work office location num direct reviewers direct reviewer is a reviewer that is explicitly added by name reviewer is same team percentagepercentage of reviewers that are on the same team as the author avg direct reviewer meetings hoursaverage number of scheduled meetings hours across assigned reviewers in the hour period after the diff is published.
num group reviewers number of reviewers added as part of a team group group reviewer is same team percentagepercentage of group reviewers that are on the same team as the author avg group reviewer meetings hoursaverage number of scheduled meetings hours across assigned reviewers in the hour period after the diff is published.
table model quality for timeinreview model and timeinreview modelnoweekends vs the baseline models.
the baseline models simply predict the median value for each distribution.
after removing weekends the model poorly predicts time in review.
all values in absolute hours.
mean ae median ae p75 ae max ae median baseline26.
.
.
.
timeinreview model19.
.
.
.
median noweekends baseline19.
.
.
.
timeinreview model noweekends19.
.
.
.
the length of the summary of the diff is likely related to the complexity of the change with more explanation around complex or difficult changes.
code changes that are done using a rule such as renaming a class are easier to review the time to review changes in the past in a directory predicts how long they will take to review in the future.
the more reviewers in a review group and the more reviews on from the same team the more likelytable importance of timeinreview model features feature importance .
day of week .
.
hour of day .
.
author timezone .
.
avg group reviewer meetings hours .
.
summary count words excluding code block .
.
is rule based changed .
.
median directory review time .
.
num group reviewers .
.
group reviewer is same team percentage .
.
avg direct reviewer meetings hours .
.
lines added .
.
line count .
.
repository .
.
file extensions .
more people will see the diff.
the number of meeting hours that directly assigned reviewer has only a weak influence on time in review likely because they prioritize assigned reviews.
surprisingly features related to the actual source code have lower importance.
the lines added and total count of changed lines have a weak influence on interval.
the time in review appears to hold 1316esec fse november singapore singapore lawrence chen peter c. rigby and nachiappan nagappan across meta repositories and across file extensions with both having a relatively low impact on time in review.
the model appears to learn clear patterns regarding weekend delays but this pattern is obvious and one that can be accounted for using rules about the weekend rather than statistical models.
we were more interested in observing whether the model can find other patterns related to the diff features or reviewer characteristics besides the impact of weekends.
our next step was to remove the weekend hours from the hours in review and create an timeinreview modelnoweekends .
this updated model is identical totimeinreview model in terms of model architecture and features but uses a modified hours in review that excludes weekend hours.
table shows that the baseline model which simply predicts the median wait time has a mae of .
hours while timeinreview modelnoweekends has a mae of .
hours.
the timeinreview modelnoweekends improves median time predictions by less than minutes.
we do not report individual feature importance because the model performs so poorly.
it is clear that once weekends are removed the other factors such as diff size number of meetings and project repository are not strong enough indicators of review time to enable our model to outperform the baseline.
we believe this is because the review time at meta is already short so the natural variance of review times will diminish any correlation effects of the remaining features.
our model s best feature was whether the review spans the weekend.
once we used a rule to account for weekends the resulting model improved the mae by only minutes relative to predicting the median review time for all reviews.
given the poor performance of the model we cannot recommend it for use in review time prediction tasks such as nudging on overdue diffs.
retrospective correlation analysis.
models that predict time in review naturally cannot contain features that are only known after the review is complete.
however a retrospective analysis can highlight factors that influence hours in review.
one potentially important factor is the degree of participation and how long it takes to get reviewer attention.
when we conducted a spearman correlation between hours in review and the time to first action or comment we found an r .90with p .
.
while correlation is not causation a correlation as high as .
indicates that hours in review is strongly related to how long it takes to get a reviewer to take a look at the diff and provide a comment or accept or reject the diff.
clearly a model that nudges developers on diffs that are taking longer than the median time for review deserve to be nudged to get the attention of a reviewer.
impact at meta as part of the team responsible for reducing hours in review our initial model that included weekends was used to determine when to nudge reviewers on diffs.
our analysis had important impacts on the tool and final rollout.
first our original model had been used to determine when a diff was beyond the expected amount of time for review i.e.overdue and nudges needed to be sent to reviewers.
figure nudgebot sends up to three diffs that are stale to a developer who is likely to review them one hour after the start of the workday.
the chat message is sent with silent which will not push notify the developer but will allow them to view the message between blocks of focus.
the time the diff has been waiting is shown along with the username of the author and a clickable link to the diff.
if the reviewer is busy they can select remind me later.
nudgebot then examines the calendar and selects the next fragmented time e.g.
between two meetings to remind the developer later in the day.
emails are sent with the same information and same ability to be reminded later.
the diff numbers log message etc are mocked to preserve confidentiality but they are representative of real diffs at meta.
however we convinced the team that no model was necessary and that a threshold should be used to nudge reviewers because our model could not outperform a simple median review time model.
this drastically reduced the engineering effort.
second we showed the enormous impact of weekends and time of day on how long a diff will take to review.
as a result the nudges are sent first thing in the morning to developers and the diffs submitted on thursday are nudged with a shorter threshold so that reviewers take a look at them before the end of day on friday.
the final nudgebot which is in use by over 30k developers at meta and sends over nudges per day on overdue diffs is shown in figure and the full design can be found in shan et al.
.
any diff that has been inactive for hours has the reviewer that is most likely to take action on the review nudged via chat.
reviewers can also snooze notification which will resend the nudge to the reviewer when the are not in deep focus e.g.
after a meeting ends or after lunch.
the tool has been successful in reducing hours in review by around in an a b test.
fewer than of developers have opted out of nudging indicating that developers find these reminders useful.
threats to validity this paper focuses on our industrial experience and it unclear that our results would generalize to other contexts.
like the microsoft nudge paper we have very low predictive power of review interval despite a very different development context meta uses a mono repo single repository for code control rather than the microsoft branch structure .
also the systems built are different 1317understanding why we cannot model how long a code review will take an industrial case study esec fse november singapore singapore the domain the programming language are all different.
this also shows the value of replicating empirical studies in different contexts to evaluate the efficacy of results to build an empirical body of knowledge .
we only look at reviews that take or more hours in our model.
this decision was made because nudging a reviewer on a diff that is less than a day old will distract and annoy developers.
other software companies may decide to look at the entire review interval but that was not desirable at meta.
the outcome of time in review was highly susceptible to weekend delays.
we corrected this metric to remove weekends.
however some reviews occur on the weekend and these were ignored.
the impact of the weekend also suggests interesting future work to correct for working hours in the metric.
this is in line with recently published results which found that two thirds of engineers did not work weekends or nights.
irrespective of this the data used in this study is real live data from development at meta and hence the vagaries that exist in the data reflect the reality without ignoring or removing true events that happened in the data.
we did not perform any hyperparameter tuning for our timeinreview model which leaves open the possibility that these models can be further improved through a rigorous hyperparameter optimization.
by using unoptimized defaults the hyperparameters were fixed before looking at the test set to ensure no contamination.
in addition there is no reason that a hyperparameter search on random forest would result in dramatic changes in which features are considered the most important which is what we focused on in the end.
the first hyperparameter of depth would mostly impact features that are less important since most major features would already be accounted for in the first few layers of the model s trees.
in addition the other hyperparamer of number of trees would be expected to have negligible impact on feature importances given that each tree in the random forest is independent.
related work code review has been a best practice in software development for over years .
in the s fagan reported that inspections took weeks or months to complete.
fagan s inspection process relied on formal roles and synchronous meetings.
votta showed in the s that scheduling inspection meetings added to the review interval.
eick et al.
found that over of the defects found during inspection could be found in the without a meeting i.e.in the individual preparation phase.
in a controlled experiment perry et al.
showed that synchronous meetings were unnecessary and that asynchronous inspections were equally effective at finding defects.
building on these results porter et al.
simplified inspection and found that it could be reduced to around one week.
theses gains in shortening the review interval by reducing the complexity of the process has continued with rigby et al.
showing that the contemporary lightweight review process used by open source projects had a review interval of approximately one day which was confirmed to be similar to the review time at microsoft and google .
meta is no exception with short code review intervals on the order of a few hours and with over of reviews being completed in less than a day.although it is simple to report how long code reviews take we are unaware of any works that have successfully modelled the time in review.
in the s porter et al.
used a regression model of review interval with the factors of the team size the number of review sessions and categorical variables for author and reviewers.
their model could only explain of the variance in the hours in review.
rigby et al.
modelled the review interval of large open source projects including apache and linux.
their model included author and reviewer experience the churn and the number of reviews involved in the review.
like porter et al.
their regression model was only able to explain of the variance.
microsoft recently created a model of review interval with the goal of nudging developers on pull requests that had taken longer than the model predicted .
unfortunately they report very low accuracy with a mean average error of .
hours and a mean relative error of .
.
our random forest model of hours in review was also inaccurate with a mean average error of .
hours and only a minute improvement over the baseline model that always predicts the median time.
concluding remarks how long will this diff be in review?
prior works on modeling the hours in review have shown low predictive power .
at meta the model shows that review times are consistent regardless of factors such as diff size the number of meetings reviewers have and the project repository.
the largest contributing factor is wait time.
this is seen in the importance of reviews that span weekends e.g.
they usually wait an extra hours and diffs submitted later in the day will wait overnight.
the key is to get the diff in front of the reviewer before the review starts to feel too slow to the author.
from the developer experience survey this roughly corresponds to diffs that have had no actions for or more hours.
after removing the weekend hours from the timeinreview model the model did not outperform the baseline model which always predicts the median hours in review from the training set.
as a result the overdue diff nudging tool shown in figure nudges developers after hours and pings them at the start of the work day.