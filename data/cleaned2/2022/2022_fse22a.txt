api recommendation for machine learning libraries how far are we?
moshi wei york university toronto canada moshiwei yorku.cayuchao huang institute of software chinese academy of sciences yuchao2019 iscas.ac.cnjunjie wang institute of software chinese academy of sciences junjie iscas.ac.cn jiho shin york university toronto canada jihoshin yorku.canima shiri harzevili york university toronto canada nshiri yorku.casong wang york university toronto canada wangsong yorku.ca abstract application programming interfaces apis are designed to help developers build software more effectively.
recommending the right apis for specific tasks is gaining increasing attention among researchers and developers.
however most of the existing approaches are mainly evaluated for general programming tasks using statically typed programming languages such as java.
little is known about their practical effectiveness and usefulness for machine learning ml programming tasks with dynamically typed programming languages such as python whose paradigms are fundamentally different from general programming tasks.
this is of great value considering the increasing popularity of ml and the large number of new questions appearing on question answering websites.
in this work we set out to investigate the effectiveness of existing api recommendation approaches for python based ml programming tasks from stack overflow so .
specifically we conducted an empirical study of six widely used python based ml libraries using two state of the art api recommendation approaches i.e.
biker and deepapi.
we found that the existing approaches perform poorly for two main reasons python based ml tasks often require significant long api sequences and there are common api usage patterns in python based ml programming tasks that existing approaches cannot handle.
inspired by our findings we proposed a simple but effective frequent itemset mining based approach i.e.
fimax to boost api recommendation approaches i.e.
enhance existing api recommendation approaches for pythonbased ml programming tasks by leveraging the common api usage information from so questions.
our evaluation shows that fimax improves existing state of the art api recommendation approaches by up to .
and .
in mrr and map respectively.
our user study with developers further demonstrates the practicality of fimax for api recommendation.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november singapore singapore association for computing machinery.
acm isbn .
.
.
.
concepts information systems recommender systems computing methodologies machine learning .
keywords api recommendation python based machine learning library empirical software engineering acm reference format moshi wei yuchao huang junjie wang jiho shin nima shiri harzevili and song wang.
.
api recommendation for machine learning libraries how far are we?.
in proceedings of the 30th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november singapore singapore.
acm new york ny usa pages.
introduction application programming interfaces apis are built in functions in software libraries that help developers build software more effectively.
as machine learning recently made great progress in both theory and application there is increasing interest in developing machine learning applications.
while there are many publicly available open source machine learning libraries and apis searching the right apis for specific tasks is not easy especially for the plenty of green hands in the field of machine learning .
although many api recommendation approaches that can help retrieve apis with high accuracy have been proposed and extensively studied most existing approaches have been evaluated mainly on general programming tasks using programming languages such as java.
little is known about their practical effectiveness and usefulness for ml programming tasks with dynamically typed programming languages such as python.
apart from the different nature of programming languages machine learning application development has different paradigms compare to traditional application development relatively more deterministic and less statistically orientated which is statistically orientated and requires many algorithms mathematical operations and data operations .
in this paper we investigate the effectiveness of existing api recommendation approaches for python based ml programming tasks which is of great value considering the increasing popularity of machine learning practices and the overwhelming number of public asked machine learning questions inesec fse november singapore singapore moshi wei yuchao huang junjie wang jiho shin nima shiri harzevili and song wang question answering websites such as stack overflow so .
for example there are more than 11k questions on stack overflow about api recommendation for tensorflow an open source library for machine learning with about new questions emerging each week1.
our study focuses on the following research questions rq1 what is the performance of existing api recommendation approaches on python based ml programming tasks?
to answer this question we present an empirical study to explore the performance of existing state of the art api recommendation approaches i.e.
biker and deepapi on python based ml programming tasks.
specifically first of all we collect 80k python based ml programming questions related to six popular machine learning libraries from so.
then we retrain biker and deepapi with our python based ml question dataset for fair evaluation and we evaluate their performance using 1k randomly selected ml questions excluded from the training data .
our experiment results show that there exists significant performance decline of these approaches on python based ml questions.
we then perform an in depth analysis to explore the reasons.
our analysis reveals that there exist two major reasons.
first most traditional java programming tasks require only one api to solve while ml questions often require many more apis because ml development tasks often require customization of data processing feature engineering model architecture optimization function and hyperparameters etc.
specifically the average length of api sequence in the answers to java programming tasks is .
while the average length is .
for ml tasks.
second most existing approaches only focus on increasing the hit rate of the first correct api recommended while ignoring the completeness of the answers recommended.
moreover the multiple apis of pythonbased ml tasks pose greater challenges to the recommendation tasks for python based ml tasks.
note that we also observe several other reasons for the performance decline which are closely related to the nature of machine learning details are in section .
.
in addition we have also observed that there exist common library specific api usages that can be useful for further improving the api recommendation for python based ml programming tasks.
for example when constructing a machine learning model in keras keras.model.sequential has to be used as a container for model layers such as keras.layers.conv2d keras.layers.dense .
current api recommendation approaches cannot capture the above information about api usage as they do not consider the relationship between api calls when recommending apis for a programming task.
rq2 can we improve the performance of the existing api recommendation approaches on python based ml tasks?
based on our findings from rq1 we propose a simple but effective booster i.e.
fimax which can significantly improve the performance of existing api recommendation approaches for pythonbased ml programming tasks by leveraging api usage patterns of ml libraries.
specifically we propose to use the frequent itemset mining technique to identify api usage patterns from the api call sequences in the answers of ml programming tasks posted in so.
then extend the recommendation results of the existing approaches with the api usage patterns.
our evaluation shows that the proposed approach improves the existing state of the art api recommendation approaches by up to .
and .
in mrr and map respectively.
we have also conducted a user study in which developers are divided into two groups using different tools to answer ml questions randomly sampled from the testing dataset.
on average the group using the fimax can improve answer correctness by and save answering time by .
this paper makes the following contributions we perform the first empirical analysis of existing api recommendation approaches on python based ml programming tasks reveal their performance degradation on python based ml programming tasks and summarize the reasons for the degradation.
we propose a simple but effective approach i.e.
fimax to augment the api sequences retrieved by existing approaches to boost their performance of api recommendations.
both our quantitative evaluation and user study show that fimax can help developers find the correct apis for pythonbased ml programming tasks more efficiently and accurately compared with state of the art baselines.
we release the source code of our tool and the dataset of our experiments to help other researchers replicate and extend our study2.
this paper is organized as follows.
section describes the background of the api recommendation research field.
section presents the setup of our empirical study.
section shows the details of our experiment and the results.
section presents the performance of our proposed approach.
section shows our case study.
section discusses open questions about this study and threats to the validity of our work.
section presents the related work.
section is the conclusion of the paper.
background on api recommendation there are many existing approaches for api recommendation .
these approaches can be divided into two orthogonal categories i.e.
information retrieval based approach and deep learning based approach.
in this paper two modern approaches one for each category are studied.
.
information retrieval based api recommendation information retrieval ir based api recommendation approaches as the name suggests leverage information retrieval techniques for recommending apis for a given programming task described in natural language.
these approaches first apply the source code parsing algorithms and heuristics to extract api call sequences from the answers of related so questions and tokenize them into bag of words.
next these approaches create indexes for features extracted from the collected so questions.
given a query recommendation for machine learning libraries how far are we?
esec fse november singapore singapore the approaches preprocess the query and then compute the similarities between the query and the collected questions using the built indices.
the most similar matches are returned as output.
the state of the art information retrieval based approach for api recommendations is biker .
it uses both stack overflow questions and api documentation to recommend apis for java programming tasks.
biker considers the api recommendation problem as a two step task.
given an input query biker first retrieves the kmost similar questions by using the text similarity between the query and all questions in its knowledge base.
then it creates a list of api candidates from the top kquestions and re ranks the apis according to the text similarity between the query and the documentation of an api.
in this paper we select biker as our baseline to represent the state of the art ir based api recommendation technique.
.
deep learning based api recommendation most deep learning based api recommendation approaches treat the api recommendation as a translation task and apply an endto end architecture where the deep learning model takes a query sentence as input and returns a list of recommended apis as output.
most deep learning based api recommendation approaches train the models on the training data i.e.
a large corpus of question and answer pairs and use the trained models for inference.
in contrast to the token similarity ranking strategy of ir based api recommendation approaches deep learning based api recommendation approaches focus on learning the semantic connection between queries and the api answers.
the deep learning model learns the semantics of a query from the training data and recommends apis based on the deep semantic connection knowledge it learns between queries and the api answers.
the state of the art deep learning based api recommendation approach is deepapi .
the core model of deepapi is a recurrent neural network rnn based encoder decoder model which is originally used for neural machine translation nmt .
deepapi was trained on a large corpus of question api pairs extracted from github repositories and treats the api recommendation problem as a sequence generation task where the input is a programming query and the output is an api sequence.
in this paper we use deepapi as our baseline to represent the state of the art deep learning based api recommendation.
empirical study setup this section describes the research questions of our work data collection approach and analysis methodology.
.
research questions our work is organized by the following two research questions rq1 what is the performance of existing api recommendation approaches on python based ml programming tasks?
in this rq we set out to investigate whether the state of the art existing api recommendation approaches i.e.
biker and deepapi perform well on python based ml questions.rq2 can we improve the performance of the existing api recommendation approaches on python based ml programming tasks?
this question aims to explore possible solutions that can improve existing api recommendation approaches based on our findings from rq1.
.
subjects of study to investigate the performance of existing api recommendation approaches on python based ml questions we select six popular python ml libraries according to the number of downloads in the python package index pypi which is the python library management program.
we manually identify the machine learning related packages from the top pypi package list3 which is ranked by number of downloads.
according to pypi statistics as of dec. the six libraries that rank by monthly downloads are numpy pandas scikit learn pyspark tensorflow and keras.
numpy is a fundamental library used in python ml development mainly for numeric array operations.
pandas is a library for data analysis of tabular data such as data tables.
it provides tabular display of parallel data and also apis covering basic table operations such as filtering sorting and indexing.
scikit learn is an important library in statistical machine learning library that provides machine learning and scientific algorithms such as matrix operations algebraic equations and differential equations .
tensorflow and keras are two popular deep learning libraries.
tensorflow developed by google is one of the most commonly used machine learning libraries in the industry as it supports many industryfriendly features such as distributed training and a visual debugging environment .
keras offers a user friendly stack style api for building and training deep learning models.
it encapsulates the detailed execution process in high level apis.
pyspark is a python adapter of apache spark.
it allows developers to use spark application features such as spark sql distributed data frame etc.
in python.
the libraries studied cover most of the current industrial practice of machine learning and also represent the key aspects of machine learning developments.
in addition they cover the area of data preprocessing and data object processing numpy and pandas scientific computing scipy and scikit learn distributed machine learning pyspark and deep learning tensorflow and keras .
although numpy and pandas are not libraries for building neural network structures they are integral to machine learning development because they are widely used as the basis for data object computation in machine learning model development.
numpy apis are typically utilised in the middle of the tensorflow modelling api sequences.
removing the numpy apis would leave the api sequences incomplete.
thus in this work we also experiment with numpy and pandas.
table shows the details of the six libraries studied.
the number of apis is calculated by counting the number of documented function nodes in the latest version of the packages using a python ast parser.
after removing duplicates for each package we get the number of unique apis in our experiment dataset.
the number november singapore singapore moshi wei yuchao huang junjie wang jiho shin nima shiri harzevili and song wang table the details of studied machine learning libraries in this work library description apis unique apis so questions in documents in dataset numpy a library for multi dimensional arrays and matrices operations.
pandas a library for data manipulation and analysis.
scikit learn a library for machine learning algorithms.
keras an interface for artificial neural networks.
tensorflow a library for deep learning developed by google.
pyspark an interface for apache spark in python.
table performance of biker and deepapi on pythonbased ml questions dataset approachevaluation metrics mrr map python mlbiker .
.
deepapi .
.
java jdkbiker .
.
deepapi .
.
of so posters for each package is collected from our python ml dataset as shown in section .
.
.
so post collection to create our python based ml question dataset we follow existing work to extract programming questions and their accepted answers from stack overflow using the data explorer provided by stack exchange .
we select questions with the six library names as stack overflow question tags to retrieve questions and only questions with accepted answers remain.
apis are extracted from the accepted answer using heuristics.
specifically we developed a heuristic parser to extract the method names from the code snippet and the package and class names are inferred based on the package import information.
if a question contains multiple apis we concatenate all apis into a list of apis to answer the question.
we only select answers that contain at least one api and all apis are in the same library to reduce noise in the training dataset.
as a result we collected a total of question api pairs for these libraries.
.
evaluation metrics we evaluate api recommendation approaches using mean reciprocal rank mrr and mean average rank map which are two commonly used evaluation metrics in information retrieval and recommendation system evaluation .
both mrr and map range from to .
mrr describes the rank of the first correctly recommended api in the recommendation list.
it is calculated by the inverse rank of the first match .
a high mrr indicates that the rank of the first correct match is high.
map checks the ranks of all correct matches instead of focusing only on the first correct answer.
a high map indicates that there are multiple correct matches in the recommendation and thus the recommendation is complete.
rq1 performance of existing approaches on python based ml questions to investigate rq1 we train and evaluate biker and deepapi on our python based ml questions collected from stack overflow see section .
for details .
in addition an in depth analysis was conducted to investigate possible reasons for the difference in performance of the two approaches on traditional programming tasks and python based ml programming tasks.
.
experimental setup training since biker and deepapi were designed for java jdk api recommendation to make it applicable for python based ml questions we replace the java question data with the python based ml question data for retraining the models.
note that biker uses both question titles and api documents for recommendation we also replace the java documentation with the python documentation.
to collect python documents we use the python ast parser to collect the docstrings of each python library used in this work.
for each library we run the ast parser from the root and collect all docstrings in the fuctiondef node and the classdef node.
to evaluate deepapi on python based ml questions we train and tune the deepapi model from scratch on our python based ml question dataset with the same configuration as reported in its original paper on a single nvidia v100 gpu with 32gb memory.
testing to evaluate the performance of the examined two api recommendation models we randomly select 1k samples for each library for building the test datasets.
according to previous studies on stack overflow stack overflow contains not only api recommendation questions but also other types of questions such as comparing different implementations asking for an explanation and asking for program debugging etc.
therefore to reduce noises in the test data we manually examine each question to remove the questions that are not suitable for api recommendations.
the questions removed in our manual analysis either do not ask for api recommendations or provide information that is too general to derive an api recommendation from.
for example questions that ask for explanations such as why am i not seeing numpy s deprecation warning?
questions that ask for debugging help such as mnist valueerror when checking target in keras api recommendation for machine learning libraries how far are we?
esec fse november singapore singapore questions that are too general to recommend apis such as constraints not working in optimization using scipy .
the authors of this work independently go through each question and identify the api recommendation related questions.
the agreement among the researchers measured by cohen s kappa coefficient is .
which is a relatively high level of agreement.
we remove the irrelevant questions and select the first samples for each library from the remaining samples as the final test set.
note that we have removed all test data from the training data for the fair evaluation.
.
performance difference table shows the mrr and map of biker and deepapi on the python ml question dataset.
in addition we also showed the performance of biker and deepapi on the java jdk question dataset used in their original papers .
in general the performance of both approaches significantly p value .
decrease when applied to the python ml dataset compared to their performance on the java jdk dataset.
for biker the mrr declines from .
on the java jdk dataset to .
on the python ml dataset declines .
and the map declines from .
to .
declines .
.
for deepapi we can observe a similar trend for both mrr and map i.e.
mrr declines by .
and map declines by .
.
we also note that for the java jdk dataset the mrr and map of both approaches are at a similar level while map for the python ml dataset is about half the mrr for both approaches.
since map measures the ranks of all correct matches instead of focusing only on the first correct answer like mrr the above results show that the existing api recommendation approaches have challenges on the api recommendation completeness i.e.
they can hardly capture the entire set of correct answers.
.
analysis of performance decline motivated by the significant performance difference showed in section .
we further investigated the two datasets i.e.
java jdk question dataset and python based ml question dataset and the recommended apis of both biker and deepapi.
we found two main reasons contributing to the performance decline of biker and deepapi on the python based ml question dataset i.e.
compared to the java question data answers of the python ml questions often require more apis see section .
.
for details and the existing api recommendation approaches mainly focus on increasing the hit rate of the first correct api recommended while ignoring the completeness of the answers recommended and do not consider the common api usages see section .
.
for details .
note that in addition to these two major reasons we also observed some other minor reasons that could decline the performance of the existing api recommendation approaches.
the details can be found in section .
.
.
.
python ml questions requires more apis .we find that the average length of api call sequence in the answers of the pythonbased ml questions is longer than that of the answers for java sdk questions which makes the recommendation for python based ml question more challenging.
specifically the average length of thetable an example of python based ml question bold apis are apis that are matched by biker underlined apis are apis that are missed by biker .
question convert vgg16 shape output from features to accepted code snippet to answer this question vgg16 model keras.applications.vgg16.
vgg16 model sequential for layer in vgg16 model.layers model.
add layer model.layers.pop freeze the layers for layer in model.layers layer.trainable false add softmax instead of earlier prediction layer.
model.
add dense activation softmax check the summary and yes new layer has been added.
model.summary ground truth apis keras.vgg16 keras.sequential keras.add keras.pop keras.dense keras.summary recommended apis by biker keras.reshape keras.concatenate keras.shape keras.add keras.input keras.ones keras.pad sequences keras.vgg16 keras.transpose keras.arange api call sequence in the ground truth answers of the java jdk questions is .
while the average length of the api call sequence in the answers for python based ml questions is .
in other words one api can solve a java jdk question while five apis are required to solve a python based ml question on average.
the median api call sequence length also supports this observation i.e.
one in java versus five in python ml.
due to the fact that most answers to java jdk questions only contain a single api existing api recommendation approaches such as biker and deepapi mainly focus on increasing the probability of the first correct answer i.e.
mrr while ignoring the completeness of the recommended apis i.e.
map .
specifically given a question biker first obtains candidate apis from a list of similar questions and then re rank the collected apis according to the similarity between the question title and the documentation of each api to ensure the correctness of the first answer without considering other apis that have low similarity scores to the question title if even they are essential for answering the question.
this hurts the completeness of the recommendation.
for deepapi it uses an lstm encoder decoder sequential model designed for the sequential task.
this mitigates the problem biker faces yet still suffers from low performance.
considering that python based ml questions usually contain or more apis the api recommendation in this scenario should consider both mrr and map i.e.
the first correct answer and all correct answers are ranked higher.
.
.
existing approaches did not consider common apis usages .table shows an python based ml question its answer and the recommended apis by using biker.
the question is askingesec fse november singapore singapore moshi wei yuchao huang junjie wang jiho shin nima shiri harzevili and song wang table occurrence of api usage patterns base add co occurrence confidence keras.add keras.dense .
keras.add keras.sequential .
for the conversion of output shape to output shape.
in the table we also show the accepted code snippet to answer this question from which we can see that the answer of api sequence includes six apis.
in the code snippet we bold the apis that are found by biker and underline the apis that are not found by biker.
specifically it first loads the vgg16 model except for the last layer and freezes the model layers.
then it adds a new dense layer to the model with the required feature size.
biker can correctly suggest keras.add and keras.vgg16 but miss the prerequisite api keras.sequential .
in keras keras.sequential acts as a container for all model layers which means that the layer operation apis must be called after a model container is initialized with keras.sequential .
biker fails to recognize this api usage pattern.
we also observe similar cases in deepapi s recommendation results.
the above example shows that existing api recommendation approaches often neglect the relationships between the apis involved when recommending apis for a given question.
motivated by this we further investigate the co occurrence of the involved apis in our experimental dataset.
the results are in table from which we can see that the api keras.add occurs times and keras.sequential co occurs times together with keras.add out of the times i.e.
.
this reveals the frequent usage patterns of apis in solving a particular task i.e.
two or more apis co occur frequently.
we have also observed there exist numerous pairs of apis that are highly likely to co occur which can be considered as api usage patterns.
the above analysis motivates us to extend the api recommendation result by existing tools e.g.
biker and deepapi by including api co occurrence relationships to improve the performance of api recommendation.
specifically when an api is recommended our approach can identify potential api usage patterns related to it and further append the involved apis from the api usage patterns into the recommended api list.
we believe that such an extension could improve current api recommendation approaches by increase the completeness of the apis recommended.
rq2 solution to enhance existing api recommendation approaches rq1 has demonstrated the significant performance degradation of existing api recommendation approaches on python based ml questions.
inspired by the findings of rq1 we propose a frequent itemset mining based approach named fimax that aims to improve the performance of existing api recommendation approaches for python based ml questions by extending existing api recommendation results with api usage information i.e.
co occurrence relationships of apis.
api list fimax api listextended api list association rules query association rule mining api sequence extension dataset recommendation modelfigure overview of fimax .
frequent itemset mining for api extension figure shows the workflow of fimax .
first fimax generates the co occurrence relation based api usage patterns of apis with association rule mining technique on api call sequences from the answers of ml programming tasks.
then fimax further extends the recommendation from an api recommendation model with the mined the api usage patterns.
.
.
association rule mining.
fimax first applies the apriori algorithm to create a set of api association rules i.e.
api usage patterns from the python based ml question dataset.
specifically we collect the api sequence from the answer of each so post in the python based ml question dataset which serves as input to the apriori algorithm for rule mining.
the apriori algorithm generates a pattern hypothesis by randomly selecting two or more apis as the base itemset aand then randomly selecting one or more apis as the extension itemset b. the algorithm calculates the confidence and support of the pattern hypothesis a b and compares it to the confidence and support threshold specified by the user.
the apriori algorithm accepts a association rule if the confidence andsupport of the rule are higher than the threshold values specified .
specifically support indicates the frequency of an api association rule with respect to the entire dataset.
confidence indicates the percentage of one or more extended api s e.g.
item set b found to be true given a base rule e.g.
item set a .
leta bbe the antecedent and the consequent mined from a list of apit the support ofa bovertis support a b freq.
a b t and the confidence ofa bis confidence a b freq.
a b freq.
a as an example the apriori algorithm proposes an api usage pattern tensorflow.sessi on tensorflow.run .
then the algorithmapi recommendation for machine learning libraries how far are we?
esec fse november singapore singapore table an example of api extension of fimax apis recommended numpy.range numpy.reshape numpy.arange rule a numpy.range numpy.empty support .
rule b numpy.range numpy.append support .
rule c numpy.reshape numpy.arange numpy.array support .
extended api list numpy.range numpy.reshape numpy.append numpy.array numpy.arange calculates the confidence and support for this pattern.
the confidence can be interpreted as the percentage of occurrences of tensorflow.run given tensorflow.session and the support of tensorflow.session tensorflow.run is the percentage of the cooccurrences of tensorflow.session and tensorflow.run over the size of the entire corpus.
if both confidence andsupport are greater than the specified thresholds the proposed rule tensorflow.session tensorflow.run will be added to the table of item set patterns along with the confidence andsupport values.
.
.
api sequence extension.
given a list of recommended apisr generated by an api recommendation approach e.g.
biker fimax searches for the extension rules for each api in the list.
if there are rules that match a particular api fimax appends the associated apis to raccording to the association rules created in section .
.
for that api.
if there are multiple rules that are eligible for extension only the rule with the highest support score will be utilized.
the final api recommendation list consists of the top k original apis concatenated with the extended apis.
the extended apis are ordered by confidence score.
table shows an example of the extension.
following the extension algorithm fimax uses rule b and rule c for extension.
in case there are duplicate apis after extension.
for a repeated recommendation we keep the first occurrence and remove the duplicates.
.
experiment setup as we discussed in section .
fimax applies apriori algorithm for association rule mining which has two parameters i.e.
support andconfidence that can significantly affect its output.
to find the best values for these two parameters we tune them together and experiment with support threshold values from .
to .
with a step of .
and confidence thresholds with values from .
to .
with a step of .
.
please note that both biker and deepapi can recommend many apis for a given question e.g.
biker recommends up to apis as a result there will be a large number of candidate rules if all the recommended apis are considered while only a few api candidates are likely to be correct in practice.
extending the wrong api would degrade the performance of a recommendationapproach.
therefore we limit the number of apis to be extended to less than a threshold i.e.
k. for our tuning we perform a grid search with all the above combinations of support threshold confidence threshold and top kand calculate the mrr value of each combination for performance comparison.
the result shows that the biker fimax model performs best when the confidence threshold is equal to .
the support threshold is equal to .
and top k is equal to while the deepapi fimax model performs best when the confidence threshold is equal to .
the support threshold is equal to .
and top k is equal to .
the reason for the different best parameter settings for biker and deepapi can be these two approaches adopt different mechanisms in api recommendation and generate different recommended apis.
it also shows the necessity of parameter tuning when applying fimax to a new api recommendation approach.
we use a nvidia v100 gpu to train models and recommend apis for deepapi and we use intel i7 cpu to recommend apis for biker and fimax.
.
performance analysis of fimax we evaluate the effectiveness of fimax on boosting two typical api recommendation approaches i.e.
biker and deepapi on pythonbased ml questions by comparing the performance of original biker deepapi to the performance of these two approaches armed with fimax.
table shows the evaluation results for biker deepapi biker fimax and deepapi fimax on questions from each python ml library.
from the table we can see that the fimax significantly increased the performance of both models.
overall the mrr and map of biker increased by .
and .
respectively after applying the fimax.
the mrr and map of deepapi increase by .
and .
respectively after applying fimax.
moreover the performance increase of map is larger than mrr for both models suggesting that fimax has a positive effect on the completeness of api recommendations.
our wilcoxon signed rank test p .
also suggests that biker and deepapi can achieve significantly better performance after applying fimax.
in addition we can also see that biker fimax has a noticeable improvement over the performance of biker for each library.
the map of biker in the pyspark package shows the most significant improvement i.e.
a .
increase after applying fimax.
the main reason for this improvement might be that pyspark is an analytics engine for big data and most of its api sequence follows the map reduce related patterns that can be learned by fimax.
for deepapi we see that the mrr for the numpy library increased by .
and the map increased by .
after the fimaxwas applied which is the most significant improvement of deepapi after applying fimax.
one of the possible reasons for this improvement is that numpy is designed for manipulating arrays and matrices exclusively.
thus its api usage patterns can be much more targeted than other libraries which makes them easy for fimax to learn.
note that although fimax can significantly improve biker and deepapi on most python ml libraries regarding both mrr and map we observe a performance decline on the scikit learn library of deepapi after applying fimax.
specifically the map ofesec fse november singapore singapore moshi wei yuchao huang junjie wang jiho shin nima shiri harzevili and song wang table performance of fimax on biker and deepapi.
librarybiker biker fimax improvement deepapi deepapi fimax improvement mrr map mrr map mrr map mrr map mrr map mrr map numpy .
.
.
.
.
.
.
.
.
.
.
.
pandas .
.
.
.
.
.
.
.
.
.
.
.
scikit learn .
.
.
.
.
.
.
.
.
.
.
.
keras .
.
.
.
.
.
.
.
.
.
.
.
tensorflow .
.
.
.
.
.
.
.
.
.
.
.
pyspark .
.
.
.
.
.
.
.
.
.
.
.
overall .
.
.
.
.
.
.
.
.
.
.
.
table time cost of biker deepapi and fimax.
approach device training recommendation biker intel i7 cpu n a .4s query deepapi nvidia v100 gpu hrs .57s query fimax intel i7 cpu mins .00576s query deepapi fimax declines .
compared to the performance of deepapi.
one of the possible reasons for this is that the scikit learn library contains many statistical algorithms that are rarely used and queried on so making it difficult for fimax to learn the correct api usage patterns while returning many false positives which further causes a performance decline.
table shows the time cost of fimax and the baselines.
in the training phase biker requires no training and deepapi requires hours to train the model.
in the recommendation phase biker requires seconds while deepapi requires .
seconds for each query.
the time cost of fimax is only minutes in training and .
seconds per query in the recommendation phase.
compared to the time cost of biker the time cost of fimax is negligible in both the training and recommendation phases.
user study in this section we conduct a user study to further investigate whether fimax can help developers find correct apis more efficiently and accurately.
.
study design to conduct our user study we randomly selected questions from our test dataset.
for each selected question we created an api retrieval task using the question title as the task description.
we invited four phd students and ten ms students familiar with machine learning development to complete the tasks.
the years of their experience in developing machine learning software based on python varied from two to six years with an average of years.
we then divided the participants into two groups g1 and g2 with experience evenly distributed in both groups.
the tasks were also randomly divided into two groups t1 and t2 .
the experiment was conducted in two phases.
in the first phase participants in g1 and g2 were asked to complete the tasks in t1 using fimax biker and biker respectively.
in the second phase the two groups exchanged tools to complete the tasks in t2.
each participant had to record his her screen during the experiment so that we could record howbiker fimax biker00.
.
.
.
a correctnessbiker fimax biker246810 b completion time mins figure results of user study.
much time he she spent on a question.
note that deepapi is not evaluated since it performs relatively poor than biker.
.
results analysis following existing studies we use two metrics to measure the performance of the participants on api retrieval task i.e.
correctness andcompletion time .
specifically correctness evaluates whether a participant can find the correct apis for a given question and we measure the proportion of correct apis submitted by a participant among all apis in the ground truth answer of the question.
completion time evaluates how quickly a participant can answer a given question.
for each question we recorded the correctness and completion time of each participant as well as the average value of the two groups of participants.
figure shows the performance of the groups with biker and fimax biker over the tasks.
using fimax biker participants completed the tasks more accurately and took less time than participants using the original biker.
on average the correctness and completion time in minutes of participants using fimax biker and biker were .
and .
versus .
and .
respectively.
we further used wilcoxon signed rank test for verifying the statistical significance of the differences.
the p values for both correctness and completion time are small than .
which indicates that the differences of fimax biker and biker in correctness and completion time are statistically significant.
discussion .
more reasons for performance decline in section .
we present two major reasons that contribute to the performance decline of existing api recommendation approaches.api recommendation for machine learning libraries how far are we?
esec fse november singapore singapore figure performance of fimax under different top k setting regarding mrr in addition there are several other reasons for the performance decline that we found in our investigation of the recommendation results which are related to the nature of machine learning.
we present these reasons as follows to motivate the future directions to improve the api recommendation on python based ml questions.
library bias we note that some users specify the library name in their questions and others do not.
when a user does not specify the library name the api recommendation approach suffers from the popularity bias i.e.
the apis of more popular libraries in the training dataset are more likely to be recommended.
for example the api used for a fully connected layer in tensorflow is tf.contrib.layers.fully connected while it is keras.layers.
dense in keras.
when a user asks questions about implementing a fully connected dense layer without mentioning the library name the model tends to recommends keras.layers.dense rather than tf.contrib.layers.
fully connected .
this is because there are many more questions about keras.layers.dense than that of tf.contrib.layers.fully connected on stack overflow and rarely used apis are less likely to be recommended.
such bias can be mitigated by specifying the library name in the question or balancing the training examples.
this also motivates the practical need for fairness study in api recommendations .
multiple solutions there are questions that have multiple solutions.
for example a question asking how to iterate through a pandas column can be answered with solutions such as df.iterrows or df.iteritems .
although both are correct recommendations following the existing evaluation method only the recommendation that matches the answer in the collected dataset is considered correct.
such problems can be mitigated by expanding the answers to their synonyms or collecting all the accepted answers.
moreover this finding also indicates how inflexible the existing evaluation method for api recommendations is and motivates the need for new evaluation criteria.
documentation issue another possible reason is that the format and organization of the documentation of python based ml libraries are different from those of java jdk which mainly affects the performance of biker.
biker employs the documentation for similarity based api re ranking under the assumption that the documentation can provide description of the corresponding api.
however the api documentation of the python ml libraries contains a detailed description of the input output caveats and examples.
figure performance of fimax under different top k setting regarding mar this severely disturbs the recommendation model when calculating the similarity between the documentation and the question resulting in performance degradation.
this motivates the need for documentation understanding and key information extraction to automatically reorganize various sections of python based ml api documentation before applying biker.
.
what is the performance of fimax against different top k settings?
both biker and deepapi can recommend top kapis for a given question.
following their suggestion in this work we evaluate the top recommended apis in the api sequences recommended by the baseline methods i.e.
biker and deepapi and also in the api sequences extended by both biker fimax and deepapi fimax.
however since the number of apis required to solve a python based ml question is much larger than that of a java jdk question details are in section .
we further evaluate the performance of fimax under more recommended apis i.e.
from to with as the interval.
figure shows the mrr of biker deepapi biker fimax and deepapi fimax under different top k configurations.
we can see that the mrr values of biker and deepapi are below .
and .
from the top to the top recommendation results respectively.
also the performance of biker and deepapi remains almost the same from the top to the top recommendations which means that biker and deepapi cannot hit more correct apis even if we include more recommendations.
after fimax is applied to both biker and deepapi the performance of both approaches increases dramatically and remains stable across different top ksettings.
from figure we can see that the performance of biker fimax is higher than biker at each top k setting and increases slightly until the top recommendation meaning that biker fimax is able to recommend the correct first match within recommendation results.
figure shows map values of biker deepapi biker fimax and deepapi fimax under top k configurations from to .
similar to the mrr result the map values of biker and deepapi remain almost the same from the top to the top .
while the map values of biker fimax and deepapi fimax increase from top to top which shows that the fimax extension approach is able to increaseesec fse november singapore singapore moshi wei yuchao huang junjie wang jiho shin nima shiri harzevili and song wang table performance of fimax on different java so questions regarding the length of api sequence in the answers.
java questionsimprovement mrr map count single answer .
.
medium answer .
.
long answer .
.
overall .
.
the completeness of the original recommendation result of biker and deepapi in top ksettings from to .
also biker fimax and deepapi fimax are able to recommend correct apis not only for the first but also for the first recommendation results.
.
generalizability of fimax in this paper fimax has been shown to be effective in improving the performance of api recommendation on python based ml programming tasks whose answer contains a long api call sequence while its performance on traditional java so questions that require few apis is unknown details are in section .
.
.
in this section we further examine the generalizability of fimax by applying it to java jdk questions.
specifically for this experiment we reuse the java jdk dataset of biker .
fimax mines the api usage rules on the training dataset and tunes its parameters i.e.
support and confidence by following the same process used in section .
.
since most java questions require few apis we further divide the test dataset into three categories based on the number of apis required i.e.
single requires one api medium requires two to five apis and long requires more than five apis .
for the single category we randomly select questions from the original test dataset of biker.
meanwhile we find that most questions in biker s test dataset are single answer questions thus for medium and long categories we randomly select samples from the biker s training data and exclude them when training biker.
table shows the improvement of fimax biker regarding mrr and map for different types of java questions compared to the performance of biker.
from the result we can see that map increases by about for the three categories and mrr improves by .
for single answer.
the increase in mrr results from cases where none of the original api recommendations are correct but the extended api hits the correct answer.
the experiment result on java jdk questions is consistent with that of python based ml questions i.e.
the fimax has a positive effect on the completeness of api recommendations.
our wilcoxon signed rank test result p .
further confirms that fimax biker could achieve significantly better performance than biker.
.
threats to validity internal threat since biker and deepapi are designed for java questions.
we need to make necessary adaptation on them for python based ml questions.
we reused the published source code of biker and deepapi for the baseline method with our dataset.
we have carefully reviewed the implementation of fimax and the process of applying fimax to biker and deepapi to ensure thatfimax works as intended.
therefore the threat to internal validity is low.
construct threat we follow the existing work and adopt the same metrics i.e.
mrr and map for performance evaluation.
mrr and map are computed by reusing the algorithm in the source code of biker.
however the performance of fimax could be different if other metrics are used.
in the future we plan to examine fimax with other metrics e.g.
precision and recall which are widely used in information retrieval .
external threat we collect the python based ml questions dataset from the official data explorer provided by stack exchange .
although we follow the same criteria as biker to filter the noise data our dataset could still contain noise.
to reduce this threat we manually checked our test dataset as described in section .
the agreement among the authors measured with the cohen s kappa coefficient is .
indicating high agreement.
related work api recommendation there are many other approaches for api recommendation other than biker and deepapi .
rahman et al.
proposed rack for class level java api recommendation using a customized co occurrence based data mining algorithm on top of the lucien engine.
biker reported better performance compared to rack at the class level.
raghothaman et al.
proposed swim for synthesizing code snippets using api usage pattern mining techniques on github code repositories.
it trains a query to api model with the api call sequences from github and then synthesizes code snippets for a given query using the model.
the difference between fimax and rack and swim is that fimax applies the apriori algorithm for mining api usage patterns while rack uses a customized technique and swim uses a mixture of several algorithms for code synthesis.
mcmillan et al.
proposed portfolio for recommending related functions in c using data mining techniques in code archives.
it applies the pagerank algorithm as well as the function call graph for association model building and a customized similarity metric for relevant function ranking.
chan et al.
proposed a graph search approach that improves the performance of portfolio .
they first build an api call graph using the text phrase in apirelated questions and then apply a customized shortest path indexing scheme for result ranking.
their approach significantly improves the performance of portfolio .
most existing studies on api recommendations have focused on java programming questions.
in this work we investigate two stateof the art approaches i.e.
bike and deepapi for python based ml programming tasks.
he et al.
proposed pyart for real time python api recommendation.
pyart is able to recommend not only apis from third party libraries but also project specific apis.
we did not compare to pyart because we believe that such a comparison can be unfair i.e.
pyart and our approach use different information for api recommendation in different scenarios.
specifically pyart focuses on real time code completion and makes api recommendations based on the context of the programming tasks.
our approach is applied to a programming qa system by providing a search service that accepts a programming question and returns a list of apis.api recommendation for machine learning libraries how far are we?
esec fse november singapore singapore mining api usages there are many studies on mining api usage .
treude et al.
proposed sise an api documentation augmentation technique that provides usage insights to developers.
it applies a pattern based approach with consideration of part of speech tags for feature extraction and a supervised machine learning approach for extracting insights from so posts.
in a comparative study with eight software developers sise was found to contribute the most useful information to api documentation.
moreno et al.
have proposed muse for mining code examples.
given a particular method it returns a list of related code examples generated by a combination of code clone detection and static slicing and ranks the result based on a set of usage based heuristics rules.
zhong et al.
proposed mapo for recommending related code snippet using frequent api usage patterns.
mapo builds a recommendation model using a frequent sub sequence mining algorithm with source code extracted from google code archives.
for a given api method it recommends the associated api calls based on the usage patterns captured by the model.
petrosyan et al.
proposed an approach to discover tutorial sections to explain a particular api type.
they create a supervised text classification model for classifying tutorial fragments based on linguistic and structural features.
nguyen et al.
proposed api2vec a model that learns the semantic relationship between apis using word embedding techniques.
they build an embedding model using the cbow word2vec model with the extracted api sequence from java and c code snippets.
they demonstrate the usefulness of api2vec with example applications including an example of a newly discovered api mapping between java and c code.
they go one step further and develop an automated api mapping discovery tool called api2api based on api2vec for api migration tasks between java and c .
jiang et al.
have proposed an unsupervised approach called frapt for finding relevant tutorial fragments for a given api.
the difference between frapt and our approach is that frapt uses pagerank and a topic model based algorithm while fimax uses the apriori algorithm for association rule mining.
conclusion in this paper we investigate the effectiveness of state of the art api recommendation approaches i.e.
biker and deepapi on python based ml programming tasks.
specifically we conducted an empirical study of programming questions related to six widely used python based ml libraries.
we find two main reasons that contribute to the performance decline of existing approaches python based ml tasks often require significant long api sequences and there exist common api usage patterns in python based ml programming tasks that existing approaches cannot handle.
inspired by our findings we proposed fimax which enhances existing api recommendation approaches by using api usage information mined from so questions.
our evaluation shows that fimax can significantly boost existing state of the art api recommendation approaches.
our user study further demonstrates the practical value of fimax for api recommendations.
in the future we plan to investigate the effectiveness of fimax on other api recommendation approaches and programming tasks in other domains and languages.