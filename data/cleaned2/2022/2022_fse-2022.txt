pareco patched clones and missed patches among the divergent variants of a software family poedjadevie kadjel ramkisoen1 john businge1 brent van bradel1 alexandre decan2 serge demeyer1 coen de roover3 and foutse khomh4 poedjadevie.ramkisoen john.businge brent.vanbladel serge.demeyer uantwerpen.be alexandre.decan umons.ac.be coen.de.roover vub.be foutse.khomh polymtl.ca 1universiteit antwerpen flanders make 2f.r.s.
fnrs university of mons 3vrije universiteit brussel belgium 4polytechnique montreal canada 5university of nevada las vegas u.s.a. abstract re using whole repositories as a starting point for new projects is often done by maintaining a variant fork parallel to the original.
however the common artifacts between both are not always kept up to date.
as a result patches are not optimally integrated across the two repositories which may lead to sub optimal maintenance between the variant and the original project.
a bug existing in both repositories can be patched in one but not the other we see this as a missed opportunity or it can be manually patched in both probably by different developers we see this as effort duplication .
in this paper we present a tool named pareco which relies on clone detection to mine cases of missed opportunity and effort duplication from a pool of patches.
we analyzed source target variant pairs with patches resulting in a curated dataset containing cases of effort duplication and cases of missed opportunities.
we achieve a precision of recall of accuracy of and f1 score of .
furthermore we investigated the time interval between patches and found out that on average missed patches in the target variants have been introduced in the source variants weeks earlier.
consequently pareco can be used to manage variability in time by automatically identifying interesting patches in later project releases to be backported to supported earlier releases.
keywords github clone own variants software family forking social coding bug fixes effort duplication clone detection introduction code reuse is the practice of using existing code to speed up the development process.
traditional code reuse is performed by declaring a dependency towards another library or another package .
an alternative code reuse is the clone own paradigm .
one would opt for the paradigm of clone own over the traditional code reuse because the involved projects have traceability links and easily share new updates.
the clone own paradigm is a commonly adopted approach for developing multi variant software systems where a new variant of a software system is created by copying and adapting an existing one and the two continue to evolve in parallel .
as a result two or more software projects will share a common code base as well as independent project specific code.
the multi variantsoftware systems are referred to as a software family orfamily in short .
with an increasing number of variants in the family development becomes redundant and maintenance efforts rapidly grow .
for example if a bug is discovered and fixed in one variant it is often unclear which other variants in the family are affected by the same bug and how this bug should be fixed in these variants.
although clone own development paradigm has limitations studies have reported their prevalence on social coding platforms like github .
this study aims to empirically quantify the extent to which divergent variants exhibit redundancy and missed essential updates concerning bug fixes.
therefore we present a tool named pareco that can support the maintenance of divergent variants.
pareco mines bugfixes patches from a pool of updates in a source variant and relies on clone detection to classify the patches as interesting i.e.
redundant missed or uninteresting in the target variants.
we present the illustration of the source target variants in fig.
.
to the best of our knowledge this is the first large scale study on automatically identifying and recommending relevant bug fixes to developers of clone own variants.
our contributions are threefold.
we analyzed source target variant pairs and validated the tool s output.
this results in a curated dataset containing cases of effort duplication and cases of missed opportunities.
the curated datasets can be accessed in our replication package .
we quantify how many cases of effort duplication and missed opportunities exist between divergent variants.
next we investigated the time interval between such patches to assess the window of opportunity for relevant bug fixes.
we developed pareco which can be used as is to support the management of variability in space concurrent variations of the system at a single point in time .
this can be achieved through mining interesting patches from one variant source and classify the patches as interesting or not interesting to the target variants.
existing tools in the github marketplace notify projects about bug fixes but are restricted to analysing only mainline repositories e.g.
qodana1 and lgtm2 .pareco can analyse both mainline and forks.
pareco can also be configured to manage variability in time sequential variations of the system due to its evolution .
this can be achieved by automatically identifying interesting patches in later project kadjel ramkisoen1 john businge1 brent van bradel1 alexandre decan2 serge demeyer1 coen de roover3 and foutse khomh4 missed opportunity figure illustration of the patch classification from source to target variant.
releases that can be backported to earlier releases of the project.
to tool is available and released under an open source license .
terminology problem and concrete examples in this section we provide an overview of the problem define the terminology used and show a concrete example.
.
terminology having explained the problem overview let us now give the formal definitions of the terms used in our study.
current date .the date when we collected the dataset for this study on .
divergence date .the date after the last synchronization of variants.
this is determined using the github api.
each variant pair has its own divergence date .
hunk.a hunk is a grouping of differing lines between two versions of a file .
a hunk is written in the format l s l s with lthe starting line number sthe number of lines the change applies to for each respective file indicating the original file and the indicating the new modified file.
buggy file.
this is a file containing buggy lines before the pull request to fix the bug is created.
patched files.
these are files that are integrated back into the main development branch at the pull request integration with buggy lines removed and new ones added.
diff file.
the resulting file after applying the diff tool on the buggy and the patched file.
it contains both the removed lines from the buggy file and added lines in the patched file and is stored in the file system where the pareco executed.
patch.
a patch is a collection of one or more diff files.
in our study we specifically refer to a patch when this collection of diff files stems from a pull request that was created to fix a bug.
git head file.
the latest version of a file retrieved from the git head on the current date in the main branch of the target variant.
social fork .these are forks that are created for isolated development to fix a bug feature refactoring and thereafter merged back into the mainline .
divergent variant .a variant fork is created by splitting off a new development branch to steer development into a new direction while leveraging the code of the mainline project .
variants in divergent variant pairs contain unsynchronized commits between themselves.
we use the github api to identify unsynchronized commits in a pair where one variant is ahead by x commits and behind by y commits.
in the unsynchronized commits we do not go deeper to identify the commits that are integrated using techniques that change the commit id .
.
classifying patches illustration figure is an illustration of clone own where variant2 forked repository was forked from variant1 original repository .
when variant2 was created fork date it inherited all commits from variant1 .
then between the fork date anddivergence date both variants synchronised commits with each other keeping both variants even.
after the divergence date the variants stopped synchronizing commits.
as a result all commits after the divergence date are unique to the respective variant.
let us assume that the developer of variant1 identified a bug after thedivergence date spread across files foo bar and lot.
the developer decided to create a social fork of variant1 patched the buggy files and finally integrated the patch back into the main branch of variant1 using a pull request.
there are four possible scenarios on the git head ofvariant2 the developer of variant2 could have applied the patch to the buggy file in one of the previous commits before the commit at thegit head .
this is a case of effort duplication ed .
the files at the git head of the target still contains the buggy lines.
this is a case of missed opportunity mo .
we can even calculate how long the target branch has missed the patch by calculating the distance between the patch integration date and the current date of the git head .pareco patched clones and missed patches among the divergent variants of a software family the file at the git head of the target contains both the buggy and the patched lines.
the developer of the target could have fixed the bug partially or only fixed one occurrence of the bug instead of all.
in this case both effort duplication and missed opportunity are present.
this is a split case sp .
the file at the git head of the target does not contain both the buggy and the patched lines.
this case would be interesting ni .
this scenario could happen when the developer of variant2 has replaced the buggy lines with new lines but the tool we use is not able to recognize these new lines as clones.
note that for illustration purposes we only present an example where a patch is variant1 as source and variant2 target.
however in the above example variant1 andvariant2 are interchangeable.
.
motivating example to put the terminologies and problem into perspective we present one concrete motivating example of a missed opportunity mo .
due to space limitations examples of effort duplication and split sp cases can be found in the online appendix .
in this example variant1 represents the source upstream qmk qmk firmware andvariant2 represents the target fork sekigon gonnoc qmk firmware that are hosted on github.
the variants have diverged between and2021 current date adding and117unique commits in variant1 andvariant2 respectively.
one of the upstream pull requests number contains one commit cc0a5f0 with one file.
the file tmk core common chibios eeprom teensy.c was changed to fix a gcc10 build warning forissue .
the pull request was merged on .
listing shows the diff file .
listing shows the latest version of that code in variant2 at2021 .
listing diff file for pr from the source variant while p u i n t t symval eeprom workarea end f l a s h e n d u i n t t u i n t t symval eeprom workarea end f l a s h e n d u i n t t p listing unpatched lines in file eeprom teensy.c in the git head 1200fa9 of target variant return while p u i n t t symval eeprom workarea end f l a s h e n d u i n t t u i n t t symval eeprom workarea end we can see that the code in variant2 is identical to the buggy code invariant1 .
however the fixed line is not found in the git head file of variant2 .
this means that the bug is still present in the forked repository even though a patch exists in the source variant that fixes the bug.
we classify this as a missed opportunity mo invariant2 because the patch that fixed the bug in variant1 is not applied to variant2 .
the patch however can be applied to variant2 to fix the bug that is still there.
the developers of variant2 may have missed the patch pr applied in variant1 since the patch is buried in a pool of changes that could be interesting.
furthermore we have shown a straightforward moexample for illustration purposes that one can easilyobserve.
however patches can range from easy to complex.
for example a pull request may contain many commits files and changed lines.
in a study very related to ours jang et al.
state that finding all unpatched code clones is tricky and involves numerous considerations.
for example how many lines of code need to be similar for a case to be reported?
is one copied line enough or are we only interested in multiple line matches?
should whitespace matter?
should the order of statements matter and if so should we only consider some syntactic classes?
jang et al.
created the clone detection tool redebug to find unpatched code clones mo in os distribution sized code bases 1billion loc that include code written in many different languages.
redebug is a lightweight syntax based code clone detection tool that identifies unpatched code clones at scale.
study design our overarching goal is to understand the extent of patch redundancy and how many divergent variants miss important patch.
.
research questions rq1 how many cases of effort duplication and missed opportunities exist between divergent variants?
since the variants are divergent effort duplication implies that variant developers could be independently fixing common bugs.
in the case of sp only a part of the patch is implemented in the target while for mo the target is still buggy.
this rq aims at finding out the prevalence of these cases in variant pairs.
target variant developers can use the patches of moandspas a starting point to fix the buggy code in their variants.
the cases of edcan be used in follow up studies to investigate how?
and why?
target variant developers clone the patches even though the variants have diverged.
rq2 how much patch technical lag exists between the source and target variants in divergent variants?
this rq is a follow up of rq1 on the missed patches i.e.
mo sp .
we would understand how long the patches introduced in the source variant have been missed reby the target variants.
gonzalez barahona et al.
proposed the concept of technical lag to reflect how outdated a software system is concerning its upstream dependencies.
in this study we define a new technical lag based metric called patch technical lagthat measures how outdated a target variant is concerning the applied patches of mo and sp in the source variant.
we aim to get a better understanding of patch technical lag in the software families.
the insights will help manage and control patch technical lag through tools designed to monitor and recommend the missed patches as soon as they are introduced.
.
data collection step divergent variant pair identification to identify divergent variant pairs we leveraged the dataset from businge et al.
who report a total of over .5k variant pairs.
we were interested in actively maintained divergent variant pairs.
in our first filter we retained variant pairs that were updated at least not earlier than six months back from the .
we next retained included pairs written in java c phporruby as these are the programming languages that our clone detection tool can process.
to ensure that we have divergent variant pairs we alsopoedjadevie kadjel ramkisoen1 john businge1 brent van bradel1 alexandre decan2 serge demeyer1 coen de roover3 and foutse khomh4 figure method overview.
applied another filter to select pairs where there was at least six months between divergence date and the earlier of the two dates ofvariant1 update date and variant2 update date .
finally since we identified the patches from pull requests we ensured that at least one of the variants in the pair had merged one pull request.
after all the filtering we retained a total of diverged variant pairs .
moreover since source and target can be interchanged we can extend this to a total of source target pairs .
c java php python ruby language100101102103104105commits fork mainline figure distribution of diverged commits in the variant pairs categorized in the different programming languages.
we wanted to compare the diverged commits of the mainline variant pairs.
to this end we collected unsynchronized commits in the variant pairs.
the boxplots in fig.
show the distributions by the programming languages we considered.
while it is not surprising that the number of commits to the mainline is always higher than to the fork it is interesting that most forks also have a pretty highnumber of commits.
this gives us confidence that we are studying real variants as opposed to social forks.
step source variant patch identification to identify source variant patches we mine the pull requests prs that were integrated between the divergence date and the current date see fig .
we specifically look for prs that contain keywords related to bug fixes such as fix resolve addresses and crash .
the full list of keywords is described in previous studies .
the bug fixing keywords have been manually validated by castelluccio et al.
achieving a precision of .
and a recall of .
respectively.
step .
file extraction from this step on we use pareco that is a result of reusing and extending the clone detection tool redebug .
redebug only finds unpatched code clones mo which only solves part of our problem.
pareco extends the logic of redebug to identify patched code clones ed by looking at the lines added in a patch.
as a result of looking at both the unpatched and patched clones pareco can also identify code snippets containing both patched and unpatched lines that we define as split cases sp .
this extension also immediately identifies uninteresting patches ni cc ne that are not part of redebug .
we extract the buggy patched and difffiles from the pr commits in the source variant see section .
and fig .
then we extract the corresponding files at the git head of the target variant.
first for every modified file in the pr of the source variant our tool will find the corresponding file at the git head in the target variant.
we identify the files at the git head by comparing paths of the pr commits and the files at the git head .
we only consider the file paths that match.
for simplicity during the file extraction at the git head our tool currently considers the deleted moved and renamed files as missing files.
to process the extracted files we reuse and extend the clone detection tool redebug resulting inpareco .redebug only finds unpatched code clones mo which only solves part of our problem.
pareco extends the logic of redebug to identify patched code clones ed by looking at the lines added in a patch.
as a result of looking at both the unpatched and patched clones pareco can also identify code snippets containing both patched and unpatched lines that we define as split cases sp .
this extension also immediately identifies uninteresting patches ni cc ne that are not part of redebug .
the tool then performs normalization i.e.
removes language comments removes all non ascii characters removes redundant whitespaces except new lines and converts all characters to lower case .
next the tool tokenizes the extracted files from both the source and the target variants.
representing source code as a token sequence enables the detection of clones with different line structures which cannot be detected by the line by line algorithm.
tokenization is performed using n grams where we maintain the n 4as used in the original tool.
in the redebug tool n grams are computed based on lines.
for every set of 4consequent lines of code one n gram is created.
for example for a file with lines of code two n grams of size are considered.
after tokenization the diff file in the source variant is extracted.
since redebug only cares about unpatched code clones the tool was implemented to detect in the buggy snippets missed opportunities .
the tool performs a clone detection between the source and the target to determine if the buggy linespareco patched clones and missed patches among the divergent variants of a software family in the buggy file are still present in the same file in the target.
in this study we care about both the patched effort duplication and unpatched missed opportunity code clones.
therefore in addition to the patched snippet identification performed in redebug our tool compares the source and target to determine if the target contains patched lines that are present in the diff file of the source variant ed .
step .
hunk classification to classify a hunk our tool checks for a code snippet with patched lines or the buggy lines in the commit at thegit head of the target.
we perform this check by comparing the tokens computed from grams generated by redebug .
if the snippet contains only buggy lines then pareco classifies the hunk asmo.
if the snippet contains patched lines then pareco classifies it as ed.
if the snippet contains both buggy and patched lines then pareco classifies it as sp.
the snippet is classified as niifpareco cannot find both the buggy and patched lines.
step .
file classification .
at this step we aggregate the code snippets hunks classifications into the classification of files.
in addition to the hunks classes files in the target can take on two other classes i cannot classify cc class is where a pull request contains only files written in programming languages that the clone detection tool cannot process.
ii non existent ne class is where the pull request contains files missing in the target variant.
if a file contains more than one hunk from different classification we first focus on the hunks in the interesting classes of mo ed and spduring the file classification.
in any given file of interest in the target variant if we can identify at least one hunk classified as mo ed and sp we assume that the associated patch in the source variant could be interesting to the target.
to this end we classify a file according to the most prevalent hunk class of mo ed and sp.
for example if a file has hunks one is classified as mo and the remaining nine hunks classified as ni cc or ne then the file is classified as mo.sp class prevails as the file classification in the case of ties between mo ed sp.
in case none of mo ed spis present then the file takes on the class of the most prevalent class of ni cc ne.
step .
patch classification .
the patch classification is an aggregation of the file classifications.
like for files in step if a patch contains more than one file we first focus on the interesting classes ofmo ed and sp.
we then follow the same criteria discussed in step to aggregate a patch from its classified files.
step .patch technical lag calculation .
we calculate patch technical lag as the elapsed time from when a developer integrates a patch into the main development branch of the source variant and the time of the commit at the git head of the target variant that still contains buggy lines see illustration in figure .
the time when the patch was integrated in the source variant can easily be determined using the github api v3 to extract the pull request merge date .
in this rq we only considered target variants that had at least one patch classified as either mo or sp.
we identified a total of target variants having a total of patches classified as mo or sp.
results discussion rq1 how many cases of effort duplication and missed opportunities exist between divergent variants?rq1aims to investigate how often there are patches classified as mo ed or sp among the variant pairs in our dataset.
here we present the results of the patch classification using pareco and the accuracy of the patch classification.
we validate the accuracy of pareco manually in section .
.
.
tool patch classifications we applied pareco on the patches identified in source variant repositories.
table shows how the patches from the diverged variant pairs were classified.
we present the aggregate statistics to show the distribution of the patch classifications in the target repositories.
the upper part presents the patch classification where source variant is the upstream repository and the target variant is the forked repository.
the lower part of the table presents the statistics with the source and target interchanged between upstream and fork.
figure presents grouped boxplots for the statistics in table .
the plot shows how the patches from the source variants are shared among the different patch classes in the target variants.
each class is represented by two boxplots.
the boxplots on the left of each class represent the results in the upper part of table and those on the right represent the results in the lower part of table .
table descriptive statistics for patches identified in the source variant and classified in the target variants.
metric mean min median max total upstream source fork target missed opportunity mo .
effort duplication ed .
split sp .
not applicable ni .
cannot classify cc .
not existing file ne .
error ee patches .
fork source upstream target missed opportunity mo .
effort duplication ed .
split sp .
not applicable ni .
cannot classify cc .
not existing file ne .
error ee patches .
from the results of table and figure we can see that most patches originated from the upstream variants only few patches originated from the forked variants.
we also observe that most patches are classified in the negative classes of ni cc and ne.
this shows that the majority of the patches contain nifiles ccfiles or nefiles.
however we do observe a good number of patches that have been classified in the interesting positive classes of mo ed andsp a total of of the .
patches.
when considering the upstream repository as the source and the fork as the target i.e.
upper part of the table we observe thatpoedjadevie kadjel ramkisoen1 john businge1 brent van bradel1 alexandre decan2 serge demeyer1 coen de roover3 and foutse khomh4 mo ed sp na cc ne ee classifications100101102number of patches source upstream fork figure distribution of the patch classification.
of the .
variant pairs contain of the patches that are classified as mo.
we also observe that of the variant pairs contain of the .
patches that are classified as ed.
finally we observe that of the .
variant pairs contain of the .
patches that are classified as split cases.
when the source and the target are interchanged to fork upstream i.e.
lower part of the table we observe that of the variants contain a total of only of the patches classified as positive classes of mo ed and sp .
we observe that very few fork variants integrate patches that could be interesting to the upstream variants from the results.
this is not surprising since when we look back in fig.
we can see that the mainlines have more updates than their fork counterparts.
we also observe that most patches are classified as uninteresting classes of ni cc and ne.
from fig.
we observe that there ccclassification unhandled programming language comprise the highest number of patches.
the patches in the ccclass could have an impact on the results we observe in mo ed and spclasses.
as an example in our dataset apache kafka which is a multi language project java .
scala .
other programming languages .
the project has patches that are classified as mo ed sp and cc .
we believe that extending pareco to accommodate scala would reduce the number of cccases.
we will also extend pareco to extract renamed and moved files from the target variant to gain interesting patches from the neclass.
the curated dataset for fig.
can be found in the online appendix .
mo ed sp classifications0.
.
.
.
.
.
.
.2proportion of interesting files in a patch figure distribution of interesting files in the patches.
for example a patch on the ed box plot with a y axis value of .
indicates that all the files in the patch were classified ed.
recall that in section .
step we stated that a patch is classified mo ed orspif it contains at least one file classified as interesting.
figure.
contains violin plots that show the distribution of patches concerning the proportion of interesting files in each patch.
for example a point on the violin plot with x axis label edand a valueclose to zero on the y axis indicates that the patch has many files with the most significant proportion classified as one of ne ni or ccand a tiny proportion classified as ed.
the negative value on the y axis is a result of a smoothed violin plot indicating the existence of value near to .
such as .
the size of the plots indicates the number of data points in the category.
focusing on the bigger sized violin plots of edandmowe can see the upper quartiles are both on .
.
however we can also see that the difference between the upper quartiles is bigger than the difference of the lower quartiles of the violin plot indicating that the plots are positively skewed.
this implies that edandmoclasses have more patches above the median where the proportion of interesting files in the patches is high.
details of fig.
can be found in the online appendix .
mo ed sp ana cc ne classifications050100150200250number of patchessource upstream fork figure all classifications of patches considered for spack spack upstream and bluebrain spack fork .
concrete example.
the variant pair spack spack upstream and bluebrain spack fork are both projects package manager for supercomputers.
the upstream is owned by spack while the fork is owned by the blue brain .bluebrain spack was forked on .
the two repositories diverged on .
as of spack spack had unique commits of commits while bluebrain spack had unique commits of commits.
figure presents the results of patch classifications in the form of a grouped bar plot of the variant pair.
the first two bars labelled mo the first bar shows the number of patches in the source spack spack source that are classified as moin the target bluebrain spack target .
the second bar short on moshows the results when the source and target interchanged.
we can see that in the variant pair shown in figure there are more cases of mothan ed.
summary tool patch classifications of the interesting patch classifications the most prominent class is effort duplication ed comprising a total of .
patches.
next is missed opportunity mo comprising a total of patches.
finally we identify a total of split cases sp containing both mo andedhunks.
we have also shown that most interesting patches have the upstream repository as the source variant.
there are very few cases having the fork as the source variant.
.
accuracy of patch classifications we use six metrics to evaluate the accuracy of our tool true positives tp false positives fp false negatives fn precision tp tp fp recall tp tp fn and the f1 score.
we manually analyse these metrics on the classification results of the patches.pareco patched clones and missed patches among the divergent variants of a software family criteria for ground truth establishment.
to find the real classifications of a patch we mimic the behaviour of our tool manually.
for a given patch we find and extract all the diff files from the source variant and the corresponding files in the commit at the git head of the target variant.
for each hunk in each diff file we manually look at the hunk lines and search if they exist in the file at the git head of the target variant.
we do this as follows we search for the first line of the hunk in the target variant.
if a match is found we go to the next line in the hunk and continue until we find a line prefixed with or i.e.
the added patched lines or the deleted buggy lines.
the other lines are called the context.
it is important to note that all other matches that we find in the file of the target variant should be in the same order as listed in the hunk.
if we only find the buggy lines in the file of the target variant then we classify it as mo if we find the patched lines only we classify it ased if we find both then we have a sp otherwise it is an ni.
once all the hunks have been manually classified we follow the criteria in section .
step to classify the files and step to classify the patches.
for the files written in a language that our tool cannot process we verify if the extension of that file indeed falls out of the scope of our tool and for the files that are found to not exist in the target we open the snapshot of the target in github and check if that file indeed does not exist.
accuracy of measurement.
we calculated a sample from our population of patches using an online sample size calculator .
we used the following parameters confidence level margin of error population proportion of corresponding to for positive cases and negative cases .
from this we determined a sample size of cases for the manual analysis.
the manual analysis was conducted by the first author of this paper and the second author validated the labels.
however while conducting the manual analysis the author observed that big patches containing many files hunks and a lot of changed lines of code took very long and yet they were not obvious to decide.
to mitigate the challenge we decided to filter out the complex cases.
we excluded patches that had changes in more than five files more than lines added and more than lines removed.
using this criteria we filtered out complex case patches .
from the total population of patches.
the new population of patches contained positive cases mo ed sp and were negative cases ni cc ne .
the new sample size using the population proportion of while maintaining the values of the other parameters becomes patches.
we group the results on two levels of classification first level .
contains the interesting cases mo ed and sp representing the positive and uninteresting cases representing the negative cases ni cc and ne .
we also present the classification results for the three levels of granularity of the bug fix i.e.
patch file and hunk .
so for example if a patch is classified by the tool as any of mo ed and sp predicted positive and during the manual classification we find out that the patch is any of ni cc and ne actual negative then we label the case as false positive fp .
second level .
here we go deeper into the positive cases to see if they were correctly classified.
for example if a patch is classified by our tool as mo predicted positive and during the manual analysis we observe that it is supposed to be ed actual negative .
therefore we label the case as false positive fp .
in table we present the confusion matrix.
prthe in the first column stands for the actual positive cases and the in the last row of the table stands for the predicted class.
the in the last row of the table stands for the predicted negative cases and the in the first column stands for the actual negative cases.
in table we present the precision recall and accuracy for the first level of classification.
from table we observe a precision of a recall of and an accuracy of for all the levels of granularity.
this means that our tool is good at correctly classifying both the positive interesting classes and the negative uninteresting classes.
table confusion matrix for the classifications on different levels after manual validation of the results of pareco .
positive cases mo ed and sp and negative cases ni cc and ne predicted patch files hunksactual table precision recall and accuracy for the different classification levels for the results in table .
precision recall accuracy f1 score patches .
.
.
.
files .
.
.
.
hunks .
.
.
.
table shows the confusion matrix for sampled data from our dataset on the second level for the mo ed and sp classes at the three different levels of granularity patch file and hunk .
table shows the precision recall and accuracy corresponding to the results in table .
for both moandedwe observe relatively high values of precision recall and accuracy all .
our validation results are comparable to the study of kim et al.
who validated redebug vulnerable code clone discovery edin our case and reported a precision of .
this gives us confidence that our tool is relatively good at identifying interesting patches.
looking at the results of the spcases we observe low values for the precision and recall at the granularity levels file and hunk yet those of the patch granularity are all .
the low values originate from the imbalanced data since we have few positive cases and many negative cases.
this issue of data imbalance is visible from the high accuracy for spat all levels of granularity.
the dataset for spis biased towards the most prominent class the negative class which is the reason for the low levels of precision and recall.
from the results presented we conclude that our tool is relatively good at performing patch classifications.
incorrect classifications.
during the manual analysis we identified misclassified hunks.
after analyzing the incorrect classifications we summarized them into four categories and indicate if the issue is resulting from the original code or our extended code .poedjadevie kadjel ramkisoen1 john businge1 brent van bradel1 alexandre decan2 serge demeyer1 coen de roover3 and foutse khomh4 table confusion matrix for interesting classifications.
predicted mo ed spactualpatch files hunk table precision recall accuracy and f1 score for confusion matrix in table .
precision recall accuracy f1 score mo patches .
.
.
.
files .
.
.
.
hunks .
.
.
.
ed patches .
.
.
.
files .
.
.
.
hunks .
.
.
.
sp patches files .
.
.
.
hunks .
.
.
.
hunk classification failure extended code cases for various reasons we found that some hunks were misclassified.
one line hashing original code cases during the manual analysis we observed that some hunks were divided into ngrams of size one instead of four resulting in misclassification.
issues with comments original code cases in some hunks we noticed that python andccomments that are supposed to be ignored were also hashed and considered in the matching.
multi line comments that are only partially present in the hunk were not removed by the regular expressions used for this during normalization.
file extension misunderstood original code cases phpis one of the programming languages that our tool can process.
we noticed in four misclassified cases that the tool could not guess the mime type of the phpfiles to be php causing the patches to be classified as cc.
in follow up studies these issues will be analysed critically so as to have a minimal misclassifications as possible.
we have already submitted issues to the redebug repository in github regarding misclassifications.
we are also planning to address the issues and we shall submit a pull request to the repository when we are confident that the issue has been solved in our tool.
summary accuracy of patch classifications although pareco classified only as interesting patches from the total patches.
the validation exercise reveals relatively high results precision recall accuracy and f1 score for patch classifications of and respectively.
.
discussion implications rq1 focused on automatically identifying a patch in one of the source repositories variant1 orvariant2 and then correctly classifying the patch as interesting or as not interesting.
in section .
we already discussed that pareco misses a lot of interesting patches thatpareco classified as ni ne cc.
we also discussed how we plan to extend pareco to identify more interesting patches.
as a preliminary analysis we have analysed the patches in the cccategory and observed other frequent languages such as javascript scala kotlin json c yaml and rust .
the tool can be extended to analyze these languages and hence gain more on the interesting patches that can be identified.
although our study is in the early stages the results reveal that clone and own variant on github exhibits redundant development and miss important updates.
we believe that the study is in the right direction towards supporting the maintenance clone and own variant.
we also believe that although pareco is still a work in progress developers can find pareco useful in supporting the maintenance clone and own variants.
furthermore follow up studies will also focus on user studies with the developers regarding the tool s results.
for example a user study can reveal the threshold of the proportion of interesting files in a patch c.f.
fig.
for a patch to be considered interesting.
actionable result pareco as is can be used by developers to manage clone and own variants.
follow up studies will be focused on extending pareco to a patch recommender tool.
missed opportunity.
the results of moare expected due to the diverged status of the variants.
however it does provide insights into the number of patches in the source variants that go undetected in the target variants.
the results are interesting since they reveal that as much as the variants have diverged some interesting changes like patches in common files could be propagated to improve the quality of the variants.
the previous studies on variants reported over of the variants pairs have uncommon maintainers.
we replicated the same experiment in this study and observed of the variants pairs have uncommon maintainers.
the results in this study are not surprising since we consider only variant pairs that have diverged.
to this end as a result of distinct development teams the variant developers may be aware that there could be some interesting changes but these changes are not easy to find in a pool of other changes.
split sp .these cases are also interesting since they reveal that the target variant only contains part of the patch applied in the source variant and the other part is missed.
in addition to the patch being incomplete in the target variant the expertise of the developers fixing the patch might also vary.
it is plausible that a less experienced developer fixed the incomplete patch.
to this end the developer of the target variant can make use of the split cases and fix the missing part of the patch in their repositories.
actionable result this study shows that there is great potential to automate the identification of those difficult to find changes that can be recommended to variant developers.
variant maintainers can already use pareco as is to uncover interesting patches from a source variant and integrate them into their repository.pareco patched clones and missed patches among the divergent variants of a software family effort duplication.
since we study only divergent pairs this implies that the majority of fork variants either re implement patches already implemented in the upstream or they clone only the interesting patches from the upstream into their projects.
in the case of cloning only the interesting patches there are a number of ways this can be achieved copying the patch code snippets from the source repository and pasting it in the target or synchronizing the variant pairs using github tools that change the commit id during the integration such as pr squash rebase and other git tools that completely change the commit history such as git squash.
patch integration using these tools would merge all the differences in the variants which is not something that the variant developers would desire .
moreover the study of businge et al.
revealed that the two techniques of pr squash rebase are infrequently used techniques to integrate change between variants.
to this end it is likely that such integration techniques may be less prominent in explaining the many cases of effort duplication.
another method that can be used to clone the patches is git cherry picking where the developer integrates only desired commits .
however from the study of businge et al.
it was observed that the cherry picking was less frequently used as an integration technique.
furthermore since we observe considerable numbers of mo it is unlikely that the integration techniques that change the commit history are being used.
a follow up study engaging with the developers could help reveal the frequently used to patch integration techniques and why?
they are preferred.
developer engagement could also tell the developers challenges in incorporating the patches.
furthermore in fig.
the edcases with a low proportion of interesting files indicate that these patches are not exact but share some clones.
the variable part of these patches can be a source for further investigation.
for example if different developers implement the same patch one of the patches in the variant pair might be more elegant than the other.
if this happens to be the case the more elegant patch should be recommended to the other variant developer.
actionable result to avoid reinventing the wheel developers can usepareco to identify interesting patches in a family of variants.
results discussion rq2 how much patch technical lag exists between the source and target variants in divergent variants?
.
results here we present the results of the patch technical lag in our target variant projects.
figure is a line plot showing how much technical lag there is in each of the patches classified as mo or sp in the target variants it plots the technical lag in each patch.
a point on the line plot represents a patch x axis and how much technical lag in terms of weeks y axis .
the median of the technical lag for these patches is weeks over half a year which is relatively high.
this implies that patches introduced in the source variant spend at least weeks before they are noticed by the target variants.
the median value could be more as of today since the current date as seen in figure is as of when this data was collected.
interesting patches mo and sp 0100200300400technical lag weeks weeks medianfigure a line plot showing the patch technical lag of the patches of mo and sp in the target variants.
target variants mo and sp patches 0100200300400technical lag weeks weeks mean median figure a line plot showing the patch technical lag of the target variants with respect to the patches of mo and sp.
since each variant may have one or more patches classified as mo or sp we calculate the technical lag of a target variant based on the technical lag of the patches in that variant.
we take the mean of all the patches technical lag as the technical lag of the target variant and plot it in figure .
figure is a line plot showing how much technical lag there is in each of the target variants.
a point on the line plot represents a target variant x axis and the technical lag in terms of weeks y axis .
we observe a median of weeks as the technical lag of the target variants which is relatively high.
summary rq2 we have found that most of the missed and the incomplete patches in the target variants have a patch technical lag of weeks or more.
we have also observed that the patch technical lag per variant is about weeks.
.
discussion implications our empirical results have revealed that many variants exhibit patch technical lag.
however a survey with variant developers reported by businge et al.
show that there is limited code integration between variants.
the authors further reveal that variant developers indicated reasons for lack of code integration such as variants have nothing to share variants have technically diverged variants are implementing different technology .
this implies that target variant developers may be ignorant of interesting patches of moin the source variants.
furthermore the developers may not have the time and effort to identify the missed patches since an interesting patch may be buried in a pool of other uninteresting changes.
these developers could use pareco to find these interesting patches and reduce the patch technical lag.
pareco can uncover and recommend the interesting patches to the target variants as they are introduced in the source variants.poedjadevie kadjel ramkisoen1 john businge1 brent van bradel1 alexandre decan2 serge demeyer1 coen de roover3 and foutse khomh4 although our tool supports variability management in the space dimension divergent variants the tool could also manage variability in time sequential releases of one variant .
the study of decan et al.
reports that older releases of packages in the package managers of npm cargo packagist and rubygems still have dependent packages.
this implies that to support the dependent packages of the different package releases the owners have to maintain the releases in parallel.
therefore a patch in a later release has to be backported to all the earlier releases with dependent packages.
with many releases involved not all patches found later can be interesting to earlier releases.
pareco can also be used to mine patches in a later release of a project and classify the patches as mo in earlier releases of the same project.
the classified mopatches can be used to benefit backporting in earlier buggy releases.
studies involved in understanding the evolution of software projects over subsequent reselases can benefit from pareco actionable result by specifying a time interval pareco can mine a patch from the source variant and classify the patch as interesting or not interesting to the target variant.
pareco can also be used to manage variability in time by automatically identifying interesting patches in later project releases that can be used for backporting in earlier still supported buggy releases.
related work throughout earlier sections of the paper we discussed a number of studies that relate to ours.
in this section we shall only discuss the studies that we have not yet discussed.
software product line spl .
a more systematic way of developing variants is through spl which consists of a set of similar software products i.e.
variants with well defined commonalities and variabilities.
horcas et.
al.
survey spl tools and give different roadmaps for existing tools that can handle all or some part of the spl processes.
lape a et.
al.
implemented an approach that ranks legacy code of similar products based on the requirements of the new product and searches for relevant methods in the highest ranked legacy code decreasing the amount of code the developers need to verify during clone own.
managed clone own variants .
the common goal of studies on managed clone own variants is to support the engineering of multivariant software systems by reducing their limitations.
mahmood et al.
design formalize and prototype a lightweight method that generalizes clone management and product lines through exploiting the spectrum between the two extremes of ad hoc clone own and fully integrated platform.
bittner et al.
proposed an approach that ranks the relevancy of legacy products for a new development at the requirements level and to locate their most significant methods for each of the new product requirements.
to support the ongoing development of clone own projects there are studies that have proposed a feature trace recording a method to infer feature traces in source code changes .
clone detection .
since we employ a clone detection technique our work can also be related to different studies on clone detection.
houand zhang present a survey on the different methods and technologies used for clone detection.
they discuss text based tokenbased tree based and metric based clone detection and program dependency graphs.
zhang and sakurai recently performed a comprehensive review of clone detection tools from a security perspective.
in total they included tools from different studies including redebug vuddy and vgraph.
our work can also be related to the study of businge et al and kawuma et al.
who studied clones in the eclipse framework.
the authors report that clones in the eclipse framework range between to .
the vuddy clone detection tool identifies type and type clones but only supports c and c .
another tool vgraph uses a technique mostly similar to redebug as they focus on the lines of source code that are modified during the patching process.
however they represent the code as a graph instead of text.
there are also other tools like sourcercc cpminer ccfinder .
these use a token based method to tokenize source code and a measure to quantify the overlap between tokens for clone detection but unlike the other three tools no information from the patch is used.
hat et al.
analyzed shared files and their variants on github and present changes might be useful for meta maintenance of cloneand own variants.
kawamitsu et al.
identify clone own reuse on file level based on the longest common subsequence similarity between a copy and candidate origin file in c programs.
threats to validity construct validity .
these threats concern the relation between the theory behind the experiment and the observed findings.
they can be mainly due to imprecision in the measurements we performed.
imprecisions in our measurements could occur during patch extraction from the source target and during clone detection to identify interesting patches for the target.
for patch extraction we have used keywords that earlier studies have validated.
for clone detection we have extended the tool redebug that has been tested on os sized distribution projects.
we have also gone ahead to validate our results manually and our validation results are comparable to earlier studies by kim et al.
that have also manually validated theredebug tool in a similar context.
based on the method employed to extract files for comparison between the source and the target we acknowledge possible imprecisions since we considered all files that pareco could not find in the target as non existent files.
last but not least we also acknowledge a possible threat resulting from considering the most prevalent positive classes of mo ed sp of the file class as the class of patch.
internal validity .
these threats concern choices and factors internal to the study that could influence the observations we made.
given that the method we use to identify the patches is entirely based on bug fixing keywords in the merged pull requests the tool may suffer from false positives wrong patches and false negatives missed patches .
to reduce the false negatives one could identify more patches by searching commit messages.
however a patch may spread in more than one commit making identifying a complete patch difficult if not impossible.
more false negative patches could be missed by pareco through other integration techniques like direct integration or using other methods like git push .
although we do not control the false negatives they do not impactpareco patched clones and missed patches among the divergent variants of a software family the classification s accuracy.
regarding the false positives while performing the manual validation on the patches we analyzed the pr titles and we did not find any false positives.
furthermore these false positives do not impact the classification s accuracy since the tool would classify them as not interesting ni .
the variant pair filtering criterion using an educated estimation of cut off dates and the filtering of complex cases using the number of changed files and changed lines of code could also be a threat to our findings.
external validity.
the threats concern whether the results can be generalized outside the scope of this study.
we cannot claim that the results represent all the patches since while validating the tool s accuracy we excluded the complex cases.
however our results are still representative since less complex cases considered in the validation exercise constituted .
of the total population.
while our dataset can be regarded as representative of variants on github we do not make any claim about its generalizability to other social coding platforms e.g.
bitbucket gitlab .
conclusion and future work to understand the extent of redundant and missed patches among divergent variants we conducted an empirical investigation on source target divergent variant pairs.
to this end we introduced and used pareco a tool that relies on clone detection to identify the redundant and missed patches among variants in both space and time.
form the source target pairs analyzed we found interesting patches classified with an overall precision recall accuracy and f1 score of and respectively.
.
of interesting patches are classified as effort duplication that are found in only a third of the divergent variant pairs.
in follow up work we plan to extend pareco with more capabilities and accuracy and to transform it into a patch recommender tool.
acknowledgement this work is supported by the joint fwo vlaanderen and f.r.s.
fnrs excellence of science project seco assist under grant number o.
.18f rg43.
we also acknowledge the support of natural sciences and engineering research council of canada nserc .