lexecutor learning guided execution beatriz souza university of stuttgart stuttgart germany beatrizbzsouza gmail.commichael pradel university of stuttgart stuttgart germany michael binaervarianz.de abstract executing code is essential for various program analysis tasks e.g.
to detect bugs that manifest through exceptions or to obtain execution traces for further dynamic analysis.
however executing an arbitrary piece of code is often difficult in practice e.g.
because of missing variable definitions missing user inputs and missing third party dependencies.
this paper presents lexecutor a learning guided approach for executing arbitrary code snippets in an underconstrained way.
the key idea is to let a neural model predict missing values that otherwise would cause the program to get stuck and to inject these values into the execution.
for example lexecutor injects likely values for otherwise undefined variables and likely return values of calls to otherwise missing functions.
we evaluate the approach on python code from popular open source projects and on code snippets extracted from stack overflow.
the neural model predicts realistic values with an accuracy between .
and .
allowing lexecutor to closely mimic real executions.
as a result the approach successfully executes significantly more code than any available technique such as simply executing the code as is.
for example executing the open source code snippets as is covers only .
of all lines because the code crashes early on whereas lexecutor achieves a coverage of .
.
ccs concepts software and its engineering software testing and debugging .
keywords execution neural models dynamic analysis acm reference format beatriz souza and michael pradel.
.
lexecutor learning guided execution.
in proceedings of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering esec fse december san francisco ca usa.
acm new york ny usa pages.
introduction the ability to execute code enables various dynamic program analysis applications.
for example running a program may expose permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse december san francisco ca usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
missing global variable inject a non empty stringmissing import and missing attribute inject an object and a method3missing variable inject a non empty list1 missing function return true2 4figure python code to execute and how lexecutor guides the execution.
bugs that trigger obviously wrong behavior such as runtime exceptions.
moreover dynamic analysis has been shown its usefulness for various tasks e.g.
mining api protocols taint analysis and type inference .
generally dynamic analysis is well known to complement static analysis providing a valuable technique for understanding and improving software.
unfortunately executing arbitrary code is challenging both at the small and the large scale.
small scale code snippets e.g.
posted on stack overflow often are missing contextual information such as imports and definitions of variables and functions .
largescale code bases often are non trivial to set up and run e.g.
due to missing third party dependencies complex build procedures and the lack of inputs that reach deeply into the project.
overall the difficulties of executing code limit the potential of dynamic analysis.
for example consider the python code snippet in figure which could be the body of a function extracted from a complex code base or code posted on stack overflow.
executing this code is difficult because of various missing pieces of contextual information.
at first the code tries to read variable all data which does not exist and hence will cause the code to crash.
even we had a definition for this variable then other obstacles would prevent the code from executing such as the missing function has min size the missing global variable config str and the missing imported method logger.info .
a skilled human can intuitively fill in the missing information by predicting it to be likely that e.g.
all data is a list has min size will return a boolean config str is a string and that logger.info is a method call likely to succeed.
given this information a human could mentally emulate the execution and hence reason about the runtime behavior of the code.
the key question we ask in this work is can we automate the prediction of likely values and use them to execute otherwise non executable code?
this paper presents lexecutor which offers a learning guided approach for executing arbitrary code snippets in an underconstrained way.
the key idea is to let a neural model predict suitablearxiv .02343v4 nov 2023esec fse december san francisco ca usa beatriz souza and michael pradel values whenever the program usually would be stuck.
the approach is enabled by three components a specifically designed and trained neural learning model that predicts runtime values based on the code context in which a value is used.
we realize this component by finetuning a model with hundreds of thousands of examples gathered from regular executions of real world programs.
an execution environment that prevents crashes due to missing values and instead fills in model predicted values.
an ast based instrumentation technique that turns arbitrary python code into lexecutable code.
for the example in figure lexecutor prevents the program from crashing and instead predicts suitable values for all undefined variables and functions.
the figure shows what values the approach predicts and then injects into the execution.
for example when the code tries to access the otherwise missing value has min size then lexecutor predicts it to be a function that returns the boolean value true .
given the injected values the code successfully executes without prematurely terminating.
there is no guarantee that the values that lexecutor injects exactly match those that would occur in a real execution of the given code.
however we find that lexecutor is able to predict realistic values in most situations where a regular execution would simply get stuck enabling the execution of otherwise non executable code.
our work has overlapping goals with four prior streams of research test generators as they also provide missing input values.
concolic execution which abstracts missing inputs into symbolic variables and constraints.
microexecution and underconstrained symbolic execution which execute arbitrary code by injecting random values into memory on demand.
in contrast to and lexecutor injects otherwise missing values at arbitrary points within an execution whereas the prior work provides values only at well defined interfaces e.g.
command line arguments or method arguments.
in contrast to all the above work lexecutor uses a neural model to predict realistic values whereas and inject random values and overapproximates possible values via symbolic reasoning.
neural models that predict various properties of code .
in contrast to lexecutor predicts runtime values during an execution whereas prior work focuses on predicting static properties of code.
we evaluate our work by applying it to two sets of code snippets functions extracted from popular open source projects and code snippets extracted from stack overflow posts.
as baselines the evaluation compares lexecutor to regular execution injecting random values and a state of the art function level test generator for python .
we show that the neural model at the core of lexecutor predicts realistic values with an accuracy of .
to .
depending in the configuration .
moreover our results show that the approach enables the execution of .
and .
of all lines in the open source code and stack overflow snippets respectively which improves over the best baseline by and .
.
finally we apply lexecutor to detect semantics changing commits by comparing the execution behavior of a function before and after a commit.
we envision lexecutor to enable various dynamic analysis applications.
for example lexecuting code could help detect bugs that instrumentation instrumented code instrumented codecode to execute executable code training predictioncontext value pairs neural model runtime enginemissing value and code context likely runtime valueexecute trainfigure overview of learning guided execution.
manifest through obvious signs of misbehavior such as runtime exceptions or assertion violations.
likewise our approach could be used to validate code generated by code synthesis techniques or generative language models .
other potential applications include to check if and how a code change to modifies the observable behavior and classical dynamic analyses such as detecting security vulnerabilities via taint analysis.
in summary this paper contributes the following a novel way of executing arbitrary code in an underconstrained and learning guided way.
a neural model designed and trained to predict realistic values to be injected into an execution.
the task addressed by the model fundamentally differs from prior work on predicting various properties of code because we here predict runtime values.
an instrumentation based runtime environment that prevents programs from crashing due to missing values and instead injects values provided by the model.
empirical evidence that the approach effectively injects realistic values and outperforms several baselines at covering and successfully executing code.
overview figure gives an overview of our approach.
lexecutor has a training phase and a prediction phase .
the training phase yields a neural model that given the code context in which a value is used during an execution predicts a suitable value.
to this end we train a model based on pairs of code context and a value used in this context which we gather by executing a corpus of code.
once the neural model is trained lexecutor is ready for the prediction phase.
the input to the approach is a possibly incomplete piece of code to execute.
to execute this code despite possibly missing information aruntime engine intercepts any use of a value such as a read of a variable.
if the value is not defined i.e.
the program would usually terminate prematurely the runtime engine queries the neural model and then injects the value proposed by the model into the execution.
both training and prediction are enabled by source to source instrumentation which takes regular python code and adds newlexecutor learning guided execution esec fse december san francisco ca usa instructions to it.
the added instructions wrap all uses of values into special value loaders that implement the functionality of lexecutor while otherwise preserving the original semantics.
for the example in figure an execution starts at line by using two values first all data and then has min size .
lexecutor wraps both into value loaders that allow the runtime engine to observe that these values are missing and to hence inject values that would likely occur in a regular execution.
as shown in the figure the approach injects a non empty list for all data and a function returning true forhas min size .
as a result the code continues to execute passes line without any need for guidance by lexecutor and then reaches line .
here several values are missing which the runtime engine provides successfully.
for the missing method logger.info the approach first injects a dummy object for logger then resolves the missing attribute info into a function and finally injects the return value none when calling the function.
by guiding the execution in this way lexecutor successfully executes all code shown in the figure.
approach the following presents in detail the three main components of lexecutor the code instrumentation the neural model and the runtime engine.
.
code instrumentation the goal of the instrumentation is to observe runtime values to be used during the training phase and to inject otherwise missing values during the prediction phase.
we perform a series of astbased code transformations via a top down left to right pass over all nodes in the given ast.
the pass transforms three kinds of ast nodes and the corresponding ast subtrees under them variable reads .
the instrumentation transforms all nodes that refer to the name of a variable that gets read.
in contrast we do not instrument variable writes.
for example in y x the reference to xgets instrumented as it might read an undefined value which lexecutor tries to prevent.
in contrast the assignment to yis not instrumented and hence executed as in the original code.
attribute reads .
the instrumentation transforms all nodes that refer to an attribute that gets read.
again we do not instrument writes of attributes.
for example in y.foo x.bar both the read of the variable x as described above and the read of the attribute barget instrumented because they might access otherwise undefined values.
calls of functions and methods .
the instrumentation transforms all calls of functions and methods we say functions to mean both .
for example in y foo the call of foo gets instrumented so lexecutor can inject a return value if the function is undefined.
each of the three kinds of instrumented nodes is replaced by a call to a special loader function .
for variable reads the instrumented code calls a loader function n and passes two pieces of information to it the name of the variable and a newly created lambda function that tries to read and then return the value of the variable.
the lambda function enables lexecutor to try to read the variable in a controlled manner and to react accordingly depending on whetheroriginal code x foo y x.bar z instrumented code x c n foo lambda foo y a n x lambda x bar n z lambda z figure example of code instrumentation.
the value is available.
for attribute reads the instrumented code passes two pieces of information to a loader function a the value of the base object which when accessing the attribute has already been evaluated and the name of the attribute.
the loader function will then try to read the attribute of the base object and react accordingly depending on whether the attribute is available.
finally for function calls the instrumented code passes the callee i.e.
the called function to a loader function c .
the loader function will then invoke the callee while handling cases where the function is undefined.
in addition to the parameters mentioned above each of the loader functions also receives a unique identifier of the instrumented source code location which lexecutor later uses to access the code context of the location.
figure illustrates the instrumentation with an example where the original and the instrumented code are shown at the top and bottom respectively.
the call of foo in the first line consists of two separately instrumented instructions reading the variable foo which gets wrapped into n and calling the function stored in this variable which gets wrapped into c .
the numbers passed as the first argument to the loader functions are the unique identifiers of code locations.
the second line of the original code contains three instructions to instrument the two variable reads of xand z and the attribute access wrapped into a .
note that the calls to the loader functions are nested into each other.
for example in the first instrumented line the value returned by n as the result of reading variable foois passed as the callee to c .
in addition to the instrumentation described above the approach adds to each instrumented file imports of the loader functions.
with these imports the instrumented file serves as a drop in replacement of the original file.
apart from changing the uses of values the instrumentation does not modify the semantics of the code.
.
neural model we next describe the neural model which predicts likely runtime values to use in a given code context.
.
.
gathering training data.
to gather data for training the model we execute and dynamically analyze tests suites of popular python projects.
the training executions are not guided by lexecutor but regular executions with all inputs and dependencies given.
via dynamic analysis the approach gathers a trace of value use events definition .
.
avalue use event is a tuple n v k l where nis the name of the used reference i.e.
a variable name an attribute name or the name of a function esec fse december san francisco ca usa beatriz souza and michael pradel vis an abstracted version of the value that gets used kindicates the kind of used value which is either variable attribute orreturn value and lis a unique identifier of the code location where the value gets used.
to gather a trace of such events lexecutor instruments all executed code section .
.
the loader functions invoked by the instrumented code load the requested value as the original code and in addition add a value use event to the trace.
at the end of the execution lexecutor stores the recorded trace into a file.
.
.
representing values for learning.
given traces of value use events we need to represent these data in a format suitable for deep learning.
an important part of this step is to abstract the concrete runtime values observed during an execution into a finite number of classes which will be the prediction targets of the neural model.
the abstraction has two conflicting goals.
on the one hand we aim for a fine grained precise prediction of what value to inject into an execution.
for example if the code uses a variable age predicting the exact integer to load would enable lexecutor to produce the most realistic possible execution.
on the other hand we aim for a highly accurate model which becomes easier if there are only few coarse grained classes of values to choose from.
for the above example such a coarse grained prediction could simply say that the value should be an integer without any more information about the specific value.
lexecutor strikes a balance between these two conflicting goals by using a fixed size set of abstracted values.
by default this set comprises classes as shown in table .
the left column gives the classes of values that the neural model distinguishes.
for some very common primitive values namely none true and false the approaches preserves the concrete values enabling lexecutor to predict them exactly.
for numeric types such as intandfloat we abstract the concrete values into three classes per type which represent negative zero and positive values respectively.
to represent strings and common data structure types such as lists the approach distinguishes between empty and non empty values e.g.
an empty list as opposed to a non empty list.
all callable values such as functions are represented as a class callable .
the resource class represents values that can be opened and closed such as file pointers which are often used with the with keyword in python.
finally all remaining non primitive types are abstracted as object .
we refer to the value abstraction described above as fine grained .
to better understand the impact of the number of classes that values are abstracted into lexecutor also supports a more coarsegrained value abstraction with only classes as shown in table .
instead of distinguishing between different values of a type the coarse grained value abstraction maps all values of a type into a single class.
for example the abstraction does not distinguish between negative zero and positive integers but simply represents all of them as integers.
unless otherwise mentioned all results are based on the fine grained variant of the approach.
.
.
representing code context for learning.
the goal of the neural model is to predict one of the abstract value classes for each missing value.
to this end we provide the following contextual information as an input to model table fine grained abstraction and concretization of values.
abstract class of values concretization python common primitive values none none true true false false built in numeric types negative integer zero integer positive integer negative float .
zero float .
positive float .
strings empty string non empty string a built in sequence types empty list non empty list empty tuple non empty tuple dummy built in set and dict types empty set set non empty set set dummy empty dictionary non empty dictionary a dummy functions and objects callable dummy resource dummyresource object dummy table coarse grained abstraction and two modes for concretizing values.
abstract class concretization python of values deterministic randomized common primitive values none none boolean true true false built in numeric types integer float .
.
.
.
strings string a a built in sequence types list tuple dummy dummy built in set and dict types set set dummy set set dummy dictionary a dummy a dummy functions and objects callable dummy resource dummyresource object dummy lexecutor learning guided execution esec fse december san francisco ca usa definition .
.
the input to the model is a sequence of tokens n sep k sep cpre mask cpost where nis the name used to refer the to be predicted value sep is a special separator token kis the kind of value to predict i.e.
variable attribute or return value .
cpreare the code tokens just before the reference to the to be predicted value mask is a special masking token and cpost are the code tokens just after the reference to the tobe predicted value.
the prediction task is related to the well known unmasking task which is commonly used to train language models .
in contrast to unmasking we do not ask the model to predict a masked token which would be trivial as it is given in nas part of the input.
instead we ask the model to predict what value the masked token most likely evaluates to.
example.
suppose the code in figure gets executed with all required contextual information e.g.
as part of a larger project and that we gather training data from its execution.
the approach keeps track of all used values such as the read of all data at line .
following definition .
the approach records a value use event n v k l with n all data v non empty list v variable and lbeing a unique identifier of the code location.
the corresponding input to the model is a tokenized version of the following all data sep variable if not has min size mask raise runtimeerror ... the prediction target for this example is non empty list .
.
.
training and prediction.
the data preparation steps described above yield pairs of input sequences and abstracted values.
before training the model we deduplicate these pairs which greatly reduces the amount of training data because programs often repeatedly load the same value at the same location e.g.
in a loop.
we then train a single model for all three kinds of values to predict allowing the model to generalize across values stores in variables and attributes as well as return values of function calls.
for the neural model our approach can build on any model that accepts a sequence of tokens as the input and then either acts as a classifier or predicts tokens from a given vocabulary.
our current implementation integrates two pre trained models codet5 and codebert which we fine tune for our prediction task.
building upon a pre trained model enables our approach to benefit from this model s abilities in understanding both code and natural language.
codet5 uses the t5 architecture a transformer based neural network architecture that maps a sequence of input tokens to a sequence of output tokens.
codebert follows the bert architecture and is pre trained to unmask missing tokens and predict whether a token has been replaced.
for both models lexecutor tokenizes the input and output using the tokenizer that comes with the respective model.
to fit the input sequence into the fixed input size of the models tokens we truncate and pad the code tokens incpreandcpost definition .
.
for training we use the adamalgorithm value loading and injection in the runtime engine.
input kind kof value model input i output concrete value v ifk name then v load value or catch nameerror ifno exception while loading the value then return v end if else if k attribute then v load value or catch attributeerror ifno exception while loading the value then return v end if else if k return value then ifcallee fis a regular function then return v f end if end if vabstr model i v concretize vabstr return v optimizer with weight decay fix .
we leave all hyperparameters at the defaults except for the batch size which we reduce to codet5 and codebert so it fits into our available gpu memory.
.
runtime engine once the model is trained the prediction phase of lexecutor executes possibly incomplete code via learning guided execution.
the core of this step is the runtime engine which intercepts all loaded values and injects otherwise missing values on demand.
to this end the runtime engine implements the loader functions added to the original code during the instrumentation section .
.
the basic idea is to load and return the original value whenever possible and to fall back on querying the model otherwise.
algorithm presents how the engine loads and if needed injects values.
for uses of variables and attributes lines to the runtime engine tries to read the value while catching any exception thrown when a variable or attribute is undefined.
if reading the value succeeds then this value is returned and the regular execution of the code continues.
otherwise the engine will predict and inject a value as described below.
for calls of functions lines to the algorithm distinguishes two cases.
if the callee is a regular function i.e.
a function that was successfully resolved without any help by lexecutor then the runtime engine simply calls this function and returns its return value.
if instead the function is a dummy value that was injected by lexecutor because the code tried to read a function value that does not exist then the engine predicts and injects a suitable return value.
whenever algorithm reaches line it has failed to load a regular value.
in a regular execution the code would terminate prematurely in this case.
instead the algorithm queries the neural model for a suitable value to inject and the model returns one of the abstract classes in tables and .
to enable the code to continueesec fse december san francisco ca usa beatriz souza and michael pradel its execution the runtime engine concretizes the abstract class into a runtime value.
in our default configuration i.e.
using the finegrained value abstraction the concrete values are those shown in the right column of table .
they are simple default values such as 1for negative integers or a for non empty strings.
whenever the engine injects an object it creates a new instance of an empty dummy class dummy .
for injecting callables i.e.
in situations where the code tries to access a non existing function the engine returns the constructor of the dummy class.
the reason is that constructors are the most common type of callable in our dataset.
when using the coarse grained value abstraction lexecutor supports two modes for deciding what concrete values to inject.
in deterministic mode the approach always injects the value shown in the middle column of table .
in contrast in randomized mode the approach randomly picks from the values shown in the right column of the same table.
for example when the model predicts a string then the deterministic mode always provides the value a whereas randomized mode returns either or a .
to avoid executions that normally would be be impossible the randomized mode ensures that repeatedly using the same variable returns a consistent value.
future work could further improve the value concretization e.g.
by trying to predict meaningful string values or by predicting the types of values a non empty list should contain.
importantly the runtime engine does not prevent all runtime errors that might occur during an execution.
instead lexecutor prevents only those exceptions that would be caused by trying to use a non existing value.
that is code executed with lexecutor may still fail e.g.
due to some logical bug in the code.
example.
suppose that we are trying to execute the code in figure during lexecutor s prediction mode.
when reaching the use of all data at line algorithm attempts to load the value of the variable which leads to a nameerror because the variable is undefined.
the runtime engine catches this error and instead queries the neural model for a suitable value to inject.
the model predicts non empty list which algorithm concretizes as shown in table into a list that contains a dummy object.
this injected value along with others injected later during the execution enable the code to execute despite some missing values.
implementation lexecutor is implemented as a fully automated tool that executes arbitrary python code snippets.
to implement the code instrumentation we build upon the libcst library 1which offers a parser and utilities for transforming the ast.
the neural models are based on codet5 made available by salesforce2and codebert made available by microsoft3 both accessed via the hugging face api.
due to hardware constraints we use the small codet5 model and the base mlm codebert model and fine tune them for ten epochs each.
to speed up the prediction of values our implementation loads the trained model once and accesses it via rest queries to an http server.
the experiments are conducted on three servers each with an nvidia tesla gpu p100 t100 and t4 respectively projects used for gathering training data.
project description unique value use events ansible automation of software infrastructure django web framework keras deep learning library requests http library rich text formatting in the terminal total table projects used for gathering functions.
project description functions loc black code formatting flask web applications pandas data analysis scrapy web scraping tensorflow deep learning total using 16gb of gpu memory per server.
deploying lexecutor does not require a powerful server except for running the neural model.
evaluation we address the following research questions rq1 how accurate is the neural model at predicting realistic runtime values?
rq2 how much code does an execution guided by lexecutor cover and how does the coverage compare to alternative ways of executing the code?
rq3 how efficient is lexecuting code?
rq4 as an application of the approach can we use lexecutor to identify semantics changing commits?
.
experimental setup .
.
datasets.
value use events.
to gather a corpus of value use events for training the neural model we execute the test suites of real world python projects.
we select projects that are popular based on the number of github stars cover different application domains and provide a test suite that is easy to execute yet covers a significant amount of the project s code.
table shows the selected projects and the number of unique value use events gathered from them.
in total the dataset consists of 226k entries which we shuffle and then split into for training and to answer rq1.
open source functions.
one usage scenario of lexecutor is to execute code that is part of a large project without having to set up the project and its dependencies and without having to provide inputs that reach the targeted code.
to evaluate lexecutor s effectiveness in this scenario we gather a dataset of functions extracted from open source python projects on github.
we searchlexecutor learning guided execution esec fse december san francisco ca usa for projects implemented in python and sort them by the number of their stars.
then from the top of the list we choose projects that cover different application domains and that are not used for constructing the dataset of value use events.
then for each of the projects listed in table we randomly select functions and extract the entire function body into a separate file.
the task of lexecutor is to execute the extracted code without access to any contextual information such as other code in the same project imports or third party libraries.
in total the dataset is composed of randomly selected functions that amount to non empty non comment lines of code.
stack overflow snippets.
another usage scenario is to execute code snippets that are inherently incomplete such as code posted in web forums.
to evaluate lexecutor in this scenario we gather a dataset of code snippets from stack overflow.
specifically we search for questions tagged with python and sort them by the number of votes.
then from each of the top questions we randomly select an answers and extract the code given in this answer.
after discarding code snippets with invalid syntax the final dataset consists of code snippets.
the total number of non empty non comment lines of code is lines.
.
.
baselines.
as is.
this baseline executes a code snippet with the standard python interpreter i.e.
without making any value predictions and instead letting the code crash whenever it tries to access a missing value.
naive value predictor.
this and the following two baselines are simplified variants of our approach which use the runtime engine to intercept exceptions caused by undefined