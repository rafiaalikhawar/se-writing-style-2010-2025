how to better utilize code graphs in semantic code search?
yucen shi shiyucen stumail.neu.edu.cn school of computer science and engineering northeastern university shenyang chinaying yin yinying cse.neu.edu.cn school of computer science and engineering northeastern university shenyang chinazhengkui wang zhengkui.wang singaporetech.edu.sg infocomm technology cluster singapore institute of technology singapore david lo davidlo smu.edu.sg school of information systems singapore management university singaporetao zhang tazhang must.edu.mo school of computer science and engineering macau university of science and technology macau chinaxin xia xin.xia acm.org software engineering application technology lab huawei hangzhou china yuhai zhao zhaoyuhai mail.neu.edu.cn school of computer science and engineering northeastern university shenyang chinabowen xu bowenxu.
phdcs.smu.edu.sg school of information systems singapore management university singapore abstract semantic code search greatly facilitates software reuse which enables users to find code snippets highly matching user specified natural language queries.
due to the rich expressive power of code graphs e.g.
control flow graph and program dependency graph both of the two mainstream research works i.e.
multi modal models and pre trained models have attempted to incorporate code graphs for code modelling.
however they still have some limitations first there is still much room for improvement in terms of search effectiveness.
second they have not fully considered the unique features of code graphs.
in this paper we propose a graph to sequence converter namely g2sc.
through converting the code graphs into lossless sequences g2scenables to address the problem of small graph learning using sequence feature learning and capture both the edges and nodes attribute information of code graphs.
thus the effectiveness of code search can be greatly improved.
in particular g2scfirst converts the code graph into a unique corresponding node sequence by a specific graph traversal strategy.
then it gets a statement sequence by replacing each node with its corresponding statement.
a set of carefully designed graph traversal strategies guarantee that the process is one to one and reversible.
g2scenables capturing rich semantic relationships i.e.
control flow data flow node relationship properties and provides learning model friendly data transformation.
yuhai zhao is the corresponding author permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november singapore singapore association for computing machinery.
acm isbn .
.
.
.
can be flexibly integrated with existing models to better utilize the code graphs.
as a proof of concept application we present two g2scenabled models gsmm g2scenabled multi modal model andgscodebert g2scenabled codebert model .
extensive experiment results on two real large scale datasets demonstrate that gsmm andgscodebert can greatly improve the state of the art models mman andgraphcodebert by92 and on r and and .
on mrr respectively.
ccs concepts software and its engineering reusability .
keywords semantic code search neural networks graph embedding acm reference format yucen shi ying yin zhengkui wang david lo tao zhang xin xia yuhai zhao and bowen xu.
.
how to better utilize code graphs in semantic code search?.
in proceedings of the 30th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november singapore singapore.
acm new york ny usa pages.
introduction semantic code search plays a vital role for software developers to serve various purposes especially code reuse which enables users to find code snippets highly matching user specified natural language queries.
for example to the query how to read an object from an xml?
code search engine returns a code snippet candidate like figure from a billion token scale code base e.g.
github .
the reuse of existing code avoids the necessity for developers to reinvent the wheel and save the time and resource cost during software development.
esec fse november singapore singapore yucen shi ying yin zhengkui wang david lo tao zhang xin xia yuhai zhao and bowen xu figure code snippet for query how to read an object from an xml ?
the task is non trivial as there is often a semantic gap between the high level intent in the natural language queries and the lowlevel implementation details in the source code .
for example the code snippet in figure contains none of the key words in the query such as read andobject or other similar texts.
thus those unimodal approaches that are based solely on keyword matching or text similarity such as information retrieval ir based code search approaches cannot address the challenge well.
recent research works have proposed various multi modal models based on deep learning techniques.
as the lexical gap between queries and source code is bridged by their semantic representation learning in high dimensional space they succeed in improving search effectiveness.
these works can be categorized into two main streams .
the multi modal models with pre training which are designed with transformer based architecture and guided by several pre training tasks to learn the generic representation of multi modal data and then fine tuned for a set of code related downstream tasks such as code search code summarization etc.
.
the multi modal models without pre training which are specifically trained for code search in an end to end way.
for simplicity we refer to the models in the above mentioned first and second main streams of work as pre trained models and multi modal models respectively.
codebert and dcs are the examples of pre trained models and multi modal models respectively.
though these models are more capable to capture more semantics they still suffer from using simple features of code and ignore the code structural information.
code graphs e.g.
abstract syntax trees asts or control flow graphs cfgs have been proven effective and expressive to capture the rich structural semantics of the source code .
the most recent research effort is made to leverage the code graphs in both of the two research streams.
for example in the multi modal model stream wan et al.
proposed a multi modal attention network mman by integrating cfgs in the model and adopted the graph neural networks gnns to learn the semantic information hidden in the code graphs followed by an attention layer to integrate the unstructured and structured features.
in the pre trained model stream graphcodebert introduced a graph guided masked attention function to incorporate the code structure that captures data flow.
however through an in depth study of the two main streams of work we have the following two findings that motivate our work in this paper.
.
both of them still have much room for improvement in the search effectiveness.
for example according to the original results reported inmman and graphcodebert the two models have achieved the state of the art in their respective tasks.
however on a widely used evaluation metric mean reciprocal rank mrr with a pdg of code in figure b node size of code graphs figure code graph pdg and its node distribution in two data sets java 2m and codesearchnet a maximum value of mman andgraphcodebert can only reach about .452and .
respectively.
compared to their respective counterpart dcs .
and codebert .
mrr is improved by only .075and .
respectively.
.
they both ignore the potential difficulties and challenges arising from the unique features of code graphs which contain diverse information and are usually small.
figure a shows the program dependency graph pdg of the code snippet in figure .
observably a pdg contains two kinds of structural information i.e.
control dependency the dashed edge for control flow and data dependency the solid edge for data flow as well as two kinds of attribute information i.e.
the node attribute the statement in each node and the edge attribute the parameter variable on a data dependency edge .
the multi modal model mman and the pre trained model graphcodebert incorporated the code structure that captures control flow or data flow into their learning task respectively.
however they still may potentially lose much valuable information since the control flow or the data flow alone can only capture the statement control relationship or the variable relationship.
more importantly the code graphs are often small in size.
as figures b shows most of the code graphs cfgs and pdgs extracted from two real data sets for code search codesearchnet and the data set we collected java 2m have only 5nodes and very few of them have more than 20nodes.
the multi modal model mman utilized gnns to embed cfgs as the structural features.
however existing research has shown that gnns often achieve better performance on the large graphs with 000to20 000nodes.
the two findings motivate us that the effectiveness of code search could be substantially improved if those unique features of code graph are more fully utilized in the semantic learning model.
in this paper to tackle the problem we propose a graph to sequence converter g2sc .
by converting the code graphs to lossless sequences that retain complete graph structure information g2sc enables to address the problem of using gnn for small graph learning through sequence feature learning and capture both the edges and nodes attribute information of code graphs.
in particular g2sc first converts the code graph into a unique corresponding node sequence by a specific graph traversal strategy.
then it obtains a 723how to better utilize code graphs in semantic code search?
esec fse november singapore singapore statement sequence by replacing each node with its corresponding statement.
a set of carefully designed graph traversal strategies guarantee that the process is one to one and reversible.
g2scenables capturing rich semantic relationships i.e.
control flow data flow node relationship properties and provides learning modelfriendly data transformation.
further to show that g2sc can be flexibly integrated with the state of the art models in the two research streams i.e.
the multi modal model and the pre trained model and greatly improve their effectiveness we present two g2sc enabled models gsmm g2sc enabled mman model and gscodebert g2sc enabled codebert model .
specifically gsmm adopts the bi directional long short term memory bilstm to learn the structured feature of code graphs from g2sc converted sequences followed by an attention layer to integrate the unstructured and structured features together.
gscodebert replaces the code sequence in the code description sequence with the g2sc converted sequence to further fine tune codebert model.
the replication package of our work has been released at github1.
the main contributions of our work are summarized as follows.
.
we present a graph to sequence converter g2sc which transforms the code graphs into lossless sequences.
through g2sc the structural information of code graphs can be effectively learned by using sequence feature learning.
to the best of our knowledge we are the first to introduce the idea into the semantic code search.
.
we propose two g2sc enabled semantic code search models g2sc enabled multi modal model gsmm and g2sc enabled codebert gscodebert .
they both demonstrate the generality and applicability of integrating g2sc into existing models to improve their capability of utilizing code graphs for semantic code search.
.
we have conducted extensive experiments to evaluate the proposed g2sc enabled models over various real datasets.
the results show that g2sc improves the state of the art models in the two streams substantially.
for example in terms of mrr gsmm improves dcs and mman by100 and respectively while gscodebert improves codebert andgraphcodebert by13 and .
respectively.
outline.
the remainder of this paper is organized as follows.
section introduces the research background of this paper.
in section we present an overview of g2sc .
section presents the two g2sc enabled models.
in section we provide experimental evaluations to address our research questions.
finally section and section provide the related work and conclusion respectively.
background in this section we introduce the overall process of deep learningbased semantic code search and four representative models from the two mainstream research i.e.
dcs mman codebert andgraphcodebert which are selected for comparison in our experiments.
.
deep learning based semantic code search as shown in figure the overall process of deep learning based semantic code search includes three main phases offline training offline code embedding and online code search.
first a deep neural network takes a large scale corpus of code description pairs as input and maps them into a unified high dimensional vector figure deep learning based code search process space that code snippets and descriptions with similar semantics are embedded into nearby vectors of the space the training phase .
then the trained model is used to compute a vector for each code snippet in a given codebase that users would like to search the embedding phase .
finally an online query e.g.
how to read an object from xml?
is also embedded into a vector when it arrives.
by computing vector similarity code snippets whose vectors are similar to the query vector such as the code snippet in figure are returned and recommended to users the online code search phase .
intuitively the quality of vector representation learned in the training phase determines the effectiveness of the solutions in performing code search.
to return code snippets sthat highly match a queryqin semantics the code search model needs to learn a correlationfbetweenqands namely f q s for this purpose the model training is expected to produce close representation of code snippets and their corresponding descriptions.
given a set of code snippets sand their natural language descriptions d the task in this phase can be formulated as d vd j vd vs vs s where d rdis an embedding function to map a code snippet description dinto ad dimensional vector space vas a vectorvd s rdis an embedding function to map a code snippet s into the same vector space as vs.j is a measure function e.g.
cosine similarity to score the similarity between vdandvs.
during the training the model is guided by the spirit of producing higher similarity between a code snippet sand its correct description d and lower similarities between sand its incorrect descriptions d simultaneously.
once the model is trained the correlation fbetweenqandscan be easily set up through their vectors embedded by functions and respectively.
.
mainstream models for code search our g2sc aims to improve both multi modal and pre trained models.
four representative works from two mainstream solution families are considered in this work including two multi modal models dcs and mman and two pre trained models codebert and graphcodebert .
a. multi modal models .
deep code search dcs dcs is the first deep learning based code search model proposed by gu et al.
it learns the code representation by extracting and fusing three aspects of information i.e.
multi layer perceptron mlp for the tokens contained in the 724esec fse november singapore singapore yucen shi ying yin zhengkui wang david lo tao zhang xin xia yuhai zhao and bowen xu figure overall framework of using g2sc for code search source code body tokens and recurrent neural networks rnn for the method names and the api sequences.
however dcs ignores the structure information that the code may contain.
.
multi modal attention network mman mman is the state of the art multi modal model proposed by wan et al which introduces the code structure information into the code search task .
it embeds asts by treelstm cfgs by ggnn gated graph neural network and code sequences by lstm followed by an attention fusion layer to integrate them into a single vector representing the entire code.
however mman ignores the fact that code graphs are usually small and contain diverse information e.g.
control dependency and data dependency in pdg .
b. pre trained models .
codebert codebert is the first pre trained model that can be used in code search proposed by feng et al.
inspired by bert it adopts the transformer architecture and uses two pre training tasks mlm masked language model and rtd replaced token detection to learn the general model parameters for a set of coderelated downstream tasks such as code search code summarization etc.
since it is not designed for the code search task alone the user needs to perform additional fine tuning to return the query result.
however codebert still ignores the code structure information.
.
graphcodebert graphcodebert is the state of the art pre trained model that can be used in code search proposed by guo et al.
it is an upgraded version of codebert introducing the variable graph structure that captures the data flow .
besides the mlm task in codebert it uses two more variable graph specific pre training tasks edge prediction and node alignment as well as a graph guided masked attention function.
however graphcodebert ignores rich semantic information other than the variable relationship such as the control relationship in figure a .
the graph to sequence converter the unique characteristics of the code graphs bring great challenges for the existing semantic code search models.
to improve their effectiveness it may be a promising way to ensure that the data can be supported and fit into the learning model nicely.
unfortunately as mentioned in section the capability of existing learning models even the gnn is not so friendly to learn the code graphs.
to tackle this issue we propose a graph to sequence converter g2sc based solution.
figure presents the overall framework ofusing g2sc in the code search model which is the core of the entire code search process.
once an effective code search model is trained we only need to apply g2sc in the later two phases in figure i.e.
offline code embedding and online code search to find code snippets highly matching user specified natural language queries.
as shown in figure our solution comprises two main components .
g2sc module.
it transforms the code graphs into a type of the model friendly data format code sequences that can losslessly retain the code graph structure information as well as the key attribute information related to the edges and nodes .
g2sc enabled model.
by learning the sequences converted by g2sc and other code information such as code textual information and natural language description through different carefully designed strategies we are able to develop various g2sc enabled models.
in this section we focus on the g2sc module.
it first converts the code graph into a unique corresponding node sequence by a specific graph traverse.
then it gets a statement sequence by replacing each node with its corresponding statement.
a set of carefully designed graph traversal rules guarantee that the process is one to one and reversible.
thus the graph structure information is retained in the converted sequence in a lossless manner.
for ease of demonstration we will explain the g2scby using pdgs.
one reason is that pdg is a more complex code graph with both control dependency and data dependency than others like cfg .
another reason is that pdg is used as the graph feature in our proposed models.
note that g2sccan be easily extended to handle different kinds of the code graphs like cfg etc.
algorithm provides the details of our graph to sequence converter algorithm.
in the first step given a pdg we need to identify the sequence entry node for the pdg sequence we want to generate.
since pdg only has one root node the root node v0of a pdg can be directly selected as the entry s0of the entire pdg sequence algorithm line .
after that starting from node s0 we recursively select an edge for expansion according to the following rules in order .
the backtracking edge the edge whose endpoint has been traversed is selected before the forward edge the edge whose endpoint has not been traversed .
when there are multiple backtracking edges or only multiple forward edges we can either choose control dependency edge or data dependency edge for the traversal after the first step.
either priority can produce a unique result.
in our graph to sequence algorithm we set control dependency edges to have higher priority algorithm line .
if the edge still cannot be determined after the second step we select the edge whose endpoint is of the highest priority as the preferentially expanded edge.
in our method the priority of nodes is determined by the order of their corresponding statements in the code snippet algorithm function edge check .
in particular if there are no connected edges the traversal continues back to the nearest ancestor node with at least one edge that has not been traversed algorithm line .
during traversing the pdg we can follow the following two steps to generate our proposed pdg sequence algorithm function sequence expand if the traversed edge eis a control dependency edge we only add the attribute of the node pointed by eto the resultant pdg sequence s if the traversed edge eis a data dependency edge we add the attribute ofeand the attribute of the node pointed by etosin turn.
725how to better utilize code graphs in semantic code search?
esec fse november singapore singapore algorithm graph to sequence algorithm input graphg v e v v0 ... vn e e0 ... em output minmum pdg sequence s s0 s1 ... 1initializes i appendv0ass0intos 2whileeis not null do select edgeeconnect with si ife contains backtracking edge then select the backtracking edge as e sequence expand edge check e s else ifecontains forward edge then sequence expand edge check e s else ifeis null then select the nearest father of siwhich has forward edgee ase sequence expand edge check e s function sequence expand e s 1ife is control dependency edge then 2s e v and remove efrome e vmeans the node v vthat e points to i i 5else if e is data dependency edge then 6s e e v and remove efrome i i function edge check e 1ifthere is only one edge then select the edge as e 3else if there are multiple edges then ifthere exists multiple control dependency edges then compare all control dependency edge ex vxand select the minmum condition exase else compare all ex vxand select the minmum conditionexase figure process of converting pdg to sequence here we give an example in figure to illustrate how to convert a pdg to its unique corresponding sequence.
given a pdg in figure the order of nodes is the order in which they are executed in the program the dashed line represents the control dependency edge and the solid line represents the data dependency edge with its attribute ei.
the first traversal is adbsince root node ais the program entry.
the second traversal is bdc1because the control dependency edge needs to be traversed first according to ourproposed edge expansion rule.
the third traversal is c1e1 a where the attribute of the edge is e1 because we define the backtracking edge has the highest priority.
in the fourth traversal we go back to c1 so the traversal is c1dd.
in the fifth traversal we traverse the backtracking edge from d which isde2 b where the attribute of the edge ise2.
after that since dandc1have no subsequent node we returnbfor the last traversal of be3 c2 where the attribute of the edge ise3.
so far we can obtain the sequence of edges of pdg by the traversal process shown in figure .
in the pdg sequence the control dependency edge has no attributes since it only means the control dependency between two nodes.
then we use the number in the lower right corner of each box to represent the sequence it corresponds to e.g.
represents sequence a b and represents sequence c1 e1 a .
finally we can obtain the pdg sequence abbc 1c1e1ac1dde 2be3c2 by connecting all circles in digital order.
note the graph to sequence algorithm is also reversible.
given the pdg sequence obtained above we can restore the traversal process by picking two adjacent nodes at a time from scratch e.g.
first a b second b c third c1 e1 a and so on .
after that we only need to reconnect the edges in the order of the numbers in the circle to regain the pdg in figure .
since the process is reversible no information will be lost when converting the sequence through our graph to sequence algorithm.
it should be noted that the graph to sequence algorithm only requires linear time consumption o e which means it does not need much time for pdg preprocessing.
4g2sc enabled models the g2sc enabled model is the second component of our solution.
g2scis designed as a general converter which can empower various learning models to integrate the code graphs for effective code search.
thus it can be easily incorporated into other models.
in this section we take the two mainstream models as examples to present two g2sc enabled models namely gsmm g2sc enabled multi modal model and gscodebert g2sc enabled codebert model .
.
data pre processing the pre processing is performed based on a large scale training corpus.
for multi modal models we first extract a collection of pairs of code snippets and their corresponding descriptions.
for the description part we extract all the words and sort them by java word split api2to get a token sequence.
for the code part we adopt the same method for extracting body token tokens in function body and method name function name from code snippet.
similar to dcs we treat tokens as unordered ones in the source code.
in addition we extract the pdgs from code snippets through an extraction tool tinypdg3.
then all pdgs are converted to pdg sequences using ourg2sc proposed in section .
for the pre trained models codebert and graphcodebert both provide pre processing source programs in their downstream tasks.
therefore we can directly utilize them in our gscodebert.
726esec fse november singapore singapore yucen shi ying yin zhengkui wang david lo tao zhang xin xia yuhai zhao and bowen xu figure the pipeline of gsmm .
the red box is our converter g2sc .
.
g2sc enabled multi modal model first we present g2sc enabled multi modal model gsmm .
in particular gsmm utilizes mlp to learn body tokens bilstm to learn method name and pdg sequence and adopts an attention mechanism to combine the three code features.
the main difference between gsmm and other multi modal models is that gsmm uses g2sc converted pdg sequence for code graph structure learning.
the overall framework of gsmm is shown in figure .
the input code snippets is represented as c c1 c2 ... cn whereciis theithcode snippet.
for each code snippet ci it is denoted as the combination of its method names body tokens and pdg ci mi ti pi wheremirepresents the method name in an ordered sequence of tokens ofci tiis the body tokens that is a token set and pi vi ei is the pdg.
gsmm first embeds the code snippets through three components text level representation graph level representation and attention based fusion and then collects the embedding of natural language description by the description representation component.
finally the correlation between code snippets and their natural language descriptions is inferred by model training .
next we present the five steps in detail respectively.
text level representation the text level representation model of gsmm is similar to that of dcs.
it treats the method name and the token in the code snippet body as the text level.
for the method namem m1 m2 ... mnm decomposed as a sequence of tokens gsmm embeds the sequence of camel split tokens using a bilstm with maxpooling ht lstm ht mt t nm ht lstm ht mnm t t nm m maxpooling where is the word embedding layer to embed each method name token mtinto ad dimensional vector htand htrepresent the hidden states of the forward and backward lstm in bilstm respectively r2drepresents the concatenation of two vectors.
the final hidden state htof bilstm is jointly represented by thecorresponding htand ht.
a method name is thus embedded as a ddimensional vector m. as for the body tokens t t1 t2 ... tnt considering that dcs considers them to have no strict order in the source code we also embed them via a mlp layer hi tanh wt ti i ... nt t maxpooling where embeds each token tiinto ad dimensional vector and wtis the matrix of trainable parameters.
all hidden states hiare summarized to a single vector tfor the body tokens by maxpooling.
graph level representation given a pdg sequence p p1 p2 pnp which is already processed by our g2sc we adopt another bilstm with maxpooling to embed it ht lstm ht pt t np ht lstm ht pnp t t np p maxpooling where is the word embedding layer to embed each token pt in pdg sequence into a d dimensional vector.
in addition gsmm gets the final pdg sequence representation pthrough maxpooling similar to the method name.
attention based fusion layer finally the vectors of three aspects are fused into one vector through an attention fusion layer i ei em et epi m t p c m m t t p p where irepresents the attention weight of each vector.
the output vectorcrepresents the final embedding of the code snippet.
description representation for the natural language description d d1 d2 ... dnd gsmm embeds the sequence of camel split tokens using a bilstm with maxpooling ht lstm ht dt t nd ht lstm ht dnd t t nd d maxpooling 727how to better utilize code graphs in semantic code search?
esec fse november singapore singapore where is the word embedding layer to embed each description worddtinto ad dimensional vector.
in addition gsmm gets the final description representation dthrough maxpooling.
model training we follow gu and wan to train the gsmm model to jointly embed code snippet and description into the intermediate semantic space with similar coordination.
the goal of joint representation is that if a code snippet and a description have similar semantics their embedding vectors are expected to be close to each other.
in other words given an arbitrary code snippet c and an arbitrary description d gsmm predicts the similarity of candd.
during the training we feed a triple c d d into the model.
for each code snippet c there is a positive description d a correct description of c as well as a negative description an incorrect description of c d randomly chosen from the pool of all d s. for any given a set of c d d triples the gsmm predicts the cosine similarities of both c d and c d pairs and minimizes the hinge range loss l c d d pmax cos c d cos c d where denotes the model parameters pdenotes the training dataset is a slack variable commonly used in machine learning and often set as .
c d andd are the embedded vectors of c d andd .
the intuition behind the ranking loss is that pushing the model to predict a high similarity between a code snippet and its correct description and a low similarities between a code snippet and incorrect descriptions.
.
g2sc enabled codebert model next we present g2sc enabled codebert model gscodebert .
like other pre trained models codebert consists of two stages pre training and fine tuning.
as the source code of the pre training codebert is not released we only use g2sc in the downstream task of codebert .
figure provides the overall pipeline of gscodebert .
each input code snippet is decomposed into two pairs d s and d p .
d d1 dn is the sequence of tokens di i n which are extracted from the natural language description dof code snippet.s s1 sm is the sequence of tokens sj j m which are extracted from the program language code sequence s. andp p1 pk is the sequence of tokens pt t k extracted from our g2scconverted pdg sequence p. all d s pairs and d p pairs are respectively fed into the pre trained codebert model with a special token indicating the semantic relevance between dands orp as input to the downstream fine tuning task.
this enables our gscodebert model to increase its understanding of code structure information while retaining codebert s understanding of natural language and code sequence.
model fine tuning we fine tune the model with a binary classification loss function where a softmax layer is used to map the output vector classlabel to a score between .
a higher score indicates the greater similarity between the code sequence pdg sequence and natural language.
our fine tuning task consists of two parts.
one is for the similarity between natural language descriptiondand the pdg sequence p and another is between dand the code sequence s. both of them utilize a similar loss function formulated as follows figure the pipeline of gscodebert .
l d x emax score d x score d x score d x where denotes the model parameters edenotes the training set ddenotes natural language description and xcan be code sequence sor pdg sequence p is a slack variable similar to that in equation is a special token indicating if d x is a positive or a negative sample.
all d x pairs are taken as the positive samples.
both d x and d x are negative samples which have the balanced number of instances and are created by randomly replacingdorxin d x withd orx respectively.
the function score is used to calculate the similarity.
our loss functions aim to increase resp.
decrease the similarity score between a sequence x and its correct resp.
incorrect description d. .
code search after the model is trained for multi modal models with a given code basecand a given query q the target is to rank all these code snippets by their similarities with query q. we first feed the code snippet cinto gsmm and feed the query qas the description to obtain their corresponding representations denoted as candq.
then we calculate the ranking score as follows sim c q cos c q ctq c q wherecandqare the vectors of code and query respectively.
the higher the similarity the more semantic related the code to the query.
for each query gsmm returns top most related results.
for pre trained models as described in section .
for each query q we feedqpaired with all code sequence c cto the model.
the input is formalized as q c .
after codebert processing the tag embedding corresponding to is returned.
further we calculate the similarity through the trained downstream classifier shown below sim c q dt codebert c q 728esec fse november singapore singapore yucen shi ying yin zhengkui wang david lo tao zhang xin xia yuhai zhao and bowen xu wherecandqare the sequences of code and a query respectively.
is a special token corresponding to each c q pair.dt represents the downstream task that scores the class label processed by codebert.
in addition the higher similarity indicates a stronger match between the code and the query.
experiment .
research questions our experiment is guided by answering the following research questions rq1 how much can g2sc improve the state of theart multi modal model for semantic code search?
we compare our gsmm with dcs andmman in code search on two real data sets to prove g2sc can effectively improve the effectiveness of multi modal models.
rq2 how much can g2sc improve the state of theart codebert based model for semantic code search?
we boost codebert by integrating our pdg sequence and compare our gscodebert with the state of the art pre trained models codebert andgraphcodebert .
rq3 how do our proposed models behave using other features converted by g2sc ?
since there are some other code features that can be used by g2sc we design a series of experiments to show the benefit of choosing text information and pdg for both the multi modal model and the pre trained model.
.
experiment setup .
.
data set.
in our experiment we have used two different datasets including codesearchnet and java 2m.
following and we construct our first dataset based on the source codes from codesearchnet as our first data set.
in particular the original dataset contains java code snippets.
after removing code snippets for which their ast api cfg or pdg cannot be extracted we retain snippets as the final training dataset.
to further investigate the models capability of handling largescale dataset following we construct our second larger data set java 2m by following three steps .
we download the github repositories published from august to september with the programming language labeled as java through github api.
on the basis of we set the minimum number of stars of the repositories as to ensure the code is of substantial quality.
.
to provide models code snippets with natural language description we retain those java methods extracted from the repositories with english annotation before the code snippet.
and we remove the method without annotation or annotated by other languages.
as a result each method consists of code snippet and its corresponding natural language annotation.
.
we extract the api ast cfg and pdg from all the extracted methods and drop those without these information.
in the end java 2m contains java methods.
for the codebase we use the same code base as codebert provided by husain et al.
and retain the methods with all the api ast cfg and pdg information.
these methods are different from the training corpus as they are considered in isolation and contain codes without descriptions.
in the end we get method candidates as our code search base.
.
.
experiment setting.
to train gsmm we set the batch size as .
we set the size of vocabulary to to ensure the word coverage rate of the corpus reaches more than .
inspired by we set the maximum length of method name natural language and token to and respectively.
we truncate the sequence whose length is beyond the maximum length.
the sequences below the maximum length are padded with a special token pad to the maximum length.
for lstm unit we set the hidden size to be .
similarly for mlp the embedding vector dimension is set to .
we update the parameters by utilizing adam optimizer.
to make the model with better generalization ability we set the dropout rate as .
.
all the experiments are implemented using the pytorch .
with python .
and the experiments were conducted on a server with nvidia rtx 2080ti.
we follow the same parameters released by the original authors and apply them to all the considered algorithms for a fair comparison dcs4 codebert5 and graphcodebert5 .
.
.
evaluation metric.
to measure the effectiveness of the approaches we employ three widely used metrics including frank successrate k and mrr .
frank also known as best hit rank is the rank of the first hit result in the result list.
it is practical measurement as it considers the pattern that users often scan the results from top to bottom.
a smaller frank implies lower inspection effort for finding the desired result.
successrate k r k also known as success percentage at k measures the percentage of queries for which more than one correct result could exist in the top kranked results.
the higher the successrate kis the better the code search performance is.
mrr denotes the mean of the reciprocal ranks of results for a given set of queries q. the reciprocal rank of a query is the inverse of the rank of the first hit result.
and the higher the mrr value is the better is the code search performance.
.
.
evaluation methodology.
for rq1 when comparing multimodal models we follow dcs and use human judgement.
for human judgement we use the java questions provided by dcs.
the manual analysis was performed independencyly by graduate students with years of experience in java and the developers performed an open discussion to resolve conflict grades for the questions.
to test the statistical significance we also apply wilcoxon signed rank test p .
for the comparison of frank for all the queries.
we conservatively treat the frank as for queries that fail to obtain relevant results within the top returned results.
the p values for the comparisons are all lower than .
indicating the improvement of gsmm over the related approaches is significant.
for rq2 note that codebert was pre trained on codesearchnet we also use the codesearchnet to train other pre trained models to facilitate a fair comparison.
then we employ the same automatic evaluation method that was used to evaluate original codebert .
for rq3 since both multi modal models and pre trained models are considered we use human judgement and automatic evaluation simultaneously.
729how to better utilize code graphs in semantic code search?
esec fse november singapore singapore .
answer to rq1 to answer rq1 we measure the effectiveness difference between gsmm gsmm w o.a gsmm without attention dcs andmman on two real data sets codesearchnet and java 2m.
table gsmm dcs and mman search effectiveness on codesearchnet model r r r mrr dcs .
.
.
.
mman .
.
.
.
gsmm w o.a .
.
.
.
gsmm .
.
.
.
table gsmm dcs and mman search effectiveness on java 2m model r r r mrr dcs .
.
.
.
mman .
.
.
.
gsmm w o.a .
.
.
.
gsmm .
.
.
.
table shows the performance of gsmm dcs andmman on codesearchnet.
the results demonstrate that gsmm consistently outperforms mman and dcs on all the evaluation metrics.
for example gsmm performs better than mman by and in terms of mrr r r and r respectively.
gsmm outperforms dcs by and respectively in terms of mrr r r and r .
we conduct the experiment on java 2m by following the same setting.
as shown in table gsmm shows significant improvement compared with dcs andmman on all the metrics.
for example gsmm improves mman by92 and on r r r and mrr respectively while it outperforms dcs by150 and respectively in terms of r r r and mrr.
further the frank statistics in table shows that gsmm has less nf not found than the two baselines.
this means gsmm can answer more questions and the correct code snippets recommended bygsmm are ranked higher in the recommended list.
to further investigate the performance improvement of gsmm we compare gsmm w o.a the version for gsmm without attention mechanism with the baselines shown in table and table .
from the results we can see that even without attention mechanism gsmm still outperforms other baselines like dcs mman .
for example gsmm w o.a is and higher than dcs andmman respectively in terms of mrr on the codesearchnet dataset and and higher than dcs andmman in terms of mrr on the java2m dataset.
this further verifies the improvement of gsmm s search effectiveness is not only from attention mechanism but also the features learned from code graphs.
moreover the results show that gsmm outperforms the two baselines significantly.
the potential reason for gsmm outperforming dcs could be that gsmm learns richer semantic information from the code graph using g2sc while dcs cannot utilize codetable frank list of first java questions to gsmm mman anddcs onjava 2m query dcs mman gsmm .
convert an inputstream to string .
create arraylist from array .
iterate through a hashmap nf .
generate random integers in a specific range .
converting string to int in java nf nf nf .
initialization of an array in one line .
how can i test if an array contains certain value .
lookup enum by string value nf nf .
breaking out of nested loops in java nf nf nf .
how to declare an array .
how to generate a random alpha numeric string .
what is simplest way to print a java array nf nf nf .
sort a map by values .
fastest way to determine if an integer s square root is an integernf nf nf .
how can i concatenate two arrays in java .
how do i create a java string from the contents of a file .
how can i convert a stack trace to a string .
how do i compare strings in java .
how to split a string in java nf .
how to create a file and write to a file in java .
how can i initialize a static map .
iterating through a collection avoiding concurrent modification exception when removing in loopnf nf .
how can i generate an md5 hash .
get current stack trace in java .
sort arraylist of custom objects by property .
how to round a number to n decimal places in java nf .
how can i pad an integers with zeros on the left nf nf nf .
how to create a generic array in java .
reading a plain text file in java .
a for loop to iterate over enum in java nf nf nf .
check if at least two out of three booleans are true nf nf nf .
how do i convert int to string nf nf .
how to convert a char to a string in java nf nf .
how do i check if a file exists in java .
java string to date conversion .
convert inputstream to byte array in java .
how to check if a string is numeric in java nf .
how do i copy an object in java .
how do i time a method s execution in java .
how to read a large text file line by line using java .
how to make a new list in java nf nf .
how to append text to an existing file in java .
converting iso compliant string to date .
what is the best way to filter a java collection .
removing whitespace from strings in java .
how do i split a string with any whitespace chars as delimiters .
in java what is the best way to determine the size of an object .
how do i invoke a java method when given the method name as a string7 .
how do i get a platform dependency new line character .
how to convert a map to list in java nf nf graph information.
the reason why gsmm is better than mman is twofold.
first the size of code graphs are usually small.
as demonstrated in section gnn s generally perform better on large scale graphs.
benefiting from g2sc gsmm can address this problem by performing a specific traversal of the code graphs and convert them into sequences in a lossless manner that can be easily learned by bilstm .
another reason is mman does not utilize node attribute information in the code graph by gnn .
the attribute of each node in the code graph is the statement corresponding to the node the information of which is exactly what attribute graph embedding needs.
g2sc can not only keep the structure information between nodes but also the order of tokens inside the node.
thus the information of code statements can also be retained intact.
answer to rq1 gsmm outperforms the state of the art model mman anddcs significantly in terms of all the evaluation metrics for code search.
730esec fse november singapore singapore yucen shi ying yin zhengkui wang david lo tao zhang xin xia yuhai zhao and bowen xu .
answer to rq2 to explore if our g2sc can improve pre trained models we choose two recent and well known models codebert and graphcodebert for comparison.
as described in section .
gscodebert is fine tuned based on both code sequences and the pdg sequences converted by g2sc .
table codebert graphcodebert and gscodebert results on codesearchnet model r r r mrr codebert .
.
.
.
graphcodebert .
.
.
.
gscodebert .
.
.
.
table presents the performance of codebert graphcodebert and our approach gscodebert .
the results show gscodebert achieves a significant better performance than codebert e.g.
by and .
in terms of mrr and r .
the potential reason may be codebert can further leverage the structure information in code snippet to learn a better mapping between code snippets and natural language with the support of g2sc .
and compared with graphcodebert gscodebert achieves better performance on all the metrics consistently e.g.
.
in mrr and in r .
the reason may be graphcodebert uses a self created variable graph that only contains the structure information between variables.
differently our pdg sequence carries richer information such as control dependency and data dependency of the code snippet.
answer to rq2 by integrating g2sc into codebert the performance can be significantly improved for code search even better than graphcodebert by large margin in terms of r and mrr.
.
answer to rq3 to explore the potential best features combination in our models we design an experiment of comparing commonly used code features with multiple combinations.
in particular we consider five types of code features text api ast cfg and pdg.
we use g2sc to convert code structural representation into code sequences and use the bilstm for feature embedding.
for ast we follow and preserve the properties of each node to obtain the sequence properties of the ast rather than the original sequence of the code.
for ast and cfg since none of their edges has attributes we ignore the related judgments of edges attributes in g2sc .
table presents the comparison between five features and four combinations in gsmm .
the results show that gsmm achieves the best performance with pdg among all.
moreover we find that both cfg and pdg perform better than code text information.
this means that graphs contain richer semantic information.
however we can still observe that text contribute its own to performance.
in particular the combination text pdg performs the best among all the considered combinations.
in table gscodebert text denotes the original codebert .
we observe the similar result with gsmm the combination text pdg table comparison of features and their combination in gsmm features r r r mrr text methodname token .
.
.
.
api .
.
.
.
ast .
.
.
.
cfg .
.
.
.
pdg .
.
.
.
text api .
.
.
.
text ast .
.
.
.
text cfg .
.
.
.
text pdg .
.
.
.
table comparison of features and their combination in gscodebert features r r r mrr gscodebert text .
.
.
.
gscodebert text api .
.
.
.
gscodebert text ast .
.
.
.
gscodebert text cfg .
.
.
.
gscodebert text pdg .
.
.
.
achieves the best performance while text cfg performs the second best.
in particular the combination text pdg outperforms text by and .
in terms of mrr and r .
answer to rq3 adopting g2sc algorithm to convert pdg is more effective than other code features in performing semantic code search.
moreover the combination of feature text and pdg can achieve promising results in both multimodal models and pre trained models.
.
threats to validity in this paper we analyse the performance of all models on codesearchnet and multi modal models on java 2m .
the two real data sets both are collected from github.
the training sets contains the source codes with the corresponding description while the search code base provided by contains all the source codes including codes without description .
according to the report in we believe the threat of overfitting for this overlap is not significant.
the queries collected from stack overflow can further meet the real world search and those are not descriptions of code snippets used for training.
in our experiments the human evaluation of code snippets is manually done and could suffer from human bias.
to mitigate this threat we have taken the below two prevention measures .
we randomly divide the code snippets into three parts on average and distribute to three developers .
the developers hold an open discussion to mitigate the manual bias on queries.
related work code search.
in the code search task many studies focus on api recommendation .
li van and nguyen 731how to better utilize code graphs in semantic code search?
esec fse november singapore singapore represented the code snippet as an api set and represented the api features based on word2vec model to recommend the appropriate api.
chan et al.
recently with the growth of opensource code repositories such as github it becomes quite common for developers to search relevant code snippets based on their own requirements.
some code retrieval tools have appeared .
traditional code searching tools are based on keyword matching.
in particular code snippets and natural language queries are regarded as token sets and appropriate code snippets are recommended according to the similarity of keywords .
lu et al extended the query semantically through wordnet.
gvero et al.
allow mixed input in english and java and constructed a probabilistic context free grammar for java constructs and library invocation.
keivanloo et al.
used a code cloning detection model to support spotting working code examples.
with the successful development of deep learning gu et al.
first applied deep learning technology in code search and proposed dcs model.
wan et al.
proposed mman to further introduce the graph information of the code into the code retrieval task.
in addition to code features api ast and graph and attention researchers also improved the code search model from other directions.
yao et al.
introduced reinforcement learning mechanism and improved the search effect of existing code search models by combining with them the performance of reinforcement learning model alone is not as good as dcs.
ye et al.
connected code summary task with code retrieval task and learned the two tasks simultaneously through dual learning method to improve the effect of code summary and code search.
deep code representation.
in software engineering there are also other code related works such as bug location code summary clone detection and code completion .
the introduction of code structure information is also applied in these deep code representation tasks.
code2vec converted code snippets into asts extracted path information of all leaf nodes and learned code features through attention mechanism to predict the function method names.
mou et al.
used convolution neural network based on tree structure to capture the features of neighbour nodes in ast and obtained the semantic information for program classification and source code similarity detection.
allamanis et al.
took ast as the structure backbone added data flow information and side information on the basis of ast to transform ast into a graph containing more information and applied ggnn to embed it for code variable misuse task.
wang et al.
present codet5 for code related generation task which is a pre trained model based on t5 .
in addition codebert and graphcodebert both designed a pre trained model to learn the relationship between programming language and natural language.
they can be applied to various tasks in code representation through different downstream tasks such as the code search task described in this paper.
conclusion and future work in this paper to better learn the code graphs we proposed g2sc an algorithm that can convert graphs to lossless sequences for semantic code search.
g2sc transforms the graph into a special sequence that is able to retain the graph information for representation learning.to the best of our knowledge this is the first time to introduce such a graph to sequence algorithm into the semantic code search task which can be effectively learnt using the deep neural network to capture the structure information on the data representation.
our experimental results show that by inducing g2sc into multi modal model gsmm outperforms the state of the art multi modal models significantly in terms of code search effectiveness.
additionally we added g2sc to the downstream task of the pre trained model and achieved a great improvement in code search performance.
in the future we will further try to apply our g2sc to other kinds of code search models.
and we will try to investigate how well our proposed g2sc can cooperate with those models.