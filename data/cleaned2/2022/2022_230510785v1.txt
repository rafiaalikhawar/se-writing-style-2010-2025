cct5 a code change oriented pre trained model bo lin linbo19 nudt.edu.cn college of computer science national university of defense technology changsha chinashangwen wang wangshangwen13 nudt.edu.cn college of computer science national university of defense technology changsha chinazhongxin liu liu zx zju.edu.cn zhejiang university hangzhou china yepang liu liuyp1 sustech.edu.cn department of computer science and engineering southern university of science and technology shenzhen chinaxin xia xin.xia acm.org zhejiang university hangzhou chinaxiaoguang mao xgmao nudt.edu.cn college of computer science national university of defense technology changsha china abstract software is constantly changing requiring developers to perform several derived tasks in a timely manner such as writing a description for the intention of the code change or identifying the defect prone code changes.
considering that the cost of dealing with these tasks can account for a large proportion typically around percent of the total development expenditure automating such processes will significantly lighten the burdens of developers.
to achieve such a target existing approaches mainly rely on training deep learning models from scratch or fine tuning existing pretrained models on such tasks both of which have weaknesses.
specifically the former uses comparatively small scale labelled data for training making it difficult to learn and exploit the domain knowledge of programming language hidden in the large amount unlabelled code in the wild the latter is hard to fully leverage the learned knowledge of the pre trained model as existing pre trained models are designed to encode a single code snippet rather than a code change i.e.
the difference between two code snippets .
we propose to pre train a model specially designed for code changes to better support developers in software maintenance.
to this end we first collect a large scale dataset containing .5m pairwise data of code changes and commit messages.
based on these data we curate five different tasks for pre training which equip the model with diverse domain knowledge about code changes.
we fine tune the pre trained model cct5 on three widely studied tasks incurred by shangwen wang is the corresponding author.
bo lin shangwen wang and xiaoguang mao are with the key laboratory of software engineering for complex systems.
yepang liu is with the research institute of trustworthy autonoumous systems.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november san francisco usa association for computing machinery.
acm isbn x xxxx xxxx x yy mm.
.
.
.
changes and two tasks specific to the code review process.
results show that cct5 outperforms both conventional deep learning approaches and existing pre trained models on these tasks.
ccs concepts software and its engineering software maintenance tools maintaining software software evolution .
keywords code change pre training deep learning.
acm reference format bo lin shangwen wang zhongxin liu yepang liu xin xia and xiaoguang mao.
.
cct5 a code change oriented pre trained model .
inproceedings of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering esec fse .
acm new york ny usa pages.
xxxxxxx introduction software undergoes continuous changes during the maintenance phase to fix defects change execution logic make the processing more efficient or introduce new features .
because of the omnipresence of code changes developers need to deal with a number of derived tasks referred to as code change related tasks in this paper .
for instance the interview conducted by fritz and murphy showed that developers have to frequently answer the question why were the code changes introduced?
in their daily development tasks.
after comprehending the intention of code change they may further need to estimate the impact of software changes troubleshoot unexpected behavior monitor the maintenance of code clones or update the associated code comments .
statistics have shown that maintaining software systems demands possibly as high as percent of the total development efforts .
furthermore a user study indicates that there is an urgent need of tool support for code change related tasks such as commit message generation and defect prediction .
prior works proposed to leverage deep learning dl techniques to deal with code change related tasks and have achieved promising results .
currently there are typically two ways to applyesec fse november san francisco usa bo lin shangwen wang zhongxin liu yepang liu xin xia and xiaoguang mao deep learning techniques.
the first is to train deep learning models from scratch with a comparatively small scale manually labelled and task specific dataset referred to as non pre training in this paper .
despite the progress such approaches have achieved their effectiveness sometimes cannot be as promising as expected possibly because of the lack of training data and there is still a large room for improvement .
for instance liu et al.
found that when generating commit messages that describe the intention of code changes a simple heuristic can outperform a sophisticated dl model and save a lot of computing resources at the same time.
another way is to leverage the state of the art pre training paradigm where the models are first pre trained on large scale unlabelled datasets to be equipped with domain knowledge of programming language pl and natural language nl and then fine tuned on various downstream tasks .
by so the parameters of the trained model can store some common knowledge compared with random initialization.
specifically one can fine tune existing pre trained models on code change related tasks .
however existing pre training techniques are mainly targeted at tasks related to encoding and understanding a given code snippet referred to as code related tasks in this paper such as code search code summarization and defect detection .
specifically the designed pre training tasks e.g.
the masked language modeling in codebert typically take a code snippet and its paired documentation i.e.
the comment as inputs.
thus the learned domain knowledge is generally related to the syntactic and semantic information of code snippets which can hardly be exploited to encode and understand code changes.
that is to say adopting the second way will inevitably lead to inconsistent inputs and objectives between pre training and fine tuning since code related and code change related tasks have natural differences the former deals with a code snippet and the key challenge is to capture the syntactic and semantic information of a code snippet while the latter deals with two code snippets and the key challenge is to understand the differences.
consequently it is sub optimal to fine tune existing pre trained code models for code change related tasks .
to help developers better deal with code changes and address the limitations mentioned above our basic idea is that models can be equipped with different domain knowledge and thus be applied to different tasks if they are pre trained by different tasks .
therefore we propose a code change oriented pretrained model cct5 which is built on top of the well known textto text transfer transformer t5 model but pre trained with code change specific inputs and objectives.
cct5 mainly embodies two advantages first by adopting the pre training paradigm the domain knowledge of code changes hidden in a large amount of unlabelled data can be absorbed by the model.
second by designing specific pre training tasks for code changes the domain knowledge learned by the pre trained model can be easily transferred to code change related downstream tasks.
to achieve our target we first build a large scale dataset named codechangenet for pre training.
specifically we collect .5m pairwise data of code change and commit message from popular github projects written in six widely used programming languages.
as the nl description of the code change during pre training the commit message playsa similar role to the code comment in existing pre training techniques.
after that we design five different pre training tasks for learning the domain knowledge about code changes which take as inputs the differences between two code snippets i.e.
code diffs and enable the model to align the nl and pl representations generate fluent nl descriptions and complete code snippets and be aware of the program structure respectively.
to evaluate the effectiveness of cct5 we fine tune it on three widely studied downstream tasks which are commit message generation just in time comment update and just in time defect prediction .
results show that cct5 outperforms both the state of the art non pre training techniques and existing pretrained models on the three tasks consistently.
for instance on a large scale multi linguistic benchmark for the commit message generation task cct5 outperforms nngen the state of the art non pre training technique and codet5 the state of the art pretrained model by and respectively in terms of the bleu values.
besides cct5 can also generalize well to code review tasks it outperforms codereviewer a recently proposed pre training technique specially designed for the code review process on the code change quality estimation and review generation tasks.
in summary our study makes the following contributions dataset we collect and release a large scale dataset for performing code change oriented pre training i.e.
codechangenet.
cct5 we propose totally five carefully curated pre training tasks based on which we release the first code change oriented pretrained model i.e.
cct5 .
performance assessment we perform extensive experiments to assess the performance of cct5 .cct5 achieves the state of theart performance on three widely studied code change related tasks and two tasks specific to the code review process.
background and related works .
code change and its related tasks software is constantly changing as new features are added bugs are fixed and performance is enhanced .
specifically it has been shown that the linux kernel changes .
times per hour .
in another study jiang et al.
found that there are over 2m code changes in the most popular 1k open source repositories on github .
both the above studies demonstrate the widespread existence of code changes.
along with the changes of code developers need to solve many derived tasks.
for instance a developer may need to write a description for the code changes he she made for better communication among the development team or in some other cases he she may need to check if a code change made by others will induce program defects or not .
given that it is not surprising that the cost of program maintenance can reach around of the total expenditure as reported by lehman .
therefore techniques that can automate such code change related tasks hold great potential to boost developers productivity.
.
code change representation techniques previously studies have demonstrated that compared with conventional heuristic based approaches deep learning techniques can perform better on code change related tasks .
consequently to deal with code change related tasks recent studiescct5 a code change oriented pre trained model esec fse november san francisco usa focus on learning the distributed representations of code changes using dl techniques.
cc2vec adopts a hierarchical attention network to successively build vector representations for code lines code chunks and finally the entire code change.
it achieves state of the art performance on a number of code change related tasks such as just intime defect prediction .
yin et al.
experimented with two different ways to embed a code change treating it as a code token sequence and treating it as the difference between two abstract syntax trees asts .
the results show that the representations could be better if the structure level information is involved.
recent studies also utilize the ast path technique for representing the code change where the ast is split into changed and unchanged parts and the paths connecting different leaf nodes are embedded to represent each part.
despite the progress achieved by these techniques they are all trained from scratch with a comparatively small scale labelled dataset.
it is hard for them to learn and use the domain knowledge of programming language hidden in the large amount of unlabelled code in the wild which limits their effectiveness .
.
existing pre training techniques training a deep learning model from scratch requires a large amount of labelled data which is rather labor intensive.
to reduce the burden of manual labelling pre training techniques have been proposed recently with the aim of equipping the model with commensense knowledge using unlabelled data after which the model can be fine tuned on downstream tasks with relatively small scale labelled data.
such a paradigm was first proposed in the natural language processing nlp domain and then adapted to coderelated tasks by designing pre training tasks to learn the domain knowledge of programming language or build the connections between a code snippet and its associated documentation .
existing pre trained models mainly target two types of coderelated tasks generation and understanding .
the generation tasks denote those that require the generation of a sequence of tokens words either in program languages e.g.
code generation and code repair or natural languages e.g.
code summarization .
the understanding tasks denote those that require to produce vectorized representations of programs and then perform downstream tasks such as classification and ranking e.g.
code search code clone detection and defect detection .
although existing pre training techniques such as codebert graphcodebert and codet5 outperform traditional supervised learning techniques for the above tasks few of them target code change related tasks.
as introduced the pre training of these models focuses on capturing the syntactic and semantic information of a code snippet whereas dealing with code change related tasks requires the model to encode and understand the differences between two code snippets rather than a single code snippet .
consequently the inputs to the models will be different from those used for pre training when the models are fine tuned for code change related downstream tasks.
such inconsistency makes the knowledge learned by pre trained models hard to be fully exploited easily leading to sub optimal results for downstream tasks .
in the literature coditt5 is evaluated on figure an example of code change with commit message.
a code change related downstream task i.e.
just in time comment update .
however its pre training paradigm is designed for better editing the code and comment rather than capturing the syntactic and semantic information of code changes.
to our best knowledge the only existing pre trained model that aims at modelling code changes is codereviewer but it is specially designed for code review tasks e.g.
generating the review comment and refining the code based on the review comment .
beyond such tasks there are many other code change related tasks where an automated tool can significantly lighten the burdens on developers such as commit message generation code comment update and defect prediction .
thus the literature lacks a pre trained model that can perform well on diverse code change related tasks.
moreover from the technical perspective the pre training of codereviewer ignores program structure information which has been shown to be critical for the model s capability .
the dataset codechangenet pre training techniques in the software engineering domain rely on capturing the syntactic and semantic information of code snippets.
previously for code related pre trained models the target is achieved by using the pairwise data i.e.
the code and its corresponding comment for pre training.
the comment which summarizes the main functionality of the code provides the model with a way to understand the semantic information of the code.
similarly if we are going to pre train a model for code changes we also need a way to reflect the semantic information of the code change.
we note that while committing a code change to a version control system developers need to document their changes using a commit message which usually summarizes what happens in the change .
a concrete example is shown in figure .
a developer changes the value of a variable from false totrue and the associated commit message describes this change as enable subqueries .
therefore in this work we choose to use the commit messages to serve as the natural language descriptions of code changes.
existing datasets on commit messages however are usually small scale.
for instance the dataset provided by jiang et al.
only contains 32k commit messages.
such a scale cannot ensure the adequacy of pre training cf.
the dataset used for code related pre training i.e.
the codesearchnet contains around 2m code comment pairs .
therefore we propose to build a multi linguistic dataset with large scale pairwise data of code changes and commit messages for pre training named codechangenet .
in the following sections we present the details of the building of dataset.
.
data collection nowadays many projects are hosted on software development platforms such as github.
with developers continuously committingesec fse november san francisco usa bo lin shangwen wang zhongxin liu yepang liu xin xia and xiaoguang mao table statistics of the pre training dataset.
language projects commits data size python 519k .0gb javascript 97k .2gb ruby 62k .5gb go 185k .0gb java 461k .0gb php 230k .8gb total 524k .6gb projects of the dataset are accessed in october .
their code changes there is a large amount of readily available commit data including the exact content of code changes and commit messages associated with code changes.
we thus build our codechangenet dataset based on commit data collected from opensource projects in github.
following codesearchnet we collect projects in six popular programming languages which are go java javascript php python and ruby.
to ensure the quality of our dataset we collect data from projects with high popularity which is indicated by the number of stars.
specifically we select projects whose numbers of stars are more than such a criterion is widely used by existing studies to indicate that a project is popular .
we then remove projects that do not have a license or whose licenses do not explicitly permit the re distribution of parts of the project.
to collect commit data we use github rest api to crawl project information from github following the common practice in the mining software repository msr domain .
specifically by calling github api the detailed repository information of each project such as the commit data can be accessed and stored in a json file.
such commit data includes code changes i.e.
the original file new file and the code diff and commit messages.
finally a set of pairs cci cmi wherecciis a code change and cmiis the associated commit message is collected as the initial data of codechangenet.
.
data filtering to further ensure the data used for pre training is of high quality we perform the following preprocessing steps to filter low quality data pairs whose cmiis shorter than three tokens including three are removed to ensure that the commit message is descriptive.
this decision follows codesearchnet which restricts that the comment of the code should contain more than three tokens.
pairs whose cciinvolves more than tokens in the code diff are removed to ensure that the model will not be affected by such extremely complex code changes.
this decision follows existing studies which build commit related datasets .
pairs whose ccioccurs in the test files are removed since we focus on code changes in the source code.
this decision also follows the preprocessing of codesearchnet.
pairs from those projects that have been used to build downstream tasks which will be introduced later in section are removed to avoid data leakage.
the resulting codechangenet dataset contains about .5m of code change commit message pairs from projects.
such a scale is comparable to that of codesearchnet and thus can ensurethe adequacy of pre training.
the detailed statistics of the dataset are illustrated in table .
cct5 in this section we introduce the details of our cct5 including the model architecture the input output representations of the model and the five different pre training tasks designed for the model.
.
model architecture following the t5 model cct5 uses an encoder decoder architecture.
the encoder and decoder both have transformer layers and in each layer attention heads are used to perform the multi head attention calculation leading to the total parameter size being 220m.
such an architecture is widely used by state of the art pre trained models .
following existing studies we initialize the parameters of cct5 with the values from codet5 with the aim of equipping the model with some domain knowledge of the programming language.
cct5 is then further trained on five different pre training tasks and then fine tuned on various downstream tasks.
.
input output representation since cct5 is designed to address code change related tasks a fundamental problem is thus how to represent code changes.
if we send the token sequences of the old code and new code into the model the inputs would be extremely long since a code change may happen across different files which could make the model hard to converge .
instead we use code diff to represent the code changes which is shown to be effective .
a diff file is generated by comparing the files before and after the code change.
specifically there are one or more diff hunks in a diff file and each diff hunk contains three different types of information lines deleted in the change indicated by a at the beginning of each line e.g.
the red line in figure lines added in the change indicated by a at the beginning of each line e.g.
the green line in figure and surrounding lines unchanged in the change which serve as the context information of the code change e.g.
the line in figure .
one advantage of using the diff format is that it reduces the input length to a large extent as the unchanged lines occur only once.
for downstream tasks cct5 takes the code diff as input and then performs a number of different tasks.
following the standard manner of the transformer the input is treated as a token sequence.
we reuse the tokenizer from codet5 to split the code into a sequence of tokens.
after that a special token is prepended to the sequence making the input with the form of c1 ... cn whereciis a source code token and ndenotes the length of the code token sequence.
this decision follows existing studies and the token will be used as the representation of the code change in understanding tasks which will be described in more detail in section .
.
to help the model better understand the code change we also insert a special token in front of each line for deleted lines we insert the token for added lines we insert the token and for unchanged lines we insert the token .
in the pre training phase some of our pre training tasks take as inputs the code change and the commit message simultaneously with the aim of building the semantic connectioncct5 a code change oriented pre trained model esec fse november san francisco usa between the programming language and the natural language.
for such tasks the inputs will be c1 .
.
.
cn m1 .
.
.
ml where is a token separating the code tokens and commit message tokens miis a commit message token and ldenotes the length of the commit message token sequence.
.
pre training tasks an important goal of cct5 is designed to accurately capture the semantic information of a given code change.
to achieve so we need to build the semantic connection between the code change and the commit message during the pre training phase.
we design totally four pre training tasks to fulfill this target.
besides inspired by graphcodebert we also design a pre training task to make the model be aware of the program structure.
figure gives an illustration of the five pre training tasks of cct5 .
details about these tasks will be introduced below.
.
.
masked language modeling.
the masked language modeling mlm pre training task is widely used in previous studies to encourage the model to align the natural language nl and programming language pl representations.
generally the mlm is to randomly mask some tokens from the source code and paired documentation and then ask the model to predict the original tokens.
in this study we design two different tasks according to masking the code change or masking the commit message.
masked language modeling for code change mlm4cc .
in this task we input the code change and commit message to the model and mask the code lines from the code change.
we focus on the line level rather than the token level in order to keep the integrity format of code diff following the previous study .
specifically we randomly sample of the lines in the code diff and mask them and the model is asked to predict the masked tokens.
note that this task is similar to the masked span prediction msp task proposed in codet5 as they both require the model to predict consecutive tokens.
the key difference between them is that in msp the number of masked tokens is randomly determined whereas in mlm4cc we ensure that the masked tokens can form a complete code line.
this task helps the model gain general knowledge about the distribution of the code change corpus and furthermore when the information from the code change is not enough the model can refer to the paired commit message to help it make the prediction which also helps the model build the connection between the nl and pl.
from a general perspective this pre training task helps the model understand what code change can fulfill the intended functionality expressed through the commit message .
note that this pre training task differs from the denoising code diff dcd task of codereviewer since we also take the commit message as input to build the nl pl correlations better.
in contrast the previous work only takes the code change as the input and thus can only help the model learn the code change distribution.
formally the loss can be described as lmlm 4cc k t logp ct cmask m c t wherecmaskis the masked code diff mis the commit message kdenotes the number of masked code diff tokens and c tis the token sequence predicted for the masked code diff so far.
masked language modeling for commit message mlm4cm .
in this task we input the code change and commit message to the model and mask the commit message tokens.
specifically we randomly sample of the tokens from the commit message which are supposed to be predicted by the model and then we replace them with the token of the time with a random token of the time and keep them unchanged of the time following existing study .
this task helps the model gain general knowledge about the distribution of the commit message corpus.
furthermore when the information from the commit message is not enough the model can refer to the corresponding code change to help it make the prediction which again helps the model build the connection between the nl and pl.
from a general perspective this pre training task helps the model understand the semantic information of the corresponding code change.
similarly this pretraining task differs from the denoising review comment drc of codereviewer since drc still merely learns the distribution of the review comments while does not build the nl pl connection.
formally the loss of this task can be described as lmlm 4cm k t logp mt mmask c m t wheremmaskis the masked commit message cis the code diff kdenotes the number of tokens in the masked commit message andm tis the token sequence predicted for the masked commit message so far.
.
.
code change commit message dual generation.
in the above pre training tasks the decoder only predicts discrete masked tokens while in the generation downstream tasks it needs to generate a fluent nl description or a complete code snippet.
to fill the gap between the pre training and downstream tasks we design two pre training tasks to train the model for an nl pl bidirectional conversion inspired by codet5 .
to our best knowledge we are the first to employ such a dual generation mode to pre train code change oriented models.
nl pl generation nl2pl .
in this task we expect the model to learn how to generate the new code based on the old code and the commit message.
we consider that code lines in the code diff beginning with and can denote the old code before the code change.
therefore we mask the added code i.e.
code lines beginning with and then send the code diff which now denotes the old code and the commit message into the model.
the model is asked to predict the masked contents which denote the added content during the code change.
considering that the lines beginning with and denote the new code after the code change the model thus learns how to generate a code snippet during this task.
note that the difference between this task and mlm4cc is that in this task we train the model to explicitly generate the added code i.e.
the lines beginning with while in mlm4cc the masked contents are randomlyesec fse november san francisco usa bo lin shangwen wang zhongxin liu yepang liu xin xia and xiaoguang mao figure pre training tasks of cct5.
selected.
similar to lmlm 4cc the loss can be described as lnl2pl k t logp ct cmask m c t wherecmask is the masked code diff mis the commit message kdenotes the number of masked code diff tokens and c tis the token sequence predicted for the masked code diff so far.
pl nl generation pl2nl .
in this task we expect the model to learn how to generate the commit message based on the code change.
specifically we send the code diff into the model and the model is expected to generate the entire commit message through which the model learns to generate fluent nl descriptions.
formally the loss can be described as lpl2nl k t logp mt c m t wherecis the code diff kdenotes the number of tokens in the commit message and m tis the token sequence generated so far.
.
.
code diff generation.
the authors of graphcodebert propose that the performance of pre trained models can be enhanced if considering the code structure during the pre training.
they design two structure aware pre training tasks and results show that graphcodebert outperforms codebert which does not involve such pre training tasks on a number of downstream tasks.
in our study we also design a structure aware pretraining task for cct5 to make it better understand the code change.
to our best knowledge we are the first to involve program structure information when pre training code change oriented models.
following graphcodebert we rely on the data flow of the code snippet to provide the semantic level structure information.
generally data flow can be considered as a graph in which the data dependency relation i.e.
where the value comes from among different variables is depicted.
in the graph the nodes represent the variables in the code and the edges represent that the two connected variables have dependencies.
data flow is crucial for code understanding since it provides a way to understand the semantics of a variable by concentrating on where its value comes from rather than focusing on the variable s name which is sometimes inpoor quality e.g.
iandj and it enables the model to consider the long range dependency for variables with the same names but occur in distant locations.
we refer the readers to the previous study for more details about data flow.
in this code diff generation cdg task we send the old data flow new data flow and old code into the model after which the model is asked to generate the corresponding code diff.
by so we expect the model could learn to understand the code change based on the data flow change and thus take the program structure into consideration.
to adapt the data flow into the acceptable format of the model we use the variable pair variablea variableb to represent an edge in the graph that connects two variables and all the edges are sent into the model sequentially.
the inputs are thus represented as oc1 ... ocn variablea variableb .
.
.
variablex variabley variablea variableb ... variablex variabley whereociis a token in the old code is a special symbol to split two kinds of data types is a special symbol to split two edges in the data flow graph variableidenotes a variable in the data flow of the old code and variablei denotes a variable in the data flow of the new code.
formally the loss can be described as lcdg k t logp ct oc odf ndf c t whereocis the old code odfis the data flow before the code change ndfis the data flow after the code change kdenotes the number of tokens in the code diff and c tis the token sequence generated so far.
.
.
final loss.
following the existing studies we treat different pre training tasks equally.
the final loss during the pretraining is calculated as min lloss llml 4cc llml 4cm lnl2pl lpl2nl lcdg cct5 a code change oriented pre trained model esec fse november san francisco usa .
implementation details our model is implemented with the popular deep learning development framework pytorch.1all the experiments are performed on a server with nvidia geforce rtx gpus.
the learning rate and batch size in the pre training stage are set to 5e and respectively.
when fine tuning our cct5 on the downstream tasks which are going to be introduced in the next section we use a batch size of and a learning rate of 2e for classification tasks i.e.
just in time defect prediction and for the remaining generation tasks we adopt the same learning rate and batch size as we used during the pre training stage.
experiments the goal of our work is to build a pre trained model that can be applied to code change related downstream tasks.
to evaluate the effectiveness of our cct5 we perform experiments on three different tasks i.e.
commit message generation just in time comment update and just in time defect prediction .
we next elaborate on the three tasks the baselines and the results.
.
task commit message generation .
.
problem formulation.
it is reported that developers usually do not have enough time to write high quality commit messages .
however commit messages are of great importance in software maintenance since they describe the intention of the code changes and thus facilitate program comprehension.
therefore developers could gain considerable benefits from automatic generation of commit messages making this task a hot topic in software engineering community .
in this code change related generation task given the code change we aim to automatically generate a brief commit message that summarizes its content.
.
.
baselines.
we select the following state of the art techniques as the baselines of the commit message generation task.
nngen .nngen is a state of the art retrieval based commit message generation approach .
it first uses bag of words to represent code changes after which the code change in the training set with the most similar vector representation to that of the test code change calculated by the cosine similarity is identified whose commit message is reused as the result.
according to the recent replication study nngen is the most effective non pre training technique so far for commit message generation.
fira .fira is another state of the art commit message generation approach.
it uses a customized graph structure to explicitly depict the code edit operations and then adopts a graph convolution network to represent the code change.
finally a transformer layer with the dual copy mechanism is used to generate the message.
codereviewer .codereviewer is a pre training technique for code changes but it is pre trained and evaluated only on code review tasks such as review generation.
to assess the effectiveness of this technique on general code change tasks we fine tune and evaluate the pre trained model on our training and test sets.
performance of different approaches measured by bleu on the mcmd dataset in .
dataset nngen codet5 codereviewer cct5 mcmd java .
.
.
.
mcmd c .
.
.
.
mcmd c .
.
.
.
mcmd python .
.
.
.
mcmd js .
.
.
.
average .
.
.
.
codet5 .codet5 is a state of the art pre trained model with an encoder decoder architecture.
it achieves the best performance on code related generation tasks .
technically it accepts token sequences as inputs so that it can be adapted to code change related tasks by feeding it with token sequences of code diffs.
comparing with codet5 can better demonstrate the rationale of cct5 and we also fine tune and evaluate the pre trained model on our dataset.
.
.
dataset metrics.
we choose to use the multi programminglanguage commit message dataset mcmd as our experiment dataset which is a recently released large scale benchmark for five programming languages including python java javascript c and c .
the total number of commits for each language is 450k and these commits are randomly split in training validation test proportions.
however fira cannot be evaluated on this dataset since it currently only supports java it uses the javalang package to perform program analysis and it requires complete class files to extract asts whereas the mcmd dataset only contains commit diffs.
therefore to compare with fira we also evaluate cct5 on the dataset used in fira s evaluation which is a commonly used large scale java dataset extracted from 1k popular github projects .
the dataset contains commits as the training set commits as the validation set and commits as the test set.
bleu is a common metric that measures lexical overlap for evaluating text generation.
among a number of the variants of bleu the case insensitive b norm has the highest correlation with human evaluations .
therefore following existing studies we use the b norm as our evaluation metric.
.
.
experiment details.
since the commit message generation is a generation task we use the entire encoder decoder architecture ofcct5 to perform such a task.
the training validation test sets are used to fine tune train validate evaluate all pre training nonpre training techniques.
we reuse the code as well as the hyperparameter values released by nngen codet5 and codereviewer to perform this experiment.
we reuse the performance of fira reported by dong et al.
to compare it with cct5 .
.
.
results.
results are shown in table .
we note that cct5 consistently outperforms existing approaches concerning all five pls.
on average the bleu score of cct5 on the whole test set is .
exceeding those of the state of the art pre training techniques codet5 and codereviewer which are .
and .
by and respectively.
similarly cct5 achieves an increase of when compared with the non pre training nngen .
vs. .
.
we further conduct a wilcoxon signed rank test between the bleu scores of cct5 and the other three baselines onesec fse november san francisco usa bo lin shangwen wang zhongxin liu yepang liu xin xia and xiaoguang mao table the results of our human evaluation.
approach adequacy conciseness expressiveness nngen .
.
.
codereviewer .
.
.
codet5 .
.
.
cct5 .
.
.
the sub dataset of each programming language.
the results confirm that the difference between the scores of cct5 and nngen codet5 codereviewer is statistically significant at the confidence level of in all the comparisons.
we also note cct5 achieves comparatively poor performance on c language.
a potential reason is that this pl is not included in the codechangenet dataset so that the domain knowledge of this pl is not learned during pre training.
nonetheless cct5 still outperforms codereviewer which is pre trained on data written in c language by around .
vs. .
.
another interesting observation from table is that codereviewer generally outperforms codet5 on the mcmd dataset which also demonstrates the benefits of training data related to code changes.
when evaluated on the dataset used by fira cct5 achieves a bleu score of .
which exceeds that of fira .
by .
.
the wilcoxon signed rank test also shows that the difference between the scores of cct5 and fira is statistically significant at the confidence level of .
.
.
human evaluation.
the evaluation metric bleu can measure the lexical gap between the generated commit messages and the