pre training code representation with semantic flow graph for effective bug localization yali du shandong university duyali2000 gmail.comzhongxing yu shandong university zhongxing.yu sdu.edu.cn abstract enlightened by the big success of pre training in natural language processing pre trained models for programming languages have been widely used to promote code intelligence in recent years.
in particular bert has been used for bug localization tasks and impressive results have been obtained.
however these bert based bug localization techniques suffer from two issues.
first the pretrained bert model on source code does not adequately capture the deep semantics of program code.
second the overall bug localization models neglect the necessity of large scale negative samples in contrastive learning for representations of changesets and ignore the lexical similarity between bug reports and changesets during similarity estimation.
we address these two issues by proposing a novel directed multiple label code graph representation named semantic flow graph sfg which compactly and adequately captures code semantics designing and training semanticcodebert based on sfg and designing a novel hierarchical momentum contrastive bug localization technique hmcbl .
evaluation results show that our method achieves state of the art performance in bug localization.
ccs concepts software and its engineering software testing and debugging maintaining software .
keywords bug localization semantic flow graph type computation role pretrained model contrastive learning acm reference format yali du and zhongxing yu.
.
pre training code representation with semantic flow graph for effective bug localization.
in proceedings of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering esec fse december san francisco ca usa.
acm new york ny usa pages.
https zhongxing yu is the corresponding author.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse december san francisco ca usa association for computing machinery.
acm isbn x xxxx xxxx x yy mm.
.
.
.
introduction while modern software engineering recognizes a broad range of methods e.g.
model checking symbolic execution type checking for helping ensure that the software meets the specification of its desirable behavior the software even deployed ones is still unfortunately plagued with heterogeneous bugs for reasons such as programming errors made by developers and immature development process.
the process of resolving the resultant bugs termed debugging is an indispensable yet frustrating activity that can easily account for a significant part of software development and maintenance costs .
to tackle the ever growing high costs involved in debugging a variety of automatic techniques have been proposed as debugging aids for developers over the past decades .
in particular numerous methods have been developed to facilitate fault localization which aims to identify the exact locations of program bugs and is one of the most expensive tedious and time consuming activities in debugging .
the literature on fault localization is rich and is abundant with methods stemming from ideas that originate from several different disciplines notably including statistical analysis program transformation information retrieval .
among them information retrieval based methods typically proceed by establishing the relevance between bug reports and related software artifacts on the ground of information retrieval techniques and this category of methods is appealing as it is amenable to the mainstream development practice which features continuous integration ci versioning with git and collaboration within platforms like github .
in line with existing literature information retrieval based fault localization hereafter is simply referred to as bug localization.
the matched software artifact at the early phase of bug localization research focuses on code elements such as classes and methods but recent years have witnessed a growing interest in changesets .
the key advantage of changesets is that they contain simultaneously changed parts of the code that are related facilitating bug fixing.
with regard to information retrieval techniques the major shift is that the dominating techniques have changed from vector space model vsm to deep learning techniques both for code elements and changesets.
to precisely locate the bug bug localization techniques essentially need to accurately relate the natural language used to describe the bug in the bug report and identifier naming practices adopted by developers in the software artifacts .
however it is quite common that there exists a significant lexical gap between them and consequently the retrieval quality of bug localization techniques is not always satisfactory .
to overcome the issue bug localization techniques necessarily need to go beyond exact term matching and establisharxiv .12773v1 aug 2023esec fse december san francisco ca usa yali du and zhongxing yu the semantic relatedness between bug reports and software artifacts.
given that deep learning architectures are capable of leveraging contextual information and have achieved impressive progress in natural language processing a number of bug localization techniques based on the neural network have been proposed in recent years .
in particular state ofthe art transformer based architecture bert bidirectional encoder representation from the transformer has been widely employed .
based on the naturalness hypothesis which states that software corpora have similar statistical properties to natural language corpora these bert based techniques first pre train a bert model on a massive corpus of source code using certain pre training tasks such as masked language modeling and then finetune the trained bert model for bug localization task.
experimental evaluations have shown that reasonable accuracy improvements can be obtained by these bert based techniques.
despite the progress made one drawback of these bert based techniques is that the pre trained bert model on source code does not adequately capture the deep semantics of program code.
unlike natural language the programming language has a formal structure which provides important code semantics that is unambiguous in general .
however the existing pre trained bert model either totally ignores the code structure by treating code snippet as a sequence of tokens same as natural language or considers only the shallow structure of the code by using graph code representations such as data flow graph .
consequently the formal code structure has not been fully exploited resulting in an under optimal bert model.
to overcome this issue we in this paper present a novel code graph representation termed semantic flow graph sfg which compactly and adequately captures code semantics.
sfg is a directed multiple label graph that captures not only the data flow and control flow between program elements but also the type of program element and the specific role that a certain program element plays in computation.
on the ground of sfg we further propose semanticcodebert a pre training model with bert like architecture to learn code representation that considers deep code structure.
semanticcodebert features novel pre training tasks besides the ordinary masked language modeling task.
in addition the overall models of existing bert based bug localization techniques ignore several points which are beneficial for further improving performance.
first the batch size is typically limited to save model space because of the huge scale of bert parameters and the number of negative samples coupled to batch size is thus limited.
a variety of existed methods emphasizes the necessity of large scale negative samples in contrastive representation learning.
in the bug localization context it implies the importance of considering the large scale negative sample interactions for representation learning of bug reports and changesets.
nevertheless existing techniques like ciborowska et.
al.
only select one irrelevant changeset in training as the negative sample for a bug report which causes inefficient mining of negative samples and poor representation of the programming language.
to alleviate this issue we propose to use a memory bank to store rich changesets obtained from different batches for later contrast.
in particular due to the constant parameter update by back propagation weutilize the momentum contrastive method to account for the inconsistency of negative vectors obtained by different models in different mini batches .
second existing bert based bug localization techniques only account for the semantic level similarity between bug reports and changesets totally ignoring the lexical similarity e.g.
same identifier which is also of vital importance for retrieval if exists.
to alleviate this issue we propose to use a hierarchical contrastive loss to leverage similarities at different levels.
on the whole we design a novel hierarchical momentum contrastive bug localization hmcbl technique to address the two limitations.
we implement the analyzer for obtaining sfg for java code and use the java corpus including functions of the codesearchnet dataset to pre train semanticcodebert.
on top of semanticcodebert we apply the hierarchical momentum contrastive method to facilitate the retrieval of bug inducing changesets given a bug report on the widely used dataset established in which includes six java projects.
results show that we achieve state ofthe art performance on bug localization.
ablation studies justify that the newly designed sfg improves the bert model and the new bug localization architecture is better than the existing ones.
our contributions can be summarized as follows we present a novel directed multiple label code graph representation termed semantic flow graph sfg which compactly and adequately captures code semantics.
we employ sfg to train semanticcodebert which can be applied to obtain code representations for various code related downstream tasks.
we design a novel hierarchical momentum contrastive bug localization technique hmcbl which overcomes two important issues of existing techniques.
we conduct a large scale experimental evaluation and the results show that our method outperforms state of the art techniques in bug localization performance.
related works this section reviews work closely related to this paper.
bug localization techniques proceed by making a query about the relevance between bug reports and related software artifacts on top of information retrieval techniques.
the investigated software artifacts can be majorly divided into two categories code elements such as classes and methods and changesets .
given changesets contain simultaneously changed parts of the code that are related and can thus facilitate bug fixing the use of changesets is gradually dominating.
with regard to information retrieval techniques the vector space model vsm is widely used for its simplicity and effectiveness especially in the early phase of bug localization research.
for instance buglocator makes use of the revised vector space model rvsm to establish the textual similarity between the bug report and the source code and then ranks all source code files based on the calculated similarity.
for another example locus represents one of the earliest works on changeset based bug localization and it proceeds by matching bug reports to hunks.
as vsm basically performs exact term matching the effectiveness will be compromised in the common case where there existspre training code representation with semantic flow graph for effective bug localization esec fse december san francisco ca usa a significant lexical gap between the descriptions in the bug report and naming practices adopted by developers in the software artifacts.
to overcome this issue bug localization techniques essentially need to establish the semantic relatedness between bug reports and software artifacts.
given the impressive progress in leveraging contextual information by deep learning architectures in natural language processing deep neural networks have been widely used by researchers to learn representations for bug localization in recent years .
for instance huo et.
al.
present the deep transfer bug localization task and propose the tranp cnn as the first solution for the cold start problem which combines cross project transfer learning and convolutional neural networks for file level bug localization.
zhu et.
al.
focus on transferring knowledge while filtering out irrelevant noise from the source project to the target project and propose the cooba to leverage adversarial transfer learning for cross project bug localization.
murali et.
al.
propose bug2commit which is an unsupervised model leveraging multiple dimensions of data associated with bug reports and commits.
in particular enlightened by the impressive achievements made by bert in natural language processing bert has been used for bug localization tasks.
lin et.
al.
study the tradeoffs between different bert architectures for the purpose of changeset retrieval.
based on the colbert developed by khattab et.
al.
ciborowska et.
al.
propose the fbl bert model towards changeset based bug localization.
evaluation results show that fbl bert can speed up the retrieval and several design decisions have also been explored including granularities of input changesets and the utilization of special tokens for capturing changesets semantic representation.
while impressive retrieval results of changesets have been achieved the colbert used by fbl bert does not adequately capture the deep semantics of program code and the overall models of fbl bert suffer from two important limitations as described in section introduction .
furthermore inspired by the success of pre training models in natural language processing a number of pre trained models for programming languages have been proposed to promote the development of code representation which is vital for a variety of code based tasks in the field of se .
for instance codebert is a pre trained model proposed by feng et.
al.
which provides generic representations for natural and programming language downstream applications.
graphcodebert imports structural information to enhance the code representation by adding the data flow graph as an auxiliary of input tokens and improves the performance of code representation compared to codebert.
kanade et.
al.
propose cubert which is pre trained on a massive python source corpus with two pre training tasks of masked language modeling mlm and next sentence prediction nsp .
buratti et.
al.
propose c bert a transformer based language model that is pre trained on the c language corpus for code analysis tasks.
xue et.
al.
propose treebert which proposes a hybrid target for ast to learn syntactic and semantic knowledge with tree masked language modeling tmlm and node order prediction nop pretraining tasks.
more recently the unixcoder is proposed to leverage cross modal information like abstract syntax tree and comments written in natural language to enhance code representation.
while these pre trained models on source code have madeprogress towards code representation one drawback of them is that they not adequately capture the deep semantics of program code as they either treat code snippets as token sequences or consider only shallow code structure by using graph code representations such as data flow graph.
hence we give a novel code graph representation termed semantic flow graph sfg to more compactly and adequately capture code semantics in this paper.
on top of sfg we further design and train semanticcodebert with novel pre training tasks.
semantic flow graph this section introduces the semantic flow graph sfg a novel code graph representation designed for compactly representing deep code semantics.
on top of the naturalness hypothesis software is a form of human communication software corpora have similar statistical properties to natural language corpora and these properties can be exploited to build better software engineering tools recent years have witnessed many innovations in using machine learning particularly deep learning techniques to help make the software more reliable and maintainable.
to achieve successful learning one important ingredient lies in suitable code representation.
the representation on the one hand should capture enough code semantics and on the other hand should be learnable across code written by different developers or even different programming languages .
there are majorly three categories of code representation ways within the literature token based ways that represent code as a sequence of tokens syntactic based ways that represent code as trees and semantic based ways that represent code as graph structures .
for token based representation while its simplicity facilities learning the representation ignores the structural nature of code and thus captures quite limited semantics.
for syntactic based representation despite the tree representation in principle can contain rich semantic information the learnability is unfortunately confined as the tree typically has an unusually deep hierarchy and there in general will involve significant refinement efforts of the raw tree representation to enable successful learning in practice.
semanticbased representation aims to encode semantics in a way that facilitates learning and a variety of graphs have been employed for code model learning including for example data flow graph control flow graph program dependence graph contextual flow graph .
while these graph based representations have facilitated the learning of code semantics embodied in data dependency and control dependency certain other code semantics are overlooked.
in particular the information of what kinds of program elements are related by data dependency or control dependency and through which operations they are related to is neglected.
we argue that this information is crucial for accurately learning code semantics.
for instance given a code snippet a m b c where a b and careboolean integer and user defined type variables respectively and mis a certain function call there will be two data flow edges b aandc aconsidering the data flow graph and the code snippet will read the values of two variables have flown into another variable .
under this circumstance as the corresponding data flow graph coincides the meaning of the code snippet has noesec fse december san francisco ca usa yali du and zhongxing yu figure an example of the semantic flow graph.
difference with a variety of other code snippets such as a b m c where a b and careboolean boolean and arbitrary type variables respectively and mis a certain function call that returns a boolean value.
but if the additional information of what kinds of program elements and which operations are taken into account the code snippet a m b c will read the value of an integer type variable and the value of a user defined type variable have flown into another boolean type variable through a function call which is more precise.
to compactly integrate these two pieces of information into graphs we design a novel directed multiple label code graph representation termed semantic flow graph sfg .
definition .
.
semantic flow graph .
the semantic flow graph sfg for a code snippet is a tuple n e t r where nis a set of nodes eis a set of directed edges between nodes in n and tandrare mappings from nodes to their types and their roles in computation respectively.
a number of points deserve comment.
first the node set ncan be further divided into node sets nvandnc which contain nodes corresponding to variables and control instructions in the code respectively.
while the variable has a one to one mapping with a certain node from nv there may be one or multiple nodes in ncfor a certain control instruction.
essentially if a control instruction has an associated condition and ndifferent branches i.e.
straight line code blocks to go depending on the condition evaluation result there will be a node in ncfor the condition a node in ncfor the convergence of the different branches and ndifferent nodes in nc for the nbranches respectively.
second a directed edge na nb na n nb n inecan be of kinds.
the first kind edrepresents a data flow between two variables if na nv nb nvholds the second kind ec embodies the control flow between two straight line basic blocks ifna nc nb ncholds and finally the third kind esdenotes the natural sequential computation flow inside or between basic blocks in case na nv nb ncorna nc nb nvholds.
in particular the edge set eis established as follows establish edamong nodes from set nvaccording to intra block and inter block data dependencies between variables.
establish ecamong nodes from set ncaccording to the specific control flow of the control instruction.
establish esfollowing these rules there will be an edge na nb i if nb ncis for the control instruction condition and na nv is for a certain variable involved in the condition ii if na ncis for the control instruction branch and nb nvis for the left most variable of the first statement inside the branch iii if na nvis for the left most variable of the last statement inside a control instruction branch and nb ncis for the control instruction convergence iv ifna ncis for the control instruction convergence and nb nvis for the left most variable of the first statement inside the basic block directly following the control instruction.
third mapping tmaps each node in nto its type encoding the needed information of what kinds of program elements are related .
for each node in nv tmaps it to the corresponding type of the variable.
for each node in nc tmaps the node to the specific part of the control instruction it refers to.
take the control instruction if then else as an example tmaps the associated nodes in ncfor it to type ifcondition ifthen ifelse and ifconverge respectively.
finally mapping rmaps each node in nvto its role in the computation encoding the needed information of through which operations program elements are related .
basically rconsiders the associated operation and control structure for the variable to determine its computation role.
from an implementation perspective for each node in nv rchecks the direct parent of the corresponding variable in the abstract syntax tree ast and the position relationship between it and the direct parent to establish the role.
for instance given a code snippet a b where aandbare variables rmaps the roles of aandbtoassigned andassignement respectively.
for another example given a code snippet a.m b where aandbare variables and mis a certain function call rmaps the roles of aandbtoinvocationtarget andinvocationargumentpre training code representation with semantic flow graph for effective bug localization esec fse december san francisco ca usa respectively.
for nodes in nc we do not consider their roles as they are implicit in their types.
note it is difficult to simply augment classical graph program representations with information of type and computation role.
existing representations like program dependence graph typically work at the statement granularity i.e.
each graph node represents a statement making it hard to encode detailed type and computation role information of multiple program elements in the statement.
the proposed sfg works at a finer granularity with two types of nodes that have a one to one mapping with program variables and a one to one or many to one mapping with program control ingredients respectively.
this kind of node representation is proposed for two reasons.
on the one hand it is convenient to analyze the types and computation roles of variables through how they are connected with other program elements and program control ingredients.
on the other hand data flow and control flow information are established respectively by analyzing variable uses and program control ingredients.
with sfg built on such a node representation data flow and control flow can be encoded through the edges between nodes and the type and computation role information can be encoded through node labels.
sfg does not have nodes for additional program elements like invocation etc.
thus it is compact but contains adequate semantic information.
overall sfg is a directed multiple label graph that captures not only the data flow and control flow between program elements but also the type of program element and the specific role that a certain program element plays in computation.
moreover sfg represents this information in a compact way facilitating learning across programs.
example .
.
figure gives an example of a semantic flow graph for a simple method.
implementation we fully implement an analyzer to get semantic flow graph sfg for a java method on top of spoon which is an open source library to analyze rewrite transform and transpile java source code.
our analyzer supports modern java versions up to java .
for nodes in nv the analyzer considers different kinds of primitive types and common jdk types and a special type named user defined type .
in total the analyzer considers types for nodes in nv.
for nodes in nc the analyzer takes all the control instruction kinds up to java into account and considers types in total.
with regard to role the analyzer considers different roles in total for nodes in nv.
semanticcodebert in this section we describe first the architecture of semanticcodebert shown in figure then the graph guided masked attention based on the semantic flow graph and finally the pre training tasks.
overall the semanticcodebert network architecture adapts the architecture of graphcodebert for the proposed novel sfg program representation and semanticcodebert also features tailored pre training tasks for the sfg representation.
.
model architecture the semanticcodebert follows bert bidirectional encoder representation from transformers devlin et.
al.
as the backbone.
figure all defined types and roles.
comment input sequence we import comments as a supplement for the model to understand the semantic information of programming code.
is the special classification token at the beginning of the comment sequence w. source code input sequence we cleanse the source code and remove erroneous characters and add the special classification token at the end of the source code and input sequence.
to represent the start of code we import a pre appended token to split the comment and source code.
the source code sequence can be represented as s. node input sequence with the procedure discussed in section we generate a semantic flow graph sfg for each code snippet.
at the beginning of the node list n a pre appended token is added to represent the start of node.
type input sequence to answer the question of what kinds of program elements are related we have identified 55possible types for the code element.
t t1 ... t represents the set of all possible types and is pre appended as the start of type.
the complete list of types is shown in figure .
role input sequence to answer the question of through which operations program elements are related we have defined 43roles to mark the role of each program element in the computation taking into account the associated operation and control structure.
r r1 ... r is the set of all 43possible roles and the preappended token represents the start of role.
the complete list of roles is shown in figure .
as intuitively shown in figure we concatenate the comment source code nodes types and roles as the input sequence x concat w s n t r .
esec fse december san francisco ca usa yali du and zhongxing yu figure the semanticcodebert takes to comment source code nodes of sfg types and roles as the input and is pre trained by standard masked language modeling node alignment marked with red lines graph prediction marked with green lines type prediction marked with blue lines and role prediction marked with purple lines .
.
masked attention we resort to the graph guided masked attention function described in to filter irrelevant signals in transformer.
the sete1indicates the alignment relation between sandn where si nj nj si e1if the nodenjis identified from the source code token si.
the sete2indicates the dependency relation in n where ni nj e2if there is a direct edge from the node nito the node nj.
the sete3incorporates the type information of the nodes where ni tj e3if the type of the node niistj.
the sete4incorporates the role information of the nodes where ni rj e4if the role of the node niisrj.
the masked attention matrix is formulated as m mij 0xi wi sj w s si nj nj si e1 ni nj e2 ni tj e3 ni rj e4 otherwise .
specifically the masked attention function blocks the transmission of unrelated tokens by setting the attention score to an infinitely negative value.
.
pre training tasks the pre training tasks of semanticcodebert are described in this section.
besides masked language modeling node alignment and edge prediction pre training tasks proposed by guo et.
al.
we define two novel pre training tasks type and role prediction.
these two novel pre training tasks represent the first attempt to leverage the attribute information of nodes for learning code representation.
masked language modeling the masked language modeling pre training task is proposed by devlin et.
al.
.
we replace of the source code with of the time a random token of the time or itself of the time.
the comment context contributes to inferring the masked code tokens .
node alignment the motivation of node alignment is to align representation between source code and nodes of semantic flow graph .
we randomly mask edges between the source codeand nodes and then predict where the nodes are identified from i.e.
predict these masked edges e1 mask .
as shown in the figure the model should distinguish that n2comes from s6andn13comes from s33.
we formulate the loss function as equation .
let e1bes n eij e1 mask is one if si nj e1 and zero otherwise.
pei jis the probability of the edge from i th code token to j th node which is calculated by dot product following a sigmoid function using the representations of siandnjoutputted from semanticcodebert.
lna ei j e1 mask eij logpei j eij log pei j .
edge prediction the motivation of edge prediction is to encourage the model to learn structural relationships from semantic flow graphs for better programming code representation.
like node alignment we randomly mask edges between nodes in the mask matrix encouraging the model to predict these masked edges e2 mask e.g.
the edges n3 n2 and n12 n11 .
we formulate the loss function as equation .
let e2ben n eij e2 mask is one if ni nj e2 and zero otherwise.
pei jis the probability of the edge fromi th node to j th node.
lgp ei j e2 mask eij logpei j eij log pei j .
type prediction the motivation of type prediction is to guide the model to comprehend the types e.g.
int double ifcondition of nodes for better programming code representation.
we pre append the full set of types tto the input nodes.
let e3be n t if the type of node niistj i.e.
ni tj e3 eij e3 mask is one otherwise it is zero.
we randomly mask edges between nodes and types and formulate the loss function as equation wheree3 maskare masked edges and pei jis the probability of the edge fromi th node to j th type.
ltp ei j e3 mask eij logpei j eij log pei j .
role prediction role indicates the computation role of the nodepre training code representation with semantic flow graph for effective bug localization esec fse december san francisco ca usa in the semantic flow graph e.g.
invocationargument assigned assignment .
role prediction can feed the model with a more informative signal to understand the correlation among different nodes.
we pre append the full set of roles rto the input nodes.
lete4ben r if the role of node niisrj i.e.
ni rj e4 eij e4 mask is one otherwise it is zero.
we randomly mask edges between nodes and roles and formulate the loss function as equation where e4 maskare masked edges and pei jis the probability of the edge from i th node to j th role.
lrp ei j e4 mask eij logpei j eij log pei j .
changeset based bug localization in this section we illustrate the utilization of the semanticcodebert towards bug localization with changesets.
the proposed bug localization model is shown in figure .
the model aims to address the two important limitations as described in section of the overall models of existing bert based bug localization techniques.
.
problem definition given a setq q1 q2 ... qm ofmbug reports the bug localization task aims to discover more relevant changesets from k k1 k2 ... kn a set including nchangesets.
more specifically for a bug report q q a bug inducing changeset p k and a not bug inducing changeset n k are selected to form a triplet q p n .
all bug inducing changesets and not bug inducing changesets are non overlapping.
the goal of learned similarity function s is to provide a high value for s q p between the anchor qand the positive sample p and a low value for s q n between the anchor q and the negative sample n .
section .
focuses on producing accurate representations of bug reports and changesets and section .
describes the estimation of similarities and the loss function for training the model.
.
representation learning the proposed model consists of three parts an encoder network projector network and momentum update mechanism with a memory bank that stores rich representations of changesets.
encoder network as mentioned before bug reports consist of natural language descriptions and project changesets consist of programming language code.
hence we introduce bert as the backbone to the encoder bug report as qfeature and semanticcodebert as the backbone to encoder relevant changeset as pfeature and irrelevant changeset as nfeature .
qfeature bert qtok pfeature semanticcodebert ptok nfeature semanticcodebert ntok where bert andsemanticcodebert are the trainable parameters of bert and semanticcodebert qtok ptok andntokare the input tokens obtained by tokenizers qfeature rd pfeature rd andnfeature rdare the refined vectors dis the dimension of the mapped spaces .
projector network after the feature vectors are extracted we use a multi layer perception neural network as a projector to compress the vectors of bug reports and changesets into a compact shared embedding space.
we replace dropout with batch normalization for regularization which can be trained with saturating nonlinearities and are more tolerant to increased training rates .
qmodel w2 bnorm w1 bqfeature pmodel w2cnorm w1cpfeature nmodel w2cnorm w1cnfeature where qmodel rd pmodel rd and nmodel rd are the projected vectors d is the dimension of the output of projector w bandw care the trainable weight matrices norm denotes batch normalization and is theleaky relu function .
momentum update mechanism with memory bank as mentioned in section it is important to consider large scale negative samples in contrastive learning for representations of changesets.
to account for this we use memory bank to store rich changesets obtained from different batches for later contrast.
in particular we build the key model for encoder and projector networks of changesets based on the momentum contrastive learning mechanism proposed by he et.
al.
.
the parameters of the query model q are updated by back propagation while the parameters of the key model kare momentum updated as follows k m k m q wherem is a pre defined momentum coefficient which is set as0.999in our experiment.
as proved in the previous study a relatively large momentum works much better than a smaller value suggesting that a slowly evolving key model is core to making use of the memory bank.
for per mini batch we use average pooling and enqueue the latest negative samples into the memory bank and dequeue the oldest negative samples.
.
similarity estimation as mentioned before the lexical similarity between bug reports and program changesets like the same application programming interfaces is also crucial for retrieval besides semantic similarity.
in this paper we use the hierarchical contrastive loss to leverage the lower feature level similarity higher model level similarity and broader bank level similarity for matching the bug report with relevant changesets.
we get the positive feature level similarity sf by calculating cosine similarity between qfeature andpfeature the negative feature level similarity sf by calculating cosine similarity betweenqfeature andnfeature the positive model level similarity sm by calculating cosine similarity between qmodel andpmodel and the negative model level similarity sm by calculating cosine similarity between qmodel andnmodel .
specifically we calculate the positive bank level similarity sb as cosine similarity between qquery andpkey and the negative bank level similarity sb i i ... k as cosine similarity between qquery andi th negative sample ni keyof the memory bank kis the size of memory bank .esec fse december san francisco ca usa yali du and zhongxing yu bug report anchor source code positive source code negative bert parameter shared linear lbelu bn linearlinear lbelu bn linearlinear lbelu bn linearparameter shared semanticcodebertencoder projector source code negative source code positive momentum update parameterskey model memory bank...qfeature pfeature nfeatureqmodel pmodel nmodelqquery pkey nkeyn1keyn2keyn3keynkkeylf lmlb feature level contrastive lossmodel level contrastive lossbank level contrastive loss figure an overview of the hierarchical momentum contrastive bug localization technique hmcbl .
we adopt infonce a form of contrastive loss functions as our objective function for contrastive matching.
the feature level contrastive loss is formulated as follows lf logexp sf exp sf exp sf .
the model level contrastive loss is formulated as follows lm logexp sm exp sm exp sm .
the bank level contrastive loss is formulated as follows lb logexp sb exp sb k k 1exp sb i wherekis the size of the memory bank and is a temperature hyper parameter that is set to be .
in our experiment.
thus the overall objective function is l l flf mlm blb where f m and bare three hyper parameters to balance the feature level model level and bank level contrasts.
.
offline indexing and retrieval after fine tuning the model on a project specific dataset we resort to the offline indexing and retrieval methods proposed by ciborowska et.
al.
.
all encoded changesets are stored in ivfpq invert file with product quantization index.
the ivfpq index is implemented using the faiss library which uses the k means algorithm to partition the embedding space into programmed partitions and assign each embedding to its nearest cluster.
in the retrieval process the query bug report is first located to the nearest partition s centroid and then the nearest instance within the partition is discovered.
for each query bug report we can identify then most similar changesets across all nchangesets stored in the ivfpq index.
therefore we only re rank the top n subset as the candidate changesets to produce the final ranking.table six projects used for evaluation.
dataset bugschangesets commits files hunks aspectj jdt pde swt tomcat zxing experimental evaluation .
dataset the semanticcodebert is trained using all the java corpus in codesearchnet and we provide the weights and the guidance to fine tune the pre trained model for downstream tasks.
to evaluate our bug localization technique we use the dataset separated by ciborowska et.
al.
from the manually validated dataset by ming et.
al.
.
the dataset includes six software projects termed aspectj jdt pde swt tomcat and zxing as shown in table .
to explore the impact of the granularity of changeset data the bug inducing changeset is further divided into file level and hunklevel code changes.
thus one bug report can have multiple pairs with files or hunks from the original inducing changes.
in total we consider three different granularities commits files and hunks.
.
evaluation metrics a set of metrics commonly used to evaluate the performance of information retrieval systems are applied to evaluate the performance of different models.
precision k p k p kevaluates how many of the top k changesets in a ranking are relevant to the bug report which is equal to the number of the relevant changesets relbi located inpre training code representation with semantic flow graph for effective bug localization esec fse december san francisco ca usa the top kposition in the ranking averaged across bbug reports p k b b i relbi k. mean average precision map map quantifies the ability of a model to locate all changesets relevant to a bug report.
map is calculated as the mean of avgp average precision of bbug reports.
avgp m j 1p j pos j n. map b b i avgpbi wherejis the rank mis the number of retrieved changesets pos j denotes whether j th changeset is relevant to the bug report nis the total number of bug reports relevant to changesets p jis the precision of top jposition in the ranking of this retrieval and biis thei th bug report.
mean reciprocal rank mrr mrr quantifies the ability of a model to locate the first relevant changeset to a bug report and is calculated as the average of reciprocal ranks across bbug reports.
1strankbiis the reciprocal rank of i th bug report which is the inverted rank of the first relevant changeset in the ranking mrr b b i 1strankbi.
.
experimental setup configurations of pre training tasks the semanticcodebert is pre trained on nvidia tesla a100 with 128gb ram on the ubuntu system.
the adam optimizer is used to update model parameters with batch size and learning rate 1e .
to accelerate the training process the parameters of graphcodebert are used to initialize the pre training model.
the model is trained with 600k batches and costs about hours.
configurations of bug localization the first half of the project s pairs of bug reports and bug inducing changesets ordered by bug opening date are selected as the training dataset and the remaining half is left as the test dataset.
the experiments are implemented with gpu support.
the adam optimizer is used to update model parameters with learning rate 3e .
all bug reports and changesets are truncated or padded to their respective length limit.
according to the experimental verification we set the trade off hyper parameters f m and bas1 and respectively.
changeset encoding strategies changesets are time ordered sequences recording the software s evolution over time.
we build upon the three changeset encoding strategies d encoding arcencoding and arcl encoding proposed by ciborowska et.
al.
to encode changesets.
d encoding does not utilize specific characteristics of changeset lines.
arc encoding divides the lines into three groups with three unique tokens.
arcl encoding instead does not group the lines and maintains the ordering of lines withina changeset.
these three strategies are based on the output of the git diffcommand which divides changeset lines into three kinds added lines removed lines and unchanged lines.
all code sequences are preprocessed by filtering the intrusive characters e.g.
docstrings comments from the original code tokens.
.
retrieval performance we compare the performance of our proposed model with the traditional bug localization tool state of the art changeset based bug localization approach and two recent state of the art pre trained models with the hmcbl framework.
bluir a structured ir based fault localization tool which builds ast to extract the program constructs of each source code file and utilizes okapi bm25 to calculate the similarity between the bug report and the candidate changesets.
fbl bert the state of the art approach for automatically retrieving bug inducing changesets given a bug report which uses the popular bert model to more accurately match the semantics in the bug report text with the bug inducing changesets.
graphcodebert a pre trained model that considers data flow to better encode the relation between variables.
unixcoder an unified cross modal pre trained model which leverages cross modal information like abstract syntax tree and comments to enhance code representation.
for bluir we fully follow the original technical description in as no open source implementation is available to get the results for the evaluation metrics.
for fbl bert we use the experimental results provided in .
for graphcodebert and unixcoder we get the results by replacing the pre trained model semanticcodebert within the hmcbl framework respectively with graphcodebert and unixcoder keeping other configurations the same .
table shows the retrieval performances of different models with different changeset encoding strategies i.e.
d arc and arcl encoding and three granularities i.e.commits files andhunks level on six projects.
limited by space the best result of the three encoding strategies is shown for each configuration.
the following observations can be obtained from the figure.
first compared with the traditional bug localization method which relies on more direct term matching between a bug report and a changeset the neural network methods perform better by obtaining semantic representations for the calculation of similarity.
second our proposed method outperforms the state of the art method fblbert by a clear margin.
in particular our proposed bug localization technique improves fbl bert by .
to .
in terms of mrr on six projects with commits level granularity.
third compared with graphcodebert and unixcoder our model using semanticcodebert as a changeset encoder consistently achieves better performance in almost all experimental configurations.
this suggests that the proposed semantic flow graph sfg captures good code semantics and the proposed framework contributes to changeset based bug localization.
the student s t test is conducted between our technique and other baselines and the results show that the improvements are significant with p .
.
we additionally observe that with the commits level granularity the obtained improvement is more significant than the other two granularities files level andhunks esec fse december san francisco ca usa yali du and zhongxing yu table retrieval performance of different models.
projects techniquecommits files hunks mrr map p p p mrr map p p p mrr map p p p zxingbluir .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fbl bert .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
graphcodebert .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
unixcoder .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ours .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
pdebluir .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fbl bert .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
graphcodebert .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
unixcoder .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ours .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
aspectjbluir .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fbl bert .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
graphcodebert .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
unixcoder .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ours .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
jdtbluir .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fbl bert .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
graphcodebert .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
unixcoder .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ours .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
swtbluir .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fbl bert .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
graphcodebert .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
unixcoder .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ours .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
tomcatbluir .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fbl bert .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
graphcodebert .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
unixcoder .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ours .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table ablation study of pre training tasks of semanticcodebert with semantic flow graph sfg .
dataset pre training tasks mrr map p p p zxing w .
.
.
.
.
w n. e. .
.
.
.
.
w n. e. t. r. .
.
.
.
.
pde w .
.
.
.
.
w n. e. .
.
.
.
.
w n. e. t. r. .
.
.
.
.
aspectj w .
.
.
.
.
w n. e. .
.
.
.
.
w n. e. t. r. .
.
.
.
.
jdt w .
.
.
.
.
w n. e. .
.
.
.
.
w n. e. t. r. .
.
.
.
.
swt w .
.
.
.
.
w n. e. .
.
.
.
.
w n. e. t. r. .
.
.
.
.
tomcat w .
.
.
.
.
w n. e. .
.
.
.
.
w n. e. t. r. .
.
.
.
.
level .
it can be attributed that the undivided bug inducing changeset carries enriched semantic information which can be captured by semanticcodebert.
this again confirms the effectiveness of the semanticcodebert based bug localization technique.
.
ablation study to evaluate the design choices in the proposed model we conduct several ablation studies.
to begin with as shown in table we analyze the contributions of node alignment edge prediction type prediction and role prediction pre training tasks on the six projects with commits granularity.
n. e. t. and r. denote the node alignment edge prediction type prediction and role prediction pre training tasks respectively.
with all of these pre training tasks we train semanticcodebert according to the proposed new code representation sfg.
according to the results after adding type and role prediction pre training tasks the obtained performance has universally improved.
this result suggests that leveraging the node attributes type and role is vital to learn code representation.
furthermore we evaluate the effectiveness of the hierarchical momentum contrastive bug localization hmcbl technique on the six projects with commits granularity.
as illustrated in table for w o hmcbl the memory bank and hierarchical contrastive loss which leverages similarities at different levels do not exist and only the representation obtained by the encoder is utilized to calculate similarity.
to demonstrate the generality the technique is evaluated with different pre training models as the encoder of the changeset including bert graphcodebert and semanticcodebert.
it is observed that overall much better performance will be obtained with hierarchical momentum contrastive learning which provides largescale negative sample interactions for representation learning andpre training code representation with semantic flow graph for effective bug localization esec fse december san francisco ca usa table ablation study of hierarchical momentum contrastive bug localization hmcbl technique where gcbert and scbert are short of graphcodebert and semanticcodebert.
technique dataset mrr map p p p bert w o hmcbl fbl bert zxing .
.
.
.
.
pde .
.
.
.
.
aspectj .
.
.
.
.
jdt .
.
.
.
.
swt .
.
.
.
.
tomcat .
.
.
.
.
gcbert w o hmcblzxing .
.
.
.
.
pde .
.
.
.
.
aspectj .
.
.
.
.
jdt .
.
.
.
.
swt .
.
.
.
.
tomcat .
.
.
.
.
scbert w o hmcblzxing .
.
.
.
.
pde .
.
.
.
.
aspectj .
.
.
.
.
jdt .
.
.
.
.
swt .
.
.
.
.
tomcat .
.
.
.
.
bert w hmcblzxing .
.
.
.
.
pde .
.
.
.
.
aspectj .
.
.
.
.
jdt .
.
.
.
.
swt .
.
.
.
.
tomcat .
.
.
.
.
gcbert w hmcblzxing .
.
.
.
.
pde .
.
.
.
.
aspectj .
.
.
.
.
jdt .
.
.
.
.
swt .
.
.
.
.
tomcat .
.
.
.
.
scbert w hmcblzxing .
.
.
.
.
pde .
.
.
.
.
aspectj .
.
.
.
.
jdt .
.
.
.
.
swt .
.
.
.
.
tomcat .
.
.
.
.
increases retrieval accuracy.
for instance compared with bert w o hmcbl which is the fbl bert exactly bert w hmcbl improves the performance in terms of mrr scores for more than projects by .
to .
.
it is indicative of the observation that the hierarchical momentum contrastive bug localization technique can be extended as a general and effective framework with different advanced pre training models.
.
threats to validity our results should be interpreted with several threats to validity in mind.
as bug inducing changes are identified using the szz algorithm one threat to the internal validity of the results is possible noise introduced by szz may make the mapping between bug reports and bug inducing changesets not very precise.
however the dataset used in the study has been validated manually so this threat is minimized.
another threat to internal validity is the dataset may contain tangled changes .
while we do believetangled changes can affect our results the dataset has been widely used for changeset based bug localization studies and removing tangled changes completely is extraordinarily difficult.
with regard to threats to external validity one potential issue is that the evaluation is conducted on a limited number of bugs from several open source projects.
however these projects feature various purposes and development styles.
also the dataset can be considered as the de facto evaluation target for changeset based bug localization studies and prior studies have widely used it .
conclusion we aim to advance the state of the art bert based bug localization techniques in this paper which currently suffer from two issues the pre trained bert models on source code are not robust enough to capture code semantics and the overall bug localization models neglect the necessity of large scale negative samples in contrastive learning and ignore the lexical similarity between bug reports and changesets.
to address these two issues we propose a novel directed multiple label semantic flow graph sfg which compactly and adequately captures code semantics design and train semanticcodebert on the basis of sfg and design a novel hierarchical momentum contrastive bug localization technique hmcbl .
evaluation results confirm that our method achieves state of the art performance.
data availability our replication package including code model etc.
is publicly available at