ieee acm 44th international conference onsoftwareengineering icse adaptive test selection fordeep neural networks xinyu gao xinyugao smail.nju.edu.cn state key laboratory fornovel software technology nanjing university nanjing china zixi liu zxliu smail.nju.edu.cn state key laboratory fornovel software technology nanjing university nanjing china abstractyang feng fengyang nju.edu.cn state key laboratory fornovel software technology nanjing university nanjing china zhenyu chen zychen nju.edu.cn state key laboratory fornovel software technology nanjing university nanjing china keywordsyining yin ynyin smail.nju.edu.cn state key laboratory fornovel software technology nanjing university nanjing china baowenxu bwxu nju.edu.cn state key laboratory fornovel software technology nanjing university nanjing china deep neural networks dnn have achieved tremendous development inthepast decade.
while many dnn driven software applications have been deployed tosolve various tasks they could also produce incorrect behaviors andresult inmassive losses.
toreveal theincorrect behaviors andimprove thequality ofdnn driven applications developers often need rich labeled data forthetesting and optimization ofdnn models.
however inpractice collecting diverse data from application scenarios andlabeling them properly isoften ahighly expensive and time consuming task.
inthis paper weproposed anadaptive test selection method namely ats fordeep neural networks toalleviate this problem.
ats leverages thedifference between themodel outputs tomeasure thebehavior diversity ofdnn testdata.
and itaims atselectingasubset with diverse tests from amassive unlabelled dataset.
weexperiment atswith four well designed dnn models andfour widely used datasets incomparison with various kinds ofneuron coverage nc .
the results demonstrate that ats cansignificantly outperform alltestselection methods inassessing both fault detection andmodel improvement capability oftestsuites.
itispromisingtosave thedata labeling and model retraining costs fordeep neural networks.
ccs concepts software and itsengineering software testing and debugging.
yang feng isthecorresponding author.
permission tomake digital orhard copies ofallorpart ofthis work forpersonal or classroom useisgranted without feeprovided that copies arenotmade ordistributed forprofit orcommercial advantage and that copies bear this notice and thefullcitation onthefirst page.
copyrights forcomponents ofthiswork owned byothers than theauthor s must behonored.
abstracting with credit ispermitted.
tocopy otherwise orrepublish topost onservers ortoredistribute tolists requires prior specific permission and or afee.request permissions from permissions acm.org.
lese may pittsburgh pa usa copyright held bytheowner author s .
publication rights licensed toacm.
acm isbn ... .
73deep learning testing deep neural networks adaptive random testing testselection acm reference format xinyu gao yang feng yining yin zixi liu zhenyu chen and baowen xu.
.
adaptive test selection fordeep neural networks.
in44th international conference onsoftware engineering icse may pittsburgh pa usa.
acm new york ny usa 13pages.
https ll .
.
1introduction deep neural networks dnn have been deployed inmany fields toassist insolving various tasks such asmedical image diagnosis autonomous driving customer services machine translations and soon.asthednn driven software demonstrates such fantastic performance onwell defined tasks influencingourdaily activities andlives their quality and reliability have raised wide concerns.
dnn driven software essentially isonekind ofsoftware program could also suffer from software defects that may cause monetary andeven human lifelosses .
therefore fordnn driven software quality assurance techniques have become exceedingly demanded.
however assuring thequality ofdnn driven software isachallenging task duetothenatural differences between dnn models and conventional software systems.
while conventional software systems often rely ondevelopers toconstruct thebusiness logic manually dnn models which form thekernel part andempower thednn driven software employ adata driven programming paradigm that needs tolearn theinternal logic from massive data .
this feature notonly makes thebehavior ofthednn model difficult tointerpret and analyze butalso disables theapplication of many conventional quality assurance methods.
with millions or even billions ofneurons co ections and activation functions dnn models construct complex nonlinear transformations tomap input features into theproper labels.
thus itishard fordevelopers tooptimize thednn driven software bymanually tuning theinternal parameters ofthednn model.
inpractice developers often authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa xinyu gao yang feng yining yin zixi liu zhenyu chen and baowen xu 1https llgithub.comisate lab ats2background this section includes several basic knowledge ofdnn neuron coverage and testselection methods.
.
the architecture ofdeep neural network the architecture ofthedeep neural networks dnn canberepresented asacomposite function chain that maps theinput data xto theoutput result y. y f x lo j l ... j d x y yly ern ilylll vi y i ?
o l iii ey ii i i i thecontributions ofthispaper could besummarized asfollows method.
wemodel thefault pattern and design ametric toevaluate their differences fordnn models.
based onthe fault pattern metric wepropose ats thefirst adaptive random testselection method fordnn models.
tool.
weimplement ats into atool that could help dnn developers toselect testcases from massive unlabelled data.
wehave released thesource code ofthetoolandexperimentaldatasets online .
study.
weconduct anextensive experiment toinvestigate theperformance ofats method coverage guided selection methods and testprioritization methods.
the results show that ats cansignificantly outperforms other test selection methods and efficiently enhances thednn model.
and thesetofallground truth isdenoted aslinthispaper take ann category classifier asanexample thegoal ofadeep neuralnetwork istoapproximate ytothetheoretical classifier y .in other words byadjusting theinside weights ofthednn model wehave y y which gives anoutput vector y f x closeto the one hot vector i.e.
corresponding ground truth i x .the finaloutput result iscalculated through thesoftmax function.
itcan beinterpreted asaprobability allelements arepositive and the sum is1 .thus theoutput domain ysatisfies theconstraint that anlong them thelength dofthechain means thedepth ofthe network.
the function inthemiddle ofthechain i.e.
fu j i ... d i isthehidden layer ofthednn.
each hidden layer fu maps theforward input into avector and each element of thevector isanindependent unit called aneuron representing a vector to scalar function.
thedimensionality dofthehidden layer fu means thewidth ofthenetwork.
inside each layer allneurons ofthis layer areindependent and actinparallel.
therefore forad layer deep neural network model mwith fixed width w ifaninput date xiscalculated bythemodel dnn weusehierwtorepresent thei layer vector value.
combiningallhidden layers vector wecould getahidden layer matrix her wxd l which contains allthehidden neuron values ofan input data.
given adnn model dnn andaninput data x denote theset neuron dnn x torepresent thehidden output values whendnnretrain thednn model with rich data tofixtheincorrect behaviors and improve theperformance ofdnn driven software.
inthisprocess they need tocollect agreat many ofdata from application scenarios andhire alarge workforce tolabel them.
under this situation identifying and selecting themost representative data become critical forimproving theeffectiveness and efficiency ofquality assurance tasks ofdnn driven software.
inspired bythegreat success that thestructural code coverage criteria achieved inconventional software programs some prior research hasproposed structural neuron coverage tomeasure theadequacy ofdnn testing .
these researches have demonstrated theeffectiveness ofstructural neuron coverage ondistinguishing thegeneral mutation tests andadversarial samples.
however similar totheconventional code coverage structural neuron coverage also requires anextremely high overhead inthecollection process thus itisdifficult toapply them onthelarge scale models .
besides several independent research groups also reveal that some neuron coverage criteria could rapidly reach the maximum coverage with very few tests which may restrict their effectiveness asaguidance criterion fortest selection .
ontheother hand another family oftechniques isproposed toprioritize testcases based onsome rules .
test prioritization techniques often value testcase that hasahigh probability ofdetecting dnn incorrect behaviors.
these techniques have demonstrated the high effectiveness ofcollecting aportion oftests from alarge size ofthedataset however they failtoconsider therelationship and distribution ofselected data andthus cannot diversify thedetected incorrect behaviors.
this feature may fundamentally hinder their applications especially when there areplenty ofsimilar orduplicate tests inthecandidate set.
this paper extends theabove techniques andalleviates their limitations.
weexplore analternative solution toassist thetestselection ofdnn models.
wefirst employ theoutput vector ofadnn model torepresent themodel behaviors ofthegiven input.
then wedefine thelocal domain ofthednn model output todescribe thefault pattern.
byprojecting thebehavior information into different local domains wecanevaluate thefault pattern ofthetest cases through extended operation which values both themodel uncertainty and behavior diversity.
further wedesign afitness metrictomeasure thefault pattern difference between thecandidate test and theselected set.based ontheabove design wepropose ats thefirst adaptive test selection method fordeep neural networks toselect more diverse tests from thecandidate set.ats can select asubset that reveals more different faults inthednn driven software and reduces thelabeling efforts fortheoptimization process.
tovalidate theeffectiveness ofats weconduct experiments for ats and baseline methods with four well designed dnn models andfour widely used datasets.
wealso realize different pollutions ofunfiltered datasets inreality and simulate them inourexperiments.
the experimental results demonstrate that ats performs well indefect detecting tasks.
moreover wealso prove that our adaptive test selection can select diverse defects faster than prioritization techniques which blindly select thetest cases without considering differences.
finally wedemonstrate that ats ismore effective than both thecoverage guided testselection methods and testprioritization methods fordnn driven system optimization.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
adaptive test selection fordeep neural networks lese may pittsburgh pa usa isexecuted with x .
test case selection methods and thefinal output result calculated bysoftmax function isdenoted as .
neuron coverage criteria inspired bytheeffectiveness ofstructural coverage onguiding the testing ofconventional software applications researchers have proposed many testing criteria based onthestructural neuron coveragetomeasure thetestadequacy ofdeep neural network systems.
with theguidance ofstructural neuron coverage several testgeneration techniques have been proposed toimprove theperformance ofdnn models.
inthis section webriefly introduce those structural neuron coverages.
neuron activation coverage nac k .
astheearliest neuron coverage criterion theprimary assumption ofnac k built upon isthat themore neurons areactivated indicates more states ofdnn areexplored.
the computation process ofnac k requires collecting each neuron s output value and counting neurons ascovered iftheir outputs exceed thethreshold k.the nac k coverage ofatestiscomputed astheratio ofthenumber ofcovered neurons tothetotal number ofneurons.
k muitisection neuron coverage kmnc k .
based onthe nac k assumption about thednn states theresearchers further assume that instead oftreating theneuron asavalue with only two states activated andinactivated buttreats theoutput ofaneuron asarange ofvalues .
inother words suppose that theoutput of aneuron 0onthetraining setisintheinterval iowa highal and divide them equally into ksegments.
the goal ofthekmnc k criterion istomake theneuron cover each segment ofksegments.
neuron boundary coverage nbc .
different from kmnc k neuron boundary coverage nbc focuses onwhether thecorner regions iowa and .
strong neuron activation coverage 5nac k .
some studiespoint outthat strongly activated neurons may have additional value fordnn so5nac k was proposed .
strong neuron activation coverage 5nac isasimplification ofnbc which only collects theratio ofsome neurons that cover theupper bound high a toallneurons.
top k neuron coverage tknc k .
top k neuron coverage tknc k focuses onthemost active kneurons ineach layer .
itiscomputed bytheratio ofthetotal number oftop k neurons on each layer tothetotal number ofneurons inadnn.
modified condition decision coverage mc dc .
similar totheconcept ofconventional software testing mc dccriteria of neuron networks models theneuron output value orsign as adecision andalltheprevious layer co ected neurons aremodeled asconditions.
neuron network mc dc consists offour implementations namely 5v v5 andvv coverage andallofthe above mentioned neuron coverage criteria could beregarded asa specific situation oftheoriginal mc dc neuron coverage .inthissection weintroduce several widely used testcase selection methods.
foranytestselection method thegoal istoobtain afixed size n subsetxs from thecandidate setx c i.e.
wehavexs xc and ixsl n. .
.
prioritization testselection.
generally foragiven candidate setxc prioritization test selection methods compute aweight p foreach test case xinthecandidate set.the weight prepresents thetestsignificance oftestx.inother words pisthepossibility of revealing model errors.
itcould bedenoted asfollows p priority x xexc the testcase with higher priority pissupposed tohave ahigher value ofdetecting faults andenhancing thednn model.
therefore theprioritization testselection canberepresented asfollows xs argmax i ipriority x xs xcaixsl n xexs simply speaking thepriority method oftestcase selection isto select thetestcases with top n weights toform atestsuite ofthe required size.
here weintroduce four different prioritization selection methods which come from different fields ofaisoftware testing and traditional airesearch.
deepgini.
recently feng etal.
propose atest prioritization technique based onastatistical perspective ofdnn named deepgini.
ittakes theuseofthegini coefficient tomeasure the likelihood oftestcase xbeing misclassified.
lsa.
kim etal.
propose atest criterion towards dnn testing called sadl surprise adequacy fordeep learning .
inthe research lsa likelihood based surprise adequacy isproposed tomeasure how close totheclass boundary thenew inputs are.
the higher lsa value ofthetest means itismore surprising to thednn.
thus itcould beregarded asapriority weight fortest selection.
ces.
lietal.
propose atestselection method based onconditioning which istoassess thenew precision onoperational environments.
the experiment results show that ces cross entropybased sampling estimator outperforms random sampling inallexperiments.
maxp.
maxp isarepresentative uncertainty sampling strategy ofactive learning .itemploys themaximal prediction probabilityoftheclassification task toindicate theprediction confidence of theclassification model and thus prioritizes theinput oftheleast prediction confidence.
.
.
coverage guided testselection.
inthis section weintroduce asetoftestselection methods guided bycoverage metrics.
inconventional software testing testers tend toselect atest case that covers more different code lines .
this kind ofselection method follows abasic assumption that early reaching themaximum coverage would lead tothehigher capability ofdefect detection .
forany coverage metric ineach iteration itfollows anadditional greedy algorithm toselect thenext test according tothe feedback from thepreviously selected set i.e.
select thetestcases that covers themaximum number ofuncovered area ofthegiven coverage criterion.
run dnn x yeyneuron dnn x h erwxd l authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa 3methodology thekeyfeature ofourselection method istoselect testcases with diverse failure directions andhigher failure probabilities.
first we introduce amotivating example foradaptive testselection ofdnn testing.
then wepropose amapping relationship which converts theoutput domain into asetofintervals todescribe thefault pattern ofagiven test orset.after that wepropose afitness metric tomeasure thedifference between thecandidate test and theselected set.finally wepropose theadaptive test selection method ats based onfault pattern and fitness metric.
.
motivation and inspirations fortheconventional software program there hasbeen some work discussing theshape andthelocation offaults from theperspective oftheinput domain .
toachieve aneven spread of test cases within theinput domain chen etal.
propose adaptive random testing art based ontheanalysis offault patterns.
even though inthepast decade plenty oftechniques have been proposed forimproving art andextending itsapplication scenarios itisdifficult toapply them tothednn model testing tasks because oftheir working nature andinput features.
the input data ofmodern dnn models areoften ofvarious types such asimages point clouds texts speech signals and so on.this variance makes itdifficult forustomeasure thefault pattern from theinput domain.
nevertheless foragiven testinput the dnn model often produces avector containing theprobability of labels andthus determines thefinal output based ontheprobability distribution.
thus based ontheprobability distribution selecting thetestwith higher uncertainty may obtain ahigher probability ofdetecting dnn faults.
foragiven output vector yern theprocess could beformalized asrun dnn x y.foraclassification model weregard aninput data xisclassified asi thcategory iff thei thelement ofyisthemaximum element i argmaxi y i .
iftheoutput probability oftheinput data ismore concentrated max y i iscloser to1 weassume that themodel hashigher confidence forclassification results.
further thetestcases which aredifferent from each other could reveal more diverse faults ofdnn models.
formultiple testcases with thesame degree ofuncertainty asimilar testmay correspond tothesame fault and thetest cases that differ from each other could better reveal different faults ofthemodel.
forexample if wehave atest setx xl x2 x3 x4 .
the output vectors are y1 .
.
y2 .
.
y3 .
.
.
and y4 .
.
.thus wecanregard thednn model asmore confident fortheclassification result ofxl and x2 x4 aremore likely tobesimilar.
above observations inspire ustointroduce a metric tomeasure thedifference offaults from theperspective of theoutput domain.
based onthismetric wedesign andimplement ats toguide thetestselection ofdnn models.
.
fault pattern computation wedesignthe fault pattern mapping pattern which converts the output vector into several intervals tosatisfy theinsights introduced above.
the sizeand location ofthesubsets reflect theinformation ofthemodel s uncertainty and test case x sfault pattern.
76xinyu gao yang feng yining yin zixi liu zhenyu chen and baowen xu .
... li4e ......i i figure asiinplified figure toillustrate step.
.
project and extend .
such amapping assists usinanalyzing andextracting thefault pattern distribution corresponding toeach test case from theoutput domain.
ontheone hand wedefine theuncertainty oftest case xetiasuncertain x run dnn x i .
forexample ifwe have anoutput vector y .
.
.
then theuncertainty is denoted as1 max y i .
.
ontheother hand weexpress thedirection astheline from theprediction one hot vector lito theoutput vector y.forexample fortheoutput vector ydenoted above itsprediction vector isdenoted asl3 thus the direction isdenoted asl y .
.
.
.finally wedesign thefault pattern based onboth direction and uncertainty.
.
.
fault pattern aftest case.
next weintroduce themain steps ofcomputing thefault pattern.
foratestsett thefault pattern of each test xexiscalculated byfour steps.
stepl.
test setclustering foreach test case xet wecluster thetestcase into nsubset oftbased onitsprediction category i.e.
wehave xetdffi argmaxi run dnn x i argmaxi y i .clustering based ontheprediction category ensures that wecananalyze testcases with similar results.
step2.
local domain determining foreach test case xinclusterhweconstruct anindex setind i i p q ip q i p q n p qen .
each element inind i represents alocal domain which isaplane spanned bylilpand lilq.byanalyzing each local domain onebyone wecould extend fault pattern information totasks with anynumber ofcategories n2 .
step3.
project operation this step aims toextract local information oftest case x soutput vector y.refer tofig.
the goal ofproject istofind they ef ..lilplqsubject to y y j..span lilp lilq .
specific calculation formula can referto .
through calculation wecould getthelocal information y corresponding toindex i p q .
step4.
extend operation extend thesubspace vector y determined by i p q toaninterval denoted aspattern x i p q .refer tofig.
weimplement theoperation inthetriangle f ..lilplq wedetermine theintersection point byextending l y tolplq.the intersection point represents the local direction ofthetest case x.after that thelocal fault pattern isdesigned asasegment gkinline lplq.the midpoint ofsegment gk istheintersection and thelength is determined bytheuncertainty ofthetestcase x.finally we normalize thelplqtotheinterval and consider the local fault pattern asthenormalized interval authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
adaptive test selection fordeep neural networks lese may pittsburgh pa usa ofsegment gk.
thenormalized length of iscontrolled y i by ry where i isahyperparameter tocontrol thegranularity.
werecord thelocal fault patterns oftestxeticorresponding toindex i p q aspattern x i p q .
steps.
local pattern gathering foreach test case xincluster hwecollect allofitslocal fault patterns patternx i p q where i p q isinind i thesetoflocal patterns isregarded astestcase x sfault pattern.
inother words wehave that pattern x u i p q elnd i i p q .when test xbelongs tocluster hthefitness metric could be expressed asfollows .
i ipatternx i p q pattern5 i p q 1fitness x ..i pattern x i p q upatterns i p q i l p q elnd l noted that theoperator and uineq.9arebasic setoperators and i.irepresent thelength ofcorresponding set.the fitness metric reflects thedifference between thetest xand the selected set5.
table anexample toillustrate fault pattern.algorithm ats test selection alg.
.
.
fitness metric.
the fitness metric aims toprovide anormalized value toquantify thefitness ofeach candidate xfrom thecandidate setcagainst theselected set5 denoted asfitness x .
.
fault pattern fitness metric design with thehelp offault pattern wepropose afitness metric that assesses thedifference between testcase xand theselected testset .based onthismetric wecould migrate thebasic idea ofadaptive random testing art into dnn test case selection.
.
.
fault pattern oftest set.
toobtain thefault pattern ofa given testsett weneed tomerge thefault patterns calculated by each testcase.
inother words forcluster ti wetake theunion ofall corresponding local fault patterns under each specific local domain i p q denoted aspatternyi u i p q elnd i i p q .noted that s isasubset ofnormalized segment interval which is obtained by5 uxetipatternx i p q .
finally wegather together fault patterns ofallclusters toget thefault pattern ofthegiven testsett patterny u u i p q yi i l .. n i p q elnd i example .here weintroduce a4 category classification exampletoillustrate thefault pattern introduced above.
forthetestcase xand itsoutput result run dnn x y .
.
.
.
.
first wedetermine i argmaxiy i .
step!.
second when n theindex setofcluster tiisconstructed asind i .
step2.
after that thelocal information y of output yiscalculated step3.
which isshown incolumn .
based ontheextend operation step4.
theintersection points are shown inintersect column.
thelength oflocal fault pattern iscalculated by l yl .
i 5inthisexample andthecovered interval of i p q isshown inthelast column.
finally wecould gather thefault pattern oftestcase x steps.
.
pattern x .
29return selected list 1procedure ats dnn c x n 2collect output vectors run dnn x 3determine output vector dimension n 4cluster cinto nsubset c i n 5construct index sets ind i i n 6foreach testxec calculate fault pattern pattern x 7initial selected subset ... 5n 8initial selected testcase list 9forj ... l!ifjdo evenly select from each cluster fori ... ndo ifc 0then forxec do lcalculate fitness x ifmax xfitness x 0then x argmax xfitness x select test xwith maximum fitness .append x added inorder u x c c x .
.
overall procedure.
wedivide thewhole selection procedure into two parts cluster bycluster selection and total selection.
from line 9toline wetend toevenly select test cases which have maximum fitness metric fitness x 5i according totheselected subset.
thus wecould select thetest case with different fault patterns and higher uncertainty ineach cluster.
ifthecandidate set isunbalanced there may exist asituation that wecannot select19c u l .....nc 20while size ndo forxecdo 22lcalculate fitness x ifmax fitness x 0then 24lx argmax xfitness x select test xwith maximum fitness else lx argmax x l i.p.q elnd i 1patternx i p q i select test xwith higher uncertainty .append x c c x linter local fault sect patternindex pattern x .
.
.
.
pattern x l .
.
.
.
pattern x l .
.
.
.64run dnn x yll0.
.
.
.1i authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa enough test cases forsome clusters.
from line 20toline we select therestpart oftheselected sets.first weprefer test cases with higher fitness metric results.
inlines ifalltest cases inthecandidate sethave thesame fitness scores then wesimply select thenext case based onitsuncertainty.
table asiinplified example ofats.xinyu gao yang feng yining yin zixi liu zhenyu chen and baowen xu generation methods baseline approaches and research questions.
toconduct theexperiments weimplement ourapproach aswell as various nc guided testselect methods upon keras .
.
with tensorflow .
.
.
allexperiments areperformed onaubuntu .
.
lts server with two nvidia tesla v100 gpu one core processor lntel r xeon r gold cpu .50ghz and 120gb memory.
candidate testcase iori round 1iset pattern pattern fitness patternround fitness4.
datasets and dnn models xl .
.
.
.
x2 .
u .
.
x3 .
.
.
.
x4 .
.
x5 .
.
.
.
example .inthispart weintroduce asimplified example ofats.
weomit theprocess ofclustering step1.
local domain determining step2.
and local fault patterns gathering steps.
.
thus only thecore part oftheselection procedure isintroduced.
inround weshow thefitness between original fault pattern and h ul fh.
ipattern x patterns 1tea tpattern 0eac test l.e.
ipatternxupatterns i forexamp e fttf t orescase xl iness xl .
.
u 1i j i .
.
.
based online 15inalg.
weselect out test x4inthefirst round.
and incolumn setpattern weupdate thefault pattern oftheselected set.
inthenext round column round wecalculate thefitness between each testincandidate set xl x2 x3 x5 and u .
finally inround weselect outthemaximum fitness testcase x5.
.
enhancing thednn model with ats both testing and enhancing thednn driven system need torely onmanually labeled data.
collecting plenty ofunlabeled data is usually easy toachieve.
however compared with thecost ofdata collection thecost ofmanual labeling ismuch greater.
fordata with strong expertise knowledge such asmedical image data itis unrealistic toblindly label allcollected data.
therefore thesignificance ofdnn testselection istoreduce thelabeling cost.
such an idea isalso inspired byactive machine learning which aims toselect data near thedecision boundary uncertainty .
programmers suppose that these uncertain data could beused tooptimize thedecision boundary ofdnn more efficiently.
besides ats also payattention tothedistributionselected data subset.
the success ofadaptive random testing art proves that theselection strategy based onpreviously selected data feedback ismeaningful.
from theperspective ofthednn model thecomplexity ofthedecision boundary makes uncertainty prioritization prefers aspecific fault pattern.
tosum up ats could notonly select testcases todetect model faults more efficiently butalso construct amore proper subset for dnn optimization through retraining.
4experiment design this section introduces our experimental settings including the data setand dnn model used intheexperiment select data set 78table dataset and dnn models.
dataset description idnn model neurons layers 28x28 hand ilenet mnist written digits lenet 32x colored iresnet cifar lillages vgg 28x28 gray lenet fashion scale images resnet street view lenet svhn numbers vgg intheresearch theexperiments areconstructed onfour wellknown publicly available dnn datasets mnist cifar svhn andfashion.
table 3presents thestatistical details onthese datasets.
mnist dataset isahandwritten digits dataset with 10labels.
itcontains input data intotal ofwhich aretraining data and aretest data.
cifar dataset consists of60 32x32 colour images in10classes with images per class.
cifar isdivided into training and testing images.
fashion isadataset ofzalando s article images consisting ofatraining setof60 examples and atestsetof10 examples.
each example isa28x28 grayscale image associated with alabel from 10classes.
svhn isareal world image dataset that canbeseen assimilar tomnist.
butitincorporates anorder ofmagnitude more labeled data over digit images .
itiscollected from house numbers ingoogle street view images.
atthe same time weselect four different scale dnn models toensure the universality ofourexperiments which arelenet lenet vgg and resnet .
foreach data set weselect two different dnn models toensure thecriterion result isstable and excellent ondifferent combinations.
.
candidate setconstruction tosimulate thedata mutation inrealistic settings wechoose tofollow theprior research convention and employ seven welldesigned benign perturbations rather than adversarial examples generators fordata augmentation.
wemake this choice isbecause adversarial examples areoften generated from carefully designed algorithms they cannot represent data collected from the real world application scenario andmay lead tounreliable conclusions .
foreach dataset wegenerate thetestdata based onseven wellused benign perturbations toretain itsoriginal label including authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
adaptive test selection fordeep neural networks shift zoom brightness rotation shearing blur and contrast ratio .when theoriginal test size isn foreach augmentation operator wegenerate thesame amount ofdata.
wedivide the original test setand each generation setinto two parts one for constructing thecandidate setand theother forconstructing the new test set.forthemnist dataset wefinally constructed acandidate setand anew test setwith thesame size i.e.
original testdata and5000 generated data forseven augmentation operators.
furthermore considering that unfiltered candidate data may include pollution and invalid data.
foreach generated candidate set wealso design different data pollution scenarios tocover differenttypes ofinvalid data that may exist inanunfiltered dataset.
in addition tothepure candidate set weconstruct four polluted candidate sets with additional invalid data including irrelevant data meaningless synthetic data repeat data and crashed data.
tosum up weconstruct five candidate sets foreach dataset including apure valid data setand four unfiltered datasets with part ofinvalid data.
.
baseline approaches during thetestselection wetake therandom sampling asabaseline method naturally.
wedenote this baseline asrs.
rsdraws samples randomly from thecandidate testsetaccording tothetargetsize.
next weintroduce other baseline approaches used inexperiments.
the baseline selection methods aredivided into two types coverage guided testselection andprioritization testselection.
we choose four typical techniques foreach type.
.
.
coverage guided testselection.
tocompared with coverageguided test selection methods weselect 4well known dnn neuron coverage criteria nac nbc tknc and snac toguide thetest selection procedure introduced insec.
.
.
.
for themore specific introduction refer tosec.
.
.noted that limited bythecomputation resource weabandoned kmnc and mc dc asthebaseline because even forasimple dnn model lenet they take more than 24hours toselect test cases.
the configurable parameters oftheneuron coverage criteria follow theauthors suggested settings oremploy default settings of theoriginal papers .
.
.
prioritization testselection.
wealso choose four prioritization selection methods introduced insec.
.
.
tocompare theeffectiveness with widely used prioritization methods.
more specifically weconduct theexperiments with deepgini lsa ces andmaxp .
noted that fordeepgini weabbreviated itasgini intherest ofthepaper.
.
research q.yestions ats isdesigned toadaptively select thenext testcase according to thepreviously selected set.itaims toselect anappropriate subset formodel faults detection and dnn model optimization.
based on thegoals oftest selection weempirically explore thefollowing research questions rq .
.
.
rql fault detection.
can ats detect more faults than baseline approaches?
79lese may pittsburgh pa usa similar toconventional software testing foragiven test selection method aselected setthat can trigger more faults means it could reveal more defects inthesoftware.
therefore wefirst compare thefault detection capabilities ofatsandbaseline approaches.
ineach dnn model dataset weselect sizeofeach candidate set filter theinvalid data and collect thecorresponding fault detection rate.
foraselected test setx thefault detection rate is defined asfollows .
ixwronglfault detectlon rate x ixi where ixiidenotes thenumber oftest cases being misclassified and ixidenotes thesize oftheselected set.
.
.
rq2 fault diversity.
can ats select testcases that cover more diverse faults?
for traditional software testing chan etal.
have observed that failure causing inputs usually arevery dense andclose tooneanother.
such insight could bemigrated todnn model testing i.e.
similar faults may reflect thesame defect indnn.
inorder toanalyze themodel more comprehensively wehope thetest selection methods could notonly detect more faults butalso detect more diverse faults efficiently.
weuseaconcept offault type toanswer this question.
fora given testcase xbeing misclassified itsfault type isdefined as fault type x label x label x where label x denotes theground truth label and label x denotes theprediction label calculated bydnn model.
forexample ifahandwritten digits xwith true label ismisclassified into i then thefault type ofxisdenoted as fault type x .
foreach candidate setwith tencategories thenumber offault types is10x9 .therefore foreach dnn model and dataset combination weaggregate thefault types ofthefive candidate sets thetotal number offault types foraspecific dnn dataset is 90x5 .
wecollect thetheoretical maximum number offault types and compare thechanging trend ofthenumber ofdetected fault types.
toquantify thecapability ofselecting diverse faults wecollect thecumulative sum ofthefault types found byspecific test selection methods and compute thecorresponding rauc ratio of area under thecurve between theselection method and theoreticalcurve.
.
.
rq3 optiinization effectiveness.
does thetestcases selected byats guide theretraining more effectively?
different from thetraditional software thednn model could not beenhanced directly.
therefore wepropose rq3 tofurther evaluate theeffectiveness ofretraining thednn model with selected testcases.
toanswer rq3 wechoose four selected setwith different sizes .
.
toadd back into theoriginal training setand retrain thednn model.
and theeffectiveness of model optimization iscompared onthenewly constructed testset introduced insec.
.
.
the experiment isrepeated twice inallcombinations toavoid random errors intheprocess ofretraining.
furthermore wenot only compute theaverage accuracy improvement butalso implement thewilcoxon rank sum test tocheck whether theats are authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa statistically significant outperform other baseline approaches at thesignificance level of0.
.
5result analysis inthissection wepresent theresults offault detection rql fault diversity rq2 and further analyze whether ats could optimize thednn model more effectively rq3 .
.
answer torq1 fault detection asshown intable wecompare theaverage fault detection rate between ats and baseline methods.
limited bythespace weonly display two selection ratio results .
other results follow thesame trend.
first wefound that allcoverage guided testselection methods have apoor performance indetecting faults.
such afinding isconsistent with theformer research .compared with some existing works that demonstrate thefault detection capability of neuron coverage inthetestsetwith adversarial examples weassume that thefaults generated bydata mutation inrealistic settings aremore natural yetchallenging tobedetected.
one of thepossible illustrations totheineffectiveness ofneuron coverageguided test selection may bethat compared toadversarial examples benign perturbations could notlead tomany different neuron activation states.
besides wealso find outthat theneuron coverage iseasy toreach themaximum during theselection process and nolonger beincreased which isalso supported byexisting researches .
compared with random sampling rs baseline both ats and most prioritization selection methods show ahigher capability of detecting more faults.
ats hasthebest result inmost dnn model dataset combinations.
weconclude that ats hasanoutstanding capability ofdetecting more faults within alimited selection size.
.
answer torq2 fault diversity asshown infig.
foreach dataset dnn model combination wedraw thecumulative sum curve ofthefault types foreach selection method.
note that thedark blue curve onthetoprepresents thetheoretical maximum number offault types that could beincluded inthecorresponding subset size.
besides wealso compute theratio ofarea under thecurve raue between theoretical and each selection method.
the closer therauc isto1 means thecorresponding selection method performs better indetecting more diverse faults.
the results ofrauc areshown intab.
.compared with random sampling rs baseline ats gini maxp and tknc show better results consistently.
however different from theresults shown inrql ats achieved thebest results under alldataset dnn model combinations.
for example asforfashion lenet l both gini and maxp show a higher fault detection rate than ats intab.
.inrq2 therauc ofgini and maxp areonly .
and .
respectively while therauc ofats is91.
.
this phenomenon means that although prioritization test selection methods can sometimes detect more faults thetestcases selected bythese methods may have anuneven distribution.
inother words prioritization test selection methods may prefer aspecific type offault.
there does notexist anadaptive adjustment step forthepriority listtoadjust theweight based on 80xinyu gao yang feng yining yin zixi liu zhenyu chen and baowen xu selected setfeedback.
such selection methods areeasy toselect a subset with limited fault types which may have anegative impact onmodel optimization.
.
answer torq3 optimization effectiveness the final goal ofdnn testing istodetect faults and optimize the dnn model toimprove thegeneralization ofthednn model.
thus weevaluate theeffectiveness ofdnn optimization byadding back valid test data into theoriginal training set.wecollect theaccuracy improvement results offour different selection ratios .
.
.
the concrete accuracy improvement results canbe found intab.
.
from theperspective ofselectionmethods theaverage accuracy improvement results ofmost selection techniques perform better than random sampling.
although lsa displays thebest accuracy improvement under thecombination ofmnist lenet theretraining results under other combinations guided bylsa are even worse than random sampling thesame size test cases forretraining.
toevaluate theeffectiveness more precisely weusethe wilcoxon rank sum test tocheck whether ats outperforms other baseline approaches.
weimplement theone side wilcoxon rank sum testtocheck whether thebaseline method isgreater or worse than theats.
ifthep value isless than .
wereject the null hypothesis hoand accept thealternative hypothesis hithat onemethod isstochastically greater than another.
otherwise we regard there isnosignificant difference between thetwo methods.
the results show that nobaseline exceeds ats statistically.
only invery few configurations there aresome prioritization methods that show similar performance toats.
furthermore considering thefashion lenet l discussed above ats achieves asignificant retraining improvement over gini and maxp.
such aresult supports ourassumption that instead ofselecting more faults with less diversity ats could select asubset with enough faults with more fault types which ismore effective toenhance thednn model.
6discussion this section further discusses theats with adaptive random testingand active learning and also demonstrates thethreats tothe validity ofthispaper.
.
the effectiveness ofats from allexperimental results ats ismore effective than other existing test selection methods.
byanalyzing theresults ofother baselines weobserve that although structural coverage criteria are effective inclassic testing applications neuron coverage criteria seem ineffective tobeaguide criterion fordnn testselection.
besides compared with existing prioritization methods ats shows a better andmore stable result.
similar totheidea ofclassic adaptive random testing art ats selects thetest case based onthefitness between each candidate cfrom thecandidate setcagainst theexecuted sete.
however atthesame time art cannot bedirectly applied todnn testing.
one reason isthat different from thelow dimensional inputdomain discussed inart theinput data dimension ofthednn model ispretty large .
the curse ofdimensionality lead tothe authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
adaptive test selection fordeep neural networks icse may pittsburgh pa usa table the average fault detection rate foreach configuration.
selecting tests ii selecting tests fault datadetect i i coverage guided iprioritization iiiicoverage guided iprioritization i model rs nac nbc snac tknc gini ces lsa maxp ats rs nac nbc snac tknc gini ces lsa maxp ats lenet5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
mnistlenetl .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fashionlenetl .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
resnet20 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
cifarvgg16 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
resnet20 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
lenet5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
svhnvgg16 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ato glnl ces nbc snac theo 1z3 !
eraont4lgeof5elect.dtiktsl j ato glnl ces nbc ptlraontag.
ors lectlid t sts ato glnl perl enlilgeofsfllectlldtest5i !
!
ato glnl .
th plin .ntilg.or5fl t izoll a mnist lenet b mnist lenet !
c fashion lenet !
d fashion resnet ato glnl ces nbc ato glnl ces nbc snac thea ato glnl ces .
.... thm an glnl .
thm 1z 567i i ercentageafselectl!
!dnstsl pl!
!rcenta jl!of5electedtestsc j l12 i pl!
!rcentageof5eil!ctedtests 561i percentageofsl!lo!
!rtedtesi!i e cifar vgg !
f cifar resnet g svhn lenet h svhn vgg !
figure the cumulative sum ofthefault types found byspecific test selection methods.
table when selecting tests theratio ofarea under the curve between each selection method totheoretical.
ravci mnist i fashion i cifar i svhn lenet5 lenetllenetl resnet20 vgg16 resnet20 lenet5 vgg16 rs .
.
.
.
.
.
.
.
nac .
.
.
.
.
.
.
.
nbc .
.
.
.
.
.
.
.
snac .
.
.
.
.
.
.
.
tknc .
.
.
.
.
.
.
.
gini .
.
.
.
.
.
.
.
ces .
.
.
.
.
.
.
.
lsa .
.
.
.
.
.
.
.
maxp .
.
.
.
.
.
.
.
ats .
.
.
.
.
.
.
.52invalidation offitness metric.
thus wechoose todesign ats inthe output domain.
the other reason isthat thetarget between random testing and dnn testing isdifferent.
art aims toreduce the resource cost ofconstructing test cases randomly.
itmeans that onlyifthecomputational cost ofart ismuch lighter than rt then wecan consider art isaneffective and efficient test selection method .
however fordnn testing thecost ofmanual labeling isoften extremely expensive especially when thelabeling task requires professional knowledge .
inthis case thecontroversy ofcomputational overheads isnolonger essential fordnn test selection because theresource cost intest execution and data selection ismuch cheaper incomparison with theconsumption of manually labeling.
tofurther discuss thesignificance ofats fordnn testing on thebasis ofrq3 weobtain theaccuracy improvement when retraining themodel with thewhole candidate set.first inrow ori ace wegive theoriginal accuracy ofeach model onthenewly authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa xinyu gao yang feng yining yin zixi liu zhenyu chen andbaowen xu table the dnns accuracy iinprovement value after retraining with theselected tests.
ii i coverage guided iprioritization iii icoverage guided iprioritization i dataset model ats nac nbc snac tknc gini ces lsa maxp rs ats nac nbc snac tknc gini ces lsa maxp rs lenet5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
mnistlenetl .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
t lenetl .
.
.
.
.
.
.
.
t .
.
.
.
.
.
.
.
.
.
u fashionresnet20u n .
.
.
.
.
.
.
.
.. .
.
.
.
.
.
.
.
.
.
vgg16 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
u u ..!
cifar ..!
v resnet20 .
.
.
.
.
.
.
.38v1.
.
.
.
.
.
.
.
.
.
j j lenet5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
svhnvgg16 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
lenet5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
mnistlenetl .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fashionlenetl .
.
.
.
.
.
.
.
.
.
t .
.
.
.
.
.
.
resnet20 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.360vgg16 .
.
.
.
.
.
.
.
.
.
e4.
.
.
.
.
.
.
vvcifarresnet20 3l1.
j .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
lenet5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
svhnvgg16 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.colored baseline blocks represent theaccuracy improvement between ats andbaseline issimilar statistically anduncolored blocks represent atsisgreater than baseline incorresponding configurations.
.foreach selection ratio webold themaximum accuracy improvement value ofeach dnn dataset combination.
table model optiinization effect compared with retraining with allcandidate tests.
dataset imnist fashion cifar svhn model lenet5 lenetllenetl resnet20 vgg16 resnet20 lenet5 vgg16 odacei91.
.
.
.
.
.
.
.
testsi6.
.
.
.
.
.
.
.
ats .
.
.
.
.
.
.
.
imp i69.
.
.
.
.
.
.
.
constructed test set.the accuracy improvement isshown intab.
row tests.
refer totab.
weshow theaccuracy improvement when use ats select test cases from thecandidate set denoted as10 ats.
and inthelast row wecalculated theimprovement ratio of10 ats to100 tests.
the results show that fordnn testing anappropriate test case selection method could optimize thednn model with amuch lighter labeling cost.
based ontheexperimental results and discussion wedraw the conclusion that theadaptive testselection method ats could select test cases from amassive unlabeled dataset automatically.
the test setselected byats contains numerous faults with diverse types.
from theresults ofrq3 wefound that such atest setcould enhance thednn effectively and efficiently.
.
comparison with active learning active learning al isasubfield ofmachine learning ml inwhich alearning algorithm aims toachieve good accuracy with fewer training samples byinteractively querying theoracles tolabel new data points .
thus both aland test case selection attempt toovercome thelabeling bottleneck byselecting data from anunlabeled candidate set.
82although thedesign motivation ofthetwo technologies overlaps slightly itisworth emphasizing that there areinherent differences between test selection and active learning.
the kernel purpose ofalistoobtain amodel ofbetter performance with less ground truth query efforts.
however from theperspective ofdnndriven system testing ats focuses onthetesting and debugging process ofapre trained model which aims toexpose unpredicted behaviors and toenhance themodel.
tothis end weimplement theresearch experiments offaults detection rate and faults diversity inrq1 rq2.
furthermore inevaluation wechoose arepresentative almethod namely maxp asoneofthetestprioritization baseline .
itis almost themost commonly applied uncertainty sampling strategy inactive learning which employs thesame way ofats tomeasure input uncertainty.
the experimental results show that compared with random sampling max p iseffective asaselection method.
meanwhile ats outperforms maxp under most ofthemodel dataset combinations because wedesign anadaptive selection strategy to overcome theweakness ofpriority based selection.
.
threats tovalidity subject selection.
the selection oftraining datasets and dnn models could beathreat tovalidity.
wealleviate this threat byemploying large scale datasets and four well designed dnn models intheexperiment.
further foreach studied dataset weemployed two dnn models with different numbers ofneurons and architecture toevaluate theperformance ofats.
however some oftheexperiment results may notbeperfectly generalized toother datasets and dnn models.
data siinulation.
further weemploy augmented data tosimulate theunseen inputs fordnns which may cause another threat.
although theaugmented operators arecommon data noises inthe authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
adaptive test selection fordeep neural networks virtual environment itisimpossible toguarantee that thedistribution ofthereal unseen input isthesame asoursimulation.
additional experiments based onreal unseen inputs needs tobeconducted infuture work.
7related works this section introduces therelated works ontwo aspects thetestingofdeep learning systems and adaptive random testing.
.
the testing ofdeep learning systems tomeasure thetest adequacy ofdeep learning systems several testcriteria have been proposed.
peietal.
proposed thefirst white box testing framework deepxplore which aims toidentify and generate thecorner case inputs that may induce different behaviors over different dnns.
maetal.
further refined theidea ofcoverage and proposed fine grained multi layer testing criteria toguarantee thequality ofthemodel entirely.
based onthemutation test maetal.
proposed deepmutation and deepmutation toevaluate thequality ofdatasets with model level mutation operators.
based ontheabove coverage criteria different dnn test application techniques areproposed.
tian etal.
presented deeptest togenerate testcases bymaximizing thenumber ofactivated neurons.
similarly zhang etal.
give anadversarial network to generate different weather conditions driving scenes toincrease thediversity ofdatasets.
however aspointed outintheintroduction section many existingtesting techniques give less consideration tothecharacteristic ofthednn driven system which results inthedifference between theimprovement ofthednn system and therepair ofthetraditional software system.
this iswhy recent studies argue theguidance ofexisting testing criteria especially theneuron coverage criteria.
inaddition other test case generation techniques orprioritization techniques aredifficult togive asuitable testing criterion orarebased onalow guidance neuron coverage test adequacy criterion.
inthis paper themethod ats isintended to break outofthescope ofneuron coverage design and start from thesight ofthednn driven software system toidentify andselect testsuites ofhigh model improvement capability.
.
adaptive random testing test selection isaclassic research topic ofsoftware testing.
there areplenty oftechniques and methods areproposed forconventional software systems.
among them one ofthewell known test selection method is adaptive random testing art .
chen etal.
proposed thefirst specific algorithm ofthismethod fscs art .
the core idea isto choose anew test case kcandidates arerandomly generated.
for each candidate ci theclosest previously executed test islocated andthedistance diisdetermined.
thecandidate with thelargest di isselected andtheother candidates arediscarded.
besides another testselection technique antirandom testing isalso based on theconcept ofdistance todistribute test cases.
itisalmost deterministic which means itrequires thenumber oftest cases tobe chosen inadvance.
the core ofthetechnology toimprove random testing efficiency istoachieve even spread test case distribution.
83icse may pittsburgh pa usa therefore anumber ofdifferent methods using different intuitions toachieve this goal have been investigated intheliterature.
anexample isrestricted random testing rrt which isbased on thenotion ofadaptive exclusion and thegoal ofthismethod isto select thetest case outside oftheexclusion zones.
art bypartitioning uses arather different intuition partitioning theinput domain inessence and allocating test cases evenly topartitions achieves even spread.
besides other attempts totake advantage offailure region contiguity butusing various other intuitions to achieve theeven spreading oftestcases including q!1asi random testing and lattice based art .
inthetesting ofdnn driven software systems itiseasy toget enough unlabeled testcases.
however choosing proper data tolabelmanually isalways difficult.
our work aims tohelp testers selectvaluable tests from massive unlabeled tests efficiently.
8conclusion inthispaper wepropose atsanadaptive testselection method for deep neural networks.
wedesign afitness computation method toadaptively determine which test inthecandidate setismore suitable tobelabeled manually.
theexperimental results inthispaperdemonstrate that ats caneffectively help testers choose more valuable testcases toimprove thequality ofthemodel.
different from current testselectionmethods ats isguided byfault pattern design and candidate fitness metric fortestselection ofdeep neuralnetworks.
weprovide analternative view foridentifying and selecting thetests.
weexpect that ats can inspire testers toselect test suites with enough diversity and faults detection ability which canefficiently improve thesystem.
acknowledgement wewould like tothank anonymous reviewers fortheir insightfuland constructive comments.
this project was partially funded bythenational natural science foundation ofchina under grant nos.
and and thescience technology and i ovation commission ofshenzhen municipality no.
cjgjzd202006171 .