revisitingneural program smoothing forfuzzing maria irina nicolae irina.nicolae bosch.com robertboschgmbh boschcenter forai stuttgart germanymaxeisele maxcamillo.eisele bosch.com robertboschgmbh stuttgart germany saarlanduniversity saarbr cken germanyandreaszeller zeller cispa.de cispa helmholtz center for informationsecurity saarbr cken germany abstract testingwithrandomlygeneratedinputs fuzzing hasgainedsignificant traction due to its capacity to expose program vulnerabilities automatically.
fuzz testing campaigns generate large amounts of data making them ideal for the application of machine learning ml .neuralprogramsmoothing aspecificfamilyof ml guided fuzzers aimstouseaneuralnetworkasasmoothapproximation ofthe program target for newtest casegeneration.
inthispaper weconductthemostextensive evaluation ofneural program smoothing nps fuzzers against standard gray box fuzzers cpu years and .
gpu years and make the following contributions we find that the original performance claims for npsfuzzersdo not hold a gap we relate to fundamental implementation andexperimentallimitationsofpriorworks.
we contribute the first in depth analysis of the contribution of machine learning and gradient based mutations in nps.
we implement neuzz which shows that addressing the practical limitationsof npsfuzzersimprovesperformance butthat standard gray boxfuzzersalmostalwayssurpass nps basedfuzzers.
asa consequence we propose new guidelines targeted at benchmarking fuzzingbasedonmachinelearning andpresentmlfuzz aplatform with gpu access for easy and reproducible evaluation of ml based fuzzers.
neuzz mlfuzz andallourdataare public.
ccsconcepts security and privacy software security engineering software and its engineering software testing and debugging computing methodologies neuralnetworks .
keywords fuzzing machinelearning neuralnetworks neuralprogramsmoothing acm reference format maria irinanicolae maxeisele andandreaszeller.
.revisitingneural program smoothing for fuzzing.
in proceedings of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering esec fse december san francisco ca usa.acm newyork ny usa 13pages.
esec fse december san francisco ca usa copyright heldby theowner author s .
acm isbn .
introduction inrecentyears fuzzing testingprogramswithmillionsofrandom automaticallygeneratedinputs hasbecomeoneofthepreferred methods for finding bugs and vulnerabilities in software mainly due to its speed low setup efforts and successful application in the industry.
google s ossfuzz initiative for instance has revealedthousands ofbugs in open sourcesoftware.
fueled by success stories of practical fuzzing researchers are constantlyseekingwaystomakefuzzersmoreefficient .the most popular approach is still coverage guided fuzzing generate newtestcasesfromprioronesusinganevolutionarysearchthat optimizes code coverage through a fitness function.
techniques used to enhance fuzzers include concolic execution or staticanalysis .alongthem machinelearningmethodshave increasinglybeenappliedtodifferentpartsofthefuzzingloopin academic research .
fuzz testing generates significant amounts of data which make a welcomeinput for machinelearning.moreover obtaining labels throughfeedbackfromthefuzzerortheprogramismostoftenfast and cheap.
constructing a dataset for training machine learning modelsisthusrelativelystraightforwardinfuzzing.however despitetheirincreasedtractionintheresearchcommunityinthepast decade ml basedfuzzers are not widely usedin practice .
recently neural program smoothing has been proposed to approximate the tested program with a neural network.
the trained model learns to predict coverage from test cases being additionallysmoothand differentiable.
thesepropertiesallow computinggradients whichcannotreadilybedoneonprograms directly.
test cases are mutated into new ones based on the predictions of the neural network using gradient descent.
the use of gradients allows to steer the mutations in the most relevant directions whichhavehigherchancesofreachingnewcoverage.
despite promisingsignificant performancegains both in terms of codecoverageandnumberofbugsfound thesemethodsarenot currently usedbypractitioners for testingrealsoftware.
motivated by the applicabilityof neural program smoothing to real worldfuzzing weprovidea systematicandthoroughanalysis ofnps guided fuzzing methods with the followingcontributions we provide a critical analysis of nps guided fuzzing uncovering fundamental conceptual and practical limitations thatwere previously ignored.
we show that neural network performancedoesnottranslatetoimprovedcoverage asthe modelfails tocapture rare edge coverage.
we compare multiple nps guided fuzzers in an extensive benchmarkagainstafl afl andtherecenthavoc mab on23targetprograms.
nps guidedfuzzersunderperformregardingcodecoverageandbugfinding whichis atoddswith thiswork islicensedunderacreativecommonsattribution4.0international license.
esec fse december3 san francisco ca usa maria irinanicolae maxeisele andandreas zeller the results from the original papers.
we explain this performancegapbyoutdatedorincorrectexperimentalpractices inprior work.
we reimplement neuzz as a custom mutator for afl and show that fixing practical limitations of npssignificantly improves fuzzing performance.
nevertheless we find that neural program smoothing methods are outperformed by state of the art gray box fuzzers despite their use of additionalcomputationresources.
based on our findings we propose better suited guidelines for evaluating ml enhanced fuzzing and present mlfuzz the first fuzzing benchmarking framework with gpu supportdedicatedto ml basedfuzzing.
mlfuzzallowsforeasy reproducible evaluation of fuzzers with or without machine learning similartostandardpracticesusedbyfuzzbench .
the remainder of the paper is structured as follows.
section introducesprior workon coverage guided fuzzingand neural programsmoothing beforetacklingourmainanalysisonlimitations ofneuralprogramsmoothingin section3.section4 presentsour implementation of npsfuzzing and the benchmarking platform.
section5 coversexperiments followedbynewexperimentalguidelinesinsection6.weconcludethisworkin section7.allourresults andcode are publiclyavailable section .
background coverage guided fuzzing.
coverage guidedfuzzersexplorethe input space of a program startingfrom a few sampleinputs called seeds.
they mutate the seeds into new test cases based on a fitness criterion whichrewardsreachingnewcodecoverageobtainedby gray box access through binary instrumentation.
test cases that increasecoveragearekeptinthecorpustobeevolvedfurther.over time the input corpus and the total code coverage grow.
during execution thefuzzerchecksthetargetprogramforunwantedbehavior notablycrashesandhangs.popularcoverage guidedfuzzers are american fuzzy lop afl its successor afl and libfuzzer .alongsidebasicmutations mostgray boxfuzzers usethehavocmutationstrategy whereafixednumberofrandomly chosen atomic mutations are chained to a more complex mutation .
motivated by the success of havoc in modern fuzzers havocmab wasdesignedtoimplementthehavoc strategyas a two layermulti armedbandit .
despitethe trivial rewardfunction used by the bandit havoc mabclaims to significantly improve code coverageover random havoc inextensive benchmarks.
fuzzing with machine learning.
mlhas been applied to varioustasksinthefuzzingloop.neuralbytesieve experiments with multiple types of recurrent neural networks that learn to predict optimal locations in the input files to perform mutations.
angora uses byte level taint tracking and gradient descent to mutate test cases towards new coverage.
fuzzergym and b ttinger etal.
formulatefuzzingasareinforcementlearning problem that optimizes coverage.
in parallel to mutation generation machine learning is naturally fit for generating test cases directly.skyfire learnsprobabilisticgrammarsforseedgeneration.learn fuzz usesasequence to sequencemodel to implicitlylearnagrammartoproducenewtestcases.ganfuzz usesgenerativeadversarialnetworks gans todothesamefor .
test case .
edge coverage7.
gradient guided mutations .
instrumented programi.
model training .
corpus .
new test casesii.
test case generation .
neural network6.
gradient computation figure neuralprogram smoothing forfuzzing.
protocols.deepfuzz learnstogeneratevalidcprogramsbased on a sequence to sequence model for compiler fuzz testing.
the applicationof mltofuzzingiscoveredmoreextensivelyin .
neural program smoothing.
program smoothing was initially introduced as a way to facilitate program analysis and overcome the challenges introduced by program discontinuities.
among the uses of machine learning in fuzzing neural program smoothingisoneofthemostrecentandpopularmethods dueto its great performance in the original studies.
neuzz trains a neural network to serve as a smooth approximation of the original programintermsofcodecoverage figure1 .first alltestcases from the corpus are executedon the instrumentedprogram toobtaintheirindividualcodecoverage i.e.edgecoveragefrom afl showmap .
the respective pairs of test case and coverage are then used to train a neural network which learns to predict the coverage for each test case.
being smooth and differentiable the neural network can be used for computing gradients the values of derivatives of the program w.r.t.
its inputs.
these indicate the direction andrate of fastest increasein the function valueand can be used to flip specific edges in the bitmap from zero to one .
eachgradientcorrespondstoonebyteintheinput.thelocations withthehighestgradientvaluesaremutated toproposenewtest cases that should reach the targeted regions of the code.
this ideaisinspiredbyadversarialexamples morepreciselyfgsm where a change in the input in the direction of the sign of the gradient issufficient to changethe modeloutcome.
mtfuzz extendsneuzzwithmultitasklearning theneural network is trained against three types of code coverage instead of only edge coverage.
context sensitive coverage distinguishesbetweendistinctcallerlocationsforthesamecoverededge whileapproach sensitive coverage introduces a third possible valueinthecoveragebitmapreflectingwhenanedgewasnearly coveredbecausetheexecutionhasreachedaneighboringedge.the threetypesofcoveragehelplearnajointembeddingthatisusedto determineinterestingbytesformutationinthetestcase.thebytes arerankedusingasaliencyscore whichiscomputedasthesumof gradientsforthatbyteinthelearnedembeddingspace.each hot byte ismutatedbytryingoutallpossiblevalues withoutfurther relying onthe gradients.
134revisiting neural program smoothingforfuzzing esec fse december3 san francisco ca usa prefuzz attempts to solve some limitations of neuzz and mtfuzz by extending neuzz in two ways.
the program instrumentation is changed to include all neighboring edges of covered ones in thebitmap.
this information isused toprobabilistically choose which edge to target next for coverage with the end goal of encouragingdiversityinedgeexploration.additionally thesuccess of havoc mutations is leveraged after the standard neuzz mutation havocisappliedprobabilisticallytopre definedsegmentsof bytesinthe test case according to theirgradient value.
analyzingneural program smoothing in this section we provide our main analysis of neural program smoothing covering both the concepts behind nps as well as existingfuzzerimplementations.wetacklethreeorthogonalperspectives i conceptualorfundamental ii implementationand usability and iii experimental considerations.
.
conceptual limitations c1 approximationerrorsoftheneuralnetwork.
beingan empiricalprocess neuralnetworktrainingcansufferfromerrors introduced in the training process by e.g.
limited training data andtrainingtime orsensitivitytohyperparameters.eveninthe idealcase beingasmoothapproximation the npsmodelwillalways differ from the actual program exactly at the most interesting points i.e.
discontinuities branches and jumps.
this approximation error is intrinsic to a smoothing approach and at the same time what allowsnpsmethods to use gradients and numeric optimization towardsproducing newinputs.
c2 capacitytoreachtargetededges.
arguably themost salientresearchquestiontoelucidateaboutneuralprogramsmoothing iswhether the gradient guidedmutation canindeedreach the targetededges.as npsisbasedonmultiplecomponents figure1 the overall performance of the fuzzer critically depends on the effectiveness ofits individualcomponents the prediction accuracyofthe neuralnetwork the capacity of the gradient based mutations to achieve the expectednewcoverageonthe target program.
the experiments we perform later in the paper show that the machinelearningcomponentasusedbyneuralprogramsmoothing hasimpairedperformance.tothebestofourknowledge prior nps studieshavenotassessedwhatthemodelwaslearningandwhether itwasreachingits objective.
c3 incomplete coverage bitmaps.
another central limitation of neural program smoothing that we uncover relates to the incompleteness of the coverage bitmaps that the neural network receives.
all npsfuzzers retrieve covered edges through afl showmap which only reports the edge ids that are reached.
whenthecoverageinformationfromallseedsisputtogetherfor theoverallbitmapusedfortrainingtheneuralnetwork itthusonly contains edges that were reached at least once by any of the seeds.
as such unseen edges are not part of the bitmap and cannot be explicitly targetedanddiscoveredby themodel.in practice ifthe neural network does discover new edges it is rather inadvertently due to randomness.
while having access to only an incomplete coverage bitmap is aconceptual limitation it can be addressedonan implementation level.
it is sufficient to change the instrumentationoftheprogramtoincludeuncoverededgestoovercomethis issue.
among existing npsfuzzers prefuzz is the only one that considersinformationaboutneighborsofreachededgesinthecoverage bitmap albeit not motivated by the limitation we uncover.
theirgoalisrathertobeabletochoosethenextedgetotargetina probabilistic fashion depending on the degree of coverage of each edge andits neighbors.
the fundamental limitations uncovered in this section while someeasiertosolvethanothers arewhatweseeasmainobstaclein theadoptionof nps basedfuzzinginpractice.aswillbeconfirmed insection the experiments are consistent with these limitations.
.
implementation andusabilitylimitations we now turn to practical aspects that make existing approaches to neural program smoothing inconvenient to use such that an independent evaluation requires majoreffortandcode rewriting.
i1 use of outdated components.
existing implementations ofneuralprogramsmoothing alongwithhavoc mab areimplementedasextensionsofaflinsteadofusingthemorerecent more performant afl as base.
moreover their dependency on outdated python tensorflow and pytorch versions impacts usability.forthepurposeofexperiments wehavepatchedthecode andupdatedthedependenciesofallthesefuzzers asevenforthe most recent ones some of their used libraries were already not available at the time oftheir publication.
i2 difficultyinbuildingtargets.
priornpsstudiesprovided thebinariesusedintheirownresearch ensuringreproducibility.
however forafuzzertobepractical itisadvisabletoratherprovide instructions on how to build new programs for its use.
this isespeciallyimportant when the fuzzer uses custom target instrumentation.mtfuzz forinstance compilesatargetprogramin fivedifferentwaysduetotheintroductionofthreeadditionaltypes ofinstrumentation.forthis reason we excludemtfuzz fromour empirical study as not being practical for real world fuzzing.
moreover we argue that the three types of coverage used by mtfuzz aretoalargeextentredundant conceptuallimitation andcould begroupedintoaunifiedcoverage thusreducingthebuildeffort for this fuzzer.
i3 useofmagicnumbers.
themagicnumbersprogramming antipattern is frequently encountered in the implementations ofneuralprogramsmoothing basedfuzzers.thesevaluesandother algorithmicchangesarenotmentionedintheoriginalpaperswhere eachnpsfuzzerisintroduced.itisthusdifficulttoestablishwhether theperformanceofeachmethodisstrictlylinkedtoitsproposedalgorithmorrathertotheimplementationtweaks.e.g.
themaximum numberofmutationguidinggradientsperseedissetto500 this valueisnot aparameterof the algorithm presentedinthe paper.
ourfindingsaboveshowthattheefforttosetupexisting nps fuzzersandbuildtargetsforthemissignificantlyhigherthanfor standardgray boxfuzzers suchasaflanditsvariants orlibfuzzer.
.
evaluationlimitations inthissection wehighlightflawsandlimitationsofpreviousexperimental evaluations of npsfuzzers and havoc mab which have ledto unrealistic performance claims.
135esec fse december3 san francisco ca usa maria irinanicolae maxeisele andandreas zeller e1 experimental protocol.
the more recent npspublications lack of comparisons with recent gray box fuzzers such asafl andlibfuzzer fuzzersthatwereavailableandconfirmed asstate of the artlongbeforetheirpublication.havoc mab has includedneuzzandmtfuzzintheirevaluationalongsideafl .
however we find that they use the same binary target for both afl and afl instead of building the program separately for afl .
afl runs on afl instrumented binaries but not efficiently.
moreover the size of the coverage bitmap is usually larger forafl thanwithaflinstrumentation hence codecoverage as measured by the fuzzers is not directly comparable.
this makes the conclusions inthe havoc mabevaluation questionable.
e2 fuzzer configuration for speed.
we note that prior studies benchmarking npsmethods compile their targets using afl gcc which results in slower targets and thus impacts fuzzing speed.
the afl documentation recommends using preferably afl clang fast orafl clang lto .additionally afl based fuzzers have multiple options for transferring fuzz data to the program.
the most basic is to have afl write test cases to file and the target program executed with command line options to process the file as input.
the more sophisticated and recommended persistent mode uses a fuzzing harness that repeatedly fetches fuzz data from aflvia shared memory andexecutes thefunction with thetestdataasinputwithoutrestartingthewholeprogram.
all professionalfuzzingusesthismode accordingtotheafl manual .
depending on the target the persistent mode can increase thethroughputby2 .previousneuralsmoothingpapers seem to run all experiments by feeding inputs via files which should considerably slow down all fuzzers.
this is consistent with theirresults wherethemoremodernafl consistentlyperforms worse than afl in the havoc mabstudy and the targets are printed with command line arguments in the original neuzz paper .weconjecturethatthistipsthescaleinfavorof ml based fuzzers which are themselves orders of magnitude slower than modern fuzzers .
this statement is validated experimentally in section .
.
implementing neuzz and mlfuzz inthissection weintroduce neuzz ourimplementationofneural programsmoothingthataimstosolvesomelimitationsidentifiedin section as well as the new experimental platform for evaluating ml basedfuzzers.
neuzz .
we implement a variation of neuzz as a custom mutator for afl which we name neuzz see figure .
this allows ourmethodtoleveragemostafl features likeitsstandardmutations and power schedule.
more importantly it allows for machine learning produced test cases and randomly mutated ones to evolve from each other.
we choose afl as base for our implementation for its state of the art performance thus addressing issue i1.
being acustommutator neuzz ismodular easytobuild andintegrated withadefaultafl installation.
in practice neuzz consists of two parts the main afl process with the custom mutator implemented in c and a python extensionthatiscalledformachinelearningoperations.thetwo processes communicate using named pipes.
we set a minimum requirementof u1d447testcasesinthecorpusforthecustommutatorto afl gradient guided fuzzingexisting nps guided fuzzersneuzz afl gradient guided custom mutatorafl mutationsmutation scheduler1 hour hours24 hoursfigure operation mode of previous nps guided fuzzers andour neuzz .
run.theseareusedtotraintheneuralnetworkforthefirsttime the model is retrained at most every hour if at least ten new test cases have been added to the corpus1.
this allows to refine the model over time with new coverage information from recent test cases.
in practice we use u1d447 this value is tuned experimentally and aimstostrikethebalanceacrossalltargetsbetweenfuzzingwith machinelearningasearlyaspossible whilewaitingforenoughdata tobeavailableformodeltraining.intuitively alargerdatasetproducesabetterperformingmodel.
afl showmap isusedtoextract the coverage bitmap.
we introduce a coverage caching mechanism formodelretrainingwhichensuresthatcoverageiscomputedonly for new test cases that were produced since last model training.
each time the c custom mutator is called by afl it waits for thepythoncomponenttocomputeandsendthegradientsofthe test case.
based on these the mutations are computed by the c mutatorandreturnedtoafl .incontrasttoneuzz thegradients arenotprecomputedpertestcase theyarenotsavedtodisk the neural network is kept in memory and the gradients are computed onlyondemand.theseoptimizationsminimizethetimespenton ml relatedcomputations keeping more time for fuzzing.
the neuralnetwork isamulti layer perceptron mlp withthe samestructureasneuzz onehiddenlayer 4096neurons .asshown intheprefuzzpaper we alsofoundthatdifferentneuralnetworkarchitecturesdonotimprovefuzzingperformance.incontrast tonpsfuzzers wekeep10 ofthetestcasesasvalidationsetfor evaluatingtheperformanceofthemodel.weusetheadamoptimizer alearningrateof andcosine decaywithrestarts.
it is easy to parallelize model training and the main afl routineforimprovedfuzzingeffectivenesswhentestingrealtargets.
however forexperimentalevaluation wechoosetohaveafl wait for the neural network to train similarly to previous implementationsofneuralprogramsmoothingfuzzers.thisallowsfor fair experimental comparison and computation resource allocation.
theoriginal neuzzimplementation appliesfour differentmutationpatternsoneachbyteselectedaccordingtothehighestranking gradients incrementing the byte value until decrementing the byte value down to inserting a randomly sized chunk at the byte location and deleting a randomly sized chunk starting at the given byte location.
we apply the same mutation pattern for neuzz .
1neuzz and prefuzz solve this issue by running afl for the first hour of fuzzing then usethe collected data for modeltraining figure2 .
136revisiting neural program smoothingforfuzzing esec fse december3 san francisco ca usa mlfuzz.
mlfuzzservesasabenchmarkingframeworkforbuilding testtargets runningfuzzing trialsinanisolated environment andanalyzing the findings.itsmain features are test targets from google fuzzer test suite are compiled with the recommended and most recent compiler of theappropriatefuzzer thebuildscriptsaremadeavailable addressing issue i2andissuee2 .
targets are compiled with addresssanitizer to detect memory errors.
six fuzzers are currently included in mlfuzz afl v2.57b afl v3.15a havoc mab neuzz prefuzz and our neuzz .
theimplementationiscontainerizedviadocker .python dependency specification is handled via virtual environmentsandpoetry .
eachfuzzingtrialrunsononededicatedcpuandoptionally one gpufor fuzzersthat support it.
all supported fuzzers have been modified to acceptseeding oftheirrandom number generatorfor reproducibleresults.
forallfuzzers coverageismeasuredbyreplayingthecorpus at the end of a run.
we use binaries instrumented with afl to ensure we do not disadvantage the afl based fuzzers andafl showmap fromafl sinceithasalarger bitmap withless hash collisions.
test cases are transmitted to fuzzers via shared memory with the option to switch to slow transmission of test cases viathe file system addresses issue e2 .
experiments this section introduces our experiments and practical analysis complementing the main findings from previous sections.
after presenting our setup section .
we assess the performance of thecomponents of nps basedfuzzers in section5.
.
we compare our neuzz to prior neuralprogram smoothingfuzzers andstandard gray box fuzzers in an extensive benchmark in section .
.
sections .
to5.6explore the added benefit of machine learning to npsfuzzers while section5.
shedslightonexperimentalprotocol differences with previous npspublications and their impact on fuzzingresults.
finally we report bugsfoundin section .
.
.
experimentalsetup allexperimentsareperformedonaserverrunningubuntu20.
withfournvidiatitanxpgpus.ourstudyincludesthesixfuzzers frommlfuzz aflandafl asstandardgray boxfuzzers havoc mab as recent fuzzer claiming state of the art performance and nps fuzzers neuzz prefuzz and our own neuzz .
we use the original implementation and parameters provided by the authors for allbaselines exceptwhenstatedotherwise.wepatchthecodeof neuzzandprefuzztoportthemtopython3.
.
cuda11.
tensorflow2.
.
andpytorch1.
astheoriginalimplementations arebasedonoutdatedlibrariesthatarenotavailableanymoreor incompatible withour hardware.
we choose google fuzzer test suite and fuzzbench as standard extensivebenchmarksforourexperimentalevaluation.
wemakeuseof23targets summarizedin table1.theseareselected forbeingaccessible havingdependenciesavailableonubuntu20.
andbeingnon trivialtocoverthroughfuzztesting.notethatwetable target programs from google fuzzer test suite andfuzzbench .
target format seedsalocb source fuzzer test suite boringssl sslprivatekey freetype2 ttf otf woff 95576c guetzli jpeg harfbuzz .
.
ttf otf ttc json json lcms icc profile libarchive archiveformats libjpeg turbo jpeg libpng .
.
png libxml2 v2.
.
xml openssl .
.2d dercertificate pcre2 .
perl regex proj4 custom re2 custom sqlite custom vorbis ogg woff2 woff source fuzzbench bloaty elf mach o etc.
curl comms.formats libpcap pcap openh264 h. stb imageformats zlib zlib compressed atargetsthatdonothaveseedsuse the default from fuzzbench.
bretrieved with cloc .
onlyincludetargetsfromfuzzbenchiftheyarenotalreadyincluded infuzzertestsuite.allresultsarereportedfor24hoursoffuzzing.
we repeat each experiment times to account for randomness unless stated otherwise.
each standard gray box fuzzer is bound to onecpucore while npsfuzzersareallottedonecpuandonegpu pertrial.themainmetricsusedforevaluationarecodecoverage andnumberofbugsfound.forcodecoverage weuseedgecoverage as defined by the afl family offuzzers.
however we emphasize thataflandafl computeedgecoveragedifferently.inorderto avoidthemeasuringerrorsintroducedwhenignoringthisaspect we count coverage by replaying the corpus using afl showmap from afl on the same binary independently of which fuzzer wasusedintheexperiment.thesetupweusefixesallexperimental limitationswe highlightedin section .
issues e1 ande2 .
.
performance ofmachine learning models wenowinvestigatethequalityofcoveragepredictionsbytheneural network and gradient based mutations in relation to concerns about the fundamental principle of neural program smoothing section .
.
we tackle the following questions can the neuralnetwork learn to predict edge coverage?
can gradient basedmutations reachtargetededges?
to this end we propose quantitative and qualitative analyses of the performance of the neural network in neural program smoothing fuzzers.
without loss of generality we investigate these based onneuzz asaproxyforallneuralprogramsmoothingfuzzers 137esec fse december3 san francisco ca usa maria irinanicolae maxeisele andandreas zeller figure predictedand actualedgecoverage on libpngforthe entire corpus.
top ml predictedcoverage pink is trivialand almostconstantovertestcases.wheneachedgeistargetedbymutations predictedcoverage orange increasesforcertain edges butmanycodeedgesremainunattainable.bottom coverageextractedwith afl showmap showsthatalledgespresent have been covered at least onceby thecorpus.
table datasetproperties andneuralnetworkevaluation.
target covered edges acc prec recall f1 pr auc bloaty .
.
.
.
.
.
boringssl .
.
.
.
.
.
curl .
.
.
.
.
.
freetype2 .
.
.
.
.
.
guetzli .
.
.
.
.
.
harfbuzz .
.
.
.
.
.
json .
.
.
.
.
.
lcms .
.
.
.
.
.
libarchive .
.
.
.
.
.
libjpeg .
.
.
.
.
.
libpcap .
.
.
.
.
.
libpng .
.
.
.
.
.
libxml2 .
.
.
.
.
.
openh264 .
.
.
.
.
.
openssl .
.
.
.
.
.
pcre2 .
.
.
.
.
.
proj4 .
.
.
.
.
.
re2 .
.
.
.
.
.
sqlite .
.
.
.
.
.
stb .
.
.
.
.
.
vorbis .
.
.
.
.
.
woff2 .
.
.
.
.
.
zlib .
.
.
.
.
.
included in our study.
as all these methods use the same neural networkarchitecture lossfunction methodoftraining etc.
itisto beexpectedthattheirmodelswillachievethesameperformance whentrainedonthesamedataset.theresultsoftheanalysescan be summarizedas followsandare detailedsubsequently table2quantifiesthemodelperformanceforalltargetsin terms ofstandardmachine learningmetrics figure 3provides a qualitative analysis of model predictions for agiven target opposing themto correctlabels.
lastly figure3alsoassessesthecapacityoftheneuralnetwork to reachedges throughgradient basedmutations.
mlperformance metrics.
to assess one factor of difficulty of themachinelearningtask weevaluatedatasetimbalanceforthe training corpus.
this measures the percentage of the positive class covered edges in our case the minority in the coverage bitmap of thetrainingset.recallthatthebitmapisproducedby afl showmap and accounts for the coverage obtained by the corpus before training thecoveragewasnotnecessarilyachievedbasedonaneural network but rather by afl mutations.
note that this value is averaged across test cases and edges rare edges might have much smallercoverageratios resulting in more difficultyin trainingan accuratemodelforthoseedges.whenfacingclassimbalance the modeltendstopreferthemajorityclass thusmakingwrongpredictions.
for this reason the performance of the neural network is assessed using precision recall f1 score and precision recall pr trade off as performance metrics for the neural network.
accuracy is also computed for completeness but keep in mind that this metric is misleading for imbalanced datasets2.
we measure the area under the curve auc of theprmetric to evaluate all theoperationalpointsoftheneuralnetwork.similartoaccuracy pr aucsaturates at one but is more sensitive to wrong predictions in the positive class.
the learning setup of neural program smoothingisamulti labelbinaryclassificationtask i.e.
foreach 2one cantrivially predict all zeros no coverage and obtainveryhigh accuracy.
138revisiting neural program smoothingforfuzzing esec fse december3 san francisco ca usa test case multiple binary predictions are made one per edge in consequence themetricsarecomputedforeachedgeinthebitmap independently thenaveragedoveralledges andfinallyaveraged over trialrepetitions.
table2reportsthemodelperformancemetrics alongwiththe percentage of the positive class in the dataset as imbalance metric.
allmodel metrics are computed on a10 holdout setof testcases that were not used for training.
as neuzz retrains the model multiple times all measurements are performed on the last trained neural network using the state of the corpus at that time.
the precision recall f1 score and pr aucvaluesin table2indicate that the neural network has low performance.
these metrics are particularlylowwhentheclassimbalanceisstronger i.e.
forsmall values of covered edges .
the dataset imbalance is quite extreme for seven targets where the positive class represents less than ofthe dataset makingpredictions particularlydifficult.
to provide an intuition into what the neural network learns wedesignaqualitativeevaluationofitspredictedcoverage.this experimentusesthetarget libpngandthetestcasesgeneratedin a hours run of neuzz .
figure shows two coverage plots for this target for the entire corpus where each column in the plotrepresentsonetestcase whileeach row isaprogramedge.
we compare the coverage predicted by a trained mlmodel for the same test cases and edges figure 3top to the true coverage extracted with afl showmap bottom .
the bottom plot isthe coverage bitmap extracted with afl showmap for the corpus and used formodeltrainingbyneuzz prefuzz andneuzz .areduction deduplication operation is applied to it which for libpngreduces the number of edges from to the present in the plot this operation also explains any visual artifacts present in the image asthe edges are reordered.the pink areas ofthe twoplots differ significantly withthemodelpredictionsbeingalmostconstantover alltestcases themodelonlypredictstrivialcoverageandfailsto capture rare edges.
while this is a consequence of the difficulty ofthemachinelearningtasks smalldataset classimbalance too few samples w.r.t.
the size of the test cases and bitmaps see table2 itresultsinlargeapproximationerrorsintheneuralnetwork asoutlinedin issuec1.moreover recallthatneuzz prefuzzand neuzz usethesame mlmodeltypeandstructure withminordifferences in the training procedure and similar model performance.
our findingsthus extend to allnpsmethods.
finally weinvestigatetheeffectivenessofgradient basedmutationsasessentialcomponentof npsfuzzers.inthesamesetup onlibpngfromthe previoussection we apply neuzz mutations tothecorpusgeneratedbya24 hoursfuzzingrunasfollows.for each edge in the bitmap we consider the case when it is explicitly targeted and generate all mutations with a maximum number of iterationsinthemutationstrategy.
figure3 top plotsthepredicted coveragefor each testcase and edge before the mutations aswell as the increment of coverage after mutation.
each edge row is considered covered by one test case column if atleast one ofthe few thousand mutations generated to target it reaches the code location.theresultsrepresentcoverageestimatedbythe mlmodel not run on the program.
however the coverage the model predicts is an optimistic estimate of the one actually achieved on the target asthemodeldictatedthemutations.notethatthemutationsaregeneratedinthesamewayforneuzz prefuzzandneuzz our analysisthus applies to allmethodsandtargets.
figure top indicates that some locations are more readily reachable through mutations.
the harder to reach edges overall match the rarer edges of the corpus as measured by afl showmap in the bottom plot.
most importantly none of the edges targeted or covered by the mutations in the top plot represent new coverage .
recall that by npsmethods design a code edge is only present in the bitmap only if it has already been covered by the initial corpus used for training issue c3 .
this becomes evident in the bottom plot offigure all edges have been covered by at least one test case.aswillbeshownlater thisfundamentalflawof npsmethods translates to a limited practical capacity of reaching new coverage.
themodelpredicts trivialedge coverage issue c1 and gradientmutations cannot target new edges issue c3 .
.
comparingcodecoverage we present the main experiment comparing the achieved code coverage of available neural program smoothing approaches to afl afl andtherecenthavoc mabintable3 averagecoverage andfigure coverage over time .
this experiment alone requires atotalcomputationtime of over cpuyears and5.
gpuyears.
overall afl obtains the best performance on ten targets followed by havoc mabwith eight targets and neuzz on par withafl winningtwotargetseach.inviewofafl performance w.r.t.afl itisclearthatnotincludingafl asabaselineinall priorneuralprogramsmoothingworksleadstooverlyoptimistic conclusionsabouttheircapacities.after afl havoc mabisthe secondmostperformantfuzzerintermsofcodecoverage.however wefindthatitdoesnotreachtheexpectedrankingadvertisedin the havoc mabpaper .
we observe that neuzz and prefuzz are never in the top two fuzzers.
moreover although they were designed to improve afl performance their coverage is in most cases lower than that of afl.
afl wins on out of targets over neuzz and out of overprefuzz.prefuzzoutperformsneuzzonmosttargets however this difference is significant only on six targets see confidence intervals in figure .
this finding is also at odds with original prefuzz results where the performance gap is significantly wider.section .
is dedicated to further explaining thedifference in performance with the initial papers.
neuzz obtains higher coveragethanneuzzandprefuzzon21programs provingthatour improvements over thesemethodsare effective.
targetslibarchive libxml2 proj4 andwoff2exhibit the most variabilityamongfuzzers.neuzzandprefuzzexhibitlargestandard deviation on woff2 where coverage varies depending if the fuzzers reach plateau or not.
for the other targets it seems afl based fuzzersdo not perform as well as afl basedones.
overall afl achieves the highestcode coverage.among nps fuzzers neuzz achieves the highestcode coverage.
139esec fse december3 san francisco ca usa maria irinanicolae maxeisele andandreas zeller table average edgecoverage andstandard deviationover30runs.
best value inbold second best underlined.
target afl afl havoc mab neuzz prefuzz neuzz bloaty boringssl curl freetype2 guetzli harfbuzz json lcms libarchive libjpeg libpcap libpng libxml2 openh264 openssl pcre2 proj4 re2 sqlite stb vorbis woff2 zlib 3table4 averageedgecoverageofml componentover30runs.
neuzz prefuzz neuzz .
codecoverage from machine learning after presenting total coverage for hour runs in table we now measure how much of the total coverage can be attributed to the machine learning component for each npsfuzzer.
on one hand the goal is to discount the coverage produced strictly by afl in the first hour of neuzz and prefuzz runs recall that they use afl for data collection see figure and only measure the nps fuzzers contribution.ontheotherhand wewishtodothesamefor neuzz and separate its contribution from that of the base fuzzer afl .asneuzz isacustommutatorforafl itsseedsusually alternate with regular afl seeds.
to this end we measure edge coveragebycorpusreplaying thistimeonlytakingintoaccount theseedsobtainedbyneuzz prefuzzandneuzz respectively.
forneuzzandprefuzz thisisequivalenttoexcludingthefirsthour ofcoverage asdonebytheoriginalauthors.inpractice thiswill includeml based mutations but also other hard coded mutations thatthemethodsapply suchashavocinthecaseofprefuzz.
table4 summarizesthe comparisonof edge coverage obtained by the ml componentsofneuzz prefuzz andneuzz .programnamesare alignedwith table3.
neuzz obtains the highest coverage in over targets with values at least one order of magnitude higher than neuzz and prefuzz.
nevertheless even on targets where neuzz does notobtainthehighest mlcoverage e.g.
freetype2 harfbuzz the overall neuzz edge coverage table is higher than that of neuzzandprefuzz withthelattertwoobtaininglowercoverage than their base fuzzer afl.
the added value of neuzz and prefuzz is low in nine respectively five targets with coverage close to zero.
in these cases neuzz and prefuzz do not achieve almost any coverage past the first hour of fuzzing with afl see also figure .thisreinforcesourpreviousconclusionthatthetimespentusing neuzzandprefuzzmightbebetterspentapplyingtheaflorafl mutation strategy.
moreover the neuzz results suggest that it mightbenefitfromthealternation between ml guidedmutations andstandardafl ones.weexplorethislastpointwithadditional analysesin section .
.
formost programs the timebudgetspentonneuzzor prefuzz is better spentonstandard gray box fuzzing.
.
quality ofmachine learning testcases we now aim to assess the quality of the test cases found by the machine learning component of neuzz .
we do so with two analyses we investigate i the inclusion of ml generated inputs in the afl powerscheduleforfurthermutation and ii therarityof code edges foundthroughmachine learning basedmutations.
first table 5presents statistics regarding ml produced test cases for each target averaged over all trials.
the column ml seeds shows the overall percentage of inputs produced through mlmutations.
out of these mlcov discover new coverage relative percentage .
finally derived is the total percentage of thecorpusproducedbydirectmutationsof ml basedinputs.we findthattheratioofmachinelearninginputsvariessignificantly acrosstargets representinguptoathirdofthecorpus.
mltestcases seemtobemostimpactfulforfindingnewcoverageonprograms wheretheyrepresentalowpercentageofthecorpus.onaverage eachmltest case is mutated at least once successfully generating newtest casesthat are kept byneuzz inthe corpus.
140revisiting neural program smoothingforfuzzing esec fse december3 san francisco ca usa 241100012000130001400015000bloaty 24286028802900292029402960boringssl 2490001000011000120001300014000curl 247500800085009000950010000105001100011500freetype2 24660067006800690070007100720073007400guetzli 2495001000010500110001150012000harfbuzz .
.
24150016001700180019002000json 241200140016001800200022002400lcms 243500400045005000libarchive 2424002500260027002800290030003100libjpeg turbo 24100015002000250030003500libpcap 241180119012001210122012301240libpng .
.
24400050006000700080009000libxml2 v2.
.
2412000125001300013500140001450015000openh264 2418601870188018901900openssl .
.2d 246500675070007250750077508000pcre2 .
2410002000300040005000proj4 2461006200630064006500660067006800re2 2419001950200020502100sqlite 24300030503100315032003250330033503400stb 242000210022002300vorbis 24100015002000250030003500woff2 24580590600610620zlib havoc mab afl prefuzz neuzz neuzz afl relative time hours edge coverage figure average edge coverage over time with confidenceinterval.table statistics for ml generated test cases of neuzz .
ml seeds and derived are computed over the total size ofthecorpus.
mlcov is relative to ml seeds .
target mlseeds mlcov derived bloaty .
.
.
boringssl .
.
.
curl .
.
.
freetype2 .
.
.
guetzli .
.
.
harfbuzz .
.
.
json .
.
.
lcms .
.
.
libarchive .
.
.
libjpeg .
.
.
libpcap .
.
.
libpng .
.
.
libxml2 .
.
.
openh264 .
.
.
openssl .
.
.
pcre2 .
.
.
proj4 .
.
.
re2 .
.
.
sqlite .
.
.
stb .
.
.
vorbis .
.
.
woff2 .
.
.
zlib .
.
.
thesecondanalysisstudieswhether npsfuzzersexplorecode areas that are harder to reach by standard fuzzers.
in that case neuralprogramsmoothingfuzzerscouldbeusedinanensemble ofdiverse fuzzers openingthepath for allfuzzers torare parts of the code .to measure the rarity of edgesreached by neuzz we comparetheedgeidsthatneuzz andafl reachon each program all trials joint.
the edge ids are obtained by replaying all the test caseswith afl showmap .
we summarize the results in table 6as follows neuzz denotedn revealslessthan0.
additionaledgesthatafl denoteda in16outof23targets.neuzz doesnotfindanysuchexclusiveedgesforeightprograms itismostsuccessfulon lcms with .
exclusiveedges.ontheotherhand afl findsupto16.
exclusive edges lacking exclusive edges on only two programs json andsqlite .
we can therefore conclude that nps guided fuzzers explore essentiallythe same code areas as traditional fuzzers.
nps fuzzers findlessrare edgesthangray box fuzzers.
.
nps based fuzzing withoutgpus due to their increased performance for linear algebra and data throughput gpusarethe defactostandardformachinelearning.
allnpsmethodsstudiedinthispaperleveragegpuaccesstotrain machine learning models and compute gradients for test case mutations.inpractice thismeansthattheyusemorecomputational resources than state of the art gray box fuzzers and that practitionersarerequiredtoinvestinadditionalhardware.inthissection we wish to assess the performance of npsmethods in the absence ofgpus.model trainingwithonlycpuaccessshouldbeslower 141esec fse december3 san francisco ca usa maria irinanicolae maxeisele andandreas zeller table reached edges forafl a andneuzz n .
target a n a n a n n a bloaty boringssl curl freetype2 guetzli harfbuzz json lcms libarchive libjpeg turbo1565 libpcap libpng libxml2 openh264 openssl pcre2 proj4 re2 sqlite stb vorbis woff2 zlib table7 averageedgecoverageof npsfuzzerswithandwithoutgpu access runs .
neuzz prefuzz neuzz target cpu gpu cpu gpu cpu gpu harfbuzz libjpeg sqlite woff2 but it should not impact the performance ofthe trained model.
as such any loss in fuzzing performance comes from spending more time training and less fuzzing.
for this small experiment we select four targets that operate on a varied range of input formats for diversity.weperformtentrialsofall npsfuzzerswithandwithout gpuaccess table7 .
nine of twelve experiments obtain more code coverage when training themodelongpu whichis to be expected.the exception is prefuzz on woff2 which is however aligned with this fuzzer s tendencyofsometimesbecomingstuckonthisprogram table3 .
overall thefuzzingperformanceongpuismarginallybetter as trainingtimesfor npsmodelsarerelativelyshort.thegapbetween cpuandgpuseemstighterforneuzz whichweattributetoan alreadyoptimizedandshorttrainingprocedure whichcannotbe muchfurther improvedbygpus.
using gpususuallyresultsin better coverage for nps fuzzers.
.
impactoftestcasetransmission method inissuee2 weunderlinedthat nps guidedfuzzersusefilestotransfer test cases to the target program.
we now show that test casetable relative degradation of edge coverage not using persistent mode runs .
target afl afl havoc mabneuzz prefuzz neuzz harfbuzz .
.
.
.
.
.
libjpeg .
.
.
.
.
.
sqlite .
.
.
.
.
.
woff2 .
.
.
.
.
.
transmission has a major impact on fuzzing performance for the methods in .
we note that afl does not reach the performance of its predecessor afl by a margin in the havoc mab work .
this is inconsistent with several other large benchmarks where afl ranks among the top fuzzers.
while notusingthepersistencemodeslowsdownall fuzzers weexpect state of the artgray boxfuzzerstobeaffectedthemost i.e.
they would lose their competitive advantage of speed.
this experiment uses the same targets andsetupas the previous section.
table 8presents the performance difference when the persistencemodeisnotused.thissetupreproducesboththeprotocoland results from havoc mab the only paper that compares npsguided fuzzers against afl .
as expected coverage decreases whenpassinginputsthroughfilesandrestartingtheprogramfor eachtestcase.mostinterestingly afl showsthelargestslowdownwithaconsistentcoveragelossover50 whiletheafl based fuzzersmainlyshowsingle digitpercentagedegradation.consequently not using the recommended persistence mode can distort the ranking of fuzzers in a benchmark.
in our opinion this setting does not yield a fair or practically relevant comparison.
worth mentioning here is that neuzz can compensate the performance loss of its base fuzzer afl obtaining more coverage in absolute values.
as conjectured results indicate that nps guided fuzzers sufferlessunder slowoperationthanotherfuzzers.despite that we are still not able to reproduce the performance of neuzz and prefuzzagainst afl reportedinthe originalpapers.
afl is most slowed down when not using the persistent mode.
.
bugs found themaingoaloffuzzingistofindasmanyuniquebugsaspossible.
the default coverage based crash identification mechanism of afl andafl tendstoovercountuniquebugs .toimprovethis behavior we apply a more precise stack trace based deduplication algorithm.wethereforeexecuteeachreportedcrashinginputon thetargetwithingnudebugger gdb andretrieveallstackframe addresses when the error occurs.
this list of addresses then serves as a unique identifier of the triggered bug.
note that deduplication based on stack traces is ineffective when stack overflow errors occur because the stack framesare then corrupted.
table 9contains the number of crashes with unique stack trace signaturesacrossalltrialsforeachtargetthatreportedanycrashes.
neuzzandprefuzzfindthelowestnumberofcrashinginputs none for most targets followed by afl their base fuzzer havoc mab significantly improves over afl.
afl is most successful in revealingcrashes withmostbugsfoundandalltargetscovered.in summary all nps based fuzzers find fewer crashing inputs than the fuzzerthey are basedupon.
142revisiting neural program smoothingforfuzzing esec fse december3 san francisco ca usa table bugsfoundafter stack trace deduplication.
target afl afl havoc mabneuzz prefuzz neuzz bloaty guetzli harfbuzz json lcms libarchive libxml2 openssl pcre2 re2 vorbis woff2 nps guided fuzzers find fewer bugs than standard fuzzers.
benchmarkingml based fuzzers fuzzerevaluation is an openresearch topic abundently studied in recent works .
a common guideline is that each fuzzer must be tested on multiple programs using multiple repetitions to account for randomness.
the recommended number of repetitions revolves around trials.
besides the average performance indicatorsof variability i.e.
confidence intervals statisticaltests are necessary to assess the significance of the results.
the main goal of fuzzers is to find bugs which suggests that unique bugs foundinfixedtimeshouldbetheevaluationmetric.however since bugs are rather rare the performance of fuzzers is often measured incodecoverageovertime.this maybejustifiedbyobservations thatmorecodecoveragecorrelateswithmorebugsfound .to complementtheseprinciples weproposethefollowingpractices when evaluatingnovel machine learning basedfuzzingmethods analyzeeachnewcomponentinthefuzzingloop.
both performance evaluations and ablation studies of mlmodels arecritical.metricsspecifictothetasksolvedshouldbeused e.g.
accuracy orprecisionandrecallforclassification mean absolute error or mean squared error for regression etc.
.
these complement the view on the overall system performance i.e.
coverageorbugsfoundinthecaseoffuzzing.
ml evaluation shouldemploya validationset distinctfromthe training data to avoid an overly optimistic estimates .
usestate of the artfuzzersandconfigurationsasbaselines.lacking strong baselines prevents one from claiming novelstate of the artaccomplishmentsintermsofcodecoverageandbugsfound.allfuzzersinanexperimentshould be configured for performance e.g.
appropriate compiler compilation options harness input feeding mode .
we also recommendintroducingnewscientificortechnicalcontributionsbasedonrecentfuzzersandevaluationplatforms as opposedto theiroldercounterparts.
usecomparablemetricsforfuzzingperformance.
as not all fuzzers measure the same type of coverage we encouragetheuseofone commonevaluationmetricbetween multiplefuzzers.inpractice thisiseasiestdonebyreplaying thecorpusattheendofafuzzingtrial asimplementedby fuzzbench andmlfuzz.
repeat trials often enough to account for variance.
we proposetouse30trialsforfuzzingevaluation resultingin tight confidence intervals.
this sample size is commonlyused in statistics and deemed sufficient for the central limit theorem tohold.asshownin figure4 ml basedfuzzers can have higher coverage variability than gray box fuzzers thus requiring more trialsfor stable baselining.
ensurereproducibleresultsbyfixingandserializing parameters.
whileitisdifficulttocontrolallsourcesofrandomness when training mlmodels on gpus it remains a goodpracticeinbothmachinelearningandsoftwaretesting tocontrolpossiblesourcesofrandomnessbyseedingrandom number generators and reusing the same seeds.
experimentalconfigurationsand inthecaseof ml hyperparameters should be documentedfor reproducibility.
ensureusabilityofproposedfuzzers.
itshouldbe possible to run a newly proposed fuzzer on programs outside the original publication study.
providing a containerized environment can sustainably decrease setup efforts.
we also supportintegrationofnewfuzzerswithexistingbenchmarkingplatforms such as fuzzbench andnowmlfuzz.
conclusion and consequences neural program smoothing for fuzzing neither reaches its advertisedperformance nordoesitsurpassolderfuzzingtechniquesthat are still state of the art.
in our in depth analysis of npsfuzzers we analyzed conceptual limitations of previously published approaches as well as implementation and evaluation issues.
our comprehensive benchmark showed that nps guided fuzzers were by far unable to reach their stated performance.
addressing the implementationissuesdidnotsufficetooutperformstate of the art gray boxfuzzers.thereasonforthelimitedfuzzingperformance lies in the difficulty of the machine learning task which yields trivial models onthe data available duringfuzzing.
to guide future fuzzing research and practical validation we developedimprovedexperimentalguidelinestargetingfuzzingwith machine learning.
our mlfuzz framework for ml based fuzzers includes patched and containerized versions of the investigated fuzzers to help with additional benchmarking.
we encourage researcherstoperformablationstudiesandprovidedeeperinsights intothe componentsthey introduce infuzzing.
while we highlight fundamentallimitationsof neural program smoothing whether and how much this technique can enhance fuzzingremainsanopentopicforfutureresearch.wehopethatthis workcontributestofairandcomprehensiveevaluationsoffuture fuzzers be they ml basedornot.
data availability the open source implementation of neuzz and mlfuzz the evaluation setup andraw results are available at .