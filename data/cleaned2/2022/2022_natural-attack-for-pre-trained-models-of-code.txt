natural attack for pre trained models of code zhou yang jieke shi junda he and david lo school of computing and information systems singapore management university zyang jiekeshi jundahe davidlo smu.edu.sg abstract pre trained models of code have achieved success in many importantsoftwareengineeringtasks.however thesepowerfulmodels are vulnerable to adversarial attacks that slightly perturb model inputs to make a victim model produce wrong outputs.
current worksmainlyattackmodelsofcodewithexamplesthatpreserve operational program semantics but ignore a fundamental requirement foradversarialexamplegeneration perturbationsshouldbenatural tohuman judges which we refer to as naturalness requirement.
inthispaper wepropose alert naturalnessawareattack a black box attack that adversarially transforms inputs to make victim models produce wrong outputs.
different from prior works this paper considers the naturalsemantic of generated examples at the same time as preserving the operational semantic of original inputs.ouruserstudydemonstratesthathumandevelopersconsistently consider that adversarial examples generated by alertare more natural than those generated by the state of the art work by zhangetal.thatignoresthenaturalnessrequirement.onattacking codebert our approach can achieve attack success rates of .
.
and35.
acrossthreedownstreamtasks vulnerabilityprediction clonedetectionandcodeauthorshipattribution.
on graphcodebert our approach can achieve average successrates of .
.
and .
on the three tasks.
the aboveoutperforms the baseline by .
and .
on the two pretrainedmodelsonaverage.finally weinvestigatedthevalueofthe generatedadversarialexamplestohardenvictimmodelsthrough anadversarialfine tuningprocedureanddemonstratedtheaccuracy of codebert and graphcodebert against alert generated adversarial examples increased by .
and .
respectively.
ccs concepts software and its engineering software testing and debugging search based software engineering computing methodologies neural networks.
keywords genetic algorithm adversarial attack pre trained models acm reference format zhou yang jieke shi junda he and david lo.
.
natural attack for pre trained models of code.
in 44th international conference on software permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa association for computing machinery.
acm isbn ... .
icse may pittsburgh pa usa.
acm new york ny usa pages.
introduction recently researchers haveshownthatmodelsofcode likecode2vec andcode2seq canoutputdifferentresultsfor the two code snippets sharing the same operational semantics one of which is generated by renaming some variables in the other.
themodifiedcodesnippetsarecalled adversarialexamples andthe models under attack are called victim models.
naturalnessisafundamentalrequirementinadversarialexample generation.forexample perturbationstoimagesareconstrained with the infinity norm to ensure naturalness .
attack for nlp models also requires adversarial examples to be fluent and natural .weproposethatthenaturalnessrequirementisalso essentialforattackingmodelsofcode.casalnuoveetal.
pr ovide a dual channel view of source code machines that compile and execute code mainly focus on the operational semantics while developers often care about natural semantics of code e.g.
names ofvariables thatcanassisthumancomprehension.althoughmany automated tools have been included into the software developmentprocess thereisnodoubtthatsoftwaredevelopmentisstill a process led by humans.
code that violates coding convention or has poor variable names may be acceptable for machines but rejectedbyhumans.forexample taoetal.
reportthat21.
ofpatchesineclipseandmozillaprojectswererejectedbecausethe patches used bad identifier names or violated coding conventions.
asaresult unnaturaladversarialexamplesmaynotevenpasscode reviews and not to mention being merged into codebases.
existing works on attacking models of code are effective but they focus on preserving operational semantics and barelypayattentiontowhetheradversarialexamplesarenaturalto human judges.
for instance the state of the art black box method mhm randomly selects replacements from a fixed set of variable names without considering semantic relationships between original variables and their substitutes.
figure b shows an adver sarialexamplegeneratedbyreplacingthevariablename bufferin figure1 a to qmp async cmd handler .eventhoughthenewprogrampreservestheoperationalsemantics qmp async cmd handler isnotanaturalreplacementof buffertohumanjudgesconsidering the context surrounding code .
the natural semantics i.e.
human understanding of bufferclearlydonotoverlapwith qmp async c md handler .
in this paper we argue that adversarial examples for models of code should consider preserving the semantics at two levels operationalsemantics cateringformachinesasaudience and natural semantics catering for humans as audience .
the neglect of naturalness requirements in current attack methodsmotivatesustopropose alert naturalnessawareattack ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa zhou yang jieke shi junda he and david lo.
static int buffer empty buffer buffer returnbuffer offset a anoriginal codesnippetthat canbecorrectly classified by a model fine tuned on codebert.static int buffer empty buffer qmp async cmd handler returnqmp async cmd handler offset b mhmgeneratesanadversarialexamplebyreplacingthevariable buffertoqmp async cmd handler.static int buffer empty buffer queue returnqueue offset c alertgeneratesanadversarialexampleby replacing the variable buffertoqueue.
figure the original example in a is from the dataset used in zhou et al.
s study .
both mhm and alertcan generate successfuladversarialexamplesbysubstitutingavariablename.buthmhusesanunnaturalreplacementwhile alertuses a more natural replacement that can better fit into the context and more closely relates to the original variable name.
ablack box attackthatis awareofnatural semanticswhen generatingadversarialexamplesofcode.similartomhm alert renames variables to generate adversarial examples.
our approach has three main parts a natural perturbation generator a heuristic method that tries to generate adversarial examples as fast as possible andageneticalgorithm basedmethodtosearchforadversarial examples more comprehensively in case the heuristic method fails.
this paper investigates the victim models that are fine tuned on the state of the art pre trained models codebert and graphcodebert .alertfirstusesthe maskedlanguagepredictionfunction in pre trained models to generate natural substitutes.
given a code snippet with several masked tokens this function canutilizethecontextinformationtopredictthepotentialvalues ofmaskedtokens.weleveragesuchafunctionincodebertand graphcodebert to generate candidate substitutes for each variable.
then to pickthe substitutesthat aresemantically closer we use pre trained models to compute contextualized embeddings of thesenewtokensandcalculateitscosinesimilarity formeasuringthesemanticdistances withembeddingsoftheoriginaltokens.
we rank these candidates according to cosine similarities and only select the top kcandidates as natural substitution candidates.
alerthas two steps to search adversarial examples using naturalsubstitutioncandidates.itfirstusesagreedyalgorithm greedyattack and then applies a genetic algorithm ga attack if the formerfails.thegreedy attackdefinesametrictomeasurethe importanceofvariablenamesinacodesnippetandstartstosubstitute variables with the highest importance.
an algorithm guided by the importancecanfindsuccessfuladversarialexamplesfasterthanthe randomsamplestrategyusedinmhm .whensubstitutinga variable greedy attack greedily selects the replacement out of all natural substitutes from which the generated adversarial example makesthevictimmodelproducelowerconfidenceontheground truthlabel.ifitfailstochangethepredictionresults greedy attack continues to replace the next variable until all the variables are consideredoranadversarialexampleisobtained.butgenerating adversarialexamplesforcodeisessentiallyacombinatorialproblem and the greedy algorithm may generate sub optimal results.if the greedy algorithm fails we use the ga attack to perform a more comprehensive search.
wefirstconductauserstudytoexaminewhethersearchingfrom substitutes generated by alertcan produce adversarial examples that are natural to human judges.
participants give a naturalness score for very unnatural and for very natural to each adversarialexample.resultsshowthatparticipantsconsistentlyprovidea higher score to alert generated examples on average .
thanexamples generated by the original mhm on average .
that selects substitutes randomly over all variables.
then weevaluatemhmand alertonthesixvictimmodels 2pre trainedmodels 3tasks .weconsiderthreerelevanttasks that may be adversely affected by such an attack vulnerability prediction clone detection and code authorship attribution.1since wearguethatadversarialexamplesshouldlooknaturaltohuman developers wemakemhmsearchonthesamesetofnaturalsubstitutesgeneratedby alert.oncodebert alertcanachieve attack success rates of .
.
and .
across three downstreamtasks.mhmonlyreaches35.
.
and19.
respectively which means that alertcan improve attack success ratesovermhmby17.
.
and16.
.ongraphcodebert ourapproachachievessuccessratesof76.
.
and61.
on thesamethreetasks outperformingmhmby21.
.
and .
.
finally we investigate thevalue of generating adversarial examples by using them to harden the models through an adversarial fine tuning strategy.
we demonstrate that the robustness of codebertandgraphcodebertincreasedby87.
and92.
afteradversarialfine tuningwithexamplesgeneratedby alert.
the contributions of this paper include we are the first to highlight the naturalness requirement in generatingadversarialexamplesformodelsofcode.wealsoproposealertthatisawareofnaturalsemanticswhengeneratingadversarialvariablesubstitutes.auserstudyconfirmsthatusingthese substitutescangenerateadversarialexamplesthatlooknatural to human judges.
alertcan also achieve higher attack success rates than a previous method.
we are the first to develop adversarial attacks on codebert and graphcodebert andshowthatmodelsfine tunedonstate ofthe art pre trained models are vulnerable to such attacks.
we show the value of alert generated examples adversariallyfine tuningvictimmodelswiththeseadversarialexamples canimprove therobustness ofcodebert and graphcodebert againstalertby .
and .
respectively.
therestofthispaperisorganizedasfollows.section2briefly describespreliminary materials.in section3 weelaborate onthe design of the proposed approach alert.
we describe the settings oftheexperimentinsection4 andpresenttheresultsofourexperi mentsthatcomparetheperformanceof alertandsomebaselines in section .
after summarising the threats to validity in section section discusses some related works.
finally we conclude the paper and present future work in section .
1forexample malicioususersmaywritevulnerablecodesnippetsanddonotwant them to be identified.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
natural attack for pre trained models of code icse may pittsburgh pa usa preliminaries thissectionbrieflyintroducessomepreliminaryinformationofthis study includingpre trainedmodelsofcode adversarialexample generation for dnn models and the metropolis hastings modifier mhm method that we use as the baseline.
.
pre trained models of code pre trainedmodelsofnaturallanguagelikebert havebrought breakthrough changes to many natural language processing nlp tasks e.g.
sentiment analysis .
recently researchers have createdpre trainedmodelsofcode thatcanboosttheperformance on programming language processing tasks.
feng et al.
propose codebert that shares the same model architectureasroberta .codebertistrainedonabimodal dataset codesearchnet a corpus consisting of natural language queries and programming language outputs.
codebert has twotrainingobjectives.oneobjectiveis maskedlanguagemodeling mlm whichaims to predict theoriginal tokens thatare masked outinaninput.theotherobjectiveis replacedtokendetection rtd inwhichthemodelneedstodetectwhichtokensinagiveninputare replaced.
experiment results have shown that in downstream tasks like code classification or code search which requires understandingthecode codebertcouldyieldsuperiorperformance although itislesseffectiveincode generationtasks.graphcodebert also uses the same architecture as codebert but the former ad ditionally considers the inherent structure of code i.e.
data flow graph dfg .graphcodebertkeepsthemlmtrainingobjective anddiscardsthertdobjective.itdesignstwodfg relatedtasks data flow edge prediction and node alignment.
graphcodebert outperforms codebert on four downstream tasks.
therearesomeotherpre trainedmodelsofcode.cubert is trainedonpythonsourcecodeandc bert isamodeltrainedon the top starred github c language repositories.
codegpt is a transformer based language model pre trained on program ming languages for code generation tasks.
in this paper we fo cus on analyzing codebert and graphcodebert as they can work on multiple programming languages.
besides recent studies have empirically shown that codebert and graphcodebert demonstrate state of the art performance across multiple code processing tasks.
.
adversarial example generation althoughdeepneuralnetwork dnn modelshaveachievedgreat success on many tasks many research works have shown thatstate of the artmodelsarevulnerabletoadversarialattacks.
adversarial attacks aim to fool dnn models by slightly perturbing the original inputs to generate adversarial examples that are natural to human judges.
many techniques have been proposed to showthatadversarialexamplescanbefoundformodelsindifferent domain including image classification reinforcement learning sentimentanalysis speechrecognition machine translation etc.
according to the information of victim models that an attacker canaccess adversarialattackscanbedividedintotwotypes whiteboxandblack box.inwhite boxsettings attackerscanaccessall the information of the victim models e.g.
using model parametersto compute gradients.
but white box attacks often lack practicality since the victim models are usually deployed remotely e.g.
on cloud services and typically attackers can only access the apis to query models as well as corresponding outputs.
black box attacks mean that an attacker only knows the inputs and outputs of victim models e.g.
predictedlabels andcorresponding confidence .this paper proposes a novel black box attack to mislead models that have the state of the art performance.
adversarial attacks can also be categorized into non targeted attack and targetedattack.
non targeted attacks only aim to make a victim model produces wrong predictions while targeted attacks forceavictimmodeltomakespecificpredictions.forexample a targetedattackmayrequireaclassifiertopredictallthedeerimagesasahorsewhileanon targetrequiresaclassifiertopredictanimage incorrectly.
the attack proposed in this paper is non targeted.
.
metropolis hastings modifier mhm considering the fact that models can be remotely deployed so that modelparametersareinaccessible wefocusonblack boxattacks for models of code.
this section introduces the baseline used in thispaper.zhangetal.
formalizestheprocessofadversarial example generation as a sampling problem.
the problem can be decomposed into an iterative process consisting of three stages selecting the variable to be renamed selecting the substitutions and deciding whether to accept to replace the variable with selected substitution.
zhangetal.proposedmetropolis hastingsmodifier mhm a metropolis hastings sampling based identifier renaming technique to solve this problem and generate adversarial examples formodelsofcode.thismethodisablack boxattackthatrandomly selectsreplacementsforlocalvariablesandthenstrategicallydetermines to accept or reject replacements.
it uses both predicted labelsandcorrespondingconfidenceofthevictimmodeltoselect adversarial examples more effectively.
mhm pre defines a large collectionofvariablenames fromwhichthereplacementsareselected.however neitherthecreationofthiscollectionnorselecting replacements considers the natural semantics.
as a result mhm producesexamplesthatarenotnaturaltohumanjudgments.for example suppose we change a variable name to an extremely long string that is not semantically close to the original variable.
in that case it may change the result of codebert and graphcodebert since the long name will be tokenized into multiple sub tokens impactingtheoutputsignificantly.however developerscertainly will not accept this code.
in this paper similar to mhm we use variable renaming as the adversarial example generation technique and explore how to produceadversarialexamplesthatarenatural.wechoosemhm as our baseline as it does not require gradient information andalso uses fine grained model outputs i.e.
predicted results and correspondingconfidence toperformrenamingandachievesgood attack success rates of one degree of magnitude higher than other black boxapproaches .forexample pouretal.
sapproach onlycausesanabsolutedecreaseof2.
to code2vec sperformance on the method name prediction task.
to ensure that renamed variablesmakenochangesinoperationalsemantics similartomhm we only rename local variables in code snippets.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa zhou yang jieke shi junda he and david lo.
methodology this paper proposes alert naturalness awareattack a blackboxattackthatleveragesthepre trainedmodelsthatvictimmodels arefine tunedon.itgeneratessubstitutesthatareawareofnatural semantics which arecalled naturalness aware substitutions in thispaper.
alerttakestwostepstosearchforadversarialexamples that are likely to be natural to human judges.
the first step greedy attack is optimized to find adversarial examples fast and thesecondstep ga attack isappliedtodoamorecomprehensive search if the former fails.
.
naturalness aware substitution alertleverages the two functions of pre trained models to generate and select naturalness aware substitutes for variables masked language prediction and contextualized embedding.
to generate natural substitutes for one single variable e.g.
index2dict it operates in three steps step .
we convert code snippets into a format that codebert or graphcodebert can take as inputs.
source code often contains manydomain specificabbreviations jargonandtheircombinations which are usually not included in the vocabulary set and cause the out of vocabulary problem .
both codebert and graphcodebertusebyte pair encoding bpe todealwithsuch out of vocabulary problems by tokenizing a word into a list of subtokens.
for example a variable index2dict can be converted into three sub words index dict and then fed into the model.
step .
then we generate potential substitutes for each sub token.
for the sake of simplicity but without any loss of generality let us imagine a case where there is only one variable e.g.
index2dict that only appears once in an input.
we use t angbracketleftt1 t2 tm angbracketright to represent the sequence of sub tokens that bpe produces from the variable name.
for each sub token in the sequence we use the masked language prediction function of codebert or graphcodebert to produce a ranked list of potential substitute sub tokens.
insteadofjustpickingasingleoutput weselectthetop jsubstitutes.
intuitively these substitutes are what pre trained models thinkcanfitthecontextbetter comparedtoothersub tokens .still not all of them are semantically similar to the original sub tokens.
step3.weassumethat angbracketleftti ti ti angbracketrightisasequenceofsub tokensof onevariablename e.g.
correspondingto index 2anddict .we replacethesub tokensintheoriginalsequence twithcandidate sub tokens e.g.
t prime i t prime i t prime i generatedinstep2toget t prime.afterthat the pre trained model computes the contextualized embeddings of each sub token in t prime and we fetch the embeddings for t prime i t prime i 1and t prime i .weconcatenatethesenewembeddingsandcomputeitscosine similarity with concatenated embeddings of ti ti 1andti 2int.
thecosinesimilarityisusedasametrictomeasuretowhatextenta sequenceofcandidatesub tokensissimilartotheoriginalvariable s sequence of sub tokens.
we rank the substitutes in descending order by the value of cosine similarity.
in the end we select top k sequences of substitute sub tokens with higher similarity values and revert them into concrete variable names.
onecodesnippetoftencontainsmultiplevariablesthatappearin variouspositions.algorithm1displayshowweapplytheaboveprocesstoeachvariableextractedfromthesourcecode.first weuseaalgorithm naturalness aware substitutes generation input c input source code m pre trained model output subs substitutes for variables 1subs 2vars extract c 3forvarinvarsdo 4foroccinvar.occurrences do var.occurrences returns all occurrences of var tmp subs perturb occ c m subs subs uniontext.1tmp subs 8end 9subs filter subs 10end 11returnsubs parser to extract variable names vars from the input extract at line and then enumerate all the variables and their occurrences inthe code line3 .
theprocess discussedabove isthen appliedtoeachvariableoccurrencetogeneratepotentialsubstitutes perturb atline6 .wetaketheunionofthesubstitutessetsfor alloccurrencesofavariable line7 .wethenremoveduplicated and invalid words e.g.
those that do not comply with the variable namingrulesorthosethatarekeywordsinprogramminglanguages filter atline9 afterwhichwereturnfilteredsubstitutes line .werefertothesefilteredsubstitutesasthe naturalness aware substitutes.
.
greedy attack .
.
overallimportancescore.
toperformsemantic preserving transformation by renaming variables an attacker first needs to decide which tokens in a code snippet should be changed.
inspired byadversarialreplacementsfornlptasks thatprioritizesmore important tokens in a sentence for each variable in a code snippet we first measure its contribution to helping the model make a correct prediction.
we introduce a metric called the importance score toquantifysuchcontribution.formallyspeaking theimportance score of the ithtoken in a code snippet cis defined as follow isi m c m c i in the above formula yis the ground truth label for cand m c represents the confidence of m s output corresponding to the labely.
a new code snippet generated by substituting variable names is called a variant.
a variant c iand is created by replacing theithtoken which must be a variable name in cwith angbracketleftunk angbracketright whichmeansthattheliteralvalueatthispositionisunknown.intuitively the importance score approximates how knowing the value of theithtoken affects the model s prediction on c.i fisi it means that the token tican help model make correct prediction on c.asstatedinsection3.
onecodesnippetoftencontainsmultiple variablesthatappearinmultiplepositions.alltheoccurrencesof a variable should be updated accordingly when performing adversarialattacks soweextendthedefinitionofimportancescorefor asingle tokento theoverall importancescore ois for avariable.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
natural attack for pre trained models of code icse may pittsburgh pa usa ois is computed as follow oisvar summationdisplay.
i var isi wherevaris a variable in c andvar means all occurrences of varinc.itisnoticedthatthedefinitionofoiscanbetterreflectthe unique property of attacking models of programming languages as compared to models of natural languages.
even though a variable atonepositionistrivial appearingmoreoftencanmakeitanimportantvariable i.e.
avulnerableword inadversarialattacks.the overall importance score can be viewed as an analogy to the gradient information in white box attacks.
for example if the gradients arelargeratsomepositionsofinputs e.g.
certainpixels thenitis easier to change the model outputs if we perturb those positions.
based on tree sitter2 a multi language parser generator tool we implement a name extractor that can retrieve all the variable namesfromsyntacticallyvalidcodesnippetswritteninc pythonor java.
more specifically to avoid altering the operational semantics weonlyextractthelocalvariablesthataredefinedandinitialized within the scope of the code snippet and swap them with valid variablenamesthathaveneveroccurredinthecode.toimprove accuracy variable names that collide with a field name are also excluded.afterextraction wecomputetheoisforeachvariable and proceed to the next step.
.
.
word replacement.
we design an ois based greedy algorithm to search substitutes that can generate adversarial examples.
algorithm illustrates the process of this greedy attack.
first we rank extracted variables from the original code snippet in descending order according to their ois line to .
we select the first variable from them and find all its candidate substitutes generated following the process described in section .
line to .
we replacethevariableintheoriginalinputwiththesesubstitutesto createalistofvariants afterwhichthesevariantsaresenttoquery the victim model.
we collect returned results and see if at least one variant makes the victim model make wrong predictions line to12 .ifthereissuchavariant thegreedy attackreturnsitasa successful adversarial example.
otherwise we replace the original inputwiththevariantthatcanmostlyreducethevictimmodel s confidence on the results and select the next variable to repeat the above processes line .
greedy attack terminates either when a successfuladversarialexampleisfound line11 orwhenallthe extracted variables are enumerated line .
considering ois information is beneficial to the greedy attack in two aspects.
first as discussed in section .
.
if a variable has a higher ois it indicates significant impacts of modifying this variableinthecodesnippet.givinghigherprioritiestovariables withlargeroiscanhelpfindsuccessfuladversarialexamplesfaster whichmeansthatfewerqueriestothevictimmodelarerequired.
it increases the usability of our attack in practice since remotelydeployed black box models often constrain the query frequency.
secondly finding successful adversarial examples early also means fewer variables are modified in an original code snippet making the generated adversarial examples more natural to human judges.
greedy attack workflow input c input source code subs substitutes for variables in c output c prime adversarial example 1c prime c 2vars extract c extract varsfromc 3vars sort vars sortvarsaccording to ois 4forvarinvarsdo 5list c 6forsubinsubs do iterate all the substitutes for var tmp c replcae c prime var sub ifm tmp c m c then c prime tmp c returnc prime end list c list c uniontext.1tmp c 14end 15c prime select list c select the adversarial example with lowest model s confidence on the ground truth label 16end 17returnc prime .
ga attack finding appropriate substitutes to generate adversarial examples is essentially a combinatorial optimization problem whose objective is to find theoptimal combination of variables and corresponding substitutes that minimizes the victim model s confidence on the ground truth label.
greedy attack can run faster but may be stuck in a single local optimal leading to low attack success rates.
we also design an attack based on genetic algorithms ga called gaattack.
if the greedy attack fails to find a successful adversarial example we apply ga attack to search more comprehensively.
algorithm 3shows the overview ofhow ga attack works.
itfirst initializes the population line more detailed are given in sec tion .
.
and then performs genetic operators to generate new solutions line2to11 .ga attackcomputesthefitnessfunction section3.
.
andkeepsolutionswithlargerfitnessvalues line .
in the end the algorithm returns the solution with the highest fitness value line to .
.
.
chromosome representation.
in ga the chromosome represents the solution to a target problem and a chromosome consists of a set of genes.
in this paper each gene is a pair of an original variableanditssubstitution.ga attackrepresentschromosomesas a list of such pairs.
for example assuming that only two variables aandb can be replaced in an input program the chromosome angbracketlefta x b y angbracketrightmeans replacing atoxandbtoy.
.
.
population initialization.
in the running of ga a population a set of chromosomes evolves to solve the target problem.
gaattackmaintainsapopulationwhosesizeisthenumberofextracted variablesthatcanbesubstituted.sincega attackwillbetriggered only after greedy attack fails it can leverage the information discoveredinthepreviousstep.foreachextractedvariable greedyattackfindsitssubstitutionthatcandecreasethevictimmodel s confidence on the ground truth label most.
given one variable and authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa zhou yang jieke shi junda he and david lo.
algorithm ga attack workflow input c input source code max iter max iteration r crossover rate child size number of generated children in each iteration output c prime adversarial example 1population greedy initialization c 2whilenot exceed max iterdo 3child list 4whilelen child lis child sizedo p ifp rthen child crossover population else child mutation population end child list.append child 12end 13population selection population child list 14end 15c prime argmax population select the one with highest fitness value 16returnc prime thesubstitutionfoundbygreedy attack ga attackcreatesachromosome that only changes this variable to the substitution and keeps other variables unchanged.
the process is repeated for each variable ina codesnippet to obtaina population.
forexample assumingthatthreevariables a bandc areextractedfromaninput program andgreedy attacksuggests angbracketlefta x b y c z angbracketright ga attack initializes a population of three chromosomes angbracketlefta x b b c c angbracketright angbracketlefta a b y c c angbracketright.
and angbracketlefta a b b c z angbracketright.
.
.
operators.
greedy attackrunsinmultipleiterations.ineach iteration two genetic operators mutation and crossover are used toproducenewchromosomes i.e.
children .weapplycrossover with a probability of rand mutation with a probability of r line8 .themutationoperation line9 ontwochromosome c1 andc2 worksasfollows wefirstrandomlyselectacut offposition h and replace c1 s genes after the position hwithc2 s genes at the corresponding positions.
as an example for two chromosomes c1 angbracketlefta x b y c c angbracketrightandc2 angbracketlefta x b b c z angbracketright and a cut off positionh thechildgeneratedbycrossoveris angbracketlefta x b y c z angbracketright.
given a chromosome in the population the mutation operator randomly selects a gene and then replaces it with a randomly selected substitute.
for instance ain angbracketlefta x b b angbracketrightis selected and a x becomes a aa.
.
.
fitness function.
ga uses a fitness function to measure and compare the quality of chromosomes in a population.
a higher fitnessvalueindicatesthatthechromosome variablesubstitutions isclosertothetargetofthisproblem.wecomputethevictimmodel s confidence values with respect to the ground truth label on the original input and the variant.
the difference between confidence values is used as the fitness value.
assuming tis the original input andt primeis a variant corresponding to a chromosome the fitness value of this chromosome is computed by fitness m t m t prime after generating children in one iteration we merge them to the current population and perform a selection operator line .
ga attack always maintains a population of the same size i.e.
numbers of extract variables .
it discards the chromosomes that have lower fitness values.
experiment setup .
datasets and tasks we introduce the three downstream tasks and their corresponding datasets used in our experiments.
the statistics of datasets are presented in table .
.
.
vulnerability prediction.
this task aims to predict whether a givencodesnippetcontainsvulnerabilities.weusethedatasetthat waspreparedbyzhouetal.
.thedatasetisextractedfromtwo popular open sourced c projects ffmpeg3and qemu4.
in zhou et al.
sdataset 318functionsarelabeledaseithercontainingvulnerabilitiesorclean.thisdatasetisincludedaspartofthecodexglue benchmark that has been used to investigate the effectiveness of codebert for vulnerability prediction.
codexglue divides the datasetintotraining developmentandtestsetthatwereuseinthis study.
.
.
clone detection.
the clone detection task aims to check whether two given code snippets are clones i.e.
equivalent in operational semantics.
bigclonebench is a broadly recognized benchmarkforclonedetection containingmorethansixmillion actualclonepairsand260 000falseclonepairsfromvariousjava projects.
each data point is a java method.
in total the dataset has covered ten frequently used functionalities.
following the settings of prior works we filtered the data which do not have a labelandthenbalancedthedatasettomaketheratiooftrueandfalse pairs to .
to keep the experiment at a computationally friendlyscale werandomlyselect90 102examplesfortrainingand for validation and testing.
.
.
authorship attribution.
the authorship attribution task is to identifytheauthorofagivencodesnippet.wedidourexperiments with the google code jam gcj dataset which is originated from googlecodejam challenge aglobalcodingcompetitionthatgoogle annually hosts.
alsulami et al.
collected the gcj dataset and made it publicly available.
the gcj dataset contains python files 70authorsandtenfilesforeachauthor butwenoticethat some python files are c code.
after discarding these c source code files we get python files in total.
of files are used for testing and of files are for training.
.
target models thispaperinvestigatestherobustnessofthestate of the artpretrainedmodels codebert andgraphcodebert .toobtain the victim models we fine tunecodebert andgraphcodebert on the three tasks mentioned in section .
.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
natural attack for pre trained models of code icse may pittsburgh pa usa table statistics of datasets and of victim models.
tasks train dev test model acc vulnerability prediction 732codebert .
graphcodebert .
clonedetection 000codebert .
graphcodebert .
authorshipattribution 132codebert .
graphcodebert .
.
.
codebert.
codebert is a pre trained model that is capable of learning from bimodal data in the form of both pro gramming languages and natural languages.
when fine tuningcodebert on vulnerability prediction and clone detection task we use the same parameter settings adopted in the codexglue exceptthatweincreasethemaximalinputlengthto512and achievea slightlyhigherperformance thanresultsreported inthe codexglue paper.
since there is no instruction on the hyper parameter setting for fine tuning on authorship attribution task we use the same settings and the obtained model can achieve .
accuracy slightly higher than the accuracy of the lstm model reported in .
.
.
graphcodebert.
graphcodebert considers the inherent structure of the program and takes advantage of the data flow representation.
we set the maximal input length of graphcodebertto512andfollowthesamesettingforotherhyper parametersinthegraphcodebertpaper tofine tuneitonthethreedownstreamtasks.ontheclonedetectiontask themodelcanachieveanaccuracyof97.
almostthesamewiththeperformanceof97.
reported in .
on the vulnerability prediction and authorship attributiontask graphcodebertalsoachievestheperformance that is comparable with the results of codebert.
the performance of these models is displayed in table .
the results we obtain are closed to results reported in their original papers and another recent paper highlighting that the victim models used in our experiments are adequately fine tuned.
.
settings of attacks alerthas a number of hyper parameters to be set including the number of natural substitutions generated for each variableand parameters for ga attack in algorithm .
our experimentsetting allows alertto generate candidate substitutions for each variable occurrence and it selects the top substitutions rankedbythecosinesimilaritywithoriginalembedding.forgaattack we set child sizeas and set a dynamic value for the maximaliterations max iter thelargeroneof5timesthenumber of extracted variables or .
the crossover rate ris set as .
.
we consider mhm as our baseline which has two hyperparameters the maximum number of iterations and the number of variables sampled in each iteration.
the mhm paper suggests settingthelatteras30butdoesnotprovideastandardsettingfor the maximal iterations.
in each iteration mhm needs to query the victimmodelmanytimes whichistime consuming.wesampled testing data from the vulnerability prediction task and found that over successful adversarial examples are found before 100figure2 resultsoftheuserstudytoevaluatenaturalnessofadversarialexamples.they axiscorrespondstotheaverageratings 5meansverynatural 1meansveryunnatural .the x axis represents distinguished independent participants.
iterations.
to make the mhm experiment within a computational friendly scale we set the maximum number of iterations of mhm to .
the original mhm can only perturb c programs so we extend it to perturb python and java code.
experiment results and analysis inthissection weperformexperimentstoanswerresearchquestions related to the performance of adversarial attacks.
we care aboutnaturalness attacksuccessratesandscalabilityaswellasthe valueofusingadversarialexamplestoimprovemodelrobustness viaadversarialfine tuning whicharediscussedbyansweringthree research questions respectively.
rq1.
how natural are the adversarial examples generated by alert?
whengenerating substitutionsfor variables incode alerttakes thenaturalsemanticsofadversarialexamplesintoconsideration.
this research question explores whether these naturalness aware substitutions can help produce adversarial examples that are more naturaltohumanjudges.toanswerthisquestion weconductauser studytoanalyzethenaturalnessofexamplesgeneratedbymhm mhm ns and the proposed alertmethod.
unlike the original mhmthatignoresthenaturalness mhm nsselectsareplacement from the same pool of naturalness aware substitutions as alert.
as the original mhm only works for code snippets written in c werandomlysamplesomecodesnippetsthatcanbesuccessfully attackedby alert mhm andmhm nsfromthedatasetofthe vulnerability detection task section .
.
we have introduced a fewmoreconstraints whensamplingthe codesnippets tosave participantsfromreadinglongcodesnippets weintentionallysample succinct and short code segments by limiting the code snippet length to tokens and the attack methods may choose to replace different variables in the same code snippet we only select theexamplesforwhichatleastonevariableismodifiedbyallthe threemethodstomakethecomparisonfair.thereare196ccode authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa zhou yang jieke shi junda he and david lo.
table comparison results of attack success rates asr on attacking codebert and graphcodebert across three tasks.
thenumbersintheparenthesescorrespondtotheabsoluteimprovementwithrespecttotheattacksuccessratesofmhm ns.
taskcodebert graphcodebert mhm ns greedy attack alert mhm ns greedy attack alert vulnerability detection .
.
.
.
.
.
.
.
.
.
clone detection .
.
.
.
.
.
.
.
.
.
authorship attribution .
.
.
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
.
.
a vcr on attacking codebert b noq on attacking codebert c vcr on attacking graphcodebert d noq on attacking graphcodebert figure comparison results of variable change rate vcr and number of queries noq on attacking codebert andgraphcodebert.they axiscorrespondstothenormalizedvaluesofvcrandnoq.thex axisrepresentsdownstreamtasks.
snippets that satisfy the aforementioned constraints.
we compute a statistically representative sample size using a popular sample size calculator5with a confidence level of and a confidence interval of .
we sample code snippets to conduct the user study which is statistically representative.
foreachselectedcodesnippet wecanconstruct3pairs.each pair contains the original code snippet and an adversarial example generatedbyeither alert mhm ormhm ns.wehighlightthe changed variables in each pair and present them to users.
users are asked to evaluate to what extent the substitutions are naturally fittingintothesourcecodecontexts.giventhestatement thenew variablenamelooksnaturalandpreservestheoriginalmeaning participantsneedtogivescoresona5 pointlikertscale where 1meansstronglydisagreeand5meansstronglyagree following thesamesettingsusedbyjinetal.
.participantsdonotknow which attack method produces which adversarial example in a pair.
the user study involves four non author participants who have abachelor masterdegreeincomputersciencewithatleastfour years of experience in programming.
each participant evaluates the100pairsindividually.wecalculatetheaverageratingsgivento adversarial examples generated by each attack method per participant andpresenttheresultsinfigure2.thex axisdistinguishes each participant and the y axis shows the average ratings.
the results show that the usage of alert generated substitutions can helpgeneratemuchmorenaturaladversarialexamples.thefour participants give average scores of close to to adversarial examplesgeneratedby alertandslightlyloweraveragescoresto adversarial examples generated by mhm ns these indicate that participants perceive that the substitutions generated by these two methods are natural.
participants consistently give lower scores accessed .
on average to examples generated by mhm showing that they think the variable substitutions are unnatural.
answerstorq1 participantsconsistentlyfindthatadversarial examples generated by alert a naturalness aware method are natural while those generated by mhm a naturalness agnostic method are unnatural.
rq2.
how successful and minimal are the generated adversarial examples?
how scalable is the generation process?
to answer this question we evaluate the effectiveness of alert andmhm nsonattackingcodebertandgraphcodebertconsideringthreedimensions.
specifically weusethree metrics each capturingonequalitydimension tomeasuretheperformanceof an adversarial example generation method.
each metric is defined basedonadataset x whereeachelement x xisacodesnippet thathasatleastonelocalvariableandavictimmodel mthatcan predictallexamplesin xcorrectly.thethreemetricsaredefined as follows.
attacksuccessrate asr theasrofanadversarialexample generation method is defined as x x x m x prime m x x where x primeis a generated example.
a higher asr indicates that an attack method has better performance.
variable change rate vcr assuming that an input code snippetxihasmilocal variables and an attacker renames ni variablesin xi wedefinevariablechangerate vcr oftheattack overxas summationtext.
ini summationtext.
imi.alowervcrispreferablesinceitmeansthat fewer edits are made to find successful adversarial examples.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
natural attack for pre trained models of code icse may pittsburgh pa usa number of queries noq in adversarial attacks especially theblack boxones thenumberofqueriestothevictimmodel needstobekeptaslowaspossible.inpractice victimsmodels are usually remotely deployed and it is expensive and maybe alsosuspicious toquerymodels toomanytimes.wecount the number of queries noq to the victim models when each attack generatesadversarialexamplesondataset x.attacksthathave lower noq are more scalable as well.
table displays the comparison results between mhm ns and alertonthesixvictimmodels 2models 3tasksasdescribedin section4.
.wealsoreporttheresultsofsolelyusinggreedy attack to emphasize the improvements brought by ga attack.
results show that greedy attack has .
.
and .
attack successrateoncodebertacrossthreedownstreamtasks which corresponds to an improvement of .
.
and .
over mhm ns respectively.byemployingga attackin alert wecan boosttheperformanceevenfurther mhm nsresultsareimproved by .
.
and .
in terms of asr.
on graphcodebert greedy attack can outperform mhm ns by .
.
and .
forthethreetasks thenumbersareboostedto21.
.
and .
when ga attack is employed.
moreover alertmakesfewereditstotheoriginalexamplesand ismorescalablethanthebaseline.figure3comparesresultsintermsof vcr and noq.
the x axis corresponds to each downstream task and the y axis represents normalized values of the two evaluation metrics.onallvictimmodels alertmodifiesfewervariablesto generateadversarialexamples.itindicatesthat alertcanmake minimal changes to input code snippets and produce more natural and imperceptible adversarial examples.
besides alertqueries victim models less than mhm ns does.
the noq of solely using greedy attackis82.
lessthanmhm ns.whenga attackis employed the noq increases but is still .
less than mhm ns which shows that alertis more practical since victim models are usually remotely deployed and may be costly to query and may prevent frequent queries.
querying victim models is the most time consumingpartofexperiments sofewernoqalsoshowsthat alerthas lower runtime.
answers to rq2 in terms of the attack success rate alertcan outperform the mhm by .
.
and .
on codebert as well as .
.
and .
on graphcodebert across three downstream tasks.
inaddition to achieving a superior attack success rate our method also makes fewer changes and is more scalable.
rq3.
can we use adversarial examples to harden the victim models?
in this research question we explore the effectiveness of using adversarial fine tuning as a defense against attacks.
we leverage alertto generate adversarial examples for each victim model on their corresponding training sets.
if a victim model predicts wronglyonanoriginalinputornolocalvariablenamecanbeextractedfromit weskipthisexample.forotherinputsinthetraining sets we select at most one adversarial example for each of them.
if alertattackssuccessfully wechoosethefirstly foundadversarialexample.if alertfailstoattack weselecttheexamplethatcan minimize thevictim model s confidenceon the ground truth label.
these generated adversarial examples are then augmented into the original training set and form the adversarial training set.
we then fine tune the victim model on the adversarial training set.
after adversarial fine tuning we obtain two models codebertadvandgraphcodebert advandevaluatethemontheadversarial examples generated in rq2.
table shows the new models predictionaccuracyonpreviouslygeneratedadversarialexamples.it is noted that the original victim models that are not hardened byadversarialretraining predictalltheseexampleswrongly i.e.
they have an accuracy of .
from table we can observe that alltheadversariallyfine tunedmodelsperformmuchbetterthan the original ones.
the average improvement on examples generated by solely using greedy attack and employing ga attack isclose.
codebert adv improves accuracy against greedy attack andalertby87.
and87.
respectively.graphcodebertadvimprovesaccuracyagainstgreedy attackand alertby92.
and .
.
accuracy improvement on adversarial examples generatedbymhm nsisrelativelymoreminor .
and75.
on codebert and graphcodebert .
answers to rq3 the adversarial examples generated byalertare valuable in improving the robustness of victim models.
adversarially fine tuning victim models withalert generated adversarial examples can improve theaccuracyofcodebertandgraphcodebertby87.
and .
respectively.
threats to validity internal validity the results obtained in our experiment can varyunderdifferenthyper parameterssettings e.g.
inputlength numbers of training epochs etc.
to mitigate the threats we setthe input length to codebert and graphcodebert as the maximalvalue toensurethattheyseethesamenumbersoftokens for the same code snippet.
for the remaining hyper parameters we keep them the same as described in .
we compare the performanceofmodelsobtainedinthispaperwithresultsreported intheliterature toshowthatourmodelsareproperly trained.
externalvalidity inourexperiments weinvestigatetwopopular pre trainedmodelsofcodeonthreedownstreamtasks.however our results may not generalize to other pre trained models and downstreamtasks.weuseagenericparsertoextractvariablenames fromcodesnippetswritteninc pythonorjava butitcannotwork in other programming languages like ruby.
related work this section describes the works that are related to this paper including the pre trained models of code and adversarial attacks on models of code.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa zhou yang jieke shi junda he and david lo.
table3 robustnessanalysisonadversariallyfine tunedvictimmodels.thenumbersarethepredictionaccuraiesofadversarially fine tuned models codebert adv and graphcodebert adv on the adversarial examples generated in rq2.
taskscodebert adv graphcodebert adv mhm ns greedy alert mhm ns greedy alert vulnerability detection .
.
.
.
.
.
clone detection .
.
.
.
.
.
authorship attribution .
.
.
.
.
.
overall .
.
.
.
.
.
.
pre trained models of code code representation models like code2vec and code2seq that use syntactic and structural information have shown good performance on a range of downstream tasks.
however some pretrained models for natural languages nl like bert and gpt haverecentlydemonstratedexcellenttransferabilityto programminglanguages pl andstrongercapabilitiesofcapturing semantics information than code2vec or code2seq.
inspired by the success of these language models pre trained models of code have recentlybecomemoreandmorepopularinthefieldofcodeintelligenceandbenefitedabroadrangeoftasks .
thesecurrentpre trained modelsofcodecanbedividedinto two types embedding models and generative models.
the two models codebert and graphcodebert investigated in our experiments are representatives of embedding models.wehavedescribedcodebertandgraphcodebertinsec tion2.
.here webrieflydescribeotherembeddingmodels.kanadeetal.
usedthesamemodelarchitectureandtrainingobjectives as bert but trained it on python source code to produce cubert.
buratti et al.
introduced c bert a transformer based language modeltrainedon100popularclanguagerepositoriesongithub.
both cubert and c bert were trained on a single programming language which limits their usage scenarios.
cubert and c bert outperformgenericbaselineslikelstmmodelsbutdonotshow superiorperformancethancodebertandgraphcodebert sowe investigate the latter two in this work.
the other branch of pre trained models is generative models which are designed for generative tasks like code completion.
svyatkovskiy et al.
introduce gpt c a variant of gpt trained on a large corpus containing multiple programming languagesandachievedimpressiveperformanceincodegeneration tasks.
lu et al.
provides codegpt which has the same model architectureandtrainingobjectiveofgpt .codegptwastrained on python and java corpora from the codesearchnet dataset .
despite their success on generation tasks these models are unable to getcomplete contextual information asthey are unidirectional decoder only models which only rely on previous tokens and ig norethefollowingones sowediscardgenerativemodelsin the investigation list of this work.
.
adversarial attack on models of code yefet et al.
proposed damp a white box attack technique that adversarially changes variables in code using gradient informationof the victim model.
although their method shows effectiveness in attacking three models code2vec ggnn andgnn film it requires victim models to process code snippets using one hotencoding which is not applicable to codebert and graphcodebert investigated in our paper as they use bpe to process tokens.
srikant et al.
apply pgd to generate adversarial examples of code.
besides these white box approaches generatesubstitutesbychangingaone hotencodingtoanotherandmappingitbacktoatoken whichcannotguaranteetosatisfynatu ralnessrequirements.suchawhite boxattackislesspracticalsince victim models are usually deployed remotely making parameter information hard to be accessed.
thereareseveralblack boxmethodsforevaluatingtherobustness of models of code.
one that has been shown to be much more effectivethantheothersismhm whichweuseasourbaseline.
we have presented the details of mhm in section .
.
here we present the other related studies.
wang et al.
provide a benchmark consisting of refactored programs and evaluate the per formanceofneuralembeddingprogramsonit.rabinetal.
also uses variable renaming to evaluate the generalizability of neural programanalyzers andshowthatggnn changesitsprediction on .
of transformed code.
pour et al.
p r o p o s e da testing framework for dnn of source code embedding which can decrease the performance of code2vec on method name predictiontaskby2.
.applisetal.
usemetamorphicprogram transformations to assess the robustness of ml based program analysis tools in a black box manner.
adversarial attack on models of code can be conducted beyond generating adversarial examples for a well trained model.
schuster et al.
show that code completion models are vulnerable to poisoning attacks that add some carefully designed files to thetraining data of a model.
nguyen et al.
show that the stateof the artapirecommendersystemscanbeattackedbyinjecting malicious data into their training corpus.
conclusion and future work in this paper we highlight the naturalness requirement in generating adversarial examples for models of code.
we propose alert naturalness awareattack a black box attack that adversarially transforms inputs code snippets to force pre trained mod els to produce wrong outputs.
alertcan generate naturalnessaware substitutes.
a user study confirms that these substitutes can help generate adversarial examples that look natural to humanjudges.
in contrast users consistently think examples generated by a prior method that employs randomselection to be unnatural.
apart from being aware of naturalness alertis also effective in finding adversarial examples.
we apply alertto victim models fine tuned on state of the art pre trained models codebert and authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
natural attack for pre trained models of code icse may pittsburgh pa usa graphcodebert .theresults showthaton attacking codebert alertcan achieve average success rates of .
.
and .
across three downstream tasks vulnerability prediction clone detection and code authorship attribution.
it outperforms the baseline by .
.
and .
.
on graphcodebert our approach can achieve average success rates of .
.
and61.
on the three tasks respectively outperforming the base line by .
.
and .
.
we also explore the value of adversarialexamplestohardencodebertandgraphcodebert throughanadversarialfine tuningprocedureanddemonstratedthe robustnessofcodebertandgraphcodebertagainst alertincreasedby87.
and92.
respectively.weopen source alert at in the future we plan to consider more victim models and more downstreamtasks.wealsoplantoboosttheeffectivenessof alert and improve the robustness of victim models further.