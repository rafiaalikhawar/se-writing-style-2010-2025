input splitting for cloud based static application security testing platforms maria christakis mpi sws germanythomas cottenier amazon web services usaantonio filieri amazon web services usa linghui luo amazon web services germanymuhammad numair mansur mpi sws germanylee pike amazon web services usa nicol s rosner amazon web services usamartin sch f amazon web services usaaritra sengupta amazon web services usa willem visser amazon web services usa abstract as software development teams adopt devsecops practices application security is increasingly the responsibility of development teams who are required to set up their own static application security testing sast infrastructure.
since development teams often do not have the necessary infrastructure and expertise to set up a custom sast solution there is an increased need for cloud based sast platforms that operate as a service and run a variety of static analyzers.
adding a new static analyzer to a cloud based sast platform can be challenging because static analyzers greatly vary in complexity from linters that scale efficiently to interprocedural dataflow engines that use cubic or even more complex algorithms.
careful manual evaluation is needed to decide whether a new analyzer would slow down the overall response time of the platform or may timeout too often.
we explore the question of whether this can be simplified by splitting the input to the analyzer into partitions and analyzing the partitions independently.
depending on the complexity of the static analyzer the partition size can be adjusted to curtail the overall response time.
we report on an experiment where we run different analysis tools with and without splitting the inputs.
the experimental results show that simple splitting strategies can effectively reduce the running time and memory usage per partition without significantly affecting the findings produced by the tool.
ccs concepts security and privacy software and application security .
keywords software security api usage checking static analysis in the cloud permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november singapore singapore association for computing machinery.
acm isbn .
.
.
.
reference format maria christakis thomas cottenier antonio filieri linghui luo muhammad numair mansur lee pike nicol s rosner martin sch f aritra sengupta and willem visser.
.
input splitting for cloud based static application security testing platforms.
in proceedings of the 30th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november singapore singapore.
acm new york ny usa pages.
.
introduction with the increasing popularity of devsecops development practices static application security testing sast shifts further to the left in the software development life cycle and becomes the responsibility of developers rather than security experts.
this creates a growing demand for easy to use solutions.
many development teams do not have the capacity or expertise to configure and maintain their own static analysis infrastructure and prefer sast platforms that offer a variety of static analyses on demand.
opensource platforms such as the software assurance marketplace swamp or shipshape and their commercial alternatives offer a convenient abstraction.
they provide a simple interface through which developers submit code and build artifacts in their languages of choice and receive recommendations on how to improve the code.
internally such cloud based sast platforms may employ a variety of static analysis tools such as .
sast platforms typically are run as a cloud based service and the individual analysis tools are containerized and instantiated on demand on cloud based machines.
developers expect such a sast platform to handle inputs codebases of arbitrary complexity and still deliver results within a certain time window.
this is especially true for customers that integrate sast platforms in their continuous integration and deployment ci cd pipelines.
to maintain a predictable response time sast platforms face the challenge that they need to be able to scale to different sizes of inputs and that every time they add a new analysis tool they have to ensure that the new tool does not slow down the response time for existing customers.
vertical scaling by adding more memory or faster machines is not a cost effective solution to the risk of running out of time or space when analyzing complex inputs.
provisioning machines large esec fse november singapore singapore christakis cottenier filieri luo mansur pike rosner sch f sengupta visser enough to handle the most complex analysis inputs would make the service unnecessarily expensive for customers that analyze smaller and simpler codebases.
in the cloud a large number of small machines is significantly less expensive than a small number of high performance machines .
moreover since many sast tools have superlinear time complexity even the most powerful machine will eventually not suffice.
much research has been conducted on adding various optimizations to improve the scalability of specific analysis engines such as summarization of method calls caching and reuse of partial results from prior analyses and incremental analysis .
however when operating a sast platform modifying the individual tools may not be an option because the tools might be proprietary or maintaining forks with custom modifications may be too costly.
thus a horizontal scaling strategy to distribute and balance the analysis load is still needed.
horizontal scaling needs to split up inputs into pieces such that each analysis tool employed by the platform can handle its input within the expected response time.
the different pieces can then be analyzed on parallel instances of a given analysis tool.
such a horizontal scaling can be configured per analysis tool but without modifying the tool itself.
more complex tools can be configured to handle smaller pieces of code than lightweight tools to ensure that the overall latency of the platform does not change when a new complex tool gets added.
in this paper we present an approach to horizontally scale analysis tools in a static analysis platform.
our approach takes as input a program and a bound for the size of code that should be analyzed by each single machine.
it then employs a configurable splitting strategy to split the input program into partitions such that the amount of code in each partition is below the provided bound.
we evaluate how this splitting process affects the accuracy of different static analysis tools and how the computational cost of analyzing partitions in parallel relates to the cost of analyzing the entire input program.
splitting code into partitions comes with several challenges.
the first challenge is that information may be lost because dependent code fragments are placed in separate partitions.
this may impact the precision and recall of static analysis tools.
for example a real defect arising from the interaction between two classes may become a false negative if those classes end up in different partitions.
similarly the evidence that a vulnerability has been correctly mitigated may become invisible when defect and mitigation split across partitions yielding a false positive.
this leads to our first research question rq1.
what is the impact of splitting a program and analyzing the partitions in isolation on a tool s accuracy?
the second challenge when splitting code into partitions is that the complexity of static analysis may not be tied just to the size of the code.
for example data flow analysis is cubic in the size of data flow facts that are tracked .
that is if data flow facts are not evenly distributed across the program splitting may not reduce the overall time or memory consumption of data flow analysis if all facts end up in the same partition.
other analysis techniques such as bi abduction used by infer may require a different type of partitioning since their complexity is not tied to data flow facts.
hence we cannot guarantee that analyzing a partition uses less time or memory than analyzing the original program.
so our second research question is rq2.
how do static analyzers performon the partitions compared to the original program in terms of time and memory usage?
a third challenge is to find a splitting strategy that works for different kinds of static analysis tools.
splitting strategies may have different complexities for static analysis tools targeting different languages.
e.g.
identifying the direct dependencies of a java class file is roughly constant since it is sufficient to look at the constant pool .
in python however one has to iterate over the entire syntax tree of a file to determine its dependencies.
a splitting strategy that takes dependencies into consideration is computationally more expensive for python than for java.
thus our third research question is rq3.
what kinds of static analysis tools would benefit from splitting strategies discussed in the paper?
to answer these three research questions we implement a splitting approach that works with two different strategies to create partitions.
the first strategy sizelimiting na vely splits the input program into partitions based on an upper bound son the number of files or classes per partition.
sorted files in lexicographical order are added to a partition until this bound sis reached and then a new partition is started.
the second strategy splitmerge uses dependency information between the files of the input program to create partitions that include the necessary dependencies of a file.
insizelimiting all partitions are disjoint while in splitmerge partitions can overlap.
we apply these two splitting strategies to a set of benchmark programs and analyze the resulting partitions with the static analysis tools rapid and infer .
we evaluate the impact of both splitting strategies over non splitting on these analysis tools in terms of reported findings and computational performance.
the contributions of this paper are as follows we motivate why input splitting is a relevant problem for sast platforms and why additional research in this area is required.
we present experimental results that input splitting can work in practice with different sast tools.
we show that with a proper selection of splitting strategy all evaluated sast tools can benefit from splitting.
yet finding the right splitting strategy depends on the complexity of the used sast tool.
while tools like rapid andinferwhich perform complex analyses benefit most from dependency guided splitting strategy like splitmerge in terms of reduction in latency memory consumption and minimizing the loss of findings for inexpensive linter like or intra procedural analyses such as bandit a naive strategy likesizelimiting may be more beneficial.
we do not claim that any of the proposed strategies are optimal nor that splitting is the only way to increase the maximum tractable problem size.
instead this evaluation demonstrates how a lightweight splitting strategy can already significantly improve latency scalability and cost effectiveness of cloud based sast platforms.
for the future we envision that such strategies can be used to reduce the cost of integrating new static analysis tools into a sast platform.
moreover instead of developing and benchmarking explicit splitting strategies for every new tool a splitting algorithm could be generalized to adjust the splitting strategy based on the number of observed timeouts.
1368input splitting for cloud based static application security testing platforms esec fse november singapore singapore motivating example we motivate the need for splitting with an example from the owasp benchmark1.
this standard benchmark for java sast tools consists of test cases for different types of security vulnerabilities.
each test case is a single java file.
the benchmark also has an additional java files that contain common helper classes which are used by multiple test cases.
the benchmark s public repository also includes score cards that show the performance of different sast tools an excerpt of which is displayed in table .
for example the open source tool findsecbugs v1.
.
analyzes the benchmark in just over two minutes and obtains a score of .
the owasp score is based on the precision and recall of a tool s findings with for finding all and only the known vulnerabilities and for only false positives and false negatives.
findsecbugs performs a lightweight analysis based on type propagation and thus scales linearly with the size of the program.
for other tools in the benchmark s score cards we can see that scalability may be an issue.
the tools denoted as sast to sast in table have running times ranging from hours to days.
delays of such magnitude might be unacceptable for ci cd customers.
if we provide a static analysis platform that runs multiple tools as a portfolio customers would have to wait for the slowest tool to terminate before getting the final results there are usually postprocessing steps like de duplication before the unified results are returned .
this makes it harder to add new tools like sast to the portfolio.
hence we would like a mechanism to split the program under analysis into smaller partitions assuming that the analysis tool that we want to integrate terminates faster on most partitions so we can analyze these partitions in parallel and return results without increasing the latency of our analysis platform.
that is for owasp we would like to split the test cases into a set of partitions each of them bounded by some size sthat ensures our analysis terminates within an acceptable amount of time.
we would also like each partition to contain the subset of the shared classes that are used by any of the tests in that partition.
finally we would like to minimize the number of partitions since a very large number of very small partitions would amplify the impact of per partition overhead thus decreasing efficiency.
we illustrate the idea of splitting and the different splitting strategies using the listing in figure a simplified version of the test benchmarktest01025 .
the test contains a cwe22 path traversal vulnerability the value received from request.getheader is used in a relative pathname without input validation.
an attacker could provide an input like .. .. etc passwd to try to access sensitive data.
this test calls helper method dosomething in class test1 which is one of the classes that are used by multiple tests.
this method is called by a total of tests in the owasp benchmark such as benchmarktest01026 andbenchmarktest01029 .
suppose the available compute instances virtual machines allow a certain tool to analyze up to files before it risks exceeding the sla service level agreement time limit.
this means we must split the owasp benchmark into a set of partitions each of them of size at most s .
53878cc8751e348b63de951b91a6d47cf29121d8 1public class thing1 implements thinginterface override public string dosomething string i string r i return r simplified version of the owasp benchmarktest public class benchmarktest01025 extends httpservlet override public void dopost httpservletrequest req httpservletresponse response throws exception string p req .
getheader foo string bar new thing1 .
dosomething p file filetarget new file .
tmp bar response .
getwriter .
println ... figure simplified version of an owasp test that uses a shared class.
the method dosomething is referenced times in different owasp tests.
na ve splitting sizelimiting .first we discuss a na ve strategy called sizelimiting which splits the codebase into non overlapping subsets of up to sfiles each.
to ensure determinism the files are sorted in lexicographical order with respect to their names.
splitting is then performed on the sorted files.
for the owasp benchmark which has test classes and shared classes for a total of files this may produce for example partitions of size and one partition of size .
since method dosomething in class test1 is called by tests we know these tests will be distributed over at least partitions.
that is all but one of these partitions will not have access to the implementation of dosomething when running the static analysis.
depending on the analysis tool and its assumption on missing methods this may result in a loss of findings if the analysis underapproximates or it may lead to false positives if the analysis overapproximates or it may crash the tool.
for this example we need a splitting strategy that is able to create overlapping partitions to reduce the number of unavailable code dependencies in each partition.
in the following sections we outline such a strategy called splitmerge and then evaluate its effect on the number of findings compared to the na ve strategy and to not splitting at all.
we also evaluate the overhead of computing partitions and possibly reanalyzing code that is shared between partitions.
table score based on precision and recall and analysis time for several sast tools on the owasp benchmark v1.
.
data taken from the owasp benchmark public repository.
tool name owasp score total time fbwfindsecbugs v1.
.
.
sonarqube java plugin v3.
.
commercial sast .
commercial sast .
commercial sast .
commercial sast .
1369esec fse november singapore singapore christakis cottenier filieri luo mansur pike rosner sch f sengupta visser the splitmerge strategy we aim to distribute the analysis of a program p consisting of nfiles2f f1 ... f n by splitting the program into partitions r r1 ... r m withm n such that each partition ricontains no more than sfiles and can be analyzed independently with the target analysis tools.
we ensure that the union of all partitions contains all files iri f .
in general partitions are not required to be disjoint i.e.
the same file may be replicated across multiple ones.
algorithm overview.
splitmerge consists of three steps.
initially a partition is created for each file in the codebase which includes the file itself and its transitive dependencies up to a distancek.
the distance kis a parameter of splitmerge that allows to trade off the size vs the degree of self containment of the initial partitions.
for the example in figure for k the initial partitions are benchmarktest01025 thing1 thinginterface and thing1 thinginterface .
the second step split ensures none of the initial partitions exceeds the maximum size sby splitting any partition exceeding the size limit while its best effort to preserve the dependency relations it contains.
this step replicates the nodes with high degree of connectivity in all the split subsets with the intuition that units with high connectivity are likely to carry semantic information shared by multiple subproblems.
finally the third step merge takes as input a set of partitions of size less or equal than sand performs two tasks eliminate redundant partitions subsumed by others and merge small partitions into larger ones to balance the load and further increase selfcontainment.
a partition is redundant if it is entirely contained into another.
in our example the partition thing1 thinginterface can be dropped since the remaining partitions entirely cover its files and local dependencies.
merging small partitions to maximize the size of their union constrained by this size being smaller than s can be framed as a restricted instance of a bin packing problem .
the optimal solution to this problem converges to the smallest number of partitions with approximately uniform size sthat cover the input codebase and is expected to balance the analysis load by assigning one partition to each executor.
in the remainder of this section we will detail each step of splitmerge with the help of a simplified example.
running example.
consider an example program pcontaining six files a b c d e f .
the dependencies among these files are described in figure 2a where a directed edge x y fromxtoy denotes that xdepends ony symmetrically that yis a dependency ofx .
such dependencies can typically be computed statically in linear time with the size of p using tools such as jdeps for java or snakefood for python.
in the following we will refer to files and vertices and dependencies and edges interchangeably via the dependency graph.
step initial partitions.
this step produces an initial set of partitions of the program paiming at preserving local dependencies.
given a program pcomposed of a finite set of files f f0 f1 ... and a neighborhood radius k alg.
constructs for each file a 2in this paper we focus on files as the elementary units to partition for analysis which is a suitable setting for java and python.
our splitting strategy can in principle be applied to other language specific units.a b c d e f a initial dependency graph of program p. a b c d e f b dependency graph of paugmented with transitive relations up to radius k red dashed edges .
figure dependency graphs of p. partition including the file itself and its neighbors up to distance k. a large value for kmakes the algorithm more conservative in preserving dependency information.
however it also increases redundancy and the likelihood to produce partitions larger than the size limits.
in alg.
after computing the dependency graph the first loop augments the dependency relation to include edges linking a vertex to its neighbors up to distance k while the second loop builds one partition per vertex including it transitive dependencies up to distance k. for a sparse enough dependency graph with n vertices and k n which is a common situation in practical systems where coupling should be minimized the algorithm runs in nearly n the worst case complexity would be o n3 fork n and a fully connected graph by reduction to computing the graph transitive closure although it is unlikely for any realistic program to resemble this situation.
the function computedependencygraph returns the vertices and edges of the dependency graph.
each vertex of the graph corresponds to one file of the program under analysis.
example.
the dependency graph of our example program pis shown in figure 2a.
after the execution of the first loop in alg.
with k the dependency relation is augmented with the transitive dependencies shown in red in figure 2b a c and e b .
the resulting initial partitions are thus a b c b c c d b c e a b c d f f step split.
some initial partitions may have size larger than the maximum s. this is especially likely for larger values of the neighborhood radius k. this step aims at splitting an oversized partitionriinto smaller sets that fit within the size limit.
however uniformly splitting riinto the minimum number or necessary disjoint subsets is likely to delete relevant dependency information.
instead we deliberately produce a non minimal number of subsets allowing redundancy to preserve dependency information.
in particular for a partition rithat exceeds the maximum size ri s we sort the vertices in descending degree of connectivity number 1370input splitting for cloud based static application security testing platforms esec fse november singapore singapore algorithm initial partitions.
input filesf f0 f1 ... neighborhood radius k output initial partitions r r0 r1 ... v e computedependencygraph f augment dependency relation 3forv vdo neighbors verticeswithindistance v k forn neighbors do e e v n end for 8end for build initial partition 10r 11forv vdo r v for v u edo r r u end for r r r 17end for 18return r of incoming and outgoing edges and identify two sets of vertices high connectivity which includes the p a percentage of vertices with the largest degrees of connectivity and low connectivity ones which includes the rest of the vertices.
the underlying intuition is that files involved with many dependency chains are likely to be relevant for the analysis of most subsets of ri.
therefore alg.
first identifies these two sets and then partitions the low connectivity vertices uniformly into small enough subsets to allow adding to each such subset the high connectivity vertices.
this operation is formalized in the split function which is applied on each partition whose size exceeds s line handles the corner case of the chosen pnot small enough to ensure splitting all oversized partitions.
example.
considers .
the partition e a b c d f exceeds this size.
in figure 2b vertex ehas a degree of connectivity b andchave degree aanddhave degree fhas degree .
let p eandbare selected as the high connectivity vertices leading to new partitions e b a c e b d f as replacement of e a b c d f where vertices with the same degree have been sorted alphabetically .
step merge.
the last step of splitmerge reduces the redundancy introduced by the previous steps and computes the final partition alg.
.
some partitions computed by the first two steps may be subsumed by others.
for example b c a b c in the partitions for our program p. in these situations the information contained in the larger set subsumes the information in any of its subsets.
the subsets can therefore be discarded without loss of information first loop in alg.
.
the second part of this step aims at grouping together partitions for the sake of balancing the analysis load distribution across multiple executors.
this can be framed as an instance of the bin packing problem where a set of items the partitions have to fit within the minimum number of bins of size s. while finding the optimal solution is np hard many heuristics have been proposedalgorithm split.
input augmented dependency graph g gt et partitions r r0 r1 ... r n fraction of high degree nodes p maximum size s output split partitions r r r ... r m m n r r r s 1forr rdo if r sthen r r r split r p g s end if 5end for 6return r 7function split r p g s vt et extractsubgraph g r 9vts sortbydegreedesc vt the phighest degree nodes are replicated in each subset 11pr p r ifpr sthen 13pr p s end if hdn high connectivity ldn low connectivity nsubsets j r pr s prk divide ldnuniformly into nsubsets parts ldn0 ldn1 ... ldnnsubsets return hdn ldn0 hdn ldn1 ... hdn ldnnsubsets to efficiently compute near optimal solutions .
among these we adopted next fit which has a time complexity of o nlogn in the number of partitions n due to sorting .
although it may result in up to twice the optimal number of partitions its fast execution time is preferred for the sake of minimizing the maximum analysis latency.
different algorithms can replace nextfit to trade off latency for a smaller number of parallel executors.
example.
in our small size example the merge phase would result in the final partitioning already after the redundancy reduction phase since any further merging by nextfit would result in an oversized partition.
the final partitions are d b c e b a c e b d f .
after the three steps of splitmerge the resulting partitions satisfy the desired properties each partition is smaller than the prescribed size s i.e.
ri s the union of the partitions contains all files of the input program i.e.
iri f. in the next section we introduce our empirical evaluation on the impact of splitting strategies in comparison to non splitting strategies.
experimental evaluation in this section we report on our experiments using splitmerge with three analysis tools on a portfolio of java and python benchmarks.
1371esec fse november singapore singapore christakis cottenier filieri luo mansur pike rosner sch f sengupta visser algorithm merge.
input partitionsr r0 r1 ... r n maximum size s output merged partitions r r r ... r m m n 1forri rdo if rj rs.t.ri rjandi jthen r r ri end if 5end for 6return nextfit r s 8function nextfit r s 9ts sortbysizeasc r r r fort tsdo if r t sthen r r t else r r r r t end for r r r return r our evaluation will revolve around the following three research questions rq1.
what is the impact of splitting a program and analyzing the partitions in isolation on a tool s accuracy?
rq2.
how do static analyzers perform on the partitions compared to the original program in terms of time and memory usage?
rq3.
what kinds of static analysis tools would benefit from splitting strategies discussed in the paper?
.
experimental settings static analysis tools.
we used two industrial static analysis tools with interprocedural analysis capabilities rapid and infer .
rapid is a tool developed at aws that performs ifds ide based type state analysis to detect incorrect usage of cloud service apis.
infer is a static analysis tool developed at facebook that uses separation logic to detect memory related issues such as null pointer exceptions resource leaks and concurrency race conditions.
a third set of experiments will instead use bandit a static analysis tool to find common security issues in python.
unlike the other tools in our experiments bandit processes each source code file individually.
benchmark programs.
we use three different benchmark suites in our experiment the owasp benchmark v1.
owasp a well known javabased web application designed to evaluate the accuracy coverage and speed of automated software vulnerability detection tools.
it contains labeled test cases that demonstrate common web app vulnerabilities including e.g.
command injection weak cryptography path traversal the juliet test suite for java juliet created by the nsa s center for assured software cas specifically for testing static analysis tools.
it comprises test cases that contain vulnerabilities for different cwes open source packages from maven central maven starting from a set of open source java packages randomly sampled from maven we ran rapid on all packages and for each package recorded the number of seeds elements in the codebase that may lead to potential findings .
this can be done in linear time.
we filtered out packages with no seeds since their analysis with rapid is very inexpensive.
we also removed any packages that crashed the tool.
to make the splitting problem more challenging out of the remaining packages we selected those with at least classes.
that left us with maven packages.
baseline and experiments.
we evaluate the splitmerge splitting strategy in comparison with the na ve sizelimiting splitting strategy described in sect.
and also against two baseline configurations that do not perform any splitting no splitting unlimited time nosplit ut no splitting 16gb memory 24h timeout.
this strategy approximates the absence of latency constraints.
we use the findings reported with nosplit utas reference to assess accuracy drops due to splitting.
no splitting unlimited memory nosplit um no splitting 144gb memory minutes timeout.
this strategy imposes the same timeout we will use for splitting but allows the analyzers to use virtually unlimited memory no tool saturated the available memory in our experiments .
splitmerge andsizelimiting are allowed 16gb of memory and minutes timeout.
we run all the experiments on amazon ec2 c5.18xlarge instances vcpus 144gb ram .
we do not limit the number of cores a tool can use.
we use amazon linux as operating system and ulimit to enforce memory limits.
performance metrics.
to evaluate our research questions we collect the following metrics throughout the experimental campaign total findings the number of unique findings reported by each tool used as a proxy to detect accuracy losses.
when different splitting strategies are applied we compare the number of findings against the baselines to estimate the impact of splitting.
notably a tool may also report false positive findings in either the baseline or after splitting.
in general we do not have a reliable means to discriminate between true and false positives and for the sake of this work we pragmatically assume that ideally a splitting strategy should result in exactly the same set of findings as nosplit ut differences would suggest an impact on the accuracy of the tool best possible latency the longest analysis time for any of the partitions of the input program.
this is the minimum waiting time for the user excluding other network and service invocation latency total time cumulative analysis time for all partitions.
an index of the cumulative cost in computation time.
its value is related to the computational overhead induced by the redundancy allowed when splitting peak heap usage the maximum java heap memory used by an analysis tool written in java.
in our experiment this metric is only measured for rapid which is written in java 1372input splitting for cloud based static application security testing platforms esec fse november singapore singapore peak memory max resident set size rss the maximum amount of memory held by the process running an analysis tool at any time no.
of part.
the number of partitions produced by a strategy for a given benchmark sum of part.
sizes the sum of storage size for all partitions produced in an experiment.
configuration.
splitmerge is executed with maximum partition sizes neighborhood radius k and the percentage of high connectivity vertices p .
.
these configuration values control the trade off between latency total cpu time maximum memory and impact on precision.
for the experiments reported in this paper we prescribed a maximum allowed latency of minutes and a maximum of 16gb of memory per tool process and systematically swept the configuration space to make sure the selected configuration comfortably allows analyzing a partition within our prescribed latency and memory limits.
we acknowledge that different latency and resource constraints benchmarks and analysis tools may require different tuning of the parameters.
.
experimental results .
.
rq1.
what is the impact of splitting a program and analyzing the partitions in isolation on a tool s accuracy?
we answer this question by looking at the total findings detected by the analyzers shown in table reporting on the owasp juliet and maven benchmarks.
in the table xmeans the analyzer did not terminate within the given timeout or crashed thus no results were reported.
on all three benchmarks splitmerge allows both rapid andinfer to detect more findings in comparison to sizelimiting .
regarding accuracy we take the results of nosplit ut as the baseline for comparison except for rapid onjuliet where this strategy did not produce a result .
on owasp splitmerge allowed rapid to detect exactly the same number of findings as with nosplit ut without losing accuracy.
we also compared the output of the tool with both strategies the set of findings is exactly the same.
in contrast we lost findings with the na ve splitting strategy sizelimiting corresponding to of the total findings that can be detected by rapid without splitting.
forinfer splitting the original code using sizelimiting impacts its recall negatively as infer detected much fewer findings compared with non splitting strategies vs. .
in contrast splitmerge allowed infer to detect exactly the same findings as with nonsplitting strategies.
onjuliet rapid did not finish the analysis using non splitting strategies which gave us no baseline to assess the impact of splitting on its accuracy besides observing that splitmerge returned more findings than sizelimiting .
similarly to owasp splitting also resulted in loss of findings on juliet for infer .
it also turns out that infer crashed exited with non zero return code when analyzing the partitions.
as shown in table the crash rate is with sizelimiting and with splitmerge .
we conjecture infer is less tolerant to absences of dependent classes in comparison to rapid but further investigation is needed.
for the maven benchmark we conducted an experiment for each of the maven packages separately and aggregated the results which show partitions for the no split strategies correspondingto the packages analyzed .
from the results on maven we can see that splitmerge has less negative impact on both infer and rapid s findings compared to sizelimiting i.e.
rapid only lost .
of the findings using splitmerge while it is using sizelimiting .
on the other hand infer detected more findings with splitmerge than nosplit ut as it crashed less frequently .
we remark once again that the number of findings is a coarse proxy to evaluate the differential accuracy while assessing precision and recall of the different tools would require scoring each tool s output against a ground truth to establish which findings are true and false which is beyond the scope of this study.
rq1 takeaway splitting the original program can sometimes negatively impact a tool s accuracy.
however our experiments show that smarter splitting strategies like splitmerge can controllably reduce accuracy loss.
for very large codebases e.g.
juliet and strict resource constraints splitting may be the only option if we want to retrieve any findings since the tools do not scale and thus end up returning no findings at all.
.
.
rq2.
how do static analyzers perform on the partitions compared to the original program in terms of time and memory usage?
after evaluating the accuracy loss of splitting with this research question we evaluate analysis time and resource demand.
a benefit of splitting a program and analyzing each partition in isolation is the possibility of running an instance of the analysis tool on each partition in parallel thus reducing the user s waiting time.
the best possible latency of analyzing partitions is the maximum analysis time required to analyze any such partitions.
on all three benchmark suites both rapid andinfer achieved much better latency with splitting strategies on both sast benchmarks owasp and juliet and real world applications maven .
using splitmerge the analyzers achieved more than 2x speedup rapid .
.
infer .
.
on the maven packages in comparison to nosplit ut .
since rapid is written in java we also measured the peak heap usage from the jvm and observed that with splitmerge rapid used less than .
of the peak heap consumption of nosplit ut on owasp and on maven .
we also measured the maximum resident set memory size which indicates a significant reduction of peak memory consumption also for infer .
rq2 takeaway splitting the codebase allows us to analyze the partitions in parallel.
on all three benchmark suites our experiments show that splitting significantly reduced the latency for both analysis tools.
splitting also significantly reduced the memory required to analyze the benchmark suites compared to non splitting.
.
.
rq3.
what kinds of static analysis tools would benefit from splitting strategies discussed in the paper?
comparing the results ofrapid andinfer the first observation is that splitting allowed rapid to analyze juliet which was intractably large for the nonsplitting strategies.
we also observed that rapid is more tolerant than infer with partitions that miss dependencies.
on owasp and juliet splitting increased the number of crashes resulting in a reduction of the number of findings.
on maven we observed the opposite effect where analyzing the packages individually 1373esec fse november singapore singapore christakis cottenier filieri luo mansur pike rosner sch f sengupta visser table comparing both sizelimiting andsplitmerge to baselines on owasp juliet and maven .
the percentages in the rows forsizelimiting andsplitmerge strategies correspond to the reduction or gain in the number of findings total time and memory usage when compared with nosplit ut .
best possible latency column shows the speedup achieved with sizelimiting andsplitmerge strategies.
in the cases where nosplit ut failed to give a result within hours we report the speedup as x and the number of findings as n a. strategytotal best possible total time peak heap peak memory no.
of sum of findings latency min min usage mb max rss mb part.
part.
sizes rapid infer rapid infer rapid infer rapid rapid infer mb owasp nosplit ut .
.
.
.
.
.
.
nosplit um x x .
x .
x x .
.
sizelimiting2 .
.
.
.
.
.
55x 130x .
.
.
.
.
splitmerge3 .
.
.
.
.
.
.
39x 130x .
.
.
.
.
juliet nosplit ut x x .
x .
x x .
.
nosplit um x x x x x x x x x .
sizelimiting7 .
.
.
.
.
.
.
.
n a x 26x n a .
n a n a .
splitmerge8 .
.
.
.
.
.
.
n a x 30x n a .
n a n a .
maven nosplit ut .
.
.
.
.
nosplit um .
.
.
.
sizelimiting991 .
.
.
.
.5x .5x .
.
.
.
splitmerge1 .
.
.
.
.
.
.
.8x .5x .
.
.
.
figure loc of the open source python projects with dependency analysis and analysis time used by bandit.
led to more crashes than grouping all their sources and splitting them with sizelimiting orsplitmerge .
however for most benchmark applications the difference in number of findings with and without splitting was limited for both rapid andinfer while rapid was much faster in analyzing all subjects significantly reducing both the best possible latency by with splitmerge and the total computation time with splitmerge .
also for infer the best possible latency dropped by while the total computation time remained around the same order of magnitude taking into account that the different number of crashes between nosplit ut andsplitmerge make it difficult to discern how much computation time ultimately led to results rather than crashing.infer also showed a significant reduction in memory demand up to on juliet .
a worst case scenario for splitmerge .to better define the target application scope of a dependency aware splitting strategy likesplitmerge we report on an additional experiment using bandit a static analysis tool for python.
bandit processes each source file independently by building an abstract syntax tree and inspecting it for error patterns.
while the previous experiments analyzed java applications splitmerge is not restricted to a specific language provided that the user can specify the elementary code units to partition e.g.
files modules or classes and can identify their dependencies.
in the case of python dependencies between its classes can be extracted using snakefood and importlab .
1374input splitting for cloud based static application security testing platforms esec fse november singapore singapore figure time used by dependency analysis compared to time used by bandit.
table timeout crash and success rates of analysis runs.
strategyrapid infer rapid infer rapid infer timeout crash success owasp nosplit ut nosplit um sizelimiting splitmerge juliet nosplit ut nosplit um sizelimiting splitmerge maven nosplit ut nosplit um sizelimiting .
.
splitmerge we ran the three tools of the analysis pipeline snakefood importlab and bandit on popular python open source projects3 and recorded the analysis time.
figure shows the lines of code loc bars vs analysis time for each program line .
the analysis pipeline including dependency analysis and bandit shows as expected very high scalability taking only about minutes to analyze tensorflow the largest program in the set.
the breakdown of the time required for each step of this analysis pipeline is shown in figure from which it is evident that dependency analysis takes a significant proportion of the pipeline time up to for youtube while bandit only takes a smaller fraction of the pipeline time.
this experiment represents a worst case scenario for splitmerge the analysis does not benefit from preserving dependency information thus dropping the benefits of splitmerge s heuristics.
to parallelize bandit s analysis it would be more effective to usesizelimiting avoiding the overhead of dependency analysis contrary to the case of costlier interprocedural analyses with rapid andinfer does not pay off with bandit s per file analysis.
rq3 takeaway our experiments show that splitting is useful to limit the per machine resource consumption of static analysis tools and different tools require different splitting strategies.
inter procedural analyses benefit most from splitting even with a naive strategy like sizelimiting .
for example splitting the input codebase allowed rapid to analyze code much faster than without splitting.
whereas for infer the most prominent benefit of splitting was the reduction in memory usage.
our experiments also show that for inexpensive linter like or intra procedural analyses such as bandit sizelimiting may be more beneficial than dependency guided splitting strategy like splitmerge .
related work sast platforms.
several sast platforms in the literature provide static analysis as a service and present a simple interface to developers that allows them to run a variety of static analysis tools.
one of the earliest platforms is review bot which integrates findbugs pmd and checkstyle into the code review process.
review bot runs in the code review process on changes small enough to be reviewed by humans and the tools are fast linting tools so it did not have an immediate need for splitting.
the khasiana1 web portal integrates three static analyzers findbugs safe and xylem under a unified service interface.
it focuses on discussing the usefulness of the reported findings.
the tools are evaluated on hand picked projects in which scalability is not an issue.
two more recent platforms are software assurance marketplace swamp and google s tricorder and its open source version shipshape .
these platforms focus on making it easy to plug in additional analyzers.
they provide orchestration as well as aggregation and management of recommendations.
they do not 1375esec fse november singapore singapore christakis cottenier filieri luo mansur pike rosner sch f sengupta visser focus on how to deal with large inputs.
our splitting approach is agnostic to the specific sast platform and could be applied to khasiana1 swamp or tricorder shipshape as well.
an approach that shares a similar motivation to ours is implemented by cheetah which integrates several static analysis tools into an ide.
cheetah achieves fast response times by gradually increasing the scope of the analysis it starts by analyzing only the method currently being edited in the ide then increases the scope to class package and project level.
this allows them to deliver some results early on while gradually increasing the precision if the user is willing to wait for it.
in this paper rather than gradually increasing the scope until reaching a time limit we reduce the scope by splitting simplifying the input to get results within certain resource constraints.
a wide range of static program analysis problems can be viewed as instances of the context free language cfl reachability problem e.g.
inter procedural dataflow analysis control flow analysis set constraints specification inference shape analysis object flow analysis pointer and alias analysis and program slicing to name a few.
unfortunately the worst case time complexity for solving cfl reachability problems is o n3 which is known as the cubic bottleneck .
as a result highly precise analysis of large scale software is challenging.
in the literature most work only focuses on very specific optimizations to a particular analysis that are not applicable in general.
splitting input representation.
singh et al.
proposed a technique to speed analysis with polyhedra domain in abstract interpretation by partitioning variables into subsets such that the constraints only exist between variables in the same subset .
to mitigate the path explosion problem in symbolic execution trabish et al.
introduced chopped symbolic execution an approach in which users can identify unimportant parts in the code and the symbolic analysis tries to avoid those parts.
barnat et al.
propose a distributed algorithm for model checking ltl formulas.
the algorithm works by first partitioning and then exploring the state space in parallel.
in model checking based on symbolic state representation the technique in partitions bdds into smaller bdds each representing a subset of states which are subsequently given to different processes.
kumar et al.
present a technique for distributed explicit state model checking to improve run time performance.
parallel distributed static analysis.
the problem of scaling static analysis to potentially very large inputs is also discussed in where the authors present a distributed call graph construction algorithm designed to run in the cloud.
we share the motivation that static analysis needs to be elastic to scale to very large inputs but our approach is designed to be agnostic to specific static analysis tools and techniques.
mendez lojo et al.
proposed a technique to parallelize inclusion based points to analysis by formulating it in terms of constraint graph rewrite rules.
su et al.
introduced a parallel solution to cfl reachability based pointer analysis by avoiding redundant graph traversals using data sharing and query scheduling.
rodriguez et al.
presented an actor model based parallel algorithm for solving ifds data flow problems.
albarghouthi et al.
proposed a framework to parallelize top down inter procedural analysis using the mapreduce paradigm.
facebook uses infer based on bi abduction which is modular generates summaries for methods in the program and compositional composes summariesat call sites.
nevertheless as we show in our experiments even a modular analysis like infer does not scale to large or complex inputs under practical resource constraints.
bigspa provides a data parallel algorithm for cfl reachability based static analysis.
graspan and grapple are single machine disk based tools that model specific static analysis problems taint analysis and type state analysis respectively as transitive closure computation on graphs.
the above approaches parallelize or distribute the analysis workload in a way that is specific to the tool or technique at hand.
most of them modify the existing tool.
in our case we do not introduce any changes to the sast tools an important design goal is to be able to add new off the shelf tools and update existing tools as new versions are released without having to maintain our own modified versions of them.
automated refactoring.
the goal of splitting is to break a large program into smaller units that are sufficiently self contained to be analyzed in isolation.
this is similar to the refactoring problem of breaking up a monolithic system into smaller components.
an overview of such refactoring techniques is given in .
recently we see approaches based on machine learning e.g.
dynamic analysis e.g.
and static analysis e.g.
.
the splitting problem discussed in this paper is simpler than the problem of automatically refactoring in the sense that our partitions do not need to be functioning programs they just need to to be sufficiently self contained for analysis purposes.
graph partitioning and communities.
there is a rich body of work on graph clustering for community detection in networks studied as graphs e.g.
social networks academic citation networks and collaboration networks which might provide a theoretical grounding for future research.
however to the best of our knowledge this line of work currently focuses on preserving maximal connectivity as opposed to our goal of attaining an effective tradeoff between preserving connectivity and load balancing partitions.
conclusion we discussed how splitting of static analysis inputs can be used to effectively limit the maximum resource consumption of an analysis tool.
we motivated that this is an important problem when operating a sast platform as adding new analysis tools to the platform must not increase the maximum latency for existing customers.
our evaluation shows that the splitting strategy has a significant impact on the outcome of the static analysis tool and that not all strategies are suitable for all tools.
we showed that for more complex super linear static analysis tools more advanced splitting strategies are needed to minimize the effect on the reported number of findings.
for inexpensive linter style tools like bandit we see that the overhead of a complex splitting strategy outweighs the benefits and that a simple splitting strategy is sufficient.
we believe that in the future the process of picking a splitting strategy and tuning the parameters when adding a tool to a sast platform can be fully automated.
a tool can be run with different configurations on regression data to identify the best combination of strategy and configuration.
also the configuration parameters for each tool can be re adjusted on the fly as the available hardware improves or the static analysis tools get updated.
1376input splitting for cloud based static application security testing platforms esec fse november singapore singapore