all you need is logs improving code completion by learning from anonymous ide usage logs vitaliy bibaev jetbrains belgrade serbia vitaliy.bibaev jetbrains.comalexey kalina jetbrains munich germany alexey.kalina jetbrains.comvadim lomshakov jetbrains saint petersburg russia vadim.lomshakov gmail.com yaroslav golubev jetbrains research belgrade serbia yaroslav.golubev jetbrains.comalexander bezzubov jetbrains amsterdam the netherlands alexander.bezzubov jetbrains.comnikita povarov jetbrains amsterdam the netherlands nikita.povarov jetbrains.com timofey bryksin jetbrains research limassol cyprus timofey.bryksin jetbrains.com abstract in this work we propose an approach for collecting completion usage logs from the users in an ide and using them to train a machine learning based model for ranking completion candidates.
we developed a set of features that describe completion candidates and their context and deployed their anonymized collection in the early access program of intellij based ides.
we used the logs to collect a dataset of code completions from users and employed it to train a ranking catboost model.
then we evaluated it in two settings on a held out set of the collected completions and in a separate a b test on two different groups of users in the ide.
our evaluation shows that using a simple ranking model trained on the past user behavior logs significantly improved code completion experience.
compared to the default heuristics based ranking our model demonstrated a decrease in the number of typing actions necessary to perform the completion in the ide from .
to .
.
the approach adheres to privacy requirements and legal constraints since it does not require collecting personal information performing all the necessary anonymization on the client s side.
importantly it can be improved continuously implementing new features collecting new data and evaluating new models this way we have been using it in production since the end of .
ccs concepts software and its engineering software development techniques automatic programming.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
conference july washington dc usa association for computing machinery.
acm isbn x xxxx xxxx x yy mm.
.
.
.
anonymous usage logs code completion integrated development environment machine learning a b testing acm reference format vitaliy bibaev alexey kalina vadim lomshakov yaroslav golubev alexander bezzubov nikita povarov and timofey bryksin.
.
all you need is logs improving code completion by learning from anonymous ide usage logs.
in proceedings of acm conference conference .
acm new york ny usa pages.
introduction integrated development environments ides are among the most important tools used in writing software code .
they provide developers with various smart features that increase their productivity highlighting of code syntactic checks various automatic quick fixes automatic refactorings and others.
to evaluate how various features are being used it is possible to collect logsfrom ides.
this can be used for high level analysis of user workflows or for evaluating the usefulness of a particular feature for example automatic refactorings .
however one needs to be careful when dealing with usage logs similar to any user data.
researchers often use the data from a limited group of volunteers with explicit consent while processing the data of thousands of users in the wild requires stronger protection of their privacy in accordance with various regulations such as the general data protection regulation gdpr .
one of the key features that defines an ide is code completion .
code completion speeds up the programming process by automatically suggesting code that the developer is about to write while also helping them avoid possible typos.
an example of code completion in action is presented in figure .
ides based on the intellij platform intellij idea pycharm webstorm etc.
developed by jetbrains naturally also have a code completion feature.
the default implementation uses a static analyzer to generate the candidates by parsing syntactic and semantic models of the project and collecting the information aboutarxiv .10692v2 sep 2022conference july washington dc usa v. bibaev a. kalina v. lomshakov y. golubev a. bezzubov n. povarov t. bryksin figure an example of code completion in the ide.
the used entities as well as the given position of the caret.
then a heuristics based ranking is used to sort the suggestions before presenting them to the user.
despite the good quality and fast inference this leads to three major problems the heuristics are based on certain assumptions that can be hard to verify quantitatively and may be statistically wrong over time maintaining a large number of heuristics becomes cumbersome and it gets harder to add new ones without breaking the existing ones supporting a new language for completion is difficult even when a lot of heuristics might be reused.
to overcome these problems and further improve code completion an existing heuristics based system can be augmented by adding a machine learning ml based ranking as the final step .
training an ml ranking model that will rearrange suggestions from the static analyzer requires ground truth data i.e.
a labeled list of code completion suggestions.
existing approaches can be divided into two principal categories from the standpoint of the data that they use for training synthetic that analyze the existing body of source code and real user behaviour that employ logs source code and edits from real users .
in a recent paper hellendoorn et al.
showed that synthetic data significantly differs from the real world usage so training on it may cause worse performance for the end users.
for this reason utilizing the usage logs seems like a promising solution.
in this work we propose and evaluate an approach that allows to use the information from the users without violating their privacy or collecting any personal information.
we propose to decouple the extraction of data by designing a set of features that are computed on the side of the client and are then collected anonymously from logs.
we then use this data to train a model for ranking the candidates for code completion and compare its performance with the default heuristics based approach.
specifically we defined a number of different features that describe the prefix before the caret the context around the suggestions the entities defined and used before the caret the history of selecting any given suggestion by the user etc.
then we integrated the collection of these features into the early access program eap of intellij based ides and collected anonymized logs from the users who agreed to send them to us.
the logs include the values of the defined features for each suggestion as well as the label positive for the suggestion that was actually selected by theuser and negative for all the rest.
this way no sensitive information is collected e.g.
the user s code but the ranking model can still be trained.
we used a catboost model because it is fast lightweight and can be easily converted for the jvm.
we carried out two different evaluations of the described approach using the data from python based projects in pycharm an offline evaluation and an online evaluation.
for the offline evaluation we collected two datasets of logs from two different groups of users the first week contained completion sessions and was used for training the model the second one contained sessions and was held out for testing.
in this setting the proposed ml based ranking demonstrated the recall at top i.e.
cases when the correct suggestion is at the very top of the list of .
against .
for default heuristics based ranking.
however while such an evaluation is much better than a synthetic one even it does not tell the full story.
to see the results in action we also performed an online evaluation that constituted an a b test some users were given the base code completion using heuristics and some were given suggestions ranked using our model.
such a setting allowed us to use more practical intuitive metrics of the completion quality.
among other things this evaluation showed that the average number of typing actions required to finish the completion lowered from .
for the heuristics based ranking to .
for the ml based one indicating the real saving of time for the users.
the latency increased slightly from .
ms to .
ms thus remaining in the comfortable range.
one important aspect of using the eap versions of an ide is that this allows us to repeat the described process cyclically in the new releases.
this way new data can be obtained from the users in a constant stream and importantly new models can be evaluated and compared in live a b tests thus ensuring constant evolution of the quality.
this process takes place in eap versions of intellij based ides since the end of .
overall the paper presents the following contributions approach for enhancing the quality of code completion in real world scenarios that formulates code completion as a ranking problem consists of a feature based catboost model trained on real anonymized user behavior data collected without violating their privacy is language agnostic while some specific features and specific trained models can be used only for a specific language the approach as a whole as well as a lot of general features does not depend on the language meets the requirements of the real world industrial application always produces syntactically correct code results in a relatively small model less than kb with low inference time ms can be continuously employed in cycles of gathering data and comparing models in live a b tests.
evaluation of the proposed approach in two different settings that employ real user data from pycharm offline evaluation on the held out user data which demonstrates that ml based ranking shows the recall at top of .
over the heuristics based ranking that has the recall at top of .
all you need is logs improving code completion by learning from anonymous ide usage logs conference july washington dc usa online evaluation via an a b test in the ide which shows that the average number of typing actions necessary to perform completion lowers from .
for the heuristicsbased ranking to .
for the ml based one.
insights into the results of using the proposed approach in a real world industrial ide and dealing with various constraints that such a setting implies.
models obtained using the described approach are currently being employed in almost all major intellij based ides.
the rest of the paper is organized as follows.
in section we briefly discuss the problem of code completion and the constraints it faces in the real world applications.
section describes in detail the proposed approach to collecting the data and building the model while section describes two different evaluations that we conducted.
in section we share insights about the usage of the proposed approach and discuss open challenges in the field.
section describes the existing related works section discusses the threats to the validly of our work and finally section concludes the paper and discusses possible directions for future work.
background the idea of code completion as an ide feature is to suggest code to the user before they can manually type it thus saving them the time and effort.
additionally code completion can help in project exploration providing new users with an opportunity to see entities from different parts of the code base.
since there is a lot of variance in the process of writing code the ide usually provides a ranked list of possible completions for the user to choose from.
code completion systems vary by the granularity of their suggestions from tokens and api usages to lines or even entire methods.
while early research focused mostly on narrow api level completion modern language models based on neural networks vary from fine grained using every possible lexical token type delimiters operators white spaces keywords etc.
to coarsegrained predicting entire lines of code .
in this work we specifically focus on token level code completion as the most balanced and important option it is more practical than heavy full line code completion and more general than just api recommendation.
in practice over time effective token level code completion can save the users a lot of effort.
however our approach is easy to extend to other types of completion and we leave applying the usage of logs for the full line version of code completion for subsequent work.
the set of possible candidate suggestions in code completion is determined by the implementation of the candidate provider .
code completion systems in ides rely on static analysis to retrieve the list of possible candidate suggestions since it is vital for suggestions to be syntactically correct.
the list of candidates is generated by considering the position of the caret the grammar of the programming language code entities that are defined and used in the opened file other files in the project and the used libraries.
before showing the list of candidates to a user the ides rank them typically employing a series of hard coded heuristics.
these heuristics range from trivial and language agnostic e.g.
which entities were used the most in the opened file to specific and languagedependent ones e.g.
type matching.thus there are at least two different ways to improve tokenbased code completion change the set of possible candidate token suggestions or change the order in which these suggestions are presented to the user.
in this work we focus on the second one building on a number of works that enhance the quality of ranking by employing statistical and machine learning approaches .
it is vital to focus on improving the ranking of the suggestions since in general modern static analyzers already generate candidates very well and their recall is rather good so it becomes a matter of prioritizing syntactically correct suggestions.
the general idea of using machine learning for this task consists in choosing the necessary features that describe the context around the completion instance and the candidates and collecting data for training the ranking model.
since the model is trained to be used in an industrial setting inside the user s ide this imposes a set of important constraints on the entire process.
firstly because of the slow and energy consuming nature of the model training it must happen on a remote server and the user should simply receive a pre trained model as a part of their ide.
secondly the inference of the model has to take place on the user s machine be fast and reliable.
this includes not using any special hardware i.e.
gpu and not accessing the network.
the last constraint is important for several reasons inferencing the model over network introduces additional latency to the process.
developers might work without access to the internet.
developers may not be comfortable with sending any information to a remote server.
the concern about the privacy of the user s data is crucial in this area and also directly relates to the data collection.
as was mentioned above training the necessary model requires having the labeled data of code completion sessions.
some of the existing works usesynthetic data and analyze the existing body of source code .
however hellendoorn et al.
demonstrated that in the case of code completion synthetic data may differ significantly from the real world data so such data collection pipelines are inherently flawed.
for this reason other works analyze real user behaviour they gather the data from logs source code and edits made by real users .
in such a setting privacy requirements are also critical and must be taken into account.
in short the privacy requirements regulate the gathering of information that might identify the user.
while this obviously includes any personal information like the name or the e mail in certain legislatures this can also include the code itself.
in particular in recent years a lot of laws were passed such as eu s general data protection regulation gdpr or california consumer privacy act ccpa that carefully regulate users privacy and data collection.
user data should be collected only in anonymized depersonalized form without their code or any code metric that is too revealing of their identity.
this is crucial both in the sense that it incentivizes inferencing the model on the user s machine without sending any data and also impacts the possible ways of collecting user data for training.
in this work we propose describe and evaluate improving the performance of code completion by collecting anonymized logs of real completion sessions and training an ml model based on them.conference july washington dc usa v. bibaev a. kalina v. lomshakov y. golubev a. bezzubov n. povarov t. bryksin calculate a set of anonymized features user s machine remote servertrain an ml model for ranking c andidates use the pre trained ranking model evaluate the model s performancecollect logs collect logsdeploy model figure the pipeline of the proposed approach.
approach employing user logs to improve code completion requires finding a proper way to collect and utilize these logs.
taking into account the mentioned limitations we propose the following pipeline for collecting the data and training the model shown in figure a set of features is calculated on the user s machine during their completion sessions.
this data is anonymized and collected as logs without any identifying personal information.
a model is trained and evaluated on the server using a large amount of such anonymized data.
this pre trained model is deployed to future users from whom we can collect new logs to evaluate the model.
in such a setting all the requirements are met no sensitive personal data is collected neither the suggestions nor the surrounding code the resource expensive training of the model happens on the server and the usage of the model requires nothing from the user.
what is even more important however is that the described sequence represents just one iteration of the process in parallel to evaluating a model new data can be collected from other users new models can be trained etc.
this opens up the possibility to compare the models in a real industrial setting.
the implementation of the described pipeline requires three key things designing a set of features to be collected and a data format for storing and utilizing them.
selecting a machine learning model to train on this data that could be conveniently inferenced in the jvm based ide.
creating a setting for continuously collecting data and testing models in the form of a b tests using the early access version of intellij based ides.
let us now describe each of these three items in greater detail.
.
data collection as was previously stated we cannot train the necessary model on the user s machine or send personal information about them including the code because of privacy concerns.
because of this the data collection consists in designing a set of anonymous features that would describe the completion session and sending this data to the server and transforming it for training the model.it should be noted right away that while a lot of modern works argue in favor of end to end neural models instead of manual feature extraction in our setting feature extraction represents a benefit it provides a clear organizational way to control and manually audit what exactly gets collected and sent via code reviews.
the collected data consists of completion sessions i.e.
cases when the pop up window with suggestions appeared.
each completion session in its turn contains one or several look ups i.e.
specific lists of suggestions.
one session can have several look ups if the user typed additional characters when the pop up already appeared thus changing the context and filtering the list.
finally each lookup consists of specific suggestions i.e.
individual tokens that are ranked for the user to choose from.
.
.
feature selection.
the aim of the features is to describe the completion context of any given suggestion as best as possible without actually giving away any of the written code.
in this case thecontext relates not only to the code right before the caret but anything that can influence the choice the information about the suggestion itself the history of choosing etc.
in our experiments we evaluated many different features and their importance for the quality of the model.
it is very important to note that the proposed approach is inherently language agnostic so some basic features are also the same for different languages while some features remain language specific.
there are several main groups of language agnostic features information about the prefix.
a major source of information about the completion is the prefix i.e.
the already typed part of the token right before the caret.
the information includes the length of the prefix the number of matched characters between the prefix and the suggestion whether the match is case sensitive whether it is exact etc.
syntactic context.
the completion session does not happen in a vacuum it takes place in a certain code location in a certain part of the codebase.
all of this can influence the necessary token.
thus features that describe the syntax include whether the suggestion is a language keyword whether the suggestion is the element from the same file or module whether it is from a third party library etc.
syntactic history.
additionally and crucially it is very important to catch the dynamic nature of software developmentall you need is logs improving code completion by learning from anonymous ide usage logs conference july washington dc usa and not treat the opened file as a static object.
previous actions of the user may indicate their intent better than the basic proximity of objects in the code.
as such we analyze the prior navigation of the user to see whether they navigated to the definition of the suggestion before etc.
session history.
also some features describe the temporal aspect of completion sessions the duration of a given session in seconds whether the user has already selected the given suggestion before etc.
language specific features include the context of the suggestion ifblock forblock etc.
the type of the suggestion method class the usage frequency or something even more specific like the number of lines between the caret and the init method of the enclosing class in python.
initially there were several hundred different features for each language.
feature selection consisted of two stages.
on the first stage we filtered out all the features that were not used by the model during training.
on the second stage we calculated permutation importance for the features and filtered out those that were not useful.
all the remaining features were used in the final production model.
we evaluated our approach on different languages for example for the java language the total final number of the used features was for the python language it was .
.
.
data format.
once the features are chosen they are deployed for the collection of logs into the early access version of the necessary ide.
the collection occurs for each action in each completion session.
the possible actions are the completion session started i.e.
the pop up appeared the user typed an additional symbol thus adding another loop up to the session changing the prefix the context and filtering the list of suggestions the user went up or down the list of suggestions using arrow keys or the completion session ended in one way or another .
the collected data contains the following information user id random and anonymous.
timestamp of the event.
the way the completion session started manually by pressing the specific hot key combination or automatically after the user started typing.
project file or snippet level features that are the same for all suggestions in the look up.
the list of individual suggestions and their features.
the way the session ended and the selected option if any.
there are four possible ways for the completion session to end explicit select if the user selected a suggestion and it was successfully auto completed.
typed select if the user completely manually typed a suggestion that was in the list.
explicit cancel if the user explicitly canceled the completion e.g.
by pressing esc or reverting .
typed cancel if the user completely manually typed a token that was not in the list thus also ending the session.
a simplified example of the collected logs is presented in figure .
in this example the full logs figure 3a show that the completion session started then two characters were typed after which the user pressed the down arrow once and selected the token thussuccessfully finishing the completion.
for each action the full list of features is recorded.
figure 3b and figure 3c show the suggestions that were present in this example before selection and several basic features calculated for them.
for training the model we use data points that ended with explicit select ortyped select i.e.
where there is ground truth.
after the user selects a certain suggestion it is labeled as the correct option in all the look ups in this completion session.
for example if the user selected the token after typing two characters the suggestion was already in the list after the first character perhaps lower and can still be considered the correct option in that list.
figure 3d shows that the selected suggestion setout was labeled with as the target value while the other suggestion was labeled with .
the remaining data can be used to evaluate the quality of the used completion.
the timestamps can be used to calculate the time that the user spends evaluating their options the typing actions can be used to see how many are required on average to finalize the completion and we can also measure how many sessions ended in each of the four possible ways.
these different metrics will be described in greater detail in section .
.
model similar to the other works motivated by industrial application we focus on improving the ranking of the candidates and formulate this task as a learning to rank problem .
researchers explored a similar architectural approach referred to as structural feature selection and feature based models only for the limited context of api recommendations over the closed vocabulary of possible api calls.
in section aside from the limitations that relate to privacy we also mentioned performance related limitations.
the model must not rely on any specific hardware and must be lightweight to operate on the user s machine.
more specifically the size of the model should not be more than mb and the inference must take tens of milliseconds in order not to make the completion generation too noticeable for the user.
such strict limitations come from the fact that during the completion session the model will be inferenced after each new character.
last but not least the model must be easy to integrate into the jvm based architecture of the ide.
to meet these requirements in particular the last one we decided to use a model based on the decision tree algorithm.
this architecture is a good choice for us because it is easy to convert into the if else code representation and thus it can be used inside the jvm process.
specifically we used a catboost model with a built in querysoftmax loss function .
we experimented with a number of alternative models including lightgbm as well as feed forward and transformer based neural models.
none of these options came close to the performance and the operational simplicity of the tree based model.
given the severe cap on the size of the model it is difficult to train a wellperforming transformer and no other architectures demonstrated results better than catboost.
in the end the catboost model was the only one actually deployed to production and thus will be the focus of the rest of the paper.
as discussed in section .
.
and shown in figure 3d an individual data point consists of a list of several code completion suggestions the accepted one being a positive example and theconference july washington dc usa v. bibaev a. kalina v. lomshakov y. golubev a. bezzubov n. povarov t. bryksin event query length static context true items type field lentgh isstatic true ... type method length parameters ... positionquery lengthtype lengthparametersis static?
... selected ... field ... method ... ... start query length static context items true typing query length static context items true typing query length static context items true down selected position query length static context items true explicit select selected id query length static context items truea c b d figure data collection and data format.
in this example the user typed two characters of the token and selected the second suggestion.
a the simplified general form of the collected logs during actions b the view inside the ide right before selection c the json structure of the collected data including several simple features d the final representation of the data with the target column that the model trains on.
rejected ones being negative.
it is also important to understand that the users mostly care about what is on the very top of the list.
for this reason we needed to choose a loss function that would prioritize the correct suggestion being at the top of the list.
catboost comes with such a loss function querysoftmax described previously in the work of cao et al.
.
in this setting given a list of suggestions the model learns to boost the correct one to the top.
the model was trained using the native catboost framework then converted into the lightweight representation of a series of if else operators.
the final model was implemented and inferenced in the java language.
.
continuous collection and validation one of the most important features of the proposed approach is that the gathering of new data and the evaluation of the models can occur in parallel since both processes rely on collecting logs.
at the same time the cyclical nature of ide releases means that this process can be repeated over and over always collecting new data with new features training fresh models and evaluating them on real users against the current best.
this process relies on the early access program eap provided for intellij based ides.
it works as follows.
in the two month period before each release of an intellij based ide the eap takes place.
if a user agrees to participate in the eap they are given free access to a beta version of the upcoming commercial release in which they can evaluate and try out new features.
in exchange they agree to provide anonymous logs of their experience that the developers of ides can use to fix any existing bugs.
in this setting it is not only possible to collect data about code completion or evaluate a single model but also carry out a fully fledged a b test in the process.
the idea of this cycle is demonstrated in figure .
let us now describe it further.
let us say that in the eap of the version i a certain model ml i was working and some data was collected.
then we take this data and train a new model that we think will do better let us call it ml i .
at the same time perhaps we already developed some new features that we want to evaluate.
so when the eap version i comes around we implement the collection of the new features and then divide the users of the eap into three groups themligroup receives the ml imodel that was obtained in the previous version the current best.
the mli 1group receives the ml i 1model the new one.
finally the separate logs group also receives the previous ml imodel and is used to collect data for the next cycle.
from all three groups the logs are collected.
this way an a b test is being conducted between the two ml based models and with the help of the metrics calculated from logs and statistical tests we can discover the best of them.
meanwhile the logs collected from the third group can be used to create a training dataset on which we can train a new model ml i .
then during the eap versioni we can repeat the process comparing models ml i andml i collecting new data with the new features and so on.all you need is logs improving code completion by learning from anonymous ide usage logs conference july washington dc usa eap i mli ab test mli logsi dataseti 2mli ab test mli logsi dataseti ... ...eap i time... ... figure the continuous way of collecting the data and comparing the models in the early access versions of the ides.
in every eap one group of users gets the current best model and another gets the new tested model between which an a b test is run.
at the same time from a separate groups of users new logs are being collected that are then used to build a new dataset and train the next iteration of the model.
it is also possible to compare more than two models at once and fit several iterations into a single eap cycle.
in this sense the eap is very convenient because it is experimental by definition and so the users are expecting to receive frequent updates that change the behavior of the ide something that would be unacceptable in a major release.
the described process has been running in the eaps of major intellij based ides since the end of .
this allowed us to evaluate different models and a lot of different features.
evaluation to highlight the usefulness of the proposed approach for using logs as well as the ability of the catboost based model to improve the quality of code completion in this section we describe two different evaluations that we conducted to compare it with the baseline heuristic based ranking.
.
data the data for the experiments was collected in september and october of from python based projects in pycharm.
overall we collected two sets of data in two consecutive weeks.
the first set contained completion sessions from unique users.
this set was split into two in the ratio of of individual users one was used for training the model and the other was used for tuning hyper parameters.
the other set collected during the second week contained completion sessions from unique users different from the ones in the first week.
this set was held out for testing.
this way the testing data was located later in time from the training data and came from different users.
the model utilized python based features described in section .
.
.
.
offline evaluation .
.
methodology.
the first evaluation was carried out offline meaning that after the data from section .
was already collected the model was trained on the data from the first week and then tested on the held out data from the second week.
on the same held out data the heuristics based ranking was also evaluated.table the results of the offline evaluation.
r kstands for recall at k initstands for initial look ups.
cc system r 1all r 5all r 1init r 5init heuristics .
.
.
.
catboost .
.
.
.
to evaluate the performance of the models we used the recall at k metric r k .
this metric represents how often the correct answer was in the top ksuggestions of the look up.
naturally r is especially important since it tracks how often the correct answer was at the very top convenient for the user.
additionally besides calculating the metric for all look ups in all completion sessions we also separately calculate them only for the initial look ups meaning the first look ups in each session before typing any additional characters.
this metric correlates well with the user experience since it is very important to suggest the correct item from the very start if the user has to type additional characters they are unlikely to pause and analyze the list after every keystroke at this point they are more likely to simply type the token themselves.
.
.
results.
table summarizes the comparison between the default ranking and our model.
it can be seen that our model provides better results for all the evaluated metrics.
the correct token is more often shown in the first position and is higher located overall with both r 5metrics reaching very high values.
the largest increase can be seen for the most important metric r 1init or the ratio of correct suggestions at the very top of the list during the initial look ups.
the catboost model increased this metric by .
percentage points from .
to .
indicating a much better performance for the user in the most crucial moments.
it is interesting to note that different types of tokens demonstrated different increases in recall.
for example functions from the global scope e.g.
abs demonstrated a significant increase in quality.
there are a lot of such functions in python that are difficult to complete using heuristics due to the lack of type checking conference july washington dc usa v. bibaev a. kalina v. lomshakov y. golubev a. bezzubov n. povarov t. bryksin whereas an ml model learned how they are used.
an opposite example where the increase in recall was small is api calls.
the reasons for this are that firstly the default completion already works well in these cases and secondly it is often difficult to understand what api method to use from the local context.
a promising direction for such cases is to learn from other api usages .
.
online evaluation a b test .
.
methodology.
the offline evaluation demonstrated good results but it is very important to understand its limitations.
while these results represent realistic usage scenarios the offline metrics themselves are only proxy metrics for real usability improvements having the correct result higher in the list indicates more efficient work but does not directly show it.
to explicitly measure more interpretable human oriented metrics we conducted an online evaluation in the form of an a b test.
the implementation of the a b test was the same as described in section .
only with the evaluated models being our catboost model and the default heuristics based ranking.
in total users were allocated to the heuristics based control group and users to the catboost group.
from the first group completion sessions were collected from the second sessions.
the setting of an a b test allows us to use all the collected data described in section .
.
to further measure the performance using several dozen specific metrics that describe every aspect of the completion sessions.
we will report several key ones explicit select .
firstly we can simply measure the fraction of the sessions that ended in the explicit selection of the token the best possible result.
typed select .
in contrast we can measure the fraction of sessions that ended with typed select meaning that the user typed the token themselves even though it was in the list.
typing actions .
to measure the actual increase in the users productivity we can calculate the average number of typing actions typed characters in the completion session.
for this metric we used a cut off at .
percentile to remove anomalous completion sessions with an extremely large number of typed characters.
prefix length .
to specifically gauge the effectiveness of explicit select we can also compare the average length of prefix at the moment of explicit selection.
this will also demonstrate when it is necessary to type fewer characters before explicitly finishing the session.
manual start .
finally to measure the overall reliance of users on code completion we can measure the fraction of sessions that were started manually i.e.
using the hot key combination.
to test the statistical significance of the difference between models we employed bootstrap .
when comparing the models we form bootstrapping re samplings of completion sessions grouped by users.
for all the metrics we report the obtained p value and consider the result significant if p .
.
.
.
results.
table summarizes the results of the online evaluation comparing the models.
once again it can be seen that all the metrics indicate the better performance of the catboost based model trained on users logs.
the fraction of the sessions that endedtable the results of the online evaluation.
the bold font indicates better results whether lower or higher .
the star indicates a statistically significant result p .
.
metric heuristics catboost p value explicit select sessions .
.
.
typed select sessions .
.
.
typing actions symbols .
.
.
prefix length symbols .
.
.
manual start sessions .
.
.
with explicit selection increased however this result did not pass the statistical significance test.
at the same time the fraction of sessions that ended with the manual typing of the entire token decreased significantly.
next we can see that the number of the necessary characters to type also decreased.
the overall average number of typing actions decreased from .
to .
and the average length of prefix at the moment of explicit selection decreased from .
to .
.
these two metrics indicate that the model actually makes it easier for the users to input a token.
finally the fraction of sessions that started manually increased significantly from .
to .
which might indicate the users interest in obtaining the results of code completion.
.
overall performance lastly it is necessary to discuss whether the obtained model complies with the limitations described in section .
since this also directly impacts the comfort of the end users.
the size of the trained model was kb which comfortably fits into the desired range.
as for the latency the a b test showed that when moving from heuristics to catboost it increased from .
ms to .
ms p .
.
this corresponds to the model adding less than ms for inference thus also remaining in the comfortable range virtually unnoticeable for the user.
overall it can be seen that the decision tree based model trained on real usage logs demonstrated its superiority over the default heuristics based ranking in all tested settings while remaining small enough and fast enough to be used in production.
even more importantly the proposed approach allows us to collect more data design more features and evaluate more models continuously making sure that they always remain relevant.
discussion open challenges .
code completion .
.
model.
we show that leveraging real world structured ide usage logs is beneficial for both training ml models similar to aye et al.
and evaluating their performance similar to proksch et al.
.
however we only use anonymous pre extracted features instead of the full record of development history or edit context.
svyatkovskiy et al.
argue in favor of using end to end neural networks and point out that feature based models... ...depend on hard coded features thus missing the opportunity to learn richer features directly from the data.
although this is true in our case it becomes an advantage we use this as a safeguard mechanism to control what data gets collected and thus prevent gathering sensitive information.all you need is logs improving code completion by learning from anonymous ide usage logs conference july washington dc usa ...introduce difficulties in manually designing and extracting relevant features that cover as many cases as possible.
this is also true but every major language has a dedicated team to support it and also organization wide tooling and infrastructure allow us to automate the experiments and lead to continuous improvements of the overall system.
...learn about individual apis and cannot generalize to unseen ones.
our approach does not have this problem as for providing the list of suggestions it relies on project wide static analysis and context specific feature extraction and not on a global vocabulary.
as far as a specific model goes we decided on using decision tree based models specifically catboost.
this choice is dictated mainly by the strict limitations that our task imposes.
such models are simple interpretable and have great runtime performance small size and low latency.
additionally they turned out to be simple to work with they provide fast training great productiongrade tooling and importantly they are easily converted into an intermediate representation that allows to re use them between different ecosystems e.g.
python and jvm .
at the same time it must be noted that the practical nature of our task does not allow us to claim state of the art results or even compare directly with many models from the literature.
our goal in this continuing research is precisely to find models that would improve the user experience while being lightweight and easy to use in an actual ide on a consumer grade device.
.
.
data.
an interesting open question when using logs of completion sessions is what exactly to consider positive examples and what to consider negative examples.
a default idea that we used consists of taking all the sessions that ended with a certain positive outcome i.e.
explicit select ortyped select using their selections as positive examples and anything that was not selected as negative examples.
however this leaves out all the fully negative cases i.e.
explicit cancel andtyped cancel that can still provide negative examples.
also it can be noticed that we use typed select as a source of positive examples however in our a b tests see section .
we try to lower the ratio of users who type the token themselves.
this can also be considered when selecting positive and negative examples.
even more granularly we can say that since our goal is to make the users type less hence using typing actions as a metric not all positive examples are of the same value.
it is possible to introduce a weight to the examples based on their length to facilitate the model to improve the raking of the longer suggestions.
for example users may fully type very simple keywords like foror defalmost immediately thus making them positive examples of thetyped select class however they might not be what we want the model to learn.
.
.
api usage.
our analysis of the user logs shows that code completion demonstrates the worst results when it is necessary to suggest api calls both internal and external.
oftentimes the information about the local context is not enough be that file module or even the entire project instead one may look at how a particular api is used in other projects.
a promising direction is using a corpus of open source code to learn similar context and incorporating this information into the ranking.
.
logs and infrastructure importantly the goal of our research and this paper does not lie only in the area of code completion.
rather we want to emphasise the importance of structured user logs and how a pipeline for their collection can be used to both develop features and evaluate them.
besides code completion the same described infrastructure can be used to improve other features of the ide refactoring recommendation code smell removal and others.
collecting user logs allows us to not make any assumptions about their behavior or at least move away from them and instead continuously take into account their feedback.
this pipeline can be used to bridge academic results with practical applications.
if researchers develop a new model for ranking code completion we can collect its output as a feature use it when training our models and see in practice whether it improves the user experience.
overall our experimental results and the experience of running such a system in production for more than a year demonstrate that it is a valuable tool for improving the ide.
we were able to run the described pipeline for many prominent languages java python kotlin javascript typescript ruby go and others with the infrastructure of the experiments being largely reused.
the default completion ranking models in many intellij based ides right now are the ones trained on the user logs.
moreover the process continues to this day with new features being evaluated and new models being tested.
related work a lot of research has been dedicated to improving code completion using machine learning methods.
as we already mentioned a principal distinction between different works is whether they are trained on synthetic data or the real user data .
in this section let us describe several key studies.
bruch et al.
proposed to use three simple ml approaches to improve code completion namely a straightforward frequencybased system an association rule mining and a modification of theknearest neighbors approach.
the authors focused on the api recommendations and used synthetic data an existing code base where some method calls were removed to simulate completion queries.
of the three tested models the latter showed the best results in terms of the f1 measure.
proksch et al.
proposed to use bayesian networks instead.
their work is also limited to api calls and deals with synthetic data however the authors suggested that a proper evaluation of a code completion system must rely not only on the quality analysis but also on the performance analysis.
for this reason the authors also took into account the size of the developed model and the inference speed and tested it on queries of different size.
more recently svyatkovskiy et al.
evaluated end to end neural networks instead of feature engineering based approaches.
the authors also formulated the task as a learning to rank problem and leveraged static analysis as a candidate provider.
similarly the authors also took into account the importance of model size and inference speed and thus evaluated them too.
in a thorough comparison the authors evaluated different token encoders and different context encoders.
at the same time in this work only the api recommendations are addressed and synthetic data is used theconference july washington dc usa v. bibaev a. kalina v. lomshakov y. golubev a. bezzubov n. povarov t. bryksin source code of the most starred python projects .
the approaches proposed by svyatkovskiy et al.
have been used as a base for code completion solutions for the visual studio code ide .
the importance of using real world user data was carefully studied in the seminal paper by hellendoorn et al.
.
the authors collected a dataset of real completions conducted by users and compared them with existing synthetic benchmarks.
not only did the authors find that some state of the art techniques demonstrate significantly worse performance on the real data they also show that some features of the real life code completion usage are invisible in a synthetic setting specifically the users spending the most time on the infrequent tokens that the models recommend with even worse quality.
similarly proksch et al.
evaluated different approaches on the real world data of queries and also showed that synthetic evaluations provide unrealistic numbers when compared to the ground truth.
however these works used real user data only for offline evaluation not for actually training better models on them probably since it is difficult to collect a necessary amount of data in the research setting.
finally aye et al.
did train a model on the real world data and also carried out an a b test to evaluate it.
more specifically the authors studied the hack dialect of php and trained end toend neural network models on three different datasets baseline synthetic static codebase of nearly one million source files autocompletion real accepted completions inside the ide and edit code edits logged during file save operations.
then among other experiments the authors conducted two live a b tests comparing models trained on two latter datasets to the model trained on the baseline dataset.
the results showed that the model trained on the autocompletion dataset performed the best indicating the usefulness of the user behavior logs.
however it is important to note that this work was carried out inside facebook which is why the authors could collect personal information specific code snippets in logs so while their datasets are undoubtedly large they are not limitless and may not generalize well to all users.
overall it can be seen that our work compliments the existing ones it uses the real world usage logs to both train and evaluate models develops an approach that takes into account the users privacy and implements a pipeline for the continuous gathering of data and evaluation of models using the ide release cycle.
threats to validity the industrial nature of the solved problem and the orientation of the solution towards production impose certain limitations on our research.
several important threats to validity can be highlighted.
user bias.
our approach relies on the logs of users training the model on their decisions with the goal of improving the experience of all the subsequent users of the ide.
however the users who participate in the early access program and who agree to send the logs might not be a perfectly representative sample of all the users.
these users are in general more active and can be more aware of various ide features.
what is more their activity and their work in the ide may also differ from that of an average user.
at the same time a large sample of thousands of users and tens of thousands of completion sessions allow us to gather data from different domains of software engineering and different levels of activity.state of the art.
as mentioned in section the positioning of our work does not allow us to claim state of the art results or directly compare with them making it possible for us to have missed a specific work or model architectures that would perform better.
however the main goal of this work was to present a pipeline for thecontinuous improvement of ide tooling making it possible to compare the necessary models in the near future.
features.
the same argument can be applied to the list of the calculated features.
privacy limitations lead us towards the manual development of features making it possible for our current set to not be optimal.
once again the developed infrastructure allows us to evaluate new features constantly and test any new developments in a continuous cycle.
while these threats are important to note we believe that they do not invalidate the usefulness of the proposed pipeline and the results of the carried out evaluations.
conclusion future work in this work we presented a pipeline for collecting anonymous logs from users to train a model for ranking code completion suggestions.
we designed a set of features that are calculated on the user s machine and are then anonymously collected to the server without gathering sensitive personal information or the code itself.
based on these features we trained a catboost model for ranking completion candidates and evaluated it in two settings.
the offline evaluation on the held out set of user data showed that the recall at k of the ranking increases when using the model.
the online evaluation consisted in an a b test between the trained model and the default heuristics based ranking and showed that the fraction of sessions that ended with the explicit selection of the token increased while the average number of the required typing actions decreased.
an important aspect of the proposed idea is that it can be used continuously gathering new data and evaluating new models in a constant cycle.
this is implemented in the early access program eap of intellij based ides users may agree to anonymously send their logs to the centralized server in exchange for being able to test new features.
in each eap release before the main release of the ide the experiments are being conducted users are divided into groups some of which get the current best performing models while some get new ml based models that are then compared in a live a b test.
at the same time logs are also being collected from other users to train future models.
this pipeline has been in production since the end of and demonstrated its usefulness for designing new features and discovering better models.
the proposed pipeline may be used for other tasks beyond code completion.
various anonymous features may be collected and used to fine tune different parts of the ide suggesting refactorings fixing bugs etc.
we hope to see more research that relates to collecting structured usage logs.
such a setting also puts research based approaches into a real world environment where the tested models have to be quick and lightweight.
in future work we plan to continue our experiments with code completion models and develop new features that would allow them to perform better.
in particular we are working on improving code completion for api calls using the information about similar contexts in other projects.
we would also like to broaden our scope and employ such machine learning models to other aspects of software development in the ide.all you need is logs improving code completion by learning from anonymous ide usage logs conference july washington dc usa