unpublished working draft.
not for distribution.using nudges to accelerate code reviews at scale qianhua shan david sukhdeo qianying huang seth rogers lawrence chen elise paradis peter c. rigby nachiappan nagappan meta platforms inc. menlo park new york and bellevue usa qshan fb.com davesukhdeo fb.com qyhuang fb.com sethrogers fb.com lchen fb.com pcr fb.com nnachi fb.com abstract we describe a large scale study to reduce the amount of time code review takes.
each quarter at meta we survey developers.
combining sentiment data from a developer experience survey and telemetry data from our diff review tool we address when does a diff review feel too slow?
from the sentiment data alone we learn that .
of developers are satisfied with the time their diffs spend in review.
by enriching the survey results with telemetry for each respondent we determined that sentiment is closely associated with the 75th percentile time in review for that respondent s diffs i.e.those that take more than hours.
to encourage developers to act on stale diffs that have had no action for or more hours we designed a nudgebot to notify i.e.
nudge reviewers.
to determine who to nudge when a diff is stale we created a model to rank the reviewers based on the probability that they will make a comment or perform some other action on a diff.
this model outperformed models that looked at files the reviewer had modified in the past.
combining this information with prior author review relationships we achieved an mrr and auc of .
and .
respectively.
to evaluate nudgebot in production we conducted an a b clusterrandomized experiment on over 30k engineers.
we observed substantial statistically significant decrease in both time in review .
p .
and time to first reviewer action .
p .
.
we also used guard metrics to ensure that most reviews were still done in fewer than hours and that reviewers still spend the same amount of time looking at diffs and saw nostatistically significant change in these metrics.
nudgebot is now rolled out company wide and is used daily by thousands of engineers at meta.
ccs concepts software and its engineering collaboration in software development .
keywords code review efficiency nudging rigby is an associate professor at concordia university in montreal qc canada.
unpublished working draft.
not for distribution.permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november singapore singapore copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
reference format qianhua shan david sukhdeo qianying huang seth rogers lawrence chen elise paradis peter c. rigby nachiappan nagappan.
.
using nudges to accelerate code reviews at scale.
in proceedings of the 30th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november singapore singapore.
acm new york ny usa pages.
.
.
introduction code review is an effective process for identifying defects and spreading knowledge.
however it is manual and time intensive and can be a bottleneck for rapid releases and continuous integration.
in the early days of inspection fagan reported that inspections took weeks or months to complete.
in the late 90s porter et al.
simplified inspection and found that it could be reduced to around one week.
in contrast rigby et al.
showed that the contemporary lightweight review process used by open source projects had a review interval of approximately one day which was confirmed to be similar to the review time at microsoft and google .
the goal of this paper was to investigate whether and how time in review could be further reduced using nudges on slow code reviews .
code review is an important part of the software development process at meta.
every code change called a diff and equivalent to a pull request must be approved by a peer before being shipped.
meta s code review tool is called the difftool see figure for an example of a diff under review .
the code review team at meta supports the difftool tool among other code review tools and aims to make code reviews as quick productive and delightful as possible.
twice annually the team leverages data from a company wide developer experience survey or devex to identify ways to improve the tool.
survey data from the second half of showed that while developers are overwhelmingly satisfied with code review in general they are less satisfied with the speed with which their code is reviewed.
in this paper we address the following research questions related to perception of review speed whom to nudge when diffs reviews are slow and the nudgebot tool and cluster randomized a b experiment.
rq1 survey and telemetry when does a diff review feel too slow?
each half at meta the code review team examines data from a developer experience survey to understand how well the tool is facilitating development and code review.
while developers are overwhelmingly satisfied with code review in general some areunpublished working draft.
not for distribution.esec fse november singapore singaporeqianhua shan david sukhdeo qianying huang seth rogers lawrence chen elise paradis peter c. rigby nachiappan nagappan dissatisfied with its speed.
to dig deeper we triangulated this survey s satisfaction data with quantitative time in review telemetry data.
we found that each developer s slowest 75th percentile of diff reviews strongly influences their level of frustration with diff review time.
based on this time in review distribution meta set a goal to reduce the number of stale diffs by nudging reviewers on diffs that had not received any interaction for hours or more.
rq2 who2nudge model who should be nudged when a diff is stale?
to determine who should be nudged on a stale diff we built a statistical model.
using features including size of change how the diff was assigned and the relationships among author and reviewer the model predicts the diffs developers are most likely to review.
we found that the most important features in the who2nudge model are the amount of time the reviewer spent viewing the author s diffs in the past days how the reviewer was assigned and the total number of assigned reviewers.
the who2nudge model has an auc and mrr of .
and .
respectively.
rq3 experiment and rollout does stale diff nudging work in practice?
we ran a cluster randomized experiment with nudgebot on code review to determine if it reduces the number of stale diffs i.e.
hours with no action by nudging a subset of reviewers to take action.
we observed statistically significant decreases in review cycle time .
p .
the proportion of diffs that take longer than three days to close .
p .
and the time to first action on a diff .
p .
.
meta also ensures that guard metrics are not negatively impacted.
we saw no statistically significant change p .
in our guard metrics including the percentage of diffs reviewed in fewer than hours i.e.diffs that are not nudged and the total time eyeball time that reviewers spend looking at diffs in review i.e.they do not rush their reviews.
the remainder of this paper is structured as follows.
in section we introduce how code review is done at meta discuss the survey and available telemetry data and provide an overview of experimentation at meta.
in section we use the survey and associated telemetry data to provide quantitative evidence of when diff should be considered stale.
in section we train and evaluate a model of who should be nudged on a stale diff.
in section we describe our cluster randomized experiment and rollout of nudgebot in production.
in section we discuss threats to validity.
in sections and we discuss our work in the context of the literature describe our contributions and conclude the paper.
background and data meta is a large online services company that works across the spectrum in the communications retail entertainment industries.
meta has tens of thousands of employees with offices spread across the globe north america europe middle east and asia .
meta has its own dedicated infrastructure teams where the tools used are a mix of commercial and in house developed systems.
the code review tool discussed here difftool is also widely used acrossthe world released publicly by meta as part of their open source efforts.
.
code review at meta following the descriptions of code review at microsoft and google we provide a detailed description of code review at meta.
code changes are called diffs and each diff goes through a review in the difftool .
the difftool dashboard is a single purpose surface used only for diff review whereas most other surfaces are multi purpose with diffs comprising only a portion of the experience.
for example the internal home page shown in figure displays various chats meetings and other action items a user may care about in addition to a widget containing up to five relevant diffs.
the author uploads the diff and after checking it and potentially adding comments to guide reviewers publishes the diff.
the act of publishing a diff sets the diff s status to needs review whereupon it becomes visible to all reviewers.
the author can assign individual reviewers and or groups of reviewers both before and after publishing.
a recommender based on how often a potential reviewer has previously modified the files in the diff also suggests potentially competent reviewers.
numerous customized rules help assign developers to a review e.g.
any diff that modifies file x automatically assigns reviewer a. when reviewers are assigned to a new diff they receive notifications across multiple surfaces including email desktop notifications the internal home page and the difftool dashboard.
sometimes the author may also directly message a reviewer with a link to the diff or otherwise notify the reviewer outside of difftool specific channels.
once the reviewer has decided that they will review a particular diff they open the diff page i.e.the web page with that specific diff s contents see figure .
from there the reviewer can read the author s summary the author s testing notes and the code change itself.
the reviewer can optionally add one or more comments to the diff which are visible to everyone.
the reviewer can also take one of several actions accept back to author resign or commandeer.
the accept action marks the diff as ready to be shipped i.e.landed on trunk.
the back to author action is similar to the reject or request changes action in other code review tools indicating to the author that changes or other follow up is required before the diff can be shipped.
the resign action removes the reviewer from the diff and the commandeer action takes control of the diff whereupon the user ceases to be a reviewer and becomes the author of the diff.
the diff author meanwhile has several actions at their disposal.
if the author wishes to amend their code change they can do so and update the diff accordingly.
the author may also comment on their own diff for instance to explain an update to the code change or to address reviewer feedback.
if the diff has been accepted the author can ship it to production.
if the diff is in review the author can mark their diff changes planned to indicate to reviewers that the contents are not actually ready for review effectively pausing the review.
and if the diff has been accepted marked back to author or marked changes planned the author can request another review typically after updating the code change.
lastly the authorunpublished working draft.
not for distribution.using nudges to accelerate code reviews at scale esec fse november singapore singapore figure a redacted view of a diff under review in difftool .
authors and reviewers can interact via the diff review page in phabricator.
the diff under review has several sections including the diff summary the actual changes that happened the assigned code reviewers the interactions between the various reviewers and author the test case information and status the results of static analysis historical information etc.
figure a redacted view of the internal home page a multipurpose surface that includes a widget showing up to five relevant diffs can abandon the diff at any time which indicates that the change is dead and will not be shipped.
throughout this process the diff author s ability to ship their code change is gated by this diff approval process.
.
meta developer experience survey the developer experience survey is a company wide survey run twice annually.
the data we use here were collected between october and november i.e.h2 .
participants were recruited from among all meta developers who had published more than diffs and more than lines of code over the past days at study launch time.
the h2 survey took about minutes to complete and covered all areas of a developer s experience from the organization of work through to coding code review testing landing etc.
the section on code review asks about developers satisfaction with1 the time it takes to get your diffs reviewed by peers your ability to effectively review your peers diffs and the quality of reviews on your diffs.
we received approximately responses to questions about participants code review experience which were shown to only a subsample of all survey respondents since employee ids are associated with participants survey answers we can run analyses that join three data sources employee demographics such as tenure organization role telemetry data such as tool usage coding languages etc.
and survey data at the user level as long as the confidentiality of each individual s response is preserved.
this allows direct research into how sentiment data is correlated with demographic and behavioral data and supports deeper insight about the intersection of experience and behavior.
.
meta developer telemetry metrics meta logs business activity associated with each developer s work.
with respect to code review every diff action is logged publishing a diff commenting on a diff adding a reviewer to a diff accepting a diff shipping a diff etc.
this telemetry also captures interactions between diffs and both humans and bots for instance when an automated rule triggers a comment on a diff.
every time a user views a diff is also logged including the specific surface used to open the diff e.g.
opening a diff via email notification vs the central diffs dashboard regardless of whether the user took any action.
various heuristics are applied to estimate the eyeball time that a user spent looking at a particular diff.
specifically we track the duration that a particular diff is shown in the active tab of a particular user s browser and sessionize user activity when the user goes idle stop or pause .
this telemetry can be aggregated into author metrics and reviewer metrics.
for instance we can compute how many diffs an author has published or how long their diffs took to review during a given time frame.
similarly we can compute how many diffs a reviewer has acted upon or how much time was spent looking at others active diffs.
finally we can also compute linkages between users for instance how many times user a has commented on userunpublished working draft.
not for distribution.esec fse november singapore singaporeqianhua shan david sukhdeo qianying huang seth rogers lawrence chen elise paradis peter c. rigby nachiappan nagappan b s diffs or how much time user c has spent looking at user d s diffs.
.
a b experiments at meta meta has a culture and infrastructure that allows engineers and data scientists to run controlled experiments on all major changes and new features.
in the controlled a b trial the old control feature a is experimentally compared with the new feature b .
the experimentation framework allows experimenters to assign users to groups through feature toggles collect experimental data and run basic hypothesis testing.
before the experiment begins the expected goal outcome measures are specified and guard metrics are used to ensure that other important outcomes are not negatively impacted.
the code review team at meta leverages a b testing to determine the efficacy of new features and make launch decisions based on the experiment results.
specifically the instrumentation infrastructure allows to answer do we observe substantial and statistically significant improvements in goal metrics without regressions in the guard metrics?
we note that the code review team at meta is faced with unique challenges with a b testing compared to externally facing user products given that the difftool is an internal tool it does not have as many users as externally facing user products.
since there is smaller sample size less data available and more noise the code review team usually launches a b tests to all developers at meta.
in this experiment we have over 30k participants.
difftool has large social effects since code review is a highly social activity.
for instance alice s behavior as a diff reviewer affects bob s experience as a diff author.
therefore to contain the spillover effect between test and control groups a cluster randomized experiment design is often needed .
we discuss the full design of the stale diff nudge experiment in the methodology section .
.
rq1 survey and telemetry when does a diff review feel too slow?
for rq1 the first step was to define time in review.
we computed this as the total time a diff spends in the needs review status i.e.
when the diff was explicitly waiting on a reviewer action summing over all rounds of review.
to illustrate this with an example consider a hypothetical diff that was submitted for review at 9am sent back to the author for revisions at 11am re submitted for review at 2pm and ultimately accepted at 3pm.
we would sum the two in review durations i.e.
9am 11am and 2pm 3pm to say this diff spent hours in review.
in this way we specifically capture the extent to which the developer was blocked waiting for the reviewer s .
from there we computed the 75th percentile i.e.p75 for each developer s time in review aggregating over all of the developer s diffs during july to november the quarter corresponding to the survey.
we selected the 75th percentile because this represents the lower bound of the of diffs that take the longest for each engineer.
we proceeded to join this data to each developer s response to the survey question how satisfied are you with the time it takes to get your diffs reviewed by peers?
to preserve the confidentiality of figure percentage of users dissatisfied with diff review time vs the time it takes for their slowest of diffs to be reviewed.
to preserve anonymity we aggregate by decile which means that each datapoint is an aggregation of users.
we see that as dissatisfaction increases with slower diff reviews.
each developer s survey response we binned all developers into deciles based on their p75 diff review time and only show these numbers for decile aggregations.
from the survey alone we learned that .
of developers are satisfied with the time their diffs spend in review i.e.
.
dissatisfied.
with the combination of survey and telemetry we are able to explore whether the developer s actual experience waiting for review in fact influences their subjective perception of review timeliness.
figure plots the level of dissatisfaction against the developer s 75th percentile of time in review.
we see a clear relationship between an increase in dissatisfaction and the amount of time it takes for engineers to get their diff reviewed.
there is no sharp cliff or threshold that separates a good experience from a bad one.
rather as an engineer s diff review time increases so does their frustration.
after discussion and examination of the time in review distribution the code review team decided to consider a diff that has had no action for hours as stale.
this threshold targets the slowest of diffs that have been waiting for review.
as a engineer s diff review time increases so does their frustration.
we set a nudge threshold at hours which roughly corresponds with the slowest of engineers diffs.
rq2 who to nudge model who should be nudged when a diff is stale?
we created a model to predict whether a reviewer will comment on or otherwise act upon e.g.
accept or reject a diff.
the features are presented in table .
we also created submodels to understand how well subcategories of features perform.
we briefly provide background on the features and their respective categories below.unpublished working draft.
not for distribution.using nudges to accelerate code reviews at scale esec fse november singapore singapore table the features and feature categories for rq3 who2nudge model.
note the relationship features include raw normalized by author and normalized by review.
category feature description diff info rule based change is this diff created using a rule?
an example if a human wants to rename a widely used class across the entire code base they would apply a tool based rule to implement the diff.
diff info num files changed number of files in the diff diff info author is bot is the code author a bot or developer diff info total num reviewer total number of reviewers assigned to the diff.
not all assignees will review the diff diff info num manually added reviewerstotal number of reviewers that were added by the author or another developer to the diff.
assignment expertise attribution scorethe target reviewer s relevance score as computed by prior file line edits prior work includes assignment expertise attribution rankthe target reviewer s rank as computed by prior file line edits assignment auto added was the target reviewer added by a bot or rule true or directly by a human being false ?
assignment only group added was the target reviewer added explicitly false or only as part of a team true ?
relationship diff reviewer count the number of diffs created in the last days where the target reviewer was a reviewer on a diff by the author whether individually or through a group prior works include relationship diff reviewer subscriber countthe number of diffs created in the last days where the target reviewer was a reviewer or subscriber on a diff by the author whether individually or through a group relationship diff eyeball time open status as reviewerthe total number of seconds the target reviewer has spent viewing the author s diffs as an assigned reviewer while such diffs were in an open status in the last days.
open status is any of needs review accepted waiting for author changes planned relationship diff eyeball time open statusthe total number of seconds the target reviewer has spent viewing the author s diffs while such diffs were in an open status in the last days.
open status is any of needs review accepted waiting for author changes planned relationship author reviewer same teamare the author and reviewer on the same team relationship author reviewer title proportion of all diffs by employees with author s job title that were acted commented on by employees with the target reviewer s job title within the past days.
captures for example how likely is a data scientist to comment on a software engineer s diff.
diff info does information about the diff impact the diff ranking for a reviewer?
we examined the impact of predictors including the type of change the size of the change and how many reviewers have been notified about the diff.
these features have less common in prior research on reviewer recommendation because they do not suggest anything about the relative importance of each reviewer.
nevertheless diffs with many reviewers may have a lower probability that any particular reviewer act upon the diff and diffs with more complex changes may attract engagement from more reviewers.
we reasoned that these features may interact with features from other categories.
how assigned how was the reviewer selected to review the diff?
was the reviewer added directly by the author or through a group by a rule or by prior knowledge of the files under review?
the first three are straightforward selections.
the fourth uses expertise attribution as measured by the number of lines in the files under review that a developer has modified in the past.relationships we examined the organizational relationship of the reviewer and author the number of times they have reviewed each other s code and the eyeball time or viewing time that they have spent on each other s diffs.
for non organizational relationships we included three variants unnormalized normalized by author and normalized by reviewer.
for example using the eyeball time features if we normalize by author we would divide by the total time that all reviewers have spent looking at that author s past diffs.
.
random forest and data to evaluate our who2nudge model and features we determined whether a reviewer will review the diff by the end of a diff review session.
if a diff went through multiple rounds of reviews and edits each round of review was considered as a distinct review session for our model training and evaluation.
we assigned a positive label to a diff review session reviewer pair when the reviewer made an action such as accepting the diff rejecting the diff or leaving a comment.
we assigned a negative label to a diff review session unpublished working draft.
not for distribution.esec fse november singapore singaporeqianhua shan david sukhdeo qianying huang seth rogers lawrence chen elise paradis peter c. rigby nachiappan nagappan reviewer pair when the reviewer resigns from the diff or takes no action.
we built a random forest for each set of features and combined them in a final model.
we split the data into a timeordered test set and a training set using data available before the review session began.
our training set included all diff review sessions during a .
month period in summer diff review sessions our test set contains all diff review sessions the subsequent week diff review sessions .
to determine feature importances we were able to extract the magnitudes directly from the random forest.
however the directionality of each feature s influence is not readily accessible from random forest models.
to infer the directionality of each feature s influence we constructed a decision tree using features for the relevant submodel and manually inspected the tree s splits along with the values in each leaf node.
these decision trees were used only for discussion purposes namely to enrich our understanding of the feature importances with directionalities along with the magnitudes at which the most important splits occur.
we use two outcome measures mrr and auc.
mrr mean reciprocal rank is defined as the average of the reciprocal of the rank of the first relevant diff for any particular set of predictions for a developer.
auc area under the curve is defined as the area under the roc curve and can be interpreted as the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one.
for a useless classifier that gives random predictions we would have an auc equal to .
and for a classifier that predicts perfectly an auc equal to .
we calculated auc across all reviews.
.
model results our model needs to nudge developers who are likely to take action so we predict the probability of a developer participating in a diff review.
table shows the auc and mrr for each submodel and the final model.
below we discuss the importance of feature categories and individual features for each submodel.
using only basic diff info we achieve an auc of .
and an mrr of .
.
examining the model we see that the total number of reviewers assigned to a review has an importance of .
.
this implies that when there are few potential reviewers for a diff it is simplier to decide who to nudge.
the remaining features contribute little.
for instance the importance score of the number of files in the diff is low .
.
expertise attribution.
many review recommenders use prior editing experience in the files under review in their rules or models e.g.
.
reviewer with more expertise on the files under review are more likely to act on a diff and we see a reasonable auc and mrr of .
and .
respectively.
how assigned.
this submodel includes the expertise attribution features along with whether the reviewer was automatically assigned or assigned as part of a group.
by adding these how assigned features we see a substantial improvement in auc and mrr to .
and .
respectively.
reviewers that were directly added are more likely to act on the diff and the most influential factor at .
is whether or not the reviewer was added as an individual vs added as part of a group.
the expertise attribution rank was the next influential factor at .
.author reviewer relationships.
the organizational relationship and latent review relationships between the author and reviewers have an auc and mrr of .
and .
respectively.
interestingly the formal position and whether the author and reviewer were on the same team had the lowest importance at .
and .
respectively.
the strongest features are related to normalized eyeball time the more time reviewers had previously spent looking at a particular author s diffs the more likely they were to act on the review.
the total importance of the eyeball time features was .
which includes importances of .
when normalized by author .
when normalized by reviewer and .
when unnormalized.
this also indicates that normalization by author is the most effective approach much more so than normalization by reviewer.
the final who2nudge model combines all the features and has improved auc and mrr of .
and .
respectively.
these results were sufficiently robust to begin productionizing the model and begin the rollout and experiment of nudgebot .
we did however eliminate the feature author reviewer title because of the additional complexity and processing time required in its calculation and its low importance to the model.
for the final production model we improved efficiency by batching predictions instead of separate predictions for each diff reviewer pair.
we also eliminated the feature author reviewer title because of the additional complexity and processing time required in its calculation and its low importance to the model.
the most important features in the who2nudge model are the amount of time the reviewers have viewed the authors diffs in the past how the reviewer was assigned and the total number of assigned reviewers.
the auc and mrr of the final who2nudge model are .
and .
.
rq3 experiment and rollout does stale diff nudging work in practice?
with nudgebot our core hypothesis is that nudging reviewers for stale diffs can reduce the review cycle time and improve percentage of diffs closed within hours and hours since we only nudge diffs after hours.
our main quantifiable concern for potential negative side effects is that reviewers may spend less time reviewing new diffs when they receive notifications for stale diffs.
we first describe the tool design and changes we made from an opt in trial.
we then describe our company wide experiment and the impact on our goal and guard measures.
.
nudgebot design and opt in trial at meta changes to the engineering infrastructure first pass through an opt in trial.
the difftool team used an early iteration of nudgebot along with other teams that opted into trail.
figure provides an example of nudgebot sending a chat message displaying three stale diffs and providing developers with the option to be reminded later.
below we discuss some of the important design decisions and feedback that lead to changes when to nudge.
based on the distribution of time in review and survey responses see section we decided to consider a diff stale and nudge a reviewer when no action had beenunpublished working draft.
not for distribution.using nudges to accelerate code reviews at scale esec fse november singapore singapore table the who2nudge model and the component parts of the model.
we see that how the diff was initially assigned and how often a reviewer has viewed the author s diffs in the past are the strongest predictors.
diff info expertise attribution how assigned relationships who2nudge model mrr .
.
.
.
.
auc .
.
.
.
.
taken on it for hours.
we attempted to create a model to predict how long a diff will be in review but were unsuccessful .
medium.
at meta chat is by far the most popular communication method we nudge on chat but also send email nudges display the nudges in the general notification list and the diff dashboard.
default to silent.
developers receive many ims every day and adding a nudgebot messages can lead to more noise.
we want developers to be aware of diffs that need attention but do not want to distract them from the task at hand.
our bot defaults to sending messages silently that developers will look at in between tasks.
time of day.
while we discussed incorporating nudges within the calendar eg nudging after a meeting we decided to send the nudges one hour after the start of the workday.
we also allowed developers to select the remind me later option.
this option will nudge the reviewer again after a meeting to avoid disrupting a longer focus block.
batch notifications.
we do not want nudgebot to be noisy with a notification each time a diff is stale.
we batch notifications and send them once in a day.
we also do not notify the same developer more than once about a stale diff.
we initially sent up to diffs that needed review.
negative developer feedback indicated that the dashboard showing stale diffs took up too much space so we reduced it to three diffs.
active reviewers.
sending a nudge to developers when they are already reviewing lots of diffs lead to negative feedback comments.
we do not nudge extremely active reviewers which we quantified as or more actions on diffs in the last days.
wording.
the initial nudge wording provided a long explanation of why we were nudging particular reviewer.
based on feedback from developers we simplified it to a short sentence asking developers to unblock a diff and simply show how long the diff had been waiting for review.
opt out.
developers are able to opt out from nudgebot .
to date less than of engineers opt out.
this is impressive given that we send between 2k and 4k nudges per day.
over the course of the company wide experiment we see less than four developers opting out per day and to developers opting back in per day.
.
experimental design experiments are only valid when we can contain the treatment effect within the test group and control group does not expose to any of the treatment effect.
this is formalized in causal inference theory as the stable unit treatment value assumption sutva .
however sutva is not held perfectly with code review since code review is inherently social activity between authors and reviewers collaborating on a diff.
consider the following scenario bob and alice are frequent diff review collaborators.
with userlevel randomization alice is assigned to the test group to receive the nudging feature while bob is assigned to the control group.
alice will get nudged for overdue diffs authored by bob but bob will not get nudged for overdue diffs authored by alice.
since alice and bob collaborate frequently on code review although bob does not receive the nudging experience his code review behavior may still change since he knows alice gets nudged for overdue diffs authored by him.
this is spill over of treatment effects which may lead to inaccurate treatment effect estimates .
to contain the spill over effects between test and control group in the experiment we set this up as a cluster randomized experiment.
in a cluster randomized experiment treatment is assigned at user cluster level rather than individual user level .
for instance users who belong to the same cluster will enter treatment or control group together.
in the context of the overdue diff nudging experiment teammates and their frequent diff collaborators will have the same experience with the nudging feature.
to generate the clusters of developers we used the louvain community detection algorithm to create clusters of developers based on the eyeball time spent reviewing each other s diffs together in the last days.
user clusters are generated with the following steps we first construct a graph with developers at meta as nodes and use the total number of seconds person a spent looking at person b s diffs in the past days while such diffs were in an open status needs review accepted changes planned waiting for author as the edge weight between the nodes.
we chose days because it is common for developers to switch teams or change code review collaborators.
we then use the louvain community detection algorithm to create clusters based on these edge weights.
louvain algorithm identifies communities in graph with two steps modularity optimization and community aggregation .
the outcome is that developers who have commonly reviewed each other s diffs in the last days are more likely be in the same community cluster to reduce spillover effects.
the clustering was locked as of october .
new employees who joined meta after october received the control experience and they were excluded from the experiment analysis.
the experiment took place over days in fall .
the experiment was launched at meta with 15k developers in the test group and 16k developers in the control group.
we anticipated there would be queue draining effect of stale diffs when the experiment started because nudging reviewers should encourage them tounpublished working draft.
not for distribution.esec fse november singapore singaporeqianhua shan david sukhdeo qianying huang seth rogers lawrence chen elise paradis peter c. rigby nachiappan nagappan figure nudgebot sends up to three diffs that are stale to a developer who is likely to review them one hour after the start of the workday.
the chat message is sent with silent which will not push notify the developer but will allow them to view the message between blocks of focus.
the time the diff has been waiting is shown along with the username of the author and a clickable link to the diff.
if the reviewer is busy they can select remind me later.
nudgebot then examines the calendar and selects the next fragmented time e.g.
between two meetings to remind the developer later in the day.
emails are sent with the same information and same ability to be reminded later.
the diff numbers log message etc are mocked to preserve confidentiality but they are representative of real diffs at meta.
review diffs that have been waiting for a long time.
to avoid abnormal fluctuations of the review cycle time goal metrics we discarded the first days and analyzed data over the subsequent days.
the experiment included reviews on over 330k diff.
we confirmed that the mean difference between control and experiment conditions was normally distributed which allows us to use a confidence interval and the z statistic to calculate statistical significance.
outcomes.
at meta each experiment change must specify outcome goal metrics as well as existing guard metrics that should not be negatively impacted.
our hypothesis is that with nudgebot the amount of time a diff is under review will decrease.
we measure this in three ways the time a diff waits in with needs review status we targeted the slowest of diffs so we look at the number of diffs that take over days to close we want to encourage early actions so we measure the time to first action.
when we nudge stale reviews we make sure that diffs that were typically reviewed quickly do not suffer because reviewers are spending more time on stale diffs.
our guard measures are the percentage of diffs reviewed in hours or less the time a reviewers speeds looking at each diff eyeball time .
.
experimental results we observed statistically significant improvement on the review cycle time goal metrics specifically the average time of diffs in needs review status time in review reduced by .
p .
.
the percentage of diffs taking longer than days to close excluding weekends was reduced by .
p .
.
the average time to first reviewer comment or action reduced by .
p .
.
we did notobserve any statistically significant regressions in any of the guardrail metrics specifically percentage of diffs reviewed in hours p .
total reviewer eyeball time while the diff is in needs review status p .
we observed substantial statistically significant decreases in our goal metrics reducing time in review the number of diffs taking longer than days and the time for a first reviewer action by .
.
.
respectively.
we did not observe any statistically significant regressions in any of our guard metrics including diffs that take under hours and the amount of time reviewers spend looking at diffs.
given the positive results we observed from experiment the code review team rolled out nudgebot to of developers at meta.
threats to validity .
generalizability drawing general conclusions from empirical studies in software engineering is difficult because any process depends on a potentially large number of relevant context variables.
the analyses in the present paper were performed at meta and it is possible that results might not hold true elsewhere.
for this reason we cannotunpublished working draft.
not for distribution.using nudges to accelerate code reviews at scale esec fse november singapore singapore assume a priori that the results of our study will generalize beyond the specific environment in which our research was conducted.
researchers become more confident in a theory when similar findings emerge in different contexts .
.
construct validity who2nudge model the labels for this model were a binary classifications of whether or not a reviewer reviewed a diff during any particular review iteration which we defined as any action including accept reject but not resign or making any comment.
one could argue that only actions should count not comments.
conversely one could argue that reviewer edits to a diff s metadata e.g.
a reviewer changing a diff s test plan or even reviewer interactions between review sessions e.g.
while a diff is in the waiting for author status should be considered.
ultimately the precise definition of what counts as a user reviewing a diff requires subjective judgement over what to include and we are confident that our definition is reasonable and aligns with standard understanding.
.
internal validity data collected by the developer experience survey in h2 might not have been fully representative of the developer population at meta since responses were not weighted.
our post hoc analyses confirmed undersampling of people with very short and very long average diff review times however the impact of this skewed distribution only minimally influenced overall estimates of sentiment towards code review times.
we did not perform any hyperparameter tuning for our who2nudge model which leaves open the possibility that these models can be further improved through a rigorous hyperparameter optimization.
by using unoptimized defaults the hyperparameters were fixed before looking at the test set to ensure no contamination.
in addition there is no reason that a hyperparameter search on random forest would result in dramatic changes in which features are considered the most important which is what we focused on in the end.
the first hyperparameter of depth would mostly impact features that are less important since most major features would already be accounted for in the first few layers of the model s trees.
in addition the other hyperparamer of number of trees would be expected to have negligible impact on ranked feature importance given that each tree in the random forest is independent.
code review is a social activity and any experiment risks having collaborating developers in different experimental conditions i.e.
spillover effects.
we are unaware of prior work in empirical software engineering that controlled for these network effects.
to limit the impact of collaborating developers having different control and experimental conditions we used a cluster randomized experimental design to algorithmically generated clusters.
there will always be some spillover effects because of error from louvain community detection algorithm and developers switching teams during the experiment but we believe these will be minimal especially compared to prior work that did not mitigate these effects.
literature and discussion code review was first formalized over years ago by fagan where author reviewers mediators met in person to examine completed work artifacts.
the inspect was conducted over weeks and sometimes months.
much of the early work simply changed the process often adding more rigidity through process e.g.
.
the use of time in review as an outcome metric dates back to the fieldchanging experiments by porter et al.
that demonstrated that formality of the review process simply extended time in review without identifying additional defects.
they showed that expertise was the most important factor in identifying defects.
votta proposed asynchronous review and perry et al.
demonstrated that this lighter weight process found the same number of defects as conducting reviews in meetings.
in these late s experiments the review time was on the order of a week.
rigby et al.
used similar metrics to show how effective the extremely lightweight hugely iterative code review process was on major open source projects.
on the linux and apache project the review time was on the order of a day.
these results generalized to industry with reviews being conducted in less than one day .
at meta we showed that reviews happen very quickly often being completed in a few hours after publishing the diff median .
hours and with around being completed in under hours.
surveys and interviews have been used extensively to understand the code review process.
previous survey research about code review has focused on the motivations for conducting code reviews challenges to code reviews reviewers experiences of confusion when reviewing code and negative experiences with pushback during code reviews .
some works have triangulated code review data with survey and interview data to understand the process and the challenges .
to our knowledge this study is the first to link telemetry data on the time it takes to perform a review with developer sentiment.
only the egelman et al.
study combined survey and telemetry data and attempted to predict code review pushback from three change request metrics rounds of a review i.e.number of times an author or reviewer sent a batch a comments for a selected change request active reviewing time i.e.time spent by the reviewer on code review related activities and active shepherding time i.e.
time spent by the author on code review related activities .
while all three were useful for recalling instances of pushback they only had low precision between and .
overall the literature to date does not seem to have explored systematically how an individual s sentiment about code review in general or about time spent under review in particular is related to that individual s objective experience of code review as monitored by telemetry.
review recommendation.
identifying the right reviewers for a given change is a challenging and critical step in the code review process .
inappropriate selection of reviewers can slow down the review process or lower the quality of inspection .
the research on reviewer recommenders focuses on the problem of automatically assigning review requests to the developers.
early works focused the files and paths that developers had reviewed in the past .
more advanced techniques have also been developed including machine learning and sociotechnical factors .
recent research has expanded theunpublished working draft.
not for distribution.esec fse november singapore singaporeqianhua shan david sukhdeo qianying huang seth rogers lawrence chen elise paradis peter c. rigby nachiappan nagappan focus of recommenders to find developers who have currently a low review workload and to ensure that knowledge is more widely spread among the development team .
in our nudging experiments we recommended reviewers who are more likely to act on a diff on a stale diff using more advanced measures including how long developers have actually viewed each other s diff eyeball time .
we found that this normalized eyeball time is the best predictor in our who2nudge model and achieved a high auc and mrr .
and .
and we successfully integrated this recommender in production for difftool .
overdue diff nudging.
the inspiration for our work is the nudge system deployed at microsoft for overdue pull requests.
the microsoft context is substantially different from the one at meta.
for example meta uses a mono repo single repository for code control rather than the microsoft branch structure .
also the systems built are different the domain the programming language are all different.
we discuss similarities and differences in our results and designs which highlight our novel contributions.
first microsoft models the end to end time including both the author s and reviewer s time.
unfortunately they report very low accuracy with a mean average error of .
hours and a mean relative error of .
.
we also modelled time in review at meta and found a similarly large error .
instead of using this inaccurate model we decided to set a goal of nudging reviews at hours which represents the of slowest diffs and also corresponds to an increase in negativity sentiment in our developer survey.
second both microsoft and meta s bots were a b tested but microsoft did not run a cluster randomized experiment.
without a clusterrandomized experiment teammates can have different experiences of diff review introducing a confounder into the results .
our experiment also covered many more reviews than that reported in the microsoft experiment.
third microsoft reduces the average end to end time spent in for code review from days to days.
we explicitly only measure the time when the diff is waiting for a review and the median time in review at meta is .
hours.
we also do not nudge the author of the diff as our tooling already includes a set of notifications that keep the author appraised of the status of their diff.
fifth regardless of these differences at both microsoft and meta nudging stale diffs improves the turnaround time for reviews and the feedback on nudging is overwhelmingly positive.
at microsoft of nudges get a positive rating.
at meta we see few developers posting concerns on the nudgebot feedback group and less than of developers have opted out of receiving nudges after a company wide rollout.
contributions and concluding remarks code review is a computationally expensive manually intensive practice that is necessary to find defects and also for compliance of products at meta.
we make code review more efficient and enjoyable with the following specific contributions.
we describe the review process used by over developers at meta which has interesting differences with those reported by microsoft and google .
we demonstrate determine when a diff review feels slow a review that takes more than hours or is above the 75thpercentile of diff review time is associated with lower satisfaction in our developer survey.
we train and test the who2nudge model to determine which developer is mostly likely to take action when nudged.
our model has an auc and mrr of .
and .
and is used in production.
we designed and developed nudgebot .
we conducted an opt in trial and adapted the design based on feedback from developers.
to evaluate nudgebot we designed a cluster randomized experiment to ensure that collaborating diffs had a similar experience .
we are unaware of cluster randomized experiment design being used in the context of software engineering.
our study involved over developers at meta.
our nudgebot experiment lead to substantial statistically significant decreases in our goal metrics with time in review being reduced by .
p .
a .
p .
decrease in the percentage of diffs taking longer than days to close and a .
p .
decrease in the time to first comment or action from a reviewer.
we did not observe any statistically significant regressions in any of our guard metrics including diffs under hours and the amount of time reviewers spend looking at diffs.
given the positive results we observed from experiment the code review team rolled out nudgebot to all developers at meta.