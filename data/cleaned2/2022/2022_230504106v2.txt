on the usage of continual learning for out of distribution generalization in pre trained language models of code martin weyssow diro university of montreal montreal canada martin.weyssow umontreal.caxin zhou singapore management university singapore xinzhou.
phdcs.smu.edu.sgkisub kim singapore management university singapore kisubkim smu.edu.sg david lo singapore management university singapore davidlo smu.edu.sghouari sahraoui diro university of montreal montreal canada sahraouh iro.umontreal.ca abstract pre trained language models plms have become a prevalent technique in deep learning for code utilizing a two stage pre training and fine tuning procedure to acquire general knowledge about code and specialize in a variety of downstream tasks.
however the dynamic nature of software codebases poses a challenge to the effectiveness and robustness of plms.
in particular world realistic scenarios potentially lead to significant differences between the distribution of the pre training and test data i.e.
distribution shift resulting in a degradation of the plm s performance on downstream tasks.
in this paper we stress the need for adapting plms of code to software data whose distribution changes over time a crucial problem that has been overlooked in previous works.
the motivation of this work is to consider the plm in a non stationary environment where fine tuning data evolves over time according to a software evolution scenario.
specifically we design a scenario where the model needs to learn from a stream of programs containing new unseen apis over time.
we study two widely used plm architectures i.e.
a gpt2 decoder and a roberta encoder on two downstream tasks api call and api usage prediction.
we demonstrate that the most commonly used fine tuning technique from prior work is not robust enough to handle the dynamic nature of apis leading to the loss of previously acquired knowledge i.e.
catastrophic forgetting.
to address these issues we implement five continual learning approaches including replay based and regularization based methods.
our findings demonstrate that utilizing these straightforward methods effectively mitigates catastrophic forgetting in plms across both downstream tasks while achieving comparable or superior performance.
corresponding author.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse december san francisco ca usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
concepts software and its engineering software libraries and repositories computing methodologies natural language processing .
keywords deep learning for code pre trained language models continual learning out of distribution generalization acm reference format martin weyssow xin zhou kisub kim david lo and houari sahraoui.
.
on the usage of continual learning for out of distribution generalization in pre trained language models of code.
in proceedings of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering esec fse december san francisco ca usa.
acm new york ny usa pages.
introduction prior research on code representation learning leverages a ubiquitous two stage procedure to effectively train and specialize pre trained language models plms for code related downstream tasks.
the first stage i.e.
the pre training involves optimizing the model using self supervised learning on a large dataset to acquire general knowledge about code.
this pre training phase allows the model to adapt to downstream tasks in the second stage i.e.
the fine tuning.
previous studies typically leverage classical transfer learning methods which consist of transferring the pretrained knowledge to the target task by fine tuning the model on a task specific loss function and data.
this approach has been successful in the fields of natural language processing nlp and deep learning for code .
in this perspective previous works have primarily focused on stationary settings neglecting the practical need for models to adapt to changing environments and data over time.
most prior research has suggested using transfer learning to fine tune the model in static environments rather than addressing the dynamic nature of real world scenarios.
in practice programming languages software libraries and apis are prone to change and evolution leading to shifts in the distribution of the underlying software data over time it is also known as concept drift .
by ignoring the actual evolution of software codebases existing studies have focused on fine tuningarxiv .04106v2 aug 2023esec fse december san francisco ca usa weyssow et al.
pre training continual ne tuning ood programs ood programs .
.
.
model model model programs fig.
continual fine tuning of a pre trained language model of code.
after pre training the model needs to adapt to new out of distribution ood program data over time.
and testing pre trained models of code using stationary datasets.
in practice the software evolution potentially leads to a noticeable difference between training and test data i.e.
distribution shift that is often not present in these stationary datasets.
this phenomenon also occurs when the model is put into production and has to deal with real world data .
we argue that creating datasets that reflect real world software evolution scenarios and distribution shifts is crucial in order to properly evaluate the out of distribution ood generalization capability of code models .
the ood generalization measures a model s ability to generalize to new unseen data with a significantly different distribution from the training data.
therefore evaluating how plms of code generalize to ood software data in software evolution scenarios appears as a prime issue.
existing works on ood generalization designed the datasets based on various distribution shifts in source code data .
however they did not address the problem of continually adapting a pre trained model of code to streams of ood data.
the prime goal of our study is to explore methods for a model to better adapt to software evolution scenarios.
in this context we ask how to effectively continually fine tune a pre trained model of code to adapt to new data while still considering the past data?
see fig.
.
over the past years continual learning cl has emerged to address this problem which is relevant to a wide range of research areas including computer vision and nlp .
although transfer learning methods are not tailored for continual learning scenarios they can still operate to fine tune a model on streams of data.
however these methods lack robustness leading to unwanted phenomena such as forgetting past information known as catastrophic forgetting .
there exist other strategies such as retraining the model from scratch using new data which are also impractical due to the tremendous computational intensity in the pre training phase.
motivated by these issues of the existing models we attempt to investigate more robust and scalable fine tuning techniques.
we hypothesize that continual learning techniques may provide significant benefits over classical transfer learning in this context.
in this paper we delve into the behavior of plms of code in a continual fine tuning scenario as depicted in fig .
our objective is twofold to assess the out of distribution generalization capability of plms of code and to investigate effective continual fine tuning strategies to fine tune the models in the presence of a stream of ood data.
specifically we address these challenges in a scenario reflecting how typical software codebases may evolve inpractice.
to this end we create five ood domain datasets each introducing new unseen apis by the models during their pre training phase.
these ood datasets intend to simulate a stream of data for continual fine tuning and each dataset entails a significant distribution shift with respect to the pre training data.
as such our setting establishes an ood generalization problem.
we consider two widely used model architectures a gpt2 like decoder and a roberta like encoder pre trained on code.
to eliminate any data leakage between the pre training and fine tuning data we decided to pre train our models from scratch.
we do not study the popular existing plms like codebert or codet5 because they may be prone to potential data leakage i.e.
seeing the ood data in pre training that we cannot precisely control.
we evaluate the models on two downstream tasks api call prediction and api usage prediction.
in the first task the model attempts to predict api calls resulting in a single code token given code tokens appearing before the call site.
on the other hand the second task involves the generation of the whole api usage resulting in a sequence of code tokens with the same input format as the prior task.
together these two tasks provide a comprehensive evaluation of the model s performance in different code generation scenarios.
we start by investigating the impact of ood data on the performance of the gpt2 like decoder on both downstream tasks in a zero shot setting i.e.
without fine tuning the model on the new ood data.
we find that the model consistently fails to generalize to ood data by highlighting significant gaps in performance compared to in distribution data across six evaluation metrics e.g.
up to drop in bleu score .
this finding strongly suggests that pretraining itself is not sufficient and cannot solve ood generalization in plms of code.
we then evaluate the models performance in the continual fine tuning scenario using classical transfer learning and observe notable catastrophic forgetting.
to address this issue we implement a straightforward yet computationally inefficient cumulative fine tuning approach by utilizing a replay buffer of infinite size.
the results show that the approach drastically mitigates forgetting.
finally we compare the performance of classical transfer learning to that of replay based and regularization based continual learning methods.
replay methods are considered toughto beat strategies for continual learning and consist of maintaining a small replay buffer containing samples from previously seen data.
during fine tuning we use the replay buffer in conjunction with the current ood training set to fine tune the plm.
we explore regularization based methods including ewc si and rwalk which add regularization terms to the loss function at fine tuning to prevent extensive changes in important parameters of the plm.
we chose those methods as they are computationally efficient well known and considered strong baselines in the continual learning literature.
we discover that those continual learning methods significantly reduce forgetting while achieving similar or superior effectiveness on both tasks.
to the best of our knowledge this work constitutes the first initiative to study continual fine tuning for ood generalization of plms of code.
we believe that the impact of continual learning in this research area has the potential to be far reaching particularly due to the inherent evolution of software data over time and we discuss this aspect in more detail in the discussion section of theon the usage of continual learning for out of distribution generalization in pre trained language models ... esec fse december san francisco ca usa paper see section .
our contributions can be summarized as follows we demonstrate that plms of code fail to generalize to ood data and highlight the need for further investigation in this area.
we conduct a study on the behavior of two pre trained model architectures of code in a continuous learning environment showing that classical transfer learning lacks robustness and is prone to catastrophic forgetting.
we compare five continual learning methods including replaybased and regularization based approaches in our continual fine tuning scenario.
we show the superiority of continual learning over classical transfer learning.
we provide a large scale dataset of java code snippets and their api usage sequences including pre training data and a procedure for extracting ood data.
organization.
in section we discuss preliminaries on continual learning.
in section we go through our experimental design.
we present the results of our experiments in section .
in section we discuss the threats to the validity of our study as well as potential broader impact and future research directions.
we introduce the related work on out of distribution generalization and continual learning for pre trained language models in section .
finally we discuss some future work and conclude this work in section .
preliminaries on continual learning existing plms such as bert or gpt typically operate in transfer learning settings.
by using a two stage pre training finetuning procedure these models can be specialized for a wide range of downstream tasks.
however in this setting the data used for pretraining or fine tuning are often assumed to be stationary which is not reflective of real world situations.
in practice transfer learning methods can still be applied to non stationary data such as a stream of data but this technique is prone to catastrophic forgetting .
to address the above issues prior works introduced the concept of continual learning and designed specific techniques to mitigate catastrophic forgetting.
the primary assumption for continual learning is that the neural network should possess the ability to adapt to new data or tasks while maintaining stability on previous data or tasks often referred to as the plasticity stability dilemma.
considering continual learning is particularly interesting for ood generalization problems as continual learning methods focus on a keeping good plasticity stability trade off.
altogether it has to potential to enhance the generalizability of plms to a broader range of data.
continual learning methods often operate in constrained scenarios and hadsell et al.
outline a comprehensive list of objectives to balance in continual learning scenarios.
there exist three main categories of methods for continual learning as defined in a previous study .replay based methods store samples from previous experiences i.e.
previous stream of data in a replay buffer or use generative approaches to generate examples similar to those of previous experiences.
the replay buffer is used in conjunction with the current experience data to train the model.
replay based methods help the network gain stability by continual ne tuningpre training dataset .
.
.fig.
procedure to extract the id data used for model pretraining and the ood data used for continual fine tuning.
enabling the network to train on previous samples i.e.
stored in the replay buffer while adapting to new data.
regularization based methods add a regularization term to the loss function to prevent catastrophic forgetting by penalizing changes to important neural network parameters.
examples of regularization based methods include ewc si and rwalk .
finally parameter isolation methods use dynamic architectures to incorporate knowledge from previous experiences to mitigate interference .
experimental design in this section we describe the experimental setup of our study.
we carefully control our data and model setup to implement our out of distribution scenario.
we first outline the construction of our dataset and the generation of ood data for continual fine tuning.
next we discuss the pre training procedure of our models the target downstream tasks and evaluation metrics.
we present the results of our experiments in section .
.
dataset construction pre training language models from scratch require a large amount of data for the loss of the model to converge.
with that in mind we constructed our large dataset using programs crawled from github using google bigquery1.
specifically we focused on java programs and began by collecting all java files stored in github repositories.
next we used groum to extract all methods defined in the java files along with their api usage sequences.
we extracted the api usage sequences to facilitate our data splitting and obtain the position of each api site inside the methods to implement our downstream tasks.
each sample consists of all the tokens of a method.
to avoid duplication bias in our experiments we deduplicated the dataset by comparing the hash of each method.
the resulting dataset contains more than 68m java methods.
for our experiments we shuffled these 68m methods and randomly selected 10m methods to constitute our initial dataset.
fig.
illustrates how we further split the data for our experiments.
because we chose the pre train plms from scratch we have to split our data into indistribution id data used for model pre training and ood data used for continual fine tuning.
we also need to properly extract the ood data to align with our scenario consisting of introducing new unseen apis over time to the plm during fine tuning.
december san francisco ca usa weyssow et al.
table out of distribution dataset details.
dataset domain package interfaces train test d1 oodgeneraljava.util.concurrent blockingqueue threadpoolexecutor 239java.math biginteger java.util base64 treeset java.net forkjoinpool proxy serversocket socketaddress urlencoder d2 oodsecurity java.security cipher codesource identity keyfactory keypair messagedigest policy provider security timestamp27 d3 oodandroidandroid.view display inputevent window 150android.widget checkbox gridlayout android.media audioformat imagereader android.hardware camera sensor android.database databaseutils d4 oodweb org.springframework cachemanager classpathresource databuffer httpmessage httprequest jdbctemplate messagechannel messagehandler taskexecutor16 d5 oodguavacom.google.common.graph graphbuilder network 489com.google.common.io bytesource bytestreams com.google.common.cache cachebuilder loadingcache com.google.common.collect listmultimap multimap com.google.common.base charmatcher splitter out of distribution dataset dood .we create five ood datasets d1 ood ... d5 ood.
each ood dataset represents a unique domain that encompasses a high level functionality of apis.
for example we have a domain security that comprises apis related to programming security related code and a domain guava that includes only apis from the guava2library.
to create each ood dataset we randomly select interfaces from packages libraries related to their domain.
finally we associate to each domain dataset all apis within the selected interfaces excluding class construction methods.
table summarizes the dataset dood which contains samples in total.
to form each ood dataset we select samples from the pool of million java methods that manipulate at least one of their associated api.
in our experiments we perform continual fine tuning on the training sets associated with the ood dataset d1 ood ... d5 oodsequentially.
therefore to prevent data leakage we exclude samples that manipulate apis from multiple domains.
this elimination of samples removes a significant threat to the validity of our ood scenario and ensures that apis are introduced as intended during the fine tuning process.
to obtain representative test sets we randomly select of samples that manipulate each api within each ood dataset and used the selected samples to form the corresponding domain test set.
in distribution dataset did .we obtaindidby removing the samples indood from the initial data.
then we shuffle did and randomly select samples for test did test .did pt contains the remaining samples for pre training and we randomly select for model validation did pt valid .
in particular those samples allow us to monitor the evolution of the loss of the model on an independent validation set to avoid overfitting the pretraining data.
in total the pre training set did pt train contains more than 9m samples to pre train the models.
models and tasks setup in this work we consider two widely used deep learning architectures for code a roberta like encoder and a gpt2 like decoder .
we deliberately exclude the utilization of large language models llms in our research due to the substantial computational resources essential for their pre training.
to comprehensively addressour ood scenario it is imperative to pre train a model from scratch prior prior to continually fine tune it on code containing new unseen apis.
consequently we opt to evaluate two smaller models architectures namely roberta and gpt which either serve as foundational models for plms like codebert or to generative models.
decoder mdec .the decoder model is based on the gpt architecture with the same hyperparameters and is pre trained using a causal language modeling objective i.e.
left to right next token prediction.
as we conducted our experiments under limited resources we implemented a small version of gpt with million trainable parameters and pre train the model for steps.
we use early stopping to select the best model checkpoint based on the loss on the validation setdid pt valid .
encoder menc .the encoder model is based on the roberta architecture with the same hyperparameters and is pre trained using a masked language modeling objective.
we implemented a base version of roberta.
the model has million trainable parameters and is pre trained similarly to the decoder model with early stopping used to select the best checkpoint.
note that conversely tomdec the encoder s architecture is not suitable for generation tasks.
therefore we add a randomly initialized language modeling head on top of it for fine tuning using the ood datasets.
as a result we expectmencto be less stable than mdecand more prone to catastrophic forgetting since the language modeling head is not pre trained.
this comparison provides valuable insights into the robustness of two different architectures.on the usage of continual learning for out of distribution generalization in pre trained language models ... esec fse december san francisco ca usa public long skip long n n public long skip long n n math.
?
?
math.min n left api call prediction api usage prediction fig.
overview of the downstream tasks.
in the api call prediction task the model outputs a list of top kcandidates to predict the api call token i.e.
min .
in the api usage prediction task the model attempts to predict all the tokens constituting the api usage interface name method name parameters and syntactical tokens .
the models only leverage left context tokens to generate a prediction.
downstream tasks .we employ two downstream tasks to evaluate the ability of our plms of code to learn and adapt to new software data that introduce new unseen apis over time.
fig.
illustrates both tasks.
for api call prediction the model takes as input all the tokens of the method preceding the call site of the api and generates top kcandidates.
for api usage prediction the model takes as input the same tokens as for the api call prediction task but attempts to generate the whole api usage interface name method name parameters and syntactical tokens which constitutes a more challenging task.
the rationale for evaluating the plms on these two downstream tasks is to select tasks where prior knowledge about the apis seems decisive to effectively perform the task.
consequently the choice for these two tasks is highly relevant to our continual ood scenario and it allows us to directly measure the impact of ood apis on the effectiveness of the plms.
in section .
we discuss the applicability of our methodology to other code related tasks.
evaluation metrics .we measure the performance of the models on both downstream tasks with metrics used in prior works.
for api call prediction we report the exact match k em k which gives the percentage of correct predictions when considering lists ofkcandidates.
for api usage prediction we report bleu score exact match em and codebleu .
to measure how the models perform in a continual learning environment we use two meta metrics adapted from prior works the average a andforgetting f metrics.
we define the averageamof a metricmon a test datasetdi oodas am tt j imj di ood wherejrefers to the next incremental learning steps after the i th included.mjdenotes an evaluation metric e.g.
em k computed at time step jon the test set and tdenotes the maximum number of fine tuning steps i.e.
five in our case.
the average metric only gives information on how accurate the model is but does not provide any insight into its ability to mitigate catastrophic forgetting.
we definetable api call prediction results in zero shot using mdec.
metrics dataset em em em did test .
.
.
dood .
.
.
.
.
d1 ood49.
.
.
.
.
.
d2 ood53.
.
.
.
.
.
d3 ood23.
.
.
.
.
.
d4 ood30.
.
.
.
.
d5 ood37.
.
.
.
.
.
the forgetting fk mof a metric mon a test datasetdi oodat time stepkas fk m mi di ood mk di ood i k. this is the difference between the first time the metric is computed i.e.
after fine tuning the model on di oodat time step i and the metric computed at time step k.fk mgives information on the stability of the model i.e.
its capability to not forget from the past.
therefore the lower fk m the better.
implementation details .to pre trainmdecandmenc we used four tesla v100 sxm2 32gb gpus.
it took about days to pretrainmdec and days to pre train menc.
for fine tuning and inference we used a single tesla v100 sxm2 32gb gpu.
we used huggingface s libraries to implement the models and store the datasets.
to implement the continual learning approaches we used avalanche .
we provide all the implementation details of our experiments and release our data publicly in our replication package see data availability section .
experimental results .
how doesmdecgeneralize to id and ood data in zero shot?
in this experiment we evaluate the performance of the model mdec on the id and ood test data in a zero shot setting for both downstream tasks.
we do not experiment with mencas the model is not capable of generating code before fine tuning and therefore cannot operate in a zero shot setting.
the purpose of this experiment is twofold.
first it aims to validate the experimental setup of our study.
if we observe significant differences in the evaluation metrics obtained on the id and ood datasets it would suggest that our ood scenario is well formed and reasonable.
secondly significant gaps between the id and ood test data imply that plms such as mdecstill require the use of robust transfer learning or continual learning techniques to generalize to new data without forgetting about past data.
api call prediction .table reports the em em and em on the id and ood test datasets.
the results show that the model performs well on id data reaching almost in em .
however when tested on ood data the performance drops significantly.
the decline in performance is less severe when considering moreesec fse december san francisco ca usa weyssow et al.
table api usage prediction results in zero shot using mdec.
metrics dataset bleu em codebleu did test .
.
.
dood .
.
.
.
.
.
d1 ood5.
.
.
.
.
.
d2 ood11.
.
.
.
.
.
d3 ood7.
.
.
.
.
d4 ood15.
.
.
.
.
.
d5 ood5.
.
.
.
.
.
api call candidates but it remains a significant issue.
furthermore variations in the performance decline are observed across different ood datasets.
for example the model performs better on the security domain d2 ood than domains such as android d3 ood or web d4 ood which likely contain more domain specific api calls.
api usage prediction .table reports the bleu score em and codebleu score on both id and ood test datasets.
the results indicate that the model performs poorly on ood data in comparison to id data with significant decreases in all evaluation metrics.
additionally we notice that the em and codebleu metrics vary similarly to the em k metrics on the api call prediction task.
the android and web domains experience the most severe drops whereas the security domain experiences the least severe drop.
our results demonstrate that the model mdec without finetuning is unable to generalize to ood data while showing strong performance on id data.
our findings also support the validity of our ood dataset as a realistic and meaningful test of the model s ability to adapt to new data in a continuous environment.
.
do models forget about past data using classical transfer learning?
in this section we evaluate how classical transfer learning i.e.
using fine tuning as in prior work performs in the continual learning scenario.
we fine tune the models mdecandmencsequentially on the stream of ood datasets d1 ood ... d5 ood.
we refer to this approach as naive fine tuning a common term used in the continual learning literature to refer to classical transfer learning as it does not utilize mechanisms to address catastrophic forgetting.
we report the results in terms of em for api call prediction and em for api usage prediction.
fig.
illustrates the evolution of the em and em metrics on the ood test sets throughout the fine tuning steps for both models.
each column of a heatmap refers to the evolution of the performance of the model on a particular test set and each row refers to a new incremental fine tuning step.
note that we do not compute the metric on a test set whose corresponding training set has not been seen yet by the model.
to quantify catastrophic forgetting we report the forgetting f metrics of the em and em metrics in table .
we do not report all the valuestable forgetting metrics for the naive fine tuning baseline.
model dataset f5 em 1f5 em mdecgeneral t .
.
security t .
.
android t .
.
web t .
.
mencgeneral t .
.
security t .
.
android t .
.
web t .
.
for every previously introduced metric as we have a strict page limit and report them in our replication package.
fine tuning details .at each time step t we fine tune the models checkpoints from the previous time step on the dataset dt ood.
we select of the training samples from each ood dataset as a validation set.
for each fine tuning we set the number of training epochs to and use early stopping by monitoring the evolution of the validation loss with a patience of two epochs.
we keep the best checkpoints of the models at each fine tuning step tand compute the task metrics on the previous and current test sets.
performance ofmdecandmenc .in fig.
each heatmap depicts the evolution of a metric on the test sets for a single model on one task.
the diagonal values in the heatmaps indicate the metric computed on the test set of the current ood dataset.
we observe substantial catastrophic forgetting for both tasks and models and all domains and metrics.
that is we observe a decline of the metrics in all columns indicating that the model forgets the previous domains when fine tuned on a new domain.
for example the em on d1 ood general drops from .
to51.
formdec.
another example is the em on d2 ood security dropping from .
to .
for the modelmenc.
a glance at the heatmaps suggests that the forgetting is more severe for the encoder menc.
overall as we increase the number of fine tuning steps the forgetting further intensifies in most cases.
in addition for the decoder the decline in the metrics after one fine tuning step is less significant compared to the encoder.
for example after one fine tuning step the em ond2 ooddrops from .
to57.
.
for the decoder.
whereas it drops from .
to32.
.
for the encoder.
this means that more fine tuning steps are required for the decoder to forget about past data more severely whereas for the encoder one fine tuning step is already enough to show a significant decline in performance.
this observation confirms our intuition expressed in section .
thatmencmay be less stable than mdecdue to the additional language modeling head randomly initialized.
forgetting metrics .in table we calculate the forgetting metric for theem 1andemmetrics and for both models.
note that we calculate the fmetric at the final time step of the continual fine tuning.
according to the heatmaps of fig.
the f5metric of a domain is the difference between the first and last value of its corresponding column.
this difference represents the amount of forgetting that has occurred on each ood domain during finetuning.
the tin the table indicates how recently the model wason the usage of continual learning for out of distribution generalization in pre trained language models ... esec fse december san francisco ca usa general security android web guavageneral security android web guava57.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
em a mdec api call prediction.
general security android web guavageneral security android web guava48.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
em b menc api call prediction.
general security android web guavageneral security android web guava45.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
em c mdec api usage prediction.
general security android web guavageneral security android web guava31.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
em d menc api usage prediction.
fig.
naive fine tuning approach results.
fine tuned on a particular domain dataset.
we notice that for the decodermdec the forgetting is less severe for the em used in the api call prediction than for the em used in the api usage prediction .
the difference can be attributed to the fact that the api call prediction task is substantially easier than the api usage prediction task.
in general we observe more severe forgetting for the encoder which further confirms our intuition about the lack of stability ofmenc.
our results and observations illustrate that the problem of forgetting about past data is a major issue for both studied models and significantly more severe for the model menc.
even with a low number of fine tuning steps catastrophic forgetting is already prominent.
by considering more fine tuning steps we can expect the problem to exacerbate.
we conclude that classical transfer learning the most commonly used fine tuning method in prior work is not sufficient and robust enough to allow the model to adapt to new data while retaining knowledge of past data.
.
how do continual learning approaches compare to classical transfer learning?
to tackle the problem of catastrophic forgetting highlighted in our previous experiments we propose to leverage some commonly used continual learning approaches from the literature.
in this experiment the naive fine tuning approach is the lower bound baseline as it has no designed mechanism to mitigate catastrophic forgetting.
we begin by introducing an upper bound approach referred to as gid00034 gid00049 gid00042 gid00001 gid00066 gid00064 gid00075 gid00075 gid00001 gid00079 gid00081 gid00068 gid00067 gid00072 gid00066 gid00083 gid00072 gid00078 gid00077 gid00001 gid00009 gid00049 gid00064 gid00082 gid00082 gid00033 gid00018 gid00010 gid00037 gid00068 gid00066 gid00078 gid00067 gid00068 gid00081 gid00001 gid00009 gid00077 gid00064 gid00072 gid00085 gid00068 gid00010 gid00038 gid00077 gid00066 gid00078 gid00067 gid00068 gid00081 gid00001 gid00009 gid00077 gid00064 gid00072 gid00085 gid00068 gid00010 gid00038 gid00077 gid00066 gid00078 gid00067 gid00068 gid00081 gid00001 gid00009 gid00066 gid00084 gid00076 gid00084 gid00075 gid00064 gid00083 gid00072 gid00085 gid00068 gid00010 gid00037 gid00068 gid00066 gid00078 gid00067 gid00068 gid00081 gid00001 gid00009 gid00066 gid00084 gid00076 gid00084 gid00075 gid00064 gid00083 gid00072 gid00085 gid00068 gid00010powered by tcpdf by tcpdf by tcpdf by tcpdf comparison of naive and cumulative fine tuning settings for both models on api call prediction em .
cumulative fine tuning which involves storing all training samples from each ood training set cumulatively.
with this approach we perform continual fine tuning using all samples from previous fine tuning steps in addition to the current ones.
this approach is usually upper bound in continual learning settings as by storing all samples from previous data the model can optimize its learning to generalize better to the whole stream of data.
however the cumulative fine tuning approach is not usable in practice for a couple of reasons we may not always have access to all previous data at any given time and it requires storing all previous samples and significantly more computations during fine tuning.
this upperbound approach aims to minimize forgetting while achieving theesec fse december san francisco ca usa weyssow et al.
gid00037 gid00068 gid00066 gid00078 gid00067 gid00068 gid00081 gid00001 gid00009 gid00077 gid00064 gid00072 gid00085 gid00068 gid00010 gid00038 gid00077 gid00066 gid00078 gid00067 gid00068 gid00081 gid00001 gid00009 gid00077 gid00064 gid00072 gid00085 gid00068 gid00010 gid00038 gid00077 gid00066 gid00078 gid00067 gid00068 gid00081 gid00001 gid00009 gid00066 gid00084 gid00076 gid00084 gid00075 gid00064 gid00083 gid00072 gid00085 gid00068 gid00010 gid00037 gid00068 gid00066 gid00078 gid00067 gid00068 gid00081 gid00001 gid00009 gid00066 gid00084 gid00076 gid00084 gid00075 gid00064 gid00083 gid00072 gid00085 gid00068 gid00010 gid00034 gid00049 gid00042 gid00001 gid00084 gid00082 gid00064 gid00070 gid00068 gid00001 gid00079 gid00081 gid00068 gid00067 gid00072 gid00066 gid00083 gid00072 gid00078 gid00077 gid00001 gid00009 gid00038 gid00046 gid00010powered by tcpdf by tcpdf by tcpdf fig.
comparison of naive and cumulative fine tuning settings for both models on api usage prediction em .
best overall performance.
we compare the cumulative and naive approaches in fig.
and fig.
.
next we introduce additional cl methods including a replay based method and three regularizationbased methods ewc si and rwalk .
one advantage of these three methods over the replay method is that they do not require storing samples from previous data while fine tuning.
we report the average a and forgetting f metrics for both tasks and models on the em and em metrics in table and table .
note that there is no forgetting metric for guava as it is the last domain the plms are fine tuned on.
fine tuning details .we use the same fine tuning procedure as in the previous experiment.
for the replay baseline we set the buffer size to i.e.
number of sampled stored from past ood training sets.
we provide all our hyperparameters and further details about the implementations in our replication package.
cumulative fine tuning .in fig.
we compare the naive and cumulative approaches for the api call prediction task em on both decoder and encoder models.
each curve illustrates the evolution of the em on a particular ood test set.
the figure further demonstrates how the naive approach bottom left part of the figure with the encoder leads to significantly more forgetting than for the decoder as previously discussed.
at the left of fig.
we observe that the cumulative fine tuning approach effectively eliminates the catastrophic forgetting issue for both models.
specifically the em does not decrease over time and even increases throughout the fine tuning indicating improvement during continual fine tuning also known as positive transfer.
in fig.
we make the same observations for the api usage prediction task on the em metric.
continual learning approaches .table reports the average and forgetting metrics of the em on each ood test set for mdec andmenc with the naive fine tuning approach as baseline.
similarly to section .
we compute the fmetric at the end of the continual fine tuning.
firstly we observe that for both models the cumulative fine tuning approach is the best option to mitigate catastrophic forgetting and generally leads to the best aem .
with the cumulative approach the f5 em 1metric is always negative which indicates a positive transfer an increase in the em .
for instance we get .02inf5 em 1formdecin the security domain i.e.
an increase of .02in the metric through fine tuning.
however we observe large gaps between the aem 1obtained using the cumulative approach and the naive approach on the guava dataset last fine tuning step .
we hypothesize that with an ever increasing replay buffer the models can no longer learn from new data and thus lose their ability to adapt with time.
in addition to being computationally intensive the cumulative fine tuning approach is not scalable and robust as previously mentioned.
overall all other cl approaches except ewc greatly reduce forgetting and show a superior average em compared to the naive approach.
the replay approach generally produces the best or second best aem .
without the cumulative approach rwalk is the best method to mitigate forgetting formdec whereas si is better for menc.
in table we report the results for the api usage prediction task.
we observe similar trends except that the replay approach is less effective for both models.
however rwalk and si are the best methods for mdecandmenc respectively.
in this final experiment we demonstrate that continual learning methods including two replay based methods replay and cumulative and two regularization based methods si and rwalk effectively reduces catastrophic forgetting while achieving similar or superior effectiveness compared to classical transfer learning on both tasks.
discussion in this section we address some threats to the validity of our study.
we then discuss the broader impact of our study and various opportunities for future work.
.
threats to validity threats to external validity .we identified a main threat regarding the monolingual aspect of our dataset.
our ood scenario requires extracting api usage sequences from the source code.
therefore integrating more programming languages demands substantial additional effort which we deliberately leave for future work.
in addition the construction of our dataset does not include any programming language specific design and avoids any data leakage between the id and ood data.
consequently it is highly likely that our results are not affected by the programming language of the data.
another threat related to the data is the choice of the ood domains and apis.
to mitigate this threat we selected five domains covering different types of programs.
specifically we selected random interfaces per domain.
our results show that catastrophic forgetting is observed consistently for all domains and the selection of different interfaces would result in different intensities in forgetting.
we leave the study of this qualitative aspect for future work.
the choice of the downstream tasks presents another external threat to validity of our study.
we employed two generation tasks api call and api usage prediction.
we focus on apis related tasks because apis are an important part of the distribution of code tokens in programs and give lots of information about the semanticson the usage of continual learning for out of distribution generalization in pre trained language models ... esec fse december san francisco ca usa table continual learning approaches results for api call prediction using the em metric.
general security android web guava model method aem f5 em aem 1f5 em 1aem 1f5 em 1aem 1f5 em 1aem 1f5 em mdecnaive .
.
.
.
.
.
.
.
.
ewc .
.
.
.
.
.
.
.
.
si .
.
.
.
.
.
.
.
.
rwalk .
.
.
.
.
.
.
.
.
replay .
.
.
.
.
.
.
.
.
cumulative .
.
.
.
.
.
.
.
.
mencnaive .
.
.
.
.
.
.
.
.
ewc .
.
.
.
.
.
.
.
.
si .
.
.
.
.
.
.
.
.
rwalk .
.
.
.
.
.
.
.
.
replay .
.
.
.
.
.
.
.
.
cumulative .
.
.
.
.
.
.
.
.
table continual learning approaches results for api usage prediction using the em metric.
general security android web guava model method aem f5 em aemf5 emaemf5 emaemf5 emaemf5 em mdecnaive .
.
.
.
.
.
.
.
.
ewc .
.
.
.
.
.
.
.
.
si .
.
.
.
.
.
.
.
.
rwalk .
.
.
.
.
.
.
.
.
replay .
.
.
.
.
.
.
.
.
cumulative .
.
.
.
.
.
.
.
.
mencnaive .
.
.
.
.
.
.
.
.
ewc .
.
.
.
.
.
.
.
.
si .
.
.
.
.
.
.
.
.
rwalk .
.
.
.
.
.
.
.
.
replay .
.
.
.
.
.
.
.
.
cumulative .
.
.
.
.
.
.
.
.
of programs.
we observe significant catastrophic forgetting in these two api related tasks and hypothesize that catastrophic forgetting could appear in other se tasks because of the importance of apis in code.
for instance previous work found that apis play important roles in writing the summarization of code detecting code clones retrieving code given a query etc.
we leave the investigation of the ood phenomenon in other tasks as future work.
we identified an external threat to validity related to the limited number of fine tuning steps in our continual fine tuning settings.
in practice a plm deployed to a real production environment would potentially face a larger number of fine tuning steps throughout its lifetime.
in this paper we showed that both plms suffer from severe catastrophic forgetting although we only consider five fine tuning steps.
we also demonstrated that more steps generally result in more forgetting about past data.
finally the selection of the size of the plms in terms of the number of trainable parameters constitutes a potential threat to the validity of our study.
while increasing the number of parameters may still result in ood generalization issues due to the design of our datasets it is uncertain whether catastrophic forgetting would occur with the same magnitude for larger models.
our experiments were performed under limited computational resources which required us to consider architectures with a limited numberof parameters.
to mitigate this threat we maximized the size of the models considering our limited resources.
we pre train plms with 110m and 125m parameters which are within the range of plms such as codebert codet5 or codegpt .
threats to internal validity .the hyperparameter choices for our cl approaches constitute the main threat to internal validity.
we selected our hyperparameters based on values used in prior works about continual learning .
these hyperparameters can be optimized for our scenario by using search methods which tend to have a high computational cost.
however this aspect is not critical to the study as we have already shown the advantages of incorporating continual learning techniques with reasonable hyperparameter values.
threats to construct validity .we identified one threat to construct validity related to the choice of our evaluation metrics.
we mitigate this threat by selecting metrics widely used in prior works to evaluate code generation tasks .
additionally we adapted continual learning metrics from prior works to evaluate our continual fine tuning scenario.
.
broader impact and opportunities our study sheds light on the performance of plms of code in a continual learning setting for out of distribution generalization.esec fse december san francisco ca usa weyssow et al.
we believe that this initial exploration of continual learning for code cl4code will inspire further investigation in this important area.
our findings highlight two potential areas for future research improving dataset and benchmark creation and expanding the application of cl4code to a wider range of use cases.
datasets and benchmarks .our findings in section .
highlight a substantial disparity in the performance of a plm between id and ood data.
our results along with a previous work indicate that evaluating plms on id data often leads to inflated metrics and results in overly optimistic conclusions in terms of the performance.
therefore it is crucial to develop ood datasets for code in order to evaluate the real world generalizability of plms as previously emphasized .
moreover aligning dataset designs with continual learning scenarios offers the potential to evaluate the plm s ability to adapt to changing environments which is crucial for practical deployment.
improving benchmarks for plms of code is another promising direction for future research.
benchmarks such as codexglue play a crucial role by providing standardized evaluations of models of code and enabling reproducible experimental results.
however as such researches progress at a rapid pace widely used benchmarks often become outdated quickly.
in particular kiela et al.
showed that benchmarks such as glue in nlp saturate meaning the milestones set by the benchmark are reached.
thus continued efforts to enhance benchmarks in deep learning for code are vital in establishing concrete goals and driving research to enhance the performance of the models being evaluated.
recently yang et al.
proposed glue x a comprehensive benchmark consisting of datasets to test plms on ood data across eight nlp tasks.
the benchmark includes ood datasets that are distinct from those in the original glue benchmark.
developing ood benchmarks for code similar to glue x would greatly contribute to the growth of research on ood generalization for plms of code.
one potential approach is to compile a new set of ood datasets that are not included in the existing codexglue benchmark and use them to test plms of code.
furthermore exploring the design of ood scenarios specific to software changes as demonstrated in the present study can provide a valuable foundation for future code benchmark initiatives.
our dataset and methodology for extracting ood samples for api evolution scenarios can serve as a starting point for these endeavors.
continual learning for code .our findings in section .
highlight the challenge of catastrophic forgetting that plms of code encounter in a continual fine tuning scenario with ood data.
our study serves as a starting point for exploring the adaptability of plms of code in a variety of continual learning scenarios.
for instance these scenarios can be based on domain adaptation where plms must adapt to new kinds of data such as new unseen programming languages or code repositories as discussed in prior studies .
additionally incorporating continual learning into a multi task learning framework is highly relevant to software engineering given the multitude of downstream tasks involved.
in section .
our results demonstrate the effectiveness of continual learning methods in mitigating catastrophic forgetting in plms of code.
we chose to explore these widely used methods as a first step in the research on continual learning for code.
in thefuture more sophisticated techniques from nlp as discussed in section .
can be evaluated.
furthermore the creation of continual learning methods specifically tailored to source code has the potential to further reduce catastrophic forgetting in plms of code.
finally we did not focus our study on large language models llms as it would require a tremendous amount of available computational resources to pre train an llm from scratch under our ood scenario.
nonetheless we foresee that continuously adapting llms to new emerging datasets and benchmarks constitutes an exciting avenue for future work.
in this context and as fully fine tuning llms is computationally costly we believe that combining continual learning with parameter efficient fine tuning peft techniques might be beneficial to further enhance the capabilities of llms.
these peft techniques have already shown promising results in llms for code intelligence .
related work .
out of distribution generalization natural language processing .recent studies have revealed that plms are susceptible to generating inaccurate predictions when encountering ood data .
in nlp this issue can manifest itself in situations where the domain of the test data differs from the pre training data .
one approach to addressing this problem is to fine tune plms on domain specific datasets using efficient transfer learning techniques.
for example demonstrated that such approaches help plms in learning domain specific knowledge and improve their generalization to unseen domains.
additionally new datasets and benchmarks allow for further research on plm domain adaptation.
for instance williams et al.
introduced the multinli dataset containing text data from a variety of domains for plm domain adaptation.
conneau et al.
proposed a cross lingual nli dataset for evaluating the cross lingual transferability of plms.
recently yang et al.
introduced glue x a benchmark for evaluating plms ability to generalize to ood data.
deep learning for code .the study of ood generalization of plms of code is an emerging research area.
assessing their generalizability and designing efficient techniques to improve their robustness to ood scenarios is essential for the practical usability of plms of code .
previous work in this field has focused on designing ood datasets that simulate specific distribution shifts of program data.
koh et al.
presented py150 wilds a python dataset in which the test data consists of code repositories not appearing in the training data.
the authors demonstrated performance gaps between the model on id and ood data.
however it is important to note that while the design choice is sound it may not reflect strong ood phenomena as the distribution of code tokens across different repositories may still be highly similar.
more recently hu et al.
proposed a benchmark to evaluate the performance of code models under different distribution shift scenarios including programmer time or token distribution shifts.
in their study the authors found that plms such as codebert were robust against distribution shifts.
however they demonstrated that on a simple classification task with small datasets.
in addition the authors did not control the pre training data of the studied plms which can result in important data leakage between the pre training and oodon the usage of continual learning for out of distribution generalization in pre trained language models ... esec fse december san francisco ca usa test data.
this problem of data leakage is critical as some of the test data may have been seen by the model during pre training.
overall this is a prime threat to the validity of the ood scenario that may lead to obtaining inflated metrics on the ood test data.
finally hajipour et al.
analyzed the performance of plms of code on a syntax based semantic based and complexity based ood scenario and highlighted that the models exhibit poor generalizability when faced with ood samples.
however it is important to point out that the ood scenarios used in this study may be too artificial.
for instance in the syntax based scenario some language specific tokens are masked at training to study how the model generalizes to unseen language tokens.
such a scenario is unrealistic as it does not reflect the nature of ood data that a plm of code is likely to encounter in the real world.
additionally there is no practical motivation for masking specific tokens while training the model.
in this study we propose an ood dataset that accurately represents the dynamic nature of software codebases in the real world.
specifically we focus on the scenario where a plm must adapt to new unseen apis over time a well established problem in the literature .
to ensure the validity of our experiments we thoroughly control our plm setup to prevent any data leakage between the pre training fine tuning and test data.
this allows us to create an ood generalization scenario that is as close to reality as possible an aspect that has been overlooked in previous works.
.
continual learning for pre trained language models continual learning has been studied to adapt pre trained language models based on the transformer architecture to new domains or tasks in nlp.
for example cao et al.
proposed a method to continually learn from new classes of events in textual data to detect them without degradation of the accuracy over time.
douillard et al.
introduced dytox a method that utilizes an encoderdecoder transformer for multiple tasks by expanding the network with task specific special tokens allowing for continual learning of new tasks with a low computational and memory footprint.
ermis et al.
proposed a memory efficient approach for transformers to continually learn new tasks by sharing information across tasks and expanding the network with task specific modules.
similarly vladymyrov et al.
proposed the hypertransformer architecture to continually learn new tasks by generating task specific convolutional neural network weights in a few shot learning setting and updating the task specific weights to avoid catastrophic forgetting.
lastly jie et al.
leverage continual learning to avoid representational shifts in plms by proposing a new hierarchical fine tuning method that prevents excessive changes in the representation spaces of the neural network in a continual fine tuning setting.
recent advances in nlp highlight the crucial need for plms to adapt to changing environments and maintain their performance on new data and tasks.
in the field of software engineering the application of continual learning to plms of code is essential for developing methods that enable the model to robustly adapt to new codebases and tasks over time.
to the best of our knowledge only a couple of prior studies utilized continual learning in the context ofcode intelligence.
baudry et al.
demonstrate the benefits of leveraging continual learning to fix bugs when considering a continuous stream of code change with continuous integration development platforms.
the scope of our study differ from this prior work in many aspects.
first contrary to this prior work our study focuses on plm architectures which broadens the potential applicability of our approach to a broader range of tasks.
secondly we compare numerous continual learning techniques in our ood scenario with plms whereas this previous work only consider a continual learning scenario without leveraging continual learning techniques such as replay buffer or ewc.
more recently gao et al.
made similar findings than ours by showing that plms suffer from catastrophic forgetting in continual learning scenarios and that replay based approaches allow to effectively mitigate forgetting.
we believe that these prior works and our study break new ground by introducing the first approaches on the utilization of continual learning for plms of code.
conclusion and future work our study exposes the limitations of pre trained language models of code in handling out of distribution data in a continual fine tuning scenario.
our results reveal that ood data significantly decreases the plms effectiveness in two api related downstream tasks compared to id data.
our findings indicate that classical transfer learning fails to adapt the plms to new unseen apis in this evolution scenario.
additionally we observe instances of catastrophic forgetting prompting us to explore methods that address this issue.
in our final experiments we demonstrate that replay based and regularization based continual learning techniques can effectively mitigate catastrophic forgetting while retaining or enhancing the performance of the plms in both downstream tasks.
in future work we intend to explore more ood scenarios to further evaluate the generalizability of plms of code and develop relevant ood generalization benchmarks for code.
additionally we plan to implement more advanced continual learning methods tailored to source code to enhance the adaptability of plms of code.
finally we aim to investigate ood detection methods to automatically identify ood data in plms thereby improving their performance.
data availability we publicly release all the code data and models to reproduce the experiments of our study.
the following repository contains instructions on how to acquire the data and pre train fine tune and test the plms