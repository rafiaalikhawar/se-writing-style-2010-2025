evaluating and improving neural program smoothing based fuzzing mingyuan wu southern university of science and technology shenzhen china and the university of hong kong hong kong china mail.sustech.edu.cnling jiang jiahong xiang yuqun zhang southern university of science and technology shenzhen china zhangyq mail.sustech.edu.cnguowei yang the university of queensland brisbane australia guowei.yang uq.edu.au huixin ma sen nie shi wu tencent security keen lab shanghai china huixinma snie shiwu tencent.comheming cui the university of hong kong hong kong china heming cs.hku.hklingming zhang university of illinois urbana champaign usa lingming illinois.edu abstract fuzzingnowadayshasbeencommonlymodeledasanoptimization problem e.g.
maximizingcodecoverageunderagiventimebudget via typical search based solutions such as evolutionary algorithms.
however such solutions are widely argued to cause inefficientcomputing resource usage i.e.
inefficient mutations.
to addressthis issue two neural program smoothing based fuzzers neuzz andmtfuzz have been recently proposed to approximate program branching behaviors via neural network models which input byte sequences of a seed and output vectors representing program branching behaviors.
moreover assuming that mutating the bytes with larger gradients can better explore branching behaviors they developstrategiestomutatesuchbytesforgeneratingnewseeds as test cases.
meanwhile although they have been shown to beeffective inthe originalpapers theywereonly evaluatedupon alimited dataset.
in addition it is still unclear how their key tech nical components and whether other factors can impact fuzzingperformance.
to further investigate neural program smoothing based fuzzing we first construct a large scale benchmark suite with a total of popular open source projects.
then we extensivelyevaluate neuzzandmtfuzzonsuchbenchmarks.
theevaluation results suggest that their edge coverage performance canbeunstable.moreover neitherneuralnetworkmodelsnormutation strategies can be consistently effective and the power of their gradient guidancemechanisms havebeencompromised.inspired mingyuan wu is also affiliated with the research institute of trustworthy autonomous systems shenzhen china.
yuqun zhang is the corresponding author.
he is also affiliated with the research institute of trustworthy autonomous systems shenzhen china and guangdong provincial key laboratory of brain inspired intelligent computation china.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa association for computing machinery.
acm isbn ... .
prefuzz which improvesneuralprogram smoothing basedfuzzerswitha resourceefficientedgeselectionmechanism toenhancetheirgradientguidanceanda probabilisticbyteselectionmechanism tofurtherboost mutationeffectiveness.ourevaluationresultsindicatethat prefuzz can significantly increase the edge coverage of neuzz mtfuzz and also reveal multiple practical guidelines to advance future research on neural program smoothing based fuzzing.
acm reference format mingyuanwu lingjiang jiahongxiang yuqunzhang guoweiyang huixin ma sen nie shi wu heming cui and lingming zhang.
.
evaluatingandimprovingneuralprogram smoothing basedfuzzing.in 44thinternationalconferenceonsoftwareengineering icse may21 pittsburgh pa usa.
acm new york ny usa pages.
https introduction fuzzing nowadays has been widely adopted to detect software bugs or vulnerabilities via feeding invalid unexpected or randomdata asinputsfor executingprogramsunder test.todate manyexistingapproaches model fuzzingasanoptimizationproblem and attempt to solve it by augmenting code coverage via mutating program seed inputs under a given time budget.
such coverage guided fuzzingtaskscanbetypicallyresolvedbyapplying search based optimization algorithms such as evolutionary algorithms .
specifically test inputs are iteratively filtered mutated andexecutedsuchthatthetestresultscanapproach the optimal solutions to satisfy the fitness functions of the adopted evolutionary algorithms which are usually designed to maximize code coverage.
however evolutionary fuzzers have been arguedthat they fail to leverage the structure i.e.
gradients or higherorderderivatives oftheunderlyingoptimizationproblem .to addresssuchissue neuralprogram smoothing basedtechniques e.g.
neuzz andmtfuzz havebeenrecentlyproposedto exploit the usage of gradients for fuzzing via neural network models.
specifically they first adopt a neural network which given the byte sequence of a seed as input outputs a vector representing its associatedprogrambranchingbehaviors.next theycomputethe ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa m. wu et al.
gradients of the collected output vectors with respect to the bytes ofthegivenseed.accordingly theysorttheresultinggradientsand developstrategiestomutatethebyteswithlargergradientsmore aggressively.eventually alltheresultingmutantsareusedastest casesforfuzzing.notethat mtfuzzfurtherattemptstooutperform neuzzby leveraging the power of multi task learning and adopts a dynamicanalysismoduletoaugmentthemutationstrategy.intheir original papers neuzzoutperforms existing coverage guided fuzzerson10real worldprojectsbyatleast3xmoreedgecoverage over24 hourrunsandfurtherdetects31previously unknownbugs.comparedto neuzzandfourotherstate of the artfuzzers mtfuzz achieves 2xto 3xedge coverageupon allthe benchmarkprojects andexposes11previously unknownbugswhichcannotbedetected by the other fuzzers.
despite the effectiveness shown in their original papers the evaluation on neuzzandmtfuzzcan be potentially biased due totheirlimitedbenchmarksuitewithonly10projects.moreover neuzzandmtfuzzadopt a different edge coverage metric from many existing fuzzers that can potentially bias the performance comparison.
furthermore the investigation on the factors that canimpact their edge coverage performanceis rather limited i.e.
they only simply presented the overall effectivenessof the techniques without investigating the contributions made byindividualcomponents e.g.
themodelstructure thegradient guidance mechanism and the mutation strategy.
in this paper to enhance the understanding of the effectiveness andefficiencyofprogram smoothing basedfuzzing wefirstconstructalarge scalebenchmarkbyretainingalltheprojectsadoptedintheoriginal neuzzandmtfuzzpapers exceptonethatwefailto run andadding19additionalopen sourceprojectsthatwerefrequently adopted in recent fuzzing research work.
we then conduct an extensive evaluation for neuzzandmtfuzzaccordingly.
the evaluationresultsuggestswhile neuzzandmtfuzzcanoutperform afl on all the studied benchmark projects by .
and .
on averageintermsofedgecoveragerespectively mtfuzzdoesnotalwaysoutperform neuzzandboththeiredgecoverageperformances arehighlyprogram dependent.wealsofindneithertheirmutation strategies nor neural network models can be consistently effective.
meanwhile although the gradient guidance mechanisms can be promising their strengths have not been fully leveraged.
inspiredbythefindingsofourstudy weproposeanimproved technique namely prefuzz upon neural program smoothingbased fuzzing.
in particular we develop a resource efficient edge selection mechanism to facilitate the exploration on unexplored edges rather than the already covered edges.
moreover we also applyaprobabilisticbyteselectionmechanism guidedbygradient informationto neuzzandmtfuzztofurtherboostedgeexploration.
ourevaluationresultssuggestthat prefuzzcansignificantlyoutperformneuzzandmtfuzz i.e.
.
more than neuzzand .
more than mtfuzzaveragely in terms of edge coverage.
to conclude this paper makes the following contributions dataset.
a dataset including real world projects that can be used as the benchmarks for future research on fuzzing.
study.an extensive study of neural program smoothingbasedfuzzersonthelarge scalebenchmarksuite withdetailed inspection of both their strengths and limitations.
technicalimprovement.
atechniqueimprovingneural program smoothing based fuzzers by combining a resourceefficient edge selection mechanism and aprobabilistic byte selection mechanism .
practicalguidelines.
multiplepracticalguidelinesforadvancing future program smoothing based fuzzing research.
background .
coverage guided fuzzers coverage guided fuzzers nowadays widely adopt evolutionary algorithms formutationstrategiessincetheycanbeadvancedin discovering program vulnerabilities without prior program knowledge.
in this section we first introduce the basic framework for evolutionaryalgorithms andthenillustratehowatypicalcoverageguided fuzzer afl integrates evolutionary algorithms.
.
.
evolutionaryalgorithm.
tosolveanoptimizationproblem anevolutionaryalgorithm ea adoptsoperationssuchasmutatingtheexistingsolutionstogeneratenewsolutions.amongsuch generated solutions an ea applies a fitness function to filter them based on their quality such that the remaining ones are retained as one population.
such process is iterated until hitting the preset time budget with the final population returned as the solutions for the optimization problem.
.
.
integrating fuzzing with ea.
coverage guided fuzzers often useincreasedcodecoverageasthefitnessfunctions.specifically they usually adopt edge coverage where an edgerefers to a basicblock wise transition e.g.
a conditional jump in programs to represent code coverage and retain only the seeds that can trigger newedgecoverageforfurthermutations.forinstance american fuzzy lop afl a widely used coverage guided fuzzer is launched by instrumenting programs such that it can acquire and store the edge coverage of each program seed input at runtime.
subsequently afl iterates and mutates each seed input according to its adopted evolutionary algorithm.
like most coverage guided fuzzers when running a seed increases edge coverage aflidentifiessuchseedasan interesting seedandretains it for further mutations.
note that the mutations in afl consist of two stages the deterministic stage afl deterministic and the havoc stage afl havoc .
in particular afl deterministic applies a fixed set of mutators e.g.
the bitflip arithmetic and interesting valuemutators for respectively mutating the bits of each existing interesting seed deterministically.
after afl deterministic all the collected interesting seeds are used to launch afl havocwhere randommutations i.e.
randomlychosenmutators areiteratively applied to the randomly selected bits of the seed inputs.
.
neural program smoothing based fuzzers programsmoothingreferstosettingupasmooth i.e.
differentiable surrogatefunctiontoapproximateprogrambranchingbehaviors with respect to program inputs .
while traditional program smoothing techniques can incur substantial performance overheads due to heavyweight symbolic analysis integrating such concept with neural network models can be rather powerful since theycanbeusedtocopewithhigh dimensionaloptimizationtasks authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
evaluating and improving neural program smoothing based fuzzing icse may pittsburgh pa usa i.e.
toresolve approximate complexandstructuredprogrambehaviors.tothisend neuzz andmtfuzz areproposedto smooth programs via neural network models and guide mutations by yielding the power of their gradients.
specifically to formulate theoptimizationproblemforfuzzing theprogrambranchingbehaviors are defined as a function f x wherexrepresents a seed input in terms of byte sequence and the solution is a vector representing its associated branching behaviors.
for instance a solution vector indicates that the first and the third edges have been accessed explored while the second one has not.
since f x is typically discrete smoothing programs i.e.
making f x differentiable is essential to cope with the usage of gradients.
we then illustrate the rationale behind neuzzandmtfuzz.
note that a program execution path i.e.
a sequence of edges can be determinedbythebytesequenceofaseedinput.accordingly an edgecanbeaccessed exploredwhenthevalueofitscorresponding bytessatisfiesitsaccesscondition.otherwise oneofits sibling edges i.e.
edgesunderonesharedprefixedge canbealternativelyaccessed.forinstance infigure1 edge e0canbeaccessedwhenthe valueofseed satisfiestheaccessconditionfor e0 i.e.
seed .
hence mutatingsuch seed canleadtoexploringanewbranching behavior i.e.
accessing e0 s sibling edge e1instead of e0.
.
x .
z x y .
if x seed .
y z x .
x .
else .
x y .
y .
.x y z x z x y if x seed y z x x x y y x y z e0 e1seed i 1i ... 1i figure1 anexampleofneuralprogram smoothingrationale neuzzandmtfuzzassumethatneuralnetworkmodelscanidentifythe promising byte s i.e.
thebyte s correspondingtotheaccess condition for a previously explored edge.
specifically the gradient of such byte s e.g.
seed in figure to the explored edge is supposed to be larger than other bytes after training illustrated insection2.
.
.accordingly mutatingsuchbyte s canindicate thattheaccessconditionofthecorrespondingedgemaynotbesat isfied i.e.
potentiallyexploringnew sibling edges.tosummarize neuzzandmtfuzzlearntoextracttheexistingbranchingbehaviors to explore new edges rather than predicting promising bytes forunseenedges.inparticular theirmechanismscommonlyconsistoftwosteps neuralprogramsmoothingandgradient guided mutations as shown in figure .
.
.
neural program smoothing.
neuzzandmtfuzzadopt an iterativetraining and mutationprocess.undereachiteration they train neural network models using interesting seed inputs collected in real time out of the seed corpus in figure .
note that figure also shows that neuzzandmtfuzzadopt different neural network models which will be further illustrated in section .
.
.
...... e0 b0 e0 bm en b0 en bm gradient calculation .
sorting gradient guided mutation crackcontext sensitiveedge cov.
approch sensitive seed corpus mutants mutantsneuzz .
mtfuzz mtfuzz mtfuzz mtfuzz neuzz .
mtfuzzstage i neural program smoothing stage ii gradient guided mutationupdate seed corpus figure framework of neuzzandmtfuzz .
.
gradient guidedmutations.
afterobtainingtheneuralnetworkmodels neuzzandmtfuzzrandomlyselectadeterministic number of the interesting seeds and the explored edges.
for each selected seed they calculate the gradients of the selected edges vectors with respect to all the bytes.
furthermore all such bytes are sorted according to their corresponding gradient rankings and then aggregated as one vector for further mutations.
in particular neuzzandmtfuzzsegment each selected seed such that the bytes inthefrontsegmentshavelargergradientsthanthebytesinthe backsegmentsandthefrontsegmentsincludefewerbytesthanthe back segments.
accordingly the promising bytes are expected to be located in the front segments.
for any segment seg all its bytes aresimultaneouslymutatedfor255times.asaresult neuzzand mtfuzzcan explore more mutation space of the front segments than the back ones i.e.
mutating the more promising bytes moreaggressively forexploringnewbranchingbehaviors.eventually alltheresultingseedsaftertheiterativetraining and mutationprocess are used as test cases for fuzzing.
.
.
mtfuzzvs.neuzz.
figure2alsodemonstratesthat mtfuzz differs from neuzzby adopting multi task learning technique and a dynamic analysis module to augment its mutation strategy.
inadditiontothewidely usededgecoverage mtfuzzadoptstwo additionaltasks theapproach sensitiveedgecoverage i.e.
howfar offanunexplorededgeisfromgettingtriggered andthecontextsensitive edge coverage i.e.
the context for an explored edge to construct the multi task neural network model for smoothing programsandfurtherguidingfuzzing.moreover mtfuzzadoptsan independentmodule namely crackinitsimplementation which uses dynamic programanalysis to explore new edges withoutgradient information.
specifically crackiterates each byte of the seed inputandmutatesittoobservewhetherthevariablesassociated with an unexplored branch can be also changed.
if so such byte is identified as a promising byte to be mutated for times.
extensive study .
benchmarks although neuzzandmtfuzzhave been shown to outperform the existing fuzzers in terms of the edge coverage in the original papers suchresultscanbepossiblybiasedbytheusedsubject authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa m. wu et al.
table statistics of the studied benchmarks benchmark class loc package bison lex y a c c .
xmlwf xml expat .
.
mupdf pdf .
.
pngimage png libpng .
.
pngfix png libpng .
.
pngtest png libpng .
.
tcpdump pcap .
.
nasm asm nasm .
.
tiff2pdf tiff libtiff .
.
tiff2ps tiff libtiff .
.
tiffdump tiff libtiff .
.
tiffinfo tiff libtiff .
.
libxml xml .
.
listaction swf 278libming .
.
listaction d swf 272libming .
.
libsass scss libsass .
.
jhead jpeg .
readelf elf binutils2.
nm elf binutils2.
strip elf binutils2.
size elf binutils2.
objdump elf binutils2.
libjpeg jpeg 9c harfbuzz ttf .
.
base64 file lava m md5sum file lava m uniq file lava m who file lava m projects.
for example popular real world projects are the main experimentalsubjectsforboth neuzzandmtfuzz ho w e v er itis notclearhowsuch10projectsareselectedandwhethertheexperimental findings can generalize to other real world projects.
to reduce such threat we extend the benchmark for evaluatingneuzzandmtfuzz.
in particular in addition to retaining the adopted9projectsintheoriginalpapers wecouldnotsuccessfully runproject zliboutofthe10originalprojects wealsoadoptadditional19projectsforourextendedevaluations.morespecifically to extend our benchmark projects we first investigate all the fuzzing papers published in icse issta fse ase s p ccs usenixsecurity and ndss in year and collect all their benchmark projects.next wesortthecollectedbenchmarkprojectsintermsof their usage in all the collected papers presented in .
we then collectthetop30mostusedbenchmarkprojectsandsuccessfully run19ofthemwhichareeventuallyincludedinourextendedbenchmarks thefailedexecutions aremainly causedby environmental inconsistencies and unavailable dependencies .
table presents the statistics of our adopted benchmarks.
specifically we consider our benchmark to be sufficient and representative due to followingreasons tothebestofourknowledge thisisaratherlarge scale benchmark suite compared with prior work the collected benchmarkscover12differentfileformatsforseedinputs e.g.
elf xml and jpeg and the loc of each program ranging from to over 120k represents a wide range of program sizes.
.
evaluation setups we conduct all our evaluations on linux version .
.
generic ubuntu18.04withrtx2080ti.followingtheevaluationsetupsof neuzzandmtfuzz foreachselectedbenchmarkproject wefirst run afl .57b on a single cpu core for hour to initialize ourseedcollection andthenrun neuzz mtfuzzandall theirvariants introducedin latersections uponthe collectedseedswith atime budgetof24hours.notethatalltheedgeswithinthe1 hourinitial seedcollectionareexcludedfromtheevaluationresultsintheremainingsessions.moreover werunourexperimentsfor5timesforeachfuzzerandpresenttheaverageresultswithcloseperformance underdifferentruns.notethatweinstrumentallthebenchmark projects with afl gcc to acquire runtime edge coverage.
in addition to studying neuzzandmtfuzz we also include afl asabaselinetechniquethroughoutourextensiveevaluationsbe cause afl is widely adopted as baseline by many fuzzing approaches andfrequentlyupgradedforimproving its performance and neuzzadopts multiple concepts originated from afl for its implementation .
.
research questions weinvestigatethefollowingresearchquestionstoextensivelystudy neural program smoothing based fuzzing.
rq1 howdoneuzzandmtfuzzperformonalarge scale dataset?forthisrq weinvestigatetheireffectivenessand efficiencyofedgeexplorationunderourlarge scalebenchmark suite.
rq2 how do the key components of neuzzandmtfuzzaffectedgeexploration?forthisrq weattempttoinvestigate howexactlytheiradoptedgradientguidancemechanisms neuralnetworkmodels andmutationstrategiescanaffect edge exploration.
.
results and analysis .
.
rq1 performance of neuzz and mtfuzz on a large scale dataset.we first investigate the edge coverage performance of all thestudiedfuzzers.inthispaper followingmanyexistingcoverageguided fuzzers we determine to adopt the number oftheedgesvia afl showmap asourdefaultedgemetric.moreover note that the edge metric of the original neuzzandmtfuzzpapers can be potentially biased since it counts the byte number of the trace bits structureimplementedbyaflandthusisinconsistent with the results provided by the guidance function i.e.
defining interesting seedsmentionedinsection2.
.
intheirimplementations.nevertheless asacomprehensivestudy wealsoevaluate all the studied fuzzers in terms of the edge metric of the original neuzzandmtfuzzpapers.
table2presentstheedgecoverageresultsofourextensivestudy forneuzzandmtfuzzunder bothadopted metrics.
for instance foraflunder bison 374correspondstoourdefaultedgemetric and corresponds to the original metric in the neuzz mtfuzz papers.
for our default edge metric we can observe that neuzz significantly outperforms afl by .
vs. explored edges intermsofedgecoverageonaverage.comparedwiththe performanceadvantage claimedinits originalpaper i.e.
.7x it isclearlydegraded.wetheninvestigatetheperformancedifferenceamongbenchmarkprojects.interestingly wecanobservethattheir performance advantage is rather inconsistent i.e.
ranging from .
to180.
.moreover neuzzonlyoutperformsaflupon10 outof19extendedprojects.suchresultssuggestthat neuzzcannot authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
evaluating and improving neural program smoothing based fuzzing icse may pittsburgh pa usa table edge coverage results of all the studied approaches benchmarks afl neuzz neuzz rev mtfuzz mtfuzz rev mtfuzz off neuzz cnn neuzz rnn neuzz brnn bison xmlwf mupdf pngimage pngfix pngtest tcpdump nasm tiff2pdf tiff2ps tiffdump tiffinfo libxml listaction listaction d libsass jhead readelf nm strip size objdump libjpeg harfbuzz base64 md5sum uniq who average a neuzz afl b mtfuzz afl figure edge coverage advantage of the fuzzers over afl always outperformafl andthe performance advantageof neuzz over afl can be program dependent.
wealsoobservethat neuzzoutperforms mtfuzzby1.
vs. explorededges averagely in termsof edge coverage on all benchmark projects.
while on of total projects mtfuzz outperforms neuzzby .
averagely neuzzoutperforms mtfuzzby .
on the other projects.
furthermore even afl outperforms mtfuzzby33.
averagelyonatotalof11projects.
suchresultsindicatethatsimilarto neuzz mtfuzzcannotperform consistently either.
we then attempt to reveal the characteristics of how the edge coverageperformancevariesamongthestudiedprojects.tothis end we delineate the correlation between the edge coverage advantage of the studied fuzzers compared with afl and the size of their studied projects via the pearson correlation coefficient analysis .figure3presentssuchresultsof neuzzandmtfuzz.in each subfigure the horizontal axis denotes the loc of each project andtheverticalaxisdenotestheratioasdividingtheedgecoverage result of each studied approach by the edge coverage result of afl.
we can observe that overall the correlation is rather strong at the significance level of .
i.e.
all the studied approaches can result inlargeredgecoverageimprovementoverafluponlargerprojectsthansmallerones.suchresultsclearlydemonstratethatprogram size can significantly impact the edge coverage performance of neural program smoothing based fuzzers.
weobservesimilardatatrendsintermsoftheedgemetricinthe originalneuzz mtfuzzpapers.inparticular neuzzcanoutperform afl by .
vs. explored edges and can outperform mtfuzzby1.
219vs.
180explorededges .notethatunder such measure for certain projects e.g.
base64 neuzzandmtfuzz explore zero edges after excluding the edges from hour initial seedcollection.suchresultscouldbemisleadingthatthestudied fuzzersperformequallypoorin base64 whilesuchperformance gaps can be clearly presented by our default edge metric.
finding the performance of neuzz and mtfuzz canbe largely program dependent.
interestingly such programsmoothing based fuzzers tend to perform better on larger programs.
notethatrandomnessisinjectedtomanyexistingfuzzers for selecting bytesto guide mutations e.g.
aflhavoc.h o w ever neuzzandmtfuzzutilize only deterministic mutation strategies i.e.
adopting no randomness for selecting bytes which can be deterministically identified based on their corresponding gradient ranking.
therefore we further investigate the edge exploration efficiencyofrandombyteselectiontoinferwhetherincludingthem inneuzzandmtfuzzcan be potentially beneficial.
specifically we involveaflinafine grainedmanner i.e.
itsdeterministicstage afl deterministic and the havoc stage afl havoc i.e.
essentially therandombyteselectionmechanism bothofwhichenablenondeterministic execution time for performance comparison with neuzzandmtfuzz.
figure presents our evaluation results in terms of the explored edge number per second namely edge discovery rate edr in this authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa m. wu et al.
paper ofneuzz mtfuzz afl afldeterministic andaflhavoc.w e canobservethatoverall neuzzandmtfuzzcanoutperformaflby .
and8.
respectively.interestingly afl havocachievesthe highest edr i.e.
.8x larger than afl deterministic .7x larger thanneuzz and7.8xlargerthan mtfuzzaveragelyonallthebenchmarks.
accordingly we can derive that aflhavoccan significantly augment edge exploration i.e.
it promptly explores edges upon thelimitedseedinputsprovidedby afl deterministic .suchresult is enlightening that applying random byte selection mechanism to neural program smoothing fuzzers can potentially boost edge exploration.
finding afl havocdominates the efficiency of edge exploration indicating that it is promising to augment edge exploration by adopting random byte selection mechanism.
.
.
.
.
edr edr edr edr edr0.
.
.
.
.
afl afldeterministic aflhavoc neuzzedge discover rate mtfuzz figure edr of the studied approaches .
.
rq2 effectiveness of the key components.
gradientguidance.
theinconsistenciesbetweenourfinding andthedeclaredresultsintheoriginalpapers i.e.
finding1 inspire us to further investigate the performance impact of the adopted mechanisms of neuzzandmtfuzz.
to this end we determine to firstinvestigatetheeffectivenessoftheirdominatingfactor i.e.
thegradient guidance mechanism.
in particular since such mechanismis proposed to facilitate the mutations on the promising bytes foredgeexplorationviagradientcomputation ourpurposeistoinvestigate whether their derived gradients can locate such bytes.
more specifically weproposeanintuitivegradientguidancemechanism instead of aggressively mutating the bytes with larger gradients in the original neuzzandmtfuzz we aggressivelymutate the bytes withsmallergradients.suchmechanismisinjectedto neuzzand mtfuzztoform theirvariants neuzz revandmtfuzz rev.wethus evaluateneuzz revandmtfuzz revto observe their performance differencefromtheoriginal neuzzandmtfuzztoinvestigatethe effect of the gradient guidance mechanisms.
wecanobservefromtable2that neuzzcanexplore8.
moreedgesthan neuzz revandmtfuzzcanexplore10.
more edges than mtfuzz revon average.
such consistent results suggest that larger gradients can be a better indicator to promising bytes i.e.
the derived gradients can reflect promising bytes.
interestingly neuzz revcan outperform neuzzon out of projects i.e.
libxml mupdf jhead tcpdumpandpngtest.meanwhile mtfuzz revcan outperform mtfuzzunder uniqandwho.
suchresultsalsoindicatethatthepowerofthegradientguidance inneuzzandmtfuzzhas not been completely leveraged.finding although the gradient guidance mechanisms adoptedbyneuzzandmtfuzzareoveralleffectiveforidentifyingthepromisingbytes theirperformancecanberather unstable on some programs.
dnnmodels.
nowthatthegradientsderivedby neuzzandmtfuzzcan be proven to be effective in reflecting promising bytes for mutations we further investigate how their corresponding neural networkmodelsimpactedgeexploration.specifically sincecompared toneuzz mtfuzzenables the independent dynamic analysis modulecrackto augment their mutation strategy we turn it off andformitsvariant mtfuzz of f i.e.
applyingthemutationstrategyofneuzzinmtfuzz suchthattheyonlydifferintheadopted neural network models.
moreover we also include the convolutional neural network cnn model and two commonly used recursive neural network rnn models i.e.
lstm and bi lstm and adopt them in the original neuzzto form its variantsneuzz cnn neuzz rnn andneuzz brnn.
note that we investigatemore rnn based modelssincetheyare typicallyused inlearning thedistribution over asequenceto predictthe future symbolsequence e.g.
forspeechrecognition andexpectedto better match the program input features than cnn based models.
eventually we determine to evaluate neuzzand all the variant techniquestodetecthowmultipleneuralnetworkmodelsimpact the edge exploration of program smoothing based fuzzers.
notethat their hyper parameter setups are introduced in our github page .
we can observe from table that overall all our studied approaches perform similarly in terms of edge coverage.
specifically neuzzslightlyoutperforms mtfuzz of fby8.
395vs20 648explorededges underperforms neuzz cnnby1.
395vs.
explored edges neuzz rnnby .
vs. explored edges and neuzz brnnby .
vs. explored edges .
meanwhile wecanalsoobservethatnoneofthestudiedapproachescandominateontopofallthestudiedprojects i.e.
neuzzdominates mtfuzz of fdominates neuzz cnndominates neuzz rnn dominates and neuzz brnndominates .
therefore we derive that upgrading neural network models cannot significantly impact the performance of edge exploration.
finding4 differentneuralnetworkmodelshavelimitedimpactontheeffectivenessofprogram smoothing basedfuzzing.
mutation strategies.
we then investigate the impact from the mutation strategy of the neural program smoothing based fuzzers.
specifically since mtfuzzdiffersfrom neuzzmainlybyenabling crackformutationsandtheirrespectiveneuralnetworkmodelsdo not significantly impact the edge exploration reflected by finding we concentrate our investigation on the impact from crack.t o this end we evaluate mtfuzzandmtfuzz of f. table demonstrates that overall mtfuzzcan outperform mtfuzz of fby .
070vs.
648explorededges .however suchadvantagecanbe rather inconsistent ranging from .
to .
upon individual projects.ontheotherhand applying crackcanbepotentiallycostineffectivesinceitisquiteheavyweight.therefore itisessential authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
evaluating and improving neural program smoothing based fuzzing icse may pittsburgh pa usa to consider whether it is worthwhile in applying such technique for neural program smoothing based fuzzing.
finding the dynamic analysis module crack adopted by mtfuzz can be cost ineffective.
.
discussion we first discuss why neural network models do not significantly impact the edge coverage performance.
to this end we ought to understand the effect of the adopted neural network models ofneuzzandmtfuzz.
in particular note that neural networks are usually used for data prediction i.e.
learning and generalizinghistorical data to predict unseen data.
accordingly researchers havedevelopedmanyneuralnetworkmodelstostrengthentheir generalization and prediction capabilities.
therefore one may misunderstand that neuzzandmtfuzzattempt to use neural network models to predict the bytes corresponding to unexplored edges.
instead as a matter of fact neuzzandmtfuzzleverage neural networkmodelswhichcomputethegradientstoreflecttherelations between explored edges and seed inputs i.e.
mutating the byte correspondingtoalargergradientcanbemorelikelytoexplorea newedgeotherthantheexistingedgeunderonesharedprefixedge.
as a result any neural network model can be applied as long as it cansuccessfullydelivergradientstoreflectsuch explorededge seed inputrelations i.e.
how its generalization or prediction capability doesnotquitematterundersuchscenarios.therefore itisquite likely that a simplistic model e.g.
feed forwarded network model adopted by neuzz can perform similarly as fine grained models e.g.
multi tasklearningmodeladoptedby mtfuzzandthernn models adopted by the studied neuzzvariants .
we then attempt to illustrate why neuzzandmtfuzzcannot alwaysbeeffective.notethateventhough neuzzandmtfuzzenable gradientguidancemechanismstoexplorenewedges theiriterative training and mutationstrategyviarandomlyselectingedgesand seeds in the beginning can nevertheless select existing edges otherthan unexplored edges to compute gradients illustrated in section .
.
i.e.
they still allow inefficient mutations.
specifically for the smaller programs where neuzzandmtfuzzcannot outperform afl their edge exploration converges faster than larger programs due to the limited number of edges i.e.
they have a higher chance to select an existing edge whose sibling edges have already been exploredbyotherseedsforgradientcomputation.thus itcanbe difficult to mutate its promising bytes for exploring new edges.
4prefuzz our findings reveal that we can possibly leverage the power of the gradientguidancemechanismtoenhancetheedgeexplorationof neural program smoothing based fuzzers.
to this end we propose prefuzz probabilistic resource efficientprogram smoothing based fuzzing .figure5presentstheworkflowof prefuzz.prefuzzfirst trains a neural network model by applying all the existing seedsas the training set.
next prefuzzadopts a resource efficient edge selectionmechanism toselectedgesforgradientcomputation.then thegradientinformationisutilizedtogeneratemutantsforfuzzing.notethatamutantwhichexploresnewedgescanbeusedasaseed...... gradient calculation .
sorting seed corpus mutants m tants gradient guided mutation pbs prefuzz neuzz .
mtfuzzstage i neural program smoothing stage ii gradient guided mutationupdate seed corpus e0 b0 e0 bm en b0 en bm resource esosoce ff fifi f fffent ciece edge selection mechanism en mec i nn training figure framework of prefuzz for further edge exploration.
meanwhile prefuzzadoptsprobabilisticbyteselectionmechanism pbsinfigure5 tofacilitatemutations.
.
the details .
.
resource efficientedgeselectionmechanism.
thepurposeof theresource efficient edge selection mechanism is to prevent exploringtheexistingbranchingbehaviors i.e.
edges .tothisend our mechanism is designed to identify the edge worthy being explored for later selecting and mutating its corresponding byte.
intuitively when one edge can identify the number of its sibling edges as defined in section .
such edge number can be a potentialindicator whether the given edge should be included for further gradient computation.
more specifically the more sibling edges have beenexplored thelesslikelynew sibling edgescanbeexploredviathe gradient computation for the given edge.
algorithm presents the details of the resource efficient edge selection mechanism .
first it is quite essential to acquire the runtime edge exploration states e.g.
the number of sibling edges of agivenedgeandhowmanyhavebeenexplored lines2to3 .to thisend wedecompiletheassembly levelprograms parsethem to the instructions via afl specific instrumentation and construct theedgeexplorationstatesviastaticallyanalyzingtheparsedinstructions.
next given one edge we derive the ratio of its explored sibling edgenumber overits total sibling number lines5 to9 .
ifsuchratioislowerthanapresetthreshold weretainthegiven edgeandstoresitina candidateedgeset wherewelaterrandomly select such edges for further gradient computation lines to .
we use figure to further illustrate such algorithm.
assuming thate0canbeexploredgiventhe seed infigure1 mutatingthe byte of the given seed corresponding to the access condition of e0canexploreits sibling edge e1.whileneuzzandmtfuzzare designedtoperformsuchmutationforedgeexploration e1could have nevertheless been explored already due to the randomnessinjected to their mechanisms illustrated in section .
.
.
thus the effectiveness of the gradient guidance mechanism may be compromised.
however our resource efficient edge selection mechanism cancollecttheexplorationinformationofthe sibling edgeof e0 i.e.
e1 before computing the gradient for e0.
if it finds out that e1has already been explored it would not select e0for gradient computation in the first place to save the computing resource.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa m. wu et al.
a neuzz edgeselection neuzz b neuzz prob neuzz c prefuzz neuzz figure edge coverage ratio upon neuzz algorithm candidate edge set construction input threshold explorededge output selectededges function construct candidate edge set candidate set corresprelation getedgerelation foredge in explorededge do explored siblings corresprelation forneighbour in corresprelation do ifneighbour in explorededge then explored explored ifexplored siblings threshold then candidate.add edge selectededges randomlyselectfromset candidate returnselectededges table edge coverage results of prefuzz benchmarks afl neuzz mtfuzz neuzz edgeselection neuzz prob prefuzz bison xmlwf mupdf13 pngimage pngfix pngtest tcpdump nasm tiff2pdf45 tiff2ps tiffdump tiffinfo libxml listaction listaction d libsass jhead readelf nm strip size objdump libjpeg harfbuzz base64 md5sum uniq who average .
.
probabilistic byte selection mechanism.
inspired by finding we further inject an additional nondeterministic stage to neural program smoothing fuzzers.
to this end we develop a probabilisticbyteselectionmechanism andappenditto neuzztoexpandedge exploration.
note that the probabilistic byte selection mechanism utilizes the gradient information generated by the resource efficient edgeselectionmechanism andgetsactivatedafterthemutationstage inherited from neuzz.
this stage contains three steps dividing each seed input into segments selecting segments by gradientbased probability distribution and randomly selecting bytes from the selected segment for mutation via afl havocmutators.
unlikeafl havocwhich randomly selects bytes from the whole seed we first divide a seed into a constant number by default in ourpaper ofequal lengthsegments.wethenselectseedsegments based on their probabilities.
note that while intuitively leveraging byte wiseprobabilitydistributionforbyteselectionismorenatural this is essentially deterministic and excludes the benefits of randomness asinfinding2 .therefore ourprobabilitydistributionis established uponseed segments ratherthan individualbytes so as to leverage the power of randomness and afl havoc.
next we calculate the fitness score for each segment presented inequation1 where summationtext.1segi j 1grad jdenotesthegradientsumforall thebyteswithinagivensegment segi length segi denotesitsbyte number andthefitnessscoreforagivensegment segiiscomputed as the average gradient of all the bytes within segi.
fitness segi summationtext.1segi j 1grad j length segi accordingly theprobability prob segiforselectingasegment segi for mutation is presented in equation i.e.
the ratio of the fitness score ofsegiover the total fitness scores of all the segments.
prob segi fitness segi summationtext.1total j 1fitness seg j finally we apply afl havocto mutate the selected segments.
in particular aflhavocrandomly selects a byte from the segment for mutationbasedonitsmechanism.notethatifthemutantsarealso interesting theyareretainedforfurthergradientcomputationand theprobabilistic byte selection mechanism .
such process is iterated until hitting the time budget.
.
performance evaluation weattempttoevaluatetheperformanceof prefuzzanditstechnical components respectively.
to evaluate the usage of the resourceefficientedgeselectionmechanism andtheprobabilisticbyteselection mechanism we form two neuzzvariants i.e.
neuzz edgeselection authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
evaluating and improving neural program smoothing based fuzzing icse may pittsburgh pa usa whichinjects resource efficientedgeselectionmechanism toneuzz andneuzz probwhich appends the probabilistic byte selection mechanismtoneuzz.notethatweretain neuzz mtfuzz andaflasour baselines for performance comparison.
the experimental setups in thissectionfollowthesamesettingsinsection3.
.the threshold for algorithm is set to .
.
.
.
edgeexplorationeffectiveness.
table3presentstheexperimentalresultsofedgeexplorationeffectiveness.wecanfindthat overall prefuzzoutperforms all the existing baselines in terms of edge coverage averagely e.g.
prefuzzcan outperform afl by .
042vs.
265explorededges and neuzzby43.
vs. explorededges .notethat undertheoriginally adopted metric of edge coverage prefuzzalso outperforms neuzzandmtfuzzby34.
and36.
.suchresultssuggestthatcombiningthe resource efficientedgeselectionmechanism andtheprobabilisticbyte selectionmechanism forneuzzcanberatherpowerful.moreover neuzz edgeselection outperforms neuzzby .
vs. explored edges and mtfuzzby .
vs. explored edges .
specifically neuzzobtains 271more edges averagelythan neuzz edgeselection on2projectswhile neuzz edgeselection obtains more edges averagely than neuzzon the rest projects.
such results indicate that the resource efficient edge selection mechanismcan enhancethe overalleffectiveness of neuzz.
inaddition neuzz probalsooutperforms neuzzby27.
636vs.
395explored edges and mtfuzzby .
vs. explored edges .
such results demonstrate that introducing randomness can alsosignificantlyincreasetheedgecoverageoftheneuralprogramsmoothing based fuzzers.
figure presents the correlation between the edge coverage advantageof neuzz edgeselection neuzz prob prefuzzoverneuzzby dividing their corresponding edge coverage results and the loc of the studied benchmark projects.
interestingly we can observe that the correlation is rather weak i.e.
all presented pvalues .
.
and .
fail to reach the significance level of .
.
it indicatesthattheedgecoverageadvantageovertheoriginal neuzzis notaffectedbytheprogramsize.moreover suchadvantageisrather consistent.specifically wedeterminetousecoefficientofvariation cv a widely used metric for measuring the dispersion of a probability distribution to measure theconsistency of their performance improvement.
note that a lower cv indicates a more consistent performance improvement.
as a result prefuzz neuzz edgeselection andneuzz probcan achieve .
.
and .
ofcvfortheirp erformanceimprovementover neuzz which areallsignificantlyreducedcomparedwiththecvof neuzz .
for its improvement overafl.
therefore we summarize thatour proposedmechanismscansignificantlyandconsistentlystrengthentheneuralprogram smoothing basedfuzzers.notethatwefindun dertheedgecoveragemetricadoptedintheoriginal neuzz mtfuzz papers the performance gain of prefuzzoverneuzzis .
vs. explored edges which is also rather significant.
figure7presentstheaveragetimetrendofedgecoveragewithin 24hoursforafl mtfuzz neuzzandprefuzzamongallthebenchmark projects.
we can observe that at any time being prefuzzcan outperform other fuzzers significantly in terms of edge coverage.
1wealsoevaluatemorethresholdsetupsandpresenttheresultsinourgithublink which indicate that changing threshold setups incurs limited performance impact.
24afl neuzz mtfuzzaverage edge coverage time hours prefuzz figure edge coverage of prefuzz in terms of time .
.
in depth ablation study.
in this section we further perform in depth ablation studies to investigate the efficacy of our resourceefficient edge selection mechanism andprobabilistic byte selection mechanism respectively.
specifically for the resource efficient edge selection mechanism we find that overall .
edges do not need tobeexploredbyapplying neuzz edgeselection undereachiteration averagely vs. edges .
moreover the probabilistic byte selectionmechanism inprefuzzismoreefficientwhencombining withtheresource efficientedgeselectionmechanism sinceprefuzz explores averagely .
more edges than neuzz prob vs. 636explorededgesintable3 .suchresultsindicatethatapplying theresource efficient edge selection mechanism can significantly savetheeffortonexploringtheedgeswhichcannotcontributeto increasing edge coverage.
we further investigate the probabilistic byte selection mechanismintermsof edgediscoveryrate .tothisend wealsoinclude afl havoc neuzz edgeselection the gradient guided mutation stage ofprefuzz andtheprobabilisticbyteselectionstageof prefuzz represented as prefuzz gradientandprefuzz prob respectively for performance comparison.
note that prefuzz gradientandprefuzz prob resultsareextractedfromthetwostagesofacomplete prefuzzrun e.g.
prefuzz probutilizes the resource efficient edge selection mechanismtoselectedgesforcomputingtheirgradientswhile neuzz prob randomly selects explored edges for gradient computation.
figure presents our evaluation results.
we can observe that overall prefuzz probcansignificantlyoutperformalltheotherstudiedapproaches on top of all the studied benchmarks e.g.
prefuzz prob can be .
more efficient than afl havoc .
vs. .
edr .
accordingly we can infer that the gradient guidance adopted by prefuzzcanprovide more high quality seedsand moreefficient guidance i.e.
gradients forlaunchingits probabilisticbyteselection mechanism to explore more new edges than afl havoc.
furthermore we can observe that the edr of prefuzz gradientcan also outperform the original neuzz edgeselection by .
.
therefore we also infer that prefuzz probcan advance the edge exploration efficiency of prefuzz gradient.
to summarize combining the two improvements can mutually advance their edge exploration.
.
.
crashes.
table presents the unique crashes exposed by neuzz mtfuzzandprefuzzin the studied benchmarks.
overall prefuzzexplores the most unique crashes by outperforming neuzz by vs. and mtfuzzby vs. .
in addition prefuzzdominates the number of the exposed unique crashes in eachbenchmark.
furthermore the crashesexposed by neuzzand authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa m. wu et al.
table4 uniquecrashesfoundby neuzz mtfuzz andprefuzz benchmarks neuzz mtfuzz prefuzz size readelf libjpeg objdump who bison jhead listaction listaction d nm strip total .
.
.
.
.
.
aflhavoc prefuzzprobneuzzedgeselection prefuzzgradientedge discover rate figure edge discovery rate of different prefuzz stages mtfuzzarealsodetectedby prefuzzinourevaluation.suchresults suggest that prefuzzcan also be more effective than neuzzand mtfuzzin terms of exposing potential vulnerabilities.
.
implications based on our findings in this paper we propose the following implications for advancing the future research on fuzzing.
simplisticneuralnetworkmodelsmaysuffice.
ourstudy resultsrevealthattheedgecoverageperformancecanbeessentially impacted by how the resulting gradients of the adopted neural network models reflect the relations between explored edges and seed inputs rather than their generalization or prediction capabilities.
thatsaid simplisticneuralnetworkmodelsmayalreadysufficefor program smoothing based fuzzing.
think twice before applying dynamic analysis.
our evaluationsindicatethatthedynamicanalysismoduleadoptedin mtfuzz can be quite effective on large programs.
however executing such module can be rather heavyweight similar as manyother program analysis techniques .
therefore we recommend to think carefully before adopting dynamic analysis techniques to enhance neural program smoothing based fuzzing.
edge selection?
yes!
gradient computation?
maybe.
our evaluationsrevealthatselecting promising edgesformutations can be quite effective in increasing the edge coverage performance onprogramsofvaryingsizes.meanwhile onequestioncanbeasked is it necessary to bind such powerful mechanism with gradient guidance?
especially when we realize that the power of neural networkscanbearguedtobe underused i.e.
theirgeneralization and prediction capabilities are underused such question can then be transformed as is it necessary to use neural networks for computinggradientstorepresenttherelationsbetweenexplorededgesandseedinputs?toanswersuchquestion itisworthwhileto attemptotherlightweightalternativestorepresentsuchrelations as potential future research directions.
probabilistic search helps.
our study results indicate that the edgecoverageperformanceoftheneuralprogram smoothing basedfuzzerscanbesignificantlyenhancedbyappendingthe probabilistic byteselectionmechanism .intuitively wesuggesttheuserstodesign suchprobabilisticsearchstrategywithmoreguidancetoanyoftheir adopted fuzzers when possible.
accordingly one possible research directioncanbehowtointegratesuchprobabilisticsearchstrategy with diverse fuzzers for optimizing the edge coverage performance.
threats to validity threatstointernalvalidity.
thethreattointernalvaliditylies in the implementation of the studied fuzzing approaches in the experimentalstudy.toreducethisthreat wereusedthesourcecodeofneuzzandmtfuzzwhenweimplemented prefuzz.meanwhile to implementthe probabilisticbyteselectionmechanism wealsoreused suchcodefromtheoriginalaflforthe prefuzzimplementation.
moreover all the student authors manually reviewed prefuzzcode carefully to ensure its correctness and consistency.
threats to external validity.
the threat to external validity mainlyliesinthebenchmarksused.toreducethisthreat weadopt the original benchmarks used by neuzzandmtfuzz and add more projects widely used for the evaluations in many popular fuzzers published recently.
threats to construct validity.
the threat to construct validity mainly lies in the metrics used.
while the edge coverage metrics adopted by neuzzandmtfuzzare not widely used by the existing fuzzers and can be arguably limited to reflect edge coverage toreduce this threat we determine to follow the majority by usingthe afl built in tool afl showmap for measuring edge coverage whilealsopresentingpartialresultsintheoriginalmeasureaswell.
notably while under our metric the performance advantages of neuzzandmtfuzzare reduced our prefuzzcan incur quite strong performance gain under both metrics.
related work asthisworkmainlystudiesdeeplearning basedfuzzingapproaches we are going to discuss closely related work in the following three dimensions the existing fuzzing approaches section .
the deep learning based fuzzing techniques section .
and the existing studies on fuzzing section .
.
.
fuzzing to date various fuzzing techniques have adopted evolutionary algorithmstoimprovetheperformanceoffuzztesting.b hmeet al.
proposed aflfastwhich designs a seed selection strategy toweighseedsviamarkovchainontopoftheoriginalafl .
they also proposed aflgo to take advantages in weighting seeds based on edge structures to explore the target point specifiedbyusers.lemieuxetal.
proposed fairfuzztoincreasegreybox fuzz testing coverage by fuzzing rare branches of program.
man s et al.
proposed ankou a grey box fuzzing solution based on differentcombinationsofexecutioninformation.fioraldietal.
introduced weizzto automatically generate and mutate inputs authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
evaluating and improving neural program smoothing based fuzzing icse may pittsburgh pa usa for unknown chunk basedbinary formats.
similar toour prefuzz manyworksalsoutilizedlight weightprogramanalysistofacilitate fuzzing efficacy.
rawat et al.
proposed vuzzerto leverage control anddata flowfeaturesbasedonstaticanddynamicanalysis toinferfundamentalpropertiesoftheapplicationwithoutanypriorknowledgeorinputformat.mathisetal.
presentedatechnique to learn program tokens by tainting for fuzzing.
padhye et al.
automatically guided quickcheck like random input generators to semantically analyze test programs for generating test oriented java bytecode.
chen et al.
introduced angora a mutation based fuzzerthatsolvespathconstraintwithoutsymbolicexecutionby taint checking and searching.
furthermore new guidance other than code coverage are proposed to fuzz specific software systems.
wuetal.
proposed simuleetoparseconstraintsofinputsfroma givengpukernelfunctionandmutatetheinputsguidedbymemory access conflict to fuzz cuda programs.
accordingly they further proposed aucs to repair the detected synchronization bugs.
wenetal.
proposedamemory usage guidedfuzzertogenerate excessive memory consumption inputs and trigger uncontrolled memory consumption bugs.
zhao et al.
synthesized programs for testing jvms based on the ingredients extracted from jvm historical bug revealing tests.
.
deep learning on fuzzing sheetal.proposed neuzz thefirstneuralprogram smoothingbased fuzzer using neural network models to discover promising bytes for a previously explored edge.
they also proposed mtfuzzto fuzz a system more efficiently via a multi task neural network.
meanwhile deep learning is also used in evolution based fuzzing.zongetal.
proposed fuzzguard adeep learning based approachtohelpevolution basedfuzzerspredictthereachability ofinputsbeforeexecutingprograms.moreover researchershave also utilized deep learning to learn how to generate valid inputs fordeeplyfuzzingasystem.lyuetal.
introduced smartseed whichusedgenerativeadversarialnetworks togenerateseeds from learning the patterns of valuable existing seeds.
liu et al.
proposed deepfuzz to automatically and continuously generate c programs by a generative sequence to sequence model .
godefroid et al.
divided fuzzing tasks into two categories i.e.
learning input format to fuzz deeper and breaking input formatto trigger defects.
zhang et al.
proposed deeproad to automatically generate driving scenes to fuzz image based autonomousdrivingsystems.zhouetal.
furthergeneratedrealisticandcontinuous physical world images to fuzz such systems.
in this paper we propose prefuzzwith theresource efficient edge selection mechanismand theprobabilistic byte selection mechanism to improve the performance of neural program smoothing based fuzzers.
.
studies on fuzzing theempiricalstudiesonfuzzinggivemanyimplicationsforfurther research.
klees et al.
provided guidelines on evaluating the effectiveness of fuzzers by assessing the experimental evaluations carriedoutbydifferentfuzzers.gavrilovetal.
proposedanew metric consistently with bug based metrics by conducting a program behavior study during fuzzing.
b hme et al.
summarized thechallengesandopportunitiesforfuzzingbystudyingexistingpopular fuzzers.
geng et al.
performed an empirical study on multiple artificial vulnerability benchmarks to understand howclose these benchmarks reflect reality.
herrera et al.
investigated and evaluated how seed selection affects a fuzzer s ability to findbugsinreal worldsoftware.wuetal.
studiedthefeatures ofthehavocmechanismadoptedbymanyfuzzersincludingafl and found it is already a powerful fuzzer which outperforms many existingones.inthispaper weconductanempiricalstudytoinves tigatethepowerandlimitationofneuralprogram smoothing based fuzzing and reveal various findings guidelines for future learning based fuzzing research.
conclusion inthispaper weinvestigatedthestrengthsandlimitationsofneural program smoothing based fuzzing approaches e.g.
mtfuzzand neuzz.wefirstextendedourbenchmarksuitebyincludingadditional projects that were widely adopted in the existing fuzzing evaluations.next weevaluated neuzzandmtfuzzontheextensive benchmark suite to study their effectiveness and efficiency.inspired by our study findings we proposed prefuzzcombining two technical improvements i.e.
the resource efficient edge selection mechanism and theprobabilistic byte selection mechanism .
the evaluationresultsdemonstratethat prefuzzcansignificantlyoutperformneuzzandmtfuzzintermsofedgecoverage.furthermore our results also reveal various findings guidelines for advancing future fuzzing research.
acknowledgement thisworkispartiallysupportedbythenationalnaturalscience foundation of china grant no.
guangdong provincial key laboratory grant no.
2020b121201001 and shenzhen peacockplan grantno.kqtd2016112514355531 .thisworkisalso partiallysupportedbynationalsciencefoundationundergrant nos.
ccf 2131943and ccf as well as ant group.