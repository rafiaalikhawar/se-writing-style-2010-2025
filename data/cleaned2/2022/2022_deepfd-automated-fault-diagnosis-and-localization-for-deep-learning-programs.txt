deepfd automated fault diagnosis and localization for deep learning programs jialun cao the hong kong university of science and technology and guangzhou hkust fok ying tung research institute china jcaoap cse.ust.hkmeiziniu li the hong kong university of science and technology china mlick cse.ust.hkxiao chen huazhong university of science and technology china xchencr hust.edu.cn ming wen huazhong university of science and technology china mwenaa hust.edu.cnyongqiang tian university of waterloo canada and the hong kong university of science and technology china yongqiang.tian uwaterloo.cabo wu mit ibm watson ai lab u.s. bo.wu ibm.com shing chi cheung the hong kong university of science and technology and guangzhou hkust fok ying tung research institute china scc cse.ust.hk abstract asdeeplearning dl systemsarewidelydeployedformissioncritical applications debugging such systems becomes essential.
most existing works identify and repair suspicious neurons on the trained deep neural network dnn which unfortunately might be a detour.
specifically several existing studies have reported that many unsatisfactory behaviors are actually originated from the faults residing in dl programs.
besides locating faulty neurons is not actionable for developers while locating the faulty statements in dl programs can provide developers with more useful information for debugging.
though a few recent studies were proposed topinpointthefaultystatementsindlprogramsorthetrainingsettings e.g.too large learning rate they were mainly designed based on predefined rules leading to many false alarms or false negatives especiallywhenthefaultsarebeyondtheircapabilities.
in view of these limitations in this paper we proposed deepfd a learning based fault diagnosis and localization framework which mapsthefaultlocalizationtasktoalearningproblem.inparticular itinfersthesuspiciousfaulttypesviamonitoringtheruntime features extracted during dnn model training and then locates corresponding author.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation onthe firstpage.copyrights forcomponentsof thisworkowned byothersthan the author s mustbehonored.abstractingwithcreditispermitted.tocopyotherwise or republish topostonserversortoredistributetolists requirespriorspecificpermission and or a fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn ... .
diagnosed faultsin dl programs.
it overcomes thelimitations byidentifyingtherootcausesoffaultsindlprogramsinsteadof neurons and diagnosing the faults by a learning approach instead ofasetofhard codedrules.
theevaluationexhibitsthepotentialof deepfd.
it correctly diagnoses faulty dl programs compared with around half achieved by the best state of the art works.
besides forfault localization deepfdalsooutperformsthe existingworks correctlylocating42 faultyprograms whichalmost doubles the best result achieved by the existing works.
ccs concepts software and its engineering software testing and debugging computing methodologies neural networks .
keywords neural networks fault diagnosis fault localization debugging acm reference format jialun cao meiziniu li xiao chen ming wen yongqiang tian bo wu and shing chi cheung .
.
deepfd automated fault diagnosis and localizationfordeeplearningprograms.in 44thinternationalconferenceon software engineering icse may pittsburgh pa usa.
acm new york ny usa pages.
introduction deeplearning dl softwarehasbeenactivelydeployedforapplicationssuchasfrauddetection medicaldiagnosis facerecognition andautonomous driving .such softwarecomprisesdl programs whichencodethestructureofadeepneuralnetwork dnn modelandtheprocessbywhichthemodelistrained.various ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa cao et al.
studies havebeenconductedtounderstand and detect dl program faults.
yet debuggingdlprogramsisstillchallenging .
unlikeconventionalsoftwareprograms thedecisionlogicbasedon whichadnnmodelmakespredictionsisnotexplicitlyencodedby thedlprogram scontrolflow .instead predictionsaremadeby propagating inputs against the tuned parameters through a trained dnnmodel.theprogramplaysthecrucialrolebydefining guiding and monitoring the model training process e.g.
defining network structures training strategies and hyperparameters based on a training set to indirectly tune the parameters of the dnn model.
duetothestochasticnatureofdlcomputation adnnmodelis unable to make every prediction correctly and an incorrect predictionresultdoesnotnecessarilyindicateafaultintheunderlying program.therefore itishardtodebugdlcomputationfaultsusing conventionalfaultlocalizationtechniques based on individual correct and incorrect prediction results.
techniques have been proposed to debug such faults by identifying and locating suspicious neurons or layers in the trained dnnmodel.
specifically they drawan analogybetween a faulty neuron or layer in a dnn model and a faulty statement or branch inaconventionalprogram.withtheanalogy theyadapt fault localization techniques such as the spectrum based ones todetect suspicious neurons or layers based on statistical analysis.
however directlytuning the weights of neuronsafter trainingcannot facilitate pinpointing the flaws in dl programs.
even worse after tuning developer may ignore the faults in the program thus leavingtherealrootcauses e.g.
inappropriatelossfunctionand optimizer setting of the unsatisfactory behavior uncovered.
recentworksaddressedtheproblem i.e.debuggingdlsystems in a more practical way.
for instance umlaut was proposed to detect program faults using predefined rules and provide advice onhowtofixthem.autotrainer wasproposedtodetectfive predefinedproblemsthatmightoccurduringtrainingdnnmodels withsolutionstoeradicatethedetectedproblems.theserule based approaches do pinpoint more accurate root causes of the faults i.e.
lackingofdatanormalization orprovidemoreactionableadviceon how to fix the dl program or tuning the hyper parameters i.e.set thelearning ratebetween 7to0.
.
however such hard coded rules may induce false alarms and are incapable of revealing faults thatgobeyondthecapabilityofthosepredefinedrules.forexample umlautreportsafaultwhenevertheoutputlayerisnotactivated bysoftmax whileinfact theactivatoroftheoutputlayervaries case by case.
besides the pre defined rules can over simplify fault detection to limited symptoms exhibited by several types of faults e.g.
the example presented in section .
in view of the above mentioned limitations we look for an alternative.
specifically we propose a learning based fault diagnosis and localization framework deepfd which maps a fault local izationtasktoalearningproblem.inparticular itdiagnosesandlocates faults in a dl program by inferring the suspicious faulttypes using the features extracted in the model training process.
the intuition behind is that the trend or distribution of certain externalorinternalvariables e.g.
lossorgradients inaprogram s 1theexistingrelatedworksmayuseothertermslike bug defect or issue .thispaperuses fault asarepresentativeofallsuchrelatedtermstorefertoanyhuman made mistake in the dl program that leads to functionally insufficient performance .
model construction 2model sequential 3model.add dense input dim output dim init glorot uniform 6model.add activation sigmoid 7model.add dense input dim output dim init glorot uniform 10model.add activation sigmoid training configuration 13sgd sgd lr .
bug l2 .
decay 1e momentum .
nesterov true 16model.compile loss mean absolute error optimizer sgd bug 17model.fit train data label nb epoch batch size bug listing a faulty code snippet extracted from stackoverflow .
trainingprocesscansuggestthelikelihoodoffaultsandtheirtypes.
such an intuition is also echoed by our observation that the value ofvariablesinadlprogramcanfollowcertainpatternsduringthe modeltrainingprocess andthesepatternsexhibitstrongcorrelations with certain types of faults.
for instance if the loss variable s value oscillates wildly the training is likely using an inappropriate learning rate .
however it is difficult to set a specific threshold forsuchoscillationsandidentifytheexistenceoflearningratesfault accordingly.
we therefore resort to a learning based approach.
however the design of deepfd needs to address two main challenges.first thereisnoexistingoff the shelfdataset i.e.
faultydlprogramswiththeground truthoffaults availablethatissufficient to enable the learning of fault related features.
though one may seedfaultsintocorrectdlprogramstoconstructsuchatraining set how to seed diverse faults effectively into the programs andfurther determine whether the dnn model is indeed faulty after faultseedingremainsunknown.second therearefewreferences ofdeployinglearningalgorithmstolocatedlprogramfaultswhile the effectiveness of a learning based technique often requires a set ofhigh qualityfeatures.ho wever sincethereareenormousparametersandoutputsduringdnntraining itisinfeasibletostoreall the values during the continuous training iterations and use such valuesasfeatures.therefore howtoselectqualifiedfeaturesthat canbeutilizedforeffectivelocalizationofdnnfaultsremainsto be challenging.
toaddressthefirstchallenge wecollectedabenchmarkwith58 realfaultsinthewildwithpatchesandanalyzedtheprevalentfault types made by developers.
as a result five common fault typesare observed.
we seeded faults of these types to hundreds of dl programs from a recent benchmark to generate a training set withthousandsoffaultydlprograms servingforthelearningpart of deepfd.
furthermore we adopted the generalised linear model glm andcohen seffectsized todeterminewhethera fault seededdlprogramisstatisticallydifferentfromandworse thantheoriginalprogram thusdeterminingtheoracleoftheseededprograms.toaddressthesecondchallenge inspiredbytheruntime informationusedinpreviousstudies wedesigned and traced the runtime information such as loss gradients andthe number of times loss increases.
then we applied statistical analysis on each trace of runtime information resulting in a list of authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
deepfd automated fault diagnosis and localization for deep learning programs icse may pittsburgh pa usa extracted runtime features which can represent the process of this dnnmodeltraining.thereby withtheextractedfeaturesasinputs and the seeded faults as ground truth labels the fault diagnosis problem can therefore be translated into a classification problem.
in summary our work makes the following major contributions originality we identify a set of features that can be used to classifymajortypesoffaultsindlprograms whichareextracted from the dnn training process.
leveraging these features we proposea learning basedfaultlocalization framework deepfd fordlprograms.itisabletodiagnosemultiplefaultsandidentify theirindividualfaulttypes.foreachofthesefaults itcanfurther locate the faulty code snippet in the concerned dl program.
benchmark we build a benchmark containing faulty dl programs collected from stack overflow and github.
each of them include faults patches and line numbers of the faulty statements.
this benchmark can serve for future researches on dnn debugging and repair tools.
usefulness we encapsulate the fault seeding faulty program checking featureextraction faultdiagnosisandlocalizationinto the deepfd framework and open sourced it .
the tool is extensibleforseedingdiversefaultsandmutationtestingindependently.itcanalsobeadaptedtoextractmoreruntimefeatures for other learning based works.
evaluation the evaluation exhibits the potential of deepfd.
it can correctly diagnose cases compared with half achieved by the best state of the art works.
besides for fault localization deepfdalsooutperformstheexistingworks reaching while the existing works range from to .
motivation listing shows a buggy dl program extracted from stackoverflow.2the program constructs a dnn model for the xor problem but the model s accuracy gets stuck at around .
the faults in this dnn program include an inappropriate learning rate line unsuitablelossfunction line16 and insufficienttraining epoch line .
it is challenging for developers to diagnose and localize these faults all at once.
weappliedthreestate of the arttechniques i.e.
autotrainer umluat and deeplocalize to this buggy program to examine whether theycan help diagnose and locatethe faults.
note thatwedonotcomparewithautomatedmachinelearning automl algorithms since these works are not for debugging a faulty program butselectingmachinelearningalgorithmsaccordingto the user provided data .
however our goal is to debug a faulty programandidentifytherootcauses.toreducetherandomnessof training process we ran each work times then report the result.
the results areshown in table .
specifically autotrainer cannot detectanyfaultsoverthe10runs letalonediagnosingthefaults.
the faults are escaped from the autotrainer s detection because this buggy program does not exhibit the abnormal symptoms that are predefined by autotrainer such as gradient vanish and dying relu .
on the other hand though umluat and deeplocalize are able to detect the existence of faults the diagnosed faults are inaccurate.
specifically umluat falsely alarmed that the last layer has two faults i.e.missing softmax layer before fault diagnosis and localization results.
method faultdiagnosis faultlocalization autotrainer notraining problem umluat critical missing softmax layer before loss critical missing activation functions warning last model layer has nonlinear activationwarning learning rate is highwarning possible overfitting deeplocalize layer error in output gradient stopa t1e poc hlayer deepfdfault1 fault fault fault lines lines lines lines loss and missing activation functions .
in fact these two alarms willbefiredaslongastheoutputlayerisnotactivatedby softmax.
theonlyrootcausethathasbeencorrectlydiagnosedbyumluat istheinappropriatelearningrate butitismarkedasawarning.on the other hand deeplocalize reports a fault at the output gradient oflayer3 i.e.
line10 duetonumericalerrors e.g.
nan occurred when propagating to this layer.
though it does locate where the symptom happens it fails to correctly pinpoint any of the faults.
takingacloserlook someexistingworksprescribethesymptoms and map them only to the existence of potential faults but cannotidentifythespecifictypesoffaults .otherworkspredefined rules relying on various predefined thresholds yet it is infeasibletotryoutallthecombinationsofvariousthresholdsto findouttheoptimalone .assuch ourlearning basedframework deepfd isabletoovercometheselimitationsbylearningthe correlations between symptoms and specific types of faults and learning the optimal thresholds automatically.
the secret of deepfd is that some runtime information exhibits significant correlations with certain types of faults.
specifically figure shows the distributions of several features see section .
formoredetails whenthelearningrateandlossfunctionarefaulty in red or not in blue .
we can observe that there are obvious statistical differences when the learning rate or loss function are set appropriately or inappropriately.
the observation enables us to performfault diagnosisand localizationas alearning problemvia leveraging the relevant stochastic runtime information of a buggy dl program.
finally for the example as shown in listing our approach deepfd can report the four types of suspicious faults intheprogram thelossfunction line16 learningrate line13 training epoch line and activation function line and line .
after repairing these faults accordingly the dnn model can achieve accuracy.
approach .
workflow of deepfd figure2showstheworkflowofdeepfd.itcomprisesthreesteps diagnostic feature extraction fault diagnosis and fault localization.
step1 diagnosticfeatureextraction.
givenaprogram deepfd constructsadnnarchitectureandcollectsruntimedatasuchasthe lossandneurons informationbytrainingthearchitecture.however storing all the weights and gradients for each neuron at each 3since it is a xor problem there is no need to consider over fitting.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa cao et al.
figure correlation between types of faults and features.
the first row illustrates correlations between certain features with unsuitable learning rate the second row with unsuitable loss function.
the naming convention of features starts with ft feature followed by the name of runtime data and the applied statistical operators.
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
figure overview of deepfd.
training iteration is expensive.
referencing recent works on dynamic trace collection and runtime monitoring deepfdcollects20runtimedata seetable2formoredetails includinglossandaccuracy thenumberoftimesthelossincreases themeanorstandardderivationofweights etc.thedataarerepeatedlycollectedatcertainintervals e.g.
every256batchesorevery epoch .finally 160diagnosticfeaturesareextractedbyapplying statistical analyses e.g.
calculating the variance and skewness to the collected data.
table2liststhe20runtimedatacollectedbydeepfdalongwith the monitored variables and detailed description.
the collection of these runtime data are inspired by various existing works and processed in a way following the settings used by existing studies .
the runtime data are collected at predefined intervals during the training stage resulting in data sequences.thedatasequencesarethenprocessedusingtheeight statistical operators in table to extract diagnostic features.
forexample the skewness of a data sequence serves as a diagnosticfeaturethatquantifiestheasymmetryoftheprobabilitydistributionofthesequencewithrespecttoitsmean.afterapplyingthestatistical operators diagnostic features are extractedtocharacterizethetrainingprocessofthegivendlprogram.
step fault diagnosis.
afterobtainingthediagnosticfeatures wetheninferthepossibletypesoffaultsthatexistinthedlprogramaccordingtothefeatures.weregarditasamulti labelclassificationproblem whichmapstheobtainedfeaturesintooneor more possible labels.
each label corresponds to a fault type.
the classificationreliesonthepredictionsmadebyasetofpretrained diagnosis models seesection .
for details .
thediagnosis result is given by the union of the diagnosed faults predicted by each diagnosis model to maximize the number of faults diagnosed.
also for each dl program under test we run them ten times to address the randomness of dnn model training .
currently thediagnosismodelsaretrainedtoclassifyfivemajor typesoffaults includingunsuitablelossfunction optimizer activation function insufficient iteration and inappropriate learning authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
deepfd automated fault diagnosis and localization for deep learning programs icse may pittsburgh pa usa table runtime data collected by deepfd during dnn model training runtime data monitored variables description loss loss losson the training set.
acc accuracy accuracy on the training set.
loss val validation loss losson the validation set.
acc val validation aacuracy accuracy on the validation set.
nan loss loss thenumber of times loss is not a number i.e.
nan nan accuracy accuracy whether there are nan in accuracy nan weight weight whether there are nan in weight nan gradient gradient whether there are nan in gradient large weight weight thenumber of times the maximum weight is larger than a certain threshold.
decrease acc trace of accuracy thenumber of times the accuracy is smaller than the last recorded accuracy.
increase loss trace of loss thenumber of times the loss is larger than the last recorded loss.
cons mean weight trace of mean of weight thenumber of times the mean of weight remains the same as the last record.
cons std w eight trace of standard deviation of weight whether the standard deviation of weight remains the same as the last record.
gap train test accuracy validation accuracy whether the gap between training accuracy and validating accuracy is too big.
test turn bad loss validation loss whether the loss on the training set decreases while on validation set increases.
slow converge trace of accuracy whether the accuracy of trained models is growing slowly.
oscillating loss trace of loss accuracy whether the loss is oscillating.
dying relu traces of gradient accuracythere has been a set of neurons whose gradients have been in the recent a few iterations and this set is large forms a large portion of the whole dnn while the accuracy of the neuron network is still low.
gradient vanish gradient accuracy whether the gradient vanish problem occurs.
gradient explosion gradient accuracy whether the gradient explode problem occurs.
rate which account for the majority .
types of faults in our benchmark see section for more details .
we use these common fault types to show the promising results of how learning based fault localization framework works and make it extensible for supporting more types of faults.
step fault localization.
after acquiring the diagnosed types offaults deepfdperformsfaultlocalizationattheprogramlevel.
specifically the program is first parsed into its abstract syntax tree ast anddeepfdgoesthroughthenodesoftheparsetree traverses assignment statement as well as expressions and then identifies the places i.e.lines where the diagnosed types of faults aredefined.forexample tolocalizethe optimizer inthesource code deepfd looks for invocations to specific model training apis i.e.
fit and parses the argument and keywords of this node and finally returns the line number where the optimizer is assigned.
however this process is not always as easy as keyword identification.
for example the learning rate as a hyperparameter of the optimizer isusually omittedintheprogram i.e.thedefaultlearning rate will be used making keyword identification infeasible.
in thiscase deepfdlocatestothelinewheretheoptimizerisdefined while reporting the fault is in the type of learning rate which can provide a more accurate debugging information for the developers.
since a fault may involve multiple lines deepfd reports a set of suspicious lines of code for each fault diagnosed.
.
diagnosis model construction two decisions are to be made in constructing diagnosis models.
first we need to choose which machine learning algorithms to construct the models.
we choose k nearest neighbors decision tree and random forest to construct three diagnosis models.
these three algorithms are chosenbecauseoftheirwideadoption explainabilityandsimplicity.second weneedtodecidehowtoaggregatetheirresults.wechoose to union their results to maximize the number of faults diagnosed.table3 statisticaloperatorsforruntimedataaggregation.
operators description max the maximum value in a feature trace.
min the maximum value in a feature trace.
median the median value of a feature trace.
mean the mean value of a feature trace.
var the variance of a feature trace.
std the standard deviation of a feature trace.
skew the skewness of a feature trace.
sem the standard error of mean of a feature trace.
we will evaluate the individual effectiveness of the three diagnosis models in section .
thethreediagnosismodelsareusedinthesecondstep i.e.fault diagnosis ofdeepfd servingastheclassifiersmappingdiagnosticfeaturesintodifferenttypesoffaultsthatmightexist.thediagnosis models are constructed in three steps as shown in figure .
step fault seeding.
this step is to prepare sufficient training samples for the diagnosis models.
since there is no off the shelf training set for this purpose we are inspired by the idea of faultseeding in mutation testing to seed faults into normal programs.however faultseedingfordlprogramsneedstoaddress multiple challenges.
first what types of faults should we seed?
though there are several existing empirical studies mostofthecollectedbuggyprogramsarenotreproduciblebecause ofincompleteormissingcodesnippets unavailabletrainingsets or program crashes.
without reproduction inappropriate plausible fixes may be included inducing noise to our study.
to address this challenge we constructedabenchmarkof 58buggydlprograms byreproducingthefailuresofthedlprogramfaultscollectedby recent empirical studies from github and stackoverflow.
for each reproduced faulty program we investigated itscharacteristics includingthenumberoffaults thefaulttypes authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa cao et al.
!
!
!
!
!
!
!
!
!
!
!
!
figure the workflow of diagnosis models construction.
aswellasthecodedifferencesbetweenthefaultyversionandits fixed version.
furthermore we found that a faulty dl program often contain faults of more than one type.
since most statements are involved in a dl program execution the effects of multiple faultsin afaulty executioncaneasily cascade.tomimic thesituation werandomlyseeduptofivetypesoffaultsineachprogram mutant.second howtoseedconcretefaultsforaspecifictypeof fault?
adapting deepcrime we designed seven fault seeding mutation operators in table for the five fault types.
weexplainthedesignoftheseoperatorsandtheirdifferences from existing works.
the first two mutation operators target at lossfunctions .lossfunctionsmeasurethedifferencebetweenthe groundtruthandthepredictedvalues whichcanbefurtherdivided into probabilistic loss functions and regression ones suiting for classification and regression tasks respectively.
when mutatingthe loss functions deepcrime randomly picks one from all the other available loss functions regardless of which category the lossfunctionis.onthecontrary wefirstfindoutthecategoryof lossusedbythegivendlprogram andthenrandomlyselectone fromanothercategory.itisbecauselossesfromanothercategory aremore likelytobe unsuitableforthe originaltask and thusthe generated mutant is more likely to be faulty.
next to seed faults in thelearning rate setting we increaseor decrease the learning rate deliberately setting them to an extremely large or small value.
whiledeepcrimeonlyconsidersettinglearningratestoextremely small values.
furthermore for the training epochs we assignthe epochtoasmallvaluebyrandomlydividingthecurrentnumberofepochwitharandomnumberwithin10to50 aimingatgenerating a small enough epoch.
while the existing work only generate a randomnumber whichmaybeevenlargerthanthecurrentnumberofepochs.finally forthe activationfunction andoptimization function werandomlychooseanotherfunctionfromtheavailable functionsetsapartfromtheoriginalone.notethatthoughexisting works canseedmorefaults theirworksaredesignedfor mutation testing which serves a different purpose from that of deepfd.
the fault seeding step in deepfd serves as a preparation forthediagnosismodeltrainingtoperformthefinalfaultdiagnosisand localization.
it is also extensible to support seeding more types of faults based on our framework.
step training set preparation.
in the previous step a set of mutants i.e.fault seededprograms are generated.however not all the mutants are necessarily faulty.
due to the randomness ofdnntraining slightvariationsofdnns performance4arenatural.
simply considering a dnn with slightly varied performance as faulty ignores the nature of randomness and may induce manyfalse alarms.
we present the criteria of faulty performance andthe procedure of seeding multiple faults.
to determine whethera mutant is faulty we first check whether there is a significant statisticaldifferencebetweenthedistributionoftheoriginaldnn s accuracy i.e.ado and that of its mutant admas adopted the following equation iskill n m x trueif effectsize ado x adm x pvalue ado x adm x false where and arethresholdsthatcontrolthestatisticalsignificance and effect size and xrepresents the testing set.
specifically to quantify the statistical significance and the effect size we adopted the generalised linear model glm and cohen s d .
if the distribution of the mutant s accuracy is statistically different fromthatoftheoriginalmodel thenwefurthercheckwhetherthe averageaccuracyofthemutantisworsethanitsoriginalone.ifthe above two requirements are satisfied at the same time the mutant is considered as faulty and is labeled as the types of faults that has been seeded.
if not the mutant is regarded as non buggy.
furthermore for mutants with more than one fault another challengeishow todeterminewhetherallthese seeded faultsare the root causes leading to the deteriorated performance?
we address this challenge by seeding faults one by one.
specifically after obtaining a mutant with one type of fault we then try to seed a secondfaultthatisindependentofthefirstone anduseequation1tocheckwhetherthesecondfaultissuccessfullyseeded.werepeat theaboveprocesstoinjectthethirdfaultandsoon.theprocessiterates until up to five types of faults are seeded in one original program.finally werunallthesegeneratedmutantstogetherwiththeoriginalprogramstocollectthediagnosticfeaturesasdescribed in the step of section .
.
these diagnostic features and their labels are then used to train the diagnosis models of deepfd.
step3 diagnosismodel training.
wetreatthefaultdiagnosis asamulti labelclassificationproblemmappingafaultyprogram tothemultipletypesoffaults thatitcontains.weconstructthree 4in this paper we use performance to refer the accuracy of loss of a dnn model as used in the existing work authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
deepfd automated fault diagnosis and localization for deep learning programs icse may pittsburgh pa usa table seeded faults and the corresponding fix patterns mutation operator target search range parameter fix patterns description change loss function to those for classification problemsloss function enumerate categorical crossentropy sparse categorical crossentropy binary crossentropy loss functionthis group of fixes adjusts the loss function which helps the training process to identify the deviation from the learned and actual examples.change loss function to those forregression based problemsloss function enumerate mean absolute error mean absolute percentage error mean squared error change activation function in layers activation function enumerate relu sigmoid softmax softplus softsign tanh selu elu linear activationthis group of fixes changes the activation function used in a layer to better match the problem.
decrease number of epoch epoch range originepoch random iterationsthis group of fixes adjusts the number of times the training process will be run.
change optimization function optimization function enumerate sgd rmsprop adam adadelta adagrad optimizerthis group of fixes modifies the optimization algorithms used by the dnn model.
decrease learning rate to an extreme small valuelearning rate range change neuralarchitecturethis group of fixes overhauls the design ofthe dnn s architecture including a new setof layers and hyperparameters.
increase learning rate to an extreme large valuelearning rate range change neural architecturethis group of fixes overhauls the design of the dnn s architecture including a new set of layers and hyperparameters.
diagnosis models using the three widely used and effective machine learning algorithms i.e.
k nearest neighbors decision tree and random forest to learn the correlations between diagnostic features and types of faults.
specifically thek nearestneighboralgorithmassumesthatsimilar samples exist in close proximity.
it clusters samples into k groups according to the distance between samples.
the decisiontree algorithm predicts the value of a target variable by learning simpledecisionrulesinferredfromthefeatures.therandomforestalgorithmisanensemblelearningmethod whichoperatesby constructing a multitude of decision trees during training and outputtingtheresultthatisfavoredbymostofthedecisiontrees.sinceasamplecanhavemultiplelabels weadoptthemulti labelversionofthesealgorithms whichclassifiesagivensampleinto aset oftargetlabels.
finally deepfdtrainsthree diagnosismodels against the constructed training set using these algorithms.
benchmark construction to investigate the characteristics of faults in real faulty programs and to evaluate the effectiveness of deepfd we build a benchmark with buggy dl programs from stackoverflow and github.
the benchmark includes the artifacts required to reproduce the bugs.
in this section we elaborate how we construct the benchmark.
subject collectionand selection.
wecollectthebenchmark in two steps.
first we revisit all the benchmarks collected by previ ousstudies e.g.
003postsfromstackoverflowand commits from github in total.besides in order to cover the recentlypostedissuesthatarenotincludedbythepreviousstudies we also search stackoverflow for recent issues following the same selection criteria as specified in .
specifically we collect the posts from stackoverflow with accepted answers with the score greater or equal to and the posted time ranges from march till april .
then we select the subject implemented by keras and the symptom of the program is poor performance i.e.
the program exhibits poor accuracy loss during the training process .
weselectkerasbecause46.
ofthepostsandcommitsconcern dlprogramsimplementedonkeras.weselectprogramswithpoorperformancesinceitisthemostcommonsymptomapartfromcrash i.e.
theprogramraisesanexceptionorcrash .weexcludethe crashedprogramsbecausetheyareuncommonforthosewritten by experienced dl developers .
for posts in stackoverflow we exclude those without accepted answers and without source codeorthetrainingdatasetsincewecannotreproducewithoutthe concerned issues.
for a similar reason we exclude those github projects that do not have training sets available.
reproduction and repair.
we reproduce and repair the collectedsubjectswiththefollowingprocedures.
werunthesource code to see whether it is executable.
if the source code crashes due to api upgrade versioning or typo issues we fix these issues otherwise we skip this subject.
we examine whether the output ofthesourcecodeexhibitsthesamesymptomasdescribedinthe post.
forgithub commits if acommit message does notdescribe the symptom we capture the symptom by running the program comparing the results before and after applying the commit.
if the performanceincreasesafterapplyingthecommit thenwetakethis commit into account.
to fix the program we adopt the patchesas suggested by the accepted answers in stackoverflow as well as other patches recommended in other replies.
for github commits werepairtheprogramaccordingtothecommittedchanges.finally weobtain abenchmark with58faulty dnnprograms alongwith thepatches usuallymorethanone thetypesoffaultsand linenumberswherethefaultsareintroduced.ifafault e.g.
adding morelayers isnotlocalizeddowntoaspecificpieceofcode we recordedthelinenumberswhetherthepatchshouldbeadded.our benchmark is made publicly available .
statistics of the benchmark.
the statistics of our benchmark aregiveninfigure4.intheleftpartofthefigure wepresentthe number of eachtype of faults in thebenchmark.
in particular the taxonomyoffaultsfollowstheexistingstudy includingtheactivationfunction optimizer lossfunction hyperparameters includ ingthesuboptimallearningrate thesuboptimalnumberofepochs and the suboptimal batch size training data quality model type andproperties layerproperties missing redundant wronglayer missingpreprocess andwrongpreprocessing.thetopfivetypesoffaults i.e.
activationfunction suboptimalnumberofepoch suboptimal learning rate loss function and optimizer are highlighted in authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa cao et al.
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
figure the statistics of benchmark.
yellow accountingforthemajority .
typesoffaultsinour benchmark.
in addition we also realized that one program usually containsmorethanonetypesoffaults sowefurtherpresentthe statisticsofthenumberoffaulttypes inblue andnumberoffaulty lines inred in oneprogram.notethatifonetypeof faultoccurs inmultipleplacesintheprogram e.g.
theactivationfunctioninall thelayersareinappropriatelyset weonlycountonesincethese faults belong to the same type.
similarly one faulty line of code is counted regardless of how many faults contained in this line.for the cases with missing layers or missing data preprocessing we regard their number of faulty lines as one.
from the second diagramoffigure4 wecanseethatoverhalfoftheprogramsin thebenchmarkcontainmorethanonefaulttype andmostofthem involve more than one line.
evaluation we study three research questions to evaluate the relevancy of diagnostic features and the effectivness of deepfd in this section.
rq1.
are the extracted features relevant to fault diagnosis?to investigate whether the diagnostic features have high correlations with certain types of faults we apply these features to perform fault diagnosis on the generated training sets to evaluatetherelevancyofsuchextractedfeaturesbasedontheresults.
the higher relevancy the more accurate the diagnosis will be.
rq2.howeffectiveisdeepfdinfaultdiagnosis?
to evaluate the effectiveness of deepfd on fault diagnosis we compared deepfdwithexistingworksonthebenchmarkintermsofthe number of identified and correctly diagnosed faulty cases.
we also showed the diagnosis information reported by each work.
rq3.howeffectiveisdeepfdforfaultlocalization?
accurate fault localization is an important step towards automated program repair.
so we further evaluate the effectiveness of fault localization on the benchmark and compare it with the existing works.
.
experiment setup weimplementeddeepfdinpython andconductedexperiments on a machine with intel i7 8700k cpu and nvidia geforce titan v 12gbvram.formutationgeneration weruneachdnnmodel20 times to obtain a reliable statistical results.
to address the randomness in dnn training we run each experiment times.
for the thresholdsindeepfd weset and tobe0.2and0.
respectively following the settings in .
we used their default parameters inthediagnosismodels.wedidnotfine tunetheparametersforbetter performance to facilitate reproducibility.
in addition parameter tuning is not the theme of this work.
we collected runtimedata using the same default interval as in prior work .
the experimental results are made available for validation .
datasetsandoriginaldlprograms.
weperformedourevaluation on six popular datasets circle blob mnist cifar imdb and reuters .
circle and blob are twodatasetsfromsklearn forclassificationtasks.mnistisa gray scale image dataset used for handwritten digit recognition.cifar10 is a colored image dataset used for object recognition.
imdbisamoviereviewdatasetforsentimentanalysis.reutersisanewswiredatasetfordocumentclassification.forthedlprograms weusetheprogramspublishedbyarecentwork astheoriginal programs for mutant generation.
specifically the work p r o vided495dnnmodelsandtheirtrainingscriptsofvariousdnn modelstructures convolutionalneuralnetwork recurrentneural network and fully connected layers only for these six datasets.
among them we were ableto reproducethe training of233 dnn models.
therefore we used these dnn models as the original models to generate mutants following the process described in section .
.
the statistics of generated mutants is shown in table entry mutant .
for rq1 the training set and validation set were splitinaproportionof7 .theevaluationwasconductedonthe validation set.
baselines.
in the experiment we compared deepfd with three baselines umlaut autotrainer and deeplocalize .
specifically thefirstbaseline umlaut isaheuristic based frameworkproviding aninterfacetodebug faultsindatapreparation modelarchitectureandparametertuning.inparticular umlautallowsuserstocheckthestructureofdeeplearningprograms modelbehaviorduringtraining.afterdetectingfaults itprovides a set of human readable error messages and repair advice.
the secondbaseline autotrainer isdesignedfordetectingfivetypesof trainingproblems i.e.
gradientvanish gradientexplode dying relu oscillating loss and slow convergence .
we use its default parametersinourevaluation.thethirdbaseline deeplocalize is abletoidentifythefaultylayerswhennumericalerroroccurs.both approaches monitor the runtime information e.g.
weight and gradient duringtraining andreporttrainingproblemsorfaultylayers once detected.
besides deeplocalize performs the fault localiza tion using two methods translating the code into an imperative representationmanuallyorinsertingcustomizedcallbackfunctions.
weusedtheirsecondmethodforcomparisonbecausethesecond oneachievesbetterperformancethanthefirstmethodaccordingto authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
deepfd automated fault diagnosis and localization for deep learning programs icse may pittsburgh pa usa theirpaper.althoughthesebaselinesdonotexplicitlypointoutthe fault typesthey addressed the fault types canstill bemappedfrom the patches or faulty layers they provided to the fault types.
after the mapping the fault types appearing in the evaluation buggy programs can be covered by the baselines.
the three baselines were not originally designed for fault localization.theydonotlocatefaultylinesdowntoprogramcode.to measure their effectiveness for fault localization we adapt their diagnosisresultsasfollows.forumlaut wemanuallyidentifythelinesofcoderelatingtotheerrormessages.forautotrainer thoughitdoesnotexplicitlylocatethefaults ittriestorepairwhentraining problems happens by applying predefined solutions.
therefore we regard the location of the solutions adopted by autotrainer as the faultitlocated andmanuallyidentifythe linesofcoderelatingto the repaired artifact e.g.learning rate layers initialization to the corresponding lines.
for deeplocalize we map the reported layer tothecorrespondinglinesintheprogram.andbecauseofthemanualworkthatisinvolvedinbothfaultdiagnosisandlocalization stages we do not compare the overall runtime of each stage.
evaluation criteria.
weadoptthefollowingcriteriaindecidingwhetherafaultissuccessfullydetected diagnosedandlocalized.
a fault is successfully detectedif the detection result corresponds totheexistenceofarealfault regardlessofwhetheritsrootcauses i.e.types of faults has been identified.
a fault is successfully diagnosedifitsrootcausehasbeenidentified.iftherearemultiple types of faults a successful fault diagnosis refers to those that can pinpointatleastoneofthem.
forfaultlocalization weexamine whetheranyofthefaultylinesiscorrectlylocatedafteracorrect diagnosis.
.
rq1.
relevancy of diagnostic features weevaluatetherelevancyoftheextracteddiagnosticfeaturesby showingtheaccuracyofthediagnosismodelstrainedwiththese features.ifthemodelsperformwellwhenusingthesefeatures then weregardthesediagnosticfeaturesasrelevanttofaultdiagnosis.
theexperimentalresultsareshownintable5.beforeanalyzingthe prediction result we first demonstrate the statistics of the mutants generatedinthefirststepofthebootstrapstage.specifically for eachdataset welistthenumberofnormaldnnmodels origin thenumber of generated mutants mutant the detailed distribution ofhowmanymutationoperatorsareappliedtothegeneratedmutants column mutation operator and the average time for training each mutant in seconds time .
on top of this training set we further trained diagnosis models with three underlying algorithms i.e.
knn dt and rf areabbreviations of k nearest neighbors decision tree and randomforest and demonstrated the accuracy of these models.
in theimplementation we normalized the features to better fit the knearest neighbors algorithm.
as shown in the entry of accuracy and average runtime of diagnosis models in table the average accuracy of these diagnosis models range from .
to .
over different datasets.
besides the accuracy obtained varies cross datasets ranging from .
to .
.
among all these datasets the best performance of three underlying algorithms is achieved onimdbdataset withtheaccuracyvaryingfrom88.
to93.
.
on the other hand the accuracy on circle and cifar tend tobe worse.
the result may be caused by the unbalanced mutants with different numbers of faulty cases.
the various performance of different underlying algorithms on datasets also suggest us to aggregate the diagnosis decisions by taking advantage of different diagnosis models.
answertorq1 theextracteddiagnosticfeatureshavestrong correlationswithourtargetedfivefaulttypes whichisreflected by the accuracy of diagnosis models with an average accuracy varying from .
to .
and up to .
for certain cases.
.
rq2.
effectiveness of fault diagnosis.
toanswerrq2 weevaluatetheeffectivenessofdeepfdinterms offault diagnosison the52 buggyprograms5fromthe benchmark.
notethatthesubjectsusedforrq2andrq3donotoverlapwith those used in rq1.
due to space limitation table shows partially ourevaluationresults 6includingthediagnosisinformationprovided by each method column diagnosis information of different methods whether they detect the existence of faults column fault detection and whether the fault diagnosis is correct or not column faultdiagnosis .umlautenumerateserrormessages with three severity levels i.e.
error critical and warning .
autotrainerreportsthetrainingproblem.deeplocalizereportsthe place e.g.
layer andthebatchatwhichthepredefinedsymptom occurs.
deepfd reports the diagnosed fault types.
the summary and overall ratio of table summarize the resultsofeachworkonthebenchmark.umlaut deeplocalize anddeepfdcandetect69 to96 faultycases.althoughumlaut detectsthemostnumber offaultycases only14ofthemare correctly diagnosed.
in contrast deepfd is able to detect faulty cases and correctly diagnosed of them.
therearedifferentreasonsforinaccuratediagnosis.umlaut isdesignedforclassificationproblems.itassumesthattheoutput layer is always activated by softmax.
if this is not the case it reports missingsoftmaxlayerbeforeloss .however thisassumption does not necessarily hold.
activation functions like linearand sigmoid arealsofrequentlyusedtoactivatetheoutputlayerfor classificationandregressionproblems.indeed mostofthefaults detected by umlaut are attributed to the violation of this assumption causingmanyfalsealarms.autotraineronlydetectsfivetrainingproblems e.g.dyingrelu leavingmostcaseswithoutapparent symptoms escaped from its detection.
deeplocalize detects faults that cause numerical anomalies such as nan and reports the placewheretheanomalieshappen.yet anomaliesrarelyoccurat the point where the faults reside making the fault diagnosis bydeeplocalize inaccurate.
on the other hand the effectiveness of deepfdislimitedbythenumberoffaulttypesthatcanbeclassified bythediagnosismodels.withmoretypesoffaultsseeded more faults can be detected and diagnosed and thus the effectiveness of deepfd can be improved.
5therestsixprogramswereomittedbecausetheyeitherwereunabletobeadaptedto launch the methods or crashed when the methods are applied.
6we listed the cases that can be correctly diagnosed by at least three methods among umlaut autotrainer deeplocalize and deepfd.
the complete experimental results are online available authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa cao et al.
table5 effectivenessofdiagnosismodelsofdeepfdonlabeleddnnset.knn dtandrfstandfortheunderlyingalgorithms k nearest neighbors decision tree and random forest of diagnosis models.
statistics accuracy and average runtime of diagnosis models origin mutant types of faultstimeknn dt rf dataset acc time s acc time s acc time s mnist .
.
.
.
.
.
.
cifar .
.
.
.
.
.
.
circle .
.
.
.
.
.
.
blob .
.
.
.
.
.
.
reuters .
.
.
.
.
.
.
imdb .
.
.
.
.
.
.
total average .
.
.
.
.
.
answer to rq2 deepfd outperforms the existing works on fault diagnosis with a fault diagnosis rate of which almost doubles that of the best baseline i.e.
.
.
rq3.
effectiveness of fault localization the fault diagnosis column in table gives the evaluation result of fault localization.
deepfd outperforms other methods by correctlylocating42 offaults whichfavorablycomparestothe by umlaut the best baseline performer.
the performance of autotraineranddeeplocalizearerelativelyunsatisfactory ranging from to .
indeed theperformanceonfaultlocalizationarenotsatisfactory ranging from to .
the reasons behind are mainly two folds.
first theratioofcorrectlydiagnosedcasesarenothigh whichisat most .
second considering the complicated types of faults and possible multiple patches an effective localization strategy is to be explored.
for example suppose we know the root cause is the use of inappropriate activation yet there are multiple activationfunctionsinseverallayers.consequently locatingfaultstoaspecific activation function at a specific layer is an outstanding challenge.
answer to rq3 the results show that deepfd significantly outperformsthethreebaselinesbycorrectlylocating42 ofthe cases.incomparison only10 to23 casescanbelocatedby the baselines.
related work .
debugging deep learning systems recently abranchoftechniqueshavebeenproposedtofacilitate thedebuggingprocessofdeeplearningsoftwaresystems.several works focus on the dnn models locating suspicious neurons and correcting them by prioritizing or augmenting the training data.
for instance ma et al.
leveraged the state differential analysis toidentifythefeaturesthatareresponsiblefortheincorrectclassificationresults andthengeneratetheinputsbasedonthesefeaturestocorrectthe behavioursofthednnmodels.eniser et al.
pr oposeddeepfault aframeworktoidentifythesuspiciousneurons inadnnandthenfixtheseerrorsbygeneratingsamplesformodel retraining.
apricot is a weight adaption approach to fix dnnmodels.theirintuitionisthatthelimitationofthecompletemodels maybeaddressed viaadaptingtheweightof thecompactmodels.
these previous studies concentrate on the the dnn models and they are not able to locate the faulty lines in dnn programs.
besides there are several studies that are closer to ours.
for instance autotrainer proposed an automatic approach to detect and fix the training problems in dnn programs at runtime.it particularly focuses on detecting and repairing five common training problems gradient vanish gradientexplode dying relu oscillating loss and slow convergence.
it encapsulates and automates the detecting and repairing process by dynamic monitoring.
however itreliesonpredefinedrulestoperformthebugdetection.
wardatet al.
proposeddeeplocalize thefirstfaultlocalization approach for dnn models such as incorrect learning rate or inappropriatelossfunction .yet itlocatestolayerswherethesymptom happens instead of where the fault s root cause resides.
amazon sagemaker debugger and umlaut both provide a set of built inheuristicstodebugfaultsindnnmodelsduringtraining.
themajordifferencebetweenourworkfromthesemethodsisthat deepfd does not rely on predefined rules making it more flexible and adaptive for various types of faults.
.
automated machine learning automated machine learning automl provides methods and processes to make machine learning available for non machine learning experts.
the user simply provides data and the automl systemautomaticallydeterminestheapproachthatperformsthe bestforaparticularapplication .inparticular therearethree mainproblemsinautoml includinghyperparameteroptimization hpo neural architecture search nas and meta learning.
hpo search for methods to set optimal hyperparameters in ml algorithms automatically thus reducing the human efforts necessary for applying ml.
yet it is not always clear which hyperparameters of an algorithm need to be optimized and in which ranges .
in contrast deepfd differs from hpo in nature.
deepfd diagnoses and locates faults in a given model while the hpomethodssearchanoptimalmodelfromscratch.
nasmethods are designed to automatically design morecomplexneuralarchitectures.meta learning aimstoimprovelearningacrossdifferenttasksordatasets.
it can significantly improve the efficiency of hpo and nas with authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
deepfd automated fault diagnosis and localization for deep learning programs icse may pittsburgh pa usa table comparison on diagnosis information fault detection diagnosis and localization between umlaut ut autotrainer at deeplocalize dloc and deepfd dfd .
post diagnosis information of different methods faultdetection faultdiagnosis faultlocalization umlaut autotrainer deeplocalize deepfd ut at dloc dfd ut at dloc dfd ut at dloc dfd critical missing softmax layer before loss warning possible overfittingexplodelayer error in forward stop at epoch batch accuracy .
loss inffault1 lines fault lines error input data exceeds typical limits warning possible overfitting warning check validation accuracy critical missing softmax layer before loss critical missing activationfunctions warning last model layer has nonlinear activationexplode fault1 lines critical missing softmax layer before loss warning last model layer has nonlinear activation batch0 layer error in delta weights terminating trainingfault1 lines critical missing softmax layer before loss critical missing activationfunctions warning last model layer has nonlinear activationunstable fault1 lines fault2 lines critical missing softmax layer before loss warning last model layer has nonlinear activationexplodelayer error in delta weights stop at epoch batch 24accuracy .
loss .
fault1 lines error input data exceeds typical limits warning check validation accuracy critical missing softmax layer before lossunstable fault1 lines critical missing softmax layer before loss critical missing activationfunctions warning last model layer has nonlinear activation error image data may have incorrect shape warning learning rate is high warning check validation accuracy unstablebatch0 layer error in output gradient terminating trainingfault1 lines fault lines error input data exceeds typical limits critical missing softmax layer before loss warning last model layer has nonlinear activationunstablebatch0 layer error in forward terminating trainingfault1 lines ...... summar y501236471410 overall ratio .
.
.
.
.
.
.
.
.
.
.
.
the help of the transferred knowledge between tasks.
note that thoughthesemethodsarenotdesignedfordebugginganexisting program itispotentialtoapplythesemethods especiallythehpo andnasonesforrepairingafterfaultsarediagnosedandlocalized.
the diagnosed information can serve as a guidance narrowing downtheparametersthatneedtobetuned andthesearchspace for nas to explore.
threats to validity in this section we discuss three threats that may affect the validity of our work.
first the construction of benchmark e.g.the reproduction rootcauseanalysisandrepair involvedmanualinspection of the source code which may be subjective.
to reduce this threat each subject was examined by three authors separately and the results were cross validated.
decisions were made only if the three authors reached an agreement.
second to prepare the training set for diagnosis models we assume the original programs i.e.programsbeforeseedingfaults arefault free yetitmaynotalwaysbe the truth.
though they are released and guaranteed to be free from five training problems e.g.gradient vanish and dying relu itisstillpossibletherearehiddenfaultsintheprogram.third to reproducetheresultsofexistingworks weadoptthedefaultvalues for the parameters which may affect their performance and efficiency.
besides some works need to manually adapt the programs in order to launch the debugging process which may introduce unexpectedvariancefromtheoriginalprogram.also withthemanual work involved the time cost by each work is hardto evaluate leaving the efficiency of each work incomparable.
finally sinceexisting works cannot locate faults to the program we carefullyinvestigate their diagnosis information and manually locate the faulty lines for fair comparisons which may also slightly affect the comparison results.
conclusion inthispaper weproposeddeepfd alearning basedfaultdiagnosis and localization framework which maps the fault localization task to a learning problem.
in particular deepfd diagnosis faults by classifying runtimefeatures intopossible types offaults e.g.inappropriateoptimizer thenlocates faultylines tothe program.
the evaluation shows the potential of deepfd.
specifically it correctly diagnoses of the cases compared with half achievedby the best state of the art works.
besides for fault localization deepfd also outperforms the existing works correctly locating faultycasesonthebenchmark whichalmostdoublestheresult achieved by the best state of the art work.
acknowledgment this work was supported by the national key research and developmentprogramofchina no.2019yfe0198100 nationalnatural science foundation of china grant nos.
the hongkongrgc grf grantno.
hongkongitf grant no.
mhp huawei phd fellowship and msra collabo rative research grant.
the authors would also like to thank the anonymous reviewers for their comments and suggestions.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa cao et al.