decompovision reliability analysis of machine vision components through decomposition and reuse boyue caroline hu university of toronto toronto ontario canada boyue cs.toronto.edulina marsso university of toronto toronto ontario canada lina.marsso utoronto.canikita dvornik waabi toronto ontario canada nikita.dvornik proton.me huakun shen university of toronto toronto ontario canada huakunshen cs.toronto.edumarsha chechik university of toronto toronto ontario canada chechik cs.toronto.edu abstract analyzing reliability of machine vision components mvc against scene changes such as rain or fog in their operational environment is crucial for safety critical applications.
safety analysis relies on the availability of precisely specified and ideally machine verifiable requirements.
the state of the art reliability framework icraf developed machine verifiable requirements obtained using human performance data.
however icraf is limited to analyzing reliability of mvcs solving simple vision tasks such as image classification.
yet many real world safety critical systems require solving more complex vision tasks such as object detection and instance segmentation.
fortunately many complex vision tasks which we call c tasks can be represented as a sequence of simple vision subtasks.
for instance object detection can be decomposed as object localization followed by classification.
based on this fact in this paper we show that the analysis of c tasks can also be decomposed as a sequential analysis of their simple subtasks which allows us to apply existing techniques for analyzing simple vision tasks.
specifically we propose a modular reliability framework decompovision that decomposes the problem of solving a c task the reliability requirements and the reliability analysis and as a result provides deeper insights into mvc reliability.
decompovision extends icraf to handle complex vision tasks and enables reuse of existing artifacts across different c tasks.
we capture new reliability gaps by checking our requirements on widely used object detection mvcs and for the first time benchmark segmentation mvcs.
ccs concepts software and its engineering requirements analysis computing methodologies computer vision .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse december san francisco ca usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
software engineering for artificial intelligence requirements engineering software analysis machine learning computer vision acm reference format boyue caroline hu lina marsso nikita dvornik huakun shen and marsha chechik.
.
decompovision reliability analysis of machine vision components through decomposition and reuse.
in proceedings of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering esec fse december san francisco ca usa.
acm new york ny usa pages.
.
introduction machine learning has enabled unprecedented progress in computer vision cv and now allows to fully automate vision tasks traditionally performed by humans .
yet most modern cv models can be surprisingly unstable w.r.t.
perturbations to the input images .
since machine vision components mvcs are currently used in safety critical systems e.g.
self driving cars mvc prediction errors can lead to fatal accidents .
thus the instability in mvc s predictions is a major safety concern that motivates the study of system reliability against visual changes in the environment .
the central challenge in mvc reliability analysis is the absence of precisely specified and machine verifiable requirements.
to address this challenge the sota image classification reliability analysis framework icraf obtains the machine verifiable requirements using human perception performance as a baseline via experiments with human participants.
specifically icraf s requirements state that if changes in a scene do not affect humans solving a vision task they should not affect an mvc solving the same task either.
however a fundamental limitation of icraf is that it is only applicable to a single atomic vision task with one type of output i.e.
image classification that given image entails producing a single class label as output .
at the same time many critical applications involve complex vision tasks that require producing multiple types of outputs .
for example parsing the surrounding environment for autonomous driving requires solving object detection that entails joint object localization and classification resulting in two output types object bounding boxes and their categories.
automatic tumor analysis involves instance segmentation that consists of localization classification and outline detection involving three outputs types object boxes their categories and segmentation esec fse december san francisco ca usa boyue caroline hu lina marsso nikita dvornik huakun shen and marsha chechik figure examples of c task decomposition object detection and instance segmentation.
top decomposition of c tasks into their subtasks.
object detection and instance segmentation share two subtasks in dotted frame localization blue and classification of localized objects orange .
bottom each vision task is illustrated with an image from the pascal voc dataset .
s system solving vv complex vision task c task o1 output type on output type n ...mv performance metrics measuring how well s solved vworkflow for analyzing solving c tasks decomposition decomposable problem eq.
not decomposed decomposable metric eq.
example vd object detection s mvc detector mv average precision ol object boxes oc l object classes figure overview of the common workflow for analyzing and solving a c task.
first row gives the workflow for solving a c task.
second row displays the decomposition characteristics of each above module in the workflow.
third row illustrates the modules for the c task object detection.
masks.
therefore there is a need of machine verifiable reliability requirements for tasks involving different output types.
c task a class of complex vision tasks.
in this paper we focus on complex vision tasks that can be represented as a sequence of atomic vision subtasks we call this class of tasks c tasks .
many c tasks are required for real world safety critical systems which include but are not limited to object detection human object interaction detection trajectory prediction and language visual grounding.
for example as shown in fig.
object detection can be sequentially decomposed into first localizing objects vl and then classifying the localized objects vc l .
the common workflow for solving and analyzing a c task is illustrated in fig.
top .
given a c task some system or component such as an mvc or a human solves it and produces multiple output types e.g.
bounding boxes and object categories each solving the corresponding subtask.
then performance metrics are applied to all system s outputs to quantify how well the task was solved e.g.
average precision ap is used to measure the quality of an object detector .
decomposition principle.
in this paper we address reliability analysis of mvcs performing c tasks with machine verifiable requirements.
to do so we define a decomposition principle dp for decomposing both the problem of solving c tasks and the performance metrics measuring how well a system solves a c task see fig.
bottom .
specifically dp decomposes c tasks into the corresponding atomic subtasks each assuming a single output type suchas there is a bijection between the output types of the c task and of the atomic subtasks.
analogously dp decomposes the performance analysis of a c task via sequentially analysing performance metrics of its subtasks.
as a result dp can decompose the entire workflow without decomposing the process solving the c task itself which enables black box analysis.
note that while we restrict ourselves to c tasks we are not aware of complex tasks deployed on safety critical systems that are not decomposable with dp.
modular reliability framework.
based on our dp we develop a modular reliability framework decompovision see fig.
for c tasks which builds on top of icraf .
given a c task our framework first decomposes it into atomic subtasks step i .
then for each subtask independently given an image transformation simulating scene changes we use human performance data to generate machine verifiable reliability requirements step ii.a and ii.b .
consequently we compose the individual subtask requirements to get the requirements for the c task step ii.c .
finally we propose a black box checking method for both the overall c task and the subtask requirements which enables failure localization in the sequence of subtasks.
crucially the modularity of decompovision allows us to reuse human performance data requirements specifications and analysis artifacts for shared subtasks across different c tasks.
for example instance segmentation and object detection share two subtasks vlandvc l see fig.
thus the experiment data requirements and testing data for vlandvc lcan potentially be shared between instance segmentation and object detection.
contributions.
in this paper we define the decomposition principle dp for c tasks and their performance metrics.
then using dp for c task we develop a modular framework decompovision that generates machine verifiable reliability requirements for the c task and its subtasks checks satisfaction of the requirements and builds the experiment measuring human performance on object detection.
we also provide human performance data for object detection and instance segmentation from about participants.
evaluation.
while decompovision is defined for any c task we demonstrate its feasibility on object detection and instance segmentation.
using widely used object detection mvcs and the pascal voc dataset we show that decompovision captures reliability gaps that the sota object detection reliability benchmark is unable to detect.
moreover we are the first to benchmark instance segmentation mvcs.
finally we show that reusing human experiment data reduces experiment time by and reusing testing data reduces runtime of checking requirements satisfaction by .
significance.
existing approaches propose reliability analysis of image classification mvcs from different se perspectives including requirements testing and verification .
decompovision allows extending the above reliability approaches to analyze mvcs performing c tasks instead of re implementing new analysis approaches.
furthermore decompovision analyzes the mvc reliability not only on the c tasks but also on their subtasks.
this leads to a deeper insight into the mvc reliability such as fault localization and guides further mvc development.
finally the subtasks based modular design of our framework enables reuse of analysis artifacts of atomic vision tasks.
542decompovision reliability analysis of machine vision components through decomposition and reuse esec fse december san francisco ca usa this paper is organized as follows sec.
provides background.
sec.
develops the decomposition principle dp.
sec.
describes the experiment with human participants.
sec.
describes reliability requirements generation for an mvc on a c task and its sub tasks.
sec.
introduces our method for checking the mvc against the reliability requirements.
sec.
reports on the experimental evaluation.
sec.
discusses related work.
sec.
concludes the paper.
background the icraf framework enables reliability analysis of image classification mvcs against scene changes that do not affect human performance i.e.
changes within the human tolerated range .
for example humans have no problem recognizing cars in the images with added frost in the first three images in fig.
as opposed to the last image in fig.
thus the amount of added frost of these images is within the human tolerated range.
to analyse reliability icraf proposes two requirement classes correctness preservation and prediction preservation that are parameterized by an image transformation simulating scene changes.
below we review the key concepts of the icraf framework for image classification the machine verifiable reliability requirements classes that use human performance as a baseline how the human performance data is collected and used to estimate the reliability requirements parameters and the method for checking the requirements satisfactions.
reliability requirements.
given an mvc fvperforming a vision taskv a distribution of input images px a transformation txwith parameter domain cand parameter distribution pc the requirement classes correctness preservation cp and predictionpreservation pp are defined on the joint distribution ptx x x of pairs of original and transformed images.
to measure the degree of visual change between original and transformed images icraf proposes a metric visual change v which allows specifying requirements for any pixel level image transformation.
examples of vcaused by the frost transformation are shown in fig.
.
thecorrectness preservation reliability requirement class cp v cp tx tc assumes the ground truth labeling function f vand a performance measure cp fv f v px which measures similarity between the output of fvandf v given input x px and a distribution of transformed images with changes within the human tolerated range v tc i.e.
ptx tc.
definition correctness preservation reqirement .
for the range of changes in images that do not affect human performance v tc the performance of mvc fv should not be affected either.
formally the performance evaluated by metric cpoffvfor transformed images is required to be greater than that for original images cp fv f v ptx tc cp fv f v px .
example lettxbe the transformation adding artificial frost and tcbe0.
.
the instantiated correctness preservation requirement for an image classification mvc is the recognition accuracy of an mvc should not decrease if the visual change in the images is within the range v .
.
theprediction preservation requirement class pp v pp tx tp requires a prediction similarity measure pp f px x which measures the expected similarity between the output of fvon an image pair drawn from px x a distribution of original and transformed image pairs that are within the human tolerated range v tp i.e.
ptx tp x x and a distribution of original and minimally transformed image pairs that are within the minimal image change v i.e.
ptx x x .
definition prediction preservation reqirement .
for the range of changes in images that do not affect human predictions the predictions of mvc should remain unaffected.
formally the prediction similarity evaluated by metric ppoffv for all transformed images v tp is required to be greater than or equal to that for minimally transformed images v pp fv ptx tp pp fv px .
example lettxbe the transformation adding artificial frost and tpbe0.
.
the instantiated cprequirement for an image classification mvc is the percentage of labels the mvc can preserve after adding frost should not decrease if visual change in the images is within the range v .
.
experiments with human participants.
in order to estimate the parameters of the requirement classes the human tolerated thresholdstcandtp for correctness preservation and predictionpreservation respectively icraf collects human performance data on image classification.
the goal of the experiment is to evaluate average human performance on original and transformed images with different degrees of visual change in order to understand the range of visual change where the average human performance stays unaffected.
specifically the experiment is performed on a given dataset xi xi px for a specific image transformation tx.
first an image xiis randomly selected from the dataset and then transformed i.e.
x i j tx xi cj where the transformation parameters cjare selected to cover all possible visual changes v. this results in a dataset of original and transformed image pairs xi x i j xi x i j ptx.
to obtain the human predictions on each image in xi x i j the icraf experiment is a forced choice image categorisation task humans are presented with an image for ms and asked to choose one of the two categories e.g.
car or not car .
between the classification attempts a noise mask is shown to minimize feedback influence in the brain .
the tasks are timed to ensure fairness in comparing humans and machines .
after the human performance data is collected it is used to estimate the human tolerated thresholds tcandtp.
the thresholds are defined as the smallest value of visual change vat which the requirement inequality in def.
or does not hold.
importantly the human tolerated thresholds tcandtp estimated above using the dataset xi are not specific to this dataset these thresholds can be used on other images that share characteristics with the experiment image data.
checking requirements satisfaction.
icraf includes a method for checking satisfaction of a given requirement defined with a metricm cp pp and a threshold t tc tp .
it takes as input a list of images a set of image transformations and an mvc under validation and generates test cases within the specified range of v i.e.
v t .
it runs the tests on the mvc and checks whether the mvc satisfies the requirements by estimating the reliability distance defined below .
the test cases are obtained via sampling test images.
after executing them on the mvc icraf estimates the metric values following the bootstrap method given an image distribution pxand a transformation txwith a parameter domain 543esec fse december san francisco ca usa boyue caroline hu lina marsso nikita dvornik huakun shen and marsha chechik i. decompose complex vision task ii.
generate composed reliability requirements iii.
checking method legend multiple per sub tasks existing method step artifact manualcomplex vision task c task image transformationi.
decompose complex vision task into atomic vision sub tasksatomic sub taskatomic sub taskatomic sub taskexperiment data of human performanceexperiment data of human performanceexperiment data of human performance ii.b generate the reliability requirementsii.a estimate the human thresholdsii.a estimate the human thresholdsii.a estimate the human thresholdsmachine verifiable requirementsmachine verifiable requirementsmachine verifiable requirements ii.c compose the reliability requirementcomposed machineverifiable requirementiii.c check composed requirement satisfactioniii.a check requirement satisfactioniii.a check requirement satisfactioniii.a check requirement satisfaction figure the decompovision framework.
step i decomposes a complex vision task into sub tasks step ii generates reliability requirements for mvcs with complex vision tasks step iii checks their satisfaction.
an original image v 0minimal changes v .005a reasonable change v .71an extreme change v .
figure imagenet images with different levels of added frost.
c a set of test images is sampled with replacement from pxand then transformed with txusing parameters sampled from cwhile ensuring the v trange.
using bootstrap icraf estimates the distribution of the metrics values on the transformed mt and original m0 images.
the requirement is considered satisfied if mt m0 0with confidence.
throughout the rest of the paper mt m0is referred to as a reliability distance .
task and metrics decomposition many complex vision tasks e.g.
object detection used in safetycritical systems can be represented as a sequence of atomic vision subtasks .
we call this class of tasks c tasks .
in this section we define a decomposition principle dp for decomposing the problem of solving c tasks so that their performance metrics can be composed from performance metrics of their subtasks.
c task problem decomposition.
letc task denote a complex vision task vthat given a visual input x such as an image or a video produces a sequence of ndifferent output types i.e.
v x .
for example object detection vd entails localizing objects on an image and then classifying each of them this effectively produces two different types of output object bounding boxes o1 and their class labels o2 .
let an atomic subtask a.k.a.
subtask be a vision task vithat produces the output type oi i.e.
vi x o1 ... oi oi where viuses previous outputs ok i k 1to produce the current output type oi.
therefore a c task vis a sequential composition of the subtasks i.e.
v vn ...v2 v1 n i 1vi where represents the sequential composition operator.
in the example with object detection see fig.
the subtask v1is object localization vl and v2is the classification of localized objects vc l. thus the full task can be represented as vd vc l vl.
system solving a c task.
given a visual input x asystems produces a sequence of ndifferent output types i.e.
s x to solve a c task v. ifssolves a c task successfully given an input x then the probability of observing the desired output is i.e.
p v p on ... o x .
in the following weshow that the probability of solving a c task p v can be decomposed into probabilities of solving its individual subtasks.
using the conditional probability chain rule the probability of observing all the outputs can be decomposed into probabilities of observing a sequence of individual outputs i.e.
p v p on ... o x n i 2p oi oi ... o x p o1 x .
according to the atomic subtask definition i.e.
vi x o1 ... oi oi every term in the above product corresponds to the probability of solving an atomic subtask vi i.e.
p vi p oi oi ... o x .
therefore by chaining the above qualities the probability of solving a c task decomposes into the probabilities of solving its atomic subtasks p v p n i 1vi n i 1p vi .
in other words eq.
shows that for any system swith input x and outputs i.e.
sis a black box one can analyze the ssolving the c task vby analyzing its atomic subtasks vi.
this effectively means that the analysis of the c tasks follows the same c task decomposition eq.
!
example given an mvc sfor object detection vd the probability of solving vdcan be decomposed into successfully solving localization vland classification given localization vc l i.e.
p vd p vc l p vl .
performance metrics decomposition.
given a c task vand a systemssolving it task specific performance metric mis used to measure how well the system ssolves v. more precisely given a visual input x the corresponding system s outputs and the ground truth the performance metric mcompares the system s output with the ground truth and returns a value indicating how close they are to each other the closer the better .
the performance of an mvc or a human on a given vision task is usually evaluated using task specific performance metrics.
for example the performance metric accuracy is used for the task of image classification it measures the percentage of images correctly classified by the system.
in object detection average precision ap is often used to describe the localization and classification performance using a single number.
even though a c task implies solving a number of subtasks its performance metric is often a single scalar value that provides no insights into how different subtasks influence the final performance.
the inability to capture the influence of every comprising subtask on the overall mvc s performance hinders the analysis of mvcs on c tasks.
to solve this problem we leverage the fact that many popular performance metrics can be described by the probability of successfully solving the c task eq.
.
for such metrics mv we can directly apply eq.
and break mvinto a combination of 544decompovision reliability analysis of machine vision components through decomposition and reuse esec fse december san francisco ca usa performance metrics mviof the underlying subtasks vi i.e.
mv n i 1mvi.
here mviis a task specific metric of an atomic vision task vi.
we call the metrics mvthat can be decomposed in such a way directly decomposable .
for example a popular metric precision used for object detection is directly decomposable as shown in the supplementary material1 it can be directly represented as a product of precision of localization vl and classification given localization vc l i.e.
precisiond precisionc l precisionl.
analogously we can decompose the recalldmetric and as a consequence the precision recall curve prd defined as a function of precision depending on recall .
some metrics such as average precision ap are more complex and are not directly decomposable.
yet apis defined as a function of the pr curve which is directly decomposable.
for such complex metrics mvthat are not directly decomposable we extend the metric compositionality definition as follows mv f m v wherem v n i 1mvi wherem vis a directly decomposable metric of the task v mvi is a metric of the i th subtask and fis a monotonic function.
in other words a metric mvisdecomposable if it can be represented as a function of some directly decomposable metric m v.2defining metric compositionality this way allows us to understand how every subtask s metric mviinfluences the c task s metric mv and thus enables us to gain more insight into the mvc analysis.
for example consider computing apfor object detection apd and instance segmentation api .
using eq.
apcan be decomposed intoapd auc prl prc l andapi auc prl prc l prs c l whereauc is area under curve on interval.
as a result being able to decompose existing c task metrics such as apdand api allows us to benefit from reusing existing popular metrics such as pr and their inherent analysis methods.
decomposition principle dp .
to summarize in this section we have shown that we can decompose a c task eq.
into its corresponding subtasks and c task performance metric eq.
and eq.
into subtask performance metrics all without decomposing the system solving the c task itself.
in the remaining of the paper we refer to this as the decomposition principle dp .
collecting human performance in this section we present our approach to collecting human performance data on c tasks which is used as a baseline in reliability requirements .
to do so we propose to decompose the c task into subtasks sec.
and collect human performance data for each subtask.
consequently the human performance on the c task is obtained by combining the per subtask performance data following dp.
this modular approach enables reuse of existing experiment design and data for the subtasks shared across different c tasks.
the human performance data for a c task vis obtained by sequentially collecting the human performance data for each of its n sub tasks vi i ... n .
specifically we first follow the icraf 1see 2see supplementary material for compound decomposable metrics.procedure for generating a distribution of transformed images ptx given an image dataset xj px and an image transformation tx sec.
.
the original and the transformed images are used in the human experiments for every subtask.
then for each subtask vi we conduct the experiment timed to ensure the experiment s fairness see sec.
and obtain the human performance results ovi using results from human performance on subtasks ok i k .
the obtained performance results are generic to datasets sharing the same characteristics e.g.
image resolution .
note on fairness to ensure that an mvc is fairly compared to a human on a given task it is crucial to limit the time given to a human to solve the task.
existing studies establish the human time bound for classical subtasks such as classification and localization .
however it is not trivial to set a timer for the c task as such task require producing multiple output types.
by decomposing the ctask experiment we avoid the above issue and can directly reuse the existing research on fairness for individual tasks.
in this paper we design a fair experiment for collecting human performance results on object detection vd problems with the help of a cognitive science expert.
we follow the above procedure for collecting human performance on vdand decompose it into localization and classification given localization i.e.
vd vc l vl and perform a human experiment on each subtask.
localization vl .given an image human participants are asked to localize the objects via bounding boxes ol .
the bounding box is drawn by clicking on the top left corner dragging across the diagonal and releasing the mouse button in the bottom right corner.
a participant is limited to 4s to draw a bounding box .
as soon as a box is placed the timer resets and the participant is given another 4s to localize other objects.
the process on the given image ends when the participant is unable to create a bounding box in 4s.
classification given localization vc l .after localization humans are expected to label the objects oc l in the previously detected bounding boxes ol .
specifically given a bounding box ol we visualize it on the white canvas of image size and ask the participant to concentrate on that region.
then we flash the image for ms as suggested by and ask participants to classify the object that appeared in the location of the bounding box.
the classification is implemented as a multiple choice question.
in both experiments each image is shown to a participant only once which means the localization and classification tasks are performed by different participants.
this gives us an unbiased estimate of the average human performance on object detection.
to ensure data quality we use qualification tests to filter out spammers .
experiment.
to collect human performance for our evaluation we selected two image transformations simulating realistic scene changes adding artificial frost and changing brightness.
we performed the experiment using the pascal voc dataset that has object detection and instance segmentation annotations.
for each selected image transformation we sampled 600original and their corresponding transformed images another asking a 3videos illustrating the localization and classification of the localized object experiments are available in supplementary material .
545esec fse december san francisco ca usa boyue caroline hu lina marsso nikita dvornik huakun shen and marsha chechik human to localize the objects.
each detected object ol was independently labelled by five different humans producing oc l. specifically we divided the images images across two image transformations into batches of .
each such batch was shown to a different participant using amazon mechanical turk to obtain the object bounding boxe ol.
overall the human participants identified bounding boxes in the localization experiment in about hours.
for the classification experiment we divided the collected bounding boxes into batches of 20and showed each batch to five different participants to obtain object labels oc l. this resulted in 000human object classification labels 5labels for each of images in about hours .
reuse of experiment data.
to demonstrate the benefits of subtask human performance reuse we consider the task of instance segmentation.
this task consists of object localization classification and consequent segmentation as illustrated in fig.
vi vs c l vc l vl.
instance segmentation shares two subtasks vc land vl with object detection this allows us to reuse detection experimental data.
therefore we conduct only one object segmentation experiment for vs c l i.e.
obtaining the object segmentation mask os c l given a box and a class olandoc l .
in this task humans draw the outline of the objects.
templates for this task are available on mechanical turk.
to demonstrate reuse in our evaluation in addition to the already available 000human predictions for bounding boxes and labels together we asked one person to provide object segmentation masks os c lper object resulting in additional 000human object segmentation outcomes which took about hours.
without reuse conducting instance segmentation experiment would have taken up to 30hours i.e.
the sum of and hours for obtaining ol oc landos c l respectively .
reusing prior results for olandoc lallows us to reduce the experiment time by .
c task reliability requirements in this section we present our method for generating reliability requirements for an mvc performing c tasks by composing reliability requirements for each subtask step ii in fig.
.
to build such requirements our method uses both the decomposable c task and the performance metrics sec.
and the icraf reliability requirements classes along with their parameter estimation method sec.
.
this modular way of requirements generation enables reuse of existing requirements for the subtasks shared across different c tasks.
requirements generation.
the requirement generation method takes a c task v a decomposable metric m a transformation tx a requirement type rq type correctness preservation cp orpredictionpreservation pp see sec.
and an mvc fvperforming the task v. first for every subtask vi the method uses human performance data to estimate the human tolerated range of change thresholds tcandtpforreq typecpandpp respectively .
the estimation of the threshold values for each subtask viis performed by applying the icraf estimation method see sec.
to object detection and instance segmentation performance metrics more detail in supplementary material1 .
using human performance obtained on 4since sota benchmarks provide class specific metrics the requirements in the paper are also class specific.
however our method is generic see examples of non classspecific requirements in supplementary materials.procedure generate composed reqs v mv metrics tx rq type t inputs v a decomposable vision task mv a decomposable metric metrics a list of subtaks metrics mvi derived from mv tx an image transformation rq type a requirements type cporpp and t a list of n thresholds estimated for each mvi.
reqs initialize the list containing requirements for each subtask fori ... n do loop over subtasks viin eq.
mvi metrics tvi t tinf t fort tift tvi list of thresholds tvi requirement for a subtask viusing the metric mvi reqs gen req subtask vi mvi tinf tx rq type end for tv min t compute the tightest threshold m v n i 1metrics composed metric in eq.
ifrq type cpthen reqv cp v m v tx tv composed requirement for vusingmv reqnaive v cp v mv tx tv naive requirement for vusing the metric mv else reqv pp v m v tx tv composed requirement for vusingmv reqnaive v pp v mv tx tv naive requirement for vusing the metric mv end if return reqs reqv reqnaive v original and transformed images for the transformation adding artificial frost and changing brightness using the mechanical turk platform sec.
we estimate the thresholds see tbl.
for artificial frost5.
then the method instantiates reliability requirement parameters for the subtasks vi using the corresponding subtask metrics mvi and the estimated thresholds see proc.
.
finally it composes subtask requirements to obtain requirements for the c task v see proc.
as shown in fig.
step ii.
below we elaborate the instantiation and the composition steps.
our method uses the above estimated parameters and calls proc.
to generate reliability requirements for the subtasks ll and then composes them to obtain the final requirement for c task v ll .
in order to generate the requirement we instantiate the parameters of the icraf requirement classes with the estimated visual change thresholds and a vision task metric see sec.
.
for each subtask vi there is an associated range of human tolerated visual changes given by the threshold tvi estimated in step which can be different for different subtasks.
to be able to compose the substask requirements they should be defined within the same range of changes.
therefore for every subtask vi we consider all the visual change thresholds tvj from other subtasks that are less or equal to the threshold tviof the current subtask vi i.e.
tinf tvj tvj tvi proc.
l .
to this end for each subtask we compute the icraf requirement instantiated with the subtask s metricmviand each of the tvithresholds proc.
l and define the final requirement for the subtask reqvi as the conjunction of the above requirements for different thresholds in tinf proc.
.
next we note that performing the c task vis at least as hard as performing the most difficult of its subtasks vi.
therefore for the ctask v we set the human tolerated visual change threshold tvto be the smallest of its subtasks thresholds i.e.
tv min t proc.
l .
specifically reqvallows to evaluate the contribution of each subtask vifor the overall reliability on v as it uses a directly decomposable metricm along with the threshold tvto instantiate the parameters of the icraf requirement.
on the other hand reqnaive vis obtained by instantiating the parameters of the icraf requirement with tv 5thresholds for changing brightness are in the supplementary material .
546decompovision reliability analysis of machine vision components through decomposition and reuse esec fse december san francisco ca usa original image frost frost a detection b detection c detection d detection e detection f detection g good detection h good detection i misclassification j misclassification k misclassification l good detection figure object detection on original and transformed pascal voc images by faster r cnn .
images from the second and third columns display different levels of additional frost with v .
.
the mvc outputs are shown directly in each image and the comparisons of the results with the ground truth are shown under them.
the red bounding boxes correspond to the class bus while the blue boxes correspond to not a bus .
and the original c task metric mv l l reqnaive vcan be seen as a naive extension of icraf requirements to c tasks and can be used as a baseline for our decomposable requirements.
example here we illustrate machine verifiable requirements instantiations shown in tbl.
generated using proc.
for the task of object detection vd vc l vl.
we take the images from the pascal voc dataset consider the task of detecting buses i.e.
objects of other categories are labeled as not a bus and useapas the detection performance metric.
using the estimated thresholds in tbl.
we show the generated cprequirements following our procedure in tbl.
.
intuitively cpvlchecks whether vl i.e.
object localization with bounding boxes is affected by frost.
in images in fig.
5e 5f faster r cnn failed to localize each bus after frost thus it is not correctness preserving for them.
then cpvc lchecks whether vc l i.e.
object classification is affected by frost.
as shown in fig.
5h 5i frost changes faster r cnn output label from the correct a bus to the wrong not a bus thus ittable estimated human tolerated visual change threshold values for transformation frost tcis forcp tpis forpp sec.
.
object vl vc l vs c l classtctptctptctp person .
.
.
.
.
.
bus .
.
.
.
.
.
table correctness preservation cp requirements conditions generated for object detection vd and instance segmentation vi of object class bus following proc.
against the image transformation frost .
requirements for vdthat are reused for viare shown in the same colour.
req s for object detection vd req s for instance segmentation vic task vreq name required conditionreq name required condition subtask reqv l cpvlpreserveprlvalue for images with v .9and v .7cpvlpreserveprlvalue for images with v .9and v .
cpvc lpreserveprc lvalue for images with v .7cpvc lpreserveprc lvalue for images with v .
n a cpvs c lpreserveprs c lvalue for images with v .
composed reqv l cpvdpreserveprdvalue for images with v .7cpvipreserveprivalue for images with v .
c task reqnaive v l cpnaive vdpreserveapvalue for images with v .7cpnaive vipreserveapvalue for images with v .
procedure gen req subtask v m tinf tx rq type inputs a subtask v m the metric of v a list of thresholds tinf an image transformation tx and a requirements type rq type .
ifrq type cpthen return t tinfcp v m tx t for correctness preservation else return t tinfpp v m tx t for prediction preservation requirements is not correctness preserving.
however in images fig.
5k 5l frost changes a wrong label to a correct one increasing both precision and recall in pr therefore is correctness preserving.
finally cpvd check for the preservation of performance on the overall object detection task.
since images in fig.
5e 5f and fig.
5h 5i do not satisfy subtask requirements they don t satisfy cpvd.
correctness.
given a c task v vn ...v2 v1and a decomposable performance metric mv such thatmv f m v andm v n i 1mvi letreqs reqv1 ... reqvn be the list of subtask requirements generated by proc.
where reqviis defined with mvi.
let reqvbe the composed c task requirement defined using m v. the theorem states that if an mvc satisfies subtask requirements reqs it also satisfies composed c task requirement reqv.
proof is in the supplement1.
theorem .
if all subtask requirements reqvi reqs are satisfied so is the composed c task requirement reqv.
reuse of subtask requirements.
defining reliability requirements via task decomposition allows us to reuse requirements for c tasks that share the same subtasks and thresholds.
for example given the same image distribution px subtasks vlandvc lhave the same threshold for object detection vd vc l vl and instance segmentation vi vs c l vc l vl because the human experiment data can be reused.
therefore as shown in tbl.
requirements for subtasks instantiated as parts of cpvl cpvc l ppvl 547esec fse december san francisco ca usa boyue caroline hu lina marsso nikita dvornik huakun shen and marsha chechik andppvc lfor object detection can be reused for instance segmentation.
thus the set of requirements for instance segmentation is obtained by extending the set of requirements for object detection with the requirements for the subtask vs c l and composing them to obtain the requirement for the overall task vi.
checking mvc reliability in this section we introduce our modular black box checking method for checking requirements satisfaction step iii in fig.
.
our method checks both the requirements for the overall c task and those of the subtasks via extending the icraf requirement satisfaction checking method.
as a result our method allows identifying reliability weaknesses among subtasks that compromise the reliability of the c task.
additionally it enables reuse of checking analysis artifacts of the subtasks.
modular checking method.
our method takes as input a list of images a set of image transformations reliability requirements for the c task and its subtasks and an mvc under validation.
first the method generates test images inputs within the greatest subtask s range of visual changes tmax i.e.
tmax max t where tis the list of subtasks thresholds using the icraf test images generation method.
as a result the generated test set can be used for checking all subtasks with t tmax .
then the method feeds the test images to the mvc to obtain the mvc outputs e.g.
detection predictions od .
second given the mvc predictions outputs the method checks the satisfaction of each subtask s requirement reqvi.
it is done by estimating the reliability distance that measures the difference in performance on the original and transformed images for mvc predictions on images with scene changes within a specified threshold.
the requirement is satisfied if the reliability distance is less than or equal to zero which is used as a quantitative measure of how much improvement is needed in order to meet the requirement.
since reqviis defined as a conjunction of multiple requirements with different thresholds sec.
the reliability distance is first estimated on each requirement in this conjunction using the icraf estimation method.
consequently the reliability distance forreqviis defined as a maximum reliability distance among all the requirements in the conjunction.
third our method checks the satisfaction of the composed requirement reqvfor the c task v using the decomposable metric m vin sec.
.
to compute the reliability distance for reqv we use the previously computed metric values for each subtask corresponding to the minimum threshold tv see proc.
l .
remarks the reliability distance estimated on each subtask requirement in the conjunction proc.
allows identifying the range of changes that is affecting the mvc reliability for the given subtask which can provide guidance to mvc developers on what to strengthen the reliability.
example to check whether an mvc satisfies the requirements for vdshown in tbl.
we first generate test images by sampling the original and transformed images with v tmax .90and obtain mvc predictions for all the sampled images.
to check the subtask requirements cpvlandcpvc l we compute the curve prl .9with all sampled test images shown in fig.
6a and with the subset of test images that have v .
we compute the curves prl .
andprc l .
.
the reliability distance rusing pris defined a prl forcpvl b prc l forcpvc l c prd forcpvd figure example of prcurves for checking requirements in tbl.
as the percentage of points on prfor transformed images blue that are below the prcurve for the original images green .
in this case rvc lis0.43andrvlis the maximum reliability distance of the two blue curves which is .
.
finally for checking the composed requirement cpvd fig.
6c is obtained using the previous results in fig.
6a and fig.
6b and r vd .
.
reuse of analysis artifacts.
modularity allows us to reuse many analysis artifacts computed for a given mvc test images mvc predictions and the testing results.
first test images and mvc predictions can be reused across requirements with different thresholds since test images are sampled with the maximum threshold tmax.
second computed metric values can be reused across requirements e.g.
reusing prl .7andprc l .7for computing prd .7andap.
when an mvc solving a vision subtask is reused to solve some c task the modularity of our framework allows us to reuse both analysis artifacts and results on that subtask for the c task analysis.
for example instance segmentation mvcs can be built using object detection mvcs by adding additional layers performing the segmentation task vs c l see fig.
.
for example the instance segmentation mvc mask rcnn is built on top of faster rcnn object detector.
for mask rcnn we can reuse the test images the analysis results for the subtasks reqvlandreqvc l i.e.
the reliability distances and use the test images with the bounding boxes previously computed for faster rcnn analysis.
by reusing test images and analysis results our modular checking method allows us to save significant computation resources.
evaluation in this section we demonstrate the feasibility of decompovision for object detection vd and instance segmentation vi .
first because the c task human experiment is decomposed into subtasks we can either estimate the human tolerated threshold per subtask and then compose them into c task threshold see proc.
or apply the icraf estimation method to estimate the thresholds of the c task directly.
we are interested in how the threshold values obtained by these two methods differ and thus aim to answer rq1 how does our composed human tolerated threshold for a c task compare with a threshold directly computed with icraf?
second we would like to determine whether checking our composed machine verifiable requirements for object detection and instance segmentation mvcs with our checking method reveals reliability gaps in the sota object detection and instance segmentation mvcs.
therefore we aim to answer rq2 how effective 548decompovision reliability analysis of machine vision components through decomposition and reuse esec fse december san francisco ca usa table comparison of the decompovision checking method with the baseline benchmark and non decomposable naive checking method for reliability evaluation of object detection and instance segmentation mvcs against image transformation frost.
second column reports apperformance metric on pascal voc c tests while the rest reports reliability distance values.
next to mvc s c task metrics we also report the rank among other mvc in that column.
n a indicates not enough points were collected for requirement checking.
next to a metric name means the higher the better and means the inverse.
vision task object detection vd vision task instance segmentation vi reliability distance per requirement rank reliability distance per requirement rank correctness preservation prediction preservation correctness preservation prediction preservationmvc namepascal voc c tests ap rank cpvl cpvc l cpvd cpnaive vd ppvl ppvc l ppvd ppnaive vd cpvs c l cpvi cpnaive vi ppvs c l ppvi ppnaive vi a retinanet r50 1x .
.
.
.
.
.
.
.
.
object detection only b retinanet r50 3x .
.
.
.
.
.
.
.
.
object detection only c retinanet r100 3x .
.
.
.
.
.
.
.
.
object detection only d r50 c4 1x .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
e r50 dc5 1x .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
f r50 fpn 1x .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
g r50 c4 3x .
.
.
.
.
.
.
.
.
.
.
.
n a .
.
h r50 dc5 3x .
.
.
.
.
.
.
.
.
.
.
.
n a .
.
i r50 fpn 3x .
.
.
.
.
.
.
.
.
.
.
.
n a .
.
j r101 c4 3x .
.
.
.
.
.
.
.
.
.
.
.
n a .
.
k r101 dc5 3x .
.
.
.
.
.
.
.
.
.
.
.
n a .
.
l r101 fpn 3x .
.
.
.
.
.
.
.
.
.
.
.
n a .
.
object class person m x101 fpn 3x .
.
.
.
.
.
.
.
.
.
.
.
n a .
.
note all mvcs are originally from facebook detectron2 .
iou threshold is set to .5for object detection .25for instance segmentation.
all numbers are rounded.
table average runtime and peak memory used to check the satisfaction of requirements against image transformation frost for object detection and instance segmentation without reusing decomposition analysis results.
all the values are computed by averaging the resources used to evaluate all mvcs and object classes from tbl.
on batches of images.
resources needed to obtain mvc outputs are not included in this table all the numbers are rounded.
with decomposition without decomposition resourcesobject detection instance segmentation object detection instance segmentation cpvlcpvc lcpvdcpvlcpvc lcpvs c lcpvicpnaive vdcpnaive vi runtime s .
.
.
.
.
.
.
.
.
peak mem mb .
.
.
.
.
.
.
.
.
ppvlppvc lppvdppvlppvc lppvs c lppvippnaive vdppnaive vi runtime s .
.
.
.
.
.
.
.
.
peak mem mb .
.
.
.
.
.
.
.
.
figure human tolerated threshold estimated either directly or by composing the subtask thresholds.
tcandtpare forcp andpp respectively.
is our method at identifying reliability gaps when compared to existing benchmarks or to non decomposable checking method?
finally we would like to determine whether decomposing the ctask analysis and reusing analysis artifacts can reduce resource usage for checking requirement satisfaction in terms of runtime and peak memory.
thus we aim to answer rq3 how does c task analysis decomposition and artifacts reuse impact resource usage for checking the requirements?
rq1.
to answer rq1 we compare the composed c task thresholds obtained by composing the subtasks thresholds tcandtp see sec.
with the c task thresholds directly computed using thehuman performance data using the icraf estimation method with the apmetric for object detection vd and instance segmentation vi .
the values of the above thresholds computed using human experiment data sec.
are shown in fig.
.
all directly estimated thresholds are between the minimum and maximum subtasks thresholds.
this logically follows since as long as the mvc is reliable on all of the subtasks it is also reliable on the c task at the same time the c task cannot be reliable when all the subtasks fail.
in contrast the composed thresholds never exceed the directly obtained ones.
this is because the composed thresholds correspond to the minimum threshold among all subtasks proc.
l which gives the lower bound estimate.
using the lower bound estimate for c task requirements implies that all subtask requirements are achievable by humans which in turn allows for a fair comparison between human and mvc performance.
unlike the threshold estimated directly with icraf the one selected by proc.
as the minimum threshold among the subtasks always ensures the lower bound of the human tolerated range answering rq1.
rq2.
to answer rq2 we aim to determine whether checking our composed c task and subtask requirements enables us to discover reliability gaps that were not identified with the sota reliability benchmark pascal voc c nor with the baseline nondecomposable checking method described below.
we define reliability gaps as undetected insufficient reliability.
pascal voc c is 549esec fse december san francisco ca usa boyue caroline hu lina marsso nikita dvornik huakun shen and marsha chechik an object detection benchmark dataset that consists of transformed pascal voc images used for testing they use five pre selected parameter values for the transformations and the apmetric to evaluate mvc correctness.
for the non decomposable checking method we implement a naive extension of icraf using the smallest per subtask human tolerated range threshold see sec.
and directly operates on the non decomposed c task metric.
the naive baseline checks correctness cpnaive v and prediction preservation ppnaive v of the mvc solely on the c task and is agnostic to its sub tasks.
decompovision is different as it explicitly considers and analyzes the subtasks during the c task analysis.
specifically decompovision evaluates the mvc s ability to preserve correctness and predictions of the composed c task cp vandpp v and of each substask cpviandppvi .
finally different from pascal voc c that is limited to pre selected transformation parameter values and evaluates one performance metric ap decompovision uses the range of visual changes that does not affect humans and evaluates the mvc s prediction and correctness perservation ability on the c task and its subtasks.
therefore we expect our checking method to provide more insight into the mvc reliability than the baselines.
moreover as far as we know our checking method is the first to enable benchmarking instance segmentation mvcs.
we evaluated mvcs provided by the detectron framework including three based on retinanet and based on faster rcnn for object detection and based on mask rcnn for instance segmentation which builds on top of faster rcnn by adding an object segmentation module.
since mvcs were originally pre trained on the coco dataset we post trained them on the pascal voc dataset for the corresponding task .
we checked reliability of the mvcs against two image transformations frost and brightness and two object classes person and bus using images from the pascal voc validation dataset6.
in the following we compare our reliability analysis results with the two baselines for the transformation frost and class person.
tbl.
reports the pascal voc c apvalues and reliability distances computed with decompovision or the naive non decomposable baseline .
below we refer to an mvc by its prefix letter in tbl .
first we start by comparing the composed requirements of decompovision analysis to the naive non decomposed baseline.
we can see that the values inside the cpnaive vdcolumn show little variation among different mvcs e.g.
mvcs a b c are the top ranked mvcs according to their ap on pascal voc c yet their cpnaive vdis similar to other mvcs which suggests that the naive baseline distance is not informative.
on the other hand our composed distance cpvdshows larger variation among different mvcs especially a b c vs others which correlates with the apvalues and per subtask mvc reliability distances cpvlandcpvc l. the above evidence suggests that checking our composed correctness preservation requirement is more informative than checking the naive baseline.
this is because checking satisfaction of reqventails checking satisfaction of allof the subtasks requirements which provides more refined analysis results.
second we compare decompovision s reliability requirements with theapanalysis on pascal voc c. while both rank mvc c as first the 2nd and 3rd best ranked mvcs are different according 6results for brightness and class bus are in the supplementary material .toapand ourcpvdmetrics see row a and b .
moreover while the benchmark reports a large gap in apbetween mvcs a and b cpvdsuggests that their difference in reliability is negligible.
this suggests that decompovision uncovers new reliability gaps not identified by the pascal voc c benchmark and provides useful feedback for developers to improve mvc reliability.
using decompovision s per subtask reliability requirements can provide additional analysis insights.
for example according to ppvd mvcs a b and c are the poorest at preserving their predictions in object detection despite being the best in correctness preservation .
analyzing per subtask requirements reveals that the predictions are well preserved for the localization task i.e.
lowppvl cvalues but it is the classification that hinders the preservation capabilities of the mvcs i.e.
high ppvcvalues .
thus analyzing performance for each atomic subtask can be used to guide improvements in mvc reliability e.g.
through directed retraining of subtask specific layers .
another observation is that mvcs k j and b c have similar apon pascal voc c and similar reliability distances for vd respectively.
by the subtask performance we see that mvc j is more correctness preserving forvl and mvc k is more so for vc l. as for preserving the predictions mvcs b and c have almost the same reliability distance forppvd however mvc c is slightly better at vland mvc b is better at vc l. better reliability performance for vc lindicates that mvcs k and c is safer for applications where the reliable object classification is important e.g.
traffic sign detection.
decompovision can also analyze reliability of mvcs solving instance segmentation vi.
as we can see in tbl.
with respect to subtask performance mvc j is better at vi indicated by the lowestcpvi even though it is considerably worse then mvc k at object detection see cpvd that instance segmentation builds upon.
yet mvc j is better than mvc k at object segmentation seecps c l which results in the overall better performance at vi.
this suggests that good quality object segmentation is more important than object detection for instance segmentation mvc reliability.
therefore checking our reliability requirement enabled us to identify new reliability gaps that could not be detected just by checking apon transformed images in the pascal voc c benchmark.
as a result we obtain more insights into the reasons for reliability failures thus answering rq2.
rq3.
here we aim to determine whether reusing analysis artifacts following our decomposition of c tasks can save resources.
for all types of mvcs requirements experiments and tests developed for object detection can be reused for instance segmentation see sec.
.
certain widely used mvcs for instance segmentation such as mask rcnn are directly built on top of faster rcnn mvcs that solve object detection .
thus the intermediate computation results for object detection can also be reused for instance segmentation.
for such mvcs we measured the runtime and peak memory needed to check the satisfaction of reliability requirements tbl.
by sampling batches of test images using mask rcnn mvcs tbl.
.
the results are shown in tbl.
.
all experiments were conducted on cpu intel r core tm i3 cpu .60ghz with 8gb of ram.
as shown in tbl.
the peak memory is not affected significantly and measuring the satisfaction of cpnaivevland ppnaivevltakes the longest time.
indeed evaluating the performance 550decompovision reliability analysis of machine vision components through decomposition and reuse esec fse december san francisco ca usa of localization is most time consuming since it compares the iou of the mvc output and the ground truth bounding boxes.
reusing such computation significantly decreases the runtime of checking satisfaction of all subtasks that come later in the sequence and the c task itself.
in total checking cpsatisfaction for instance segmentation by reusing results from object detection took .57sec.
while checking without reuse took .21sec.
checking pptook .02sec.
with reuse and .71sec.
without.
on average reuse can decrease runtime by up to answering rq3.
summary.
we empirically showed that rq1 for c tasks our composed human tolerated thresholds represent lowerbound threshold estimation rq2 using c task decomposition our modular checking method enables reliability analysis on c tasks not covered by the existing work and can discover and locate reliability gaps that are missed by the existing methods and rq3 by decomposing and reusing analysis artifacts runtime of checking satisfaction of the reliability requirements can be significantly decreased.
thus our evaluation suggests that decompovision is efficient and effective.
threats to validity.
inherited from icraf during testing we assumed that the parameter values of the transformations are uniformly distributed.
however this may need to be adjusted depending on the deployment environment of the mvc.
due to budget considerations our experiments with humans were limited to a small set of transformations and only two c tasks.
we leave further experiments to future work.
related work below we discuss approaches for defining and accessing mvc reliability.
specifying reliability of mvcs.
requirements specification of mvcs that perform c tasks has been approached by the specification of a set of quality characteristics of datasets used to train the mvcs or ml related requirements for each phase of software development processes .
yet these requirements do not have a machine analyzable representation and thus cannot be checked automatically.
the sota reliability framework icraf that specifies machine verifiable reliability requirements for mvcs is only applicable to a single atomic vision task with one type of output such as image classification a single class label per image .
to tackle this limitation this paper proposes a modular reliability framework for c tasks which builds on top of icraf.
furthermore the subtasks based modular design of our framework enables reuse of analysis artifacts of atomic subtasks.
assessing mvc reliability.
existing approaches propose mvc reliability analysis against image transformations including metamorphic testing semantic analysis reachability analysis verification .
these works mainly focus on one particular vision task with one single type of output which is not directly applicable to more complex vision tasks.
meanwhile our decomposition principle allows extending the above reliability approaches to analyze mvcs performing c tasks.
moreover our decompovision analyzes mvc reliability not only on the ctasks but also on their subtasks which leads to a deeper insight into the mvc reliability and guides further mvc development.decomposing mvc.
existing work decomposed convolutional neural network cnn used for image classification into modules responsible for classifying each object class or into a hierarchical representation .
the benefit of such a decomposition is reusing parts of the models to reduce training time.
similar work concerning modular neural networks has been proposed to increase interpretability to achieve better training by understanding communication between modules and memorizing a set of features .
in contrast our decomposition focuses on decomposing the reliability analysis of an mvc performing a vision task with the goal of reusing analysis methods and artifacts rather then the mvc layers themselves.
conclusion we proposed decompovision a modular framework for analyzing reliability of mvc performing complex vision tasks c task that are decomposable into a sequence of atomic subtasks.
decompovision is built on top of the sota reliability framework for atomic vision tasks .
decompovision s modularity enables reuse of existing subtask requirements and analysis artifacts and to get deeper insights into the mvc reliability by understanding how subtasks contribute to the analysis results.
through evaluation with popular mvcs we showed that our modular framework enables reliability analysis of c tasks not covered by existing approaches identifies reliability gaps previously missed and reduces usage of resources by reusing methods and artifacts.
we believe that other types of reliability analyses such as verification can also benefit from our decomposition principle for handling c tasks while reducing resource usage.
additionally we plan to improve our checking method by using our refined requirements analysis to build diagnostics to help software engineers improve mvc reliability.
discussion.
our theroem.
states that if all subtask requirements are satisfied so is the composed requirement.
this is because satisfying all subtask requirements is much stricter than satisfying the composed requirement.
one limitation of this is that if not all subtask requirements are satisfied there is no definite way of judging whether the composed requirement is satisfied by simply looking at the results of all subtask requirements.
however despite this since we are testing each subtask separately our decomposition can still give useful information about which subtask is not performing well through reliability distance.
building distance measures or other decomposition methods for supporting stronger implications when subtask requirements are not satisfied should be future work.
data availability the implementation and all the data necessary to reproduce our experiments are available online here.