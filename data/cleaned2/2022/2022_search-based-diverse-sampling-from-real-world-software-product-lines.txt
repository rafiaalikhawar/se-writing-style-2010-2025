search based diverse sampling from real world software product lines yi xiang xiangyi scut.edu.cn south china university of technology guangzhou chinahan huang hhan scut.edu.cn south china university of technology guangzhou chinayuren zhou school of computer science and engineering sun yat sen university guangzhou china sizhe li south china university of technology guangzhou chinachuan luo microsoft research beijing chinaqingwei lin microsoft research beijing china miqing li university of birmingham birmingham ukxiaowei yang xwyang scut.edu.cn south china university of technology guangzhou china abstract real world software product lines spls often encompass enormous valid configurations that are impossible to enu merate.
to understand properties of the space formed byall valid configurations a feasible way is to select a small and valid sample set.
even though a number of sampling strategies have been proposed they either fail to producediverse samples with respect to the number of selected fea tures an important property to characterize behaviors ofconfigurations or achieve diverse sampling but with lim ited scalability the handleable configuration space size islimited to .
to resolve this dilemma we propose a scalable diverse sampling strategy which uses a distance metricin combination with the novelty search algorithm to produce diverse samples in an incremental way.
the distance metric is carefully designed to measure similarities between config urations and further diversity of a sample set.
the noveltysearch incrementally improves diversity of samples throughthe search for novel configurations.
we evaluate our sam pling algorithm on real world spls.
it is able to generatethe required number of samples for all the spls includingthose which cannot be counted by sharpsat a state of the art model counting solver.
moreover it performs better than corresponding authors.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided thatcopies are not made or distributed for profit or commercial advan tage and that copies bear this notice and the full citation on the firstpage.
copyrights for components of this work owned by others thanacm must be honored.
abstracting with credit is permitted.
to copyotherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissionsfrom permissions acm.org.
icse may pittsburgh pa usa association for computing machinery.
acm isbn ... .
at least competitively to state of the art samplers regarding diversity of the sample set.
experimental results suggestthat only the proposed sampler among all the tested ones achieves scalable diverse sampling.
ccs concepts software and its engineering search based software engineering mathematics of computing optimization with randomized search heuristics .
keywords software product lines diverse sampling novelty search dis tance metric acm reference format yi xiang han huang yuren zhou sizhe li chuan luo qingwei lin miqing li and xiaowei yang.
.
search based diverse sampling from real world software product lines.
in 44th international conference on software engineering icse may21 pittsburgh pa usa.
acm new york ny usa pages.
introduction softwareproductlines spls beinghighlyconfigurable allow users to derive products by selecting and deselectingfeatures which are increments of product functionality.
that is a set of features defines a unique product or configuration of an spl.
clearly as the number of features increases the number of all possible configurations grows exponentially .
a common tool for representing all valid configurations is a tree like structure called a feature model fm in which features and constraints among them are ex plicitly specified.
the space formed by all valid configura tionsiscalleda configuration space denotedas henceforth.
in practice because large real world spls often encompass hundreds of thousands of features the size of i.e.
.
ui oufsobujpobm pogfsfodf po 4pguxbsf ohjoffsjoh icse may pittsburgh pa usa xiang et al.
could be astronomically large greatermuch1010 .
therefore it is rarely possible to enumerate every valid configuration.
to achieve a certain goal e.g.
learning a performance predic tion model testing spls we need to sample from the configuration space a small set of valid configurations called asample set where each valid configuration is known as asample.
quite often sample sets must be well chosen based on domain knowledge.
for instance a sample set should cov er allt wise feature combinations in the context of t wise sampling .
if no domain knowledge is available however sample sets are expected to cover the configurationspace as widely and uniformly as possible .
there have been several sampling strategies in the literature e.g.
random sampling solver based sampling coverage oriented sampling a n d uniform sampling .
these sampling strategies focus on different aspects of sampling from spls and come with different strengths and weaknesses detaileddiscussions are available in section .
inthispaper wefocusonanotherkindofsampling known asdiverse sampling which seems to be largely ignored.
diverse sampling brings lots of benefits.
for example it couldreduce the risk of missing important configurations with dis tinct performance behavior when deriving a performanceprediction model see and it forms a scalable and flex ible alternative to t wise sampling see .
recently kaltenecker et al.
proposed a diverse sampling strate gy called diversified distance based sampling ddbs which pursues to derive diverse samples regarding the number of selected features.
in fact the number of selected features for a configuration c denotedas t c isimportanttocharacterize the behavior of this configuration.
pursuing diversity in terms of this number can directly improve the accuracyof performance prediction which has been shown in .moreover the number of selected features is directly treatedas an optimization goal in multi objective spl configuration .
in such scenarios a common and importantissue is how to improve the samples diversity regarding thisnumber.
though important achieving diverse sampling regarding the number of selected features poses great challenges tostate of the art samplers.
figs.
a and b show the dis tribution of the number of selected features for samples gen erated by two recent uniform samplers i.e.
smarch andunigen3 on the hipacc feature model .
as seen compared with ddbs both of them areunable to produce diverse samples concerning the numberof selected features.
notice that even though ddbs couldsample more diverse configurations on this model it faces the scalability issue.
in fact ddbs failed to handle configuration space larger than .
this is also confirmed by our experiments performed in section .
and explainedlater in section .
.
in summary these samplers either fail toproduce diverse configurations or achieve diverse samplingbut with limited scalability.
0smarch a 0unigen3 b 0ddbs c 0nsbs d figure distribution of the number of selected fea tures for samples generated by four samplers onhipacc .
in this figure the x axis is the number of selected features in a configuration denotes the estimated boundaries of the above number seesection .
.
note that these boundaries may notbe reachable.
we pursue to cover the range betweenthe two boundaries as diversely as possible.
to enable scalable diverse sampling this paper provides an alternative perspective i.e.
search based sampling.
thekey idea is to generate initial samples using efficient off theshelf sat solvers and then incrementally improve the diversity of the sample set using a specific search algorithm.this sampling strategy relies on a special distance metricand a search technique called novelty search ns .we name this sampling algorithm ns based sampling nsbs for short .
to demonstrate merits of nsbs we compareit with several state of the art sampling algorithms using 39real world spls adopted by oh et al.
.
experimental re sults reveal that nsbs indeed enables a scalable diverse sampling from spls.
in particular it successfully generates the requested number of configurations i.e.
configurationsin our setting for all spls including the largest ones onwhich most state of the art samplers fail to generate evenone configuration within an hour.
main contributions of the paper are summarized as follows.
a tailored distance metric.
by using the number of s elected features the configuration space is mappedto a small behavior space b t c c .ad i s tance metric is designed to measure similarities be tween configurations in both band .
we show both theoretically and experimentally that using this distance metric not only improves the coverage in the behavior space but also promotes diversity in the o riginal configuration space.
considering diversity inboth spaces could improve the representativeness ofthe sample sets.
a befitting search technique.
we choose ns as thesearch engine because of its good theoretical proper ties that well fit the goal of diverse sampling.
1the behavior space is a concept introduced in ns algorithms search based diverse sampling from real world software product lines icse may pittsburgh pa usa precisely ns has been shown to tend towards a diverse uniform sampling of the behavior space .the above property could help to improve diversityof the sample set in the behavior space.
as shown in fig.
d samplesgeneratedbynsbsareasdiverseas those of ddbs which as mentioned early is a tailoreddiverse sampler.
flexibility in the sampling process.
since diversity isimproved in an incremental way it is easy for user s to achieve a desired trade off between diversity andefficiency.
if a higher quality sample set is required then more execution time can be specified.
the flex ibility in the sampling process is one of the main advantages of nsbs over other state of the art samplers most of which are not controllable regarding the exe cution time.
preliminaries in this section we provide necessary preliminaries on sam pling from configuration spaces and the space mapping s trategy.
moreover a brief introduction to ns is also present ed.
.
sampling from configuration spaces formally an fm can be seen as a tuple angbracketleftf c angbracketright w h e r ef f1 ... f n is the set of nfeatures and cis the set of all constraints among features.
a configuration c represented by f1 ... fn is defined as a set of selected or deselected features.
precisely f iand fiindicate that the feature fiis selected and deselected respectively.
in programming ccan be represented by a binary string with indicating a selected feature and a deselected one.
due to constraintsinc not all configurations are valid.
the configuration that satisfies all the constraints is called a valid configuration and all valid configurations form the configuration space .
many software engineering tasks require to derive a smalls a m p l es e tf r o m .i nt h i sc o n t e x t asample set s s ... s n wherendenotes the sample size is a subset of i.e.
s .
manually deriving samples is error prone and time consuming even for tiny fms.
therefore automat ed solvers like sat solvers have been widely adopted togenerate samples from .
it is well known thatan fm can be easily converted into a propositional formula .
the derived is then used as the input of automated solvers which are internally run to find solutions to .
.
mapping to behavior spaces as mentioned previously by characterizing the behavior ofa configuration using the number of selected features c is mapped to an integer.
accordingly the configurationspace is mapped into the behavior space b.l e t bbe the space formed by all configurations with exactly b b selectedfeatures then b b b.thatistosay thewhole configuration space is decomposed into b subspaces.
we can then sample configurations that are diversely distributedamong these subspaces.understanding bis much easier than due to that b is significantly smaller than .
in fact the lower and upper bounds for bcan be approximated by using the number of core features denoted by core and the number of dead features denoted by dead respectively.
notice that core features must be selected in every valid configuration while dead features must not be selected.
to be more specif ic min b core a n dm a x b n dead w h e r e nis the total number of features.
therefore the size of bis at most n core dead .
in contrast grows exponentially with respect to n. hence can be astronomically large especially for large real world spls.
for example is as large as .
417for the uclinux config model .
we mustmentionthat intheory knowingexactly bisashardas knowing because every configuration should be investigat ed in the worst case.
however bcan be well approximated by the following set b prime core core ... n dead which contains all possible integers from core ton dead .
it is possible that there exist some integers to which no con figurations are mapped.
therefore bis a subset of b prime.i n section .
b primewill be used to calculate performance indicators.
.
novelty search as mentioned in section ns is adopted in oursearch based diverse sampling.
therefore it is necessary togive a brief introduction to this search technique.
ns is oneof the main divergent search algorithms and its promi nent feature is to abandon objectives it replaces theconventional goal oriented objective by a criterion measur ingnoveltyof individuals.
this criterion is referred to as novelty score defined as the average distance of an individu al to itsknearest neighbors where kis a constant.
formally x the novelty score of x is given as follows .
x kk summationdisplay j 1d x xj wherexjis thej th nearest neighbor of xamong an archive of previously explored individuals and the current popula tion in the behavior space d x x j is any distance metric.
x estimates the sparseness of xin the behavior space.
if this score is large then xis in a sparse area in contrast it is in a dense area in case that the novelty score is small.
ingeneral individuals in sparse regions are preferred to thosein dense regions as the exploitation around sparse regionsis helpful to perform a diverse exploration of the behaviorspace .
following the practice in the calculation of the novelty score can be extended from a single configuration to asample set s s ... s n .
specifically s n summationdisplay i 1 si where si is the novelty score of a single configuration as given by eq.
.
clearly the higher the novelty score themore diverse the sample set.
icse may pittsburgh pa usa xiang et al.
finally it is worth mentioning that there is an interesting search behavior of ns.
that is the sampling produced by ns covers the whole reachable behavior space .
this suggests that ns explores the behavior space diversely.
it is a good property which well matches the goal of diverse sampling from spls.
therefore we choose ns as the search engine.
ns based diverse sampling the nsbs procedure is outlined in algorithm .
the keyidea is to continuously improve diversity of the initial sample set through the search for novelindividuals i.e.
configurations .
the algorithm takes the propositional formula derived from a given fm and the sample size nas input and outputs a set of samples stored in an archive a. algorithm nsbs algorithm input propositional formula n sample size output a archived samples 1initialize the archive aby generating nsolutions to using the randomized sat4j solver 2initialize the distance matrix d dij n n wheredij i j ... n as given in eq.
is the distance between xi aandxj a 3for each x a calculate its novelty score x b a s e d on eq.
4whilethe termination condition is not met do p1 p2 matingselection a c1 c2 crossover p1 p2 7fori do ci mutation ci ifciis invalid then repairciusing the probsat solver end a updateachive a ci 13end 14end 15returna .
initialization as shown in line of algorithm ais initialized with n configurations generated by the randomized sat4j solver in which the order how the logical clauses and the lit erals are parsed is randomized .
according to the im plementation in there exist three parsing strate gies i.e.
negativeliteralselectionstrategy positiveliteralselectionstrategy andrandomliteralselectionstrategy.e a c hs trategy has an equal chance of being chosen when generatinginitial configurations.
in particular the first strategy prefersnegative assignments to literals and thus emphasizes con figurations with less selected features.
therefore the lowerbound of bcan be approximated by using this strategy.
similarly the second strategy helps to approximate the upperbound of b. the third strategy randomly assigning trueorfalseto literals is able to improve randomness of the generatedconfigurations.usingsimultaneouslythreestrategiesaimsat improving diversity of the initial sample set.
in particular bounds of bcould be well approximated.
we should mention that this sat based seeding instead of random seeding isused here because the former always generates valid configu rations while the latter is highly likely to generate unwantedinvalid ones due to the constraints.
.
distance metric to measure similarities between two configurations we de fine the following distance dij d xi xj parenleftbiggabs t xi t xj n parenrightbigg parenleftbigg xi xj n parenrightbigg wherexi xj a xi negationslash xj are two different configurations t xi denotes the number of selected features in xi abs returns the absolute value of a number and returns the cardinality of a set.
is a constant as given below.
braceleftbigg1 max t xi t xj t xi t xj n n min t xi t xj otherwise.
asseen thisdistancemetricconsistsoftwoweightedparts.
the first part measures the similarity between configura tions in the behavior space while the second part in the original configuration space.
in fact xi xj nis the hamming distance between xiandxj.
note that using the above two parts is intended to sample configurations covering diversely in the behavior space and also keeping asdissimilar as possible in the configuration space.
in section .
we will experimentally verify this distance metric.
it is also worth noting that is set based on our theoretical analysis which shows that is needed to mitigate biases towards sampling specific configurations.
in other words eq.
without using can introduce biases in the behavior space and thus can hamper diversity of the sample set.detailed analysis can be found in section s of the supple ment .
in section .
we will experimentally investigate s effects.
therein one will find that using indeed improves diversity of the sample set in the behavior space.
according to line in algorithm the distance matrix d n n is initialized by working out the distance between each pair of configurations in a. note that since d is symmetric we only need to calculate distances for half ofthese pairs.
we would like to mention that the size of dis n n rather than n n because we reserve spaces for storing distances when evaluating a new config uration see algorithm .
after obtaining d a ss h o w ni n line of algorithm the novelty score for each x ais calculated based on eq.
.
2ifxi xj dijis forcibly set to .
3the online supplement is available at zenodo.
search based diverse sampling from real world software product lines icse may pittsburgh pa usa .
genetic operations like in genetic algorithms we perform in order the mating selection crossover and mutation to generate new individu als.
as shown in line of algorithm the matingselection procedure chooses from atwo parents p 1andp2each time.
the basic idea of choosing a parent is to select the one withlarger novelty score from two different random members ina.
in case of a tie a random selection is performed betweenthe two members.
clearly the above mating selection em phasizes individuals located in sparse regions.
explorationaround sparse regions could potentially improve diversity ofthe samples.
algorithm a updateachive a c input a c output a 1ifacontains cthen 2returna 3end 4fori ... ndo 5di n d xi c 6d n i di n 7end 8d n n 9for each x a c calculate its novelty score x based on eq.
10xworst argmin x a x find the worst member in a 11if c xworst then 12xworst c update d 13forj ... ndo dj worst dj n dworst j d n j 16end 17dworst worst 18end 19returna once two parents p1andp2have been selected the uniform crossover is applied to generate two children c1andc2 line in algorithm .
to be specific for each index j ... n we generate a random number rand.i frand .
then c1 j a n dc j are set to p1 j a n dp2 j respectively.
otherwise they are set to p2 j a n dp1 j respectively.
notice that c1 j denotes the value taken in the j th position of c1.
the newly generated individuals are then subjected to bit wise mutation line in algorithm .
specifically for each bit the value is changed from true to false or vice versa.
often the ratio of bits to be changedis controlled by a parameter p called mutation probability.
in this work we set p to .
following the common practice in .
itisnotuncommonthattheresultingconfigurations after crossover and mutation are invalid.
in this case as shownin line of algorithm the probsat solver one of thehigh performing stochastic local search sls sat solvers isadopted to repair invalid configurations.
the variables to beflipped by the solver are chosen based on probabilities such that more promising variables are given more chances to be selected.
in fact probsat has been adopted to repairinfeasible configurations in prior work in the con text of optimal products selection from spls.
in particular the empirical study in suggested that probsat is moreeffective than walksat another popular sls solver inimproving diversity of a configuration set.
for more detailson probsat we direct readers to the original study .
no tice that internal parameters of this solver are set following the practice in and .
therefore a tuning phase is not required in this work.
configurations operated by probsat could still be invalid even though they are valid most of the time i np a r t i c u l a r for large scale fms.
in case of invalidity we simply requestto the randomized sat solver as described in section .
to return a valid configuration.
.
updating archive the archive astores novel configurations discovered during the search process.
its update procedure is presented in al gorithm .
to improve diversity as shown in lines theproducer rejects the entrance of any configuration that is i dentical to already archived ones.
when a totally differentconfiguration cis available we need to fill the distance matrixdby working out distances between cand each x i a. thesedistancesarestoredin thelastrowand thelastcolum n. in what follows as indicated in line of algorithm the novelty score for each member x a cis calculated based on eq.
.
we should note here that the novelty scores are computed taking into account not only members in abut also the new configuration c. this enables an evaluation of the novelty with respect to both previously explored individ uals and the current one that represents the most recentlyvisited point .
in line of algorithm we find the worst member from a and this member is denoted by x worst where the index worstis its position in the archive.
in case that the novelty score of cis higher than that of xworst we will replace xworstbyc.
subsequently the distance matrix should be updated.
this is achieved by simply copying the last row column to the worst th row column see lines .
at last d worst worst should be set to .
according to the above update procedure the algorithm consistently looks for novel individuals pushing individualsto constantly move in the behavior space in new and unexplored areas first but also then in already explored areas as their density of individuals is never exactly homogeneous .
this way diversity of the samples can be persistentlyimproved.
4different from conflict driven clause learning solvers such as sat4j sls type sat solvers offer no guarantees on finding valid assignments.
icse may pittsburgh pa usa xiang et al.
.
termination conditions termination of nsbs can be flexibly specified by users.
we offer the following two termination strategies.
strategy terminationcontrolledbythemaximumrunning time max t .
this is a common way of stopping a search algorithm and the setting of maxtdepends largely on the demands of users.
strategy automatic termination when the algorithm gets relatively steady.
this is achieved by adding the follow ing piece of codes after line in algorithm .
if tildewidea a tildewidea .
counter else counter end where tildewideais the old archive while ais the newly updated one.
if the change ratio of novelty scores for the two archives is below .
then counter is increased by one otherwise it is reset to .
the algorithm will terminate once counter exceedsr at h r e s h o l ds p e c i fi e db yu s e r s .i no u re x p e r i m e n t s we setrto .
with this setting it is found that nsbs automatically terminates on almost all of the tested fms.
we would like to mention that we give users freedom to specify the termination of the sampling process.
most of the state of the art samplers are not controllable with respectto the execution time.
indeed it may take excessively long before a set of samples is returned .
instead the proposed nsbs allows users to make a desired trade off betweenquality primarily diversity and efficiency.
if users want ahigher quality sample set he she can set rormax tto a relatively larger value.
the flexibility regarding terminationsis one of the main advantages of nsbs over other state of the art samplers.
experiment setup inthissection westartbyintroducingourresearchquestion s rqs .
then we give information about fms used in our empirical study.
subsequently we describe how the performance of different samplers can be measured using special ized indicators.
finally detailed implementations are given.
.
research questions the distance metric is expected to play an important rolein sampling products that are diverse not only in the be havior space but also in the original configuration space.
itis necessary to investigate the effect of components in eq.
.
with this regard we aim at answering the following tworqs.
rq1 what are the benefits brought by using the two weighted parts in the distance metric?
rq2 does the factor matter in the distance metric of nsbs?
to address rq1 we perform an ablation study where the distancemetricdefinedineq.
iscomparedagainstamod ified one in which the second part is removed.
our primarygoal is to sample diverse configurations in the behavior spaceb.
on top of this we also expect that they are as diverse as possible in the configuration space .
the first part in thedistance metric is designed for the primary goal and elimi nating it will lead to failure of this goal because the second part only measures similarity in .
hence we only remove the second part in the ablation study.
according to our theo retical analysis the goal of in eq.
is to alleviate biases towards specific configurations in the behavior space.
thesecond research question amounts to experimentally verify ing this.
moreover we intend to answer two more research questions regarding the effectiveness of nsbs in comparison withseveral state of the art samplers and impacts of the parameterk.
rq3 how effective is nsbs concerning both scalability and diversity in comparisons with state of the art samplers?
rq4 how is the performance of nsbs affected by its key parameter k?
to address rq3 we compare nsbs with sat based sampling ddbs unigen3 and smarch .
weexpect that nsbs performs better than or at least competi tively to them with respect to both scalability and diversity.finally the fourth research question seeks to provide useful guidelines for tuning ns in the context of diverse sampling from spls.
.
subject feature models in our experiments we consider fms that have been care fully selected by oh et al.
in their evaluation of smarch.table gives an overview of the subject fms including thenumber of features f the number of cnf constraints c the size of the configuration space the number of core dead and unconstrained 5features i.e.
core dead and uc .
note that is counted by sharpsat which fails on the last five largest fms.
all fms are publiclyavailable in dimacs format the standard format for sat solvers.
.
performance indicators performance indicators are required to evaluate the quali ty of a sample set s s ... s n .
to measure whether swidely covers the behavior space b b1 ... b b m o t i vated by the definition of inverted generational distance the following indicator which we call spread is defined.
spread s b b b summationdisplay i 1dmin bi s wheredmin bi s denotes the minimum distance from bito s. mathematically dmin bi s is in the following form dmin bi s n min j 1abs bi t sj .
5unconstrained features here are those that are not involved in the cnf constraints.
6all fms are downloaded from search based diverse sampling from real world software product lines icse may pittsburgh pa usa table overview of the subject feature models fm f c core dead uc lrzip .44e llvm .02e x264 .15e dune .34e berkeleydbc .56e hipacc .35e jhipster .63e 0polly .00e 7z .86e javagc .93e vp9 .16e fiasco .00e axtls214 .00e fiasco .58e 6toybox .45e 0axtls .29e 0uclibc ng .00e toybox075 .40e uclinux .63e ref4955 .20e adderii .57e ecos icse11 .97e 0m5272c3 .37e 0pati .90e olpce2294 .19e 0integrator arm9 .06e at91sam7sek .45e se77x9 .20e phycore229x .77e busybox .
.
.50e 0busybox .30e embtoolkit .10e 1freebsd icse11 .39e 50uclinux config .78e 0buildroot 12freetz .
.
.
icse11 .
.
2var .
.
.
2var regarding b as mentioned in section .
it is not exactly known butcanbeeasilyapproximatedby b prime.inpractice w e therefore calculate spread s b prime instead of spread s b .
it is clear that a smaller value of spreadindicates a more diverse distribution in the behavior space.
in addition tospread the novelty score of a sample set s as given in eq.
also serves as a performance indicator.
it measuresthe diversity of sin the original configuration space.
.
detailed implementations for each fm we sample configurations and compute theaverage sampling time per configuration measured in mil liseconds to compare efficiency.
all samplers except nsbs terminate once configurations are sampled or the sampling time takes more than milliseconds i.e.
onehour .
since nsbs is able to quickly sample configura tions it terminates either automatically based on strategy 2as described in section .
or forcibly when the sampling time reaches a timeout of one hour.
note that to mitigaterandom bias all samplers are independently run times and we present and analyze experimental results regardingmean values of the performance indicators.
all experiments are performed on a quad core .
ghz with gb of ram running ubuntu .
.
.
source codes of sat based sampling ddbs unigen3 and smarch are downloaded from their authors repositories and they are all executed on a single thread i.e.
withoutparallelization following the practice in .
the codes ofnsbs can be found in our repository .
5r e s u l t s in this section we provide a series of experimental results re garding the research questions.
due to limited space raw re sultsaregivenintabless 1tos 3intheonlinesupplement .
to determine whether the difference between different algo rithms over all the runs is significant or not followingguidelines suggested by arcuri and briand the mannwhitney u test with a .
significance level is performed for each fm.
in these tables test results are represented bythree symbols and indicating that the algorithm in the first column performs better than equivalently to andworse than the algorithm in other columns respectively.
inthe following subsections the performance comparisons arebased on these results.
.
rq1 benefits brought by using the two weighted parts in the distance metric to investigate benefits brought by using two weighted parts in eq.
we consider two different distance metrics.
thefirst one as given in eq.
uses two weighted parts.
here after we call it weighted distance.
the second one retains only the first part and therefore measures only the similar ity in the behavior space.
this metric is called unweighted distance.
both distance metrics are tested within the same ns framework in which k .
this setting of khas been widelyemployedin ns related literature .
ourtuning experimentspresentedinsection5.4suggestthat k 5i sa l s o a good setting in our context.
when the weighted distance is used the sampling algorithm automatically terminates ac cording to strategy .i nt h ec a s eo funweighted distance the termination is controlled by strategy i nw h i c hmax t is set to the running time consumed by the correspondingalgorithm using the weighted distance.
this setting allows us to investigate the benefits while eliminating potential im pacts brought by using different running time.
regarding spread t h eweighted distance performs significantly better than its counterpart on out of all the 39fms but worse on only one fm i.e.
the simplest llvm.for all the remaining fms the two distance metrics havesimilar performance.
the above results suggest that usingalone the first part of eq.
which measures similarities inthe behavior space is enough to obtain good spread in the behavior space in the majority of the cases.
icse may pittsburgh pa usa xiang et al.
regarding the novelty score the weighted distance shows significant improvements over the unweighted one on out of39fms anddegenerationsononlylrzipand2.
.
.32var.
clearly the second part of eq.
is necessary in promoting diversity in the original configuration space.
therefore theanswertorq1isclear.
using the two weighted parts in eq.
indeed brings benefits it improves coverage in the behavior space at the same time it also promotesdiversity in the original configuration space.
boosting diver sity in both spaces is beneficial for enhancing representative n e s so ft h es a m p l es e t .
.
rq2 matters in the distance metric according to section .
in eq.
is used to mitigate potential biases towards specific configurations.
in this sec tion we are going to experimentally examine the effects ofthis factor.
to this end we compare nsbs described in al gorithm against its variant i.e.
nsbs in which is omitted in the distance metric.
the only difference betweenthe two algorithms is the presence or absence of .
1000nsbs 6000nsbs 1000nsbs a 6000nsbs b figure configurations sampled by nsbs widelycover the behavior space while those sampled bynsbs fail.
a ref4955 b .
.
.
icse11 as shown by the spreadresults nsbs performs better than or at least comparably to nsbs on all the fms.
in particular nsbssignificantlyoutperformsitscounterparton25 of the fms.
moreover the improvements are mostly observed on large fms.
taking ref4955 and .
.
.
icse11 as example fig.
graphically shows the distributionof the sampled configurations in the behavior space.
as seen configurations sampled by nsbs are distributed more wide ly than those sampled by nsbs on the two fms.
more specifically configurations of nsbs cover intensively in the middle part but sparsely at the boundaries.
the above experimental results bring out the following.
the indeed matters when is omitted diversity of the sampled configurations is significantly affected in the behav ior space.
in particular boundaries of the behavior space tendnot to be sufficiently covered.
the above experimental result s are in line with our theoretical findings stating that theadoption of is able to mitigate bias towards sampling specific configurations.table time taken to sample a configuration inmilliseconds .
timeout one hour fm nsbs sat based ddbs unigen3 smarch lrzip llvm x264 dune berkeleydbc hipacc jhipster polly 7z javagc vp9 fiasco1710 timeout axtls2146 timeout fiasco timeout toybox timeout axtls timeout uclibc ng timeout toybox0757 timeout uclinux timeout ref4955 timeout timeout adderii timeout timeout ecos icse11 timeout timeout timeout m5272c3 timeout timeout pati timeout timeout olpce2294 timeout timeout integrator arm9 timeout timeout at91sam7sek timeout timeout se77x9 timeout timeout timeout phycore229x timeout timeout busybox .
.
timeout timeout timeoutbusybox timeout timeout embtoolkit timeout timeout timeoutfreebsd icse11 timeout timeout timeoutuclinux config timeout timeout timeoutbuildroot timeout timeout timeout freetz timeout timeout timeout .
.
.
icse11 timeout timeout timeout .
.
2var timeout timeout timeout2.
.
.
2var timeout timeout timeout .
rq3 effectiveness of nsbs in comparison with state of the art samplers table gives the average time measured in milliseconds to sample a single configuration for all fms.
if the samplingcan not finish within one hour then we declare a timeout.
accordingtotable2 sat basedsimplerscalesverywell be ing able to sample one configuration within millisecondseven for the largest fm i.e.
.
.
.
2var.
quite often thesampling takes no more than millisecond.
for ddbs it canonlyhandle11smallfmswith .
.forunigen3 it succeeds in dealing with fms with .
.
regarding smarch it scales better than ddbs and unigen3 but still fails on fms.
for our nsbs it dose not en counter a timeout for all fms.
we would like to mention that even though nsbs runs out of one hour on .
.
2varand .
.
.
2var it successfully samples configurationsas requested.
therefore we do not declare a timeout.i nf a c t this is totally different from the timeout of other samplers search based diverse sampling from real world software product lines icse may pittsburgh pa usa 1000102030selected features nsbs 1000102030selected features sat based 1000102030selected features ddbs 1000102030selected features unigen3 1000102030selected features smarch 100012difference 100012difference 100012difference 100012difference 100012difference figure for configurations generated by each sampler on jhipster the number of selected features and the difference of this number between two successive configurations are shown in histograms.
which are unable to sample configurations within one hour.
regarding the sampling speed as shown in table nsbs is slower than the sat based sampler but much faster than smarch.
table summarizes wilcoxon s test results for each pairwise comparison between nsbs and each sampler regardingspread.
in this table available cases refer to those withoutatimeout for both samplers.
as can be found ns performs better than or at least competitively to other samplers in al most all the available cases.
the only exception is observedon the pairwise comparison between nsbs and ddbs on the simplest llvm.
in this case nsbs is significantly worse than ddbs.
this exceptional case accounts for of all theavailable cases for the pair nsbs v.s.
ddbs.
in summa ry nsbs is more effective than sat based sampler unigen3and smarch in generating diverse configurations nsbs andddbs are able to sample configurations covering similarly inthe behavior space.
however as discussed previously ddbssuffers from the scalability issue being only able to handlevery small fms.
table summary of wilcoxon s test results regardingspread for pairwise comparisons between nsbs and each sampler nsbs v.s.
sat based ddbs unigen3 smarch available cases for configurations generated by the five samplers on jhipster chosen as an example we plot in fig.
the number ofselected features and the difference of this number betweentwo successive configurations.
notice that configurations inthese sample sets are sorted in increasing order based on thenumber of selected features.
it can be found in fig.
thatthe number of selected features for nsbs increases more reg ularly than that for other samplers.
to be more specific theincrement of this number for nsbs is steady being alwaysone while it is either one or two for ddbs and smarch.
in addition configurationssampledbynsbscanbepartitioned into nearly equal sized subsets based on the number of se lected features.
for other samplers however this partition isless balanced.
this can be observed from histograms for the difference indicating that some groups have more configu rations than others.
the above graphical results suggest thatnsbs is capable of sampling configurations that are widelyand nearly uniformly distributed in the behavior space.
experiments performed in this section emphasize the following.first nsbs and sat based sampling are the two best samplers regarding scalability and both of them can handleall fms under study.
second nsbs and ddbs perform bestconcerning diversity of the samples in the behavior space.therefore only nsbs among all samplers tested in this sec tion achieves scalable diverse sampling.
.
rq4 parameter study on k in nsbs kis an important parameter which determines how many configurations in the archive are used to calculate thenovelty score.
to investigate the impact of this parameter we consider six values for k i.e.
and .
notice that is the minimum possible value for k.w h e n k the novelty score of a configuration is evaluated based on its two closest neighbors including itself .
the value 15has been widely employed in ns related literature whilevalues and are and of thesample size i.e.
respectively.
testing multiple valuesofkallows us to observe the trend of the performance as kincreases.
note that to eliminate the impacts of initial population the same set of configurations is initialized forall values of kin each of the independent run.
fig.
presents in the form of boxplots spreadvalues over all runs on four representative fms.
it can be foundthatk performs significantly worse than k on all fms.
according to these results kshould be set to relatively small values e.g.
k .
furthermore it can be found that k yields the best performance on most of the feature models.
hence k is advisable.
icse may pittsburgh pa usa xiang et al.
k020406080100spreadadderii k20406080spreadpati k20406080100spreadbusybox 1 28 0 k200400600800spread2.
.
.
icse11 figure parameter study on kin nsbs.
based on these results k 5is recommended.
finally the following conclusions could be drawn based on the above parameter study.
first the performance of nsbs is indeed affected by its parameter k. in general small values forkare preferred to large ones.
second the value is recommended for this parameter.
that is to say in the context of search based diverse sampling we can directly set kto the value that has been widely used in ns related studies .
.
discussions it is not surprising that nsbs performs better than sat based sampler regarding diversity.
in fact initial populationin nsbs is the outcome of the sat based sampler.
this ini tial population is sequently improved by ns in an incremen tal way.
as shown in fig.
novelty scores of the samplesets are persistently improved during the sampling processof nsbs naturally leading to more and more diverse sam ples.
.
.
.
.
time s .
.
.
.
.
novelty scoredune time s .
.
novelty score2.
.
.
icse11 figure novelty scores of the sample sets are persistently improved during the sampling process ofnsbs it is also easy to explain why ddbs is computationally much more expensive than nsbs.
in fact nsbs uses geneticoperations to generate temporary configurations and then adopts probsat to repair them if necessary.
this is an efficient way of creating new configurations.
instead eachtime ddbs requests to the z3 constraint solver to find aconfiguration with exactly dselected features where dis uniformly drawn from the set of all possible distances.
however it is not always easy to find such configurations because theymay not exist.
sometimes the constraint solver takes long tofind a feasible configuration or fails to return any one evenafter a long time of running.
hence ddbs suffers from lowefficiency.
the following is the reason why the two uniform samplers unigen3 and smarch can not generate diverse samplesin the behavior space.
in fact unigen3 and smarch aim atderiving uniform samples in the configuration space.
theuniformity in this space cannot guarantee diversity in thebehavior space.
recall in section .
that b b b.i fa subspace bis larger then more configurations will be sampled from this subspace.
all these configurations collapse toa single point in the behavior space.
clearly this mechanism could hamper diversity in the behavior space.
.
threats to validity in this section we briefly discuss threats to internal validity and external validity as well as how they could be mitigated.
internal validity.
this type of threats can be caused by potential errors in our implementation of nsbs and thesamplers used for comparisons.
to rule out errors in the im plementation we have thoroughly tested our codes by ana lyzing the outcomes step by step on small fms.
for samplers used in performance comparisons they were implemented by codes provided by their authors.
due to stochastic nature of the samplers under study outcomes of different runs could be different.
to diminish ran dom biases we independently run the samplers times and compare them based on mean values of performance in dicators.
in addition statistical tests are utilized to makereliable comparisons.
external validity.
this threat is related to the degree to which we can generalize from the experiments.
to increase external validity we select real world fms from different domains and most of them have been widely used by otherstoevaluatetheirsamplingalgorithms .thesefm s are representative with respect to the configuration size which ranges from 2to more than .
therefore we are confident that our results could generalize to many morefms.
search based diverse sampling from real world software product lines icse may pittsburgh pa usa related work there are different strategies for sampling configurations fromspls randomsampling solver basedsampling coverage oriented sampling and uniform sampling.
random sampling the simplest way to create a sample set is to randomly assign trueorfalseto each feature for each configuration .
due to constraints amongfeatures however this method is very likely to generate in valid configurations.
instead of randomly selecting features there exist sampling approaches randomly selecting config urations either from all the enumerated configuration space or by using the monte carlo method without exhaustedenumeration .
nevertheless these approaches also select invalid configurations or suffer from low efficiency because of the time consuming or even impractical enumeration.
solver based sampling off the shelf sat or satisfiability modulo theories constraint solvers have been widely used to derive samples.
these solvers include sat4j picosat z3 solver and stochasticlocal search sat solvers .
this kind of samplinggenerally scales well to large real world spls but offers noguarantees about randomness or coverage .
in particular to improve the diversity of configurations henard et al.
randomized the order how the logical clauses and theliterals are parsed.
the resulting randomized sat4j solver which has been extensively adopted in different contexts is selected as a baseline in this paper.
accordingto our results this solver cannot give any guarantees aboutcoverage in the behavior space though.
coverage oriented sampling it creates a sample set according to a specific coverage criterion.
one of the prominent example is t wisesampling in which all possible tfeature combinations must be covered .
nowadays various t wise sampling approaches are available e.g.
chvatal icpl incling yasa and casa .
based onthe evaluations in however most of the t wise sampling techniques can only deal with small fms considering oftent 1o rt .
for large real world spls and or high tinteraction strengths they often run out of memory do notterminate or take too much running time .
uniform sampling achieving uniform sampling is important to understand properties of the whole configurationspace .
recently uniform sampling has caught increasingattention from both sat and spl communities .unigen2 partitions the configuration space as evenly aspossible using hashing functions.
subsequently sampling isdone by choosing a partition at random and then generatinga valid configuration in that partition using an sat solver.unigen2 also supports parallelism on sampling and its improved version i.e.
unigen3 is now available.
several strategies perform counting based uniform sampling.
typi cally they subsequently partition the configuration space onvariable assignments and then count the number of config urations of the resulting parts.
in the number of validconfigurations can be easily counted since an fm is encodedas a binary decision diagram.
both spur andsmarch rely on sharpsat to count the number of valid config urations.
the above samplers guarantee uniform sampling but may encounter a bottleneck in some cases.
according tosundermann et al.
none of their evaluated model counting solvers including sharpsat can count the number of valid configurations for some large industrial spls.
alter natively quicksampler performs an efficient samplingof configurations using only a small number of max satsolver calls.
this sampler however offers no guarantees onuniformity or even validity of the samples .
conclusions this paper focuses on diverse sampling from spls.
in prac tice the number of selected features for a configuration isimportant to characterize its behaviors.
by using this num ber the configuration space is mapped to a small behaviorspace.
deriving a small set of valid configurations that hasa good coverage in the behavior space is required in manysoftware engineering tasks.
however most existing samplingstrategies fail to achieve this goal.
in this paper we proposea search based sampling strategy which adopts an efficient off the shelf sat solver to generate an initial sample set and then improves its diversity in an incremental way.
thisis achieved by using a special distance metric in combinationwith the novelty search algorithm.
experimental results on39 real world spls demonstrate that our sampling algorith m can not only improve coverage in the behavior space butalso promote diversity in the original configuration space.moreover we show both theoretically and experimentally that the designed distance metric is able to mitigate bias towards covering specific parts in the behavior space.
finally our results show that only the proposed sampling algorithmachieves scalable diverse sampling among all the five evalu ated samplers.
focusing on sampling diverse configurations from behavior spaces this paper provides a search based sampler whichis a general tool.
other than the number of selected features other metrics can also be applicable.
in addition it will bevery useful in the future to design dedicated genetic oper ators in the search algorithm.
currently we focus on the sampling mechanism itself.
as one of the subsequent studies we will apply the tool to some real world problems e.g.
t wise testing and performance prediction .