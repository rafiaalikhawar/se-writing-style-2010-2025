refty refinement types for valid deep learning models yanjie gao microsoft research china yanjga microsoft.comzhengxian li microsoft research china v zhli20 microsoft.comhaoxiang lin microsoft research china haoxlin microsoft.com hongyu zhang the university of newcastle australia hongyu.zhang newcastle.edu.auming wu shanghai tree graph blockchain research institute china ming.wu confluxnetwork.orgmao yang microsoft research china maoyang microsoft.com abstract deep learning has been increasingly adopted in many application areas.
to construct valid deep learning models developers must conform to certain computational constraints by carefully selecting appropriate neural architectures and hyperparameter values.
for example the kernel size hyperparameter of the 2d convolution operator cannot be overlarge to ensure that the height and width of the output tensor remain positive.
because model construction is largely manual and lacks necessary tooling support it is possible to violate those constraints and raise type errors of deep learning models causing either runtime exceptions or wrong output results.
in this paper we propose refty a refinement type based tool for statically checking the validity of deep learning models ahead of job execution.
refty refines each type of deep learning operator with framework independent logical formulae that describe the computational constraints on both tensors and hyperparameters.
given the neural architecture and hyperparameter domains of a model refty visits every operator generates a set of constraints that the model should satisfy and utilizes an smt solver for solving the constraints.
we have evaluated refty on both individual operators and representative real world models with various hyperparameter values under pytorch and tensorflow.
we also compare it with an existing shape checking tool.
the experimental results show that refty finds all the type errors and achieves precision and recall demonstrating its effectiveness.
ccs concepts software and its engineering software verification and validation.
keywords deep learning validity checking type error refinement type work performed during the internship at microsoft research.
corresponding author.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa association for computing machinery.
acm isbn .
.
.
.
reference format yanjie gao zhengxian li haoxiang lin hongyu zhang ming wu and mao yang.
.
refty refinement types for valid deep learning models.
in 44th international conference on software engineering icse may pittsburgh pa usa.
acm new york ny usa pages.
https introduction in recent years deep learning dl has been successfully applied to many application areas such as image recognition gaming and natural language processing.
to design layered data representations called deep learning models aka deep neural networks developers employ tensor oriented mathematical operations provided by deep learning frameworks such as pytorch and tensorflow .
these operations are known as operators that manipulate one or more tensors i.e.
multi dimensional arrays including for example matrix multiplication and 2d convolution.
developers use additional arguments called hyperparameters e.g.
the batch size and the kernel size to control the model learning process.
like functions in conventional programming languages operators also enforce computational constraints on both tensors and hyperparameters to construct valid dlmodels.
let us take conv2d the 2d convolution operator as an example for illustration a complete description is presented in section .
.
the input tensor of conv2d should have exactly four dimensions batch n channel c height h and width w .
depending on the position of the channel dimension nchw channels first and nhwc channels last are two widely used tensor formats.
pytorch accepts nchw only tensorflow supports both but developers need to explicitly specify the actual format.
the kernel size a hyperparameter of conv2d that specifies the height and width of the 2d convolution window is an array of two positive integers.
besides the values cannot be overlarge to ensure that the height and width of the output tensor remain positive too.
because model construction is largely manual and lacks necessary tooling support it is possible to violate the above constraints and raise type errors of dlmodels just like those in conventional programs causing either a training inference job to crash e.g.
an overlarge kernel size or a model to produce totally wrong output results e.g.
nhwc training data being fed to a pytorch model by mistake .
recent empirical studies indicate that type errors of dlmodels are not uncommon.
for example zhang et al.
discovered that out of the about .
tensorflow program bugs collected from stack overflow andgithub were caused by ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa yanjie gao zhengxian li haoxiang lin hongyu zhang ming wu and mao yang incompatible tensor shapes i.e.
the lengths of all dimensions .
it is challenging to detect and eliminate these errors before job execution at compile time because the hybrid programming paradigm ofdlframeworks hides the internal computation from high level programs written by developers.
the type errors of dlmodels not only waste significant shared resources including cpu gpu network i o and storage but also severely slow down development productivity.
their ill effects are even worse in the widely adopted practice of automated machine learning automl where an experiment launches many trial jobs simultaneously.
that is to say if one trial job encounters a type error other tens or hundreds of the jobs with similar neural architecture and hyperparameter vector i.e.
a value tuple of all hyperparameters could also experience the same error and fail.
a simple workaround is to run dljobs for a while and check whether any runtime exception is thrown.
however such a dynamic method is both resource and time consuming it is especially unaffordable in the automl scenario because a large number of possible neural architectures and hyperparameter vectors exist.
furthermore invalid dlmodels that do not lead to job crashes but produce wrong output results cannot be caught.
type based techniques have been promising to detect type errors.
however these previous works target programs written in conventional programming languages such as haskell c c or java therefore we cannot apply them directly to dlmodels because of the wide differences in representation structure.
recently a few tools such as pythia are able to find some shape incompatibility errors in certain dl programs.
nevertheless their approaches are tensorflow specific and cannot capture non shape issues.
in this paper we propose refty a refinement type based tool for statically checking the validity of deep learning models.
refinement types are types endowed with logical formulae that constrain values for example int v v stands for positive integers.
we observe that the algorithmic execution of a dlmodel can be represented as iterative forward and backward propagation on the model s computation graph whose nodes denote operators.
our key insight is to refine each type of dloperator with logical formulae that describe the computational constraints on both tensors and hyperparameters including those on how the output tensors are produced.
these logical formulae are framework independent being formulated from the mathematical definitions of operators.
to check whether a pytorch or tensorflow model is valid developers provide its neural architecture which is described in the protocol buffers language or given from a serialized model file and hyperparameter domains.
refty then traverses the computation graph in strict accordance with the operator execution ordering and generates a set of constraints from the above logical formulae.
therefore the problem of checking model validity is reduced to a constraint satisfaction problem csp .refty utilizes a satisfiability modulo theories smt solver e.g.
microsoft z3 to obtain the unsatisfiable hyperparameter vectors that result in potential type errors.
to accelerate constraint solving we apply some special optimization techniques.
refty is extensible to incorporate new operators and custom tensor shapes element types formats it can also be adapted to other dl frameworks.1import torch.nn as nn 2class cnnmodel nn.module 3def init self super cnnmodel self .
init self.conv nn.conv2d in channels out channels kernel size self.pool nn.avgpool2d kernel size stride self.fc nn.linear in features out features self.softmax nn.softmax dim def forward self x x self.conv x x self.pool x x x.reshape x.size x self.fc x x self.softmax x return x figure a sample pytorch model constructed with the conv2d avgpool2d reshape linear and softmax operators.
figure computation graph for training the above model.
we have implemented refty and evaluated it on both individualdloperators conv2d maxpool2d linear add and concat and representative real world models alexnet vgg inception v3 lstm based seq2seq and gru based seq2seq under the pytorch and tensorflow frameworks.
we also compare refty with pythia a static shape checking tool for tensorflow python programs.
the experimental results show thatrefty finds all the type errors achieves precision and recall and outperforms pythia demonstrating its effectiveness.
in summary this paper makes the following contributions we propose a novel refinement type based approach for checking the validity of dl models ahead of job execution.
we implement a tool named refty that generates a set of constraints for dlmodels and utilizes an smt solver to detect potential type errors.
we demonstrate the practical effectiveness of refty with a rich set of experiments.
background .
deep learning models a deep learning dl model is a layered data representation learned from massive training data .
the model is formalized by frameworks like pytorch and tensorflow as a computation graph i.e.
a tensor oriented directed acyclic graph .
each graph node denotes a mathematical operation called an operator that manipulates a list of tensors.
the node may contain numerical learnable parameters i.e.
weights and biases which are iteratively updated during the model learning process.
a directed edge from nodeato another node bdelivers one output tensor of atobas authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
refty refinement types for valid deep learning models icse may pittsburgh pa usa table common type errors of deep learning models.
dimension category description hyperparameterillegal valuethe value of a hyperparameter is outside the domain.
for example typical hyperparameters such as the batch size and stride are positive by definition but zero or negative values are passed.
improper valuethe value of a hyperparameter does not meet the computational constraints.
for example the stride mistakenly exceeds the kernel size of conv2d causing some input data to be skipped.
tensorunsupported format the format of a tensor is not supported.
for example an nhwc tensor is fed to the pytorch conv2d .
illegal shapea certain dimensional length of a tensor is zero or negative.
for example if the kernel size of conv2d is too large the output height or width may be reduced to a non positive value.
incompatible shapethe shape of a tensor is unacceptable or multiple tensors do not have matched shapes.
for example the input tensor of conv2d does not have exactly four dimensions.
incompatible element type a tensor s element type is unacceptable or multiple tensors do not have matched element types.
input and specifies their execution dependency.
in this paper the terms node and operator are used interchangeably.
atensor is a multi dimensional array of numerical values of the same element type.
its order aka rank is the number of dimensions .
let rfdenote the set of all bit floating point numbers.
suppose that xis ann order bit floating point tensor andiiis the length a positive integer of its i th dimension where i n. then x rfi1 i2 in.
the array is called the shape ofx.
for example is the shape of a order nchw tensor representing sixteen images of handwritten digits .
because each dimension of xmay have applicationspecific semantics e.g.
representing the batch size or the image width dimensional orderings derive different tensor formats.
for instance nchw channels first and nhwc channels last are two widely used formats for a batch of 2d images.
figure shows a simple pytorch training program which sets up a sequential model with the framework built in conv2d 2d convolution with a 3kernel size avgpool2d 2d average pooling with 2kernel size and stride reshape flattening the input into one dimension without affecting the batch size linear fully connected layer with ten output features and softmax normalizing the probability distribution over kdifferent classes operators lines .
the variables such as kernel size stride and out features arehyperparameters since they participate in the computation of operators to control the learning process.
to obtain the optimal model learning performance e.g.
predictive accuracy developers largely adopt a trial and error strategy by running multiple jobs each with a different value tuple of all hyperparameters.
figure demonstrates the corresponding computation graph for training the model.
the operators on the left are specified by developers in the training program.
the auxiliary operators in the middle are automatically crafted by frameworks for computing gradients under backward propagation.
an optimizer at the bottom right is responsible for weight update and loss minimization marking the end of one training iteration.
although the model is simple enough developers may make the same mistakes mentioned in section at conv2d andavgpool2d lines .
furthermore developers may pass a wrong number of input features to the linear operator via the in features parameter in line and raise an incompatible tensor shape error because such a number has to be manually calculated from the output tensor shapes of the predecessor avgpool2d andconv2d.
.
common type errors of dl models table lists sixcategories of common type errors which are summarized from related empirical studies and our experience.
we further group these categories into two major dimensions hyperparameter and tensor.
errors in the former dimension are directly caused by hyperparameter values.
illegal value means that the value of a hyperparameter is outside the domain.
typical hyperparameters are positive integers by definition however since they are declared as signed integers in python developers may pass zero or negative values by mistake.
for example a user encountered a crash when setting the stride to zero .improper value means that the value of a hyperparameter violates the intrinsic computational constraints although it is within the domain.
for instance the stride should be less than or equal to the kernel size otherwise some input data will be skipped .
another example is that an out of bounds exception is thrown when the axis specified by dim of the pytorch gather operator exceeds the input tensor s order.
tensor related errors result from inappropriate formats shapes or element types of tensors.
unsupported format means that the format of a tensor is not supported by the operator which is demonstrated by the previous nhwc vs.nchw example.
because the output shape is computed by the input shape s and hyperparameters it is possible that a certain dimensional length of an output tensor becomes less than or equal to zero and raises an illegal shape error.
for instance if the kernel size of conv2d is too large the output height or width may be reduced to a non positive value .incompatible shape means that the shape of a tensor is unacceptable or multiple tensors do not have matched shapes.
this category is also referred to as unaligned tensor mismatched tensor or shape inconsistency in the related studies.
shapeflow shows an example that the developer mistakenly interchanged the images and their labels.
another example is that the two input tensors of the addition operator do not have exactly the same shape.
similarly errors in the incompatible element type category are caused by unacceptable or mismatched element types.
.
refinement types in programming languages the type system is an important and useful formal method to enforce the expected behaviors of programs.
a type is an attribute of data that permits developers to specify correct values for data operations.
therefore a large portion of unintended software faults aka type errors occurring at run time can be detected by compilers before execution.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa yanjie gao zhengxian li haoxiang lin hongyu zhang ming wu and mao yang matmul x1 ts x1 px1 x2 ts x2 px2 ts y py px1 x1.order x1.shape x1.shape px2 x2.et x1.et x2.order x2.shape x1.shape x2.shape x2.shape py y.et x1.et y.order y.shape x1.shape y.shape x2.shape figure refinement type of the matmul operator.
in freeman and pfenning first introduced the concept of refinement types to standard meta language sml a general purpose functional programming language.
a refinement type lets developers endow logical formulae from a decidable logic to limit the value set of the original type.
for instance the following natandposrefinement types specify non negative integers and positive integers respectively nat int v v pos int v v .
for another example a finite set of positive integers ni ni n nifori can be specified by the refinement type pos v v n1 v n2 v nk .
with refinement types developers can write more precise contracts for programs to improve code testing and verification.
in the following we take the matmul matrix multiplication operator as an example and illustrate its refinement type in figure .
let tsbe the basic tensor type.
for a tensor variable x we usex.et x.order andx.shape to denote the element type e.g.
float ordouble order and shape of x respectively.
x.shape refers to the length of thei th dimension of x. we ascribe matmul a function type specifying that it accepts two input tensor parameters denoted by x1and x2 and produces one output tensor parameter denoted by y .
the logical formula px1states thatx1is a legal matrix that is to say the order ofx1is and the length of either dimension is positive.
px2 declares that x2is a legal matrix with the same element type as x1 and the number of rows of x2is equal to the number of columns of x1.
finally pyclaims that the output tensor yis also a matrix with the same element type as the two inputs and yhas the number of rows ofx1and the number of columns of x2.
to save space we do not explicitly assert the legality of y which has been implied by the existing formulae.
.
satisfiability modulo theories smt in the fields of mathematical logic and computer science satisfiability modulo theories smt refers to deciding the satisfiability of mathematical formulae.
compared with boolean satisfiability sat whose formulae are built up from only boolean variables and logical connectives e.g.
negation and conjunction smt significantly generalizes the expressiveness by allowing more complex first order formulae with equality quantifiers and and symbols of constants e.g.
functions e.g.
and predicates e.g.
.
the satisfiability of smt formulae is interpreted within a theory of integers real numbers bit vectors arrays strings etc.
which explains the origin of the name of smt.
the theory defines a variable domain e.g.
the set of real numbers and assigns a definite meaning to each constant function predicate symbol e.g.
being the lexicographical ordering on strings .
for example the formula x y y x is satisfiable within the theory of integer arithmetic and x y is one solution.
satisfiability modulo theories library smt lib is a prominent standard that provides rigorous specifications of background theories and an input output language for expressing smt formulae.
the syntax of the smt lib language is very similar to that of lisp .
the following shows a portion of the smt lib program for the above formula whose expressions use prefix notation.
set logic qf nia quantifier free integer arithmetic declare const x int x is an integer variable declare const y int y is an integer variable assert and x y y x smt solvers are software tools that deduce whether or not a set of formulae is satisfiable among which microsoft z3 stp and yices are popular.
most smt solvers accept an smt lib program as input.
to facilitate writing formulae they may also provide apis for other programming languages e.g.
python and java .
because of the powerful functionality smt solvers have been adopted as an essential module for various tools across many application areas such as software testing and program verification.
approach and implementation .
problem formulation we summarize the syntax of types in figure .
an element type is an atomic primitive type e.g.
float for the set of bit floatingpoint numbers .
currently there are four allowable element types.
atensor type represents single or multi dimensional arrays whose elements have the same type.
for convenience we use x.et x.order x.shape x.fmtto denote the element type order i.e.
the number of dimensions shape i.e.
an array of all dimensional lengths and format of a tensor variable x ts respectively.
terms are mathematical expressions built from constants and variables by applying n place elementary operations .
because binary operations such as addition subtraction multiplication division and modulo denoted by mod are mainly used we specifically list their syntax rules.
in most cases refinement types use quantifier free formulae including boolean constants arithmetic comparison between two terms and negation conjunction disjunction of existing formulae.
certain formulae may involve every dimension of a tensor variable whose order is not known in advance.
therefore we add an implication rule with the universal quantifier v b.p p2 meaning that for each vof basic type b if the condition p1holds then so mustp2.
however the usage is restricted by bounded quantification to ensure that the generated constraints remain decidable that is to say vis placed under an upper bound in the condition p1 e.g.
v x.order .
a refinement v p is a pair such that vis a variable and pis a logical formula.
types then include either refined base types i.e.
basic types endowed with refinements for tensors and hyperparameters or dependent function types i.e.
function types depending on parameter values for dloperators.
note that the type of an operator with ninputs is written asx1 1 xn n .
acontext is a sequence of variable type bindings to record which variables are in scope.
the basic types are distinct from each other so there is no subtype relationship between any two refined base types with authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
refty refinement types for valid deep learning models icse may pittsburgh pa usa symbols sy v x y constants variables element types et bool int float double tensor type ts et ts basic types b et ts terms e sy constants and variables e1 e2 e1 e2 e1 e2 e1 e2 e1mode2 place expressions fe1 en n place expressions formulae p true false booleans e1 e2 e1 e2 place predicates p p1 p2 p1 p2 negation conjunction disjunction v b.p p2 implication refinements r v p known types b r refined base v dependent function context v variable type binding figure syntax of types.
different basic types.
supposing that pis a formula p denotes that all the free i.e.
not restricted by quantifiers occurrences ofvinpare substituted by z .
subtyping denoted by on b x px andb y py is defined as follows b x px b y py x b.px py .
sub base therefore posis a subtype of nat i.e.
pos nat because v int.
v vis a valid formula.
another example is that the 3matrix type is a subtype of the order valid tensor type which is the type of the first input parameter of matmul in figure .
as mentioned before a dlmodelmis formally represented as the following directed acyclic computation graph m v opi n i e opi opj i j hp hpi k i .
each nodeopiis an operator specified by developers e.g.
conv2d andavgpool2d in figure .
we ignore the automatically inserted operators for backward propagation because they are crafted by frameworks to calculate gradients in strict accordance with the forward operators.
we assume that a backward operator should not introduce any type errors listed in table if its corresponding forward operator executes correctly.
a directed edge opi opj delivers a tensor from opitoopjand specifies that opjmust wait until the execution of opifinishes.
each hyperparameter hpiis endowed with a refinement type bi hpi pi that defines the domain ofhpi denoted by i .biis a basic type such as intor the basic tensor type and piis a logical formula.
for unified handling the initial input tensors are treated as hyperparameters.
we denote the hyperparameter configuration space ofmas 1 2 k. each is called a hyperparameter vector.
first we explain the methodology of checking the validity of m. let hp1 b1 hp1 p1 hpk bk hpk pk be the context of the modelm.
for an operator opi similar to a conventional function call we check for each input argument vi kwhether its type is a subtype of denoted by that of the corresponding formal input parameter vi k. in other words the context must deduce the following judgment bvi k vi k pvi k bvi k vi k pvi k .
chk in if so we conclude that each output argument yi lis associated with denoted by the type of the corresponding formal output parameteryi l with variable substitution yi l byi l yi l pyi l .
chk out for a directed edge opi opj similar to an assignment statement we conclude that opj sn th input tensor xj nis associated with the type ofopi sm th output tensor yi m with variable substitution xj n byi m xj n pyi m .
chk edge next we show how to generate a set of constraints whose satisfiability implies that mis valid.
we observe that the algorithmic execution ofmis represented as iterative forward and backward propagation on its computation graph.
model inference can be thought of as single pass forward propagation.
let s opi1 opi2 opin be the actual runtime execution ordering of operators which is linearly extended from the edge ordering by referring to the framework implementations .
we traverse the computation graph by strictly following sand generate the constraints according to the above three judgments.
for the chk in judgment we first generate bvi k bvi k i.e.
both basic types should be identical .
we then generate vi k bvi k.pvi k pvi k from the previous subtyping definition.
however since any defective hyperparameter vector leads to unsatisfiability we eliminate the universal quantifier to discover all the defective vectors.
we also notice that pvi keither is implied by ifvi kis a hyperparameter or has been generated during the visit of a predecessor operator.
therefore we finally use the simplified constraint pvi k .
for the chk out judgment we generate byi l byi landpyi l .
for the chk edge judgment we generate a simpler constraint xj n yi m. finally we formulate the problem of checking the validity of mas a constraint satisfaction problem csp .
being a general and useful concept csp abstracts a problem as finding a solution to a set of imposed constraints that must be satisfied as conditions by the variables.
it is an important research subject in artificial intelligence ai .
well known csps include eight queens puzzle and graph coloring.
our problem is defined as a triple v d c v v1 v2 vk vi hpifori d d1 d2 dk di ifori c n i 1ci xj n yi m opi opj e ci bvi k bvi k pvi k byi l byi l pyi l .
vis a set of variables that are actually the hyperparameters.
d represents the respective variable domains such that each diis the domain of hpi.ccontains the generated constraints mentioned above.
for a hyperparameter vector v1 v2 vk mis valid with if the above set cis satisfiable when each vi is assigned the corresponding vi otherwise results in type errors.
.
refinement types of dl operators in this section we take conv2d the 2d convolution operator as an example to demonstrate how to define refinement types.
we consider the following hyperparameters number of input channels ic number of output channels oc tensor format fmt kernel size ks stride sd padding pad dilation dil and number of groups authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa yanjie gao zhengxian li haoxiang lin hongyu zhang ming wu and mao yang conv2d x ts x px px ic pos oc pos fmt nchw nhwc ks pos pad pos dil pos sd pos sd psd psd gp pos gp pgp ts y py py py px x.order x.shape x.shape x.shape x.shape x.et float x.et double px x.fmt fmt ispt x.fmt nchw x.shape ic psd sd ks sd ks psd istf dil dil sd sd pgp icmodgp ocmodgp py y.et x.et y.order x.order y.fmt x.fmt y.shape x.shape y.shape oc py y.shape x.shape pad dil ks sd y.shape x.shape pad dil ks sd py y.shape y.shape y.shape y.shape figure refinement type of the conv2d operator.
tunable hyperparameters include the numbers of input ic and output oc channels tensor format fmt kernel size ks stride sd padding pad dilation dil and number of groups gp .posrefers to the refinement type of positive integers and tsis the basic tensor type.
to save space channel height and width denote the indices of the channel height and width dimensions respectively.
as usual x.et x.order x.shape x.fmtdenote the element type order shape and format of the tensor variable x respectively.
gp .
the tensor format is either nchw ornhwc other hyperparameters require positive integer values although their declarations use the python inttype.
we assume that each of the kernel size stride padding and dilation is an array of two positive integers that are used for the height and width dimensions respectively.
the core computation of conv2d is described as follows y i j bias j ic k 0weight j k x i k where i the batch size j oc xandyare the input and output tensors and is the 2d cross correlation operation.
from the mathematical definition and framework implementations of conv2d we extract the following computational constraints on both tensors and hyperparameters and show the formalized refinement type of conv2d in figure where item labels correspond to the names of the logical formulae.
px 1the input tensor should have exactly four dimensions whose lengths are positive integers.
its elements are floating point numbers i.e.
the element type is either float ordouble.
px 2the actual format of the input tensor should be equal to the tensor format hyperparameter for pytorch models it must benchw .
the latter is a pytorch specific constraint so we use a boolean control variable ispt initialized to true when the framework is pytorch.
the length of the input tensor s channel dimension should be equal to the number of input channels.
note that the dimensional indices except that of the batch dimension always are not constants because of two possible tensor formats.
to save space in figure we usechannel height and width to denote the indices of the channel height and width dimensions respectively.
psd 1each element of the stride cannot exceed that of the kernel size otherwise some input data is mistakenly skipped.
psd 2tensorflow additionally requires that the stride and dilation cannot both be greater than .
similarly we use another boolean control variable istf in this constraint which is initialized to true when the framework is tensorflow.pgpthe number of groups specifies the number of blocked connections from input channels to output channels therefore the numbers of input and output channels should both be divisible by it.
py 1the output tensor has the same element type order format and length of the batch dimension with the input tensor.
the length of its channel dimension should be equal to the number of output channels.
py 2the output height and width are calculated as follows hout hin pad dil ks sd wout win pad dil ks sd .
both pytorch and tensorflow also allow developers to pass a string valid or same as the value of the padding hyperparameter.
valid means no padding.
same means that the framework automatically pads the input evenly therefore the above calculation can be simplified ashout hin sd andwout win sd .
when using the same padding pytorch requires that both elements of the stride must be so the output has the same height and width as the input.
py 3the output tensor is also legal that is to say each length of the four dimensions is a positive integer.
at present we support types of dloperators.
refty is extensible to incorporate new operators which is discussed in section .
.
.
workflow figure illustrates how refty works.
refty accepts a computation graph a model specification and a tool specification as input from developers or automl tools.
the computation graph is reconstructed from a serialized pytorch or tensorflow model in the form of disk files using our front end parsers that invoke the framework built in model deserialization apis.
also developers can describe authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
refty refinement types for valid deep learning models icse may pittsburgh pa usa figure workflow of refty.
such a graph manually in the protocol buffers language using the intermediate representation of mmdnn whose syntax is very similar to that of onnx .
the model specification includes the tunable hyperparameters and their domain definitions.
unspecified hyperparameters will use the framework default values.
the tool specification contains the framework type version solving mode optimization choice etc.
refty traverses the computation graph by following the operator execution ordering to generate a set of constraints that the dl model should satisfy.
it then utilizes microsoft z3 to obtain the unsatisfiable hyperparameter vectors that result in type errors.
refty chooses smt solvers for two reasons the constraints can be specified with smt solvers built in types e.g.
integer and array algebraic data types e.g.
scalar and record functions e.g.
multiplication and modulo and value comparison e.g.
equality and less than they handle nonlinear e.g.
calculating the output size ofconv2d and higher order mathematical expressions very efficiently.
nevertheless the scalability of smt solvers may potentially limit the usability of refty which is discussed in section .
.
in the final report to users refty presents the corresponding hyperparameter vector for each detected error.
if needed for further diagnosis refty additionally reports all the error related constraints and the operators to which these constraints belong.
.
graph traversal and constraint generation for each supported type of operator refty aforehand translates the logical formulae of its refinement type to smt lib statements in a reusable template using the python api of z3.
the symbol names of tensors and hyperparameters are parameterized to handle variable substitution.
refty declares integer constants for the available element types and known tensor formats.
to denote the tensor type it defines a custom record type with four fields representing the element type format order an integer and shape an array of the tensor respectively.
the manipulation over all the dimensions of a tensor with a non constant order e.g.
calculating the total number of tensor elements is implemented by recursive functions.
refty first creates a z3 instance checks the basic types of the hyperparameters specified in the model specification and defines all the hyperparameter symbols.
it then traverses the computation graph one operator by another by following a predefined sequence s opi1 opi2 opin that denotes the actual runtime execution ordering of operators.
sis linearly extended from the edge ordering by reference to the framework implementations .when visiting an operator op refty locates the smt lib template byop s type checks the basic types of both input and output arguments against those of the corresponding formal parameters fills in the actual symbol names derived from op s name and executes the smt lib statements to add constraint formulae to the solver instance.
refty also inserts equality constraints that assert that op s input tensor arguments are exactly the same as those outputs of the immediate predecessors.
after the graph traversal finishes refty finally adds multiple formulae defining the domain of each hyperparameter.
.
constraint solving refty utilizes microsoft z3 to solve the generated constraints.
to reduce the nondeterminism in z3 s conflict driven clause learning cdcl implementation we split a constraint formula into as many smaller self contained ones as possible and add them one by one to the solver instance in our smt lib templates.
if users need to diagnose the root cause of an error refty employs the minimal unsatisfiable core i.e.
an unsatisfiable subset containing the least number of the original formulae returned by z3 to report all the error related constraints and the operators to which these constraints belong.
refty implements two solving modes individual mode and bulk mode.
under the individual mode refty independently solves the constraints for each vector in the hyperparameter configuration space.
this mode is simple requires less effort and facilitates tool integration.
for example automl tools can work nearly as usual they need to invoke refty with the hyperparameter vector being explored just before launching a trial job.
however there exist continuous significant warm up overheads of traversing the computation graph generating the constraints and initializing the solver instance.
refty applies the context reuse technique to improve solving performance it reuses the existing z3 instance and almost all the initialized constraint formulae pops out the domain definition formulae that were added last and pushes new domain definition formulae for the next hyperparameter vector.
under the bulk mode refty solves the constraints for once with the whole hyperparameter configuration space.
however z3 returns only one satisfiable hyperparameter vector when there is any instead of supplying all at once.
therefore refty implements a simple allsat all solutions sat feature it iteratively invokes the same z3 instance by appending the negation of the newfound solution to derive the next one.
supposing that v1 v2 vk is a newfound satisfiable vector refty will append the constraint hp1 v1 hp2 v2 hpk vk .
as more and more solutions are discovered allsat may get slower and slower because the appended negation formulae are becoming too many.
after obtaining the set of the satisfiable hyperparameter vectors refty uses the set difference operation to derive all the unsatisfiable ones.
finally it reruns completely from the beginning under the individual mode for each unsatisfiable hyperparameter vector to get the minimal unsatisfiable cores.
constraint solving may slow down significantly if there exist a large number of constraints or a huge hyperparameter configuration space.
refty adopts two optimization techniques proposed by authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa yanjie gao zhengxian li haoxiang lin hongyu zhang ming wu and mao yang dnnsat .
the first is parallel using worker threads and distributed using spark solving which divides the solving task into subtasks and processes them simultaneously.
the other is tiny subspaces the original hyperparameter configuration space is divided into numerous disjoint small subspaces to reduce the allsat overhead and tackle the skew problem .
these two techniques can be used in conjunction with context reuse.
evaluation .
experimental design refty is evaluated on individual operators and real world models under two mainstream dlframeworks of pytorch v1.
.
and tensorflow v1.
.
.
we aim to answer the following research questions rqs rq1 how effective is refty on individual dl operators?
rq2 how effective is refty on real world dl models?
rq3 how efficient is refty in constraint solving?
rq4 how does refty compare with related work?
we fine tune several hyperparameters that are often tuned by developers in practice including the batch size input image size height width for convenience tensor format kernel size stride number of features etc.for the hyperparameter values we first select some typical ones such as and for the batch size nhwc andnchw for the tensor format and 5for the kernel size of conv2d 2for the kernel size of maxpool2d 1and 2for the stride and for the number of features.
the initial input size for the imagenet dataset is usually while inception v3 uses 299by default.
some neighboring values e.g.
4for the kernel size and stride are also selected.
we further try a few smaller e.g.
for the number of features larger e.g.
12for the kernel size or even invalid values that may lead to potential errors.
in order to obtain the ground truth of whether or not a hyperparameter vector results in type errors we feed to the model training program and run it with one batch of input data.
assertions are also added to the program to detect errors that may not fail the execution but cause the final model to produce wrong output results e.g.
an input tensor having an unsupported format .
if the program crashes we say that is an actual positive otherwise is an actual negative.
we then run refty on the hyperparameter configuration space of the model compare each result including the hyperparameter vector and possible root cause with the ground truth and calculate the numbers of true positives false positives i.e.
correct hyperparameter vectors being misreported as defective true negatives and false negatives i.e.
defective hyperparameter vectors not being detected .
we adopt the standard metrics of precision andrecall to assess the effectiveness of refty which are defined as follows precision tp tp fp recall tp tp fn .
the above tp fp tn and fndenote the numbers of true positives false positives true negatives and false negatives respectively.
consequently the numbers of actual positives and actual negatives are equal to tp fnandfp tn respectively.
the higher precision and recall values the more effective refty is.we conduct the experiments on an azure standard nd12 virtual machine that has intel xeon e5 2690v3 vcpus .
ghz 30m cache and gb main memory running ubuntu .
.
.
rq1 how effective is refty on individual dl operators?
we select five typical operators of conv2d maxpool2d 2d max pooling linear add addition of two tensors and concat concatenation of a list of tensors in a given dimension to evaluate the effectiveness of refty.
forconv2d andmaxpool2d we tune their batch size and tensor format nchw andnhwc number of input channels input image size and kernel size and and stride and .
forlinear we use a order image tensor as input and tune its batch size and number of input channels and input image size and .
we also tune the numbers of input features and and output features and of linear.
foradd we use two order image tensors and independently tune their batch size and input image size and and number of input channels and .
forconcat the input sequence consists of two order image tensors.
we independently tune their element type float and bool batch size input image size and and number of input channels and .
we devise the above hyperparameter domains to cover all the error categories shown in table .
for example when the input image size is 7and the kernel size is 15in the conv2d experiments illegal shape errors are raised.
we also use a negative value as the number of output features of linear which leads to illegal value errors.
to obtain the ground truth we compose a program for training a model that contains only the experimental operator.
because of the small hyperparameter configuration spaces refty runs under the individual solving mode for simplicity.
the average warm up time of refty is .
second for conv2d maxpool2d and linear and is .
second for addandconcat .
the solving time is .
second per hyperparameter vector on average.
table shows the experimental results.
refty finds all the type errors and does not introduce any false alarms.
it achieves precision and recall which demonstrates the effectiveness of our formulated refinement types of dl operators.
.
rq2 how effective is refty on real world dl models?
we evaluate refty on five real world dlmodels alexnet vgg inception v3 lstm based seq2seq andgru based seq2seq .
they are representative in the areas of computer vision natural language processing and speech recognition.
some of them such as lstm andgru are also widely used in various dlfor software engineering applications .
foralexnet andvgg we set the number of input channels to input batch size to or and input image size to .
we tune the kernel size and and stride and of the first conv2d operator.
we also authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
refty refinement types for valid deep learning models icse may pittsburgh pa usa table experimental results on individual operators.
metricsoperatorconv2d maxpool2d linear add concat pytorch t otal true positive true negative false positive false negative precision recall tensorflow t otal true positive true negative false positive false negative precision recall try different kernel sizes and and strides and for the first maxpool2d operator.
forinception v3 we use identical hyperparameter domains except that the input batch size is and the input image size is .
for the other two seq2seq models we tune the number of features in the hidden state and and the number of recurrent layers and for both encoder and decoder.
we implement alexnet by ourselves download the training programs and serialized model files of vgg andinception v3 from the official websites and implement seq2seq by referring to open source code .
it takes about to seconds to run the training programs with one batch of input data and one hyperparameter vector for obtaining the ground truth excluding the time of data preparation and model validation testing.
refty still runs under the individual solving mode.
the numbers of z3 symbols and constraint formulae of alexnet vgg inception v3 lstm based seq2seq and gru based seq2seq are and respectively.
the average warm up time and solving time in seconds of refty are .
.
.
.
.
.
.
.
and .
.
respectively.
table shows the experimental results.
refty again achieves precision and recall in all the experiments which demonstrates its effectiveness on dlmodels.
we notice a discrepancy in the ground truth of the vgg andinception v3 experiments.
the root cause lies in the differences between the official training programs.
in the tensorflow vgg experiments many hyperparameter vectors cause the output shape of the last maxpool2d to violate the requirement of the succeeding linear which brings more crashes.
instead the pytorch program inserts an extra adaptiveavgpool2d 2d adaptive average pooling operator with proper padding to resolve this issue.
in the pytorch inception v3 experiments the fixed padding values make the pytorch program more vulnerable to illegal shape errors and lead to more crashes.
.
rq3 how efficient is refty in constraint solving?
we evaluate refty on the constraint solving efficiency with the optimization techniques mentioned in section .
.
inception v3 table experimental results on real world dl models.
metricsmodel alexnet vgg inception v3seq2seq seq2seq lstm gru pytorch t otal true positive true negative false positive false negative precision recall tensorflow t otal true positive true negative false positive false negative precision recall is selected as our experimental object it is one of the representative real world models in computer vision and has a more complex neural architecture in terms of width and height than the other four dlmodels.
the tunable hyperparameters include the input batch size and input image size different sizes and kernel size different sizes for the first three conv2d operators.
therefore the hyperparameter configuration space of inception v3 consists of vectors in total.
to increase the solving difficulty we carefully devise the above hyperparameter domains to make every hyperparameter vector satisfy the constraints.
for parallel solving we create and worker threads.
more threads are not considered since there are only vcpus on the experimental machine.
for tiny subspaces we try subspace sizes of equipartition and .
equipartition means that the original hyperparameter configuration space is simply divided equally by the number of threads.
for example if we use worker threads each will independently solve one subspace that includes about 094hyperparameter vectors.
context reuse is enabled by default for all the experiments.
the reason is that the overhead of a complete z3 instance initialization is evident .
seconds on average as reported in section .
and causes some experiments to run too long.
table demonstrates the end to end execution time in seconds ofrefty under both individual and bulk solving modes.
we also show the speedup relative to the baseline experiment individual mode thread bulk mode thread equipartition .
our results confirm the strong effects of the optimization techniques.
under the individual mode parallel solving achieves a near linear speedup from .9x to .8x since the computational complexity of solving each hyperparameter vector is identical.
under the bulk mode the overall speedup ranges from .5x to .0x.
simply creating more worker threads see the equipartition row achieves a superlinear speedup from .3x to .4x because smaller hyperparameter configuration spaces observably reduce the overhead of allsat .
tiny subspaces see the thread column are also effective for the above same reason.
after combining these two techniques refty achieves a more promising and near linear speedup from .3x to authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa yanjie gao zhengxian li haoxiang lin hongyu zhang ming wu and mao yang table runtime performance of refty with parallel solving and tiny subspaces.
context reuse is enabled by default.
modesize of number of threads subspace individual .
.
.
.
.
s .
s .
s .
s bulkequipartition1.
.
.
.
.
s .
s .
s .
s .
.
.
.
.
s .
s .
s .
s .
.
.
.
.
s .
s .
s .
s .
.
.
.
.
s .
s .
s .
s note the two values in a cell represent the speedup and execution time in seconds respectively.
.0x.
as the subspace size decreases the overhead of allsat also continues to reduce and the performance gain gradually increases.
.
rq4 how does refty compare with related work?
we compare refty with pythia a static shape checking tool for tensorflow python programs.
pythia detects incompatible shapes by modeling and tracking tensor shapes across tensorflow api calls.
the analysis of pythia uses the wala framework and declarative datalog rules.
we select the unaligned tensor ut1 program1as the experimental object because pythia does not support the tensorflow slim api used by the training programs of our evaluated real world models section .
.
ut is one of the fourteen programs provided by zhang et al.
that contain unaligned tensor bugs and it has been successfully analyzed by pythia .
ut trains a convolutional model for mnist handwritten digit classification which is much more complicated than those implemented in other ut programs.
since pythia checks tensor shape mismatches we devise the hyperparameter configuration space such that only illegal shape and incompatible shape errors if any may be raised.
before the experiment we have actually tried some hyperparameter vectors that led to errors in the other four categories and noticed that pythia could not detect them.
ut invokes the primitive neural net api e.g.
tf.nn.conv2d that uses a new filter hyperparameter instead of the kernel size for convolutional operators.
the filter is a order tensor of the shape kernel height kernel width number of input channels number of output channels .
we randomly select and shapes such as and for the filter hyperparameter of the two conv2d operators respectively.
we also tune the kernel size and of the first maxpool2d operator and set the padding of the two maxpool2d operators to valid .
therefore there are hyperparameter vectors in total.
for each vector we fill the corresponding hyperparameter values in the ut program and then pass the program file to pythia for analysis.
pythia starts with a ten minute warm up and after that stackoverflow ut buggy mnist.pytable comparison with pythia.
metricstoolpythia refty tensorflow t otal true positive true negative false positive false negative precision recall .
each checking takes seconds on average.
refty runs under the individual solving mode and its warm up time and solving time are about .
second and .
second per vector respectively.
table shows the experimental results.
refty still achieves precision and recall.
although the precision of pythia is its recall is only .
a rather low percentage.
the reason is that pythia does not detect buggy cases.
in of them a tensor of an illegal shape is produced by a conv2d operator.
in the other cases the input and filter tensors of the first conv2d operator do not have a matched number of input channels which causes incompatible shape errors.
discussion .
extensibility of refty currently refty supports two mainstream dlframeworks pytorch and tensorflow and types of commonly used operators.
refty can be adapted to other frameworks such as apache mxnet and onnx because they also adopt the same abstraction to represent models as tensor oriented computation graphs.
the syntax and semantics of their graphs and operators are very similar to those of pytorch and tensorflow therefore existing smt lib templates of the refinement types can be reused with minor modifications.
nevertheless the graph traversal may need to be adjusted to accord with the actual operator execution ordering of other frameworks.
refty is also extensible to incorporate new dloperators.
to support a new type of operator users need to extract the computational constraints formulate a refinement type for each formal input output parameter based on the operator s mathematical definition and framework implementations and implement an smt lib program template.
.
threats to validity we discuss the following main threats to the validity of our work.
refinement types.
we formulate the refinement types of dl operators by our domain experience based on their mathematical definitions.
we also examine the source code of both pytorch and tensorflow to incorporate framework specific computational constraints into the formulation.
nevertheless because of the large manual effort involved in formulating refinement types and preparing smt lib templates there might be a potential inaccuracy.
to mitigate this threat we strived to reach group consensus before making decisions and we continuously refined our approach by carefully referring to the documentation and framework source authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
refty refinement types for valid deep learning models icse may pittsburgh pa usa code.
in our experiments refty achieves precision and recall confirming the validity of the refinement types of dl operators.
smt solving.
refty utilizes microsoft z3 to solve the generated constraints.
as dlmodels become more sophisticated and hyperparameter configuration spaces enlarge gradually the computational complexity of constraint solving will increase observably and thus may reduce the usability of our tool.
currently refty implements context reuse and adopts another two optimization techniques of parallel distributed solving and tiny subspaces from dnnsat to accelerate constraint solving.
our experiments on a fairly large hyperparameter configuration space with vectors have demonstrated the strong effects of these optimizations .
in the future we may advance smt solvers and try larger scale distributed solving to better tackle this problem.
related work there are some recent empirical studies on deep learning program defects and job failures.
zhang et al.
studied tensorflow program bugs collected from stack overflow pages and github commits about .
of which were caused by unaligned tensors.
islam et al.
extended this work to cover more frameworks such as torch and caffe they discovered that the same reason was a major root cause about .
of the total bugs and was the leading cause of torch bugs.
zhang et al.
conducted an empirical study on failed dljobs collected from an internal platform of microsoft they found that about .
out of the dl specific failures were caused by mismatched tensors.
zhang et al.
examined stack overflow questions to study common challenges in dlapplication development many of which asked for help on shape inconsistency bugs.
these empirical studies indicate that type errors of dlmodels are not uncommon and motivate us for a formal and systematic solution.
there have been many type based techniques to perform correctness checking on programs.
some works enrich the type system with specific tensor properties for static verification.
for example eaton proposed strongly typed linear algebra for numerical algorithms in which the dimensions of matrices were exposed.
a few others reason whether the array index is out of bounds.
for instance index checker is an open source tool to discover array access errors in java programs whose type system is conditioned by cooperating hierarchies of dependent types.
as described before these previous works target programs written in conventional programming languages e.g.
haskell and java and cannot be applied to dlmodels directly.
recently wen et al.
leveraged refinement types and constrained the values of tensor elements to verify the adversarial robustness of neural networks which was commonly characterised as the deviation in the neural network s outputs given perturbations of its inputs .
two libraries implemented in f and liquid haskell were provided for constructing fully connected neural networks that use four common activation functions.
to make verification tractable users need to reduce the tensor dimensionality and model size manually.
in comparison refty targets a different problem and supports more types of dloperators it constrains hyperparameter values and tensor shapes element types formats to check the validity of models and eliminate common type errors.
leonardoet al.
proposed tensorsafe a haskell library that helps developers define the neural architectures of dlmodels and leverages the haskell type system to validate the structural correctness.
tensorsafe does not use refinement types to describe further computational constraints on tensors and hyperparameters.
therefore although tensorsafe detects incompatible shape errors it cannot find errors in the other categories shown in table .
popular dlframeworks adopt a hybrid programming paradigm the runtime is implemented in c for high execution performance while developers use other language bindings e.g.
python for flexible programmability.
some recent works try to integrate dl frameworks directly into programming languages.
swift for tensorflow which was archived in february implements language integrated differentiable programming and allows developers to construct models with the swift language and the familiar tensorflow api.
onnx scala brings full support for onnx to the scala ecosystem.
powered by the type systems of swift and scala the two works are able to detect some type errors of dl models at compile time.
for example swift for tensorflow checks whether the types of hyperparameter arguments are expected but it does not inspect hyperparameter values and track tensor shapes.
furthermore they only support a specific framework and cannot be adapted to others.
recently researchers proposed static and dynamic techniques to detect bugs in dlprograms.
ariadne applies wala program analysis to tensorflow python code and tracks tensors via a custom type system.
pythia is a static shape checking tool for tensorflow programs that uses wala and declarative datalog rules.
pythia models and tracks tensor shapes across tensorflow api calls and checks whether incompatible shapes exist.
shapeflow modifies the implementation of tensorflow api to capture and manipulate tensor shapes and runs dlprograms to detect shape incompatibility issues.
these tools are friendly to developers since they directly operate on python code.
however they are tensorflow specific focus on incompatible shape errors and have difficulty detecting other errors caused by improper hyperparameter values unsupported tensor formats etc.refty is a novel refinement type based tool for both pytorch and tensorflow.
it rigidly formulates the computational constraints enforced by operators and systematically detects invalid dlmodels caused by sixcategories of type errors.
refty also achieves good runtime performance for example it checks hyperparameter vectors of inception v3 in about seconds.
conclusion in this paper we have presented refty a refinement type based tool for statically detecting common type errors of deep learning models.
these errors are caused by tensors and hyperparameters that violate the computational constraints.
the problem of checking the validity of a model is formulated as a constraint satisfaction problem and refty utilizes an smt solver for solving the constraints.
our experiments demonstrate that refty is very effective at detecting potential type errors before job execution.