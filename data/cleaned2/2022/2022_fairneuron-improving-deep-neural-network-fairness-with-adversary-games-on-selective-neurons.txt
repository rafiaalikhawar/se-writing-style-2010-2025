fairneuron improving deep neural network fairness with adversary games on selective neurons xuanqi gao xi an jiaotong university xi an china gxq2000 stu.xjtu.edu.cnjuan zhai rutgers university united states juan.zhai rutgers.edushiqing ma rutgers university united states shiqing.ma rutgers.com chao shen xi an jiaotong university xi an china chaoshen mail.xjtu.edu.cnyufei chen xi an jiaotong university xi an china yfchen sei.xjtu.edu.cnqian wang wuhan university wuhan china qianwang whu.edu.cn abstract withdeepneuralnetwork dnn beingintegratedintoagrowing number of critical systems with far reaching impacts on society there are increasing concerns on their ethical performance suchas fairness.
unfortunately model fairness and accuracy in manycases are contradictory goals to optimize during model training.
to solve this issue there has been a number of works trying to improvemodel fairnessby formalizingan adversarialgame inthe model level.
this approach introduces an adversary that evaluates the fairness of a model besides its prediction accuracy on the main task and performs joint optimization to achieve a balanced result.
in this paper we noticed that when performing backward prop agationbasedtraining suchcontradictoryphenomenonarealso observableonindividualneuronlevel.basedonthisobservation wepropose fairneuron adnnmodelautomaticrepairingtool to mitigate fairness concerns and balance the accuracy fairness trade offwithoutintroducinganothermodel.itworksondetecting neurons with contradictory optimization directions from accuracy andfairnesstraininggoals andachievingatrade offbyselective dropout.comparingwithstate of the artmethods ourapproach is lightweight scaling to large models and more efficient.
our evaluation on three datasets shows that fairneuron can effectively improve all models fairness while maintaining a stable utility.
keywords fairness path analysis neural networks acm reference format xuanqi gao juan zhai shiqing ma chao shen yufei chen and qian wang.
.fairneuron improvingdeepneuralnetworkfairnesswith adversary games on selective neurons.
in 44th international conference on software engineering icse may pittsburgh pa usa.
acm new york ny usa pages.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation onthe firstpage.copyrights forcomponentsof thisworkowned byothersthan the author s mustbehonored.abstractingwithcreditispermitted.tocopyotherwise or republish topostonserversortoredistributetolists requirespriorspecificpermission and or a fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn ... .
introduction deepneuralnetworks dnns aregraduallyadoptedinawiderange of applications including image recognition self driving andnaturallanguageprocessing .oneofthemosttrendy applications is decision making systems which requires a high utility dnn with fairness.
as examples artificial intelligent ai judge or human resource hr try to judge who should get a loan or interview.
these systems should provide objective supposedly consistentdecision basedon the givendata although thereareoftensocietalbiasinthesedata .wewishthesesystemscouldcounteractunfairdecisionmadebyhumans butthey still exhibit unfair behavior which affects individuals belonging to specific social subgroups.
the compas system is an example.
it predictsrecidivismofpretrialoffenders andcontinuestomake decisions that favor caucasians compared to african americans.such bias has made very negative societal impacts.
therefore itis crucial to have systematical methods for automatically fixing fairness problems in a given dnn model.
intuitively fairness problems happen when a model tends to make different decision for different instances which only differentiated by some sensitive attributes such as age race and gender.
dependingonspecifictasks theprotectedorsensitiveattributes can vary.
similarly there are different fairness notations defined in existing dnn literature e.g.
group fairness individual fairness and max min fairness .
according to existing study thesefairness definitions arecorrelated with each other.inpractice weusuallyconsiderafewrepresentativeones i.e.
demographicparity demographicparityrateandequalopportunity.
in this paper we also consider these.
existing dnn training frameworks e.g.
tensorflow and pytorch have provided no support for fairness problems detectionandfixing.someotherworkstrytofixothermodelproblems .
there are existing fairness fixing frameworks suchasfad andethicaladversaries thattrytoprovide suchfunctionality.basedontheobservationsthatoptimizingaccuracy and fairness can be contradictory goals in training these frameworksintroduceanadversarythatmonitorsthefairnessof the current training.
when fairness issues are detected they solve itbyvariousmethods e.g.
dataaugmentation thatis leveraging theadversarymodeltogenerateadversaryexampleswhichhelpfix the unfair problem and using them as part of the new trainingdata.similartogenerativeadversarynetworks trainingsuch ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa xuanqi gao juan zhai shiqing ma chao shen yufei chen and qian wang anadversarycanbetime consumingandchallenging.ithasalot of practical problems such as mode collapse which is hard to solve.
moreover such methods usually require using a more complex model training protocol which is heavyweight.
we observe that the essential challenge of fixing model fairness isthatoptimizationonaccuracyonlycanleadtotheselectionofthe usageofsensitiveattributes.forexample anaihrthatusesthe sensitiveattributesgenderasanimportantfeaturewillbebiased.
moreover suchfeatureselectionhappenincertainneuronsorpaths whichisdifferentformtheonesusingallfeaturesordistinguishable features.andsuchpaths neuronstakeasmallportionofthewhole network otherwise the network will have low accuracy for all samples.basedonourobservations weproposed fairneuron a fairness fixing algorithm that detects and repairs potential dnn fairness problems.
it works by first identify conflict paths with a neural network slicing technique.
conflict paths refer to the paths thatcontainalotofneuronsthatselectsensitiveattributestomake predictions rather than distinguishable ones.
then we leverage such paths to cluster samples by measuring if they can trigger the selection of sensitive attributes.
lastly we retrain the model by selectiveretraining.thatis forsamplesthatcancausethemodelto select sensitive attributes as main features to make predictions we enforcethednntoreconsiderthisbymutingotherneuronsthat are not in the conflict paths.
by so the conflict path neurons havetoconsiderallfeatures otherwise itwillverylowaccuracy onothersamples.thishelpsremovetheimpactsofbiasedsamples and fix the fairness problem.
fairneuron hasbeenimplementedasaself containedtoolkit.
our experiments on three popular fairness datasets show that fairneuron improvestwicefairnessperformanceandtakesonefifthusageoftrainingtimeonaveragethanstate of the artsolution ethicaladversaries .notethat fairneuron onlyreliesonlightweightprocedureslikepathanalysisand dropout whichmakesit much more effective and scalable than existing methods.
in summary we make the following main contributions we propose a novel model fairness fixing frameworks.
itavoids training an adversary model and does not requiremodifying model training protocol or architecture.
it also features lightweight analysis and fixing leading to high efficiency repairing.
wedevelopaprototype fairneuron basedontheproposed idea and evaluate it with popular public datasets.
the evaluationresultsdemonstratethat fairneuron caneffectivelyandefficientlyimprovefairnessperformanceofmodels whilemaintainingastableutility.onaverage thefairness performancedpcanbeimprovedby57.
whichis20 higher than that of state of the art adversary training based method ethical adversaries.
ourimplementation configurationsandcollecteddatasets are available at .
roadmap .
this paper is organized as follows.
section presents thenecessarybackgroundonfairnessnotionsandfixingalgorithms.
insection wediscuss fairneuron indetail.section 4shows ourexperimentsetupandresults.wereviewrelatedworksin and conclude this paper in section .threattovalidity .fairneuron iscurrentlyevaluatedon3datasets which may be limited.
similarly there are configurable parameters used infairneuron and even though our experiments show that theyaregoodenoughtoachievehighfixingresults thismaynot holdwhenthesizeofmodelissignificantlylargerorsmaller.besides weassumethatmostsamplesactivatealimitednumberofpaths and most paths are activated by samples with certain features.thishasbeenobservedbyexistingworks .wealso empiricallyvalidatethisassumptionin .
.however itispossible that this assumption may not hold for some models.
to mitigate thesethreats alltheoriginalandrepairedtrainingscripts model architecture and training configuration details implementation including dependencies and evaluation data are publicly available at for reproduction.
background and motivation .
fairness depending on concrete task specifications fairness can have different notations .
these notions can be categorized into two groups individualfairness whichmeasuresif individuals inthedatasetistreatedequallybythelearnedmodel andgroupfairness which concerns about whether subpopulation with different sensitive attributes are treated equally.
for example foranonlineshoppingrecommendationsystem allcustomersin thedatasetshouldbetreatedequally whichasksforindividualfairness.foranaipoweredhiringsystem applicantswithsensitive attributes e.g.
different genders should be treated equally which is a typical case of group fairness.
beforediscussingdifferentfairnessnotations wefirstdefinea set of notations.
we denote the sensitive attribute as sand other observable insensitive attributes as a. we assume that the subpopulationwith s 1isthedisadvantagedgroup andtheprivileged group is the subpopulation with s .
also we represent the true label as y and the predicted output i.e.
positive negative as y which is a random variable depending on attributes sanda.
y and y are the positive and negative outcomes respectively.
followingsuchnotations wecandefinecommonlyuseddifferent fairness notations as follows demographicparity dp .demographicparity orstatisticalparity isoneoftheearliestdefinitionsoffairness .itviewsfairness as different subpopulations i.e.
s and s should have an equal probability of being classified to the positive label.
formally demographic parity measures the probability differences between different groups dp barex barexp y s p y s barex barex in an ideal case we say that a model is when dp which indicates that the prediction output yand sensitive attribute sare statisticallyindependent.ifso theoutputisnotaffectedbythesensitive attribute and hence the model is not biased towards certain valuesof thesensitive attributeshowing fairnessinprediction.
in practice dp 0ishardtogetandweviewamodelasfairwhen dp where is a threshold value that is determined by real world tasks and requirements.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fairneuron improving deep neural network fairness with adversary games on selective neurons icse may pittsburgh pa usa demographicparity ratio dpr .demographicparityratio or disparateimpact issimilartodemographicparity.thekeydifference is that it represents the equality or similarly of prediction on different groups as a ratio instead of a substitution .
formally it is defined as dpr p y s p y s likedemographicparity dpr 1indicatesafairmodelinthe ideal case and in practice we say a model is fair when dpr where isthefairnessthreshold.
moreover italsofocuseson the probability of different groups being classified to the positive label.
thekeydifferenceisthatdprmeasuresthedifferencesinaratio.
this is because its origins are in legal fairness considerations for selectionprocedureswhichtheparetoprinciple a.k.a.
the80 rule iscommonlyused .tomakeadirectcomparisonwith80 dpr calculated the ratio instead of substitution.
equalopportunity eo .alimitationofdpanddpristhatthey donotconsiderpotentialdifferencesincomparedsubgroups.equal opportunity eo overcomesthisbymakinguseofthefpr false positive rate and tpr true positive rate between subgroups .
formally eo is defined as eo barex barexp y s y p y s y barex barex a model achieves eo fairness when eo namely the prediction is conditional independent of the sensitive attribute s.i n practice we say an model is eo fair when eo and here is the fairness threshold value.
besidesthesediscussednotions therearemanyotherfairness definitions suchasfairnessthroughunawareness ftu disparate treatment disparatemistreatment counterfactual fairness ex antefairnessandex postfairness etc.friedler et al.
compareddifferent notations and measuredtheir correlationsonthericciandadultdatasets.resultsshowthatdifferent notationshavestrongcorrelationswitheachother.asaresult most work usually pick a few representative ones.
following existing related work we choose three most common notations i.e.
dp dpr and eo in our study.
.
improving dnn fairness many machine learning algorithms including dnns suffer from the bias problem.
namely the model can make a decision based on wrong attributes.
for example a biased hiring ai may make admissions based on applicants gender information.
such issues can be caused by the biased training data or the algorithm itself.
dnn hasshowntobeabiasedalgorithm andpotentiallytraineddnn models can make unfair predictions despite its high accuracy.
this canleadtosevereproblemsespeciallywhendnnsarebecoming more and more popular including applications like ai judge aibasedauthentication aihr etc.forexample compas a popular system that predicts the risk of recidivism claimed that black people re offend more the first beauty contest robot beauty.ai founddarkskinunattractive andthemicrosoft chatbot tay became a racist and sex crazed neo nazi .
biased ais in such systems can lead to severe ethical concerns potentially threatening our daily life and economy.
as a response to this issue existingworkhasproposedmethodstoimprovednnfairnessby removing such bias.
fad.adeletal.
proposedafairadversarialframeworkfad which leverages gradient reversal which acts as an identity function during forward propagation and multiplies its input by during back propagation to fix model fairness problems.
the authors introduced an adversarial network to encode fairness into themodel apredictornetwork fpandanadversarynetwork fa.
the goal of the predictor is to maximize accuracy in ywhile the adversarynetworktriestomaximizefairnessinprotectedattribute s. for fairness fixing we need a new model architecture which can i predict the true label y and ii not be able to predict the sensitive attribute s lfp lce lfa where lfp lce lfadenote the predictor loss predictor logistic loss a.k.a.
ce loss and the adversary logistic loss respectively.
the hyperparameter regulates the accuracy fairness trade off.
after that it uses a post training process to align tp true positive and fp false positive across all classes by adjusting class specific thresholdvaluesoflogitswitharocanalysisintroducedbyhardt et al.
.ethical adversaries .
delobelle et al.
proposed the ethical adversaries framework to solve the fairness problem.
the framework hastwoparties theexternaladversary a.k.a.
thefeeder andthe reader which represents the protected attribute s. it is an iterative trainingprocedures duringwhicheachpartyinteractswitheach other.
the readeris trained with thetarget label atthe same time and each time it evaluates if the training has bias or not.
if so it propagatestherelatedgradientbacktothenetwork.thefeedercan beviewedas adataaugmenterwhichperforms evasionattacksto find adversarial examples that can be used in the adversarial training.duringthisadversarialtraining thetargetlabel i.e.
maintask ofthemodel andthefairnessgoalisadjustedbyahyperparameter which is similar to the fad framework.
pre post processingmethods .fadandethicaladversariesare online methods which solves the fairness issue during training.
there are other methods that leverages pre processing or postprocessing to solve this problem e.g.
reweighing and reject optionclassification roc .
reweighing assignsdifferent weights to input samples in different group to make the dataset discrimination free pre processing .rocgivesfavorableoutcomes to unprivileged groups and unfavorable outcomes to privilegedgroups in a confidence band around the decision boundary with the highest uncertainty post processing .
.
motivation and basic idea limitations of existing work.
existing work has a few limitations.
firstly they introduce another model as the adversary in the trainingprocedure.inheritingfromexistingadversarialnetworks training such models is not easy.
problems like mode collapse failingtoconverge andvanishinggradientsarequitecommon in such a model structure.
this will require extra efforts in solv ing such problems.
secondly there is no guarantee that trainingsuch adversary networks willalways converge fornow.
there is atheoretical guaranteethatgan generativeadversarynetwork authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa xuanqi gao juan zhai shiqing ma chao shen yufei chen and qian wang willconverge despiteitspracticaldifficulty.asaminimaxgame trainingsuchganmodelswillconvergewhenitachievesthenash equilibrium .
however fad and ethical adversaries empiricallyobservethatmodelaccuracyandfairnessmayconflictwith each other in some cases and may not conflict with each other in other cases.
on one hand this shows that there exist models that are both accurate and fair.
on the other hand it also indicates thatthedesignedadversarytrainingisnotazero sumgame and there is no guarantee to show the existence of nash equilibrium in thisgame.
asa result existing workcan failto convergewhen trainingthemodelbecausethegamehasnosolutions.empiricalre sultsconfirmthisconclusion.
.2reportsthatfadmayexacerbate fairnessproblem.astable4shown fadresultsindecreasingof dproncensusandincreasingofeooncompas whichmeanthe fairnessproblemshasnotbeenmitigatedfromtheseperspectives.
elazar and goldberg made an empirical observation on leakage of protectedattributesforneuralnetworkstrainedontextdata which can also demonstrate this conclusion .
whybiashappensinadnntraining?
based on existing literatures and our experiences we make a few key observations that are important for us to develop our method.
observation i optimizing accuracy anddifferentfairnessobjectives canbecontradictory toeachother butnotalways.existingwork hasshownthataccuracyanddifferentfairnessgoals e.g.
dp dpr and eo including different fairness goals themselves can be contradictorytoeachother.thisisthereasonwhysomemodels with high accuracy are highly biased when optimizing during training directionswithhigheraccuracygainmaybecontradictory todirectionswithhigherfairness.thegoodnewsisthatexisting workhasempiricallydemonstratethatitispossibletotrainamodel with high accuracy and fairness at the same time .
observation ii aneuronrepresents acombination ofdifferentfeatures andmodelbiasindicates thatthemodelfocusesoncertainfeatures thatitshouldnot.asageneralunderstandingofdnns eachneuron inthenetworkisextractingfeaturesfromtheinput.formtheinputlayer to the final prediction layer the extracted features are becomingmoreandmoreabstract.eachneuronisrepresentingasetof featuresitreceivesfromthepreviouslayer andweightscanhelp it determine which set of features are more important compared withothers.amodelisbiasedindicatesthatamodelisfocusingon thewrongfeatures e.g aijudegesshouldbeaffectedbysenstive attributeslikegenders.forexample ahiringaiisbiasedongender whenitselectsgenderfeatureratherthanothersasanimportant factortodecideifacandidatecangetaninterview.noticethatsuch importance is represneted by weights in the dnn.
now we can use our observations to explain why bias happens in training a deep neural network.
when training a dnn the optimizertriestopickimportantfeaturesbasedongradientinformation.
when updating individual neurons it may encounter cases where the fairness and accuracy optimization subjects are pointing to differentdirections.ifitonlyconsidersaccuracyasitstraininggoal it will select the direction that optimizes the accuracy the most which can lead to low fairness.
furthermore we know that the gradient information is calculated based on given samples.
if we are able to detect such samples we can potentially fix the problem by enforcing the optimizer to pick the correct set of features.ouridea.
basedonourobservations wearguethatitisnotnecessarytointroduceanadversarythatdetectsthepotentialconflictsof optimizing the accuracy and fairness.
instead we first monitor the training process to detect neurons whose accuracy and fairness optimization get conflicts with each other.
then we identify samples that causes such contradictory optimizations.
lastly we enforce the optimizer to decide a balanced optimal direction that optimizes both accuracy and fairness.
by so we remove the need of introducinganadversary.itsimplifiesthetrainingprocedureand is more lightweight compared with existing solutions.
design .
overview workflow.
figure1presentstheworkflowof fairneuron .ittakes abiasedmodel anditstrainingdata asinputs andoutputsa fixed model.firstly fairneuron performs neuralnetworkslicing which detects neurons and paths that have contradictory optimization directions.noticethatbecauseofthedenseconnectionsofdnn suchneuronsaretypicallyconnectedwitheach passingthebiased features from one layer to the next.
as such we do this in the path granularity.
in this step we leverage a neuron slicing technique whichperformsadifferentialanalysistoidentifythetargetpaths.
next we leverage such paths to identify the samples that cause such effects known as the sample clustering.
after this step we canseparatethesamplesintotwoclusters biaseddatasamplesand benignsamples.lastly weperform selectiveretraining toenforce the model to learn unbiased features.
essentially for samples in differentclusters wehavedifferenttrainingstrategies.forsamples in the benign data cluster we do not change anything while for samples in the biased cluster we enforce detected neurons to consideralargersetoffeaturesandweighthemtolearn allfeatures that are important for prediction rather than the biased ones.
algorithm.
the overall algorithm of fairneuron is presented inalgorithm1 denotedasprocedure fairneuron .asmentioned before it takes a biased model biasedmodel and training dataset traindataset as inputs and outputs a fixed model referred to as newmodel in the algorithm.
in the main algorithm fairneuron analyze the relationship between dataset and model by gettingactivation paths of each input sample line .
after acquiring pathinformation fairneuron groupsthetrainingdatasetintotwo parts line6 .thefirstoneisconsistsofbenignsampleswhoseactivationpathsareclusteredbysamples andthesecondoneisconsists ofbiasedsamplesandcorrespondingpaths.then fairneuron performsdifferent trainingstrategies onthem itdeactivates dropout layers when training benign samples and activated them when training biased samples.
.
neural network slicing inneuralnetworkslicing wetrytofindpathsandneuronsthat containtheoptimizerfindscontradictoryoptimizationdirections foraccuracyandfairness.figure2showstheneuralnetworkslicingmethodof fairneuron .theinputofthisalgorithmisthetraining dataset and the biased model to fix a neural network which hasalready learned the weights based on a training dataset.
we will use this example to demonstrate how it works in this section.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fairneuron improving deep neural network fairness with adversary games on selective neurons icse may pittsburgh pa usa figure overview of fairneuron.
first fairneuron getstheaveragebehaviorofaneuron byleveraging its activation values.
the behavior of a neuron can be represented in many ways and in fairneuron we use the most simple andnaiverepresentation itsaveragedactivationvalue.specifically fairneuron calculatestheaverageactivationvaluesoftheneuron for a given dataset which is usually the training dataset.
then fairneuron performsa forwardanalysis tounderstand the diversity of a neuron behavior.
similar to the first step we also use the activation values of a neuron to represent the behavior oftheneuron.inthisstep wefeedindividualinputstothednn and record the activation value differences between the average activation and the value for this concrete input.
by so we canestimatethecontributionsofeachneurontotheoutputfora given sample.
this helps us to identify neurons that contain biased features.
afterwards weobtainpathsthatcontainbiasedfeatures.notice that a dnn is a highly connected network and as a layered structure behaviors of a single layer will be passed to the next layer.
becauseofthis biasedfeatureswillbeaccumulatedinthisnetwork and asa result neurons inthe last fewlayers willcontribute alot tothebiasedprediction.ontheotherhand theseneuronsdonot denote the root cause of such bias.
to completely fix the neuron network itisimportanttoidentifythewholechainofsuchpropagation.
so we comprehensively consider neurons and synapses andcalculatetheircontributions andbacktrackthesecontributions in the network.
we show the detail of this phase in the proce duregetactivationpath in algorithm .
starting from the output neuron weiterativelycomputethecontributionsoftheprevious neurons line which is similar to the backward propagation.
then wesortthemindescendingorder line23 andaddthekey synapses and corresponding neurons into the path set line .
todetermineifasynapseisakeysynapseornot weneedtocalculate whether the sum of all the synapses that are connected to the samesuccessorneuronisstilllessthanthethreshold.thethreshold isdeterminedbytheactivationvalueofsubsequentneuronsand the hyperparameter .
lastly we identify conflict paths namely paths that contain features causing the biased prediction.
based on our observations weknowthatwhenmakingpredictions themodelusesthebenignfeature set to make predictions for benign samples and use the biased feature set to make predictions for biased samples.
consideringthat a neuron represents a set of features we know that biasedsamples are activating neurons paths that are different from theothers.
notice that biased paths neurons takes a relatively small portion of the whole neural network.
otherwise the network will makepredictionsonalotofbiasedfeatures leadingtolowaccuracy.
based on this intuition we obtain such conflict paths by analyzing the frequencyof the activatedpaths.
more specifically weset the activationfrequencyofthemostfrequentlyactivatedpathasthe standard and compare the activation frequency of each path with it.
if the activation frequency of a certain path is less than a cer tain percentage of the standard determined by then it can be considered as a conflict path.
example.
assume the biased model is a simple neural network showninfigure2.theweightvalueshavebeenlabeledonthecor respondingsynapsesinfigure2 a .firstweperformpathprofiling and the results are set to for simplicity.
then we feed a sample intothemodel andcalculateitsrelativeactivationvalueon eachneuron.takethetopneuronofthesecondlayerasanexample its relative activation value is as shown infigure2 c .finallywebacktrackthecontributionsofsynapses to get the activation path.
figure d f shows how we get a path iteratively.letusdenotethe k thneuroninthe m thlayeras nm k. at first q n4 assuming .
we add n3 2intoq primeand do not addtheothersbecause .
.thenwelet q q prime n32 andadd n2 3since .
.lastweadd n11andn12intoq prime because .
and .
.ultimately we get all the paths iteratively.
the conflict paths detection can be regarded as the preceding step of sample clustering and the example in .
shows its process.
square .
sample clustering thesampleclusteringaimstomeasuretheimpactofinputsamples onfairness.afterdetectingconflictpaths wecandistinguishthese neurons exhibiting biased behavior.
we handle the corresponding authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa xuanqi gao juan zhai shiqing ma chao shen yufei chen and qian wang !
figure abnormal path detection example.
samples of these neurons denoted as biased sample to improve their fairnessperformance.
since werecorded the relationshipbetweenpathsandsamples line4inalgorithm1 wecaneasilyfind thesecorrespondingsamplesandgetthetrainingdatasetdivided into two groups.
as shown in algorithm pathlist is a list which contains each path and its corresponding activation samples.
first we count the totalnumberofpath scorrespondingactivationsamplesonebyone andthenumberisdenotedas activationfrequency line34 .second wesorttheactivationfrequencylistwegetabove andrecordits maximum as m line .
third we check whether these paths activation frequencies are greater than the threshold m.w e denote the paths which donot meet the above condition as biased path and denote their corresponding samples as biased sample.
after we detecting the biased paths these biased samples can be separated from ordinary samples line .
example.
suppose our training dataset has samples as shown in figure .
we feed the training dataset into the biased model and obtain different paths a b c and d based on the procedure getactivationpath in algorithm .
then we count the number of samplesactivatingthese4paths andget25 and2fora b c and d respectively.
weassume that .
so the threshold is .
.
since the maximum of path activation statistics is .
then pathcanddwillbothbeclassifiedasbiasedpaths which resultsin3samplesactivatingpathcand2samplesactivatingpath d being grouped in biased samples.
square figure sample clustering example.
.
selective training finally we perform ordinary training on the ordinary samples and dropouttrainingonbiasedsamplesobtainedabove.wedonotneed tochangethe modelstructure onlyneedto changetheactivation stateofthedropoutlayers.ordinarytrainingmeansdeactivating thedropoutlayersfortraining.withthecurrenttrainingsystem we can activate dropout layers when training on these biased samples and vice versa.
by performing dropout training on these biased neurons weenforcethemtolearnmoreunbiasedfeaturesrather than biased ones to mitigate the fairness problems.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fairneuron improving deep neural network fairness with adversary games on selective neurons icse may pittsburgh pa usa algorithm fairneuron algorithm input biasedmodel a biased model to fix input traindataset training dataset output newmodel trained model after fixing procedure fairneuron pathlist forsample traindataset do p getactivationpath biasedmodel sample p. sa mple sample append pathlist p o s getsamplesdivided pathlist newmodel biasedmodel foro odo ordinarytraining newmodel o fors sdo dropouttraining newmodel s return newmodel input model model to analyze input sample samples used in analyze input hyperparameter to determine the activation of neurons output p path activated procedure getactivationpath p q q outputneuron while q do q prime forq qdo n getpreneuron q forn ndo contriblist computecontrib n sortedlist sort contriblist sum fori 0to len contriblist do ifsum q.valuethen sum sum sortedlist .value q prime q prime sortedlist .index p p sortedlist .index q.index q q prime return p input pathlist a list of paths used to cluster samples input hyperparameters used to find conflict paths output ol al benign and biased samples respectively.
procedure getsamplesdivided ol al pathlist.count count pathlist.samples m max pathlist.count fori 0to len pathlist do ifpathlist .count mthen append ol pathlist .samples else append al pathlist .samples return ordinarylist abnormallisttable experimented dnn models.
dataset model accuracy census hidden layer fully connected nn .
credit hidden layer fully connected nn .
compas hidden layer fully connected nn .
evaluation .
experiment setup .
.
hardware andsoftware.
we conducted ourexperiments ona gpu server with cores intel xeon .10ghz cpu gb system memoryand1nvidiatitanvgpurunningtheubuntu16.
operating system.
.
.
datasets.
weevaluatedourmethodonthreepopulardatasets the uci adult census compas and german credit.
uci adult census.
the uci adult census was extracted from the census bureau database gathering instancesrepresentedby9featuressuchasage education and occupation.
the gender is considered as the sensitive attribute.
compas.
the compas system is a popular commercial algorithmusedbyjudgesforpredictingtheriskofrecidivism and the compas dataset is a sample outcome from the compassystem.theraceofeachdefendantisthesensitive attribute.
german credit.
this is a small dataset with records and20attributes.theoriginalaimofthedatasetistogive anassessmentofindividual screditbasedonpersonaland financial records.
the gender is the sensitive attribute.
.
.
models.
inourexperiment webuiltafully connectedneural network with three hidden layers for each dataset respectively.
for thecompasandthegerman creditdataset eachhiddenlayeris composedof32neurons whilefortheuciadultcensusdataset each hidden layer is composed of neurons due to its larger encodedinput.thedetailsofthemodelsusedintheexperiments are shown in table .
we use the softmax activation function for census and german credit to achieve binary classification and the linear activation functionforcompastogetrecidivismscores.werandomlyseparate the dataset into the training validation and test sets by a ratio of respectively.
the neural network is trained by the adam optimizer with 1 .
2 .
and initial learning ratelr .
which is scheduled by a factor of .
when reaching a plateau.
.
.
hyperparametertuning.
toobtainthesuitablehyperparameters and we run a parallel grid search for hyperparameters to optimize training loss function.
we sample between the interval proportionally and sample between the interval .followingthestandardpracticeinmachinelearning the gridsearchisperformedonasmallsubsetdrawnfromthetraining set in a certain proportion e.g.
and we utilize the ray tune toolto perform it automatically .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa xuanqi gao juan zhai shiqing ma chao shen yufei chen and qian wang .
.
metrics and baseline methods.
we compare our algorithm with other popular in processing state of the art fixing algorithms such as fad and ethical adversaries .
besides we also comparedwiththerepresentativealgorithmsoftheothertwokinds of fixing algorithms reweighing in pre processing and reject option classification in post processing .
we aim to answer the following research questions through our experiments rq1 how effective is our algorithm in fixing bias model?rq2 how efficient is our algorithm in fixing bias model?rq3 how parameters affect model performance?rq4 how our algorithm perform in image datasets?
.
effectiveness of fairneuron experimentdesign toevaluatetheeffectivenessof fairneuron wetestthefollowingmodels thenaivebaselinemodel modelsfixed by fad by ethical adversaries and by fairneuron .
due to the randomness in these experiments we ran the training times to ensurethereliabilityofresultsandenforcedthesefixingalgorithms tosharethesameoriginaltrainingdataset.tomeasuretheeffec tiveness of fairneuron we compare the performance between fairneuron and the other algorithms in terms of both utility and fairness.
to demonstrate the effectiveness of the three components offairneuron i.e.
neural network slicing sample clustering and selectivetraining weconductedadetailedcomparisonbetween our algorithm and other popular works.results the details of the comparison results are presented in table4.thefirstcolumnliststhethreedatasets.thesecondcolumn shows the different algorithm.
the third column lists the utility and the remaining columns list the fairness criteria.
the model utilitiesareevaluatedbybinaryclassificationaccuracy acc and thefairnessperformancearemeasuredbydemographicparity dp demographicparityratio dpr andequalopportunity eo .the best results are shown in bold.
analysis the experimental results demonstrate the effectiveness of our algorithm.
firstly fairneuron can effectively fix the fairness bias of all models trained on different datasets.
secondly fairneuron achieves the highest utility among all models with fairness constraints and even surpasses the naive model on compas and credit.
table4showsthefairnessimprovementofnaivemodelsoncensus creditandcompas respectively.
fairneuron improvesdpr by .
.
and .
mitigates fairness problem by .
.
and38.
intermsofeo and74.
.
.
in terms of dp.
compared with the other algorithms fairneuron achieves the best fairness performance on census and compas.however the eo and dp results of fairneuron on credit is not satisfactory.afterourcarefulanalysis wefoundthatourneuron network slicing is not fully functional since credit only has instances.thus howtoimprovetheutilityofmodelstrainingon such small datasets will be one of our future works.
besides table4demonstratesthat fairneuron haslittleimpact on model utility after a successful fairness fixing and even has theadvantageofincreasingaccuracybyfixingfairnessproblems.
theaverageutilityof fairneuron isthehighestamongallmodels withfairnessconstraints whichexceedsrocby27.
reweighingtable random clustering vs. our clustering method acc dp dpr eo random .
.
.
.159ours .
.
.
.
by .
ethical adversaries by .
and fad by .
and evensurpassesthenaivemodelonthegermancreditandcompasdatasets.thedetailedaverageaccuracychangeis .
.
and .
.
we found that it is mainly because fairneuron improves the utility by mitigating the overfitting problem in model training procedures and the size of census dataset is relatively large so its overfitting problem is unobvious.
in summary fairneuron can effectively fix the bias training procedures and has little impact on the model utility while improving the fairness performance significantly.
then we prove the effectiveness of each step in fairneuron separately.
.
.
effectiveness of neuron network slicing.
figure shows a example of the distribution of abnormal paths.
here the maximum of path activation statistics is and we assume .
so the threshold is .
as the green lineshows.
we can seethat most of non zero paths are concentrated near but the proportion of their correspondingsamplesisnothigh.thesepathsaretheabnormal paths detected by our approach.
figure result of neuron network slicing.
the blue step line presents the probability density function of path activation statis tics theredlinepresentstheaccumulationofsampleratio andthegreen line presents the threshold.
.
.
effectivenessof sampleclustering.
todemonstrate theeffectiveness of sample clustering we compare the fixing performance betweenoursampleseparationandtherandomclusteringmethods.
we set the number of randomly obtained abnormal samples to be the same as that of fairneuron.
table reports the performance of different clustering methods.
with our method the average accuracy is improved by .
and the fairness performance has also been greatly improved which are .
.
and .
of dp dpr and eo respectively.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fairneuron improving deep neural network fairness with adversary games on selective neurons icse may pittsburgh pa usa table comparison among dropout ordinary and selective training.
training approach acc dp dpr eo ordinary .
.
.
.
dropout .
.
.
.
selective .
.
.
.
table results on the three datasets.
best results are in bold.
dataset model acc dp eo dpr censusnaive model .
.
.
.
roc .
.
.
.
reweighing .
.
.
.
fad .
.
.
.
ethical adversaries .
.
.
.
fairneuron .
.
.
.
creditnaive model .
.
.
.
roc .
.
.
.
reweighing .
.
.
.
fad .
.
.
inf ethical adversaries .
.
.
.
fairneuron .
.
.
.
compasnaive model .
.
.
.
roc .
.
.
.
reweighing .
.
.
.
fad .
.
.
.
ethical adversaries .
.
.
.
fairneuron .
.
.
.
.
.
effectiveness of selective training.
to show the effectiveness ofselectivetraining weprovideacomparisonamongpuredropout pure ordinary and selective training.
table presents the results of different training approaches.
selectivetrainingsurpassestheordinarytrainingby38.
.
.
and .
while surpassing the pure dropout trainingby .
.
.
and .
in acc dp dpr and eo respectively.
it confirms that the selective training in fairneuron can achieve high accuracy and fairness.
.
efficiency of fairneuron experiment design to evaluate the efficiency of fairneuron we measured the time usage of ordinary training ethical adversariesand fairneuron trainingonallthreedatasets.weperformed 10trialswhichusesrandomtraining testdatasplitting naivemodel training hyperparametertuningandmodelrepairing forethical adversariesand fairneuron andcomputedtheaverageoverhead toavoidrandomness.table5presentshowmuchtimeittakesto complete its fixing for each method.
for ethical adversaries it shows the time for per iteration in iterations.
for fairneuron it shows the time usage per trial.
we also recorded the time usage of each step in fairneuron .
results and analysis are presented below.table time to train a model.
dataset naive ea iteration fairneuron trial census .74s .96s .41s credit .07s .24s .49s compas .92s .93s .31s table time used in each step.
dataset para selection slicing clustering training census .41s .37s .70s .37scredit .98s .20s .73e 4s .30s compas .76s .09s .06s .40s results table shows the comparison of time usage among ordinarytraining ethicaladversariesand fairneuron training.the firstcolumnliststhethreedatasetsandtheremainingcolumnsshowthedifferenttrainingmethods.onaverage fairneuron takes5.
times of ordinary training and .
of ethical adversaries.
table reports the time costs of each step.
the first column also liststhethreedatasetsandtheremainingcolumnsshowthetime costsofhyperparametersselection neuronnetworkslicing sample clustering and selective training respectively.
analysis forordinarytraining theruntimeoverheadallcomes fromthetrainingprocedure butfor fairneuron thehyperparameterstuningaccountsforalargerratioofthetotaltimeusage as shownintable6.so fairneuron takesonlylessthantwiceofthe timeusageofordinarytrainingonlargedatasetslikecensus but on small datasets like the german credit dataset it takes relatively a long time.
if fairneuron tries more times the average time will be reduced because the hyperparameters tuning is only conducted once.
overall fairneuron is more efficient than ethical adversaries infixingmodels withanaveragespeedupof180 .
.
effects of configurable hyperparameters fairneuron leveragestwoconfigurablehyperparameters and tofixfairnessproblems.thehyperparameters representsthe threshold of neuron activation and its value affects the complexity of the path.
as its value decreases more neurons and synapses areincludedinthe path resultinginamorecomplex path.and represents the threshold of neuron network slicing.
the lower is the the fewer paths are classified as abnormal.
we conduct a comparison experiment of these hyperparameters.
varies between the interval and varies between the interval .
note that we use logarithmic coordinates for since its value is sampled proportionally.
figure shows how hyperparameters assignments will affect the performance.
based on our results in comparison with thenaive model and ethical adversaries we can conclude that our algorithmdoesperformbetteronthistaskandisnotsensitiveto hyperparameters assignments except for eo figure c g .
by increasingtheweightofeoinhyperparametertuninglossfunction we can constrain its fluctuations.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa xuanqi gao juan zhai shiqing ma chao shen yufei chen and qian wang a accuracy b dp c eo d dpr e accuracy f dp g eo h dpr figure effect of hyperparameters and .
is sampled proportionally so we take the logarithm of as x axis.
.
performance on image datasets experimentdesign wealsoexploredthepossibilityofusingour method in fixing models on image datasets which has not done by baseline methods due to their inefficiency.
in our experiment we leverage a layer fully connected nn trained on mnist andresnet trainedoncifar compare fairneuron with the naive model and random dropout.
we use class wise variance cv and maximum class wise discrepancy mcd as fairness metrics.results table summarizes our results.
the first column lists the datasets.
the second column shows the different model and the remainingcolumnslisttheperformance.thebestresultsareshown in bold.
as we can see from the table fairneuron can effectively improvethemodelfairnessby20 formcdand80 forcv.we discuss it further in .table results on image datasets.
best results are in bold.
dataset model acc cv mcd mnistnaive model .
.66e .057random dropout .
.58e .052fairneuron .
.54e .
cifar 10naive model .
.04e .
random dropout .
.55e .464fairneuron .
.16e .
related work neural network slicing .
pathanalysis ordataflow analysis is a fundamental technique in traditional software engineering tasksliketesting debuggingandoptimization.itoffersawindow to study program s dynamic behavior.
in recent years with the development of ai security especially adversarial attack and defense authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fairneuron improving deep neural network fairness with adversary games on selective neurons icse may pittsburgh pa usa conflict path detection has been used for interpretability.
wang et al.
proposed a method to interpret neural networks by extracting the critical data routing paths cdrps and they demonstrated its effectiveness on adversarial sample detection problem.
qiu et al.
treataneuralnetworkasadataflowgraph whichcanbe appliedtheprofilingtechniquetoextractitsexecutionpath.zhang et al.
apply the dynamic slicing on deep neural networks.fairness of ml .
with the increasing use of automated decisionmaking approaches and systems fairness considerations in ml havegainedsignificantattention.researchersfoundmanyfairness problems with high social impact such as standardized tests inhigher education employment and re offence judgement .besides governments e.g.theeu and the us organizations and the media have called for more societal accountability and social understanding of ml.
to address the concern above numerous fairness notions are proposed.inhighlevel thesefairnessnotionscanbesplitintothree categories i individual fairness whichrequires that similarindividualsshouldbetreatedsimilarly ii groupfairness whichconcernsaboutwhethersubpopulationwithdifferentsensitive characteristics are treated equally iii max minfairness which try to improve the per group fairness .
fairnesstestingisalsoanimportantresearchdirection andits approachesmostlybasedongenerationtechniques.themis considersgroupfairnessusingcausalanalysisandusesrandomtest generation to evaluate fairness.
aequitas inherits and improves themis and focuses on the individual discriminatory instances generation .later adfcombinesglobalsearchandlocalsearch tosystematicallysearchtheinputspacewiththeguidanceofgradient .symbolicgeneration sg integratessymbolicexecution and local model explanation techniques to craft individual discriminatory instances .
the ml model needs to be repaired after the fairness problem is found.theseapproachescanbegenerallysplitintothreecategories i pre processingapproaches whichfixthetrainingdatatoreduce the latent discrimination in dataset.
for example the bias could be mitigatedbycorrectinglabels revisingattributes generatingnon discriminationdata andobtainingfairdata representations .
ii in processingapproaches whichrevisethe training of the bias model to achieve fairness .
more specifically these approaches apply fairness constraints propose an objective function considering the fairness of prediction or design a new training frameworks .
iii post processing approaches which directly change the predictive labels of bias models output to obtain fairness .
conclusion inthispaper weproposedalightweightalgorithm fairneuron to effectivelyfixingfairnessproblemsfordeepneuralnetworkthrough pathanalysis.ouralgorithmcombinesapathanalysisprocedure andadropoutproceduretosystematicallyimprovemodelperformance.fairneuron searches bias instanceswith the guidance of path analysis and mitigates fairness problems by dropout training.
ourevaluationresultsshowthat fairneuron hassignificantlybetterperformancebothintermsofeffectivelyandefficientlyinfixing bias models.
for cnn model we can only perform fairneuronon the last full connected layer so its performance is not ideal.
we will improve fairneuron on cnn in the future.
acknowledgement we thank the anonymous reviewers for their constructive comments.thisresearchwaspartiallysupportedbynationalkeyr d program 2020yfb1406900 nationalnaturalsciencefoundation ofchina u21b2018 u20b2049 u1736205 andshaanxiprovincekeyindustryinnovation program 2021zdlgy01 .
chao shen is the corresponding author.
the views opinions and or findings expressed are only those of the authors.