singapor e management univ ersity singapor e management univ ersity institutional k nowledge at singapor e management univ ersity institutional k nowledge at singapor e management univ ersity resear ch collection school of computing and information systems school of computing and information systems how t o formulate specific how t o questions in softwar e how t o formulate specific how t o questions in softwar e development?
development?
mingwei liu xin peng andrian m arcus christ oph treude singapor e management univ ersity ctreude smu.edu.sg jiazhan xie see next page for additional authors follow this and additional works at https ink.libr ary.smu.edu.sg sis r esear ch part of the softwar e engineering commons citation citation liu mingwei peng xin m arcus andrian treude christ oph xie jiazhan xu huanjun and y ang yanjun.
how t o formulate specific how t o questions in softwar e de velopment?.
.
esec fse proceedings of the 30th a cm joint e uropean softwar e engineering conf erence and symposium on the foundations of softwar e engineering singapor e singapor e no vember .
.
available at available at https ink.libr ary.smu.edu.sg sis r esear ch this conf erence pr oceeding ar ticle is br ought t o you for fr ee and open access b y the school of computing and information systems at institutional k nowledge at singapor e management univ ersity .
it has been accepted for inclusion in resear ch collection school of computing and information systems b y an authoriz ed administr ator of institutional k nowledge at singapor e management univ ersity .
for mor e information please email cher ylds smu.edu.sg .
author author mingwei liu xin peng andrian m arcus christ oph treude jiazhan xie huanjun xu and y anjun y ang this conf erence pr oceeding ar ticle is a vailable at institutional k nowledge at singapor e management univ ersity https ink.libr ary.smu.edu.sg sis r esear ch how to formulate specific how to questions in software development?
mingwei liu fudan university chinaxin peng fudan university chinaandrian marcus the university of texas at dallas usa christoph treude the university of melbourne australiajiazhan xie fudan university chinahuanjun xu fudan university china yanjun yang fudan university china abstract developers often ask how to questions using search engines technical q a communities and interactive q a systems to seek help for specific programming tasks.
however they often do not formulate the questions in a specific way making it hard for the systems to return the best answers.
we propose an approach taskkg4q that interactively helps developers formulate a programming related how to question.
taskkg4q is using a programming task knowledge graph task kg in short mined from stack overflow questions which provides a hierarchical conceptual structure for tasks in terms of and .
an empirical evaluation of the intrinsic quality of the task kg revealed that .
of the annotated questions in the task kg are correct.
the comparison between taskkg4q and two baselines revealed that taskkg4q can help developers formulate more specific how to questions.
more so an empirical study with novice programmers revealed that they write more effective questions for finding answers to their programming tasks on stack overflow.
ccs concepts software and its engineering software maintenance tools documentation .
keywords programming task query formulation stack overflow acm reference format mingwei liu xin peng andrian marcus christoph treude jiazhan xie huanjun xu and yanjun yang.
.
how to formulate specific how to questions in software development?.
in proceedings of the 30th acm joint m. liu x. peng j. xie h. xu and y. yang are with the school of computer science and shanghai key laboratory of data science fudan university china.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november singapore singapore association for computing machinery.
acm isbn .
.
.
.
software engineering conference and symposium on the foundations of software engineering esec fse november singapore singapore.
acm new york ny usa pages.
.
introduction how to questions are questions that ask for instructions for example how to read a json file in java .
developers often ask how to questions using search engines technical q a sites and interactive q a systems seeking help for their programming tasks.
for example treude et al.
found that .
of stack overflow so questions are how to questions while other researchers have analyzed and categorized so questions with how to questions consistently emerging as one of the most important categories shared by all taxonomies.
previous studies found that it is not easy for developers to formulate a good question .
rahman et al.
conducted an empirical study on code search using google and found that developers often miss important technical details e.g.
programming languages or operating systems in their initial queries.
a programming task usually consists of essential elements such as and .
for example in the task read a json file in java read is the json file is the and in java is the .
for a how to question the key to whether it can be understood and answered correctly by a human or automated approaches is whether the question is specific enough i.e.
the essential task elements in the question are completely and precisely expressed.
for example if the constraint in java is missing one may get answers in python likewise if the object is file or json instead of json file one may get answers for reading an xml file or a json array.
at the same time a programming task can be expressed in many different ways e.g.
by using different sentence patterns.
hence developers may struggle to choose the best formulation and need to refine and revise their questions before getting the right answers.
there are a total of edit records for so question titles and about of the questions with the keyword how to in the title have title edits as of underlining this phenomenon.
more so in a sample of how to questions with title edits we found that task elements i.e.
and were involved in of 306esec fse november singapore singapore mingwei liu xin peng andrian marcus christoph treude jiazhan xie huanjun xu and yanjun yang the edits see sec.
which indicates that developers may struggle to describe the task elements well enough the first time.
software engineering researchers proposed several techniques for helping developers improve their questions or queries for different tasks e.g.
bug localization and code search.
some approaches focus on automatically reformulating questions by expanding them with relevant terms while others help users improve their questions interactively .
for example zhang et al.
designed chatbot4qr to help developers refine their so queries by asking clarification questions based on tags from existing posts.
however it only helps adding details to the initial query written by the user and it cannot replace poorly chosen terms in the initial query.
one common aspect to these past research efforts is that they do not address how to questions explicitly.
we argue that due to their prevalence helping the formulation of how to questions deserves specific solutions rather than one size fits all approaches and this is the main goal of this paper.
our premise is that new how to questions asked by developers are similar to existing task related questions hence the structure and content of these past questions can help formulate new ones.
based on this idea we propose an interactive approach and tool taskkg4q that can help developers formulate specific how to questions.
taskkg4q uses a programming task knowledge graph task kg in short constructed from so question titles.
the task kg provides a hierarchical conceptual structure for tasks in terms of and .
based on the task kg taskkg4q recommends appropriate and specific and to developers when they formulate how to questions and also suggests details that may be missing.
our task kg construction approach sec.
relies on a set of three linguistic patterns we manually identified in high quality so questions.
from here on the approach is automated and it extracts a set of seed tasks from so questions matching these patterns incrementally and iteratively discovers additional tasks using task extension mechanisms and annotates questions with the extracted task elements.
the resulting task kg includes tasks with new concepts and corresponding questions with new linguistic patterns beyond those provided initially.
also the task kg can evolve automatically as more data becomes available.
we constructed a task kg using so questions and found that our approach can accurately .
accuracy extract tasks from questions that refer to those tasks sec.
.
.
to evaluate taskkg4q s usefulness we asked novices to complete four programming tasks and write questions with without the help of taskkg4q during this process sec.
.
.
they were able to write more effective how to questions for stack overflow using the tool and deemed the tool easy to use.
in summary the contributions of this paper are a conceptual model for describing tasks task related elements and relations between them.
an incremental and iterative approach that mines a task kg from so questions by combining top down and bottom up task extension strategies.
a task kg with tasks mined from so questions.
a first of its kind interactive tool that helps developers formulate specific how to programming questions.
instanceoftask action object constraintquestionsynonyms antonyms isa hasaction hasconstraint hasobjectisa conceptreferto refertofigure main concepts schema while our evaluation focused on writing how to question on stack overflow and the task kg is built based on stack overflow taskkg4q can be used independent of stack overflow with any other programming related q a system or forum.
conceptual model fig.
shows the main concepts used in this paper and the relations between them.
a is a specific programming task that a developer wants to complete and is described by multiple elements an an and multiple .
an is a verb or verb phrase to describe the main action of the task.
an is a noun phrase representing the direct object of the action.
a is a prepositional phrase representing a constraint for completing the task.
for example the read a json file in java includes an i.e.
read an i.e.
json file and a i.e.
in java .
we record this as the tuple task read json file in java in the order of and .
as for relations an and a may refer to the same concept e.g.
json file and from json file .
there are hierarchical relations i.e.
isa between concepts e.g.
java isa programming language .
this type of conceptual relation may come from existing knowledge bases e.g.
general knowledge graph wikidata .
at the same time there could be synonym e.g.
remove and delete or antonym e.g.
read and write relations between .
furthermore there are hierarchical conceptual relations i.e.
isa between as well e.g.
task read json file in java isa task read json file .
a describing a with its elements i.e.
and is an instance ofthe e.g.
the question how to read a json file in java is aninstance of task read json file in java .
we can annotate all elements in the and obtain theannotated i.e.
how to read ajson file in java .
read json file andin java are the annotated elements in the with a role in the .
we use colors instead of tags to improve readability.
words in red orange and blue represent annotated annotated and annotated respectively.
an annotated question may imply a linguistic pattern about describing a task and multiple questions may follow similar linguistic patterns e.g.
how to read ajson file in java and how toread apdf file in java only differ on the annotated .
motivational study to understand issues with task related questions we sample a set of such questions from so and investigate how they were changed.
all the data used in this study is provided in the replication package .
data.
so provides a data dump with question title edit records for questions .
one question could have multiple edit records.
among questions .
questions have 307how to formulate specific how to questions in software development?
esec fse november singapore singapore questionsinitial seed task extractionconcept mutation based task extraction linguistic mutation based task extraction task kginitial seed tasks seed tasks seed questionstasks and questions questions questions task kg construction tasks concepts questions and relations figure task kg mining of taskkg4q at least one edit record.
of the questions with the keyword how to in the title representing typical how to questions .
of have at least one edit record.
note that not all how to questions have the how to keyword in the title.
we randomly selected questions with title edit records from the dump and manually removed questions not related to how to tasks.
for example the question react native or react js memory leak is about debugging hence it is not a task related question.
the sampling and removal were conducted several times questions per sample until we obtained task related questions.
the sample size of exceeds the sample size required to guarantee confidence interval of at a confidence level i.e.
.
the manual removal was conducted by two ms students independently with more than four years of java experience with a cohen s kappa agreement of .
i.e.
almost perfect agreement.
a third student was assigned to resolve the conflicts.
protocol.
we compared each title edit record with the previous version of the question title i.e.
initial title or the last title edit record and classified the edit record into four categories edit e.g.
change load a file to read a file edit e.g.
change load a file to load a large file edit e.g.
change load a file in java to load a file in java others e.g.
adding how to to the question beginning without changing the task elements .
one edit record could be classified into multiple categories as it may have multiple edits.
this classification was done by two students same students as above independently.
the cohen s kappa agreement is .
i.e.
almost perfect agreement .
results and analysis.
we identified edit records in the edit category in edit in edit and in the others category.
.
edit records are about modifying task elements.
there are edit records modifying only one element edit records modifying two elements and edit records modifying three elements.
on average one question has .
edit records min.
med.
max.
and .
edit records modifying elements min.
med.
max.
.
overall elements from .
of the questions have been edited.
summary.
developers make frequent changes to so task related questions suggesting that they are often incomplete or unclear.
task kg mining an overview of the task kg mining is presented in fig.
.
it starts with seed task extraction which identifies a small set of highquality tasks as seed tasks based on manually defined linguistic patterns sec.
.
.
based on the set of initial seed tasks the approach employs an iterative process to extract additional tasks and annotated questions in two steps concept mutation based taskextraction sec.
.
and linguistic mutation based task extraction sec.
.
.
the collection of produced tasks and annotated questions with the relations between them constitute the task kg sec.
.
.
all these steps are automated except the definition of the linguistic patterns for the extraction of the initial seed tasks.
concept mutation based and linguistic mutation based task extraction promote and complement each other.
concept mutationbased task extraction is to extract tasks with similar task elements and linguistic mutation based task extraction is to extract tasks described with a similar linguistic pattern.
during iteration concept mutation based task extraction can discover annotated questions with new linguistic patterns and linguistic mutation based task extraction can discover tasks with new concepts.
note that we do not aim to create new questions that do not exist in so using mutation but use so questions to validate mutated tasks and questions.
only valid tasks and questions are added to the task kg.
we compute a confidence score for each generated task and corresponding annotated questions.
tasks with high confidence scores are selected as seeds for the next concept mutation based task extraction while annotated questions with high confidence scores are selected as seeds for the linguistic mutation based task extraction.
when no new tasks can be produced the approach ends with the resulting task kg as the output.
.
running example fig.
shows part of the task kg describing tasks related to reading files.
the white ellipses denote tasks and the gray ellipses denote questions instance of tasks.
the solid lines between the ellipses denote isa relations between tasks and the dashed lines denote instanceof relations between questions and tasks.
for brevity we omit in the figure some entities e.g.
or and some relations e.g.
hasaction hasobject .
we construct the task kg to represent similar tasks to form a hierarchical knowledge structure e.g.
read a pdf file with different languages or on different platforms or read different files.
to mine such a task kg we obtain a list of questions from soas the corpus to run taskkg4q on e.g.
questions with popular tags such as java android javascript and python .
before the iterative task and question extraction step we select high quality tasks from the corpus as initial seeds based on manually defined linguistic patterns see sec.
.
.
for example we extract task read pdf file in java from the question how to read a pdf file in java and task read xml file in java from the question how to read xml file in java as initial seeds.
in each round of the iteration we first perform concept mutation based task extraction and then linguistic mutation based task extraction.
next we illustrate how we perform a round of iteration starting from the seeds task read pdf file in java and task read xml file in java as shown in fig.
.
forconcept mutation based task extraction first we obtain mutation tasks from the seed task based on domain knowledge see sec.
.
.
.
those mutation tasks are related to the seed tasks at concept level and may be task candidates e.g.
task read pdf file in python task read xml in java .
then we use those task candidates including seed tasks and mutation tasks to match with questions in the corpus to find the tasks instance questions 308esec fse november singapore singapore mingwei liu xin peng andrian marcus christoph treude jiazhan xie huanjun xu and yanjun yang read pdf file in java read pdf file in python read pdf file with phpread pdf file in android read pdf file in iphoneread pdf file read binary fileread fileread json file read xml read xml in javaread json file with dojo read structured binary file with javahow to read json file with dojo read xml file using javascripthow to read xml in java which php function to use to read a binary file into a stringhow to read a pdf file in java python to read pdf files read pdf files with phpandroid read pdf files?
how to read pdf file in iphonebest way to read xml in java read xml file read xml with xpath in java how to read xml using xpath in javaread xml file in javascript best way to read structured binary files with javaread structured binary file read binary file into string in php functionread csv file from urlread csv file how to read a csv file from a url figure a part of the task kg related to reading files.
white nodes are and gray nodes are .
solid lines are isarelations and dashed lines are instanceof relations.
t1 task read pdf file in java t2 task read xml file in java iteration q1 python to read pdf files q2 best way to read xmlin java q3 how to read xmlin java... t10 task read structured binary file with java iteration t1 task read pdf file in java t2 task read xml file in java ... t6 task read pdf file in python t7 task read xml in java ... seed question selectiontask kgtask kg construction task kg constructionconcept based task mutationtask based matching linguistic pattern based matchingq1 python to read pdf files q2 best way to read xmlin java...lp1 npto read np lp2 best way to read npadp java...linguistic pattern generationq6 linqto read xml q7 best way to read structured binary files with java...seed task selection figure an example of one round iteration for task kg mining see sec.
.
.
.
for example we can identify the question python to read pdf files as an instance of task read pdf file in python and best way to read xml in java as an instance of task read xml in java .
we further annotate the elements of tasks in the matching questions to get the annotated questions i.e.
python to read pdf files and best way to read xml in java .
at the end of the concept mutation based task extraction tasks with corresponding annotated questions are added to the task kg and conceptual relations are also added during this process see sec.
.
e.g.
task read pdf file in python isa task read pdf file .
we further compute confidence scores for tasks and annotated questions in the task kg and select the top k unselected questions with the highest scores as the input for linguistic mutation based task extraction see sec.
.
.
.
as a result we select python toread pdf files and best way to read xml in java as the seeds which describe tasks in linguistic patterns different from patterns defined for seed task extraction.
forlinguistic mutation based extraction we generate linguistic patterns from selected annotated questions by mutating the annotated elements see sec.
.
.
e.g.
linguistic patterns lp1 np toread npand lp2 best way to read npadp java .
based on the generated linguistic patterns we can extract instance questions of the new task from the corpus see sec.
.
.
e.g.
the question linq to read xml matches with the first linguistic patterns and the annotated question linq toread xml is extracted the question best way to read structured binary files with java matches with thesecond linguistic patterns and the annotated question best way to read structured binary files with java is extracted.
two new tasks task read xml in linq and task read structured binary file with java are extracted.
as before the tasks with their corresponding annotated questions are added to the task kg and the necessary relations are added.
at the end of the linguistic mutation based task extraction we compute confidence scores for tasks and annotated questions in the task kg as well and select the top n unselected tasks with the highest scores as the seed tasks for the next concept mutation based extraction see sec.
.
.
.
for example task read structured binary file with java is selected as a seed task for the next iteration.
in this way we may extract new tasks related to binary files in the next concept mutation based task extraction.
.
initial seed task extraction to obtain the initial seed tasks for the iterative extraction we use manually defined linguistic patterns to extract high quality tasks from given so questions.
we inspected the top voted questions with the java tag on so and then summarized the following linguistic patterns to identify and extract tasks.
how to vnpin np how can i vnpin np how do i vnpin np 309how to formulate specific how to questions in software development?
esec fse november singapore singapore table mutation operations with examples mutation operation mutated elements task mutated task action based action task read pdf file in java task load pdf file in java task write pdf file in java constraint deletion based constraint task read json file in java from server task read json file from server task read json file wikidata based object constraint task read pdf file in java task read pdf file in python task read pdf file in python so tag based object constraint task read xml file in java task read xml file in jdk task read pdf file in openjdk morphology based object constraint task read pdf file in java task read pdf in java task read file in java task kg based object constraint task read string from file task read json string from file where vmatches with a verb as the npmatches with a noun phrase as the in np matches with a noun phrase starting with in as the .
the other parts of the pattern must be matched literally.
in order to ensure the quality of the seeds we limit the length of the matched noun phrases to four words or less a convention in nlp .
for example the question how to read a pdf file in java will be matched with the first linguistic pattern and we extract task read pdf file in java .
the articles a an and the at the beginning of noun phrases are removed.
we use spacy to implement pattern matching and task extraction.
further we calculate the confidence score for each extracted task.
the confidence score of a task is the number of questions which contain the task s elements i.e.
.
for example assuming that read pdf file and java appear in questions respectively then the confidence score for thetask read pdf file in java is i.e.
.
note that when calculating the number of occurrences of the constraints we ignore the preposition because the preposition may be different in different questions for the same constraint e.g.
in java and with java .
finally we rank the extracted tasks from high to low according to their confidence scores and then select the highest top k tasks as initial seeds.
.
concept mutation based task extraction to extract tasks related to seeds at concept level we first identify new candidate tasks by mutating the task elements of given seed tasks based on domain knowledge.
then we verify the candidate tasks by matching them with so questions and extract valid new tasks with corresponding annotated questions.
.
.
concept based task mutation.
for any task discussed on so there may be related questions.
for example if a developer found a question about task read pdf file in java it is natural to think that there may be questions for similar tasks on so such astask read pdf file in python and task write pdf file in java .
this association is based on the background knowledge of the developer such as knowing that both java and python are programming languages and that read and write are opposite actions in programming.
based on this idea we design different ways to obtain candidate tasks by mutating given tasks.
table shows mutation operations with examples.
note that one mutation type may generate multiple candidate tasks for a given task.
action based mutation .
replace the in the with its synonyms and antonyms e.g.
mutate the from read to load or write .
xie et al.
summarized common categories for describing api functionality where each category contains verbs provided by previous research .
for example read and load belong to the same functionality category about reading something from sources.
in this work we treat the verbs in thesame functionality category as synonyms.
among the functionality categories some represent opposite functionalities e.g.
the categories represented by read and write .
functionality verbs from opposite categories are considered to be antonyms.
constraint deletion based mutation .
if the has multiple new tasks with all subsets will be generated.
for example for task read json file in java from server shown in table with two constraints three tasks are obtained after mutation two tasks have one constraint and one has no constraint .
wikidata based mutation .
replace concepts and with similar concepts from wikidata.
liu et al.
identified software concepts from wikidata.
many wikidata concepts have aliases e.g.
python and python are aliases for python .
if a concept in the or matches with any alias of a software concept we replace it with other aliases of the matched software concept and aliases of sibling software concepts of the matched software concept.
two concepts are sibling concepts if they have the same hyponymy relations i.e.
subclass of instance of or part of with the same concept e.g.
java and python are sibling concepts because they have an instance ofrelation to programming language .
so tag based mutation .
replace and with similar concepts from so tags.
each so tag has synonyms e.g.
the java tag has the synonyms jdk jre oraclejdk and openjdk .
if a concept in the or the matches with a synonym of a so tag we replace it with other synonyms of the same tags e.g.
we replace java with jdk .
further zhang et al.
classified so tags into categories e.g.
library framework tool .
if a concept in the or the matches with a synonym of a so tag we replace it with other tags in the same category e.g.
we replace gson with fastjson .
morphology based mutation .
replace noun phrases in or with all noun phrases extracted e.g.
from the object pdf file we extract two noun phrases pdf and file and replace pdf file with the two extracted noun phrases.
task kg based mutation .
replace the concept in the or with other sub concepts in our task kg e.g.
string could be mutated to json string because we extracted two objects json string and string from tasks in previous iterations e.g.
task parse json string in python andtask read string from console and create a relation json string isa string in the task kg see sec.
.
.
in this way we can mutate the concepts based on the previous task extraction iterations.
.
.
task based matching.
we match so questions with the candidate tasks seed tasks and mutated tasks .
if a question includes the and of one task at the same time we consider this question to match with the task and it is an instance of the task.
note that a question includes a if it includes 310esec fse november singapore singapore mingwei liu xin peng andrian marcus christoph treude jiazhan xie huanjun xu and yanjun yang the noun phrase of the .
we ignore the preposition in the because the same may have different prepositions or no prepositions at all e.g.
python to read pdf files .
the matching is lemmatization based rather than literal based thus reading and pdf files in a question may match with read and pdf file in a task respectively.
in this way we verify the mutated tasks with real questions.
invalid mutation tasks that have no corresponding question are filtered out.
sentence parsing errors caused by nlp tools will not affect this step because the matching is based on task elements and not on the sentence parsing result.
.
.
task and annotated question completion.
for each matching question with a task we further annotate the task components i.e.
in the question to obtain the annotated question for the task.
after that we analyze the entire question and try to complete and correct the annotated question and the task.
we attempt to extract more prepositional phrases as additional constraints and identify whether the sentence has a more suitable direct object as the .
for example based ontask read file in java we identify the question how to read data from file in java with stream as a matching question.
the annotated question will be how to read data from filein java with stream at first.
after completion the annotated question will be how to read data from file in java with stream corresponding to a new task task read data from file in java with stream .
in some cases the preposition in a constraint could be missing e.g.
android read pdf files .
if the concept in the constraint already exists in the task kg we add the preposition that is most commonly used with the concept as a supplement.
we add a default preposition in for the constraint e.g.
the corresponding task after completion istask read pdf file in android for the annotated question with a missing preposition.
.
.
seed question selection.
to select high quality annotated questions as seeds for the linguistic mutation based task extraction sec.
.
we compute the confidence scores for all tasks and annotated questions in the task kg and select seed questions according to the confidence scores.
we define the confidence score for questions differently than for tasks before which is a frequency count .
we consider the coverage and selectivity of a task corresponding to the question defined in eq.
and eq.
eq.
is short for equation respectively.
task coverage reflects the task s availability in the corpus and it is based on the number of questions related to the task in the corpus.
if a task covers more questions in the corpus it is more likely to lead to the extraction of new tasks.
task selectivity reflects the task s ability to select questions from the corpus and it is based on the number of extracted annotated questions related to the task.
if there are more task related questions extracted then the task is more likely to help extract new tasks.
selectivity is computed using the annotated questions that have been extracted while coverage uses all the questions in the corpus.
we introduce notations necessary for formally defining the confidence score.
an annotated question is marked as aq a task is marked ast the task for which aqis an instance is marked as taq.erepresents an element of a task t i.e.
or andwis a word in a task t except for prepositions in constraints .
entandwntrepresent the number of elements or words in a task t respectively.the confidence score of a task tis defined using eq.
.
score t coverage t selectivity t we use the noisy or model to combine the coverage and selectivity.
in this way if either coverage or selectivity is large then the final confidence will be large.
we measure task coverage from two perspectives task element level and word level using eq.
and eq.
respectively.
task coverage is then the average of task element level coverage coveragee t and word level coverage coveragew t .
coverage t coveragee t coveragew t considerqnis the number of questions in the corpus qneis the number of questions containing the element eof a taskt and qnwis the number of questions containing word wof a taskt.
when computing coveragee t we first compute the coverage of each element efrom tasktin the corpus that is qne qn.
because the number of questions in the corpus may be large we use the natural logarithm of both qneandqnto reduce the gap i.e.
lnqne lnqn .
the product of the maximum and average of the coverage of each element lnqne lnqn is taken ascoveragee t ranging from .
coveragew t is defined in the same way except that the proportion of each word wis calculated as lnqnw lnqn .
coveragee t max e t lnqne lnqn e tlnqne lnqn ent coveragew t max w t lnqnw lnqn w tlnqnw lnqn wnt only if the maximum coverage and average coverage of all elements are high the element level coverage will be high.
we measure the selectivity from two perspectives task element level and word level using eq.
and eq.
respectively.
task selectivity is the average of task element level selectivity selectivity e t and word level selectivity selectivity w t .
selectivity t selectivity e t selectivity w t in eq.
and eq.
aqn is the number of annotated questions in the task kg aqneis the number of annotated questions containing the element eof a taskt andaqnwis the number of annotated questions containing word wof a taskt.coveragew t is defined in the same way except that the proportion of each word wis computed as lnqnw lnqn .
selectivity e t max e t lnaqne lnaqn e tlnaqne lnaqn ent selectivity w t max w t lnaqnw lnaqn w tlnaqnw lnaqn wnt finally the confidence score of an annotated question aqis defined using eq.
.
we consider two factors for computing the confidence score of an annotated question task confidence and task information ratio.
in eq.
harmonic mean is used to combine task confidence and task information ratio as we deem both equally important quality factors.
score aq score taq wnt wnaq score taq wnt wnaq the higher the confidence of the corresponding task taqis the higher the quality of the annotated question taq.
as for the task information ratio we consider it as the ratio of the number of words in taskwntand the number of words in question wnaq.
the larger the ratio the more standard and concise is the way in which the question describes the task and the more likely the linguistic pattern generated from this annotated question could match with new questions.
for example for the same task task read json 311how to formulate specific how to questions in software development?
esec fse november singapore singapore file in java read a json file in java is better than read a json file that is very large in java because the previous question contains more words related to the task and is more likely to have other questions sharing a similar linguistic pattern.
note that we only used element level coverage and selectivity in the beginning.
we found that the seed tasks selected are overlapping on high frequency task elements.
therefore to increase the variability of seeds we added word level coverage and selectivity.
in this way we can select seed tasks with low frequency task elements yet with shared words with high frequency task elements e.g.
the low frequency task element image file benefits from the high frequency task element file .
when new annotated questions and tasks are added to the task kg we re compute the confidence scores for all tasks and annotated questions in the task kg.
the seeds selected for the next iteration may be affected by the scores.
for example if in one iteration we extract many tasks with annotated questions relevant to pdf file this may indicate that there are many questions discussing tasks related to pdf files in the corpus.
the confidence scores for previously extracted tasks and annotated questions related to pdf file will become higher and those tasks and annotated questions are more likely chosen as seeds for the next task extraction.
.
linguistic mutation based task extraction developers usually follow certain linguistic patterns to describe tasks e.g.
the linguistic patterns that we used in the seed task extraction see sec.
.
.
to extract tasks with similar linguistic patterns to the ones contained in the seed tasks we first mutate the annotated elements of selected seed questions to generate linguistic patterns automatically.
based on the generated linguistic patterns we identify new annotated questions and extract new tasks from these new annotated questions.
.
.
linguistic pattern generation.
for a given annotated question with n annotated elements we mutate it to generate multiple linguistic patterns.
we randomly select ... n annotated elements in turn for mutation.
different annotated elements are mutated to different linguistic elements is mutated to v adp?
matching with any verb or verb phrase e.g.
set up where adp refers to a preposition is mutated to np matching with any noun phrase is mutated to adp np matching with any noun phrase starting with a preposition.
in order to ensure the quality of the generated linguistic patterns we make sure that at least one annotated element of the generated pattern is not mutated.
all other words in the annotated question except for mutated elements remain in the generated linguistic patterns.
for example from the annotated question how to read pdf file in java we generate six linguistic patterns i.e.
we mutate .how to read npadp np is one of the linguistic patterns generated by mutating one object and one constraint.
in order to generalize the generated patterns and be able to match them with questions with subtle differences we allow for missing articles numbers and punctuation from the original annotated question.
we mark these as det?
num?
and punct?
respectively in the pattern.
.
.
linguistic pattern based matching.
we use the generated linguistic patterns to find matching so questions and extract the corresponding annotated questions and tasks following the same process as described in sec.
.
.
after extracting tasks with annotated questions we follow the same steps as described in sec.
.
.
to complete the annotated questions and the tasks.
we perform lemmatization of the actions objects and constraints and remove stop words e.g.
a an my at the beginning of the involved noun phrases.
in order to extract more possible questions without introducing too much noise we remove annotated questions where noun phrases in the object or constraints have more than six words based on our experience which is more relaxed than the threshold in sec.
.
.
.
.
seed task selection.
to select high quality tasks as seeds for concept mutation based task extraction sec.
.
we compute the confidence scores for all tasks and annotated questions in the task kg and select seeds according to the confidence scores.
the confidence score is the same as for the seed question selection in sec.
.
.
.
.
task kg construction to build the hierarchical conceptual structure in the task kg we identify relations between concepts and tasks when extracted tasks and annotated questions are added to the task kg at the end of the task extraction steps.
when adding tasks to the task kg we add the task elements i.e.
and build thehas relations i.e.
hasaction hasobject hasconstraint while instanceof relations are added between the annotated questions and their corresponding tasks.
further we identify the relations between concepts and add them to the task kg.
this part is very similar to the previous concept based mutation described in sec.
.
.
.
the synonym andantonym relations between actions are identified based on the functionality categories provided by xie et al.
.
we identify isarelations between concepts c1andc2referenced by or in three ways wikidata hyponymy relations.
c1andc2have corresponding software concepts sc1andsc2respectively and sc1andsc2have one of three hyponymy relations i.e.
subclass of instance of part of between them e.g.
json and file format .
so tag categorization.
if c2is matching with one of the categories defined by zhang et al.
andc1is matching with one of the tags belonging to the category.
morphology characteristics.
if c2is a prefix or suffix of another concept c1.
as for tasks a task has an isarelation to another task if it contains additional constraints e.g.
task read pdf file in java and task read pdf file or it refines the object or constraint with more specific sub concepts e.g.
task read large text file and task read text file .
if a task with fewer constraints does not exist in the task kg the task is added to the task kg.
.
resulting task kg we could mine the task kg from all so questions.
however in order to improve efficiency we only selected questions tagged with at least one of the six most popular tags i.e.
java python 312esec fse november singapore singapore mingwei liu xin peng andrian marcus christoph treude jiazhan xie huanjun xu and yanjun yang figure user interface for taskkg4q javascript php c android from the so data dump with at least one upvote as the corpus.
note that our approach is not limited to specific programming languages.
we only consider the title of questions.
the corpus includes questions.
we further filtered out low quality questions or questions not related to tasks if one of the following criteria is met questions containing no verbs or only passive verbs questions with more than words of the questions were filtered out based on this threshold questions starting with why what does it mean or when or including the words difference vs exception or error .
these keywords indicate that the question is related to technical comparison or debugging not to programming tasks.
after filtering we obtained questions.
for seed task extraction we extracted tasks using the three manually defined linguistic patterns mentioned before and selected the top tasks with the highest confidence score as the initial seed tasks for taskkg4q.
in each iteration we selected the top annotated questions as the seeds for linguistic mutation based task extraction and selected the top tasks as the seeds for concept mutation based task extraction.
the selected confidence thresholds are both .
.
the thresholds were determined via trial and error.
the resulting task kg contains tasks and annotated questions .
the task kg has actions objects constraints and concepts .
there are isarelations between tasks and isarelations between concepts.
a tool for interactive how to question formulation we developed an interactive tool to help developers write how to questions based on taskkg4q.
fig.
shows the user interface of our tool.
the tool enforces the canonical format of action objects and constraints for the how to question.
the users can use the drop down boxes to fill in task elements i.e.
the and highlighted in different colors click on the suggestions or type freely.
if the users need to write more than one constraint in the task they can click the add button on the right to add more input boxes about constraints.
as the users fill in the task elements the tool will provide onthe fly suggestions on the right based on what they already wrote.
the suggested options in the drop down boxes are updated as well accordingly.
the tool shows two types of suggestions missing suggestions and refinement suggestions.
for example if no is entered the right side will show missing suggestions for the .
if the is entered and no is entered then the right side will show missing suggestions for the and soon.
the missing suggestions are based on the popularity of candidate suggestions.
we define the popularity of a task element as the number of questions that includes it.
if the user has entered some task elements such as and then the tool selects questions that contain at least one of the entered task elements as candidate questions from the task kg.
we determine the popularity for all candidate suggestions using eq.
and list the top as missing suggestions based on the popularity.
the users can expand the suggestion list to see more suggestions if needed.
in eq.
ecis a candidate suggestion task element eis a set of task elements have been entered and eeis a task element that has been entered.
cooccur ec ee is the number of questions including ecandeeat the same time and occur ec is the number of questions includingec.
popularity ec ee ecooccur ec ee e occur ec e we remove the concepts that are sub concepts or siblings of the entered task elements from the suggestions.
for example if in java is entered as the then python and java will be removed from the suggestions for the missing .
we also show refinement suggestions for the task elements already entered.
for the and the refinement suggestions are the sub concepts in the task kg ranked by popularity.
for the the refinement suggestions are with higher popularity than the current .
for example if the current and are load and file the tool will show read as a refinement suggestion for the because we find that more questions are an instance of task read file than task load file .
evaluation we conducted empirical studies to evaluate the intrinsic quality of the task kg and taskkg4q s effectiveness in helping developers formulate specific how to questions.
as extrinsic evaluation we investigated taskkg4q s usefulness in helping developers complete actual programming tasks.
more specifically we focus on answering the following research questions rq1 what is the intrinsic quality of the task kg?
rq2 how effective is taskkg4q in helping developers formulate specific how to questions?
rq3 how useful is taskkg4q in helping developers complete programming tasks?
all the data used in these studies is provided in the replication package .
313how to formulate specific how to questions in software development?
esec fse november singapore singapore .
rq1 intrinsic quality we evaluated the intrinsic quality of the task kg by assessing the correctness of the annotated questions in the task kg.
.
.
protocol.
similar to previous studies we adopt a sampling method to ensure that ratios observed in the sample generalize to the population within a certain confidence interval at a certain confidence level.
for a confidence interval of at a confidence level the required sample size is .
we randomly selected annotated questions with corresponding tasks from the task kg.
we invited two master students not affiliated with this work with extensive development experience and familiarity with so to assess the label accuracy of the annotated questions independently.
the criterion is that the question is an instance of the corresponding task and that all elements are annotated correctly in the question.
for each sampled question if it was assessed differently by the two students a third student was assigned to give an additional assessment to resolve the conflict by a majority win strategy.
.
.
results.
the accuracy as determined by the annotation is .
i.e.
annotated questions are correct and annotated questions are at least partially incorrect e.g.
one task element annotated incorrectly .
the cohen s kappa agreement is .
i.e.
almost perfect agreement.
we analyzed reasons for the errors.
some errors are caused by the nlp tool.
for example we only extracted disable on google map from the question disable double left click on google map and failed to identify the correct object.
the nlp tool identifies the action disable as an adjective and can not identify double left click as the object of disable .
another reason is that questions are not related to a task e.g.
we extracted task enter custom text from the question combobox doesn t allow enter custom text if databinding is used .
however in this case we argue that the task is still somewhat meaningful.
future work will improve the accuracy further for example by training a question classifier to identify task related questions similar to the work of beyer et al.
.
.
.
summary.
.
of the annotated questions with tasks extracted by taskkg4q from the question corpus are correct.
.
rq2 effectiveness we evaluated the effectiveness of taskkg4q by asking participants to formulate how to questions for programming tasks and comparing with two baselines.
.
.
programming tasks.
we randomly selected questions from the task related questions we collected in the motivational study from sec.
as programming tasks.
for each selected programming task we removed the original title and only kept the question body and the question tags as the context for formulating how to questions.
those programming tasks cover different programming languages e.g.
java javascript and different topics e.g.
database user interface .
.
.
baselines.
we compared taskkg4q with two baselines chatbot4qr and withouttool i.e.
participants formulate how to question without any tool .
chatbot4qr is an interactive query refinement approach designed by zhang et al.
for question retrieval.given a query chatbot4qr can generate several clarification questions to interact with the user e.g.
what programming languages do you prefer?
e.g.
java or c .
chatbot4qr combines the answers of clarification questions with the original query to generate a refined query and retrieve so questions based on the refined query.
taskkg4q is different from chatbot4qr because we help developers construct the question from zero while chatbot4qr helps developers refine an existing question.
to enable the comparison we asked the participants to formulate an initial how to question without any tool first and then refining the initial question with chatbot4qr.
we used the implementation of chatbot4qr provided by zhang et al.
in their replication package.
.
.
protocol.
we invited students not affiliated with this work with years of development experience to conduct this experiment.
we first randomly divided the tasks into groups of tasks each.
the tasks in the same group are assigned to the same three participants to formulate how to questions with different approaches.
we use a within subject design where each participant only uses one approach per task i.e.
participant x formulates a question for task a without any tool for task b with chatbot4qr and for task c with taskkg4q participant y formulates a question for task a with chatbot4qr for task b with taskkg4q and for task c without any tool and participant z formulates a question for task a with taskkg4q for task b without any tool and for task c with chatbot4qr.
as a result each participant formulates how to questions for tasks from two groups i.e.
six tasks two task with taskkg4q two tasks with chatbot4qr and two tasks without any tool.
when participants formulate how to questions for tasks the body and tags of the original questions are provided as the context.
we collected how to questions formulated by the participants for tasks three questions per task formulated with three approaches.
we asked another two ms students to rank the how to questions for each task according to the quality of the questions.
the quality of a question was evaluated by its completeness understandability and conciseness.
the criterion for completeness is whether the question contains all the necessary and specific information for the task to be understood and answered correctly.
understandability and conciseness require that the question is understandable and contains no or very little unnecessary or redundant information e.g.
without complicated clauses.
the evaluators were asked to consider all criteria together when ranking the questions for a task.
note that they were not informed how the questions were generated or the purpose of the study.
moreover the three questions for each task were presented in random order for them to rank.
if the rankings of the three questions for one task are different a third ms student is assigned to resolve the conflict by a majoritywin strategy.
the cohen s kappa agreement is .
representing substantial agreement.
.
.
results.
the results of the comparison are shown in fig.
.
the average rankings of taskkg4q chatbot4qr and withouttool are .
.
and .
respectively i.e.
participants were able to formulate better how to questions with the help of taskkg4q.
taskkg4q ranks higher than chatbot4qr for .
of the questions taskkg4q ranks higher than withouttool for .
of the questions and chatbot4qr ranks higher than withouttool for 314esec fse november singapore singapore mingwei liu xin peng andrian marcus christoph treude jiazhan xie huanjun xu and yanjun yang figure ranking of the how to questions formulated with taskkg4q and the baselines .
of the questions.
we used welch s t test to verify the statistical significance of the difference between taskkg4q chatbot4qr and withouttool on rankings.
the differences between taskkg4q and withouttool as well as taskkg4q and chatbot4qr are statistically significant p .05for both .
.
.
summary.
taskkg4q helps developers formulate better howto questions than chatbot4qr or without tool support.
.
rq3 usefulness we evaluated the usefulness of taskkg4q by asking participants to complete a set of programming tasks.
when they need to learn how to solve a specific task we asked them to find the specifics on so.
they did that by writing questions on their own and with the taskkg4q tool.
we then compared the questions written with or without the tool support and assess which ones are better.
.
.
participants.
novices are the main audience of taskkg4q as we expect experienced developers can write good questions without tool support.
hence we focused our recruitment on novices in a certain programming language i.e.
familiar with the basic syntax but not familiar with many apis .
such novices will have at least one question and seek answers when completing their tasks.
to recruit participants we used a questionnaire to select qualified students from a class of students.
the questionnaire assessed their programming knowledge and whether they would need help to complete the four given programming tasks.
.
.
programming tasks.
we designed four small programming tasks such that they contain several features and that participants may need to search on so to find the solution e.g.
save a json file in java .
these features are independent meaning that participants should not find the answer to one while looking for another.
the programming tasks are extracted from lab assignments from a data structures class and modified slightly to include the above mentioned features.
the four tasks are t1 import a list of students with their grades in json format and convert it to a csv file which also includes their gpa .
t2 get all file names in a given directory rank them by size and calculate the file numbers for each type of file extension .
t3 read a text file and determine the frequency of each word and output the top most frequent words to a text file .
t4 read a list of students names from the console then randomly divide them into n groups and save the groups into a json file .
.
.
protocol.
we divided participants into two comparable groups pa and pb using similar programming languages i.e.
in each group figure comparison between taskkg4q and baseline there are and participants using python java and c respectively because they are novices in those languages.
the questions were divided into two roughly equal groups ta t1 t2 and tb t3 t4 based on task difficulty.
for pa participants complete ta with the baseline i.e.
without our tool and tb with our tool.
for pb participants complete tb with the baseline and ta with our tool.
when completing a task an example input and output are provided with more detailed task context included in our replication package .
participants must submit the complete code for each task and the code is reviewed by the authors to confirm its correctness.
while the participants complete a task they can search on so and we ask them to record their queries.
if they reformulate the questions we record the reformulations too.
if they found an answer they want using a question we asked them to record the answer with the ranking of the answer in the search results list.
when the participants use our tool they will write the question and refine it with the help of the tool then use it to search on so.
on the other hand participants using the baseline can only construct the query by themselves and reformulate the query based on the feedback from so search results.
each participant was first assigned tasks where they had to manually formulate the questions and then tasks where they used the tool to avoid a learning effect.
after completing all the programming tasks we also conducted interviews to get participants feedback on our tool.
participants were asked to evaluate our tool in terms of usefulness and usability on a points likert scale disagree somewhat disagree somewhat agree agree by rating the following statements usefulness the tool s suggestions were helpful for writing the questions.
usability the tool was easy to use.
.
.
results.
all participants agreed that the tool is helpful .
somewhat agreed and .
agreed and it is easy to use .
somewhat agreed and .
agreed .
from the experiment we collected queries with our tool and with the baseline .
only of these are formulated the same in the two conditions.
we manually analyzed the queries and grouped them based on features that participants needed to complete i.e.
we aligned queries by features.
if queries related to one feature are only raised when using our tool or the baseline we remove the queries to facilitate comparison.
after filtering we obtained queries with our tool and with the baseline for features.
we calculated mqn mean query number for our tool and the baseline i.e.
for each feature the number of queries that participants have to write to obtain the correct answers on average.
we also computed mrr fq mean reciprocal rank for the first 315how to formulate specific how to questions in software development?
esec fse november singapore singapore query for our tool and the baseline.
mrr mean reciprocal rank is the reciprocal rank of the correct answer in the search result list.
fig.
shows mqn and mrr fq when using our tool and the baseline.
using our tool participants found answers with fewer queries .
vs .
on average compared to the baseline.
at the same time the reciprocal rank of the correct answer with the first query is better .
vs .
on average compared to the baseline.
we used welch s t test for verifying the statistical significance of these differences.
the difference in mqn is statistically significant with p .
.
.
the difference in mrr fq is not statistically significant at an alpha level of .
with p .
.
in addition the informal feedback we received from participants shows that when using the baseline they are often frustrated because the queries they write cannot match with questions on stack overflow but the answers they actually want are often found by changing the query with appropriate technical terms.
when using our tool participants could write queries step by step guided by our tool and suggestions provided are useful for them to choose more appropriate technical terms.
several participants also asked for similar support tools for other types of questions e.g.
questions related to bug fixes.
we will consider this in future work.
.
.
summary.
taskkg4q can help novices write better how to questions for so and they consider it easy to use.
threats to validity a potential threat to internal validity stems from the use of the natural language processing library spacy.
we extract seed tasks and perform linguistic mutation with spacy.
no natural language processing library achieves accuracy on any large data set and spacy s performance was found to be on par with the state of the art and outperforming other libraries when applied to software documentation .
we design our approach to tolerate errors of nlp tools to a certain extent as explained in sec.
.
.
and .
.
.
the empirical study and the evaluation share common threats to validity.
a threat to the internal validity is the subjective judgments in different parts.
to alleviate this threat we used at least two judges and reported the agreement for each subjective judgment or the corresponding statistical significance.
a threat to the external validity is the limited number of subjects e.g.
programming tasks participants considered in different parts.
we also cannot claim generalizability of taskkg4q beyond questions related to the six most popular so tags.
related work previous research has categorized stack overflow questions and how to questions are identified as an important question category on stack overflow.
for example treude et al.
identified ten categories of stack overflow questions i.e.
how to discrepancy environment error decision help conceptual review non functional novice and noise .
in this paper we focus on formulating how to questions.
existing studies focus on extracting task related knowledge or similar verb phrases from identifier names api functionality description documentation and comments .
for example treude et al.
extracted tasks fromdocumentation to help developers navigate documentation.
different from these works we standardized the representation of tasks i.e.
action object constraints and then designed a top down and bottom up combination to iteratively extract tasks from a largescale corpus with the hierarchical conceptual relations between tasks.
in the field of software engineering some studies build other types of kgs from different sources such as kgs for bugs api caveats domain terminology api concept and descriptive knowledge and api comparison .
our work is the first creating a task kg for how to questions from stack overflow.
many studies focus on automatically reformulating queries by expanding them with relevant terms for different retrieval tasks e.g.
bug localization and code search.
for example hill et al.
reformulate queries with natural language phrasal representations of method signatures.
similarly lu et al.
also extend queries with synonyms based on wordnet to improve the hit rate of code search.
for searching on stack overflow zhang et al.
designed chatbot4qr to help developers refine their queries suggesting tags from existing posts.
similarly cao et al.
proposed sequer to support automated software specific query reformulation based on query logs provided by stack overflow.
in contrast we focus here on helping developers formulate questions from scratch rather than refining an existing query and we only focus on how to questions.
calefato et al.
provided suggestions for writing questions on stack overflow while other research studied the factors that affect the quality of questions and how to automatically predict the quality of questions .
unlike that research we help developers write quality how to questions from the beginning.
conclusions and future work we observed that stack overflow questions are subject to many edits indicating that developers struggle to write good questions from scratch.
we also found that of the edits to how to questions are modifying the elements of the underlying task i.e.
.
this is evidence that developers could benefit from tool support in formulating how to questions.
we posit that how to programming questions posed by developers are important and frequent enough that they need specialized tool support as opposed to one size fits all query reformulation approaches.
our solution taskkg4q is an interactive approach that leverages the structure and content of past how to questions.
we built a task knowledge graph from so questions extracting actions objects constraints concepts and relations between them.
we also found that using the task kg to guide novices when formulating how to questions related to programming tasks helps them produce better questions.
in the future we will combine taskkg4q with existing automatic q a systems e.g.
answerbot or apibot .