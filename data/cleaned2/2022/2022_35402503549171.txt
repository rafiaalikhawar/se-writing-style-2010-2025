utango untangling commits with context aware graph based code change clustering learning model yi li new jersey institute of technology new jersey usa yl622 njit.edushaohua wang new jersey institute of technology new jersey usa davidsw njit.edutien n. nguyen university of texas at dallas texas usa tien.n.nguyen utdallas.edu abstract during software evolution developers make several changes and commit them into the repositories.
unfortunately many of them tangle different purposes both hampering program comprehension and reducing separation of concerns.
automated approaches with deterministic solutions have been proposed to untangle commits.
however specifying an effective clustering criteria on the changes in a commit for untangling is challenging for those approaches.
in this work we present utango a machine learning ml based approach that learns to untangle the changes in a commit.
we develop a novel code change clustering learning model that learns to cluster the code changes represented by the embeddings into different groups with different concerns.
we adapt the agglomerative clustering algorithm into a supervised learning clustering model operating on the learned code change embeddings via trainable parameters and a loss function in comparing the predicted clusters and the correct ones during training.
to facilitate our clustering learning model we develop a context aware graph based code change representation learning model leveraging label graph based convolution network to produce the contextualized embeddings for code changes that integrates program dependencies and the surrounding contexts of the changes.
the contexts and cloned code are also explicitly represented helping utango distinguish the concerns.
our empirical evaluation on c and java datasets with and 14k tangled commits show that it achieves the accuracy of .
.
and .
.
relatively higher than the state of the art commit untangling approaches for c and java respectively.
ccs concepts software and its engineering software evolution .
keywords commit untangling deep learning code change embeddings acm reference format yi li shaohua wang and tien n. nguyen.
.
utango untangling commits with context aware graph based code change clustering learning corresponding author permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november singapore singapore association for computing machinery.
acm isbn .
.
.
.
in proceedings of the 30th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november singapore singapore.
acm new york ny usa pages.
introduction during software evolution developers make changes over time to perform software maintenance tasks.
the changes to source files committed to the repository in the same transaction are referred to as a change set or a commit .
for separation of concerns each commit should be about one purpose or concern regarding the programming task at hand.
unfortunately it has been reported that many commits tangle different concerns including the changes for bug fixing refactoring enhancements improvements or documentation .
such change sets are called tangled code changes ortangled commits .
the prior work reported two reasons for tangled commits from developers perspective time pressure in committing the changes and unclear relations between the concerns for code changes .
tangled commits pose several issues in software development.
first they affect software quality as they hamper program comprehension and reduce the separation of concerns in code changes .
second the tangled commits might contain the bug fixing changes for one bug that are mixed with the fixes for other bugs as well as other types of changes for refactoring enhancements or documentation .
those tangled commits have negative impact on the accuracy of bug prediction or bug localization models that rely on the changes mined from the repository .
those models are significantly affected by the tangled commits as they consider an entire commit as for fixing or non fixing.
recognizing the need of tools that untangle i.e.
decompose a commit into untangled changes researchers have proposed several approaches.
the automated commit untangling approaches can be broadly classified into two categories mining software repositories and program analysis .
first the earlier approaches leveraged mining software repositories msr techniques to untangle commits.
herzig et al.
utilize a confidence voter technique with agglomerative clustering on change operations for untangling.
the voters consider data dependencies call graphs change couplings and distances.
kirinuki et al.
consider a commit as tangled if it includes another commit in the past.
however there are tangled commits whose parts have not occurred in the past.
dias et al.
use confidence voters on the fine grained change events in an editor.
the confidence scores are converted into the similarity scores via a random forest regressor which are then used in an agglomerative clustering algorithm to partition the tangled changes.
esec fse november singapore singapore yi li shaohua wang and tien n. nguyen the second category of untangling approaches leverage the static analysis techniques.
roover et al.
use program slicing to segment a commit across a program dependency graph pdg .
however it is limited in handling inter procedural and cross file dependencies.
barnett et al.
use def use chains and cluster them.
if the def use chain all falls into a method it is considered as trivial otherwise non trivial.
because ignoring the trivial clusters it can miss tangled concerns.
flexeme uses multi version pdg augmented with name flows in the edges and applies agglomerative clustering using graph similarity on that graph to untangle the commits.
smartcommit uses a graph partitioning algorithm on a graph representation to capture the relations among code changes hard and soft links refactoring links cosmetic links etc.
.
despite their successes the state of the art untangling commit techniques still have limitations.
first the boundaries across concerns in a commit do not necessarily and naturally map to clustering criteria of a clustering algorithm running on the pdg name flows program slices change operations or the changes themselves.
the concerns might be linked via multiple edges.
to apply a clustering algorithm on the pdg program slices or change graphs it is challenging to deterministically specify the right criteria to obtain perfect partitions matching with the concerns.
second the goal is to decompose the changes in a commit.
however the existing approaches do not consider a change w.r.t.
the context of surrounding code with a clear distinction of the changed elements and the unchanged ones in the context .
such context could help distinguish the concerns for the changes.
finally not all the changes in the same concern need to have program dependencies among them.
the logic connection among the co changed code in the same commit could be due to the reasons different than program dependencies.
for example two pieces of cloned code realizing the same bubble sorting algorithm have the same bug e.g.
at the comparison operator.
they might be changed for the same concern to fix that logic bug despite that they have no data control dependency.
to address those challenges in this paper we propose utango anovel code change clustering learning model that learns to untangle a commit by clustering the code changes represented by the embeddings into groups for different concerns.
while deterministic clustering criteria on the pdg slices or change operations do not always produce the clusters that naturally map to the boundaries between the concerns a machine learning ml model is expected to learn to cluster the changes thus untangling a commit.
our ml model learns from the changes belonging to the same concerns in the version history.
to build the training data for such learning we adapt herzig et al.
s method to mine the changes for the same concerns in the version history section .
to facilitate learning to cluster code changes we develop a context aware graph based code change representation learning rl model leveraging label graph based convolution network to produce the contextualized embeddings for code changes .
our clustering learning model and context aware graph based rl model have the following unique characteristics that facilitates the untangling of code changes.
first we use label gcn graph convolutional network to model the changes and surrounding code by integrating both the versions before and after changes in a multi version program dependence graph pdg .
pdg encodes the program dependencies among the changed andunchanged statements .
second to decompose the changes we explicitly represent the surrounding code context of each change .
the explicit representation of the context could help utango learn the important features of a change e.g.
code structures data control dependencies to distinguish its concern among others.
third utango also considers an implicit relationship the cloned code that is similar to the changed code under study.
the idea is that thetwocloned code with similar logic might be changed in the same manner in the same concern in a commit .
fourth to untangle a commit in our code change clustering learning model agglomerative clustering runs on contextualized embeddings for code changes that integrates richer encoded information than the pdg or program slices thus helping better distinguish the concerns of the changes.
finally we adapt the agglomerative clustering algorithm into a supervised learning clustering model with trainable parameters and a loss function to adjust the parameters by comparing the predicted clusters and the correct ones during training.
we have conducted several experiments to evaluate utango .
our experimental results on a real world c dataset with tangled commits show that utango achieves the accuracy of .
.
.
and .
relatively higher than the baseline approaches flexeme pdg cv herzig et al.
and barnett et al.
.utango can correctly cluster of the changed statements into correct concerns.
we also evaluated utango in a java dataset with 14k tangled commits.
the results show that utango achieves .
accuracy relatively higher than the state of the art approaches including smartcommit .
our sensitivity analysis shows that all designed components in utango contributes positively to its accuracy.
we show that the changed statements in the same concerns are projected nearer to each other than the ones in different concerns helping utango better in change clustering.
the key contributions of this work include .utango an ml based commit untangling approach with a novel code change clustering learning model.
it is the first ml model that learns to untangle the commit by learning to cluster the code changes.
utango learns from the changes belonging to the same concern in the version history.
we adapt agglomerative clustering into a supervised learning clustering model.
.
a novel context aware graph based representation learning for code changes.
we design gcn based model to produce thecontextualized embeddings for the code changes that integrates program dependencies changes contexts and cloned code.
.
extensive empirical evaluation.
we evaluated utango against the state of the art approaches for untangling commits to show its better performance.
our tool and data are available at .
motivation .
motivating examples let us present a few real world examples to motivate utango .
.
.
example .
figure shows the changes at the commit r1192 of the jhotdraw project with the log fixes npe and implements indentation of xml elements .
this is a tangled commit with two concerns purposes the fix for null pointer exception occurred at lines and of the drawing class the null argument was replaced with collections.emptylist the implementation of the indentation of xml elements occurred at lines and a few 222utango untangling commits with context aware graph based code change clustering learning model esec fse november singapore singapore commit r1192 fixes npe and implements indentation of xml elements.
trunk jhotdraw8 src main java org jhotdraw8 draw drawing.java ... public final static key list url author stylesheets new ... authorstylesheets list.. new class ?
uri.class .
null public final static key list uri author stylesheets new ... authorstylesheets list.class new class ?
uri.class ... collections.emptylist ... public final static key list uri user agent stylesheets new simplefigurekey useragentstylesheets list.class new class ?
uri.class ... null public final static key list uri user agent stylesheets new simplefigurekey useragentstylesheets list.class new class ?
uri.class ... collections.emptylist ... public final static key list string inline stylesheets new ... inlinestylesheets list... new class ?
string.class .
null public final static key list string inline stylesheets new ... inlinestylesheets list.class new class ?
string.class ... collections.emptylist trunk jhotdraw8 src main java org jhotdraw8 draw io simplexmlio.java public document todocument drawing ... collection figure selection ... for figure child ordered writenoderecursively doc docelement child docelement.appendchild doc.createtextnode ... writenoderecursively doc docelement child linebreak docelement.appendchild doc.createtextnode linebreak return doc ... figure a tangled commit at r1192 of jhotdraw commit r1023 fixes bugs in figurestylemanager trunk jhotdraw8 src main java org jhotdraw8 draw drawing.java public final static key list uri author stylesheets new .
authorstylesheets list.class uri ... null public final static key list uri author stylesheets new .
authorstylesheets list.. new class ?
uri.class .
null public final static key list uri user agent stylesheets new .
useragentstylesheets list.class uri ... null public final static key list uri user agent stylesheets new simplefigurekey useragentstylesheets list.class new class ?
uri.class ... null public final static key list string inline stylesheets new .
inlinestylesheets list.class string ... null public final static key list string inline stylesheets new .
inlinestylesheets list.. new class ?
string.class .
null figure same statements as in r1192 were changed at r1023 of jhotdraw and belonged to only one concern other lines of code in the simplexmlio class and one line of code in thedrawing class not shown .
.
.
example .
figure shows the changes committed at r1023 earlier in jhotdraw.
the changes at the lines and of the drawing class were to the same statements as the ones in the r1192 commit in figure .
however the commit at r1023 was for only one concern as stated in the commit log fixes bugs in figurestylemanager .
this example motivates us to build a ml model to learn from the co changes for the same concern in the version history to untangle the current commit.
observation .
history of the co changed statements with the same concern could be a good source for a machine learning model to learn to cluster the changed statements thus untangling the current commit .
.
.
example .
figure shows another example in jhotdraw project.
at the commit r463 two changes at line and line are exactly the same with the addition of figure.isselectable to1 public void mousepressed mouseevent evt ... selectiontool.java if figure !
null if figure !
null figure.isselectable newtracker createdragtracker figure else if !
evt.isshiftdown ... protected void updatehoverhandles drawingview view figure f ... selectareatracker.java figure f if figure !
null if figure !
null figure.isselectable hoverhandles.addall figure.createhandles ... figure same change in two contexts for different concerns at r463 of jhotdraw check whether a figure is selectable or not.
however those two exact changes occurred in two different methods mousepressed and updatehoverhandles for two different concerns purposes as noted in the commit log.
line was aimed to fix the tracking of a dragging object as the mouse is pressed fixed bug where setselectable on a dragging figure did not work .
line was a fix for a different bug as the hovered object needs to be selected selection classes now check the flag on the hovering figures to be selected.
.
the addition of figure.isselectable is common for checking if a figure is selectable in the two tasks of dragging and hovering a figure.
without the surrounding contexts one could not tell whether two changes are for the same purpose or not.
observation .
the surrounding context consisting of un changed code is crucial to determine the concern of a change.
the same change in two contexts could be for different concerns .
figure also gives us an interesting observation.
the changes to fix the npe at lines and are exactly the same null collections.emptylist .
the three statements are cloned code of one another to support for different stylesheets author user agent and inline stylesheets .
in fact the changes in figure also occurred at the cloned statements.
the three changes belong to the same concern purpose since they serve as a fix for the same logic despite that there is no program dependency among them.
the existing untangling approaches that require the explicit program dependencies among the changes with the same purpose will not classify these committed code as belonging to the same concern.
observation .
two fragments of cloned code have the same similar logic thus have implicit dependencies and could be modified in the same manner to serve the same purpose.
.
key ideas inspired from the above observations we propose utango with the following ideas.
.
.
key idea .
instead of deciding a deterministic clustering criterion on the concrete artifacts pdgs program slices code changes operations or change graphs from observation we build an ml model to untangle the commits by learning to cluster code changes represented by embeddings w.r.t.
different concerns.
the model learns from the history of the co changed statements in the same commits for the same 223esec fse november singapore singapore yi li shaohua wang and tien n. nguyen concerns and applies to cluster the changes in the current commit.
we also modify an agglomerative clustering algorithm into a supervised learning clustering model via trainable parameters and a loss function to compare the predicted and correct clusters.
.
.
key idea context aware graph based representation learning for code changes .
we design a context aware graph based representation learning model to learn the contextualized embeddings for the code changes that integrates program dependencies among the program elements and the contexts of code changes.
leveraging the changes in the same concern in the history we train a label graph based convolution network to learn the embeddings and learn to cluster them.
for prediction we build a supervised learning agglomerative clustering algorithm that operates on the embeddings of code changes to produce the clusters for the purpose of untangling the commit.
we expect that supervisedlearning clustering on vectors is more effective than on those artifacts since the embeddings capture richer information integrated from dependencies and contexts.
.
.
key idea explicit context representation as a weight to compute vectors for code changes .
as seen in observation the contexts of the code changes can help distinguish their concerns in the commits.
we represent code changes and the surrounding context of a change via the multi version program dependence graph pdg consisting of the elements of both versions before and after the changes and program dependencies.
the context is defined as the surrounding nodes of the changed statement node in that graph.
the label gcn is used to model the statements and their program dependencies in pdg as well as to learn the vector representing the context for a change.
the context vectors are then used as the weights in learning contextualized embeddings for the code changes.
.
.
key idea .
as seen in observation the cloned code exhibits implicit dependencies with regard to whether they can be changed in the same commits for the same concerns.
thus during the process of producing the final result we also integrate the code clone relationships to adjust the clusters produced by the clustering learning model.
approach overview .
important concepts let us first define some important concepts used in utango .
definition program dependence graph .
theprogram dependency graph pdg is a directed graph with a set nof nodes and a set eof edges such that each node n nrepresents a program statement or a conditional expression each edge e e represents the data or control flow among the statements.
figure shows the code change in a commit in which the statementint next i is replaced by var next i .
figure a and b display the pdgs of the method formatyear before and after the change.
all the nodes of the pdg before the change are marked withi and those of the pdg after the change are marked with j. definition multi version program dependence graph .
pdg .
a pdgi jis a directed graph generated from the disjoint union of all nodes and edges in the pdgs at versions iandj .
figure multi version program dependence graph figure c displays the multi version pdgi j pdgi j that are built from the two versions iandjof the method formatyear before and after the change.
in pdgi j the nodes labeled with either i orjappear only in the pdg for the version ior the version j. the nodes labeled with i jappear in the pdgs at both the versions.
definition changed un changed nodes .
in the multiversion pdg pdgi jfor the versions before and after the change the changed nodes represent the changed statements and are labeled with eitheriorj while the un changed nodes are labeled with i j. in figure c the node labeled with irepresents the deleted statement the node labeled with jrepresents the added one while the nodes labeled with i jare for the un changed statements.
definition context .
the context cof a changed node n is a sub graph of the multi version pdgi jthat includes all the un changed nodes within the k hop neighbors of the changed node n together with all the inducing edges among them.
in figure c when k the context for the changed node statement at line consists of all three nodes labeled with i jbecause they are one hop from the changed node for int next i .
.
architecture overview figure illustrates the overview of our model utango .
.
.
step .
building multi version pdg pdgi j and contexts.
the first step is to build the pdgi jgraph and extract the context sub graphs for each changed statement from the two versions i andjbefore and after the changes.
we adopt the multi version graph building algorithm from flexeme .
specifically we first generate the pdgs for both versions iandj.
we use the git diff tool on the source code to determine the changed and unchanged nodes for the statements.
the added nodes are kept in pdgi jwith the labelsjas they appear in the newer version j. we also retain the deleted nodes and use the label ifor them.
the unchanged nodes between the versions are matched by using string similarity among the respective statements to filter the candidates and line span proximity to rank them.
when considering the edge changes we back propagate the delete nodes to the edges flowing into them.
we also add all the unmatched edges in the newer version jto the multiversion pdgi jas the edges relevant to the added nodes.
details on building pdgi jcan be found in another document .
224utango untangling commits with context aware graph based code change clustering learning model esec fse november singapore singapore figure utango architecture overview after constructing pdgi j for each changed node in the graph we collect all unchanged nodes within the k hops and all their inducing edges to build a sub graph as the context for the changed node.
pdgi jand the contexts for the changed nodes will be used as the input for the next step.
this step of building pdgi jand contexts is used in both training and predicting processes.
.
.
step .
context aware graph based code change clustering learning model.
the task of this model is to learn to cluster the code changes represented by the changed nodes and corresponding contexts in pdgi j. for that utango first learns to construct the contextualized embeddings to represent the code changes via our novel context aware graph based code change representation learning model .
in that model to build the contextualized embeddings we leverage the label gcn that can deal with the nodes with multiple labels used to denote the versions i j to learn the representation vector vfor each node nin the graph.
for a changed node nc we collect the vectors for the un changed nodes in the context for ncinto a matrix.
we use a fully connected layer to convert the matrix into a vector vctxto encode the contextual information for the node nc.
the context vector vctxis then used asa weight to represent the impact of the context on the learning to produce the final vector for the changed node nc.
with the contextualized embeddings built for all the changed nodes in pdgi j our code change clustering learning model uses the hierarchical agglomerative clustering algorithm to cluster the changed nodes represented by their embeddings.
that clustering algorithm is adapted into a supervised learning clustering model as follows.
during training we know the correct clusters of the changed nodes from the training data.
we define a trainable thresholdfor the linkage when merging the smaller clusters into a larger one.
the trainable parameters of the model and the trainable threshold are computed over many iterations in training as the predicted clusters are compared against the correct clusters in the oracle.
we design a loss function considering such comparison to adjust the model s parameters over the iterations.
for predicting the trained model is used to cluster the changed nodes in pdgi j. the changed nodes in the same cluster are treated as having the same concern.
.
.
step .
updating clusters via code clone detection.
after having the resulting clusters from step we use a code clone detection tool to detect if there is a cloned statement s mof a changed statementsmin the pdgi j. if so we check the clusters containing smands m. when the clusters are different we merge those clusters for one concern if needed.
after iterating over all the changed statementssm we obtain the final resulting clusters.
.
training and predicting processes .
.
training and predicting.
all the steps in figure are shared between the training and predicting processes.
the key difference is that in training the ground truth labels with respect to the clusters concern concern etc.
for the changed statements are known while in predicting utango predicts the clusters concern concern etc.
and finally updates them via our code clone detection process.
during clustering the changes in a new commit the number of clusters is unknown and will be decided by our model.
.
.
training corpus.
to train our clustering learning model we need to have a corpus of the past commits that were un tangled into multiple clusters each for a different concern.
we reuse the data collection methodology by herzig et al.
which was later used by partachi et al.
to build the artificial corpus of tangled commits.
the method mimics the way of a developer committing multiple consecutive work units as a single patch thus mimics the tangled commits that s he makes.
the goal is to compose the tangled commits from the atomic ones that were mined from the repositories.
specifically we detected the atomic commits and tangled them to produce the tangled commits in a training dataset.
we consider the commits to satisfy the following conditions the changes have been committed by the same developer within days with no commit by the same developer in between them the change namespaces whose names have a large prefix match the commits contain frequently changed together files and the commit logs do not contain fix bug feature etc.
multiple times.
with this data collection methodology we can obtain the tangled commits consisting of the clusters of atomic changes for training.
context aware graph based code change clustering learning model after the first step we obtain the multi version pdgi jand the contexts of the changes section .
we present in this section our context aware graph based code change cc clustering learning model.
our clustering learning model has two tasks taking the computed pdgi jto learn the representation vectors embeddings for the changed code and performing clustering on those embeddings to cluster the changed statements.
during training we have the ground truth on the clusters of the changed statements thus we have the cluster labels concern concern etc.
for the changed nodes in pdgi j. during predicting i.e.
clustering the input pdgi jwill be fed into the trained clustering learning model to produce the cluster labels for the changed nodes.
note for a given pdgi j the number of clusters is unknown to our model 225esec fse november singapore singapore yi li shaohua wang and tien n. nguyen figure context aware graph based code change clustering learning model and will be decided by the model during agglomerative clustering.
let us detail those two tasks of our clustering learning model.
.
representation learning for code changes this task aims to build the vector representations i.e.
embeddings for the code changes i.e.
changed statements represented by the changed nodes in pdgi j. a characteristic of the embeddings for code changes is context aware orcontextualized the same code change in different contexts will have different embeddings.
.
.
label graph convolutional network label gcn .
to achieve that utango first feeds the multi version pdgi jto a graphbased ml model to learn the contextualized embeddings for the nodes in the graph.
since pdgi jcontains the labels i.e.
i j i j representing the un changes we use label gcn to learn the graph structure and the node features with change labels.
similar to gcn label gcn takes the graph with node features as input and produces the vectors for the nodes when considering the features of the neighboring nodes of each node.
in addition for the current node in the first layer label gcn considers the change labels of the neighboring nodes as part of the feature vectors.
it computes the vectors in the first layer as follows h1 ax diag a k j 1ejet j w0 a d a d a a i wherehis the output for the first hidden layer ais the adjacency matrix dis the diagonal node degree matrix wis the weight matrix xis the input and x rnx d k nis the number of nodes dis the dimension of node features kis the number of types of change labels in the input ejet ja single entry matrix and diag a k j 1ejet jis used to eliminates the self loops for the components of the feature vectors corresponding to the labels.
in label gcn the following layers after the first layer follow the same process as in the gcn model to compute the hidden states.
the computation in a following layer lis as follows hl ahlwl l .
.
using label gcn to model pdgi j.let us explain how we process the multi version pdgi jto produce the input for the label gcn model.
for each node nof a statement sin pdgi j we breaksdown into the code tokens t and build the vector etfortusing a word embedding technique .
the vector for the node n is the average vector avgnof the vectors of all the tokens twithin the corresponding statement s. because each node nhas a change labeli j or i j for the versions we combine avgnwith the onehot vector of the length representing the labels i j or i j .
as a result the combined vector vn with the length of len avgn is the node feature vector forn.
then we build the graph with the same structure as pdgi jin which a node nis replaced with the node feature vector vn and feed that graph to the label gcn model to obtain the embeddings vnfor all the nodes nin pdgi j. .
.
building contexts and contextualized embeddings.
after using label gcn we obtain the vectors vnfor all the nodes n. for a changed node nc we collect the nodes in its context ctx i.e.
all un changed nodes that are the k hop neighbors of ntogether with all the inducing edges among them .
we merge all the vectors for the nodes in ctxinto a matrix accordingly to the order of the statements in source code.
we then use a fully connected layer to build the vectorvctxrepresenting the context ctxof the changed node nc.
finally we perform a cross product between the context vector vctxand the vector vncproduced by label gcn for a changed node nc to build the contextualized embedding v ncfornc.
figure illustrates the process of building the contextualized embeddings for the code change example in figure .
s3 ... s7are the statements at the corresponding lines.
after building pdgi j utango goes through the process described in section .
.
i.e.
building token embeddings statement embeddings and combining with the labels to produce the node feature vectors vnfor the nodes in pdgi j. the label gcn model takes the graph with the vectors vnto produce the graph with the same structure in which each node is represented by the vector vn v3 ... v7 .
let us use the node for v6as an example.
the context of hop neighbors includes v3 v4 andv7.
by merging those vectors as a matrix and passing through a fully connected layer we obtain the vector vctxrepresenting the hop context for v6.
the cross product vector v vctx v6is the contextualized embedding representing the changed statement s6.
.
learning to cluster code changes .
.
supervised learning hierarchical agglomerative clustering.
after building the contextualized embeddings v ncfor all the changed nodesncin pdgi j utango performs clustering on those vectors to untangle the commit.
we modified the hierarchical agglomerative clustering algorithm into a supervised learning clustering model to cluster those vectors based on the clusters of code changes 226utango untangling commits with context aware graph based code change clustering learning model esec fse november singapore singapore in the training data.
specifically the training process for our code change clustering learning model works as follows.
step .
we treat each changed node ncas a separate cluster clc.
step .
we merge any two clusters whose cluster similarity is the largest and higher than a threshold t. step .
we repeat the merging in step to form larger clusters until there is no cluster that can be merged.
after this step we obtain the predicted clusters clat the current iteration.
step .
the predicted clusters clat this iteration are compared against the correct clusters cloracle in the ground truth.
we develop a loss function for our training to minimize the differences between the predicted clusters cland the correct clusters cloracle .
the parameters and the trainable threshold twill be updated accordingly to the loss function for the next iteration.
the training will stop when the process converges and we obtain the most suitable parameters for our clustering learning model.
next let us explain the key components in our algorithm.
.
.
cluster similarity.
to compute the similarity between two clusterscl1andcl2 we take all the pairs of the changed nodes n1 n2 wheren1 cl1andn2 cl2.
we then compute the cosine similarity between the corresponding vectors v n1andv n2for each pair.
the similarity between two clusters is calculated as the average of all the similarity scores of all the pairs n1 n2 .
.
.
trainable threshold t. in our model we treat the merging thresholdtbetween smaller clusters as a trainable parameter .tis updated after each iteration in accordance with the criteria defined in the loss function as any other parameters of the model.
.
.
loss function.
we need to have a loss function that minimizes the differences between the predicted set of clusters cl cl1 cl2 ... clm and the correct set cloracle co1 co ... con in the oracle.
because the predicted and correct sets might have different numbers of clusters m n we first make them have the same sizem max m n by adding the empty clusters to the smaller set between clandcloracle .
because we do not know what predicted cluster cliinclis mapped to a cluster cojin the correct setcoracle we consider all possible orders of the clusters in bothclandcloracle .
the number of clusters is usually small as reported in thus it is manageable to consider all m!
possible orders in cland allm!possible orders in coracle .
let us consider an order in cl cl ... cl m and an order incoracle co ... co m .
for a changed statement scwith the corresponding changed node ncin pdgi j we build the hot vectorxof the lengthmthat represents the cluster for the node nc predicted by the model as follows.
if ncis predicted to belong to a clustercl i the value at the ithposition of the vector xwill be set to otherwise it is set to .
for example if m and the changed nodencis predicted to belong to the cluster cl the vector xis .
similarly we build the hot vector yto represent the correct cluster for ncin the oracle if ncbelongs to a cluster co i the value at the ithposition of the vector yis otherwise it is .
for a specific order in cl cl ... cl m and a specific order incoracle co ... co m for a changed node nc we use the crossentropy loss function in the multi class classification x x1 ... xm y y1 ... ym are the predicted and correct vectors for nc loss x y m i 1wilogexp xi exp m j 1xj yi to adjust formula for our clustering problem we consider all possible orders in the cluster set cland those in coracle .
the crossentropy loss function for a changed node ncof a changed statement scis the minimum value among all the values on the right hand side of formula .
that is our loss function is computed as follows.
loss x y min all m!
orders m i 1wilogexp xi exp m j 1xj yi the loss function for each changed node ncwill be used for the model to adjust the parameters in the next iteration.
.
.
predicting clustering process.
after training we obtain all model s parameters and the trainable merging threshold.
for clustering utango takes a pdgi jas input and generates the resulting set of clusters cl.
updating clusters via code clone detection because pdgi jmight not cover the changed statements that are clones of one another we use a code clone detection tool to find the changes that might belong to the same concerns but having no data control dependencies with one another.
the clone detection tool returns a list of clone candidates in the clone groups with different sizes.
first we consider only the clone groups with the cloned fragments containing the changed statements.
if multiple clone fragments contain the same changed statement we choose the clone fragment with the largest size to avoid the duplication because the larger clone contains the smaller one .
after predicting clustering the changed nodes statements as explained in section we have marked each changed statement with a concern.
for each changed statement scin a clone group we check the clusters of its cloned statements and update the cluster forscvia majority voting.
for example a changed statement sc marked with concern and it belongs to a cloned fragment in which the other statements in that fragment and their clones are marked with concern in their majority then we change scto be about concern .
if there is no majority we adjust the cluster of sc to the concern cluster with a lower index for consistency.
empirical evaluation .
research questions to evaluate utango we seek to answer the following questions rq1.
comparative study on an c dataset.
how well does utango perform in comparison with the state of the art commituntangling approaches on an c dataset?
rq2.
comparative study on a java dataset.
how well does utango perform in comparison with the state of the art commituntangling approaches on a java dataset?
rq3.
within project analysis.
how well does utango perform when training and testing are done in the same project?
rq4.
sensitivity analysis.
how do the key features in utango affect its overall performance?
227esec fse november singapore singapore yi li shaohua wang and tien n. nguyen rq5.
code change embedding analysis.
what are the properties of the contextualized embeddings from our context aware graph based representation learning model for code changes?
the answer will show the contribution of code change embeddings.
.
datasets we have conducted our evaluation on two datasets.
the first one is a c dataset that has been used in the commit untangling work flexeme .
this c dataset contains tangled commits each has concerns in github projects.
we use this c dataset for all rqs except rq2.
in rq2 to evaluate utango on java and compare against the state of the art smartcommit working on java only we collected a new dataset by following the same process in that paper.
the java dataset contains 14k tangled commits in github projects.
the number of concerns in a commit is from .
.
experimental methodology .
.
rq1.
comparison with existing approaches on c dataset.
baselines.
we compared utango against the state of the art commituntangling approaches that work on c see more details in section barnett et al.
herzig et al.
flexeme and pdg cv pdg cv is a model introduced by the authors of flexeme via combining their pdg with confidence voting in herzig et al.
.
procedure.
we took all the commits in the c dataset and sorted them in the chronological order based on the creation time of the commit logs.
for utango we then used of the oldest commits for training of the next oldest ones for tuning and the latest of the commits for testing.
for the baselines all of them do not need training thus we ran them on the testing data.
we did not use the setting of leave one out by project testing on part of a project and training on other projects .
the reason is that after sorting in a chronological order we could not test on the oldest project due to no training data.
even with the 2nd 5th oldest ones there is no sufficient training data of the commits that came from the older projects and before the test commits in those projects.
we tuned utango with automl for the following key hyperparameters to have the best performance epoch size batch size learning rate .
.
.
.
vector length of word embeddings and its output .
we empirically set the context size of rq4 .
.
.
rq2.
comparison with existing approaches on java dataset.
baselines.
we compare utango with the state of the art untangling approach that works on java source code smartcommit and the baselines used in their paper base a rule based approach that puts all changes into one group base a rule based approach that puts the changes in each file into one group and base a rule based approach that considers only def use use use and same enclosing method relations .
procedure.
we used the same procedure tuning with automl and parameters as in rq1 on the java dataset.
the baselines do not need training thus we ran them on the testing data.
.
.
rq3.
within project analysis.
we randomly selected one of the largest projects from each dataset the commandline project in c and the netty project in java.
for each project we sorted all of thecommits in the chronological order based on commit time.
we split the commits into for training tuning and testing and using the older data for training and the newer data for tuning and testing.
we separately trained fine tuned and tested a model under study on the commits in each project.
.
.
rq4.
sensitivity analysis.
we built the variants of utango by removing its important components one at a time.
first we removed from utango the context by not using the context vector when computing the vector for a changed statement.
second we removed from utango the clone detection component and used the resulting clusters from the code change clustering learning model as the final results.
third we studied the impact of the size k of a context on the accuracy.
we also studied the impact of the data size.
we used the c dataset and the same setting procedure as in rq1.
.
.
rq5.
code change embedding analysis.
we use statistical p test to confirm refute the hypothesis that the changed statements in the same concerns are projected nearer to one another than the changed ones in different concerns in the vector space.
evaluation metrics.
we use two evaluation metrics.
first accuracyc is defined as the percentage of the changed statements that are labeled with a correct cluster concern in all the statements in a commit accuracyc changedstmtsw.correctclusters allchangedstmtsincommit.
note that a model might label the statements with a different permutation of cluster labels than the one in the ground truth.
for example we have five statements and three concerns.
a model can predict and the ground truth has .
a naive evaluation would give an accuracy of .
.
however a permutation of the labels for clusters would give indeed an accuracy of .
.
thus we use the hungarian algorithm to find the permutation that gives the maximum accuracy and use that for accuracyc.
in rq1 for the comparison with flexeme we also used another metric that was used in their paper which is similar to accuracycexcept that it considers all statements accuracya stmtsw.correctclusters allstmtsincommit.
we also consider all label permutations.
experimental results .
rq1.
comparative study on c dataset .
.
general results.
table shows the comparison on accuracyc when it was measured on the changed statements.
as seen utango improves barnett et al.
herzig et al.
pdg cv and flexeme in overallaccuracycby462.
.
.
and .
respectively.
table shows the results on accuracya.
whenaccuracyawas measured on all the statements changed and un changed in a commit the results for all models are higher because they have correct classifications for the changed statements by default.
we include accuracyafor the comparison purpose with flexeme as its authors used this metric in their paper .
as seen utango also improves barnett et al.
herzig et al.
pdg cv and flexeme by .
.
.
and9.
in overallaccuracya respectively.
utango s accuracies are consistently better than those of the baselines for all the projects with respect to different numbers of concerns in a commit cs or table .
some data points are unavailable since those commits are older in the chronological order and appear only in the training data but not in the testing data.
228utango untangling commits with context aware graph based code change clustering learning model esec fse november singapore singapore table rq1.
comparison on c dataset accuracyc barnett et al.
herzig et al.
pdg cv flexeme utango cs oa oa oa oa oa cl cm hf hu le na nj ni rs oa cl commandline cm commonmark hf hangfire hu humanizer le lean na nan cy nj newtonsoft.json ni ninject rs restsharp oa overall no avail data point.
table rq1.
comparison on c dataset accuracya barnett et al.
herzig et al.
pdg cv flexeme utango cs oa oa oa oa oa cl cm hf hu le na nj ni rs oa cl commandline cm commonmark hf hangfire hu humanizer le lean na nan cy nj newtonsoft.json ni ninject rs restsharp oa overall no avail data point.
a accuracyc b accuracya figure boxplots for results in table and table .
orange boxes concern data yellow boxes concern data figure shows the boxplots for both accuracycandaccuracya results in tables and .
the orange boxes are for the commits with two concerns while the yellow ones are for those with three concerns.
as seen the median accuracy of utango is higher than those of all baselines on both types of two and three concerns.
moreover the gap in between utango and the baselines in accuracycis larger than the gap between them in accuracyabecause all models have correct classifications by default for the changed statements which are many more than the changed statements.
.
.
accuracy results for concerns with different sizes.
to better understand utango s accuracy on concerns with different sizes figure accuracies for concerns with different numbers of statements in c dataset table rq2.
comparison on java dataset accuracyc base base base smartcommit utango sb es rj gu re du gh zx dr eb oa sb spring boot es elasticsearch rj rxjava gu guava re retrofit du dubbo gh ghidra zx zxing dr druid eb eventbus oa overall we collected all concerns with different numbers of changed statements and measured the accuracies for them.
as seen in figure accuracycvalues for the concerns of small sizes having changed statements are higher ranging from .
that is among all the changed statements utango correctly classified about of them into the correct clusters.
for the larger concerns accuracycdecreases as expected because it is more challenging to get correct classifications for more changed statements in a concern.
however accuracycdecreases gradually with the lowest value of .
note there are a few commits with the addition of large files of .3k lines.
despite challenges with large concerns utango correctly classified100 of all the changed statements for commits with up to statements.
flexeme fails to reach for those commits.
there are only commits that flexeme correctly classified all the changed statements and utango did not reach .
both models correctly classified of all the changed statements of commits.
training of utango on80 of c dataset takes .
hours and predicting on of c dataset takes .
.
seconds per commit.
.
rq2.
comparative study on java dataset .
.
general results.
table shows accuracycwhen it was measured on the changed statements for java code.
as seen on the java dataset utango improves base base base and smartcommit in overall accuracycby100.
.
.
and .
respectively.
utango s accuracies are consistently better than those of the baselines for all subject projects.
figure shows the boxplot foraccuracycresult in table .
as seen the median accuracy of utango is higher than those of the baselines.
229esec fse november singapore singapore yi li shaohua wang and tien n. nguyen figure boxplots for results in table figure accuracies for concerns with different numbers of statements in java dataset .
.
accuracy results for concerns with different sizes.
as seen in figure accuracycvalues for the concerns of smaller sizes having changed statements are higher ranging from .
that is among all the changed statements utango correctly classified of them into the correct clusters.
for larger concerns accuracycdecreases gradually.
even in the concerns with up to changed statements accuracycis .
the lowest accuracy is .
moreover it correctly classified of all the changed statements for commits in which smartcommit failed to reach .
there are commits that smartcommit correctly classified all the changed statements and utango did not reach .
both models correctly classified of all changed statements of commits.
training of utango on80 of the java dataset takes hours and predicting on of java dataset takes .
.
seconds per commit.
note the java dataset is much larger than the c dataset.
.
rq3.
within project analysis table shows utango s results in the within and cross project settings.
as seen with sufficient within project data accuracycin the within project setting is slightly better than that of the crossproject setting for both c and java projects.
this is expected since the changes in the same project could be more similar than the changes across projects.
thus utango works well in both withinand cross project settings for c and java projects.
.
rq4.
sensitivity analysis table shows the accuracy results when the key components in utango were removed.
as seen while both context and clone detection positively contribute to utango s accuracy the context plays a more important role as expected.
without the context vector table rq3.
results for within and cross project settings project within project cross project commandline c .
.
spring boot java .
.
table rq4.
impacts of key features on accuracy accuracyc concerns overall utango w o context .
.
.
utango w o clone detection .
.
.
utango .
.
.
table rq4.
impact of the number kof hops for context accuracyc concerns overall utango k .
.
.
utango k .
.
.
utango k .
.
.
utango k .
.
.
utango k .
.
.
utango full graph .
.
.
table impact of the size of training data splitting on c dataset accuracyc45 the accuracy decreases by .
.
and .
for the commits with two three and all concerns respectively.
without the clone detection the accuracy decreases by .
.
and .
for the commits with two three and all concerns respectively.
table shows the accuracy when we vary the size kof a context i.e.
the number of hops from the changed node .
as seen when the number of surrounding nodes of the changed node increases fromk the accuracy increases and reaches its peak at k .
as k the immediate surrounding nodes cannot capture well the relevant nodes for utango to distinguish the concerns of the changed statements.
with two hops from the changed node the context seems to sufficiently contain the crucial nodes w.r.t.
determining the concerns.
however as the size continues to increase the accuracy decreases.
when the entire graph is used as the context the accuracy is the lowest.
if more nodes are considered in the context more noises are added as more irrelevant data is used.
the trend is the same for the commits with two three or all concerns.
as seen in table the training data s size has impact on accuracy.
the more training data the higher the accuracy.
even reducing it from to utango s accuracy is still higher than flexeme s. .
rq5.
analysis on code change embeddings to study the projections of the changed statements in the same and different concerns we first randomly chose the commits with two or more clusters concerns and in one of the cluster concern there are at least two changed statements.
let us use cto denote that cluster concern.
we randomly chose two changed statements s1ands2inc.
we then randomly selected another changed statements3such thats3 cands3belongs to another cluster in the same commit with s1ands2.
we measured the distance d1 s1 s2 230utango untangling commits with context aware graph based code change clustering learning model esec fse november singapore singapore figure code change embedding visualization via t sne andd2 s1 s3 .
we repeated the process for all the commits satisfying the above conditions to get triples of s1 s2 s3 .
based on the population in our dataset the size of samples gives the confidence level of and the confidence interval of .
we used statistical p value to confirm our hypothesis h1 d1 s1 s2 d2 s1 s3 .
the null hypothesis is h0 d1 s1 s2 d2 s1 s3 .
when we set the significance level .
thep value is .
calculated on these samples .
in this case the p value is smaller than meaning the null hypothesis would be rejected at the level of .
.
thus our hypothesis h1 d1 s1 s2 d2 s1 s3 is confirmed.
that is the changed statements in the same concerns are projected nearer to one another than the changed statements in different concerns.
this result is an indication that our context aware graph based embeddings for code changes are helpful for utango in clustering the code changes via the embeddings into different concerns .
for illustration we randomly picked a commit in our above sample.
the commit contains three concerns in which one concern has changed statements another one has changed statements and the last one has changed statements.
we used the t distributed stochastic neighbor embedding t sne to visualize the embeddings of the changed statements in the vector space.
t sne is a statistical method for visualizing high dimensional data by giving each data point its projected location in a two dimensional vector space.
as seen in figure we marked the embeddings for the changed statements in each concern with a different color.
the correctly clustered changes are within the corresponding boundary for a concern.
despite that utango misclassified a few statements we can observe that the changes in each concern are projected to the nearby locations in the vector space.
this figure illustrates that the our code change embeddings facilitate our supervised learning clustering model to correctly cluster the changed statements.
.
discussions .
.
threats to validity.
our study is only on c and java projects.
our methodology is language independent.
the datasets of tangle commits were built artificially from atomic commits.
however this methodology was used in flexeme and herzig et al.
.
despite that we did not use the setting of leave one out by project our setting reflects the actual use of our tool in which all the commits in other projects that occurred before the current commit are used.
.
.
limitations.
utango is less accurate for the commits with too many changed statements or with too many concerns .
as with any data driven approaches it does not work well with little data.
we currently integrate only the code clone relations.we will explore other types of implicit relations e.g.
in event driven source code or other links as in smartcommit.
trade offs between utango and program analysis approaches without needing a threshold for clustering utango is more flexible in learning the boundaries among clusters than pa approaches which rely on explicit program dependencies for clustering.
despite needing no training data flexeme requires a median time of .
seconds to untangle a commit.
utango takes only .
.
seconds.
related work tangled commits have been reported by researchers to have negative impacts on software maintenance .
mining software repositories.
herzig et al.
combine confidence voting with agglomerative clustering.
each confidence voter is responsible for an important aspect e.g.
call graphs change couplings data dependencies and distance measures.
in kirinuki et al.
s if there is a commit mincluding the same changes as a past commit and other changes the commit mis called inclusive change and considered as tangled.
dias et al.
uses confidence voting on fine grained change events in an ide and partition them.
static analysis.
roover et al.
s approach builds pdg and computes the changes to the asts of the files in a commit.
it then groups these fine grained changes according to the slices through the pdg they belong to.
clusterchanges relates separate regions of change within a changeset of a commit by using static analysis to uncover relationships such as definitions and their uses present in these regions.
utango adapts multi version pdg from flexeme however we build the contextualized embeddings for the code changes and a model to learn to cluster rather than clustering using graph similarity on multi version pfg.
smartcommit uses a graph partition algorithm on code changes related via several types of links representing different purposes.
there are a rich literature on supervised hierarchical clustering .
the dissimilarity between cluster pairs is measured via a linkage function f .
learningfis performed by training the pairwise dissimilarity function to predict dissimilarity for all within and across cluster data pairs .
inutango supervised learning clustering is made with the loss function.
conclusion we present utango a ml based approach that learns to untangle the changes in a commit.
we develop a novel code change clustering learning model that learns to cluster the code changes represented by the embeddings into different groups with different concerns.
utango overcomes the key issue with the static analysis approaches in which the boundaries across concerns in a commit do not naturally map to clustering criteria.
our ml direction fits well with this problem thanks to a methodology from herzig et al.
and flexeme to collect the changes in the same concern and merge them to build tangled commits to form a training dataset.