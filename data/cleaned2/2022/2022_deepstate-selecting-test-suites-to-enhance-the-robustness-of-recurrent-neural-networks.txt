deepstate selecting test suites to enhance the robustness of recurrent neural networks zixi liu zxliu smail.nju.edu.cn state key laboratory for novel software technology nanjing university nanjing chinayang feng fengyang nju.edu.cn state key laboratory for novel software technology nanjing university nanjing china yining yin ynyin smail.nju.edu.cn state key laboratory for novel software technology nanjing university nanjing chinazhenyu chen zychen nju.edu.cn state key laboratory for novel software technology nanjing university nanjing china abstract deepneuralnetworks dnn haveachievedtremendoussuccess in various software applications.
however accompanied by outstanding effectiveness dnn driven software systems could alsoexhibit incorrect behaviors and result in some critical accidents andlosses.thetestingandoptimizationofdnn drivensoftware systemsrelyonalargenumberoflabeleddatathatoftenrequire many human efforts resulting in high test costs and low efficiency.
although plenty of coverage based criteria have been proposed to assistinthedataselectionofconvolutionalneuralnetworks itis difficulttoapplythemonrecurrentneuralnetwork rnn models due to the difference between the working nature.
inthispaper weproposeatestsuiteselectiontool deepstate towardstheparticularneuralnetworkstructuresofrnnmodels for reducing the data labeling and computation cost.
deepstate selectsdatabasedonastatefulperspectiveofrnn whichidentifies the possibly misclassified test by capturing the state changes of neurons in rnn models.
we further design a test selection method toenabletesterstoobtainatestsuitewithstrongfaultdetection andmodelimprovementcapabilityfromalargedataset.toevaluatedeepstate we conduct an extensive empirical study on popular datasets and prevalent rnn models containing image and textprocessingtasks.theexperimentalresultsdemonstratethat deepstate outperforms existingcoverage based techniques inselectingtestsregardingeffectivenessandtheinclusivenessofbug cases.meanwhile weobservethattheselecteddatacanimprove the robustness of rnn models effectively.
yang feng is the corresponding author.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation onthe firstpage.copyrights forcomponentsof thisworkowned byothersthan the author s mustbehonored.abstractingwithcreditispermitted.tocopyotherwise or republish topostonserversortoredistributetolists requirespriorspecificpermission and or a fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn ... .
concepts software and its engineering software testing and debugging.
keywords deep learning testing deep neural networks recurrent neural networks test selection acm reference format zixi liu yang feng yining yin and zhenyu chen.
.
deepstate selecting test suites to enhance the robustness of recurrent neural networks.in 44thinternationalconferenceonsoftwareengineering icse may pittsburgh pa usa.
acm new york ny usa pages.
introduction deep neural networks dnn has been widely adopted in many fields to assist in solving various tasks such as image classifica tion speech r ecognition and natural language processing etc.
although the dnn driven systems have made remarkable progress in many aspects they suffer from quality and reliabilityissues.theerroneousbehaviorsproducedbydnn drivensystemscouldcausesignificantlosses.forexample thernn drivendialoguesystem amazon ssmartspeakeralexacouldbefooledby somecornerinputcasesastomakescreepylaughs whichscares theelderlyandchildrenandaffectstheirlives .theerroneous behaviorsproducedbydnn drivensystemscouldleadtomisunderstanding threats to personal safety or even political conflicts .therefore thetestingandoptimizationofdnn driven systems have become an urgent yet challenging task.
the dnn driven systems are constructed upon the data driven programming paradigm which requires plenty of data with ground truth i.e.
labeled data for modeltraining and evaluation.
unfortunately collecting a high quality data set for building dnndrivensystemsrequiresplentyofhumaneffortstolabelthedata whichmakestheprocessexpensiveandtime consuming.itisinefficientandimpracticaltoemploythedatacollectedfromusage scenariosforthemodel stestingandoptimizationdirectly.becauseinamassivedataset onlyasmallamountofcases which cantriggerthesystem spotentialerrorsareespeciallycrucialfor testing the dnn driven systems.
furthermore due to the nature of ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa zixi liu yang feng yining yin and zhenyu chen dnnmodels optimizingthemvia retrainingwithenormousdata often costs plenty of time and computational resources.
inspiredbytheeffectivenessofcodecoverageinconventional software testing researchers have proposed several neuron coverage nc criteria to evaluate the testing adequacy andguidetestselection therebyreducingthelabelingcostandsaving computational resources.
recently the confidence based deepgini and the feature based prima have been proposed toprioritize tests.however mostofthe existingneuroncoverage criteriaaremainlydesignedforthefeedforwardneuralnetworks fnn such as convolution neural networks cnn and fullyconnectedneuralnetworks fc .thegeneralrationaleof thesecriteriaistocalculatethecoverageratebyseparatelycalculating the activation of each layer and neuron.
howeve r it is difficult totransferthemtornnwhichisatypicalfeedbackneuralnetwork due to the differences in working nature and network structures between rnn and fnn.
only a few test criteria have been proposed for rnn models which are calculated by analyzing the neuron states when the model is processing different test cases.
theexistingresearchoftenregardsthehiddenstateateachiteration as the neuron layer in the fnn.
yet such criteria are specially designed for detecting adversarial examples that are manually generated with unrealistic transformations to attack the trained models.
moreover the current research on neuron coverage is still in the controversial stage and there are plenty of discussions on their effectiveness and usage scenarios .
different from these criteria in this paper we propose a test selectiontool namely deepstate especiallyforidentifyingtestcases thatmaytriggererrorsinthernn drivensystems.
deepstate is designed upon a stateful perspective rather than the neuron coverage of rnn.
we measure rnn s uncertainty for a given test and analyzetheoutputbehaviorsbyexpandingthehiddeninternalstate of the rnn according to time steps.
specifically we first capture thehiddenstatechangesofrnnneuronsovertimesteps.then we design two metrics i.e.
the changing rate and changing trend of the hidden states to analyze the output behaviors of rnn models.
to select test suites with a high bug detection rate from massive data we design a test selection method based on the changing rate and changing trend of hidden states.
we implement deepstate and evaluate it on four rnn based systemscoveringimageclassificationandtextclassificationtasks.
besides wevalidateitseffectivenessonavarietyofrnnmodels includingthemostcommonlyusedlstm bilstm andgru.to evaluate the effectiveness of deepstate we compared with random and existing neuron coverage based selection methodologies.
the experiment results show that deepstate can effectively select test cases with a high bug detection rate which can help totestrnnmodelsandfindpotentialdefects.besides weevaluate deepstate seffectivenessbycalculatingtheinclusivenessoftheselected testsuites.
the resultsunder multiple selectionratios show that deepstate can filter out the tests that can reveal potential flaws in rnn models.
further we evaluate its capability of improving the quality of rnn.
we employ the selected data to retrain the originalrnnmodelandrecordtheaccuracyimprovements.the experiment resultsindicate that deepstate can improvethe rnn models by retraining the model with the selected data.
in summary the main contributions of this paper are as follows.
approach.
we present an approach to model the internal states of rnn into a stateful perspective that can assist usin analyzing the behaviors of rnn models.
based on the statefulperspective wefurtherproposethemetricsofthe changing rate and changing trend to reflect the behaviors of rnn s hidden states.
tool.basedonthestatefulperspective wedesignandimplementatestselectiontool namely deepstate toassistin identifying a smallportion of tests withhigh bug detection rate from a massive dataset efficiently.
the source code of deepstate is publicly available1.
study.weevaluate deepstate onthetestselectionofimage andtextclassification.the experimentresultsdemonstrate that the test suite selected by deepstate has a high bug detectionrateandcanbeappliedtoretrainrnnstoimprove the robustness of the models.
background inthissection weintroducethepreliminaryknowledgeofrnn ex istingneuralnetworktestcriteria especiallythoseforrnnmodels and conventional test selection techniques.
.
recurrent neural networks theneuralnetworkscanbedividedintofeedforwardneuralnetworks and feedback neural networks based on their information propagation methods .
the feedforward neural networks suchasconvolutionalneuralnetwork cnn andfullyconnected neural networks fc are composed of multiple layers of neurons.
the neurons in each layer receive the output of the previouslayerandoutputtothenext.onthecontrary theneurons inthefeedbackneuralnetworkcannotonlyreceivesignalsfrom other neurons but also receive their own feedback signals.
therecurrentneuralnetwork rnn isatypicalkindoffeedback neuralnetworks .
fig.1illustratesa generalstructureofrnn.
withthernnunfolded eachneuroninrnn shiddenlayerupdates its state and weight iteratively over time steps .
the hidden states output htat time step tis calculated upon current input xtas well as ht 1from the previous time step and then passed tothesoftmaxlayertooutputtheprediction yt .whilernn can output the prediction result at each time step it can regard the outputatthelasttimestepasthefinalpredictionofrnn.thelstm long short term memory model is the most widely used optimized rnn which is primarily designed to solve the problem of gradient disappearance and gradient explosion in the training processoflongsequences.thelstmmodeladdsaforgetgateto thehiddenlayertodiscardunimportantinformation soitcanbetterexpressthelong termandshort termdependencelocally .rnn is particularly effective in processing time series data i.e.
a string ofinterrelateddata suchasimagedescription textgeneration text classification and other tasks.
.
neuron coverage criteria neuron coverage nc was first proposed by pei et al.
t o findinputsfordlsystemsthatcantriggerdifferentialbehaviors.
analogoustothesourcecodecoverageofconventionalsoftware authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
deepstate selecting test suites to enhance the robustness of recurrent neural networks icse may pittsburgh pa usa 844state vectoroutput inputsoftmax h h h h h unfoldsoftmax softmax time step figure a general rnn structure.
testing theneuroncoverageisusedtojudgewhethertheoutput value of each neuron after passing the activation function exceeds a certain threshold k and consider the neurons that exceed the threshold as activated i.e.
covered.
the rate of nc k for a test is defined as nc k covered neurons total neurons recently some coverage criteria have been proposed especially for rnn such as deepstellar testrnn and rnntest .deepstellar calculatesthebasicstatecoverage bscov and the basic transition coverage btcov for quantitative analysis of rnns.
bscovis designed tomeasurehow thoroughly the test inputs tcoverthemajorfunctionregionvisitedwhiletraining.the abstract states visited by the training inputs mand the test inputs tare denoted by smand st and the bscov is defined as bscov t m st sm sm btcovtargetstheabstracttransitionsactivatedbyvariousinput sequences.
btcov compares the abstract transformation of the hidden state of the rnn model during training and testing phases whichare denotedas tand m respectively.thus thebtcov is defined as btcov t m t m m testrnn proposed step wise coverage sc for evaluatingthechangesintheoutputofthehiddenlayersinrnnsoverthe timestep.thechangeofthehiddenvectorinadjacenttimesteps is defined as follows h t h t h t h t h t where h trepresents the sum of all positive components in thehiddenstate hattimestep t and h trepresentsthesumof all negative components in the hidden state hat time step t. then sc is defined as sc h t vsc t ... n h t t ... n in the formula vscis a customizable threshold.
in general the maximumvalue thatcan bereached duringtraining isselectedas the threshold for testing.
rnn test defines the hidden state coverage hscov as the ratio of the hidden states that achieve the maximum value during testing.
assume hdenotes all the hidden states of an rnnmodelofgiveninputs whichisafour dimensionalmatrixofshape t l b e where t l b e are the number of time steps layers batches and hidden units respectively.
the hscov is defined as hscov e e h h h e max h h .
test data selection thetestdataselectiontechniqueisoriginallydesignedtoselecttest casesintheregressiontestingscenarioandsavetheexecutiontime cost and resources.
it selects the tests that are deemed necessary to verifythemodifiedsoftwarefromtheexistingtestset.suppose p is a procedure or program p primeis a modified version of p andtis a set of tests a test suite created to test p. tests that are valid for p mayberedundantfor p prime becausetheirexecutiontrajectorydoes not go through the modified code in p prime.
the process of identifying themodifiedpartoftheexecutiontrajectorythrough p primeiscalled test selection.
twoprimarycoverage basedtestcasesprioritizationtechniques areknownasthecoverage totalmethod ctm andthecoverageadditional method cam .
coverage total method ctm it is regarded as the next best strategy.ctmtakesnoconsiderationontheselectedtests.
it always selects the test with the highest coverage from the candidate test suite.
coverage additionalmethod cam it is a selection strategydrivenbytheadditionalgreedyalgorithm.camdynamicallyadjuststhenextselectedtestbasedontheselectedtests.
italwaysselectsthetestthatcancovermostuncoveredcode structures from the candidate test suite.
inspired by the test selection in regression testing in this paper we propose deepstate to select a subset from massive data for rnn models.
approach wemodelthernninternalstatechangesintoastatefulview.based on this stateful view we design and implement a tool namely deepstate toselectasubsetofdatafromalarge unlabeled dataset forthernnmodel.asshowninfig.
deepstate firstunfoldsthe rnn model according to the time steps and captures the predicted label sequence based on a stateful view as discussed in section .
.
the label sequence can represent the internal state of the rnn hidden layers when predicting a given input data.
then deepstate calculates the changing rate and the changing trend which are presentedinsection3.
.thesemetricsareemployedtoevaluate the uncertainty and internal state transition of the rnn when processing theinput data.
we introduce the detailedimplementation ofdeepstate selectingtestcasesinsection3.
.finally section3.
discusses how to enhance rnn quality with deepstate.
.
the stateful view of rnn rnns are often employed to process the sequence data i.e.
a series of interdependent data streams because the neurons in rnns can dynamically update the hidden states based on the received information over time.
suppose an rnn is designed for processing atextclassificationtaskandtheinputisasequencedata x xn wherexistheinputdomain and ndenotesthelengthoftheinput authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa zixi liu yang feng yining yin and zhenyu chen label sequence selected datamassive test seta stateful view of rnn selecting test suites changing rate cr changing trend ct uncertainty similarity original test data unfolded by time stepstest case rnn units1332213 223 3t1 t3 t2 t4 lowhigh sortingct distance ?
test1 test2test1 or test2yes noh h h h h softmax softmax softmaxsame cr test1 test2 ...generated test data figure overview of deepstate.
sequence.the i thwordintheinputsentence xisrepresentedas xi x. for a given input sentence x the rnn model iteratively receives each element in x i.e.
xtis received at time step t and maintainstheinternationalhiddenstatevector s sn.specifically the time step represents the number of iterations and is equal to n. at time step t the hidden state output is denoted as st s andst isupdateduponboth xtandst .then thernnmodeloutputthe classification prediction result ytthrough an activation function f .generally inmulti classificationtasks thesoftmaxfunction isappliedtoreflectthehigh dimensionalhiddenstatevectorinto the probability distribution vector for each label.
from the stateful view of rnn we analyze the rnn model s predictionuncertaintyandbehaviorfeaturesuponthehiddenstate ateachtimestep.becausethehigh dimensionalhiddenstatevector sisdifficulttoanalyze weapplythecorrespondingpredictedlabel y toreflecttheinformationofthehiddenstate.fortwoadjacenttime stepstandt the change of hidden state can be regarded as the corresponding label ytandyt .
note that in the earlier time steps whenthernnmodelhasnotyetbeenwelltrained itisnaturalfor it to produce different prediction results and has low confidencefor the output.
as the received information increases the rnn model sconfidenceforthepredictionresultsalsoincreases andthe prediction results at later time steps are supposed to be consistent.
table1 theexampleofhiddenstateoutputsoftwoimages in the mnist dataset.
id figure label sequence changes output toselectteststhatmaytriggerthepotentialerroneousbehaviors of rnn models we weigh the cases that cause a high changing rateofthernnmodel spredictionoveralltimesteps.forexample table shows a lstm model s prediction process of two similar imagesfromthemnist dataset.thecorrectlabelofthesetwo images is nine but the rnn mispredicted the second image id to three and the first picture id is predicted correctly.
sincethe input data is a 28x28 image the data is divided into time steps i.e.
each time step inputs a row of pixels and input into thernn model.
we capture the hidden state output at each time step and the corresponding predicted label.
we list the prediction labels with confidence greater than .
at each time step which is shown in the label sequence column in table .
the label sequence for thefirstfigurehas3changes overthetimesteps.
similarly the label sequence corresponding to the second figure hastwochanges .althoughthechanges numberof the second label sequence is one less than that of the first one the firstsequenceislonger andthesecondsequencehasachangeat the back of the sequence which indicates the uncertainty of rnn.
.
metric computation basedontheabovestatefulview weemploythepredictionlabelsequencetorepresentthehiddenstateoutputstatusofrnnprocessing a given input data.
then we propose two analysis metrics i.e.
the changing rate and changing trend of hidden states.
the changingratedescribes theuncertaintydegreeofthernn model for agiven input testcase while thechanging trend describesthe similarityoftheoutputstateofrnnprocessingdifferenttestcases.
definition .
label sequence .
for a given input x a label sequence is a list seq x c y1 y2 ... yn where cis the confidence threshold.
at the i time step the rnn model s hidden state vectorcanberepresentedasthecorrespondingpredictedlabel yi throughrnn sactivationfunction.wemaintainthepredictedlabel with confidence greater than cin the label sequence.
and nis thenumberofoutputlabelswithconfidencegreaterthan cinthe output of all time steps i.e.
n time steps .
based on the label sequence we measure the uncertainty of thernnforagiveninputbycalculatingthechangingrateofthe predicted labels.
definition3.
changingrate .
thechangingrateindicatesthe ratio of thenumber of adjacent labelsthat are not thesame as the total sequence length in a label sequence seq x c .
the changing rate cr seq x c can be defined as cr seq x c count yt yt n becausethepredictionsateachtimestepmaynothaveahigh degreeofconfidence especiallyinthefirstperiodoftime wecalculate the changing rate on the label sequence where each label is predicted with a confidence higher than a given threshold.
wecalculatetheweightedchangingrate consideringthatasthe time step increases rnn can receive more information and the basisofthepredictionresultissufficient.thus thelaterchanges authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
deepstate selecting test suites to enhance the robustness of recurrent neural networks icse may pittsburgh pa usa in the seq x c should be more weighted.
therefore the changing rate we applied to select tests is defined as follows cr seq x c count yt yt weig ht t weig ht t 2a weig ht t t2 where t ... n .
2b we calculate the changing trend of the internal state of the rnn by mapping the specific transition between two states to the predicted label s transition.
specifically the changing trend can be expressed as a set of conversions ctof two adjacent labels yt yt in the label sequence seq x c .
definition3.
changingtrend .
foralabelsequence seq x c the changing trend is a set ct seq x c of tuples composed of adjacent labels yt yt in chronological order.
for example the label sequence corresponding to the test id in table is and the corresponding changing trend is .
similarly the change sequence of the test id is and the changing trend is .
to evaluate the similarity of two changing trends we apply the jaccardsimilaritycoefficient forevaluation.thesimilarityis calculated as follows j ct1 ct2 ct1 ct2 ct1 ct2 where ct1andct2representthechangingtrendofthernncorrespondingtotwodifferenttestcases.thevaluerangeof j ct1 ct2 is .when j ct1 ct2 isgreaterthanagiventhreshold itindicates these two tests are similar to each other and then we only keep one of them in the selection.
.
rnn test suite selection algorithm deepstate weightsthetestcaseswithahighchangingrateoflabel sequence especially the predicted label changes at a later time step.specifically deepstate firstsortseachtestcaseinthedataset according to the changing rate which indicates the uncertainty degree of rnn for a given test.
we believe the uncertainty degree indicates the probability of the rnn model triggering bugs.
yet it is not suitable to select many tests with the same changing rate directly because the selected test suites may not be representative.
thus deepstate selectstestswithdifferentstates changingtrends as the representative from the tests with the same changing rate.
specifically algorithm presents the process of deepstate selecting test suites for testing and optimizing the rnn models.for a given rnn model mto be tested and a massive data set d deepstate firstmakesthernnexecutealltestdatainturnandcalculatesthechangingrate crandchangingtrend ctcorresponding toeachdata line1 .second deepstate sortsalltestsinreverse orderaccordingtothechangingrate line8 andsetsasimilarity threshold for the changing trend line .
third for the test case set in dsorted by changing rate deepstate traverses each test case in turn line .
if the changing rates of the two test cases are different both of them are selected line .
for tests with the same changing rate we calculate the changing trend similarity between them line .
both tests are selected if the distance is smaller than the threshold line .
otherwise only one testisselected.finally wecheckthelasttest dn d.ifthechanging ratecris different from that of dn and it is not selected then we add it to dataselected line .
algorithm selecting test suites input d the original data set input m the tested rnn model output dataselected the selected data 1i 2j 3whilei n 1do 4di.cr getchangerate di m 5di.ct getchangetrend di m 6i i 7end 8d reverse sortby d.cr 9 setthreshold if the distance between two cts is greater than then we keep one of them.
10whilej ndo 11dataselected .add dj 12ifdj .cr dj.crthen j j 14else k j whiledk.cr dj.crandk n 1do dis jaccarddis dj.ct dk.ct ifdis then dataselected .add dk end k k end j k 24end 25end 26ifdn.cr dn .cranddn dataselected then 27dataselected .add dn 28end 29returndataselected arunningexample.
assumethatwehavesixtests a b c d e andfas well as an rnn with five time steps.
the rnn modelis designedtoclassifytestsintofourclasses i.e.
thelabelrangesfrom 1to4.table2showspredictionlabelsequences changingrates and changing trends of some tests.
assume that the distance threshold for judging whether the changing trends of two tests are similar is .
i.e.
.
.
these test cases are sorted into a descending order upon the value of the changing rate.
deepstate first selects test caseabecause it has the highest changing rate and no other case has the same changing rate value.
next because the changing rate valuesoftestcases b c anddarethesame deepstate firstselects caseb andthencalculatethejaccarddistanceofchangingtrend between bandc whichis1 .
.because1 7islessthan we expectthat althoughthesetwo testcaseshave thesamechanging rate they trigger different states of the rnn so deepstate selects bothbandc.however becausethejaccarddistanceofchanging trend between canddis .
which is greater than deepstate passes d. similarly deepstate only selects one case between eandfbecause the jaccard distance of changing trend between them is .
.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa zixi liu yang feng yining yin and zhenyu chen insummary forthisexampleintable2 deepstate selecttest cases a b c e asitsoutput.iftherequirementistoselectmore than4testcases then deepstate selectsthetestcase dandfin turn.becausetheinternalstatebehaviorsofrnnwhenprocessing testscandd oreandf areverysimilar thereisnoneedtoapply both tests for testing or optimization.
table an example to show how deepstate selects tests.
testlabel seq.
changing rate changing trend a b c d e f j b c j c d j e f .
enhancing rnn with deepstate to improve the practical performance of modern rnn driven software applications a general approach is to retrain the rnn model periodicallywiththedatacollectedintheusagescenarios.theprocessoftenrequiresthedatatobelabeledproperly however data labeling is an expensive and time consuming task.
moreover retrainingthernnmodelswithalargeamountofdataisalsoawaste oftimeandresources.
deepstate providesanautomaticsolution to this problem.
it is designed for selecting a subset from a large unlabeled dataset.theselecteddatahastheabilitytodiscoverpotential defects of the rnn which can activate the state transitions of different rnns ensuring the adequacy of the retraining data.
deepstate allows us to find and label as many tests that may triggerrnn serroneousbehaviorsaspossibleinalimitedtimebudget.
we observe that the tests selected by deepstate are more effectiveinenhancingrnnthanthetestsselectedbycoverage based selection techniques.
experiment design wehaveconductedextensiveexperimentstoevaluatetheperformanceof deepstate .thissectionintroducestheexperimentsettingsandtheresearchquestions.toconducttheexperiments we implement deepstate upon python .
.
and keras with tensorflow .
.
.
all experiments are performed on a ubuntu .
.3ltsserverwithteslav100 sxm2 one10 coreprocessor with .50ghz and 32gb physical memory.
.
datasets and rnn models asshownintable3 weexperiment deepstate withimageclassificationandtextclassificationmodels.foreachdatatype weemploytwowidely useddatasets andweappliedeachdatasetontwornnmodelstoensurethegeneralityoftheexperimentresults.thus we haveeightcombinationsofdatasetandmodelthatcoverpopular rnn variants.the mnist dataset is for handwritten digits recognition containing60 000trainingimagesand 000testingimages.
the fashion datasetisadatasetofzalando sarticleimagesconsistingof60 000trainingimagesand10 000testingimages.snips natural language understanding benchmark is a dataset of over crowdsourced queries distributed among different user intents.
agnews dataset is collected by more than millionnewsarticles whicharedividedintofourclasses i.e.
world sports business and sci tec.
the total number of training samples is and the testing is .
the lstm long short term memory model is the most widely used optimized rnn variant.
it employs forget gates to recordthelong termmemoryandshort termmemoryoutputofthe rnn .
lstm can effectively solve the problem of gradientdisappearanceandgradientexplosionofthernnmodel.the bilstm bidirectional longshort term memory is thebidirectionallstmmodel combiningtheforwardlstmandbackward lstm .
the gru gated recurrent unit model is another variantoflstm.itcombinestheforgetgateandinputgateoflstm into a single gate which is regarded as the reset gate .
table the details of subject datasets and rnn models.
datasetrnn modelsstate vec.
shape trainable parametersdataset description mnistlstm handwritten numbers bilstm fashionlstm zalando s article images classes gru snipsbilstm intent recognition classes gru agnewslstm news classification classes bilstm .
test data collection to ensure that the experiment data can reflect the realistic data mutation in the usage scenarios we employ widely used benigndatamutationoperatorsratherthanadversarialexamplegenera tion techniques to augment test data.
specifically we adjust the parametersoftheseoperatorsandlimitthemutationextenttobe lessthan5 .thus theaugmenteddatamaintainsthesamelabel as the original data.
for each image classification model we generate the data by adding perturbations with existing transformation techniques includingcontrast brightness shift rotation scaling and shearing.
all the implementation of image transformation is based on the keras imagedatagenerator tool .
for each figureintheoriginaltestset wegeneratethecorrespondingfigure with random parameters of multiple operators.
for the text data wegeneratetheaugmenteddatabyaddingperturbationswithexisting transformation techniques including synonym replacement back translation word insertion and abstract summarization which is a data synthesis method for paragraphs.
the textual transitions are implemented based on an open source tool called nlpaug .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
deepstate selecting test suites to enhance the robustness of recurrent neural networks icse may pittsburgh pa usa .
baseline approaches wecompare deepstate withothermethodsbasedonneuroncoverageasintroducedinsection2.
.thenwesortthetestsbyapplying ctm and cam as introduced in section .
and then select the tests to satisfy the selection ratio requirement.
we apply both cam and ctm strategies to sort the nc and sc testrnn and select the corresponding test cases.
we apply hscov rnntest withthecamselectionstrategy.hscovcan onlyindicatewhichrnnhiddenneuronisactivatedforeachgiven input so it cannot be selected after sorting by ctm.
we apply bscovandbtcov deepstellar withthectmselectionstrategy.
thecriteriaproposedindeepstellarcannotbeselectedbasedon cam because it converts the output state of the hidden layer into an abstract model and calculates the coverage.
in addition we also employarandomselectionmethodasthebaseline i.e.
randomly selecting a fixed proportion of test cases from the candidate set.
.
research questions deepstate is designed for facilitating testers of rnn driven systems to quickly select tests that can trigger the potential erroneous behaviors andeffectively enhance therobustness.
to this end we empirically evaluate its performance based on the following three research questions rq .
rq1.
effectiveness can the dataset selected by deepstate detect more defects than neuron coverage based methods?
similartothetestcaseselectiontechnologyintraditionalsoftwaretesting thegoaloftestcaseselectionfordeeplearningmodels is also to filter out cases that can trigger potential defects.
we provide answers to rq1 by evaluating the bug detection rate of the test data set selected by deepstate and other baseline approaches.
to generate the candidate dataset we apply the benign augmentation methods described in section .
on the original test set dtest.theaugmenteddataisdenotedas d prime test whichhasthesame sizeas dtest.werandomlyselect30 sizeofthedatafrom dtest andd prime test respectively and then merge them together to form a candidate test set dselect.
thus the size of dselectis of the size of dtest.
to alleviate the potential bias we follow the above step and repeat times to randomly sample candidate sets denotedas t1 t2 ... t30.foreachmodelandtestsuite weapply all the selection techniques to select test suites from dselect and we record the corresponding bug detection rate when the selection ratior .weemploy titodenotethecandidateset and ti bugto denote the bug cases in ti then the bug detection rate can be calculated as follows bug detection rate ti ti bug ti rq2.inclusiveness candeepstate filteroutthebugtestcases in the candidate test data set?
in rq2 we investigate the extent to which deepstate selects tests that can reveal potential flaws in rnn models.
referring tothe evaluation metric of inclusiveness proposed by rothermel et al.
inclusiveness can measure the capability of regression test selectionmethodsinchoosingmodification revealingtestsfromthecandidateset.considering deepstate isdesignedtoselectasubset for manual labeling rather than for regression testing we adapt thenotionofinclusivenesstofittheperformanceevaluationofourapplication scenario which measures the capability of selecting bug revealingtestsfromthe candidate set.specifically wedenote the original test set as dtest the selected test set as dselect and denotetheteststhatcandetecterroneousbehaviorsinthesetwo sets as dtest bug anddselect bug respectively.
the inclusiveness can be calculated as follows inclusiveness dselect bug dtest bug rq3.guidance candeepstate guidetheretrainingofanrnn to improve its accuracy?
theoretically the selectedtest suiteswith ahigh bugdetection rate can be applied to optimize the rnn models.
thus we propose rq3 to evaluate whether the selected data can be applied for retraining and further enhance the robustness of rnn models.
toanswerrq3 wegenerateacandidatedataset dselectcontainingaugmentedtrainingdataandtheoriginaltrainingdata.tokeep dselectdiverse werandomlyselect10 dtrainasthedataset dpart andkeepitseparatefromtheoriginaltrainingprocess.then we generatetheaugmenteddata d prime partbasedon dpart.finally dselect iscomposedof d prime part dpartandpartof dtrain andtheaugmented datad prime partis kept to account for of dselect.
we employ each selection technique to select data from dselectto retrain the original model.
to demonstrate that improvements in the accuracy are not influenced by inconsistencies in the data we verify the model s accuracy improvement on both augmented test set d prime testand the mixed test set dmix augmented test datad prime test originaltestdata dtest respectively.weconsiderthat the robustness of an rnn model is improved when the accuracy of the retrained model on the original test set and the augmented test set both improved compared to the original model.
result analysis this section presents the experiment result and then analyzes the performance of our approach.
.
answer to rq1 effectiveness the average bug detection rate of time experiments corresponding to and of selection rates is shown in table .
fig.
showsthebugdetectionrateofeachselectionmethodwhenthese lectionratioissetto10 .comparedwithbaselines deepstate has achieved thehighest bugdetectionrate underdifferent selectionratios which indicates deepstate can effectively can help rnn models to detect more potential defects under given resource constraints.
among these four data sets mnist and fashion are both imagedata whilesnipsandagnewsaretextdata.duetotheshort originaltextlengthandlimitedtransformationmethods thetext classificationmodelshavearelativelyhighaccuracyscoreonthe augmented text data set so the bug detection rate is lower than that of the other two models.
compared with random selection strategies hscov cam sc cam nc cam and deepstate can select tests with a higher bug detection rate which indicates that these methods are moreeffective than random selection.
among them the bug detectionrateofthetestcasesselectedby deepstate achievesthehighest.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa zixi liu yang feng yining yin and zhenyu chen 60btcov ctm bscov ctm nc ctm nc cam sc ctm sc cam hscov cam randomdeepstatemnist lstm 60fashion lstm 30snips bilstm 50agnews lstm 60btcov ctm bscov ctm nc ctm nc cam sc ctm sc cam hscov cam randomdeepstatemnist bilstm 60fashion gru 30snips gru 50agnews bilstm figure the bug detection rate of different selection methods with selected tests.
thisisbecausethecamstrategycanachievethemaximumcoveragewithasmallnumberoftests leadingtosomeselectedtests being invalid.
meanwhile in fact it is difficult to activate neurons attheearliertimestep.therefore basedontheselectionstrategyof camandcoverage onlyasmallpartofthetestswithdiversitycan be selected.
from the perspective of selection strategy the cam generally can select more bug cases than the ctm.
this is because thecamcandeterminethedatatobeselectednextbasedonthe selectedresultstoensurethatthenumberofactivatedneuronsis as large as possible.
however ctm directly sorts and selects accordingtothecoveragerate.itislikelythatarelativelysimilartestcaseisselected whichcausesthebugdetectionratetoberelatively low.therefore comparedwiththerandomselectionstrategy the selectedtestsetbasedonctmhasarelativelylowerbugdetection rate while the tests selected based on cam generally achieve a higher bug detection rate.
table4 thebugdetectionrateof10 and20 selectedtests.
model sel ran.hscov sc sc nc nc bscov btcov deep cam cam ctm cam ctm ctm ctm statemnistlstm10 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
bilstm10 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.63fashionlstm10 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
gru10 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.56snipsbilstm10 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
gru10 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.25agnewslstm10 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
bilstm10 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
answer to rq2 inclusiveness foreachmodelanddataset weevaluatetheinclusivenessofthe selecteddatasetwhentheselectionratio k ... .asshowninfig.
thex axisofeachlinechartrepresentstheselectionratio kfromthecandidatedataset andthey axisrepresents theinclusivenessoftheselecteddataset.withtheselectionratio k increasing theinclusivenessofthetestdataselectedbyallmethodsis improved.
among them deepstate outperforms other selection methods which indicates that deepstate can select more bug test cases under the same select proportion.
in most combinations of models and data sets the inclusiveness of the test suite selected by hscov cam is better than that of random.
we can safely conclude that hscov is more effective than randomasaselectionstrategy butitisnotaseffectiveas deepstate .
the effects of sc cam and nc cam are similar to those of randomselection.
thismay be becausethe maximumcoverageis reachedquicklywhenapplyingthecamselectstrategy andthe remaining partkeeps thesame effect asrandom selection.
forthe snips dataset it seems that the inclusiveness of all the methodsexcept deepstate are similar to random selection.
this may be due to the small size of the snips data set and the short sentencelength.
meanwhile input sequence lengths in the snips datasetare inconsistent which may cause some methods that calculate coverage based on the output of the rnn hidden layer to be valid.
.
answer to rq3 guidance table shows the accuracy of all models after retraining with the data selected by all methods.
each grid represents the accuracy score of the model on the mixed test set afterretraining.
the data inparenthesesrepresentstheaccuracyimprovementonthetestsetcomparedtotheoriginalmodel.additionally wecalculatetheaccu racyimprovementafterretrainingwithallthedatainthecandidate set as reference results.
as can be seen from table in all models anddata sets compared torandomly selectingdata forretraining thedataselectedby deepstate cangreatlyimprovetheaccuracy of the rnn models.
in most combinations of models and data sets the accuracy improvement of deepstate s selection method exceeds of selecting all the data for retraining.
we can safely concludethattherobustnessofrnncanbegreatlyimprovedby retrainingwithasmallamountofdataselectedthrough deepstate .
the effectof deepstate on mnist blstmis slightly worsethan hscov cam .
this is because the bidirectional neural network hasmoreneuronsthantheotherrnnmodelsandrequiresmore authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
deepstate selecting test suites to enhance the robustness of recurrent neural networks icse may pittsburgh pa usa selection rate0102030405060inclusivenessrandom deepstate rnntest hscov cam deepstellar bscov ctm deepstellar btcov ctm testrnn sc ctm testrnn sc cam nc ctm nc cam a mnist lstm0 selection rate01020304050inclusivenessrandom deepstate rnntest hscov cam deepstellar bscov ctm deepstellar btcov ctm testrnn sc ctm testrnn sc cam nc ctm nc cam b fashion lstm0 selection rate0102030405060inclusivenessrandom deepstate rnntest hscov cam deepstellar bscov ctm deepstellar btcov ctm testrnn sc ctm testrnn sc cam nc ctm nc cam c snips bilstm0 selection rate010203040506070inclusivenessrandom deepstate rnntest hscov cam deepstellar bscov ctm deepstellar btcov ctm testrnn sc ctm testrnn sc cam nc ctm nc cam d agnews lstm selection rate01020304050inclusivenessrandom deepstate rnntest hscov cam deepstellar bscov ctm deepstellar btcov ctm testrnn sc ctm testrnn sc cam nc ctm nc cam e mnist bilstm0 selection rate01020304050inclusivenessrandom deepstate rnntest hscov cam deepstellar bscov ctm deepstellar btcov ctm testrnn sc ctm testrnn sc cam nc ctm nc cam f fashion gru0 selection rate0102030405060inclusivenessrandom deepstate rnntest hscov cam deepstellar bscov ctm deepstellar btcov ctm testrnn sc ctm testrnn sc cam nc ctm nc cam g snips gru0 selection rate0102030405060inclusivenessrandom deepstate rnntest hscov cam deepstellar bscov ctm deepstellar btcov ctm testrnn sc ctm testrnn sc cam nc ctm nc cam h agnews bilstm figure the inclusiveness of different selection methods.
testcasestomaximizethehscovcoverage.theexperimentresults showthatastheproportionofdataselectionincreases theeffectof deepstate cangradually outperform thehscov cam selection method.
besides although the retraining effect of sc ctm onsnips blstm is slightly higher than that of deepstate it is less effectivethan deepstate onothermodels whichindicatesthatthe strategy of sc ctm is not stable and effective.
for the text classification tasks the accuracy of the original model on the augmented test set is relatively high and thus theaccuracy improvement after retraining is also limited compared withtheimageclassificationmodels.theaccuracyimprovement of retraining with all candidate data may be less than selectingpart of the data for retraining.
this may be because there are anamount of original data in the candidate set which leads to the model not learning the features of the augmented data well during retraining.
in addition the features of text augmentation are not as obvious as image transformation which may affect the learning and optimization of rnn models.
selection rate0.
.
.
.
.25acc imp aug random deepstate rnntest hscov cam deepstellar bscov ctm deepstellar btcov ctm testrnn sc ctm testrnn sc cam nc ctm nc cam a the augmented test set.
selection rate0.
.
.
.
.
.12acc imp aug random deepstate rnntest hscov cam deepstellar bscov ctm deepstellar btcov ctm testrnn sc ctm testrnn sc cam nc ctm nc cam b the mix test set.
figure5 themnist lstm saccuracyimprovementonaug mented and mixed test set with different selection rate.
in our experiments we evaluated the accuracy improvement after retraining under the selection ratio from to .
since most modelshave thesame trendofaccuracy afterretraining weshow theaccuracyofonemodel mnist lstm underdifferentselectionratios asdepictedinfig.
.todemonstratetheimprovementsin theaccuracyarenotinfluencedbyinconsistenciesinthedata we evaluated the retrained model on both the augmented test set and themixtestset.theaccuracyimprovementsontheaugmentedtest set are greater than it on the mix test set.
the accuracy improvementsonthemixedtestsethaveshownthattheimprovementof the retrained rnn model does not result from data inconsistency.
discussion this section discusses the comparison with existing nc guided test selection techniques application scenarios of deepstate and threats to validity.
.
comparison with nc guided methods by comparing deepstate with other existing test case selection methodsbasedoncoveragecriterion wedemonstratethat deepstate is more effective than other existing methods.
the selected testshavethecapabilityofdetectingbugsandcanbeemployedtoen hance the robustness of the rnn models.
for a given test case deepstate evaluates the uncertainty degree of the rnn model basedonthechangesofthehiddenstate soutputovertimesteps.
comparedwithothercoveragecriteriathatjudgetheneuron sactivationbasedonthevalueofthehiddenneuron soutput deepstate can better capture the state transition of rnn based on time steps.
further differentfromthebaselinemethodsapplyingctmorcam for test case selection the selection algorithm of deepstate incorporates the advantage of both ctm and cam.
we evaluate the uncertaintyofrnnforagiventestcasebasedonthechangingrate the process of sorting tests based on the changing rate is similar to ctm.
meanwhile we select based on the similarity of their changing trends for cases with the same changing rate.
this method can beregardedascam sselectionstrategy whichaimsatselecting the cases that trigger different behaviors of the rnn.
the selection strategy of deepstate makes the selected test set more diverse and we conjecture this fundamentally benefits its performance.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa zixi liu yang feng yining yin and zhenyu chen table the rnns accuracy score on mix test data set after retraining with selected tests.
methoddataset mnist fashion snips agnews lstm blstm lstm gru blstm gru lstm blstm random .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
hscov cam .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
bscov ctm .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
btcov ctm .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
sc ctm .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
sc cam .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
nc ctm .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
nc cam .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
deepstate .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
tests .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
application scenarios rnnisakindoffeedbackneuralnetworkthatspecializesinprocessing time seriesdata.
it isdifferent from conventionalfeedforward neuralnetworks andmanyexistinganalysiscriteriafortraditional neuralnetworksarenotsuitableforrnns.
deepstate canbeapplied to select tests from massive unlabeled data collected from the usagescenarios.itcanautomaticallyandefficientlyidentifytests thathaveahighprobabilityoftriggeringtheincorrectbehaviorsof rnn driven systems from plenty of unlabelled data and thus help reducethecostofmanuallylabeling.further theexperimentresults also show that the accuracy of the model can be improved after retrainingwiththeselecteddata.applyingsmall scaleselecteddata forretrainingthemodelcanalsoenhancetheefficiencyofmodel optimization and save computational resources.
deepstate can be potentially applied to detect malicious attack samples against rnn models such as backdoor attack data.
it is capable of analyzing thestates changingtrendsofrnnafterreceivingdifferenttests because attack samples and ordinary samples are likely to cause significant changes in rnn models.
we will explore the related study in our future work.
.
threats to validity testsubjectselection.
theselectionofdatasetsandrnnmodels selection is one of the primary threats to validity.
there are many variantsofthernnmodel includinglstmandmultiplestructures and the corresponding effects can be different.
on the other hand thetrainingofthernnsreliesonthedataset andthequalityofthe datamayhaveaninfluenceonthemodel saccuracy.wealleviate thisthreatbyemployingfourcommonlyuseddatasets including image and text data types.
further for each studied dataset we employed two rnn models with different numbers of neurons and architecture to evaluate the performance of deepstate.
parameters settings.
anotherthreatcouldbe theparametersettingsin neuroncoverages.to comparewithother coverage based testsetselectionmethods wereproducedotherexistingcoverage methodsforrnn whichmayincludeparameters.withfine tuning the parameters settings the selected data could be different.
to alleviatethepotentialbias wefollowtheauthors suggestedsettings or employ the default settings of the original papers.
test data simulation.
thelastthreattovaliditycomesfromthe augmented test input generation.
although the augmented op erators are common data noises in the actual environment it isimpossible to guarantee that the distribution of the real unseeninput is the same as our simulation.
to ensure the reliability of the augmented data we refer to the existing image and text transformationmethodsbasedonsomeopen sourcetools.webelieve that only some minor fine tuning or adjustments are needed to supplement and simulate other transformations.
related work thissectiondiscussestherelatedworkintwogroups testselection methods for conventional software and testing techniques for rnn models.
.
test selection the test selection technique aims to find the test suite so that software testerscan getthe most benefitswithin limitedresource budgets.
the test selection technique was first proposed in conventionalregressiontesting whichaimsatimproving the testing efficiency of a modified software system.
rothermel et al.
outlined the issues related to regression testing selection and proposed a series of metrics for evaluating regression testing selection techniques.
based on the relevant metrics wemadesomefine tuningontheinclusivenesstoadaptfor the testing of deep learning models.
rothermel et al.
proposed a safe test selection algorithm that constructs control flow graphs for the program and its modified versions.
then the tests are selectedbasedonthesegraphsandappliedtoexecutethemodifiedcodefromtheoriginaltestsuite.hyrts wasthefirsthybrid regression test selection technique which performs analysis onmultiple granularities to combine the advantages of traditionalselection techniques with different granularities.
different from theapproachesmentionedabove deepstate stestcaseselection technique is mainly aimed at testing rnns rather than traditional software testing based on data flow and control flow.
inthefieldofdnntesting activelearning asaspecialcase ofmachinelearning caninteractivelyquerytheinformationsource tolabelnewdatapointsfortrainingdnnmodels .themost commonly used query framework is uncertainty sampling which aims to select unlabeled examples that the dnn finds hardest to classify.
these uncertainty degree can be calculated basedon conditional random fields margin and entropy .
different from the active learning methods deepstate is designed authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
deepstate selecting test suites to enhance the robustness of recurrent neural networks icse may pittsburgh pa usa toidentifyasubsetoftestdatawiththehighestprobabilityofrevealing bugs in pre trained rnn models.
it is effective in testing and optimization scenarios different from active learning focusing on the training process.
meanwhile while most active learning techniques leveragethe outputof dnnmodels such asboundary andposteriorprobability asguidanceforselectingdata deepstate employsstructuralcharacteristicsofrnnmodelsandcapturestheir internal state to reach the goal.
.
testing recurrent neural networks for the quality assurance tasks of rnn driven software systems some coverage based testing approaches have been proposed in recentyears.deepstellar adaptsfivecoveragecriteriaofdeepgauge to test and analyze rnn models which is first transformed into a discrete time markov chain dtmc a sa n abstraction.
testrnn proposes a series of neuron coverage metrics of the lstm network and develops a coverage guided fuzzing approach for deep learning models with lstm network structure.
then some random mutation enhanced with the coverage is designed to generate test cases.
rnn test defines the hidden state coverage as the ratio of the hidden states that achieve the maximum value of all the hidden states during testing.
differentfromthesecoverage guidedadversarialexamplegeneration methods deepstate explores another solution to guide the test cases selection.
deepstate analyzes the changing rate and changing trend of the hidden states in rnn models and selects tests effectively among the unlabeled large scale data sets.
on the other hand we focus on selecting test suites from massive dataset for testing and optimizing the rnn models.
thus the criterion of deepstate relies on the relationship among different cases ratherthanevaluatingforasingletest.ourstudydemonstratedthat deepstate couldmoreeffectivelyselectdatasetforrnntesting and optimization and the oracle problem is alleviated.
conclusion in this paper we propose deepstate for selecting massive test suitestoenhancetherobustnessofrnns.basedonastatisticalviewofrnn weexpandtheoutputofthernnmodelaccordingtotime steps and analyze the uncertainty degree of the rnn model as the time step changes.
we design and implement a test suite selection tool deepstate by calculating the changing rate and changing trend of the rnn output sequence over time.
the experimental resultsdemonstrate that deepstate caneffectively helptestersto choose test cases with a high ability to detect bugs and improve thequalityofthernnmodels.
deepstate caneffectivelyensure that the test data set has a high bug detection rate while greatly reducingthecostofdatalabelingandimprovingtheefficiencyof rnn model testing and optimization.
acknowledgement wewouldliketothankanonymousreviewersfortheirinsightful andconstructivecomments.thisprojectwaspartiallyfundedby the national natural science foundation of china under grant nos.
and andthe science technology and innovation commission of shenzhen municipality no.
cjgjzd20200617103001003 .