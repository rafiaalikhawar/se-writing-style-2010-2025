discovering parallelisms in python programs siwei wei statekey laboratory ofcomputer science instituteofsoftware chinese academy ofsciences and university ofchineseacademy ofsciences beijing china weisw ios.ac.cnguyang song antgroup beijing china guyang.sgy antgroup.comsenlinzhu antgroup hangzhou china senlin.zsl antgroup.com ruoyiruan antgroup hangzhou china ruoyi.ruanry antgroup.comshihao zhu statekey laboratory ofcomputer science instituteofsoftware chinese academy ofsciences and university ofchineseacademy ofsciences beijing china zhush ios.ac.cnyancai statekey laboratory ofcomputer science instituteofsoftware chinese academy ofsciences and university ofchineseacademy ofsciences beijing china yancai ios.ac.cn abstract parallelizationisapromisingwaytoimprovetheperformance of python programs.
unfortunately developers may miss parallelization possibilities because they usually do not concentrate on parallelization.
many approaches have been proposed to parallelizepythonprogramsautomatically however theyareeither domain specificorrequiremanualannotation.thustheycannot solve the problem well in general.
in this paper we propose pypar aneffectivetoolaimingatdiscoveringparallelizationpossibilities inreal worldpythonprograms.
pypardoesn tneedmanualannotation and is universally applicable.
it first drives a data dependence analysisto determinewhether two piecesofcodecanrun concurrently.
the key is the use of a graph theoretic approach.
next it adopts a dynamic selection strategy to eliminate inefficient parallelisms.
finally pyparproduces a parallelism report as well as a referential parallelized program which is built by pyparusing one of the three parallelization methods thread based processbased and ray based .
we have implemented a prototype of pyparandevaluateditonsixwell designedwidely usedreal world pythonpackages scikit image scipy librosa trimesh scikit learn andseaborn.intotal 240functionsaretested and pyparfound 127parallelizablefunctionsamongthem.basedonmanualfiltering only7ofthemarefalsepositives i.e.
a94.
precision .the remaining120areparallelizable almost10 amongallfunctions under test and most of them can be efficiently sped up by gaining an acceleration of up to with an average of .
the acceleration in practice is close to theoretical estimation.
the results show that even well designed practical python programs can be corresponding author esec fse december san francisco ca usa copyright heldby theowner author s .
acm isbn .
parallelized for speeding up and pyparcan bring effective andefficient parallelization on real world python programs.
ccsconcepts softwareanditsengineering softwarelibrariesandrepositories computing methodologies parallel algorithms .
keywords parallelism python ray acm reference format siwei wei guyang song senlinzhu ruoyi ruan shihaozhu and yancai .
.discoveringparallelisms inpythonprograms.in proceedingsof the 31stacmjointeuropeansoftwareengineeringconferenceand symposium on the foundations of software engineering esec fse december san francisco ca usa.
acm new york ny usa 13pages.https introduction python is a widely used high level interpreted language.
it is becoming more and more popular .
python has a vast community and an increasing number of open source packages making it user friendly and accessible.
it is now used in various areas suchasmachinelearning scientificcomputing andimage processing.
however python spoorperformancehasalwaysbeenoneofits majorshortcomings .theinterpretativenatureofpython improves its flexibility but makes it hundreds of times slower than compiled languages.
also python has a global interpreter lock gil .gilpreventspythonbytecodefrombeinginterpreted ondifferentthreadsconcurrently makingitdifficulttobenefitfrom thread basedparallelization.
parallelprocessing isoneofthemostimportant waystoimprovetheperformanceofpythonprogramsbyleveraging themulti coreprocessingcapabilityofmoderncomputers.there areawealthofpythonparallelizationframeworks whichwillbe introducedinsection .
.eachof these frameworkshasitsmerits and drawbacks.
these frameworks make parallel processing in thiswork islicensedunderacreativecommonsattribution4.0international license.
esec fse december3 san francisco ca usa siweiwei guyang song senlinzhu ruoyi ruan shihao zhu andyancai sp1 fft in1 fshape axes axes sp2 fft in2 fshape axes axes ret ifft sp1 sp2 fshape axes axes for i sigma in enumerate sigmas lambda1 lambdas compute hessian eigenvalues image sigma sorting abs mode mode cval cval ... a b figure1 parallelismsinreal worldpackagesfoundby pypar python easier.
nevertheless the key issue remains as to where and how to parallelize a piece of python program and it is still left to programmers.
manyautomaticparallelizationmethodsforpythonhavebeen proposed aiming at transforming serial programs into parallelized versionsautomatically.automphc andautoparallel apply the polyhedral optimization technique to python programs.
quantcloud and hpat aim at parallelizing domain specific python programs.
pydron uses a dynamic scheduling strategy to parallelize tasks during execution.
however thesemethodsallhavecertaindrawbackssuchasinvolvingmanual annotations causing large overhead and being designed for domain specificprogramsonly.thesedrawbacksreducetheusefulness of the above mentioned methods hindering them from being appliedto generalreal world pythonprograms.
whenwritingprograms programmersusuallyconcentratemore on correctness and readability than the possibility of improving performancebyparallelization.also someprogrammersmaybe domainexpertswhoareprofessionalincertainareasbutarenot so familiar with parallelization techniques.
as a consequence programmerscanmissalargeamountofparallelizationpossibilities.
this happens even in widely used python packages.
figure 1gives two examples where the pieces of code in red dotted line boxes are parallelizable.
specifically figure a is a piece of code in function filters.frangi from scikit image which implements amulti scalefilteringalgorithm.theparallelismshowsthecomputation of eigenvalues at different scales i.e.
different sigmas can be done concurrently.
figure b shows three lines in function signal.
freq domain conv from scipy .
it calculates the convolution of two signals.
fast fourier transformations fft of two inputsignals are firstcalculated andthey can run concurrently.
in this paper we propose pypar an effective and easy to use toolfordiscoveringparallelizationpossibilitiesinpythonprograms.
compared with existing automatic parallelization methods pypar isfastandfreeofmanualannotation.moreimportantly pyparis universally applicable to real world pythonpackages.
pyparaimstofindparallelismsthatdon tchangethebehavior oftheoriginalprogram.
pypardiscoversprogramsegmentsthat canrunconcurrentlybasedondatadependence .firstly pypar decomposesthetargetprogramintosmallpiecescalledprogram units.
then by utilizing data dependence analysis a relation betweenprogramunitsisdefined whichindicateswhetheraprogram unitshouldbeexecutedbeforeanother.afterthat programunitsthat don t have to run serially are selected and they are composed intotasksusingheuristicstrategies.meanwhile thetargetprogram isexecutedand timedtodeterminetherunningtime ofeachpart.
basedonthetimingresults pyparselectstasksthatareworthwhile toparallelize.finally areportcontainingthediscoveredparallelism andexpectedtimesavingisgenerated andaparallelizedversion ofthe target program isproduced.
amongtheproceduresof pypar thekeypointistofindparallelism using the extracted dependence relation.
to achieve this we proposeagraph theoreticapproach.firstly adependencegraph representing the dependence relation between program units is built.
for sequentially executed program units we assign depth tothemandfindparallelismaccordingtothedepth.forprogram units in a loop we use strongly connected components scc to determine whether a program unit can run concurrently with itself in different iterations.
the soundness and completeness of the proposedalgorithminfindingparallelismsusingthedependence relation are provedinsection .
.
wehaveimplementedaprototypetool pyparandevaluatedit on six widely used and well designed real world python packages.
pyparfound almost of all functions under test to be parallelizable withaprecisionof94.
.afterparallelizingthem we gainup to speed acceleration with anaverage of44 .
the evaluation result shows there are lots of parallelization possibilities in real world python programs pyparcan effectively find parallelisms and the parallelisms found by pyparcan improve the performance ofthe target programs.
in summary the contributionof this paper isthree fold we propose an effective method to detect parallelization possibilities inreal world pythonprograms.
we present two proportions to show the soundness and completenessofour algorithms.
we have implemented a prototype tool pypar available at .weconductedan evaluationonsixwidely usedpythonpackagesandfound 120parallelizable functions.
backgroundand related work inthissection wewillintroduce parallelcomputingframeworks in python and automatic parallelization techiques for compiled languagesandpython.the latter willbe our focus.
.
parallelcomputing in python in python there are mainly two types of parallel processing methods process based ones and thread based ones.
process based parallelizationusesprocessestoexecuteparalleltasks.itcanintroduce extra overhead because creating and recycling processes can be expensiveandtransferringlargedataamongprocessesalsocostsalot.
incomparison threadsarelight weighted andthereisnocostto transfer data between threads.
however there is global interpreter lock gil inpython which makessure that therewill be only onethreadexecutingpythonbytecodeatatime.1gilessentially prevents two threads from executing concurrently.
thus threads 1actually it is the cpython interpreter that has gil.
but as cpython is the default and mostwidely used implementation of python weuse the notionin thispaper.
833discoveringparallelismsin pythonprograms esec fse december3 san francisco ca usa are useless for accelerating python programs and they are mainly usedfor asynchronous tasks.
there are lots of parallel processing frameworks for python somerepresentatives are shownbelow.
python built inparallelprocessing modules multiprocessing is a built in module that provides processbasedparallelism.itimplementsaprocessclass tomanageprocess andapis for data transferring synchronization etc.
threading is a built in module which provides thread based parallelism.
it is useful mainly for i o bound tasks.
apis for managing thread local data andlocksare provided.
concurrent.futures isabuilt inmoduleforexecutingcallables asynchronously.itsupportsboththread basedandprocess based parallelizationand implementsthe executorclassasa high level interfacefor running callablesconcurrently.
third party parallelprocessing packages joblib provides light weight pipelining for python.
it implementsaclassparallelforwritingparallelloops.italsoprovides interfacesfor operating sharedmemory reusableworkers etc.
spark is a multi language engine for data science that supports distributed computing.
the resilient distributed datasets rdds is its core data structure.only high level parallel operationsonrddssuch as mapandreduce are provided.
dask is a package for parallel and distributed computing in python.
it provides parallelized versions of commonly used data structures such as dataframe in pandas .
in addition it provideseasy to useapis for executingcode asynchronously.
ray isaunifieddistributedcomputingframeworkinpython.
itprovideseasy to useapis forexecutingtasksinparallel.ray also provides the actor model as a stateful worker.
some optimizationtechniquessuchasread onlysharedmemoryarealso leveragedfor betterperformance.
.
automaticparallelization .
.
automatic parallelization for compiled languages.
automatic parallelization forcompiled languages suchas fortran and c c has been thoroughly studied yielding a variety of automatic parallelizationtools includingcetus pluto polariscompiler andpar4all .
automaticparallelizationtechniquesincompiledlanguagesfocus mostly on nested loops since usually they are performance bottlenecks andparallelizingthemcanachievesignificantimprovementinperformance.suchtechniquesarecalledloopnestoptimizations .amongthem thepolyhedraloptimization is the most widely used.
it uses the polyhedral model to represent nested loops where each loop iteration corresponds to a lattice point in a polyhedron.
the parallelization can be achieved by performing affine or none affine transformations on the polyhedral model whichresults inan equivalentbut parallelizable loop.
though loop nest optimizations are widely applicable in lowlevelcompiledlanguages theyarenotsuitableforpython.loops can be hundreds of times slower in python because of its interpretative nature.
in practice python is not used to directly implement algorithmssuch asmatrix multiplicationandfactorization which contain nested loops asthe performance bottleneck.
instead such algorithms are implemented in low levelcompiled languages andareimportedintopythonprograms asisdoneinnumpy .there arefewcasesinreal worldpythonprogramsthatcanbeparallelized using loopnestoptimization techniques.
.
.
automatic parallelization for python.
there are some works about automatically parallelizing python programs in recent years.
we classify them into categories and show why they are not suitable for discovering parallelism in real world python programs.
polyhedraloptimizationinpython autoparallel andautomphc bothparallelizepythonprogramsusingpolyhedral optimizationtechniques.autoparallelimplementsaneasy to use python module that achieves polyhedral optimization by wrappingthepluto parallelizationtool.automphcusespolyhedral optimizationaswellasothertechniquesforexploitinghardware resources.
both methods need manual annotation to assist parallelization.
evaluations are conducted on a special set of python programs that are suitable for polyhedral optimization and achieve significant performance improvement.
however as discussed in section .
.
python usually dispatches algorithms with nested loops to low level compiled languages and imports them thus loop nest optimization techniques are not suitable for most real world pythonprograms.
domain specific parallelization hpat and quantcloud proposeautomaticparallelizationtechniquesforspecificdomains using semantics.
hpat focus on parallelizing programs similartologisticregression andproposeadataflowframework to find data distributions.
it requires the input programs to follow certain prerequisites and needs user annotations as assistance.
quantcloud focuses on parallelizing quantitative finance applications.thesemethodsbothconcentrateonspecialtypesof programs making them hard to be widelyapplicable.
the need of manual annotations and prior semantic knowledge hinder them from gaining large scale automatic applications and thus they are not suitable for discovering parallelism missed by programmers in real world packages.
dynamicparallelization pydron proposestodynamically find and dispatch parallelizable tasks.
to achieve this it first needs the userannotation to indicatewhich functionis the performance bottleneckandneedsoptimization.
duringexecution pydrondynamically builds the dependence relationship between tasks finds tasks that can run concurrently and then dispatches them.
this methodcanparallelizesomepythonprograms butithasshortcomings.dynamicparallelizationcanintroducesignificantoverhead.
also the needofmanual annotation iscostlyto satisfy.
ourapproach pypar .
overview pypartakesapieceofpythonprogramasinput.itprocessesthe target program and produces a parallelism report and a referential parallelizedcode as output.itsworkflowisshowninfigure .
in step a pypardecomposes an input python program e.g.
a helloworld program intoprogramunits.instep b adependence graph with program units as nodes is built yielding the dependence relation between program units.
in step c pypar utilizes the dependence graph to find program units that can be executed concurrently e.g.
the program units in the same rectangle .
834esec fse december3 san francisco ca usa siweiwei guyang song senlinzhu ruoyi ruan shihao zhu andyancai python code h hello w world r hw h w hello world h ... w ... hw h w r ...hello world h ... w ... hw h w r ...hello world h ... w ... hw h w r ...h hello w world task1 task2h hello w world task1 task2hello .0s world .0s hw h w .0s found parallelism h hello w world save time .0sreport ray.remote def task 0 world w world return w ray.remote def task 1 hello h hello return h tmp 0 task 0.remote world tmp 1 task 1.remote hello w ray.get tmp 0 h ray.get tmp 1 r hw h w b .
extract dependence c .
discover parallelism d .
recompose a .
decompose e .
execute and time f .
select h .
generate report g .
rewrite figure the workflow of pyparon a helloworld program r f f1 a f2 b g c r .. ..f .. .. g c f1 a f2 b coarse grained statementprogram units with hierarchical structure figure decomposition ofacoarse grained statement instep d theparallelizableprogramunitsarerecomposedtoform coarse grained tasks based on heuristics which aims to reduce the number of tasks to avoid extra overhead.
to filter the tasks unworthy of parallelization the target program is executed and timed to assign a weight for each task as shown in step e .
in step f only taskswithsufficientlyhighweightsareselected.finally instep h aparallelismreportisgenerated showingpossibleparallelismin the target program and expected time savings after parallelization.
also in step g a referential parallelized code isgenerated inthis figure the parallelized program is implemented using ray and other parallelization frameworks can also be used.
these steps will be illustratedindetailinthe following sub sections.
.
decomposition in step a pypardecomposes the target program into small units whichwe callprogram units.
the procedure of decomposition is given by algorithm .
it first parsesthetargetprogramandobtainsanabstractsyntaxtree ast usingthebuilt inmoduleast line1 .thenallthestatement sub treesareextractedfromtheast line2 .astatementcanbe coarse grained and should be further decomposed.
for example considerthestatementinfigure whichhasacomplexinternal structure.
since function calls usually cost the most in python programs weonlyextractfunctioncallsintargetstatements.thesame procedureisrepeatedfor coarse grainedfunctioncalls andeventually wegetadecompositionwithahierarchicalstructure.the proceduretodecomposecoarse grainedstatementsisdenotedasdecomposestmt.coarse grained statementsare further decomposed whileotherstatements are retained lines4 .algorithm1 decompose input code output programunits 1ast parseast code 2statements getstatementsubtrees ast 3programunits 4forstmt statements do 5ifiscoarsegrained stmt then units decomposestmt stmt programunits programunits units 8else programunits programunits stmt 10end 11end 12returnprogramunits figures2and3give examples of decomposition.
in figure the hello world program consists of three statements and they are all coarse grained.
the statement h h u1d452 u1d459 u1d459 u1d45c is decomposed to program units h ...andh u1d452 u1d459 u1d459 u1d45c .
the other two statements are decomposed similarly.
finally the program is decomposed into six programunits.figure 3givesanexampleofdecomposingcoarsegrained statements.
all function calls are extracted from the statement whichtogetherwiththeoriginalstatementareprogramunits.
the program unitsform ahierarchical structure.
.
extracting dependence to determine valid orders of executing program units we adopt data dependenceanalysis which hasbeen studiedin depth .
data dependence analysis is originally conducted at statement level and we extend it to program unit level.
the notationsusedareconsistentwith .inthissection we definethedirectdatadependencerelation u1d6ff andthendefinethe indirectdata dependence relation asthe transitive closure of u1d6ff .
.
.
extracting u1d437 u1d452 u1d453and u1d448 u1d460 u1d452sets.the read and write portion is all we need to make sure that a program unit can run normally and to determine its effects.
in pypar we use the set of variables a program unit uses or modifies to represent the portion of memory aprogramunitreads orwrites which isstraightforwardtoimplement.
there might be cases where two variables pointing to the 835discoveringparallelismsin pythonprograms esec fse december3 san francisco ca usa h hello w world r hw h w use def hello hello h .. h world world w .. w hw h w hw h w r .. r hello world h .. w .. hw h w r ..compose compose composeraw raw a b c figure dependence analysis for sequentially executed statements for data in data list res.append f data use def f data f data res.append .. res res f data res.append .. compose waw.war raw a b c figure dependence analysis forloopstatement same memory address.
but such situations are rare see section and the resulting false positives can be easily identified and corrected.
to be consistent with previous work we define the u1d448 u1d460 u1d452 read and u1d437 u1d452 u1d453 write setsas definition u1d448 u1d460 u1d452 u1d437 u1d452 u1d453 .
u1d448 u1d460 u1d452isafunctionwhich mapsaprogramunit u1d462tothesetofvariables u1d462reads.
u1d437 u1d452 u1d453isafunctionwhich mapsaprogramunit u1d462to theset ofvariables u1d462writes.
examples of u1d448 u1d460 u1d452 u1d437 u1d452 u1d453sets are shown in figures 4and5.
figure a shows the original program segment and figure b gives the u1d448 u1d460 u1d452 u1d437 u1d452 u1d453sets.
the program unit h u1d452 u1d459 u1d459 u1d45c reads the variable h u1d452 u1d459 u1d459 u1d45cbecause it calls this variable while it writes nothing.
the program unit h ..reads nothing while it writes the variable h because it assigns a new value to this variable.
u1d448 u1d460 u1d452 u1d437 u1d452 u1d453sets of other program units are computed similarly.
figure a shows the original program segment and figure b gives the u1d448 u1d460 u1d452 u1d437 u1d452 u1d453sets.
the programunit u1d453 u1d451 u1d44e u1d461 u1d44e reads thevariables u1d453and u1d451 u1d44e u1d461 u1d44ebecause onevariableiscalledandtheotheristakenasanargument whileit writesnothing.theprogramunit u1d45f u1d452 u1d460.
u1d44e u1d45d u1d45d u1d452 u1d45b u1d451 .. readsthevariable u1d45f u1d452 u1d460because it calls a method u1d44e u1d45d u1d45d u1d452 u1d45b u1d451of the u1d45f u1d452 u1d460object.
the u1d45f u1d452 u1d460 variable is also put into the u1d437 u1d452 u1d453set because the call of method u1d44e u1d45d u1d45d u1d452 u1d45b u1d451maychangethe internal state ofthe u1d45f u1d452 u1d460object.
to make the process of extracting u1d448 u1d460 u1d452 u1d437 u1d452 u1d453sets easier and viableforstaticanalysis weproposeseveralsimplifications.foran arraysubscriptexpression u1d44e if u1d456isaloopindex wetake u1d44e asa singlevariablein u1d448 u1d460 u1d452 u1d437 u1d452 u1d453sets otherwise wetake u1d44e u1d456in u1d448 u1d460 u1d452 u1d437 u1d452 u1d453 sets respectively.
also we assume a function call of an external function won t change the value of arguments.
these simplifications might result in false positive parallelisms.
figure 6gives such anexample.acodepieceinfunction u1d459 u1d456 u1d45b u1d44e u1d459 u1d454.
u1d45a u1d44e u1d461 u1d453 u1d462 u1d45b u1d450 u1d460.
u1d452 u1d465 u1d45d u1d45a from scipyisshown where pyparreportedthatthestatementwithin the red dotted line boxes is parallelizable in different iterations.
however the function u1d45d u1d44e u1d451 u1d452 u1d448 u1d449 u1d450 u1d44e u1d459 u1d450 which is implemented in compiled languages so pyparcan t infer its behavior modifies its firstargument whichis u1d434 u1d45a sothisstatementisnotparallelizable.
the task of refining data dependence analysis to eliminate such false positives is left as future work see section .
in practice such falsepositivesarerare asshowninsection .moreover thesefalse positives can be easily foundandcorrected.
.
.
constructingdependencegraph.
the u1d448 u1d460 u1d452 u1d437 u1d452 u1d453setisusedto definethedependencerelation betweenprogram units.
we definefor ind in product ... ... pade uv calc am n m ... figure afalsepositive caused by simplifications four dependencies three of them are data dependencies which have been discussed in detail in .
and one comes naturally from the hierarchical structure duringdecomposition.
raw read after write raw is also called flow dependency.
twoprogramunits u1d462and u1d463havearawdependenceifduringthe execution u1d462executes before u1d463and u1d462writes some memory which will later be read by u1d463.
that is u1d437 u1d452 u1d453 u1d462 u1d448 u1d460 u1d452 u1d463 .
to preserve program behavior u1d462must be executed before u1d463 otherwise u1d463cannotread correctvalues.
war write after read war is also called anti dependency.
twoprogramunits u1d462and u1d463haveawardependenceifduringthe execution u1d462executesbefore u1d463and u1d462readssomememory which is latter overwrote by u1d463.
that is u1d448 u1d460 u1d452 u1d462 u1d437 u1d452 u1d453 u1d463 .
again u1d462must be executed before u1d463 otherwise what u1d462reads will be changedby u1d463.
waw write after write waw isalsocalledoutputdependency.
two program units u1d462and u1d463have a war dependence if during theexecution u1d462executesbefore u1d463and u1d462writesthesame memory as u1d463.
that is u1d437 u1d452 u1d453 u1d462 u1d437 u1d452 u1d453 u1d463 .
then u1d462must be executedbefore u1d463 otherwise thememorythat u1d462 u1d463bothwrites willbe changedandaffectsubsequent computations.
compose compositiondependencecomesnaturallyfromthe hierarchical structure when decomposing a coarse grained statement see figure .
for two program units u1d462 u1d463where u1d462is a sub expressionof u1d463 u1d462mustbe executedbefore u1d463.
withthesetypesofdependence wethenconstructadependence graph as below in which nodes are program units and directed edgesarethe dependence relationbetweencomputationunits as showninfigure c .
definition dependencegraph .
a dependencegraphofa pieceofprogramisagraph u1d43a u1d449 u1d438 where u1d449isthesetofprogram units and u1d438 u1d449 u1d449is the directed edge set.
for u1d462 u1d463 u1d449 u1d462 u1d463 u1d438 ifandonlyifthereisoneofthefourtypesofdependence raw war waw compose from u1d462to u1d463.
the dependence graph of sequentially executed program pieces is a directed acyclic graph dag .
however the dependence graph of a program piece inside a loop can be cyclic.
consider a program unit u1d462insidealoopstatement suchasaforstatement .different from the sequential case u1d462can be executed multiple times.
as a result there can be dependence between u1d462and itself resulting in a loopinthe dependence graph as showninfigure c .
we definethe directdata dependence relation definition directdatadependencerelation .
thedirect data dependence relation is a binary relation over the set of program units.fortwoprogramunits u1d462 u1d463 u1d462 u1d6ff u1d463ifandonlyifthereisoneof the four types of dependence raw war waw compose from u1d462to u1d463 i.e.thereis anedge in the dependencegraphfrom u1d462to u1d463.
we then definethe indirectdata dependence relation 836esec fse december3 san francisco ca usa siweiwei guyang song senlinzhu ruoyi ruan shihao zhu andyancai definition indirect data dependence relation .
the indirectdatadependencerelationisabinaryrelationoverthesetof programunits.itisthetransitiveclosureof u1d6ff .fortwoprogramunits u1d462 u1d463 u1d462 u1d463ifandonlyifthereareprogramunits u1d4641 ... u1d464 u1d45b u1d45b such that u1d462 u1d6ff u1d4641 ... u1d464 u1d45b u1d6ff u1d463 i.e.
there is a path of length at least one in the dependencegraphfrom u1d462to u1d463.
twoprogramunitscanbeexecutedconcurrentlywithoutchangingsemantics if neither u1d462 u1d463nor u1d463 u1d462.
.
discoveringparallelism for sequentially executed program units the dependence graph is a dag.
to ease our presentation we add an u1d452 u1d45b u1d461 u1d45f u1d466.altnode in the dependence graph that has edges to all nodes with in degree.
similarly weaddan u1d452 u1d465 u1d456 u1d461nodethathasedgesfromallnodeswith0 out degree.
obviously each graph has exactly one u1d452 u1d45b u1d461 u1d45f u1d466.altnode and one u1d452 u1d465 u1d456 u1d461node.
we definethe u1d451 u1d452 u1d45d u1d461hofaprogram unitas follows definition depth .
given a graph u1d43a u1d449 u1d438 the u1d451 u1d452 u1d45d u1d461hof u1d462 u1d449isthelength i.e.
thenumberofedges ofthelongestpathfrom u1d452 u1d45b u1d461 u1d45f u1d466.altto u1d462.thatis u1d462 u1d449 u1d451 u1d452 u1d45d u1d461h u1d462 max u1d45d u1d45d u1d44e u1d461h u1d452 u1d45b u1d461 u1d45f u1d466.alt u1d462 u1d459 u1d452 u1d45b u1d454 u1d461h u1d45d the u1d451 u1d452 u1d45d u1d461his well defined because the dependence graph is a dag.
we then definethe u1d451 u1d452 u1d45d u1d461h u1d460 u1d45d u1d44e u1d45b ofaprogram unit definition depthspan .
given a graph u1d43a u1d449 u1d438 the u1d451 u1d452 u1d45d u1d461h u1d460 u1d45d u1d44e u1d45b of u1d462 u1d449 u1d452 u1d45b u1d461 u1d45f u1d466.alt u1d452 u1d465 u1d456 u1d461 is the set of all depths between u1d462and its shallowest i.e.
with the least depth successor.
that is u1d462 u1d449 u1d452 u1d45b u1d461 u1d45f u1d466.alt u1d452 u1d465 u1d456 u1d461 u1d451 u1d452 u1d45d u1d461h u1d460 u1d45d u1d44e u1d45b u1d462 u1d451 u1d441 u1d451 u1d452 u1d45d u1d461h u1d462 u1d451 min u1d460 u1d460 u1d462 u1d450 u1d450 u1d462 u1d451 u1d452 u1d45d u1d461h u1d460 figure7 a showsanexampletoillustratethetwoconcepts u1d451 u1d452 u1d45d u1d461h and u1d451 u1d452 u1d45d u1d461h u1d460 u1d45d u1d44e u1d45b .
in the figure we have u1d451 u1d452 u1d45d u1d461h u1d434 1because the longestpathfrom u1d452 u1d45b u1d461 u1d45f u1d466.altto u1d434is u1d452 u1d45b u1d461 u1d45f u1d466.alt u1d434 whoselengthis .the u1d451 u1d452 u1d45d u1d461hsof u1d452 u1d45b u1d461 u1d45f u1d466.alt u1d435 u1d436 and u1d452 u1d465 u1d456 u1d461arecomputedsimilarly.andwehave u1d451 u1d452 u1d45d u1d461h u1d460 u1d45d u1d44e u1d45b u1d434 because u1d451 u1d452 u1d45d u1d461h u1d434 1and u1d451 u1d452 u1d45d u1d461h u1d452 u1d465 u1d456 u1d461 where the u1d452 u1d465 u1d456 u1d461is the only successor of a. the u1d451 u1d452 u1d45d u1d461h u1d460 u1d45d u1d44e u1d45b of u1d435and u1d436are computedsimilarly.
giventwoprogramunits u1d462 u1d463 iftheysharesomecommondepth i.e.
u1d451 u1d452 u1d45d u1d461h u1d460 u1d45d u1d44e u1d45b u1d462 u1d451 u1d452 u1d45d u1d461h u1d460 u1d45d u1d44e u1d45b u1d463 thentheyareparallelizable.
intuitively thisisbecause u1d462and u1d463canbeputonthesame level in the dependence graph.
formally we showthis as proposition proposition .giventwonodes u1d462 u1d463in adependencegraph if u1d451 u1d452 u1d45d u1d461h u1d460 u1d45d u1d44e u1d45b u1d462 u1d451 u1d452 u1d45d u1d461h u1d460 u1d45d u1d44e u1d45b u1d463 thenneither u1d462 u1d463nor u1d463 u1d462.
in figure a u1d434is parallelizable with both u1d435and u1d436by proposition1 aswehave u1d451 u1d452 u1d45d u1d461h u1d460 u1d45d u1d44e u1d45b u1d434 u1d451 u1d452 u1d45d u1d461h u1d460 u1d45d u1d44e u1d45b u1d435 and u1d451 u1d452 u1d45d u1d461h u1d460 u1d45d u1d44e u1d45b u1d434 u1d451 u1d452 u1d45d u1d461h u1d460 u1d45d u1d44e u1d45b u1d436 .
ontheotherhand wealsoaimtofindallpossibleparallelism opportunities.
based on u1d451 u1d452 u1d45d u1d461h u1d460 u1d45d u1d44e u1d45b we can find all as shown in proposition proposition .givena node u1d462in a dependencegraph ifthere exists some u1d464 u1d462suchthat neither u1d462 u1d464nor u1d464 u1d462 then there exists u1d463 u1d462 u1d460.
u1d461.
u1d451 u1d452 u1d45d u1d461h u1d460 u1d45d u1d44e u1d45b u1d462 u1d451 u1d452 u1d45d u1d461h u1d460 u1d45d u1d44e u1d45b u1d463 .
forexample infigure a neither u1d434 u1d435nor u1d435 u1d434 soweknow there is some node u1d463such that u1d451 u1d452 u1d45d u1d461h u1d460 u1d45d u1d44e u1d45b u1d434 u1d451 u1d452 u1d45d u1d461h u1d460 u1d45d u1d44e u1d45b u1d463 whichcan be u1d435or u1d436.ab centry exitdepth depth 3depth depthspan depth depthspan depth depthspan a b c d a b figure using dependencegraph to find parallelizableprogram units parallelizable nodes are in red dotted line boxes algorithm2 findparallelizablesinserialprograms input g the dependencegraph output parallelizable parallelizable programunits 1g addentryexit g 2depth getdepth g 3depthspan getdepthspan g depth 4parallelizable 5for u1d451 u1d451 u1d452 u1d45d u1d461h 1do u1d446 u1d462 u1d449 u1d451 u1d451 u1d452 u1d45d u1d461h u1d460 u1d45d u1d44e u1d45b 7if u1d446 2then parallelizable parallelizable s 9end 10end 11returnparallelizable due to page limit the proofs of the two propositions are shown inappendix inthe supplementary file.
based on the above we present algorithms 2and3to find all parallelizable program units.
algorithm 2works for program units in sequentially executed programs and algorithm 3works for program units in loops to be explained in the next paragraph .
it first addsnodes u1d452 u1d45b u1d461 u1d45f u1d466.altand u1d452 u1d465 u1d456 u1d461tothedependencegraph line1 .then the u1d451 u1d452 u1d45d u1d461hofeachnodeinthedependencegraphiscalculatedusing abreadth firstsearchalgorithm line2 andthe u1d451 u1d452 u1d45d u1d461h u1d460 u1d45d u1d44e u1d45b iscalculatedaccording to itsdefinition line3 .after that itenumerates allpossibledepths.andifonedepthiscontainedin u1d451 u1d452 u1d45d u1d461h u1d460 u1d45d u1d44e u1d45b of at least two program units then such program units are marked parallelizable lines4 .
consider program units within a loop such as the two program units in figure c .
they can execute multiple times.
we therefore have to determine whether a program unit u1d462in different iterations can be executed concurrently.
that is to determine whether the relation u1d462 u1d462holds.fromgraphtheory wehave u1d462 u1d462ifandonlyif u1d462 isinsomenon trivialstronglyconnectedcomponents scc .if u1d462is notinanynon trivialscc then u1d462isparallelizable.figure b gives an example.
there are three sccs u1d434 u1d435 u1d436 and u1d437 among which only u1d437 is trivial so only u1d437can execute concurrently in differentiterations ofthe loop.
algorithm 3findsparallelizableswithinaloop.firstly thetarjan algorithm isusedtofindsccs line1 .nodesintrivialsccs are markedparallelizable lines2 .
the comprehension expression inpython which is actually aspecializedloop ishandledsimilarlyto handling generalloops.
837discoveringparallelismsin pythonprograms esec fse december3 san francisco ca usa algorithm3 findparallelizablesinloop input g the dependencegraph output parallelizable parallelizableprogramunits 1sccs getsccs g 2parallelizable 3forscc sccsdo 4if u1d456 u1d460 u1d447 u1d45f u1d456 u1d463 u1d456 u1d44e u1d459 u1d460 u1d450 u1d450 then parallelizable parallelizable scc 6end 7end 8returnparallelizable algorithm4 recompose parallelizable program units input parallelizable the set of parallelizableprogramunits g the dependencegraph output tasks the set of tasks 1tasks u1d462 u1d462 u1d45d u1d44e u1d45f u1d44e u1d459 u1d459 u1d452 u1d459 u1d456 u1d467 u1d44e u1d44f u1d459 u1d452 2whiletruedo 3for u1d448 u1d449 u1d461 u1d44e u1d460 u1d458 u1d460 u1d448 u1d449do if u1d462 u1d448 u1d463 u1d449 u1d462 u1d463 u1d463 u1d462and u1d462 u1d448 u1d463 u1d449 u1d462 u1d6ff u1d463 u1d463 u1d6ff u1d462then tasks tasks u1d448 u1d449 u1d448 u1d449 end 7end 8iftasksnotupdated then break 10end 11end 12returntasks .
recomposition inthisstep werecomposeparallelizableprogramunitstoconstruct tasks that correspond to a process or thread and can run concurrently.
it is possible to assign each parallelizable program unit a task.
but this will add extra overhead because creating and recycling processes or threads can be expensive.
in general we want thenumberoftaskstobeassmallaspossible whilenotharming the totalparallelism.
pyparusesheuristicstrategiestocomposeparallelizableunits into tasks.
it aims to merge adjacent parallelizable program units.
thisideaisstraightforwardandtheprocedureisshowninalgorithm4.initially wetakeeachparallelizableprogramunitasatask line .
then we try to merge different tasks.
at each step for two different tasks u1d448 u1d449 let u1d462 u1d456 u1d463 u1d457 u1d456 u1d448 u1d457 u1d449 beparallelizableprogramunitsbelongingto u1d448 u1d449respectively.if every pair of program units u1d462 u1d456and u1d463 u1d457are dependent i.e.
either u1d462 u1d456 u1d463 u1d457or u1d463 u1d457 u1d462 u1d456 andthereexistsapair u1d462 u1d4561 u1d463 u1d4571s.t.
u1d462 u1d4561 u1d6ff u1d463 u1d4571 u1d463 u1d4571 u1d6ff u1d462 u1d4561 then we merge u1d448 u1d449intoasingletask line2 .
whenloops areinvolved theaboveheuristicstrategy canalso be applied to compose parallelizables.
for a program unit u1d462in a loop which is parallelizable between different iterations i.e.
a map operation we first dynamicallymeasure thenumber ofiterations ofthisloop.ifthenumberofiterationsislargerthanathreshold say then the iteration of the loop is blocked and executions of u1d462inthe same blockare packedintothe same task.
.
dynamicselection in addition to parallelizability the usefulness of parallelizable programunitsisalsocrucial.parallelismmaynotnecessarilyleadtoacceleration.
for example if all tasks run very quickly it is not worthwhiletoparallelizethem.todeterminewhetheritisworth parallelizing we need toassign weight to each task and select the mostpromisingones according to their weights.
pypardynamically determines the weight of tasks.
the running timeofthetargettaskistakenasitsweightbecauseitisstraightforward andaccurate.
pypardirectly runs the target program and timesitscomponents.thebuilt intracemoduleofpython is used to time each function call and each task s running time is estimated with the timing result.
the number of executions of the targetprogramisconfigurable andtherunningtimesareaveraged.
amongallthe tasks onlythosewhose runningtime ishigher than a predetermined threshold are selected.
if a selected task does not have othertasks to run concurrentlywith becausesuch tasks arenotselected thenitwillberemoved.iftherearestillremaining tasks pyparthen generates a parallelism report to show parallelizable tasks and expected time savings.
the expected time savings are computed by subtracting the maximum running time of all parallelizable tasks from the total running time of all parallelizable tasks.that is u1d460 u1d44e u1d463 u1d452 u1d451 u1d461 u1d456 u1d45a u1d452 u1d461 u1d44e u1d460 u1d458 u1d45d u1d44e u1d45f u1d44e u1d459 u1d459 u1d452 u1d459 u1d456 u1d467 u1d44e u1d44f u1d459 u1d452 u1d461 u1d44e u1d460 u1d458 u1d460 u1d45f u1d462 u1d45b u1d45b u1d456 u1d45b u1d454 u1d461 u1d456 u1d45a u1d452 u1d461 u1d44e u1d460 u1d458 u1d45a u1d44e u1d465 u1d461 u1d44e u1d460 u1d458 u1d45d u1d44e u1d45f u1d44e u1d459 u1d459 u1d452 u1d459 u1d456 u1d467 u1d44e u1d44f u1d459 u1d452 u1d461 u1d44e u1d460 u1d458 u1d460 u1d45f u1d462 u1d45b u1d45b u1d456 u1d45b u1d454 u1d461 u1d456 u1d45a u1d452 u1d461 u1d44e u1d460 u1d458 for example if tasks u1d4611 u1d4612and u1d4613are parallelizable and their running times are 1s 2s .5s respectively.
then the expected time savingwillbe u1d460 u1d44e u1d463 u1d452 u1d451 u1d461 u1d456 u1d45a u1d452 u1d460 u1d460 .
u1d460 .
u1d460 u1d460.
inpractice testinputisfedtotargetprogramtofindparallelisms in the test phase and the parallelisms can be applied in the release versioninwhichotherinputscan be used.
.
rewriting pyparcanrewritetheoriginalpythonprogramtoaparallelizedversion given a desirable parallelization framework.
currently pypar supports rewriting to code using concurrent.futures python built inparallelmodule orray adistributedcomputingframework .
rewriting to other parallel frameworks can be implemented similarly.
due tosome implementation challenges auto rewritten code may suffer from issues in readability and performance for someparallelizationframeworks.
we discuss this insection .
.
.
.
discussion on possibleissues in this section we discuss some important issues regarding the usage andusefulnessof pypar.
.
.
inputsensitivity.
thedynamicselectioninsection .6uses specificinputtorunthetargetprogram.onemayarguethatthe detected parallelisms seem to be dependent on the choice of input.
here we explainthat the impact of input sensitivityislimited.
firstly the usefulness of parallelization can not be assured withoutknowingtheinputdistributioninreal worldcases.ifthetarget programrunsquicklywithallreal worldinputs parallelizationwill not be useful.
parallelization can be useful only if it can achieve considerable acceleration with some real world inputs.
thus the knowledge of input distribution is indispensable.
the dependence on input is not a drawback of pyparbecause no parallelization toolcanguaranteeusefulnesswithoutknowingreal worldinput distribution.secondly theinabilityof pypartoproveusefulnessof 838esec fse december3 san francisco ca usa siweiwei guyang song senlinzhu ruoyi ruan shihao zhu andyancai discovered parallelismsdoes not mean it is useless.
pyparaims not toprove usefulness but tofind parallelizationpossibilities.
andit does find parallelizationpossibilities which is useful at least with the input used in the dynamic selection step.
thirdly the dynamic selection step can be thought of as a way to take input distributionintoconsideration.whentheinputusedisrepresentativeof real world inputs the discoveredparallelism islikely to be useful.
.
.
falsepositives.
assection .3shows because of the imprecision of dependenceanalysis pyparmay report false positive parallelisms.hereweexplainwhyfalsepositivesarealmostinevitable anddiscuss howto handle them.
falsepositivesexistbecausethedependenceanalysisisnotaccurate.
according torice s theorem there is no general method to statically infer program properties so the static dependence analysis can not be accurate and there is a trade off between false positivesandfalsenegatives.thelackoftypeinformation since python is dynamically typed and the use of compiled functions makeitevenhardertoconductdependenceanalysisforpython.to be sound a parallelization tool will lose a large number of parallelization possibilities which makes it less useful.
so pyparadopts anunsoundanalysistocaptureasmanyparallelizationpossibilities as possible.
the experiment in section .3shows that pyparis highly precise among real world programs.
to eliminate false positives programmers need to manually check discovered parallelisms.
usually with domain knowledge of the target program it is relatively easy for programmers to find falsepositives.ifthesemanticsofthetargetprogramisavailable programmers will know what task statements do and they will know whether there are dependencies.
programmers can know whatafunction reallydoesfromitsnameorcomments and then theycanknowwhethertherearesideeffects.forexample consider theprograminfigure b thecodeinthedottedlineboxperforms fastfouriertransformationfortwoarrays andsincethetwoarrays areindependent itcanbeconcludedthatthereisnodependence between the two statements.
also there is a simple automatic method to check false positives.
run the original program and the corresponding parallelized version with the same random seed thencomparetheirresults.iftheresultdiffers thentheparallelism is false positive.
otherwise the parallelism is highly likely to be correct.this methodissimplebut effective as section .3shows.
.
.
human costs.
the human costs involved when using pypar arerelativelylow andtheyarelistedasfollows.firstly programmers need to manually check the discovered parallelisms and rule out falsepositives.
thisprocess willnot costtoo muchbecauseof the domain knowledgeof programmers see section .
.
and the highprecisionof pypar seesection .
.secondly programmers may want to slightly adjust the auto rewritten code to increase its readabilityandreduceoverhead asdiscussedinsections .7and .
.
.this processisrelativelysimple.
evaluation this section presents an evaluation of pyparon real world python packages.theevaluationisconductedonamachinewithintel r xeon r platinum cpu cores in total installed with the ubuntu20.
.3os andthe official cpython3.
.
interpreter.table statistics ofthe selected python packages package version category stars github loc func scikit image .
.
image processing .2k .0k scipy .
.
scientific computing .6k .5k librosa .
.
audio processing .6k .8k trimesh .
.
triangularmeshes .2k .7k scikit learn .
.
machinelearning .3k .3k seaborn .
.
datavisualization .7k .4k .
packagesundertest toevaluatetheeffectivenessof pypar wehandpickedasetofsix real world open source python packages.
we select packages from twocollectionsofpythonprograms ongithubasfollows.we firstskip packages undersomecategoriesthat are not suitable for evaluating pypar.thisisbecausesomeofthemaremainlywritten in low level compilable languages such as fortran or c for performancereasonsandonlyprovidepythonapis.thesepackages include numpy for numerical computing tensorflow for machinelearning etc.andsomearealreadywellparallelized such aspackagesunderthecategoriesofdatabaseandwebcrawling.
also applications in some categories are not performance critical such as packages under the categories of command line tools and gui development.
afterthat weselectpackagesunderthecategoriesofaudio imageprocessing machinelearning datavisualization science and meshing.foreachofthesecategories weselect themostpopular package under it according to github stars.
finally widely used real world open source python packages are selected scikit image scipy librosa trimesh scikit learn andseaborn.table1summarizes the statistics of these package including their versions categories the number of stars the number of lines of code loc and the number offunctions func .we brieflyintroduce thembelow.
scikit image isapythonpackagefordigitalimageprocessing.itisacollectionofimageprocessingalgorithms including apis for filtering morphology segmentation andothers.
scipy isapythonpackageforscientificcomputing including supportforlinearalgebra sparsematrices etc.itextendsnumpy andprovidesadditionaltoolsandeasy to usedatastructures.
librosa isapythonpackageformusicandaudioanalysis.it providesbuildingblocksformusicinformationretrievalsystems.
trimesh isapythonpackageforprocessingtriangularmeshes.
itprovidesutilitiesforloadingandprocessingtriangularmeshes especiallyfor watertight surfaces.
scikit learn isapythonpackageformachinelearning.it provides simple and efficient tools for predictive data analysis.
it isaccessible to everybody andreusableinvariouscontexts.
seaborn is a python data visualization package based on matplotlib.itcomeswithahigh levelinterfacefordrawingattractive andinformative statisticalgraphics.
.
input generation as discussed in section .
dynamic selection is important for determining whether some discovered parallelism can bring acceleration.
to time a function we have to run it.
so we need to find or generate inputs for it.
however inputs that are suitable for findingparallelismsarenotimmediatelyavailableforthefollowing 839discoveringparallelismsin pythonprograms esec fse december3 san francisco ca usa table parallelization statistics package func time reported fp par.
par.rate precision scikit image 15m .
.
scipy 30m .
.
librosa 5m .
.
trimesh 2m .
.
scikit learn 24m .
.
seaborn 2m .
.
total 78m .
.
reasons.first theinputsprovidedbythesepackagesarenotsuitable.
this is because the contained sample inputs for functions are trivial theyaremostlytestinputsfordemonstratingpackagesetup andcannotreflecttheusefulnessofparallelisms.second itisnot feasibletouseopen sourceprogramsthatusethesepackagesfor inputs.thisisbecausesuchprogramsusuallyonlycoverasmall portion of functions in target packages thus they cannot test them thoroughly.
furthermore even such programs themselves need inputs buttypicallyonlytrivialinputsareavailable.tosolvethis problem weuseempiricalmethodstogenerateadditionalinputs for thesepackages.
.
.
inputgenerationforscikit image.
scikit image isanimage processing package and most of its functions have at least one input argument named image or im .
as the input image gets larger suchfunctionsrunslower andweexpectthatwithaninput imageofpropersize thefunctionwillrunforsometime makingthe timingresultmeaningful.theinputimagesarerandomlygenerated and we use binary search to determine the proper size.
if there are multiple imageinputs we keeptheirshape the same.
besides the image arguments other arguments need to be determined.scikit image usespytest for unit tests with test scripts coveringmostfunctions.weruntheunittestandusebuilt intrace module tocollectargumentvalues.thesevaluesareusedto feed argumentsother than images andtheyremain the same during the binary search.
finally we generated inputs successfully for 242functions makingthemrun for more than1second.
.
.
inputgenerationforscipy scikit learn andseaborn.
scipy isanextensionof numpy.mostofitsfunctionshandledataoftype u1d45b u1d451 u1d44e u1d45f u1d45f u1d44e u1d466.altdefined in numpy which is n dimensional arrays and vectors and matrices are special types of u1d45b u1d451 u1d44e u1d45f u1d45f u1d44e u1d466.alt.scipyalso has a richsetof pytestunittestscripts andbytracingtheirexecutions we collectvaluesofargumentsforeachfunction.manyofthemhaveat least one argument withtype u1d45b u1d451 u1d44e u1d45f u1d45f u1d44e u1d466.alt.we expectthese functions to run slower as the u1d45b u1d451 u1d44e u1d45f u1d45f u1d44e u1d466.alttyped input gets larger.
similar to whatisdescribedinsection .
.
weuseabinarysearchmethod to determine the proper size of input u1d45b u1d451 u1d44e u1d45f u1d45f u1d44e u1d466.alt.
the scaled u1d45b u1d451 u1d44e u1d45f u1d45f u1d44e u1d466.alt inputsaregeneratedrandomly.iftherearemultiple u1d45b u1d451 u1d44e u1d45f u1d45f u1d44e u1d466.alttyped inputs theirsizesarescaledproportionally.inputsthatarenotof type u1d45b u1d451 u1d44e u1d45f u1d45f u1d44e u1d466.altremain the same.
similarly scikit learn uses u1d45b u1d451 u1d44e u1d45f u1d45f u1d44e u1d466.alt object to represent training data for machine learning and seaborn uses u1d45b u1d451 u1d44e u1d45f u1d45f u1d44e u1d466.altobject to represent the data it aims to visualize.
so weadaoptedthesameapproachtogenerateinputforthem.finally we generated inputs successfully for functions in scipy functions in scikit learn and functions in seaborn making them run for longer than1second.
.
.
input generation for librosa and trimesh.
we didn t find any empiricalrulesforfunctioninputin librosaandtrimesh.sowemanually construct inputs using their documentations as