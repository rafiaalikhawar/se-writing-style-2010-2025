varclr variable semantic representation pre training via contrastive learning qibin chen qibinc cs.cmu.edu carnegie mellon universityjeremy lacomis jlacomis cs.cmu.edu carnegie mellon universityedward j. schwartz eschwartz cert.org carnegie mellon university software engineering institute graham neubig gneubig cs.cmu.edu carnegiemellon universitybogdan vasilescu bogdanv cs.cmu.edu carnegie mellon universityclaire le goues clegoues cs.cmu.edu carnegie mellon university abstract variable names are critical for conveying intended program behavior.machinelearning basedprogramanalysismethodsusevariable name representations for a wide range of tasks such as suggesting newvariablenamesandbugdetection.ideally suchmethodscould capture semantic relationships between names beyond syntactic similarity e.g.
thefactthatthenames averageand meanaresimilar.unfortunately previousworkhasfoundthateventhebestof previousrepresentationapproachesprimarilycapture relatedness whether two variables are linked at all rather than similarity whether they actually have the same meaning .
we propose varclr a new approach for learning semantic representations of variable names that effectively captures variable similarity in this stricter sense.
we observe that this problem is an excellent fit for contrastive learning which aims to minimize the distancebetweenexplicitlysimilarinputs whilemaximizingthe distancebetween dissimilarinputs.
thisrequires labeled training data and thus we construct a novel weakly supervised variable renaming dataset mined from github edits.
we show that varclr enables the effective application of sophisticated general purpose languagemodelslikebert tovariablenamerepresentationand thus also to related downstream tasks like variable name similarity search or spelling correction.
varclr produces models that significantly outperform the state of the art on idbench an existing benchmarkthatexplicitlycapturesvariablesimilarity asdistinct from relatedness .
finally we contribute a release of all data code andpre trainedmodels aimingtoprovideadrop inreplacementfor variable representations used in either existing or future program analysesthatrely on variable names.
ccsconcepts softwareanditsengineering softwarelibrariesandrepositories computingmethodologies learninglatentrepresentations naturallanguageprocessing neuralnetworks .
this work is licensed under a creative commons attribution international .
license.
icse may pittsburgh pa usa copyright held by the owner author s .
acm isbn .
reference format qibin chen jeremy lacomis edward j. schwartz graham neubig bogdan vasilescu andclairelegoues.
.varclr variablesemanticrepresentation pre training via contrastive learning.
in 44th international conference on software engineering icse may pittsburgh pa usa.
acm newyork ny usa 13pages.
introduction variable namesconveykey informationaboutcodestructure and developer intention.
they are thus central for code comprehension readability and maintainability .
a growing array of automatic techniques make use of variable names in the context of tasks like but not limited to bug finding or specificationmining .beyondleveragingtheinformationprovidedby names in automated tools recent work has increasingly attempted to directly suggest good or improved names such as in reverse engineering or refactoring .
developing and evaluating such automated techniques or name basedanalyses reliesinlargepartontheabilitytomodel andreasonabouttherelationshipsbetweenvariablenames.forconcreteness consider an analysis for automatically suggesting names in decompiled code.
given a compiled program such that variable names are discarded that is then decompiled resulting in generic names like a1 a2 a renaming tool seeks to replace the generic decompiler provided identifiers with more informative variable names for the benefit of reverse engineers aiming to understand it.
goodnamesinthiscontextarepresumablycloselyrelatedtothe namesusedintheoriginalprogram beforethedeveloper provided names were discarded .
a variable originally named max for example and then decompiled to a2 should be replaced with a name at leastcloseto max like maximum.modelingthisrelationshipwellis key for both constructing and evaluating such analyses.
accurately capturing and modeling these relationships is difficult.
a longstanding approach has used syntactic difference likevariousmeasuresofstringeditdistance toestimatetherelationshipbetweentwovariables suchasforspellchecking .
however syntacticdistanceisquitelimitedincapturingunderlying name semantics.
for example the pairs minimum maximum and minimum minimal are equidistant syntactically with a levenshteindistanceof two but maximumand minimumare antonyms.
morerecentworkhassoughttoinsteadencodevariablename semanticsusingneuralnetworkembeddings informingavarietyof ieee acm 44th international conference on software engineering icse icse may21 pittsburgh pa usa qibin chen jeremy lacomis edward j. schwartz graham neubig bogdan vasilescu and claire le goues name based analyses .
unfortunately although state ofthe art techniques for variable name representation better capture relatedness theystillstruggletoaccuratelycapturevariablename similarity in terms of how interchangeable two names are.
variables may be related for a variety of reasons.
while maximumand minimumare highly related they certainly cannot be substituted foroneanotherin a codebase.
minimumand minimal onthe other hand arebothrelatedandverysimilar.inrecentwork wainakh etal.
presentedanoveldataset idbench basedonahuman survey on variable similarity and interchangeability and used it to evaluatestate of the artembeddingapproaches.theyempirically establishedthatthereremainssignificantroomforimprovement in terms of capturing similarity rather than merely relatedness.
in this paper we formulate the variable semantic representation learning problem as follows given a set of variable data learn a function fthatmaps avariablenamestring toalow dimensional densevectorthatcanbeusedinavarietyoftasks likethetypesof name basedanalysesdiscussedabove .tobeuseful suchamapping function should effectively encode similarity i.e.
whether two variables have the same meaning.
that is f minimum and f minimal should be close to one another.
importantly however the function should also ensure that variable names that are not similar regardlessofrelatedness!
are farfromoneanother.that is f minimum andf maximum should be distant.
our first key insight is that this problem is well suited for a contrastive learningapproach .conceptually contrastive learning employs encoder networks to encode instances in this task variables into representations i.e.
hidden vectors withagoalofminimizingthedistancebetween therepresentation of similarinstancesandmaximizingthedistancebetween therepresentation of dissimilar instances.
contrastive learning requires asinputasetof positivepair examples ofsimilarvariables in our case fortraining.
oursecondkeyinsightisthatwecanconstructasuitableweaklysupervised dataset of examples of similar variables by taking advantage of large amounts of source control information on github.
following the definition of similarity from prior work weconsidertwovariablenamesaresimilariftheyhavethesame meaning orare interchangeable .wethereforeautomaticallymine sourcecontroleditstoidentifyhistoricalchangeswheredevelopers renamedavariablebutdidnototherwiseoverlymodifythecode in which it was used.
although potentially noisy this technique matchesanintuitiveunderstandingofvariablenamesimilarityin terms of interchangeability and allows for the collection of a large dataset whichwe call githubrenames.
finally we observe that the variable semantic representation learningproblemrequiresmorepowerfulneuralarchitecturesthan word2vec based approaches .1such approaches are limited both empirically as wainakh et al.
showed and conceptually notefor example that theycannot capture componentordering such as the difference between idx to word andword to idx .
meanwhile pre trained language models plms based onthepowerfultransformerarchitecture haveachievedthe state of the art on a wide range of natural language processing 1word2vec is an embedding algorithm based on the distributional hypothesis which assumes words that occur in the same contexts tend to have similar meanings.tasks including text classification question answering and summarization anddialogsystems .plmstailoredspecifically for programming languages such as codebert and codex are useful in a variety of tasks such as code completion repair andgeneration thoughnotyetforvariablenamerepresentation.
encouragingly previouswork showsthat contrastive learning can strongly improve bert sentence embeddings for textualsimilaritytasks .and contrastivelearninghasbeenshown to benefit from deeper and wider network architectures .
wecombinetheseinsightstoproducevarclr anovelmachine learningmethodbasedoncontrastivelearningforlearninggeneralpurposevariablesemanticrepresentationencoders.invarclr the contrastivelearningelementservesasapre trainingstepforatraditionalencoder.whilepowerfulmodernapproacheslikecodebert performpoorlyonthevariablerepresentationproblemoff the shelf we show that varclr trained models dramatically outperform the previous state of the art on capturing both variable similarity and relatedness.varclrisdesignedtobegeneraltoavarietyofuseful downstreamtasks wedemonstrateitseffectivenessforboththebasic variable similarity relatedness task using the idbench dataset asagoldstandardbaseline aswellasforvariablesimilaritysearch and spelling error correction.
to summarize our main contributions are as follows varclr a novel method based on contrastive learning that learns general purpose variable semantic representations suitablefor a variety of downstream tasks.
anewweaklysuperviseddataset githubrenames forbetter variable representation learning consisting of similar variablenames collected from real world github data.
experimentalresultsdemonstratingthatvarclr smodels significantlyoutperformstate of the artrepresentation approaches on idbench an existing benchmark for evaluating variable semantic representations.
these results further substantiate the utility of more sophisticated models like codebert withlargermodelcapacity inplaceoftheprevious word2vec basedmethodsforlearningvariablerepresentations while showing that the contrastive learning pre trainingstepiscriticaltoenablingtheeffectivenessof suchmodels.
experimental results that demonstrate that both unsupervised pre training and our proposed weakly supervised contrastivepre trainingareindispensablepartsforadvancing towards the state of the art for the former takes advantage of greater data quantity by leveraging a huge amount of unlabeled data while the latter takes advantage of better dataquality with ournew githubrenamesdataset.
finally we contribute a release of all data code and pre trained models aimingtoprovideadrop inreplacementforvariablerepresentationsusedineitherexistingorfutureprogramanalysesthat rely on variable names.
problem domain variable names critically communicate developer intent and are thusincreasinglyusedbyavarietyofautomatedtechniquesasa centralsourceofinformation.suchtechniquesincreasinglyrelyon 2code data andpre trainedmodelavailableat 2328varclr variable semantic representation pre training via contrastive learning icse may pittsburgh pa usa machine learning and embedding based representation approaches to encode variable name meaning for these purposes.
however recent work shows that while neural embeddings based on techniques like word2vec do a better job of capturing relationships between variables than syntactic edit distance does they stillstruggletocaptureactualvariablesimilarityintermsoftheir interchangeability.
in this paper we show that this problem is amenable to a contrastive learning approach enabling accurate general purposerepresentations of variable name semantics.
we define the variable semantic representation learning problem as follows given a collection of suitable variable data learn a functionfthatmapsavariablenamestringtoalow dimensionaldense vectorthatcanbeusedtobenefitvariousdownstreamtasks like variablesimilarityscoringin the simplest case or arbitrarily complexname basedanalyses .agoodmappingfunction fforvariable namerepresentations should capture similarity.
fshould encode similarnames such that theyareclosetooneanother.twonamesaresimilarwhen they have similar or generally interchangeable meanings likeavgandmean.
this isespecially important for variables thatarerelatedbutnotsimilar suchas maximumandminimum.
indeed antonymsareoftencloselyrelatedandcanappear in similar contexts maxand minfor example may be used togetherin loops finding extrema .
capture component ordering and importance.
variables often consist of component words or sub words.
we observe that the order of such components can affect meaning.
for example idx to word andword to idx contain the same subwords but have different meanings.
moreover the importanceofdifferentcomponentwordsinavariablecanbedifferent and the importance of the same word can vary between variables.
forexample invariables onaddand onremove on is less important while addandremoveare more important.
inturnonandturnoff onandoffare more important than turn.
a good mapping function fshould be able to capture these differences instead of treating variables as an unordered bag of sub words.
transferability.
therepresentationshouldbegeneral purpose andusableforawiderangeoftasks.benefitsofatransferable sharedrepresentationincludetheabilityto improve accuracy on unsupervised or data scarce tasks where it can be hard to obtain high quality variable representations from scratch and for complex tasks consisting of many subtasks makebetteruseoflabeleddatafrommultiplesub tasks via multi tasklearning.
thisformulationoftheproblemmotivatesouruseof contrastive learning which is an effective way to learn similarity from labeled data.conceptually givenanencodernetwork f andasetofsimilar positive pairs contrastive learning returns a newencoder that attemptstolocatesimilar positivepair instancesclosertogetherand dissimilar negativepair instancesfartherapart.inpractice this can be accomplished by re training the original encoder on a new pre training task instance discrimination .
instance discrimination casts the contrastive learning problem as a classification problem where only the positive pair instances are equivalent.
ratherthanexplicitlyadjustingthedistancesbetweenpoints the7rnhql hu variable varclr qfrghu 9duldeoh 5hsuhvhqwdwlrq rzqvwuhdp 7dvnv 6lplodulw 6frulqj 6lplodulw 6hdufk 6shoolqj uuru ruuhfwlrq5hodwhgqhvv 6frulqj lw xe5hqdphv variable pairs encodervarclr rqwudvwlyh 3uh wudlqlqj f f prime figure conceptualoverview of varclr.
encoder s parameters are trained to optimize its performance at discriminating similar instance from dissimilar instances.
this naturallyadjuststheparametersoftheencodersuchthatsimilarinstances are moved closer together and vice versa for dissimilar instances .
the actual output of the contrastive learning process is a newencoder f primethatisidentical to theoriginal encoder inneural architecture but has a different set of parameters primeresulting from training on the instance discrimination task.
there are two central design choices in applying contrastive learning however.
first which neural architectures should beused forf ?this is usually decided by the problem domain in question.
for example in computer vision resnet for learning image representations innaturallanguageprocessing simple word embedding orbert for learningsentence representations and in data mining graph neural network forlearninggraphrepresentations .second howtoconstruct similar positive and dissimilar negative training pairs?
unsuperviseddataaugmentationlikecroppingorclippinghasbeenusedto create different views of the same image as similar pairs in image processing word dropout can augment text sentences for natural language processing .
for supervised contrastive learning positive pairs can be created from labeled datasets directly orviasamplinginstancesfromthesameclass .note thatdissimilarpairstypicallyneednotbeexplicitlydefined.instead in batch negatives can be sampled from instance pairs that are not explicitly labeled as positive.
thechoiceofsimilarinstancesisveryimportant asitinfluences the learned similarity function and impacts downstream effectiveness .
for example consider how training can lead to unintentionalpropertiesofalearnedsimilarityfunctionfor word2vec .at a highlevel word2vec canbe viewed asa form ofunsupervisedcontrastivelearning.itemploysawordembeddinglayeras the encoder and treats words co occurring in the context window as similar pairs while treating other words in the dictionary as dissimilarones.3duetoitschoiceof similarinstances itlearns moreofassociation orrelatedness betweenwords insteadofsimilarityin terms of how interchangeable two words are.
for example word2vec embeddings of cohyponym words such as red blue white green are very close.
while this might not be a problem innlpapplications word2vec leadstounsatisfactorybehavior 3weleaveouttheminordifferencethat word2vec producestwosetsofembeddings whilecontrastive learning usually uses a unified representation.
2329icse may21 pittsburgh pa usa qibin chen jeremy lacomis edward j. schwartz graham neubig bogdan vasilescu and claire le goues whenappliedtovariablenames e.g.
byidentifying minlength and maxlength as similar.
method figure1showsahigh levelconceptualoverviewofvarclr our frameworkfor learningeffective semanticrepresentations ofvariable names.
varclr consists of a contrastive pre training phase that takes two inputs a positive set of similar variable name pairs and aninputencoder.thesetofsimilarvariablesiscrucial forvarclr sperformance.wethusproducegithubrenames a novelweakly superviseddatasetconsistingofpositiveexamples of similar variables by examining large amounts of source code historyavailablefromgithub section3.
.thesevariablesmust be suitably tokenized for encoding in a way that captures and retains relevant information section .
both for pre training and for downstream tasks.
varclr also takes an input encoder f withlearnableparameters section3.
.thisencoderisthen trained using contrastivelearning section .
.
the outputof our framework is a contrastively trained varclr encoder that converts tokenized variables into semantic representations suitable for a variety of tasks and name based analyses including similarity scoringor spelling error correction among others.
.
similarvariables githubrenames ahigh leveldefinitionof similarity isthedegreetowhich twovariableshavethesamemeaning.contrastivelearningrequires positiveexamplesfortraining andthusweneedasetofappropriate positive pairs of similar variable names.
as discussed in section theseneednotbemanuallyconstructed.althoughidbench providescuratedsetsofhuman judged similar variables theyare too small for training purposes the largest set has variable pairs .thismotivatesanautomatedmechanismforconstructing trainingdata withtheaddedbenefitthatweneednotbeconcerned about training and testing on the same dataset as we use idbench for evaluation .
instead we observe that one way to define variable similarity is toconsiderthedegreetowhichtwovariablesareexplicitly interchangeable in code close to idbench s definition of contextual similarity .
we therefore collect a weakly supervised dataset of interchangeablevariablenamesbyminingsourcecontrolversion historiesforcommitswherevariablenameschange.thesevariable pairsareconsideredsimilarbecausetheyappearinterchangeable in the same code context.
concretely webuiltuponexistingopen sourcedatasetcollection code used to mine source control for the purpose of modeling changes .4givenarepository thiscodeminesallcommitsof lessthansixlinesofcodewhereavariableisrenamed.theintuition is to look for commits that do not make large structural changes thatmightcorrespondtoamajorchangeinavariable smeaning.
we applied dataset collection to an expanded version of the list of repositories used in ref consisting of c projects.5the final githubrenames dataset contains 855variable pairs each consisting of a variable name before and after a renaming commit.
developers were not asked to label variable pairs explicitly.
the dataset may thus be noisy and in particular we did not attempt to filter out renames corresponding to bug fixes.
indeed we note that anumberofpairsingithubrenamescorrespondtofixingspelling mistakes section .
.
overall however we note that our method transfers well to the idbench validation set and expect that more datawill onlyimprove varclr s effectiveness.
.
inputrepresentation avariablenameasatextstringmustbepreprocessedtobeused as input to a neural network encoder.
we observe two interesting aspects of variable names that inform our preprocessing.
first variablenamesareoftencomposedofmultiplewordswithinterchangeablecasestyles e.g.
max iteration vsmaxiteration .s ec ond variable names are sometimes composed of short words or abbreviations withoutanunderscoreoruppercasetoseparatethem.
e.g.
filelist sendmsg.
forthefirstproblem weapplyasetofregexrulestocanonicalize variable names into a list of tokens e.g.
.
thesecondproblemismorechallenging andcouldcauseout ofvocabulary oov problems.
to solve this we use the pre trained codeberttokenizer whichisunderlyingabytepairencoding bpe model trained on a large code corpus based on token frequencies.whenencounteringanunknowncompositevariable name such as sendmsg it is able to split it into subword tokens e.g.
where means this token is a suffix of the previous word.
.
encoders generally a neural encoder takes the input sequence and encodes and aggregates information over the sequence to produce a hiddenvector.thatis givenasequenceoftokens v v1 v2 ... vn corresponding to a tokenized variable name an encoder outputs a hidden vector h rd wheredis the dimension of the hidden representation h f v f denotes the encoder withlearnableparameters .
notethatvarclrisapplicabletoanyencoderwiththisform.
in this paper we instantiate it specifically for word embedding averaging varclr avg thelstmencoder varclr lstm and bert varclr codebert .
word embedding averaging.
averaging the embeddings of input tokens is a simple but effective way to represent a whole input sequence givensufficientdata .therefore weconsiderthis asasimplebaselineencoder.formally giventhetokenizedvariable namev v1 v2 ... vn vi v and a word embedding lookup tablel v rd h nn summationdisplay.
i 1l vi wherevis the vocabulary i.e.
the collection of all tokens the model can handle r v d is the learnable embedding matrix.
although simple and efficient this word embedding averaging encodersuffersfrom two issues order.
the averaging operator 2330varclr variable semantic representation pre training via contrastive learning icse may pittsburgh pa usa 9duldeoh v1 i encoder f encoder f 9duldeohv2 i v2 j v2 kfrv g 6fruhv f 5hsuhvhqwdwlrqv rqwudvwlyh rvvv1 iv2 i v2 j v1 j v2 k v1 k lw xe5hqdphvvlplodu glvvlploduqi d 9duldeoh 3dluv e 0lql edwfk lnceki kj kk figure2 overviewof varclr scontrastivepre trainingmethod.a githubrenames containsinterchangeablevariablepairs.b ateachtraining step sampleamini batchofvariablepairs andaimtopullclosethevariablesrepresentationswithinapair e.g.
v1 iandv2 i whilepushing awaytherepresentationsofothervariables e.g.
v1 iandv2 j.c toachievethis anencoder f withlearnableparameters isadoptedtoencode thevariablestringtohiddenvectors.d contrastivelossiscalculatedbasedonthesimilarityscoresasthecosinedistancebetweenencoded hiddenvectors the encoder f is optimized with gradient descent.
discards word orderinformation inthe input sequence and thus poorlyrepresentsvariablenameswherethisorderisimportant e.g.
idx to word andword to idx .
tokenimportance .anunweighted average of word embeddings ignores the relative importance of words in a variable name as well as the fact that the importance of a word can vary by context.
lstm encoder.
recurrent neural networks rnns generalizefeed forwardneuralnetworkstosequences.giventhetokenized variablename v v1 v2 ... vn astandardrnncomputesasequence of hidden vectors h1 h2 ... hn .
ht sigmoid parenleftbig whxl e vt whhht parenrightbig wherewhx whh rd dareweightmatrices and eistheembedding matrix as in equation .
rnns process the input sequence by reading in one token vtat a time and combining it with the pastcontext ht .thiscapturessequentialorderinformation.after processing all input tokens we can average the hidden states at each step to output a representation of the original variable h nn summationdisplay.
i 1hi.
we use bi directional long short term memory lstm models a variant of rnns widely used in natural language processing.lstmsintroduceseveralnewcomponents includingthe input and forget gates controlling how much information flows from the current token and how much to keep from past contexts respectively.
this better handles the token importance problem by dynamically controlling the weight of the input token at each step.
bert.transformer basedmodels typicallyoutperformlstms andareconsideredtobethebetterarchitectureformanynlptasks.
pre trainedlanguagemodels plms builtupontransformers can leverage massive amounts of unlabeled data and computational resources to effectively tackle a wide range of natural language processing tasks.
useful plms for programming languages includecodebert and codex plms not only capture component ordering and token importance that lstms do but provide additionalbenefits bert basedmodelsarealreadypre trained withself supervisedobjectivessuchasmaskedlanguagemodeling mlm onalargeamountofunlabeleddata.itprovidesagood initialization to the model parameters and improves the model s generalizationability requiringfewer data to achievesatisfactory performance .
transformerencodersaremuchmorepowerful thanpreviousmodelsthankstothemulti headself attentionmechanism allowingforthemodeltobemuchwideranddeeperwith more parameters.
we therefore propose to use plms for programs as our most powerful choice of variable name encoder.
effectiveness versus efficiency.
although bert has the largest model capacity ofthese encoders it alsorequires highercomputation cost for both training and inference and suffers from a longer inference latency.
the trade off posted between effectiveness and efficiency can vary according to different downstream applications.
therefore wefinditmeaningfultocompareallencodersinvarclr.
different or better encoder models can be directly plugged into the varclr framework in the future.
we omit further interior technicaldetailsofbothlstmandbertmodelsastheyarebeyond the scope of this paper.
.
contrastive learning pre training varclrimplementsthedesignchoicesforinputdata variabletokenization andinput encoderin a contrastivelearning framework.
figure provides an overview.
conceptually contrastive learning uses encoder networks to encode instances in this task variables intorepresentations i.e.
hiddenvectors andaimstominimizethe distance between similar instances while maximizing the distance between dissimilar instances.
specifically givenachoiceofencoderandsetoflabeled positive pairs of variable names we use instance discrimination a s ourpre trainingtask andinfonce asourlearningobjective.
given a mini batch of encoded and l2 normalized representations 2331icse may21 pittsburgh pa usa qibin chen jeremy lacomis edward j. schwartz graham neubig bogdan vasilescu and claire le goues ofksimilarvariablepairs braceleftbig v1 i v2 i i ... k bracerightbig wefirstencode themto hidden representations qi f v1 i bardblex bardblexf v1 i bardblex bardblex2 ki f v2 i bardblex bardblexf v2 i bardblex bardblex2 where bardbl bardbl2isl2 norm f denotes the encoder.
then we define the infonceloss as lnce q k e parenleftbigg logeqi latticetopki summationtext.1k j 1eqi latticetopkj parenrightbigg where is the temperature hyperparameter introduced by .
intuitively this objective encourages the model to discriminate the corresponding similar instance v2 iof an instance v1 ifrom other instances in the mini batch v2 j. this learning objective is very similar tothe cross entropyloss forclassification tasks while the difference is that instead of a fixed set of classes it treats each instance as a distinct class.
following we further make the loss symmetricand minimizethe followingobjective function l 2lnce q k 2lnce k q .
in our task this objective encourages the encoder to push the representations of a pair of similar variables to be close to each other so that they can be discriminated from other variables.
we refer to this process as pre training in the sense that the training is not intended for a specific task but is learning a generalpurpose variablerepresentation.
experiments in this section we evaluate varclr s ability to train models for variable representation along several axes.
section .
addresses setup datasets andbaselinescommontotheexperiments.then we begin by addressing a central claim how well do varclr models encode variable similarity as distinct from relatedness ?
we answer this question by using pre trained varclr models to compute similarity and relatedness resp scores between pairs of variables andevaluatetheresultsonhuman annotatedgoldstandardground truthbenchmark section .
.
next weevaluatevarclr trainedmodelsontwootherdownstreamtasks demonstratingtransferability variablesimilaritysearch section4.
and variable spelling error correction section .
.
finally weconductanablationstudy section4.
lookingatthe influence of training data size pre trained language models and pre trained embeddings from unsupervised learning contribute to varclr s effectiveness.
.
setup pre training.
forvarclr avgandvarclr lstm weusethe adam optimizer with 1 .
2 .
a learningrateof0.
andearlystopaccordingtothecontrastive lossonthevalidationset.weuseamini batchsizeof1024.theinput embedding and hidden representation dimensions are set to and150respectively.wealsoinitializetheembeddinglayerwiththe codebertpre trainedembeddinglayer.forvarclr codebert weusetheadamwoptimizer withthesameconfigurationandlearningrate andamini batchsizeof32.6weusethebertmodel architecture and initialize the model with pre trained weights from codebert .
for all three methods we apply gradient normclippingintherange andatemperature of0.
.a summary of the hyper parameters can be found along with our data code and pre trained models at dataset.while we use the githubrenames for training varclr we use the idbench dataset for evaluation.7idbench is a benchmark specifically created for evaluating variable semantic representations.itcontains pairsofvariablesassignedrelatedness andsimilarityscoresbyreal worlddevelopers.idbenchconsists of three sub benchmarks idbench small idbench medium and idbench large containing pairs of variables respectively.
ground truth scores for each pair of variable are assessed by multiple annotators.
pairs with disagreement between annotatorsexceedingaparticularthresholdareconsidereddissimilar the three benchmarks differ in the choice of threshold.
the smaller benchmark provides samples with higher inter annotator agreement while the larger benchmark provides more samples with commensuratelyloweragreement.themediumbenchmarkstrikes abalance.wedescribecustomizationsoftheidbenchdatasetto particulartasks in their respective sections.
baselines.
we compare varclr models to the previous state ofthe art as presented in idbench .
we reuse the baseline results providedbytheidbenchframework.theidbenchpaperevaluates anumberofpreviousapproachesaswellasanewensemblemethod thatoutperformsthem we includeas baselines asubset of those previoustechniques andtheensemblemethod.ofthestringdistance syntacticfunctions stillbroadlyusedinvariousname related applications weinclude levenshteineditdistance lv the number of single character edits required to transform one string into the other it performs in the top half of techniques on scoring similarity and is competitive with the other syntactic distance metric on relatedness.
of the embedding based single models we include fasttext cbow ft cbow and sg ft sg extensionsof word2vec thatincorporatesubwordinformation tobetterhandleinfrequentwordsandnewwords.these were the best performing embedding based methods on both relatedness and similarity.
finally we include two combined models.
idbench p r o posesanensemblemethodthatcombinesthescoresofallmodels and variable features.
for each pair in idbench the combined model trains a support vector machine svm classifier with all otherpairs thenappliesthetrainedmodeltopredictthescoreof the left out pair.
note that this approach is trained on the idbench benchmarkitselfandis notdirectlycomparabletoothermethods.
for comparison we add varclr avg varclr lstm varclrcodebert scores as additional input features to the combined approach and report the results for combined varclr.
6larger mini batch sizes make the contrastive learning task more challenging and improvethequalityoflearnedrepresentation asshownin andourpreliminary experiments.
we use batch size of for varclr codebert due to gpu memory limitations.
7the idbench evaluation scripts were updated after publication leading to minor differencesinevaluationscores.weusetheirlatestcodeasofmay1st 2021toevaluate thebaselinesandour models.
2332varclr variable semantic representation pre training via contrastive learning icse may pittsburgh pa usa .2variable similarity and relatedness scoring our central claim is that varclr is well suited to capturing and predictingvariablesimilarity.formally giventwo variables uand v we obtain variable representations with pre trained varclr encoderf primeand compute the variable similarity score as the cosine similaritybetween the two vectors hu hv f prime u f prime v s u v hu hv bardblhu bardbl2 bardblhv bardbl2 where s u v denotes the varclr s predicted similarity score.
following idbench we then compare the similarity scores of pre trainedvarclrrepresentationswithhumanground truthsimilarityscoresbycomputingspearman srankcorrelationcoefficient between them.
this correlation coefficient falls in the range where1indicatesperfectagreementbetweentherankings indicatesperfect disagreement and indicates no relationship.
note that the varclr pre training task is explicitly optimizingthedistancebetweensimilarvariablepairs.thus thevariable similarity scoring task only really evaluates the performance of the pre training itself.
to more fully evaluate whether our method leadstobetterrepresentationsthatcantransfer wealsoevaluate on the variable relatedness scoring task.
results.table1showsthemodels performanceonthesimilarity and relatedness tasks in terms of spearman s rank correlation with groundtruth.table1ashowsthatvarclr codebertimproves overthepreviousstate of the artonallthreeidbenchbenchmarks withanabsoluteimprovementof0.18onidbench smalland0.13on idbench largecomparedtothepreviousbestapproach ft cbow.
thisshows thatvarclralignsmuchbetterwith humandevelopers assessment of variable similarity than any of the previously proposed models.
interestingly varclr avg also outperforms ftcbow by a large margin .
on idbench small .
this suggests that most of our gains do not come from the use of a more powerfulencoderarchitecturesuchasbert.instead weconcludethat thegithubrenamesdatasetiseffectiveatprovidingsupervision signalsofvariablesimilarity andthecontrastivelearningobjective is effective.
although their architectures are very similar varclravg outperformsft cbow.
that said the improvements in varclr codebert .
and varclr .
over varclr avg verify our assumption that powerful models with larger representational capacity are necessary for learning better variablerepresentations since they are able to capture and encode more information e.g.
sequential order and tokenimportance thanthe embedding averaging methods.
table1bshowsthatvarclralsoachievesthestate of the art performance on idbench in terms of relatedness prediction.
it surpasses the previous best by .
on idbench small and .
on idbench large.
this is noteworthy because varclr training does not explicitly optimize for relatedness.
this suggests that the varclr pre training task learns better generic representations ratherthanoverfittingtothetargettask i.e.
variablesimilarity .
thisisveryimportant andsupportsourmajorcontribution bypretrainingforthesimilaritylearningtaskongithubrenameswith a contrastive objective varclr achieves better representations whichcan be applied to general tasks.table spearman s rank correlation with idbench small idbench medium idbench largeofsinglemodels top and ensemblemodels bottom by increasing performance.
a similarity scores method small medium large ft sg .
.
.
lv .
.
.
ft cbow .
.
.
varclr avg .
.
.
varclr lstm .
.
.
varclr codebert .
.
.
combined idbench .
.
.
combined varclr .
.
.
b relatedness scores method small medium large lv .
.
.
ft sg .
.
.
ft cbow .
.
.
varclr avg .
.
.
varclr lstm .
.
.
varclr codebert .
.
.
combined idbench .
.
.
combined varclr .
.
.
.
variable similarity search we next evaluate our learned representations in the context of a more applied downstream application similar variable search.
similar variable search identifies similar variable names in a set of namesgivenaninputquery.thiscanbeusefulforrefactoringcode orforassigningvariablesmorereadablenames e.g.
replacing fd with file descriptor .
for a given set of variables vand a pretrained varclr encoder f prime we compute representation vectors k f prime v v v .foraqueryvariable u wefindtop ksimilar variablesin vwith thehighestcosine similarityto f prime u .
toquantitativelyevaluateeffectivenessinfindingsimilarvariables we created a new mini benchmark varsim from the original idbenchbenchmark.weselectvariablepairswhichhavehumanassessed similarity scores greater than .
in idbench.
this leaves uswith100 similar variablepairsfromall291variablepairsinthe idbench largebenchmark.weusethevariablecollectionprovided inidbenchcontaining208 434variablesastheoverallcandidate pool.weusehit kasourevaluationmetric computingthecosine similarityoftherepresentationsofaqueryvariable uandallthe variables in the candidate pool.
we select the top k variables with the highest similarity scores and check whether the corresponding similar variable vis in the top k list.
we choose k to be .
results.asshowninfigure3a varclr codebertachievesthe best similaritysearch performance with47 at k and76 at k compared to ft cbow at k at k .
this 2333icse may21 pittsburgh pa usa qibin chen jeremy lacomis edward j. schwartz graham neubig bogdan vasilescu and claire le goues table variable similarity search.
top most similar variables found by the idbench method and varclr codebert .
variable method top similar variables substrft cbow substring substrs subst substring1 substrcount varclr codebert substr substring substrs stringsubstr substrcount itemft cbow itemnr itemj iteml itemi itemat varclr codebert pitem itemel mitem itemels itemvalue countft cbow counttbl countint countrto countsasnum countone varclr codebert scount countof counts countint countth rowsft cbow roworrows rowxs rows 1 rowsar rowids varclr codebert drows allrows rowsarray ows nrows setintervalft cbow resetinterval settimeoutinterval clearinterval getinterval retinterval varclr codebert pinterval mfpsetinterval settickinterval clocksetinterval iinterval mintextft cbow maxtext minlengthtext microsectext maxlengthtext minutetext varclr codebert minlengthtext mincontent maxtext minel min filesft cbow filesobjs filesgen filesets extfiles libfiles varclr codebert filesarray afiles allfiles fileslist filelist minyft cbow min y minby minx minpt min z varclr codebert ymin ymin miny minys minxy top k0 hit kvarclr avg varclr lstm varclr codebert ft cbow a similarity search top k20 hit kavg lstm codebert b spelling error correction figure hit k score comparison on varsimandvartypo.
indicatesthatourmethodiseffectiveatfindingsimilarvariables able to distinguish the most similar variable to the query variableoutof200distractorsaround76 ofthetime.8interestingly 8since we evaluate the hit score in a candidate pool of size the resolution of this retrieval task is1000 .
although inspecting the top may not be practical as an real world application itself it is still an informative metric of the representation quality and may indicate effectiveness in other settings e.g.
varclr avg and varclr lstm are less effective at similarity search than ft cbow even though they outperform ft cbow by a large margin in the similarity scoring task.
embedding based methods are still a strong baseline for variable similarity search.
however contrastive methods still amplify the effectiveness of unsupervised embedding methods.
similarity scoring and similarity search are distinct tasks and so it is not unexpected that techniques will be equally effective on both.
for example word2vec tends to put the embeddings ofsimilarrarewordsclosetosomecommonfrequentword.this behavior does not affect the similarity search effectiveness because the rare words are able to find each other and the frequent word is closeenoughto itssimilarwordthantotheserarewords.however this will hurt similarity scoring between the rare words and the frequentvariable sincetheyareactuallynotsimilar.incomparison varclr is able to avoid these kinds of scoring mistakes.
casestudy.
wedemonstrateourresultsqualitativelybychoosingthesamesetofvariablesusedtodemonstratethistaskinthe idbenchpaper anddisplayingthecomparativeresultsinfigure3a.
for space we omit two of the variables rowsand count i nt h e set the two methods perform comparably such as on substr .
we observe that the overall qualities of the two methods results are similar.thisisunderstandablesincethegapbetweenthetwomethods on variable similarity search is relatively small as shown in table .
meanwhile it is worth noting that varclr codebert is better at penalizing distractive candidates that are only related but not similar.
for example for mintext varclr codebert ranks minlengthtext mincontent before maxtext while ft cbow suggests the opposite.
for miny varclr codebert ranks ymin ymin minyastop whileft cbowsuggestsrelatedbutdissimilarvariablessuchas minbyandminx.thisprovidesadditionalevidencethat a developer looking at the top similar variables from a limited candidates which has the same requirement on resolution.
another possible application is to use varclr to retrieve a large candidate pool as the first stage to other methods e.g.
naturalvariablenamesuggestion.
2334varclr variable semantic representation pre training via contrastive learning icse may pittsburgh pa usa our method is able to better represent semantic similarity rather than pure relatedness.
.
variable spelling error correction spelling error correction is a fundamental yet challenging task innaturallanguageprocessing .weexplorethepossibilityof applyingvarclrmodelstoperformspellingerrorcorrectionon variable names.
if the representations of misspelled variable names are close to their correct versions corrections may be found via nearest neighbor search.
fortunately the githubrenames dataset enablesthisgoal becauseaportionofrenamingeditsingithubrenames are actually correcting spelling errors in previous commits.
we can therefore reformulate this problem as a variable similarity search task since our method treats these misspelled names as similarto their corrected versions.
we create a new synthetic variable spelling error correction dataset vartypo with misspelled variables and their corrections.
specifically we create this dataset by sampling variables fromthe208 434variablepoolfromidbench andusethe nlpaug9 package tocreatemisspelledvariablesfromthecorrectones.
we use keyboardaug which simulates typo error according to characters distanceonthekeyboard.thistaskischallengingbecause our method does not leverage any external dictionary or hand craftedspellingrules.meanwhile althoughstringdistance functions such as levenshtein distance can potentially perform better these functions require expensive one by one comparisons betweenthequeryvariableandeveryvariableinthepool whichis very time consuming while our method uses gpu accelerated matrix multiplication to compute all cosine distances at once and can potentially adopt an even more efficient vector similarity search librarysuchas faiss.therefore webelieveitisstillaninformative benchmark for evaluating variable representations.
results.similar to variable similarity search we evaluate the effectiveness as the hit k score of using the representation of misspelled variables to retrieve the corresponding correct variable.
asshowninfigure3b varclrcansuccessfullycorrectthe29.
of thetimeattop and73.
ofthetimeattop .oneinteresting observation we find is that in this task the gap .
at top and .
at top between varclr avg and the other two powerful encoders is relatively small.
it even outperforms varclrcodebert after k .
one possible explanation is that fixing a typo requires neither word sequential order or word importance information i.e.
being able to model the variable as a sequence insteadof a bag of words does not benefit this task.
case study.
for illustration we randomly select misspelled variable names and use our varclr to find the most similar correct variablenames.asshownintable3.
ourmodelisabletocorrect some of the misspelled variables including insertions deletions andmodifications whilefailingtorecoverothers.notably variable names consisting of multiple words such as minsimilarity can be corrected successfully.
thetop 3mostsimilarvariablestomisspelledvariables foundby varclr.
variable top similar variables temepratures temperatures temps temlp similar lity similarity similarities similar minsimilar lity minsimilarity similarity minratio program able programmable program program6 supervis ior superior superview superc produc itons obligations proportions omegastructors trans altion transac trans transit .
ablationstudies so far we have demonstrated the importance of both contrastive learning and sophisticated models like codebert for varclr performance.here performablationstudiestomeasuretheeffectof additionaldesigndecisionsinvarclr oftrainingdatasize ofusing pre trained language models and of using pre trained embeddings from unsupervised learning.
.
.
effect of data size on contrastive pre training.
pre training varclrrequiresweakly superviseddatascrapedfrompublicrepositories.
thus we evaluate how much data is required to train an effective model to elucidate data collection costs.
to evaluate this we train varclr avg varclr lstm varclr codebert on .
.
.
.
percent of the full dataset measuringthe similarityscore on idbench medium.
figure4showstheresults.forallthreevarclrvariants training data size has a significant positive effect on effectiveness.
this is especiallytrueforvarclr codebert butperformanceflattens andconvergesastrainingdatasizeapproaches100 .thissuggests that githubrenamesis of an appropriate size for this task.
another interesting observation is that varclr avg outperforms varclr lstm with smalleramounts of training data.
this indicatesthemorepowerfullstmmodeldoesnotsurpassasimple one until the data size reaches a critical threshold.
this is likely becauseamorecomplexmodelhasmoreparameterstotrainandrequires more data to reach convergence.
with sufficient data larger models win thanks to their representational capacity.
this suggests a caveat in applying representation learning models it is importanttochooseamodelwithanappropriatecomplexitygiven the amount of available data rather than defaulting to the bestperforming model overall.
.
.
using a pre trained language model.
before contrastive pretraining on githubrenames varclr codebert is initialized withamodel pre pre trainedonalargecodecorpus.theeffectof thispre trainingisalsoillustratedinfigure4.althoughvarclrcodeberthasamuchlargernumberofparameters itoutperforms varclr avg and varclr lstm after contrastive pre training on only of githubrenames.
while this seems to contradict the conclusion reached in the comparison between varclr lstm and varclr avg it displays the benefit of initialization with a pretrainedmodel.comparedtovarclr lstm whichcontainsrandomlyinitializedparametersthathavetobetrainedfromscratch varclr codebert parameters produce reasonable representations from the start.
therefore it requires less data to converge 2335icse may21 pittsburgh pa usa qibin chen jeremy lacomis edward j. schwartz graham neubig bogdan vasilescu and claire le goues .
percent of trainin gdata0.
.
.
.5spearman s rank correlation avg lstm codebert figure effect of contrastive pre training data size on learned varclrrepresentations evaluated on idbench medium.
table effect of pre trained codebert embeddings on similarity score effectiveness spearman s .
models are either randomly initialized and contrastively pre trained contrastive initialized with codebert embeddings codebert or both varclr .
method small medium large contrastive avg .
.
.
codebert avg .
.
.
varclr avg .
.
.
contrastive lstm .
.
.
codebert lstm .
.
.
varclr lstm .
.
.
and thanks to its large model capacity ultimately outperforms the othertwo variants by a large margin.
despite the fast convergence directly applying codebert without contrastive pre training leads to poor performance .
at data .
one possible reason is that codebert was originally trained for whole program representations and using it with variable names as inputs leads to a problematic divergence from its trainingdata distribution.
.
.
effect of pre trained codebert embeddings.
both varclravgandvarclr lstmareinitialized withthe wordembeddings fromcodebertbeforecontrastivepre training.tostudytheeffect of these pre trained embeddings we measure the spearman s correlation coefficient of the similarity scores of the models modified in two ways one with randomly initialized embeddings that is thencontrastivelypre trained contrastive intable4 andone thatisinitializedwithcodebertembeddingsbut notcontrastively pre trained codebert in table .
the results show that pre trained codebert embeddings are essentialtotheperformanceofvarclr avgandvarclr lstm.
however directly adopting the pre trained embeddings alone is stillinsufficient especiallyforlstms.thisimpliesthatbothunsupervised pre training and weakly supervised pre training are indispensableforusefulvariablerepresentations the former takesadvantageof dataquantity byleveragingahugeamountofunlabeleddata whilethelattertakesadvantageof dataquality using the weakly supervisedgithubrenamesdataset.
related work variablenamesandrepresentations.
variablenamesareimportant for source code readability and comprehension .
becauseofthis therehasbeenrecentworkfocusingonautomatically suggesting clear meaningful variable names for tasks such as code refactoring and reverse engineering .
a common approach involves building prediction engines on top of learned variable representations.
representation learning is acommontaskinnaturallanguageprocessing nlp andthese techniquesareoftenadaptedtosourcecode.simplerapproaches model variable representations by applying word2vec t o codetokens whilemoreadvancedtechniqueshave adapted neural network architectures or pre trained language models .
source code representation is a common enough task thatresearchershavedevelopedbenchmarksspecificallyforvariable and program representations .
similarityandrelatedness.
afundamentalconcernwithexisting variablerepresentationsandsuggestionenginesisthedifferencebetween related and similar variables .
related variables referencesimilarcoreconceptswithoutconcernfortheirprecise meaning while similar variablesaredirectlyinterchangeable.for example minweight andmaxweight arerelatedbutnotsimilar while avgand meanare both.
unlike state of the art techniques which only model relatedness varclr explicitly optimizes for similarity by adapting contrastive learning techniques from nlp and computer vision research.
in nlp systems are often designed to focus ontextrelatedness similarity orboth .while document search might only be concerned with relatedness similarityis particularly important in systems designed for paraphrasingdocuments .
varclr relies on contrastive learning to optimize for similarity.
contrastive learning isparticularly useful for learning visual representations without any supervision data but has also been used for nlp .
recent work has applied contrastive learning to the pre training of language models to learn text representations and similar to our task learn sentence embeddingsfortextualsimilaritytasks .contrastivelearning has also been used for code representation learning where source to source compiler transformation is applied for generating different views of a same program.
different from this work we focus on learning representations for variable names and leverage additionaldatafrom github for better supervision.
string similarity and spelling errors.
efficient string similarity searchremainsanactiveresearcharea .mostofthese methodscanbecategorizedas sparseretrieval methods focusing ondistancefunctionsontheoriginalstringorn grams.thesealgorithmsdependonthelexicaloverlapbetweenstringsandthus cannotcapturethesimilaritybetweenvariablespairssuchas avg andmean.
more recently dense retrieval methods have been shown effectiveinnlptasks .thesemethodsperformsimilarity search inthespace oflearnedrepresentations so thatsequences 2336varclr variable semantic representation pre training via contrastive learning icse may pittsburgh pa usa with similar meanings but low lexical overlap can be found.
meanwhile extremelyefficientsimilaritysearchframeworksfordense vectorssuchas faiss canbeapplied.varclrintroducesthe conceptofdenseretrievalintothevariablenamesdomain enabling moreeffectiveandefficientfindingofalistofcandidatesthatare similarto a given variable name.
neuralmodels forspelling errorcorrection usuallyrequire parallel training data which are hard to acquire in practice .
recentworkadoptsdifferentmechanismstocreatesyntheticparalleldata includingnoiseinjection andback translationmodels .weleaveadetailedcomparisontofuturework butnote that varclr shows promise without expensive training data.
name andmachinelearning basedprogramanalyses.
ourdownstream tasks are examples of program analyses based on informationgatheredwithmachinelearning ml .name basedbased program analyses predicated on machine learning have been used in many contexts.
in the context of code modification they have been used for variable name suggestion from code contexts method and class name rewriting and generation code generation directly from docstrings and automated program repair .theyhavealsobeenusedfortypeinferencefrom naturallanguageinformation detectingbugs anddetectingvulnerabilities .varclrcanserveasadrop in pre trainingstepfor suchtechniques enablingmoreeffective use ofthesemanticinformationcontainedinvariablenamesforawide range of such analyses.
discussion in this paper we study variable representation learning a problem with significant implications for machine learning and name based programanalyses.wepresentanovelmethodbasedoncontrastive learning for pre training variable representations.
with our new weakly supervisedgithubrenamesdataset our methodenables theuseofstrongerencoderarchitecturesinplaceof word2vec based methods for this task leading to better generalized representations.ourexperimentsshowthatvarclrgreatlyimproves representation quality not only in terms of variable similarity but alsoforotherdownstreamtasks.whilethesedownstreamtasksmay notbeimmediatelypracticalthemselves ourapproachispromising as a drop in pre training solution for other variable name based analysistasks whichwehopeotherswillattemptinfuturework.
for example varclr can replace the the word2vec cbow embeddings used in a name based bug detector or the n gram based language model used as a similarity scoring function for name suggestion .
existing dictionary based ide spell checkers may also benefit from using varclr to rank suggestions based on the pretrained semantic similarity.
wenotelimitationsandpossiblethreatsinourstudy.ourdataset is automatically constructed from git commits from github and likely contains noise that can harm contrastive learning performance .
however our results show that despite this noise our models transfer well and our evaluation is based on an entirely distinct test set.
knowledge distillation and self training methods such as momentum distillation can be applied to deal with the noise in weak supervision data .in this work we applied varclr exclusively to unsupervised downstreamtasks.fine tuningvarclrmodelswithlabeleddata might further enable significant performance improvements for more complicated tasks like natural variable name suggestion .
beyond constructing similar variable names it is also conceptually possibletoconstructsimilarpairsof largercodesnippetsfromgit diffs describing patches.
applying contrastive learning on these pairscanpotentiallyimprovecodebertcode representationand understanding which could benefit tasks well beyond variable similarity such as code search.
finally we used instance discrimination toguideourcontrastivelearningapproach withpromising results.
this suggests that more advanced contrastive learning methodssuchasmoco byol swav beadaptedto this taskforbetter representation learning in general.