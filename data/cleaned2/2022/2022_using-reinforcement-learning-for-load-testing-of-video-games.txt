using reinforcement learning for load testing of video games rosalia tufano seart software institute universit della svizzera italiana switzerlandsimone scalabrino stake lab university of molise italyluca pascarella seart software institute universit della svizzera italiana switzerland emad aghajani seart software institute universit della svizzera italiana switzerlandrocco oliveto stake lab university of molise italygabriele bavota seart software institute universit della svizzera italiana switzerland abstract differentfromwhathappensformosttypesofsoftwaresystems testing video games has largely remained a manual activity performed by human testers.
this is mostly due to the continuous andintelligentuserinteractionvideogamesrequire.recently reinforcementlearning rl hasbeenexploitedtopartiallyautomate functional testing.
rl enables training smartagents that can even achieve super human performance in playing games thus being suitable to explore them looking for bugs.
we investigate the possibilityofusingrlforloadtestingvideogames.indeed thegoal ofgametestingisnotonlytoidentifyfunctionalbugs butalsoto examinethegame sperformance suchasitsabilitytoavoidlags andkeepaminimumnumberofframespersecond fps whenhighdemanding3dscenesareshownonscreen.wedefineamethod ology employing rl to train an agent able to play the game as a human while also trying to identify areas of the game resulting in adropoffps.wedemonstratethefeasibilityofourapproachon threegames.twoofthemareusedasproof of concept byinjecting artificial performance bugs.
the third one is an open source 3d gamethatweloadtestusingthetrainedagentshowingitspotential to identify areas of the game resulting in lower fps.
ccs concepts softwareanditsengineering softwaremaintenancetools .
keywords reinforcement learning load testing permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa association for computing machinery.
acm isbn ... .
introduction the video game market is expected to exceed billion in value in2023 .insuchacompetitivemarket releasinghigh quality gamesand consequently ensuringagreatuserexperience isfundamental.
however the unique characteristics of video games from hereon games make their quality assurance process extremely challenging.
indeed besides inheriting the complexity of software systems gamesdevelopmentandmaintenancerequireadiverseset ofskillscoveredbymanystakeholders includinggraphicdesigners story writers developers ai artificial intelligence experts etc.
also games can hardly benefit from testing automation techniques since even just exploring the total space available in a givengamelevelrequiresan intelligent interactionwiththegameitself.forexample inaracinggame identifyingabugthatmanifests whenthefinishlineiscrossedrequiresaplayerabletosuccessfully drivethecarforthewholetrack i.e.
requirestheabilitytodrive the car .
thus random exploration is not a viable option here.
therefore it comes without surprise that game testing is largely a manual process.
zheng et al.
report that human testers were employed for testing one of the games used in their study.
also thechallengesintestinggameshavebeenstressedbylin etal.
whoshowedthat80 ofthe50populargamestheystudied have been subject to urgent updates.
to support developers with game testing researchers have proposed several techniques.
these include approaches to test the stabilityofgameservers model basedtesting using domain modeling for representing the game and uml state machinesforbehavioralmodeling aswellastechniquesspecifically designed for testing board games .
when looking at recent techniquesaimedatproposingmoregeneraltestingframeworks those exploiting reinforcement learning rl are on the rise.
this isduetotheremarkableresultsachievedbyrl basedtechniques inplayinggameswithsuper humanperformancereportedinthe literature .
rl is a machine learning technique aimed to train smartagents able to interact with a given environment e.g.
a game and to take decisions to achieve a goal e.g.
win the game .
rl is based on the simple idea of trial and error the agent performs actions in theenvironment of which it only has a partial representation and receivesa rewardthatallowsittoassessitspastactions behavior with respect to the desired goal.
ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa rosalia tufano simone scalabrino luca pascarella emad aghajani rocco oliveto and gabriele bavota recently researchers started using rl not only to play games but also to test them and in general to improve their quality.
the commonideabehindtheseapproachesistoreducethehumaneffort in playtesting using intelligent agents.
rl based agents have been used to help game designers in balancing crucial parameters of the game e.g.
power up item effects and in testing the gamedifficulty .also rl basedagentshavebeenusedto look for bugs in games .
while agents are usually trained to play a game with the goal of winning theaforementionedworkstrainedtheagenttonotonly advanceinthegamebutalsotoexploreittosearchforbugs.for example ariyurek et al.
combine rl and monte carlo tree search to find issues in the behavior of a game given its design constraints and game scenario graph.
the icarusframework isabletoidentifycrashesandblockersbugs e.g.
thegamegetstuck for a certain amount of time while the agent is playing.
similarly theapproachbyzheng etal.
alsoexploitingrl canidentify bugs spotted by the agent during training e.g.
crashes .
while theseapproachespioneeredtheuseofrlforgametesting theyare mostly aimed at testing functional e.g.
finding crashes or designrelated e.g.
level design aspects.
however these are not the only types of bug developers look for in playtesting.
in a recent survey politowski et al.
reported that for two out of the five games theyconsidered i.e.
leagueoflegends byriotand seaofthieves by rare developers partially automated game performance checks e.g.
frame rate .
similarly naughty dog used specialized profiling tools1for finding which parts of a given scene caused a drop in the number of frames per second fps in the last of us.
truelove etal.
reportthatgamedevelopersagreethat implementation responseproblems may severely impact the game experience.
despitesuchastrongevidenceabouttheimportanceofdetecting performanceissuesinvideogames tothebestofourknowledgeno previousworkintroducedautomatedapproachesforloadtesting videogames.wepresent reline reinforcementl earningfor load testing games an approach exploiting rl to train agents able to play a given game while trying to load test it with the goal of minimizing its fps.
the agent is trained using a rewardfunction enclosing two objectives the first aims at teaching the agent how to advance in the game.
the second rewards the agent when itmanages to identify areas of the game exhibiting low fps.
theoutput of relineis a report showing areas in the game being negative outliers in terms of fps accompanied by videos of thegameplays exhibiting the issue.
such reports can simplify theidentification and reproduction of performance issues that areoftenreportedinopen source3dgames see e.g.
and that in some cases are challenging to reproduce see e.g.
.
we experiment relinewith three games.
the first two are simple2dgamesthatweuseasaproof of concept.inparticular weinjectedinthegamesartificial performancebugs tocheck whether the agent is able to spot them.
we show that the agent trained using relinecan identify the injected bugs more often than i arandomagent and ii arl basedagentonlytrainedto playthegame.then weuse relinetoloadtestanopen source3d game showingitsabilitytoidentifyareasofthegamebeing negative outliers in terms of fps.
game info s a r experience fps info reward function rl model gamestate reward action1 figure relineoverview rl to load test video games in this section we explain from an abstract perspective the idea behindreline.
we describe in the study designs how we instantiatedrelinetothedifferentgamesweexperimentwith e.g.
details about the adopted rl models .
relinerequires three main components the gameto load test arl model representing the agent that must learn how to play the gamewhileloadtestingit anda rewardfunction usedtorewardthe agentsothatitcanevaluatetheworthofitsactionsforreaching thedesiredgoal i.e.
playingwhileloadtesting .the rlmodel is trainedthroughthe4 steploopdepictedinfig.
seethecircled numbers .
the continuous lines represent steps performed at each iteration of the loop while the dashed ones are only performed afterafirstiterationhasbeenrun i.e.
aftertheagentperformed at least one action in the game .
when the first episode i.e.
a run ofthegame ofthetrainingstarts step1 ateachtimestep the gameprovidesitsstate s .suchastatecanbe forexample aset of frames or a numerical vector representing what is happening in thegame e.g.
theagent sposition .the rlmodel takesasinput s step2 andprovidesasoutputtheaction a toperforminthegame step .
when the agent has no experience in playing the game at thestartofthetraining theweightsoftheneuralnetworkinthe rl model are randomly initialized producing random actions.
the actiona isexecutedinthegame step4 which inturn generates the subsequent state s .
afterthefirstiteration i.e.
afterhavingreceivedatleastone a the gamealso produces at eachiteration the dataneeded tocompute the reward function.
in relinewe collect i the information needed to assess how well the agent is playing the game e.g.
time since the episode started and the episode score and ii the fpsat time .
it is required that the game developer instruments the game and provide apis through which relinecan acquire such pieces of information.
we assume that this requires a minor effort.
thereward function aims at training an agent that is able to i play the game thanks to the information indicating how well theagentisplaying and ii identifylow fpsareas thankstothe informationaboutthefps.theoutputofthe rewardfunction isa numberrepresentingtherewardobtainedbytheagentattime .in reline the reward function for a given action is composed of two sub functions a game reward function depending on how good the actionis inthe game rg andaperformancereward function depending on how the action impacts the performance rp .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
using reinforcement learning for load testing of video games icse may pittsburgh pa usa a b c preliminary study case study figure screenshots of games used in the preliminary study section a cartpole and b mspacman and in thecase study section c supertuxkart.
wecombinesuchfunctionsin r rg rp .thegamereward function clearly depends on the game under test a function designedforaracinggamelikelymakesnosenseforarole playing game.
in general defining the reward function for learning to play shouldbeperformedbyconsidering i whatthegoalofthegame is e.g.
drive ona track and ii whichinformation the game provides about the successful behavior of the player e.g.
is there ascore?
.eveniflessintuitive theperformancerewardfunction is game dependent as well assuming a tiny fps drop e.g.
the reward can be small for a role playing game in which it likely does not affect the whole experience while it should be high for anactiongame inwhichitcouldevencausethe unfair player s defeat.unlikethegamerewardfunction weexpecthoweverminor changes to be requiredto adapt the performance reward function to a different video game i.e.
tuning of the thresholds to use .
thestates theaction a andthereward r arethenstoredinan experience buffer.
when enough experience has been accumulated it is used to update the network weights.
how experience is stored and used to update the network depends on the used rl model.
theepisodeendswhenafinalstateisreached.again thedefinition of the final state depends on the game and it could be based on a timeout e.g.
each episode lasts at most seconds or on a specific condition that must be met e.g.
the agent crosses the finishline .oncetheepisodeends thegameisreinitializedandthe looprestarts.thetrainingisperformedforanumberofepisodes sufficient to observe a convergence in the total reward achieved byanagentduringanepisode e.g.
ifthetrainedagentobtainsa rewardof100fortenconsecutiveepisodesthetrainingisstopped .
preliminary study injecting artificial performance issues thispreliminarystudyaimsatassessingtheabilityof relinein identifyingartificial performancebugs wesimulateintwo 2d games.
it is important to highlight that the goalof this study is onlyto demonstrate theapplicability of relinefor loadtesting games as a proof of concept.
a case study on a 3d open source game is presented in section .
.
study design we select two 2d games cartpole and mspacman .
the former fig.
a is a dynamic system in which an unbalanced pole is attached to a moving cart and the player must move the cart to balance the pole and keep it in a vertical position.theplayerlosesifthepoleismorethan12degreesfromvertical orthecartmovestoofarfromthecenter.thelatter fig.
b istheclassicpac mangameinwhichthegoalistoeatalldotswithouttouchingtheghosts.bothgamesemploysimple2dgraphicswhich boundtheplayer spossiblemovesinonlyone e.g.
leftandright for cartpole or two e.g.
left right up and down for mspacman dimensions.
this is one of the reasons we selected these games forassessingwhetherarl basedagentthatlearnedhowtoplay themcanalsobetrainedtolookforartificial performancebugs we injected.
also both games are integrated in the popular gym python toolkit developed by openai .
gymcanbeusedfordevelopingandcomparingrl basedagents in playing games.
it acts as a middle layer between the environment the game and the agent a virtual player .
in particular gym collects and executes actions e.g.
go left go right generated by the agent and returns to it the new state of the environment i.e.
screenshots withadditionalinformationsuchasthescoreinthe episode.gymcomeswithasetofintegratedarcadegamesincluding the two we used in this preliminary study.
.
.
bug injection.
we injected two artificial performance bugs incartpoleandfourinmspacman.theideabehindthemissimple whentheagentvisitsspecificareasforthefirsttimeduringagame the bugs reveal themselves simulation of heavy resource loading .
a natural way of achieving this goal would have been to introduce thebugsinthesourcecodeofthegameandtoimplementthelogic tospotfpsdropsintheagentaccordingly.this however would havesloweddownthetrainingoftheagent.therefore wechose touseamorepracticallysoundapproach inspiredbythesimulation of heavy weight operation hwo operator for performance mutation testing we directly assume that the agents observe the bugs when they visit the designated areas and act accordingly.
incartpole theagent canonlymoveon the xaxis i.e.
leftor right .
when the game starts the agent is in position x i.e.
centeroftheaxis anditcanchangeitspositiontowardspositive by moving right or negative left xvalues.
the two bugs we injected manifest when x andx dashed lines in fig.
a .
we use intervals rather than specific values e.g.
.
because the position of the agent is a float if it moves to position .
we want to reward it during the training for having found the injected bug.
concerning mspacman weassumethataperformancebugmanifestswhentheagententers the fourgatesindicated by the white arrows in fig.
b .
asdetailedinsection3.
.
weassesstheextenttowhich reline isabletoidentifythebugsweinjectedwhileplayingthegames.to have a baseline we compare its results with those of a rl basedagent only trained to play each of the two games from hereon rl baseline and with a random agent .
sincerelinewill be trained withthegoalofidentifyingthebugs detailsfollow weexpectit to adapt its behavior to not only successfully play the game but to also exercise more often the buggy areas of the games.
.
.
learning to play rl models and game reward functions.
we trainedthe rl baseline agent i.e.
theone onlytrained tolearn how to play for cartpole using the cross entropy method a s rl model.
we choose this method because despite its simplicity it has been shown to be effective in applications of rl to small environments such as cartpole .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa rosalia tufano simone scalabrino luca pascarella emad aghajani rocco oliveto and gabriele bavota the core of the cross entropy method is a feedforward neural network fnn thattakesasinputthestateofthegameandprovides as output the action to perform.
the state of the game for cartpole is a vector of dimension containing information about the x coordinateofthepole scenterof mass thepole s speed itsangle withrespecttotheplatform and itsangularspeed.therearetwo possible actions go right go left.
once initialized with random weights the agent i.e.
the fnn starts playing while retaining the experienceacquiredineachepisode theexperienceisrepresented bythestate theaction andtherewardobtainedduringeachstep of the episode.
the goal is to keep the pole in balance as long as possible or until the maximum length of an episode that we set to steps isreached.the gamerewardfunction isdefinedsothattheagent receives a reward for each step it manages to keep the pole balanced.the totalscore achievedis alsosavedat theend ofeach episode.
after n consecutive episodes the agent stops playing selects the m episodes having the highest score and uses the experience in those episodes to update the weights of the fnn nandmhave been set according to .
instead wetrainedthe rl baseline agentformspacmanusinga deep qnetwork dqn .
inour context a dqnis aconvolutionalneuralnetwork cnn thattakesasinputasetofcontiguous screenshots of the game in our case as done inpreviousworks representing the state of the game and returns for each possible action defined in the game five in this case go up go right godown goleft donothing avalueindicatingtheexpectedrewardfortheactiongiventhecurrentstate qvalue .themultiple screenshots are needed to provide more information to the model aboutwhatishappeninginthegame e.g.
inwhichdirectionthe agent is moving .
the goal of the dqn is the same as the fnn selecting the best action to perform to maximize the reward given the current state.
differently from the previous model the dqn is updated not on entire episodes but by randomly batching expe rience instances among 10k steps saved during the most recent episodes.an experienceinstance issavedaftereachstep and is represented by the quadruple s a s r wheres 1is the inputstate a istheactionselectedbytheagent s istheresulting state obtained by running a ins 1andr is the received reward.
thecnnisinitializedwithrandomweights andtheagentstarts playing while retaining the experience of each step.
when enough experience instances have been collected 10k in our implementation thecnnstartsupdatingateachstepselectingarandom batch of experience instances.
the reward function for mspacman provides a reward every time the agent eats one of the dots and a reward otherwise.
.
.
instantiating reline performance reward functions.
totrain relinetoplaywhilelookingfortheinjectedbugs weuseasimple performance reward function in both the games we give a reward of every time the agent during an episode spots one of the injectedartificialbugs.aspreviouslymentioned thebugsreveal themselves only the first time the agent visits each buggy position this means that the performance based reward is given at most twice for cartpole and four times for mspacman.
.
.
data collection and analysis.
wecompare relineagainst the two previously mentioned baselines rl baseline and therandomagent .bothrelineandrl baseline havebeentrainedfor3 episodesoncartpoleand1 000onmspacman.thedifferentnumbers are due to differences in the games and in the rl model weexploited.
inbothcases we used a number ofepisodes sufficient forrl baseline tolearnhowtoplay i.e.
weobservedaconvergence in the score achieved by the agent in the episodes .
once trained the agents have been run on both games for additional episodes storing the performance bugs they managed to identify in each episode.
since different trainings could result in modelsplayingthegamefollowingdifferentstrategies werepeatedthisprocesstentimes.thismeansthatwetrained10differentmodelsforboth relineandrl baseline and then weused eachofthe 10modelstoplayadditional1 000episodescollectingthespotted performancebugs.similarly weexecuted randomagent 10times for episodes each.
in this case no training was needed.
we report descriptive statistics mean median and standard deviation of the number of performance bugs identified in the 000playedepisodesbythethreeapproaches.ahighnumberof episodes in which an approach can spot the injected bugs indicate its ability to look for performance bugs while playing the game.
.
preliminary study results table shows for each of the two games cartpole and mspacman the number kof artificial bugs we injected and for each of the threetechniques i.e.
reline rl baseline andthe randomagent descriptivestatisticsofthenumberofepisodes outof1 they managed to identify at least nof the injected bugs with ngoing from to kat steps of .
for both games it is easy to see that the random agent is rarely able to identify the bugs.
indeed this agent plays without any strategyasitisabletoidentifybugsonlybychanceinafewepisodesoutofthe1 000itplays.thisisalsoduetothefactthatthe random agentquickly looses the played episodes due to its inability to play the game.
this confirms that these approaches are not suitable for testing video games.
concerning cartpole both relineandrl baseline are able to spot at least one of the two bugs in several of the episodes.
the median is for relineand for rl baseline .
the success ofrl baseline issoonexplainedbythecharacteristicsofcartpole considering where we injected the bugs see fig.
a by playing the game it is likely to discover at least one bug e.g.
if the playertendstomovetowardsleft thebugontheleftwillbefound .
whatitisinsteadunlikelytohappenbychanceistofindbothbugs withinthesameepisode.wefoundthatitisquitechallenging even for a human player to move the cart first towards one side e.g.
left and then towards the other side right without losing dueto thepolemovingmore than12 degreesfrom vertical.
asit canbe seen in table relinesucceeds in this on average for episodes out of median as compared to the median ofrl baseline .thisindicatesthat relineispushedbythereward function to explore the game looking for the injected bugs evenif this makes playing the game more challenging.
similar results have been achieved on mspacman.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
using reinforcement learning for load testing of video games icse may pittsburgh pa usa table number of episodes out of in which reline rl baseline and the random agent identify the injected bugs.
game injected bugs reline rl baseline random agent bugs found mean median stdev mean median stdev mean median stdev cartpole mspacman in this case the dqn is effective in allowing relineto play whileexercisingthepointsinthegameinwhichweinjectedthe bugs.
indeed on average relinewas able to spot all four injected bugsin879outofthe1 000playedepisodes median while rl baseline could achieve such a result only in episodes.
summary of the prelimiary study.
relineallowsobtain agents able not only to effectively play a game but also tospotperformanceissues.comparedto rl baseline themain advantageof relineisthatitidentifiesbugsmorefrequently while playing.
case study load testing an open source game werunacasestudytoexperimentthecapabilityof relineinload testing an open source 3d game.
differently from our preliminary study section3 wedonotinjectartificialbugs.instead weaim at finding parts of the game resulting in fps drops.
.
study design for this study we use a 3d kart racing game named supertuxkart see fig.
c .
this game has been selected due to the following reasons.
first we wanted a 3d game in which as compared to a 2d game fps drops are more likely because of the more complexrenderingprocedures.second supertuxkartispopularopen source project that counts at the time of writing over 3k stars on github.
third it is available an open source wrapper that simplifies the implementation of agents for supertuxkart .
the existence of a wrapper like the one we used is crucial since itallows forexample toadvanceinthegameframebyframe thussimplifyingthegenerationoftheinputstotherlmodel toexecute actions e.g.
throttleorbrake andtoacquiregameinternals e.g.
kartcentering distancetothefinishline .also usingthiswrapper itispossibletocomputethetimeneededbythegametorendereach frame and consequently calculate the fps.
finally the wrapper allowstohavesimplifiedgraphics e.g.
removingparticleeffects like rain that could make the training more challenging .
.
.
learning to play rl models and game reward functions.
the training of the rl baseline agent has been performed using the dqn model previously applied in mspacman.we use the previously mentioned pysupertuxkart to make the agent interact with the game.
for the sake of speeding upthe training the screenshots extracted from the game have been resizedto200x150pixelsandconvertedingrayscalebeforetheyare providedasinputtothemodel.moreover aspreviouslydonefor mspacman multiple four screenshotsarefedtothemodelateach step.
thus the representation of the state of the game provided to themodelisa4 150tensor.thedetailsofthemodelandits implementation are available in our replication package .
a critical part of the learning process is the definition of the gamerewardfunction.beingsupertuxkartaracinggame anoption could have been to penalize the agent for each additional steprequired to finish the game.
consequently to maximize the final score the agent would have been pushed to reduce the number of stepsand therefore to driveas fastas possibletowards thefinish line.however consideringthenon trivialsizeofthegamespace sucharewardfunctionwouldhaverequiredalongtrainingtime.
thus wetookadvantageoftheinformationthatcanbeextracted from the game to help the agent in the learning process.
supertuxkart provides two coordinates indicating where the agent is in the game path done andcentering.
theformerindicatesthepathtraversedbytheagentfromthe starting line of the track while the latter represents the distance of the agent from the center of the track.
in particular centering equals if the agent is at the center of the track and it movesawayfromzeroastheagentmovestoeitherside goingtowards rightresultsinpositivevaluesofthe centering value goingleftin negative values.
we indicate these coordinates with x centering andy path done andwedefine yasthepathtraversedbythe agentinaspecificstep given yithevaluefor path done atstep i we compute yasyi yi .
basically ymeasures how fast the agent is advancing towards the finish line.
givenxand yforagivenstep i wecomputetherewardfunction as follows rgi braceleftbigg 1i f x max min y m otherwise first if the agent goes too far from the center of the track x wepenalizeitwithanegativereward.otherwise iftheagent isclosetothecenter x wecanhavetwoscenarios i ifit is notmoving towardsthe finishline y wedo notgive any reward theminimumrewardis0 ii ifitismovingintheright direction y we give a reward proportional to thespeedat which it is advancing y up to a maximum of m. authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa rosalia tufano simone scalabrino luca pascarella emad aghajani rocco oliveto and gabriele bavota in our experimental setup we set because it roughly representsthedoubleof x whentheagentapproachesthesidesof theroadinthelevel and m 10asitisthesamemaximumreward alsogivenbythe performancerewardfunction asweexplainbelow.
finally werewardtheagentwhenitcrossesthefinishlinewithan additional bonus.
.
.
instantiating reline performance reward function.
todefinetheperformancerewardfunction ofrelineforsupertuxkart thefirststeptoperformistodefineawaytoreliablycapturethe fpsofthegameduringthetraining.inthisway wecanrewardthe agentwhenitmanagestoidentifylow fpspoints.aspreviously said weusepysupertuxkarttointeractwiththegameandsuch a framework keeps the game frozen while the other instructions ofreline e.g.
theidentification ofthe actionto execute arerun.
sincetheframeworkrunsthegameinthesameprocessinwhich we runrelineand since we do not use threads we can safely use a simple method for computing the time needed to render the four frames we get the system time before tbefore and after tafter we trigger the rendering of the frames and we compute the time needed at step iasrti tafter tbefore.
such a value is negatively correlated with the fps higher rendering time means lower fps .
theperformance reward function we use is the following rpi braceleftbigg if x rti t otherwise wegiveaperformance basedrewardof10whentheagenttakes more than tmilliseconds to render the frames at a given point causinganfpsdrop .weexplainthetuningof tlater.wedonot give such a reward when x the kart is far from the center sincewewanttheagenttospotissuesinpositionsthatarelikely to be explored by real players i.e.
reasonably close to the track .
finally in relinewedonotgiveafixed 000bonusreward whentheagentcrossesthefinishlinebutweassignabonuscomputed as summationtext.1steps i 1rpi i.e.
proportional to the total performancebasedrewardaccumulatedbytheagentintheepisode.thisisdone to push the agent to visit more low fps points during an episode.
.
.
data collection and analysis.
as done in our preliminary study we compare relinewithrl baseline i.e.
the agent only trained to play the game and with a random agent .
training rl baseline and reline.
whileweuseddifferentrewardfunctionsforthetworlagents weappliedthesametraining processforbothofthem.wetrainedeachmodelfor2 300episodes with one episode having a maximum duration of seconds or ending when the agent crosses the finish line of the racing track theagentisrequiredtoperformasinglelap .wesetthe90seconds limitsinceweobservedthat bymanuallyplayingthegame seconds are sufficient to complete a lap.
the episodes thresholdhasbeendefinedbycomputingtheaveragerewardobtainedby thetwoagentsevery100episodesandbyobservingwhenaplateau was reached by both agents.
we found episodes to be a good compromiseforbothagents graphsplottingtherewardfunction are available in the replication package .
the trained rl baseline agent has been used to define the thresholdtneeded for the reline s reward function i.e.
for identifying when the agent found a low fps point and should be rewarded .inparticular oncetrained werun rl baseline for300episodes storing the time needed by the game to render the subsequent fourframesaftereveryactionrecommendedbythemodel.
2thisresulted in a total of data points sfps representing the standard fps of the game in a scenario in which the player is only focused on completing the race as fast as possible.
starting from the sfpsdata points collected in the episodes played by the trained rl baseline agent we apply the five rule to compute a threshold able to identify outliers.
the five rulestatesthatinanormaldistribution suchas sfps .
of observed data points lie within five standard deviations fromthe mean.
thus anything above this value can be considered asan outlier in terms of milliseconds needed to render the frames.for this reason we compute tb mean sfps sd sfps as a candidate base threshold to identify low fps points.
however tbcannot be directly used as the tvalue of our reward function.
indeed we observed that the time needed for rendering frames during the reline s training is slightly higher as compared to the time needed when the trained rl baseline agent is used to play the game.
this is due to the fact that the load on the server and in particular on the gpu is higher during training.
to overcome this issue we perform the following steps.
at the beginning of the training we run warmup episodes in whichwecollectthetimeneededtorenderthefourframesafter each action performed by the agent.
then we compute the first qtr and the third qtr quartile of the obtained distribution and compare them to the q1andq3of the distribution obtained in the episodes used to define tb i.e.
those played by the trained rl baseline agent .duringthe warmupepisodes theagentselectsthe action to perform almost randomly it still has to learn therefore itwouldnotbeabletoexploreasubstantialareaofthegame i.e.
of the racing track thus not providing a distribution of timingscomparable with the ones obtained when the trained rl baseline agent that played the episodes.
for this reason during the warmupepisodes ofthetraining theactiontoperformisnotchosen bytheagentcurrentlyundertraining butbythetrained rl baseline agent i.e.
thesameusedinthe300episodes .thisdoesnotimpact in any way the load on the server that remains the one we have during the training of relinesince the only change we have is to ask for the action to perform to the rl baseline agent rather than to theoneundertraining.however thewholetrainingprocedure e.g.
capturing the frames and updating the network stays the same.
we compute the additional cost brought by the training in rendering the frames during the game using the formula max qtr q1 qtr q3 .
we use the first and third quartiles since theyrepresenttheboundariesofthecentralpartofthedistribution i.e.
they should be quite representative of the values in it.
we took as themaximumofthetwodifferencestobemoreconservative inassigningrewardswhentheagentidentifieslow fpspoints.the final value twe use inour reward function when training reline to load test supertuxkart is defined as t tb .
.
2since we wanted to measure the frames rendering time in a standard scenario in which the agent was driving the kart we stopped an episode if the agent got stuck against some obstacle.
3weidentifyaslow fpspointstheonesinwhichthefpsislowerthan218.sucha numberis stillveryhigh more thanenoughfor anyhumanplayer inpractice.note that we run the game using high performance hardware and most importantly with thelowestgraphicsettings.theequivalentinnormalconditionswouldbemuchlower.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
using reinforcement learning for load testing of video games icse may pittsburgh pa usa 140milliseconds to render frames actions1314 figure rendering times for episodes same actions .
thus ifrelineisable duringthetraining toidentifyapointin the game requiring more than tmilliseconds to render four frames then it receives a reward as explained in section .
.
.
thetrainingof rl baseline took 3hours while relinerequires substantially more time due to the fact that after each step performed by the agent we collect and store information about thetime needed to render the frames this is done million of times .
this pushed the training for relineup to hours.
reliability of time measurements.
it is important to clarify thatthefpsofthegamecanbeimpactedbythehardwarespecificationsandthecurrentloadofthemachinerunningit.inother words runningthesamegameontwodifferentmachinesoronthe same machine in two different moments can result in variations of the fps.
for this reason all the experiments have been performed onthesameserver equippedwith2x64coreamd2.25ghzcpus 512gbddr43200mhzram andannvidiateslav100s32gbgpu.
also theprocessrunningthetrainingoftheagentsorthecollectionofthe48 sfpswiththetrained rl baseline agentwasthe only process running on the machine besides those handled by the operatingsystem ubuntu20.
.ontopofthat theprocesswas alwaysrunusingthe chrt rr option thatinlinuxmaximizes the priority of the process reducing the likelihood of interruptions.
despitetheseprecautions itisstillpossiblethatvariationsare observedinthefpsnotduetoissuesinthegame buttoexternal factors e.g.
changes in the load of the machine .
to verify the reliability of the collected fps data we run a constant agent performing always the same actions in the game for episodes.
the setofactionshasbeenextractedfromoneoftheepisodesplayed by therl baseline agent that was able to successfully conclude the race.
then we plottedthe timeneeded by thegame torender the four frames following each action made by the agent.
since we are playing300timesexactlythesameepisode weexpectto observe the sametrend in termsof fps foreach game.
ifthis is thecase it meansthatthewaywearemeasuringthefpsisreliableenoughto reward the agent when low fps points are identified.
fig.
shows the achieved results the y axis represents the millisecondsneededtorenderfourframesinresponsetoanagent s action x axis performedinaspecificpartofthegame.while as expected small variations are possible the overall trend is quite stable pointsofthegamerequiringlongertimetorenderframes areconsistentlyshowingacrossthe300episodes resultinginaclear trend.
we also computed the spearman s correlation pairwise across the distributions adjusting the obtained p values using the holm s correction .we found all correlations to be statistically significant adjusted p values .
with a minimum .
strong correlation and a median .
verystrongcorrelation .thisconfirmsthecommon fps trends across the episodes.
runningthethreetechniquestospotlow fpsareas.
afterthe 300trainingepisodes weassumethatboth therl based agentslearnedhowtoplaythegame andthat relinealsolearned how to spot low fps points.
then as also done in our preliminary study wetrainbothagentsforadditional1 000episodes storingthe time needed to render the frames in every single point they exploredduringeachepisode whereapointisrepresentedbyits coordinates i.e.
centering xandpath done y .wedothesame also with the random agent .
data analysis.
theoutputofeachofthethreeagentsisalist of points with the milliseconds each of them required to renderthe subsequent frames.
since each agent played episodes it is possible that the same point is covered several times by an agent withslightlydifferentfpsobserved aspreviouslyexplained smallvariationsinfpsarepossibleandexpectedacrossdifferent episodes .weclassifyas low fpspoints thosethat requiredmore thantmillisecondstorenderthefoursubsequentframesmorethan of times they have been covered by an agent.
thismeansthat ifacrossthe1 000episodesapoint pisexercised times by an agent at least times the threshold tmust be exceeded to consider pas a low fps point.
in practice a developer usingrelinefor identifying low fps points could use a higher threshold to increase the reliability of the findings.
however for the sake of this empirical study we decided to be conservative.
then we compare the characteristics of the low fps points identified bythe threeapproaches.
specifically we analyze i how many different low fps points each approach identified ii thenumber of times each low fps point has been exercised by eachagent in the episodes iii the confidence of the identified points i.e.
thepercentageoftimesanexercisedpointresultedin low fps .
given the low fps points identified by each agent wedraw violin plots showing the distribution of timings needed to rendertheframeswhentheagentexercisedthem thehigherthe timings the lower the fps .
we compare these distributions using mann whitneytest withp valuesadjustmentusingtheholm s correction .wealsoestimatethemagnitudeofthedifferencesby usingthecliff sdelta d anon parametriceffectsizemeasure for ordinal data.
we follow well established guidelines to interpret the effect size negligible for d .
small for .
d .
medium for .
d .
and large for d .
.
.
study results fig.
summarizes the main findings of our case study.
fig.
a showsthedistributionoftimeneededtorenderthegameframes i.e.
our proxy for fps for four groups of points.
the first violin plot on the left i.e.
regular fps shows the timing for points that have never resulted in a drop of fps in any of the episodes played by the three agents each .
these serve as baseline to better interpret the low fps points exercised by the agents.
theother three violin plots show the distributions of timing for the low fps points identified by reline blue rl baseline green and therandom agent red .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa rosalia tufano simone scalabrino luca pascarella emad aghajani rocco oliveto and gabriele bavota c centering 10reline rl baseline min medianmaxregular fps random a b low fps points low fps points low fps points avg.
conf.
minmedianmax60 avg.
conf.
minmedianmax51 avg.
conf.
path donemilliseconds to render frames figure results of the study a reports the distributions of timings for the low fps points with summary statistics while b and c depict the path done and centering coordinates at which the such points were observed respectively.
below each violin plot we report the number of low fps points identified by each agent and descriptive statistics average median min max oftheconfidenceforthelow fpspoints.a100 confidencemeansthatalltimesthatalow fpspointhasbeenexercisedinthe1 000episodesplayedbytheagentitrequiredmore thant .36millisecondstorenderthesubsequentframes.the tthreshold is represented by the red horizontal line.
on average relineexercisedeachlow fpspoint89timesinthe1 000episodes against the of rl baseline and the of the random agent the same point can be exercised multiple times in an episode .
relineidentified low fps points as compared to the ofrl baseline and the of the random agent .
the confidence is similar for reline median and rl baseline median while it is lower for the random agent median .
thus the low fpspointsidentifiedbythetworl basedagentsare overall quite reliable.
concerning the number of low fps points identified relineidentifies more points as compared to rl baseline 173vs .
this is expected since it has the explicit goal of load testingthe game however what could be surprising at first sight is the highnumberof low fps pointsidentifiedbythe randomagent .
fig.
b and fig.
c help in interpreting this finding.
fig.
b plots the path done ycoordinate for eachlow fps point identified by each agent using the same color schema of the violin plots e.g.
blue corresponds to reline .ifmultiplepointsfallinthesamecoordinate i.e.
same path done but different centering they are shown with a red border.
the scale of the path done has been normalized between and where corresponds to the starting line of the track and to its finishline.similarly fig.
c plotsthe centering xcoordinate for the low fps points.
the line at represents the center of the track whilethecontinuouslinesinposition 18and 18depict thelimitsofthetrack.finally thedashedlinesrepresentthearea of the game we asked relineto explore based on our reward function we penalize the agent for going outside the range that normalized corresponds to .
also rl baseline is penalized outside of this area.
asexpected the randomagent isnotabletoadvanceinthegame the low fps points it identifies are all placed near the starting line reddotsinfig.
b .thisindicatesthatarandomagentcanbe usedtoexerciseaspecificpartofagame butitisnotabletoexplore the game as a player would do.
this is also confirmed by the red dotsinfig.
c withthe randomagent exploringareasofthegame farfromthetrackandthatahumanplayerisunlikelytoexplore.
also it is worth noting that in supertuxkart each episode lasts based on our setting seconds if the agent does not cross the finishline.however asshowninourpreliminarystudy inother games such as mspacman a random agent could quickly lose an episode without having the chance to explore the game at all.
the low fps points identified by reline blue dots and by rl baseline green are instead closer to the track and for what concerns reline they are within or very close the area of the game we ask it to explore see dashed lines in fig.
c .
thus by customizing the reward function it is possible to define the area of the game relevant for the load testing.
looking at fig.
b we can see that relineis also able to identify low fps in different areas of the game with however a concentration close to the beginning and the end of the game.
it is difficult to explain the reason for such a result but we hypothesize two possible explanations.
first it is possible that the central part of the game simply featureslesslow fpsareas.thiswouldalsobeconfirmedbythefact thatrl baseline only found one low fps point in that part of the game.
also the training and the reward function could have drivenrelinetoexploremorethestartingandtheendingofthe game.
the starting part is certainly the most explored since at the beginning of the training the agent is basically a random agent.
thus itmostly collects experience aboutlow fps points found in thebeginningofthegamesince similarlytothe randomagent itis notableto advanceinthegame.
itisimportantto rememberthat the data in fig.
only refers to the gamesplayed by reline after the training games so we are not including the random explorationdoneatthebeginningofthetraininginfig.
.however once the agent learns several low fps points in the starting of the game it can exercise them again and again to get a higher reward.
concerning the end of the game we set a maximum duration of 90secondsforeachgame butweknowthatawell trainedagent can complete the lap in seconds.
it is possible that the agent used the remaining time to better explore the last part of the game before crossing the finish line thus finding a higher number of low fpspointsinthatarea.additionaltrainings possiblywitha differentreward function are needed to better explain our finding.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
using reinforcement learning for load testing of video games icse may pittsburgh pa usa concerning the violin plots in fig.
a we can see that reline andrl baseline exhibitasimilardistribution with relinebeingable toidentifysomestrongerlow fpspoints i.e.
longertimetorender frames .
all distributions have as expected the median above the tthreshold with reline s one being higher .
vs21.
for rl baseline and .
for random agent .
the highest value of the distributions is .
.
fps for reline against .
.
fps forrl baseline and .
.
fps for random agent .
remember that all these values represent milliseconds to load frames after an action performed by the agents.
table2 resultsofmann whitneytest adjusted p value and cliff sdelta d whencomparingthedistributionsofrendering times boldface indicates higher times.
test p value or relinevsrl baseline .
.
medium relinevsrandom agent .
.
medium rl baseline vsrandom agent .
.
small table2showstheresultsofthestatisticalcomparisonsamongthe three distributions.
in each test the approach reported in boldface istheoneidentifyingstrongerlow fpspoints i.e.
moreextreme points requiring longer rendering time for their frames .
the adjustedp valuesreportasignificantdifference p value .
in favor ofrelineagainst both rl baseline and therandom agent in both cases with a medium effect size .
thus the low fps points identified by relinetend to require longer times to render frames.
fig.
c shows an example of low fps point identified by reline crashing against the sheep results in a drop of fps.
finally it is worth commenting about the overlap of low fps pointsidentifiedbythethreeagents.indeed relineandrl baseline found14low fpspointsincommon i.e.
samexandycoordinates whiletheoverlapisof11pointsfor relineandrandomagent and 10forrl baseline andrandomagent .themostinterestingfindingof thisanalysisisthat rl baseline wasabletoidentifyonly19points missed by reline while the latter found points missed by rl baseline .
this supports the role played by the reward function in pushingrelineto look for low fps points.
summary of the case study.
relineisthebestapproach for finding low fps points in supertuxkart.
a random agent is not able to spot issues that require playing skills and rlbaselineonly finds a small portion of the low fps points.
threats to validity threatstoconstructvalidity .themainthreatstotheconstruct validityofourstudyarerelatedtotheprocessweadoptedinour case study section to identify low fps points.
based on our experiments and in particular on the findings reported in fig.
ourmethodologyshouldbereliableenoughtoidentifyvariationsinfps.still somelevelofnoisecanbeexpected andforthisreasonall our analyses have been run at least times while episodes were played by each of the experimented approaches.concerning ourpreliminary study section3 itis clear thatthe bugsweinjectedarenotrepresentativeofrealperformancebugs in the subject games.
however they are inspired from a performancemutationoperatordefinedintheliterature .ourpreliminary study only serves as a proof of concept to verify whether by modifyingtherewardfunction arl basedagentwouldadaptits behavior to look for bugs while playing the game.
threats to internal validity.
in our case study to ease the trainingwedidnotusethe real game butitswrappedversion i.e.
pysupertuxkart .whilethecoregameisthesame theversion we adopted does not contain the latest updates and it includes additional python code that may affect the rendering time.
weassume that such a time is constant for all the frames since itsimply triggers the frame rendering operation in the core game.
besides weforcedthegametorunwithlowestgraphicssettings to speed up r endering for example we excluded dynamic lighting anti aliasing and shadows.
therefore the low fps points found in pysupertuxkart may be irrelevant in the original game or with othergraphicsettings.also weappliedthefive ruletodefinea thresholdfordefiningwhatalow fpspointis.thethresholdwe set might be not indicative of relevant performance issues.
still the goal of our study was to show that once set specific requirements e.g.
the threshold t the area to explore etc.
the agentisabletoadapttryingtomaximizeitsreward.thus wedo not expect changes in the threshold to invalidate our findings.
threats to conclusion validity.
inourdataanalysisweused appropriatestatisticalprocedures alsoadopting p valueadjustment when multiple tests were used within the same analysis.
threats to external validity besides the proof of concept studywepresentedinsection3 ourempiricalevaluationof reline includes a single game.
this does not allow us to generalize ourfindings.
the reasons for such a choice lie in the high effort weexperiencedasresearchersin i buildingthepipelinetointeract withthegame ii findingandexperimentingwithareliableway to capture the fps iii defining a meaningful reward function that allowedtheagenttosuccessfullyplaythegameinthefirstplace and then toalsospotlow fpspoints.thesestepswerealongtrialand error process with the most time consuming part being the trainingsneededtotestthedifferentrewardfunctionsweexperimentedbeforeconvergingtowardstheonespresentedinthispaper.indeed testinganewversionofarewardfunctionrequiredatleast one week of work with the hardware at our disposal including implementation training and data analysis .
thiswasalsoduetotheimpossibilityofusingmultiplemachines or to run multiple processes in parallel on the same server.
indeed asexplained usingtheexactsameenvironmenttorunallourexper imentswasastudyrequirement.itisworthnotingthat becauseof similar issues other state of the art approaches targeting different game properties were experimented with only one game as well seee.g.
.webelievethatinstantiating relineona newgamewouldbemucheasierbycollaboratingwiththegame developers.whilethiswouldonlyslightlysimplifythedefinition of a meaningful reward function the original developers of the game could easily provide through apis all information needed by reline including e.g.
the fps cutting away weeks of work.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa rosalia tufano simone scalabrino luca pascarella emad aghajani rocco oliveto and gabriele bavota related work threerecentstudies suggestthatfindingperformance issuesinvideogamesisarelevantproblem accordingtobothgame developers andplayers .nevertheless tothebestofour knowledge no previous work introduced automated approaches for load testing video games.
therefore in this section we discuss some important works on thequality assurance of video games in general.
we first introduce the approaches defined in the literature for training agents able to automatically play and win a game.
then we show how such approaches are used for play testing for i finding functional issues and ii assessing game level design e.g.
finding unbalanced levels or mechanics .
.
training agents to play reinforcement learning rl is widely used to train agents able to automatically play video games.
mnih et al.
presented the firstapproachbasedonhigh dimensionalsensoryinput i.e.
raw pixels from the game screen able to automatically learn how toplay a game.
the authors used a convolutional neural network cnn trained with a variant of q learning to train their agent.
the proposed approach is able to surpass human expert testers in playing some games from the atari benchmark.
vinyalsetal.
introducedsc2le arlenvironmentbasedon thegame starcraftii thatsimplifiesthedevelopmentofspecialized agents for a multi agent environment.
hesselet al.
analyzed six extensions of the dqn algorithm forrlandtheyreportedthecombinationsthatallowtoachievethe best results in terms of training time on the atari benchmark.
bakeretal.
exploredtheuseofrlinamulti agentenvironment i.e.
thehideandseek game .theyreport thatagentscreate self supervisedautocurricula i.e.
curriculanaturallyemerging fromcompetitionandcooperation.asaresult theauthorsfound evidence of strategy learning not guided by direct incentives.
berneret al.
reported that state of the art rl techniques weresuccessfullyusedinopenaifivetotrainanagentabletoplay dota2andtodefeattheworldchampionin2019 teamog .finally mesentier etal.
reportedthataiagentscouldbeeasilytrained to explore the states of a board game ticket to ride performing automated play testing.
.
testing of video games functionaltestingofvideogamesaimsatfindingunexpectedbehaviors in a game.
defining the test oracle i.e.
determining if a specificgamebehaviorisdefective isnottrivial.severalcategories of test oracles were identified to determine if a bug was found crash the game stops working stuck the agent can not winthegame gamebalance gametooeasyortoohard logical an invalid state is reached anduser experience bugs related to graphic and sound e.g.
glitches .
while heuristics can be used to find possible crash stuck and game balance related bugs logical and user experience bugs may require the developers to manually define an oracle.
iftikharet al.
proposed a model basedtesting approach for automatically perform black box testing of platform games.
more recent approaches mostly rely on rl.pfauetal.
introducedicarus aframeworkforautonomous play testingaimed atfinding bugs.icarussupports thefully automated detection of crashandstuckbugs while it also provides semi supervised support for user experience bugs.
zhenget al.
used deep reinforcement learning dlr in theirapproach wuji.wuji balancesthe aimof winningthegame and exploring the space to find crash stuck game balance and logicalbugs in three video games one simple block maze and two commercial l10andnsh .
bergdahl etal.
definedadlr basedmethodwhichprovides supportforcontinuousactions e.g.
mouseorgame pads andthey experimented it with a first person shooter game.
wuet al.
used rl to automatically perform regression testing i.e.
to compare the game behaviors in different versions of agame.theyexperimentedwithsuchanapproachonamassive multiplayer online role playing game mmorpg .
ariyurek et al.
experimented rl and monte carlo tree search mcts todefinebothsyntheticagents trainedinacompletely automated manner and human like agents trained on trajectories used by human testers.
finally ahumadaandbergel proposedanapproachbased on genetic algorithms to reproduce bugs in video games by reconstructing the correct sequence of actions that lead to the desired faulty state of the game.
.
game and level design assessment one of the main goals of a video game is to provide a pleasantgameplay to the player.
assessing the game balance and other aspectsrelatedtogame andlevel designis therefore ofprimary importance.
for this reason previous work defined several approaches for automaticallyfindinggame andlevel designissuesinvideogames.
zooket al.
proposed an approach based on active learning al tohelpdesignersperforminglow levelparametertuning.they experimented such an approach on a shoot em up game.
gudmundsson et al.
introduced an approach based on deep learningtolearnhuman likeplay testingfromplayerdata.they usedacnntoautomaticallypredictthemostnaturalnextactiona playerwouldtakeaimingtoestimatedifficultyoflevelsin candy crush saga andcandy crush soda saga.
zhaoet al.
report four case studies in which they experimenttheuseofhuman likeagenttrainedwithrltopredictplayer interactionswiththegameandtohighlightpossiblegame design issues.onasimilarnote pfau etal.
useddeepplayerbehavioral models to represent a specific player population for aion a mmorpg.
they used such models to estimate the game balance and they showed that they can be used to tune it.
finally stahlke etal.
definedpathos atoolaimedathelping developers to simulate players interaction with a specific game level to understand the impact of small design changes.
conclusions and future work we presented reline an approach that uses rl to load test video games.relinecan be instantiated on different games using different rl models and reward functions.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
using reinforcement learning for load testing of video games icse may pittsburgh pa usa ourproof of conceptstudyperformedontwosubjectsystems showsthefeasibilityofourapproach givenarewardfunctionable torewardtheagentwhenartificialperformancebugsareidentified the agent adapts its behavior to play the game while looking for those bugs.
we performed a case study on a real 3d racing game supertuxkart showing the ability of relineto identify areas resulting infpsdrops.ascomparedtoaclassicrlagentonlytrainedtoplay the game relineis able to identify a substantially higher number of low fps points vs33 .
despite the encouraging results there are many aspects that deserve a deeper investigation and from which our future research agenda stems.
first we plan additional tests on supertuxkart to betterunderstandhowtheagentreactstochangesinthereward function e.g.
isitpossibletofindmorelow fpspointsinthecentral part of the game?
.
also with longer training times it should bepossibletotrainanagentabletoplaymorechallengingversions of this game featuring additional 3d effects e.g.
rainy conditions possibly allowing to find new low fps points.
we also plan to instantiate relineon other game genres e.g.
role playing games possibly by cooperating with their developers.
in our replication package we release the code implementing the models used in our study and the raw data of our experiments.
acknowledgment this project has received funding from the european research council erc under the european union s horizon research and innovation programme grant agreement no.
.
anyopinions findings and conclusions expressed herein are the authors and do not necessarily reflect those of the sponsors.