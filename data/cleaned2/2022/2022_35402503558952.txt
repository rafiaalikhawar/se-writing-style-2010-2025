leveraging test plan quality to improve code review efficacy lawrence chen meta platforms inc. usarui abreu meta platforms inc. usatobi akomolede meta platforms inc. usa peter c. rigby concordia university canadasatish chandra meta platforms inc. usanachiappan nagappan meta platforms inc. usa abstract in modern code reviews many artifacts play roles in knowledgesharing and documentation summaries test plans and comments etc.
improving developer tools and facilitating better code reviews require an understanding of the quality of pull requests and their artifacts.
this is difficult to measure however because they are often free form natural language and unstructured text data.
in this paper we focus on measuring the quality of test plans at meta.
test plans are used as a communication mechanism between the author of a pull request and its reviewers serving as walkthroughs to help confirm that the changed code is behaving as expected.
we collected developer opinions on over test plans from more than 500meta developers then introduced a transformer based model to leverage the success of natural language processing nlp techniques in the code review domain.
in our study we show that the learned model is able to capture the sentiment of developers and reflect a correlation of test plan quality with review engagement and reversions compared to a decision tree model our proposed transformer based model achieves a higher f1 score.
finally we present a case study of how such a metric may be useful in experiments to inform improvements in developer tools and experiences.
ccs concepts software and its engineering software creation and management acceptance testing walkthroughs .
keywords code reviews pull requests test plans natural language processing.
acm reference format lawrence chen rui abreu tobi akomolede peter c. rigby satish chandra and nachiappan nagappan.
.
leveraging test plan quality to improve code review efficacy.
in proceedings of the 30th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november singapore singapore.
acm new york ny usa pages.
work performed while on sabbatical at meta platforms inc. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november singapore singapore copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
introduction at meta the test plan is a component of our code review process and culture.
in addition to the automated test suites that are run for each code change developers also provide free form documentation summarizing the testing process employed to verify the code changes.
the goal is to describe the manual steps the developer took during the development process to sanity check the code changes such as commands to run specific automation screenshots of expected ui changes or steps to conduct a full end to end test.
these test plans are similar to the ones found in the software quality assurance process or the test plans described in the ieee standard for software test documentation .
however meta s test plans are generally concerned with the scope of one set of code changes at a time rather than the entire software product.
hence test plans are complementary to other forms of validation.
for example in code review the formal compilable source code is studied to find defects in testing regressions are identified by test failures.
in contrast test plans serve as simple written walkthroughs of the system behavior that provide assurances and context to reviewers.
test plans are highly effective at meta and one of our goals is to describe test plans to the larger se community.
the flexible and lightweight nature of test plans make them easy to write and easy to understand but this makes measuring the quality of test plans significantly more difficult.
our goal is to develop a model that can differentiate good test plans from bad which helps us better understand the overall state of test plans at meta.
this information could also be used to help suggest to developers when they may need to improve the quality of test plans to better improve the code review process.
in particular this experience paper aims to gain an understanding of the following research questions modeling quality can we use a data driven approach to model holistic test plan quality that aligns with the opinions of the developers?
applying nlp techniques do the state of the art transformer based models used in nlp tasks translate to the code review domain and outperform more trivial methods that require feature engineering?
correlation with review engagement is our datadriven test plan classifier correlated with reviewer engagement?
correlation with regressions do pull requests that get reverted or are associated with outages have test plans with lower quality?
to help us answer these questions we first need to understand how test plans are used at meta for details see section .
but at esec fse november singapore singapore lawrence chen rui abreu tobi akomolede peter c. rigby satish chandra and nachiappan nagappan a high level test plans at meta are used to augment the code review process acting both as a procedure to exercise functionality and as part of the documentation.
yet another fundamental characteristic of a test plan is that it should be repeatable the contract is that executing the specified steps should be sufficient to have confidence that the feature works correctly.
we have some intuition of data that can be mined from test plans such as counting words attached media or links and we could turn these into metrics.
these generally make sense as desirable features to have in a test plan and could indirectly signal the quality of the test plan.
however they are not easily consolidated into a single holistic quality metric and any trivial heuristic is likely neither flexible nor robust.
but most importantly these trivial extractions ignore the most valuable data in a test plan which is the free form natural language text.
developers often use test plans to detail replication steps or end to end integration steps.
the conciseness and clarity of the natural language descriptions could be informative of a test plan s quality as might interactions between the text and attached media or links.
therefore we look to data driven natural language processing nlp techniques to leverage such information.
we define the problem as a binary classification model with labeled test plans professionally rated as good orbad by500 meta developers.
we leverage a pre trained roberta language model for our architecture which takes the test plan as a natural language input.
we tokenize the test plan string and generate an embedding using that model then utilize the few shot learning matching network architecture to classify the test plan embedding as either good orbad .
we observed that the natural language approach via fine tuning a roberta based model outperformed our decision tree baseline and improved the f1 score by .
this suggests that the general pre trained roberta model is successful at learning the patterns of test plan quality simply through fine tuning on this code review specific domain.
by capturing both structural and semantic information our transformer based approach outperforms the baseline without the need of manual feature engineering for its input features.
to summarize this paper makes the following contributions we define test plans at meta and how they are used to augment the code review process.
we demonstrate that nlp architectures such as a robertabased model can be applied to the code review domain to quantify the quality of test plans.
we evaluate our approach on the task of measuring the quality of test plans and demonstrate how such a quality heuristic may be used to inform improvements in developer tools.
the main take away message of this paper is that pull requests with high quality test plans are observed to be involved in fewer outages be reverted fewer times and have more reviewer engagement.
the remainder of this paper is structured as follows in section we describe how code review is done at meta discuss what testplans are in the context of code review at meta and introduce our internal code review engagement metric.
in section we introduce our nlp based holistic approach to modeling test plan quality.
we also introduce a baseline technique based on decision trees.
in section we present our results and discuss threats to validity.
in sections and we discuss our findings in the context of related work and describe potential applications to downstream tasks.
in section we conclude the paper.
background this section details the meta code review process introduces a custom code review engagement metric and discusses what test plans are in that context.
it is noteworthy that test plans are a communication channel between the author and the reviewers not aproof that the changes were tested.
one common workflow is allowing a reviewer to try out the modification in the pull request before it gets committed.
.
code review workflow at meta phabricator an open source project is the backbone of meta s continuous integration system1 and is the surface for the modern code review process.
developers use phabricator both to submit pull requests and to comment on others requests before they are accepted into the codebase or are discarded .
more than pull requests are committed to the central repository every week at meta using phabricator as a central gate keeping reporting curating and testing system .
the author uploads the changed code to phabricator includes a test plan assigns reviewers or groups of reviewers and publishes the pull request .
the act of publishing a pull request sets its status to needs review making it visible to all assigned reviewers.
at least one reviewer s approval is required to accept and therefore ship a pull requests.
phabricator s ui displays the contents of the pull request see figure a title summary the code changes and the test plan detailed in section .
.
the reviewer can add comments which are visible to everyone accept send back to the author requiring further changes resign or commandeer becoming the author .
the author of the pull request meanwhile has several available actions.
they can amend their code change and update the pull request to a new iteration request another review pass add comments to e.g.
explain an update or address reviewer feedback pause the review process until all changes are complete abandon the request or once the request is accepted ship it to production.
shipping a code change is gated on approval by reviewers.
.
code review quality and engagement one research question explored in this paper involves tying our holistic test plan classifier to downstream metrics of review quality and engagement.
to quantify review quality and engagement we developed a custom heuristic that is a function of the size of the changes the number of review comments and the time spent reviewing.
code review teams at meta utilize several review commentary and cycle characteristic metrics which are then combined into 1321leveraging test plan quality to improve code review efficacy esec fse november singapore singapore figure an example view of a pull request under review in phabricator .
authors and reviewers can interact via the pull request review page in phabricator .
the review has several sections the summary changes reviewers interactions between the reviewers and author test case information and status results of static analysis historical information etc.
figure a simplified example test plan for a diff that implements a blocked list of users.
the test plan details the set of steps and ui outcomes that will verify that the code works as expected.
a single engagement metric to conveniently monitor the level of engagement that pull requests receive.
this is also interpreted as a proxy for the quality of the review at meta as it indicates how developers interact during a pull request review which we define to be the process of inspection and discussion undertaken by the reviewers subscribers and author of a pull request.
moreover we limit discussion to mean the set of review comments posted withinthe meta s code review tool phabricator .
this definition excludes any review discussion that might occur by any other means such as in person direct messaging or over videoconferencing.
in the context of this paper this exact definition is not crucial to our work.
we treat this pull request engagement metric as a black box an interested reader can define her own quality and engagement heuristic to replicate our study in a different company context and the main idea is that this metric is used at meta to measure and represent the quality and engagement of the pull request review.
we will outline the set of features that are input to this supervised machine learning based engagement metric which comes from two main dimensions review commentary and cycle characteristics.
regarding review commentaries the set of features are number of substantive comments2 number of substantive comment threads number of substantive head level non reply author comments and the number of reviewers that leave substantive comments.
with respect to cycle characteristics the set of features are number of pull request versions and number of times the pull request was set to needs revision status before landing.
the key takeaway of using an engagement metric is that it measures the activity and interaction between the reviewer and the authors.
more engagement and activity logically lead to better reviews and thus to generally fewer reversions after shipping code.
we validated our engagement metric and found that this was indeed the case our internal analysis of this engagement metric indicates that pull requests that have low engagement scores are .
more likely to be reverted than pull requests with high engagement scores.
hence we use this engagement metric to measure the 2a substantive comment is defined as a comment that is considered nontrivial.
we filter out trivial comments that are not engaging.
this is a somewhat loose definition but includes simple comments of affirmation such as good job or nice one .
1322esec fse november singapore singapore lawrence chen rui abreu tobi akomolede peter c. rigby satish chandra and nachiappan nagappan review quality of a pull request and in later sections we demonstrate that better test plans as measured by our proposed technique result in better review quality and less pull request reversions.
.
what are test plans?
as mentioned before a test plan is a repeatable list of steps which documents what the author has done to verify the behavior of a change and is a required component of the code review process at meta.
a good test plan convinces a reviewer that the author has been thorough in making sure the change works as intended and has enough detail to allow someone unfamiliar with the code change to verify its behavior.
it is worth noting that test plans are not meant to replace automated testing mechanisms that may be added as part of the pull request e.g.
unit testing or ui testing .
instead test plans are complementary to automated test plans in the sense that they are intended as an aid to the reviewer of the pull request in replicating the functionality that is being implemented.
hence a test plan is a mostly textual step by step guide to anyone trying to figure out the impact of the modifications in the pull request the clear the steps the more easily developers can execute it.
we observe that test plans at meta are used to allow reviewers to try out features before they are committed.
allow reviewers to identify edge case user behaviors to consider in an end to end test plan.
document clear steps for consistent reproduction or testing if the pull request is updated during the review process.
document testing procedure for future engineers to verify what the expected behavior of the code should be.
figure is a simplified example of a test plan for a change in ui.
the plan contains links to verify that the changes are as expected.
screenshots demonstrating the before and after make it clear to the reviewers what the intended effects are rather than only relying on unit tests which may be less intuitive to check ui changes during code review.
moreover writing ui test scripts can be time consuming and difficult to maintain as well as ui scripts are inherently fragile .
a non ui example could be for new api endpoints in which test plans can demonstrate example calls to the new endpoints and detail the expected inputs and outputs after the code changes.
this makes it easier for reviewers to quickly confirm that they agree with the current behavior of a new feature on an api level rather than having to parse test cases which may be less readable.
it is important to note that test plans are used not just for functional changes pull requests with non functional changes also benefit from high quality test plans.
moreover note that test plans are not meant to replace any type of testing but rather to explain to reviewers how you know your code is behaving as expected.
test plans enhance the code review process serving as additional readable documentation along with more rigorous unit tests that are also enforced at meta.
the contract of a test plan is that if a developer were to replicate the steps of a test plan the developer should be confident that the code changes work as expected.
this means not just testing the new code but also making sure that nothing else will be broken.
inan ideal world there should be tests protecting the rest of the code but it is also the responsibility of the pull request s author to make sure that the pull request is not introducing any regressions.
test plans allow authors to easily demonstrate that with more clarity and precision.
for instance if code is to be deleted in a pull request the test plan should include a change impact analysis explaining how the developer knows this code is not being used e.g.
using test coverage or code search.
in general a good test plan should give clear concise and reproducible instructions that someone else can easily follow.
our test plan classifier approach we approached the holistic test plan classifier by modeling it using nlp techniques.
the current state of the art for nlp problems such as text classification machine translation and text generation typically involve the now ubiquitous transformer based architecture and we are interested to see if such success can transfer over to our code review domain specific use case.
specifically we used the roberta transformer architecture to model test plan quality.
aside from a few basic preprocessing steps such as using keywords to represent screenshots or links we were able to use the test plan string as the raw input data like natural language data.
roberta short for robustly optimized bert pre training approach is a pretrained natural language processing system that improves on bidirectional encoder representations from transformers or bert the self supervised method released by google in .
bert achieved state of the art results on a range of nlp tasks while relying on unannotated text drawn from the web as opposed to a language corpus that has been labeled specifically for a given task.
the technique has since become popular both as an nlp research baseline and as a pre trained model backbone for downstream language tasks.
for roberta the objective was to optimize the training of bert architecture in order to take less time during pretraining.
roberta has been shown to produce state of the art results including the impact of training data and training time on the widely used nlp benchmarks such as glue as well as superglue and squad dataconstrained setting .
our model architecture consists of a roberta model which produces embeddings of length for our test plan text input3 which is then fed into a simple fully connected layer for classification.
we started with an open source roberta model from hugging face that is pretrained on general nlp tasks 4and then fine tuned the whole model end to end on our test plan data.
we used the default pretrained tokenizer adding no additional custom tokens.
our only data preprocessing involved replacing links or screenshots that may be arbitrarily long with shorter standardized keywords.
specifically all multi line code markup sections often representing large log outputs were replaced with codeblock url links were replaced with url and attached images or other media were replaced with screenshot .
note that these keywords were just string replacements not explicitly added as special 3test plans that are longer than tokens are truncated.
note however that most test plans in our dataset fit within this limit.
1323leveraging test plan quality to improve code review efficacy esec fse november singapore singapore tokens to the vocabulary.
the chosen keywords were arbitrary and only meant to clean and shorten input data with standardized representations.
in this supervised learning modeling we also need labels for our data.
our labels were good orbad quality ratings that we gathered from experienced software developers at meta.
in total we collected over data samples of test plans and their corresponding ratings as labeled by about 500meta engineers see section .
for more details .
our goal with the model is to learn a model representation of what reviewers at meta deem to be good quality test plans.
essentially this task is similar to a sentiment analysis nlp task e.g.
but specifically for meta s code review domain.
by using a pre trained roberta model we were able to leverage the natural language representations learned from general nlp tasks and apply them to test plans via transfer learning.
in addition we experimented with replacing the fully connected layer in the roberta classifier with a matching network wrapper to take advantage of one shot learning principles which further improved performance.
matching networks which are designed to work well with little data work by utilizing a support set .
in a high level description matching networks are a deep learning k nearest neighbors in which the model learns to embed inputs with the context of a support set acting as the neighbors .
our approach is to use this contextual embedding technique on top of the roberta embeddings.
we argue that this will allow us to maintain the roberta classifier s strength of flexibility and performance while addressing its potential weaknesses of overfitting smaller datasets.
additionally one potential advantage that we get from using a matching network architecture is that it provides a lightweight notion of interpretability.
the matching network architecture compares the input test plan to a support pair ofgood andbad labeled test plans and the output of the model effectively indicates which example test plan in the support pair is most similar to the input test plan in question.
this sort of interface may be more intuitive to a human user than a simple binary label output.
however incorporating a more fine grain notion of model explanation is left as future work.
for this extension on the roberta model we simply just reimplemented the matching network architecture as proposed in the paper .
but instead of deriving the input features to the matching network from a convolutional neural network cnn as in the original paper did for image processing our inputs are the roberta model embeddings generated from our test plan data.
the whole model is then trained end to end.
the matching network architecture uses an example support set data allowing it to learn the similarities between new data samples and the support set data.
multiple inference passes can be done with the matching network architecture by utilizing different pairs of support data allowing this model to effectively behave as an ensemble model as well further improving performance.
we do not have any models or heuristics as the status quo that we can use as our baseline comparison so we also developed a simple decision tree model that takes the feature engineered metrics as input as listed in table .
this model is not used in practice but rather it is a representation of a naive solution which we use to compare with our new proposed technique.
it is an example oftable list of manually constructed features for the decision tree baseline model.
feature name description non code length length of test plan excluding sections formatted as multi line code markup num url number of url links included in test plan has codeblock whether test plan contains sections formatted as multi line code markup has codeline whether test plan contains single line code markup typically representing run commands has test command whether test plan contains common test commands e.g.
hack unit tests jest etc.
has screenshot whether test plan contains images videos or other media attachments which are typically screenshots or screen recordings has common commandswhether test plan contains common run commands such as for linters formatters or static analyzers how one might come up with manually hand crafted heuristics or features to model test plan quality which we argue is less scalable and less performant than data driven approaches that we explore.
as for the features listed in table not that it is not always the case that the more the better.
as an example having many urls with no text to explain it may be an indication of a bad test plan.
this list is a good representation of the type of manually defined metrics the code review team may have used as their main signal of test plan quality prior to our work.
the decision tree therefore represents what a possible simple heuristic unifying the metrics might look like for a holistic model classifier.
the key difference for this baseline is that it requires manual feature engineering and heuristic definition as opposed to our roberta based models that require little data preprocessing.
this decision tree baseline also fails to capture syntactic structure or semantic information.
figure shows the three approaches considered in our experiments.
all in all our proposed models have the benefit of no feature engineering other than tokenization simple regex replacement for media and links capturing natural language data including both syntactic and semantic information being more generalizable without relying on rigid rules.
empirical evaluation this section details the empirical evaluation of our test plan classification approaches.
the industrial context of the empirical evaluation is meta s continuous integration system.
.
setup to obtain test plan labels we identified a list of 500developers who have conducted the most code reviews in the past year.
for this list of top reviewers we randomly sent them a survey during the code review process asking them to rate the test plan quality as either good orbad .
using this method we collected over 1324esec fse november singapore singapore lawrence chen rui abreu tobi akomolede peter c. rigby satish chandra and nachiappan nagappan figure approaches considered in our study.
human rated test plans as this fold our training set.
as we wanted to gauge the opinions of the reviewers in a realistic setting each test plan was rated by the reviewer often only one of the pull request using phabricator .
for training each of our three models we employed a data split for training validation and testing.
note that the support set used during the matching network training and validation is sampled from their respective datasets.
during the inference or testing phase for matching network we use a separate support set that we set aside consisting of 3support pairs.
following the questions outlined in the introduction concretely we look to answer four research questions in our empirical experiments can we use a data driven approach to model holistic test plan quality that aligns with developer opinions?
do state of the art transformer based models used in nlp tasks translate to the code review domain and outperform more trivial methods that require feature engineering?
is our data driven test plan classifier correlated with reviewer engagement?
do pull requests that get reverted or are associated with outages have test plans with lower quality?
for the first two research questions we simply evaluate our model performance on the test set using the f1 score as our metric.
the f1 score is a widely used metric for binary classification which makes it a suitable and simple way to compare the models with which we are experimenting.
formally the f1 score is the harmonic mean of precision and recall f1 score precision recall precision recall where precision is the number of true positive results divided by the number of all positive predictions including those not identified correctly and recall is the number of true positive results dividedby the number of all samples that should have been identified as positive.
the latter two research questions are concerned with tying our data driven quality metric with some related downstream groundtruth metric.
to do so we will be applying our data driven model on unlabeled data to measure their test plan quality then do simple metric comparisons to find correlations.
the third question is interested in how test plan quality correlates with review quality and engagement as discussed in section .
.
the final question is concerned with test plan quality for pull requests that are related to some form of regression and thus intuitively of lesser quality.
.
results we measure the model performance by measuring the f1 score of the predictions on our test set see table .
we observed that the roberta simple classifier model and roberta matching network model improved the average f1 score by about and respectively compared to the decision tree baseline model.
our observations suggest that our model is able to capture and model the sentiment of meta developers towards sample test plans with about accuracy.
this is quite substantial considering developer ratings of test plan quality is to a certain extent a subjective measure.
roberta simple classifier model and roberta matching network model improved f1 score by about and on average respectively compared to the decision tree baseline model.
we also observe that our predicted test plan quality is correlated with a pull request s review quality or review engagement as shown in table .
first we predicted the quality of unlabeled test plans using our matching network model.
when controlled for lines of code because pull requests of different sizes may have different levels of review engagement pull requests with predicted good test plans have on average .
higher reviewer engagement.
1325leveraging test plan quality to improve code review efficacy esec fse november singapore singapore table model performances on test set predicting good orbad labels for test plans.
good label f1 scorebad label f1 scoreaverage f1 score decision tree baseline0.
.
.
roberta simple classifier0.
.
.
roberta matching network0.
.
.
if we look specifically at the moderately sized pull requests which avoids confounding variables of trivial or overly complex code changes we see that pull requests with good predicted test plans have .
higher reviewer engagement a statistically significant increase.
this suggests that our test plan classifier does not just align with our developer sentiment but can also be tied to concrete metrics that demonstrate the impact of test plan quality.
pull requests with predicted good test plans have on average .
higher reviewer engagement this value increases to .
higher reviewer engagement if only considering pull requests with moderately sized changes.
finally we also gathered a dataset of pull requests associated with known outages at meta in particular we collected an order of104diffs associated with reversions and half as many associated with outages.
these pull requests may not necessarily be the direct cause of these outages but are mentioned in outage reports which strongly suggests they may have caused regressions.
moreover we also gather a dataset of reverted pull requests.
when we run the inference on the test plans of these pull requests we see that their predicted quality is noticeably lower.
as shown in table for pull requests associated with outages we see that the average test plan quality rating is .
lower than the rating for other pull requests.
however we observed that this difference is not statistically significant.
we argue that a potential reason for this observation is that we have a very limited amount of sample outage related pull requests as there are not that many outages at meta.
when we look at reverted pull requests which is a larger sample size we see that the predicted test plan quality is .
lower than the ratings for other pull requests.
with the larger dataset we see that this difference is indeed statistically significant.
by using our model we can quantify that lower test plan quality is correlated with poorer quality pull requests that cause regressions or are eventually reverted matching our intuition that test plan quality affects the overall pull request and code review quality.
the average test plan quality of pull requests associated with outages is .
lower than the average test plan quality rating for other pull requests the average test plan quality associated with reverted pull requests is .
lower than the average test plan quality rating for other pull requests.table review engagement metric comparison for pull requests with good vs.bad control test plans as predicted by trained roberta matching network model and controlled by lines of code of pull request.
lines of code avg.
metric changet value p value bottom third quantile lines .
.
.
middle third quantile lines and lines .
.
.
top third quantile lines .
.
.
table test plan classification as predicted by trained matching network model comparing metric change for outage related or reverted pull requests with other pull requests.
avg.
good test plans change t value p value other pull requests0.
pull requests associated with outages0.
.
.
.
reverted pull requests0.
.
.
.
.
threats to validity this subsection discusses potential external construct and internal threats.
.
.
external validity generalizability.
drawing general conclusions from empirical studies in software engineering is difficult because any process depends to a degree on a potentially large number of relevant context variables.
while this analysis was performed at meta it is possible such results might not hold in other environments domains companies.
as an example the results might not hold for small and or local teams where the value of explicitly written test plans may not bring value over in person communications.
for this reason we cannot assume a priori that the results of a study generalize beyond the specific environment in which it was conducted.
researchers become more confident in a theory when similar findings emerge in different contexts .
therefore we urge other researchers to replicate a similar study in other environments.
.
.
construct validity.
construct validity is used to determine whether the dependent and independent variables accurately represent the concepts they are supposed to measure.
a particular threat to construct validity of our work is that we leverage an in house metric to review quality review engagement that has otherwise not been validated by the scientific community.
to mitigate this 1326esec fse november singapore singapore lawrence chen rui abreu tobi akomolede peter c. rigby satish chandra and nachiappan nagappan threat we have conducted an extensive study with the metric at meta hence the metric is validated by developers in a real context.
other threats to construct validity are related to the suitability of our evaluation metrics and the quality of the labeled datasets that we use.
f1 score is widely used to evaluate solutions such as our approach.
as for labeling the datasets there is a possible confounding variable that teams who spend more time writing good test plans have a culture that also separately results in them writing better pull requests reviews.
moreover the quality labels of test plans that indeed have an element of subjectivity.
however given that the proposed approach is to be used by humans we argue that this subjectivity is acceptable.
in particular we wanted to see if a model could capture the general trend or opinions of human opinions even if subjective.
we then found that our model was decent at modeling human subjective opinions and even more interestingly they have significant correlations with objective metrics like review engagement reversions and outages.
yet another potential threat to the validity regarding the labels of the test plans is that one might question whether a binary classification is a good representation of test plans.
one might argue that there may be more levels to the quality of the plan other than simply good orbad .
.
.
internal validity.
one potential threat to internal validity relates to errors that we may have made in our experimental setup pipeline.
this threat has been mitigated by careful peer review of the pipeline by the authors.
moreover as mentioned before we gathered a dataset of pull requests associated with known outages at meta.
however these pull requests may not necessarily be the direct cause of outages they are mentioned merely in outage reports.
although strongly suggesting that they may have caused regressions outage associated pull requests are not necessarily guaranteed to be the root causes of outages.
additionally note that the size of the dataset is limited.
we did not perform any hyperparameter tuning for our models which leaves open the possibility that these models can be further improved through a rigorous hyperparameter optimization.
our decision tree baseline was trained with a depth threshold of the criterion set to entropy min samples leaf set to and all other hyperparameters set to default in the sklearn api5 .
for our gradient based models we trained both for 50epochs with batch sizes of .
we set the weight decay to 0and the learning rate to .
these hyperparameter decisions were largely driven by the motivation to keep resource usage manageable and reasonable.
by using non optimized defaults the hyperparameters were fixed before looking at the test set to ensure no contamination.
discussion downstream usages one strong implication of our findings is that higher quality test plans will lead to better review engagement.
that is a bad test plan will make it difficult for reviewers to properly review or test the code whereas a good test plan means reviewers can more efficiently catch issues or provide reviewers more context and confidence prompting actual productive conversations or reviews to happen.
explain below usage scenarios for external researchers and engineers to leverage such a system motivated by usage at meta.
i use it to surface high quality test plans as recommendations to developers.
at meta we have internal tools that help developers find test plans from related code changes and files.
we can use the proposed model to filter or rank higher quality test plans when developers request test plan examples.
this can encourage higher quality test plans at meta and also improve the developer and code review experience.
moreover surfacing high quality test plans may serve well as an educational tool for novice developers.
ii the second is to use it as a distinctive feature in other prediction modeling approaches.
as an example phabricator offers a functionality to predict the time it will take to review a pull request.
one can imagine building a model that considers the test plan classifier as one of its input features.
we argue that better test plans will help with the accuracy of such predictions.
iii to use it as a downstream metric in our experiments.
returning back to our original motivation we would like to make sure that new features or code review efforts do not hurt the quality of our pull requests.
in our experiments we can use our classifier to make sure that the quality of test plans does not deteriorate or maybe we can even find ways to improve it.
a example case study is using test plan linters.
at meta we developed some linters for test plans hoping to nudge reviewers to create higher quality test plans.
for example to ensure that the test plans provide as much context as possible we developed an attach screenshot linter and missing link linter.
the attach screenshot linter predicts when a test plan should contain a screenshot based on historical data and recommends developers attach an image if they are missing one.
the missing link linter asks the developer to also provide an url link to document the source of the attached media if it is not included.
in our experiment we want to test if these linters would indeed improve test plan quality and thus reduce other downstream metrics we care about or perhaps these linters were ignored or even worse resulted in developers the bare minimum instead.
what we found was that pull request staleness had decreased and test plan quality as measured by our model had increased.
this is a great empirical anecdote of how test plan quality is tied closely with other important code review metrics and can help us understand the impacts of new features in our experiments.
related work there are numerous studies in academia investigating how developers perceive review quality.
in code review quality how developers see it the authors surveyed developers at mozilla and found that key aspects of a well done code review include thorough feedback advice around correctness issues and the quality of the code changes themselves.
in characteristics of useful code reviews an empirical study at microsoft the authors surveyed developers to build a set of criteria with which to label code review comments as useful or not useful .
they then employ this criterium to manually label a training set with which they train a classifier that detects the usefulness of a comment.
it should be noted that the above studies look at human based subjective measures of quality.
the former study directly surveys 1327leveraging test plan quality to improve code review efficacy esec fse november singapore singapore developers whereas the latter builds a classifier around criteria formed from human sentiment.
the next series of studies mentioned will look at code review quality from the perspective of objective measures of quality such as the incident of post release defects or occurrence of anti patterns.
mcintosh et al.
studied whether various code review participation metrics have an effect on post release defects in the context of modern asynchronous code reviews.
rather than attributing defects to pull requests directly the authors attribute defects to components which are folder level aggregations of files.
post release defects are identified by searching for version control commit messages that contain keywords like bug fix detect or patch .
metrics on review characteristics and human factors such as the number of authors that touch a component are calculated for the six month period prior to a product release rather than individual pull requests.
they find that component reviews without discussions and reviews with lower reviewer participation rates i.e.
self approved reviews tend to have higher post release defect counts.
a replication of the the above mcintosh et al.
study on the google chrome project is discussed in another work .
this study again associates post release defects with files at the directory level rather than individual patches themselves.
post release defects are identified by scraping the chrome issue tracker searching for issues submitted in the time period after the current release.
they find that review participation measures have an inconsistent explanatory power across projects and releases leading them to model the problem with a bayesian network based approach .
the bayesian network methodology showed that review participation measures have an indirect effect on post release defects with prior defects component size and the number of inexperienced authors contributing the most to the incident of post release defects.
m ntyl and lassenius studied whether code review finds issues related to the functional correctness of code or evolvability issues such as the structure or documentation of source code .
they give a taxonomy of software defects and find that .
of the findings in industrial code reviews are related to evolvability defects with .
of the findings corresponding to functional defects and .
being false positives.
it should be noted that the code review methodology used in this study was a synchronous in person and recorded meeting in which multiple reviewers commented on code changes in real time.
in the industrial setting nine review sessions were observed in total.
the author of the study observed code review sessions and manually identified defects as being related to either functional or evolvability issues.
thus defects in this study are attributed to what reviewers identify before the changes are shipped.
it should also be noted that this review methodology is starkly different from the meta s asynchronous code review practices.
mcintosh et al.
have further expanded on their previous study on the relationship between code review and software quality this time specifically trying to determine whether code reviews inhibit the occurrence of anti patterns in code .
as in the previous mcintosh et al.
study anti pattern defects are attributed to components or directory level file sets.
seven anti pattern types aredetected using automated tooling.
they find that code review coverage and participation reduce this incident of anti patterns in software components.
there is a work that makes the strongest claims as to the value of code review in relation to finding software defects.
the authors state that only of the code reviewer comments indicate a possible defect.
the usefulness of the review comment as judged by the author is correlated with the experience of the reviewer with more experienced reviewers giving more useful feedback.
they find that review usefulness decreases with the size of the changed file set with a noticeable drop off occurring around changed files.
review participation in modern code review examines what factors lead to faster reviewer engagement rather than what aspects of reviewer engagement impact software quality.
it is worth mentioning because its results align with those of the above studies around the relationship between reviewer characteristics and code reviews.
in this study not all pull requests that are shipped receive code review and thus there is a question as to why some pull requests receive review and others do not.
this study finds that the greater the number of prior pull requests reviewed by an assigned reviewer the more likely the pull request will actually receive reviews.
this aligns with previous studies which find that reviewer characteristics such as the amount of previous contributions to the code components the amount of code reviews performed in the past and social factors such as a reviewer s personality or standing in the team impact the quality of code review and the resulting software.
to summarize the prior literature in this space indicates that it is an unanswered question whether review characteristics impact software quality.
existing studies find that other characteristics such as those relating to the reviewer or the code quality itself can have more explanatory power on whether post release software defects occur so much so that these other characteristics are directly controlled for in many of these studies .
to put it plainly it is still unproven that code review reliably catches bugs.
if the vast majority of code review is truly concerned with evolvability and stylistic issues as suggested by the above studies an argument could be constructed that it would be wiser to invest more in automated code quality enforcement tooling rather than better code review practices.
before we get too carried away with this it is important to make some very clear distinctions between the code review practices examined in this prior literature and those employed at meta as well as the underlying questions being investigated.
we do argue and our study suggests that including a highquality test plan to a pull request will lead to better code quality overall.
in this study we are concerned with the quality of the test plan in a pull request.
our question is what characteristics of a given pull request if any reliably predict whether it is likely to be a blame pull request .
to refresh the reader s mind a blame pull request is one that is implicated in an outage or otherwise reverted for any other reason.
conclusions and future work as part of the effort to measure and improve pull request quality in this industrial experience paper we describe our efforts in devising a classifier for test plans.
we started from the hypothesis 1328esec fse november singapore singapore lawrence chen rui abreu tobi akomolede peter c. rigby satish chandra and nachiappan nagappan that increased test plan quality could help improve review quality and engagement.
our goal is to demonstrate that data driven approaches could be leveraged as a meaningful heuristic.
note that we are not attempting to set a new benchmark with high evaluation accuracy instead we are showing that out of the box language models make for good heuristics that align with both human subjective opinions and objective metrics.
we frame the problem of quantifying quality of a test plan as a binary classification model.
leveraging the success of nlp based techniques in other domains we propose an approach based on a pre trained roberta language model which takes in the test plan as a natural language input.
the model was fine tuned using opinions of about 500developers on over test plans.
our empirical evaluation suggests that the proposed approach is able to capture developer sentiment and reflect a correlation of test plan quality with review quality engagement outages and reversions.
compared to a decision tree model the baseline model our proposed transformer based model achieves a higher f1 score i.e.
the proposed approach is more accurate than the baseline and produced statistically significant correlations with the objective metrics.
we have also presented a case study of how such a metric may be useful in experiments to inform improvements in developer tools and experiences.
we have discussed potential downstream applications in a previous section paving the way to several interesting ideas for future work.
yet another potential area of exploration from the point of view of model development is to inform the model with source code related features because test plan quality may sometimes be dependent on the code changes that are being tested.
due to the limited dataset and the sparse nature of source code data we leave this as future work.
moreover we have shown that roberta based approaches have the advantage of being more flexible and generalizable while achieving better performance than a rule based decision tree.
however despite the fact that the matching network architecture provides a lightweight notion of interpretability to some extent through the support set examples the challenge of incorporating a fine grain notion of model explanation remains as future work.