mate!
a re you real ly aware ?
an explainability guided testing framework for robustness of malware detectors ruoxi sun csiro s data61 australiaminhui xue csiro s data61 cybersecurity crc australiagareth tyson hong kong university of science and technology gz china tian dong shanghai jiao tong university chinashaofeng li peng cheng laboratory chinashuo wang csiro s data61 cybersecurity crc australia haojin zhu shanghai jiao tong university chinaseyit camtepe csiro s data61 cybersecurity crc australiasurya nepal csiro s data61 cybersecurity crc australia abstract numerous open source and commercial malware detectors are available.
however their efficacy is threatened by new adversarial attacks whereby malware attempts to evade detection e.g.
by performing feature space manipulation.
in this work we propose an explainability guided and model agnostic testing framework for robustness of malware detectors when confronted with adversarial attacks.
the framework introduces the concept of accrued malicious magnitude amm to identify which malware features could be manipulated to maximize the likelihood of evading detection.
we then use this framework to test several state of the art malware detectors ability to detect manipulated malware.
we find that i commercial antivirus engines are vulnerable to amm guided test cases ii the ability of a manipulated malware generated using one detector to evade detection by another detector i.e.
transferability depends on the overlap of features with large amm values between the different detectors and iii amm values effectively measure the fragility of features i.e.
capability of feature space manipulation to flip the prediction results and explain the robustness of malware detectors facing evasion attacks.
our findings shed light on the limitations of current malware detectors as well as how they can be improved.
ccs concepts security and privacy software and application security .
keywords malware detectors explainability robustness permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse december san francisco ca usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
reference format ruoxi sun minhui xue gareth tyson tian dong shaofeng li shuo wang haojin zhu seyit camtepe and surya nepal.
.
mate!are you rea lly aware ?
an explainability guided testing framework for robustness of malware detectors.
in proceedings of the 31st acm joint european software engineering conference and symposium on the foundations of software engineering esec fse december san francisco ca usa.
acm new york ny usa pages.
introduction the anti malware market is at the forefront of cybersecurity innovation constantly driving anti malware vendors to update their solutions to protect users against a wide range of malicious software variants.
the global antivirus software market is expected to reach more than billion usd in .
despite this recent research has shown that the total number of malware infections has continued to rise hitting a new high during the covid pandemic .
there are also numerous high profile cases of zero day attacks are being used in offensive campaigns.
for example just a few hours before the russian ukraine conflict microsoft s threat intelligence center identified a never before seen malware foxblade that targeted ukraine s government ministries and financial institutions .
this trend suggests that traditional signature based and behaviorbased methods cannot keep up with the rampant growth of novel malware.
hence commercial antivirus companies have started using machine learning to enable detection without the need for signatures.
however it has been shown that attackers can evade machine learning based detectors by manipulating the features that such malware detectors use .
because of this commercial antivirus systems are susceptible to adversarial attacks .
although there have been numerous works looking at adversarial attacks in computer vision where adversaries change specific pixels adversarial attacks on malware are far less understood.
we therefore leverage this adversarial approach to build a testing framework to evaluate malware detectors.
however we find that it is difficult to fully identify the root causes that impact the decisions of malware detectors.
we therefore reduce this causalarxiv .10085v4 nov 2023esec fse december san francisco ca usa r. sun m. xue g. tyson t. dong s. li s. wang h. zhu s. camtepe and s. nepal discovery problem to identifying the explainable factors that are measurable to impact the robustness of malware detectors.
to implement the testing framework for malware detectors it is necessary to build techniques that can i generate adversarial malware variants and ii measure the explainable features that drive the malware detector s ultimate decision benign or malicious .
one existing approach for generating adversarial malware test samples isobfuscation .
this focuses on changing the semantic meanings of code snippets in the problem space i.e.
source code and further obfuscating the malicious signatures or patterns including hiding the control flow inserting dummy code and manipulating variable names thereby fooling rule based malware detectors.
however defenses against obfuscation are well researched.
for example recent research looks into malware behavior distillation or program behavioral variability analysis towards training robust malware classifiers.
furthermore pierazzi et al.
argue that the use of mass obfuscation may be counterproductive rendering antivirus companies to be on the alert .
in this research we aim to test the robustness of malware detectors against a new type of adversarial generation feature space manipulation .
this involves manipulating the malware to trigger changes in the feature space used by detectors.
such threats are becoming more prominent because machine learning based detectors have reduced the efficacy of problem space attacks.
feature space manipulation aims to introduce the intended changes to the feature space i.e.
extracted feature vectors by precisely modifying the problem space i.e.
code .
these manipulating actions could be for example adding a redundant section e.g.
adding a new code section without linking its address in the section table or injecting dead code that is unreachable e.g.
adding a file i o request under an always false condition so that the dummy code will never be executed .
similar techniques have been implemented by demetrio et al.
in a black box optimization of adversarial windows malware.
although they still focus on problem space it is possible to apply such techniques in a feature space manipulation.
due to its urgency we argue that building a comprehensive testing framework that can automate the identification of limitations in malware detectors is vital.
several challenges need to be solved during the establishment of a testing framework for robustness of malware detectors.
the first challenge is how we can mimic a random attacker in the real world who may have limited capacity and knowledge.
this means that we are limited to off the shelf and easy to obtain techniques in the testing.
the second challenge is since most commercial malware detectors are not open source this must be done in a detector agnostic manner.
the third challenge is how to interpretively understand the testing results e.g.
why test cases work across different detectors .
with the above challenges in mind to ensure the reproducibility and coverage of our framework we have several criteria on the development of our testing framework i easy to obtain we only utilize open source and off the shelf tools or techniques ii modelagnostic we decouple the testing strategy from the specifics of the detector and iii explainable an explainable approach is proposed which will help us to explain the root cause of test cases transferability and identify potential weaknesses in malware detectors.
in this paper we first propose accrued malicious magnitude amm to guide feature space manipulation finding the correctaction s on the problem space that will influence the feature values but without changing run time functionality.
we then evaluate the robustness of state of the art malware detectors with generated test cases.
to establish such a strategy we generate test cases by perturbing the feature space and converting the manipulations back into the problem space.
to achieve our goal the problem is split into two sub problems i generating test cases and ii testing malware detectors.
our research helps security researchers as well as vendors who develop anti malware solutions to better understand the robustness of malware detectors and provide insights into how to improve malware defense strategies in an interpretive manner.
the main contributions of this paper are four fold we propose an explainability guided and model agnostic testing framework for malware detectors .
our framework generates test cases while preserving the malicious functions of the malware.
we introduce the concept of accrued malicious magnitude amm to guide the feature selection approach for feature space manipulation.
we further project the manipulated features back into the problem space with a binary builder.
we use amm to test the robustness of state of the art malware detectors .
we show that commercial antivirus engines are vulnerable to amm based test cases .
.
experimental results indicate that feature space manipulated test cases have significant evasion capability which decreases the detection rates of state of the art android malware detectors by .
on average and bypasses an average of .
.
antivirus engines in virustotal .
we also highlight the generalizability of our amm approach by applying it to winpe malware detectors .
.
we explain how manipulations trained on one detector can work on another detector i.e.
transferability through our explainability guided approach .
indicating that this transferability relies on the overlap features that have large amm values between different machine learning models.
we further investigate an approach to improve machine learningbased detectors through excluding high sensitive but less important features during training .
.
results show that amm values can effectively measure the capability of features of flipping classification results.
we suggest that machine learning based anti virus products should consider using the amm values to improve their robustness.
to the best of our knowledge this is the first paper to systematically test the robustness of malware detectors in a way that combines feature space manipulations with semantic explainability.
preliminaries in this section we introduce a motivating example and related research.
.
motivating example to motivate the need for our testing framework we analyze the source code from an example android malware which is tagged as malicious by detectors from virustotal vt .
from the source we find a snippet of malicious code shown in the top part of figure .
as shown in lines and the malware executes a native scripts via root permissions by su c .
script1 command.mate!
a re you real ly aware ?
an explainability guided testing framework for robustness of malware detectors esec fse december san francisco ca usa dummy function calls addedoriginal malicious code snippet1 public void gogogo ... try long time system.currenttimemillis if time new mediaplayer .reset string cmd2 su c .
script1 if time settings.system.putint null runtime.getruntime .exec cmd2 if time this .getsystemservice connectivity service .getnetworkinfo ... catch exception e1 e1.printstacktrace ... public void gogogo ... try string cmd2 su c .
script1 runtime.getruntime .exec cmd2 ... catch exception e1 e1.printstacktrace ... figure an intuitive example of injecting dummy benign function calls into malicious malware which bypasses antivirus engines in virustotal and evades drebin.
static features are commonly used in malware detectors where the results is determined through pattern recognition weighted algorithms or signature matching.
thus it is possible to mislead detectors by introducing more benign oriented elements into malware.
hence we insert several benign function calls with always false condition closure e.g.
time to ensure they are unreachable during run time preserving the original malicious functionality.
after rebuilding the source code the modified binary is identified by scanners fewer than originally and it bypasses the machine learning detector provided by drebin .
the remainder of this paper develops an explainability guided testing framework and problem space rebuilding tool that can automate this intuitive idea for the testing of detector robustness.
.
related work malware detectors.
many modern antivirus engines utilize rulebased analysis such as signature matching static unpacking heuristics matching and emulation techniques .
however rulebased antivirus engines rely heavily on expert knowledge.
with the advantage of feature extraction derived from machine learning techniques there has been a flurry of work that integrates machine learning models into malware detectors .
we focus our evaluation on detectors that use static features due to their prevalence in providing pre execution detection and prevention for many commercial endpoint protection solutions such as kaspersky avast and eset .
evaluation of malware detectors.
a few studies have explored the effect of obfuscations on anti malware products utilizing off the shelf tools.
hammad et al.
conducted a largescale empirical study that evaluates the effectiveness of the top anti malware products including open source academic and commercial obfuscation tools.
several studies have evaluated machine learning based malware classifier models with the adversarial samples generated by generative adversarial networks gans or automated poisoning attacks.
recent research proposed methods to cope with concept drift or dataset shift which may lead to performance degradation of malware detectors.
compared to our research the scope of these studies only covers either the rule based products or the machine learning based models in isolation rather than both .
adversarial samples against malware detectors.
the goal of the adversarial attacks is to generate a small perturbation for a given malware sample that results in it being misclassified.
this type of attack has been extensively explored in computer vision and previous research efforts have also investigated the applicability of such techniques to malware classification.
xu et al.
proposed a genetic programming based approach to perform a directed search for evasive variants for pdf malware.
demetrio et al.
demonstrated that genetic programming based adversarial attacks are applicable to portable executable pe malware classifiers.
two recent works also apply deep reinforcement learning to generate adversarial samples for pe malware to bypass machine learning models.
compared to our research these studies focus on proposing adversarial attacks rather than testing the robustness of malware detectors and further explaining it.
threat model problem definition in this section we define the threat model we use in testings and present our problem definition.
.
threat model our testing framework generates adversarial samples to test the robustness of malware detectors.
for this we must define our assumed threat model.
we follow the methodology by carlini et al.
and describe this threat model with the adversary s goals capabilities and knowledge.
adversary s goal.
the adversary s goal is to manipulate malware samples to evade the detection of malware detectors including both white box and black box detectors.
in the testing we only use binary detectors which determine if the software under test is benign or malicious.
thus the goal of attackers is to cause the malicious samples to be misclassified as benign.
adversary s capability and knowledge.
in this work we assume that an attacker has full knowledge of onemachine learning model that has been trained for malware detection including its architecture and training dataset.
this is reasonable as many machine learning models such as lgbm are open source.
such a white box model will be used as the source of adversarial sample generation.
with this white box model the attacker is capable of ranking the contribution of features and manipulating a malicious sample accordingly to influence its representation in the feature space.
for other black box detectors under test including machine learning classifiers and antivirus engines the adversary has no knowledge about the detectors training dataset inner structure or detection mechanism.
for instance the adversary cannot inject poisoned data into the training dataset or manipulate any code of detectors.
however they will still have some basic knowledge about machine learning based detectors including access to open source datasets which could be part of the actual trainingesec fse december san francisco ca usa r. sun m. xue g. tyson t. dong s. li s. wang h. zhu s. camtepe and s. nepal explainability guided feature selection malicious sampletest case generation feature extractionfeature space manipulation feature manipulation datasets f1 v1fn vn.
.
.
.
.
.f v2input features f1 v11f2 v12f n v1n... ...... vm1vm2 vmn.........s11s12 s1n ...... sm1sm2 smn.........shap matrix m test casesdetermining the capacity of manipulationrange of shap per feature d of samples have larger shap c determining the amount of manipulable samples accrued malicious magnitude amm d c f arg max amm most manipulable feature v arg min m most benign oriented value greedy alg.learning based detectortraining feature patch f i vif j vjf k vkproblem space builder decompiling seed malware adding unreachable api calls inject dummy itemsbinary buildermalware detector testing transferability analysis malware detector weakness generalizability analysis regression t esting to improve detectors figure the overview of our testing framework.
dataset generic or popular feature extraction methods and off the shelf machine learning detectors.
.
problem definition our goal is to test the robustness of malware detectors using the generated test cases.
consider a malware detector mapping a piece of softwarex xto a classification label l where represents benign and represents malicious .
the adversary is trying to mislead this prediction.
our key test metric is the detection rate on test cases d xm xt gen xm d xt detectionrate d nn i 1d xi t wheredis the malware detector which could be either a machine learning detector a feature extraction method plus a trained model or an antivirus engine.
xmis the original malware sample and gen is the generator of test case xt while keeping its malware functionality the same as xm.detectionrate d is defined as a detector s detection rate on ntest cases indicating the robustness of d. testing framework our testing framework consists of three key components see figure i explainability guided feature selection to select the features for manipulation ii a test case generator that relies on the previously selected features and iii malware detector testing to identify which detectors are robust against the adversarial malware samples.
before diving into the testing framework we would like to introduce preliminary knowledge about shap the model explanation technique we used in our testing.
.
a primer on shap research into explainable machine learning has proposed multiple systems to interpret the predictions of complex models.
we rely on shap based on the coalitional game theory concept of shapley values as prerequisite knowledge to bootstrap the testing framework.
the shap framework subsumes several earliermodel explanation techniques together including lime and integrated gradients .
shap has the objective of explaining the final value of a prediction by attributing a value to each feature based on its contribution to the final result.
to accomplish this task the shap frameworks train a surrogate linear explanation model g of the form f x g x g x 0 m j 1 jx j wherefis the original model xis the input sample to be attributed x is the coalition vector of x.
0 ex f x is the average prediction of the original model on sampled dataset x. the shapley value j ris the feature attribution for the jthfeaturex jto the model s decision.
summing the effects of all feature attributions approximates the difference of prediction for xand the average of the original model aiming to explain any machine learning based model without internal knowledge.
lime uses a linear explanation model g x to locally approximate the original model where locality is measured in the simplified binary input space i.e.
x m. to find lime minimizes the following objective function arg min g gl f g x g wherelis the squared loss over a set of samples in the simplified input space weighted by the kernel function x and penalizes the complexity of g gwheregis hypothesis space.
therefore based on the input feature vectors and the output predictions of the model in this research we use shap to approximate the fragility of each feature selecting features that can be manipulated to flip the prediction.
.
step feature selection in the first step of our methodology we utilize shap to create explainability guided adversarial test cases.
in this step we use a single detector model to generate shap values for the input dataset mate!
a re you real ly aware ?
an explainability guided testing framework for robustness of malware detectors esec fse december san francisco ca usa table feature extraction methods used in different android malware detectors and in our measurement.
featuresdrebin mamadroid revealdroid droidspan our measurement permissions hardware components app components api calls strings e.g.
network addresses call graphs native call the feature is involved the feature is indirectly involved.
calculating how much one feature contributes to an individual prediction.
it is assumed that an adversary would have access to this single model.
the workflow of the explainability guided feature selection is illustrated in algorithm .
for a set of seed malware xs we generate a corresponding test case set xt.
pre processing.
we extract features from the training samples xof a trained machine learning model m line .
here we will use a generic feature extraction approach that is adopted from open sourced detectors to mimic the adversary s capability and knowledge.
using android as an example table summarizes the feature extraction methods of several state of the art for android malware detection.
we adopt features that are representative and easy to extract.
it is possible to generate stronger evasive adversarial test cases with more features involved.
however our goal is to evaluate malware detectors with an easy to obtain approach which should provide a lower bound of robustness.
note we use a similar strategy to determine the feature extraction methods for other operating systems as described in .
.
then the vectorized samples x and the model are input to shap to calculate the shap value matrix m line .
the matrix is then used to select the most evasive features and the most benignoriented values i.e.
the most negative values that exist in the selected features as represents benign .
feature selection.
to select the feature that has largest malicious magnitude we propose the concept of accrued malicious magnitude amm .
the amm is defined as the product of the magnitude of shap values in each feature and the number of samples that have malicious oriented values i.e.
values towards the positive side as represents malicious in the corresponding feature.
by calculating amm values we select the feature that has the largest modifiable capability and has the most samples to be modified as the test cases i.e.
samples that have shap values towards the positive malicious side which have the potential to be manipulated to benign.
specifically starting from the getrange m in line we first calculate the range of shap values in each feature and store the results in a one dimension vector d.dindicates the potential magnitude we can modify on each feature i.e.
eachdi dpresents the difference between the maximum shap value and the minimum shap value of feature fi.
next for each feature we count how many samples have a shap value larger than the mean shap value of that feature the countlarge m in line and collect the results in a one dimensional vector c. therefore a larger ci cmeans that for featurefi there are more samples that have a shap value towards malicious such that more samples can be manipulated towards benign.
therefore we select the most evasive feature according to the amm values denoting the dot product of the range of shapalgorithm amm based feature space selection input machine learning model m datasetx and the number of features to be selectedn.
output feature patch p. 1p map feature value 2x vectorize x 3m shap x m 4whilesize p ndo 5d getrange m 6c countlarge m 7amm d c 8f arg max amm 9v arg min m ifismanipulatable f then 11p p f v foreachx x do ifx vthen idx getindex x x m m m x x x 17returnp values d and the number of shap values greater than mean c line .
value selection.
once we have identified the feature fto compromise the next step is to choose the value for the selected feature to guide the manipulation.
we select the most benign oriented value v in the feature space.
this corresponds to the most negative value inm the shap value of the feature f line .
updating the feature.
after obtaining f v if the selected featurefis manipulable we add the pair into a map p as the feature patch to be used in the feature space manipulation line .
note that due to the strong semantic restrictions of the binaries we cannot simply choose any arbitrary pairs of feature and values for the test manipulation.
instead we restrict the feature space manipulation to only features and values that are independent iid and can be modified with original functionalities preserved.
for example consider the feature that counts the size of a binary when we modify the value of another feature the former will be modified indirectly.
therefore the features and values we select to be manipulated follow two principles employed by the previous literature .
these principles are i features are manipulable in the original problem space and ii selected features have no dependencies or cannot be affected by other features.
we described manipulable features for android and winpe datasets later in .
and .
.
greedy strategy.
after obtaining feature value pairs we conduct a greedy strategy removing samples that have the same value v for featureffrom the dataset lines to .
we do this to make sure that the same feature value pair will not be selected again.
the procedure repeats until we find nfeature value pairs.
these npairs are then used as feature patch in the next stage to generate test cases.
.
step test case generator in the next step the test case generator conducts feature manipulation according to the feature patch obtained from prior steps.esec fse december san francisco ca usa r. sun m. xue g. tyson t. dong s. li s. wang h. zhu s. camtepe and s. nepal feature space manipulation.
equation summarizes the featurespace manipulation x m vectorize xm x t manipulatefeature x m p xt gen xm x t wherex mis the result of applying feature extraction on a malicious samplexmusing vectorize .manipulatefeature manipulates the sample in feature space guided by the selected feature and value pairs p. note that the sample generator gen will take the manipulated feature space sample x tand the original seed sample xmas input and implement the changes in feature space back to problem space to generate the test case while keeping its malware functionality as detailed in binary builder below .
binary builder.
to ensure that no loss of functionality is inadvertently introduced as a side effect of feature manipulation we only apply these changes to unreachable areas of binaries so that these changes will never be executed during run time.
therefore we guarantee that test cases are executable and can be applied in the testing of malware detectors in the wild.
then we apply these changes on seed binaries with the help of open source binary builders.
for simplicity we present our tooling for android malware yet we emphasize that our framework works with other operating systems see .
.
we adopt a similar feature extraction method of drebin on the android apk.
since features are a vector of boolean values representing the existence of a feature the feature value could only be modified from absence to presence to preserve original functionalities.
we first leverage apktool to decompile an apk file into smali code a structured assembly language.
api calls and network urls are transformed to smali instruction code which is wrapped by an unreachable disclosure e.g.
an always false condition closure such as if time .
the smali code is then inserted into the smali file of the main activity.
features representing android manifest components are inserted into androidmanifest.xml file directly.
finally we utilize apktool to assemble all decompiled and manipulated files into an adversarial apk sample.
if a feature manipulation cannot be implemented in this way we skip it and continue with the next feature in the selected patch.
.
step malware detector testing after the test cases are generated our framework programmatically executes a series of tests on the malware detectors.
this involves i calculating the per detector robustness and ii testing the transferability of attacks across detectors.
we conclude this subsection by proposing techniques that can improve detector performance.
testing detector robustness.
the methodology to test detector robustness is straightforward.
we first input the seed malware into each detector and collect the detection rate on the unaltered malware dataset.
next the test cases generated from the white box model will be input to the detectors including the white box model itself .
we then compare the difference between the detection rate of the seed vs.test cases.
this allows us to test which detectors are vulnerable to test cases i.e.
whether the manipulation on amm features changes the detector results.transferability analysis.
considering that machine learningbased detectors may use similar feature extraction methods it is possible that multiple detectors are susceptible to the same feature manipulations.
specifically we posit the test cases generated from one machine learning model are likely to be effective on other models that are trained on the same data distribution due to the similarity of decision boundaries.
therefore the test framework also calculates the ability to transfer an evasion trained on one detector to another i.e.
transferability.
this is important as the more powerful the transferability a malware test case has the more effective it is against other detectors.
to evaluate the transferability of test cases generated by the amm based approach we generate cases from white box models and apply them to black box models.
if transferability exists the test framework calculates the feature space overlaps among models to explore the root cause of transferability.
this can be used by developers to understand the weaknesses in their feature engineering.
regression testing for detector improvement.
inspired by recent research a detection model can be improved by removing important features from the training phase where the important features refer to the ones that are highly contributing to the model prediction accuracy.
shapley additive global importance sage is a framework that measures how much a feature contributes to the prediction accuracy of a model.
we apply the improvement on the detection models with sage and test their robustness against adversarial samples.
specifically we first calculate sage values of each feature.
since the most important features are the ones with largest sage values we sort the features by sage values in a descending order and select the top features.
then we remove the top features from samples and generate a new training set.
finally an improved model miis trained with the new training set.
to establish a thorough comparison we train another model mathat excludes top amm based features to compare and explain the improvement of malware detectors.
testing framework setup in this section we describe the setup of our tests including the detectors under test the datasets and how we trained the models.
.
detectors under test to showcase our testing framework we experiment with a number of malware detectors.
for an malware detectors we test the robustness of state of the art machine learning based detectors a combination of feature extraction methods and machine learning models for winpe detectors we test detectors feature extraction method accompany models .
we also test antivirus engines available using virustotal .
the detectors under test are listed in table .
specifically we involve drebin mamadroid and ember in our testing because i they proposed unique features extraction methods ii high accuracy rates are reported on large datasets and iii their source code and datasets have been made publicly available.
to follow the conventions of prior studies we select off the shelf machine learning based malware models that are commonly used in malware detectors.
in our threat model the adversary has full knowledge of one machine learning basedmate!
a re you real ly aware ?
an explainability guided testing framework for robustness of malware detectors esec fse december san francisco ca usa table detectors under test.
type name description feature extraction methodsdrebin a lightweight method for android malware detection extracting sets of features from an application s code and manifest.
mamadroid a static analysis based system that abstracts app s api calls and builds a model from the call graph of an app as markov chains.
ember a static winpe malware classifier extracting eight groups of raw features that include both parsed features and format agnostic histograms and counts of strings.
machine learning modelslgbm lightgbm an open sourced gradient boosting framework based on the decision tree algorithm.
svmsupport vector machine a supervised learning method based on statistical learning frameworks.
rfrandom forests an ensemble learning method that combines decision trees to provide classification.
dnna feed forward neural network with with one input layer and three fully connected hidden layers the last one ends with a softmax function .
antivirus enginesvt virustotal a website that aggregates more than antivirus products and online scan engines allowing a user to check for viruses that the user s own antivirus software may have missed.
model.
without loss of generality we set lgbm as this white box model.
note any machine learning model could serve this role as our approach is model agnostic.
for the antivirus engines we use virustotal an online service that provides over antivirus scanners to detect malicious files and urls.
we find that scanners are always available while the others are not stable.
therefore we include these scanners in our evaluation.
all antivirus engines fall into the black box category as the attacker has no specific knowledge about them.
.
datasets and model training in our experiments we conduct testings using an android dataset and a winpe dataset.
the android application package apk is the package file format used by the android operating system for distribution of mobile apps.
we use the well studied drebin dataset which contains benign samples and malicious.
we also include malware samples that have been uploaded on virusshare .
since the ratio of malicious and benign apps is unbalanced we randomly select benign and malicious samples making up samples.
further we create a random split of samples for training validation and testing to train a lightgbm model.
test cases are generated from randomly selected malicious samples.
the details about the winpe dataset establishing and model training are provided in .
.
to train the ml models mentioned above we employ androguard to extract raw features from apk samples.
androguard is a python tool to analyze and manipulate android files.
it disassembles an apk file and converts its byte code and resource files into a readable and structured format.
we further extract features from the manifest file and from the dalvik executable dex file.
these features are then used to train and evaluate the machine learning based detectors.
all android features are manipulable as they are independent to each other.
testing framework results we next employ our explainability guided test framework to evaluate the robustness of the detectors.
specifically we compare the detection rates between original samples and the test cases generated by our proposed strategy.
to choose the number of features to manipulate we conduct pilot experiments on android and winpe datasets.
.
pilot experiment number of features.
to choose the number of features to manipulate in the explainability guided test case generation we conduct pilot experiments on the android and winpe datasets.
here we only report the experiment on the android dataset as they follow the same strategy.
the pilot experiment compares the detection rate of the lgbm model while manipulating and features on sets of seed samples samples in total .
as shown in figure with more features manipulated the detection rate decreases.
the trend turns at features with .
test cases detected.
we therefore choose n 75for the apk binaries.
.
testing results on android malware detectors in our android malware detector testing we generate apk test cases from the lgbm model and test on machine learning based detectors using feature extraction methods and models in white box and in black box and antivirus engines black box from vt. testing machine learning detectors.
all machine learning based detectors have reasonable detection rate above on average on the original malware samples as shown in figure .
note the y axis refers to the detection rates of samples and each detector is presented on the x axis.
on the drebin based detectors the test cases averagely reduce the detection rate by .
ranging from .
on drebin rf to .
on drebin svm .
noticeably all test cases bypass the detection of drebin svm.
this may be attributable to i our feature extraction method being close to drebin and ii the svm being a simple linear model making its prediction easier to flip.
we further analyze the robustness of the svm model as a case study in .
while on mamadroid based detectors considering that mamadroid utilizes call graph features which are indirectly related to our api call features extracted from android source code it is expected that fewer test cases can bypass the detection.
specifically the detection rates of mamadroid based detectors decrease .
on average ranging from .
on mamadroid rf to .
on mamadroid svm on test cases.
the detection rates decrease down to .
avg.
which is even lower than random guess.
we conclude that all detectors involved in our android malware detector testing are vulnerable to the test cases generated by the explainability guided feature space manipulation.
note in white box scenarios i.e.
the detectors using lgbm model it is expected that the generated test cases are more effective against white box models.
to further explore the reason why our explainability guided approach also works in black box scenarios we present further test results on transferability in .
.esec fse december san francisco ca usa r. sun m. xue g. tyson t. dong s. li s. wang h. zhu s. camtepe and s. nepal detection rate number of manipulated features020406080100 figure detection rates of manipulating different numbers of features.
drebin mamadroiddetection rate lgbm svm rf dnn lgbm svm rf dnn lgbm svmdetection rate .
.
.
.
.
.
.
.
seed malware adversarial samples seed malware adversarial samples figure testing results on machine learning based detectors.
of engines that detect the malware20304050 seed malware amm based adversarial figure number of virustotal anti virus engines that can detect original malware samples and test cases.
of antivirus engines detection rate dropping 100figure histogram of antivirus engines with respect to the detection rate decreasing.
testing virustotal engines.
we next turn our attention to the antivirus engines accessible via virustotal.
figure plots the number of virus engines that successfully flag the seed malware and test cases as malicious.
the average detection rate of seed malware is .
i.e.
on average .
vt detectors detect a sample as malicious.
in contrast only an average of .
detectors flag test cases as malicious.
this lowers the detection rate of vt to .
on average.
this result indicates that not all detectors in vt are robust against these test cases.
to illustrate the performance of each antivirus engine and explain the overlapping results between samples figure presents a histogram of the detection rate reductions for each antivirus engine.
the x axis represent the range of detection rates decreasing and the y axis refers to the numbers of antivirus engines.
to obtain a meaningful result engines that have detection rates less than on seed malware samples are excluded.
according to figure engines are relatively resistant to the test cases with detection rate reductions of under .
this explains the overlapping results between the upper percentile of test cases and the mean of the seed malware samples.
in contrast there are engines with detection rates that decrease by over indicating that these antivirus engines are vulnerable to the amm based test cases.
takeaway machine learning based detectors are vulnerable to the test cases generated by the amm based strategy which manipulates the most evasive features.
test cases can evade detection by the black box antivirus engines in virustotal.
this indicates that the majority of anti malware vendors could use our testing framework to examine and improve their detectors.
rf svm lgbmlgbm svm rf0 normalized amm v alues .
.
.
.
.
.0figure amm values of top features that have the highest amm values in lgbm row svm row and rf row models separately.
.
transferability analysis transferability is the ability for a test case to be effective against multiple learning based detectors.
to study this taking drebin as an example we next generate amm based test cases from each of the detectors and input them to different detectors.
here we seek to understand how a manipulation guided by one detector performs against other detectors.1specifically we measure the transferability in two aspects feature overlaps and detection rates.
feature overlap.
to explore the reason why the test cases can transfer across detectors we present the top features that have the highest amm values in the generation models across each detector in a heatmap shown in figure .
in each subplot we present the features as rows by columns of dots normalized to where the darker dots represent higher values of amm which indicates a greater possibility to be selected as a feature to be manipulated .
further we sort the features in the generation models according to the amm values in descending order.
therefore more darker dots scattered in the upper zone of the nine subplots indicate that there are more features having been selected across detectors.
from the heatmap we can observe that i features with large amm values in lgbm dark dots overlap with most of the counterparts of svm and ii many features with large amm values in rf are out of the scope of the counterparts of lgbm.
we further explain such overlaps with the result of detection rates across different models.
1as the calculation of shap matrix on dnn model requires large amount of computing time using our facility we conduct transferability analysis on generation models and we believe such experimental setting is sufficient.mate!
a re you real ly aware ?
an explainability guided testing framework for robustness of malware detectors esec fse december san francisco ca usa generation models machine learning detectorslgbm svm rflgbm svm rf detection rate .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
figure detection rates of three drebin based detectors on test cases generated from lgbm svm and rf.
ember lgbm svm rf dnndetection rate .
.
.
.
.
.
.
.
seed malware adversarial samplesfigure detection rates of four detectors on winpe seed malware and amm based test cases.
seed malware amm based of important features excluded of important features excludeddetection rateimproved model 300figure detection rates of seed malware and amm based test cases detected by drebin based detectors excluding different amount of important features.
detection rate.
we further evaluate the transferability of test cases by inspecting the detection rates.
figure shows the detection rates of test cases across machine learning detectors.
the y axis represents three generation models and the x axis shows the target drebin based detectors.
a darker color indicates a higher detection rate representing lower transferability .
we use the cosine similarity numbers in brackets of amm values of the top features between models to quantify the overlaps i.e.
a higher cosine similarity value indicates a heavier overlap of high amm features between models.
from the results in figure we observe that a higher similarity value reflects stronger transferability for example the transferability from lgbm to svm outperforms the transferability from lgbm to rf the overlaps resonate with the detection rate results.
thus the overlaps explain why the test cases transfers across learning based detectors.
simply put if we manipulate enough features across different learningbased models i.e.
feature overlaps the evasion can be transferred.
takeaway .
the transferability of test cases depends on the overlaps of features with large accrued malicious magnitude amm values between different learning based detectors.
.
generalizability analysis to examine whether our testing framework can generalize to other operating systems we next conduct testing on windows portable executable winpe files.
we use sorel 20m as the winpe dataset in our experiment.
sorel 20m is a representative public dataset of malicious and benign winpe samples used for malware classification consisting of dimensional feature vectors extracted from benign and malicious samples as well as corresponding malicious binaries.
it leverages the feature extraction function from ember .
we randomly choose benign and malicious samples to train detectors with lgbm svm rf and dnn models.
feature manipulation.
in a winpe file many of the features are derived from the same underlying structures potentially leading to contradictory that cannot be manipulated concurrently.
for instance each addition of a writable section the overall section count inevitably rises.
previous work shows that only features can be modified directly and indirectly to preserve the functionality of winpe binaries.
the pilot experiment on winpe dataset suggests that we should choose nas .
we leverage lief to detection rate prediction a original detector b m i c m a s m s a s is m s a s i s m s a s is m s a s i s m s a s is m s a s ifigure detection rates and prediction values of the original model and two improved models.
extract features and pefile to apply feature manipulation on the winpe binaries.
testing results.
figure shows the detection rates of emberbased detectors on original malware samples and test cases.
the test cases have remarkable evasion performance on the lgbm svm and rf detectors on average the detection rates are decreased to .
across the three detectors.
however since most winpe features correlate with each other the method of parsing and generating winpe binaries i.e.
directly modifying values and adding empty sections may negatively affect the evasion illustrated by the dnn.
in a nutshell the test result shows that our proposed explainability guided testing framework is also effective at identifying limitations in windows malware detectors.
.
revisiting amm with improved detectors our evaluation shows that machine learning based detectors are vulnerable to test cases generated by amm.
therefore we seek an interpretive approach to improve the robustness of existing detectors.
improving detectors.
we first follow the methodology in .
and generate an improved drebin lgbm detector miby excluding sage based important features in the training phase.
specifically we first generate new machine learning detectors each with a different number of features excluded from the training set.
the trend of detection rates on seed malware and amm based test cases are shown in figure .
from the result we find that excluding important features allows the improved detector to attain the highest detection rate .
on test cases.
we therefore employ excluding important features in the following experiment.
we also generate another improved detector maby excluding fragile features.
these are features that can be manipulated to flip the prediction i.e.
features with high amm values but low sage values taking .2esec fse december san francisco ca usa r. sun m. xue g. tyson t. dong s. li s. wang h. zhu s. camtepe and s. nepal figure distributions of evaded test cases.
as a threshold the minimal amm value of features we selected in the pilot experiment is .
.
measuring improved detectors.
we further generate two groups of test cases si by sage and sa by amm .
meanwhile we introduce the seed samples smand the original drebin lgbm detector as a benchmark.
in the experiment we leverage seed malware to generate test cases where we randomly select samples for round tests on each detector.
figure presents the detection rates and prediction values.
we present results for three malware datasets seed malware set sm test case sets saandsi.
we define a sample as malicious when the prediction value is greater than or equals to .
.
on the original detector less than of samples in bothsaandsiare classified as malicious indicating a poor robustness against test cases.
on mi the average detection rate of the sasamples increase to .
while their prediction values range from .
to .
in contrast the detection rates of sionmaare around .
while their prediction values range from .
to .
which means that macan detect more test cases than mi.
note the detection rate of maon seed malware has been improved to .
compared with the .
detection rate of mi.
from the experiment result a trade off to apply amm on the improvement of anti malware detector is presented compared with the original detector .
detection rate degradation on seed malware vs. more than increasing on the detection rate against test cases.
this result further indicates that models with important features removed mi is inefficient against test cases with amm based features manipulated sa .
meanwhile improved detectors guided by amm values ma have better robustness on seed malware and adversarial detection.
comparison of amm and sage.
next we compare how fragile and important features impact the detection.
figure illustrates the distribution of top amm and sage featues features are selected both by amm and sage .
the x axis indicates normalized amm values the y axis is normalized sage values the size of the scatter point indicates the number of evaded samples manipulating the corresponding features.
as shown in the figure samples that manipulate important features alone account for only a small portion of the evaded samples while most evaded samples are generated by manipulating features with large amm values pink vs. blue which indicates that amm is more efficient than sage to generate test cases.
in addition this result also explains why ammbased test cases can bypass the improved model even they removed important features since these models are still vulnerable to test cases that only manipulate fragile features with large amm values.
a b figure a decision function values of samples with different numbers of features manipulated b distribution of weights of features in svm model.
takeaway machine learning based antivirus products should consider improving the detectors with our amm approach.
amm selects the fragile features and values that are efficient to flip the prediction results.
discussion in this section we conduct two case studies to understand why some test cases can or cannot evade detection and further discuss threats to validity of our testing framework.
.
a case study on svm robustness in prior experiments the drebin svm detector could not detect test cases from all three generation strategies.
here we conduct a case study to explore the detection robustness of svm against test cases.
first we generate test cases by manipulating between and features selected from drebin lgbm on one seed.
we then calculate the decision function value for each test cases which represents the distance from the test case to the decision boundary.
positive decision function values represent malicious while negative ones represent benign.
as shown in figure a test cases generated by amm based strategy invert the prediction results after manipulating features and push the generation towards benign with more features manipulated.
from the result we speculate that the features selected from drebin lgbm may occupy large weights in the drebin svm so that they can invert the prediction quickly.
to verify this we export the weights of each feature in the svm detector and compare the weight values of selected features.
figure b illustrates the weight values of the selected features where the y axis represents the weight of each feature.
we see that most weight values of features selected by the amm strategy from drebin lgbm have relatively large negative values in the svm model making the prediction decision values negative i.e.
benign which matches the result in figure .
this case study confirms that features selected from lgbm occupy large negative weights in the svm making the prediction result of the adversarial sample benign.
.
a case study on seed selection our prior results have shown that not all test cases can evade detection.
the reason could be either that the number of manipulable features in a seed is not enough to invert the prediction or that the manipulated features have a limited impact on the prediction.
tomate!
a re you real ly aware ?
an explainability guided testing framework for robustness of malware detectors esec fse december san francisco ca usa a original sample b t est case generated from sample c original sample d t est case generated from sample figure shap values of two apk samples before and after test case generation.
explore the reason we choose two seed malicious apk examples sample and sample to generate their test cases.
we further generate shap values of the original and test cases with n features selected of sample andsample to analyze the impact of the manipulated features.
the x axis in figure indicates the prediction value and the y axis is the feature id.
f x andf xa are raw scores of the original and test cases given by the drebin lgbm detector.
we use red bars to indicate positive shap values and blue bars to highlight negative shap values of each feature i.e.
jin equation .
feature ids shown in the figure are parts of manipulated features that had the greatest impact on the two samples.
the test case of sample inverts its prediction as benign and that of sample remains malicious.
figure b shows that the shap values of manipulated features change significantly towards negative blue bars thereby pushing the output f xa towards negative.
in contrast the shap values of the features of sample shown in figure d change far less.
specifically only features and are manipulated towards negative with less magnitude comparing tosample while is not towards negative at all.
this means that we cannot manipulate enough features to force the decision making towards benign for sample .
this result indicates that the manipulated features have limited impact on sample to invert the result from malicious to benign.
in practice after we increasing the number of manipulated features to sample is identified as benign.
therefore the capacity of a seed depends on how many features have malicious oriented values that we can manipulate in the sample.
however infinitely increasing the number of selected features would lead to a heavy computational load and decrease the efficiency of measurement.
.
threats to validity adaptive attacks against our testing framework.
existing technology against adversarial generation for example adversarial training and differential privacy dp could be effective against our proposed testing framework.
for adversarial training a machine learning based detector could be trained with amm generated test cases to falsely raise the detection rate on test cases.
however this may also increase the false positive rate on benign samples as the model will be forced to learn benign features as malicious leading to a degraded performance.
on the other hand dp based robust machine learning techniques cannot bypass our tests because unbounded random perturbations may break the generated samples functionality.
dynamic detection.
since we only insert static unreachable instructions into the malware sample a detector with dynamic detection will easily pass our testing.
feature space manipulation andproblem space obfuscation rely on static syntactic and structural modification.
these modifications can be used to test static machine learning based detectors and rule based antivirus engines.
however the malicious behaviors preserved in the test cases will still be exposed during run time and identified by the detectors that adopt dynamic analysis.
considering that dynamic feature detection consumes more resources to monitor this approach may be impractical on a large scale and static approaches are still widely used in practice.
in this research we focus on exposing the weaknesses of malware detectors with an explainable method.
conclusion this paper has proposed an explainability guided malware detector testing framework.
the framework performs test case generation relying on accrued malicious magnitude amm to guide the feature selection and a binary builder to map feature space manipulations onto problem space binaries.
we then use our framework to test the robustness of state of the art malware detectors.
our research includes the following key findings i commercial antivirus engines and state of the art machine learning detectors are vulnerable to amm based test cases ii the transferability of amm test cases relies on the overlaps of features with large amm values between different machine learning models and iii amm values can effectively measure the fragility of features and explain the capability of flipping classification results.
according to our findings we suggest that machine learning based av products should consider using the amm values to improve their robustness.
exploring the latter constitutes our key line of future work as we believe this could prompt a new approach to defending against malware evasion attacks.
data availability the source code excluding the test case generator of this project is publicly available at considering the potential security issues we will not release the test case generator and any test cases as well as the information of commercial antivirus involved in our evaluation except for academic uses that are approved by our institutional ethics committee.