towards training reproducible deep learning models boyuan chen centre for software excellence huawei canada kingston canada boyuan.chen1 huawei.commingzhi wen huawei technologies shenzhen china wenmingzhi huawei.comyong shi huawei technologies shenzhen china young.shi huawei.com dayi lin centre for software excellence huawei canada kingston canada dayi.lin huawei.comgopi krishnan rajbahadur centre for software excellence huawei canada kingston canada gopi.krishnan.rajbahadur1 huawei.comzhen ming jack jiang york university toronto canada zmjiang eecs.yorku.ca abstract reproducibilityisanincreasingconcerninartificialintelligence ai particularlyintheareaofdeeplearning dl .beingableto reproducedlmodelsiscrucialforai basedsystems asitisclosely tied to various tasks like training testing debugging and auditing.
however dlmodels are challengingto be reproduced dueto issues like randomness in the software e.g.
dl algorithms and non determinism in the hardware e.g.
gpu .
there are various practices to mitigate some of the aforementioned issues.
however manyofthemareeithertoointrusiveorcanonlyworkforaspecificusagecontext.inthispaper weproposeasystematicapproach totrainingreproducibledlmodels.ourapproachincludesthree mainparts asetofgeneralcriteriatothoroughlyevaluatethe reproducibilityofdlmodelsfortwodifferentdomains aunified framework which leverages a record and replay technique tomitigatesoftware relatedrandomnessandaprofile and patch technique to control hardware related non determinism and a reproducibilityguidelinewhichexplainstherationalesandthemit igationstrategiesonconductingareproducibletrainingprocessfor dl models.
case study results show our approach can successfully reproduce six open source and one commercial dl models.
ccs concepts softwareanditsengineering empiricalsoftwarevalidation.
keywords artificial intelligence deep learning software engineering reproducibility acm reference format boyuanchen mingzhiwen yongshi dayilin gopikrishnanrajbahadur and zhen ming jack jiang.
.
towards training reproducible deep permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa association for computing machinery.
acm isbn ... .
44thinternationalconferenceonsoftwareengineering icse may pittsburgh pa usa.
acm new york ny usa pages.
introduction in recent years artificial intelligence ai has been advancing rapidly both in research and practice.
a recent report by mckinsey estimates that ai based applications have the potential market valuesrangingfrom .5and .8trillionannually .manyofthese applications whichcanperformcomplextaskssuchasautonomous driving speechrecognition andhealthcare areenabledbyvariousdeeplearning dl models .unliketraditional softwaresystems whichareprogrammedbasedondeterministic rules e.g.
if else the dl models within ai based systems are constructedinastochasticwayduetotheunderlyingdlalgorithms whose behavior may not be reproducible and trustworthy .
ensuringthereproducibilityofdlmodelsisvitalfornotonlymany productdevelopmentrelatedtaskssuchastraining testing debugging and legal compliance but also facilitating scientific movements like open science .
oneoftheimportantstepstowardsreproducibleai basedsystemsistoensurethereproducibilityofthedlmodelsduringthe training process.
a dl model is reproducible if under the same training setup e.g.
the same training code the same environment and the same training dataset the resulting trained dl modelyields the same results under the same evaluation criteria e.g.
thesameevaluationmetricsonthesametestingdataset .
unfortunately recent studies show that ai faces reproducibility crisis especiallyfordlmodels .
in general there are three main challenges associated with this randomnessinthesoftware randomnessisessentialin dlmodeltraininglikebatchordering datashuffling andweight initialization for constructing robust and high performing dl models .however randomnesspreventsthedlmodels frombeingreproduced.toachievereproducibilityinthetraining process the current approach is to set predefinedrandom seeds beforethetrainingprocess.althoughthisapproachiseffectivein controlling the randomness it has three drawbacks it might cause the training process to converge to local optimums and notabletoexploreotheroptimizationopportunities itisnontrivial to select the appropriate seeds as there are no existing ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa chen and wen et al.
techniques for tuning random seeds during the hyperparameter tuningprocess non trivialmanualeffortsareneededtolocate randomness introducing functions and instrument them with seeds for the imported libraries and their dependencies.
non determinism inthe hardware trainingdlmodels requires intensive computing resources.
for example many matrixoperationsoccurinthebackwardpropagation whichconsistsofahugeamountoffloatingpointoperations.asgpushave way more numbers of cores than cpus gpus are often used for runningdltrainingprocessesduetotheirabilitytoprocessmultiple operations in parallel.
however exe cuting floating point calculationinparallelbecomesasourceofnon determinismsince theresultsoffloating pointoperationsaresensitivetocomputation orders due to rounding errors .
in addition gpu specific libraries e.g.
cuda and cudnn by default auto select the optimal primitive operations based on compar ing different algorithms of operations during runtime i.e.
the auto tuning feature .
however the comparison results might be non deterministic dueto issueslike floatingpoint computation mentioned above .
these sources of non determinism from hardware need to be controlled in order to construct reproducible dl models.
case by case solutions have been proposed to tackle specific issues.
for example both pytorch and tensorflow provideconfigurationsondisablingtheauto tuning feature.unfortunately noneofthesetechniqueshavebeenempirically validated in literature.
furthermore there is still a lack of a general technique which can work across different software frameworks.
lack of systematic guidelines various checklists and documentationframeworks havebeenproposedon asset management to support dl reproducibility.
there are generallyfourtypesofassetstomanageinmachinelearning ml in order to achieve model reproducibility resources e.g.
dataset and environment software e.g.
source code metadata e.g.
dependencies andexecutiondata e.g.
executionresults .
however priorwork showstheseassetsshouldnot be managed with the same toolsets e.g.
git used for source code .
hence newversion managementtools e.g.
dvc andmlflow arespecificallydesignedformanagingmlassets.
however even by adopting the techniques and suggestions mentionedabove dlmodelscannotbefullyreproduced due toproblemsmentionedintheabovetwochallenges.asystematic guideline is needed for both researchers and practitioners in order to construct reproducible dl models.
toaddresstheabovechallenges inthispaper wehaveproposed a systematic approach towards training reproducible dl models.
our approach includes a set of rigorously defined evaluation criteria arecord and replay basedtechniqueformitigatingrandomnessinsoftware andaprofile and patch basedtechniqueformitigating non determinism from hardware.
we have also provided a systematicguidelinefordlmodelreproducibilitybasedonourexperience on applying our approach across different dl models.
case studies onsixpopularopensourceandonecommercialdlmodelsshow that our approach can successfully reproduce the studied dl models i.e.
thetrainedmodelsachievetheexactsameresultsunderthe evaluation criteria .
to facilitate reproducibility of our study weprovideareplicationpackage whichconsistsoftheimplementation of open source dl models our tool and the experimental results.
in summary our paper makes thefollowing contributions althoughthere arepreviousresearch workwhich aimed atreproducible dl models e.g.
to the authors knowledge our approach is the first systematic approach which can achieve reproducible dl models during the training process.
case study resultsshowthatallthestudieddlmodelscanbesuccessfully reproduced by leveraging our approach.
compared to existing practices for controlling randomness in the software a.k.a.
presetting random seeds our recordand replay based technique is non intrusive and incurs minimal disruption on the existing dl development process.
compared to the previous approach on verifying model reproducibility ourproposedevaluationcriteriahastwoadvantages it is more general as it covers multiple domains classification and regression tasks and it is more rigorous as it evaluatesmultiplecriteria whichincludesnotonlytheevaluationresultsonthetestingdataset butalsotheconsistencyofthe training process.
paper organization.
section provides background information associatedwithdlmodelreproducibility.section3describesthe details of our systematic approach to training reproducible dl models.section4presentstheevaluationofourapproach.section5 discusses the experiences and lessons learned from applying our approach.
section presents our guideline.
section describes the threats to validity of our study and section concludes our paper.
background and related work inthissection wedescribethebackgroundandtherelatedwork associated with constructing reproducible dl models.
.
the need for reproducible dl models severaltermshavebeenusedinexistingliteraturetodiscussthe concepts of reproducibility in research and practice .
we follow the similar definitions used in whereaparticularpieceofworkisconsideredas reproducible ifthe samedata samecode andsameanalysisleadtothesameresults or conclusions.
on the other hand replicable research refers to that different data from the same distribution of the original data combinedwith samecodeand analysis resultinsimilar results.in thispaper wefocusonthereproducibilityofdlmodelsduringthetrainingprocess.thesametrainingprocessrequirestheexactsame setup which includes the same source code including training scripts and configurations the same training and testing data and the same environment.
training reproducible dl models is essential in both research andpractice.ononehand itfacilitatestheopensciencemovement by enabling researchers to easily reproduce the same results.
open sciencemovement promotessharing researchassetsina transparent way so that the quality of research manuscripts canbe checked and improved.
on the other hand many companies arealsointegratingthecutting edgedlresearchintotheirproducts.
having reproducible dl models would greatly benefit the product development process.
for example if a dl model is reproducible the testing and debugging processes would be much easier authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
towards training reproducible deep learning models icse may pittsburgh pa usa astheproblematicbehaviorcanbeconsistentlyreproduced .in addition manydl based applicationsnowrequire regulatorycomplianceandaresubjecttorigorousauditingprocesses .itisvital thatthebehaviorofthedlmodelsconstructedduringtheauditing process closely matches with that of the released version .
.
current state of reproducible dl models .
.
reproducibilitycrisis.
in2018 huston mentioneditis very difficult to verify many claims published in research papers due to the lack of code and the sensitivity of training conditions a.k.a.
the reproducibility crisis in ai .
similarly gundersen and kjensmo surveyed research papers from ijcai and aaai and found that only of papers provided experiment code.
similarly insoftwareengineeringresearch liuetal.
surveyed se research papers which leveraged dl techniques and only .
ofresearchdiscussedreproducibilityrelatedissues.isdahlandgundersen surveyed state of the art ml platforms and found thepopularmlplatformsprovidedbywell knowncompanieshavepoorsupportforreproducibility especiallyintermsofdata.instead of verifying and reporting the reproducibility of different research work we focus on proposing a new approach which can construct reproducible dl models.
.
.
effortstowardsimprovingreproducibility.
variouseffortshave been devoted to improve the reproducibility of dl models e1 controlling randomness from software.
liu et al.
found that the randomness in software could impact the repro ducibility of dl models and only a few studies e.g.
reportedusingpresetseedstocontroltherandomness.similarly pham et al.
found that by controlling randomness in software the performance variances in trained dl models decrease signif icantly.
sugimura and hartl mentioned that a random seed needs to be set as a hyperparameter prior to training for reproducibility.determined.ai acompanythatfocusesonproviding servicesfordlmodeltraining alsosupportssettingseedsforreproducing dl experiments.
however none of the prior studies discussed how to properly set seeds or the performance impact of different set of seeds.
compared to presetting random seeds our record and replay basedtechniquetocontroltherandomnessin the software is non intrusive and incurs minimal disruption on the existing dl development.
e2 mitigating non determinism in the hardware.
pham et al.
discussed using environment variables to mitigate nondeterminismcausedbyfloatingpointroundingerrorandparallelcomputation.jooybaretal designedanewgpuarchitecture for deterministic operations.
however there has been a lack of thoroughassessmentoftheproposedsolutions.inaddition ourap proachmainlyfocusesonmitigatingnon determinismoncommon hardware instead of proposing new hardware design.
e3 existing guidelines and best practices.
to address the reproducibility crisis mentioned above major ai conferences such as neurips icml and aaai hold reproducibility workshops and advocate researchers to independently verify the results of published research papers as reproducibility challenges.
various documen tationframeworksfordlmodels orchecklists have beenproposedrecently.thesedocumentationsspecifytherequired information and artifacts e.g.
datasets code and experimentalresults that are needed to reproduce dl models.
similarly ghanta et al investigated ai reproducibility in production where they mentioned many factors need to be considered to achieve repro ducibility such as pipeline configuration and input data.
tatman etal.
indicatedthathighreproducibilityisachievedbymanaging code data and environment.
they suggest in order to reach the highest reproducibility the runtime environment should beprovided as hosting services containers or vms.
sugimura andhartl built an end to end reproducible ml pipeline which focusesondata feature model andsoftwareenvironmentprovenance.
in our study we mainly focus on model training with theassumption that the code data and environment should be con sistent across repeated training processes.
however even with consistent assets mentioned above it is still challenging to achieve reproducibility due to the lack of tool support and neglection of certain sources of non determinism .
.
industrial assessment huaweiis a large it company which provides many products and services relying on ai based components.
to ensure the quality trustworthiness transparency andtraceabilityoftheproducts prac titioners in huaweihave been investigating approaches to training reproducible dl models.
we worked closely with practitioners who are either software developers or ml scientists with ph.d degrees.
their tasks are to prototype dl models and or produc tionalizedlmodels.wefirstpresentedthecurrentresearchand practices on verifying and achieving reproducibility in dl models.
then we conducted a two hour long semi formal interview with thesepractitionerstogathertheiropinionsonwhethertheexisting work can help them address their dl model reproducibility issues in practice.
we summarized their opinions below randomness in the software practitionersareawarethatcurrently the most effective approach to control the randomness inthe software is to set seeds prior to training.
however they are reluctant to adopt such practice due to the following two reasons a variety of usage context for example in software testing they would like to reserve the randomness so that more issues can be exposed.
however after the issue is identified they find it difficult toreproducethesameissueinthenextrun.settingseedscannot meettheirneedsinthiscontext.
sub optimalperformance dl models often require fine tuning to reach the best performance.
currently thedltrainingreliesoncertainlevelsofrandomnessto avoid local optimums.
setting seeds may have negative impacts on themodelperformance.althoughtoolslikeautoml havebeen recentlywidelyadoptedforselectingtheoptimalhyperparameters therearenoexisting techniqueswhich incorporaterandomseeds as part of their tuning or searching processes.non determinisminthehardware thereareresearchandgrey literature e.g.
technical documentations blogposts describing techniques to mitigate the non determinism in hardware orproposingnewhardwarearchitecture .however inanindustrialcontext adoptingnewhardwarearchitectureisimpractical due to the additional costs and the lack of evaluation and support.
inaddition thementionedapproaches e.g.
settingenvironment variables are not extensively evaluated on the effectiveness and authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa chen and wen et al.
overhead.
hence a systematic empirical study is needed before applying such techniques in practices.
reproducibilityguidelines theyhavealreadyappliedbestpracticestomanagetheassets e.g.
codeanddata usedduringtraining processes by employing data and experiment management tools.
however theyfoundthedlmodelsarestillnotreproducible.in addition they mentioned that existing techniques in this area does not cover all of their use cases.
for example existing evaluation criteria for dl model reproducibility works for classification tasks e.g.
but not for regression tasks which are the usage contexts for many dl models within huawei.
hence they prefer a systematicguidelinewhichstandardizesmanyofthesebestpractices across various sources and usage context so that they can promote and enforce them within their organizations.
inspiredbytheabovefeedback webelieveitisworthwhileto proposeasystematicapproachtowardstrainingreproducibledl models.wewilldescribeourapproachindetailsinthenextsection.
our approach here we describe our systematic approach towards reproducing dl models.
section .
provides an overview of our approach.
section .
to .
explain each phase in detail with a running example.
.
overview there are different stages in the dl workflow .
the focus of ourpaperistrainingreproducibledlmodels.hence weassume thedatasetsandextractedfeaturesarealreadyavailableandcanbe retrieved in a consistent manner.
figure presents the overview of our approach which consists of five phases.
during the conducting initial training phase we prepare the training environment and conduct the training process twicetogeneratetwodlmodels modeltargetandmodelrepro.
duringthe verifyingmodelreproducibility phase thetwodlmodels fromthepreviousphaseareevaluatedonasetofcriteriatocheckif theyyieldthesameresults.
ifyes modeltargetisreproducibleand theprocessiscompleted.wealsowillupdatethereproducibility guideline if there are any new mitigation strategies that have been introducedduringthisprocess.ifnot wewillproceed tothenext phase.
during the profiling and diagnosing phase the system callsandfunctioncallsareprofiled.suchdataisusedtodiagnose and identify the root causes behind non reproducibility.
during theupdating phase to mitigate newly identified sources of nondeterminism the system calls that need to be intercepted by the record and replaytechniqueareupdatedandthenon deterministicoperationsduetohardwarearepatched.
duringthe record andreplayphase thesystemcalls whichintroducerandomnessduring training are first recorded and then replayed.
two dl models modeltargetandmodelrepro areupdatedwiththedlmodelsduring the recording and replaying steps respectively.
these two updated dlmodelsareverifiedagaininphase2.thisprocessisrepeated until we have a reproducible dl model.
toeaseexplanation intherestofthesection wewilldescribe our approach using lenet as our running example.
lenet is a popular open source dl model used for image classification.
thedatasetusedfortrainingandevaluationismnist which consists of a set of images for training images fortesting.eachimageisassignedalabelrepresentingthehandwritten digits from to .
.
phase conducting initial training the objective of this phase is to train two dl models under the sameexperimentalsetup.thisphasecanbefurtherbrokendown into the following three steps step1 settinguptheexperimentalenvironment.
inthisstep weset uptheexperimentalenvironment whichincludesdownloadingand configuringthefollowingexperimentalassets thedataset s the source code for the dl model and the runtime environment based on the required software dependencies and the hardware speci fications .
generally the experimental assets are recorded in documentationslikeresearchpapers reproducibilitychecklist or model cards and data sheets .
for our running example documentations are from research papers .
the code for lenet 5isadaptedfromapopularopensourcerepository and the mnist dataset is downloaded from .
we further split the dataset into three parts training validation and testing similar tothepriorwork .inparticular wesplitthe10 000imagesin testinginto7 500imagesand2 500images.the7 500imagesareused for validation in the training process and the images are used to evaluate the final model which arenot exposed to the training process.
we deploy the following runtime environment for the software dependencies we use python .
with tensorflow .14gpuversion.forthehardwarespecification weuseasuse linux enterprise server machine with a tesla p100 16gb gpu.
the gpu related libraries are cuda .
.
and cudnn .
.
.
step training the target dl model.
in this step we invoke the trainingscriptstogeneratethetargetdlmodel called modeltarget.
duringthetrainingprocess wecollectthefollowingsetofmetrics lossvalues thetrainingepochs andthetrainingtime.thissetof metrics is called processmetricstarget.
in our running example we invokethetrainingscriptsforlenet 5toconstructthedlmodel and record its metrics.
step3 verifyingassetsandretraining.
inthisstep wefirstverify whethertheexperimentalassetsareconsistentwiththeinformationprovidedinstep1.therearemanyapproachestoverifyingtheexperimental assets.
for example to verify the dataset s we check ifthesha 1checksumisconsistent.toverifythesoftwareenvironment wecheckthesoftwaredependencyversionsbyreusing the same environment e.g.
docker vm or simply checking all the installed software packages by commands like pip list .
once the assets are verified we perform the same training process asstep to generate another dl model named as modelrepro.w e alsorecordthesamesetofmetrics calledas processmetricsrepro during the training process.
the two dl models along with the recorded set of metrics will be used in the next phase for verifying model reproducibility.
in our running example we reuse the same experimental environment without modifying the source code and the datasets to ensure the asset consistency.
then we repeat the trainingprocesstocollectthesecondlenet 5modelanditsmetrics.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
2206towards training reproducible deep learning models icse may pittsburgh pa usa approach yes l unsupported reproduc oon det stic eratiom .
conducting initial training .
verifying model reproducibility .
profiling and diagnosing .
updating .
record and replay artifacts guideline model d evaluation process model.
evaluation process random profile m metri !
figu re an overview of our approach.
.
phase verifying model reproducibility the objective of this phase is to verify if the current training process is reproducible by comparing the two dl models against a set of evaluation criteria.
this phase consists of the following three steps step verifying the reproducibility of the training results.
in this step we evaluate the two dl models modeltar get and modelrepro on the same testing dataset.
depending on the tasks we use different evaluation metrics classification tasks for classification tasks we evaluate three metrics the overall accuracy per class accuracy and the predic tion results on the testing dataset.
consider the total number of instances in testing datasets is n .
the number of correctly labeled instances is ncorrecr for label i the number of instances are ntest .
the correctly labeled instances of label i is ncorr.
t .
hence the overall accuracy is calculated as overall accuracy .
for each label i the per class accuracy is calculated as ust n per class accuracy label i nmcr1 in addition we collect the te.st1 prediction results for every instance in the testing dataset.
regression tasks for regression tasks we evaluate the mean absolute error mae .
the total number of instances in the testing dataset is ntest consider for each instance the true ob served value is xi and the predicted value is yt.
mae is cal lntut iy1 xd culated as mae m ntest .
these metrics are called as evaluationmetrics ar get and evaluationmetricsrepro for these two models respectively.
in our running example we use evaluation metrics for classification tasks as lenet is used for image clas sification.
step verifying the reproducibility of the training process.
in this step we compare the collected metrics for modeltar get and modelre pro i.e.
evaluationmetrics ar get vs. evalu ationmetricsr pro and processmetricsro pro vs. processmetrics a rg by a python script.
for evaluation metrics we check if evaluationmetrics ar get and evaluationmetrics .
pro are exactly identical.
for process metrics we check if the loss values during each epoch and the number of epochs are the same.
step reporting the results.
a dl model is reproducible if both the evaluation metrics and the process metrics are identical except for the training time .
if the dl models are not reproducible we move on to the next phase.
in our running example the two dl models emit different evalua tion metrics.
the overall accuracy for the two models are .
and .
respectively.
for per class accuracy the maximum absolute differences could be as large as .
.
among the prediction results of them are inconsistent.
none of the loss values during the epochs are the same.
the total number of trainin g epochs are as it is pre configured.
this result shows that the two dl models are not reproducible.
hence we proceed to the next phase.
.
phase profiling and diagnosing the objective of this phase is to identify the rationales on why the dl models are not reproducible through analysis of the profiled results.
the output of this phase is a list of system calls that intro duce software related randonmess and a list of library calls that introduce hardware related non determinism.
this phase consists of the following four steps step profiling.
this step is further divided into two sub steps based on the type of data which is profiled step .
profiling system calls after inspecting the documentation and the source code of the dl frameworks we have found that the randonmess from software can be traced to the underlying system calls.
for example in tensorflow the random number generator is controlled by a special file e.g.
dev urandom in the linux environment.
when a random number is needed in the training the kernel will invoke a system call to query dev urandom for a sequence of random bytes.
the sequence of random bytes is then used by the random generation algorithm e.g.
the philox algorithm to generate the actual random number used in the training process.
step .
profiling library calls to mitigate the sources of non determinism in the hardware popular dl frameworks start to pro vide environment variables to enhance reproducibility.
for exam ple in ten sorflow .
and above setting the environment variable authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa chen and wen et al.
tf cudnn deterministic tobe true couldindicatethecudnn librariestodisabletheauto tuningfeatureandusethedeterministic operationsinsteadofnon deterministicones.however thereare still many functions that could introduce non determinism even after the environment variable is set.
in addition lower versions of tensorflow e.g.
.
which does not support such configuration are still widely used in practice.
to address this issue nvidia hasreleasedanopensource repository todocumenttheroot causes of the non deterministic functions and is currently working onprovidingpatchesforvariousversionsoftensorflow.notallthe operations could be made deterministic and ongoing efforts are being made .
hence to diagnose the sources of non determinism in hardware we perform function level profiling to check if any ofthe functions are deemed as non deterministic.
different from profilingthesystemcalls whichextractscallinformationatthekernel level the goal of profiling the library calls is to extract all the invoked function calls at framework level e.g.
tensorflow.shape .
inourrunningexample werepeatthetrainingprocessoflenet5withtheprofilingtools.weuse stracetoprofilethelistofsystem callsinvokedduringthetrainingprocess.
straceexposestheinteractions between processes and the system libraries and lists all the invoked system calls.
we use cprofile a c based profiling tool to gather the list of invoked functions at the framework level.
step diagnosing sources of randomness.
in this step we analyze the recorded data from straceto identify the set of system calls which can contribute to software related randomness.
we consult with the documentation of system calls and identify the list of system calls which causes randomness.
this list varies depending on the versions of the operating systems.
for example the system callgetrandom isonlyusedinlaterversionoflinuxkernel version .
and after .
prior to .
only dev urandom is used.
hence wehavetonotonlysearchforthelistofrandomnessintroducing systemcallsinthe stracedata butalsocheckingifthefunction parameterscontain dev urandom .figure2 a showsasnippet ofthesampleoutputsfrom straceinourrunningexample.each line corresponds to one system call.
for example line showsthat the program from usr bin python3 is executed with the scriptmnist lenet 5.py and the return value is .
the system call recorded at line reads from dev urandom and system call getrandom recorded at line is also invoked.
both of the two system calls introduce software related randomness.
step diagnosing sources of non determinism in hardware.
in this step wecross checkwiththenvidiadocumentation toseeif any of the library functions invoked during the training process triggersthenon determinismfunctionsattheharwarelayer.ifsuchfunctionsexist wecheckifthereisacorrespondingpatchprovided.
if no such patch exists we will document the unsupported nondeterministicoperationsandfinishthecurrentprocess.ifthepatchexists we will move on to the next phase.
figure b shows a snippet of the sample outputs of cprofile for our running example.
thefunctions softmax weights bias add areinvoked3 and 2times respectively.wefindthat bias add leveragesthecuda implementationof atomicadd whichiscommonlyusedinmatrixoperations.thebehaviorof atomicadd isnon deterministic because of the order of parallel computations is undetermined s h s h s h s h s s s s a .
the sample outputs of strace.
b .
the sample outputs of cprofile.
figure2 sampleoutputsnippetsfrom straceand cprofile.
whichcausesroundingerrorinfloatingpointcalculation .
the other function calls do not trigger non deterministic behavior.
.
ph a s e4 u p dating in this phase we update our mitigation strategies based on the diagnosisresultsfromthepreviousphase.thisphasecanbefurther broken down into two steps step1 updatingthelistofsystemcallsforrecording.
fortherandomness introducing functions we will add them into the list ofintercepted system calls for our record and replay technique so thatthereturnvaluesoftherelevantsystemcallscanbesuccessfullyrecorded describedinthenextphase .inourrunningexample we willaddtheinvocationofreading dev urandom andgetrandom into thelist of interceptedsystem callsto mitigate randomnessin the software.
step applying theright patches for non deterministiclibrarycalls.forthenon deterministicfunctionsrelatedtohardware wecheckifthereareexistingpatchesthataddresssuchproblemsandintegratethem into the training scripts.
in our running example after checkingthedocumentationfromthenvidiarepository wefound onepatch which replaces bias add callswith patch bias add.
we then integrated the patch to the source code of the training scripts by adding these two lines of code from tfdeterminism import patch andpatch .
in this way during the subsequent trainingprocessoflenet thenon deterministicfunctionswill be replaced with the deterministic alternatives.
.
ph a s e5 r ec o r d and replay as explained in section presetting random seeds is not preferred by practitioners due to various drawbacks.
there are libraries e.g.
numpy which support the recording and replaying of random statesthroughexplicitapicalls.however thismethodisalsointrusiveandwouldincuradditionalcostswedescribedbefore.more importantly mainstreamdlframeworkssuchastensorflowand pytorch do not provide such functionality.
hence we propose arecord and replaytechnique overviewshowninfigure3 toaddress these challenges.
this phase has two steps authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
towards training reproducible deep learning models icse may pittsburgh pa usa dynamic library system librarytraining processrecording replaying random profiletraining process dynamic library system libraryoutputread returnapi hook api hook figure our record and replay technique.
s t e p1 r e c o r ding.in this step we record the random values returned by system calls during the training process.
we will run the identical training process as in phase with the our record ing technique enabled.
we leverage the api hook mechanism tointercept the system calls by pointing the environment variable ld preload to our self implemented dynamic library.
it tells the dynamic loaders to look up symbols in the dynamic library de fined in ld preload first.
the functions of the dynamic library will be first loaded into the address space of the process.
our dynamiclibraryimplementsalistoffunctionswhichhavethesame symbols of the randomness introducing system calls in the system libraries.
these self implemented functions will be loaded first and invoketheactualrandomnessintroducingsystemcallstogetthe returned random bytes.
the sequences of random bytes emitted bythesystemcallsarethenrecordedintoanuser definedobject.
these objects are then serialized and written into files called the random profile.
we replace modeltargetandprocessmetricstarget with the dl model and the process metrics generated in this step.
in our running example two types of system calls are intercepted i.e.
getrandom and the read of dev urandom and the returnvaluesaresuccessfullyrecorded.theoutputtedrandomprofile is stored at a pre defined path in the local file system called urandom.conf andgetrandom.conf .
for the process related metrics wecollectthelossvaluesforeachepoch e.g.
thelossvalueof thefirstepochis1.
thetrainingtime .9seconds andthe number of training epochs .
step2 replaying.
inthisstep werepeatthesametrainingprocess asthepreviousstepwhilereplayingtherandomvaluesstoredinthe random profile by leveraging the api hook mechanism.
as shown in figure our dynamic library will search for existing random profile.ifsuchrandomprofileexists therecordedrandombytesare usedtotoreplacetherandombytesreturnedbythesystemcalls.
we also replace modelreproandprocessmetricsreprowith the dl modelandtheprocessmetricsgeneratedinthisstep.inourrunning example we compare the execution logs between our recording andreplayingstepsandverifythatthesamesetofrandomnumbers are generated in these two steps.
once this phase is completed the two updated dl models are senttophase2forverifyingtheirreproducibilityagain.thisprocessisrepeateduntilthedlmodelisshowntobereproducibleorwefindcertainsourcesofnon determinismthatcurrentlydonothaveexist ingsolutions.forexample thefunction tf.sparse.sparse den m atmulisnotedin thatnosolution hasbeenreleasedyet.the reasonsfornon reproducibilityshouldbeincludedinthedocumentations along with released dl models.table the dl models used in our case study.
models datasets of labels setup task lenet mnist allclassificationlenet lenet resnet cifar 10resnet wrn cifar g1 g10 modelx dataset x g1 g5 regression inourrunningexample intermsof model reproandmodel target evaluationmetricsreproandevaluationmetricstargetare identical i.e.
overall accuracy the per class accuracy and prediction results on the testing datasets of two dl models are identical .
except for thetrainingtime the processmetricsreproandprocessmetricstarget arealsoidentical.inconclusion weconsiderthetrainedlenet models to be reproducible.
results in this section we evaluate our approach against open source andcommercial dl models.
section .
describes our case study setup.
section .
presents the analysis of our evaluation results.
.
case study setup we have selected six commonly studied computer vision cv relateddlmodelssimilartopriorstudies .the implementations of these models are adapted from a popular open source repository used by prior studies .
table1showsthedetailsaboutthestudieddatasetsandthemodels.thestudiedmodelsarelenet lenet lenet resnet resnet andwrn .thesemodelsmainlyleveragetheconvolutionalneuralnetwork cnn astheirneuralnetworkarchitectures.
we use popular open source datasets like mnist cifar and cifar .
the models and datasets have been widely studied and evaluated in prior se research .
for models in lenet family we train for 50epochs.
for models inresnet family and wrn we train for epochs .
wealsostudymodelxusedinacommercialsystemfrom huawei.
modelx is a lstm based dl model used to forecast energy usages.
modelx is trained with the early stopping mechanism and theepochs are not deterministic.
the training process will automat ically stop when the loss values have not improved for epochs.
the maximum number of epochs in the training is set to be .
modelx uses proprietary time series data as their training and testing datasets and is deployed in systems which are used by tensof millions of customers.
due to the company policy and reviewstandards we cannot disclose the detail design of the dl model.
the implementation of other open source models is disclosed in our replication package .
forbothopensourceandcommercialmodels weperformthe training processes with different setups.
in total we have different setups listed in table first there are two general groups of setups cpu based and gpu based toassesswhetherourapproachcanaddressdifferent sources of hardware related non determinism.
for cpu based authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa chen and wen et al.
table2 theinformationofalltheexperimentsetups.r r represents record and replay.
id hardware software seed r r patch c1 cputf1.
c2 yes c3 yes c4 tf2.
c5 yes c6 yes g1 gputf1.
g2 yes g3 yes g4 yes yes g5 yes yes g6 tf2.
g7 yes g8 yes g9 yes yes g10 yes yes experiments i.e.
c1 c6 weonlytrainthemodelsofthelenet family and resnet family as the training for wrn and thecommercialprojecttakesextremelylongtime longerthan aweek andnotpracticaltouseinfield.forgpu basedexperiments i.e.
g1 g10 we conduct experiments on training all theaforementionedmodels.thecpuusedfortheexperiments is intel r xeon r gold 6278c cpu with cores and the gpu weuseistesla p100 16gb.thegpurelatedlibrariesare cuda .
.
andcudnn .
.
and cuda10.
and cudnn .
for tensorflow1.14andtensorflow2.
respectively.weusetwo sets of hardware related libraries due to compatibility issues mentioned in the official tensorflow documentation .
then within thesame hardwaresetup wealso conductexperimentsbyvaryingthesoftwareversions.foropensourcemodels we use both tensorflow .
and tensorflow .
.
we choose to carry out our experiments on these two tensorflow versions as major changes have been made from tensorflow .x to tensorflow2.xandtherearestillmanymodelswhichuseeither orbothversions.hence wewanttoverifyifourapproachcan workwithbothversions.formodelx weonlyusetensorflow .
as it currently only supports the tensorflow .x apis.
for cpu based experiments in a particular software version e.g.
tensorflow .
we have three setups c1 is to run the training process without setting seeds and without enabling the record and replaytechnique.c2istorunthetrainingwithseeds whereasc3istorunthetrainingwithrecord andreplayenabled.
forgpu basedexperimentsinaparticularsoftwareversion e.g.
tensorflow .
we have five setups g1 and g2 are similar to c1andc2.g3istoruntheexperimentswithpatchingonlyto evaluate the variance related to software randomness.
g4 and g5 are both running with patches but configured with either setting seeds or enabling record and replay respectively.
foreachsetup weruntheexperiments16timessimilartoaprior study .thetrainingdatasetissplitintobatchesandfedintothetrainingprocess thevalidationdatasetisusedforevaluatingthe lossesduringtraining andthetestingdatasetisusedforevaluatingthefinalmodels.inotherwords thetrainingandvalidationdataset are known to the trained dl models while the testing data is completelynewtothemodeltomimictherealisticfieldassessment.
we further divide the runs of dl experiments into pairs eachofwhichconsistsoftworuns.wethencomparetheevaluation metrics from each pair of runs to verify reproducibility.
for the setupswithrandomseedsconfigured wechoose8mostcommonlyusedrandomseedsforeachpair e.g.
0and42 .wecollectthe process and evaluation metrics as described in section .
.
in addition for each experiment we also collect the running timeforeachoftheaboveexperimenttoassesstheruntimeoverhead incurred by our approach.
we only focus on the experiments conductedongpu asgpu basedexperimentsareexecutedona physical machine.
cpu based experiments are conducted on a virtual machine in the cloud environment which can introduce large variances caused by the underlying cloud platform .
for example comparingthetimeofg1andg3couldrevealtheperformance impact on enabling deterministic patch for gpu.
comparing the timeofg3andg5couldrevealtheoverheadintroducedthrough record and replay technique.
to statistically compare the time differences weperformthenon parametricwilcoxonrank sumtest wsr .toassessthemagnitudeofthetimedifferencesamongdifferent setups we also calculate the effect size using cliff s delta .
finally as our approach also stores additional data e.g.
the recorded random profile during the store and replay phase we evaluatethestorageoverheadbroughtbyourapproachbycomparing the size of dl models with the size of random profiles.
.
evaluation analysis and results hereweevaluateifthestudiedmodelsarereproducibleafterapplyingourapproach.thenwestudythetimeandstorageoverhead associated with our approach.reproducibility by applying our approach .
the results show that the six open source models can be successfully reproduced by applyingourapproachwithdefaultsettings.inotherwords allthe predictions are consistent between the target model and the reproducedmodel.thedefaultrecord and replaytechniqueintercepts two types of randomness introducing system calls i.e.
the read of dev urandom andgetrandom .
the default patch is the version .
.
of tensorflow determinism released in pypi for tensorflow .
.for tensorflow2.
weneed tosetthe environmentvariable tf cudnn deterministic to true .
the results demonstrate the effectiveness of our approach on training reproducible dl models.
unfortunately modelxundersuchdefaultsetupcannotbereproduced.
while applying our approach during the profiling and diag nosingphase wefoundonelibraryfunction unsorted segment s um invokedfrommodelx whichcannotbemitigatedbythedefault patch.wecarefullyexaminedthesolutionsdescribedin and discovered an experimental patch that could resolve this issue.
we appliedtheexperimentalpatchalongwiththerecord and replay techniqueandareabletoachievereproducibilityformodelx i.e.
all the predictions are consistent.overhead .
we evaluate the overall time overhead incurred by our approachbycomparingtrainingtimebetweenthesetupwithout authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
towards training reproducible deep learning models icse may pittsburgh pa usa seed record and replay andpatchagainstthesetupwithrecordand replayandpatch a.k.a.
ourapproach .weonlycomparethe training time among open source models as modelx adopts the early stopping mechanism as described above section .
.
asshown in table training takes longer when applying our ap proach than the setups without.
this is mainly because patchedfunctions adopt deterministic operations which do not leverage operations e.g.
atomicadd thatsupportparallelcomputation.the time overhead ranges from to in our experiments.
al though our approach makes training on gpu slower compared with training on cpus training on gpu with our approach is still muchfaster e.g.
trainingwrn 10oncputakesmorethan7 days .
we further evaluate the time overhead brought by patching and record and replay alone.
we compare the setup with patching enabled against the setups without it e.g.
g1 vs. g3 .
we also compare the setupwith record and replay patching enabledwith the setup with patching only e.g.
g3 vs. g5 .
the results show that the record and replay technique does not introduce statistical significantoverhead p value .
.inotherwords patchingis the main reason that our approach introduces the time overhead.
table3 comparingthetimeandstorageoverhead.time o represents the average training time in hours for originalsetup and time r represents the average training time inhours forthesetupusingourapproach time r .thetimeis italicized if p value is .
and the effect size is large with .
rp represents for random profile.
model time o time r model size rp size lenet .
.
kb kb lenet .
.
kb kb lenet .
.
kb kb resnet .
.
.
mb kb resnet .
.
.
mb kb wrn .
.
mb kb modelx kb kb table3alsoshowstheaveragesizeoftraineddlmodelsandthe randomprofiles.theabsolutestoragesizesoftherandomprofile are very small ranging between kb to kb depending on the dlmodels.comparedtothesizeofthemodel thebiggestmodel iswrn 279mb .therandomprofileisonly0.
ofthe model in terms of the size.
when the model is less complex e.g.
lenet the additional cost becomes more prominent.
in lenet therandomprofileincurs37 additionalstorage.however thetotal storage size when combining the model and the random profile for letnet is less than kb which is acceptable under most of the use cases.
summary case study results show that our approach can successfullyreproduceallthestudieddlmodels.patching i.e.
replacenon deterministicoperationsfromhardwarewithdeterministicones incurslargetimeoverheadasthetrade offfor ensuringdeterministicbehavior.therecord and replaytechnique does not incur additional time overhead in the training process with very small additional storage sizes.
discussions in this section we conduct the variance analysis and discuss the lessons learnt when applying our approach.
.
variance analysis tomeasurethevariancesintroducedbydifferentsourcesofnondeterminism wecomparetheevaluationmetricsamongdifferent setups.suchanalysis demonstratesthevariancesbetweenourapproachwiththestate of the arttechniquestowardsreproducingdlmodels.forexample variancescausedbysoftwareareanalyzedby comparing the evaluation metrics between each pair in g3 where patchingisenabledtoeliminatehardwarenon determinism i.e.
theapproachproposedby .tomeasurethevariancescaused by hardware we compare the evaluation metrics between each pairing2org7 wheretherandomseedsarepresettoeliminate softwarerandomness i.e.
theapproachproposedby .inaddition to measuring the software variance and hardware variance whichresultfromapplyingtwostate of the arttechniques wealso showthevariancesincurredfromtheoriginalsetupwithnopresetseed record and replaynotenabled andpatchingnotenabled.theresultsofourapproach whichincurszerovariances arealsolisted in the table.
thedetailedresultsareshownintable4.weonlyincludetheresultsforthesixopensourceprojectsduetoconfidentialityreasons.
three evaluation metrics are used overall accuracy per class accuracy and the consistency of predictions.
for each type of metric we calculate the maximum differences and the standard deviations of the differences.
for example for resnet the largest variance of overall accuracyintheoriginalsetup is2.
whilethelargestvariancesintroduced by software randomness and hardware non determinism are .
and .
respectively.
for per class accuracy the largest varianceintheoriginalsetupis10.
whilethelargestvariancesintroduced by software randomness and hardware non determinism are .
and .
.
for predictions the largest number of incon sistent predictions in the original setup is while the largest numberofinconsistentpredictionscausedbysoftwarerandomness and hardware non determinism are and respectively.
in summary the variances caused by software are generally larger than those caused by hardware yet the variances caused byhardwarearenotnegligibleandneedtobecontrolledinorder totrainreproducibledlmodels.theresultsdemonstratetheimportanceandeffectivenessofapplyingourapproachfortraining reproducible dl models as our approach is the only one that does not introduce any variances.
.
generalizability in other dl frameworks other than the dl framework studied in section .
we have also applied our approach on another popular dl framework pytorch.
experiment results show that for common models such as lenet andresnet 56withpytorchversion1.
ourapproachcanworkout of the box.
in the future we also plan to experiment our approach onmoredlframeworksandmoredlmodelsacrossdifferenttasks.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa chen and wen et al.
table4 comparingvariancesbetweenourapproachandthestate of the arttechniques.softwarevariancereferstothetechnique for only controlling hardware non determinism .
hardware variance refers to the technique for only controlling software randomness .
original variance refers to the variance caused by the original setup.
our variance software variance hardware variance original variancediff sdev diff sdev diff sdev diff sdev overall acc.lenet1 .
.
.
.
lenet4 .
.
.
.
lenet5 .
.
.
.
resnet38 .
.
.
.
.
.
resnet56 .
.
.
.
.
.
wrn .
.
.
.
.
.
per class acc.lenet1 .
.
.
.
lenet4 .
.
.
.
lenet5 .
.
.
.
resnet38 .
.
.
.
.
.
resnet56 .
.
.
.
.
.
wrn .
.
.
.
.
.
predictionslenet1 .
.
lenet4 .
.
lenet5 .
.
resnet38 .
.
.
resnet56 .
.
.
wrn .
.
.
.
documentations on dl models mitchell et al.
proposed model cards to document ml models.
a typical model card includes nine sections e.g.
model details andintendeduse eachofwhichcontainsalistofrelevantinformation.
for example in the model details section it suggests that the information about training algorithms parameters fairnessconstraints or other applied approaches and features should beaccompanied with released models.
such a practice would help other researchersor practitionersto evaluate ifthe models canbe reproduced.
however the current practice would still miss certain details.
we share our experience below to demonstrate this point.
tensorflowandkerasaretwoofthemostwidelyuseddlframeworks.
keras is a set of high level apis designed for simplicity and usability for both software engineers and dl researchers while tensorflowoffersmorelowleveloperationsandismoreflexibleto design and implement complex network structures.
there are two waysofusingkerasandtensorflowindltraining.thefirstwayis to import keras and tensorflow separately by first calling import kerasand then verify if the backend of keras is tensorflow.
if yes tensorflow can be imported by import tensorflow .
this way is referred to as keras first.
the second way is to directly use the keras api within tensorflow by first importing tensorflow.
then we use another import statement from tensorflow import keras.
this way is referred to as tf first.
we conduct experiments to evaluate if the two different usage of apis have an impact on trainingreproducibledlmodels.asaresult thefollowingfindings are presented when training on cpus using keras first will lead to unreproducible results even after mitigating all the sources of nondeterminism.thisissuecanbereproducedbyusingvariouskerasversionfrom2.
.2to2.
.
.onthecontrary using tf firstwith thesamesettingwillyieldreproducibleresults.thisissuedoes not exist in training on gpus.
whiletrainingwithkerasversion2.
.0andabove weareable toreproducetheresultsbothfor keras first andtf firstusing our approach.
ho wever the dl models trained using keras first andtf firstare not consistent with each other.
bothfindingshavebeensubmittedasissuereportstotheofficial kerasdevelopmentteamwhosuggestedustousenewerversions of keras instead .
the findings highlight that not only the versions of dependencies but also how the dependent software packages are used can impact the reproducibility of dl models.
unfortunately existing dl model documentation frameworks like model cards do not specify how the software dependencies should be described.
hence we suggest ml practitioners look into theapproachadoptedfortraditionalsoftwareprojectslikesoftware bills of materials sbom for rigorously specifying software dependencies.
guideline inthissection weproposeaguidelineforresearchersandpractitioners who are interested in constructing reproducible dl models.
our guideline consists of five steps use documentation frameworks such as model cards to document the details such as model training.
consider leveraging sbom to document software dependencies.
ensure the documentation co evolves with the model development process.
use asset management tools such as dvc and mlflow tomanagetheexperimentalassetsusedduringtrainingprocess.
to mitigate the risks of introducing non determinism from authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
towards training reproducible deep learning models icse may pittsburgh pa usa assets we suggest using virtualization techniques to provide a complete runtime environment.
useanddocumenttheappropriateevaluationcriteriadependingonthedomainofthedlmodels.someofthesemetrics e.g.
evaluation metrics may be domain specific whereas other metrics e.g.
process metrics are general.
randomnessinthesoftwareandnon determinismfromhardwarearetwoofthemainchallengespreventingthereproducibility of dl models.
use record and replay technique to mitigate sources of randomness in the software when presetting seed is not preferred.
use patching to mitigate the non determinism from hardware if the overhead is acceptable.
if dl models are still not reproducible by applying our approach double check if the list of system calls which introduce randomness changes or if the deterministic operations are not currentlysupportedbythehardwarelibraries.documentthe unsupported non deterministic operations and search for alternative operations on the same operation.
threats to validity externalvalidity.
currently wefocusondltrainingusingpython along with tensorflow and keras framework under linux.
we are currently working on extending our approach to support dl models developed in other dl frameworks and additional operating systems.
in addition we have applied our approaches ontwo popular domains of dl classification and regression tasks.
we plan to investigate other tasks such as natural language processingand reinforcement learning.
gpus and cpus are common and widely adopted hardware for dl training.
hence in this paper we choose to focus on evaluating the dl training on gpus and cpus.
however dl training on other hardware such as tpu and edge devices also might encounter reproducibility issues.
we believe theideaofourapproachcanbeappliedinthesecontextsaswell.future work is welcomed to extend our approach to different platforms.
internal validity.
when measuring the variances incurred by different sources of non determinism we control the other con founding factors to ensure internal validity.
for example when measuring the overall accuracy variance caused by randomness in software weonlycomparetherunswithpatchingenabledandwith the same dependencies.
in addition in our evaluation we repeatthe model training process for at least times for each setup to observe the impact of different non deterministic factors.constructvalidity.
the implementation code for the dl models used in our case studies has been careful reviewed by previousresearchers .
our record and replay technique for controlling the software factors work when low level random functions are dynamically linked and invoked.
conclusions reproducibility is a rising concern in ai especially in dl.
priorpracticesandresearchmainlyfocusonmitigatingthesourcesofnon determinism separately without a systematic approach andthorough evaluation.
in this paper we propose a systematic approachtoreproducingdlmodelsthroughcontrollingthesoftware andhardwarenon determinism.casestudiesonsixopensource and one commercial dl models show that all the models can besuccessfullyreproducedbyleveragingourapproach.inaddition we present a guideline for training reproducible dl models and describe some of the lessons learned based on our experience of applyingourapproachinpractice.last weprovideareplication package to facilitate reproducibility of our study.