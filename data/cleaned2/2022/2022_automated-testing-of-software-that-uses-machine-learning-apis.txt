automated testing of software that uses machine learning apis chengcheng wan university of chicago cwan uchicago.edushicheng liu university of chicago shicheng2000 uchicago.edusophie xie whitney young high school sxie2 cps.eduyifan liu university of chicago liuyifan uchicago.edu henry hoffmann university of chicago hankhoffmann uchicago.edumichael maire university of chicago mmaire uchicago.edushan lu university of chicago shanlu uchicago.edu abstract anincreasingnumberofsoftwareapplicationsincorporatemachine learning ml solutions for cognitive tasks that statistically mimic human behaviors.
to test such software tremendous human effort isneededtodesignimage text audioinputsthatarerelevanttothe software and to judge whether the software is processing these inputsasmosthumanbeingsdo.evenwhenmisbehaviorisexposed it is often unclear whether the culprit is inside the cognitive ml api or the code using the api.
this paper presents keeper a new testing tool for software that uses cognitive ml apis.
keeper designs a pseudo inverse function for each ml api that reverses the corresponding cognitive task in anempiricalway e.g.
animagesearchenginepseudo reversesthe image classificationapi andincorporatesthesepseudo inverse functions into a symbolic execution engine to automatically generate relevant image text audio inputs and judge output correctness.
once misbehavior is exposed keeper attempts to change how ml apisareusedinsoftwaretoalleviatethemisbehavior.ourevaluationonavarietyofopen sourceapplicationsshowsthatkeeper greatly improves the branch coverage while identifying many previously unknown bugs.
ccs concepts software and its engineering software testing and debugging computing methodologies machine learning information systems restful web services.
keywords software testing machine learning machine learning api acm reference format chengchengwan shichengliu sophiexie yifanliu henryhoffmann michael maire and shan lu.
.
automated testing of software that usesmachinelearningapis.in 44thinternationalconferenceonsoftware engineering icse may21 pittsburgh pa usa.
acm new york ny usa pages.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa association for computing machinery.
acm isbn ... .
introduction .
motivation machine learning ml offers powerful solutions to cognitive tasks allowingcomputerstostatisticallymimichumanbehaviorsincomputer vision language and other domains.
to facilitate easy use of these ml techniques many cloud providers offer well designed well trained and easy to use cognitive ml apis .
indeed many software applications in a variety of domains are incorporating ml apis .
thus effectively testing these applications which this paper refers to as ml software has become urgent.
tobetterunderstandthistestingtask considerphoenix afirealarmapplication.asshowninthetophalfoffigure1 phoenixuses the google label detection api to perform image classification on aninput photo and then triggersan alarm if anyof the top classificationlabelsreturnedbytheapiincludesthekeyword fire .
this simple demo application turns out to be difficult to test.
first randominputsworkpoorly astheyrarelycontainfireandhence cannot exercise the critical alarm branch.
second even with carefully collected image inputs manual checking is likely neededtojudgetheexecutioncorrectness i.e.
whetheranalarm shouldbetriggered .finally evenafterafailedtestrun e.g.
the picture on the right of figure fails to trigger the alarm it is difficult to know whether the failure is due to the statistical nature oflabel detection whichhastobetolerated ortheapplication s incorrect use of the api which has to be fixed.
in fact this case belongstothelatter therightfigureactuallyhasatop 3label flame returned by label detection not checking for the flame label this application may miss fire alarms in many critical situations.
!
!
!
figure an example of using ml cloud apis .
this example has demonstrated several open challenges in testing ml software.
infinite yetsparseinputspaces.thespacesofimages texts oraudios typicalinputformsofcognitivemlapis areinfinitelylarge yet realisticinputsthatare relevanttothesoftware under test ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa c. wan s. liu s. xie y. liu h. hoffmann m. maire and s. lu are spread sparsely throughout this space.
for example only a tiny portion of real world images contain fire and are relevant to the fire alarm software.
existinginputgenerationtechniquesareineffectivehere.random input generators cannot produce realistic inputs through random pixel images or random character strings.
fuzzing techniquesthatapplyperturbations whitenoises blockreplacement ormapping toseedinputstendtoproduceinputsthatare either unrealistic or similar with the seed.
for example no fuzzing canturntheleftphotointotherightphotoinfigure1.symbolicexecution techniques also do not work as it is difficult to express the inputrealismasasolvableconstraint.furthermore noneofthese techniquessolvestherelevancechallenge.totellwhichimagesarerelevantforafirealarmapplicationrequiresbothanunderstanding ofthesoftwarestructure i.e.
knowingthatabranchpredicateis aboutfireintheinput andtheabilitytoperformtheverycognitive task we need to test i.e.
judging whether a photo contains fire .
outputcorrectnessrelyingonhumanjudgement.cognitive ml apis are designed to statistically mimic human behaviors e.g.
identifying the objects in an image interpreting the emotionalsentimentinasentence etc.consequently tojudgethecorrectness of ml software ideally we want to ask many people to process the same set of inputs and see if their decisions statistically match withthesoftwareoutputs a processthatisinherentlydifficultto automate.forexample itisdifficulttotellwhetherthefirealarm should be triggered or not without manual inspection figure .
in traditional testing the execution correctness often can be checkedautomaticallyusingthemathematicalrelationshipbetween theinputsandtheoutputsorcertaininvariantsexpectedtohold by the execution.
these techniques are still useful for the noncognitivepartsofthemlsoftware butcannothelpthecognitive parts.
previous work generated test oracles for domain specificapplications like an image dilation software a blood vessel categorizer animageregiongrowthprogram abiomedical textprocessor .theirdesigneachtargetsaparticularcognitive task and cannot be applied for general ml software.
probabilisticincorrectnessthatisdifficulttodiagnose .
when ml software produces outputs that differ from most human beings judgement which we refer to as an accuracy failure developers must attribute this failure to either the ml api or the surrounding software s use of the ml api.
this attribution is difficultasmlapisusestatisticalmodelstoemulatecognitivetasks andareexpectedtoproduceincorrectoutputsfromtimetotime.in other words developers need to distinguish failures caused by the probabilistic nature of the ml api which simply must be tolerated as part of using this specific ml api from a misuse of the api which represents a bug and must be fixed by the developer.
again thissituationisdifferentfromthatintraditionalsoftware testing where a test failure like a crash indisputably points out something incorrect with the software that needs to be fixed.
note that much recent work studies how to test and fix neuralnetworks.however theyfocusonimprovingthe accuracy fairness and security of the neural network itself e.g.
makingsurethenetworkisrobustagainstadversarialsamplesor does not contain certain biases etc.
they do notconsider how the neural network is usedin the context of an application and do not test how well the application using the neural network functions.
.
contributions this paper proposes keeper a testing tool designed for software that uses cognitive machine learning apis ml software .
totackletheuniqueinputspaceandoutputoraclechallenges keeper designsa set ofpseudo inverse functions forcognitive ml apis1.
for an api fthat maps inputs fromdomain ito outputs in domain o its pseudo inverse function f primereverses this mapping at the semantic level.
we make sure that the mapping by f primehas been confirmed by many people to have high accuracy.
for example the bing image search engine is a pseudo inverse function of google s image classification api.
keeper then integrates the pseudo inverse functions with symbolicexecutiontoreachthesparseprogram relevantinputspace.
specifically keeperfirstusessymbolicexecutiontofigureoutwhat valuesanml apioutputcantaketofulfillbranchcoverage e.g.
fire labels .desc infigure1 .keeperthenautomatically generates realistic inputs that are expected to produce the desired ml apioutputs leveragingpseudo inversefunctions.forexample thetwoimagesshownin figure1areamongtheimagesreturnedby a bing image search with the keyword fire .
keeperalsomakespseudo inversefunctionsaproxyofhuman judgement and automatically judges the correctness of software outputsthatarerelatedtocognitivetasks.sinceourpseudo inverse functions are notanalytically inverting ml apis i.e.
f prime f i i is possible a test input generated by keeper may not cover thetargeted software branch like the right image in figure failingtocoverthe alarmbranch.atthesametime sincethesepseu functions have been approved by many human beings keeperreportsan accuracyfailure whenoverathresholdportion of inputs fail to cover a particular target branch.
ofcourse keeperalsomonitorsgenericfailuresymptomslike crashes during test runs and helps expose bugs in code regions that require specific ml inputs to exercise.
finally to help developers understand the root cause of an accuracyfailure keeperexploresalternativewaysofusingmlapisand informsthedevelopersofanycodechangesthatcanalleviatethe accuracy failure.
for the example in figure keeper would inform developers that comparing the returned labels with not only fire butalso flame wouldmakethesoftwarebehaviormoreconsistent with common human judgement.
puttingthesealltogether wehaveimplementedkeeperthatcan beusedeitherthroughacommand linescriptoraplug ininside the vscode ide .
given a software application keeper first highlights all the functions that directly or indirectly call ml apis.
foranyfunctionthatdeveloperswanttotest keeperautomatically generatesmanytestcasestothoroughlytesteverybranchinthe specified function and its callees.
keeper analyzes the test runs andreports anyfailures aswellas potentialpatchesfor accuracy failures to developers.
we evaluate keeper on the latest version of open source python applications that cover different problem domains and ml apis.
due to the relatively young age of ml apis these applicationsaremostlyresearch projects hackathonproducts anddemo programs.
keeper achieves branch coverage on average for 1thecurrentimplementationofkeepersupportsgooglecloudaiapisandcanbe easily extended to support similar apis from other service providers.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
automated testing of software that uses machine learning apis icse may pittsburgh pa usa theseapplications.intotal keepercovers21 morebranches thanalternativetechniquesthatdirectlyusemachinelearningtrainingdatasetorrandomfuzzing.keeperexposes35uniqueaccuracy and crash failures from out of these applications.
background and definitions thissectionprovidesabriefoverviewofmlapis theirinputsand outputs and how they are typically used in software.
mlcloudapis.
mlapisofferedbydifferentserviceproviders all cover three main categories of machine learning tasks visiontasks language tasks and speech tasks.
keeper handles all the commonlyusedapisinthesethreefamilies asshownintable1.
keepercurrentlydoesnothandlevideointelligenceapis fromthe vision family translation apis from the language family and speechsynthesisapis fromthespeechfamily astheyareused much less frequently in open source applications .
inadditiontoimage text audioinputs somemlapisalsotakein configuration parameters.
for example analyze sentiment takes in not only a text string but also configurations like language encoding and input type as shown in figure .
these configurations are set to constant values mostly the default values offered by google inallofthemlsoftwarewehavechecked.therefore in this paper keeper focuses on generating image text audio inputs.
1document content text content type type.
plain text language en 2response client.analyze sentiment request document document encoding type encodingtype.utf8 figure an example of google cloud api with text input.
theoutputofamlapimayincludemultiplerecords likemultiple classification results multiple objects detected and so on.
each recordtypicallycontainsakeyresultfieldoftenofastringoranenum type like the classification label of an image the emotion ofaface andsoon andaconfidencescorefield whichindicates how likely this result is correct.
unless otherwise specified the remaining paper refers to these key result fields as ml api output as summarized in table .
note that some of these apis do output other auxiliary information.
for example the face detection apialso outputs the bounding box of each face detected in the inputimage.
these auxiliary result fields may be used to make control flow decisions although such usage has not been observed in anyof the applications collected by a previous ml api study .
ml software.
sometimes ml apis are only loosely connected with the remaining part of the software with their output directly printedoutwithoutfurtheruseinthesoftware.testingthistype ofsoftwaresimplyneedstotestmlapisandtheremainingpart of the software separately and hence is not the target of keeper.
in some other cases ml apis are more closely connected with their results used to impact the control flow of the software execution.thesecasespresentnewchallengestosoftwaretestingas discussed in section and hence is the focus of this paper.
test input generation keeperisatestingtoolforsoftwarewhosecontrolflowisinfluenced by ml apis.
as shown in figure keeper includes two major figure an overview of keeper.
components test inputgeneration whichwepresentinthissection and test output processing which we present in section .
keeper s input generation is built upon an existing symbolic execution engine dse .
given a function fto test2and all the function parameters represented as symbolic variables a symbolic path constraint is generated for every branch solving all the path constraints produces a test suite that offers full branch coverage.
inthis section weexplain howkeeperhandles caseswhenml apisarepartofthepathconstraintsandgeneratesinputsforml apis which are not handled by existing techniques.
a naive solution is to symbolically execute ml apis implementation.
unfortunately this is too expensive to carry out for state of the artdeepneural networks dnn .notto mentionthat theexactdnnsusedbymlapiprovidersareunknown.forexample a state of the art image classification network efficientnet l2 has480 million parameters.it takes in a224 x 224pixel image and generates the output after about billion floating point operations.solvingapathconstraintthatinvolvesthisnetworkwith more than x symbolic variables would take days.
keeper decomposes the problem of generating inputs for ml apis into two parts first it identifies the ml api outputs that are neededtosatisfypathconstraintsusingsymbolicexecution section .
andthensynthesizestheml apiinputsthatareexpectedto produce those outputs using carefully designed pseudo inverse functions section .
.
as we will see this decomposition not only avoidsthecomplexityofdirectlyapplyingsymbolicexecutionto dnns but also help judge the execution correctness section .
.
identifying relevant ml outputs to identify the desired ml api outputs keeper makes its symbolic execution skip any statement that calls an ml api and instead marks api output that is used by following code as symbolic.
this way the output instead of input of ml apis will be part of the path constraints and by solving the constraints keeper obtainstheapi outputvaluesthatareneededtoexercisecorresponding branches.
the only tweak keeper makes here is to have the symbolic executionenginesometimesgeneratingonepathconstraintforeach branch sub condition instead of the whole branch.
specifically a common code pattern that we have observed is to decide the 2users of keeper can choose any function to test including the mainfunction.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa c. wan s. liu s. xie y. liu h. hoffmann m. maire and s. lu ml task main output constraint example pseudo inverse function visionimage classification image class class fire search on internet keyword object detection object name object tableware search on internet keyword face detection face emotion emotion joy search on internet keyword human face text detection extracted text text print on an image languagedocument classification document class class food search on internet keyword sentiment detection score magnitude score select tweets from sentiment140 dataset entity detection entity name type type person use text generation technique seed or speech speec hrecognition transcript text turn on the light usespeec h synthesize technique on table different ml apis handled by keeper and their pseudo inverse functions.
1defsmart can img 2labels client.label detection image img 3classes 4forcinclasses 5ifc food return organic 7ifc paper orc aluminum return recyclable 9return non recyclable figure a smart can application heap sort cypher execution path based on whether or not an ml api outputs a label that belongs to a pre defined set.
for example the smart canapplication in figure executes the recyclable path when the output of label detection contains a label that is either paper oraluminum .sincedifferentlabelsoftenrepresentdifferenttypes ofreal worldinputs keeperwillgenerateonepathconstraintfor every condition clause instead of one for the whole branch.
for example fortheline 7branchinfigure4 keepergeneratestwoconstraints paper classes and aluminum classes which then prompts keeper to generate two separate sets of images to satisfy these two constraints.
in our implementation this is accomplished by enabling a corresponding feature of the underlying symbolic execution engine.
for example for a branch condition a or b or c four constraints will be formed representing a is true b is true c is true and noneofa b cistrue.solvingtheseconstraintsleadsto four inputs or input sets that satisfy these constraints separately.
.
identifying ml api inputs given an ml api fand an output o keeper aims to automatically generateasetofinputs isothatf i i iisexpectedtoproduce oaccordingtocommonhumanjudgement.forexample thetwo images in figure are expected to make label detection output fire andtheimagesineverycolumnoffigure5areexpectedto makelabel detection output the corresponding column header.
to achieve this keeper designs a pseudo inverse function f primefor every api f so thatf prime o will produce the input set iforf.w e wantf primeto have the following properties.
first f primeis not an analytical inversion of f. ideally f primeshould be built independently from f e.g.
not based on the same training data set so that f primecan help not only input generation but also failure identification in a way similar to n version programming.
second f primeshould be a semantic inverse of f reversing the cognitive task performed by fin a way that is consistent with most figure keeper generated test cases for figure humanbeings.thisway testinputsgeneratedbykeepercanexpect to cover most of the software branches unless the ml api is unsuitable for the software or is used incorrectly.
third f primeshould producemore thanone outputfor each inputit takesin.thiswillallowkeepertogeneratemultipleinputsfor fto exercise a corresponding branch and get a statistically meaningful test result given the probabilistic nature of ml apis.
with these goals in mind we have designed three types of pseudo inverse functions as summarized in table .
.
.
search based pseudo inversion.
for many vision and languageapis searchenginesoffereffectivepseu take in a keyword and return a set of realistic images texts thatreflect the keyword.
search engines have several properties that serve keeper s testing purposes.
first they offer great semantic inversion astherearemultiplesearchenginesthathavebeenusedby hundredsofmillionsofusersformanyyearswithhighsatisfaction .
theirtop search resultstypically match the commonhuman judgement.second theyarenotananalyticalinversionofmlapis and we will use non google engines to minimize potential correlations.third theyacceptawiderangeofsearchwordsandproduce many ranked results which means a large number of high quality testinputsforkeeper.specifically keeperusesdifferentengines and search keywords for different ml apis vision tasks.
image classification and object detection apis returnstringlabelsthatdescribetheimageandtheobjectsinsidetheimage respectively.forbothapis keeperusesthebing image searchengineandusesthedesiredlabeldescriptionorobjectname as the search keyword.
for example the images in each columnof figure were the top search results returned by bing using authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
automated testing of software that uses machine learning apis icse may pittsburgh pa usa the keywords listed atop.
the only exception is the last column when there is no specific keyword requirement like c !
food and c!
paper and c !
aluminum keeper uses a blank image and images generated by a random image generator .
theface detectionapidetectshumanfacesinan image.some mlsoftwareusesthereturnedemotionstringassociatedwitheach face e.g.
joy sorrow etc.
todecideexecutionpath.togenerate corresponding images keeper uses human face as a keyword to search the bing image.
languagetasks.
document classificationapisprocessadocument andreturn categoriesbased onthe documentcontent like pets health sports and others.
keeper uses the desired category name as keyword and searches it at knowledge graph websites wikipedia and britannica and bing web searchengines.keeperthenusesthetextextractedoutfromeach returned web page as the ml api input.
.
.
synthesis based pseudo inversion.
the semantic inversion of some ml apisdoesnot match the functionality ofsearch engines.
fortunately we find ways to synthesize inputs for them.
thetext detection api extracts printed or handwritten text from an image.
unfortunately image search engines tend to return imageswhosecontentreflectsthesearchkeyword insteadofimages that contain the keyword as text within the image.
therefore givenatextstring keeperprintsitonabackgroundimageusing the python pillow library .
keeper adopts both printed and hand writing fonts different font settings produce different test images.todecidethebackgroundimage keepercheckswhether thetext detection api shares its input image with another vision api.
if so the test images keeper generated for the other api willbeusedasthebackground otherwise ablankimageandsome randomimageswillbeused.figure6showssomeofthetestimages that keeper generates for application wanderstub which has a branch checking if the input image contains total .
figure test inputs generated for wanderstub .
theentity detection apiinspectstheinputsentenceforknown entities thereareintotal13entities suchasaddress date etc.
since the search engines usually return long documents keeperinstead uses a popular language model gpt to synthesize anynumberofsentencesthatstartwithapre definedword phrase that corresponds to the desired entity type.
thespeech recognition apitranscribestheinputaudioclip and outputs the transcript.
keeper uses speech synthesis tools particularlythepyttsx3 pythonlibrary togeneratethedesired audioclipsbasedona giventranscript.keepergeneratesmultipleaudio clips using different voice settings supported by this library.
.
.
ml benchmarks for pseudo inversion.
thesentiment detectionapipresentstwo challenges.first althoughthisapi aimsto identifytheprevailingemotionalopinionwithinthetext itdoesnot directly output a categorical result.
instead it returns two floatingpoint numbers scoreandmagnitude for developers to derive emotioncategoriesfrom.thereisnoperceivablewaytogenerate textthatcanoffertheexact scoreormagnitude .second evenif we just hope to generate text that contains positive or negative emotion no search engine or synthesizer can accomplish this.
facing these challenges keeper resorts to the sentiment140 dataset which contains tweets manually labelled as positive negative and neutral.
keeper randomly samples the same numberofpositive negative andneutraltweetsas test inputsfor any sentiment detection api called inside an ml software with theexpectationthatthesetweetswillhelpcoverdifferentbranches in the software that are designed for different emotions.
notethat wetreatmlbenchmarksasthelastresortformultiple reasons.first thelabelsassociatedwithdatainsidemlbenchmarks either have few categories or have limited quality.
for example imagenet contains manually labeled image categories which is too few compared with the labels of google vision ai.on thecontrary openimagehas 9millionimages with20 labels.howeve r89 ofthelabelsaregeneratedbydnns and53 of thehuman verifiedonesareincorrect .second mlbenchmarks are built with pre processed real world data.
such clean data has less variety as they share similar size resolution and encoding format.third somebenchmarksmaybepartofthetrainingdata setofgooglemlapis whichmakesthetestinputsbiasedtowards theonesapiscanperformwellonandhencelesslikelytoreveal problems.finally generativeadversarialnetworksynthesizesnew data following the distribution of the training set .
it covers different domains including generating images from text .
we donotuseit asthisapproachrequiresmuchtrainingdataandends up generating non real world data that has similar distribution with the training set whose limitations we discussed earlier.
.
putting everything together overall keeper generates test inputs for any function fin the followingsteps.first itssymbolicexecution section3.
generates a set of inputs ithat offer full branch coverage unless some path constraintsareun satisfiable.ifnobranchin foritscalleesdepends ontheoutputofanmlapi theinputgenerationisdone.otherwise if there is such an ml dependent branch b those inputs that are expectedtocover b denotedas ib i containfieldsthatrepresent the desired outputs of ml apis and require further processing.
next foreach desired output oof anml api f keeperapplies f s pseudo inverse function f primeonoto generate a set of image text audio inputs for f section .
.
if f s input is exactly an input of the function under test f i.e.
it is not derived from an input of fthrough pre processing the input generation is done.
keeper updateseveryinputin ibwiththeimage text audioinformation.if there were kinputs in ib keeper now gets k nbinputs with nb being the number of image text audio inputs keeper generated for the ml api fto exercise b. developers can configure nb or the totalnumberoftestinputstogenerate.keeperwillthencompute nb authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa c. wan s. liu s. xie y. liu h. hoffmann m. maire and s. lu so that every ml dependent branch sub condition gets exercised by about the same number of inputs.
iff sinputisderivedfromaninputoffunction fthroughpreprocessing keeperrunssymbolicexecutiononthatpre processing code to figure out the desired input of fand finishes the input generation.
for example if a function deletes the first character of astringparameterandfeedstheresultingstringtoanmlapi f keeper will add a character to the beginning of every input generatedforftogetthestringparameterofthisfunction.thesymbolic execution engine used by keeper can handle pre processing relatedto text i.e.strings but notthoserelated toimagesoraudio such as image audio clipping.
future work can extend keeper with common image audio transformation routines.
finally these test inputs generated by keeper are ready to be executed.
particularly in order for a software to consume a test imageoraudiofile filegeneratedbykeeper keeperchangesthe file path embedded in the software to a path that points to file.
test output processing once all the test inputs are generated and executed keeper works on failure identification and attribution.
.
failure identification keeper looks for three types of failure symptoms low accuracy dead code and generic failures like crashes.
.
.
low accuracyfailures.
whensoftwareincorporatescognitivemlapisinitscomputation judgingtheoutput scorrectness becomeschallenging bydefinitionofcognitivetasks thisoutput needstobecheckedwithmanypeopletoseeifitmatcheswithcommonhumanjudgement duetotheprobabilisticnatureofml apis an occasional mismatch is expected.
of course frequent mismatchesare un acceptableandseverely hurt userexperience like nottriggeringfirealarmswhenneeded figure1 orconsistently categorizing garbage incorrectly figure .
totacklethefirstchallenge keeperusespseudo inversefunctions as an approximation of common human judgement to tackle the second challenge keeper considers the software to suffer from a low accuracy failure or an accuracy failure for short only when overathresholdportionofinputsofaparticulartypehaveproduced outputs that are inconsistent with common human judgement.
specifically for all the inputs ibthat are generated to cover a branchb keeper checks which of them exercise bat run time denoted as isucc band calculates the recallofb i.e.
isucc b ib .
if the recall drops below a threshold by default.
keeper reports an accuracyfailureassociatedwith b.thesettingof canbeadjusted butshouldnotbe100 asmlapisareprobabilisticandpseu functions cannot guarantee to be correct all the time.
forthefire alarmexampleinfigure1 keeperidentifiesanaccuracy failure associated with the fire branch as its recall is for the smart can example in figure keeper identifies an accuracy failure as the recall of the recyclable branch is only .
forabranch bthatdependsontheoutputofasentiment detection api keeper identifies failures slightly differently as inputs are generatedforsentiment detectionapidifferentlyasdiscussedin section .
.
.
during test runs keeper checks all the inputs that1labels client.label detection image img 2temp label .desc label .desc label .desc 3if fire intempor flame intempor ash intemp 4alarm figure a fixed version of figure suggested by keeper.
exercisebtoseewhatportionofthemarelabeledashavingpositive emotionandwhatportionarelabeledasnegative.ifbothgoabovea threshold indicating that branch bis not accurately differentiating inputs with different emotions keeper reports an accuracy failure.
rootcausesofaccuracyfailures.
notethat theseaccuracy failures are notequivalent with low precision or low recall of the mlapiitself.thelatterisjustoneofthepossiblerootcausesof the former.
keeper intentionally does not calculate the precision or recall of any ml api but instead focuses on the overall software.
onepossiblecauseisthatdevelopersmissedsomerelatedlabels in a branch condition which we refer to as an incomplete label problem.
forexample the label detection api doesnot return fire asatop 3labelformanytopfireimagesreturnedbythebing image search.
this by itself is notconsidered a failure by keeper.
if thesoftwareusestheapiproperly likeraisingafirealarmupon not only a fire label but also a flame label and an ash labelas shown in figure no accuracy failure would be reported asthe recall of the alarm related branch is as high as and the precision is in our experiments.
another possible cause is that developers used a non existing label whichdoesnotexistintheapi slabelsetandcanneverbe theoutput.thisisnotasurpriseasthelabelsthatcanbeoutputby googlevisionapiaretoomany fordeveloperstomemorize.
for example an applicationcompares the label detection output with clothes and pants which are non existing labels.
instead clothing and trousers are valid labels.
.
.
dead codefailures.
theseoccurwhenabranchisnotcovered after all the testing runs.
they happen under two scenarios.
onescenarioisthatkeepergeneratesasetoftestinputs ibexpectedtocoverabranch b andyetbisnotexercisedbyanyinputin ib.suchanextremecaseoflowbranchrecall i.e.
isoftencaused by the branch comparing a ml api output with a non existing label.if thiscomparisonisone ofmultiplebranch sub conditions an accuracy failure would likely occurr i.e.
a low but non zero recall if itis the only conditionclause a deadcode failure occurs.
forexample asmartphotoapplicationfesmkmitl checks the output of label detection against the string face .
unfortunately among the category labels that could be output by this api none of them is face .
instead human face is one of the valid labels for this api which the developers should have used.
the other scenario is that keeper fails to generate any inputs to cover a branch which triggers a dead code failure report before any test runs.
sometimes this is caused by a typo in the branch condition.forexample keeperexposessuchafailureinverlan .
verlanuses object detection tojudgewhetheranimagecontains an animal or not.
unfortunately it wrongly uses animal instead ofobj.name animal in its branch condition making the if statementalways true.itwillregard everyimagethatcontains at least one object as an animal image!
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
automated testing of software that uses machine learning apis icse may pittsburgh pa usa 1object client.object detection image img 2forobjinobjects 3ifobj.name dog or animal 4do a figure dead code bugs in verlan .
.
generic failures.
these have symptoms like crashes that do not require special techniques to observe.
comparing with traditional testing techniques keeper offers extra benefit in two sce narios.
thefailuresarecausedbybugslocatedonapaththatrequiresspecificmlapiinputstotrigger.keepercontributesby generating the needed ml api inputs to exercise the path.
the failures are directly relatedto the corner casesof ml api inputs such as blank images that cause label detection to return no labels.anexampleofsuchabugexposedbykeeperisillustrated in figure .
1text client.text detection image img 2labels text .descr iption.split n 3forlabelinlabels 4do something figure crash failure in fortnitekillfeed a blank image returns an empty array textand trigger an index out of range.
.
failure attribution tohelpdevelopersunderstandandtackleaccuracyfailures keeper attempts to automatically patch the software by changing how ml apis outputisused.keepersuggeststhechangetodevelopersand ifallattemptsfailed keepersuggestsdeveloperstoconsiderusingadifferent moreaccuratemlapi oraddingextrainputscreeningor pre processing.
specifically keeper attempts two types of changes to the branch bwhere the failure is associated with.
labelchanges.
whenbranch bcomparesamlapioutputwith a set of labels keeper tries to expand the set of labels with three goals in mind.
recall goal more test inputs that are expected to exercisebcan now satisfy b s condition precision goal most inputs that are not expected to exercise bshould continue to fail theconditionof b semanticgoal theaddedlabelsarerelatedto the original label s in bin terms of natural language semantics.
withoutlossofgenerality imaginethat btakestheformof if o label0 withobeingtheoutputofanmlapi f.keeperfirst collectsthesetoflabels loutputby fforeveryinputin ifail b the set of inputs that are expected to exercise bbut fail to do so.
then considering the semantic goal keeper filters out every labelinlthatisneitheradjacenttonorsharingacommonneighbor withlabel0in the wikidata knowledge graph .
for example amber isprunedoutbykeeperwhileprocessingtheaccuracyfailure in figure because it is far away from fire in the knowledge graph.
instead flame and ash both remain as they are both adjacent to fire on the graph.
next keeperusesagreedyalgorithm to iterativelyexpandthe set of labels compared with oinb.
every time keeper adds to the set a label l lso thatloffers the biggest improvement in b s recall without reducing b s f1 score i.e.
the harmonic mean oftheprecisionandtherecall .here theprecisionofbranch biscomputedas isucc b isuc amongalltheinputsthatexercise b howmany ofthemareexpectedtodoso.thisprocedurecontinuesuntilthe recall ofbgoes above the accuracy failure threshold or when there is no eligible candidate label remaining in l. exactly through this process keeper suggests to the developers that the alarm branch in figure should check more labels like that in figure as by checking more labels the branch s recall can increase from to on those test cases generated by keeper.
this suggestion is proposed through a text description instead of a code patch if you additionally check flame and ash in the branch condition on line your program will agree with most human beings judgementfor85 oftestinputs animprovementfrom40 of your original code .
threshold changes.
as discussed earlier an accuracy failure is reportedwhenabranch b whichchecksthe scoreand ormagnitude outputofasentiment detectionapi getsexercisedbymanyinputs labeled ashaving positive emotions and alsomany inputs labeled as having negative emotions.
keeper applies logistic regression to theseinputtexts withthe score magnitude outputofeachinput asfeaturevectorsandthelabeledemotionasaclass.keeperthen suggests the linear formula of logistic regression as a new branch checking threshold to developers letting them know that this new formula can better differentiate text inputs with different emotions.
implementation we have implemented keeper for python applications that use googlecloudaiapis themostpopularcloudaiserviceson github .
the core algorithm of keeper is general to other languages and ml cloud apis.
keeper uses dynamic symbolic executionframeworkpyexz3 whichimplementsthedsealgorithm andusescvc4 forconstraintsolving.keeperusespythonbuiltintracebacktool tocheckbranchcoverage andpyan and jedi forcallgraphandprogramdependencyanalysis.keeper uses python scikit learn library for linear regression models.
figure keeper ide plugin interface wehaveimplementedanidepluginforvisualizedinteraction withkeeper asthedebuggingandfixingofaccuracyfailuresparticularly requires developers participation as illustrated in figure .
the plugin is an extension in visual studio code a popular code editor supporting multiple languages.
for any python software keeper firstidentifiesallfunctions thatinvokeml apis authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa c. wan s. liu s. xie y. liu h. hoffmann m. maire and s. lu failure type root cause related ml task keeper rreal rreal noise fuzz.
crash failuresout of bound accesses text detection entity detection missing input validation document classification missing type conversion accuracy failuresimproper labels image classi.
object detect.
document classi.
api limitations image classification object detection improper threshold sentiment detection dead code failurestypos image classification text detection non existing label image classification table unique failures exposed by keeper.
this crash disappeared later with the most recent version of google api.
directlyorindirectlythroughcallees anddisplaysthemontheside bar under relevantfilesandcodes infigure10.fromthatlist developerscanselectthefunctiontotest.oncetheyhavemadetheselection theywillbeaskedtoprovidetypeinformationoffunc tionparameters aspythonisadynamicallytypedlanguage.keeper will then start the testing.
at the end of the testing which usually takes1 2minutes anyexecutionfailurethathasbeenexposedis listed in the side bar right under functions with failures in the figure.
source code related to each failure is highlighted togetherwithahoveringwindowthatoffersdetailedinformation like failure description triggering inputs and patch suggestions.
a demo of the keeper plugin can be found at our artifact .
evaluation our evaluation aims to answer several questions does keeper help improve the branch coverage in testing?
is keeper able to find bugs during its testing?
is keeper able to suggest fixes for accuracy failures?
.
methodology .
.
applications.
we evaluate keeper using python applications that are from two sources.
from the open sourceapplications assembled by a previous study of ml apis we found45pythonapplicationsthatusemlapisinanon trivialway i.e.
theapioutputaffectscontrolflow .
weadditionallychecked about100randompythonapplicationsongithubthatusemlapis and found applications that use ml apis in a non trivial way.
these 63applications usea rangeof mlapis includingvision apps language apps and speech .
their sizes range from54linesofcodetomorethan100 000linesofcode with582 lines of code being the median3.
they have a median age of months at the time of our study apr.
1st .
among these applications 16applicationshavereceived1ormultiplestarson github the other applications have not received any stars.
the details of each application including the link to each github code repository are included in table .
despiteourbesteffortinapplicationcollection unfortunately mostofthese63applicationsseemtoberesearchprojects hackathon products ordemoprograms basedontheirlimitedpopularityin github.
this is probably due to the young age of ml apis.
con sequently our evaluation results may not generalize to mature software that has a solid user base.
3filesfromtemplates frameworks and librariesarenotincluded intheloccounting.vision app.
language app.
speechapp.
keeper .
.
.
random real .
.
.
random real noise .
.
.
fuzzing .
.
.
table average branch coverage across applications.
for more than half of the applications we simply specify mainas the function to test.
in other cases the function under test is the entry function to the software feature related to ml apis.
the average number of branches in these functions to test is .
.
.
baselines.
wecomparekeeperwith3othertechniques.each technique generates test inputs for each function under test.
randomreal werandomlypickinputsfromwellestablished data sets including imagenet that contains million images twitter us airline sentiment that contains tweets and a set of audio clips synthesized for daily sentences .
random real noise we add random noise to inputs picked byrandomreal.for animage werandomlyadded noisesfollowing gaussian distribution for an text input we randomly decide whether to add noise and if so randomly changed the word orders.
for audio input we do not add noise here as we found that addingsmallnoisesdoesnotaffectmlapiandyetaddingbignoiseswould turn the audio clip into what the third approach will generate.
fuzzing we use a coverage based fuzzing tool pythonfuzz to generate images text and audio.
for every image input we useanintegerlisttofillitsrgbmatrixinarepeatedway.forevery text inputs we generates ascii character sequences.
for audio inputs we directly generates the audio data.
.
software testing evaluation .
.
branchcoverage.
foreachofthe63functionsspecifiedtotest eachfromoneapplicationinourbenchmarksuite wecomputetheaccumulativebranchcoverageachievedbythe100inputsgenerated by each testing technique.
table shows the overall results.
acrossdifferenttypesofapplications keeperconsistentlyachieves high branch coverage around on average.
the uncoveredbranches are either related to dead code failures that keeper discovers orrelatedtocodethatourunderlyingsymbolicexecution engine cannot handle.
in comparison the fuzzing technique performedtheworst coveringlessthan50 ofthebranchesforvision andspeech applications confirmingour intuition thatit is important to use realistic inputs to test ml apis.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
automated testing of software that uses machine learning apis icse may pittsburgh pa usa application w link description stars locbranch coverage keeper exposed failures branches keeper rreal rreal noise fuzz accuracy crash dead code selfmailbot telegram bot fortnitekillfeed game assistant fb mmhm meme inspector audio sentencesplit audio splitter calbot nutrition tracker hapi produce analyzer stockmine investment helper tone smart music player uottahack speechemotion detector blindhandassistance blind assistant ingredientprediction recipe recommender recipego recipe recommender devfest public opinion analyzer klassroom note taker uofthacks6 news summerizer hackthe6ix insurance manager average branch coverage total failures exposed by keeper aander etl smart album alpr license recognization artificial intelligence calorie calculator emotion2music smart music player experiments product info analyzer fesmkmitl emotion tagger heapsortcypher garbage classifier image analyzer chat bot chat bot ns online toolkit game assistant phoenix fire alarm researchspring2019 prescription reader seefarbeyond coin finder smart can garbage classifier snapcal smart calendar twimage search landmark recognizer wanderstub exchange convertor verlan animal finder thgml calorie calculator sbhacks2021 smart camera flood depths flood monitor image tagging fruit checker shecodes hack clothes checker sunhacks2019 blind assistant snaptrack hack112 nutrition tracker lahacks quaranteen image checker plant watcher plant manager senior project smart album animal analysis image sharing platform calhacksv2 movie review analyzer carbon hack sentiment public opinion analyzer cloud computing food delivery ec601 twitter keyword investment helper electionsentimentanalysis tweet analyzer geoscholar scholar database journalbot journel manager notescript note taker sarcatchtic makespp19 text tone checker twitter mining gae disaster news analyzer badgif discord bot mind reading journal journel manager newschronicle timeline generator ocr contratos contract analyzer most anoying app ever smart music player pottypot swear remover readingmachine book reader swearremoval swear remover translator consecutive interpreter average branch coverage total failures exposed by keeper table4 informationandresultsof63applications.
eachapplicationnamecontainsahyperlinktoitsgithubrepository theloc numbersreferonlytotheactualapplicationcode notlibraries templates orotherfilesintherepository branches thenumberofbranches in the function under test coverage all test cases crash the program execution before reaching any branches.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa c. wan s. liu s. xie y. liu h. hoffmann m. maire and s. lu randomrealperformsbetterthanfuzzing butstillfailstocover aboutaquarterofbranchesinvisionapplicationsandhalfofthe branches in speech applications.
adding random noises to random realistic inputs does not help.
keeper covers and more branches than random real for vision and speech applications respectively askeeperleveragessymbolicexecutionandpseu functions to generate inputs targeting different branches.
applications that use language apis appear to be the easiest to cover even fuzzing achieves coverage.
this is probably because language apis output like document type or entity name has much less variation than that of vision and speechapis.
table4showstheexactbranchcoverageofferedbyeachtechniqueforeachbenchmarkapplication.aswecansee keeperoffers the highest branch coverage for all applications.
.
.
failureexposingandattribution.
asshownintable2 keeper exposedmanyfailuresbyrunningthose100testinputsitgenerated failures from the latest version of applications.
these failures cover a range of symptoms and root causes.
except for one failure caused by missing type conversion the others are all related to different types of cognitive ml tasks as shown in the table.
in comparison alternative testing techniques missed crash failurescaughtbykeeper.furthermore unlikekeeper theycannot automatically recognize accuracy failures and dead code failures.
accuracy failures.
among the accuracy failures exposed by keeper of them are related to label checking for vision apis and document classification api and are related to threshold checking for the sentiment detection api.
for all of the failures related to sentiment detection keeper manages to suggest better checking threshold that fixes the failure.
there are accuracy failures that keeper manages to fix by making the failure branch check for extra labels.
the failure in figure is one such example.
as another example one application checksiftheoutputof label detection containseither building or estate or mansion .
this branch s recall is very low .
keepersuggestsadding house architecture and window to the label set which would improve the recall to be above .
fortheremaining6vision relatedaccuracyfailures codechanges bykeepercanalleviatetheproblembutcannotpushtherecallof the related branch to be above suggesting fundamental api limitations.
two of these cases actually involve non existing labels.
for example the aluminum in line of figure is actually a nonexisting label.
keeper suggests checking metal instead which increases the branch s recall to close to but still below .
deadcodefailures occurredin3applications.oneofthemis duetonon existinglabels.twoarebecauseoftyposinbranches that process ml api output like the one in figure .
crashfailures aremainlycausedbyout of boundaccessesto listsreturnedbymlapis asshowninfigure9.onecrashiscausedbybuggycodeinsideabranchbodythathandlesimageswithcoins inside.
this failure cannot be exposed by other testing techniques as they did not produce images with coins inside.
falsepositives.
keeperhastwofalsepositivesintotal theyare not included in table .
one application tries to detect sensitive documentbycheckingifanyoutputofthedocument classification apicontainsa ensitive sub string.keeperfeedsitspseudo inverse function with ensitive and fails to get any test inputs and hence figure end user preference original vs. keeper version.
incorrectly reports a dead code failure.
the other application has a branchthatgetscoveredonlywhenanmlapigeneratesaspecific outputwithlowconfidence.keeperisnoteffectiveatgenerating low confidence inputs and wrongly reports an accuracy failure.
thresholdsetting.
asdiscussedinsection4.
therecallthreshold issetto0.75bydefaultwhendetectingaccuracyfailures.naturally morefailureswouldbereportedwhen islarger.increasing to0.
whichisunreasonablyhigh wouldcreates5morefailure reports decreasing to .
would have fewer failure reports.
resultsacrossapplications.
asshownintable4 the35failuresexposedbykeeperarefrom25differentapplications.these includebothapplicationsthathavereceivedstarsongithuband thosethathavenot boththesmallestapplicationinourbenchmark suite wanderstub and the largest one researchspring2019.
.
user studies to better evaluate the accuracy failures and the code changes suggestedbykeeper werecruited100participantsonamazonmechan icalturk mturk forasoftware usersurvey.thesurveyincludes4 applicationsfromourbenchmarksuites 2image relatedapplicationsand2text relatedapplications.oneachsurveypage abrief description is given for an application and user study participants are told to review how two versions of this application perform onasetofinputs.then thewebpagedisplaysanumberofinput images textandthecorrespondingoutputsofapplicationversion andapplicationversion .thesetwoversionsaretheoriginalapplication and the application with suggested code changes from keeper referredtoas fixedinfigure11 werandomlydecidewhich one of them is version and which is version on each survey pagetoreducepotentialbias.eachparticipantisaskedtoanswer questions about for each input which version s output they prefer and whichversion they thinkis betterwith everything considered.
participants were compensated after the survey.
asummaryoftheuserstudyresultsisshowninfigure11.as we can see in all cases a dominate portion of end users prefer the version with changes suggested by keeper over the original version supportingkeeper sjudgementaboutaccuracyfailuresand keeper sattemptinfixingtheaccuracyproblems.atthesametime we also noticed that there are of user study participantswho prefer the original software and who feel the two versions are about the same.
these results confirm the fact that cognitivetasksareinherentlysubjective evenhumanbeingsoften do not agree with each other on these tasks.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
automated testing of software that uses machine learning apis icse may pittsburgh pa usa threats to validity internal threats to validity.
keeper assumes that search engines top results are mostly consistent with human judgement which could be incorrect.
the failure identification and fixing attempts inkeeperareinherentlyprobabilistic.therecallthatkeepercalculated for each branch could vary depending on the test inputs.
more test inputs would make the testing procedure more robust.
some inputs generated by keeper may not be the inputs that the software aims to handle like the image being a photo taken indoorandyetthesoftwaremeanttobeusedoutdoor.whenkeeper expands a branch s comparison label set the increase of the recall sometimes comes with the decrease of the precision i.e.
more inputsnotexpectedtoexercisethebranchdoesexercise .althoughkeeper usesthe f1 scoreto balance precisionand recall ultimatelydevelopersneedtomakethecodechangedecision.weimplemented keeper ide plug in aiming to help developers make informed decision about how their software uses ml apis.
whenaninputexpectedbykeepertocoverabranch bfailsto doso thisinputmaycoveranotherbranch b primewhosebodyconducts the same computation as b. this would confuse keeper s failure identification although we have not observed such situations.
externalthreatstovalidity.
mostofapplicationsinourbenchmark suite including those used as examples in the paper are research applications hackathon projects or demo programs.
consequently observations and results obtained from them might not generalizetomorewidelyused real worldapplications.ourtool is only tested with python applications using google ai not other ml cloud api services.
related work ml related software.
prior work studied development phases of software that contains machine learning components.
they do not look at how to test such software.
arecentstudymanuallyidentifiedanti patterns fromsoftwarethatusesmlapis.keeperdiffersfromthisstudybyproposing testing techniques that can automatically expose failures and attributefailurecauses.onthecontrary thisrecentstudyobtained all its anti patterns through manual code inspection.
it managedto build automated detectors for some performance related anti patterns like repeatedly calling a ml api with a constant input but does not have automated bug detection or testing solutions for any correctness related anti patterns.furthermore due to the different design goals the type of failure root causes covered by keeperalsodiffersfromthepreviousstudy.inthe45applications thatareevaluatedbothbykeeperandthepreviousstudy keeper automaticallyexposed32failures amongwhichonly3werealso identified by the previous study.
another line of work studies testing autonomous systems.
they are tailored for the characteristics of autonomous driving and spatial temporal data and thus not applicable to most ml software targeted by keeper.
ml relatedtesting.
muchresearchhasbeendonefortesting andfixing neuralnetworks intermsofaccuracy fairness and security.
other work studies implementation bugs of neuralnetworkarchitectures andothermachinelearning models .
they are orthogonal to keeper.as discussed in section some previous work looked at how to test specific software that contains ml components .
unfortunately their solutions do not apply to general ml software.for example one work trained a svm classifier to judge the correctness of an image dilation program leveraging the fact that the input image and the output image should contain the same objects .totestablood vesselimagecategorizer previouswork generates blood vessel images with certain density branches and other features and use these features to generate output ground truth.
previous work uses metamorphic approaches to test entity detection and image region growth programs.
they require application specific rules about inputs and outputs relationship e.g.
after we concatenate inputs of entity detection the output becomes the concatenation of individual outputs .
priorwork studiesautomatictestingand bugdetectionofmachinelearningapis includingframeworksforimplementingneural networks andrestapisthatprovidemachinelearning solutions .
they focus on the implementation inside ml apis not how they interact with other software components.
test generation using search engines.
previous work explored using search engines to generate string inputs for software under test.specifically when a program identifier correspondstoacommonconcept suchas emailaddress thisidentifier can be used as a keyword to search for related web pages.
the resulting web pages can then be processed to help generate related stringinputs e.g.
arealisticemailaddress .clearly keepertackles fundamentally different problems from previous work although keeper also leverages search engines.
conclusion it is challenging to efficiently and effectively test software containing machine learning components.
we present keeper an auto mated coverage guided testing framework that helps developersto detect bugs and provide fixing suggestions for their softwareimplementation.
keeper automatically generates test cases via anoveltwo stagesymbolicexecutionandkeeper designedmlinverse functions.
we evaluate keeper with a variety of open source machinelearningapplicationsandachievehighcodecoveragewith a small set of test cases.
it identifies bugs that leads to software crash lower inference accuracy or dead code.
data availability we release our benchmarks the tool source code experimental results and user study results online .
acknowledgement we thank the reviewers for their insightful feedback.
the authors researchissupportedbynsf cns1764039 cns1956180 ccf1837120 ccf2119184 cns1952050 ccf1823032 aro w911nf1920321 and a doe early career award grant desc0014195 .
ad ditional support comes from the ceres center for unstoppablecomputing cdac summer lab the marian and stuart rice re search award microsoft research dissertation grant uchicago college research fellow grant and research gifts from facebook.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa c. wan s. liu s. xie y. liu h. hoffmann m. maire and s. lu