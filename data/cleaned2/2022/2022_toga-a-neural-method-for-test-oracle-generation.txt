toga a neural method for test oracle generation elizabeth dinella university of pennsylvania edinella seas.upenn.edugabriel ryan columbia university gabe cs.columbia.edu todd mytkowicz microsoft research todd.mytkowicz gmail.comshuvendu k. lahiri microsoft research shuvendu microsoft.com abstract testing is widely recognized as an important stage of the software development lifecycle.
effective software testing can provide benefits such as bug finding preventing regressions and documentation.
in terms of documentation unit tests express a unit s intended functionality as conceived by the developer.
a test oracle typically expressed as an condition documents the intended behavior of a unit under a given test prefix.
synthesizing a functional test oracle is a challenging problem as it must capture the intended functionality rather than the implemented functionality.
in this paper we propose toga a neural method for testoracle generation a unified transformer based neural approach to infer both exceptional and assertion test oracles based on the context of the focal method.
our approach can handle units with ambiguous or missing documentation and even units with a missing implementation.
we evaluate our approach on both oracle inference accuracy and functional bug finding.
our technique improves accuracy by over existing oracle inference approaches achieving overall accuracy on a held out test dataset.
furthermore we show that when integrated with a automated test generation tool evosuite our approach finds real world bugs in large scale java programs including bugs that are not found by any other automated testing method in our evaluation.
acm reference format elizabeth dinella gabriel ryan todd mytkowicz and shuvendu k. lahiri.
.
toga a neural method for test oracle generation.
in 44th international conference on software engineering icse may pittsburgh pa usa.
acm new york ny usa pages.
.
introduction unit testing is a critical aspect of software development.
effective unit tests for a component a method class or module can provide documentation find bugs and prevent regressions.
in terms of documentation unit tests express the unit s intended functionality as conceived by the developer.
documenting the unit s functionality performed this work while interning at microsoft.
equal contributor.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may pittsburgh pa usa association for computing machinery.
acm isbn .
.
.
.
a test conveys the unit s intended usage.
the test also serves as a mechanism for detecting functional bugs during development.
when executed a test checks for mismatches between intended and implemented functionality.
such a mismatch causes a test failure indicating a bug in the implementation.
furthermore unit tests can alert the developer when future code changes introduce bugs.
effective unit testing during development can prevent release of buggy software and reduce costs by billions of dollars .
a unit test is composed of two parts a prefix which drives the unit under test to an interesting state and an oracle which specifies a condition that the resultant state should satisfy.
a sufficiently expressive test suite should document functionality under both normal invocations where the precondition is met and exceptional behaviors where the precondition is violated.
figure shows two examples of unit tests for a stack class.
the tests document a normal invocation figure 1a and an exceptional invocation figure 1b .
figure 1a shows a normal invocation of the unit where the test prefix instantiates a stack and makes sequential calls to push andpop.
the test oracle highlighted in red asserts that the stack s isempty method should return true at the resultant state.
if the unit contains a bug related to the tested behavior e.g.
if pop always fails to remove an item from the stack this test can aid in detecting the bug.
on the other hand figure 1b shows the unit s expected behavior when the precondition of popis not satisfied.
in this case the intended behavior of calling popon an empty stack is to raise an exception.
as such the test oracle is the expected exception.
the try catch structure ensures that the unit does indeed raise an exception.
if the unit contains a bug and does not raise an exception the test will fail by executing assert.fail .
public void testpop stack int s new stack int int a s. push a s. pop bool empty s. isempty asserttrue empty a normal invocation of poppublic void testpop try stack int s new stack int s. pop assert .
fail fail catch exception e pass b exceptional invocation of pop figure unit tests of a stack class.
the test oracles are highlighted in red.
a correct implementation of stack will be empty after a sequential push and pop and must raise an exception if pop is called on an empty stack.
ieee acm 44th international conference on software engineering icse authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa elizabeth dinella gabriel ryan todd mytkowicz and shuvendu k. lahiri it is clear that testing has immense benefits.
however authoring high quality unit tests is time consuming.
on average developers spend of their time writing tests .
as such extensive work has been devoted to automated unit test generation .
however test generation tools have no definitive knowledge of the developer s intended program behavior.
this creates a challenge for generating functional test oracles.
instead these tools consider program crashes and undesirable exceptions e.g.
null dereference or out of bound array accesses as the test oracles.
these tests are capable of finding numerous safety bugs in the unit s implementation but are not sufficient to find violations of intended functionality and thus do not replace the need for manual unit tests.
complimentary to automated test generation tools extensive work has been devoted to test oracle creation from documentation and comments .
we refer to these techniques asspecification mining methods for test oracle generation.
these methods rely on a restricted structure of documentation and a set of handcrafted rules to infer exceptions and assertions for a unit.
however given that users do not follow a prescribed format for writing documentation or omit them altogether these methods fail to extract interesting oracles on most real world software components.
in our evaluation we show that these methods cannot infer bug finding assertions for a benchmark of real world java projects.
recently neural generative models have shown promise in generating functional test oracles .
neural methods are more flexible than specification mining approaches as they do not rely on fixed patterns.
this flexibility makes neural generative models robust to imprecise or even missing documentation.
however we find in our evaluation that these methods struggle to generate accurate oracles due to the large space of possible assertions.
in summary an effective test generation approach must infer both exception and assertion oracles that accurately reflect developer intent and find bugs in real world programs.
additionally such an approach must gracefully handle cases with ambiguous or missing documentation or even missing implementations.
we propose a neural approach to infer both exceptional and assertion bug finding test oracles toga .
to address the limitations of existing neural generative methods we propose a new approach that reformulates the oracle generation problem as a ranking over a small set of highly likely possible oracles.
we base our approach on the empirical observation that oracles in developer written unit tests typically follow a small number of common patterns.
we describe a taxonomy on these patterns and define a simple grammar that expresses this taxonomy.
we use this grammar along with type based constraints to restrict the space of candidate oracles and produce well formed test oracles satisfying syntactic and type correctness.
to perform ranking we develop a two step neural ranking procedure using pretrained transformers finetuned to score candidate oracles.
we evaluate our approach on both test oracle inference and bugfinding.
our technique improves accuracy by over existing oracle inference approaches achieving accuracy on a held out test dataset that fits our grammar and constraints and accuracy on an overall assertion benchmark a relative improvement of over existing methods.
furthermore we show that when integrated with a randomized test generation tool evosuite our approach finds real world bugs in java benchmark defects4j .
ourapproach finds bugs that are not found by any other automated testing method in our evaluation.
we provide an open source implementation of toga at contributions.
in summary this paper introduces a transformer neural network based approach to generating both exceptional and assertion oracles without relying on the unit s implementation.
derives adapted datasets for exceptional and assertion oracle training that incorporate method signatures and docstrings.
these datasets are included in our open source release.
implements toga an end to end test generation technique that integrates neural test oracle generation with the automated test generation tool evosuite.
performs an extensive evaluation on test oracle inference.
we demonstrate that our approach improves oracle inference accuracy by and finds real world bugs including bugs that are not found by any other method in our evaluation.
related work we broadly categorize related work on unit test generation into i automated test generation methods ii specification mining methods and iii neural methods.
.
automated test generation tools automated unit test generation techniques use a combination of black box or white box techniques to generate interesting test prefixes for a unit.
for example tools such as randoop use random fuzzing of apis of a unit to construct test prefixes that drives the unit to interesting states.
fuzzers such as afl use fuzzing on the data inputs of a method to derive interesting values to drive a method.
korat performs test generation for data structure inputs based on lazy unfolding of the type structure.
pex performs concolic execution to enumerate paths in a program and synthesize inputs using a constraint solver to derive inputs.
however none of these tools explore the generation of test oraclesto find functional bugs in a unit.
they rely on program crashes from implicit or explicit assertions present in the code or use exception type heuristics to distinguish between desirable and undesirable behavior.
for example null dereferences or out of bounds exceptions may be considered as undesirable.
regression oracles used by tools such as evosuite are intended to find future bugs and assume the unit under test is correctly implemented.
this assumption allows for generating assertions from observed execution behavior.
however expecting a correct implementation is not always a safe assumption.
when the implementation is buggy the regression oracles are incorrect with respect to the intended behavior.
that is regression oracles are incapable of catching nonexceptional bugs introducing false negatives.
consider the example in figure 2a that shows a buggy no op implementation of stack pop.
figure 2b shows a generated unit test with a regression oracle.
the test creates a stack and makes sequential push and pop calls.
since the pop method has a buggy noop implementation the stack will have one element after executing authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
toga a neural method for test oracle generation icse may pittsburgh pa usa class stack public void pop no op ... a buggy implementation.public void testpop stack int s new stack int int a s. push a s. pop bool empty s. isempty assertfalse empty b regression oracle test.public void testpop try stack int s new stack int s. pop pass catch exception e fail assert .
fail c safety oracle test.
figure regression and safety oracles for a buggy pop method.
the regression oracle employed by evosuite assumes that the current behavior is correct resulting in an incorrect oracle asserting that the stack is non empty.
the safety oracle employed by randoop assumes that any non crashing behavior is correct.
as such it results in an incorrect oracle asserting that an exception should notbe raised when calling pop on an empty stack.
correct oracles for pop are shown in figure .
pop.
thus the regression oracle is an incorrect assertion the stack should notbe empty.
similarly qualifying any exceptional output as a bug safety oracle can fail on correctly implemented methods causing false positives e.g.
the intended behavior of calling pop on an empty method is to throw an exception .
figure 2c shows a generated unit test with a safety oracle.
a method that relies on safety oracles will also generate a passing test on the buggy pop implementation.
since pop is implemented as a no op an exception will not be raised when calling pop on an empty stack.
in this case the test oracle is implicit and asserts that an exception will not be thrown.
therefore although automated test generation techniques find numerous non functional bugs and are useful for detecting regression bugs for future code changes they are not a substitute for manually written unit tests documenting intended functionality.
.
specification mining methods specification mining works aim to generate test oracles that accurately reflect the intended behavior as in figure .
unlike randomized test generation methods specification mining approaches do not have any knowledge of the unit s implementation and as such do not require execution.
instead they rely on docstring documentation.
specification mining methods typically define a set of natural language docstring patterns.
these patterns cannot capture all docstrings as program comments can be written flexibly without any necessary syntax or structure.
tcomment defines natural language patterns along with heuristics to infer nullness properties.
however it cannot generalize to other property or exception types.
an example heuristic tcomment employs is generate an expected nullpointerexception oracle if the keyword param has the words null andnot within words of each other.
toradocu uses a combination of pattern lexical and semantic similarity matching.
unlike tcomment toradocu is not limited to nullness properties.
however toradocu can only generate oracles for exceptional behavior.
jdoctor is an extension of toradocu that can also generate assertion oracles.
more recently memo uses equivalence phrases in javadoc comments to infer metamorphic relations e.g.
sum x y sum y x which are also used as test oracles.
these methods can precisely determine oracles when code comments fit their expected patterns but do not generalize when comments fall outside these patterns.lastly c2s generates jml specifications from docstrings.
c2s does not manually define patterns but instead performs a search over jml tokens.
however c2s relies on a developer written test prefix to filter candidate assertions.
c2s has performance improvements over jdoctor in terms of specification synthesis accuracy but does not improve performance in bug finding.
on average real world java projects lack precisely structured docstring documentation.
in our evaluation we show that specification mining methods struggle to infer bug finding oracles for a benchmark of real world java projects.
invariant mining.
there is a long line of work in deriving program invariants for the observed execution behavior of the program.
these include systems such as daikon and dysy which extends the derived program invariants with symbolic execution.
recently evospex combines observed executions with mutations to generate samples of both valid and likely invalid program states and applies a genetic algorithm to infer invariants for method postconditions.
gassert also utilizes an evolutionary approach to make inferred program invariants more accurate and compact.
these approaches can be used to generate specifications and associated test oracles from the inferred invariants but because they are based on the execution symbolic behavior of the current implementation they will generate regression oracles and cannot detect if bugs are already present in the unit under test.
.
neural methods recently neural models have shown promise in generating test oracles and even entire unit tests.
in contrast to specification mining methods neural methods are not tied to hard coded patterns and can generalize to flexibly written docstrings.
furthermore unlike randomized test generation tools neural methods do not necessarily require knowledge or execution of the unit under test.
we refer the reader to codebert for a discussion on the transformer architectures as applied to code.
a transformer like a recurrent neural network maps a sequence of text into a high dimensional representation which can then be decoded to solve downstream tasks.
while not originally designed for code transformers have found many applications in software engineering .
atlas is a neural network based approach to generate assertion oracles.
given a test prefix and the unit under test atlas generates assertions using a recurrent neural network.
atlas relies on the unit s implementation and does not have any knowledge of authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa elizabeth dinella gabriel ryan todd mytkowicz and shuvendu k. lahiri the docstring documentation.
atlas exclusively targets assertion oracle generation and does not attempt to infer any exceptional oracles.
subsequent methods have improved upon atlas by using a transformer based seq2seq architecture pretrained on natural language and code.
a transformer seq2seq model outperforms atlas in terms of inference accuracy.
however in section we show that in combination with a test prefix generator it struggles to find real world bugs in java projects.
lastly athenatest is a transformer model approach to generate entire unit tests including both prefixes and oracles.
athenatest takes as input the unit s context e.g.
surrounding class method signatures etc.
and implementation.
like the previous neural methods it does not have any knowledge of the docstring documentation and relies on the implementation for inferring intended behavior.
structure of an oracle our approach addresses the limitations of existing neural methods by employing a ranking architecture over a set of candidate test oracles rather than a generative model.
in this section we develop a grammar for describing this set of test oracles.
we first describe a taxonomy of commonly occurring oracle structures based on a qualitative investigation of a unit test dataset and then use this taxonomy to inform the construction of our oracle grammar.
we develop a taxonomy of common oracle structures based on unit tests from methods2test a dataset of java unit tests collected from github.
we describe methods2test in section .
unit test oracles typically test either exceptional behavior i.e.
verifying an expected exception is raised or return behavior assertion oracles .
additionally an implicit exception oracle is usually present in tests with assertion oracles.
that is a test with an assertion oracle is not expected to raise an exception.
taxonomy we develop the following taxonomy of oracle usage drawn from our observations of almost 200k developer written tests.
to develop this taxonomy we manually inspected random samples and categorized the most frequently occurring types of oracles we observed.
to ensure that our grammar generalized well and did not overfit to our inspected samples we evaluated the proportion of tests in the dataset that fit the grammar section .
.
expected exception oracles.
expected exception oracles verify that executing the test prefix with some invalid usage raises an exception.
they are most frequently expressed with the following structure try unit .
methodcall invalidinput assert .
fail catch exception e verifyexception e exceptiontype assertion oracles.
assertion oracles verify correct return behavior although they will also fail if any exception is thrown.
we observe several common assertion patterns a boolean assertions.
boolean assertions are used to check some property of the unit under test is true false .
they are typically asserted directly on method return values unit .
methodcall input asserttrue unit .
getstatus b nullness assertions.
nullness assertions usually check the return value of a method call that processes some input.
assertnotnull unit .
processinput input assertnull unit .
processinput invalidinput c equality assertions.
developers typically write equality assertions to check the return value of a single method call.
the return value is usually checked against a constant or literal representing the expected value.
in many cases especially when the unit under test incorporates some data structures the expected value was previously passed as an argument to some method in the test prefix.
string msg foo unit .
sendmessage msg assertequal unit .
getlastmessage msg as we demonstrate in section .
this taxonomy captures a majority of tests of a large dataset of developer written tests .
this coverage could potentially be expanded by including other assertion types e.g.
assertsame however in developing the oracle taxonomy our goal is not to express the entire grammar of java test oracles.
instead we aim to identify a minimal syntactic subset which represents many semantically equivalent oracles.
such a grammar greatly restricts the output space for the oracle generator to consider.
uncommon oracles.
we note several other patterns that occur more rarely including equality assertions on arrays or assertions on multiple method calls as opposed to a method call and a constant .
we also note that there are some assertion patterns that we did not observe in anyunit test although they are often used to express invariants within programs.
these include assertions with logical connectives and assertions with inequality constraints.
test oracle grammar.
based on the taxonomy of common oracle structures we develop a restricted grammar that expresses commonly used test oracles.
test t o p prefix p statement p p oracle o p e p r p except oracle e p try p fail catch exception e return oracle r p p a assertion a assertequals const var expr asserttrue expr assertfalse expr assertnull expr assertnotnull expr intuitively toga is a code generation model for tests that is explicitly designed to exploit the structure of a unit test.
this grammar succinctly describes a set of test oracles that are possible candidates for generation.
in particular given a test prefix p we can synthesize either an exceptional oracle e p or an assertion oracle on the return value of a method r p .
further the assertion oracle can be constructed using one of the five assert constructs when instantiated with the return value and other constants and variables.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
toga a neural method for test oracle generation icse may pittsburgh pa usa in the sections that follow we demonstrate how to i prune this set using type constraints and ii rank the resulting possible test oracles using neural models.
4toga neural test oracle generation in this section we present our approach for inferring test oracles that reflect developer intent.
unlike previous works toga is capable of inferring both exception and assertion oracles.
furthermore toga can handle units with vaguely written or absent docstrings or even absent implementation.
our approach infers test oracles from only a given test prefix and unit context.
unit context may refer to method signature s or a docstring if present .
notably the unit context need not include the unit s implementation.
.
method overview toga depicted in figure contains two key components the exceptional oracle classifier and the assertion oracle ranker.
the exceptional oracle classifier described further in section .
is a pretrained transformer model fine tuned on a binary decision task.
the model decides if an exception should be thrown according to the developer intent conveyed through the unit context.
if the classifier infers that the given test prefix should raise an exception toga has found an exceptional oracle and can now generate a complete test.
the resulting test has the expected exception oracle format shown in section .
otherwise the classifier predicts that the input should not raise an exception and toga continues in the test generation process by invoking the assertion oracle ranker.
the assertion oracle ranker described in section .
similarly uses a pretrained transformer model backbone.
to address the limitations of existing neural assertion generation methods our approach treats oracle inference as a ranking over a small set of possible common oracles.
we base our approach on our observed taxonomy and defined grammar described in section .
we use this grammar along with type based constraints to restrict the space of candidate oracles and enforce syntactic and type correctness.
the model is is fine tuned on ranking the set of candidate assertions given the test prefix and unit context.
each assertion in the set is ranked and the highest ranked candidate is selected as the assertion oracle.
lastly toga generates a test with the given test prefix and the inferred assertion oracle.
.
exceptional oracle classifier as mentioned previously the exceptional oracle classifier is based on a pretrained bert transformer model.
in particular we use the codebert model trained on both natural language and code masked language modelling.
to train the exceptional oracle classifier we fine tune the pretrained model on the task of exceptional oracle inference.
the fine tuning is performed using a supervised datasetd p c l ... p c l n wherepis a test prefix cis a unit context and lis a binary label l .
a label of indicates that the sample should raise an exception while a label of indicates that it should not raise an exception.
methods2test dataset.
our training dataset dis variation of the methods2test dataset we call methods2test .
as the name suggests methods2test is a corpus of unit methods and corresponding developer written unit tests extracted from over 91k open sourcejava projects.
originally created to train athenatest methods2test is structured for the translation task from methods to tests.
we adapt methods2test to our setting of exception oracle inference.
our adapted dataset methods2test has modifications in both the input methods and developer written tests.
the input method s implementation is removed and the method docstring if present is added.
the tests are modified to remove any exception or assertion oracles.
these stripped oracles are used to create binary labels for expected exceptions.
lastly we normalize the test method name to prevent potential information leakage.
for example a test method named testthrowsexception would leak label information to the model.
to remedy this we rename all tests to follow the format testn where n is a positive integer.
in summary methods2test is a supervised dataset for exception oracle inference.
it excludes unit implementation and includes docstrings if present.
our resulting dataset contains a training set of more than labeled samples.
.
assertion oracle ranker the assertion oracle ranker is also based on the pretrained codebert model.
to train the assertion oracle ranker we finetune the pretrained model on the task of assertion oracle inference.
the fine tuning is performed using a supervised dataset d p c a l ... p c a l nwherepis a test prefix cis a unit context ais a candidate assertion and lis a binary label l .
a label of indicates that the given candidate assertion accurately reflects developer intent.
for a given pandconly oneacan have a label of .
the other assertions in the candidate set will have a negative label.
atlas dataset.
our training dataset dis a variant of the atlas dataset .
atlas is a corpus of test case prefixes corresponding method units and assertions.
atlas was collected from 9k open source java projects on github.
we modify atlas to create our variant dataset atlas .
similar to our construction of methods2test we remove the method implementation normalize the test method name and remove the assertion from the test case.
then we generate a set of assertion candidates for each sample and construct our labels to indicate the correct assertion in the set.
our negative samples are also taken from the candidate set of assertions.
in total the resulting atlas dataset contains over labeled p c l samples for supervised training.
.
candidate assertion set generation to generate a candidate set of assertions we use our grammar along with type based constraints to restrict the space of candidate oracles and enforce syntactic and type correctness.
based on the return value of the unit under test we iteratively construct a set of candidate assertions.
our assertion candidate generation algorithm is shown in algorithm .
if the assertion that is being added requires an additional value assertequals our approach draws likely candidates from global and local dictionaries.
global constant dictionary.
the global constant dictionary contains the most frequently occurring constant values in the training data.
our global dictionary contains the top k values of each type.
the use of a global dictionary is inspired by our observation that the vast majority of constants in test asserts are a few common values.
for example over of the integer constants in asserts in authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa elizabeth dinella gabriel ryan todd mytkowicz and shuvendu k. lahiri exceptional oracle classifier exception expected exception not expected test prefix method context assertion oracle ranker figure overall toga framework.
the system takes as input a test prefix and a unit context.
the unit context contains method signature s and docstrings but not the implementation.
it outputs a unit test with an inferred test oracle.
the system has two main components the exceptional oracle classifier and the assertion oracle ranker.
the atlas dataset are one of the top most frequently occurring integer values.
local dictionary.
in addition to the global constant dictionary we also build a local dictionary based on values that appear in the test prefix.
note that these values are not necessarily constants.
variables that appear in the test prefix are also valid local dictionary entries.
the use of a local dictionary is based on the observation that many assertions check against values that were previously passed as arguments to methods called in the test prefix.
at inference time our method makes calls to the assertion oracle ranker for each assertion in the set of candidates.
the model outputs a predicted label along with a confidence score.
we use this confidence score in post processing to select the highest ranked assertion.
the test prefix along with the selected assertion oracle is output as the generated test.
.
end to end evosuite integration we have described a method toga to infer functional test oracles given a test prefix and unit context.
however in order to catch bugs a test prefix that exercises the buggy behavior is necessary.
to obtain a high quality test prefix we use the randomized test generation tool evosuite.
as mentioned in section .
evosuite generates a set of tests guided by coverage.
we extract test prefixes by stripping evosuite s oracles from each test.
in cases where a test contains multiple assertions we extract the test prefix for each assertion individually.
for each of the generated test prefixes we invoke toga to infer a test oracle.
in combination with a large set of prefixes that attempt to cover the entirety of the unit our approach is able to generate functional test oracles that find real world bugs.
when we obtain prefixes from evosuite we assume that prefixes will be written in evosuite s standardized format.
this allows us to identify the variables on which evosuite generated assertions in theextractretval method algorithm .
lastly we apply a confidence threshold to the assertion oracle ranker to suppress low confidence assertions.
in these cases only the exception oracle is applied to the test.
conceptually this allows the model to avoid generating incorrect assertions in cases where the model believes all the candidate assertions are incorrect.algorithm assertion template creation procedure createcandidatetemplates globaldict k test cs template candidates retval extractretval test t type retval localdict createlocaldict test ifretval is an object then cs cs assertnull retval cs cs assertnotnull retval else if retval is a boolean then cs cs asserttrue retval cs cs assertfalse retval forglobalval globaldict.get t do cs cs assertequals globalval retval forlocalval localdict.get t do cs cs assertequals localval retval returncs procedure createlocaldict test localdict forval in getvalue test do loop over all values in prefix localdict val return localdict procedure extractretval test assign getlastline test last line will be an assignment retval getlhs assign return retval evaluation research questions.
we consider the following research questions in our evaluation rq1 istoga s grammar representative of most developer written assertions?
rq2 can toga infer assertions and exceptional behavior with high accuracy?
rq3 can toga catch bugs with low false alarms?
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
toga a neural method for test oracle generation icse may pittsburgh pa usa .
evaluation setup datasets.
our evaluation uses the atlas and methods2test datasets described in sections .
and .
respectively.
for exceptional oracle inference we evaluate on a methods2test held out test set of size .
for assertion oracle inference we evaluate on an atlas held out test set of size .
bug benchmark.
we evaluate real world bug finding on the defects4j benchmark.
defects4j is a benchmark of bugs from real world java projects.
each sample in the benchmark includes both buggy and fixed code versions.
each fixed program version is based on a minimal patch to fix the bug and passes all the project tests while each buggy program version fails at least one test.
each bug is based on an error that was logged in the project s issue tracker involves source code changes and is reproducible i.e.
with a deterministic test .
the benchmark also includes utilities for generating and evaluating test suites on the programs to determine if generated tests pass on the fixed versions and catch bugs.
test environment.
the evaluation was conducted on a linux machine with intel r xeon r e5 v3 cpu .60ghz and 112gb main memory.
as in the defects4j environment we use jdk .
.
rq1 oracle grammar we evaluate rq1 on the original atlas dataset which contains a total of assertions mined from java projects.
to answer rq1 we parse each assertion and check if it can be expressed in the grammar based on the assertion method name and structure of the ast.
after excluding samples that fail to parse we find that can be expressed by our grammar.
of the of assertions that cannot be expressed in our grammar the majority use assertion methods that we do not include e.g.
assertthat assertsame .
in many cases based on a manual inspection of samples the non matching assertions appear to be symbolically equivalent to assertions expressible in our grammar figure .
assertthat counter .
get corematchers .
equalto vs. assertequals counter .
get figure the first assertion highlighted in red cannot be expressed in our grammar.
however the equivalent assertion highlighted in green does fit our grammar.
other assertions that did not match our grammar include equality assertions on expressions rather than variables or literals.
for example assertequals id1.hashcode id2.hashcode although we deliberately exclude generic assertions like these from our grammar we note for a test executing in a deterministic environment an equivalent property could be enforced through a syntactic rewrite.result of the developer written assertions in the atlas dataset are in our grammar and many other assertions are semantically equivalent to assertions expressed in our grammar.
.
rq2 oracle inference accuracy to answer rq2 tables and reports accuracy results on a heldout test set.
we include results for both exceptional and return test oracle inference.
for exceptional oracle inference our experimental setup involves the methods2test dataset described in section .
.
there are no neural techniques for exceptional oracle inference that we are aware of.
instead we include a random baseline weighted coin to illustrate the complexity of the problem space.
the coin performs a random choice weighted on the distribution in our training set.
in our training set we observed that of samples are non exceptional.
as such the coin predicts negative labels frequently and usually correctly but rarely predicts a positive.
the coin performs similarly to our approach in terms of accuracy but significantly worse in terms of f1 score as it rarely predicts a positive label correctly.
for assertion oracle inference our experimental setup involves the atlas dataset described in section .
.
the accuracy metric is syntactic a suggestion is considered correct if it is an exact lexical match.
as a baseline we compare to a sequence to sequence seq2seq return test oracle model .
the seq2seq model is a transformer pre trained on natural language and code with a beam search decoder.
in contrast to our approach which performs ranking over a set of template assertions the seq2seq model generates a test oracle token by token.
as such the model suffers due to the large space of possible oracles.
we report results on two held out test sets an overall set and an in vocab set.
the in vocab set is the subset of the overall set that can be expressed by our grammar and vocabulary based on the local and global dictionaries.
our model achieves accuracy on the in vocab set compared to by the seq2seq model and overall accuracy an relative improvement over the seq2seq model.
result our assertion oracle inference model achieves over accuracy compared to accuracy from existing approaches.
our exceptional inference model achieves accuracy with an f1 score of .
relative to a weighted coin baseline s .
f1 score.
vocabulary size ablation.
we perform an study on k the vocabulary size of our global dictionary to examine the tradeoff between generating a larger number of assertion candidates and ranking the assertion candidates accurately.
figure shows the overall model accuracy percent of samples supported by the vocabulary and accuracy on samples supported by the vocabulary evaluated on the atlas test set.
fork the global dictionary is unused and only variables and constants in the local dictionary are considered the assertion generation.
using only the local dictionary can still generate correct assertion candidates for approximately of the samples in the test set.
increasing kcauses the model accuracy to decline slightly but causes overall accuracy to improve because more correct assertion candidates are generated using the global dictionary.
once the authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa elizabeth dinella gabriel ryan todd mytkowicz and shuvendu k. lahiri appr oach accuracy pr ecision recall f1 score toga model .
.
.
weighted coin .
.
.
table rq2 evaluation of exceptional oracle inference appr oach in v o cab accuracy overall accuracy toga model seq2se q table rq2 evaluation of assertion oracle inference appr oach bugs found tps fpr evosuite gr ound truth evosuite toga ours randoop evosuite seq2seq athenatest evosuite jdoctor .
table rq3 overall bug finding.
athenatest fpr based on projects vocabulary becomes too large however the model accuracy starts to drop off and setting higher ks reduces overall accuracy.
in rq2 we set k based on tuning on the atlas validation set.
this setting achieves the best tradeoff between high model accuracy on the candidate set and supporting a large set of likely assertions.
k0.
.
.
.
.
.0accuracy t emplate ratio overall accuracy matched t emplate accuracy assertions in t emplates figure evaluation of global dictionary size kon overall accuracy.
matched template accuracy indicates model accuracy when the candidate assertion set included the correct assertion.
assertions in templates indicates the percentage of dataset assertions that appear in the candidate assertion set for a given k.exception exception assertion appr oach raised not raised failure evosuite gr ound truth evosuite toga ours randoop evosuite seq2seq athenatest evosuite jdoctor table rq3 number of bugs found by oracle type.
note that some bugs can be detected by multiple oracle types.
.
rq3 bug detection to answer rq3 we run our end to end test generation system integrated with evosuite.
as described in section .
the system uses evosuite to generate test prefixes guided by coverage.
our models are invoked to generate the test oracles.
baselines.
we consider the following baselines in this evaluation randomized test generation.
to represent randomized test generation we run randoop which is a widely used and actively maintained test generation tool used for bug finding.
we also run evosuite as a baseline although evosuite s intended use case for regression testing limits its ability to find bugs present in the program.
we run both randoop and evosuite for minutes per tested program following the procedure used in .
neural test oracle generation.
to test neural methods we compare with a seq2seq transformer finetuned to generate assertions .
we also evaluate against a whole test generation model athenatest .
specification mining.
we use jdoctor s open source implementation to evaluate specification mining approaches.
jdoctor supports exception oracle generation by parsing specific patterns in docstrings .
we integrate the generated oracles with the same evosuite generated tests used by toga in this evaluation.
note that we do not evaluate on c2s because the implementation is not publicly available.
evaluation setting.
we evaluate rq on the defects4j benchmark.
to evaluate the effectiveness of oracles in detecting bugs present in the program the generated tests are run on a buggy version of the unit under test.
we consider a bug is found if a generated test both fails on the buggy program and passes on the fixed program.
since each fixed program is distinguished from the buggy program by a minimal patch fixing the specific bug a test must be failing due to the specific bug if it only fails on the buggy version.
for the oracle generation methods in the evaluation that require a test prefix toga seq2seq jdoctor we evaluate on a set of bugreaching evosuite test prefixes that exercise buggy behavior and therefore can detect a bug given the right test oracle .
we obtain this bug reaching test prefix set by running evosuite with default settings i.e.
coverage guided on the fixed program versions to generate regression tests and then selecting tests that fail the buggy program version indicating they exercise buggy behavior.
we extract these tests prefixes as an evaluation set.
methods evaluated on this set are denoted evosuite method in tables and .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
toga a neural method for test oracle generation icse may pittsburgh pa usa it is important to note that our evaluation setting is fundamentally different from the regression evaluation setting in which the defects4j benchmark is most often used.
in a regression evaluation tests are generated on the fixed program version and evaluated on the buggy version.
regression studies of randomized test generation tools report finding larger numbers of bugs than in our setting as they use regression assumptions to generate higher quality oracles .
in our setting where tests are generated on the buggy program version regression test oracles will not find bugs as they assume the observed buggy behavior is correct.
in addition to evaluating the number of bugs found we use pertest metrics as defined in .
these metrics include false positives to evaluate the performance of an oracle generation method from a usage perspective.
a method that generates many erroneously failing tests will not usable in a realistic application setting where a developer must inspect each failure to determine if they represent real bugs or false alarms.
a failing test is considered a positive while a passing test is a negative .
however a positive does not necessarily indicate that the oracle caught the bug.
a failing test can indicate one of two things true positive the test has a correct oracle and fails due to the buggy implementation.
false positive the test has an incorrect oracle and fails on the correct functionality of the unit in the fixed version.
to distinguish between these cases we run the same test on the unit s fixed version.
if the test fails on the fixed version we can safely assume the test has an incorrect oracle and is a fp.
similarly a passing test can indicate one of two things true negative the test has a correct oracle and is testing correct functionality.
false negative the test has an incorrect oracle and is testing buggy functionality.
again to distinguish between these cases we run the same test on the unit s fixed version.
if the test fails on the fixed version we can safely assume the test has an incorrect oracle and is a fn.
we summarize the meaning of these metrics in figure .
in our evaluation we summarize these metrics in the false positive fpr which represents the rate of incorrectly failing tests on non buggy code.
a high fpr implies that a developer will need to validate many tests that have no utility and thus is a good metric for a bug finding tool.
figure bug finding metrics discussion of rq3 results.
table reports overall bug finding performance.
evosuite ground truth is a measure of evosuite sability to generate bug reaching tests.
these tests were generated from the fixed program versions with regression oracles to obtain ground truth.
we use this to distinguish between evosuite prefix generation performance from test oracle generation performance.
evosuite ground truth detects bugs indicating the best possible performance that any of the oracle generation methods can achieve on the evosuite test prefixes.
toga finds total bugs including that are not found by any other method in our evaluation.
the next best performing method randoop finds bugs but with a much higher false positive rate.
of the two tested neural methods athenatest does not generate any bug finding tests.
the seq2seq model run on evosuite generated test prefixes finds bugs but incurs a higher error rate.
the specification mining tool jdoctor only finds one bug but is the most precise oracle generation method in the evaluation.
table reports a breakdown of bug finding performance on three different bug types unexpected exception raised expected exception not raised and assertion failures.
toga s ability to infer exception oracles correctly is critical to its bug finding performance.
overall of the bugs it finds are exceptional and involve expected exceptions not being raised.
none of the other methods in the evaluation detect any expected exception not raised bugs.
of the other evaluated methods athenatest and jdoctor are both capable of generating expected exception bugs in principle but in practice do not generate any in the evaluation.
for raised unexpected exceptions toga exception model correctly identifies of them are unexpected exceptions.
this demonstrates the value of using a neural model for exception oracle generation which is more flexible than the fixed rules used by a tool like randoop.
toga also identifies assertion bugs.
the only other method in the evaluation to generate assertion oracles that catch bugs is the seq2seq generative model which catches bugs.
this shows that while toga ranking based oracle generation procedure is effective for bug finding its overall performance in bug finding comes from providing a unified method for oracle generation that can detect all three types of bugs.
in contrast none of the methods in the evaluation are successful in generating oracles for more than one type of bug although jdoctor and athenatest can in theory generate oracles for all three classes of bugs.
the athenatest and seq2seq assertion generation models do not effectively find bugs.
this evaluation illustrates the challenges in neural oracle generation.
in practice we found that both athenatest s whole test generation and the seq2seq assertion model generated many tests and oracles that were not executable.
the athenatest authors noted this issue in their evaluation where they found that only of the generated test cases were executable without errors and actively tested the unit under test .
the oracle generation model generated executable oracles and of these we observed that a further were tautologies resulting in an overall yield of potentially meaningful oracles.
in contrast the ranked oracle generation used by toga always generates oracles that are executable and exercise the unit under test.
note that due to the large volume of generated test candidates per tested method that must be individually compiled and run when following the procedure in we estimate the false positive rate of athenatest on five projects and otherwise only generate tests specifically on methods exercising buggy code.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may pittsburgh pa usa elizabeth dinella gabriel ryan todd mytkowicz and shuvendu k. lahiri class keyedvalues public void removevalue int i this .
keys .
remove i this .
values .
remove i bug misses update if i this .
keys .
size rebuildindex public int itemcount return this .
index .
size a buggy implementation.public void testkeyedvalues keyedvalues kv kv new keyedvalues short short new short kv.
insertvalue short0 kv.
removevalue asserts buggy itemcount is correct and misses bug assertequals kv.
itemcount b regression oracle test.public void testkeyedvalues try keyedvalues kv kv new keyedvalues short short new short kv.
insertvalue short0 kv.
removevalue no exception raised catch exception e fail misses bug c safety oracle test.public void testkeyedvalues keyedvalues kv kv new keyedvalues short short new short kv.
insertvalue short0 kv.
removevalue asserts itemcount should be test fails and identifies bug assertequals kv.
itemcount d toga generated oracle.
figure different types of test oracles for a bug in the removevalue method from the java chart project.
the bug causes a data structure to return an incorrect item count when the most recently added item is removed.
although the test input exposes this behavior regression and safety oracles will generate a false negative by passing the buggy behavior either by generating an incorrect assert statement or because the bug does not cause any exceptions to be thrown.
only the oracle generated by toga correctly asserts that itemcount should be after an item is inserted and removed detects the bug.
toga is the only system in our evaluation that correctly identifies this bug.
public void teststack try numberutils .
createnumber 0xt safety oracle catch exception e fail a safety oracle test.public void teststack try numberutils .
createnumber 0xt expected exception fail expecting exception catch exception e verifyexception e numberformatexception b toga generated oracle.
figure generated oracles testing a buggy createnumber method in the java lang project.
the bug prevents a numberformatexception from being raised on an invalid input.
the oracle generated by toga correctly checks that an exception should be raised on the invalid input and fails when no exception is raised due to the bug.
a safety oracle cannot detect the absence of an exception.
toga is the only system in our evaluation that detects this bug.
the specification mining method jdoctor also does not effectively find bugs but it generates oracles precisely.
jdoctor only produces an exceptional test oracle if there is a docstring comment indicating specific behavior.
however on the projects in the defects4j benchmark this approach only succeeds in generating test oracles to catch a single bug.
we observed that in practice many buggy methods either had vaguely worded docstrings or lacked docstrings entirely and jdoctor created few test oracles as a result.
jdoctor s inability to generate sufficient oracles to effectively find bugs illustrates why robustness to vague or missing docstrings is a important requirement for effective oracle generation.
in many cases the bugs detected by toga occurred on methods that lacked docstrings entirely where any specification mining approach would not be able to identify them.
evosuite vs. toga performance finding bugs requires both test prefixes that reach buggy behavior and oracles that correctly identify the bug.
for the oracle generation methods in this evaluation we distinguish the performance of the test prefix generator evosuite by evaluating the generated test prefixes with the ground truth oracles.
out of the bugs in the defects4j benchmark the evosuite generated tests reach bugs.
that is overall evosuite toga misses defect4j bugs due to evosuite not generating reaching test prefixes and bugs due to toga not generating correct oracles.
this result highlights that generating test prefixes to reach buggy code remains a challenging open problem and improving the test prefix generator used with toga could have large impact on bug detection performance.
toga exception oracle error analysis for a single focal method evosuite often generates multiple test cases.
for some focal methods evosuite generates both exceptional and non exceptional input states.
however toga rarely predicts differing exception oracles for the same focal method regardless of input state.
this observation suggests that toga is conditioning primarily on the focal method signature rather than particular input states.
toga assertion oracle error analysis we performed a manual analysis of ground truth oracles and found that of total assertion oracles were predicted correctly.
the remaining predictions can be broken down as follows of the ground truth assertions could not be expressed with the given vocabulary could not be expressed with the grammar and were not predicted because toga incorrectly predicted an exceptional oracle.
in the remaining samples the ground truth oracle could be expressed by the vocab and grammar but the model made the wrong prediction resulting in an in vocab accuracy of on the bug reaching evosuite tests.
this is significantly lower than toga s in vocab accuracy on atlas .
the difference in performance suggests that the distribution of tests in atlas is very different from evosuite s generated tests.
a model trained specifically on evosuite generated test oracle pairs instead of atlas pairs may result in better performance.
result our approach finds bugs in real world java projects of which are not found by any other method in the evaluation.
.
.
case studies.
we consider two case studies of bugs that are detected by toga in our evaluation but not by other methods.
assertion bug case study.
the first case study shown in figure involves a bug in a key value store used in the chart java project.
the buggy method shown in figure 7a contains incorrect logic that prevents the data structure from updating its index when the most recently added item is removed.
this causes the itemcount authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
toga a neural method for test oracle generation icse may pittsburgh pa usa method to return an incorrect count because it bases the item count on the index.
the evosuite generated test for this method shown in figure 7b uses a regression oracle and generates an assertion based on the observed execution behavior.
because the method is buggy this results in an incorrect assertion being generated which not only fails to catch the bug but also could potentially make future detection of the bug more difficult.
figure 7c shows a simplified version of an unexpected exception oracle which is the approach used by randoop in the evaluation.
in contrast to these two approaches toga generates the correct oracle by performing a ranking over a small number of assertions on integers and the return value of kv.itemcount .
this identifies that after calling removevalue the most likely assertion is assertequals kv.itemcount .
expected exception case study.
figure illustrates how toga is able to catch an expected exception bug detected in our evaluation.
the bug in the numberutils.createnumber method of the java lang project prevents the method from correctly detecting invalid inputs and raising an exception.
the exception ranking model predicts that the createnumber 0xt call should raise an exception based on the method signature and context and toga generates an oracle based on this prediction to pass the test if an exception is raised on fail otherwise.
in contrast a safety oracle that checks for unexpected exceptions cannot detect this type of bug where a raised expected is desired behavior.
of the bugs found the toga in the evaluation are expected exception bugs and no other tool in evaluation finds any expected exception bugs.
.
threats to validity we consider three potential sources of bias that could conceivably threaten the validity of our results i test dataset bias ii bug dataset bias and iii bias from evosuite performance.
both of the unit test datasets atlas and methods2test sourced tests from publicly available java projects and filtered their results using heuristics such as github star count and presence of matching focal methods to select tests for inclusion.
bias in these datasets towards specific applications or types of tests may effect the validity of rq1 and rq2.
however we note that these datasets are large sourced from 91k open source java projects in the case of methods2test and therefore likely to be representative of common patterns in java unit testing.
our rq3 bug evaluation dataset defects4j is much smaller with samples from projects due to difficulty in constructing minimal bug samples so bias towards specific applications or bug types is possible.
however defects4j only contains large widely used projects and difficult real world bugs so evaluations on this benchmark are likely to be indicative of real world performance on large software projects.
finally bias in evosuite s test prefix generation is also a potential threat to validity for rq3.
evosuite can only generate bug reaching tests for a fraction of the defects4j bugs out of and may be biased towards classes of bugs that are easier to reach with coverage guided exploration.
limitations grammar and vocabulary toga makes the tradeoff of supporting a restricted set of commonly used oracles but predicting oracles in that set accurately.
a limitation of this approach is that toga can only generate oracles that can be expressed by the grammar and exclusively contain values that appear in the vocabulary.
we conducted a manual analysis of toga predictions in rq3.
when toga did not correctly predict a bug finding assertion in of the cases the assertion value did not appear in the vocabulary and in .
of cases the assertion could not be expressed in the grammar.
for example toga could not predict the following ground truth assertion as the string literal is not contained in either the global or local dictionaries assertequals qdxd 5 q degm string0 while our grammar limitation is strict we found that approaches with unlimited vocabularies also did not correctly predict these oracles.
out of distribution training toga is also limited by its dependence on datasets of primarily developer written unit tests for both training and vocabulary learning.
however the rq3 test set is taken from evosuite an out of distribution sample set.
as a future direction toga could be trained on an evosuite generated dataset for a model that more closely fits an end to end automated testing distribution.
dependencies on evosuite toga assumes a particular structure of the test prefixes generated by evosuite to select the variable to assert on.
however as long as the assertion variable is specified to toga and defined somewhere in the test prefix the test prefix could conceivably have any format.
therefore integrating toga with another test generation method might require integrating a suitable mutation analysis tool such as pit to select variables on which to generate assertions.
conclusion this paper presents toga a neural technique to infer both exception and assertion test oracles from a given test prefix and unit context.
toga is a two step transformer based architecture that is capable of generating oracles for units without implementation or docstrings.
it improves upon generative neural assertion oracle inference techniques by ranking a small set of likely candidate assertions.
when integrated with a random test generation tool evosuite to obtain prefixes toga finds real world bugs out performing existing test oracle inference techniques.
additionally this paper presents two datasets for future work in neural exception and assertion test oracle inference.