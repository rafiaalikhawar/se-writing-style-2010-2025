an empirical study on program failures of deep learning jobs ru zhang microsoft research v ruzha microsoft.comwencong xiao alibaba group wencong.xwc alibaba inc.comhongyu zhang the university of newcastle hongyu.zhang newcastle.edu.au yu liu microsoft research v yucli microsoft.comhaoxiang lin microsoft research haoxlin microsoft.commao yang microsoft research maoyang microsoft.com abstract deep learning has made significant achievements in many application areas.
to train and test models more efficiently enterprise developers submit and run their deep learning programs on a shared multi tenant platform.
however some of the programs fail after a long execution time due to code script defects which reduces the development productivity and wastes expensive resources such as gpu storage and network i o. this paper presents the first comprehensive empirical study on program failures of deep learning jobs.
real failures are collected from a deep learning platform in microsoft .
we manually examine their failure messages and classify them into 20categories.
in addition we identify the common root causes and bug fix solutions on a sample of 400failures.
to better understand the current testing and debugging practices for deep learning we also conduct developer interviews.
our major findings include .
of the failures occur in the interaction with the platform rather than in the execution of code logic mostly due to the discrepancies between local and platform execution environments deep learning specific failures .
are mainly caused by inappropriate model parameters structures and framework api misunderstanding current debugging practices are not efficient for fault localization in many cases and developers need more deep learning specific tools.
based on our findings we further suggest possible research topics and tooling support that could facilitate future deep learning development.
ccs concepts software and its engineering software defect analysis.
keywords deep learning jobs program failures empirical study most of this author s work is done as an intern at microsoft research.
corresponding author.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may seoul republic of korea association for computing machinery.
acm isbn .
.
.
.
reference format ru zhang wencong xiao hongyu zhang yu liu haoxiang lin and mao yang.
.
an empirical study on program failures of deep learning jobs.
in42nd international conference on software engineering icse may seoul republic of korea.
acm new york ny usa pages.
introduction in recent years deep learning dl has rapidly emerged as one of the most successful machine learning techniques.
with the massive growth in computing power and data volume deep learning has been widely applied in various areas such as speech and image recognition natural language processing gaming with reinforcement learning etc.
leading to many significant and exciting results that may previously have seemed out of reach.
to help data scientists train and test their deep learning models enterprises build dedicated platforms such as microsoft azure machine learning amazon sagemaker and google cloud ai that allow multiple developers to submit and execute their deep learning programs.
these platforms are shared multi tenant and equipped with a large number of cpus gpus or new ai accelerators like tpus providing support for a variety of deep learning frameworks such as tensorflow tf pytorch mxnet and cntk .
philly is a similar deep learning platform in microsoft built with widely used open source technologies and typical computing hardware.
every day thousands of deep learning jobs are submitted to philly by tens of research and product teams.
however we found that a noticeable percentage of these jobs threw runtime exceptions due to code or script defects and failed to complete.
such failures especially those happened after a long time of execution led to an expensive waste of shared resources such as gpu storage and network i o. for example a job had been running for hours on nvidia tesla p100 gpus and then triggered a directorynotfound exception due to a wrong path configuration of test images.
furthermore the stochastic nature of deep learning training process could also cause sudden unexpected failures.
therefore understanding the categories and root causes of job failures is very important for improving program quality and saving precious resources.
moreover it can also provide guidance for preventing detecting debugging and fixing of program defects associated with deep learning jobs.
although there have been a number of empirical studies on defects of machine learning deep learning programs there is little work on program failures of deep learning jobs running on a remote shared platform.
the most related research is reported ieee acm 42nd international conference on software engineering icse icse may seoul republic of korea ru zhang wencong xiao hongyu zhang yu liu haoxiang lin and mao yang by zhang et al.
and islam et al.
on deep learning program bugs.
however their subjects are collected from github issues and stack overflow questions which are different in many aspects from the industrial jobs on philly .
another related work by jeon et al.
studies the behavior of deep learning jobs within microsoft.
however their purpose is to understand cluster gpu utilization rather than reducing job failures.
in this paper we conduct the first comprehensive empirical study on program failures and fixes of deep learning jobs.
we study failed jobs submitted by hundreds of developers in microsoft .
those failures are caused by developers code script defects and we exclude the underlying hardware or system failures.
programs may have been tested locally before job submission hence some failures that could be resolved by local testing may not be present in our study.
the purpose of this study is to provide a systematic and generalized understanding on deep learning job failures which could facilitate failure reduction and resource saving in a shared platform.
specifically our study intends to address the following research questions rq1 what types of deep learning job failures are more frequent?
to answer this question we manually examine the failure messages of failed jobs and classify them into categories.
we obtain many findings.
for example .
of the failures occur in the interaction with the platform rather than in the execution of code logic mostly due to the discrepancies between local and platform execution environments.
the detailed results will be reported in section .
rq2 what are the common root causes of failed deep learning jobs?
to answer this question we select a sample of 400failed jobs and manually identify their root causes by examining the source code and associated job scripts.
for some intricate ones we contact the developers for clarification.
we also investigate how developers fixed the faults.
we obtain many findings.
for example deep learning specific failures .
are mainly caused by inappropriate model parameters structures and api misunderstanding.
the detailed results will be reported in section .
rq3 what are the current testing and debugging practices in deep learning programming?
to answer this question we conduct in depth face to face interviews with representative developers from microsoft .
we find that current testing and debugging practices are inefficient in many cases.
the detailed results will be reported in section .
based on our empirical study we also summarize the lessons learned and suggest future tooling support for deep learning testing and debugging.
for example we suggest possible extensions for philly and dl frameworks.
in summary this paper makes the following contributions we perform the first comprehensive study on program failures of deep learning jobs.
we classify failures and manually analyze the root causes of among them.
we point out implications of our findings and suggest possible improvements for the development of deep learning platforms and frameworks.
the rest of the paper is organized as follows.
in section we give an overview of the new deep learning programming paradigm scheduler job queue users distributed storagegpu servers feed datajobs code script datamodelfigure overview of the infrastructure and job life cycle.
and the platform philly .
section describes the study methodology.
section and section present the failure classification root causes and fixes.
section presents our user study on current testing and debugging practices in the production environment.
section discusses the generality of our study and future research work for failure reduction.
we survey related work in section and conclude this paper in section .
background .
deep learning programs deep learning dl is a subfield of machine learning which learns layered data representations called neural networks or models.
developers write dl programs using frameworks such as tensorflow pytorch mxnet or cntk plus toolkit libraries like numpy dltk detectron and fairseq .
python is the most popular programming language while c or java are also used in some cases.
typical dlcode is composed in three logically consecutive stages data pre processing model training validation and model evaluation.
the first stage is usually for input data cleaning and augmentation e.g.
randomly cropping input images for more training data .
next developers construct the model using computational primitives such as basic neural network layers e.g.
cnn activation functions e.g.
relu loss function optimizers e.g.
adam and variable initializers.
the model consists of weight parameters of layers and allows tensors multi dimensional arrays of numerical values to flow across.
training is actually to find the best weight parameters by updating the model iteratively until its learning performance e.g.
loss and accuracy meets the requirements.
validation is executed once every several training iterations to provide timely feedback for decision of hyperparameter e.g.
learning rate number of hidden units tuning or early stop.
in the last evaluation i.e.
testing stage developers quantify the final model performance.
.
deep learning jobs on philly philly is adlplatform in microsoft deployed on multiple physical clusters equipped with different generations of gpus.
every day thousands of jobs from both research and product teams are executed on philly including machine translation e.g.
transformer reading comprehension e.g.
bert object detection e.g.
mask 1160an empirical study on program failures of deep learning jobs icse may seoul republic of korea r cnn gaming e.g.
dqn advertisement e.g.
wide deep etc.
the workflow of job submission and execution in philly is similar to that of microsoft azure machine learning amazon sagemaker and google cloud ai.
figure gives an overview of the job life cycle inphilly .
developers first upload their code scripts and input data to the distributed storage.
then they specify the job configuration such as the numbers of desired processes gpus docker image input output folders and the main python code file.
philly offers docker images of standard deep learning toolchain to establish a hermetic job execution environment.
nevertheless custom docker images are also allowed to suit additional software requirements e.g.
to install dependent python libraries .
multiple jobs with the same program but different hyperparameters could be submitted together in either manual or automated way to search for the best model.
jobs initially wait in queues for scheduling.
once a job is chosen philly instantiates docker containers to execute the launching shell script and later run the main python code file.
the job may fail if a python exception or shell error code is thrown.
philly could automatically re run it for a certain number of times to recover from non deterministic system failures for fault tolerance.
each run instance is called a retry.
in the end the job will enter one of the three terminal states succeeded killed proactive termination by developers and failed.
note that although philly is developed by microsoft it is actually similar in principle to other commonly used industrial dlplatforms.
the hardware system architecture and job submission execution mechanism of philly are widely adopted .
furthermore most dlprograms executed on philly also use the programming paradigm such as python and tensorflow that is common in dl development.
methodology .
subjects we took failed deep learning jobs on philly as our study subjects.
these jobs were submitted by research and product teams in microsoft and had the final status failed .
for each failed job we collected all related information including input data source code job scripts execution logs and runtime statistics for analysis.
failures in our study manifested as unexpected runtime errors that led to job termination.
we did not study semantic errors where a job finished successfully but its results were different from the expectation as we had no test oracles.
we also excluded failed jobs caused by hardware malfunction or system issues because they were beyond the scope of this study.
since developers may have tested their programs locally before job submission some failures that could be resolved by local testing may not be present in the subjects.
to investigate failure classification we chose failed jobs caused by developers code script defects within a three week period in october .
we called this set the full sample set .
due to the retry mechanism of philly a failed job may have several instances and we only considered the last one.
to further understand the root causes and fixes we randomly selected 400out of the total failed jobs and performed detailed analysis of them.
we called this set the small sample set .
.
failure classification job failures in full sample set were manually classified based on their runtime error types.
for each failed job we first located the failure messages by searching the execution logs with keywords such asassert wrong error exception fault fail crash unsuccessful etc.
then we manually inspected all the related log messages around the failures understood the context filtered out the false positives located the actual failure that led to job termination and categorized it.
for example a failure with the message cuda runtime error out of memory is categorized into gpu out of memory instead ofcpu out of memory .
we also grouped all failure categories into major dimensions from the execution point of view deep learning specific execution environment general code error and data.
we discussed the category of each failure in our group meetings until a consensus was reached.
if there was a discrepancy we contacted the job submitter for help.
.
root cause and fix identification we manually studied the source code and scripts of each failed job insmall sample set .
for data related failures additional inspection on the corresponding input data was carried out.
for each failure we then analyzed its occurrence stage and root cause.
we also identified a later fixed version of the job by matching both job and submitter names and examined the bug fixing changes by comparing the failed and fixed versions.
if a complicated fix was beyond our understanding we contacted the job submitter for clarification.
to calculate a failure s runtime cost we simply counted its total gpu service time i.e.
gpu number job execution time .
.
threats to validity threats to internal validity .
due to the complex nature of the work and a large amount of manual effort subjectivity may exist in failure classification and root cause analysis.
to reduce this threat we strived to achieve group consensus before making decisions.
for unclear or difficult job failures we also directly communicated with the developers to seek clarification.
threats to external validity .
our study subjects are all collected from philly .
although there are apparent similarities in the deep learning software stack platform architecture and job management between philly and other deep learning platforms it is possible that some findings might not hold in other platforms or in other companies.
to mitigate this threat in this study we try not to draw too specific conclusions that only pertain to philly andmicrosoft .
in section we discuss findings that could be generalized to other platforms similar to philly.
failure classification in this section we present the classification of job failures in full sample set .
table summarizes the 20failure categories and figure illustrates failure distribution across the dimensions.
.
dimension deep learning specific .
of the total failures are deep learning specific and are divided into categories.
tensor mismatch means that the shape or name of a tensor does not match its expectation.
there are usually three cases interconnected model layers have incompatible tensor 1161icse may seoul republic of korea ru zhang wencong xiao hongyu zhang yu liu haoxiang lin and mao yang dimension category failure description no.
ratio deep learning specificgpu out of memoryinsufficient gpu memory to continue the dl computation434 .
cpu out of memory insufficient main memory .
framework api misuse api usage violates framework assumptions .
tensor mismatchtensor shape or name does not match the expectation57 .
loss nan loss value is not a number .
subtotal .
execution environmentpath not found file or directory cannot be found .
library not foundpython modules or dependent dlls cannot be found on the search path309 .
permission deniedinsufficient permission to perform actions e.g.
installing python packages into system folders .
subtotal .
general code errorillegal argumentargument does not satisfy program or function requirement722 .
type mismatchapplying an operation or function to an object of inappropriate type566 .
key not found accessing collection items with a non existent key .
null reference dereference on null value objects .
attribute not foundreferencing a non existent python class field function etc.
.
syntax error violation of the grammatical rules .
illegal indexaccessing array elements with an out of range or non integer index64 .
undefined variable referencing a variable before its definition .
not implemented functionality is not implemented yet .
division by zero dividing a decimal value by zero .
subtotal .
datacorrupt data exceptional schema or contents in data .
unexpected encoding data cannot be correctly encoded or decoded .
subtotal .
total .
table classification of job failures.
figure distribution of job failures.
requirements input data with unmatched shape is fed to a model a mismatched model file is restored to the constructed model.
gpu out of memory failures occur when the working set exceeds gpus available physical memory.
it is the top category in this dimension.
since gpu memory is relatively limited developers need to size the model very carefully.
cpu out of memory failuresoccur when a job runs out of the main memory e.g.
processing large amounts of data .
loss nan indicates that the calculated loss value becomes undefined or unrepresentable e.g.
log .
the last category is framework api misuse which means that a framework api call violates the inherent assumptions.
for example the tensorflow api session.run assumes that all the variables should have been initialized properly.
otherwise it throws an exception with the message failedpreconditionerror attempting to use uninitialized value .
finding .
of the total failures are deep learning specific.
among the categories gpu out of memory is the largest one accounting for .
of the failures in this dimension.
.
dimension execution environment execution environment related failures occur in the interaction with the platform rather than in the execution of code logic accounting for nearly half .
of the total failures.
it is apparent that the job execution environment provided by a platform is rather different 1162an empirical study on program failures of deep learning jobs icse may seoul republic of korea to that for local development.
for example the required python modules e.g.
fairseq or dependent dlls e.g.
libcudnn.so may not have been pre installed in the docker container then a library not found failure happens.
developers are allowed to make in place modifications to the job execution environment.
however if they do not have sufficient permissions such operations fail and trigger permission denied failures.
for example a developer executes pip install fairseq in the launching script for the required fairseq library.
this command cannot succeed since root permission is required.
she needs to specify the user option so as to install the library into her home directory.
the dominating category in this dimension is path not found .
which means that a nonexistent file or directory is accessed.
for example developers forget to change the local file path in code before job submission.
finding nearly half .
of the total failures occur in the interaction with the platform rather than in the execution of code logic.
among the categories path not found is dominating which accounts for .
of the failures in this dimension.
.
dimension general code error we find that .
of the total failures are common and similar to those in other computer programs.
.
are illegal argument argument not satisfying the requirement and nearly half of them occur while feeding arguments to python code in the launching script.
there are also other categories such as key not found accessing collection items with a non existent key attribute not found referencing a non exist python class field or function null reference and syntax error e.g.
incorrect python indentation .
because python is a dynamic programming language type mismatch failures are rather common .
.
for example concatenation of a posixpath object and a string using the os.path.join function throws an exception with the message typeerror join argument must be str or bytes not posixpath .
finding failures in the general code error dimension account for a large percentage .
of the total failures.
illegal argument andtype mismatch are the top two categories.
.
dimension data failures in this dimension are related to unexpected data that cannot be processed possibly because of unsuccessful uploading or misunderstanding of data properties.
one of the two categories is corrupt data indicating that the data integrity is compromised.
for example a json file which may be accidentally truncated loses the value part in an attribute value pair.
the other is unexpected encoding which means that some data fields cannot be correctly encoded or decoded.
we notice a real program that uses the default ascii encoding to decode a string with some non ascii characters.
finding a number of deep learning jobs .
fail to handle data errors such as corrupt data and unexpected data encoding.
root causes and fixes we conducted root cause and fix analysis on small sample set failures since it required a large amount of manual efforts to study the source code scripts.
we also contacted the developers to seek clarification when necessary.
we have identified 5common root figure distribution of failure categories vertical and root causes horizontal across job failures.
the full name of each root cause abbreviation can be found in the titles of subsections in section .
causes.
figure illustrates the distribution of failure categories and root causes across the 400failures.
in the following subsections we will describe the root causes in detail and demonstrate some real world examples of failed programs and corresponding fixes.
.
differences in environment de although philly establishes a hermetic job execution environment via docker images of standard dltoolchain failures insmall sample set are caused by the discrepancies between local and platform execution environments and account for .
gpu service time.
major differences include environment variables input output paths dependent software versions of python frameworks and toolkit libraries etc.
another significant difference is that processes of a dljob run in docker containers under a new credential therefore the granted permissions are very likely different from those used on the local development machine.
most of these failures are in the execution environment dimension.
the fix of path not found category is to parameterize file folder paths and obtain them from external arguments or configuration files.
in addition developers should verify these paths as early as possible because later failure occurrences e.g.
those in the model evaluation stage will waste a lot of resources.
library not found and part of permission denied installing python packages into system folders failures can be solved by a custom docker image with all desired software pre installed.
sampled syntax error failures are due to the misuse of an early version of python and can be also solved in this way.
in case that the dependent software is manually installed during job initialization developers should specify the user option to the pipcommand and install python packages into their home folders.
the remaining permission denied failures 1163icse may seoul republic of korea ru zhang wencong xiao hongyu zhang yu liu haoxiang lin and mao yang are due to the insufficient permission in operating external files and folders.
the fix is to set up the correct access modes in advance e.g.
executing chmod in the launching shell script and validate them early in code.
finding most failures in the execution environment dimension are caused by the environmental discrepancies between local and platform.
the many discrepancies in installed software storage paths granted permissions etc.
make dl programs error prone.
implication developers are encouraged to use custom docker images with all desired software pre installed modify code to be more environment adaptive and verify paths permissions as early as possible.
.
inappropriate model parameters structures ips all of the .
deep learning specific failures in small sample setare caused by inappropriate model parameters e.g.
learning rate and batch size or model structures e.g.
numbers of layers and hidden units .
.
.
gpu out of memory.
most of the failures are due to overlarge batch size and or overcomplicated model.
figure shows an example.
larger batch size and more sophisticated model may improve the model learning performance however they significantly increase gpu memory consumption.
at present developers largely rely on their domain knowledge to choose the optimal model configuration due to the lack of necessary tooling support.
for example developers must use the memory efficient kernel convolution under certain circumstances.
some on going works are proposed to address out of memory failures.
for example gpu memory consumption is statically estimated for certain computer vision tasks .
another promising technique is gpu memory virtualization where data can be automatically swapped in out between host and gpu when necessary .
tensorflow also has basic gpu memory swapping mechanism i.e.
the swap memory argument which is enabled in certain model layers e.g.
tf.nn.dynamic rnn .
swapping should be implemented efficiently otherwise data latency will slow down the computation.
finding gpu out of memory failures are mostly caused by overlarge batch size and or overcomplicated model.
implication developers should proactively choose the optimal model parameters and structures taking into consideration both available gpu memory and expected learning performance.
estimation and virtualization of gpu memory are two promising techniques.
.
.
tensor mismatch loss nan.
although there are only such failures in small sample set they are very dlspecific and deserve more investigation.
one of the two tensor mismatch failures is caused by variable name mismatch between the saved model file and the constructed model.
after communicating with the developer its root cause was identified to be the different variable naming rules between singlegpu and multi gpu in pytorch.
more specifically the saved model1from keras.models import sequential 2from keras.layers import lstm dense dropout 4batch size 5max context len read dataset by batch using iterator 8input data batch iterator batch size dataset ... model construction 11model sequential 12model.add lstm hidden unit input length max context len ... 13model.add dense activation relu 14model.add dropout .
... 16train model input data figure an example of gpu out of memory failure caused by overlarge batches and overlong input sequences.
batch size and max context len are arguments specified in the configuration file.
the fix is to reduce both of them yet keep the model learning performance acceptable lines .
1import torch 2import torch.nn as nn 4input channels 5out channel 7class discriminator nn.module def init self ... super discriminator self .
init self.main nn.sequential input shape should be nn.conv2d input channels out channel ... ... def forward self input return self.main input data is in format indata data.to device indata data.permute .to device 22netd discriminator ... .to device 23result netd indata figure an example of tensor mismatch failure caused by the wrong shape line of the initial input data.
the fix line is to permute data to the correct shape line .
file is from a multi gpu training job while the constructed model is for single gpu inference.
the model restoration fails to match these variable names.
the fix is using matched variable names in the constructed model or switching to multi gpu inference.
the other failure is demonstrated in figure whose root cause comes from shape mismatch between the initial input data and constructed model.
the fix is to permute data to the correct shape.
loss nan failures root in the stochastic nature of deep learning algorithms.
one of them occurs long after the training validation stage starts.
we contacted the developer for help as it was rather hard to discover the root cause.
the developer told us that the job was to fine tune a pre trained model.
however if such pretrained model has certain problematic parameters the gradient 1164an empirical study on program failures of deep learning jobs icse may seoul republic of korea may lead to a bad training situation and cause the loss to be nan.
a quick fix is to try a new pre trained model from the candidate set.
another possible root cause is the overlarge learning rate whose fix is simply decreasing its value.
the developer also suggested that loss nan was sometimes due to exceptional data in the initial input or the intermediate results.
filtering them out should solve the problem.
loss nan failures are non deterministic and very difficult to reproduce therefore the fix largely depends on the domain knowledge.
.
api misunderstanding apim .
failures are caused by misunderstanding of the complex assumptions made by framework library apis.
figure shows aframework api misuse failure triggered by an incorrect api argument value.
the error message is valueerror variable attweights kernel weights conv already exists disallowed.
did you mean to set reuse true or reuse tf.auto reuse in varscope?
.
since the tensorflow variable kernel weights conv line is shared among several gpus line its parent scope line should be constructed with the parameter reuse explicitly set to tf.auto reuse .
however the developer was unaware of this assumption and forgot to set the reuse parameter.
therefore a default value was used and tensorflow runtime failed to create the variable at the second time.
a few failures are caused by the evolution of internal apis.
the interfaces between dl related components are altered after software upgrade therefore breaking the internal compatibility.
developers may think that their programs will still work since those changes are invisible to them.
figure demonstrates a failed keras job with the error message typeerror softmax got an unexpected keyword argument axis while using layers.dense .
execution logs indicate that keras .
.
is installed in a tensorflow .
docker container.
the developer may have thought that this latest version of keras would be compatible with tensorflow .
.
however keras .
.
upgrades its softmax implementation and requires tensorflow .
or higher.
the fix is to downgrade keras to a compatible version with tensorflow .
.
also to manage complex dependencies among the packages dlplatforms could provide a global package manager and a dependency checker.
finding developers may not fully understand the complex assumptions made by framework library apis due to the rapid evolution of dl related software which results in failures related to framework api misuse illegal argument etc.
such internal compatibility issues are often invisible to the programs.
implication developers need deeper understanding of framework library apis.
a custom docker image could help mitigate part of these failures.
.
exceptional data ed insmall sample set failures are caused by the exceptional data.
figure illustrates such an example.
the developer wants to get the area under the roc receiver operating characteristic curve i.e.
auc in the model evaluation phase .
however there is only a single category existed in the labels variable line which fails to satisfy the multiple category requirement of calculating auc.
proactively writing exceptional data handling code could1config.encode layer 2model custmodel config ... 4class custmodel basemodel def init self config kwargs self.encode layer config.encode layer self.build graph def build graph self with tf.device device gpu define some variables on the first gpu with tf.variable scope encoder self.model encoder def model encoder self gpu id for iinrange self.encode layer ifi gpu id with tf.device device gpu d gpu id output encoder block ... def encoder block ... with tf.variable scope attweights reuse tf.auto reuse var1 tf.get variable kernel weights conv shape dtype ... figure an example of framework api misuse failure caused by an incorrect api argument value.
the fix is to set the parameter reuse to tf.auto reuse explicitly line .
x layers.dense classes activation softmax x in user code def softmax x axis softmax implementation in keras ... return tf.nn.softmax x axis axis a softmax in keras .
.
requires tensorflow .
or higher.
def softmax logits axis none name none dim none b tensorflow .
def softmax logits dim name none c tensorflow .
has different parameters.
figure an example of framework api misuse failure caused by framework evolution.
keras .
.
is mistakenly installed with tensorflow .
.
the fix is to downgrade keras to a compatible version.
help avoid this type of failures.
for example the fault in figure can be fixed with try catch to capture the thrown exception thus avoiding the calculation of auc in erroneous scenarios.
dldevelopers could learn from the distributed data parallel programming practices manual dataset cleaning before submission defensive programming for exceptional data handling and input data sampling for local testing .
more sampling tools are needed which could help developers sample out representative data to not only ensure the distribution but also take some corner 1165icse may seoul republic of korea ru zhang wencong xiao hongyu zhang yu liu haoxiang lin and mao yang 1from sklearn.metrics import roc auc score 3def creat model wide create wide ... deep create wide ... model combine wide deep return model 9model creat model 10for iinrange args.stpes train model auc for category model x test labels read from file ... scores model.predict x test try auc roc auc score labels scores logging.info test roc auc score str auc except valueerror logging.info fail to calculate roc auc score may due to the same category in labels figure an example of corrupt data failure caused by the unexpected data distribution of input data for auc calculation i.e.
all values in labels are the same .
the fix is to catch the thrown exception lines .
cases into account for better testing locally.
ideally the dlplatform should also provide data schema checker to ensure the data correctness automatically before job execution hence improving user experience.
finding the exceptional data handling problem is important for deep learning programming.
dldevelopers could learn from the distributed data parallel programming practices to avoid this problem.
implication the exceptional data handling code could be improved.
dlframeworks could provide dataset apis to handle this problem.
a data validation procedure or tool could be used to sample and check the validity of the data before a full scale training is conducted.
.
common programming errors cpe nearly half .
of the failures in small sample set are caused by common programming errors.
most of them e.g.
the path concatenation example in section .
are easy to understand and have quick fixes by referring to the error messages and failure sites in code.
however a few failures are fairly complex and need thorough examination of the code logic.
figure presents a key not found failure of an nlp job.
the original program is much simplified to ease presentation.
the exception is thrown at line in the model evaluation stage however no clues are found here or in the same function.
after reading the complete source code we notice two hints at lines and which are far away from the original failure site.
it turns out that the developer mistakenly disables beam search at program entrance while the failed code needs beam search.
the fix is to add a check for the beam width variable.
according to our observation dldevelopers often use many parameters arguments in their code and scripts to control the experiments.
however sometimes missing or inappropriate arguments1beam width original code reads it from a program argument generate nantural language output 4def gen text bs infer ... n bs infer.shape hyps dict e for einrange beam width ... gereate text for beamsearch for iinrange n for iiinrange beam width hyps .append get natural bs infer return hyps 14def infer evaluate save data ... for batch indata.batch q iter feed dict inputs batch results model.infer sess feed dict bs infer results batch hyps gen text bs infer ... if beam width bs turn dict question question answer batch hyps figure an example of key not found failure caused by incorrect beam width value.
the fix is to add a check line .
can violate the implicit assumptions in the code and lead to incompatible configurations.
the failure symptoms include illegal argument and type mismatch which dominate the cpe according to our statistics in figure .
a possible solution is to perform formal code review or advanced static code analysis to detect the programming errors earlier.
user study on current testing and debugging practices .
study design to find out why failures of dl jobs happen and how they are resolved we need to better understand the current practices of testing and debugging dl jobs.
more specifically testing practices.
we would like to know whether developers test their programs locally in the same environment as philly .
if not what their current testing environment is.
we would also like to know the challenges developers face when testing dl programs.
debugging practices.
we would like to know how developers currently debug their failed jobs how they detect nondeterministic bugs resulted from the stochastic nature of dl what support they want from the debugging tools and whether a capture and replay tool is useful.
to answer the above questions we conducted in depth face toface interviews with representative developers of microsoft .
all of the interviewees are algorithm engineers or researchers with .
to years of dlexperience.
they work on different products and tasks that are related to gaming computer vision natural language processing speech graphics and advertisement.
table shows the interviewee demographics and their experience in dl.
note that although only developers are chosen the cost of this study is significant as each interview is one on one and lasts hours.
1166an empirical study on program failures of deep learning jobs icse may seoul republic of korea id fielddl exp.
year u1 nlp u2 rl for gaming u3 object detection .
u4 speech generation u5 graphics u6 ad.
click through rate .
table interviewee demographics and their experience.
.
testing practices since philly does not offer a local simulator one challenge is to obtain a testing environment comparable to philly with the same hardware and dl related software .
we first study how developers prepare dltesting from three aspects hardware development environment and input data.
four interviewees u1 u2 u5 u6 had a few gpus on hand.
however such gpus were much smaller in memory and much slower in performance than those on philly .
considering the hardware limitation all interviewees admitted reducing the batch size model complexity and gpu count in local testing.
regarding to the development environment u3 said that he took philly as the testbed due to reasons such as lacking powerful gpus.
this may explain the existence of some simple bugs that could be solved before job submission.
the others had their own local workstations.
they manually set up a native python dlenvironment by referring to the docker images of standard dltoolchain provided by philly .
in addition they simulated environment variables of philly such as the input output directories.
about input data examination three people u1 u4 u6 would use a sampled smaller dataset for testing since the volume of original input data was large.
except u3 they always placed data files on their local machines.
the interviewees also told us a challenge about the test space indltesting.
there are a large number of hyperparameter combinations to test due to the stochastic nature of dl.
for example u2 used to submit a bunch of automl jobs to philly .
however they could only afford testing a very small set of candidates.
furthermore the interviewees described a challenge about testing at different dlphases.
they usually focus more on the correctness of training pipeline e.g.
data reading model construction and serialization .
four interviewees u2 u5 did not pay enough attention to the model validation and evaluation phases.
they may stop testing after the first model checkpoint was saved successfully.
to further investigate failures in different dl phases we divided the execution of dl jobs into phases and analyzed the failure occurrences of small sample set as shown in figure .
the number of failures occurred at the initialization phase accounts for .
of total failures and they only consume .
of total gpu service time.
the number of failures occurred at the model evaluation phase accounts for of total failures but they consume of total gpu service time.
these results show that many failures actually happen in the model validation and evaluation phases which could have been thoroughly tested.
figure the number left and gpu service time right distribution of job failures at different execution phases.
finding the current dltesting practices are often insufficient due to the characteristics of deep learning.
there are three major challenges incomparable testing environment large test space necessity of testing at different dl phases.
implication developers are encouraged to test more cases and all the dlphases.
the local simulator of the platform estimation of gpu memory consumption and test data generator could be useful for dl testing.
.
debugging practices we are also interested in how developers resolve failures.
the six interviewees use conventional code editors e.g.
visual studio code and debugging methods e.g.
logging breakpoints and single stepping for debugging dlprograms.
all of them adopt similar debugging practices for failed jobs on philly they first examine the log files understand the problem context and locate the fault in source code script.
for evident bugs they simply apply fix to code e.g.
correcting the object type data e.g.
replacing with a cleansed dataset and experiment configurations.
for the complex bugs they turn to more debugging methods e.g.
adding code to filter out the exceptional data and resolve them by leveraging domain knowledge e.g.
reducing learning rate for loss nan or framework documents e.g.
the example described in figure .
there are also some dlspecific tools to help debugging such as tensorflow debugger and tensorboard .
developers could use such tools to track the inf nan value and locate the operator that generates it.
there are still some failures that are difficult to reproduce locally therefore developers had to debug them through a trial and error process submitting the modified programs to philly repeatedly and observing the results.
forgpu oom reducing the batch size is often developers first solution to this problem then followed by reducing the network structure complexity.
they also have some domain specific parameters such as the max sequence length in nlp and the image resolution in graphics which can all be used to reduce the model complexity.
forloss nan developers usually try to reproduce it by printing the intermediate results and checking the calculations.
this process is usually without any debugging tool support and involves a lot of time consuming manual work.
1167icse may seoul republic of korea ru zhang wencong xiao hongyu zhang yu liu haoxiang lin and mao yang u1 u2 u3 and u4 mentioned that they wanted a tool to visualize the change of variable values which they thought would be helpful to debug and improve the model learning performance.
all interviewees think that it could be good to provide a replay mechanism which can capture some useful data in the computation process and replay them when there is a failure.
this would help locate bugs that are related to the stochastic nature of dland exceptional data.
finding the current dldebugging practices are not efficient for fault localization in many cases.
existing debugging tools may not work well due to the discrepancies between local and platform execution environments and the stochastic nature of dl.
implication developers need more dlspecific debugging tools.
a good replay mechanism for saving the intermediate results and restoring them at later re run could be helpful for debugging.
a memory usage estimation tool could be complementary to current debugging methods.
discussion .
generality of our study although our study is conducted exclusively in microsoft we believe that our failure categories are prevalent and most of our results can be generalized to other dlplatforms.
the main reason is that most deep learning programs executed on philly use the common programming paradigm e.g.
python language stochastic algorithms frameworks and toolkit libraries and target at common applications e.g.
image video speech recognition and nlp .
besides the hardware system architecture and job submission execution mechanism of philly are widely adopted .
as an example findings and reveal gpu out of memory failures.
it is well known that dlmodels with sophisticated network structures and large batch sizes may improve the model learning performance but also significantly increase memory consumption .
therefore we believe findings and are general to other platforms offering gpus or even other ai accelerators like tpus.
as another example findings and are about environmentrelated failures which are not unusual to philly jobs even when philly establishes a hermetic job execution environment via docker images of standard dltoolchain.
the underlying reason is that such hermetic environment could not always be identical to the local environment developers use.
we believe these findings apply to other platforms too as developers are also likely to make the same mistakes.
for instance the troubleshooting webpage of google cloud ai lists several similar environmental failures1.
.
future research direction based on our study we propose the following future research work platform improvement.
avoiding unnecessary retries.
automatic retry is a common design in various platforms mainly for recovery from non deterministic system failures.
developers can also leverage this mechanism for non deterministic deep learning specific failures e.g.
loss nan .
however if a failure roots in the dl program deterministically e.g.
a non existent data folder re running the job multiple times is apparently unnecessary and would simply waste resources.
the platform could design more refined heuristics to infer whether a failed job deserves retry and avoid the unnecessary ones.
local simulator.
from findings and we can see that unawareness of the environmental differences results in nearly half of the failures.
therefore it is desirable to provide a platform simulator for local development.
the local simulator can behave like a single node implementation of the platform which exports the same environment variables accesses the distributed storage and emulates one or more gpu devices.
tool support.
estimation of gpu memory consumption.
it is rather challenging to debug and fix gpu out of memory failures.
to choose good model parameters structures that satisfy memory consumption requirements developers largely depend on their domain knowledge or adopt a trial and error strategy i.e.
submitting several jobs with different model configurations .
an estimator can be developed to infer the upper bound of gpu memory consumption based on features such as current data distribution model structure and batch size.
a prediction model can be built by analyzing source code and historical gpu out of memory failures.
such tools could help developers better estimate memory usage of their dlprograms and reduce out of memory failures.
static program analysis.
static program analysis is a powerful technique to detect code defects without actually running the programs.
it will have great potential to proactively handle many kinds of job failures including framework api misuse tensor mismatch and those in the general code error dimension etc.
performing whole program analysis across function calls can reduce the false alarms.
data synthesis.
motivated by finding it is useful to develop a data synthesis tool for testing the robustness of data processing logic by extending symbolic execution techniques .
this tool could generate special datasets conforming to the initial input distribution to trigger potential bugs.
framework improvement.
automatic gpu memory management.
although dlframeworks hide most complexity of gpus developers still need manual gpu memory management for data placement and consumption control.
frameworks could provide an automatic gpu memory management mechanism like what oses have done to the main memory.
for example cold data on gpu can be automatically swapped out to avoid gpu memory exhaustion.
another example is that the mechanism could prefetch the input data and model partitions used in the upcoming computations.
replay.
debugging loss nan and other non deterministic failures is challenging since it is hard to reproduce the same failing execution.
frameworks could use existing replay techniques to record the precise execution sequence and interaction with the environment.
hence when a developer wants to track down a failure she can deterministically replay to the faulty state which can greatly facilitate fault diagnosis.
1168an empirical study on program failures of deep learning jobs icse may seoul republic of korea related work machine learning deep learning bugs.
thung et al.
and sun et al.
performed empirical analysis of bugs in machine learning systems.
such system defects can also lead to job failures however our study focuses on program script bugs which are written by dl application developers.
zhang et al.
conducted a research on github and stackoverflow to study the tensorflow program bugs.
islam et al.
extended the work to cover more dlframeworks.
some of the bugs they found e.g.
bugs due to api evolution are consistent with our observation.
our study analyzes real world industrial dljobs with multiple dlframeworks e.g.
tensorflow pytorch and cntk .
we found many different types of bugs occurred in a cloud based dlplatform which could not be easily discovered by the developers through local testing.
empirical study.
there are some empirical studies on the faults ofbig data platforms and software systems .
for example xiao et al.
studied non deterministic bugs in sql like mapreduce programs.
li et al.
analyzed code data defects in distributed data parallel programs.
zhou et al.
performed an empirical study on service quality issues of a production big data computing platform which has a variety types of issues including hardware failures and user errors.
zhang analyzed the distribution of faults in large scale software systems.
our study concentrates more on program failures of production deep learning jobs.
cluster infrastructure.
much research aims to improve the gpu cluster utilization for deep learning by studying the unique job characteristics.
jeon et al.
pioneered in the understanding of low gpu utilization of a multi tenant cluster.
such low utilization comes from gang scheduling resource locality and job failures.
those job failures distribute across infrastructure dlframework and user code.
apart from classifying the failure symptoms our work studies dlprogram failures and presents a deep analysis of their root causes.
testing.
recently there is a large amount of work on testing dl models.
for example deepxplore proposes a new metric to measure the test coverage of a deep neural network.
deeptest further introduces a domain specific automated testing tool to maximize the neuron coverage through realistic transformations over the image data.
tfx builds a machine learning pipeline that eases user efforts in deployment which includes a set of system components for data analysis transformation and validation.
therefore the platform can significantly reduce training failures and improve model quality.
our work provides a systematic analysis of program failures of dljobs which can help the community design better more practical testing and debugging toolkits for dlprogramming.
conclusion like other computer programs deep learning programs could also fail to execute.
in this paper we describe the first empirical study on program failures of deep learning jobs.
we manually examine the failure messages collected from failed jobs in microsoft and classify them into 20categories.
in addition we identify the common root causes and bug fix solutions on a sample of 400failures.
to better understand the current testing and debugging practices for deep learning programs we also conduct developer interviews.based on our findings we suggest possible research topics and tool support that could facilitate deep learning training and testing.
we believe our work could help understand the quality issues of deep learning programs and provide valuable guidelines for future deep learning development.
acknowledgment we would like to thank all interviewees participated in our user study.
hongyu zhang would like to acknowledge the support of arc dp200102940.