ignorance and prejudice in software fairness jie m. zhang university college london london uk jie.zhang ucl.ac.ukmark harman university college london london uk mark.harman ucl.ac.uk abstract machine learning software can be unfair when making human related decisions having prejudices over certain groups of people.
existing work primarily focuses on proposing fairness metrics and presenting fairness improvement approaches.
it remains unclear how key aspect of any machine learning system such as feature set and training data affect fairness.
this paper presents results from a comprehensive study that addresses this problem.
we find that enlarging the feature set plays a significant role in fairness with an average effect rate of .
importantly and contrary to widely held beliefs that greater fairness often corresponds to lower accuracy our findings reveal that an enlarged feature set has both higher accuracy and fairness.
perhaps also surprisingly we find that a larger training data does nothelp to improve fairness.
our results suggest a larger training data set has more unfairness than a smaller one when feature sets are insufficient an important cautionary finding for practising software engineers.
index terms software fairness machine learning fairness i. i ntroduction prejudice is the child of ignorance.
william hazlitt on prejudice machine learning software has become an inseparable part of our daily lives.
it is widely adopted to make decisions such as to select job applicants to evaluate employees performance to predict recidivism to predict credit risks and to predict medical treatment.
machine learning tends to learn what human features and data teach it.
however humans may have bias over cognition further affecting the data collected or labelled and the algorithm designed leading to unfairness in machine learning software.
the unfairness adversely affects the benefit of people in minority groups or historically disadvantageous groups.
it may also lead to consequences for software engineering if software run afoul of laws against discrimination such as the civil rights act .
recently fairness in machine learning software has drawn substantial attention in the software engineering community.
for example brun and meliou mentioned that numerous software engineering challenges in the areas of requirements specification design testing and verification need to be tackled to solve this problem .
chakraborty et al.
said it is the ethical duty of software engineers to strive to reduce software discrimination.
zhang et al.
described fairness as a non functional property for machine learning software that deserves substantial testing effort from softwaredevelopers.
much progress has been achieved in the direction of software engineering for machine learning such as test generation for detecting fairness violations training data mutation for locating the unfairness1 and empirical studies to understand the effectiveness and efficiency of existing fairness improvement methods .
this paper presents a large scale study on the impact of the size of feature set and training data on machine learning fairness ml fairness which means the fairness of machine learning software .
the findings will provide implications for software developers and machine learning practitioners for building fairer machine learning software.
our study is inspired by two facts.
first the size of feature set and training data set are well acknowledged critical practices to optimise ml software .
the impact of these factors on model performance e.g.
test accuracy has been well studied in the literature.
nevertheless their critical role in performance improvement might be so well known that people may have ignored to study their roles in fairness improvement.
second among the studies of human prejudices in the social psychology domain it is recognised that knowledge enhancing is an effective way to reduce human prejudice while older adults with more experiences have a tendency to be more prejudiced than their younger counterparts .
thus we are curious whether the amount of features analogous to the knowledge level of the model and training data analogous to the experience level of the model of machine learning exhibit similar impact on its fairness.
we conduct our study with five widely explored datasets in the fairness literature and four widely studied fairness metrics.
we investigate the impact of the feature set size and training data size on ml fairness separately.
we also check the coupling effect between these two aspects as well as how data balance condition and fairness improvement methods affect our findings.
we include a discussion of the relationship between human and ml prejudices and provide practical suggestions to developers for building fairer machine learning software based on our findings.
our study reveals the following interesting findings feature set size has a notable impact on ml fairness with an average change rate of across our evaluation subjects.
perhaps surprisingly we do not observe that a larger training 1in this paper we use unfairness and bias interactively to refer to the opposite of fairness.
ieee acm 43rd international conference on software engineering icse .
ieee data has greater fairness.
indeed we found that in of the cases a larger set of training data even has greater unfairness than a smaller set.
the negative impact of a larger training data is more pernicious when the feature set size is small.
fairness improvement methods are effective in reducing the negative impact brought by a larger training data set.
fairness is naturally a domain specific problem but in this paper we show that there are crosscutting generic fairness drivers in the size of the feature set and the amount of training data.
these are two key dimensions for the design of any machine learning software.
therefore thereby we can provide general principles to help software engineers to improve the fairness of their systems.
to conclude this paper makes the following contributions a systematic empirical study on the impact of enlarging feature set and training data set when building fair machine learning software.
implications on the impact of feature and training data size for building fairer ml models.
the rest of the paper is organised as follows.
section ii introduces the preliminaries.
section iii provides the details for our experimental setup.
results and analysis are presented in section iv.
section v discusses the findings implications and actionable conclusions.
section vi introduces the related work.
section vii concludes.
ii.
p reliminaries this section provides the preliminaries including the definitions terms and metrics in ml fairness in section ii a as well as the current progress in software engineering for fairness in section ii b .
a. fairness definitions and metrics definitions on fairness.
machine learning is a widelyadopted statistical method that aids decision making such as income prediction and medical treatment prediction.
during the process of these critical decisions the characteristics that are sensitive and need to be protected against unfairness are called protected attributes also called protected characteristics or sensitive attributes .
examples of legally recognised protected attributes include race sex age pregnancy familial status disability status and so on.
such protected attributes are not universal but application specific.
protected attributes partition a population into different sub groups the privileged group andunprivileged groups where unprivileged group members are often at systematic disadvantage.
for example when predicting income sexis a protected attribute.
the predictive model may favour male groups over female groups where the male group is the privileged group the female group is the unprivileged group.
machine learning fairness is defined in terms of protected attributes and privileged unprivileged groups.
there are several types of machine learning fairness definitions proposed in the literature .
fairness through unawareness ftu means that an algorithm is deemed to be fair if the sensitiveattributes are not explicitly used in the decision making process .
another type of fairness is group fairness .
a model has group fairness when privileged groups and unprivileged groups are treated equally e.g.
have an equal probability of decision outcomes or predictive performances .
there is also individual fairness .
a model with individual fairness should give similar predictive results among similar individuals.
in this paper we focus on group fairness due to the following reasons.
first group fairness is more widely adopted and studied in the literature .
second group fairness has well defined and acknowledged mathematical fairness metrics that measure fairness quantitatively.
third group fairness aligns better with legal regulations on fairness .
group fairness metrics studied in this paper.
this section introduces the most popular fairness metrics for group fairness.
let xbe the quantified features of a sample.
let a2f0 1gbe a binary protected attribute.
for unprivileged group a .cis the predictive outcome.
yis the original label with y being the favourable one2.
statistical parity difference also called demographic parity difference is one of the most well known criteria for fairness .
it is the difference between the acceptance rates of the applicants from the privileged and unprivileged groups p p average absolute odds difference is the average of difference in false positive rate and true positive rate for unprivileged and privileged groups jp p j jp p j equal opportunity difference is the true positive rate difference between unprivileged and privileged groups p p disparate impact is the ratio of the acceptance rate of the unprivileged group applicants against that of the privileged group applicants p p among these metrics disparate impact suggests the greatest fairness when it equals .
the remaining metrics suggest the greatest fairness when they equal .
for ease of presentation and observation we turn all the values into their absolute values.
for disparate impact we normalise it to be between and .
in this way for all the fairness metrics larger metric values indicate more bias.
2favourable label is a label whose value corresponds to an outcome that provides an advantage to the recipient 1437b.
software engineering for fairness violations of fairness regulations are regarded as fairness bugs in the software engineering community.
as early as finkelstein et al.
used multi objective search based methods to aid optimising software fairness in requirement engineering.
brun and meliou mentioned that software fairness is analogous to software quality and that numerous software engineering challenges in the areas of requirements specification design testing and verification need to be tackled to solve this problem .
chakraborty et al.
claim that software bias detection and mitigation should be included in the software life cycle.
in the recent survey on machine learning testing fairness is classified as a non functional property that merits significant testing effort from developers.
there have been numerous successful applications of software testing methodology and techniques for fairness improvement.
for example galhotra et al.
proposed themis which uses random test generation techniques to evaluate the degree of fairness.
udeshi et al.
proposed aequitas which first randomly samples the input space to discover the presence of discriminatory inputs then searches the neighbourhood of these inputs to find more of them.
agarwal et al.
used symbolic execution together with local explainability to generate test inputs.
tramer et al.
presented a comprehensive testing tool aiming to help developers test and debug fairness bugs with an easily interpretable bug report.
there are also empirical studies in software engineering seeking to understand software fairness and to get practical implications for developers.
chakraborty et al.
studied whether fairness improvement methods damage model prediction performance as well as the efficiency of fairness improvement methods.
sharma and wehrheim studied the causes of unfairness via checking whether the algorithm under test is sensitive to training data mutations.
biswas and rajan conducted an extensive study on the effectiveness and efficiency of existing bias mitigation methods.
in this paper as in previous work we regard ml fairness as a type of non functional software property.
in the software life cycle the traditional roles of feature set elicitation and training data extension are well known and reported upon.
however no previous work has studied whether the two factors have a role to play in the construction of fair software.
iii.
e xperimental setup a. research questions the evaluation answers the following research questions.
rq1 how does the feature set size affect ml fairness?
rq2 how does the training data size affect ml fairness?
to answer the first question we use the full set of training data but build different models with gradually increased size feature set.
to answer the second question we use the full set of features but build different models with different sized random data samples.
under each research question we first use visualisation to answer the top level question.
we then design sub questionsto deep dive into the results with further statistical analysis more details in section iii e .
rq3 what is the coupling effect between feature set size and training data size on ml fairness?
different from the first two research questions this question investigates the coupling effect of the feature set size and training data size.
the purpose is to investigate how these two aspects collectively impact fairness thereby revealing any important interactions between them.
to answer this question for each of the different sized datasets we build models with different sized feature sets.
we use 3d surface plots to visualise the changes of fairness in which two dimensions are feature set size and data size respectively.
we design further analysis to seek for practical solutions for building software with greater fairness and better accuracy.
the analysis is presented in section v. b. datasets in this paper we use five datasets as listed in table i. the first four datasets are the most widely adopted in the literature of machine learning fairness research and software engineering for fairness .
the fifth dataset is implemented by the ibm fairness tool aif360 in their tutorial as a representative fairness dataset.
table i fairness datasets used in this paper name abbr .
features protectedattributes size adult income adult sex race bank marketing bank age compas score compas sex race german credit german sex age medical survey meps race below briefly introduces each dataset adult a dataset built to predict whether income exceeds 50k yr based on census data.
bank a dataset related with direct marketing campaigns phone calls of a portuguese banking institution.
the goal is to predict whether the client will subscribe a term deposit.
german a data set used to predict people s credit risk levels.
compas this dataset is used to assess the likelihood that a criminal defendant will re offend.
meps this dataset consists of data on the cost and use of health care and health insurance coverage across the united states.
each dataset has its protected attribute s which are determined by its provider depending on specific tasks.
for simplicity of exposition we consider just one protected attribute each time following previous work .
this leads to eight dataset attribute pairs e.g.
adult sex adult race bank age .
these eight pairs are the evaluation subjects for this paper.
c. fairness metrics most fairness papers adopt one or two metrics .
in this paper we use all the four fairness metrics introduced in section ii a2 to measure fairness.
the consistency in the observations with different metrics will increase our confidence in getting answers for the research questions.
1438as introduced in section ii a2 the metrics have different ranges and fairness optimal values.
for ease of observation we turn all the values into their absolute values.
for disparate impact we calculate its distance to one and normalise the distance to be between to .
in this way all the fairness metrics are positive.
larger metric values indicate more bias.
d. fairness improvement methods fairness improvement methods are used for the deep dive into our conclusions as an exploration for practical suggestions and solutions if necessary.
there are three types of fairness improvement methods in the literature pre processing to transform the training data so that the underlying bias in the data is removed or reduced in processing to modify and change the learning algorithms in order to optimise fairness during the model training process post processing to postprocess the prediction results after model training and predictions.
this paper studies the impact of feature set size and training data size on building fair machine learning models which has no intersection with post processing methods.
thus we only consider pre processing and in processing methods.
in particular for pre processing we use reweighing .
this technique weights the samples in each group differently to ensure fairness before classification.
for in processing we useprejudice remover which adds a discrimination aware regularisation term to the learning objective.
in addition the imbalance of samples in privileged and unprivileged groups are also regarded as a cause for ml unfairness .
thus we also explore the impact of the training data size with balanced samples.
e. analysis approaches we demonstrate the changes of fairness metric values with different feature training sets in the following three ways.
first we visualise the changes of the mean metric values as well as the standard deviation within the runs across different feature training set size to answer the top level research questions.
larger values represent more bias in the models.
for rq1 and rq2 we use line plots so as to observe the trend of changes.
for rq3 we use 3d surface heat plots.
second we use one way analysis of variance anov a to deep dive into the statistical significance in the fairness differences among the models built with different feature data sets.
we report the f statistics the ratio of variation between sample means against the variation within the samples the pvalues with a significance level of .
and the tukey honest significant differences tukeyhsd it reports the statistical significance in the differences between each two group .
third we report the absolute changes in the mean metric values and relative changes the change ratios between the minimum and maximum feature set size data size to further investigate the fairness changes.
we only report the changes that have been determined as statistically significant by anov a analysis.f .
experimental details by default we show the results of fairness changes with models built from decision trees which are widely used in industry due to their high efficiency and interpretability .
we use another three widely used models i.e.
logistic regression random forests adaboost.
results are on our homepage to check whether our conclusions are model dependent.
for each dataset we split the data into training data and test data.
when investigating the impact of feature set size we conduct feature sampling on both training data and test data.
we start from a minimum feature set of three that contains the protected attribute to ensure a reasonably effective machine learning model.
we then gradually augment features following the default feature order in the aif360 implementation so as to obtain different sizes of feature sets3.
when investigating the impact of training data size for the training data we randomly select different size subsets with a proportion of ... to build different models.
due to the instability of machine learning fairness metric values we repeat each prediction process times.
this amount of iterations also allows us sufficient data to conduct the one way anov a analysis.
iv.
r esults a. rq1 impact of feature set size on ml fairness the first research question investigates the influence of feature set size on ml fairness.
to answer this question for each dataset we leave its training data size untouched i.e.
the default of the full dataset while gradually augment its features one by one.
figure shows the visualisation of the arithmetic mean fairness values y axis with different feature set size x axis .
we observe that most of the fairness metrics exhibit notable increases in fairness when feature set size increases.
for the german sex dataset all the metric values remain unvaried when the number of features increases.
this is because most metric values show good fairness close to even when there are only three features thus there is not much bias to mitigate.
these observations reveal that a larger feature set exhibits a notable positive influence on the fairness of machine learning models.
we next take a deep dive into the results with one way anov a analysis and absolute relative changes analysis.
rq1.
what is the statistical significance of the fairness changes among different feature sets?
we use oneway anov a analysis to answer this question.
for a datasetattribute pair and a fairness metric each feature set size corresponds to a group with values coming from the runs .
we checked the data and confirmed that they meet the assumptions for one way anov a analysis.
for the tukeyhsd results when there are ngroups there would be all together n difference significance results between each two groups.
3section v a discusses this threat with the default feature order.
.
.
.
statistical parity average abs odds equal opportunity disparate impact .
.
.
.
.
adult race .
.
.
.
.
adult sex .
.
.
.
.
bank age .
.
.
.
.
compas race .
.
.
.
.
compas sex .
.
.
.
.
german age .
.
.
.
.
german sex .
.
.
.
.
meps racefig.
rq1 visualisation of the impact of feature set size on fairness.
each line represents the arithmetic mean values for a fairness metric.
the shadows represent the standard deviation across multiple runs.
we observe that when the number of features increases the metric values tend to draw closer to .
this observation indicates that the size of the feature set has a notable influence on fairness.
for ease of observation we report the percentage of the results with significant p values among all the combinations.
table ii shows the results.
as shown by table of the cases .
exhibit p values smaller than .
.
the fstatistic values the percentage of variation between sample means against the variation within the samples are often much larger than one with .
of cases larger than .
these observations suggest that the fairness changes brought by a larger feature set are significant.
table ii rq1.
one way anov a analysis results for fairness differences among different sized feature sets.
most differences are significant with f statistics values larger than p values smaller than .
and large percentages of significant tukeyhsd results.
statistical parity average abs odds equal oppo.
disparate impact f p hsd f p hsd f p hsd f p hsd adult se x adult race .
german sex german age .
.
.
.
compas race compas sex 3e29 2e29 e29 2e29 bank age meps race f f statistics p p value hsd the percentage of group differences with significant tukeyhsd p values.
p value smaller than .
p value between .
and .
p value between .
and .
.
rq1.
what are the absolute and relative fairness changes when changing the feature set size?
we calculate the absolute and relative changes in fairness metric values between the smallest and the largest feature sets.
table iii shows the results.
the fairness for the cells marked with is considered to be unchanged because the changes are statistically insignificant in the anov a analysis.
light grey dark grey cells are those whose values are increased decreased respectively.we observe that among the significantly changed metric values .
are increased .
have a change rate larger than .
for the german sex dataset there are two decreased metric values but the decreases are minor and are likely caused by randomness.
if we treat the changes in the blank cells as zero the average absolute change in fairness metric values is .
the average change rate is .
.
these observations indicate that the difference of fairness between different sized feature sets are notable.
table iii rq1.
absolute and relative fairness changes in brackets when changing the size of feature sets.
cells marked with denote statistically insignificant changes according to the anov a analysis .
positive negative values indicate increased decreased fairness.
most changes .
have a change rate of over indicating that adding more features may considerably improve ml fairness.
dataset statistical parity average abs odds equal opportunity disparate impact adult se x .
.
.
.
adult race .
.
.
german sex .
.
.
.
german age compas race .
.
.
.
compas sex .
.
.
.
bank age .
.
.
.
meps race .
.
.
.
average .
.
overall our observations lead to the following conclusion for the first research question answer torq1 richer feature sets exhibit a notable positive influence on the fairness of machine learning models.
the average fairness change rate is .
across our evaluation subjects when the feature set size increases.
this highlights the importance of feature enrichment for building fair ml models.
these observations may be due to the fact that more features bring e xtra information for the model to make fairer decisions.
the strong connection between the protected attribute and the labels is one cause for unf airness .
with more features such connection will be weakened.
during the process of changing feature set a new feature may contain information that has a strong correlation with the protected attrib ute thereby allo wing e xtra unf airness to creep into via correlation.
from figure we only observ e that forgerman a ge the bias increases a bit when augmenting the fifth and sixth feature which are the percentage of investment against income and the length of current residence respectively.these tw o features have connections with the protected attribute age.
however as we observe including other features later on eliminates the extra bias brought by these twofeatures.
b. rq2 impact of training data size on ml f airness toget the answer to rq2 we use each dataset s full set of features while adjusting the training data size ratio from .
1440to .
with a step size of .
.
the other experimental settings are the same as those for rq1.
figure shows the visualisation results.
by sharp contrast to the observations for rq1 we do not observe a unified pattern on the impact of training data size.
all but one metric remain almost unvaried over different training data sizes.
for disparate impact onadult sex andmeps race the metric value increases together with the training data size increases.
these observations suggest that unlike the richness of feature set more information in terms of training data is not able to increase ml fairness.
on the contrary with a larger set of training data the ml fairness may even get worse.
.
.
.
statistical parity average abs odds equal opportunity disparate impact .
.
.
.
.
.
.
.
.
.
adult race .
.
.
.
.
.
.
.
.
.
adult sex .
.
.
.
.
.
.
.
.
.
bank age .
.
.
.
.
.
.
.
.
.
compas race .
.
.
.
.
.
.
.
.
.
compas sex .
.
.
.
.
.
.
.
.
.
german age .
.
.
.
.
.
.
.
.
.
german sex .
.
.
.
.
.
.
.
.
.
meps race fig.
rq2 visualisation of the impact of training data size on fairness.
most fairness metrics remain unchanged over different training data size with several increase with more data.
this suggests that a larger set of training data does not have better ml fairness than a smaller one.
rq2.
what is the statistical significance of the fairness changes among different training data sets?
table iv shows the one way anov a analysis results for the fairness of ml models built with different training data sizes.
unlike the results in table iii we observe out of the cases .
exhibit insignificant changes i.e.
with p values larger than .
for different training data sizes.
in addition the fstatistics values are much smaller than those for feature set size impact.
the proportions of significant tukeyhsd values are also smaller.
these observations suggest that for most of the time with the full set of features the fairness changes brought by a larger set of training data are insignificant.
rq2.
what are the absolute and relative fairness changes when changing the size of the training data set?
the absolute and relative changes of fairness metric values are shown by table v. we observe that out of the cases .
have decreased fairness.
together with the cases with insignificant changes this means that for .
cases a larger set of training data does not bring significant improvement in fairness or even decreases fairness compared to a smaller set.
note that this conclusion is obtained ontable iv rq2.
one way anov a analysis for fairness differences among different sized training data.
most changes are in significant with f statistics values equal to or smaller than p values larger than .
and small percentages of significant tukeyhsd results.
statistical average abs odds equal opp.
disparate impact f p hsd f p hsd f p hsd f p hsd adult se x .
.
.
adult race .
.
german sex .
.
.
.
german age .
.
.
.
compas race .
compas sex .
bank age .
.
.
meps race .
.
.
f f statistics p p value hsd the percentage of group differences with significant tukeyhsd p values.
p value smaller than .
p value between .
and .
p value between .
and .
.
thefull set of features.
in section iv c we will show more negative impact from a larger set of training data when there are fewer features.
table v rq2.
absolute and relative changes in brackets in fairness metrics when changing training data size.
with a larger set of training data there is a decrease of fairness for metric statistical anddisparate .
dataset statistical parity average abs odds equal opportunity disparate impact adult se x .
adult race .
.
german sex german age compas race .
.
.
compas sex .
.
.
bank age .
meps race .
average .
.
the outcome for rq2.
and rq2.
is surprising given the intuition that insufficient data can be a source of unfairness .
to further explore the reason we first check the bias in the original training data.
rq2.
what is the unfairness in the training data?
can it explain our observations?
we use statistical parity and disparate impact to show data bias.
the remaining two metrics require both the original and the predicted label thus are not applicable for measuring data bias.
table vi shows the data bias results in the full training data set4.
remember that from figure for adult sex and meps race their disparate impact bias increases to around .
and .
respectively.
these two values are very close to the corresponding disparate impact data bias shown by table vi.
thus we suspect that a larger set of training data may allow the model to learn training data bias better.
for other cases the prediction unfairness does not change much because when the data size is small its bias is already close to the training data bias.
we now have found evidence that with a larger set of training data there can be more unfairness.
however enlarging training data is also a critical practice to optimise model accuracy.
thus there is a conflict between improving accuracy 4we use random selection to get different sized training sets thus the bias measured by the two metrics in different sets is expected to be similar.
1441table vi rq2.
bias in the training data.
combined with figure when the training data set is larger the model prediction bias is approaching the training data unfairness.
dataset statistical parity disparate impact dataset statistical parity disparate impact adult se x .
.
adult race .
.
german sex .
.
german age .
.
compas race .
.
compas se x .
.
bank age .
.
meps race .
.
and fairness through changing the size of training data.
in the following we dig deep into the influence of training data size so as to investigate possible solutions to avoid or reduce the accuracy fairness optimisation conflict in the practice of training data extension.
in particular we further investigate the influence of training data size when the data are balanced for privileged and unprivileged groups and after applying two popular fairness improvement methods.
rq2.
what is the impact of changing the size of training data on fairness when the data are balanced?
there have been discussions that the imbalance in the data for privileged and unprivileged groups is one cause for the unfairness in ml models .
indeed intuitively if there are more data for privileged groups than unprivileged groups the model may have better performance when predicting the results for privileged groups leading to larger differences in the performance metrics and more bias in terms of the widely studied fairness metrics we explore.
when we change the training data size gradually more data will be added for privileged groups than for unprivileged groups which may lead to a negative impact of training data size on ml fairness.
to investigate whether this intuition is true for our experiments we first check the data balance condition for privileged groups and unprivileged groups for each dataset.
the results are shown by table vii.
we find that five out of eight datasetattribute pairs have more data in the privileged group than in the unprivileged group.
however there are three pairs with the opposite circumstances compas race compas sex and mepsrace.
these three still suffer from unfairness on the ml models built on them and negative impact from more training data according to table v .
this may indicate that the imbalance in data is not the primary reason for the negative impact of training data size on ml fairness.
table vii rq2.
data size in privileged groups and unprivileged groups for each dataset.
for compas race compas sex andmeps race there are more data in the unprivileged group than in the privileged group.
dataset total size size for privileged group size for unprivileged group adult se x .
.
adult race .
.
german sex .
.
german age .
.
compas race .
.
compas sex .
.
bank age .
.
meps race .
.
we further conduct experiments on the five dataset attributepairs which have more privileged data than unprivileged data.
when sampling training data we ensure that privileged and unprivileged groups have equal sizes then calculate the fairness metric values on the balanced data.
table vii shows the results of absolute and relative changes.
for ease of comparison we also show the results of the original imbalanced data on the five dataset attribute pairs the bottom sub table .
overall when the data is balanced the size of training set has more positive impact on fairness.
nevertheless the improvement is rather limited with a larger data set improved from .
to .
on average.
table viii rq2.
influence of training set size with balanced imbalanced data top bottom table .
the average effect of a richer balanced training data on fairness is slightly improved i.e.
from .
to .
on average .
influence of training data size balanced data dataset statistical parity average abs odds equal opportunity disparate impact adult se x .
adult race .
.
german sex .
.
german age bank age .
.
.
average .
.
influence of training data size imbalanced data dataset statistical parity average abs odds equal opportunity disparate impact adult se x .
adult race .
.
german sex german age bank age .
average .
.
rq2.
what is the impact of training data size with fairness improvement methods applied?
we also investigate whether fairness improvement methods affect our observations on the impact of training data size.
in particular we investigate reweighing for pre processing and prejudice remover for inprocessing methods see more details in section iii d .
table ix shows the results.
we observe that after applying the fairness improvement methods there are more light grey cells and fewer dark grey cells when the training data set is larger.
in particular without fairness improvement methods there are light dark grey cells in table v .
with reweighing there are light dark grey cells with prejudice remover there are light dark grey cells.
these changes indicate that with fairness improvement methods there is a higher probability that a larger training data would bring greater fairness.
the observations with reweighing which is a pre processing method confirm our previous conjecture that the negative impact of sampling more training data might be caused by the bias in the original training data.
when the reweighing reduces the bias in the original training data the negative impact from sampling more training data is also reduced.
overall for the second research question our observations lead to the following conclusion 1442fig.
rq3 3d surface plot showing the collective impact of feature set size and training data size.
the first row is for the adult sex dataset the second row is for compas race .
we observe that for all the metrics the surfaces rise more steeply when the feature set size is small.
this indicates that when there are fewer features the negative impact of larger training data on fairness is more significant.
table ix rq2.
comparison on the influence of training data size with fairness improvement methods.
influence of training data size with reweighing dataset statistical parity average abs odds equal opportunity disparate impact adult se x .
.
adult race .
.
.
.
german sex .
.
german age .
.
.
compas race compas sex bank age .
.
.
.
meps race .
.
.
.
average .
.
influence of training data size with prejudice remover task statistical parity average abs odds equal opportunity disparate impact adult se x .
.
adult race .
.
.
.
german sex .
.
german age .
.
.
compas race compas sex bank age .
.
.
.
meps race .
.
.
.
average .
.
answer torq2 perhaps surprisingly a larger training data does not exhibit more fairness.
when training data size increases ml fairness decreases in of the cases and does not exhibit significant changes in of the cases.
the overall fairness change rate is .
.
however fairness improvement methods can turn this change rate into .
.
c.rq3 coupling effect of feature set size and t raining data size on ml fairness this research question is designed to investigate whether there are anynotable inte ractions between the size of feature set and training set in their collectiv e impact on fairness.
f oreach feature set we investigate different training data sizes and record the fairness metric values.
wethen dra w 3d surface plots to visualise the changes of fairness with dif ferent training data size and number of features.
figure shows the results.
forbrevity we only sho w the results for theadult sex top ro w and compas race bottom row datasets.
each sub figure is for one fairness metric.
different colours represent different values with lighter colour representing lar ger unf airness.
our first observation is that in each sub graph the largest unfairness appears at a position with the smallest feature set and the largest training set.
the smallest unfairness appears at a position with the largest feature set and t he smallest training data in all b ut one case for the first sub figure the value changes are very variable .
this is consistent with our pre vious conclusions that greater feature set size brings greater fairness while a larger training data may ha vea negativeaffect on fairness.
interestingly when t he feature set size is small the surf aces rise more steeply when going from a small to alarge training set.
this indicates that when there are fe wer features larger training data introduces more unfairness.
howe ver adding more training data is a common practice to improv e model accuracy .
this observation highlights the importance of feature sufficienc y to reduce the ne gativeimpact brought by a larger set of training data.
answer torq3 when there are fewer features the unfairness increases faster with a larger training set.
v. d iscussion in this section we discuss the threats to validity and the trade of f between fairness and accurac y.wealso deri ve 1443implications and actionable conclusions from our findings for practising software engineers.
finally we discuss the relationship between human prejudices and ml bias.
a. threats to validity the primary threat to internal validity lies in the implementation of the study.
to reduce this threat the authors independently reviewed the experimental scripts to check their correctness.
we also used ibm aif360 a widely adopted fairness tool in software fairness research to obtain the fairness and accuracy of a model and the results of bias mitigation methods.
the threats to external validity lie primarily with the subjects.
we use five datasets that are widely adopted in the literature of fairness research.
we use four widely adopted fairness metrics to improve the generalisation of our conclusions.
we took several steps to address the threats to construct validity.
first we use different machine learning models e.g.
decision trees logistic regression random forests adaboost to examine whether the chosen machine learning model is a factor that would affect our conclusions.
second for the default decision trees model we use different complexity configuration i.e.
different fixed maximum depths and also grid search for each feature set and training data to check whether complexity is a factor that would affect our conclusions.
third we try different orders when changing the size of the feature set when answering rq1.
we avoid adding features data points not found in the original datasets to reduce the threat brought by unreliable data as well as to better control variables.
instead we construct different sized feature sets and training data sets via feature and data sampling.
this is of course unrealistic in practice where developers often have reliable resources to extend their feature set and training data.
in future we plan to explore the impact of extending feature data sets in realistic scenarios.
this paper provides results with the default configuration with one model and one fixed complexity configuration.
the full results for other configurations together with our code are available at our homepage .
all results demonstrate that the default configuration is not a threat to our conclusions.
b. relationship between fairness and accuracy seeking better fairness usually comes at the price of affecting the accuracy as also reported by many previous studies .
figure shows the test accuracy for theadult sex dataset with different feature set sizes and data sizes.
it is important for software engineering to be able to find a solution approach that improves both fairness and accuracy.
the most immediately actionable finding of this paper is that there does exist such a sweet spot to have a richer feature set.
this finding is a positive counterpoint to previous tradeoff theory between accuracy and fairness.
our observations implicate that you can have your cake and eat it through changing the feature set.
we also observe that there is a conflict between accuracy and fairness improvement with a larger training data.
we feature set size0.
.
.
.86test accuracy fig.
test accuracy for different feature set sizes and data sizes.
contrary to widely held beliefs that accuracy has to be traded for fairness our findings indicate that there is a way to improve both fairness and accuracy to get richer features.
analyse and provide practical solutions to tackle this conflict next.
c. implications for software practitioners and researchers implications on feature engineering.
it is well known that the role played by the features in the success of learning algorithms is crucial with poor features uncorrelated with the target labels learning could become challenging or even impossible .
our findings suggest that a larger feature set not only helps improve model accuracy but also helps to substantially improve ml fairness.
the choice of features reflect the software engineer s prior knowledge about the learning task.
our finding highlights the importance of feature engineering for building fairer software.
implications on training data extension.
enriching training data is also a well acknowledged critical practice to optimise model accuracy .
however from rq2 rq3 and figure a larger set of training data may make the model learn greater degree of bias especially when the feature set size is small.
this makes the practice of enriching training data challenging and developers may often face a choice between accuracy and fairness.
based on our findings developers can have three options to alleviate the negative impact from large training data with a fixed feature set .
they can either reduce the training data size ensure the training data is balanced if the unprivileged group is a minority or apply bias mitigation methods.
our previous findings have demonstrated that each of these three options works in reducing bias with the third one being the most effective.
we then compare the accuracy loss of each option.
as shown by table x applying fairness improvement methods is the overall best solution that preserves the most accuracy except for prejudice remover on the compas dataset .
this indicates that applying fairness improvement methods is effective to reduce the negative impact from data extension without sacrificing too much accuracy.
for researchers our findings imply that it is important to compare fairness improvement method effectiveness on a level playing field which gives all techniques the same amount of training data and the same feature set for a dataset.
1444table x accuracy and fairness changes for practical options dataset original data smaller data balanced reweigh prejudice remover adult se x .
.
.
.
.
adult race .
.
.
.
.
german sex .
.
.
.
.
german age .
.
.
.
.
compas race .
.
.
.
compas sex .
.
.
.
bank age .
.
.
.
.
meps race .
.
.
.
d. connection between human and ml prejudices human learning and machine learning share many similarities.
this makes it highly tempting to speculate on simulation between ml fairness and human bias prejudice.
this may yield insights for both domains of study.
humans often learn by examples and experiences.
they process the information feature engineering then find patterns in the information build a learning model by connecting the features to the labels to aid decision making .
william hazlitt once said prejudice is the child of ignorance .
in human learning the term ignorance usually refers to a lack of knowledge or information.
the term prejudice means a prejudgement based on inadequate knowledge.
in the domain of social psychology knowledge enhancement has long been regarded as an approach to prejudice reduction .
interestingly our findings about ml fairness accord well with the existing literature on human prejudice.
in particular if we regard feature sufficiency as knowledge of ml our findings highlight that for ml models prejudgement based on inadequate features leads to greater prejudice.
moreover if we regard the size of training data as analogous to human experience our findings reveal that when knowledge is inadequate more experience could not reduce prejudices.
instead the more inadequate the knowledge is the more harmful additional experience is.
previous findings have found that older adults have a tendency to be more prejudiced than their younger counterparts .
however age differences in prejudice are well documented but poorly understood .
our findings on ml fairness reveal that when the experience training data size is richer the prejudice for knowledgeable data with many features remain stable or slightly increase for unknowledgeable data with a few features the prejudice increases much faster.
this may make it interesting to design social psychology experiments to compare human prejudice changes over time between knowledgeable groups and less knowledgeable groups.
of course there are many differences between human learning and machine learning.
it might be possible that the connections we find between human prejudices and ml prejudices are coincidental.
nevertheless we believe these feature knowledge adequacy relationships may shed further light on possible solutions to tackling both ml and human bias.
we raise the connection here to motivate future research on this potential for successful cross fertilisation.vi.
r elated work fairness has been studied in the software engineering literature since .
research on fairness focuses on measuring discovering understanding and coping with unfairness.
this section introduces the work that is most related to ours.
a. software engineering for fairness we introduced the compelling visions on software fairness in section ii.
here we discuss the progress that has been made in se for fairness.
galhotra et al.
proposed themis which uses random test generation techniques to evaluate the degree of discrimination based on fairness scores .
udeshi et al.
proposed aequitas focusing on test generation to uncover discriminatory inputs and those inputs essential to understand individual fairness.
the generation approach first randomly samples the input space to discover the presence of discriminatory inputs then searches the neighbourhood of these inputs to find more such inputs.
agarwal et al.
used symbolic execution together with local explainability to generate test inputs.
the key idea is to use the local explanation specifically local interpretable model agnostic explanations to identify whether factors that drive decisions include protected attributes.
sun et al.
proposed to combine input mutation and metamorphic relations to automatically testing and improving the fairness of machine translations.
tramer et al.
proposed a comprehensive fairness testing tool aiming to help developers test and debug fairness bugs with an easily interpretable bug report.
the tool is available for various application areas including image classification income prediction and health care prediction.
sharma and wehrheim used data mutation to locate fairness bugs by checking whether the algorithm under test is sensitive to the mutants.
they mutated the training data in various ways to generate new datasets such as changing the order of rows columns and shuffling feature names and values.
out of classifiers were found to be sensitive to these changes.
b. empirical studies on software fairness chakraborty et al.
empirically studied the effectiveness and efficiency of existing fairness improvement methods.
they further studied the impact of model complexity parameters on ml fairness .
biswas and rajan conducted a largescale study on the effectiveness and efficiency of existing bias mitigation methods .
kearns et al.
studied the effectiveness and fairness accuracy tradeoffs of rich subgroup fairness with four datasets.
there has also been work exploring fairness with human studies.
dodge et al.
conducted a human study to explore how different styles of explanation impact people s fairness judgement of machine learning systems.
harrison et al.
performed a survey on mechanical turk workers that investigated their attitudes to difficult choices when faced with fairness related trade offs.
wang et al.
used human annotated data to analyse the similarity between different examples 1445for compas dataset to facilitate individual fairness.
grgichlaca et al.
studied humans attitudes towards whether different attributes should be regarded as protected attributes.
as far as we know there are no theoretical or empirical studies of the impact of feature set size and training data size on ml fairness.
vii.
c onclusion this paper presented a large study on the influence of feature set size and training data size on the fairness of machine learning software.
we found that a larger feature set has more fairness than a smaller one on average.
in addition a larger feature set can substantially slow down the increase of unfairness brought by larger training data.
based on our conclusion we provided practical implications for software engineers and researchers.
we also discussed the potential connection revealed by our findings between ml bias and human bias.
our findings suggest a potential for cross fertilisation between social psychology and software engineering.
data availability the five data sets that support the findings of this study were accessed following the instructions of aif3605.
the code and extra results for this paper are made public at figshare with identifier .
m9.figshare.
.
acknowledgement the authors are supported by the erc advanced grant with no.
.