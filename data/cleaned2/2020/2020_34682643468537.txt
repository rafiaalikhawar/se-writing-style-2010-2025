bias in machine learning software why?
how?
what to do?
joymallya chakraborty jchakra ncsu.edu north carolina state university raleigh usasuvodeep majumder smajumd3 ncsu.edu north carolina state university raleigh usatim menzies timm ieee.org north carolina state university raleigh usa abstract increasingly software is making autonomous decisions in case of criminal sentencing approving credit cards hiring employees and so on.
some of these decisions show bias and adversely affect certain social groups e.g.
those defined by sex race age marital status .
many prior works on bias mitigation take the following form change the data or learners in multiple ways then see if any of that improves fairness.
perhaps a better approach is to postulate root causes of bias and then applying some resolution strategy.
this paper checks if the root causes of bias are the prior decisions about a what data was selected and b the labels assigned to those examples.
our fair smote algorithm removes biased labels and rebalances internal distributions so that based on sensitive attribute examples are equal in positive and negative classes.
on testing this method was just as effective at reducing bias as prior approaches.
further models generated via fair smote achieve higher performance measured in terms of recall and f1 than other state of the art fairness improvement algorithms.
to the best of our knowledge measured in terms of number of analyzed learners and datasets this study is one of the largest studies on bias mitigation yet presented in the literature.
ccs concepts software and its engineering software creation and management computing methodologies machine learning .
keywords software fairness fairness metrics bias mitigation acm reference format joymallya chakraborty suvodeep majumder and tim menzies.
.
bias in machine learning software why?
how?
what to do?.
in proceedings of the 29th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse august athens greece.
acm new york ny usa pages.
.
.
introduction it is the ethical duty of software researchers and engineers to produce quality software that makes fair decisions especially for highstake software that makes important decisions about human lives.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august athens greece association for computing machinery.
acm isbn .
.
.
.
there are too many examples of machine learning software exhibiting unfair biased behavior based on some protected attributes like sex race age marital status.
for example amazon had to scrap an automated recruiting tool as it was found to be biased against women .
a widely used face recognition software was found to be biased against dark skinned women .
google translate the most popular translation engine in the world shows gender bias.
she is an engineer he is a nurse is translated into turkish and then again into english becomes he is an engineer she is a nurse .
prior works on managing fairness in machine learning software have tried mitigating bias by exploring a very wide space of control parameters for machine learners.
for example johnson et al.
executed some very long grid search that looped over the set of control parameters of a learner to find settings that reduced bias.
another approach published in fse by chakraborty et al.
used stochastic sampling to explore the space of control options using some incremental feedback operator to adjust where to search next .
while these approaches were certainly useful in the domains tested by johnson and chakraborty et al.
these methods are dumb in a way because they do not take advantage of domain knowledge.
this is a problem since as shown by this paper such domain knowledge can lead to a more direct and faster and more effective bias mitigation strategy.
the insight we offer here is that the root causes of bias might be the prior decisions that generated the training data .
those prior decisions affect a what data was collected and b the labels assigned to those examples.
hence to fix bias it might suffice to apply mutators to the training data in order to a remove biased labels and b rebalance internal distributions such that they are equal based on class and sensitive attributes.
this fair smote tactic uses situation testing to find biased labels.
also it balances frequencies of sensitive attributes and class labels whereas the older smote algorithm just balances the class labels .
we recommend fair smote since fair smote is faster than prior methods such as chakraborty et al.
s method.
models generated via fair smote are more effective measured in terms of recall and f1 than chakraborty et al.
and another state of the art bias mitigation algorithm .
overall this paper makes the following contributions we demonstrate two main reasons for the training data being biased a data imbalance b improper data label.
we combine finding bias explaining bias and removing bias.
we show that traditional class balancing techniques damage the fairness of the model.
esec fse august athens greece joymallya chakraborty suvodeep majumder and tim menzies why?
how to find?
what to do?
causal discrimination themis local search aequitas symbolic generation sg adversarial sampling white boxpre processing optimized preprocessing.
reweighing in processing adversarial debiasing prejudice remover regularizer post processing equalized odds reject option classification figure many tools try to find or explain or mitigate bias.
fair smote addresses all three problems at the same time.
prior works compromised performance while achieving fairness.
we achieve fairness with better recall and f1 score.
to the best of our knowledge this study explores more learners and datasets than prior works that were based on just a handful of datasets and or learners e.g.
.
before beginning we digress to clarify two points.
firstly in this paper training data is mutated by fair smote but the test data remains in its original form.
secondly one danger with mutating data is that important associations between variables can be lost.
hence in this work we take care to mutate by extrapolating between the values seen in two neighboring examples.
in that mutation process fair smote extrapolates all the variables by the same amount .
that is if there exists some average case association between the pre mutated examples then that association is preserved in the mutant.
to facilitate open science the source code and datasets used in this study are all available online1.
related work fairness in ml model is a well explored research topic in the ml community.
recently the software engineering community has also become interested in the problem of fairness.
software researchers from umass amherst have developed a python toolkit called fairkit to evaluate ml models based on various fairness metrics .
icse and ase conducted separate workshops for software fairness .
big software industries have started taking this fairness problem seriously as well.
ibm has created a public github repository ai fairness where most popular fairness metrics and mitigation algorithms can be found.
microsoft has created a dedicated research group for fairness accountability transparency and ethics in ai .
facebook has developed a tool to detect bias in their internal software .
this section reviews the fairness literature in order to find two baseline systems which we will use to evaluate fair smote .
our first baseline is chakraborty et al.
from fse that viewed bias mitigation as a multiple goal hyperparameter optimization problem.
their stochastic search found learner control parameters that reduced prediction bias.
we prefer chakraborty et al.
s method over the grid search of johnson et al.
since grid search a can be very slow and b it is strongly deprecated in the machine learning literature .
that said one drawback with the of chakraborty et al.
is that it did not guide its search via domain knowledge such as those listed in the introduction .
for a second baseline system we reviewed popular bias mitigation works including reweighing prejudice remover regularizer adversarial debiasing equality of opportunity and reject option classification .
while all these tools are useful for mitigating bias as a side effect of improving fairness these methods also degrade learner performance as measured by accuracy f1 .
hence a truism in the research community is that as said by berk et al.
it is impossible to achieve fairness and high performance simultaneously except in trivial cases .
in maity et al.
tried to reverse the berk et al.
conclusion.
to do so they had to make the unlikely assumption that the test data was unbiased .
our experience is that this assumption is difficult to achieve.
also based on our experience with fair smote described below we argue that this assumption may not even be necessary.
we show below that it is possible to mitigate bias while maintaining improving performance at the same time.
the trick is how the data is mutated.
fair smote extrapolates all the variables by the same amount .
in this way associations between variables in the pre mutated examples can be preserved in the mutant.
as to other work a recent paper from google talked about data labels being biased.
this motivated us to look into data labels.
yan et al.
commented that under representation of any sensitive group in the training data may give too little information to the classifier resulting bias .
this work inspired us to look into data distribution and class distribution to find root causes for bias.
from the remaining literature we would characterize much of it as looking vertically down a few columns of data while in our approach our mutators work horizontally across rows of data .
further much prior works only testfor bias whereas in our approach we propose how to fixthat bias .
there are many examples of vertical studies that just test for bias.
zhang et al.
built a directed acyclic graph dag containing cause effect relations of all the attributes to see which features directly or indirectly affect the outcome of the model.
loftus et al.
show methods to detect if sensitive attributes such as race gender are implicated in model outcome.
themis randomly perturbs each attribute to see whether model discriminates amongst individuals.
aequitas uses random semi directed and fully directed generation techniques that make it more efficient than themis.
recently ibm researchers have developed a new testing technique by combining lime local interpretable model 430bias in machine learning software why?
how?
what to do?
esec fse august athens greece table description of the datasets used in the experiment.
dataset rows featuresprotected attributedescription adult census income sex race individual information from u.s. census.
goal is predicting income .
compas sex race contains criminal history of defendants.
goal predicting re offending in future german credit sex personal information about individuals predicts good or bad credit.
default credit sex customer information for people from taiwan.
goal is predicting default payment.
heart health age patient information from cleveland db.
goal is predicting heart disease.
bank marketing age contains marketing data of a portuguese bank.
goal predicting term deposit.
home credit sex loan applications for individuals.
goal is predicting application accept reject.
student performance sex student achievement of two portuguese schools.
target is final year grade.
meps race surveys of families individuals medical providers employers.
target is utilization .
table definition of the performance and fairness metrics used in this study.
performance metricideal valuefairness metricideal value recall tp p tp tp fn 1average odds difference aod average of difference in false positive rates fpr and true positive rates tpr for unprivileged and privileged groups .
tpr tp tp fn fpr fp fp tn aod .
false alarm fp n fp fp tn 0equal opportunity difference eod difference of true positive rates tpr for unprivileged and privileged groups .
eod tpru tprp0 accuracy tp tn tp fp tn fn 1statistical parity difference spd difference between probability of unprivileged group protected attribute pa gets favorable prediction y probability of privileged group protected attribute pa gets favorable prediction y .
spd p p precision tp tp fp 1disparate impact di similar to spd but instead of the difference of probabilities the ratio is measured .
di p p f1 score precision recall precision recall explanation and symbolic execution .
zhang et al.
developed a fairness testing method that is applicable for dnn models they combined gradient computation and clustering techniques .
finally we can find one paper that seems like a competitive technology to the methods of this paper.
optimized pre processing op by calmon et al.
tries to find then fix bias.
op is a datadriven optimization framework for probabilistically transforming data in order to reduce algorithmic discrimination.
op treats bias mitigation as a convex optimization problem with goals to preserve performance and achieve fairness.
we chose this work as a baseline because like fair smote it is also a pre processing strategy.
in summary prior work suffered from some methods only find bias without trying to fix it.
some methods for fixing bias have an undesired side effect leaner performance was degraded.
for the rest of this paper we explore a solution that finds root causes of bias and directly implement mitigation of those causes resulting in less bias and better performance than seen in prior work .
what are the root causes of bias?
to understand what might cause bias at first we need some terminology.
table contains datasets used in this study.
all of them are binary classification problems where the target class has only two values.
a class label is called a favorable label if it gives an advantage to the receiver such as receiving a loan being hired for a job.
aprotected attribute is an attribute that divides the whole population into two groups privileged unprivileged that have differences in terms of receiving benefits.
every dataset in table has one or two such attributes.
for example in case of credit card application datasets based on protected attribute sex male is privileged and female is unprivileged in case of health datasets based on protected attribute age young is privileged and old is unprivileged.
group fairness is the goal that based on the protected attribute privileged and unprivileged groups will be treated similarly.
individual fairness means similar outcomes go to similar individuals.
table contains five performance metrics and four fairness metrics we used.
these metrics are selected since they were widely used in the literature .
prediction performance is measured in terms of recall false alarm accuracy precision f1 fairness is measured using aod eod spd di .
all of these can be calculated from the confusion matrix of binary classification containing four cells true positives tp false positives fp true negatives tn and false negatives fn .
for recall accuracy precision f1 larger values arebetter for false alarm aod eod spd smaller values arebetter .
431esec fse august athens greece joymallya chakraborty suvodeep majumder and tim menzies adult sexadult racecompassexcompas r acegermansexdefault credit sexheart agebank agehome credit sexstudent sexmeps r acemeps r ace020406080favorable privileged favorable unprivileged unfavorable privileged unfavorable unprivileged of data points figure most of the datasets showing not only class imbalance but also imbalance based on the protected attribute.
sex default sex smo te race default race smo te of data points figure effects of smote class balancing technique on adult dataset for two protected attributes sex and race .
di is a ratio and there is no bias when value of di is .
for comprehensibility while showing results we compute abs di so that all four fairness metrics are lower the better means no bias .
prior research has shown that classification models built from the datasets of table show bias.
why?
what are the natures of these datasets that result in bias?
in this paper we postulate that data has a history and that history introduces bias.
for example consider a team of humans labeling data as risky loan applicant or otherwise.
if that team has any biases conscious or otherwise against certain social groups then the bias of those humans introduces improper unfair data labels.
another way history can affect the data is selection bias .
suppose we are collecting package data relating to the kinds of items shipped via amazon to a particular suburb.
suppose there is some institutional bias that tends to results in low annual incomes for persons in certain suburbs2.
the delivery data collected from those suburbs would contain all the political and economic biases that tend to perpetuate lower socio economic neighborhoods.
the next two sections explore the intuitions of the last two paragraphs in more detail.
specifically we will look into data imbalance 2e.g.
if politicians spend less money on schools in a poorer district then that district has a fewer exceptional schools b fewer graduates with job skills in for high paying jobs in high demand c consistently lower incomes from generation to generation.and improper data labeling.
this in turn will lead to the fair smote algorithm that deletes biased labels and balances all distributions between positive and negative examples.
.
data imbalance when a classification model is trained on imbalanced data the trained model shows bias against a certain group of people.
we mention that such imbalances are quite common.
figure displays distributions within our datasets.
note that in most cases we see aclass imbalance i.e the number of observations per class is not equally distributed.
further we see that the imbalance is not just based on class but also on protected attribute .
for example consider the adult dataset.
here we are predicting the annual income of a person.
there are two classes.
high income which is favorable label and low income which is unfavorable label .
the first grouped bar in figure has two bars grouped together.
the first bar is for high income class and the second bar is for low income class.
it is clear that the number of instances for low income is more than three times of instances for high income .
this is an example of class imbalance.
now both the bars are subdivided based on protected attribute sex male and female .
for high income or favorable label instances are male and only are female.
for low income or unfavorable label instances are male and are female.
overall this dataset contains more examples of male privileged getting favorable label and female unprivileged getting unfavorable label .
class imbalance is a widely studied topic in ml domain.
there are mainly two ways of balancing imbalanced classes oversample the minority class or undersample the majority class.
we want to see how various class balancing techniques affect fairness.
in table we are showing the results for five most commonly used class balancing techniques on adult dataset.
one of them is undersampling rus random under sampling other three ros432bias in machine learning software why?
how?
what to do?
esec fse august athens greece table effects of various class balancing techniques on adult dataset note for the metrics with more is better and for the metrics with less is better .
for each metric cell with best score is highlighted.
recall false alarm precision accuracy f1 score aod eod spd di default .
.
.
.
.
.
.
.
.
rus .
.
.
.
.
.
.
.
.
ros .
.
.
.
.
.
.
.
.
smote .
.
.
.
.
.
.
.
.
kmeans smote .
.
.
.
.
.
.
.
.
table percentage of data points failing situation testing for datasets.
adult sex adult race compas sex compas race german sex default credit sex hearthealth age bank marketing age home credit sex student sex meps race meps race of rows failed11 random over sampling smote synthetic minority over sampling technique and kmeans smote are oversampling techniques.
table shows values of nine metrics first five of them are performance metrics and last four of them are fairness metrics.
we used logistic regression model here.
the important observation here is all four of the class balancing techniques are increasing bias scores mean damaging fairness lower is better here .
to better understand this see figure .
it gives a visualization of using smote on adult dataset.
smote generates synthetic samples for minority class data points to equalize two classes.
suppose a data point from minority class is denoted as x wherex1 x2 .. xnare the attributes and its nearest neighbor is x x x .. x n .
according to smote a new data point y y1 y2 .. yn is generated by the following formula y x rand x x smote definitely balances the majority class and minority class but it damages the protected attribute balance even more.
thus figure explains the results of table .
for space reasons we have shown this with one dataset and one technique smote but in other datasets also we got similar pattern.
that said traditional class balancing methods improve performance of the model but damage fairness usually .
the reason isthese techniques randomly generate discard samples just to equalize two classes and completely ignore the attributes and hence damage the protected attribute balance even more .
to fix this we propose fair smote.
like smote fair smote will generate synthetic examples.
but fair smote takes much more care than smote for exactly how it produces new examples.
specifically it balances data based on class and sensitive attributes such that privileged and unprivileged groups have an equal amount of positive and negative examples in the training data for more details see .
.
improper data labeling some prior works argue that improper labeling could be a reason behind bias.
we used the concept of situation testing tovalidate how labeling can affect fairness of the model.
situation testingis a research technique used in the legal field where decision makers candid responses to applicant s personal characteristics are captured and analyzed.
for example a pair of research assistants a male and a female with almost equivalent qualities undergo the same procedure such as applying for a job.
now the treatments they get from the decision maker are analyzed to find discrimination.
situation testing as a legal tactic has been widely used both in the united states and the european union .
luong et al.
first used the concept of situation testing in classification problems.
they used k nn approach to find out similar individuals getting different outcomes to find discrimination.
later zhang et al.
used causal bayesian networks to do the same.
the core idea of our situation testing is much simpler flip the value of protected attribute for every data point.
see whether prediction given by the model changes or not.
in our implementation a logistic regression model is trained first and then all the data points are predicted.
after that the protected attribute value for every data point is flipped e.g.
male to female white to non white and tested again to validate whether model prediction changes or not.
if the result changes that particular data point fails the situation testing.
table shows the median of ten runs for all the datasets.
we see all the datasets more or less contain these kinds of biased data points.
that means we have found biased labels .
note that we use logistic regression model for situation testing but any other supervised model can be chosen.
we picked logistic regression because it is a simple model and can be trained with a very low amount of data compared to dl models .
choosing a different model may change the outcome a bit.
we will explore that in future.
methodology summarizing the above we postulate that data imbalance and improper labeling are the two main reasons for model bias.
further we assert that if we can solve these two problems then final outcome 433esec fse august athens greece joymallya chakraborty suvodeep majumder and tim menzies will be a fairer model generating fairer prediction.
this section describes experiments to test that idea.
.
fair smote fair smote algorithm solves data imbalance.
at first the training data is divided into subgroups based on class and protected attribute.
if class and protected attribute both are binary then there will be subgroups favorable privileged favorable unprivileged unfavorable privileged unfavorable unprivileged .
initially these subgroups are of unequal sizes.
fair smote synthetically generates new data points for all the subgroups except the subgroup having the maximum number of data points.
as a result all subgroups become of equal size same with the maximum one .
as stated in the introduction one danger with mutating data is that important associations between variables can be lost.
hence in this work we take care to mutate by extrapolating between the values seen in two neighboring examples.
in that mutation process fair smote extrapolates all the variables by the same amount .
that is if there exists some average case association between the premutated examples then that association is preserved in the mutant.
for data generation we use two hyperparameters mutation amount f and crossover frequency cr like differential evolution .
they both lie in the range .
the first one denotes at which probability the new data point will be different from the parent point .
means of the times and the latter one denotes how much different the new data point will be.
we have tried with .
probability .
.
and got best results with .
.
algorithm describes the pseudocode of fair smote.
it starts with randomly selecting a data point parent point p from a subgroup.
then using k nearest neighbor two data points c1 c2 are selected which are closest to p. next according to the algorithm described a new data point is created.
separate logic is used for boolean symbolic and numeric columns.
fair smote does not randomly create a new data point.
rather it creates a data point that is very close to the parent point.
thus the generated data points belong to the same data distribution.
this process is repeated until all the subgroups become of similar size.
after applying fair smote the training data contains equal proportion of both classes and the protected attribute.
.
fair situation testing at first we use fair smote to balance the training data.
fairsmote is an oversampling technique and thus increases the size of train set.
we then use situation testing as mentioned in .
to find out biased data points in the training data.
we call this situation testing asfair situation testing because this is making the training data fairer.
after finding biased data points we remove them from training data.
as table shows small percentage numbers we do not lose much of the training data.
we will show in the results section that this does not affect performance of the model much.
after removal of biased data points we train the model on the remaining training set and finally make the prediction on test data.algorithm pseudocode of fair smote input dataset protected attribute p attrs class label cl output balanced dataset 1def get ngbr dataset knn rand sample id random len dataset parent dataset ngbr knn.kneighbors parent c1 c2 dataset dataset return parent c1 c2 7count groups get count dataset p attrs cl 8max size max count groups 9cr f .
.
user can pick any value in 10forc in cl do forattr in p attrs do sub data dataset cl c p attrs attr sub group size count groups to generate max size sub group size knn nearestneighbors sub data fori in range to generate do p c1 c2 get ngbr sub data knn new candidate forcol in parent candidate.columns do ifcol is boolean then ifcr random then new val random p c1 c2 else new val p new candidate.add new val else if col is string then new val random p c1 c2 new candidate.add new val else if col is numeric then ifcr random then new val p f c1 c2 else new val p new candidate.add new val dataset.add new candidate .
experimental design here we describe how we prepared the data for experiments to answer the research questions in .
our study used datasets and classification models logistic regression lsr random forest rf and support vector machine svm .
in fairness domain datasets are not very large in size and also have small dimensions.
that is why we see most of the prior works choose simple models like us instead of deep learning models.
for every experiment we split the datasets using fold cross validation train test and repeat times with random seeds and finally report the median.
the rows containing missing values are ignored continuous features are converted to categorical e.g.
age young age old non numerical features are converted to numerical e.g.
male female finally all the feature values are normalized converted between to .
these basic conversions are done for all the experiments.
classification model is first trained on training data and then tested on test data we report the median of ten runs.
figure shows the block diagram of one repeat of our framework.
434bias in machine learning software why?
how?
what to do?
esec fse august athens greece datasettrain settest setsynthetic over sampling based on minority group of protected attribute and classremove data points based on situation testingmodel trainingtrained modelpredictionfair smote80 figure block diagram of fair smote .
statistical tests while comparing fair smote with other techniques we use scottknott test to compare two distributions.
scott knott is a recursive bi clustering algorithm that terminates when the difference between the two split groups is not significant.
scott knott searches for split points that maximize the expected value of the difference between the means of the two resulting groups.
if a group lis split into two groups m and n scott knott searches for the split point that maximizes e m l e e n l e e where m is the size of group m. the result of the scott knott test is ranks assigned to each result set higher the rank better the result.
scott knott ranks two results the same if the difference between the distributions is not significant.
results the results are structured around five research questions.
rq1.
can we find bias by just looking at the training data?
in we said a machine learning model acquires bias from training data.
rq1 asks for the signals we must see to identify whether training data has bias or not.
that is an important question to ask because if that is doable and that bias can be removed before model training then the chances of bias affecting the final decision reduce significantly.
table shows results for three different models logistic regression lsr random forest rf support vector machine svm and table shows results for one model lsr only.
the row default is when we train the model on raw data.
results show that default row has significantly high bias scores for all the datasets that means trained model is showing discrimination.previously we have dived deep into these datasets to find reasons for bias.
here we are summarizing the sanity checks every dataset should go through before model training to avoid discrimination.
data distribution the training data should be almost balanced based on class and protected attribute.
data label every data point should go through situation testing to see whether label has bias or not.
if we find bias in the training data we should apply fair smote to remove that and get fair outcomes.
thus the answer for rq1 is yes we can find bias by just looking at the training data rq2.
are standard class balancing techniques helpful to reduce bias?
for answering rq2 we chose the most used class balancing technique which is smote .
table contains results for three datasets and three different learners lsr rf svm .
in that table for a particular dataset and for a particular performance metric cells with darker backgrounds denote treatments that are performing better than anything else.
conversely cells with a white background denote treatments that are performing worse than anything else .
smote consistently increases bias scores aod eod spd di mean damaging fairness but performs similar better than default in case of performance metrics as measured by recall false alarm precision accuracy f1 .
thus the answer for rq2 is no standard class balancing techniques are not helpful since in their enthusiasm to optimize model performance they seem to also amplify model bias.
rq3.
can fair smote reduce bias?
table answers this question.
looking at the bias metrics aod eod spd di fair smote significantly reduces all the bias scores mean increasing fairness see the darker colored cells .
as to the performance measures recall false alarm precision accuracy and f1 it is hard to get a visual summary of the results just by looking at table .
for that purpose we turn to rows of table .
based on scott knott tests of .
these rows count the number of times fair smote wins losses or ties compared to smote here tie means is assigned the same rank by scottknott .
those counts in rows confirm the visual patterns of table as to the performance measures recall false alarm precision accuracy and f1 these methods often tie.
but looking at the highlighted bias metrics results for aod eod spd di fair smote is clearly performing better than smote.
thus the answer for rq3 is yes fair smote reduces bias significantly and performs much better than smote.
rq4.
how well does fair smote perform compared to the state of the art bias mitigation algorithms?
table compares fair smote against other tools that try to find and fix bias.
default shows the off the shelf learners fairway 435esec fse august athens greece joymallya chakraborty suvodeep majumder and tim menzies table results for rq1 rq2 rq3.
in this table default means off the shelf learner smote is an algorithm by chawla et al.
from and fair smote is the algorithm introduced by this paper.
cells show medians for runs.
here the darkest cells show top rank note for the metrics with more is better and for the metrics with less is better .
the lighter and lightest cells show rank two and rank three respectively the white cells show the worst rank.
rankings were calculated via scott knott test .
.
datasetprotected attributealgorithmsrecall false alarm precision accuracy f1 score aod eod spd di default lsr .
.
.
.
.
.
.
.
.
default rf .
.
.
.
.
.
.
.
.
default svm .
.
.
.
.
.
.
.
.
smote lsr .
.
.
.
.
.
.
.
.
smote rf .
.
.
.
.
.
.
.
.
smote svm .
.
.
.
.
.
.
.
.
fair smote lsr .
.
.
.
.
.
.
.
.
fair smote rf .
.
.
.
.
.
.
.
.22adult census incomesex fair smote svm .
.
.
.
.
.
.
.
.
default lsr .
.
.
.
.
.
.
.
.
default rf .
.
.
.
.
.
.
.
.
default svm .
.
.
.
.
.
.
.
.
smote lsr .
.
.
.
.
.
.
.
.
smote rf .
.
.
.
.
.
.
.
.
smote svm .
.
.
.
.
.
.
.
.
fair smote lsr .
.
.
.
.
.
.
.
.
fair smote rf .
.
.
.
.
.
.
.
.29adult census incomerace fair smote svm .
.
.
.
.
.
.
.
.
default lsr .
.
.
.
.
.
.
.
.
default rf .
.
.
.
.
.
.
.
.
default svm .
.
.
.
.
.
.
.
.
smote lsr .
.
.
.
.
.
.
.
.
smote rf .
.
.
.
.
.
.
.
.
smote svm .
.
.
.
.
.
.
.
.
fair smote lsr .
.
.
.
.
.
.
.
.
fair smote rf .
.
.
.
.
.
.
.
.02compas sex fair smote svm .
.
.
.
.
.
.
.
.
default lsr .
.
.
.
.
.
.
.
.
default rf .
.
.
.
.
.
.
.
.
default svm .
.
.
.
.
.
.
.
.
smote lsr .
.
.
.
.
.
.
.
.
smote rf .
.
.
.
.
.
.
.
.
smote svm .
.
.
.
.
.
.
.
.
fair smote lsr .
.
.
.
.
.
.
.
.
fair smote rf .
.
.
.
.
.
.
.
.10compas race fair smote svm .
.
.
.
.
.
.
.
.
default lsr .
.
.
.
.
.
.
.
.
default rf .
.
.
.
.
.
.
.
.
default svm .
.
.
.
.
.
.
.
.
smote lsr .
.
.
.
.
.
.
.
.
smote rf .
.
.
.
.
.
.
.
.
smote svm .
.
.
.
.
.
.
.
.
fair smote lsr .
.
.
.
.
.
.
.
.
fair smote rf .
.
.
.
.
.
.
.
.19meps race fair smote svm .
.
.
.
.
.
.
.
.
is the chakraborty et al.
system from fse and op is the optimized pre processing method from nips .
here the learner is logistic regression since for performance measures lsr has best results in the fair smote results of table .
rows of table summarize these results.
measured in terms of bias reduction all the methods often tie.
but observing the highlighted cells in that table we see that the fairsmote performs much better in terms of recall and f1 than anything else.
thus the answer for rq4 is fair smote performs similar or better than two state of the art bias mitigation algorithms in case of fairness and consistently gives higher recall and f1 score.
that means we do not have to compromise performanceanymore to achieve fairness.
fair smote is able to provide both better fairness and performance.
this is the biggest achievement of this work.
rq5.
can fair smote reduce bias for more than one protected attribute?
in the literature we are unaware of any work trying to reduce bias for more than one protected attribute at a time.
typically what researchers do is try to eliminate bias based on one protected attribute one at a time.
we experimented on adult dataset having two protected attributes sex race .
the idea is to balance the training data with respect to class and two protected attributes.
that means we need to find out among the eight possible subgroups which 436bias in machine learning software why?
how?
what to do?
esec fse august athens greece table results for rq3 rq4 learner logistic regression .
in this table.
default denotes off the shelf logistic regression op is calmon et al.
s system from nips fairway is chakraborty et al.
s system from fse and fair smote is the algorithm introduced by this paper.
cells show medians for runs.
here the darker cells show top rank note for the metrics with more is better and for the metrics with less is better .
the lighter cells show rank two white shows lowest rank worst performance .
rankings were calculated via the scott knott test .
datasetprotected attributealgorithmsrecall false alarm precision accuracy f1 score aod eod spd di default .
.
.
.
.
.
.
.
.
op .
.
.
.
.
.
.
.
.
fairway .
.
.
.
.
.
.
.
.11adult census incomesex fair smote .
.
.
.
.
.
.
.
.
default .
.
.
.
.
.
.
.
.
op .
.
.
.
.
.
.
.
.
fairway .
.
.
.
.
.
.
.
.32adult census incomerace fair smote .
.
.
.
.
.
.
.
.
default .
.
.
.
.
.
.
.
.
op .
.
.
.
.
.
.
.
.
fairway .
.
.
.
.
.
.
.
.08compas sex fair smote .
.
.
.
.
.
.
.
.
default .
.
.
.
.
.
.
.
.
op .
.
.
.
.
.
.
.
.
fairway .
.
.
.
.
.
.
.
.07compas race fair smote .
.
.
.
.
.
.
.
.
default .
.
.
.
.
.
.
.
.
op .
.
.
.
.
.
.
.
.
fairway .
.
.
.
.
.
.
.
.11german credit sex fair smote .
.
.
.
.
.
.
.
.
default .
.
.
.
.
.
.
.
.
op .
.
.
.
.
.
.
.
.
fairway .
.
.
.
.
.
.
.
.12default credit sex fair smote .
.
.
.
.
.
.
.
.
default .
.
.
.
.
.
.
.
.
op .
.
.
.
.
.
.
.
.
fairway .
.
.
.
.
.
.
.
.18heart health age fair smote .
.
.
.
.
.
.
.
.
default .
.
.
.
.
.
.
.
.
op .
.
.
.
.
.
.
.
.
fairway .
.
.
.
.
.
.
.
.05bank marketing age fair smote .
.
.
.
.
.
.
.
.
default .
.
.
.
.
.
.
.
.
op .
.
.
.
.
.
.
.
.
fairway .
.
.
.
.
.
.
.
013home credit sex fair smote .
.
.
.
.
.
.
.
.
default .
.
.
.
.
.
.
.
.
op .
.
.
.
.
.
.
.
.
fairway .
.
.
.
.
.
.
.
.07student performancesex fair smote .
.
.
.
.
.
.
.
.
default .
.
.
.
.
.
.
.
.
op .
.
.
.
.
.
.
.
.
fairway .
.
.
.
.
.
.
.
.12meps race fair smote .
.
.
.
.
.
.
.
.
default .
.
.
.
.
.
.
.
.
op .
.
.
.
.
.
.
.
.
fairway .
.
.
.
.
.
.
.
.16meps sex fair smote .
.
.
.
.
.
.
.
.
one has the most number of data points.
then we need to make other subgroups of the same size with the one having most number of data points.
that is done by generating new data points using fair smote.
table shows results that bias is reduced for both the attributes along with higher recall and f1 score.
hence the answer of rq5 is yes fair smote can simultaneously reduce bias for more than one protected attribute.
discussion why fair smote?
here we discuss what makes fair smote unique and more useful than prior works in the fairness domain.combination this is only the second se work after fairway in fairness domain where primary focus is bias mitigation.
there are a lot of papers in ml domain where various techniques are provided to remove bias from model behavior.
still when it comes to applicability we see software practitioners find ml fairness as a complicated topic.
because finding bias explaining bias and removing bias have been treated as separate problems and thus created more confusion.
that s where this work makes a significant contribution by combining all of them together.
we first find data imbalance and improperly labeled data points by situation testing and then use oversampling to balance the data and remove 437esec fse august athens greece joymallya chakraborty suvodeep majumder and tim menzies table rq3 rq4 results.
summarized information of comparing fair smote with smote fairway optimized preprocessing based on results of datasets and learners lsr rf svm .
number of wins ties and losses are calculated based on scott knott ranks for each metric.
highlighted cells show fair smote significantly outperforming others.
recall false alarm precision accuracy f1 score aod eod spd di total smote vs fair smote win tie loss 4win tie fairway vs fair smote win tie loss 8win tie optimized pre processing vs fair smote win tie loss win tie table rq5 results.
fair smote reducing bias for sex and race simultaneously adult dataset .
best cells are highlighted.
protected attributerecall false alarm precision accuracy f1 score aod eod spd di sex .
.
.
.56defaultrace0.
.
.
.
.
.
.
.
.
sex .
.
.
.27fair smoterace0.
.
.
.
.
.
.
.
.
improperly labeled data points.
as an outcome we generate fair results.
uncompromising our framework improves fairness scores along with f1 score and recall.
it does not damage accuracy and precision much also.
that said unlike much prior work we can do bias mitigation without compromising predictive performance.
we attribute our success in this regard to our sampling policy.
none of our mutators damage the associations between attributes.
rather we just carefully resample the data to avoid certain hard cases where the training data can only see a few examples of each kind of row .
group individual our data balancing approach takes care ofgroup fairness where goal is based on the protected attribute privileged and unprivileged groups will be treated similarly.
our situation testing method takes care of individual fairness where goal is to provide similar outcomes to similar individuals.
generality we entirely focused on data to find and remove bias.
there are works where optimization tricks have been used while model training to remove bias .
these works are model specific and most of the time combine with internal model logic.
however fair smote does not require access to inside model logic.
thus it is much more general as it can be used for any kind of model.
in this work we used three simple models and got promising results.
in future we will explore deep learning models.
versatility we used fair smote for only classification datasets here.
however the core idea of fair smote is keeping equal proportion of all the protected groups in the training data.
we believe the same approach can be applied to regression problems.
in futurewe would like to explore that.
besides the same approach can be applied for image data face recognition to train model with equal proportion of white and black faces so that model does not show racial bias.
that means fair smote can be easily adopted by other domains to solve bias issues caused by data imbalance.
threats to validity sampling bias as per our knowledge this is the most extensive fairness study using real world datasets and classification models.
still conclusions may change a bit if other datasets and models are used.
evaluation bias we used the four most popular fairness metrics in this study.
prior works only used two or three metrics although ibm aif360 contains more than metrics.
in future we will explore more evaluation criteria.
internal validity where prior researchers focused on attributes to find causes of bias we concentrated on data distribution and labels.
however there could be some other reasons.
a recent amazon paper comments on some other reasons such as objective function bias homogenization bias .
we could not validate these biases in our datasets as it was out of scope.
in future we would like to explore those reasons if industry datasets become available.
external validity this work is based on binary classification and tabular data which are very common in ai software.
we are currently working on extending it to regression models.
in future work we would extend this work to other domains such as text mining and image processing.
438bias in machine learning software why?
how?
what to do?
esec fse august athens greece conclusion this paper has tested the fair smote tactic for mitigating bias in ml software.
fair smote assumes the root causes of bias are the prior decisions that control a what data was collected and b the labels assigned to the data.
fair smote a removes biased labels and b rebalances internal distributions such that they are equal based on class and sensitive attributes.
as seen above fair smote was just as effective at bias mitigation as two other state of the art algorithms and more effective in terms of achieving higher performance measured in terms of recall and f1 .
also fair smote runs faster median value across ten data sets than chakraborty et.al .
based on the above we offer three conclusions we can recommend fair smote for bias mitigation.
we can reject the pessimism of berk et al.
who previously had been worried that the cost of fairness was a reduction in learner performance.
more generally rather than blindly applying some optimization methods it can be better to reflect on the domain use insights from that reflection to guide improvements in that domain.