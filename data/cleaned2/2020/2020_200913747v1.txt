dynamic slicing for deep neural networks ziqi zhang key laboratory of high confidence software technologies moe dept of computer science peking university beijing china ziqi zhang pku.edu.cnyuanchun li microsoft research beijing china yuanchun.li microsoft.comyao guo key laboratory of high confidence software technologies moe dept of computer science peking university beijing china yaoguo pku.edu.cn xiangqun chen key laboratory of high confidence software technologies moe dept of computer science peking university beijing china cherry pku.edu.cnyunxin liu microsoft research beijing china yunxin.liu microsoft.com abstract program slicing has been widely applied in a variety of software engineering tasks.
however existing program slicing techniques only deal with traditional programs that are constructed with instructions and variables rather than neural networks that are composed of neurons and synapses.
in this paper we propose nnslicer the first approach for slicing deep neural networks based on data flow analysis.
our method understands the reaction of each neuron to an input based on the difference between its behavior activated by the input and the average behavior over the whole dataset.
then we quantify the neuron contributions to the slicing criterion by recursively backtracking from the output neurons and calculate the slice as the neurons and the synapses with larger contributions.
we demonstrate the usefulness and effectiveness of nnslicer with three applications including adversarial input detection model pruning and selective model protection.
in all applications nnslicer significantly outperforms other baselines that do not rely on data flow analysis.
ccs concepts computing methodologies neural networks software and its engineering dynamic analysis .
the first two authors contributed equally.
this work is partly done while ziqi zhang was an intern at microsoft research.
correspondence to yuanchun li yao guo.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november virtual event usa association for computing machinery.
acm isbn .
.
.
.
program slicing deep neural networks dynamic slicing data flow analysis acm reference format ziqi zhang yuanchun li yao guo xiangqun chen and yunxin liu.
.
dynamic slicing for deep neural networks.
in proceedings of the 28th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november virtual event usa.
acm new york ny usa pages.
.
introduction program slicing is widely used in software engineering for various tasks such as debugging testing and verification .
it aims to compute a set of statements named program slice that may affect the values at some points of interest named slicing criterion .
for example by setting the slicing criterion to a specific output that generates an error one can get a program slice that may be relevant to the error but much smaller in size than the whole program thus much easier to analyze.
existing program slicing techniques are mainly designed for traditional programs that are constructed with human defined functions and instructions.
deep neural networks dnns which have achieved remarkable success in many data processing applications in recent years can also be viewed as a special type of programs constructed with artificial neurons a neuron is a mathematical function that receives one or more inputs and computes an output such as the weighted sum or the maximum.
and synapses the connections between neurons .
however the weights of synapses are learned by the machine and are usually hard for a human to understand.
to the best of our knowledge it has not been studied on whether and how a dnn can be analyzed meaningfully using program slicing techniques.
we apply the concept of program slicing to the area of dnns and define dnn slicing ascomputing a subset of neurons and synapses that may significantly affect the values of certain interested neurons.arxiv .13747v1 sep 2020esec fse november virtual event usa ziqi zhang yuanchun li yao guo xiangqun chen and yunxin liu slicing a dnn is interesting for a number of reasons.
first it is a widely concerned problem that the decisions made by dnns are difficult to explain or debug.
program slicing hopefully can be used to extract the operations that lead to a decision making it easier to interpret.
second the size of dnn is growing rapidly in recent years with more than million parameters mb in size in a state of the art computer vision model and million parameters mb in size in a state of the art natural language understanding model .
how to improve the model efficiency has become an important research problem.
to this end we believe program slicing has the potential to help reduce the model size significantly.
last but not least partitioning the model into important slices and less important slices can also benefit model protection as one can prioritize the important slices if protecting the whole model is difficult or impossible.
dnn slicing introduces several new challenges as compared with traditional program slicing.
first unlike the instructions and variables in traditional programs that are themselves meaningful a neuron in a dnn is usually a meaningless mathematical operator whose behavior is determined by its learned weights and connections with other neurons.
thus it is a challenging problem to understand the behavior of each neuron based on its connections and weights.
second each output value of a dnn is affected by almost all neurons in the model.
to generate a meaningful and concise slice we must differentiate and characterize the neurons based on their contributions to the slicing criterion.
finally the data flow graphs in traditional programs are usually sparse and small scale while the data flow graphs of a dnn may contain millions of neurons densely connected with each other.
analyzing a graph in such a large scale poses a much higher requirement on system efficiency.
in this paper we present nnslicer a dynamic slicing technique for dnns based on data flow analysis on neural networks.
the slicing criterion is defined as a set of neurons with special meanings such as the neurons in the last layer of an image classification model whose outputs represent the probabilities of categories while a neural network slice is defined as a subset of neurons in the neural network that exhibit larger effects to the slicing criterion.
nnslicer focuses on dynamic slicing in which a slice is corresponding to a set of input samples rather than static slicing which is input independent.
nnslicer consists of three phases a profiling phase a forward analysis phase and a backward analysis phase.
the profiling phase aims to model the average behavior of each neuron.
the behavior of a neuron can be characterized by its activation values which changes by feeding different data samples into the model.
we feed all training data into the model and compute the average activation value of each neuron .
these average values are used as the baseline to understand the reaction of each neuron to specific data samples.
the forward analysis feeds the interested data samples the samples we want to compute slice with into the model and records the activation value of each neuron.
the difference between the recorded value and the average activation value computed in the profiling phase represents the neuron reaction to the data samples.
the magnitude of the value difference indicates the sensitivity of the neuron with regard to the data samples.however the neurons with higher sensitivity are not necessarily more important for the slicing criterion since the effect of the neuron may be eliminated or redirected to other outputs by its subsequent neurons.
thus we further perform a backward analysis that backtracks the data flow from the output neurons to understand the contribution of each neuron.
specifically the slice is initialized with the output neurons specified in the slicing criterion.
we then iteratively analyze each neuron in the slice by calculating the contributions of its preceding neurons.
the preceding neurons with higher contributions are added into the slice for further backward analysis.
we implement nnslicer in tensorflow through instrumentation and support the common operators in convolutional neural networks cnns .
our implementation is able to deal with large state of the art cnn models such as resnet .
the time spent by nnslicer to compute a slice for a data sample is around seconds on resnet10 and seconds on resnet18.
computing slices for batch input is much faster about 3s and 40s per sample on resnet10 and resnet18 .
to demonstrate the usefulness and effectiveness of nnslicer we further build three applications for adversarial defense model pruning and model protection respectively.
first we show that nnslicer can be used to effectively detect adversarial samples.
specifically we show that the slice computed for a data sample reflects how the prediction decision is made by the model and the slices computed from adversarial samples significantly differ from the slices computed from normal samples.
on average the adversarial input detector implemented based on nnslicer achieves a high precision of .
and a perfect recall of .
.
second we show that nnslicer can be used to customize dnn models for a certain label space.
given a subset of model outputs nnslicer computes a slice for the outputs and generates a smaller model that is composed of the neurons and synapses in the slice.
we show that the sliced model significantly outperforms other model pruning methods.
notably the sliced model can achieve high accuracy above even without fine tuning.
finally nnslicer can also be used to improve model protection.
specifically we can selectively protect the important slices rather than the whole model in order to reduce the protection overhead.
we show that by hiding parameters selected by nnslicer the exposed part can be nearly immune to model extraction attacks .
this paper makes the following contributions to the best of our knowledge this is the first paper to systematically explore and study the idea of dynamic dnn slicing.
we implement a tool nnslicer for dynamic dnn slicing on the popular deep learning framework tensorflow.
our tool is scalable and efficient.
we develop three interesting applications using dnn slicing techniques and demonstrate the effectiveness of nnslicer.
background and related work .
deep neural networks deep neural networks dnns are inspired by the biological neural networks that constitute animal brains.
a neural network is based on a collection of connected mathematical operation units called artificial neurons.
each connection synapse between neurons candynamic slicing for deep neural networks esec fse november virtual event usa transmit a signal from one neuron to another.
the receiving neuron can process the signal s and then signal downstream neurons connected to it.
typically neurons are organized in layers.
different layers may perform different kinds of transformations on their inputs.
for a certain kind of neuron how it processes the signal is determined by its weights which are learned by considering examples.
for example in image recognition the neural network learns from example images that have been manually labeled as cat or no cat and uses the learned knowledge to identify cats in other images.
neural networks are good at capturing complex mapping relations between inputs and outputs that are difficult to express with a traditional rule based program.
today dnns have been used on a variety of tasks including computer vision natural language processing recommendation systems and various software engineering tasks where they have produced results comparable to and in some cases superior to human experts.
a simple neural network is shown in figure a .
the neural network contains neurons input neurons output neurons and intermediate neurons organized in hidden layers and synapses.
the first hidden layer contains neurons that receive signals from the input neurons and send signals to the second hidden layer which contains neurons that further process the signals and forward to the output neurons.
in this example each neuron except the input neurons performs a weighted sum operation which multiplies each received signal with a learned weight marked on the synapses and computes the sum as the neuron s value.
such weighted sum operations are common in today s neural networks while usually accompanied by other operations such as rectifier maximum etc.
the example neural network is for illustration purpose only and does not produce meaningful output.
real world deep neural networks typically have millions of neurons and synapses .
.
program slicing program slicing is a fundamental technique to support various software engineering tasks in traditional programs such as debugging testing optimization and protection.
it was originally introduced by mark weiser in for imperative procedureless programs.
it aims to compute a program slice sthat consists of all statements in program pthat may affect the value of variable vin a statement x. the slice is defined for a slicing criterion c x v where xis a statement in program pandvis variable in x. the slicing criterion represents an analysis demand relevant to an application e.g.
in debugging the criterion could be the instruction that causes a crash.
at first only static program slicing was discussed which analyzes the source code to find the statements that can affect the value of variable vat statement xfor any possible input.
korel and laski introduced the idea of dynamic program slicing which tries to find the statements that actually affect the value of a variable vfor a particular execution of the program rather than all statements that may have affected vfor any arbitrary execution of the program.
program slicing techniques have been seeing a rapid development since its original definition.
various approaches are proposed to improve the slicing algorithms introduce other forms ofslicing and extending slicing ability to more programming languages and platforms .
meanwhile many applications of program slicing techniques are proposed.
today program slicing is widely used in various software engineering tasks including debugging testing software verification software maintenance and privacy analysis .
there are many comprehensive surveys that summarize the advances in this area.
in this paper we try to implement program slicing for deep neural networks a completely different type of program that consists of mathematical operations with learned weights rather than developer written statements or variables.
.
program analysis for neural networks prior to ours researchers had already attempted to analyze neural networks by applying or borrowing ideas from traditional program analysis techniques.
one of the most widely discussed applications of neural network analysis is to test the robustness of neural networks against adversarial attacks which add small perturbation to the input to fool the dnn models.
deepxplore proposed to use neuron coverage the number of activated neurons to measure the parts of a deep learning system exercised by a set of test inputs and higher coverage usually means higher robustness.
since then several new coverage metrics were introduced and various approaches were proposed to generate test inputs that maximize the coverage .
training the model with the generated test inputs can improve its robustness and accuracy.
in addition to testing many studies have attempted to detect adversarial inputs based on the internal behavior of neural networks.
for example gopinath et al.
and ma et al.
attempted to extract properties or invariants from the neuron activation state and use them to detect adversarial inputs.
wang et al.
borrowed the idea of mutation testing and found that adversarial samples are usually more sensitive to model mutations.
qiu et al.
and wang et al.
extracted a path from the neural network that is the most critical for a sample which can be used to distinguish normal and adversarial samples.
the slice computed in our approach can also be viewed as the decision logic of the neural network and used to identify adversarial samples discussed in section .
.
as neural networks are inherently vulnerable and imprecise researchers had also tried to provide a formal guarantee of security and correctness with the help of program analysis techniques such as constraint solving interval analysis symbolic execution and abstract interpretation .
while promising these techniques usually suffer from poor scalability most of them cannot be applied to today s large neural networks.
there are also some existing work incorporating the idea of slicing to neural networks.
shen et al.
proposed slicing cnn feature maps to understand the appearance and dynamic features in the videos.
cai et al.
proposed to slice a dnn into different groups that can be assembled elastically to support dynamic workload.
however these approaches are not related to program slicing that aims to understand the internal logic of a program.
instead they focused on training or assembling different parts of a dnn.esec fse november virtual event usa ziqi zhang yuanchun li yao guo xiangqun chen and yunxin liu qiu et al.
s work is the closest to ours.
given an image classification model they compute an effective path for each class which contains the neurons and synapses that positively affect the prediction result.
however with regard to the slicing criterion their effective paths may be incomplete i.e.
missing important neurons and synapses such as the ones with negative contributions and imprecise i.e.
including less important neurons and synapses such as the ones yielding a large value for any input .
such shortcomings make their method less useful on applications other than adversarial defense details in section .
motivation and goal .
motivation similar to traditional programs we argue that slicing a dnn is also meaningful and useful for many important software engineering tasks as illustrated below.
first a dnn is a black box whose decisions are hard to interpret .
as a result it is usually hard or even impossible for developers to understand when and why a dnn makes mistakes.
as in traditional programs the input would take a different control flow or data flow if it leads to failures.
it would be potentially beneficial if there is a technique to automatically analyze the decision logic in dnns.
second the size of state of the art dnns and their required computing power have been growing rapidly in recent years thus it is highly desirable to reduce the size of dnns to improve efficiency without sacrificing too much accuracy.
model pruning removing some neurons and synapses is one of the most widely used techniques .
however how to prune the model i.e.
which neurons and synapses to remove is a key question as we do not want to remove the critical structures that may lead to severe performance degradation.
deciding which neurons and synapses to prune is quite similar to computing a program slice.
third model protection i.e.
preventing the model from getting stolen is on increasing demand as models are traded and shared across different organizations.
various techniques such as homomorphic encryption and hardware enclave can be used to protect models but protection often brings performance degradation.
a practical solution is to protect a part of the model instead of the whole model .
thus partitioning the neural network to important and unimportant slices may be beneficial as we can assign limited protection resources to more important slices.
the similarity between these tasks is the demand to find a subset of neurons and synapses that are more important in the decisionmaking process which is the goal of this paper.
.
problem formulation this section defines the concepts and symbols that will be used in this paper and formulates the goal of dnn slicing.
we first formulate the definition of neuron and synapse two key concepts used throughout this paper.
a neuron nin a neural network is a mathematical operator that takes one or more numerical inputs and yields one numerical output.
nis said to be activated if its mathematical operation is executed and the operation resultyis called the activation value.
a neuron nhas one or more synapses s1 s2 ... sk weighted with w1 w2 ... wk respectively.table definition of symbols commonly used in this paper.
symbol meaning m n s modelmwith neuron setnand synapse sets n y neuron nand its activation value y s x w synapse s its input value xand weight w i input dataset iand an input sample i o o output neuron set oand an output neuron o o c mcslicing criterioncand its corresponding slice cont ri bcumulative contribution of a neuron or a synapse i.e.
the contribution to the slicing criterion contriblocal contribution of a neuron or a synapse i.e.
the contribution generated in an operation hyperparameter to control the slice quality each synapse siscales the activation value of another preceding neuron xiwith wiand passes the scaled value to the neuron nas input.
similarly the activation value of neuron nis also passed to other succeeding neurons by other synapses.
the very last neurons that do not have succeeding neurons are the output neurons whose activation values are the output of the neural network model.
any modern dnn architecture can be viewed as a combination of such neurons and synapses.
for example a fully connected layer that maps inputs to outputs can be seen as a combination of neurons each of which computes the sum of values from weighted synapses.
a 32filter in a convolutional layer can be viewed as neurons each of which computes the sum over weighed synapses.
a rectified linear unit relu can be viewed as a neuron with only one synapse.
note that a neuron may be activated several times with different input values during the inference pass of a sample such as the neurons in convolutional layers.
based on the concept of neurons and synapses we further define the symbols that will be commonly used later as shown in table .
the formal definition of neural network slicing is given as follows definition .
neural network slicing letm n s represents a neural network and c i o is a slicing criterion.
i 1 2 .
.
.
nis a set of model input samples of interest and o o1 o2 .
.
.
okis a set ofm s output neurons of interest.
the goal of slicing is to compute subsets nc n andsc s with respect to c denoted asmc nc sc that significantly above a predetermined threshold contributes to the value of any output o o for any input sample i. .
challenges there are three main challenges to slice a neural network.
understanding the behavior of each neuron.
unlike an instruction or a function in traditional programs a neuron is typically a simple mathematical operation that does not have any high level semantic meaning.
the weights of all neurons in a model are learned as a whole to fit the training data while each neuron is just a small building block whose functionality is vague.
however to compute a slice we must first be able to differentiate the neurons based on their behavior.dynamic slicing for deep neural networks esec fse november virtual event usa quantifying the contribution of each neuron.
in traditional program slicing each instruction s contribution to the slicing criterion is binary an instruction either affects or is irrelevant to the values of the criterion.
in neural network slicing almost all neurons are connected to the output neurons in the slicing criterion and contribute to the outputs more or less.
it is difficult to quantify the contribution of each neuron to extract the most important neurons.
dealing with large models.
today s state of the art neural networks typically contain millions of neurons that are densely connected.
analyzing a network on such a scale poses a higher demand for efficiency.
how to design algorithms that can leverage existing computing resources to speed up the analysis is also a challenging problem.
our approach nnslicer we introduce nnslicer to address the above challenges.
section .
presents an overview of our approach.
section .
describes how we understand neuron behaviors through differential analysis.
section .
introduces our backward data flow analysis technique that quantifies the contribution of each neuron to the slicing criterion.
finally section .
briefs how the computation power of gpus and multi core cpus are utilized to improve the efficiency of our method.
.
approach overview the overview of our approach is illustrated in figure .
the program under analysis in our system is a pretrained neural network model whose weights are already learned to fit a training dataset.
in figure a the weight values are labeled on the corresponding synapses in the network.
our approach mainly consists of three phases including a profiling phase a forward analysis phase and a backward analysis phase.
in the profiling phase all samples in the training dataset are fed into the model each sample produces an activation value at each neuron.
we log the activation values of each neuron for all input samples and compute the mean activation value which is the output of the profiling phase as labeled on each neuron in figure b .
the mean activation values can be viewed as the behavioral standard of a neuron.
then in the forward analysis phase each interested sample in the slice criterion is fed into the model.
we record the activation value of each neuron and compute its difference with the mean activation value obtained through profiling as labeled on each neuron in figure c .
such relative activation values represent the neuron reaction to the input sample.
finally in the backward analysis phase we start from the output neurons defined in the slicing criterion and iteratively compute the contributions of preceding synapses and neurons.
the synapses and neurons with larger contributions are the slices computed for the slicing criterion.
each step is detailed and formulated in the following sections.
.
profiling and forward analysis the behavior of a neuron during an inference pass is represented as an activation value or a list of activation values if the neuron was activated several times .
the activation values are arbitrarynumbers produced by simple mathematical operations.
we first need to make sense of the activation values.
specifically does the neuron react positively or negatively and how much?
our method is inspired by the work on differential power analysis which decodes the power consumption measurements of a circuit by testing the circuit with different inputs.
the power consumption difference can be used to infer the input and program logic.
in our case the activation value of a neuron is like the power consumption measurement that barely makes sense by itself but the difference between the activation values for different input samples can reveal how the neurons react to each sample.
specifically we use the difference between the neuron behavior for an input sample andits average behavior for all training samples to understand the neuron reaction to the input.
suppose is an input sample and nis a neuron of model m. by feeding intom we would observe an activation value yn at neuron n.yn meanm i 1yn i ifnis activated multiple times where yn i is the i th activation value and mis the total number of activations of n e.g.m 1ifnis a neuron in a fully connected layer and mequals to the number of convolution operations performed by the filter if nis in a convolutional layer .
the average neuron activation value over the whole training dataset dis calculated by yn d dyn d such average activation values can be viewed as the behavioral standard of the neurons which can be used as the baseline to measure a neuron s reaction to a specific data sample.
since ynis not dependant on any specific input or output it only needs to be computed once and can be used for different slicing goals.
in the forward analysis phase we quantify the reaction of the neuron nfor a specific data sample as its relative activation value yn yn yn d a positive yn means that neuron nreacts more positively to the sample than most other samples and vice versa.
the magnitude of yn represents the sensitivity of nwith regard to .
as an example the output neuron of an image classification model that is trained to detect cats would be more sensitive and positively react to an image of a cat as compared with an image of a truck.
.
backward analysis and slice extraction the backward analysis aims to compute the contribution of each neuron and each synapse to the interested outputs in the slicing criterion.
note that the neuron s reaction to an input sample computed through the profiling and forward analysis is not equivalent to its contribution.
for example in an image classifier a neuron that reacts sensitively to cat images may not have any contribution if our interested output is the truck label.
to compute the contribution we introduce a backward data flow analysis method.
in traditional programs extracting the instructions and variables that contribute to a certain output is easy based on the data flow graph dfg which defines the data dependencies between the instructions and variables.
a neural network can also be viewed as a data flow graph but the graph is densely connected.
for modern dnns that are organized layer by layer almost every neuron in one layer is connected to all neurons in the previous layer as shown inesec fse november virtual event usa ziqi zhang yuanchun li yao guo xiangqun chen and yunxin liu 2output output 2input input 2output output 2input input 2output output 2input input 31profiling feed all training samples to the model 2forward analysis feed the interested samples to the modelpretrained modeleach neuron s average activation value in this example all average values are set to for simplicity each neuron s reaction to input i.e.
difference between the activation value and the average1 2output output 2input input the slice for output and input 3backward analysis backtrack from the slicing criterion neuron figure the overview of our approach.
figure .
thus we need to further analyze the data flow graph to measure the contribution of each neuron.
the contribution of a neuron or synapse is quantified as an integer in nnslicer denoted as contrib .nis a critical neuron if contrib n and a critical neuron may contribute positively contrib n or negatively contrib n to the slicing criterion.
the same for the synapses.
our method to compute contrib is to recursively compute the contributions of preceding neurons and synapses from back to front.
given a neural network and a list of target neurons we first consider the neurons that are directly connected to the interested neurons whose contribution can be extracted with their activation values in detail later .
then we remove the target neurons from the network and set the neurons with non zero contribution as the target neurons.
we repeat the process until the target neurons do not have any neighboring neurons.
the algorithm is described in algorithm .
note that in practice neurons are organized as partially ordered layers thus each iteration of algorithm deals with a single layer.
algorithm simplifies the problem of computing cumulative contributions of all neurons and synapses in the whole network to computing local contributions of preceding neurons and synapses in an operation line .
local contributions mean the contributions generated solely by the operation.
we use the weighted sum operator a common operator in neural networks to illustrate how we compute the contributions of preceding neurons and synapses.
in the weighted sum operator the central neuron nhasksynapses si s2 ... sk that connect kpreceding neurons n1 n2 ... nk ton.
the activation value of nis computed as y k i 1wixi wherealgorithm computecontrib computing the contributions of neurons and synapses to a list of target neurons for an input sample require a neural network model m n s an input sample and a list of target neurons o. a global table contrib that stores the cumulative contribution of each neuron and synapse during the inference pass of initialized to .
terminate ifois empty initializeo foreach neuron o odo find o s preceding neurons and synapses n s compute local contributions of n ands ascontrib update contrib with contrib end for foreach neuron n ndo addntoo ifnis a predecessor of oandcontrib n end for obtainn by removing neurons in ofromn callcomputecontrib by settingo o andn n return the global cumulative contribution table contrib wiis the weight of synapse siandxiis the activation value of ni.
suppose the cumulative contribution of niscontrib n the local contribution contrib iofniandsiis computed as contrib i contrib n yn wi xi in which ynis the relative activation value of the central neuron given by equation and xiis the relative activation value of the neuron ni i.e.
yni .
the product of ynandwi xirepresentsdynamic slicing for deep neural networks esec fse november virtual event usa the impact that niandsimay have on the global contribution contrib n. for example if ynis negative and wi xiis positive it means that nienlarges the negativity of n yielding an contribution that is opposite to contrib n. the weighted sum operators take the vast majority in today s dnns but there are also other types of operators.
in this paper we focus on convolutional neural networks cnns .
table shows five common operators that are enough to handle most existing cnn models.
to support other architectures one only needs to define the method to compute local contributions for new operators as shown in table .
the cumulative contribution contrib of neuron niand synapse siin the operation is updated by their local contribution contrib ni si n contrib i contrib si si n contrib i we only keep the sign of the local contribution as different operations may have different scales of local contributions.
however updating the cumulative contribution for all neurons and synapses is time consuming a neuron with non zero cumulative contribution introduces a new branch during backtracking and may accumulate contributions from unimportant neurons and synapses.
thus we limit the number of local contributions used to update contrib .
the importance of a local contribution is represented by its magnitude and those with smaller magnitude can be excluded when updating contrib .
specifically we first sort the local contributions in ascending order of their magnitudes.
the preceding neurons are sorted as n1 n2 ... nk.
then we try to find a maximum index jso that n1 ... njcan be excluded while the influence on the activation value of nis below the threshold .
for example in a weighted sum operation the influence of excluding n1 ... njis k i jwi xi y .
controls the amount of excluded local contributions with minimal influence on the functionality of an operation and thus the generated slice can be directly used to make predictions without retraining evaluated in section .
.
the value of the threshold can be set by different applications to control the size of the resulting slice.
so far the cumulative contribution contrib captures the contribution of neurons synapses during the inference of a single input sample.
for a slicing criterion c i o that may contain multiple interested samples the final cumulative contribution is the sum of the contribution for each sample i. a slice for cis mc nc sc wherencandscare the neurons and synapses with non zero contributions.
one can also control the size of slice based on the contributions as in .
.
.
gpu and multi thread acceleration nnslicer takes a forward analysis pass and a backward analysis pass for each data sample i when computing the slices.
it might be very time consuming if i is large.
since the process of computing slice for a data sample is independent of each other we can take advantage of the parallel characteristic of gpu and multi threading to accelerate the overall slicing process.
specifically for a large set of data samples i we first run the profiling and forward analysis phases on gpu using large batches as these two phases only involve forward computation.
then alarge batch is separated into several small batches.
the backward analysis of each small batch runs on the cpu as a separate thread.
finally the batches are merged together to generate the slicing result.
implementation overhead we implemented nnslicer in python with tensorflow.
the profiling and forward analysis are implemented based on tensorflow s instrumentation mechanism.
the multi thread computing is implemented by the distributed python library ray we evaluated the time overhead of nnslicer on a server that has geforce gtx 1080ti gpus intel xeon cpus with cores and 64gb memory.
table reports the slice time and the architecture complexity of three models.
the time spent by nnslicer to compute slice for a data sample is roughly 4s 39s and 553s for lenet resnet10 and resnet18 respectively.
when computing slice for a batch of inputs the speed is much faster which is about .6s .4s and .2s per data sample for the three models respectively.
note that the profiling phase is not included when computing the slicing speed as it only needs to run once for a model.
applications in this section we describe three applications to demonstrate the usefulness and the effectiveness of nnslicer including adversarial defense model pruning and model protection.
in each application we describe why the application is meaningful how nnslicer can help and how nnslicer performs compared with other methods.
the main method which we compare nnslicer against is the state of the art work by qiu et al.
denoted as effectivepath below .
we also include some other baselines for more comparisons.
.
adversarial defense adversarial examples are carefully crafted inputs that may lead to wrong predictions.
they are usually generated by adding small permutation to a benign input which is barely noticeable by a human.
adversarial attacks may cause severe consequences especially in safety and security critical scenarios.
as a result adversarial defense became a hot research topic in both ai and se communities.
many approaches tried to make the dnns more robust through training or adding advanced architectures but it is still hard to obtain a robust model.
instead some researchers opted to take another direction adversarial input detection with which the deep learning system can raise warnings or stop serving once suspicious inputs are detected.
thus severe attacks can be avoided.
in this section we discuss how nnslicer can be used to detect adversarial inputs.
.
.
method.
our insight is that the slice computed by nnslicer can be viewed as an abstraction of the decision process and the decision processes of normal examples and adversarial examples are intuitively different.
as shown in figure although the normal image and the adversarial one are indistinguishable for a human their slices are different.
thus by learning from the slices of large scale normal examples we can understand the normal decision process of the dnn.
therefore given a new input if its slice is distinctly different from the normal slices it is very likely an adversarial input.esec fse november virtual event usa ziqi zhang yuanchun li yao guo xiangqun chen and yunxin liu table neuron operations considered in nnslicer.
operation usage math form local contribution of i th input weighted sum convolutional layers and fully connected layers etc.y k i 1wixi contrib n y wi xi average average pooling layers.
y kk i 1xi contrib n y xi maximum max pooling layers.
y maxk i 1xi contrib n y xii f x i yelse0 rectify relu activation.
y xifx 0else0contrib n y x i f x 0else0 scale batch normalization.
y x contrib n y x table the time spent to process an input sample in each phase.
the profiling and forward analysis phases take the same amount of time as they both only require an inference pass.
model paramsprofiling forward backward single batch single batch lenet .0s .3s .5s .3s resnet10 300k .9s .4s .1s .0s resnet18 11m .6s .8s .0s .4s normalexample predictedasdog normalexample predictedasairplane adversarialexample predictedasdog figure normal and adversarial examples top and their visualized slices bottom .
each pixel in the visualization represents a neuron from a random convolutional layer separated to two rows .
the neurons with non zero contributions are colored blue for neurons with positive contributions and red for those with negative contributions .
specifically suppose mis the dnn model that may accept adversarial inputs is an input sample and m is the label of predicted bym.
using nnslicer we can compute a slice m for each input by setting the slicing criterion as c m .
we build a slice classifier fthat predicts the label of an input based on the slice computed for the input m .
by training fwith a large number of normal samples it can capture the mapping pattern between the slice shape and the corresponding output category.
with the trained slice classifier f an input is identified as adversarial iff m m i.e.
the prediction made by the slice classifier is different from the prediction of the original dnn model.the input of the slice classifier i.e.
a slicem is represented as a vectorvec .
each element in vec corresponds to a synapse and its value is the contribution of the synapse as described in section .
.
for the simplicity of the input and output representations many classification algorithms may be used to build the slice classifier.
we chose to use the decision tree as it is easy to implement and debug.
applying nnslicer to adversarial input detection has three advantages nnslicer does not require modifying or retraining the original model and thus nnslicer can support any dnn models.
nnslicer can scale up to support large state of the art dnn models while existing methods like ones by ma et al.
and gopinath et al.
can only support small models.
nnslicer requires only the normal samples to build the defense but existing methods need to train a detector with both normal and adversarial examples.
as the attackers can always use new adversarial examples nnslicer is a much more realistic solution than those existing methods.
.
.
evaluation.
we compare our detection method with two baselines.
for a fair comparison the baseline methods use the same classifier to identify adversarial inputs as ours while the inputs of the classifier are different.
featuremap is a naive baseline that uses the feature maps of convolutional layers as the inputs of the classifier.
effectivepath is a more advanced baseline that uses the effective path generated by qiu et al.
to train the classifier.
the experiments were conducted on resnet10 and the cifar dataset image size .
all the classifiers were trained with normal samples using their respective feature extraction methods.
we tested nnslicer and the two baselines on attacks covering gradient based attacks score based attacks and decision based attacks.
these attacks include fgsm with per pixel maximum modification of and relative to and referred to as fgsm 2 fgsm 4 and fgsm 8 respectively deepfool with constrain norm l2andl referred to as deepfooll2 and deepfoollinf jsma attack pgd attack with random start and perpixel maximum modification of and referred to as rpgd 2 rpgd 4 and rpgd 8 the l2version of cw attack cwl2 adef attack an attack that just perturbs a pixel singlepixel a greedy local search attack localsearch a boundary attack boundary an attack of spatial transformation spatial andynamic slicing for deep neural networks esec fse november virtual event usa attack that performs binary search between a normal sample and its adversarial instance pointwise and an attack that blurs the input until it is misclassified gaussianblur .
all the attacks are implemented with foolbox .
for each attack method we generate adversarial examples from randomly picked normal examples.
the examples that successfully mislead the model are fed to the detector with their normal examples.
we compute the precision recall and f1 score of each detector on each attack as shown in table .
according to the experiment result nnslicer is very effective in detecting adversarial inputs with an average recall of and an average precision of which means that nnslicer is able to correctly identify all the adversarial examples generated with these attack methods no false negative .
meanwhile most of the inputs identified by nnslicer are indeed adversarial inputs while only a few normal samples are misidentified false positives .
although effectivepath also achieves a perfect recall its precision is much lower meaning that the detector may easily misclassify normal samples as adversarial inputs.
the average recall of in featuremap represents the feature maps between normal examples and the adversarial examples are barely discriminative.
this phenomenon indicates the demand for nnslicer to explore the mechanism of neural networks.
.
network simplification and pruning the size and complexity of dnn models grow rapidly.
although these huge models achieve high scores on complicated datasets they are cumbersome and slow in real world task specific applications.
how to reduce the model size and speed up the computation is crucial to the dnn applications.
one acceleration technique is to prune trivial synapses of a large model to generate a light weight one.
with redundant weights trimmed off the computation of executing the model may be reduced.
existing network pruning methods focus on reducing the network architecture of models for all the output classes .
with dnn slicing nnslicer enables more flexible network simplification and pruning by focusing on a targeted subset of output classes.
that is for a subset of the original output classes of a model nnslicer can decide the proper model slices for the targeted output classes.
thus nnslicer can generate a smaller model for the targeted output classes with higher model accuracy.
this advantage of nnslicer is highly desirable in real world applications that usually deal with a small set of output classes e.g.
classifying only different dogs rather than types of animals .
.
.
method.
nnslicer can pick out neurons and synapses critical to a slice criterionc i o .
by settingoto the set of interested target classes nnslicer can compute contrib sfor each synapse s which represents the synapse s importance to the target classes.
we can trim out the less important synapses and get a model that still functions on the target classes.
specifically suppose we want to prune mfor target classesot with prune ratio r. letitbe the set of data samples belonging to the interested classes.
contribtis the cumulative contributions computed by nnslicer and contribtsis the contribution of synapse s. for each layer l we sort all synapses in the layer sl by the ascending order of their contributions magnitudes.
the first figure accuracy of the pruned models without finetuning.
r s l synapses are pruned and a neuron is also pruned if its synapses are all pruned.
.
.
evaluation.
to evaluate the ability of nnslicer to targeted pruning each of subsets of cifar10 s output classes is used as the target classes ot.nnslicer targeted represents to prune synapses according to the contributions computed for the target classesot.nnslicer all represents to prune according to the contributions computed for all output classes o. the comparison between nnslicer targeted andnnslicer all demonstrates nnslicer s ability in target classes.
we also compare it with several baselines.
effectivepath represents pruning synapses based on the feature computed in .weight is based on the absolute synapse weights where the synapses with the smallest weights are trimmed .
similarly channel represents to prune the least important neurons by the average connected weight value .
both weight andchannel are widely used techniques in the field of network pruning.
figure shows the average accuracy over possible target classes.
the accuracy of nnslicer targeted is always high and is around when of weights are pruned.
the accuracy of effectivepath andchannel are both low in the figure.
the accuracy of nnslicer all andweight is high only when the prune ratio is below .
the comparison between nnslicer targeted andnnslicer all demonstrates the ability of nnslicer to prune for specific classes.
the large gap between nnslicer targeted andeffectivepath indicates the advantage of nnslicer to the feature computed by .
when the prune ratio becomes larger we further evaluate the performance with fine tuning.
to do it the pruned models are retrained on 10k samples for epoch.
figure shows the performance of the fine tuned models on two sets of target classes.
the finetuned model of nnslicer is noticeably higher than other methods.
it shows that nnslicer targeted preserves the model s capability on targets even when a large portion of weights is trimmed.
a short fine tuning epoch in this case is enough for the model to achieve high accuracy.
one possible reason for nnslicer s good performance is that it preserves the model s ability to target classes at the cost of other non target classes.
in an extra experiment the performance of nnslicer targeted on non target classes is remarkably lower than target classes.
on the other hand the difference of weight is small.
it means nnslicer can decompose the model over classes and makeesec fse november virtual event usa ziqi zhang yuanchun li yao guo xiangqun chen and yunxin liu table adversarial input detection accuracy for different attack methods.
attack methodeffectivepath featuremap nnslicer f1 precision recall f1 precision recall f1 precision recall gradient basedfgsm 2 .
.
.
.
.
.
.
.
.
fgsm 4 .
.
.
.
.
.
.
.
.
fgsm 8 .
.
.
.
.
.
.
.
.
deepfoollinf .
.
.
.
.
.
.
.
.
deepfooll2 .
.
.
.
.
.
.
.
.
jsma .
.
.
.
.
.
.
.
.
rpgd 2 .
.
.
.
.
.
.
.
.
rpgd 4 .
.
.
.
.
.
.
.
.
rpgd 8 .
.
.
.
.
.
.
.
.
cwl2 .
.
.
.
.
.
.
.
.
adef .
.
.
.
.
.
.
.
.
score basedsinglepixel .
.
.
.
.
.
.
.
.
localsearch .
.
.
.
.
.
.
.
.
decision basedboundary .
.
.
.
.
.
.
.
.
spatial .
.
.
.
.
.
.
.
.
pointwise .
.
.
.
.
.
.
.
.
gaussianblur .
.
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
.
figure accuracy of the pruned models after fine tuning for one epoch.
a trade off to conserve the ability on target classes.
a similar phenomenon is observed in model protection and will be discussed in section .
.
.
model protection dnn models are becoming valuable assets due to the high cost of the training process including collecting a large amount of data expensive gpu usage and enormous power consumption.
however an attacker may retain or steal the functionality of a model at a comparatively low cost .
how to protect models from being stolen is becoming an increasingly important problem particularly in the emerging edge computing where models are deployed to edge servers or even end devices.
existing solutions of model protection usually leverage encryption using homomorphic encryption or zero knowledgeproof or running a model inside trusted execution environments .
all sensitive computation is conducted in the encrypted mode.
however the cost of these protected computations is high.
for example cryptonets takes around 300s to execute a model on the small mnist dataset.
to reduce the cost of model protection one approach is to secure the important computation only where nnslicer may help.
.
.
method.
the existing model protection work is constrained to protecting the model w.r.t.
the whole label space .
but the importance of outputs may vary.
for some outputs the data is more difficult to collect or the annotation is particularly more expensive.
because nnslicer can slice model for certain classes it can help to find significant components for the expensive classes and protect them.
we propose to incorporate targeted protection in this scenario.
compared to existing work our method is more flexible and can customize the protection target.
nnslicer selects synapses from a model and protects their weights.
the way to select synapses is similar to section .
but nnslicer selects the most crucial synapses for the target classes.
the selected synapses are protected from attackers who have to recover the protected synapses through retraining to obtain the whole model.
.
.
evaluation.
in the experiment we assume a strong attacker who has a training dataset.
the attacker s dataset size is called the budget .
as nnslicer protects a limited ratio of synapses we use the metric of the accuracy of protected classes after re training for epochs.
a lower accuracy stands for better protection.
we compare with three baselines effectivepath weight and random .
effectivepath andweight are the same methods used in section .
.
random is to randomly select synapses.
figure shows the accuracy of the protected classes target classes the left figure and the accuracy of all classes all classes the right figure .
it can be observed that after guarded by nnslicer dynamic slicing for deep neural networks esec fse november virtual event usa figure the accuracy achieved by retraining the models for epochs.
of the parameters selected with different methods in the model are hidden and the attacker tries to recover them through retraining.
the x axis is the attacker s budget i.e.
number of samples used to retrain .
a lower accuracy achieved with a fixed budget means better protection.
the retrained accuracy on target classes is below even when the budget i.e.
number of samples achieves 50k.
the small accuracy stands for strong protection.
on the contrary the accuracy of other methods is all above .
for effectivepath the accuracy achieves which means it can not protect target classes at all.
the right figure of figure illustrates why nnslicer achieves better protection.
compared to weight although the accuracy of nnslicer on target classes is obviously lower the accuracy over the whole dataset is higher.
it means the accuracy of non target classes is very high and nnslicer do not protect them.
this tradeoff between target classes and non target classes is similar to the finding in section .
and may be valuable for applications that desire to protect a small set of target classes.
limitations and discussion this section highlights some of the limitations of nnslicer and discusses possible future directions.
dnn architectures.
we only considered five common operations that are commonly used in cnn models while some operations used in other architectures are not included such as recurrent neural networks rnns and graph convolutional networks gcn .
these architectures should be easy to support in the future by adding backtracking rules for new operators.
scalability.
in this paper we did not conduct experiments on very large models and datasets due to limited time.
for large dnn models with millions of weights nnslicer takes about minutes to compute the slice for an input sample as shown in table .
building an adversarial defense as in section .
for such a large model may take several days on a single machine.
although the process is slow especially for in lab experiments we think it is acceptable in practice considering the fact that companies usually train a model on large clusters for several weeks.
slicing criterion.
we mainly discuss the slicing criterion concerning only output neurons but slicing for an intermediate neuronmay also be interesting similar to inspecting an intermediate variable in traditional programs .
such a flexible criterion definition may enable new applications e.g.
interpreting or debugging the neural network in finer granularity.
more applications.
beside the three applications discussed in this paper there are many other applications that are interesting to consider.
for example is it possible to compose different slices to a new model?
if it is the case the way of training networks might be changed.
besides is it possible to slice certain attributes from a trained model such as a discriminatory attribute race gender etc.
which we want to exclude from consideration when making decisions?
last but not least how can nnslicer be used to debug model and diagnose fragile weights?
section .
has proved its ability to detect adversarial examples a step forward is to find the deviant neurons or synapses that are critical for errors.
masking them out or adjusting their value may improve the model accuracy.
other slicing techniques.
nnslicer relies on a set of inputs to compute the slice i.e.
dynamic slicing .
there are various other slicing techniques that may be interesting to be applied to neural networks.
for example static slicing might be used to compute input independent slices as in section .
much faster as each input doesn t need to be processed separately.
conditioned slicing may help the developers to understand the conditions e.g.
illumination viewpoint etc.
under which the dnn is more vulnerable.
amorphous slicing may be used to merge neurons and synapses inside the network and slim the network structure .
concluding remarks this paper proposes the idea of dynamic slicing on deep neural networks and implements a tool named nnslicer to compute slices for convolutional neural networks.
the working process of nnslicer consists of a profiling phase a forward analysis phase and a backward analysis phase.
the profiling and forward analysis phases model the reaction of each neuron based on its activation values.
the backward phase traces the data flow recursively from back to front and computes the contributions of each neuron and synapse which are used to calculate the slice.
the usefulness and effectiveness of nnslicer are demonstrated with three applications on adversarial input detection targeted model pruning and selective model protection.
the code and data of nnslicer and all applications will be made available to the community.