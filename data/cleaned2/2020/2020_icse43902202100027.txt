towards automating code review activities rosalia tufano luca pascarella michele tufanoy denys poshyvanykz gabriele bavota seart software institute universit a della svizzera italiana usi switzerland ymicrosoft usa zsemeru computer science department william and mary usa abstract code reviews are popular in both industrial and open source projects.
the benefits of code reviews are widely recognized and include better code quality and lower likelihood of introducing bugs.
however since code review is a manual activity it comes at the cost of spending developers time on reviewing their teammates code.
our goal is to make the first step towards partially automating the code review process thus possibly reducing the manual costs associated with it.
we focus on both the contributor and thereviewer sides of the process by training two different deep learning architectures.
the first one learns code changes performed by developers during real code review activities thus providing the contributor with a revised version of her code implementing code transformations usually recommended during code review before the code is even submitted for review.
the second one automatically provides the reviewer commenting on a submitted code with the revised code implementing her comments expressed in natural language.
the empirical evaluation of the two models shows that on the contributor side the trained model succeeds in replicating the code transformations applied during code reviews in up to of cases.
on the reviewer side the model can correctly implement a comment provided in natural language in up to of cases.
while these results are encouraging more research is needed to make these models usable by developers.
index terms code review empirical software engineering deep learning i. i ntroduction code review is the process of analyzing source code written by a teammate to judge whether it is of sufficient quality to be integrated into the main code trunk.
recent studies provided evidence that reviewed code has lower chances of being buggy and exhibit higher internal quality likely being easier to comprehend and maintain.
given these benefits code reviews are widely adopted both in industrial and open source projects with the goal of finding defects improving code quality and identifying alternative solutions.
the benefits brought by code reviews do not come for free.
indeed code reviews add additional expenses to the standard development costs due to the allocation of one or more reviewers having the responsibility of verifying the correctness quality and soundness of newly developed code.
bosu and carver report that developers spend on average more than six hours per week reviewing code .
this is not surprising considering the high number of code changes reviewed in some projects rigby and bird show that industrial projects such as microsoft bing can undergo thousands of code reviews per month 3k in the case of bing .
also as highlighted by czerwonka et al.
the effort spent in codereview does not only represent a cost in terms of time but also pushes developers to switch context from their tasks.
our long term goal is to reduce the cost of code reviewing by partially automating this time consuming process.
indeed we believe that several code review activities can be automated such as catching bugs improving adherence to the project s coding style and refactoring suboptimal design decisions.
the final goal is not to replace developers during code reviews but work with them in tandem by automatically solving or suggesting code quality issues that developers would manually catch and fix in their final checks.
a complete automation besides likely not being realistic would also dismiss one of the benefits of code review the sharing of knowledge among developers.
in this paper we make a first step in this direction by using deep learning dl models to partially automate specific code review tasks.
first from the perspective of the contributor i.e.
the developer submitting the code for review we train a transformer model to translate the code submitted for review into a version implementing code changes that a reviewer is likely to suggest.
in other words we learn code changes recommended by reviewers during review activities and we try to automatically implement them on the code submitted for review.
this could give a fast and preliminary feedback to the contributor as soon as she submits the code.
this model has been trained on code pairs of cs!cr where csis the code submitted for review and cris the code implementing a specific comment provided by the reviewer.
once trained the model can take as input a previously unseen code and recommend code changes as a reviewer would do.
the used architecture is a classic encoder decoder model with one encoder taking the submitted code as input and one decoder generating the revised source code.
second from the perspective of the reviewer given the code under review c s we want to provide the ability to automatically generate the code crimplementing on csa specific recommendation expressed in natural language r nl by the reviewer.
this would allow i the reviewer to automatically attach to her natural language comment a preview of how the code would look like by implementing her recommendation and ii the contributor to have a better understanding of what the reviewer is recommending.
for such a task we adapt the previous architecture to use two encoders and one decoder.
the two encoders take as input csandrnl respectively while the decoder is still in charge of generating cr.
the model has been trained with triplets hcs rnli!cr.
ieee acm 43rd international conference on software engineering icse .
ieee note that the two tackled problems are two sides of the same coin in the first scenario cris generated without any input provided by the reviewer thus allowing the usage of the model even before submitting the code for review.
in the second scenario cris generated with the specific goal of implementing a comment provided by the reviewer thus when the code review process has already started.
we quantitatively and qualitatively evaluate the predictions provided by the two approaches.
for the quantitative analysis we assessed the ability of the models in modifying the code submitted for review exactly as done by developers during real code review activities.
this means that we compare for the same code submitted for review the output of the manual code review process and of the models both in the scenario where a natural language comment is provided or not as input .
the qualitative analysis focuses instead on characterizing successful and unsuccessful predictions made by the two models to better understand their limitations.
the achieved results indicate that in the contributor scenario encoder model the model can correctly recommend a change as a reviewer would do in to of cases depending on the number of candidate recommendations it is allowed to generate.
when also having available a reviewer comment in natural language i.e.
reviewer scenario encoder model the performances of the approach are boosted with the generated code that correctly implements the reviewer s comment in to of cases.
these preliminary results can pave the way to novel research in the area of automating code review.
ii.
u sing transformers to automate code review fig.
shows the basic steps of our approach.
in a nutshell we start by mining code reviews from java projects hosted on github and or using gerrit as code review platform step in fig.
.
given a code submitted by a contributor for review we parse it to extract the list of methods it contains.
indeed in this first work on automating code reviews we decided to focus on small and well defined code units represented by methods.
we identify all submitted methods ms. then we collect reviewer s comments made on each ms by exploiting information available in both github and gerrit linking a reviewer comment to a specific source code line.
we refer to each of those comments as rnl i.e.
a natural language recommendation made by a reviewer .
in such a phase a set of filters is applied to automatically discard comments unlikely to recommend and results in code changes e.g.
thank you well done etc.
.
if the contributor decides to address some of the received rnl this will result in a revised version of msaddressing the received comments.
we refer to such a version as mr. both msandmrare abstracted to reduce the vocabulary size and make them more suitable for dl .
to increase the likelihood that mractually implements in msa specific reviewer s comment we only consider msthat received a single comment in a review round.
thus if a revised version ofmsis submitted we can conjecture that it implements a single comment received by a reviewer .such a process results in a dataset of reviewed commented code triplets rccts in the form hms rnli!mr.
such a dataset is used to train a transformer architecture using two encoders one processing msand one rnl and one decoder generating mr .
such a model is able given a java method ms and a reviewer comment about it r nl to generate a revision of msimplementing rnl i.e.
mr .
starting from this dataset we also generate a dataset of code pairs ms!mrobtained by removing rnlfrom each of the previous dataset triplets.
this dataset has been used to train a second transformers based model having one encoder processing ms and one decoder generating mr .
once trained this model can take as input a previously unseen java method m s and recommend a revised version of it m r that would likely result from a review round.
since no input is required from the reviewer in this model it can be used by the contributor to double check her implementation before submitting it for review.
next we describe the different steps behind our approach.
a. mining code review data we built two crawlers for mining from gerrit and github code review data.
before moving to the technical details it is important to clarify what the goal of this mining process is.
once a code contribution i.e.
changes impacting a set of existing code files or resulting in new files is submitted for review it can be a subject to several review rounds.
let us assume that csis the set of code files submitted for review since subject to code changes.
a set of reviewer comments frnlgcan be made on csand if some all of them are addressed this will result in a revised version of the code cr1.
this is what we call a review round and can be represented by the triplethcs frnlgi!
cr1.
the completion of a review round does not imply the end of the review process.
indeed it is possible that additional comments are made by the reviewers oncr1and that those comments are addressed.
this could result for example in a second triplet hcr1 frnlgi!
cr2.
the goal of our mining is to collect all triplets output of the code review rounds performed in gerrit and in github.
to this aim we developed two miner tools tailored for systematically querying gerrit and github public apis.
the double implementation is required because despite the fact that both platforms provide a similar support for code review the public apis used to retrieve data differ.
gerrit does not offer an api to retrieve all the review requests for a given project but it is possible to retrieve them for an entire gerrit installation i.e.
an installation can host several projects such as all android related projects .
starting from this information we collect all review rounds and finally we reorganized the retrieved data by associating the own set of reviews to each project.
overall we mined six gerrit installations for a total of projects.
github instead offers an api to collect a list of ids of all review requests per project.
in this case we mined a set of github java repositories having at least prs obtained by querying the github apis.
164reviews mining abstractionstring get please add parenthesis in if statement in getid methodgithub gerrit public class student int id string name public int getid if id return id return public string getname return name public class student int id string name public int getid if id return id return public string getname return name java submittedjava revisedcomment please add parenthesis in if statement in getid methodcomment public int getid if id return id return public int getid if id return id return method submitted extractedmethod revised extracted idioms src2abs lizard please add parenthesis if statement getid method public int getid if id return id return public int getid if id return id return abstract cleaned comment abstract submitted method abstract revised method ms mr rnl ms mr dp dt multi head attention feed forward multi head attention feed forwardmasked multi head attention 2x 4x multi head attention feed forward 2xencoder encoderdecodermulti head attention feed forwardmulti head attention feed forwardmasked multi head attention 1x 3xencoderdecoderopennmt tf transformer encoder transformer encoders 890k comments review 234k comments and methods extractedcomments and methods abstracted1 17k pairs 17k tripletsfig.
approach overview.
the output of this process is represented for each review round by i the set of code files submitted for review ii the comments received on this code files with information about the specific impacted lines character level information is available for gerrit and iii the revised code files submitted in response to the received comments.
b. data preprocessing after having collected the data from gerrit and github we start its preprocessing with the goal of building the two datasets of triplets hm s rnli!mr and pairs m s!mr previously mentioned.
methods extraction and abstraction we start by parsing the java files involved in the review process both the ones submitted for review and the ones implementing the code review comments using the lizard python library.
the goal of the parsing is to extract the methods from all the files.
indeed as said we experiment with the dl models at method level granularity as also done in previous work .
after this step for each mined review round we have the list of java methods submitted for review the reviewers comments and the revised list of methods resubmitted by the author to address some of the received comments.
then we adopt the abstraction process described in the work by tufano et al.
to obtain a vocabulary limited yet expressive representation of the source code.
recent work on generating assert statements using dl showed that the performance of sequence to sequence models on code is substantially better when the code is abstracted with the procedure presented in and implemented in the src2abs tool .
triplets for which a parsing error occur during the abstraction process on the msor on the mrmethods are removed from the dataset.
fig.
shows an example of abstraction procedure we perform.
the top part represents the raw source code.
src2abs uses a java lexer and a parser to represent each method as a stream of tokens in which java keywords and punctuation symbols are preserved and the role of each identifier e.g.
whether it represents a variable method etc.
as well as the type of a literal is discerned.
public pageproperties getproperties if hasproperties return properties else return null public type 1 method 1 if method 2 return properties else return null raw source code abstracted codefig.
example of abstraction.
ids are assigned to identifiers and literals by considering their position in the method to abstract the first variable name found will be assigned the id of v ar likewise the second variable name will receive the id of v ar .
this process continues for all identifiers as well as for the literals e.g.
string x int x float x .
since some identifiers and literals appear very often in the code e.g.
variables i j literals method names such as size those are treated as idioms and are not abstracted.
we construct our list of idioms by looking for the most frequent identifiers and literals in the extracted methods list available in our replication package .
the bottom part of fig.
shows the abstracted version of the source code.
note that during the abstraction code comments are removed.
src2abs is particularly well suited for the abstraction in our context since it implements a pair abstraction mode in which a pair of methods can be provided in our case msandmr and the same literals identifiers in the two methods will be abstracted using the same ids.
as output of the abstract process src2abs does also provide an abstraction map m linking the abstracted token to the raw token e.g.
mapping v ar1tosum .
this allows to go back to the raw source code from the abstracted one .
linking and abstracting reviewer comments each collected reviewer comment is associated with the specific set of code lines it refers to.
this holds both for gerrit and github.
165using this information we can link each comment to the specific method if any it refers to given lsandlethe start and the end line a given comment refers to we link it to a method miif both lsandlefall within mi s body signature or annotations e.g.
override .
if a comment cannot be linked to any method e.g.
it refers to an import statement it is discarded from our dataset since useless for our scope.
after having linked comments to methods for each review round we are in the situation in which we have for each review round a set of triplets hms mr andfrnlgi where msandmrrepresent the same abstracted method before and after the review round and frnlgis a set of comments ms received in this round.
at this point we also abstract all code components mentioned in any comment in frnlgusing the abstraction map obtained during the abstraction of msandmr.
thus assuming that the comment mentions change the type of sum to double and that the variable sum has been abstracted tov ar the comment is transformed into change the type ofv ar 1to double .
on top of this any camel case identifier that is not matched in the abstraction map but that it is present in the comment is replaced by the special token code .
such a process ensures consistency in i the representation of the code and the comment that will be provided as input to the encoder model and ii the representation of similar comments talking about different code elements.
filtering out noisy comments through a manual inspection of the code review data we collected we noticed that a non negligible percentage of code comments we were collecting while linked to source code lines were unlikely to result in code changes and thus irrelevant for our study.
for example if two reviewers commented on the same method one saying looks good to me and the other one asking for a change please make this method static it is clear that any revised version of the method submitted afterwards by the contributor would be the result of implementing the second comment rather than the first one.
with the goal of minimizing the amount of noisy comments i.e.
comments unlikely to result in code changes provided to our model we devised an approach to automatically classify a comment as likely to lead to code changes from now on simply relevant orunlikely to lead to code changes irrelevant .
we started by creating a dataset of comments manually labeled as relevant orirrelevant.
to this aim we randomly selected from our dataset a set of comments and related methods ms. these comments come from reviews performed on gerrit and performed on github.
on our dataset that we will detail later such a sample guarantees a significance interval margin of error of with a confidence level of .
the comments have then been loaded in a web app we developed to support the manual analysis process that was performed by three of the authors.
the web app assigned each comment to two evaluators and in case of conflict i.e.
one evaluator said that the comment was relevant and one that was irrelevant the comment was assigned to a third evaluator that solved the conflict through majority voting.conflicts arose for of the analyzed comments.
examples of comments labeled as irrelevant include simple and obvious cases such as thanks!
and nice but also more tricky instances difficult to automatically identify e.g.
at least here it is clear that the equals method of the implementors of treenode is important .
the final labeled dataset consists of comments of which have been labeled as relevant and as irrelevant.
we tried to use a simple machine learning ml based approach to automatically classify a given comment as relevant or not.
we experimented with many different variants of mlbased techniques for this task.
as predictor variables i.e.
features of each comment we considered n grams extracted from them with n2f1 3g.
thus we consider single words as well as short sequences of words grams and grams in the comment.
before extracting the n grams the comment text is preprocessed to remove punctuation symbols and put all text to lower case.
in addition to this only when extracting grams english stopwords are removed and the porter stemmer is applied to reduce all words to their root.
these two steps are not performed in the case of and grams since they could break the meaning of the extracted n gram e.g.
from a comment if condition should be inverted we extract the gram if condition by removing stopwords the if would be removed breaking the gram .
finally in all comments we abstract the mentioned source code components as previously explained.
after having extracted the features we trained the weka implementation of three different models i.e.
the random forest j48 and bayesian network to classify our comments.
we performed a fold cross validation to assess the performance of the models.
since our dataset is substantially unbalanced of the comments are relevant we re balanced our training sets in each of the 10fold iterations using smote an oversampling method which creates synthetic samples from the minor class.
we experimented each algorithm both with and without smote.
also considering the high number of features we extracted we perform an information gain feature selection process aimed at removing all features that do not contribute to the information available for the prediction of the comment type.
this procedure consists of computing the information gain of each predictor variable.
this value ranges between i.e.
the predictor variable does not carry any useful information for the prediction to maximum information .
we remove all features having an information gain lower than .
.
we analyze the results with a specific focus on the precision of the approaches when classifying a comment as relevant.
indeed what we really care about is that when the model classifies a comment as relevant it is actually relevant and will not represent noise for the dl model.
the achieved results reported the random forest classifier using the smote filter as the best model with a precision of .
meaning that out of comments classified as relevant are actually relevant .
while such a result may look good it is worth noting that of the comments in our dataset are relevant.
166this means that a constant classifier always answering relevant would achieve a precision.
thus we experimented with a different and simpler approach.
we split the dataset of comments into two parts representing and of the dataset.
then one of the authors tried to define simple keyword based heuristics with the goal of maximizing the precision in classifying relevant comments on the subset.
through a trial and error process he defined a set of heuristics that we provide as part of our replication package .
in short these heuristics aim at removing i useless word comments e.g.
nice ii requests to change formatting with no impact on code e.g.
please fix indentation iii thank you approval messages e.g.
looks good to me iv requests to add test code that will not result in changes to the code under review e.g.
please add tests for this method v requests for clarification e.g.
please explain vi