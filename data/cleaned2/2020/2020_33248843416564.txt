marble model based robustness analysis of stateful deep learning systems xiaoning du nanyang technological university singaporeyi li nanyang technological university singaporexiaofei xie nanyang technological university singapore lei ma kyushu university japanyang liu nanyang technological university singaporejianjun zhao kyushu university japan abstract state of the artdeeplearning dl systemsarevulnerabletoadversarialexamples whichhinderstheirpotentialadoptioninsafetyandsecurity criticalscenarios.whilesomerecentprogresshasbeen madein analyzingtherobustnessoffeed forwardneuralnetworks therobustnessanalysisforstatefuldlsystems suchasrecurrent neural networks rnns still remains largely uncharted.
in thispaper we propose marble a model based approach for quanti tative robustness analysis of real world rnn based dl systems.
marble builds a probabilistic model to compactly characterize the robustness of rnns through abstraction.
furthermore we propose an iterative refinement algorithm to derive a precise abstraction whichenablesaccuratequantificationoftherobustnessmeasurement.
we evaluate the effectiveness of marble on both lstm and grumodelstrainedseparatelywiththreepopularnaturallanguagedatasets.theresultsdemonstratethat ourrefinementalgorithmismoreefficientinderivinganaccurateabstractionthantherandom strategy and marbleenablesquantitativerobustnessanalysis in rendering better efficiency accuracy and scalability than the state of the art techniques.
acm reference format xiaoningdu yili xiaofeixie leima yangliu andjianjunzhao.
.
marble model basedrobustnessanalysisofstatefuldeeplearningsystems.
in35th ieee acm international conference on automated software engineering ase september virtual event australia.
acm new york ny usa pages.
introduction with the booming of big data and hardware acceleration deep learning dl has achieved tremendous success in many applica tions such as image processing speech recognition xiaofei xie xfxie ntu.edu.sg is the corresponding author.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
ase september virtual event australia association for computing machinery.
acm isbn ... .
and board games .
in spite of achieving high accuracy deep neural networks dnns are still vulnerable to adversarial attacks .
for example an image classifier can easily be fooled by pixel level perturbations or inevitable noise in physical world situations .
hence quality and reliability assurance of dl systems are urgently needed especially for those applied in safety and security critical scenarios.
robustnessanalysisaimstoestimatethecapabilityofaneural networkintoleratinginputperturbations whichnaturallyoccurin the physical environment or are intentionally applied by malicious parties e.g .
adversarial attacks .
a neural network is said to be robust ifitspredictionresultscouldnotbemisledbyanysmallperturbations e.g .
imperceptiblebyhuman .infact ithasbeenshown thatmostneuralnetworksarevulnerabletosmallmanipulationsto theinputs .byfar therearetwotypesofanalysismethods to quantify robustness robustness verification that provides theoretical guarantees on the level of perturbations a network is immuneto and robustnessquantification whichestimates the robustness score of a neural network with the difficulty of findinginputsthatleadtoincorrectpredictionresults.intuitively the harder it is to generate such inputs the more robust the model is.
moreover the quantification methods focus on either generating human imperceptible adversarial input perturbations to manipulate a dnn s decision e.g .
gradient based attacks or producingsystematictests tosimulaterealisticnoiseand transformations that might occur in physical environment.
generallyspeaking thequantificationmethodsareoftenmorescalable thantheverification basedtechniques andtheycanbegeneralized to differentnetwork architecturesmore easily.however it isnontrivialtoimprovemodelrobustness evenwithsamplesexplicitly exposingtheweaknesses.lackingrobustnessoveraninputsample indicatesthattheresultproducedbythednnispotentiallyunreliable.iftherobustnessanalysiscanbeperformedinreal time adecision making system is able to fallback to alternative backup plans whenever the dnn reveals poor robustness.
existingmethodsfacechallengesintermsofefficiencyandapplicability especially in handling large models.
in particular the testing and attack based techniques often require a large number ofinputstobegeneratedandtested whichisexpensiveandhardlyapplicabletoreal timeapplications.inaddition thevastmajorityofexistingrobustnessanalysistechniquesonlyfocusonfeed forward neural networks fnns leaving other types of networks such as recurrentneuralnetworks rnns largelyuntouched.rnnsaredesigned to process sequential inputs e.g .
natural languages audios 35th ieee acm international conference on automated software engineering ase andsignals andalsovulnerabletoadversarialattacks.forinstance thernn basedtoxiccontents e.g .
insultsandviolence detector in online discussions can be circumvented by replacing or modifyingasingleword .toourbestknowledge popqorn is currentlytheonlyworkaddressingtherobustnessverificationof rnns.itassumescontinuousinputdomainsandreliesonexpensiveboundcalculationspropagatedthroughlayers i.e .
onunrolled rnns .
as a result it is rather imprecise when handling discrete domaininputs suchasnaturallanguagetexts andfacesscalability and efficiency issues.
the problem of efficiently evaluating the robustness of rnns in processing sequential inputs remains an openchallenge.solvingthisproblemisanessentialsteptowards improvingthereliabilityofreal timeapplications suchasmachine translationforsimultaneousinterpretation speechrecognition and perception tasks in cyber physical systems.
toaddresstheaftermentionedissues weproposeamodel based approach named marble for evaluatingtherobustnessofrnns effectively.
the aim is to build an abstract model offline and enable light weightrobustnessestimationonlineforreal timeapplications.
toovercometheperformancechallengesandavoidmakingmany perturbationsatruntime weconstructarobustness awareabstract model for rnns to capture the behaviors of an rnn given various inputs in a compact form.
due to the stateful nature of rnns the output produced at each step is affected by both the input elementandthecontextofit.forexample insentimentanalysis thepositive negativeopinionspredictedatacertainwordwithin asentenceareaffectedbytheworditselfandalsowordsbeforeit.
ourabstractmodelmaintainspreviouslyseenstatesandtransitions amongstates whichareessentialincharacterizinghowdifferent inputs are handled under various contexts.
thecontextualinformationcanalsobeusefulin estimatingrobustness.intuitively underasimilarcontext therobustnessover the same input element is expected to be similar as well.
for instance whenprocessingtwomoviereviews ilovethismovie and ilovethatmovie anrnnoftenmaintainsverysimilarcontexts before processing the final word movie .
with this insight we computerobustnessmeasuresfortraininginputsandencodethis informationintotheabstractmodel whichisthenusedtoestimate the robustness of new previously unseen test inputs.
this makes real timeidentificationofunreliablepredictionspossibleandcould improve the overall quality of the decision making process.
in particular we build an abstract model based on markov decisionprocess mdp foragivenrnnmodel andproposearefinement algorithm to iteratively improve the precision of the abstractioninquantifyingrobustness.therefinementprocesscontinues until the robustness estimation error i.e .
the difference of the robustness estimation from mdp and the dynamic input perturbations isreducedunderathreshold.weappliedmarbletoquantifythe robustness of six rnns which are trained for news title classifi cation toxiccommentdetectionandsentimentanalysis.theresults show that marble is more effective in refining the abstract model thanrandomstrategiesandproduces atleast2times smalleraccurate mdps.
marble is more efficient than the state of the arttechnique the average time taken by marble per input is .03seconds while popqorn requires .
minutes.
marble is alsomoreprecise itachieveshigherattacksuccessrateontheleastrobust inputs and lower attack success rate on the most robust inputs compared with popqorn.
the main contributions of this paper are three fold.
we proposed to abstract rnn as an mdp model in order to perform a quantitative robustness analysis of an rnn.
weintroducedamutation basedrefinementtechniquetocontinuously refine the mdp for more accurate robustness estimation of an rnn.
weperformedanin depthevaluationontwopopulardatasets todemonstratetheeffectivenessoftherefinementalgorithmand the estimation precision of marble.
problem formulation .
formalization of rnn and traces definition .
recurrent neural network rnn .
an rnn is a tupler x s o f suchthat x s andodenotethedomains of the inputs hidden states and outputs respectively and fis a differentiable parameterized function.
rtakes as input a sequence x xnof length n maintains a sequence of hidden states s sn 1of length n and applies the function fon each state and input pair si xi to produce an output sequence y onsuch that si yi f si xi wherexi si andyidenotethe i thelementof the respective sequences.
following weformalizeanrnnabstractlyasabove.the hidden states s rmarem dimensional real number vectors and the initial state s0is a vector of mzeros.sd iis used to denote the d th dimension of the state vector si whered m .
given an rnn each input sequence xinduces a finite sequence ofstatetransitions whichformsa tracedenotedby t x .thei th elementofatrace denotedby ti x isthetransitionfrom sitosi after accepting an input element xiand producing an output yi.
thetracesofanrnncanberepresentedcompactlyasafinitestate transducer fst which is formally defined as follows.
definition .
finite state transducer fst .
given an rnn r x s o f wedefineitstracesinducedbyasetofinputs d 2xn asafinitestatetransducer tr d x s o s0 f wheresis a non empty finite set of states xis the input alphabet ois the output alphabet s0 sis the initial state f sis the set of final states and s x o s is the transition relation.
example2.
.
fig.1givesanexampleofhowtorepresentthernn behaviors as an fst with transitions represented by solid black arrows.thernn r isassumedtoworkonsentimentanalysisand is trained to classify movie reviews into two categories namely negative and positive.
note that is used to represent negative opinions and is used for positive ones.
given a set of movie reviews d i really like this movie three traces are induced.
they share the same initial state s0and consistoffive fourandfourtransitions respectively.transitions arerepresentedbyarrows originatingfromonestateandtransit to another state labeling with its input and output pair.
for the transition from s0tos1 the label i denotes that this transition is induced by the input element iand emits the output .
forthefstdefinedoverthesethreetraces thesetofinputalphabetisx i we really like the that this movie 424 figure fst example.e z d w k d w d z z d w z h z d w figure the overview of marble.
thesetofstatesis s s0 ... s12 thesetofoutputsis o the initial state is s0 the set of final states is f s5 s8 s12 andthetransitionrelation definesthesetof12transitions e.g.
s0 i s1 and s1 really s2 .
.
pointwise robustness of an rnn thequantificationoffnnrobustnesshasbeenwidelystudiedin the literature whereas little work has been done to address therobustnessofrnnmodels.unlikefnnthattakesaninputasa whole rnn consumes a sequential input element by element and perturbationsoneachinputelementmaycausedifferentlevelsof impacttothemodeloutputs .theoretically thechangeat asolitaryinputframecouldmakealong lastinginfluenceonthe process of upcoming frames.
also due to the complicated training and working mechanism of rnns even well trained rnns tend to reveal poor robustness to perturbations at a single input frame.
in linewiththeassumptionsinpopqorn westudythepointwiserobustnessofrnns insteadofallowingmutationsoverawiderangeofinputframes.suchastrictconstraintcanbemoreinstructivefor the quality assurance of rnns at current stage.
to measure the robustness of an rnn model w.r.t.
an input we allow the input to be perturbed within a certain range and observe their impact on the induced traces.
instead of searchingfor the lower bound of perturbations that would alter the rnn decision welookintohowlikelyperturbationswithintherange could make a difference.
because the lowerbound measure overdiscrete data e.g.
natural language is highly influenced by the sparsityofthedata.topreciselyquantifythepointwiserobustness wefirstspecifyhowinputelementsaretobeperturbed formally definedbya pointwisemutationfunction.notethat foragivenset q theprobabilitydistributionover qisafunction d q mapsto such that q qd q .wedenotethesetofallprobabilitydistributions overqasdist q .
definition .
pointwise mutation function .
a pointwise mutationfunction x dist x mapsaninputelement x xtoa probabilitydistributionofthepossiblemutants x prime x suchthat x x prime is the probability of mutating xintox prime.
thepointwisemutationfunctioncanbeusedtodescribevarious typesofperturbationstotheinputs.forexample p normball isoftenusedintheliteraturetodescribeperturbationshappened uniformly within a sphere around the given input.
ingeneral the robustnessofamodelcanbeexaminedagainstdifferentreal world attack patterns by simulating the ranges and frequencies of the potential mutations appeared in practice.
we define the step pointwise robustness of an rnn at an input elementxiasthelikelihoodthattheimmediatenexttransition tiinducedby xistaysunchangedaftersome perturbation isapplied toxi.
this definition can be further generalized to observe the impactoftheperturbationonlyafteracertainnumberoftransitions instead of the immediate next one.
definition .
k step pointwise robustness .
letrbe an rnn andx xnbe an input of length n respectively.
let xibe the input element at the i th position of x wherei n. let be a pointwise mutation function generating a new input x xi x prime i by mutating xi.
thek step pointwise robustness of rw.r.t.xat thei th position against perturbations defined by denoted by k r x i isgivenastheprobabilitythatthetraceinducedby x primeproduces the same output at the i k th transition where i k n. more formally k r x i summationdisplay.
x prime i x ti k x o ti k x xi x prime i xi x prime i whereo denotes the equality of outputs for both sides.
thepointwiserobustnessdefinitionisparameterizedbythenumberofsteps afterwhichtheimpactoftheperturbationtothemodel outputare observed.specifically t x andt x xi x prime i represent the two traces induced by x before and after xiis mutated respectively.
the k step pointwise robustness only observes the outputs producedatthe i k thtransition denotedby ti k x andignores any intermediate states and transitions.
in particular for the step pointwise robustness we observe how the transition tiinduced by xiisaffectedbyperturbationsover xi.insteadofparameterizing pointwiserobustnessbytheinputsequenceandpositionindices we sometimes use an equivalent notation 0 r si xi such that 0 r si xi summationdisplay.
x prime i x f si xi o f xi x prime i xi x prime i .
example .
.
following fig.
we briefly illustrate how the 0step pointwise robustness is calculated with a simple example.here we compute the robustness of rover the input sequence atitsfourthinputelement i.e.
that .forthemutationfunction weassumethat that canbe mutated into any element in the that this with an equal probability i.e.
.theperturbationof that wouldaffectthetransitionrightafterthestate s3 andweassumethetwonewtransitions induced by the replacements are s3 this s13 s3 the s14 respectively.
for the replacement to itself the transition keeps still as s3 that s4 .obviously among all these three transitions two out of them emit the same output as originally.
therefore the step robustness of rover the input sequence i really like that movie at that i s2 .
.
overview followingdefinition2.
thepointwiserobustnessofanrnncan be estimated through a random sampling from mutations.
one can randomly generate mutants according to the probabilistic distributiongivenbythemutationfunction andobservetheirimpacton the model outputs directly.
yet this sampling approach likely does not scale in practice.
to get precise robustness measurements w.r.t.
even a single input the number of mutants required can be huge if the rnn model is non trivial.
to overcome this challenge we propose a model based analysis technique marble toefficientlycomputethepointwiserobustness of a given rnn with respect to an input element.
figure summarizestheworkflowof marble.wefirstconstructa robustnessawareabstractmodelbyobservinghowthernn sbehaviorsare affected by pointwise mutations.
specifically marble profiles the rnn s behaviors on the training data and constructs an fst.
then foreachtransitionwithinthefst marbleexaminesits step robustness with input replacements sampled following the mutation probabilistic distribution.
themutation function is specific tothe application domains e.g .
audio and image and pre defined by the users.
we then build an abstract model in the form of a markov decisionprocess mdp byabstractingthestatesandtransitions inthefst andannotatethemdpwithrobustnessmeasurements adapted with the abstraction.
givenaninput wecanthencomputearobustnessscore regardingeachinputframe basedonthemdpmodel.iftheobtainedscore is inconsistentwith the resultof the dynamic mutation sampling i.e.
their difference exceeds a threshold we iteratively refine the abstract modelso thatthe robustnesscan bemore preciselyquantified.finally weobtainapreciseabstractmodelaftertheresults ofalltrainingdatafromtheabstractmodelisconsistentwiththe mutation sampling.
based on the abstract model we can analyze the robustness of the rnn against new inputs that follow a similar distribution of training data efficiently.
rnn abstraction using labeled mdp as shown in definition .
the robustness of an rnn is defined with respect to a concrete input sequence at a certain position.
yet thenumberofstatesandtracesenabledduringthetrainingstage of an rnn can be huge.
it can be impractical during runtime to compute the robustness for each trace individually with either classicalmathematicalestimationmethods ormutationtesting techniques .
to make the computationmore efficient we nowintroduce an rnn abstraction model based on labeled mdps.
.
aggregated pointwise robustness thekeyideainderivingasuitableabstractionistogroupthetraces withsimilarrobustnessmeasurestogether whichcanbeusedto estimate the robustness of an rnn for various inputs at various contexts in a more cost effective manner.
given a set of states s from which transitions are induced by an input frame x w ec a n calculate the step robustness for each such transition against its respective output.
we define the aggregated pointwise robustness at storeflecttheoverallrobustnessofasetofinput inducedtraces against perturbations.
the collection of transitions from smay emitdifferentoutputs.toeasethecomputationoftheprobabilityof keepingtheoriginaloutputscollectively werecordtheprobabilitiesemitting different outputs as a distribution over the output domain.
specifically we denote the probability of emitting yats swhen mutating input xwith as 0 r s x summationdisplay.
x prime x f s x prime o y x x prime .
weusethenotation 0 r s x todenotetheprobabilitydistributionovertheoutputdomain andomittheparameters rand when they are clear from the context.
given that it is a categorical distribution themeanof this distribution is a vector p o f length o wherepi 0 r s x for anyyi o. further 0 r s x issometimesalsoabusedtorepresentthe meanvector.
for an instance originally yielding y the robustness is obtained via taking the corresponding probability 0 r s x .n o w t h e aggregatedrobustnessdefinitionrequiresintegratingasetofdistributionsobservedatdifferentstates.acommonpracticetoreconcile different probability distributions is to take the average of their probabilitydensities .hence wedefinetheaggregatedpointwise robustness as follows.
definition .
aggregated pointwise robustness .
letrbe an rnnand tr d x s o s0 f beitstracesinducedbyaset ofinputsd.letxbeaninputelementand beapointwisemutation function.
let s sbe a set of states such that for all s s f s x emitsthesameoutput.theaggregatedpointwiserobustnessof rat sw.r.t.xagainstperturbationsdefinedby denotedby r s x isgivenastheaverageofthe0 steppointwiserobustnessof rw.r.t.
the states in s. more formally r s x s summationdisplay.
s s 0 r s x .
the aggregated pointwise robustness is essentially the average value of all pointwise robustness observations for a set of states.
in particular for an output y w eh a v e r s x s summationdisplay.
s s parenleftbig 0 r s x parenrightbig whichtakestheaverageoftheprobabilitiesofemitting yindifferent distributions.
with eq.
we are able to represent the overall robustness at a set of states obeying proper statistical rules.
.
abstraction as labeled mdp abstractiononstatesisessentiallydividingthestatespacesuchthat stateswithsimilarrobustnessmeasuresareclustered.aftersuch clustering anabstractstatemayhavemultiplenon deterministicoutgoingtransitionsevenafterreceivingthesameinput.markov decisionprocess mdp iswidelyusedtodescribeprobabilistic systems and to characterize the probabilistic state transitions triggeredbydifferentinputelements.here weusethelabeledmdptoformalizetheabstractionofrnntraces andthedetaileddefinition is given in definition .
.
the labeling function is used to record the robustness measure of each state and input element pair.
definition .
labeled markov decision process .
a labeled mdp m isatuple x s o s0 consistingofafiniteset sofstates a finiteset xofinputalphabet aninitialstate s0 afiniteprobabilistic transition relation s x dist s where states and inputs are in relation with distributions of successor states and a labeling 426function s x dist o whichmapsthestateandinputpairs to distributions of outputs.
now we demonstrate how to derive a labeled mdp from a set of observed traces represented as an fst.
in order to represent the behaviors of an rnn in a more compact manner we allow abstractions over the state domain as well as the input and output domains andmoredetailsabouttheabstractionfunctionswillbe discussed in section .
.
given an fst tr d x s o s0 f of an rnn ron the input set d an input abstraction function i x mapsto x a state abstraction function s s mapsto sand an output abstraction function o o mapsto o we can establish a labeledmdp m x s o s0 fortr d .theinput stateand output domains in mare abstracted from those in tr d with the abstraction functions.
specifically s0 s s0 and for each transition s x s prime y it is abstracted as s s s x i x s prime s s prime y o y and included as an abstract transition in m. with these abstractions the probabilistic transition relation and thelabelingfunction canbederivedaccordingly.
s x s prime denotes the conditional probability of visiting s primegiven the current abstract state sandanabstractinputelement x and s prime s s x s prime .the transition probability is calculated as the number of concrete transitions from sto s primevia xover the number of all outgoing concrete transitions from saccepting abstract input element x i.e.
s x s prime s x s prime s s x x s prime s prime s x s s x x .
the labeling function records the step aggregated pointwise robustnessof runderamutationfunction .notethattherobustness definitionindefinition3.1ispresentedundertheconcreteinput and output domains and here we adjust it to reflect their abstract counterparts.similartodefinition3.
givenanabstractstate sand anabstractinput x theoutputsofthecorrespondingconcretestate and input pairscan vary.
therefore wealso label the robustness under abstract states and abstract inputs as a distribution over the output domain.
formally we define step aggregated pointwise robustness at sover x when emitting the abstract output y as s x y s x summationdisplay.
s x s x parenlefttpa parenleftexa parenleftbta summationdisplay.
y y 0 r s x parenrighttpa parenrightexa parenrightbta wheres x s x s x fors s x x is the set of state and input pairs which source from concrete states in sand acceptconcreteinputelementsin x.theinnersumcalculatesthe robustnessat soverx whilerelaxingtheconstraintontheexpected outputtobewithintheabstractoutput y.then wetaketheaverage value for robustness observations over all pairs inside s x. example3.
.
continuingwiththeexampleinfig.
wedrawthe abstractmdpmodelderivedfromitinfig.
a andfig.
b .states andtransitionsinducedbymutationsover the that and this arehighlightedasreddotsandreddashedarrows respectively.we assumethatthesethreewordssharethesamemutationprobability distribution i.e.
each could be mutated to others including itself withequalprobability i.e.
.thecorrespondingoutputs either0or areannotatedalongtheredarrows.forinstance bothtransitions froms10tos16and from s10tos17emit output and they are inducedbyreplacing this with the and that respectively.inputelementsareomittedtokeeptheillustrationconcise.fig.
b shows the mdp abstraction of the concrete traces in fig.
a .
now weelaborate on the input output abstraction and thestate transition abstraction carried out during the mdp construction.
in this example we assume an identity abstraction function for theoutputspace.thefullinputabstractionfunctionispresented inthedashedboxatfig.
a .forinstance the that and this are abstracted as x4.
the state space are abstracted via the grids drawnindashedlinesinfig.
a resultinginfiveabstractstates i.e.
s0 s1 s2 s3 and s4 eachofwhichismappedtoasetofconcrete statesinsidethecorrespondinggrid.foreachabstractstate s w e can then compute the transition function by deciding the set of abstractinputsacceptedat sandtheprobabilisticdistributionsof thesuccessorstatesundereachabstractinput.forexample there arethreeconcretetransitionsoriginatedfrom s1 whichleadtotwo abstractsuccessorstates s2and s3.theabstractstate s1acceptsonly oneabstractinput x4 inducingadistributionofthesuccessorstates i.e.
s1 x4 s2 s1 x4 s3 .thetransitionprobabilities are marked over the transitions between abstract states in fig.
b as abstract input and probability pairs.
finally we show how the labeling function is computed which signifies the step aggregated pointwise robustness.
in fig.
b thispartofinformationishighlightedwithblueboxesasideeach abstract state.
according to eq.
the step aggregated pointwiserobustnessiscalculatedbytakingtheaverageofrobustness observed concretely.
we take the abstract state s1and the abstract input x4asexample.fromtheconcretestatetransitionsinfig.
a we can see that there are three concrete states within s1 and all of them accept concrete input elements in x4.specifically at s3 according to the mutation function the probability to yield output 0is1 andtoyield1is2 .theprobabilitydistributionovertheoutputsare markedfor s3 s6ands10 whenaccepting inputelements belong to x4.finally we can calculate the step aggregated pointwiserobustnessat s1over x4 i.e.
s1 x4 and s1 x4 .
hence we label the output probability distribution of x4at s1as .the concrete distributionannotationsareomittedinfig.
a forotherconcretestates and transitions since they can be easily computed when no mutations are applied to them.
for the concrete transition s0 i s1 theoutputprobabilitydistributionis indicatingoutput0is yielded with probability .
since there is only one concrete state in s0accepting x0 the aggregated robustness is directly .similarly we can calculate the output distribution for each abstract state over its accepting abstract inputs.
.3k step aggregated pointwise robustness based on the abstracted mdp model we define the k step aggregated pointwise robustness.
firstly we define traces over the mdp inducedbyanabstractinputsequencestartingfromadesignated abstractstate.givenanmdp m x s o s astartabstract state s0 sandanabstractinput x xnoflengthn wedenotethe setoftracesover mtriggeredby xandstartingfrom s0as x s0 .
thei th element in a trace x s0 is the transition from sito si 1via xi.inparticular the0 thtransitionistriggered by x0and transits from s0to s1.
moreover we use to denote the trace probability of which is the product of transit probabilities for 427 a abstraction over the state space.
b mdp abstraction and modeling.
c refinement of the state abstraction.
figure mdp construction and refinement.
transitionsin formally producttext.1n i 0 si xi si .specially we define as the emptyabstract input element and forany abstract state the self transition via happens with probability .
hence given astheinputsequenceoflength1 wehave s0 contains theonlytrace with a single self transition of s0.to facilitate the notations weuse x todenotethesubsequenceofasequence x which starts from its i th element and ends at the j th element ifi j thesubsequenceis .also weuse rtodenotethe reached state of its last transition.
we give the definition of k step aggregated pointwise robustness in definition .
.
definition .
k step aggregated pointwise robustness .
given anrnnr letm x s o s0 bealabeledmdpestablished for it.
let x xnbe an input of length n and the corresponding abstractinputis xwith xi i xi fori n.thek stepaggregated pointwiserobustnessof rw.r.t.xatthei th i n positioncanbe calculated over mas k m x i summationdisplay.
r xi k y wherethesourcestateof ti x issi whichmapstoabstractstate si the output of ti k x maps to the abstract output y and x i i k si simplynotedas isthesetoftracesover minitiatingfrom siand triggered by the abstract input sub sequence x .
the above definition is consistent with our step aggregated pointwise robustness defined under the abstract domain in eq.
.
inthatcase thesub sequence x becomes thus si contains only onetrace which consists of a single self transition of sivia and we have k m x i si xi y .
model refinement strategy withrnnabstractedasalabeledmdp wehopetocalculatethe robustnessovertheabstractionforbetterefficiency.toderivean abstraction which allows for accurate estimation of the robustness as obtained from mutation testing see definition .
we design a refinement process to reduce the estimation errors iteratively.
the aimistoreducetheestimationerrorsfromthe k stepaggregated pointwiserobustnessandgraduallyapproachestheground truthrobustnessreflectedbymutationtesting.inthefollowing weassume appropriate input and output abstraction functions are predefined andpresenttherefinementalgorithmfocusingontherefinement of the state abstraction function in section .
.
.
input and output abstraction input abstraction aims to gather similar input elements ideally the ones which are able to reveal a similar level of robustness at thesame states.
as a heuristic we group inputs with identical or similar mutation probability distributions together to form an abstract input.thisway theinputabstractionisalignedwiththepointwise mutationfunction definition2.
andinputswithsimilarsemantics and mutants are grouped and treated equivalently in the mdp abstract model.
the mutation functions are designed according to thespecificpropertiesofinputdomains consideringfactorssuchas whether the domain is continuous e.g .
speech audios or discrete e.g.
naturallanguages andwhichtypesofdistancemeasures e.g .
lpnormdistanceorcosinedistance aremoresuitableinrestricting themutationmagnitude.takingthenaturallanguageprocessing nlp applicationasanexample itisnaturaltoconsidersynonyms as mutants of a word which maintains semantic similarities.
practically words with cosine distances within a predefined threshold canbeidentifiedassynonyms.weassumethemutationprobability of a word to all its synonyms uniformly distributed.
hence differentwordssharingthesameorsimilarsetofsynonymscanbe mappedintothesameabstractinput.thebenefitofthischoiceis thattherobustnessmeasurescomputedforoneinstancewithinthe synonyms can be more easily generalized for other instances.
fortheoutputabstractionfunction itshouldbedesignedspecificallyfordifferenttypesofdeeplearningtasks.similarly theabstraction on the output domain aims to gather similar outputs together based on how tolerant the users are about the output variations.
forclassificationtasks identityfunctioncanbeusedfortheoutputabstraction sinceanyclassificationresultotherthanthetruthlabelisregardedasafailure.fortasksattemptingtopredictavaluefrom continuous domain e.g.
the steering angle of autonomous cars an abstraction function can be designed to map the outputs to a finite discrete domain with techniques such as predicate abstraction .
the predictions of such tasks are deemed as correct as long as they are within a certain range of the truth label.
in general the output abstraction functions can be derived for different domains.
similarity analysis of inputs outputs can be a fundamental task when conducting robustness analysis and several metrics have beenproposedandwidelyused e.g.
p normandcosinedistance.
for different dl applications we could select suitable similaritymetrics.
the quality of the input output abstraction could makeasignificantdifferenceontheaccuracyof marble.forexample a coarse input abstraction grouping inputs that are dramatically differentishardtoberefinedtobeaccurateforeachinputelement.
in this work we take the widely used metric e.g .
cosine distance toperform theinput output abstractionon theselectedtasks and leave the more comprehensive study as future work.
.
refinement of state abstraction we propose an algorithm for refining the state abstraction which aims to reduce the gap between the k step aggregated pointwise robustness definition .
and the individual k step pointwise robustness definition .
within an abstract state.
from definition .
we see that the k step aggregated pointwise robustness can be deduced from the robustness labeling function.
definition .
gives the mean squared error mse of the estimation calculated using an mdp abstraction compared with the pointwise robustness obtained by mutating individual concrete inputs.
the accuracy of the mdp abstraction in measuring robustnessisdeterminedbyhowconcretestatesareclustered because errors arise when the aggregated robustness deviates from the robustnessobserved concretely.ideally we wouldliketodesign the stateabstraction functionsuchthatstates withsimilarrobustness distributionsareclusteredintoanabstractstate.maximummean discrepancy mmd iswidelyusedtomeasurethedistance between distributions and defined as mmd d1 d2 bardblex bardblexex d1 ey d2 bardblex bardblex h whered1andd2are two distributions and the mmd takes the distancebetweenthemeanofthetwodistributionsinthereproducing kernel hilbert space rkhs h. here we take the euclidean distance with euclidean space being a widely used member of rkhs.
in definition .
we sum up the squared mmd distance in euclidean space between all aggregated robustness and concrete robustness pairsto representthe errorsintroduced by theabstraction.
definition .
mean squared error .
given an rnn r a set of samplesd lettr d x s o s0 f andm x s o s0 be the fst and the labeled mdp established accordingly.
the mean squared error is calculated as summationdisplay.
s s parenlefttpa parenleftexa parenleftbta summationdisplay.
s x parenlefttpa parenleftexa parenleftbta summationdisplay.
s s x x s x bardblex bardblex 0 s x s x bardblex bardblex2 parenrighttpa parenrightexa parenrightbta parenrighttpa parenrightexa parenrightbta wheres sandx x. next we elaborate on how to refine a state abstraction function foragivenmsethreshold andsketchtheprocessinalgorithm1.
givenanrnnmodel r amutationfunction athreshold and a set of samples d the algorithm produces a labeled mdp with which the estimated step aggregated pointwise robustness of samples in dachieves an estimation error less than .
here the pointwise robustnesscomputed by mutation testingwith the mutation function is used as the ground truth.
we assume that i and oarepredefinedbyusers and sisinitializedsuchthatall concrete states are mapped to a single abstract state.
firstly we profile the rnn model rwith samples in dand represent the traces as an fst tr d line .
we then conduct mutationsamplingbyreplacingtheinputofeachtransitionwith nmutants lines4to9 .themutantsaresampledaccordingtothe probabilisticdistributiondefinedby .viacountingthefrequencies ofemittingdifferentoutputsafterapplyingthereplacements we obtainthe0 steprobustness aprobabilisticdistributionoverthe output domain for each transition.
lines to show the core refinement steps.
in each iteration we attempt to improve the accuracy of the abstraction functionalgorithm refinement algorithm for a labeled mdp.
input r x s o f rnn mutation function threshold d samples output m labeled mdp 1prepare input output state abstraction functions i o s 2mutation sampling count n 3tr d x s o s0 f finite state transducer of r 4for s x y s prime do mutation sampling for robustness estimation 5 0 s x o 6forjin n do x prime i selectmutation xi y prime f s x prime i 0 s x n 10do 11m x s o s0 build mdp tr d i o s 12refinable false 13for s sdo refine each abstract state for s x do refine the first refinable transition spw bpw for s x s x do b o for y odo pointwise robustness b summationtext.
y y 0 s x spw bpw spw s bpw b if1 bpw summationtext.
b bpw bardbl s x b bardbl2 then to refine k fitkmeanscluster bpw clusters c fitsvmclassifier spw k.labels s addsubabstracter s s c refinable true break 26whilerefinable 27returnm for each abstract state in m. as the first step a labeled mdp is computedfrom tr d withtheinput output andstateabstraction functions.foreachabstractinputelement x triggeringatransition sourcingfrom s line14 weexaminewhethertheestimationerror ofthispairisbelow line20 .wecalculatethemutation based robustness related to sand xwith lines to and store the resultsin bpw.wealsokeeptheconcretestatevectorsin spw oneto one mapped to those robustness in bpw.
the step aggregated pointwiserobustnessestimatedforthe s x pairis s x which equalstotheaverageofthevaluesin bpw.ifthemseofthissubset ofobservationsisgreaterthan line20 wemakearefinement on ssuch that the overall mse on dis reduced theorem .
.
wedesignatwo steprefinementstrategytonarrowthedistance betweentheestimatedaggregatedrobustnessandeverycorresponding mutation testing robustness.
we aim to cluster the concrete statesinspwaccordingtotheirmutation basedrobustness line21 suchthatstateswithsimilarrobustnesswouldbegatheredtogether.
in the first step we leverage k means to separate the robustness distributionsin bpwintotwoclusters.online21 kdenotesthe fittedk meansclassifier and k.labelsisusedtorepresentthelistof obtainedclusterlabels.clusteringovertherobustnessdistributions offersimplicationonhowtodividethestatespace.inthesecond step we treat the cluster index as the label and employ svc classifierctoapproximatethedecisionboundaryoverthestatespace line .
finally we append cas a sub abstraction function of state sand update the state abstraction function s line .
after 429examining all abstract state and input element pairs if there exists anypairtoberefined were buildthemdp mwiththeupdated sand continue the iteration.
otherwise if neither of the abstract states requires further refinement the procedure terminates and returns the refined mdp model.
example .
.
following fig.
b we illustrate how the refinement algorithm works to reduce the estimation error.
we assume a thresholdof1 27ontheestimationerror.whentheloopiteratesat s1 x4 at line the list of concrete states is spw and the corresponding list of pointwise robustness observations is bpw .asweknowfromthelabelingfunction the step aggregated robustness at s1over x4is s1 x4 .the local estimation error yields as parenleftbig bardblex bardblex bardblex bardblex parenleftbig1 parenrightbig parenleftbig4 parenrightbig bardblex bardblex bardblex bardblex2 bardblex bardblex bardblex bardblex parenleftbig1 parenrightbig parenleftbig4 parenrightbig bardblex bardblex bardblex bardblex2 bardblex bardblex bardblex bardblex parenleftbig2 parenrightbig parenleftbig4 parenrightbig bardblex bardblex bardblex bardblex2 parenrightbig indicating a further refinement is required.
first weapply k meansover bpw andsettheexpectedcluster number to two.
since k means aims to put closer points into the samecluster itislikelytoreturntwoclusters c0 bracketleftbig parenleftbig parenrightbig parenleftbig parenrightbig bracketrightbig andc1 bracketleftbig parenleftbig parenrightbig bracketrightbig .
hence the labels for the list of states are k.labels .then wecanfitthesvcwhichsplitsthe abstract state s1into two.
as demonstrated in fig.
c we assume thereddashedcurvetobetheboundarydeterminedbythesvc.
thus the original abstract state s1is refined into two new abstract states s5and s6.
now the state abstraction is refined and the mdp model is to be reconstructed.
theorem .
error reduction .
given an rnn r a set of samplesd letmandm primebe labeled mdps obtained from rand d before and after the execution of the refinement step lines to in algorithm .
we have msem prime msem wheremsem andmsem primeare the mean squared errors of mandm prime respectively.
the proof of theorem .
is available on our website .
with theorem4.
themseof misreducedaftereachrefinementstep.
since there is only a finite number of concrete states in s and the number of abstract states sis guaranteed to increase after each iteration msemis eventually approaching zero.
therefore the threshold will be reached after finite number of iterations.
evaluation weimplementedmarbleinpythonbasedonthepytorch .
.
frameworkandconductedevaluationonsixreal worldrnnsubject models to evaluate the refinement strategies and the robustness quantification method.
specifically our experiments are designed to answer the following research questions rq1 how effective is the refinement algorithm in generating mdpabstractions?howaretheabstractionsintermsofsuccinctness and generalization?
rq2 how is the scalability and efficiency of the robustness quantification by marble?
rq3 does marble provide a better quantification of rnn robustness than state of the art approaches?subject datasets and rnn models we selected three popular nlpdatasetsandtrainedthecorrespondingrnnmodels.inparticular weleveragedthepre trainedwordembeddingvectorsglove toacceleratethetrainingprocessandachievecompetitiveaccuracy.
the cogcomp qc dataset abbrev.
qc includes news titles labeled with different types of topics.
there are 20k samples fortrainingand8ksamplesfortesting andeachsamplecontains9.
words onaverage.
we followedthe same configurations as in totrainanlstmmodelwithatestaccuracyof83.
andagru model with a test accuracy of .
respectively.
thejigsawtoxiccommentdataset abbrev.toxic used inkagglechallengeincludesasetofcommentsfromwikipedia s talkpageeditsandislabeledastoxicornot withanaveragesamplelengthof54.
.theoriginaldatasetisdesignedtobeseverelyimbalanced forthe challenge usage.
herein order toobtain an accurate model we use 25k non toxic samples and 25k toxic samples forthe model training and testing .
we trained an lstm model with .
test accuracy and a gru model with .
test accuracy with hidden nodes respectively.
the sentiment analysis dataset abbrev.
imdb containsimdbmoviereviewslabeledwithbinary positiveornegative sentimental classifications.
there are 25k training data and 25k test data where each sample contains .
words on average.
we trained an lstm model and a gru model each with hidden nodes which achieve .
and .
test accuracy respectively.
statespreprocessandinputabstraction.
tohandlehigh dimensionalstatevectors weappliedprincipalcomponentanalysis to reduce the data dimension to major components as in .
to build the mutation function for nlp applications we use cosinesimilarity tomeasurethesemanticdistancesbetweenwords.
based on the glove embedding vector we performed the input abstractionbymakingsynonymgroups amongwhichthecosine similaritiesareaboveathreshold.inourevaluation wesetthea higherthreshold .
toguaranteeaconservativemetamorphic relation and obtained synonym groups.
during the mutation we allowed a uniform probability distribution on words within the same synonym group.
.
rq1 effectiveness of the refinement we first examined the performance of our refinement strategy underdifferentconfigurationinstances where istheretained dimensionofthestatevectorsafterpcaand isthemsethreshold.
as a baseline for comparison we also implemented a random split basedstrategyforabstractstaterefinement.therandom splitstrategy differs from marble at only two steps cf.
line and line in algorithm which refines an abstract state by randomly separatingthestateinstancesintotwogroups andfitthesvcclassifier.
our evaluation was conducted on the six rnn models both lstm and gru of the three datasets.
for each rnn model we experimented on six configurations.
table summarizes the results of refinement obtained from the lstm models of the three datasets under three configurations.
the full results of all models are on our website.1column config.
shows the evaluated configuration instances i.e .
which are used in both of marble andtherandomstrategy.theothercolumnsincludethenumber 430table measures of mdp models refinement.
config.
strategy iter.
time s state tran.
msetmiss qc .
marble .
.
.
random .
.
.
.
marble .
.
.
random .
.
.
.
marble .
.
.
random .
.
.47toxic .
marble .
.
.
random .
.
.
.
marble .
.
.
random .
.
.
.
marble .
.
.
random .
.
.44imdb .
marble .
.
.
random .
.
.
.
marble .
.
.
random .
.
.
marble .
.
.
random .
.
of iterations taken by the refinement iter the refinement time time s the number of states state.
the number of transitions tran.
in the refined mdp.
further we examined the generalizationabilityofthelabeledmdpwhenappliedtotherobustnessestimationontheneverseentestdata andreportedthe percentageofmissedtransitionsincolumn miss aswellasthe msemeasureincolumn mset .werefertoatransitionasmissed ifitisnotincludedinthemdpmodel.
mseiscalculatedinasimilar wayas whenbuildingthe mdpmodel where weprofiledthe test data obtained the mutation based robustness and compared them with the estimation by the labeled mdp.
theresultsconfirmtheadvantagesoftherefinementstrategyof marblecomparedwitharandom splitapproachinthat marble terminateswithlessiterationsacrossconfigurationsanddatasets.
forthethreedatasets theaveragenumberofiterationstakenby marbleis19.
.7and29.
whilethatbyrandomstrategyis53.
.3and69.
.
marblealwaysderivesasmallermdpmodelwith less states and transitions through the refinement.
for example the average number of states for the datasets is .
.0and8459.
respectively whilethatbyrandomstrategyis2263.
.
and .
which are .
.
and .
times larger thanmarble.
marble always derives a better mdp model which demonstratesnotonlyalesstransitionmissratewhendealingwith new samples from the test dataset but also a smaller estimation error.
for instance the average miss rate on the datasets is .
.
and .
while the random strategy causes larger miss rate withrespectively54.
.
and45.
.however asatrade off marbletakeslongertimetocompletetherefinement.forexample the average time used is .5s .1s and .4s which are larger than that by the random strategy with respectively .3s .3sand .7s.
this is due to the application of the k means which better refines the abstract state with robustness estimation.
finally we summarize implications on applying marble to datasetofdifferentscalesandwithvariousconfigurations.basically under thesame configuration it takesmore time anditerations to complete the refinement of larger dataset and more complicated rnn models and the resulted mdp is also larger.
under the sameconfiguration of the larger threshold is the less effort it takes to complete the refinement resulting in a less complicated mdp butwithslightlyhigherestimationerror.acoarsemdpoftenendowsbettergeneralizationcapabilitywhenprocessingnewsamples leading to less missed transitions.
answer to rq1 marbleissuperiorthantherandom split refinement strategy and can deliver a more accurate abstract mdpmodelwithsmallersizeandbettergeneralizationability.marbletakeslessnumberofiterationsbutcostsslightlymore time to accomplish the abstraction.
.
rq2 performance efficiency in this experiment we evaluate the time taken to quantify the robustness over every element of a sample the efficiency of which canbehighlydesirableforreal timemonitoringapplications.we present the results of three lstm models and full results can be found on website .
foreachoftherefinedmdp weapplyittoestimatethe0 step robustness of samples in the test data and report the average time usedpersampleintable2.notethatwetakethetotaltimeused togetthe0 steprobustnessforeveryelementinasample which indicates that the longer the sample the more time would be used.
intable2 thefirstcolumnshowsthedatasetandthesecondcolumn config.
presentsthe configurationsusedtoobtained themdp andthetimeusedbytherobustnessestimationisgivenincolumn time s .
overall the robustnessof an input canbecalculated efficiently by marble with the time magnitude of thousands or dozens of millisecond.
on average the three datasets take .03s .24s and .07s to estimate the robustness of an individual sample.larger dataset tends to take more time to finish the estimation.
wecomparethescalabilityandefficiencyof marblewiththe state of the art robustness quantification approach forrnns i.e.
popqorn.ho wever wecannot comapremarblewithpopqorn inallmodels fortwo reasons scalabilityisamajorlimitationfor popqorn whichfailstocompleteonlargerrnnstrainedfrom toxicandimdbdatasetsand2 popqornismodel dependentand the current released version does not support gru.
thus we only run popqorn on the lstm model of qc dataset.
we ran popqorn on all sentences from the test dataset to calculate their robustness scores and set a timeout i.e.
hours.
finally only the robustness score of sentences are calculated successfully.onaverage popqorntakes52.8minutestocomplete the estimation of a single input.
answer torq2 compared with popqorn marble offers betterscalabilityforhandlinglargeandcomplicatedmodels for accepting longer inputs and is also efficient in robustness calculation thus can be applied for real time robustness monitoring of larger rnn applications.
.
rq3 accuracy of the robustness measures due to the absence of the ground truth on the robustness measurement itisnotpossibletogaugetheabsoluteestimationaccuracy.
431table attack success rate and estimation efficiency.
dataset config.asrl asrm time s marble vs random vs popqorn marble vs random vs popqorn qc .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
avg.
.
.
.
.
.
.
.
toxic .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
avg.
.
.
.
.
.
imdb .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
avg.
.
.
.
.
.
the asr differences between marble and other approaches are examined with mann whitney u test and the displayed results are statistically significant with p .
.
instead weinspectedwhetherthelocationswitnessingworserobustness is more vulnerable to attack i.e.
to examine relatively ordertherobustnessestimations.worserobustnessindicatesthat perturbationsoverthatlocationcaneasilymakeadversarialattacks.
weevaluatedtheeffectivenessoftheobtainedrobustnessmeasures by examining their helpfulness in launching adversarial attacks.
according to the robustness reverted by the quantification tools attackingtheleastrobustlocationsleadstothemostefficientattack.
to measure the attack efficiency we assigned a fixed number of attack chances to each sample and calculated the attack success rate asr overasetofsamples i.e.
theportionofsamplesthatare successfully manipulated into adversarial.
our evaluation was conducted on all the refined mdps of the lstm and gru models and withthetestdataset.weselecttwobaselines randomstrategy thatrandomlyselectsalocationtomutateand2 robustness guided attack based on the measurements by popqorn.
for the qc dataset since popqorn is only successfully executedon42samples weusedthesamesetofsamplestoevaluate the mdps refined from the qc lstm model to make a fair comparison.specifically foreachsentence weperformedtwosetsof experiments to attack the least robust word and the most robust word respectively.
we replaced the selected word to one of its synonymsinarandommannerandtheattackwasclaimedtobe successful whenever the prediction result became different.
we repeated the experiments times and report the averageasr.anoverallcomparisonamongthesethreeapproachesis demonstratedasabarplotinfig.
.foreachdataset itdisplaysthe averageperformanceof marblecalculatedoverallabstractions theperformanceofpopqornaswellastherandomattackstrategy.we also present the detailed results of marble under various configurations in table .
the three columns under asrl reports the asr when attacking the least robust locations and the increase rateofourapproachwhenrespectivelycomparedwiththerandom strategy and popqorn while the results of attacking the most robust locations are under asrm .
here the asr values are averagedoverthe100 timeexecutions.theperformancedifference is also statistically significant i.e .
confirmed by mann whitney u test at p .
confidence level .
from fig.
we can see that the random attack strategy makes a successrateof11.
.
and1.
respectivelyonthe3dataset.
compared with random strategy marble achieves higher asr i.e.
onaverage20.
.
and3.
whenattackingattheleast robust locations and lower success rate i.e .
on average .
.
whenattackingthemostrobustlocations.inthebestcases as shown in table marble outperforms the random strategy with anasr increaseof93.
.
and176.
when attacking the least robust locations.
for attacks on the most robust locations theasrbymarblecanbe77.
.
and41.
lessthanthat of the random strategy.
on the qc dataset popqorn achieves anasrof14.
whenattackingtheleastrobustlocations which isonly27.
higherthantherandomstrategyandworsethanall the resultsby marble.
whenattacking themost robust locations popqornstillraisesahighasrof10 whilemarbleonlymakes .
.
answer to rq3 in our evaluation marble calculates robustnessmoreaccuratelythan popqorn.marblecanalso achieve up to times higher attack success rate than the random strategy.
a qc dataset.
b tosic dataset.
c imdb dataset.
figure4 attacksuccessratesbasedontherobustnessscores calculated by different methods.
.
threats to validity first sixsubjectrnnmodelsandalimitsetofconfigurationswere examinedforthestateabstractionrefinement.wehavetriedour besttocoverawiderangeofmsethresholds whichisanimportant and direct factor influencing the accuracy of the robustness estimation.evenwiththis conclusionsdrawnonthelimitedgroup of subjects may not generalize to other models and configurations.
second fortheattackingexperimentsaimingtoevaluatethemodel accuracy thesetofsamplestobeattackedisofarelativelysmall size.
for qc dataset to accommodate the scalability of popqorn only42 sampleswere investigated.hence thecomparisonresults withpopqornmaylackstatisticalsignificance.finally randomness is hard to be avoided for both the refinement process and the attacking experiments.
the initial cluster centers of k means were randomly selected thus the state abstraction achieved may vary between executions.
moreover mutations were randomly selected accordingtothemutationprobabilitydistribution duringtheattack procedure.
given a fixed number of chances whether we can construct an adversarial sample is non deterministic.
in order tooffset the randomness to a certain extend we repeated all the experiments for five times and reported the average values.
related work adversarial attacks on rnns nlp and automatic speech recognition are the two typical domains where existing rnn robustness attackapplies.toattacknlpmodels paper isanearlywork to launch adversarial attacks on text classification task with fast gradientsignmethod.paper providedatechniquetolocate importantandsensitivewordswithreinforcementlearning.followingthesimilarline paper proposedtoscoreeachwordwith an importance metric and leverage word manipulations including swap substitution deletionandinsertion togenerateadversarial examplesforreadingcomprehensionsystems.fortextclassificationmodels therehavealsobeenworks ongeneratingadversarial examples to fool general purpose sequence to sequence models.
for attacks of speech recognition models research work presented an approach to generate untargeted audio attacks while paper proposedatechniquetogeneratephoneticallysimilar phrasesandmadeitpossibletogeneratetargetedattacks.paper furtheradvancedtheadversarialaudiogenerationtoproduceimperceptibleattacksforanygiventargetsandevaluatetheirapproach onpopularmodeldeepspeech .theafter mentionedapproaches all reply on gradient based algorithms which require the white box information on the full parameters of the subject models.
a gan basedblack boxattackgenerationisdesignedin withthe demonstrations on textual entailment and machine translation for untargetedattacks.analyzingthedifficultiesofleveragingexistingattackstornnthrougheitherdirectapplicationortransferattacks enables to estimate robustness of an rnn against an input.robustnessanalysisofdnn insteadofansweringwhetherthere exists an attack to compromise the dnn robustness verificationaims to identify the minimum adversarial distortion bound such that no attack exists with distortion under this lower bound.
existing works along this direction make some progress to verify the robustnessoffnns.theminimumadversarialdistortioncalculation of relu networks is shown to be np hard .
therefore further works attempt to compute a non trivial certified lower bound ortofindanestimationoftheminimumadversarialdistortion .bastani etal.
proposedtoencodethednn as a linear programming problem and defined a robustness metric based on the adversarial examples discovered.this metric depicts anupperboundoftheminimumdistortionandalsodependentson specific attack algorithms.
in contrast the clever score p r o vides an attack agnostic estimation of the lower bound.
paper developed the first sound analyzer for dnn with abstract interpretation to automatically prove robustness properties.
to our best knowledge the only research on the robustness verificationofrnnispopqorn whichproposesarobustness quantification framework to develop a guaranteed lower bound of theminimumdistortioninrnnattacks.however itsuffersfrom scalabilityissues whichcannothandlehundredsofhiddenlayers as demonstrated in our empirical evaluation.
conclusion thispaperproposedmarble amodel basedtechniqueforquantitative robustness analysis of rnn based dl systems.
our evaluation marbleenablesmoreaccurateandefficientquantificationofthe robustnessofrnns.inthefuture weplantoconductstudieson theeffectparameters k inthek steptrace basedrobustness onthe robustnessquantificationandapplymarbleondiversepractical applications such as image classification and automatic speech recognition.