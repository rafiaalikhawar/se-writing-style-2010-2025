ide support for cloud based static analyses linghui luo paderborn university germany linghui.luo upb.demartin sch f amazon web services usa schaef amazon.com daniel sanchez amazon alexa usa danjsan amazon.comeric bodden paderborn university fraunhofer iem germany eric.bodden upb.de abstract integrating static analyses into continuous integration ci or continuous delivery cd has become the best practice for assuring code quality and security.
static application security testing sast tools fit well into ci cd because ci cd allows time for deep static analyses on large code bases and prevents vulnerabilities in the early stages of the development lifecycle.
in ci cd the sast tools usually run in the cloud and provide findings via a web interface.
recent studies show that developers prefer seeing the findings of these tools directly in their ides.
most tools with ide integration run lightweight static analyses and can give feedback at coding time but sast tools used in ci cd take longer to run and usually are not able to do so.
can developers interact directly with a cloudbased sast tool that is typically used in ci cd through their ide?
we investigated if such a mechanism can integrate cloud based sast tools better into a developers workflow than web based solutions.
we interviewed developers to understand their expectations from an ide solution.
guided by these interviews we implemented an ide prototype for an existing cloud based sast tool.
with a usability test using this prototype we found that the ide solution promoted more frequent tool interactions.
in particular developers performed code scans three times more often.
this indicates better integration of the cloud based sast tool into developers workflow.
furthermore while our study did not show statistically significant improvement on developers code fixing performance it did show a promising reduction in time for fixing vulnerable code.
ccs concepts theory of computation program analysis human centered computing user studies usability testing .
keywords ide integration static analysis cloud service sast tools security testing permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august athens greece association for computing machinery.
acm isbn .
.
.
.
reference format linghui luo martin sch f daniel sanchez and eric bodden.
.
ide support for cloud based static analyses.
in proceedings of the 29th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse august athens greece.
acm new york ny usa pages.
introduction many companies are providing static analysis as a service e.g.
coverity scan veracode checkmarx and lgtm .
these tools fit well into ci cd since ci cd allows time for deep static analyses e.g.
inter procedural data flow analysis of the complete code base without taking up resources on a user s machine.
there are many benefits for performing static analysis tasks in the cloud.
from the user s perspective it can provide central storage and tracking of analysis results.
cloud based sast tools usually offer hooks to integrate with popular ci cd systems such as github actions jenkins or travis ci and offer a browser based dashboard for developers to manage findings.
from the supplier s perspective parallelism in the cloud can improve the performance of these tools.
as reported by microsoft moving static analysis for windows drivers to the cloud significantly reduced the analysis time spent for thesvb bugbash suite with .5x speedup.
in addition the cloud environment provides a central configuration of the analysis.
sast tool suppliers can tune the analysis engine to keep the false positive rate low and update the analysis rule set without shipping constant updates to customers.
despite all these benefits of static analysis in the cloud multiple studies have shown that the ideal reporting location for static analysis is the developers ide .
so there is a disconnect between the typical workflow where sast tools perform deep analysis in ci cd and developers expectation of interacting with these tools much earlier in the development lifecycle directly from the ide.
some cloud based sast tools provide ide integration to trigger an analysis manually from the ide.
e.g.
veracode static for ide allows developers to upload binaries to the cloud start a scan on demand and triage the findings from the ide.
does this style of ide integration meet developers expectation?
integrating such a cloud based sast tool into the developers workflow comes with a set of challenges.
in ci cd it is acceptable if an analysis spends several minutes computing in the cloud.
how would such waiting time impact user experience in the ide?
another challenge of designing such an ide integration is dealingesec fse august athens greece luo et al.
with the desynchronization between the code that is analyzed in the cloud and the code in developers ide.
while the analysis is running remotely developers might write more code which makes the analysis results out of date .
how should such results be displayed in the ide?
especially for long running analysis are developers aware of the time to run it?
the main goal of this research is to explore how ide support for a purely cloud based static analysis that is typically used in ci cd should be designed to meet the expectations of developers.
we identify the key design elements for such ide support and investigate whether it fits better into developers workflow in comparison to a web based solution.
specifically does it encourage more usage of the analysis improve developers performance i.e.
less time to fix code and perceived usability?
to investigate whether an ide solution can improve developers workflow we conducted a user study due to covid all interviews and usability tests were done remotely .
the four stages of the user study were user interviews section we started by interviewing developers to understand their expectations of how cloud based analyses should be triggered from an ide how the findings should be displayed there and what ux features developers would like.
prototyping section guided by the user interviews we developed an ide prototype for the existing tool codeguru reviewer using its infrastructure for ci cd.
second round interviews section we presented the ide prototype to the same developers in to evaluate whether the design met their expectations.
while developers were satisfied with most features implemented in the prototype they found existing mechanisms for ci cd e.g.
code uploading via git were cumbersome in the ide.
usability testing section finally we assessed our ide prototype with a larger group of developers.
in this test we applied both quantitative and qualitative research methods to determine if the ide solution was an improvement.
we found that using the ide prototype developers performed code scans three times more often than using the web based solution.
our measurements also show a promising reduction in time for fixing code.
we found that bringing the findings of the tool into the ide didn t necessarily improve developers workflow.
specifically they expected more education on capabilities and limitations of cloud based sast tools real time feedback on analysis progress e.g.
progress bars quick validation of each fix which implies incremental analysis on code changes seamless analysis of code e.g.
an analysis button without going through steps such as uploading it more interactive ways to suggest rescan integrated into current workflows.
background.
our study was conducted with developers at amazon web services aws .
we focused on the cloud based sast tool codeguru reviewer which is used as part of the ci cd process inside aws .
ataws every commit to its code bases is required to go through a code review process first.
teams can configure different sast tools including codeguru reviewer in their code review process.
codeguru reviewer has an expected running time of under minutes.
currently codeguru reviewerintegrated in the code review process only gets triggered to run along with other quality assurance tools when developers submit code changes to a remote repository via an internal pull request tool.
this internal tool pushes code to a detached branch and developers have to wait until codeguru reviewer and other quality assurance tools finish running.
the findings of these tools are displayed in a web application.
developers have to address the findings before merging the code changes.
usually developers address the findings together with comments from their teammates.
however developers told us they would like to get findings from codeguru reviewer before their code reviews which is not the case in ci cd so our focus of this study is to explore how ide support could be an improvement over the current flow and whether an ide solution is better than a web solution.
codeguru reviewer also provides a public api and a web interface to trigger a scan of a specific commit and fetch the findings.
we used this api to build an ide prototype.
user interviews first we wanted to identify developers expectations from ide support for cloud based sast tools.
with user interviews we aimed to answer the following two research questions rq1 what do developers expect from ide support for a cloudbased sast tool?
rq2 how could such ide support fit into developers workflow?
research methods.
we wanted to understand how ide support for a cloud based sast tool could fit better into developers workflow in comparison to a web solution.
thus we interviewed developers who already used a cloud based sast tool in practice.
we conducted semi structured interviews with developers using contextual inquiry .
contextual inquiry allows us to understand how developers work with codeguru reviewer on a day to day basis.
before the interview we sent each participant a link to one of their code reviews on which codeguru reviewer detected issues.
during the interview each participant was first asked to talk through their code review regarding codeguru reviewer s findings.
participants were asked to demonstrate how they fixed those issues in their ides together with the vulnerable code.
afterwards while they had their ides opened they were asked about their expectations on the ide support and also to describe the features and demonstrate them in their ides if possible.
the detailed questions list can be found in .
to differentiate from common static tools that run analysis on the same machine as the ide we explicitly told participants that the analysis is running in the cloud and that a scan takes minutes.
each interview lasted minutes to hour.
participants.
we interviewed participants who were all software development engineers from different teams and countries within aws .
to ensure that participants were already familiar with sast tools and willing to use them we started by finding developers who were involved with code reviews on which codeguru reviewer had found issues n and then invited developers n who replied to the codeguru reviewer findings.
all the interviewees had experience using static analysis tools codeguru reviewer findbugs checkstyle eslint sonarqube coverlay intellij built in static tools .
in the following we denote the developers with p1 .ide support for cloud based static analyses esec fse august athens greece data collection.
all interviews were recorded and transcribed.
they were carried out over video conferencing and all participants shared their screens during the interview so that their ide activity could be captured.
data analysis.
the responses were analyzed using thematic analysis.
we used both deductive codes derived from the questions we prepared for the interviews and inductive codes derived from the responses coding .
the codebook contains codes that were discussed and agreed upon by two researchers.
the list of codes and their definitions can be found in .
the coding itself was first done by the researcher who conducted the interviews.
to ensure reliability in the coding a second researcher checked and discussed all coded data together with the first researcher.
adjustments were made where disagreement occurred.
we applied an inductive approach to extract emerging themes which could be used to answer our research questions.
we hit saturation after the 7th participant whereby no new information was obtained.
.
result of the user interviews the analysis produced five themes analysis triggering mechanism result retrieval mechanism result display mechanism ux features and workflow integration.
in the following we will talk about how the first four themes of the ide solution could fit into developers workflow the fifth theme .
.
.
analysis triggering mechanism.
in this section we introduce how developers expect cloud based sast tools to be triggered from their ides how code in their ides could be uploaded to the cloud and other expectations on this topic.
ways of triggering the participants mentioned four ways the analysis should be triggered from the ide manual n build n fully automated n and semi automated n .
the most mentioned way was manual .
participants said the analysis should be manually triggered by clicking a button in the ide or by pressing a key shortcut.
participants would like control over the timing when their code is analyzed as p7 told us i would want to control it by myself.
if i would have a simple button to do the analysis in preparation i will do the testing before sending the code review i would upload the code to get the review by the machine.
most participants n also would like the analysis to be triggered in the project build process.
participants expected it to work this way since they used other lightweight static analysis tools like findbugs that can be configured as a build target.
some participants n mentioned that the analysis should be triggered in a fully automated way.
the developers don t want to do anything else to trigger the analysis except writing the code.
real time feedback from the analysis was expected.
p9 explained us the reason i don t want to introduce new behavior if there is a button during my normal flow i m very likely to forget that button.
p7 mentioned a semi automated way he expected that the analysis can be configured to run when he presses control s to save a file.
code uploading since the analysis is running remotely we interviewed the participants to understand how they expected the code to be uploaded to the cloud.
the participants mentioned two ways uploading with analysis triggering n and continuous 1we denote the numbers in fractions with the denominator being the sample size.uploading n .
the majority of participants n expected the code especially the changes to be uploaded when the user triggers the analysis.
their responses indicate that they expected the ide support will do it for them.
some participants expected the code changes or diffs to be continuously uploaded in the background.
developers have two mental models for how cloud based static analyses should be triggered from the ide via active triggering manual and build or passive triggering fully automated and semi automated .
developers with the first mental model would like to control the timing when they want feedback from the analysis.
they actively search and fix issues once they are done with their coding task.
the others prefer not thinking of the timing when they want feedback they expect the ide solution to provide feedback right after they make mistakes.
developers want to interact with the analysis as seamlessly as possible in a way that matches their individual workflows.
.
.
result retrieval mechanism.
all participants expected the ide support to retrieve analysis results automatically from the cloud.
they did not want to download an analysis report from the cloud and import it into their ides.
timing all participants expected the result to be retrieved to their ides directly after the analysis is completed.
this can be in the build phase if the build triggers the analysis after the user manually triggered the analysis or while coding if real time analysis is possible.
three participants mentioned that it would be sufficient if the result could be retrieved before they published code reviews.
although we told participants that the analysis is as time consuming as codeguru reviewer their responses indicated that they were not aware of codeguru reviewer s capabilities in terms of performance.
they used phrases like after several seconds and at most seconds .
some developers told us that they usually go on working on other tasks after submitting a code review and get notification emails when the analysis result is ready.
they only check the result of multiple tools after their teammates review their code.
this probably explains why some developers don t have a sense of the analysis time of a specific tool.
despite usage of cloud based sast tools in the ci cd process some developers were not aware of the capabilities and limitations of these tools e.g.
they were unaware how long codeguru reviewer takes to run.
project scope the participants mentioned four project scopes entire project n diffs n selection n and real time changes n .
scope has a twofold meaning either they only want the code in respective project scope to be analyzed or they only want to see the result in the scope.
seven of them expected to see the result of the entire project they were working on.
five of the seven also wanted partial code to be analyzed or to only see the result in partial code.
partial code can be diffs or selection e.g.
selected packages files or methods .
we also noticed that the scope often comes with developer s primary goal as p9 explained if i just added some code i am really interested in modifications i made.
if i am working on making the code better i would want to see all the issues.
six participants mentioned that they would like to see the analysis result in their code changes diffs if theyesec fse august athens greece luo et al.
knew previously they passed all the analysis checks.
only two participants expressed that they would like to see analysis result in real time changes e.g.
p6 if i write something the plugin would tell me immediately are you sure if you want to do this?
which part of the analysis result to be displayed in the ide depends on what developers primary goals are.
if they are interested in improving overall code quality showing findings in the entire project is preferred.
if their primary goal is to implement a feature they would like to see only findings that are context close to the code they are working on e.g.
diffs .
.
.
result display mechanism.
when talking about how the analysis result should look in the ide many participants demonstrated their expectations in their ides with compiler errors.
all participants suggested to visually highlight orunderline the problematic code and display a warning message which explains the issue when the user hovers over the line of code.
in addition all participants believed the severity of the issue should be included in the warning because it helps them to prioritize tasks.
some developers expected only critical issues to be shown and they must fail or block the build as p8 told us if there is a failure you have to fix it.
however if there is a warning it is basically ignored.
it is useless.
many participants n suggested to have a list view of all issues which allows direct navigation to the line of code when clicking on it.
one of them expected to see issues grouped in packages.
three participants would have liked to have quick fixes attached to the warnings.
p8 would like to have code snippet vulnerable code attached to the warnings such that he can easily see what the problem was .
display of invalid result since the analysis is running in the cloud by the time the analysis result is back to the user s ide the user might have made more changes to the code.
thus we interviewed developers to understand how they expected these invalid or old results to be handled.
five of the nine participants expected to see only issues where the code is still in place otherwise it is misleading as p1 told us.
also they did not want to spend time on investigating issues which might not be there anymore due to code changes.
p9 suggested the plugin can see this suggestion was for this particular line or file if this line or file changed the suggestion will not be displayed.
also two participants wanted to be informed about the code changes and a rescan rerun the analysis to be suggested by the ide support as p3 told us developers should be informed if they make changes to the code after they trigger the analysis they would have to redo the analysis for the changes.
they should be informed that the changes after triggering the analysis would t be considered.
if we show out of date recommendations in the ide the developers should be informed that these recommendations are for the past and they might be not valid now.
developers expect to be warned in their code just like the way their ides usually show compiler errors.
they do not want to spend time on issues which are out of date and expect the ide solution to remind them to rescan.
.
.
ux features.
the most mentioned feature by participants was quick fix n as p7 describes how he expected it to work you type alt enter it will offer you some fixes .
four participants would like to suppress warnings either false positives or issues which are less severe.
p1 expected it to be a list of previously suppressed warnings to re enable them or something like checkboxes .
p3 suggested to import a configuration file containing suppressed warnings as checkstyle does.
p9 would like to add a line of comment such as disable codeguru reviewer to the code to suppress.
participants also expected to customize the rule set of the analysis n and even the warning severities n to decide which warnings should be displayed.
both warning suppression and customization of rule set were mentioned as developers talked about features that would be beneficial for their teams.
developers expect the ide solution to not only pinpoint issues in their code but also to fix them.
they do not fully trust static analyses based on previous experience.
they expect to suppress or prioritize warnings based on their own judgment.
prototyping based on what we learned from the user interviews and the public api of codeguru reviewer we developed an ide prototype as a visual studio code vs code extension for codeguru reviewer .
in the following we introduce this prototype with respect to the themes derived from the interviews.
figure control panel of our ide prototype figure reminder notifications asking users to rescanide support for cloud based static analyses esec fse august athens greece analysis triggering result retrieval and display.
the prototype provides a control panel for users to interact with codeguru reviewer in vs code as shown in figure .
the show recommendations button allows users to view the recommendations findings provided by codeguru reviewer directly in the ide.
the prototype automatically compares the local code version to the remote code version and fetches the result to the ide if a matched analysis is found.
if there are local code changes which haven t been uploaded to the remote repository the prototype displays pop up notifications to remind the user for a rescan as shown in figure .
the user can choose to display the result of the most recent analysis on the current branch with the no still show recommendations button.
only recommendations in unchanged files will be displayed in the ide since developers told us they would not want to spend time on issues which might be invalid see subsubsection .
.
.
if the user chooses to push code and rescan a pop up window will ask for a commit message and code changes will be pushed to the remote repository.
after that the prototype triggers a new analysis in the cloud.
a notification will then be shown to tell the user about the estimated analysis time to minutes according to the official documentation and the result will be automatically retrieved once the analysis is completed.
the run repository analysis button allows the user to run a new analysis on the remote repository.
similarly it also reminds the user to push code if there are uncommitted code changes.
figure shows a typical workflow using our ide prototype .
developer modifies code and pushes changes to a remote git repository.
.
developer clicks the run repository analysis button to request codeguru reviewer to run a new analysis on the git repository.
.
codeguru reviewer receives the request and clones the git repository.
.
codeguru reviewer analyzes the cloned repository and generates recommendations.
.
the ide prototype automatically fetches the recommendations once codeguru reviewer finishes the analysis or the developer clicks the show recommendations button to fetch the recommendations to the ide.
at step while codeguru reviewer is running the developer can continue working on the code or switch to other tasks.
recommendations are displayed in a list view at the bottom of the ide as shown in figure .
they are organized in groups according to the source files.
from the interviews we learned that developercodegit repository3.
clone .
push code changesrecommendationscodeguru reviewercloud4.
analyze2.
request analysis .
fetch recommendations figure a typical workflow using our ide prototype figure recommendations are displayed in a list view.
figure warning suppression is shown as a code action.
some developers expect issues with fix suggestions to be prioritized.
for recommendations with fix suggestions we used the red marker as an attentional cue that the warning was actionable to help developers quickly get to the code.
it also indicates these issues are more severe and must be fixed.
although codeguru reviewer itself does not report the severity of an issue our consultation with the engineers of codeguru reviewer revealed that when the tool provides fix suggestions then it s typically for more severe issues.
yellow markers were used for all other findings.
these two markers are the default markers provided by vs code.
we also included weakness types and code snippets in the recommendations which were not provided by codeguru reviewer before.
clicking on a recommendation in the list navigates to the line of code.
the code is highlighted and underlined as shown in figure .
recommendations are also displayed in hover messages when the user hovers over the code.
the hover message supports markdown thus the urls to best practices in the recommendations are also clickable.
except quick fix we addressed all expectations on result display from developers as introduced in subsubsection .
.
.
although we would have liked to provide quick fixes this feature was not supported by codeguru reviewer at the time and most issues cannot be easily fixed by adding removing replacing a code string.
other ux features.
the prototype was built to support warning suppression and rule set customization because these were the most wanted features by developers.
warning suppression is provided as a code action automatic refactoring source code attached to the recommendation as shown in figure .
when the user chooses to suppress a warning the line of code will not be marked as an issue anymore and a special line code comment suppress codeguru reviewer is automatically added.
users can also manually add the suppression comment to code.
no warning will be shown at lines with that comment.
because we could not change codeguru reviewer to allow its rule set to be customized we implemented a rule set filter to allow users to select unselect the weakness types as shown under the settings section in figure .
only recommendations with the selected weakness types would be displayed in the ide.
developers also mentioned they would like to limit the display of findings to specific packages or classes so the prototype provided a scope filter to select files or packages they were interested in.
vs code alsoesec fse august athens greece luo et al.
has a built in filter for the warning markers such that users can filter the recommendations based on the severities in the issue list.
the configuration in the rule set filter and scope filter were locally stored by the prototype.
second round interviews after implementing the prototype we re invited the developers we interviewed for a second round interview.
five p1 p4 p7 p8 and p9 of them accepted our invitations.
these second round interviews allowed us to fix minor issues prior to the usability test with a larger group of developers to ensure the usability feedback was focused on core issues rather than surface level design concerns.
the interviews were structured by demonstrating the features of the prototype addressing the topics from previous interviews.
each interview was about minutes.
after demonstrating a feature we reminded the participant what she he told us in the previous interview and asked how the feature differs from what she he expected.
we transcribed and coded the interviews to assess user sentiment negative and positive across the themes extracted from the first round of interviews.
participants were all very positive about how analysis results were automatically retrieved and displayed in the ide.
they also liked the warning suppression and filters feature.
four participants didn t expect the code needs to be pushed to the remote repository to trigger the analysis.
p7 explained because for me it was like making dirty commits and i don t like it.
.
p8 gave us his reason i am not using test branch at all i am only using the mainline.
he felt it was not helpful if he needs to setup a remote branch for his changes to run the analysis before sending a code review.
although participants were critical about pushing code to the remote git repository we could not change the public api of codeguru reviewer to support other channels.
regarding old findings in changed files p8 said he would expect to see something even i change the file unless i change exactly that line.
after we explained that there might be case that an issue is fixed when new lines are added to the file he responded with i know the system doesn t know if it is fixed but i would like to keep track of what was the issue.
however the prototype reminds the user to rescan if there are local changes and the findings displayed in the ide will not be removed unless the user clears them intentionally or requests for an new scan.
before we started the usability test with a larger group of developers we tested the prototype with six developers and fixed bugs discovered in the interviews and during the test.
while code uploading mechanism via git push is widely accepted in ci cd integration some developers found it cumbersome in the ide due to different working habits e.g.
they only commit once per feature or do not use the gitflow workflow.
usability testing study design.
to test if the ide solution was an improvement over the web based solution we conducted a within subjects usability test with developers.
in comparison to between subjects studies it eliminates problems concerning individual differences .
we wanted to compare the condition with the ide prototype to thetable four treatments treatmentsession session system task system task t1 n ide x web y t2 n ide y web x c1 n web x ide y c2 n web y ide x 10less than years2 to years5 to yearsmore than years t1 t2 c1 c2 figure years of professional coding experience in java web based solution of codeguru reviewer inaws console where users can request analyses for their git repositories and view recommendations in a web browser.
for simplicity we use ideto represent our ide prototype and web to represent aws console in the following.
we prepared two tasks x and y. in each task participants were asked to fix issues in a prepared java application either with help of the ide prototype or aws console .
all issues can be detected by the analysis engine of codeguru reviewer .
the prepared applications use aws services with the aws sdk for java and each of them contains issues with different weakness types.
the test applications and issue list can be found in .
although the official documentation of codeguru reviewer gives minutes as the average analysis time for the two applications used in our study the analysis time was just minutes for each.
we applied different treatments to participants as listed in table .
t1 t2 are the treatments in which participants first test ide while in c1 c2 participants started with web.
from the study in we learned that the typical length of a working session with a sast tool of developers is minutes see table in .
the authors refer to sast tools with dedicated tools .
thus we chose minutes as the session length in our study.
in each session the participants were given maximally minutes to solve the task.
after each session participants were asked to fill out an exit survey see the survey in and take an interview with us to examine how participants used the tools and how the tools affected their behaviors.
participants.
we sent invitation emails to different mailing lists at aws .
in the email we asked people to fill out a demographic survey if they accepted our invitations.
we received survey responses and based on the responses we removed participants who do not write code in java.
we chose of them for our study.
the participants were located in different countries.
n of them never used codeguru reviewer before.
half n of them write code in vs code and n had written applications before with the aws sdk.
figure shows their professional coding experience in java.
more than half of them n have at least years experience.
we refer to these participants with g1 .ide support for cloud based static analyses esec fse august athens greece study setup.
the participants were assigned round robin to one of the four treatments.
in all treatments participants were asked to perform the tasks in vs code.
after a brief introduction to the study the participants were given the tasks in written form.
we explicitly told the participants that codeguru reviewer is a cloudbased sast tool and the expected analysis time to be a few minutes.
we ran codeguru reviewer on the test application before each session and made sure that participants saw the codeguru reviewer s findings displayed in either vs code or in aws console before they started the task.
we also provided participants user guides of the tested tool i.e.
ide prototype or aws console .
we told participants they could read them if they had questions.
participants were asked to solve the tasks independently without any help from us.
they were also asked to give us clear signals as they started and finished the tasks to record the time.
after each session participants were asked to fill out an exitsurvey containing the system usability scale sus questions .
in the survey we also asked participants to evaluate the difficulty of the task using a likert scale select features of the tool they thought were helpful estimate the number of issues they fixed and answer some open ended questions.
in each treatment group we randomly chose participants for monitoring.
we asked these participants to share their screen with us and think aloud as they were performing the tasks.
the others were not monitored.
because not all participants could take interviews with us after their sessions due to scheduling constraints we only interviewed of them after they completed the exitsurveys.
we asked them about the experience using the tool for the given task and whether the tool worked as they expected.
.
quantitative analysis developers tend to have different working habits when it comes to fixing issues in code as we learned from previous interviews.
while some developers tend to validate the fix every time they address an issue others fix all issues at a time and check them at once.
we wanted to investigate how different solutions for a cloudbased sast tool impact developers interaction with the tool.
our within subjects design allows us to test the effect on individual participants.
we also wanted to investigate whether our ide prototype was sufficient to improve developers performance in code fixing and perceived usability of the tool.
with the quantitative data we collected during the test we answer the following questions rq3 does the ide prototype encourage developers to interact more with the cloud based sast tool in comparison to the aws console ?
rq4 do developers fix issues more efficiently with the ide prototype in comparison to the aws console ?
rq5 do developers perceive the ide prototype to be more usable than the aws console ?
behavior rq3 our alternative hypothesis for rq3 is h1 using the ide prototype developers will rescan more frequently than using the aws console .
to test h1 we logged how many times each participant ran repository analysis successfully.
participants ran the analysis three 120ideweb t1 t2 c1 c2figure how often did participants rescan run analysis ?
the numbers shown in each bar are the total number of rescans performed by all participants in the associated treatment group in the condition.
table results of two tailed wilcoxon signed rank tests with .
.nis sample size without ties.
wcritis the critical value for nand .
p values .
are marked with .
a number of scans group n w value wcritat n p .
p value t1 t2 .
c1 c2 .
all .
b average time to fix an issue group n w value wcritat n p .
p value t1 t2 .
c1 c2 .
all .
c sus score group n w value wcritat n p .
p value t1 t2 .
c1 c2 .
all .
times more often from the ide in total than from the aws console in total as figure shows.
we used the shapiro wilk test to test whether our data is normally distributed.
since the data doesn t distribute normally we applied a two tailed wilcoxon signed rank test with .
which is non parametric and used for repeated measures.
although our hypothesis is one sided we used two tailed testing to ensure the statistical power in both directions .
we present the results in table with participants grouped by their treatments.
the statistics in table a suggests that there is a significant difference w wcritandp values are much smaller than .
in number of scans.
using the ide prototype developers performed analysis significantly more often than using the web based solution despite the fact that the analysis engine was the same and the tasks were similar.
this indicates that the ide solution fits better into developers workflow.
developers wanted validation of their fixes more often when addressing static findings and the ide prototype allowed them to run analysis easier.
based on survey responses to the question how did you know that you fixed the issues?
participants were more confident about the number of fixed issues they estimated in the idecondition.
while participants gave the answer saying that they didn t know in the web condition only participants were not sure as they used the ide prototype.
later in subsection .
we will introduceesec fse august athens greece luo et al.
table ide feature usage feature usage feature usage show recommendations rule set filter run repository analysis warning suppression clear recommendations scope filter 160ideweb t1 t2 c1 c2 figure how many issues did participants fix?
web ide t1 t2 c1 c2 figure boxplots for average time participants used to fix an issue.
average values are marked with .
the opinions of developers and how they felt their workflows were impacted in the two conditions.
the ide prototype also logged the total number of usage by all participants for each feature as shown in table .
participants clicked the show recommendations button times which is .
times than they clicked the run repository analysis button.
the huge difference between the usage of these two buttons suggested that participants didn t choose to rescan but opted for displaying old findings as they were the tasks.
while most participants actively clicked the run repository analysis button to rescan times some participants took suggestions from the ide prototype times and selected yes in the pop up notification to rescan as shown in figure .
other features were rarely used.
this is likely due to the short time planned for each session.
we asked developers to select features they thought were useful in the survey of the participants selected warning suppression for the rule set filter and for the scope filter.
performance rq4 our alternative hypothesis is h2 given an application containing issues that can be detected by codeguru reviewer developers using the ide prototype will be faster than using the aws console to fix an issue.
while participants did not finish the task timed out in the session using the ide prototype the number with the aws console is .
in three of the four groups t2 c1andc2 participants fixedmore issues in the idethan in the web condition as shown in figure an issue was considered fixed if codeguru reviewer didn t report it again.
.
surprisingly participants fixed the same number of issues in total when using the ide prototype and theaws console .
this is close to the number of issues participants estimated in the exit survey i.e.
in the web and in the ide condition.
for each participant we computed the average time used to fix an issue.
since one participant didn t fix any issue with aws console we excluded this data from the test.
our test failed to reject the null hypothesis as the statistics shown in table b .
however as the boxplots in figure show there is a promising time reduction average and maximum values are lower in the ide condition among most groups of participants t2 c1andc2 .
in these groups developers performance was also more consistent in theidecondition since the boxes are smaller.
usability rq5 although we were comparing a research prototype to a commercial tool designed by professional ux designers we formulated the alternative hypothesis in an optimistic way.
h3 developers will rate the ide prototype with higher susscores.
we evaluated the survey responses to the sus questions and computed the sus scores.
the higher the score is the better the perceived usability.
again we applied wilcoxon signed rank test to the sus scores and the result is shown in table c .
there is no statistically significant difference between the two conditions regarding the sus scores.
however we found out that participants tended to rate the tool they tested later with higher sus scores as the boxplots in figure show.
we observed that many participants who started testing the ide prototype at first i.e.
t1 and t2 were actually not expecting the analysis to be time consuming.
these participants were more confused as the ide prototype did not display results immediately as they tried to run the analysis.
note that n of the participants never used codeguru reviewer before.
in contrast participants who tested the web condition first had a better sense of the asynchronous nature and the analysis time as they tested the ide prototype later.
we will discuss this further with the qualitative data in subsection .
.
web ide ide web t1 t2 web ide c1 c2 figure boxplots for sus scoreside support for cloud based static analyses esec fse august athens greece although we pseudo randomly sampled participants into groups to control for key factors like experience with pre existing tools we had a few issues specifically affecting group t1.
the data were affected by four participants in t1who fixed more issues in the web condition less time per fix as shown in figure and rated extremely low sus scores in the ide condition.
participant g34 had more than years of development experience and was very familiar with sast tools used in ci including codeguru reviewer so he was extremely fast in the web condition only used half of the time as in the ide and fixed more issues.
in the interviews participants g20 and g21 mentioned that they did not understand there was no local analysis and got confused in the idecondition while in theweb condition it was straightforward for them and they could better focus on the task.
g2 was observed spending time exploring the features of the ide prototype rather than fixing issues.
he gave a much lower sus score for idethan for web.
developers perceived usability of the ide prototype was impacted by both their pre existing expectations on ide integration of lightweight static analyses and familiarity with the cloud based sast tool.
.
qualitative analysis we coded the after session interviews and survey responses to tell us what needs to be improved .
we reused the codes from previous interviews introduced in section and section and also added new codes see codes in .
we answer the following questions rq6 how does the ide prototype differ from the expectations of developers?
rq7 how did developers think the ide prototype impacted their workflows?
.
.
rq6 how does the ide prototype differ from the expectations of developers?
the positive things developers mentioned about the ide prototype were similar to those we heard from previous interviews introduced in section .
moreover developers were very satisfied with the quality of the analysis result.
they felt the recommendations were precise and informative.
this is probably the reason why warning suppression was rarely used in the test.
in the following we focus on the major issues of the ide prototype pinpointed by developers.
we also identified some issues in the aws console and reported them to the engineering team.
analysis triggering mechanism the biggest issue mentioned by participants n was uploading code via git.
some participants felt uncomfortable to push code without a code review.
others felt less confident to push code without testing it locally.
although the test applications provide unit tests these participants didn t run them at first instead they expected to get feedback before testing.
many participants n were expecting fast local validation of fixes when they clicked on run repository analysis button as g8 said when i modified the code it was strange to see the squiggly lines here and there saying there was an error.
although there were pop up notifications shown in the ide mentioning the analysis time and suggesting rescan some participants seemed to pay little attention to those pop ups.
these participants are mostly from the groups who tested the ide prototype at first.developers expected ide support to allow usage of cloud based sast tools before their code goes into the next phases test ci cd in the development lifecycle.
they wanted the analysis of code without going through steps such as uploading it.
result retrieval and display mechanisms most problems came from the analysis time and poor indication of the analysis progress in the ide.
codeguru reviewer needs about minutes for reanalyzing each test application which is shorter than the official average analysis time minutes given by codeguru reviewer .
still it was painfully slow to the point i was worried the plugin was unresponsive as g15 told us.
as mentioned above pop ups were not sufficient for informing participants about the analysis time.
as g8 told us although he read the pop ups he thought it was a generic message and didn t consider it would be the exact time .
more than one third of the participants n we interviewed expected to see a progress bar or a dynamic display of analysis status in the ide.
they wanted to see what it the service is currently so not just all of a sudden a pop up comes up saying the result is ready.
this is also the pain point in the aws console since there is no progress indication either.
using the ide prototype many participants told us they kept working after started a new scan.
most confusion came from displaying old findings.
some participants didn t rescan but chose to see them in the ide and clicked the show recommendations button multiple times as they were fixing the issues.
they thought this button performed local validation of a fix although this button actually checks code version and only displays the findings in unchanged files.
this led to the problem finding disappeared and i had trouble to get them again.
as g12 described.
this can probably be resolved by reading the user manual or a mechanism that checks local changes better.
other participants felt the outdated messages are annoying i d rather have them disappear when a line gets updated to invalidate them.
since they were expecting the prototype to be as reactive as lightweight static analysis tools.
pop ups were less effective in educating developers on mechanisms built in the ide solution.
if the analysis is not returning results instantaneously or developers activities in the ide are not blocked the display of old findings and unpredictable waiting time for new findings are deal breakers .
this suggests developers need clear visual cues e.g.
progress bars to understand when the analysis is running and if findings are outdated.
ux features five participants mentioned that they were not aware that the analysis was running remotely thus they were expecting real time feedback due to previous experience.
as g20 told us that he didn t think about the analysis was running in the cloud.
his initial perception is that this is going to be some sort of static analysis tool and he was expecting it to be a similar experience to other static analysis tools he has used.
these participants suggested more obvious visual indication for the asynchronous nature of the ide solution.
another feature that was missed by participants for both the ide prototype and aws console is a way to keep track of addressedesec fse august athens greece luo et al.
issues.
g9 told us he was using the rating buttons thumb up and down symbol in the aws console as almost a checklist.
i know which i have done because i marked them helpful.
although quick fix was the most mentioned feature in our first user interviews introduced in section only participants mentioned it this time.
developers understood that it was harder to provide quick fixes for more complex issues detected by a sast tool than a linter.
two participants suggested that the ide prototype should forbid or auto cancel multiple scans on the same commit since it s a wasted action .
even though most developers were aware that the cloud based sast tool performs a deeper analysis and acknowledged that the analysis takes longer they still complained about the waiting time and that findings and code ran out of sync.
this suggests that using the same visual components for the cloud based analysis that are also used by lightweight local static analyzers e.g.
problem list windows error markers on code may create unrealistic expectations about the behavior of the tool.
.
.
rq7 how did developers think the ide prototype impacted their workflows?
as we show for rq3 the ide prototype impacts developers behavior in fixing code the qualitative data also indicates the same.
although developers interacted with the same analysis engine they approached the tasks differently in the two conditions.
a participant used a metaphor to express the different feelings the thing in the aws console felt like integration test.
having it in the ide was like unit test.
in the aws console because it is in a browser participants felt a disconnect between running the analysis and editing .
a few participants perceived the list of findings as a task list as g9 told us i saw the task list and i went to work on that code.
it just didn t click for me that i can go back to the aws console and rerun the analysis.
they felt that they were supposed to pick a workflow in which they would only rescan once they addressed all issues.
not seeing the result immediately was less frustrating because it was clear that there was no synchronization.
while some participants chose to address all issues at once others felt their workflows were paused in the web condition as g29 told us i was somehow encouraged to wait to see what happens.
i felt that if go back working i would not be aware when the execution finishes.
using the ide prototype without switching between the browser and the ide some developers rescanned more often.
a participant who addressed all issues at once in the web condition said in the ide it was like i saw the thing turned red i fixed it kept iterating on it until error free then i move on to the next one.
so i was expecting some feedback.
i changed something hit on run repository analysis another participant told us he felt encouraged to change code and rescan even before the previous scan is completed such that he could work more efficiently.
using the ide prototype developers wanted to validate their fixes more often and felt encouraged not to wait for the analysis execution but continue working on fixing other issues.despite of all the problems identified in the ide prototype many participants expressed that they would prefer the ide support to interact with cloud based sast tools.
g21 told us he would prefer ide because less time wasted having to go through other screens.
i can push code and let analysis run on branch while continuing workflow in the ide.
less context switching.
g1 also preferred the prototype but wished it was more interactive it runs analysis on the file you are working on and tells you if you fixed it correctly or not.
also g22 thought that the ide integration is the better path because it s closer to the activity being performed writing code.
despite the delay of the analysis he would much rather see a list of suggested problems fixes in my editor than changing screens back and forth.
threats to validity external validity we conducted our study with developers of a single company.
among them only a few participants were female.
this may lead to limited generalizability of our findings to the whole developer community.
however the participants were located in different countries have years of professional experience and work on different kinds of products.
furthermore we only studied the effect of ide integration for one cloud based sast tool and one ide.
as we demonstrated the response time of the sast tool is a major factor so our findings cannot be generalized to tools that are significantly faster or slower.
a follow up study can determine this effect by artificially introducing delays when retrieving findings.
our prototype is based on language server protocol that integrates with most modern ides so we expect the impact of the ide choice to be minimal but developers familiarity with an ide could affect the study.
we only compared the ide solution to the web based solution ofcodeguru reviewer regarding repository analysis.
we did not consider the impact on development lifecycle management with the issue board in the web based solution.
it is likely that project managers and team leaders would have a higher preference for the web based solution.
moreover we did not consider cost security and trust.
some participants were critical about pushing code for a rescan and proposed to hide the action however real customers of codeguru reviewer might not want their code to be uploaded silently due to security concerns.
internal validity the first threat is the session time.
some participants told us they felt stressed and they did not have enough time to fix all issues.
while the available time limited the performance of some participants it also simulated the pressure of software development on tight deadlines e.g.
before releases.
we also observed that some participants were less motivated to fix code but more interested in playing with the features of the tools.
another threat was attrition.
we had four developers who did the first interviews with us not attend the second round interviews and two their data are not included in the paper did not participate in the usability test.
we are aware that our findings are likely to be based on a biased sample of developers who have higher motivation to use static analysis tools or cloud services.
moreover the tasks in the usability test were artificial.
due to unfamiliarity with the projects or the used java libraries some participants may have performed worse than in real development situations.
however weide support for cloud based static analyses esec fse august athens greece only selected developers who have professional experience in java and the majority n of them used aws sdk before.
regarding the impact of developers familiarity with the ide we applied wilcoxon singed rank test to the sample grouped by tasks and grouped by the experience with vs code the result indicated there was no significant difference between the groups.
regarding the issues detected by codeguru reviewer they were all true positives.
lastly the ide prototype was not designed by professional ux designers but researchers.
it is likely that developers would perceive a significant improvement of the usability using a professionally designed ide solution in comparison to the web based solution.
although we were comparing a prototype to a web application with real customers our result indicates that the prototype is not worse.
related work the usability of static analysis tools has been studied by many researchers.
johnson et al.
interviewed experienced developers to understand why developers were not widely using static analysis tools .
they found out that false positives and bad warnings were the major reasons for developers dissatisfaction.
christakis and bird surveyed developers at microsoft to understand what developers want and need from static analysis tools .
their study shows that developers would like static analysis tools to detect more critical issues for them such as security or concurrency issues and display the findings directly in their ides.
beller et al.
studied the usage of static analysis tools on open source projects .
they found out that most open source developers only use static tools sporadically and they need to be made aware of the benefits of using these tools.
vassallo et al.
studied developers behavior using static analysis tools over different development stages .
they found out that severity is the most important factor for developers in prioritizing issues to fix which was confirmed in our study.
steidl et al.
suggested to prioritize issues that are easy to refactor .
their study indicates prioritizing by low refactoring costs matches greatly the developers opinions.
in our study we also heard expectation of such prioritization mechanism from some developers.
a more recent study from nguyen quang do et al.
took a user centric approach to understand why developers use static analysis tools and which decision they make when using these tools .
according to their study with developers at software ag ides are still the ideal reporting locations wanted by developers.
however we observed that there exists a disconnect between the typical usage of cloud based sast tools in ci cd and developers wish to interact with them earlier in the lifecycle in their ides.
our work focuses on exploring how ide support for cloud based sast tools that are typically used in ci cd should be designed.
we approached the exploration from developers perspective with a user study.
we found out that developers expected more than just seeing the findings of these tools in their ides.
in recent years we see an increased interest in studies that apply static analysis tools at scale .
facebook s static analyzer infer has detected over issues that have been resolved by facebook s developers since .
as reported by google their static analysis platform tricorder could analyze code review changes per day.
more than issues perday were tagged to be fixed by developers.
these studies discuss tools that are integrated in the code review process.
our work builds on the results of these papers and asks the question how we can give developers access to cloud based sast tools directly through their ides and if this improves developers workflows.
we share challenges and lessons learned in the exploration that can be beneficial for suppliers that wish to build such ide support.
many researchers have studied the impact of cloud services on the user experience .
kaisa v n nen vainio mattila et al.
studied the user perceptions of wow a positive user experience when using cloud services .
they proposed a few design implications for achieving wow such as pushing dynamic features to keep the user stimulated.
tang et al.
interviewed users of file synchronizing and sharing services to understand the cloud based user experience .
they found out that users understanding and usage of cloud functionalities are limited by their existing practices.
similarly we also learned that developers expectations of ide support for cloud based sast tools were affected by their awareness of the limitations of these tools and their previous experience with lightweight analysis tools.
developers overexpectations hugely impacted the perceived usability when interacting with our ide prototype.
through a usability test we identified important design elements and mechanisms required for a better tool support.
conclusion to investigate how ide support for cloud based sast tools should be designed we conducted a multiple staged user study.
we first interviewed developers at aws to understand their expectations.
developers feedback indicates that they expected the ide support for cloud based analyses to behave similar to the lightweight static analysis tools they already use in their daily work.
their responses also indicate that they have limited understanding of the capabilities and limitations of sast tools.
guided by the user interviews we developed an ide prototype that was positively confirmed by the same group of developers.
we tested this ide prototype on developers.
this usability test showed that allowing developers to interact with a cloud based sast tool through their ide significantly increased their interaction with the tool i.e.
they ran the analysis much more frequently than using the web based solution.
this might impact the code quality in a long time span.
moreover we found promising reduction in fix time even in our small size study.
a larger longitudinal study on this impact should be conducted in the future.
however we also found out that reusing the same visual components for the cloud based analysis that are also used by lightweight static analyzers e.g.
problem list windows error markers on code created confusion and that developers need clear visual cues to understand the asynchronous nature of cloudbased analyses.