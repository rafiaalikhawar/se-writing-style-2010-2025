how disabled tests manifest in test maintainability challenges?
dong jae kim software peformance analysis and reliability spear lab concordia university montreal quebec canada k dongja encs.concordia.cabo yang department of computer science and software engineering concordia university montreal quebec canada b yang20 encs.concordia.cajinqiu yang department of computer science and software engineering concordia university montreal quebec canada jinqiu.yang concordia.catse hsun peter chen software peformance analysis and reliability spear lab concordia university montreal quebec canada peterc encs.concordia.ca abstract software testing is an essential software quality assurance practice.
testing helps expose faults earlier allowing developers to repair the code and reduce future maintenance costs.
however repairing i.e.
making failing tests pass may not always be done immediately.
bugs may require multiple rounds of repairs and even remain unfixed due to the difficulty of bug fixing tasks.
to help test maintenance along with code comments the majority of testing frameworks e.g.
junit and testng have also introduced annotations such as ignore to disable failing tests temporarily.
although disabling tests may help alleviate maintenance difficulties they may also introduce technical debt.
with the faster release of applications in modern software development disabling tests may become the salvation for many developers to meet project deliverables.
in the end disabled tests may become outdated and a source of technical debt harming long term maintenance.
despite its harmful implications there is little empirical research evidence on the prevalence evolution and maintenance of disabling tests in practice.
to fill this gap we perform the first empirical study on test disabling practice.
we develop a tool to mine 122k commits and detect changes that disable tests from open source java systems.
our main findings are test disabling changes are more common than regular test refactorings such as renames and type changes.
our life cycle analysis shows that of disabled tests are never brought back to evaluate software quality and most disabled tests stay disabled for several years.
we unveil the motivations behind test disabling practice and the associated technical debt by manually studying evolutions of unique disabled tests achieving a confidence level and a confidence interval.
finally we present some actionable implications for researchers and developers.
ccs concepts software and its engineering software evolution maintaining software .
dong jae kim and bo yang contributed equally to this paper.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august athens greece association for computing machinery.
acm isbn .
.
.
.
test disabling test smell test maintenance technical debt acm reference format dong jae kim bo yang jinqiu yang and tse hsun peter chen.
.
how disabled tests manifest in test maintainability challenges?.
in proceedings of the 29th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse august athens greece.
acm new york ny usa pages.
introduction modern software development handles an increasing complexity of feature enhancements.
to ensure that the software quality remains on par with consumer expectations software testing has been playing a pivotal role in software development.
with the availability of junit and other testing frameworks writing test code is becoming widely popular .
most large scale systems utilize testing practices routinely and expose faults early.
however test code can be subject to age and quality issues like the production code under test.
for example a test can also contain test specific design issues that may hinder its ability to guard against regressions.
prior studies have found that flaky tests may hinder the reliability of testing results that may fail for reasons other than recent changes.
similarly researchers introduced the concept of test smells which are design issues specific to the test code that may negatively impact test code comprehension and maintenance .
to mitigate the efforts to fix broken tests developers need to maintain and improve test code continuously.
challenges in maintaining and improving test code are more than fixing flaky tests and refactoring test smells.
a prior study by pinto et al .
highlights both the importance and potential of studying the evolution of how tests are modified added or deleted.
understanding such evolution is essential to understand how the test becomes obsolete why it is difficult to fix and how it should be repaired.
the findings can inspire future research and provide better testing support and tools.
in the context of software testing developers can disable tests by commenting out the test method or class.
in addition with the introduction of annotations in java frameworks such as junit and testng introduce the annotations ignore and test enabled false allowing developers to disable failing tests temporarily.
although disabling tests can be seen as an added flexibility for developers to alleviate maintenance difficulties one can suspect that it may introduce technical debt.
with such flexibility developers may disable hard to fix tests as a compromising solution to meetesec fse august athens greece dong jae kim bo yang jinqiu yang and tse hsun peter chen project deliverables.
despite the potential challenges that arise later from disabling tests there exist limited studies on disabled tests as a source of technical debt.
we believe that studying why developers disable test code is of paramount importance for both practitioners and researchers it indicates a source of potential technical debt that can direct future research efforts it may provide additional evidence on how bugs are fixed to improve automatic tool support and can help prevent future encounters of bugs.
to the best of our knowledge there are no studies in the literature investigating disabled tests as a source of technical debt and providing tools for tracking all types of disabling changes in the test code.
to address this issue we develop an automated tool to identify all kinds of disabling and re enabling practice at commit level a commented out test code instance commenting out or deleting test using ignore from junit and setting test enable false in testng.
our approach could detect the disabling re enabling practices with an overall precision of .
in total we study test disabling practices in open source systems of different sizes and from diverse domains.
our study focuses on understanding the disabled tests from the following aspects how often do developers disable a test i.e.
the prevalence and evolution and why developers disable a test i.e.
motivations behind disabling and re enabling a test .
in particular we answer the following three research questions rqs rq1 how common are test disabling changes?
test disabling practices are more common than regular test refactorings such as renames and type changes.
even though we find that disabling tests are prevalent there is no prior study on how they may affect test maintenance.
rq2 what is the change pattern of disabled tests?
through analyzing the change pattern and final destination of the disabled tests we find that most disabled tests stay disabled.
many of the disabled tests have been disabled for several years.
we also find that for the disabled tests that are resolved many are deleted directly.
rq3 why do developers utilize test disabling practice?
we qualitatively uncover test disabling practices and how they are used to bypass maintainability challenges.
we find that most tests were disabled in the first place due to issues such as test failures but many tests remain disabled even when the bugs are fixed.
some bugs may be marked as won t fix with the tests being disabled.
we also find that developers often use disabling changes to handle other maintenance challenges in testing such as test dependency and refactoring.
our findings highlight potential future directions on helping developers improve test maintenance and detect potential issues in test code.
in summary our findings provide actionable implications for two groups of audiences researchers we open an avenue for further research directions on detecting disabled tests and their relation with other aspects of software development i.e.
quality maintainability and performance improvements .
we also highlight potential directions on assisting developers in tracking disabled tests and their co evolution with production code.
developers our findings reveal the usage of disabled tests in many different aspects of test maintenance.
disabled tests may be used in ad hoc ways to hide real faults or bypass test failures.
most tests are temporarily disabled until a fix is found however thedisabled tests are not re enabled after the fix i.e.
a bug report is closed .
these findings indicate the need to assist developers with the best testing practice to trace disabled tests in practice.
paper organization.
section studies related work.
section discusses our methodology and provides preliminary results.
section presents our quantitative analysis results and section presents our qualitative analysis results.
section summarizes the implication of our findings.
section discusses threats to validity.
section concludes the paper.
related work in this section we discuss prior studies in two areas test maintenance and evolution and technical debt.
.
test maintenance and evolution software testing is an important practice for developing highquality software.
however similar to source code there may be maintenance issues related to test code.
many prior researches focus on studying various testing practices such as test quality maintenance comprehension and evolution .
bavota et al .
found that similar to regular code smells test smells are prevalent in software systems and may hinder test comprehension and maintenance.
kim et al .
are the first to study test annotation maintenance.
they uncovered test annotation smells and provided opportunities for refactoring test code using test annotations.
they found that ignore is one of the commonly used annotations in test maintenance.
our work is different as we target test disabling ignore practices that are not limited to annotation changes.
our work considers all types of test disabling from commenting out tests to using ignore.
we also propose approaches to track commented uncommented test code with high precision.
through a survey peruma et al .
found that developers believe using ignored may increase compilation time and may be harmful to code comprehension.
our work studies further why test cases are disabled at the finer level and shows that software bug is only one of many reasons causing tests to become disabled.
zaidman et al .
proposed several views to mine and visualize the co evolution of test and production code.
borle et al .
further study a wide spectrum of how rigorously systems on github utilized test driven development.
they found that most test and production code are not updated together i.e.
the use of tdd was rare .
the most relevant prior work is the study done by pinto et al .
.
they conducted an empirical study on how test suites evolve i.e.
test deletion addition and modification to understand the reasons for test changes.
they found that in addition to test repair most test changes are related to refactoring deletion and addition.
in particular they found that the key reason for test deletion is test obsolescence.
different from prior studies our work is the first to study test maintenance from the perspective of test disabling practices.
we implemented a java annotation parser and a tool to track how disabled tests evolve.
we found that test disabling changes are prevalent during test evolution and many disabled tests remain disabled.
we also manually studied the reasons for the tests to be disabled and discussed potential challenges and futurehow disabled tests manifest in test maintainability challenges?
esec fse august athens greece directions on test maintenance.
moreover pinto et al .
reported .
test code deletion that was once alive while our study looks at disabled tests that are deleted i.e.
.
.
.
technical debt cunningham discussed the concept of technical debt where short term rewards may induce higher maintenance costs in the long run.
potdar and shihab discussed the concept of selfadmitted technical debt satd where developers intentionally introduced some commented code as a form of temporary fixes.
wehaibi et al .
later showed that satd induces less future defect than non satd however satd changes are more complex to perform.
our work is the first to investigate technical debt specific to practices of disabling test code by analyzing evolution history.
tracking history has long been recognized as a crucial artifact for many empirical studies for understanding the rationale for software changes .
several approaches have been proposed to help developers and researchers better leverage source code histories.
tsantalis et al .
developed refactoringminer that can accurately track commit level refactoring history.
grund et al .
implemented codeshovel to accurately track method changes in evolution history.
in our work on tracking disabled tests we propose a new approach to handle newly added test code commented out and uncommented out which was never handled in prior studies.
methodology in this section we discuss our methodology for tracking test code evolution to identify test disabling related changes.
tracking the evolution of program elements in commit history is an ongoing research direction and most existing tools are not designed specifically to work on annotations and commented out code .
therefore we propose an approach that detects disabled tests through analyzing annotations and comments and tracks the evolution of the detected disabled tests.
our approach first detects all tests including both disabled and active i.e.
enabled tests in each studied version section .
and second performs version byversion comparison to track the evolution of the tests section .
.
.
definition of test disabling practice when using testing frameworks such as junit or testng developers can use the test annotation to specify a method or methods in a class as test cases.
as systems evolve some tests may become obsolete or require some changes and developers may want to temporarily disable a test.
developers can use framework supported annotations i.e.
ignore removing the test annotation or commenting out the test method class to disable a certain test.
our goal is to study such test disabling practices in the codebase and their potential impact on test maintenance.
in particular we consider the following code changes as test disabling changes in contrast we consider the reverse operations as test re enabling changes adding ignore at both the class and method level.
setting test enable false .
deleting test annotation.
commenting out the entire test method or class.
.
detecting disabled and active tests given one version our approach first detects all the disabled and active i.e.
enabled tests.
detecting active tests is straightforward we leverages javaparser to find all the tests with an test annotation as the testing frameworks junit and and testng of the studied systems require to have such an annotation for tests.
detecting disabled tests requires one to design techniques per each type of test disabling practice.
each type of disabling related change as defined in section .
corresponds to one unique type of disabled test.
the first two types i.e.
adding ignore and setting parameters in test will result in adding explicit annotations to the disabled test methods.
the third type will produce methods in test classes without an test ... annotation.
the fourth type will result in complete test methods embedded in comments.
similar to detecting active tests based on test annotation for the first three types we also utilize javaparser for analyzing method annotations.
for the last type we propose an algorithm to identify commentedout test methods.
.
.
analyzing annotations for detecting disabled and active tests.
for detecting active tests and some types of disabled tests we use the off the shelf javaparser to extract annotations per method in test classes.
the following rules are applied to decide the status of a method in test classes.
if the annotations contain test without enable false parameter the associated method represents an active test.
if the annotations contain either test enable false or ignore the associated method represents a disabled test.
if the annotations do not contain test the associated method represents a candidate disabled test.
note that the lack of test annotation is indefinite to decide a disabled test since it is common for developers to write nontest methods e.g.
helper methods in test classes.
such candidate disabled tests will be further confirmed through analyzing the evolution i.e.
tracking .
if a previous later version of a candidate disabled test is an active test this test method is confirmed disabled for the current version.
.
.
detecting disabled tests in comments.
in addition to the disabled tests expressed by annotations we also identify the tests that are disabled through commenting out.
algorithm describes how we extract disabled tests in comments.
the input commenttarget is either a block comment or a list of comments in consecutive lines.
a commenttarget may contain zero or more commented out tests.
for onecommenttarget our tool first detects an test annotation line and then continues to detect a in the following comment lines.
the text in between could be an annotation and a method signature.
then we use a javaparser api parsemethoddeclaration to confirm whether the text is indeed a method signature.
in addition we use the existence of a paired right bracket to filter out incomplete test methods that only contain method signatures.
.
tracking the evolution of disabled tests upon detecting candidate disabled and re enabled tests in one version we continue to track the evolution in the commit history and pinpoint the related changes of such tests e.g.
one disabled testesec fse august athens greece dong jae kim bo yang jinqiu yang and tse hsun peter chen algorithm commented out test method detection input commenttarget output cotests i.e.
is a list of commented out tests function detectcotests commenttarget lines commenttarget.split n cotests fori i lines.length i do line lines ifline contains test then location location of the f irst occurence of in this or the f ollowin lines break if location is null notmethodsi nature is based on a javaparser api continue if notmethodsi nature strinbetween i location end f indendo f methodbod y lines location cotests.add the detected commented out test i end end if end for end function is later re enabled.
tracking the evolution requires our approach to match program elements in every two consecutive versions.
we adapt refactoringminer to perform the matching because refactoringminer is shown to have the highest precision .
and recall compared to other refactoring tools and ast diff tools and refactoringminer can detect various types of refactoring operations such as method class renaming moving and extracting methods and modifying method signatures.
furthermore we incorporate tracking commented out tests in our approach.
as commented out code may be incompatible with the live code and may result in compilation errors not supported by refactoringminer we use a lightweight approach to handle the commented out test code.
in particular a test id fully qualified class name method name method parameter types list is constructed based on the extracted information of commented out tests section .
.
.
for each commented out test method its test idis compared with the ones in the parent and child commits for finding the paired test methods.
the abovementioned points allow our approach to collect an accurate and comprehensive dataset for our study.
for each pair of matched tests in two consecutive versions if the status of the test is modified from active i.e.
enabled to disabled we decide the commit is disabling related changes.
the reverse status change is determined as a test re enabling change.
if there is no matching test in the latter commit the unmatched test is deemed deleted.
if there is no matching test in the prior commit the unmatched test is deemed newly added by the current analyzed commit e.g.
one commit may introduce commented out tests.
approach evaluation.
we performed an evaluation of our approach with regards to detecting disabled tests in comments section .
and detecting disabled tests in commit history as these two steps may produce incorrect results.
for detecting disabled tests in comments we took all the detected commented out tests examined them manually to decidetable precision of test tracking i.e.
detecting the three types of commit changes including disabling a test reenabling a test and deleting a disabled test.
disabling re enabling deleting an total a test a test disabled test sample precision the correctness.
our approach yields a precision of .
however we cannot evaluate the recall due to the lack of oracles.
for as tracking is performed at commit level e.g.
whether one commit is disabling related we used stratified sampling to take a statistically significant sample of cases on three types of changes namely disabling a test re enabling a test and deleting a disabled test .
then we manually examined the correctness of each sampled commit.
table shows the precision of the three types of changes.
our approach achieves a precision of for all the sampled cases and a precision of and for the three change types respectively.
upon manual examination we identify the following sources of false positives.
first due to framework migrations developers may need to add or delete annotations.
for example migrating from testng to junit4 will remove the test on the class which will be detected as ignoring a test class and adding test on each method will be detected as unignoring test methods.
second due to the limitations of refactoringminer duplicating one file to two similar files and merging two files into a single file cannot be detected.
for example in apache camel 9da3f5af the web3jconsumerintegrationtest is duplicated to web3jconsumertransactionstest andweb3jconsumerlogtest .
however refactoringminer only reports web3jconsumerintegrationtest is renamed to web3jconsumertransactionstest .
thus we detect web3jconsumerlogtest as a newly added class.
in apache flink 8d3a74f9 statefuljobsavepointfrom12migrationitcase and statefuljobsavepointfrom13migrationitcase are merged to statefuljobsavepointmigrationitcase but refactoringminer only reports statefuljobsavepointfrom12migrationitcase is renamed to statefuljobsavepointmigrationitcase .
thus we detect statefuljobsavepointfrom13migrationitcase as a deleted class.
lastly refactoringminer does not work for commented out tests so renaming a commented out test will be detected as deleting a commented out test and adding a new commented out test.
.
studied systems table shows an overview of the studied systems.
to obtain highquality repositories to make our results more reliable we select the studied systems by following three selection criteria.
first we selected the top java systems on github ordered by popularity i.e.
stargazer count .
we also ensured that the repositories are not forks as they may not be part of the main branch and not actively maintained.
although it would be interesting to study disabled tests from other branches that consist of feature additions or bug fixing activities as they may involve more frequent usages of test disabling our research only considers disabled tests that are merged into the main branch as they may indicate more challenging maintainability tasks that could not be resolved at the time.how disabled tests manifest in test maintainability challenges?
esec fse august athens greece table an overview of the studied systems from to .
test loc source loc systems total commits camel 355k 533k 289k 607m cassandra 28k 78k 177k 216k cloudstack 51k 80k 1m 519k druid 22k 147k 64k 242k flink 82k 371k 105k 407k hadoop 349k 660k 463k 810k hbase 168k 287k 433k 409k hive 104k 269k 524k 1m ignite 162k 500k 253k 577k incubator pinot .6k 75k 11k 196k kafka 2k 133k 11k 132k maven 14k 17k 44k 48k openfire .2k .2k 179k 94k orientdb 74k 188k 140k 360k storm .7k 35k 61k 240k total .4m .3m .8m .9m 122k second we discarded the systems that are below the 90th percent quantile in terms of size i.e.
lines of code repository popularity i.e.
stars and the number of commits collectively.
namely we only study repositories that fall inside the top in all of the mentioned criteria.
finally we discarded inactive repositories that did not have any commits in .
we ended up with systems i.e.
camel cassandra cloudstack druid flink hadoop hbase hive ignite incubator pinot kafka maven openfire orientdb storm.
we analyze the code changes in these systems from january to january .
these studied systems cover different domains ranging from distributed databases stream processing frameworks message brokers and group chat servers.
a quantitative study on the test disabling practice in this section we quantitatively analyze the prevalence of the test disabling practice.
we also study the time it takes for developers to re enable a test and the evolution patterns of disabled tests.
rq1 how common are test disabling changes?
motivation.
many prior studies focus on studying maintenance challenges caused by technical debt .
however there is less empirical evidence on the technical debt in the test code thus far especially on technical debt related to disabling practices.
developers may disable tests as code evolves which may cause future maintenance challenges.
as a stepping stone to understanding test maintenance challenges in this rq we study the frequency of test disabling practices.
approach.
we study how frequently developers disable a test at both class and method levels.
disabling tests at the class level would prevent executing all test cases within the class whereas disabling at the method level would stop executing a single test case e.g.
a method with an test annotation .
to provide a comparative statistic we show the prevalence of test disabling changes i.e.
disable re enable delete along with common test code transformations at the same program element level by following prior studies .
specifically we compare test disabling changes at method level with rename method rename parameter and changetable frequency comparison between test disabling changes and the common test code refactorings at the same program element level.
method level class level total total total per commit total per commit changes commits test disabling changes .
.
refactoring .
.
percentage .
table the frequency of various types of test disablingrelated changes.
disabling tests shows the total number of changes that disable tests.
re enabled tests shows the total number of changes that re enable tests and whether developers simultaneously modified the tests i.e.
modified vs. unmodified .
deleting disabled tests shows the number of changes that delete disabled tests.
disabling re enabling tests deleting tests modified unmodified disabled tests method class total .
.
.
.
parameter type and at class level with rename class.
we use a tool called refactoringminer implemented by tsantalis et al .
to detect rename and type changes.
tsantalis et al .
reported that refactoringminer could detect refactoring activities with an average precision of over and recall of over .
despite differences in the two practices such comparison is reasonable because these common code refactorings occur at the same program level.
results.
test disabling changes is prevalent during test maintenance and has a similar change frequency compared to test refactorings.
table compares the prevalence of the disabling changes with that of the refactoring changes in test code.
as shown in table at the method level the number of test disabling changes is comparable to the number of test refactorings i.e.
.
difference and at the class level the test disabling changes are performed more frequently than the rename class refactorings i.e.
differences .
we also observe that the total test disabling changing commits are less than the total test refactoring commits.
despite this at both method and class level the average test disabling changes per commit are higher than that of test refactorings i.e.
difference .
based on these results we find that test disabling changes are prevalent in practice and are comparable to traditional refactorings.
table presents the frequency of three types of test disablingrelated changes disabling a test re enabling a test or deleting a disabled test.
we find that .
i.e.
of the disablingrelated changes disable the test which is the most frequent change among all the change types.
.
i.e.
of the disablingrelated changes re enable the test.
moreover a non trivial percentage i.e.
of the re enabling changes modify the test.
in other words many of the disabled tests remain the same when they are re enabled.
we also find that a significant number .
of the test disabling changes delete disabled tests.
in the next rq we further study the destination of each disabled test and its change pattern.esec fse august athens greece dong jae kim bo yang jinqiu yang and tse hsun peter chen table change patterns of disabled tests.
unresolved tests represent the tests that remain disabled at the end of the studied period.
resolved tests represent the tests the are either re enabled or deleted completely at the end of the studied period.
change pattern frequency change patterns for unresolved tests disabled disabled re enabled disabled disabled deleted disabled total change patterns for resolved tests disabled re enabled disabled deleted disabled re enabled disabled re enabled disabled re enabled disabled deleted disabled re enabled disabled re enabled 1disabled re enabled total developers frequently disable tests in software development and the frequency of such practice is comparable to common refactorings at the same program element level.
we also find that many disabled tests may be re enabled without any changes or may be deleted directly.
rq2 what is the change pattern of disabled tests?
motivation.
as shown in rq1 test disabling practice has a nonnegligible presence during test maintenance and evolution.
in this rq we further study the evolution patterns of disabled tests their destination e.g.
finally re enabled or stay disabled and how long a test remains disabled.
studying the evolution of disabled tests may quantitatively show the process of how developers maintain disabled tests whether the corresponding issues are fixed immediately or persist for a long time and whether the corresponding issues are improperly fixed and cause the tests to become disabled again in the future.
approach.
our goal is to study the life cycle of every disabled test.
in rq1 we analyze the test disabling changes by studying their frequency.
however bugs that are hard to fix may cause multiple rounds of changes to the test code.
hence we carefully track the evolution history by utilizing the full commit level history of the disabled test to report their evolution pattern and final destination.
to study the change pattern more accurately we trace refactoring activities e.g.
rename performed on disabled tests using refactoringminer .
additionally we study the longevity of disabled tests by mining the number of days between the changes that disable the tests and the changes that either re enable or delete the disabled tests.
results.
many disabled tests remain disabled in the studied period.
for the resolved disabled tests were deleted in the codebase.
table shows the evolution patterns of disabled tests which highlights how disabled tests evolve ever since they were born.
note that since a test may undergo multiple rounds of changes in the studied period the total frequency of evolution patterns should be lower than rq1 where we report the total rawtable the distributions of the average time in days for a disabled test to become re enabled deleted or remain disabled i.e.
developers did not modify the test in the studied period after it was re enabled .
time in days min.
max.
mean re enabled .
.
.
.
.
.
deleted .
.
.
.
.
.
remain disabled .
.
.
.
.
.
frequency.
we also categorize the statistics into unresolved and resolved cases to indicate the final destination of disabled tests.
unresolved tests refer to the tests that stay disabled and resolved tests refer to the ones either being re enabled or deleted.
we find that most disabled tests stay unresolved as the destination.
for the resolved cases become deleted and become re enabled.
there are cases where developers tried to resolve a disabled test but eventually changed them back to being disabled.
after some investigation we find that these tests are disabled again due to three main reasons.
first developers revert the commit due to a mistake.
second developers re disable the test code later when the test fails again.
third developers re disable the test code whenever there is a version update of one external dependency.
there are also cases where developers tried to resolve the ignored test but eventually deleted them.
we observe that these tests were disabled for a long time and may have become obsolete as developers suggest deleting the tests instead of updating them.
table shows the distributions of the average time in days it takes for a disabled test to become re enabled or deleted i.e.
resolved .
we also show similar statistics for the tests that stay disabled i.e.
unresolved .
we observe that the median time for developers to re enable a test is days.
however it often takes over three months median is days for developers to delete a disabled test.
in general there is a higher possibility that a disabled test may become deleted if it has been disabled for a more extended period.
one likely explanation is that many of the disabled tests may have become obsolete.
we notice similar patterns of obsoleteness across all types of disabled tests.
finally for the tests that remain disabled most of them have been disabled for several years.
it is likely that these disabled tests are forgotten by developers and remain in the codebase.
in rq3 we manually study the reasons for the tests to be disabled and re enabled.
overall it takes a longer time for the disabled tests to be deleted median time is three months than to be re enabled median time is days .
many tests that remain disabled have been disabled for years.
a qualitative study on disabled tests rq3 why do developers utilize test disabling practice?
motivation.
as previous rqs reveal disabling tests is a ubiquitous practice during software evolution.
the test disabling practice is ahow disabled tests manifest in test maintainability challenges?
esec fse august athens greece double edged sword.
on the one hand it provides developers with convenience in bypassing test related issues.
on the other hand it may be used to bypass some maintenance difficulties which can result in a silent and long waiting period for the tests to be reenabled if ever.
test disabling mechanism may hinder software reliability as the disabled tests may remain disabled indefinitely in codebases.
in particular there is a lack of tools to manage the life cycle of disabled tests and assist developers in proactively reenabling the temporarily disabled tests.
in this rq we perform a qualitative study to understand why developers utilize test disabling practice i.e.
the scenarios that developers utilize such convenience of disabled tests.
categorizing the scenarios will reveal the common challenges developers may face in maintaining disabled tests and test maintenance in general.
obtaining such understanding on disabling tests will inspire future tools that can better manage disabled tests for improving quality assurance activities.
approach.
to understand the motivations of utilizing test disabling practice we analyzed and categorized a statistically significant sample of disabled test instances.
we combined the disabled tests from all the studied systems and adopted the stratified sampling technique to sample each studied system independently for producing a statistically significant sample .
we also found incorrectly detected instances by our tool i.e.
a .
false positive rate and excluded them in table .
our manual study involves two phases phase i. the first two authors of the paper a1 and a2 independently derived an initial categorization by manually inspecting the relevant software artifacts such as commit messages test code comments surrounding the test code and bug reports if available.
additionally we use git log to check out other relevant commits on the same set of modified source code files to gain supplementary insights if the current commit lacks sufficient information.
phase ii.
a1 and a2 unified the derived reasons and compared the assigned reason for each evolution pattern.
any disagreement was discussed until reaching a consensus.
the inter rater agreement of the coding process has a cohen s kappa of .
indicating a substantial level of agreement .
to encourage the replication of our results we have made the dataset available1.
results.
table shows our manually derived taxonomy of the reasons developers disable the tests.
below we discuss each category in detail.
hiding test failure .
developers frequently disable some tests when test failures occur.
the most common cause is that bugs are introduced during software maintenance.
while working on fixing the bugs developers may temporarily disable the failing test cases especially when the bugs require non trivial effort and time to fix.
however only of the disabled tests are reenabled after the relevant bugs are fixed.
in this case we also notice that developers do pay a certain effort to provide some traceability of the disabled tests such as creating bug reports on disabled test cases as a reminder to re enable such tests.
in contrast some issues are difficult to fix and test failures may have persisted for a nontrivial time.
developers adopt test disable to remove test failures explicitly without working on fixing the bugs.
of the tests were disabled due to failure and were never re enabled in our studied .
for such cases test disabling practice is used by developers as a convenient way of bypassing test failures while keeping the test code in the repository.
however we found that there is no traceability provided for developers to track these disabled tests.
we could not find any mention of the disabled tests in jira issues or in documents.
these disabled tests may remain forgotten with the issues remain unfixed.
another main motivation is to avoid test failures caused by flaky tests.
as flaky test results are nondeterministic diagnosis can be challenging.
after developers fix the issues i.e.
flaky tests no longer fail they may re enable the tests.
in our studied cases only flaky tests are re enabled later aligning with the study by who reported that over half of flaky tests remain unfixed.
we also uncovered two test failures due to slow tests for which developers simply disable the slow tests without either fixing the test or the source code.
finally one failure is caused by library incompatibility in travis ci.
in openfire ddb20ffe developers discuss that parameterized is not supported by the ant build system installed in travis ci causing failure in tests where parameterized is used.
developers disable the affected tests as a convenient solution while waiting for the incompatibility to be resolved by framework developers.
in total only of the tests are re enabled after the bug fix.
the remaining disabled tests are either disabled indefinitely i.e.
due to lack of solution or deleted.
we also find that for such permanently disabled tests there is often a lack of traceability in github or jira issues.
precautions during feature maintenance .
developers commonly utilize test disable practice to avoid potential test failures during maintenance activities such as adding new features and refactoring.
for cases we find that developers precariously disable the test cases that may fail in the process of feature implementation due to incomplete functionalities e.g.
the feature takes more than one commit to finish .
for example in flink df448625 developers commented out the tests related to retrieving log files and left a comment saying that todo activate this test after logging retrieval has been added to the new web frontend .
we also notice that developers may indicate relevant bug issue ids when disabling test cases.
for example in hadoop 18fe65d75 developers left a comment as requires hdds .
requires once feature is in place .
although we find that developers may try to add traceability on disabled tests the current practice remains ad hoc i.e.
by using only comments in the code .
developers may use disabled tests to harbor some temporary tests and delete such tests after the feature is complete i.e.
being replaced by an official test e.g.
orientdb 0fc9bee1 .nevertheless among the cases we examined in this category only tests are re enabled and the others remain disabled in the studied systems .
in addition to new features we find that developers may disable the tests of deprecating features instead of removing them .
developers may choose to disable the tests temporarily while replacing deprecated features and adopt the disabled tests once the new implementation is finished.
however we observe there are cases where the tests for the deprecated features remain disabledesec fse august athens greece dong jae kim bo yang jinqiu yang and tse hsun peter chen table qualitative analysis result a taxonomy of why developers disable tests.
categories motivation frequency hiding test failures .
disabling tests while working on bug fixes developers temporarily disable failing tests while working on fixing the bugs.
flaky test developers may disable flaky tests to avoid occasional failures.
won t fix bugs in code test developers disable tests to avoid failures as they will not work on fixing the failures due to difficulty.
slow test test failures due to long running time so developers disable such tests to avoid failures.
library incompatibility junit annotation parameterized was not supported by ant in the used ci platform.
thus the test was disabled.
precautions during feature maintenance .
new improved features in the process of implementing new or improving existing features developers may temporarily disable relevant tests or introduce new tests that are disabled e.g.
commented out tests to avoid potential failures.
deprecation developers may disable relevant tests in the process of deprecating features.
refactoring developers may disable tests during refactoring.
diverting to manual testing .
require manual input developers need to manually run tests that need to be manually configured e.g.
database setup and secret keys .
expensive test developers need to manually run tests that are expensive to run and may not have to be run all the time e.g.
performance and migration test .
experimentation developers manually run tests that are under experimentation.
dependency issue .
difficulties in maintaining external dependencies external dependency is hard to maintain due to various reasons.
developers may need to disable tests when there are bugs in the external dependency unexpected version changes or dependencies are hard to integrate or use.
waiting for functionality update in external dependency developers disable tests while waiting for feature improvement in the external dependencies.
test design issues .
selective test inheritance developers disable tests to disable unneeded inherited tests while selectively reusing some inherited tests.
redundant test developers disable tests that are redundant and covered by a different test class.
other reasons .
unknown lacks of explicit mention of why tests are disabled e.g.
no relevant jira issues and comments .
obsoleteness developers disable obsolete tests.
disable by mistake developers accidentally disable the test.
in the codebase.
lastly developers sometimes disable tests during refactoring .
for example in openfire 97f7cf3f and in truststoreconfigtest.java developers commented out the entire class prior to extracting openfirex509extendedtrustmanagertest and removing few code duplications using a helper class keystoretestutils.java .
developers may temporarily disable tests during feature maintenance or refactoring.
however we find that of such disabled tests remain disabled.
diverting to manual testing .
.
developers may disable some tests with the plan to manually running them.
for the studied cases in this category test reconfigurability was the most common problem causing developers to disable the test .
for example we find that some tests are disabled for manual testing due to the need to manually configure the access key and secure key e.g.
camel ba22a8175f94a or setup databases e.g.
cloudstack 96c38bf4 .
such cases require manual testing as the tests depend on resources that must be manually started or configured priorto the test execution.
another of the tests in this category are disabled because they are expensive to run e.g.
migration or performance tests .
for example as discussed in flink b7ae3e5338 manualwindowspeeditcase when a release we should manually run theses tests on the version that is to be released and on an older version to see if there are performance regressions.
however it is difficult to know if developers remember to manually run these tests before each release and disabling these tests may result in higher maintenance costs e.g.
only discovering issues before the release .
finally we find one rare case in camel bd1661b248 javasockettests where the developer introduces a commented out test code as it is part of experimental code.
developers manually test expensive resources that should only be executed under particular circumstances such as migration or performance testing.
dependency issue .
.
without good mocking strategies we find that external dependencies may become hard to maintain and a source of technical debt causing tests to fail andhow disabled tests manifest in test maintainability challenges?
esec fse august athens greece be disabled.
for example in camel 35b83b1d we find that bugs in the external dependencies cause test failures i.e.
upgrade smack due to bug in smack .
.
.
in another case in camel 40ae73c4 due to unexpected version changes in external dependencies the test fails and is disabled i.e.
it looks like the problem is embedded xmpp server.
it was overridden to .
currently .
in total only cases were re enabled after the issue was fixed.
interestingly as shown in ignite the test continues to be disabled even if the bug report is reported closed with a wont t fix resolution due to difficulties in test maintenance.
the other tests remain disabled due to similar reasons.
developers may also disable tests while waiting for new features to be added in external dependencies.
for example in druid 4b3bd8bd developers comment out several tests as they need to wait for an external dependency i.e.
joda to release a new feature.
however we find that only cases become re enabled later.
developers may disable tests due to issues updates in external dependencies.
however of the disabled tests in this category remain disabled or are deleted due to reasons such as test maintenance difficulties.
test design issues .
developers may disable tests when changing improving test design.
we find that developers may leverage inheritance to reuse part of the tests in parent classes.
however developers may disable some of the inherited yet unneeded tests in child classes.
a reverse may also happen where developers reenabled the disabled tests in the child classes ignite 63b9e1653d .
our finding shows that there may be maintenance or designing challenges when handling test inheritance.
therefore developers need to decide whether an inherited test should be executed or not.
additionally there are cases where developers find the same tests that exist in multiple test classes and decide to disable the redundant tests.
developers may disable tests to bypass test design limitations related to test inheritance.
other reasons .
.
we categorize the remaining disabled tests that do not belong to any of the above mentioned categories as other reasons .
we find that there are cases of disabled tests where we cannot find any discussion comment.
we categorize these cases as unknown.
for example developers may only include a commit message such as disable test without any other explanation or reference to other software artifacts e.g.
jira where only out of were eventually re enabled.
we find cases where developers disable the tests because they become obsolete.
finally there are cases where developers disable the test accidentally e.g.
commented out the test and thus were re enabled immediately afterward.
developers did not provide the reasons nor traceability e.g.
jira issues for many of the disabled tests.
most of such disabled tests remain disabled in the studied systems.
we also find cases where developers disabled obsolete tests and they may also disable some tests by mistake.
discussion and implication based on our empirical findings we present actionable implications and future work for two groups of audiences researchers and application developers and testers.
.
discussion and implication for researchers r1 developers use disabled tests to bypass test failures which may affect test maintenance and code quality.
future studies should investigate its impact on software quality.
as we find in rq2 tests may be hard to fix immediately and may remain disabled for an extended period.
some re enabled tests are again disabled later due to inadequacy of the bug fixes as bug fixing tasks are difficult.
moreover as found in rq3 developers may disable tests to temporarily hide test failures such as disabling flaky tests while the bugs remain unfixed.
although these tests are necessary for revealing faults they may no longer guard the software against regressions and uncover the possible presence of new bugs.
therefore future research must study the impact of disabled tests on software quality from two aspects.
first an interesting direction is to study the relationship between disabling tests and software defect proneness to provide additional insights into the impact of disabled tests on software quality.
second future studies may quantitatively assess the impact of disabled tests on fault detection capabilities using mutation analysis .
r2 there is a lack of automated support on tracking disabled tests which may lead to forgotten tests.
future studies may consider providing traceability support to developers.
during our manual study we notice that many disabled tests are not referenced in issue reports and are not tracked by any software artifact.
for example developers may commit test disabling changes with only mentioning in the commit message that a test is disabled.
in some cases we find that the bug may be resolved but the test is still disabled.
due to the lack of traceability and documentation most of these disabled tests may then be forgotten and stayed disabled for several years and are never re enabled.
future research may consider providing automated traceability to track the disabled tests and better assist developers during test evolution.
r3 developers may disable some tests and divert them to manual testing.
future studies should investigate approaches to provide better automation support.
as we find in rq3 of the tests are disabled because they are difficult to run automatically or are time consuming.
however delaying test execution may result in accumulated maintenance overhead e.g.
bugs are only revealed at the late stage of the development or before the release .
since these tests need to be manually executed it is also possible that developers may forget to run them.
future studies may also investigate better mocking approaches and adopt test reduction or prioritization to ensure these tests can still be included in part of the continuous integration process while maintaining acceptable test overhead.
r4 future studies should investigate the impact of test obsolescence and the co evolution between test and source code.
in our study we find that developers use test disabling for a wide range of maintainability tasks yet only a few were ever re enabled in practice.
many of the disabled tests are deleted as they becomeesec fse august athens greece dong jae kim bo yang jinqiu yang and tse hsun peter chen figure an example of an invalid block comment that cannot be detected by the tool.
invalid comment represents a mix between natural language and code.
original method commented using block comment test this is a demo public void demo test this is a demo public void demo obsolete.
as systems evolve some tests may become outdated and may need to be updated.
however it is not clear to which degree do developers maintain tests to keep up with the development and whether there are replacement tests for the tests that were deleted.
future studies on test obsolescence and their co evolution with source code may provide better support to developers on test maintenance.
.
discussion and implication for developers d1 assisting developers with best testing practices about how to maintain disabled tests.
as found in rq1 and rq2 test disabling is prevalent in test maintenance but most disabled tests remain disabled.
developers often disable tests due to a bug however they rarely open a new jira issue to track the process of re enabling the test code.
for example out of the samples that remain disabled rq3 only are related to unresolved issues and of the tests that remain disabled do not have any documentation on jira.
for the remaining of the disabled tests they remain disabled even after the issues were fixed.
as an example in hive 22df53b6 several tests remain disabled and forgotten even after their bug issues were closed e.g.
hive .
to better trace these disabled tests developers should consider adding jira issues or similar artifacts to document tests that require an update.
otherwise the non traceability of disabled test code may lead to worse software quality in the long term.
threats to validity in this section we discuss threats to validity of our study.
.
internal validity since commented code may exist in countless different formats it may be impossible to find a generalized rule to detect all the cases.
figure shows one such example where there is a mix between the commented out code and natural language.
therefore our tool may not be able to detect all the commented out methods.
nevertheless to minimize the threat we treat the consecutive line comments as a single target and detect each target as multiple commented out methods.
we apply the same rule when there could be blank lines between the different parts of a commented out method.
namely we allow our rules to be tolerant for a single line of code but we treat it as two different targets for a number higher than one.
despite this issue the precision of our tool for test status is high where in fact there are no false positives in terms of commented out tests.
our approach also relies on refactoringminer to detect and track refactoring changes.
as shown by tsantalis et al .
refactoringminer has high precision and recall which should not affect our results.
it is possible that some commented out code isonly meant as a template code for future implementation and could be non compilable.
our tool does not check for these instances and they may exist in our study and skew our understanding of the manual study.
moreover we only consider test annotation to determine the test method and test class.
it is also possible that files that use an older version of junit i.e.
junit3 may not use test annotation to indicate a test method or class.
our tool does not check for these instances and may miss some disabled tests.
finally we do not consider partially commented out tests such as commenting out assertions.
compared to disabling a test completely partially commented out tests may involve more complex change such as when tests contain multiple assertions in a test.
.
external validity our studied systems are all open source implemented in java so the result may not be generalized to all systems.
to minimize the threat we follow a set of criteria to select systems that are popular on github large in scale and actively maintained.
the studied systems cover various domains and are frequently used in commercial settings.
future studies are encouraged to replicate our experiment on other systems and systems implemented in different programming languages.
.
construct validity in rq3 we conduct a manual study on the reasons that the tests become disabled.
we conduct the study on a statistically significant sample using a confidence level and confidence interval.
to reduce the biases in our manual study result two of the authors independently studied the sample and compared the results.
any discrepancy is discussed until a consensus is reached.
we computed the cohen s kappa and found that the level of agreement is substantial between the two authors .
.
conclusion similar to source code there are bugs and maintenance challenges in test code.
as a result developers may bypass a test failure by disabling the test i.e.
adding ignore or comment out the test.
such tests disabling practices when misused may cause technical debt and harm the long term maintenance.
in this paper we conduct the very first empirical study on test disabling practice in java systems.
we first implement a tool to detect and track the changes of disabled tests in software development history.
then we conduct both quantitative and qualitative studies on test disabling changes.
we find that test disabling is a prevalent practice in test evolution and has a similar frequency level compared to test refactoring at the same program element level.
most disabled tests remain disabled and have been disabled for years.
many of the disabled tests are either re enabled without any code change or deleted directly.
our manual study highlights the reasons for the tests to be disabled.
we find that most tests are disabled due to maintenance challenges e.g.
flaky tests and required to do manual testing rather than waiting for bug fixes.
moreover most disabled tests remain disabled even after the bugs are fixed.
our discussions provide possible future research directions to further improve test maintenance and suggestions to developers to better track disabled tests so that they are not forgotten .how disabled tests manifest in test maintainability challenges?
esec fse august athens greece