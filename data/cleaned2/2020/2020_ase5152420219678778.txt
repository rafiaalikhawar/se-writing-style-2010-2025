deep gui black box gui input generation with deep learning faraz y azdanibanafshedaragh school of information and computer sciences university of california irvine usa faraz.yazdani uci.edusam malek school of information and computer sciences university of california irvine usa malek uci.edu abstract despite the proliferation of android testing tools google monkey has remained the de facto standard for practitioners.
the popularity of google monkey is largely due tothe fact that it is a black box testing tool making it widely applicable to all types of android apps regardless of theirunderlying implementation details.
an important drawback ofgoogle monkey however is the fact that it uses the most naiveform of test input generation technique i.e.
random testing.in this work we present deep gui an approach that aims to complement the benefits of black box testing with a moreintelligent form of gui input generation.
given only screenshotsof apps deep gui first employs deep learning to construct amodel of valid gui interactions.
it then uses this model togenerate effective inputs for an app under test without theneed to probe its implementation details.
moreover since thedata collection training and inference processes are performedindependent of the platform the model inferred by deep guihas application for testing apps in other platforms as well.we implemented a prototype of deep gui in a tool calledmonkey by extending google monkey and evaluated it for its ability to crawl android apps.
we found that monkey achievessignificant improvements over google monkey in cases where anapp s ui is complex requiring sophisticated inputs.
furthermore our experimental results demonstrate the model inferred usingdeep gui can be reused for effective gui input generation acrossplatforms without the need for retraining.
i. i ntroduction automatic input generation for android applications apps has been a hot topic for the past decade in the software engineering community .
input generators have avariety of applications.
among others they are used forverifying functional correctness e.g.
security e.g.
energy consumption e.g.
andaccessibility e.g.
of apps.
depending on the objectiveat hand input generators can be very generic and simply crawlapps to maximize coverage or can be very specific looking for certain criteria to be fulfilled such as reachingactivities with specific attributes .
common across the majority of existing input generators is the fact that they are white box i.e.
require access to implementation details of the app under test aut .f o r instance many tools use static analysis to find the rightcombination of interactions with the aut while othertools depend on the xml based gui layout of the aut tofind the gui widgets and interact with them .
theunderlying implementation details of an aut provide thesetools with insights to produce effective inputs but also posesevere limitations that compromise the applicability of thesetools.
first there is a substantial degree of heterogeneityin the implementation details of apps.
consider for instancethe fact that many android apps are non native e.g.
builtout of activities that are just wrappers for web content.
inthese situations the majority of existing tools either fail tooperate or achieve very poor results.
second the source codeanalyses underlying these tools are tightly coupled to theandroid platform and often to specific versions of it makingthem extremely fragile when used for testing apps in a newenvironment.
black box input generation tools do not suffer from the same shortcomings.
google monkey is the most widely usedblack box testing tool for android.
despite being a randominput generator prior studies suggest google monkey outper forms many of the existing white and gray box tools .this can be attributed to the fact that google monkey issignificantly more robust than almost all other existing tools i.e.
it works on all types of apps regardless of how theyare implemented.
however google monkey employs the mostbasic form of input generation strategy.
it blindly interacts withthe screen without knowing if its actions are valid.
this mightwork well in apps with a simple gui where the probabilityof randomly choosing a valid action is high but not in appswith a complex gui.
for instance take figure .
in figure1a since most of the screen contains buttons almost all of thetimes that google monkey decides to generate a touch action it touches something valid and therefore tests a functionality.however in figure 1b it is much less probable for googlemonkey to successfully touch the one button that exists on thescreen and therefore it takes much longer than needed for itto test the app s functionality.
this article presents deep gui a black box gui input generation technique with deep learning that aims to addressthe above mentioned shortcoming.
deep gui is able to filterout the parts of the screen that are irrelevant with respect toa specific action such as touch and therefore increases theprobability of correctly interacting with the aut.
for example given the screenshot shown in figure 1b deep gui firstproduces the heatmap in figure 1c which shows for eachpixel the probability of that pixel belonging to a touchablewidget.
it then uses this heatmap to touch the pixels with a 36th ieee acm international conference on automated software engineering ase 36th ieee acm international conference on automated software engineering ase .
ieee .
ase51524.
.
.
ieee a b c d e fig.
two examples where it is respectively easy a and difficult b for google monkey to find a valid action as well as the heatmaps generated by deep gui associated with b for touch c scroll d and swipe e actions respectively.
note thatin c the model correctly identifies both the button and the hyperlink and not the plain text as touchable.
probability that is proportionate to their heatmap value henceincreasing the chance of touching the button in this example.
in order to produce such heatmaps deep gui undertakes a deep learning approach.
we further show that this approachis a special case of a more general method known as deepreinforcement learning and we discuss how this method canbe used to develop even more intelligent input generationtools.
moreover what makes deep gui unique is that it usesa completely black box and cross platform method to collectdata learn from it and produce the mentioned heatmaps andhence supports all situations applications and platforms.
italso uses the power of transfer learning to make its training more data efficient and faster.
our experimental evaluationshows that deep gui is able to improve google monkey sperformance on apps with complex guis where googlemonkey struggles to find valid actions.
it also shows that wecan take a deep gui model that is trained on android anduse it on other platforms specifically web in our experiments for efficient input generation.
in summary this article makes the following contributions we propose deep gui a black box approach for generation of gui inputs using deep learning.
to the bestof our knowledge this is the first approach that usesa completely black box and cross platform approach fordata collection training and inference in the generationof test inputs.
we provide an implementation of deep gui for android called monkey by extending google monkey.
we make this tool available publicly.
we present detailed evaluation of deep gui using androtest benchmark consisting of real world mo bile apps as well as the top websites in the us .our results corroborate deep gui s ability to improveboth the code coverage and the speed with which thiscoverage can be attained.
the remainder of this paper is organized as follows.
section ii describes the details of our approach.
section iiiprovides our evaluation results.
section iv reviews the mostrelevant prior work.
finally in section v the paper concludeswith a discussion of our contributions limitations of our work and directions for future research.
906ii.
a pproach we formally provide our definition of the problem for automatically generating inputs in a test environment.
suppose that at each timestep t the environment provides us with its statest.
this can be as simple as the screenshot or can be a more complicated content such as the ui tree.
also supposewe define a ... n as the set of all possible actions that can be performed in the environment at all timesteps.for instance in figure 1b all of the touch events associatedwith all pixels on the screen can be included in a. note that these actions are not necessarily valid.
we define a validaction as an action that results in triggering a functionality like touching the send button or changing the ui state likescrolling down a list .
let us define r t r st at to be ifatis valid when performed on st and otherwise.
our goal is to come up with a function qthat given st produces the probability of validity for each possible action.
that is q s t at identifies how probable it is for atto be a valid action when performed on st. therefore qis essentially a binary classifier valid vs. non valid conditioned on stindependently for each action in the set a. for simplicity we also define q st as a function that given an action returnsq st .
that is q st q st .
in deep gui we consider stto be the screenshot of aut at each timestep.
set aconsists of touch up and down scroll and right and left swipe events on all of the pixels of thescreen.
we also define r tas follows r st at braceleftbigg if equals st st otherwise that is if the screenshot undergoes a legitimate change afteran action we consider that action to be a valid one in thatscreen.
we define what a legitimate change means later in thissection.
note that we defined s t a andrtindependent of the platform on which aut operates.
therefore this approach canbe used in almost all existing test environments.
this work consists of four components a. data collection this component helps in collecting nec essary data to learn from.
b. model at the core of this component is a deep neuralnetwork that processes s tand produces a heatmap q st for all possible actions at such as the ones shown in figure .
the neural network is initialized with weightslearned from large image classification tasks to providefaster training.
c. inference after training and at the inference time thereare multiple readout mechanisms available for using theproduced heatmaps and generating a single action.
thesemechanisms are used in a hybrid fashion to provide uswith the advantages of all of them.
d. monkey this is the only component that is specializedfor android and its application is to fairly compare deepgui with google monkey.
it also provides a convenientmedium to use deep gui for testing of android apps asit can replace google monkey and be used in practicallythe same way.
figure shows an overview of these four components andhow they interact.
a. data collection since we reduced the problem to a classification problem each datapoint in our dataset needs to be in the form of a three way tuple s t at rt where our model tries to classify the pair st at into one of the two values that rtrepresents i.e.
whether performing the action aton the state stis valid or not.
training a deep neural network requires a large amount ofdata for training.
to that end we have developed an automaticmethod to generate this dataset.
as defined above r trepresents whether the screen has a legitimate change after an action.
we here define legitimatechange as a change that does not involve an animated partof the screen.
in other words if specific parts of the screenchange even in case of no interaction with the app we filterthose parts out when computing r t. for instance in android when focused on a textbox a cursor keeps appearing and dis appearing every second.
we filter out the pixels correspondingto the cursor.
for data collection we first dedicate a set of apps to be crawled.
then for each app we randomly interact with theapp with the actions in the set aand record the screenshot the action and whether the action resulted in a legitimate change.in order to filter out animated parts of the screen before eachaction we first record the screen for seconds and considerall pixels that change during this period to be animated pixels.while this method does not fully filter all of the illegitimatechanges as our experimental results suggest it is adequate.
a keen observer would realize that this method of data collection is a very natural choice in the realm of android.
foryears google monkey has been used to crawl android appsfor different purposes but the valuable data that it produceshas never been leveraged to improve its effectiveness.
that is even if a particular app has already been crawled by googlemonkey thousands of times before when google monkey isused to crawl that app it still crawls randomly and makes allof the mistakes that it has already made thousands of timesbefore.
the collection method described here is an attempt to share these experiences by training a model and exploitingsuch model to improve the effectiveness of testing as wediscuss next.
b. model while as discussed above the problem is to classify the validity of a single action a twhen performed on st it does not mean that each datapoint st at rt cannot be informative about actions other than at.
for instance if touching a point results in a valid action touching its adjacent points may also result in a valid action with a high probability.
this can 2for instance if an accumulative progress bar is being shown this method may not work.
907fig.
overview of the components comprising deep gui monkey .
make our training process much faster and more data efficient.
therefore we need a model that can capture such logic.
input and output as the first step toward this goal in our model we define input and output as follows.
input is a3 channel image that represents s t the screenshot of the aut at timet.
for output we require our model to perform the classification task for all the actions of all types i.e.
touch scroll swipe etc.
and not just a t. while we do not directly use the prediction for other actions to generate gradients whentraining this enables us to use a more intuitive model and use the model at inference time by choosing the action thatis most confidently classified to be valid.
we use a t channel heatmap to represent our output tbeing the number of action types i.e.
touch scroll swipe.
note that we do not differentiatebetween up down scroll or left right swipe at this stage.
eachchannel is a heatmap for the action type it represents.
foreach action type the value at i j of the heatmap associated with that action type represents the probability that the modelassigns to the validity of performing that action type at location i j .
for instance in figure the three heatmaps 1c 1d and 1e show the model s confidence in performing touch scroll and swipe respectively at different locations on the screen.
unet we also would need a model that can intuitively relate the input and output as defined above.
we use a unet architecture since it has shown to be effective in applicationssuch as image segmentation where the output is an alteredversion of the input image .
in this architecture the inputimage is first processed in a sequence of convolutional layers known as the contracting path.
each of these layers reduces the dimensionality of the data while potentially encodingdifferent parts of the information relevant to the task at hand.the contracting path is followed by the expansive path where various pieces of information at different layers are combinedusing transposed convolutional layers 3to expand the dimensionality to the suitable format required by the problem.
in ourcase the output would be a channel heatmap.
in order forthis heatmap to produce values between and as explainedabove it is processed by a sigmoid function in the last step of the model.
as one can notice because of the nature ofconvolutional and transposed convolutional layers adjacentcoordinate pairs are processed more similarly than other pairs.
3in some