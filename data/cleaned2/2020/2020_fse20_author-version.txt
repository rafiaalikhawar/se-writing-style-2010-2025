zurich open repository and archive university of zurich university library strickhofstrasse ch zurich year dynamically reconfiguring software microbenchmarks reducing execution time without sacrificing result quality laaber christoph w rsten stefan gall harald c leitner philipp posted at the zurich open repository and archive university of zurich zora url conference or workshop item published version originally published at laaber christoph w rsten stefan gall harald c leitner philipp .
dynamically reconfiguring software microbenchmarks reducing execution time without sacrificing result quality.
in 28th acm joint european software engineering conference and symposium on the foundations of software engineering virtual event usa december december .
acm .
reconfiguring softwaremicrobenchmarks reducingexecution timewithout sacrificing resultquality christophlaaber universityof zurich zurich switzerland laaber ifi.uzh.chstefanw rsten universityof zurich zurich switzerland stefan.wuersten uzh.ch harald c. gall universityof zurich zurich switzerland gall ifi.uzh.chphilippleitner chalmers universityof gothenburg gothenburg sweden philipp.leitner chalmers.se abstract executingsoftwaremicrobenchmarks aformofsmall scaleperformance testspredominantly used forlibraries and frameworks is a costlyendeavor.fullbenchmarksuitestakeuptomultiplehours ordaystoexecute renderingfrequentchecks e.g.
aspartofcontinuous integration ci infeasible.
however altering benchmark configurationstoreduceexecutiontimewithoutconsideringthe impact on result quality can lead to benchmark results that are not representative ofthe software strue performance.
we propose the first technique to dynamically stop software microbenchmark executions when their results are sufficiently stable.
our approach implements three statistical stoppage criteria and iscapableofreducingjavamicrobenchmarkharness jmh suite executiontimesby .
to86.
.atthesametimeitretainsthe sameresultqualityfor .
to87.
ofthebenchmarks compared to executingthe suite for the defaultduration.
theproposedapproachdoesnotrequiredeveloperstomanually craftcustombenchmarkconfigurations instead itprovidesautomated mechanisms for dynamic reconfiguration.
hence making dynamic reconfiguration highly effective and efficient potentially pavingthe wayto inclusionofjmhmicrobenchmarks inci.
ccs concepts generalandreference measurement performance softwareanditsengineering softwareperformance software testinganddebugging .
keywords performance testing software benchmarking jmh configuration acmreference format christoph laaber stefan w rsten harald c. gall and philipp leitner.
.
dynamically reconfiguring software microbenchmarks reducing execution time without sacrificing result quality.
in proceedings of the 28th acm joint european software engineering conference and symposium esec fse november 8 13 virtual event usa copyright heldby the owner author s .
publicationrightslicensed to acm.
thisistheauthor sversionofthework.itispostedhereforyourpersonaluse.not for redistribution.
the definitive version of record was published in proceedings of the28thacmjointeuropeansoftwareengineeringconferenceandsymposiumonthe foundationsofsoftwareengineering esec fse november8 13 virtualevent usa .on the foundations of software engineering esec fse november 8 virtual event usa.
acm new york ny usa 13pages.https introduction performancetestingenablesautomatedassessmentofsoftwareperformanceinthehopeofcatchingdegradations suchasslowdowns inatimelymanner.avarietyoftechniquesexist spanningfrom system scale e.g.
load testing to method or statement level such assoftwaremicrobenchmarking.forfunctionaltesting cihasbeen arevelation where unit testsareregularlyexecutedtodetectfunctional regressions as early as possible .
however performance testingisnotyetstandardcipractice althoughtherewouldbea needforit .amajorreasonfornotrunningperformancetests on every commit is their long runtimes often consuming multiple hoursto days .
to lower the time spent in performance testing activities previousresearchappliedtechniquestoselectwhichcommitstotest or which tests to run to prioritize tests that are more likelytoexposeslowdowns andtostoploadtestsoncethey become repetitive or do not improve result accuracy .
however none of these approaches are tailored to and consider characteristicsofsoftwaremicrobenchmarks andenablerunning full benchmark suites reduce the overall runtime while still maintainingthe same result quality.
inthispaper wepresentthefirstapproachtodynamically i.e.
during execution decide when to stop the execution of software microbenchmarks.
our approach dynamicreconfiguration determines at different checkpoints whether a benchmark execution is stable and if more executions are unlikely to improve theresult accuracy.
it builds on the concepts introduced by he et al .
applies them to software microbenchmarks and generalizes the approach for any kindofstoppagecriteria.
to evaluate whether dynamic reconfiguration enables reducing executiontimewithoutsacrificingquality weperformanexperimental evaluation on ten java open source software oss projects withbenchmarksuitesizesbetween 16and995individualbenchmarks rangingfrom .31to191.81hours.ourempiricalevaluation comprises of three different stoppage criteria including the one from he et al .
.
it assesses whether benchmarks executed with dynamicreconfigurationincontrolled bare metalenvironmentsesec fse november8 13 virtualevent usa christophlaaber stefan w rsten harald c.gall andphilippleitner maintaintheirresultqualityand haveshorterexecutiontimes comparedto being executedwiththe defaultjmhconfiguration.
wefindthatforthemajorityofstudiedbenchmarkstheresult quality remains the same after applying our approach.
depending on the stoppage criteria between .
and87.
of the benchmarksdonotproducedifferentresults withanaverageperformance changeratebetween .
and3.
.eventhoughcomputationof the stoppage criteria introduces an overhead between and dynamicreconfigurationstillenables savingatotalof66.
to82 of the execution time across all projects.
for individual projects benchmark suites take .
to86.
less time to execute.
our empirical results support that dynamic reconfiguration of software microbenchmarks is highly effective and efficient in reducing executiontime withoutsacrificingresult quality.
contributions.
the main contributionsofour study are we present the first approach to dynamically stop the executionofsoftwaremicrobenchmarkusingthreedifferent stoppagecriteria.
we provideempiricalevidencethat demonstratestheeffectivenessandefficiencyofdynamicreconfigurationforten oss applications.
we provide a fork of jmh that implements our approach on github andas part ofour replication package .
toinvestigatewhetherreal worldbenchmarksuitescould benefitfromourapproachtosavetime wecollectthelargest datasetofjmhossprojects 753projectswith 387benchmarks includingextractedsourcecodepropertiessuchas benchmarkconfigurationsandparameters.
java microbenchmarkharness jmh jmh1isthede factostandardframeworkforwritingandexecuting softwaremicrobenchmarks inthefollowingsimplycalledbenchmarks forjava.benchmarksoperateonthesamelevelofgranularityasunittests i.e.
statement methodlevel andaresimilarly definedincodeandconfiguredthroughannotations.differentfrom unit tests where the outcome is binary i.e.
a test passes or fails disregarding flakiness benchmarks produce outputs for a certain performance metric such as execution time or throughput.
astheseperformancemetricsareeasilyaffectedbyconfounding factors suchasthecomputer shardwareandsoftware background process or even temperature one must execute benchmarks repeatedly to obtain rigorous results that are representative of the software strue performance .
figure1depicts a standard execution of a jmh benchmark suite b wherebenchmarks baresequentiallyscheduled.everybenchmarkexecutionstartswithanumberofwarmupforks wf tobring thesystemintoasteadystate whoseresultsarediscarded.aforkis jmhparlanceforrunningasetofmeasurementsinafreshjavavirtual machine jvm .
the warmup forks are followed bya number ofmeasurementforks f oftensimplycalledforks .duetodynamic compilation everyforkisbroughtintosteadystatebyrunninga seriesofwarmupiterations wi afterwhichaseriesofmeasurement iterations miare executed.
an iteration has a specific duration wtormtfor warmup time and measurement time respectively source codeexamples microbenchmark suite b1 fnf1 warmup wi1wi2...winmeasurement mi1mi2...min... wfn wf1...bn ... mt wtinvocation samplesoccurrences performance metricbbenchmark wf warmup fork wiwarmup iteration ffork mi measurement iteration mt measurement timewt warmup time figure jmhexecution for which the benchmark isexecuted as often as possible and the performance metrics for a sample of the invocations is reported.
performance metrics from warmup iterations are discarded and theunionofthemeasurementiterationsacrossallforksformthe benchmark sresult.allthesevaluescanbeconfiguredbythedeveloperthroughjmhannotationsorthecommandlineinterface cli otherwisedefaultvaluesare used.
jmhsupportsbenchmarkfixtures i.e.
setupandteardownmethods aswellasparameterizationofbenchmarks.aparameterized benchmark has a number of parameters with potentially multiple values jmh then runs the benchmark once for every parameter combination which are formed as the cartesian product of the individualparameters.
jmh usesdifferent setsofdefaultconfiguration values depending on the version .20and .
.
versions until .20use10 forks f running 40iterations 20wiandmieach withaniteration time wtandmt of1s starting from .
defaults are 5forks f 5iterations both wiandmi and10siterationtime both wtand mt .
jmhdoes not use warmupforks wf bydefault.
consequently and as fig.
1depicts we can define the overall warmuptimeas tbw wf wi wt mi mt f wi wt the overall measurement time as tbm f mi mt and the benchmark executiontimeas tb tbw tbm tb fix wheretb fixisthetimespent inbenchmarkfixtures.finally the full microbenchmarksuite executiontime tisthesumofallbenchmarkparametercombinations defined as t summationtext b b summationtext p pbtbp wherepbthe set of parameter combinations for a benchmark b. these definitions will be used in the remainderofthe paper.
pre study tomotivateourwork weconductapre studyinvestigatingwhether benchmarkexecutiontimesareinfactaprobleminreal worldoss projectsusing jmh.
.
data collection we create to the best of our knowledge the most extensive oss jmh data set to date from github by querying and combiningdynamicallyreconfiguringsoftware microbenchmarks esec fse november8 13 virtualevent usa three sources google bigquery s most recent github snapshot2 queried for org.openjdk.jmh import statements github ssearchapplicationprogramminginterface api with anapproachasoutlinedbystefanetal .
and mavencentral searchingfor projectswithjmhas dependency.our finaldataset consists of 753pre study subjects after removing duplicate entries repositories that do not exist anymore projects without benchmarksinthe mostrecent commit andforkedprojects.
for each project we apply the tool bencher to construct abstractsyntaxtrees asts forjavasource codefilesandextract informationrelatedto executionconfiguration fork warmup measurement and benchmarkmode and benchmark parameterization param .
in addition we extract the jmh version from the buildscript mavenandgradle .
.
summaryofpre study subjects the753projectshaveintotal 387benchmarkmethodsand parametercombinations.
.
projectsfeaturefewerthan benchmarks and .
projectscontain 50benchmarksormore.
on average a project has .
.9benchmarks with a median of .
the project with the largest benchmark suite is eclipse eclipsecollections with515benchmarks.
benchmarkparameterizationisquitecommonwithprojectshaving on average .
.3parameterized benchmarks with a medianof9.
.
ofthebenchmarkshave 10parametercombinations or fewer.
we find the largest number of parameter combinations in the project msteindorfer criterion with4 132combinations and the project withthe the most parameter combinations fora single benchmark is apache hive which contains an individual benchmark3withan astounding different combinations.
however themajorityofthebenchmarksarenotparameterized i.e.
.
.
extracting the jmh version is crucial for our analysis as the defaultvaluesoftheexecutionconfigurationshavebeenchanged with version .
see also section .
however automatically extracting the jmh version is not possible for each project.
we are able to successfully extract the jmh version from build scripts for573 of our pre study subjects containing .
benchmarks.
about of the projects containing .
benchmarks already use the mostrecent jmhversion.
.
results we use this data to analyze how much time benchmark suites in the wild take to execute.
figure 2adepicts a summary of benchmark suite execution times tfor the573studied projects where jmh version extraction was successful.
the runtimes vary greatly ranging from 143millisecondsfor protobufel protobuf el tonoless than7.4years for kiegroup kie benchmarks clearly this project does not intend to execute all benchmarks at once with a median of26.7minutes.
benchmarksuitesrunforanhourorless whichisprobablyacceptable evenincienvironments.however suites take longer than 3hours with 22projects dataset d github extracts p fh bigquery 3vectorgroupbyoperatorbench.testaggcount execution time projects cum a benchmark suite execution timest decrease factor log benchmarks cum b decreased tbcompared to jmh defaults figure impact of custom configurations on the execution timesof a benchmarksuites and b benchmarks.
exceeding 12hoursruntime.forexample thepopularcollectionslibraryeclipse eclipse collections hasatotalbenchmarksuiteruntime of over16days executing 515benchmarks with 575parameter combinations.weconcludethatatleast ofthepre studysubjects would greatly benefit from an approach to reduce benchmark executiontimes given their currentconfiguration.
thebenchmarksuiteexecutiontimeisbasedontheextracted jmh configurations from the projects.
we speculate that developers specifically apply custom configurations to reduce the defaultsettingsof jmh.indeed benchmarkshaveaconfiguration change that affects its runtime of which have a decreased benchmark time tbwith respect to jmh defaults seefig.
2b .weobservethatforthemajorityofthebenchmarks the execution time is in fact drastically reduced for and2 by a factor and respectively.
still benchmarks are reduced by a factor .
while only a minority of of the benchmarks belonging to just .
ofprojectsareconfiguredtoincreaseexecutiontimecomparedto the defaults.
pre study summary.
oss developers extensively customize benchmark configurations often setting their values considerably lower than the jmh default.
despite these changes of the projects still have a benchmark suite execution time of over hours.thesefindingsindicatethatdevelopersofmanyprojects could be supported by a data driven way to reduce the execution time ofjmh benchmarks.
dynamicreconfiguration in section we found that real world oss benchmark suites often are configured to considerably reduce runtime with respect to jmh sdefaults stillmanyrunformultiplehours makingiteffectively impossible toassessperformance on every software change.
we hypothesize that this time reductionis an effort by developers tokeepbenchmarksuiteruntimesreasonablewithoutconfirming that benchmarkresults remainstable accurate .
this section introduces an approach to dynamically stop benchmarkswhentheirresultisstable withthegoalofsavingexecution time withoutsacrificingquality.esec fse november8 13 virtualevent usa christophlaaber stefan w rsten harald c.gall andphilippleitner i1i2i3i4i5i6i7i8i9i10i11i12i13i14i15i16i17i18i19i20 f1 f2 f3 f4 f5 a staticconfiguration i1i2i3i4i5i6i7i8i9i10i11i12i13i14i15i16i17i18i19i20 f1 f2 f3 f4 f5 t b dynamic reconfiguration figure standard jmh execution with static configuration vs. dynamic reconfiguration approach.
a yellow box is a warmup iteration a blue box is a measurement iteration and a dashed box is a skipped iteration.
a solid line indicates that the stoppage criterion is met and a dashed line indicates theopposite.
.
approach jmh allows developers to define benchmark configurations before execution either through annotations or cli parameters and then executesallbenchmarksaccordingtothisconfiguration seesection2 .wecall thisthe staticconfiguration ofa benchmarkexecution.figure 3ashowsthestaticconfigurationwhereeveryrow indicatesajmhfork f 1 f5 andeverycolumn rectangleaniteration i1 i20 of the corresponding fork.
yellow rectangles i 1 i10 indicatewarmupiterations andbluerectangles i 11 i20 indicate measurementiterations.thisstaticconfigurationbearstheproblem thatallforksareexecutedwiththesameconfiguration irrespective oftheaccuracyoftheresults potentiallywastingpreciousruntime.
inordertostopbenchmarkexecutionswhentheirresultisaccurateenough weproposedynamicbenchmarkreconfiguration i.e.
an approachthatdynamicallydecides at certain checkpoints when the benchmark results are unlikely to change with more executions.
this happens at two points within a fork when the execution reaches a steady state i.e.
the warmup phase was executedlongenough and afterafork sexecution whenitis unlikely that more forks will lead to different results.
figure 3b illustrates dynamic reconfiguration.
vertical bars indicate checkpoints after iterations line horizontal bars indicate checkpoints afterforks line andwhite dashedboxesrepresentiterations that are skipped.
algorithm 1depicts the pseudo code for our dynamic reconfiguration approach.
the algorithm takes the benchmark bto execute itsextendedjmhexecutionconfiguration cb astabilityfunction stablethat is executed at every checkpoint and a threshold tfor decidingwhatisconsideredstable.
cbisatupleofconfigurationvalues defined as cb wimin wimax mi fmin fmax wf wt mt seealsosection .notethatcheckpointsonlyhappenafteri 5andalgorithm1 dynamicreconfigurationalgorithm input b b the benchmarkto execute cb wimin wimax mi fmin fmax execution configurationfor b stable m t mapsto true false stability function at thresholdt tfor a set of measurements m m t stability threshold specificfor stable data execute b mapsto m executes a benchmarkiteration result measurements mbof the benchmark b 1begin 2mb 3forf 1tofmaxdo mw dynamic warmup phase forwi 1towimaxdo mw mw execute b cb warmup stoppage ifwi wimin stable mw t thenbreak measurement phase for1tomido mb mb execute b cb fork stoppage iff fmin stable mb t thenbreak 11returnmb f2in the example defined as wiminandfmin.
if a benchmark is notstableatacheckpoint thebarisdashed solidotherwise and the warmupphasecontinues oranotherforkisspawned.
tocircumventthesituationwhereabenchmark swarmupphase never reaches a steady state or the overall measurements are never accurateenough ourapproachtakesamaximumnumberofwarmup iterations wimax and forks fmax e.g.
f3has a dashed bar after thelastwarmupiteration.thisguaranteesthatasinglebenchmark executionneverexceedsaconfigurabletimebudget whichdefaults to jmh swarmupiterations wi andforks f .
benchmarks often exhibit multiple steady states resulting in multi modaldistributions andoutliers duetonon deterministic behavior might still occur even after stableconsidered a fork to be in a steady state .
therefore our approach uses a fixed number of multiple measurement iterations mi lines8 9 as a single measurement iteration would not accurately represent a fork sperformance.
.
stoppagecriteria todecidewhetheraforkreachedasteadystate line orthegatheredmeasurementsarestable line ourapproachneedstodecide whether more measurements providesignificantlymore accurate results.
for this we rely on statistical procedures on the performance measurement distributions.
thatis if more measurements i.e.
data points are unlikely to change the result distribution we consider the measurement stable.
there are three key aspects to consider a stability criteria sc m mapsto r that assigns a stability values rto a set of measurements m m a threshold t tthat indicates whether a stability value sis considered stable and a stability function stable m t mapsto true false that basedonasetofstabilityvalues extractedfromasetof measurementsm m and a threshold t t decides whether a set of performance measurements isstable ornot.dynamicallyreconfiguringsoftware microbenchmarks esec fse november8 13 virtualevent usa .
.
stopping warmup phase.
the first stoppage point line decides whether a fork is in a steady state which indicates the end ofthewarmupphaseand thestartofthemeasurementphase.for this the dynamic reconfiguration approach uses a sliding window technique where the measurement distributions at the last iterationsarecomparedalongastabilitycriteria.letusconsidertheset of warmup measurements mw across multiple warmup iterations suchthat mi mwisthemeasurementatthe ithiteration.wethen definethesliding windowwarmupvector wi afteracurrentiterationi a sliding window size sw and the resulting start iteration ofthe window i i sw i 1ineq.
.
wi angbracketleftbig sc parenleftbigx uniondisplay i i mi parenrightbig barex barex barexi x i angbracketrightbig .
.
stopping forks.
the second stoppage point line decides whetherthebenchmarkmeasurementresults mbaresufficiently stable andnoadditionalforkneedsto be spawned therefore stopping the execution of benchmark b. let us consider the set of measurements mb across multiple forks such that mb f mbis the subset of measurements at fork number f. we then define the fork vectorff after acurrentfork f ineq.
.
ff angbracketleftbig sc parenleftbigx uniondisplay f 1mb f parenrightbig barex barex barex1 x f angbracketrightbig .
.
stabilitycriteriaandfunction.
thedynamicreconfiguration approach allows for different stability criteria sc and functions stable andwe identifiedandevaluatedthree coefficientofvariation cv coefficient of variation cv is a measure of variability under the assumption that the distributionisnormal.however performancedistributionsareusually non normal e.g.
multi modal or long tailed .
as a stability criteria sc cv might still be a good enough proxy to estimateabenchmark sstability especiallyduetoitslowcomputational overhead.
depending on the benchmark the stability values in the vector v wi ff converge towards different values makinga globalthreshold tforallbenchmarksunrealistic.
instead we compare all stability values from vsuch that the delta between the largest and the smallest is at most t. formally stablevar m t true max v min v t. relative confidenceinterval width rciw thesecondstability criteria sc relative confidence interval width rciw is similar to cv as it estimates a benchmark s variability hence stablevaralsoapplieshere.differentfromcv weemployatechnique based on monte carlo simulation called bootstrap toestimatetherciwforthemean.forthis weutilizethetool pa thatimplementsatechniquebykaliberaandjones .
it uses hierarchical random resampling with replacement which is tailored to performance evaluation.
the hierarchical levels are invocations iterations and forks we refer to pa andkalibera andjones for details .
kullback leiblerdivergence kld the third stability criteria scuses a technique outlined by he et al .
that constructs a probability that two distributions d1andd2are similar based on the kullback leibler divergence kld .sccomputes thisprobability foreveryelementofthevector v whered1isthemeasurement distribution excluding the last measurement warmup iterationiorforkf andd2isthemeasurementdistribution includingthelastmeasurement.consequentlyanddifferentfroma variability basedstabilitycriteria thevector vconsistsofprobabilities rather than variabilities.
the stability function stable checkswhetherthemeanprobabilityofthestabilityvaluesfrom v areabovethethreshold t.formally stableprob m t true mean v t. .
modified jmh implementation weimplementedthedynamicreconfigurationapproachwiththe three stoppage criteria for jmh version .
by adding a reconfigurationbenchmarkmodewithstoppagecriteria scandstable andthreshold t properties annotationpropertiesfor wiminand fmin andcorrespondingcliflags.additionally weadaptedjmh s console and javascript object notation json result file output to include the newconfigurationoptionsandaddedawarningif the stability criterion has not been met for a benchmark.
the modified fork of jmh is available on github and part of our replication package .
empirical evaluation toassesswhetherdynamicreconfigurationiseffectiveandefficient weconductanexperimentalevaluationonasubsetofthejavaoss projects identified in our pre study see section .
our evaluation comparesthreedynamicreconfigurationapproaches oneforevery stoppage criterion .
as a baseline for comparison we use standard jmhwithstaticconfigurationandthe defaultvalues.
to support open science we provide all evaluation data and scripts inareplication package .
.
research questions first we want to ensure that dynamic reconfiguration does not changetheresultscomparedtostaticconfiguration.iftheresults of the same benchmark executed with static configuration and withdynamicreconfigurationareequal weconcludethatdynamic reconfigurationiseffectiveinpreservingresultquality.forthis we formulate rq1 rq 1how does dynamic reconfiguration of software microbenchmarksaffecttheirexecutionresult?
second wewanttoevaluateifdynamicreconfigurationimproves theoverallruntimeofabenchmarksuite comparedtostaticconfiguration includingtheoverheadimposedbythestoppagecriteria computation.for this we formulate rq2 rq 2how much time can be saved by dynamically reconfiguring software microbenchmarks?
asabenchmark sresultquality accuracy andruntimearecompeting objectives the combination of the results from rq and rq2validateswhetherdynamicreconfigurationenables reducing executiontime withoutsacrificingresult quality .
.
studysubjects evaluating the dynamic reconfiguration approach on all 753prestudy subjects see section is infeasible as executing benchmarkesec fse november8 13 virtualevent usa christophlaaber stefan w rsten harald c.gall andphilippleitner table selected studysubjects.all projectsare hosted on github except the ones indicated name project version benchs.
param.
benchs.1exec.
timedomain byte buddy raphw byte buddy c24319a .42h bytecode manipulation jctools jctools jctools 19cbaae .56h concurrentdata structures jdk jmh jdk microbenchmarks2d0fab23 .81h benchmarksof the jdk jenetics jenetics jenetics 002f969 .56h geneticalgorithms jmh core jmh core benchmarks3a07e914 .28h benchmarksof jmh log4j2 apache logging log4j2 ac121e2 .83h logging protostuff protostuff protostuff 2865bb4 .31h serialization rxjava reactivex rxjava 17a8eef .06h asynchronous programming squidlib squidpony squidlib 055f041 .97h visualization zipkin openzipkin zipkin 43f633d .47h distributedtracing 1the numberscorrespond to succeeding benchmarks and excludes 38failing parameterizations.
seeourreplicationpackage for a list 2repository 3moduledirectoryin repository suitespotentiallytakesalongtime.hence we performpurposive sampling to select a subset of ten non trivial projects from a widevarietyofdomainswithsmall tolarge benchmark suites.
our evaluation executes all 969benchmark parameter combinationsofthetenstudysubjects whichare .
ofthe48 parametercombinationsfrom the pre study.
table1liststhestudysubjectswiththeirnumberofbenchmarks benchs.
and benchmark parameter combinations param.
benchs.
gitversionusedfortheevaluation version andexecutiontime when using jmhdefaultvalues exec.
time .
.
studysetup we execute all benchmarks retrieve the benchmark results and afterwards apply dynamic reconfiguration and the stoppage criteria to the obtained data set.
this allows us to experiment with thresholdsand parameterswithout having to rerunthe fullbenchmarksuiteswithourmodifiedjmhimplementation withdynamic reconfiguration .
.
.
execution and data gathering.
as performance measurements are prone to confounding factors we apply the subsequent steps to follow a rigorous methodology in order to increaseresult reliability.
allbenchmarksuites are patchedwithjmh .
.
wecompileandexecuteallbenchmarkswithadoptopenjdk andjavahotspotvirtualmachine vm version .
.0 222 b10 exceptlog4j2whichrequiresajavadevelopmentkit jdk version hence we employ version .
we run the benchmarks on a bare metal machine with a12 core intel xeon x5670 .93ghz cpu 70gib memory andasamsungssd 860prosataiiidisk running archlinuxwithakernel version .
.
arch .
all non mandatory background processes except ssh are disabled without explicitly disabling software hardware optimizations.
regardingbenchmarksuiteexecution weconfigureandexecuteallbenchmarkswithfiveforks f 100measurementiterationsmi 1smeasurementtime mt andjmh s samplemode setthroughjmh scli.thisconfigurationcorrespondstothe jmh1.21defaults only mtchangesfrom 10sto1sbut atthe sametime miincreasesbyafactorof whichgrantsour approach more checkpoints.
note that warmup iterations wiare settozerobut miis doubled from 50to100 which is required to obtain results for every iteration to dynamically decide whentostop the warmupphase.
the resulting execution configuration is then cb 0s 1s .
we remove outliers that are a magnitude larger than the median.
.
.
approach.
withtheobtainedperformance resultsfromthe suite executions we evaluate dynamic reconfiguration with the followingconfigurationparameters.recalltheconfigurationdefinitioncb wimin wimax mi fmin fmax wf wt mt see section4.
.
staticconfiguration baseline .
thebaseline i.e.
jmhwithstatic configuration uses the jmh .21default configuration for all benchmarks.forthis weremovefromthegathereddatathefirst 50iterations correspondingto wi fromeachforkandusethe remainingiterationsas mi.hence thebaselinehasthefollowing configuration cb 1s 1s .
we consciously decided for the jmh default configuration as baseline and against the developers custom benchmark configurationsforthefollowingreasons ofthepre studybenchmarks change the benchmark execution time through custom configurations hence of the benchmarks still use the jmh default configuration the majority of these benchmarks of all pre studybenchmarks onlyuseasinglefork f whichisconsideredbadpracticeasinter jvm variabilityiscommon basically invalidating developers custom configurations for rigorous benchmarking and aunifiedbenchmarkconfigurationasthebaseline enables comparabilityacrossour study subjects.
dynamicreconfiguration.
forthedynamicreconfigurationapproaches weemploytheconfiguration cb 1s 1s for all benchmarks which changes the minimum warmup iterations wimin and minimum forks fmin compared to thedynamicallyreconfiguringsoftware microbenchmarks esec fse november8 13 virtualevent usa baseline.notethatwealsoreduce mito10insteadof whichthe baseline uses.
initialexperiments showed that an increasein measurementiterations afterasteadystateisreached hasonlyaminor effectonresult accuracy butwithconsiderably longer runtimes.
weusethefollowingparametersforthethreedynamicreconfiguration approaches one per stoppagecriterion .
wedrawaweightedsampleof 000invocationsperiteration to reduce computationaloverheadat checkpoints.
the sliding windowsize issetto sw .
cvuses a threshold t .
which corresponds to a maximumvariability difference inthe slidingwindowof1 .
rciwusesa99 confidencelevel 000bootstrapiterations which is a good tradeoff between runtime overhead and estimation accuracy and a threshold t .
following bestpractice .
kldpartitionsthedistributions d1andd2into1 000strips for the kld calculation removes outliers that are more than .
iqraway from the median and uses a threshold t .
which corresponds to a mean probability within theslidingwindowof orlarger.morestripswouldresult in longer calculation times for the kernel density estimation and consequently inahigherruntimeoverhead.without theoutlierremoval kldwouldnotconvergeabovetheprobability threshold t and hence our approach would not stop thebenchmarkexecution.notethattheoutlierremovalis onlyperformedaspartofthestoppagecriteriacalculation ofourapproach fortheevaluation weconsiderallmeasurementsand donotremove any outliers see section .
.
.
results andanalysis we now present the results of our empirical evaluation by comparing the benchmark results of the static configuration to the ones of ourdynamicreconfigurationapproacheswiththethreestoppage criteria.
.
.
rq1 resultquality.
toassesswhetherapplyingdynamic reconfiguration changes benchmark results and to answer rq we perform two analyses between the execution results coming from the baseline with static configuration and each ofthe three dynamicreconfigurationapproaches statisticala atestsand mean performance changerate.
a atests.
ana atestcheckswhetherresultsfromtwodistributionsarenotsignificantlydifferent wherenodifferenceisexpected.
in our context this means if an a a test between static configurationanddynamicreconfiguration foreachstoppagecriterion doesnotreportadifference weconcludethatdynamicreconfigurationdoesnotchangethebenchmarkresult.followingperformance engineeringbestpractice weestimatetheconfidence interval for the ratio of means with bootstrap using10 000iterations and employing hierarchical random resampling with replacement on invocation iteration and forklevel againrelyingon pa .iftheconfidenceinterval oftheratio straddles there is no statistically significant difference.
note that thisprocedureisdifferentfromthestoppagecriteriarciw seesection4 herewecomparetheresults allmeasurementiterations mitable result quality differences between static configuration approachanddynamic reconfiguration approaches cv rciw kld a a tests notdifferent .
.
.
meanchangerate .
.
.
.
.
.
benchs .
.
.
benchs .
.
.
benchs .
.
.
fromallforks f oftwotechniques whereasrciwusesconfidence intervalwidthsas avariability measure of asingletechnique.
the first row of table 2shows the a a results.
for a majority of the3 969benchmarkparametercombinations applyingdynamic reconfiguration does notresult in significantly different distributions.
about or more of the benchmarks have similar result distributions comparedtothestatic configuration.rciwachieves thebestresultwith .
whilecvandkldperformsimilarlywell with78.
and79.
respectively.
note that the static approach uses50measurementiterations mi whilethedynamicapproach only runs indicating that if a steady state is reached which is onegoalofdynamicreconfiguration moremeasurementiterations have anegligibleimpact onthe overallresult.
changerate.
inadditiontoa atests weassesstheperformance change rate between the static configuration approach and each of the dynamic reconfiguration approaches i.e.
by how much the means of the performance result distributions differ.
the change rate augments the a a tests binary decision by showing how differentthebenchmarkresultsbecomewhenapplyingdynamic reconfiguration.
thesecondrowoftable 2showsthemeanchangerateacross all benchmarks in percent and its standard deviation.
the mean changeratebetweenthethreestoppagecriteriaandthestaticapproach is or lower for all three.
note that following a rigorous measurement methodology could still be caused by jvm instabilities unrelated to our approach .
again rciw is the best criterion with .
.
.
finally the last three rows show how many benchmarks have a change rate below and3 for all stoppage criteria.
we observe that rciw outperforms the other twosignificantly followed bykld.
ofthebenchmarkshave achangeratebelow below2 and below3 .this suggests that rciw is a highly effective technique for stopping benchmarkexecutions.
figure4depicts the change rate distributions per project and stoppage criterion where every data point corresponds to a benchmark s mean performance change.
considering the median change rateofaproject sbenchmarks rciwperformsbestforallprojects exceptjenetics jmh core andsquidlib where kld is slightly superior.
cv consistently has the largest change rates of the three stoppagecriteria nonetheless itperformsonlyslightlyworsein mostcases.consideringthemeanchangerate rciwisthemostaccurate stoppage criteria for 10projects with only jmh corebeing morestablewhenkldisemployed.notethatfortheprojectswhere rciw is not the best stoppage criterion both mean and medianesec fse november8 13 virtualevent usa christophlaaber stefan w rsten harald c.gall andphilippleitner byte buddy jctooolsjdkjenetics jmh corelog4j2protostuff rxjava squidlibzipkin study subjectchange ratestoppage criteria cv kld rciw figure mean change rate per study subject and stoppage criteria.
the bar indicates the median the diamond the mean theboxtheiqr andthewhiskers .
iqr.
changeratesarebelow .theprojectswiththemostdiverging benchmarks between static configuration and dynamic reconfiguration execution are byte buddy jctools log4j2 andsquidlib.
thebenchmarksoftheseprojectsarelessstablecomparedtothe other projects likely dueto executing non deterministicbehaviour such as concurrency and input output i o .
results from benchmarksthatarelessstablewillpotentiallyhavestatisticallydifferent distributionsand therefore not maintain the same result quality.
unreachblestabilitycriteria.
ifthestabilityfunction stablenever evaluatesthemeasurementsafterawarmupiterationoraforkas stable themaximumnumberofwarmupiterations wimax orforks fmax areexecuted.thiscorrespondstothestaticconfiguration of jmh.
we analyzed how often stability is not achieved according to the three stoppage criteria across all study subjects.
cv is the most lenient criterion with only .
of the benchmarks forks not considered stable after 50warmup iterations and of the benchmarksinsufficientlyaccurateafterfiveforks.kldachieves similar numbers .
for warmup iterations however .
of the benchmarks were not considered stable after five forks.
rciw is even more restrictive where .
and37.
of the benchmarks do not reach the stability criteria after wimaxandfmax respectively.thisrestrictivenessimpactsthea atestandmeanchange rate results leading to benchmark results with higher quality.
not reachingthestabilitycriteriacaneitherhappenifthethreshold t is too restrictive or the benchmark is inherently variable whichis acommon phenomenon .
rq summary.
applying dynamic reconfiguration does not changetheresultqualityofthemajorityofthebenchmarks when compared to the static configuration.
the rciw stoppage criteria outperforms kld and cv with .
of the benchmarks maintainingtheirresultqualityandameanperformancechangerate of1.
.
.
.
rq2 timesaving.
themaingoalofdynamicreconfiguration is to save time executing benchmark suites.
for this and to answer rq2 we measuretheruntimeoverheadofthethreestoppage criteria estimatethetimesavingforallprojectscomparedtothestatic configuration and show at which checkpoint warmup or fork more time can be saved.
runtimeoverhead.
tomeasuretheruntimeoverheadofthethree stoppagecriteria weexecutethebenchmarksuiteof log4j2once with standard jmh .
i.e.
static configuration and once for eachstoppagecriteriawithourjmhforkimplementingdynamic reconfiguration.
to ensure a valid comparison between the four measurements staticconfiguration dynamicreconfigurationof threestoppagecriteria weusethesameconfigurationforthestatic andthedynamicapproachesof cb 1s 1s butdo notstopatthestoppagecheckpoints.wemeasuretheend to end executiontime tb ofeverybenchmark bwhenexecutedthrough jmh s cli.
this time includes jvm startup benchmark fixtures benchmark execution and stoppage criteria computation which is negligible comparedto the duration of the measurement.
note that thenumberofdatapointsusedforthestoppagecriteriacalculation isindependentofthestudysubjectbyconstructionofjmhandour approach thereforeitissufficienttomeasuretheoverheadbased onone project see adiscussiononthis insection .
the overheads o oof allbenchmarksfor astoppagecriteria is o uniontext b btb dyn tb sta where tb dynis the execution time of the dynamic reconfiguration with a specific stoppage criteria and tb sta istheexecutiontimeofthestaticconfiguration.theoverheads o are independent of the number of iterations and forks executed becausetheyarefactorsoftheruntimedifferencebetweendynamic reconfiguration with one stoppage criterion and the static configuration i.e.
standard jmh and all our overhead measurements use the same configuration cb.
the overheads we measure are ocv .
.
for cv orciw .
.
forrciw okld .
.
for kld.
note that changing the iteration time of 1s and executing benchmarks on different hardware might affect the overhead.
the considerable difference in overhead is explained by the complexity ofthestoppagecriteriacalculations.whereascviscomputationally cheap it only needs to compute standard deviation mean and their difference rciw is computationally intensive due to the simulations required for bootstrap.
because there is hardly any overheadvariability amongallbenchmarks weconsiderthe overheadconstantandusethemeanvaluefortheremainderofthe experiments.
time saving estimation.
to estimate the overall time that can be saved with dynamic reconfiguration we adapt the execution timeequation tb seesection toincorporatethestoppagecriteria.
the dynamic reconfiguration benchmark execution time is then tb dyn summationtext f forks .forkscorresponds to the number of executed forks fof a benchmark according to the stoppage criterion wifto the number of warmup iterations in this fork f and the rest according to cbfrom section .
.
for simplicityandbecauseofthelowvariabilitybetweenbenchmark overheads we disregard benchmark fixture times.
the total benchmark suite execution time when using dynamic reconfiguration is thentdyn summationtext b b tb dyn whereb is the set of benchmark parameter combinations.dynamicallyreconfiguringsoftware microbenchmarks esec fse november8 13 virtualevent usa table time savingperprojectandstoppage criteria project time saving cv rciw kld byte buddy .42h .
.62h .
.22h .
jctools .42h .
.45h .
.13h .
jdk .32h .
.57h .
.41h .
jenetics .78h .
.37h .
.52h .
jmh .76h .
.69h .
.42h .
log4j2 .56h .
.12h .
.96h .
protostuff .43h .
.91h .
.44h .
rxjava .91h .
.55h .
.68h .
squidlib .07h .
.70h .
.11h .
zipkin .17h .
.93h .
.59h .
total .84h .
.92h .
.48h .
table3shows the time saving perprojectand stoppage criteria inabsolutenumbers hours andrelativetothestaticconfiguration.
weobservethatdynamicreconfigurationwithallthreestoppage criteriaenablesdrastictimereductionscomparedtostaticconfiguration.
in total cv and kld save and rciw of the benchmarksuiteexecutiontimesofallprojectscombined.forindividualprojects thetimesavingrangesbetween .
and86.
for cv .
and83.
forrciw and .
and83.
forkld.even withthecomputationallymostexpensivetechnique i.e.
rciw we can save at least .
of time.
in total numbers the savings are between .43h and157.32h for cv .62h and135.57h for rciw and3.44hand154.41h for kld.
stoppage criteria checkpoints.
dynamic reconfiguration defines two points during benchmark execution when to stop after the warmup phaseif measurements are stablewithin a fork and aftera forkifmeasurementsacross forksarestable.inour analysis the range of warmup iterations is from five wimin to50 wimax andforksarebetweentwo fmin andfive fmax seecbinsection4.
.althoughcvandkldsaveasimilaramountoftime they havedifferentstoppagebehavior.wherecvrequiresmorewarmup iterations .
.
thankld .
.
theoppositeisthecasefor forkswith .
.2vs.
.
.
respectively.rciw whichsavesconsiderablylesstime demandsmorewarmupiterations .
.
to consider a fork stable but lies between cv and kld in terms of forks .
.
.
the reported numbers are arithmetic means of warmupiterationsandforks withstandarddeviationsacrossall benchmarks of all study subjects.
generally warmup iterations aremorereducedthanforksinoursetup indicatingthatfork toforkvariabilityismorepresentthanwithin forkvariance thatis variability across multiple jvms compared to within a jvm respectively.dynamicreconfigurationenablesfindingthesweetspot betweenshorteningwarmupiterationsandforksincombination withacertainstoppagecriteria.
rq2summary.
withruntimeoverheadsbetween and dynamicreconfigurationenablesreducingbenchmarksuiteruntimesby .
to .
compared to jmh sdefaultruntime.
discussion and recommendations our pre study see section shows that developers often drastically reduce benchmark execution times.
we see two potential reasonsforthis thebenchmarksuiteruntimesaretoolong and consequently developers trade shorter runtimes for inaccurate results or jmh defaults are overly conservative and benchmarks withshorterruntimesoftenstillproduceresultsthatareconsidered sufficientlyaccurate.wehypothesizethattheformerismorelikely butleavethedeveloperperspectiveforconfigurationchoicesfor futurework.inanycase theproposeddynamicreconfigurationapproachenablesreducingtimewhilemaintainingsimilarbenchmark results as our empirical evaluation shows.
recommendationsfordevelopers.
developersareadvisedtoeither assess their benchmark accuracies when executed in their environment and adjust configurations accordingly or employ dynamicreconfigurationwhichisabletoadjusttodifferentexecution environments.thechoiceofstoppagecriteriadependsontherequiredresult qualityand therefore the performancechangesizes desiredtobedetected.forslightlylessaccurateresultsbutmore time reduction we recommend using kld otherwise rciw is preferred.
the exact threshold tdepends on the stability of the execution environments the benchmarks that are run in it.
if a controlled bare metal environment is available we suggest the thresholds of our study.
in a virtualizedor cloud environment the thresholds needtobeadjusted see alsoheetal .
.the effectiveness of our technique in non bare metal environments such as in the cloud is subject to future research.
moreover whether a combination of different stoppage criteria e.g.
stopping when both kld and rciw deem a benchmark run to be stable improves result accuracy also requires further research.
such a combination would however negativelyaffecttheruntimeoverheadofdynamic reconfiguration.
microbenchmarks in ci.
the long benchmark execution times see section 3and are a major obstacle for including microbenchmarks in ci .
to overcome this hurdle a combinationofourtechniquewithbenchmarkselection benchmark prioritization and riskanalysisoncommits would reduce therequiredtimeformicrobenchmarkingandpotentiallyenableci integration.continuouslyassessingsoftwareperformancewould increaseconfidencethatachangedoesnotdegradeperformance andlikely be beneficialfor performance bugroot cause analysis.
choosingjmhconfigurationparameters.
choosingjmhconfigurationparametersthatkeepexecutiontimelowandresultaccuracy highis non trivial and developers decrease configurations drastically.ourresultsshowtheimportanceofsettingthewarmupphase correctlyandutilizingmultipleforksforbenchmarkaccuracy.with a large number of benchmarks expecting developers to pick the right values becomes unrealistic.
our dynamic reconfiguration approach helps in this regard by deciding based on data and per benchmarkwhen the results are accurateenough.
iterationtimeandforks.
thewarmupandmeasurementtimes affectbenchmarkresultaccuracyandcontrolthefrequencywith which stability checkpoints occur.
jmh .21changed the iteration timefrom1sto10s andreducedthenumberofforksfromtentoesec fse november8 13 virtualevent usa christophlaaber stefan w rsten harald c.gall andphilippleitner five .theopenjdkteamarguedthat1sistooshortforlarge workloads .weperformedanadditionalanalysiswhetherresult accuracy changes when switching from 10s to 1s but did not observe differences in most cases.
hence we decided for 1s iterations to give the dynamic reconfiguration approach more checkpoints toassessabenchmark sstability.whereas10sisasafechoicefor staticconfigurations webelievethat1sprovidesmoreflexibility andworksbetterwithdynamicreconfiguration.ourresultssupport reducing to five forks which indicates that most fork to fork variability iscaptured.
choosing stability criteria parameters.
choosing optimalmetaparameters for the stability criteria can affect the effectiveness and efficiency of the overall approach.
dynamic reconfiguration supportsthesliding windowsize sw thethreshold twhenastability criterion value cv rciw or kld is considered stable and stability criterion dependent parameters see section .
.
we base our parameters on common statistical practice and previous research seesection .
.onlytheslidingwindowsize sw is manually set by us.
our empirical evaluation shows that the employedparametersworkwellacrossallstudysubjects.however futureresearchshouldexplorethemeta parameterspacethrough experimentation.it isimportant toemphasize thatchoosing these meta parametersisanofflineactivity whichisdoneonceandbefore executing the benchmarks hence the cost for choosing these parametersisnot part ofthe overheadestimationsinrq2.
unreachablestabilitycriteria.
althoughthestabilitycriteriais frequently not met for warmup iterations or forks of individual benchmarks atleastwhenusingkldandrciw theoverallruntime of the full benchmarksuites is considerably reduced see section5.
.
dynamic reconfiguration uses upper bounds for warmup iterations wimax and forks fmax therefore it doesnot exceed theruntimeofstandardjmhwithstaticconfiguration.incaseof anunreachablestabilitycriteria ourjmhimplementationwarns thedeveloper whocanthenadjustthisbenchmark supperbounds to obtain better results.
our approach could also automatically lift thecapsifthedesiredresultqualityisnotreached whichshould be exploredbyfuture research.
threats to validity construct validity.
our pre study see section relies on information extracted from source code i.e.
configurations based on jmh annotations.wedonotconsider overwrittenconfigurations through cli arguments which might be present in build scripts or documentation in the repositories.
reported runtimes do not consider fixture setup and teardown times jvm startup and time spent in the benchmark harness of jmh and they assume iteration timesareasconfigured whileinrealitytheyareminimumtimes.
therefore reported times might slightly underestimate the real executiontimes.
the results and implications from rq are based on the notion of benchmark result similarity.
we assess this through statistical a a tests based on bootstrap confidence intervals for the ratio of means and mean performance changerate similar to previous work .othertestsforthesimilarityofbenchmarkresults such as non parametric hypothesis tests and effect sizes mightleadto differentoutcomes.
we base the time savings from rq on overhead calculations fromasingleprojectandassumethisoverheadisconstantforall stoppage points and benchmarks.
there is hardly any reason to believethatoverheadschangebetweenstudysubjects benchmarks and stoppage points because the number ofdatapoints usedfor stoppage criteria computation are similar.
this is due to how jmh and our approach work see sections 2and4 and how our experiment is designed see section the measurement time mtis fixed irrespectiveofthebenchmarkworkload thenumberof iterations miandforks fisfixed benchmarkfixtures i.e.
setup and teardown are constant and of negligible duration compared to themeasurementduration and thestoppagecriteriacalculation usesasliding windowapproach sw and therefore thenumber ofiterations usedfor the calculationisconstant.
further we perform post hoc analysis on a single benchmark execution data set for all stoppage criteria.
that is we execute the benchmarksuiteswithfiveforksand 100measurementiterations 1s and then compute the stoppage points.
computing the stoppage pointswhileexecutingtestsuitesmightleadtoslightlydifferent results.
finally weuseasliding windowapproachfordeterminingthe endofthewarmupphasewithawindowsize sw offive.different window sizes might impose a larger runtime overhead and change the stoppagepointoutcomes.
internal validity.
internal validity is mostly concerned with our performance measurement methodology and the employed thresholds.
we follow measurement best practice and run experimentsonabare metalmachine toreducemeasurementbias .wedidnotexplicitlyturnoffsoftwareandhardwareoptimizations whichmightaffectbenchmarkvariabilityand therefore our results.
regardingthethresholds westartedfrompreviousworks andadaptedthemtofitthecontextofmicrobenchmarks.aswe usedthesamethresholdsforallbenchmarksandprojects weare confidentthattheyaregenerallyapplicableforjavamicrobenchmarksexecutedonasimilar machine to ours.
further the times reported in section 3rely on the jmh version ofabenchmark weappliedsimpleheuristicstoextracttheversion which might not be fully accurate in case of for instance multimoduleprojectsordynamic jmhversiondeclarations.
externalvalidity.
generalizabilitymightbeaffectedwithrespect tothestudiedprojects.weonlyfocusonossprojectsfromgithub and it is unclear whether our findings are equally valid in the context of industrial software or projects hosted on other platforms.
especially the ten selected projectsfor our empirical evaluation see section might not be a representative sample for all jmh projects.
due to the long benchmark suite execution times more projects would not have been feasible to study.
we aimed for adiversesetofprojects spanningmultipledomains seetable covering of the benchmarks from the pre study see section .
the effectiveness and efficiency results of dynamic reconfigurationdependsontheenvironmentusedforexecutingthebenchmarks.ourexperimentalevaluationfavorsinternalvalidityover external validity by using a controlled bare metal environment.dynamicallyreconfiguringsoftware microbenchmarks esec fse november8 13 virtualevent usa therefore wecanbemoreconfidentthatthereportedcomparisons betweenstudysubjectsandstoppagecriteriaareindeedcorrectand notduetouncontrollablefactorspresentinvirtualizedandcloudenvironments.
executing benchmarks with dynamic reconfiguration insuch otherenvironmentsmightleadto differentresults.
moreover our focus has been on java projects that use jmh as their benchmarking framework.
although the concepts from section4also translate to other frameworks and languages the exact results might be different.
we opted for java jmh because it is a dynamically compiled language where warmup phases andmultipleforksareessential jmhbenchmarksuitesarelong running andcanbenefitgreatlyfromdynamicreconfiguration and jmh is a mature framework with many features offering greatopportunitiesfor our approach.
finally switching to different java virtual machines such as eclipseopenj9orgraal might change the results duetodifferent performance characteristics.
related work performance testing is a form of measurement based performance engineering whichcomesintwomainflavors system level tests and method statement level tests.
historically research focussedonsystem leveltests suchasloadandstresstesting withmorerecentadvancestargetingindustrialapplicabilityand practice .
the otherflavor i.e.
software microbenchmarks and performance unit tests has only recently gained popularity in research.
studies on oss projects found that adoption lags behindtheirfunctionalcounter parts i.e.
unittests.
one problem isthathandlingperformancetestsiscomplexandrequiresin depth knowledgefromdevelopers.toreducethisfriction dingetal .
studied utilizing unit tests for assessing performance properties.
bulejetal .
proposedaframeworkthatletsdevelopersspecify performance assertions and handles rigorous statistical evaluation.
hork et al .
compose performance unit test outcomes into codedocumentationtoraiseperformanceawareness anddamascenocostaetal .
uncoverbadpracticesinmicrobenchmark code through static analyses.
generating tests removes the need towritetestsbyhand autojmhhelpsavoidingpitfallsrootedin compileroptimization pradeletal .
generateperformance regression tests for concurrent classes and perfsyn synthesizes inputsthroughmutationthatexposeworst caseperformancebehaviour .ourworkisorthogonaltotheaforementionedworks itdynamicallyadaptssoftware microbenchmarkconfigurations to stop theirexecutiononcetheirresult isstable.
longexecutiontimes anduncertainresults are well known to complicate the usage of performance tests in general including software microbenchmarks.
there are a few approachesthatreducethetimespentinperformancetestingactivitieswithoutconsideringresultquality predictingcommits that are likely toimpact performance prioritizing and selecting thetestsinasuitethataremorelikelyto expose performance changes.
our approach pursues the same goal of reducing benchmarking time but with a focus on running all benchmarks similartoprioritization aslongasnecessarywhile maintainingthe same result quality.resultqualityisimpairedbynotrunningenoughmeasurements as well as measurement bias which requires careful experiment planningandexecution .tomitigatemeasurement bias georges et al .
outlined a rigorous methodology how to asses performance of java programs which we base our measurementtechniqueon.usingthecorrectstatisticaltechniques to assess performance is paramount with estimated confidence intervals using bootstrap being the state of the art .
one ofourstoppingcriteriaisbasedonandourresultqualityevaluation usesconfidenceintervalswithbootstrap.todecidehowmanymeasurementsareenough approachesusingstatisticaltechniqueshave beenproposed employingcv confidenceintervals andthekullback leiblerdivergence kld .withthese performanceexperimentssuchasbenchmarkexecutionsrununtiltheir resultsareaccurate stableenoughandthenabortexecution ideally reducingexecutiontime.ourstoppagecriteriausethesethreetechniquesandapplytheminthecontextofsoftwaremicrobenchmarks after the warmupphaseandafter every fork.
closest to our approach are the ones by maricq et al .
and heetal.
.maricqetal .
estimatethenumberoftrialsand iterations using a bootstrap technique.
while they perform this estimationbeforeexecutingbenchmarks weevaluateresultquality during execution.
he et al .
stop system level performance tests executed in cloud environments once they reach a certain stabilitycriteria.differentfromthebenchmarksusedintheirstudy microbenchmarks are much shorter with runtimes in the order of seconds instead of multiple hours.
our work builds on top of their statistics based approach using kld for system benchmarks adapts it for microbenchmarks and extends it to other stoppage criteria.
conclusions this paper introduced a dynamic reconfiguration approach for softwaremicrobenchmarks whichreducesbenchmarkexecution time andmaintains the same result quality.
inapre studybasedonreal worldconfigurationsof 387microbenchmarks comingfrom 753projects we find thatdevelopers make extensive use of custom configurations to considerably reduce runtimes for of the benchmarks.
still about of the projectshave benchmarksuite runtimesof more than3hours.
our dynamic reconfiguration approach implements data driven decisions to stop microbenchmark executions assisting developers withtheintricatetaskofcorrectlyconfiguringmicrobenchmarks.
withoverheadsbetween and11 itachievesatimereduction of48.
to86.
withbetween .
and87.
ofthemicrobenchmarkspreservingtheirresult quality.
these results show that dynamic reconfiguration is highly effectiveandefficient andweenvisionittoenableregularperformance microbenchmarkingactivities such as part of ci.