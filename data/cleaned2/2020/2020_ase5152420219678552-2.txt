automating user notice generation for smart contract functions xing hu bardbl zhipeng gao xin xia david lo and xiaohu yang school of software technology zhejiang university ningbo china faculty of information technology monash university melbourne australia school of information systems singapore management university singapore davidlo smu.edu.sg college of computer science and technology zhejiang university hangzhou china xinghu yangxh zju.edu.cn zhipeng.gao xin.xia monash.edu davidlo smu.edu.sg abstract smart contracts have obtained much attention and are crucial for automatic financial and business transactions.
for end users who have never seen the source code they can readthe user notice shown in end user client to understand whata transaction does of a smart contract function.
however dueto time constraints or lack of motivation user notice is oftenmissing during the development of smart contracts.
for end users who lack the information of the user notices there is noeasy way for them to check the code semantics of the smartcontracts.
thus in this paper we propose a new approach s mart docto generate user notice for smart contract functions automatically.
our tool can help end users better understandthe smart contract and aware of the financial risks improvingthe users confidence on the reliability of the smart contracts.
s mart docexploits the transformer to learn the representation of source code and generates natural language descriptionsfrom the learned representation.
we also integrate the pointermechanism to copy words from the input source code insteadof generating words during the prediction process.
we extract7 angbracketleftfunction notice angbracketrightpairs from smart contracts written in solidity.
due to the limited amount of collectedsmart contract functions i.e.
functions we exploit atransfer learning technique to utilize the learned knowledge toimprove the performance of s mart doc.
the learned knowledge obtained by the pre training on a corpus of java code thathas similar characteristics as solidity code.
the experimentalresults show that our approach can effectively generate usernotice given the source code and significantly outperform thestate of the art approaches.
to investigate human perspectives onour generated user notice we also conduct a human evaluationand ask participants to score user notice generated by differentapproaches.
results show that s mart docoutperforms baselines from three aspects naturalness informativeness and similarity.
index t erms smart contract user notice generation deep learning i. i ntroduction recent years have seen an emerging interest in cryptocurrencies e.g.
bitcoin and ethereum on distributed ledgers a.k.a.
blockchains from both industry and academia.as one of the largest cryptocurrency platform ethereumhas become a widely used platform to enable financial andbusiness transactions.
as the core of ethereum smartcontracts are turing complete programs and executed bardblalso with pengcheng laboratory.
corresponding author.on the ethereum blockchain.
after the deployment the end users can interact with a smart contract by sending transactionsto its functions.
each transaction consumes a certain amount of gas whose price is given in ethereum cryptocurrency namedether .
per unit of ether as of dec .
dueto the high stakes of smart contracts and the potential riskof financial loss for users it is necessary to assist end usersbetter understand the functionality of smart contracts.
two groups of people interact with smart contracts developers and end users.
when developers implement a smartcontract they need to translate financial operations e.g.
transfer into one or more contract transactions then the end users start the transaction which triggers the execution of afunction defined within the smart contract.
in this study weargue that the end users are often non tech savvy consumersof the contracts.
to assist these users who cannot read thesource code solidity one of the most popular programminglanguage for smart contracts provides a mechanism that canprovide notices for end users.
an example of a smart contractfunction and how it is used by an end user are illustratedin figure and figure .
consider alice is an end user ofsmart contracts who knows nothing about programming whenshe submits a transaction to the above function with a targetaddress of 0x83 cc and mintedamount of then the user notice will be rendered to alice as create tokens and send it to 0x83 cc .
after reading the user notice alice can better understand the contract andthus can better make informed decision i.e.
reject or confirmthe transaction.
unfortunately user notices are often lacking for a large number of smart contracts.
even though the official guideline of solidity recommends that the smart contracts shouldbe annotated with user notice for all public interfaces thispractice is often neglected or ignored by developers duringsmart contract development.
this will make the end userscompletely clueless and uninformed which may discouragethe participation of end users and the usability of the smartcontract.
moreover due to the unalterable feature of theblockchain system unlike traditional software user notice cannot be added once the smart contract is deployed.
therefore itis desirable to have a tool that can automatically generate usernotices for smart contract developers whenever they forget to 36th ieee acm international conference on automated software engineering ase 36th ieee acm international conference on automated software engineering ase .
ieee .
ase51524.
.
.
ieee !
fig.
.
an example of a smart contract function do so.
existing approaches mainly focus on generating comments for common programming languages e.g.
java and python.
these comments are provided to developers and help them tounderstand the source code.
however generating commentsespecially user oriented comments i.e.
user notices has notgained much attention yet.
making such a tool for smart con tracts is a non trivial task considering the following challenges i dynamic expressions mechanism.
the user notice in smart contracts supports dynamic expressions.
different fromthe general code comments of other programming languages e.g.
java comments the solidity compiler produces theuser notice dynamically from the source code.
the dynamicexpression mechanism requires that certain words in the usernotice should be identical with the corresponding tokens insource code.
for the example shown in figure the word mintedamount and target are both copied from thesource code.
this mechanism causes the user notice to beclosely related to the source code.
even though the existingdocumentation generation approaches have achieved a hugesuccess for general code comments generation e.g.
commentsfor java method it is not clear whetherthey can be successfully applied to user notice generationfor smart contracts.
how to copy variable names correctlyfrom the smart contracts is still challenging for these deeplearning models.
ii data hungry.
compared with other popular programming languages i.e.
java it is more difficultto collect large scale datasets for smart contracts.
even thoughthe ethereum blockchain has accumulated a great numberof smart contracts data hungry problem still exists.
thatis only a small proportion of smart contracts have usernotices.
according to our preliminary study only outof smart contracts contain user notices if we look atthe functions the proportion is even smaller.
how to utilizethe limited labeled data for generating accurate user notice ischallenging for this work.
in this paper we propose a new approach named s mart doc to address the aforementioned challenges.
we aim to understand functions in smart contracts and automaticallygenerate user notices i.e.
notice for functions in a smart contracts.
the main idea of our approach is two folds while generating user notice s mart doccan predict a word or copy a token from source code.
it exploits transformer equipped with pointer mechanism to predict user notice.
exist ing code comment generation approaches usually use recur rent neural network e.g.
lstm and gru to generate codecomments.
however these techniques are difficult to capturelong range dependencies between code tokens.
in this paper we exploit the transformer architecture that can generate user m !
m !
m m m !
m 0lqw7rnhq km m c3 e m dg l7 i m m uhdwh wrnhqv dqg vhqg lw wr fhh h h h e d ig f 0lqw7rnhq m m m m h idee g feie f ei ih h g ffi egege e i 7dujhw fhh h h h e d ig f m !
m !
m m m !
1rw rxqgkm m c3 e m dg l7 i m m m m m m h idee g feie f ei ih h g ffi egege e i rxqg ee iieee ie ii fig.
.
an example of an end user submitting a transaction with on the right and without on the left user notice notices to reinforce the capability of capturing the long range dependencies between code tokens.
considering the dynamicexpression mechanism many words in the user notice canbe copied from the smart contract functions.
therefore weintegrate the pointer generator in our approach to overcomethe first challenge.
the pointer mechanism can copy a wordby pointing tokens in source code.
in order to alleviate thelimitation of minimal labeled data we propose to use transferlearning that transfers the knowledge of general commentgeneration for java methods into user notice generation forsmart contract functions.
solidity and java languages aresomewhat similar in that both are object oriented and high level programming languages.
models that have learnt how toconvert java methods into comments can be a good start tothe user notice generation.
to evaluate our proposed model we extract angbracketleftfunction notice angbracketright pairs from verified smart contract.
the automatic evaluation results show that s mart doc achieves the best performance when compared with baselinesincluding attendgru ast attendgru and re 2com regarding the he bleu score and rouge l score.
to explorethe practitioners perspective on the generated notice we alsoconduct a human evaluation.
each practitioner is asked toevaluate user notice generated by various approaches fromthree aspects the similarity of generated notice and humanwritten notice naturalness grammatical correctness and fluency of the generated notices and their informativeness the amount of content carried over from the input code to thegenerated notices ignoring fluency of the text .
experimentsshow that our approach can achieve the best performance whencompared to the baseline techniques.
the main contributions of this paper are as follows we are the first to investigate characteristics of smartcontract user notices.
our study highlights a problemthat has been neglected in the literature but has practicalimplications.
we propose a novel approach s mart doc for smart contract user notice generation which aims to help end users understand smart contracts when they are executedin blockchain platforms.
we integrate the pointer mechanism into transformer for 6better user notice prediction.
the approach can generate words or copy tokens from source code.
we exploittransfer learning to alleviate the effect of minimal labeleddata on training a deep learning model.
the experimentalresults show that our approach outperforms the state of the art techniques.
we build the first dataset with respect to user notice whichcontains angbracketleftfunction notice angbracketright pairs.
to the best of our knowledge this is the first dataset of user notices forsmart contract functions.
this paper is organized as follows.
in section ii we provide the preliminaries of smart contracts.
section iii presents ourapproach for smart contract user notice generation.
sectioniv evaluates our approach on actual contracts collected fromthe ethereum blockchain.
section v and section vi illustrateexperimental results and practitioners perspectives on thegenerated user notice.
section vii discusses our proposedapproach.
section ix presents the related works.
section xconcludes the paper.
ii.
p reliminaries in this section we present the data hungry issue related to user notices.
then we show the correlation between usernotice and transactions .
a. user notice hungry ethereum has attracted increasing attention as a blockchain platform and smart contracts deployed on ethereum have been applied to many business domains toenable efficient and trustable transactions .
whendeveloping smart contracts solidity provides a special formof comments named the ethereum natural language spec ification format natspec to document contracts and func tions .
the notice tag is the main natspec tag and its audience is end users.
considering that smart contract end users are often non tech savvy consumers the user notices canbridge the information gap between smart contract developersand end users.
by interacting with the user notices the end users can better assess the financial risks and make betterinformed decisions.
however according to our preliminarystudy only a small proportion of smart contracts include usernotices.
for example among the contracts that we havecollected only of them contain user notices moreover only of the functions in smart contracts have user notices.
b. user notice transactions in smart contracts functions are the executable units of code and can be called by end users.
intuitively user notice of functions can help end users understand the smart contracts and thus improve the probability of smart contracts trans actions.
to verify this conjecture we collect the transactioninformation of smart contracts and investigate whether smartcontracts with user notice have more transactions.
figure 3presents the function distribution in smart contracts and thethe distribution of the average amount of transactions of smart a function distribution in smart contracts b the average amount of transac tions of smart contracts with differ ent amount of user notice fig.
.
function distribution and the transaction distribution of smartcontracts.
contract name smt transactions contract token ... notice send value token to to from msg.sender ... function transfer address to uint256 value public returns boolsuccess notice send value token to to from from on the condition it is approved by from ...function transferfrom address from address to uint256 value public returns boolsuccess ... contract name lookscoin transactions contract erc20 ... function transfer address to uint256 value public returns boolsuccess function transferfrom address from address to uint256 value public returns boolsuccess ... fig.
.
motivating examples contracts with different amount of user notice.
from figure a we can observe that almost all smart contracts haveless than functions.
thus we analyze the transactions ofthem and find that smart contracts with more user noticestend to have more transactions shown in figure b .
thisis reasonable because detailed user notices can make end users well aware of the financial risks and improve the users confidence in the reliability of the smart contracts thereforeend users prefer to start a transaction through smart contractswith more high quality user notices.
figure shows the source code of two smart contracts i.e.
smt and lookscoin in which smt has 785transactions and lookscoin has transactions.
althoughthese two smart contracts implemented the same functions such as transfer andtransferfrom in this example the smt contract provided adequate user notices while lookscoindid not make any notice for end users.
the detailed user noticecan help end users better understand of their operations andthus make the smart contract more popular among end users.
iii.
a pproach figure illustrates the overall framework of our approach smart doc.
it mainly consists of three phases pre training fine tuning and application.
in the pre training phase we exploit the java dataset prepared by hu et al.
to pre trainthe model of s mart doc.
similar to java solidity is also an object oriented programming language.
the two programminglanguages share similar coding conventions and syntax.
there fore the knowledge such as variable naming conventions sequential information among code tokens learned from javacan be reused into solidity.
to better exploit the existing fig.
.
overview of our approach knowledge of source code we transfer the pre trained weights of java encoder into solidity encoder.
in the fine tuning phase we further train the user notice generation model on the corpus of annotated angbracketleftfun doc angbracketrightpairs extracted from smart contracts.
the encoder is initialized bythe learned encoder from the pre trained model.
except forthe source code encoder parameters of other components aretrained from scratch.
after training we can get a trainedneural network.
then given a new function of smart contract corresponding user notice can be generated by the trainedmodel.
figure is an overview of the network architecture of our proposed deep learning based model.
the architecture of ourmodel follows the transformer framework which has been successfully adopted in machine translationtasks.
the architecture mainly consists of three submodules source code encoder.
this module aims to representthe source code and exploits the multi head self attentionto learn the sequential information of the source code.
notice generation decoder.
this module aims to generatenotice through the self attention layer and the encoder decoderattention layer.
the encoder decoder attention layer helps thedecoder focus on appropriate places in the input sequence.
pointer generator.
the pointer generator is used to copyvariables from source code.
we will elaborate on each component in this framework in the following subsections.
a. encoder the encoder aims to learn representations for a smart contract function x x x2 ... x m. each token is embedded into a vector i.e.
x x1 ... xm before fed into the encoder.
to help s mart docfocuses on the important information of the function x the encoder adopts a multihead self attention layer to capture important parts of the input.
then the output of the multi head self attention layer is fedinto a feed forward neural network.
the multi head self attention layer depicted in figure exploits scaled dot product attention to calculate attentionweights.
given an input vector i i rd in this paper ii represents the embedding of each token the first step is tocreate three vectors i.e.
a query vector q i a key vector ki and a value vector vi.
fig.
.
the structure of our neural network then we use the query vector qiof theith input and the key vector kjof each word of the input sentence to calculate the attention scores through dot products.
the attention scoreagainst the ith input is computed as follows i j qi kj d wheredis the dimension of qiandkj.
the score determines how much focus to place on the jth input as we encode the ith input.
then we get the normalized scores by a softmax function i j softmax i exp i j summationtext texp i t to keep the values of the tokens we want to focus on intact and drown out irrelevant tokens we multiply each value vectorby the softmax score and sum up the weighted value vectors z i summationdisplay j i jvj for faster processing the calculation can be done in matrix form shown as follows attention q k v s o f t m a x qkt dk v in addition s mart doc adopts the multi head attention withhheads to focus on different channels of the input vectors.
the outputs of hheads self attention are concatenated into one matrix and then are linearly projected by the linearlayer a c o n c a t attention i qi ki vi w wherewis the parameter matrix of the linear layer.
then the outputs ofthe multi head self attention layer are fed into a feed forwardneural network.
b. decoder the decoder component mainly consists of two parts namely the self attention layer and the encoder decoder attention layer.
the self attention layer is similar to that in theencoder component except that it only deals with generatedwords in the output sequence.
different from the self attentionlayer the encoder decoder attention layer learns the relation ship between the source code and the target user notice.
thecalculation of attention is similar to self attention.
the queriesmatrix qcomes from the output of the self attention layer and the key kand v alues matrix vfrom the output of the encoder component.
for each step the decoder outputs a state vectorvwhich can be turned into a word of a sequence.
8c.
pointer generator as described in section ii the dynamic expressions mechanism of smart contracts user notice causes that words in user notices can be copied from the source code.
so we integrate the pointer generator in our approach to solvethis problem.
the pointer generator is calculated from the vocabulary distribution p vocab the copy distribution pcopy and the generation probability pgen.
v ocabulary distribution according to the transformer architecture the vocabulary distribution pvocab is calculated by a softmax layer which follows a linear layer pvocab softmax w v w is a learnable parameter that projects the vector binto a larger vector i.e.
has the same size as the vocabulary size .
copy distribution copy distribution pcopy is the probability of copying a word from the input sequence i.e.
sourcecode in this work.
it is computed from the attention distri bution namely the output of the encoder decoder attentionlayer in decoder.
the calculation of attention distributionbetween encoder and decoder is similar to the self attentionlayer excepted that the key vector kis from the outputs of the encoder r r ... rm .
the attention distribution jbetween the target words yjand the source code tokens w1 ... w mis j softmax qjkt dk then the copy distribution pcopy is calculated as follows pcopy summationdisplay i w i yj j i final distribution at last the model uses a soft switch pgen to choose between generating a word from vocabulary pvocab or copying a token from the input source code pcopy .
similar to see et al.
the generation probability pgen is for predicting yjcalculated from the concatenation of decoder input yj decoder state vj and the attention distribution j pgen wgen b wherewgen andbare learnable parameters.
is the sigmoid function and pgen .
at last the final distribution foryjis p yj pgenpvocab pgen pcopy d. transfer learning during the training process deep learning models need large amounts of labeled data.
however the parallel dataof smart contract function and notice is limited.
to betterlearn the latent knowledge in smart contracts we exploitthe transfer learning technique to reuse learned knowledge.transfer learning is an effective technique to alleviate the datahungry issue.
transfer learning goes beyond specific tasksand domains in this paper comment generation and triesto leverage knowledge from pre trained models and use it tosolve target problems in this paper user notice generation .considering the features of smart contract functions we selectcomment generation for java methods as the source task t s to learning features of programming language.
the learned knowledge of the java method namely source domainds is then transferred into the target task tt.
pre training procedure to get a good code representation model for solidity functions we first pre train ourmodel on the java corpus d s. this is reasonable due to the following reasons.
first the java corpus is much largerthan the size of the solidity functions i.e whichhas provided sufficient data sets for training a comprehensivemodel.
second the solidity language is similar to the javalanguage with respect to their grammars and syntax thesesimilar features learned from java corpus may also be effectivefor solidity functions.
to pre train the model we exploit theencoder to learn the semantic representation of java methodsand use the decoder to generate java comments accordingto the learned representation.
the encoder and the decoderare introduced above.
the pre trained encoder contain theknowledge that convert a source code in java language tothe semantic representation.
thus we can obtain the javamethod knowledge d sby accessing the pre trained encoder parameters.
fine tuning procedure when the model is pre trained we then fine tune it on the user notice generation task.
thefine tuning process can quickly adapt the knowledge from thejava pre trained model to learn the code semantics and struc tures of solidity functions.
during the fine tuning procedure we reused the pre trained encoder to learn the representation ofsmart contract functions.
the downstream task of generatinguser notices can be implemented by a decoder which receivesuser notice representations from the pre trained encoder.
inthis way we can reuse the pre trained knowledge from thejava programming language.
iv .
e v alua tion in this section we firstly describe the evaluation corpus of the task.
we then introduce the baselines to compareand evaluation metrics.
lastly we explain our experimentalsettings.
the replication package is available .
a. dataset we use the raw smart contract dataset provided by chen et al.
which contains verified solidity files eachfile may contain multiple smart contracts crawled from ether scan .
we describe how we prepared the dataset for usernotice generation as follows.
preprocessing first we exploit the solidity parser to parse smart contracts and extract functions.
we exclude the constructor functions since they are trivial to generate noticefor such functions.
as solidity parser does not support nat spec comments extraction we define regular expressions to 9table i the sta tistics of our collected smart contracts and functions contract functionfunctions with user noticeaverage function loc .
a code length distribution b user notice length distribution fig.
.
length distribution of the training data extract natspec comments that are tagged with notice for functions.
table i provides the statistics of the preprocessed dataset.
we extract functions from smart contractsand get functions with user notices.
the averagenumber of lines of code loc of smart contract functionsis .
.
filtering then we extract the user notices natspec comment labeled notice from smart contract functions and filtered out non english samples.
considering that duplicate code has negative impacts on neural networks and introducesbias in evaluation we remove duplicate functions and usernotices.
finally we get angbracketleftfunction notice angbracketright pairs.
generating training test sets we split dataset into training set and test set.
we randomly select 1k pairs for testing andthe rest for training.
figure illustrates the length distributionof functions and user notice on the training data.
we find thatmore than code snippets have less than tokens anduser notice has less than words.
in addition the mode oftheir lengths are and respectively.
tokenization to convert functions into sequential text we tokenize the source code via solidity parser.
then we tokenize the user notice by natural language toolkit nltk .the vocabulary size of code and notice is and respectively.
b. baselines we compare our model with the following baselines attendgru attendgru exploits the attentional seq2seq model to generate code comment.
it includes an encoder and a decoder that are both gated recurrent unit gru .
the encoder aims to learn the representationfrom the source code and the decoder generates commentsfrom learned representation.
they propose to use an attentionmechanism to attend words in the output summary sentence towords in the code word representation.
during the predictionphase they use a greedy search algorithm for inference thatminimizes the number of experimental variables and compu tation cost.
ast attendgru ast attendgru integrates structural information on the basis of attendgru.
the structural information comes from the abstract syntax tree ast .
inaddition to the code encoder it also contains an encoder to pro cess asts.
they traverse asts into sequences by structure based traversal sbt proposed by hu et al.
before fedinto neural networks.
a separate attention mechanism is usedto attend the words to parts of the ast.
then they concatenatethe vectors from each attention mechanism to create a contextvector.
finally they predict the comment one word at a timefrom the context vector following what is typical in seq2seqmodels.
re 2com re2com is the state of the art code comment generation approach that integrates three kinds oftechniques namely ir template and neural networks.
themodel consists of two modules a retrieve module and arefine module.
in the retrieve module re 2com exploits ir techniques to retrieve the most similar code snippet from alarge parallel corpus of code snippets and their correspondingcomments and treat the comment of the similar code snippetas an exemplar.
in the refine module it applies a novelseq2seq neural network whose encoder takes the given codesnippet the similar code snippet and the exemplar as inputand the decoder generates the token sequence of a comment.
c. evaluation metrics bleu following wei et al.
we evaluate different approaches using the metric bleu .
it calculates the similarity between the generated notice and