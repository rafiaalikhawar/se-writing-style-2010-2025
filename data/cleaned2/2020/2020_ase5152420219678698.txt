learning domain specific edit operations from model repositories with frequent subgraph mining christof tinnes timo kehrer mitchell joblin uwe hohenstein andreas biesdorf sven apel siemens ag technology m unchen germany humboldt universit at zu berlin berlin adlershof germany saarland university informatics campus saarbr ucken germany abstract model transformations play a fundamental role in model driven software development.
they can be used to solve or support central tasks such as creating models handlingmodel co evolution and model merging.
in the past various semi automatic approaches have been proposed to derive model transformations from meta models or from examples.
these ap proaches require time consuming handcrafting or the recordingof concrete examples or they are unable to derive complex trans formations.
we propose a novel unsupervised approach called o ckham which is able to learn edit operations from model histories in model repositories.
o ckham is based on the idea that meaningful domain specific edit operations are the ones thatcompress the model differences.
it employs frequent subgraph mining to discover frequent structures in model difference graphs.
we evaluate our approach in two controlled experiments and one real world case study of a large scale industrial model drivenarchitecture project in the railway domain.
we found that ourapproach is able to discover frequent edit operations that haveactually been applied before.
furthermore o ckham is able to extract edit operations that are meaningful to practitioners in anindustrial setting.
i. i ntroduction software and systems become increasingly complex.
v arious languages methodologies and paradigms have been developed to tackle this complexity.
one widely used methodology is model driven engineering mde which uses models as first class entities and facilitates generating documentation and parts of the source code from these models.
usually domain specific modeling languages are used and tailored to the specific needs of a domain.
this reduces the cognitive distance between the domain experts and technical artifacts.
akey ingredient of many tasks and activities in mde are model transformations .
we are interested in edit operations as an important subclass of model transformations.
an edit operation is an in place model transformation and usually represents regular evolution of models.
for example when moving a method from one class to another in a class diagram also a sequence diagram that uses the method in message calls between object lifelines needs to be adjusted accordingly.
to perform this in a single edit step one can create an edit operation that executes the entire change including all class and sequence diagram changes.
some tasks can even be completely automatized and reduced to the definition of edit operations edit operations are usedfor model repair quick fix generation auto completion model editors operation based merging model refactoring model optimization metamodel evolution and model co evolution semantic lifting of model differences model generation and many more.
in general there are two main problems involved in the specification of edit operations or model transformations in general.
firstly creating the necessary transformations for the task and the domain specific modeling languages at hand usinga dedicated transformation language requires a deep knowledge of the language s meta model and the underlying paradigm of the transformation language.
it might even be necessary to define project specific edit operations which causes a largeoverhead for many projects and tool providers .
secondly for some tasks domain specific transformations are a form of tacit knowledge and it will be hard for domain experts to externalize this knowledge.
as on the one hand model transformations play such a central role in mde but on the other hand it is not easyto specify them attempts have been made to support their manual creation or even semi automated generation.
as for manual support visual assistance tools and transformation languages derived from a modeling language s concrete syntax have been proposed to release domain experts from the need of stepping into the details of meta models and modeltransformation languages.
however they still need to deal with the syntax and semantics of certain change annotations and edit operations must be specified in a manual fashion.
to this end generating edit operations automatically from a given meta model has been proposed .
however besides elementary consistency constraints and basic well formedness rules meta models do not convey any domain specific information on how models are edited.
thus the generation of edit operations from a meta model is limited to rather primitive operations as a matter of fact.
following the idea of model transformation by example mtbe initial sketches of more complex and domain specific editoperations can be specified using standard model editors.however these sketches require manual post processing to be turned into general specifications mainly because an initial specification is derived from only a single transformation example.
some mtbe approaches aim at getting rid of this limitation by using a set of transformation examples as input which are then generalized into a model transformation rule.
still this is a supervised approach which requires sets of 36th ieee acm international conference on automated software engineering ase 36th ieee acm international conference on automated software engineering ase .
ieee .
ase51524.
.
.
ieee dedicated transformation examples that need to be defined by domain experts in a manual fashion.
as discussed by kehrer et al.
a particular challenge is that domain experts need to have at least some basic knowledge on the internal processing of the mtbe tool to come up with a reasonable set of examples.moreover if only a few examples are used as input for learning mokaddem et al.
discuss how critical it is to carefully select and design these examples.
to address these limitations of existing approaches we propose a novel unsupervised approach o ckham for mining edit operations from existing models in a model repository which is typically available in large scale modeling projects cf.
section ii .
o ckham is based on an occam s razor argument that is the useful edit operations are the ones that compress the model repository.
in a first step o ckham discovers frequent change patterns using frequent subgraph mining on a labeled graph representation of model differences.
it then uses a compression metric to filter and rank these patterns.
we evaluate o ckham using two controlled experiments with simulated data and one real world large scale industrial case study from the railway domain.
in the controlled setting we canshow that o ckham is able to discover the edit operations that have been actually applied before by us even when we apply some perturbation.
in the real world case study we find that our approach is able to scale to real world model repositories and to derive edit operations deemed reasonable by practitioners.
we evaluated o ckham by comparing the results to randomly generated edit operations in five interviews with practitioners of the product line.
we find that the edit operations represent typical edit scenarios and are meaningful to the practitioners.
in a summary we make the following contributions we propose an unsupervised approach called o ckham that is based on frequent subgraph mining to derive edit operations from model repositories without requiring any further information.
we evaluate o ckham empirically based on two controlled simulated experiments and show that the approach is able to discover the applied edit operations.
we evaluate the approach using an interview with five experienced system engineers and architects from a realworld industrial setting in the railway domain with more than engineers 300gb of artifacts and more than years of modeling history.
we show that our approach is able to detect meaningful edit operations in this industrial setting and that is scales to real world repositories.
ii.
m otiv a tion a nindustrial scenario our initial motivation to automatically mine edit operations from model repositories arose from a long term collaboration with practitioners from a large scale industrial model driven software product line in the railway domain.
the modeling is done in m agic draw using sysml and there is an export to the eclipse modeling framework emf which focuseson the sysml parts required for subsequent mde activities e.g.
code generation .
modeling tools such as m agic draw come with support for model versioning.
in our setting themodels are versioned in the magicdraw teamwork server.
wetherefore have access to a large number of models and change scenarios.
discussing major challenges with the engineers of the product line we observed that some model changes appear very often together in this repository.
for example when the architect creates an interface between two components s he will usually add some ports tocomponents and connect them via the connectorends of a connector.
expressed in terms of the meta model there are changes to add such an interface.
we are therefore interested to automatically detect these patterns in the model repository.
more generally our approach o ckham is based on the assumption that it should be possible to derive meaningful patterns from the repositories.
these patterns could then be used for many applications .
the background is that in our case study the models have become huge over time approx.
.
million elements split into submodels and model differences between different products have become huge up to changes in a single submodel .
the analysis of these differences for example for quality assurance of the models or domain analysis has become very tedious and time consuming.
to speed up the analysis of the model differences it would be desirable to reduce the perceived size of the model difference by grouping finegrained differences to higher level more coarse grained and more meaningful changes.
for this semantic lifting of model differences the approach by kehrer et al.
which uses a set of edit operations as configuration input can be used but the approach requires the edit operations to be defined already.
these large model differences have actually been our main motivation to investigate how we can derive the required edit operations semi automatically.
we will use the data from this real world project to evaluate ockham in section v. iii.
b ackground in this section we provide basic definitions that are important to understand our approach presented in section iv.
a. graph theory as usual in mde we assume that a meta model specifies the abstract syntax and static semantics of a modeling language.
conceptually we consider a model as a typed graph aka.
abstract syntax graph in which the types of nodes and edges are drawn from the meta model.
figure illustrates how a simplified excerpt from an architectural model of our case study from section ii in concrete syntax is represented in abstract syntax typed over the given meta model.
we further assume models to be correctly typed.we abstain from a formal definition of typing using type graphs and type morphisms though.
instead to keep our basic definitions as simple as possible we work with a variant of labeled graphs in which a fixed label alphabet represents node and edge type definitions of a meta model.
given a label alphabet l alabeled directed graph gis a tuple v e wherevis a finite set 931fig.
.
we consider models as labeled graphs where labels represent types of nodes and edges defined by a meta model.
for the sake of brevity types of edges are omitted in the figure.
of nodes eis a subset of v v called the edge set and v e lis the labeling function which assigns a label to nodes and edges.
if we are only interested in the structure of a graph and typing is irrelevant we will omit the labeling and only refer to the graph as g v e .
given two graphs g v e andg prime v prime e prime prime g prime is called a subgraph ofg written g prime g i fv prime v e prime e and x prime x for each x v prime e prime.
a weakly connected component component for short c vc ec gis an induced subgraph of gin which every two vertices are connected by a path that is u v vc n ns.
t. braceleftbig v v1 v1 v2 ... vn u bracerightbig ec ec where ecis the set of all reversed edges that is u v ecbecomes v u ec.
b. frequent subgraph mining we will use frequent subgraph mining as the main ingredient for o ckham .
we distinguish between graph transaction based frequent subgraph mining and single graph based frequent subgraph mining.
in particular we are considering graphtransaction based frequent subgraph mining which typicallytakes a database of graphs and a threshold tas input.
it then outputs all the subgraphs with at least toccurrences in the database.
an overview of frequent subgraph miningalgorithms can be found in the literature .
a general introduction to graph mining is given by cook and holder who also proposed a compression based subgraph miner called s ubdue .
s ubdue has also been one of our main inspirations for a compression based approach.
o ckham is based on g aston which mines frequent subgraphs by first focusing on frequent paths then extending to frequent trees and finally extending the trees to cyclic graphs.
c. model transformations and edit operations the goal of o ckham is to learn domain specific edit operations from model histories.
in general edit operationscan be informally understood as editing commands that can be applied to modify a given model.
in turn a difference between two model versions can be described as a partially ordered set of applications of edit operations transforming one model version into the other.
comparing two models can thus be understood as determining the applications of the edit operation applications that transform one model into the other.
a major class of edit operations are model refactorings which induce syntactical changes without changing a models semantics.
other classes of edit operations include recurring bug fixes and evolutionary changes.
in a classification by visser et al.
edit operations can describe regular evolution that is the modeling language is used to make changes but they are not meant to describe meta model evolution platform evolution or abstraction evolution.
more technically in mens et al.
s taxonomy edit operations can be classified as endogenous i.e.
source and target meta model are equal in place i.e.
source and target model are equal model transformations.
for the purpose ofthis paper we define an edit operation as an in place model transformation which represents regular model evolution.
the model transformation tool h enshin supports the specification of in place model transformations in a declarativemanner.
it is based on graph transformation concepts and it provides a visual language for the definition of transformation rules which is used for example in the last step of figure .
roughly speaking transformation rules specify graph patterns that are to be found and created or deleted.
iv .
a pproach we address the problem of automatically identifying edit operations from a graph mining perspective.
as discussed in section iii we will work with labeled graphs instead of typed graphs.
there are some limitations related to this decision which we discuss in section vi b. ockham consists of the five steps illustrated with a running example in figure .
our main technical contributions are step2 and step .
for step step and step we apply existing tooling s idiff g aston and h enshin cf.
section iii .
step compute structural model differences to learn a set of edit operations in an unsupervised manner o ckham analyzes model changes that can be extracted from a model s development history.
for every pair of successive model versionsnandn in a given model history we calculate astructural model difference n n to capture these changes.
as we do not assume any information e.g.
persistentchange logs to be maintained by a model repository we use astate based approach to calculate a structural difference which proceeds in two steps .
first the corresponding model elements in the model graphs gnandgn are determined using a model matcher .
second the structural changes are derived from these correspondences all the elements in gnthat do not have a corresponding partner in gn are considered to be deleted whereas vice versa all the elements ingn that do not have a corresponding partner in gnare considered to be newly created.
932fig.
.
the step process for mining edit operations with o ckham .
for further processing in subsequent steps we represent a structural difference n n in a graph based manner referred to as difference graph .
a difference graph g n n is constructed as a unified graph over gnand gn .
that is corresponding elements being preserved by an evolution step from version nton appear only once in g n n indicated by the label prefix preserved while all other elements that are unique to model gnandgn are marked as deleted and created respectively indicated by the label prefixes delete and create .
for illustration assume that the architectural model shown in figure is the revised version n of a version nby adding the ports along with the connector and its associated requirement.
figure illustrates a matching of the abstract syntax graphs of the model versions nandn .
for the sake of brevity only correspondences between nodes in gnandgn are shown in the figure while two edges are corresponding when their source and target nodes are in a correspondence relationship.
the derived difference graph g n n is illustrated in figure .
for example the corresponding nodes of type component occur only once in g n n and the nodes of type port are indicated as being created in version n .
our implementation is based on the eclipse modeling framework.
we use the tool s idiff to compute structural model differences.
our requirements on the model differencing tool are support for emf the option to implement a custom matcher because modeling tools such as magic draw usually provide ids for every model element which can be employed by a custom matcher and an approach to semantically lift model differences based on a set of given edit operations because we intend to use the semantic lifting approach for the compression of differences in the project mentioned in section ii.
other tools such as emfc ompare could also be used for the computation of model differences and there are no other criteria to favourone over the other.
an overview of the different matching techniques is given by kolovos et al.
a survey of model comparison approaches is given by stephan and cordy .
step derive simple change graphs real world models maintained in a model repository such as the architecturalmodels in our case study can get huge.
it is certainly fair to say that compared to a model s overall size only a small number of model elements is actually subject to change in a typical evolution step.
thus in the difference graphs obtained in the first step the majority of difference graph elementsrepresent model elements that are simply preserved.
to this end before we continue with the frequent subgraph mining in step in step difference graphs are reduced to simple change graphs scgs based on the principle of locality relaxation only changes that are close to each other canresult from the application of a single edit operation.
we discuss the implications of this principle in section vi b .b y close we mean that the respective difference graph elements representing a change must be directly connected i.e.
not only through a path of preserved elements .
conversely this means that changes being represented by elements that are part 933of different connected components of a simple change graph are independent of each other i.e.
they are assumed to result from different edit operation applications .
more formally given a difference graph g n n a simple change graph scg n n g n n is derived from g n n in two steps.
first we select all the elements ing n n representing a change i.e.
nodes and edges that are labeled as delete and create respectively .
in general this selection does not yield a graph but just a graph fragment f g n n which may contain dangling edges.
second these preserved nodes are also selected to be included in the simple change graph.
formally the simple change graph is constructed as the boundary graph of f which is the smallest graph scg n n g n n completing fto a graph .
the derivation of a simple change graph from a given difference graph is illustrated in the second step of figure .
in this example the simple change graphcomprises only a single connected component.
in a realistic setting however a simple change graph typically comprises a larger set of connected components like the one illustrated in step of figure .
step apply frequent connected subgraph mining when we apply the first two steps to a model history we obtain a set of simple change graphs braceleftbig scg n n n ... n bracerightbig where n is the number of revisions in the repository.
in this set we want to identify recurring patterns and therefore find some frequent connected subgraphs.
a small support threshold might lead to a huge number of frequent subgraphs.
for example a support threshold of one would yieldevery subgraph in the set of connected components.
this does not only cause large computational effort but also makes it difficult to find relevant subgraphs.
as it would be infeasible to recompute the threshold manually for every dataset wepre compute it by running an approximate frequent subtree miner for different thresholds up to some fixed size of frequent subtrees.
we fix the range of frequent trees and adjust the threshold accordingly.
alternatively a relative threshold could be used but we found in a pilot study that our pre computation works better in terms of average precision.
we discuss the effect of the support threshold further in section vi.
werun the frequent subgraph miner for the threshold found viathe approximate tree miner.
step of figure shows thisfor our running example.
we start with a set of connectedcomponents and the graph miner returns a set of frequent subgraphs namely g1 g2 g3 withg1 g2 g3.
we use the gaston graph miner since it performed best in terms of runtime among the miners that we experimented with gspan gaston and dims pan in a pilot study.
in our pilot study we ran the miners on a small selection of our datasets and experimented with the parameters of the miners.
for many datasets gspan and dims pan did not terminate at all we canceled the execution after 48h .
g aston with embedding lists was able to terminate in less then 10s on most of ourdatasets but consumes a lot of memory typically between 10gb 25gb which was not a problem for our 32gb machine in the pilot study.
to rule out any effects due to approximatemining we considered only exact miners.
therefore we also could not use s ubdue which directly tries to optimize compression.
furthermore s ubdue was not able to discover both edit operations in the second experiment see section v without iterative mining and allowing for overlaps.
enabling these two options s ubdue did not terminate on more than of the pilot study datasets.
for frequent subtree mining we use h ops because it provides low error rates and good runtime guarantees.
step select the most relevant subgraphs motivated by the minimum description length principle which has been successfully applied to many different kinds of data the most relevant patterns should not be the most frequent onesbut the ones that give us a maximum compression for our original data .
that is we want to express the given scgs by a set of subgraphs such that the description length forthe subgraphs together with the length of the description ofthe scgs in terms of the subgraphs becomes minimal.
thisreasoning can be illustrated by looking at the corner cases a single change has a large frequency but is typically not interesting.
the entire model difference is large in terms of changes but has a frequency of only one and is typically also not an interesting edit operation.
typical edit operations are therefore somewhere in the middle.
we will use our experiments in section v to validate whether this assumption holds.
we define the compression value by compr g parenleftbig supp g parenrightbig parenleftbig vg eg parenrightbig wheresupp g is the support of gin our set of input graphs i.e.
the number of components in which the subgraph is contained .
the in the definition of the compression value comes from the intuition that we need to store the definition of the subgraph to decompress the data again.
the goal of this step is to detect the subgraphs from the previous step with a high compression value.
subgraphs are organized in a subgraph lattice where each graph has pointers to its direct subgraphs.
most of the subgraph miners already compute a subgraph lattice so we do not need a subgraph isomorphism test here.
due to the downward closure property of the support all subgraphs of a given sub graph have at least the same frequency in transaction based graph mining .
when sorting the output we need to take this into account since we are only interested in the largest possible subgraphs for some frequency.
therefore we prune the subgraph lattice.
the resulting list of recommendations is then sorted according to the compression value.
other outputs are conceivable but in terms of evaluation a sorted list is a typical choice for a recommender system .
more technically let sg be the set of subgraphs obtained from step we then remove all the graphs in the set sg braceleftbig g sg g sg withg g supp g s u p p g compr g compr g bracerightbig .
our list of recommendations is then sg sg sorted according to the compression metric.
for our running example in step of figure assume that the largest subgraph g3occurs times without overlaps .
even though the smaller subgraph g1occurs twice as often 934we find that g3provides the best compression value and is therefore ranked first.
subgraph g2will be pruned since it has the same support as its supergraph g3 but a lower compression value.
we implement the compression computation and pruning using the networkx python library.
step generate edit operations as a result of step we have an ordered list of relevant subgraphs of the simple change graphs.
we need to transform these subgraphs into model transformations that specify our learned edit operations.
as illustrated in step of figure the subgraphs can be transformed to henshin transformation rules in a straightforward manner.
we use h enshin because it is used for the semantic lifting approach in our case study from sec.
ii.
in principle any transformation language that allows us to express endogenous in place model transformations could be used.
a survey of model transformation tools is given by kahani et al.
.
v. e v alua tion in this section we will evaluate our approach in two controlled experiments and one real world industry case study in the railway domain.
a. research questions we evaluate o ckham w.r.t.
the following research questions rq isockham able to identify edit operations that have actually been applied in model repositories?
if we apply some operations to models o ckham should be able to discover these from the data.
furthermore when different edit operations are applied and overlap it should still be possible to discover them.
rq isockham able to find typical edit operations or editing scenarios in a real world setting?
compared to the first research question o ckham should also be able to find typical scenarios in practice for which we do not know which operations have been actually applied to the data.
furthermore it should be possible to derive these edit operations in a real world setting with large models and complex meta models.
rq what are the main drivers for ockham to succeed or fail?
we want to identify the characteristics of the input data or parameters having a major influence on o ckham .
rq what are the main parameters for the performance of the frequent subgraph mining?
frequent subgraph mining has a very high computational complexity for general cyclic graphs.
we want to identify the characteristics of the data in our setting that influence the mining time.
for rq we want to rediscover the edit operations from our ground truth whereas in rq the discovered operations could also be some changes that are not applied in only one step but appear to be typical for a domain expert.
we refer to both kinds of change patterns as meaningful edit operation.b.
experiment setup we conduct three experiments to evaluate our approach.
in the first two experiments we run the algorithm on syntheticmodel repositories.
we know the relevant edit operations in these repositories since we define them and apply them to sample models.
we can therefore use these experiments to answer rq .
furthermore since we are able to control many properties of our input data for these simulated repositories we can also use them to answer rq and rq .
in the third experiment we apply o ckham to the dataset from our case study presented in section ii to answer rq .
the first two experiments help us to find the model properties and the parameters the approach is sensible to.
their purpose is toincrease the internal validity of our evaluation.
to increase external validity we apply o ckham in a real world setting as well.
none of these experiments alone achieves sufficient internal and external validity but the combination of all experiments is suitable to assess whether o ckham can discover relevant edit operations.
we run the experiments on an intel core tmi7 5820k cpu .30ghz and .
gib ram.
for the synthetic repositories we use cores per dataset.
experiment as a first experiment we simulate the application of edit operations on a simple component model.
the meta model is shown in figure .
for this experiment we only apply one kind of edit operation the one from our running example in figure to a random model instance.
the henshin rule specifying the operationconsists of a graph pattern comprising nodes and edges.
we create the model differences as follows we start with an instance m0of the simple component meta model with 87packages components 5swimplementations ports 6connectors and requirements.
then the edit operation is randomly applied etimes to the model obtaining a new model revision m1.
this procedure is applied iteratively dtimes to obtain the model history m0 m1 ...m d md.each evolution step mi mi yields a difference m i mi .
since we can not ensure completeness of o ckham i.e.
it might not discover all edit operations in a real world setting we also have to investigate how sensible the approach is to undiscovered edit operations.
therefore to each application of the edit operation we apply a random perturbation.
more concretely a perturbation is another edit operation that we apply with a certain probability p. this perturbation is applied such that it overlaps with the application of the main edit operation.
we use the tool h enshin to apply model transformations to one model revision.
we then build the difference of two successive models as outlined in section iv.
in our experiment we control the following parameters for the generated data.
d the number of differences in each simulated model repository.
for this experiment d .
e the number of edit operations to be applied per model revision in the repository that is how often the edit operation will be applied to the model.
for this experiment e ... .
p the probability that the operation will be perturbed.
for this experiment we use p .
.
... .
.
935this gives us datasets for this experiment.
a characteristics of our datasets is that increasing e the probability of changes to overlap increases as well.
eventually adding more changes even decreases the number of components in the scg while increasing the average size of the components.
ockham suggests a ranking of the top ksubgraphs which eventually yield the learned edit operations .
in the ranked suggestions of the algorithm we then look for the position of the relevant edit operation by using a graph isomorphism test.
to evaluate the ranking we use the mean average precision at k map k which is commonly used as an accuracy metric for recommender systems map k d summationdisplay dap k wheredis the family of all datasets one dataset represents one repository and ap k is defined by ap k summationtextk i 1p i rel i total set of relevant subgraphs where p i is the precision at i and rel i indicates if the graph at rank iis relevant.
for this experiment the number of relevant edit operations or subgraphs to be more precise is always one.
therefore we are interested in the rank of the correct edit operation.
except for the case that the relevant edit operation does not show up at all map gives us the mean reciprocal rank and therefore serves as a good metric for that purpose.
for comparison only we also compute the map k scores for the rank of the correct edit operations according to the frequency of the subgraphs.
furthermore we investigatehow the performance of subgraph mining depends on other parameters of o ckham .
we are also interested in how average precision ap that is ap depends on the characteristics of the datasets.
note that for the first two experiments we do not execute the last canonical step of our approach i.e.
deriving the edit operation from a scg but we directly evaluate the resulting subgraph from step against the simple change graph corresponding to the edit operation.
to evaluate the performance of the frequent subgraph miner on our datasets we fixed the relative threshold i.e.
the support threshold divided by the number of components in the graph database to .
.
we re run the algorithm for this fixed relative support threshold and p .
.
experiment in contrast to the first experiment we want to identify in the second experiment more than one edit operation in a model repository.
we therefore extend the first experiment by adding another edit operation applying eachof the operations with the same probability.
to test whether ockham also detects edit operations with smaller compression than the dominant in terms of compression edit operation we choose a smaller second operation.
the henshin rule graph pattern for the second operation comprises nodes and 5edges.
it corresponds to adding a new component with its swimplementation and a requirement to a package.since the simulation of model revisions consumes a lot of compute resources we fixed d and considered only e for this experiment.
the rest of the experiment is analogous to the first experiment.
experiment the power of the simulation to mimic a realworld model evolution is limited.
especially the assumption of random and independent applications of edit operationsis questionable.
therefore for the third experiment we usea real world model repository from the railway software development domain see section ii .
for this repository we do not know the operations that have actually been applied.
we therefore compare the mined edit operations with edit operations randomly generated from the meta model and want to show that the mined edit operations are significantly more meaningful than the random ones.
for this experiment we mined pairwise differences with changes on average which also contain changed attribute values one reason for that many changes is that the engineering language has changed from german to english .
the typical model size in terms of their abstract syntax graphs is nodes on average out of meta model classes are used as node types.
to evaluate the quality of our recommendations we conducted a semi structured interview with five domain experts of our industry partner system engineers working with one of the models system engineer working cross cutting chief system architect responsible for the product line approach and the head of the tool development team.
we presented them of our mined edit operations together with edit operations that were randomly generated out of the meta model.
the editoperations were presented in the visual transformation language of h enshin which we introduced to our participants before.
on a point likert scale we asked whether the edit operation represents a typical edit scenario is rather typical can make sense but is not typical is unlikely to exist and does not make sense at all .
we compare the distributions of the likert score for the population of random edit operations and mined edit operations to determine whether the mined operations are typical or meaningful.
in addition we discussed the mined edit operations with the engineers that have not been considered to be typical.
c. results experiment in table i we list the map k scores for all datasets in the experiment.
table iii shows the spearman correlation of the independent and dependent variables.
if we look only on datasets with a large number of applied edit operations e the spearman correlation for average precision vs. dand average precision vs. pbecomes .
instead of .
and .
instead of .
respectively.
the mean time for running g aston on our datasets was .17s per dataset.
experiment in table ii we give the map k scores for this experiment.
table iv shows the correlation matrix for the second experiment.
the mean time for running gaston on our datasets was .02s per dataset.
936table i themap k scores for the results using compression and frequency for experiment .
map map map map compression .
.
.
.
frequency .
.
.
.368table ii themap k scores for the results using compression and frequency for experiment .
map map map map compression .
.
.
.
frequency .
.
.
.
table iii spearman correla tions for experiment .
mean p mining e d nodes time per comp ap .
.
.
.
.
ap for e .
.
.
.
.
mining time .
.
.
.83table iv thespearman correla tion ma trix for experiment .
mean p size at mining e nodes threshold time per comp ap .
.
.
.
.
p .
.
.
size at threshold .
.
.
mining time .
.
e .
experiment table v shows the results for the likert values for the mined and random edit operations for the five participants of our study.
furthermore we conduct a t test and a wilcoxon signed rank test to test if the mined edit operationsmore likely present typical edit scenarios than the random ones.
the p values are reported in table v. null hypothesis h0 the mined edit operations do not present a more typical edit scenario than random edit operations on average.
we set the significance level to .
.
we can see that for all participants the mean likert score for the mined operations is significantly higher than the mean for the random operations.
we can therefore reject the null hypothesis.
table v sta tistics for the likert v alues of mined and random edit opera tions for experiment .
participant mean mean p value p value mined random t test wilcoxon p1 .
.
.
.
p2 .
.
.
.
p3 .
.
.
.
p4 .
.
.
.
p5 .
.
.
.
total .
.
.
.
after their rating when we confronted the engineers with the true results they stated that the edit operations obtained by ockham represent typical edit scenarios.
according to one of the engineers some of the edit operations can be slightly extended see also section vi .
some of the edit operations found by o ckham but not recognized by the participants where identified to be a one off refactoring that has been performed some time ago .
in this real world repository we found some operations that are typical to the modeling language sysml for example one which is similar to the simplified operation in figure .
we also found more interesting operations for example the additionof ports with domain specific port properties.
furthermore we were able to detect some rather trivial changes.
for example we can see that typically more than just one swimlane is added to an activity if any.
we also found simple refactorings such as renaming a package which also leads to changing the fully qualified name of all contained elements or also refactorings that correspond to changed conventions for example activities were owned by so called system use cases before but have been moved into packages.
vi.
d iscussion a. research questions rq isockham able to identify relevant edit operations in model repositories?
we can answer this question with a yes .
experiment and show high map scores.
onlyfor a large number of applied operations and a large size ofthe input graphs o ckham fails in finding the applied edit operations.
we can see that our compression based approach clearly outperforms the frequency based approach used as a baseline.
rq isockham able to find typical edit operations or editing scenarios in a real world setting?
the edit operations found by o ckham obtained significantly higher mean median likert scores than the random edit operations.
furthermore a mean likert score of almost .
shows.
from this we canconclude that compared to random ones our mined editoperations can be considered as typical edit scenarios on average.
when looking at the mined edit operations it becomes clear that o ckham is able to implicitly identify constraints which where not made explicit in the meta model.
the edit operations recommended by o ckham are correct in most cases and incomplete edit operations can be adjusted manually.
we cannot state yet that the approach is also complete i.e.
is able to find all relevant edit scenarios though.
rq what are the main drivers for ockham to succeed or fail?
from table iii we observe that increasing the number of edit operations has a negative effect on the average precision.
937table vi the main drivers for ockham to fail in detecting the correct subgraph in experiment .
mean p nodes size at mining per comp threshold time overall mean .
.
.
.
mean for undetected operation0.
.
.
.
increasing the perturbation has a slightly negative effect which becomes stronger for a high number of applied edit operations and therefore when huge connected components start to form.
the number of differences d i.e.
having more examples has a positive effect on the rank which is rather intuitive.
for the second experiment from table iv we can observe a strong dependency of the average precision on the perturbation parameter which is stronger than for the first experiment.
on the other hand the correlation to the number of applied edit operations is weaker.
to analyze the main drivers further we take a deeper look into the results.
we have to distinguish between the two cases that the correct edit operation is not detected at all and the correct edit operation has a low rank.
edit operation has not been detected for the second experiment in out of examples o ckham was not able to detect both edit operations.
in of these cases thethreshold has been set too high.
to mitigate this problem in a real world setting the threshold parameters could be manually adjusted until the results are more plausible.
in the automatic approach further metrics have to be integrated.
other factors that cause finding the correct edit operations to fail are the perturbation average size of component and the size at threshold as can be seen from table vi.
given a supportthreshold t the size at threshold is the number of nodes of thet largest component.
the intuition behind this metric is the following for the frequent subgraph miner in order to prune the search space a subgraph is only allowed to appear in at most t 1components.
therefore the subgraph miner needs to search for a subgraph at least in one component with size greater than the size at threshold.
usually the component size plays a major role in the complexity of the subgraph mining.
when the t largest component is small we could always use this component or smaller ones to guide the search through the search space and therefore we will not have a large searchspace.
so a large size of the component at threshold could be an indicator for a complicated dataset.
looked deeper into the results of the datasets from the first experiment for which the correct subgraph has not been identified we can see that for some of these subgraphs there is a supergraph in our recommendations that is top ranked.
usually this supergraph contains one or two additional nodes.
since we have a rather small meta model and we only use four other edit operations for the perturbation it can happen rarely that these larger graphs occur with the same frequency as the actual subgraph.
the correct subgraphs are then prunedtable vii possible drivers for a low rank .
mean d e p nodes size at average rank per comp threshold precision .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
away.
edit operation has a low rank first note that we observe a low rank rank only very rarely.
for the first experiment it happened in out of datasets while for the second experiment it did not happen at all.
in table vii we list the corresponding datasets and the values for drivers of a low rank.
one interesting observation is that for some of the datasets with low ranked correct subgraph we can see that the correct graph appears very early in the subgraph lattice for example first child of the best compressing subgraph but rank 99in the output or first child of the second best subgraph but rank in the output.
this suggests that this is more a presentation issue which is due to the fact that we have to select a linear order of all subgraph candidates for the experiment.
in experiment we only found two mined edit operations that received an average likert score below from the five practitioners in the interviews.
the first one was a refactoring that was actually performed but that targeted only a minority of all models.
only two of the participants where aware of this refactoring and one of them did not directly recognize it due to the abstract presentation of the refactoring.
the other edit operation that was also not considered as a typical edit scenario was adding a kind of document to another document.
this edit operation was even considered as illegal by out of the participants.
the reason for this is the internalmodeling of the relationship between the documents which the participants were not aware of.
so it can also be attributed to the presentation of the results in terms of henshin rules which require an understanding of the underlying modeling language s meta model.
for four of the edit operations of experiment some of the participants mentioned that the edit operation can be extended slightly.
we took a closer look at why o ckham was not able to detect the extended edit operation and it turned out that it was due to our simplifications of locality relaxation and also due to the missing type hierarchies in our graphs.
for example in one edit operation one could see that the fully qualified name name location in the containment hierarchy of some nodes has been changed but the actual change causing this name change was not visible because it was a renaming of a package a few levels higher in the containment hierarchy that was not directly linked to our change.
another example was a cut off referenced element in an edit operation.
the reason 938why this has been cut off was that the element appeared as different sub classes in the model differences and each single change alone was not frequent.
to summarize the main drivers for o ckham to fail are a large average size of components and the size at threshold.
the average size is related to the number of edit operations applied per model difference.
in a practical scenario huge differences can be excluded when running edit operation detection.
the sizeof the component at threshold can be reduced by increasing the support threshold parameters of the frequent subgraph mining.
with higher threshold we increase the risk of missing some less frequent edit operations but the reliability for detectingthe correct more frequent operations is increased.
having more examples improves the results of o ckham .
rq what are the main parameters for the performance of the frequent subgraph mining?
from table iii we ca observe a strong spearman correlation of the mining time with the number of applied edit operations e .
and implicitly also the average number of nodes per component .
.
if we only look at edit operations with rank we observe a strong negative correlation of .51with the average precision not shown in table iii .
this actually means that large mining times usually come with a bad ranking.
the same effect can be observed for experiment table iv .
we can also see that the mining time correlates with the size at threshold.
b. limitations locality relaxation one limitation of our approach is the locality relaxation which limits our ability to find patterns that are scattered across more than one connected component of the simple change graph.
as we have seen in our railway case study this usually leads to incomplete edit operations.
another typical example for violating the relaxation are naming conventions.
in the future we plan to use natural language processing techniques such as semantic matching to augment the models by further