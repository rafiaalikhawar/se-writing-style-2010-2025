typewriter neural type prediction with search based validation michael pradel university of stuttgart michael binaervarianz.degeorgios gousios delft university of technology g.gousios tudelft.nl jason liu jasonliu fb.com facebooksatish chandra schandra acm.org facebook abstract maintaining large code bases written in dynamically typed languages such as javascript or python can be challenging due to the absence of type annotations simple data compatibility errors proliferate ide support is limited and apis are hard to comprehend.
recent work attempts to address those issues through either static type inference or probabilistic type prediction.
unfortunately static type inference for dynamic languages is inherently limited while probabilistic approaches suffer from imprecision.
this paper presents typewriter the first combination of probabilistic type prediction with search based refinement of predicted types.
typewriter s predictor learns to infer the return and argument types for functions from partially annotated code bases by combining the natural language properties of code with programming languagelevel information.
to validate predicted types typewriter invokes a gradual type checker with different combinations of the predicted types while navigating the space of possible type combinations in a feedback directed manner.
we implement the typewriter approach for python and evaluate it on two code corpora a multi million line code base at facebook and a collection of popular open source projects.
we show that typewriter s type predictor achieves an f1 score of .
.
in the top top predictions for return types and .
.
for argument types which clearly outperforms prior type prediction models.
by combining predictions with search based validation typewriter can fully annotate between to of the files in a randomly selected corpus while ensuring type correctness.
a comparison with a static type inference tool shows that typewriter adds many more non trivial types.
typewriter currently suggests types to developers at facebook and several thousands of types have already been accepted with minimal changes.
introduction dynamically typed programming languages such as python and javascript have become extremely popular and large portions of newly written code are in one of these languages.
while the lack of static type annotations enables fast prototyping it often leads to problems when projects grow.
examples include type errors that remain unnoticed for a long time suboptimal ide support and difficult to understand apis .
to solve these problems in recent years many dynamic languages obtained support for type annotations which enable programmers to specify types in a fashion similar to a statically typed language.
type annotations work performed while on sabbatical at facebook menlo park.are usually ignored at runtime nevertheless they serve both as hints for developers using external apis and as inputs to gradual type checkers that ensure that specific programming errors cannot occur.
to cope with legacy code bases type annotations can be introduced gradually in such cases the type checker will check only code that is annotated.
as manually annotating code is time consuming and error prone developers must resort to automated methods.
one way to address the lack of type annotations is type inference via traditional static analysis.
unfortunately dynamic features such as heterogeneous arrays polymorphic variables dynamic code evaluation and monkey patching make static type inference a hard problem for popular dynamic languages such as python or javascript .
static type inference tools typically handle these challenges by inferring a type only if it is certain or very likely under some assumptions which significantly limits the number of types that can be inferred.
motivated by the inherent difficulties of giving definitive answers via static analysis several probabilistic techniques for predicting types have been proposed.
a popular direction is to exploit the existence of already annotated code as training data to train machine learning models that then predict types in not yet annotated code.
several approaches predict the type of a code entity e.g.
a variable or a function from the code contexts in which this entity occurs .
other approaches exploit natural language information embedded in source code e.g.
variable names or comments as a valuable source of informal type hints .
while existing approaches for predicting types are effective in some scenarios they suffer from imprecision and combinatorial explosion .
probabilistic type predictors are inherently imprecise because they suggest one or more likely types for each missing annotation but cannot guarantee their correctness.
the task of deciding which of these suggestions are correct is left to the developer.
because probabilistic predictors suggest a ranked list of likely types choosing a type correct combination of type annotations across multiple program elements causes combinatorial explosion.
a na ve approach would be to let a developer or a tool choose from all combinations of the predicted types.
unfortunately this approach does not scale to larger code examples because the number of type combinations to consider is exponential in the number of not yet annotated code entities.
this paper presents typewriter a combination of learning based probabilistic type prediction and a feedback directed search based validation of predicted types.
the approach addresses the imprecision problem based on the insight that a gradual type checker canarxiv .03768v2 mar 2020michael pradel georgios gousios jason liu and satish chandra predicted argument type int str bool predicted return type str optional none 3def find match color args color str color to match on and return candidates get colors for candidate incandidates ifcolor candidate return color return none predicted return types list list str def get colors return figure example of search for type correct predicted types.
pinpoint contradictory type annotations which guides the selection of suitable types from the set of predicted types.
to make the search for a consistent set of types tractable we formulate the problem as a combinatorial search and present a search strategy that finds type correct type annotations efficiently.
typewriter makes use of the variety of type hints present in code through a novel neural architecture that exploits both natural language in the form of identifier names and code comments similar to prior work and also programming context in the form of usage sequences.
to illustrate the approach consider the two to be typed functions in figure .
given this code the neural type model of typewriter predicts a ranked list of likely types for each argument type and return type as indicated by the comments.
typewriter starts by adding the top ranked predictions as type annotations which introduces a type error about an incorrect return type of find match though.
based on this feedback the search tries to change the return type of find match to the second best suggestion optional .
unfortunately this combination of added types leads to another type error because the return type is inconsistent with the argument keybeing of type int.
the search again refines the type annotations by trying to use the second best suggestion str for the argument key.
because the resulting set of type annotations is type correct according to the type checker typewriter adds these types to the code.
we implement typewriter for python and apply it on two large code bases a multi million line code base at facebook that powers applications used by billions of people and a corpus of popular opensource projects.
we show that the neural model predicts individual types with a precision of and a recall of within the top top top predictions which outperforms a recent closely related approach by and respectively.
based on this model the feedback directed search finds a typecorrect subset of type annotations that can produce complete and type correct annotations for to of all files.
comparing typewriter with a traditional static type inference shows that both techniques complement each other and that typewriter predicts many more types than traditional type inference.
in summary this paper makes the following contributions a combination of probabilistic type prediction and searchbased validation of predicted types.
the feedback directed search for type correct types can be used with any probabilistic type predictor and any gradual type checker.
a novel neural type prediction model that exploits both code context and natural language information.
empirical evidence that the approach is effective for typeannotating large scale code bases with minimal human effort.
the initial experience from using typewriter at facebook on a code base that powers tools used by billions of people has been positive.
approach figure gives a high level overview of the typewriter approach.
the input to typewriter is a corpus of code where some but not all types are annotated.
the approach consists of three main parts.
first a lightweight static analysis extracts several kinds of information from the given code section .
.
the extracted information includes programming structure information such as usages of a function s arguments and natural language information such as identifier names and comments.
next a neural type predictor learns from the already annotated types and their associated information how to predict missing types section .
.
once trained this model can predict likely types for currently unannotated parts of the code.
finally a feedback directed search uses the trained model to find a type assignment that is consistent and type correct according to a static gradual type checker section .
.
the overall output of typewriter is code with additional type annotations.
.
static extraction of types and context information the first part of typewriter is an ast based static analysis that extracts types and context information useful to predict types.
the analysis is designed to be lightweight and easy to apply to other programming languages.
we currently focus on function level types i.e.
argument types and return types.
these types are particularly important for two reasons i given function level types gradual type checkers can type check the function bodies by inferring the types of some local variables.
ii function level types serve as interface documentation.
for each type the static analysis gathers four kinds of context information which the following describes and illustrates with the example in figure .
each of the four kinds of information may provide hints about an argument type or return type and our model learns to combine these hints into a prediction of the most likely types.
identifier names associated with the to be typed program element.
as shown by prior work natural language information embedded in source code can provide valuable hints about program properties.
for example the argument names name anddo propagate in figure suggest that the arguments may be a string and a boolean respectively.
to enable typewriter to benefit from such hints the static analysis extracts the identifier name of each with function and each function argument.typewriter neural type prediction with search based validation python code ast based extraction word embeddingst oken embeddingsidentifier rnn code rnn word rnnhidden layer softmax type vector vector vector vector vectoridentifierscode tokens comments available typesstatic type checker feedbackdirected search static analysis neural type predictionsearch for consistent typespython code with additional type annotations figure overview of typewriter.
1from html import htmlelement 3def update name name do propagate element update the name and optionally propagate to dependents.
first name name.split element.first first name ifdo propagate for dindependents d.notify name first name figure example of a to be typed python function.
code occurrences of the to be typed program element.
in addition to the above natural language information typewriter exploits programming language type hints.
one of them is the way a to betyped program element is used as a hint about argument types the analysis considers all usages of an argument within the function body.
another kind of information is code that defines the to betyped program element as a hint about return types the analysis considers all return statements in a function.
for each of these code locations the analysis extracts the corresponding sequence of code tokens o1 ... ok. specifically the analysis extracts a window of tokens around each occurrence of an argument default size of window and all tokens of a return statement.
for example the analysis extracts the token sequence n first name name .
split around the usage of name at line .
as an alternative to extracting token sequences typewriter could perform a more sophisticated static analysis e.g.
by tracking data flows starting at arguments or ending in return values.
we instead focus on token sequences because it provides a sufficiently strong signal scales well to large code bases and could be easily ported to other programming languages.
function level comments.
similar to identifier names comments are another informal source of hints about types.
for the example in figure a developer might infer from the function level comment that the function has some side effects but probably does not return any value.
to allow the approach to benefit from such hints the static analysis extracts all function level comments i.e.
docstrings in python.
for a given function the approach uses this commentboth for predicting the argument types and the return type of the function.
available types.
to annotate a type beyond the built in types of python the type needs to be either imported or locally defined.
because types used in an annotation are likely to be already imported or locally defined the analysis extracts all types available in a file.
to this end the analysis parses all import statements and all class definitions in a file.
for the example in figure the analysis will extract htmlelement as an available type which hints at the argument element being of this type.
based on these four kinds of type hints the analysis extracts the following information for argument types and return types respectively definition .
argument type information .
for a function argument a the statically extracted information is a tuple nfct narg nargs c u t where nfctis the function name nargis the name of the argument a nargsis the sequence of names of other arguments if any c is the comment associated with the function uis a set of usage sequences each of which is a sequence o1 ... okof tokens and tis the type of the argument.
definition .
return type information .
for the return type of a function f the statically extracted information is a tuple nfct nargs c r t where nfctis the function name nargsis the sequence of argument names cis the comment associated with the function ris a set of return statements each of which is a sequence o1 ... okof tokens andtis the return type of f. if any of the above information is missing the corresponding elements of the tuple is filled with a placeholder.
in particular the static analysis extracts the above also for unannotated types to enable typewriter to predict types based on the context.
.
neural type prediction model given the extracted types and context information the next part of typewriter is a neural model that predicts the former from themichael pradel georgios gousios jason liu and satish chandra latter.
we formulate the type prediction problem as a classification problem where the model predicts a probability distribution over a fixed set of types.
the neural type prediction model summarized in the middle part of figure combines the four kinds of information described in section .
into a single type prediction.
to represent identifier names source code tokens and words in a way suitable for learning typewriter maps each into a realvalued vector using a word2vec embedding.
we train two embeddings a code embedding ecode for code tokens and identifier names and a word embedding eword for words in comments.
ecode is trained on sequences of tokens extracted from source code files while eword is trained on sequences of words extracted from comments.
to mitigate the problem of large vocabularies in source code typewriter preprocesses each identifier using a helper function norm which tokenizes lemmatizes and lowercases each identifier.
.
.
learning from identifiers.
this neural submodel learns from the identifier names of functions and function arguments.
the model combines all identifiers associated with a type into sequence.
given argument type information nfct narg nargs c u t the sequence is norm narg s norm nfct norm nargs where flattens and concatenates sequences and sis a separator.
given return type information nfct nargs c r t the sequence is norm nfct s norm nargs for example the sequence for the return type of the function in figure is update name sname do propagate element .
typewriter learns from these sequences of words by summarizing them into a single vector using a bi directional recurrent neural network rnn based on lstm cells.
to ease parallelization we pad sequences that are too short and truncate sequences that are too long default length .
the final hidden states of the rnn serve as a condensed vector representation vids of all identifier related hints.
.
.
learning from token sequences.
this neural submodel learns from source code information associated with a type.
similar to the submodel for identifiers this submodel composes all relevant tokens into a sequence and summarize them into a single vector vcodeusing an rnn.
for arguments and return types the sequence consists of the tokens involved in the usages u definition .
and the return statements r definition .
respectively.
before feeding these sequences into an rnn we bound the length of each token sequence default k and of the number of token sequences default .
.
.
learning from comments.
this neural submodel learns type hints from comments associated with a function.
to this end typewriter splits a given comment into a sequence of words bound the length of the sequence to a fixed value default and summarizes the sequence via another rnn.
the result is a fixed length vector vcomments .
.
.
learning from available types.
the fourth kind of information that typewriter learns from is the set of types available in thecurrent source code file.
the approach assumes a fixed size vocabularytof types default size .
this vocabulary covers the vast majority of all type occurrences because most type annotations either use one of the built in primitive types e.g.
strorbool common non primitive types e.g.
list ordict or their combinations e.g.
list ordict .
any types beyond the type vocabulary are represented as a special unknown type.
to represent which types are available we use a binary vector of sizet called the type mask .
each element in this vector represents one type and an element is set to one if and and only if its type is present.
the resulting vector vavailtypes of available types is passed as is into the final part of the neural model.
.
.
predicting the most likely type.
the final submodel concatenates the four vectors vids vcode vcomments and vavailtypes into a single vector and passes it through a fully connected layer that predicts the most likely type.
the output vector has size t and represents a probability distribution over the set of types.
for example suppose the type vocabulary had only four types int bool none and list and that the output vector is .
in this case the model would predict that bool is the most likely type following by none .
there are two ways to handle uncertainty and limited knowledge in the model.
first we interpret the predicted probability of a type as a confidence measure and only suggest types to a user that are predicted with a confidence above some configurable threshold.
second we encode types not included in the fixed size type vocabulary as a special unknown type.
the model hence learns to predict unknown whenever none of the types in the vocabulary fit the given context information.
during prediction typewriter never suggests the unknown type to the user but instead does not make any suggestion in case the model predicts unknown .
.
.
training.
to train the type prediction model typewriter relies on already type annotated code.
given such code the approach creates one pair of context information and type for each argument type and for each return type.
these pairs then serve as training data to tune the parameters of the different neural submodels.
we use stochastic gradient descent the adam optimizer and crossentropy as the loss function.
the entire neural model is learned jointly enabling the model to summarize each kind of type hint into the most suitable form and to decide which type hints to consider for a given query.
we train two separate models for argument types and function types each learned from training data consisting of only one kind of type.
the rationale is that some of the available type hints need to be interpreted differently depending on whether the goal is to predict an argument type or a return type.
.
feedback guided search for consistent types the neural type prediction model provides a ranked list of kpredictions for each missing type annotation.
given a set of locations for which a type annotation is missing called type slots and a list of probabilistic predictions for each slot the question is which of the suggested types to assign to the slots.
a na ve approach might fill each slot with the top ranked type.
however because the neural model may mis predict some types this approach may yield typetypewriter neural type prediction with search based validation algorithm find a correct type assignment for a file f function assign types f t all type slots in f p1.
.k t predictions t k t t topkpredictions a p1 t t t initial type assignment a.score typecheck a f feedback function work set new states a p t done a while min x.score x done work set do a pick work set biased random selection a.score typecheck a f ifgreedy a.score a.parent .score then work set new states a p t else if non greedy then work set work set new states a p t done end if done done a end while return argmin x.score x done end function function new states a p t children for all t tdo for allpj twhere j rank of current a do achild modify ato usepj tatt children achild end for achild modify ato not use any type at t children achild end for return children end function assignments where the added annotations are not consistent with each other or with the remaining program.
to avoid introducing type errors typewriter leverages an existing gradual type checker as a filter to validate candidate type assignments.
such type checkers exist for all popular dynamically typed languages that support optional type annotations e.g.
pyre andmypy for python and flow for javascript.
typewriter exploits feedback from the type checker to guide a search for consistent types as presented in algorithm and explained in the sections below.
.
.
search space.
given a set tof type slots and kpredicted types for each slot we formulate the problem of finding a consistent type assignment as a combinatorial search problem.
the search space consists of the set pof possible type assignments .
for t type slots and kpossible types for each slot there are k t type assignments the 1is for not assigning any of the predicted types .
.
.
feedback function.
exhaustively exploring pis practically infeasible for files with many missing types because invoking the gradual type checker is relatively expensive typically in the orderof several seconds per file .
instead typewriter uses a feedback function typecheck to efficiently steer toward the most promising type assignments.
the feedback function is based on two values both of which the search wants to minimize nmissing the number of missing types.
nerrors the number of type errors.
typewriter combines these into a weighted sum score v nmissing w nerrors .
by default we set vto1andwto the number of initially missing types plus one which is motivated by the fact that adding an incorrect type often leads to an additional error.
by giving type errors a high enough weight we ensure that the search never returns a type assignment that adds type errors to the code.
.
.
exploring the search space.
typewriter explores the space of type assignments through an optimistic search strategy algorithm .
it assumes that most predictions are correct and then refines type annotations to minimize the feedback function.
each exploration step explores a state a which consists of a type assignment the score computed by the feedback function and a link to the parent state.
the initial state is generated by retrieving the top predictions frompfor each type slot tand invoking the feedback function lines and .
the next states to be explored are added to a work set while the explored states are kept in the done set.
the algorithm loops over items in the work set until either the feedback score has been minimized or the search explored all potential type assignments line .
the assignment with the minimal score is returned as a result line .
to retrieve the next type assignments to possibly explore from the current state typewriter invokes the new states helper function.
it adds all type assignments that can be obtained from the current state by modifying exactly one type slot either by using a lower ranked type suggestion or by not adding any type for this slot lines to .
the main loop of the algorithm lines to picks a next state to evaluate from the working set line queries the feedback function line and updates the done set with the explored state line .
the pick function is a biased random selection that prefers states based on two criteria.
first it prefers states that add more type annotations over states that add fewer annotations.
second it prefers states that modify a type annotation at a line close to a line with a type error.
intuitively such states are more likely to fix the cause of a type error than a randomly selected state.1the working set is updated with all new states that have not been currently explored.
typewriter implements two variants of the search a greedy and a non greedy one.
the greedy strategy aggressively explores children of type assignment that decrease the feedback score and prunes children of states that increase it line .
the non greedy performs no pruning i.e.
it can explore a larger part of the search space at the expense of time line .
as an optimization of algorithm typewriter invokes the assign types function twice.
the first call considers only type slots for return types whereas the second call considers all type slots for argument types.
the reason for this two phase approach is that 1the reason for relying on line numbers as the interface between the type checker and typewriter is to enable plugging any type checker into our search.michael pradel georgios gousios jason liu and satish chandra many gradual type checkers including pyre the one used in our evaluation type check a function only if its return type is annotated.
if typewriter would add argument type annotations before adding return type annotations the feedback function might not include all type errors triggered by an incorrectly added argument annotation.
implementation the implementation of typewriter builds upon a variety of tools in the python ecosystem.
for the static analysis phase we apply a data extraction pipeline consisting of python s own astlibrary to parse the code into an ast format and nltk and its wordnetlemmatizer module to perform standard nlp tasks lemmatization stop word removal .
the pipeline is parallelized so that it handles multiple files concurrently.
the neural network model is implemented in pytorch.
for obtaining embeddings for words and tokens we pretrain a word2vec model using the gensim library.
the search phase of typewriter builds upon the libcst2to add types to existing python files.
we use pyre for static type checking.
our lstm models all use dimensional hidden layers and we train for epochs with a learning rate of .
using the adam optimizer.
evaluation we structure our evaluation along four research questions.
rq how effective is typewriter s model at predicting argument and return types and how does it compare to existing work?
rq how much do the different kinds of context information contribute to the model s prediction abilities?
rq how effective is typewriter s search?
rq how does typewriter compare to traditional static type inference?
.
datasets typewriter is developed and evaluated within facebook.
as the internal code base is not publicly available and to ensure that the presented results are replicable we use two datasets internal code base we collect python from a large internal code repository.
oss corpus we search github for all projects tagged as python3 .
we also search libraries.io for all python projects that include mypy as a dependency.
we then remove all projects that have less than stars on github to ensure that the included projects are of substantial public interest.
to ease future work to compare with typewriter all results for the oss corpus are available for download.
the resulting dataset statistics can be found in table .
the internal dataset is much larger in size but both datasets are comparable in terms of the percentage of annotated code.
by restricting the type vocabulary to a fixed size we exclude around of all type occurrences for both datasets.
this percentage is similar for both datasets despite their different sizes because types follow a longtail distribution i.e.
relatively few types account for the majority of all type occurrences.
we ignore some types because they are trivial internal and open source datasets.
metric internal oss repositories files lines of code .7m functions .
.
.
with return type annotation .
.
.
with comment .
.
.
.
.
with both .
.
.
.
ignored because trivial .
.
arguments .
.
.
with type annotation .
.
.
ignored because trivial types return .
.
.
occurrences ignored out of vocab.
.
.
types argument .
.
.
occurrences ignored out of vocab.
.
.
training time min sec .
.
.
parsing several minutes .
.
.
training embeddings several minutes .
.
.
training neural model several minutes not available for disclosure to predict such as the return type of str which always is str or the type of the self argument of a method which always is the surrounding class.
typewriter could easily predict many of these trivial types but a simple syntactic analysis would also be sufficient.
we ignore trivial types for the evaluation to avoid skewing the results in favor of typewriter.
.
examples figure shows examples of successful and unsuccessful type predictions in the oss dataset.
example presents a case where typewriter correctly predicts a type annotation.
here the code context and comments provide enough hints indicating that token is of type callable .
example presents a case where typewriter does not correctly predict the type but the prediction is close to what is expected.
we hypothesize that this case of mis prediction is due to the fact that typewriter tries to associate associations between natural language and types or in this case the word path and the type path .
.
rq effectiveness of the neural model prediction tasks.
to evaluate the neural type prediction we define two prediction tasks i returnprediction where the model predicts the return types of functions and ii argumentprediction where the model predicts the types of function arguments and metrics.
we evaluate the effectiveness of typewriter s neural type predictor by splitting the already annotated types in a given dataset into training and validation data.
the split is by file to avoid mixing up types within a single file.
once trained on the training data we compare the model s predictions against the validation data using the already annotated types as the groundtypewriter neural type prediction with search based validation table effectiveness of neural type prediction.
corpus task model precision recall f1 score top top top top top top top top top internal returnprediction typewriter .
.
.
.
.
.
.
.
.
nl2type .
.
.
.
.
.
.
.
.
deeptyper .
.
.
.
.
.
.
.
.
na ve baseline .
.
.
.
.
.
.
.
.
argumentprediction typewriter .
.
.
.
.
.
.
.
.
nl2type .
.
.
.
.
.
.
.
.
deeptyper .
.
.
.
.
.
.
.
.
na ve baseline .
.
.
.
.
.
.
.
.
oss returnprediction typewriter .
.
.
.
.
.
.
.
.
nl2type .
.
.
.
.
.
.
.
.
deeptyper .
.
.
.
.
.
.
.
.
na ve baseline .
.
.
.
.
.
.
.
.
argumentprediction typewriter .
.
.
.
.
.
.
.
.
nl2type .
.
.
.
.
.
.
.
.
deeptyper .
.
.
.
.
.
.
.
.
na ve baseline .
.
.
.
.
.
.
.
.
prefecthq ct f blob master src prefect utilities notifications.py commit 864d44b successful annotation of return type def callback factory ... callable ... returns state handler callable a state handler function that can be attached to both tasks and flows ... def state handler ... ... return state handler example awslabs sockeye blob master sockeye average.py commit bcda569 incorrect annotation of return type expected list def find checkpoints ... list ... return list of paths corresponding to chosen checkpoints.
... params paths os.path.join model path c.params name point for point intop n ... return params paths example figure examples of successful and unsuccessful type predictions github prefecthq ct awslabs sockeye .
truth.
we compute precision recall and f1 score weighted by the number of type occurrences in the dataset.
similarly to previous work if the prediction model cannot predict a type for a type slot i.e.
returns unknown we remove this type slot from the calculation of precision.
specifically we calculate precision asprec ncor r nall where ncorr is the number of correct predictions andnallis the number of type slots for which the model does not return unknown .
we calculate recall as rec ncor r d where d is total number of type slots in the examined dataset.
we report the top kscores for k .
baseline models.
we compare typewriter s top kpredictions against three baseline models.
the na ve baseline model considers the ten most frequent types in the dataset and samples its prediction from the distribution of these ten types independently of the given context.
for example it predicts none as a return type more often than list because none is used more often as a return type than list .
the deeptyper baseline is a python re implementation of the deeptyper model.
deeptyper learns to translate a sequence of source code tokens to a sequence of types and zeros for tokens without a type .
to make it directly compatible with typewriter we do not consider predictions for variable annotations in function bodies even though we do perform basic name based type propagation in case an annotated argument is used in a function body.
finally the nl2type baseline is a reimplementation of the nl2type model for python which also learns from natural language information associated with a type but does not consider code context or available types.
results.
table presents the results for rq .
our neural model achieves moderate to high precision scores e.g.
in the top and in the top for on the internal dataset for the returnprediction task.
the recall results are good but less high than precision indicating that typewriter is fairly confident when it makes a prediction but abstains from so when it is not.
all models have slightly worse performance on the oss dataset which we attribute to the smaller size of that dataset.
the fact that the top and top scores are significantly higher than top in all cases motivates our work on combinatorial search section .
.
compared to the baselines typewriter outperforms both the state of the art and the na ve baseline across all metrics for both datasets and all three prediction tasks.
the differences betweenmichael pradel georgios gousios jason liu and satish chandra returnprediction argumentpredictioninternal oss .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
precisionrecall configuration typewriter typewriter typemasktypewriter token seqs typewriter namestypewriter documentation nl2type figure precision recall curves for different typewriterconfigurations.
each data point represents a prediction threshold level.
typewriter and nl2type are higher in the case of the returnprediction than the argumentprediction task.
the context information as obtained by analyzing token sequences is helping the typewriter prediction model more in the returnprediction task.
compared to deeptyper both nl2type and typewriter are better the latter by a significant margin in top but not in top or top predictions.
given that all models learn primarily from identifier names the relatively close upper bound performance seems to indicate that performance improvements may only be achieved by introducing different e.g.
structural information to the model.
.
rq comparison with simpler variants of the neural model the main novelty of typewriter s prediction component is the inclusion of code context information and a local type mask in the prediction model.
to explore the influence of the different type hints considered by typewriter we perform an ablation study.
specifically we turn off parts of the model both in training and in testing and then measure top precision and recall at various prediction threshold levels.
we start with the full model typewriter and then we remove in order the type mask the token sequences the method and argument names and the documentation.
as a baseline we also include nl2type a configuration functionally equivalent with nl2type which corresponds to typewriter without token sequences and without a type mask.
the results of the ablation study can be seen in figure .
overall the combined information of natural language token sequences and type masks helps typewriter to perform better than previous models such as nl2type.
the main contributor tothis improvement is the token sequences component.
moreover the results seem to reinforce the main thesis of nl2type i.e.
that natural language information and types are strongly related if we remove the argument and function naming information from typewriter its performance drops significantly.
contrary to our initial expectations the type mask component is not contributing significantly in the returnprediction task while only slightly improving the argumentprediction results.
we attribute this to the current implementation of the type mask data extraction process the extractor currently neither performs an in depth dependency resolution to retrieve the full set of types available in the processed file s name space nor does it track type renamings e.g.
import pandas as pd .
the low predictive capability of comments can be explained by the fact that only a small number of the methods in both datasets have documentation at the method level.
.
rq effectiveness of search to evaluate the search we collect a ground truth of fully annotated files that are randomly sampled from the industrial code base at facebook.
we ensure that they type check correctly.
the files we select originate from different products and vary in size and complexity the files average median annotations.
the total number of annotations is .
for each file in the ground truth we strip its existing annotations and then apply typewriter to predict and evaluate the missing types.
we configure both the greedy and the non greedy search strategies to stop when the number of states explored is seven times the number of type slots.
this threshold empirically strikes a reasonable balance between investing time and detecting correct types.
we use the same prediction model trained on the facebook dataset as in section .
.
table shows the results on two levels individual type annotations and files.
on the annotation level column type correct shows how many type slots the type assignment returned by the search fills recall that the search ensures each added type to be typecorrect .
column ground truth match shows how many of all added annotations match the original developer produced type annotations.
on the file level a complete and type correct solution is a file that typewriter fully annotates without type errors.
this metric does not include files where typewriter discovers a type correct but partially annotated solution.
the ground truth match is the subset of the complete and type correct solutions where the solution is identical to the ground truth for all types in the file.
it is possible to find a type correct annotation that does not match the ground truth.
for example typewriter may correctly annotate the return type of a function as a list but a human expert might choose a more precise type list both are type correct but the human annotation provides more guarantees.
both search strategies successfully annotate a significant fraction of all types.
on the annotation level they add between and of all types in a type correct way out of which to match the ground truth depending on the search strategy.
on the filelevel typewriter completely annotates to of all files and to of all files perfectly match the developer annotations.
comparing the two search strategies we find that at the annotationlevel greedy search discovers more type correct annotations withtypewriter neural type prediction with search based validation table effectiveness of various search strategies for type inference.
strategy top kannotations files typecorrectground truth matchcomplete typecorrectground truth match greedy search1 nongreedy search1 upper bound prediction pyre infer top and top predictions while non greedy search actually finds fewer annotations.
this is due to the exponential increase in search space which makes it less likely that the non greedy search finds a type correct solution.
in contrast the results suggest that the greedy search explores a more promising part of the search space.
at the file level both search approaches provide more annotations and fully annotate more files as the number of available predictions per slot increases.
in the greedy case a search using the top results still improves the outcome significantly this suggests the search strategy can efficiently leverage the neural model s moderate improvement when kincreases beyond .
to better understand how effective the search is we also show how many ground truth matching types the top kpredictions include upper bound prediction .
note that these numbers are a theoretical upper bound for the search which cannot be achieved in practice because it would require exhaustively exploring all combinations of types predicted within the top k. comparing the upper bound with the results of the search shows that the search gets relatively close to the maximum effectiveness it could achieve.
for example a top 5exploration with greedy search finds a complete and type correct solution that matches the ground truth for files while the theoretical upper bound is files.
we leave developing further search strategies e.g.
based on additional heuristics for future work.
overall the results show that a greedy search among top ktypes can uncover more types when given more predictions while also maintaining type correctness.
k 5provides the best annotation performance.
while a non greedy search should not immediately be disregarded it should be considered in terms of how exhaustive the developer will allow the search to be.
.
rq comparing with static type inference we compare typewriter with a state of the art static type inference toolpyre infer .
the type inference is part of the pyre type checker and is representative of conservative static analysis based type inference that adds only types guaranteed to be type correct.
we runpyre infer on the same set of randomly chosen fully annotatedtable comparison of typewriter and a traditional static type inference pyre infer .
top greedy top non greedy total type slots .
.
.
added by typewriter only .
.
.
added by pyre infer only .
.
.
added by both tools .
.
.
same prediction .
.
.
neither could predict 40nonestrintlist optional experimentboollist floatgeneratortop uni00a010 uni00a0types uni00a0annotated uni00a0by uni00a0pyre uni00a0infer0 40strnoneboolintlist anyoptional experimentdatelist top uni00a010 uni00a0types uni00a0annotated uni00a0by uni00a0typewriter figure distribution of types found by typewriter and pyre infer.
files as in section .
and then compare the added annotations with typewriter s top search results.
tables bottom and show the results.
in a head to head comparison typewriter is able to provide type correct predictions for about seven times the number of files that pyre infer can.
it also discovers significantly more types adding a total of 188types whereas pyre infer adds only .
additionally of the 82type slots for which both tools suggest a type the suggestions are the same in63cases.
effectively the types that typewriter suggests are a superset of those inferred by pyre infer aspyre infer does not uniquely find many types.
to further illustrate the differences we plot the distribution of the top correctly predicted types in figure .
we see that pyre infer can infer more precise types but the majority of its inferences are on methods with no return types.
moreover some of the inferred types are of dubious usefulness e.g.
optional indicating the difficulty of applying static type inference on dynamicallytyped languages and reinforcing our thesis on the value of predictionbased type inference.
discussion effectiveness of neural type prediction.
typewriter implements the first neural type prediction model for python.
as all existingmichael pradel georgios gousios jason liu and satish chandra type prediction models target javascript code it is difficult to draw conclusions as to whether the typewriter architecture is the best for the task.
two facts seem to suggest so i typewriter is better by a comfortable margin than a re implementation of the two best in class javascript models and ii typewriter s performance is stable across two very different datasets.
type correctness vs. soundness.
due to the way current python type checkers work the types that typewriter produces are guaranteed to be type correct within the context of a given module.
type correctness is different from type soundness as the later can only be verified using human intuition.
this means that if a module is used within another context the type checker might invalidate an initially correct prediction.
in turn this makes typewriter a soundy rather than a sound approach.
limited type vocabulary.
typewriter only predicts types that are part of its type vocabulary.
when the vocabulary size is configured at types it can account for of the available types in both our datasets.
however as software evolves developers create new types or change the names of existing ones.
this may lead to situations where the model would predict a wrong type because its name changed or because it simply does not know that the type exists.
the out of vocabulary problem is well know in software engineering research .
recent work for by karampatsis et al.
uses sub word information to account for neologisms with very good results.
we believe that typewriter would benefit significantly from such an approach for embedding identifier names as it would enable it to learn semantically similar name variants e.g.
abstractclass andclass orlist andlist .
further improvements.
typewriter is a prototype stemming from a general effort within facebook to make their python code base more robust.
typewriter can be improved in several dimensions some of which are presented below better data the ablation study results suggest that type masks and documentation components of the typewriter model are only marginally contributing to its prediction capabilities.
this goes against both intuition and published work in the authors show that code documentation is an important signal.
we could however exploit the fact that highly used libraries such as flask or the python standard library itself feature both type annotations in thetypeshed4repository and excellent documentation.
moreover we can obtain better type masks using lightweight dependency analysis such as importlab 5to identify all types that are in context.
faster search feedback typewriter s execution speed is currently constrained by the type checker used to obtain feedback.
one natural way to improve this would be to integrate the typewriter type predictor into a static type inference loop when the type inference cannot predict a type for a location it can ask the neural model for a suggestion.
while the theoretical cost of searching for types is similar in practice the type inference will be able to quickly examine suggested types given that all required data is loaded in memory.
reinforced learning as with most neural models typewriter can benefit from more data.
one idea worth exploring is to apply 4github python typeshed in batches consisting of application of an initial set of neural predictions reviewing proposed types through the normal code review process at facebook and then retrain the model on the new data.
at the scale of the facebook code base we expect that the feedback obtained accepted modified and rejected suggestions could be used to improve the learning process.
related work type inference for dynamic languages.
static type inference computes types using e.g.
abstract interpretation or type constraint propagation.
these approaches are sound by design but due to the dynamic nature of some languages they often infer only simple or very generic types .
they also require a significant amount of context usually a full program and its dependencies.
dynamic type inference tracks data flows between functions e.g.
while executing a program s test suite.
these approaches capture precise types but they are constrained by coverage.
typewriter differs from those approaches in two key aspects i it only requires limited context information i.e.
a single a source code file ii it does not require the program to be executed and hence can predict types in the absence of a test suite or other input data.
probabilistic type inference.
the difficulty of accurately inferring types for dynamic programming languages has led to research on probabilistic type inference.
jsnice models source code as a dependency network of known e.g.
constants api methods and unknown facts e.g.
types it then mines information from large code bases to quantify the probability of two items being linked together.
xu et al.
predict variable types based on a probabilistic combination of multiple uncertain type hints e.g.
data flows and attribute accesses.
they also consider natural language information but based on lexical similarities of names and focus on variable types whereas typewriter focuses on function types.
deeptyper uses a sequence to sequence neural model to predict types based on a bi lingual corpus of typescript and javascript code.
nl2type uses natural language information.
our evaluation directly compares with python re implementations of both deeptyper and nl2type.
besides advances in the probabilistic type prediction model itself the more important contribution of our work is to address the imprecision and combinatorial explosion problems of probabilistic type inference.
in principle any of the above techniques can be combined with typewriter s search based validations to obtain type correct types in reasonable time.
type checking and inference for python.
the python community introduced a type annotation syntax along with a type checker mypy as part of python .
version in .
the combination of the two enables gradual typing of existing code where the type checker checks only the annotated parts of the code.
similar approaches have also been explored by the research community .
since type annotations have seen adoption in several large scale python code bases with products such as dropbox6and instagram 7reportedly having annotated large parts of their multimillion line code bases.
typewriter helps reduce the manual effort required for such a step.
6dropbox blog how we rolled out one of the largest python migrations ever 7instagram engineering blog introducing open source monkeytypetypewriter neural type prediction with search based validation machine learning on code.
our neural type prediction model is motivated by a stream of work on machine learning based program analyses .
beyond type prediction others have proposed learning based techniques to find programming errors predict variable and method names suggest how to improve names search code detect clones classify code predict code edits predict assertions and automatically fix bugs .
typewriter contributes a novel model for predicting types and a search based combination of predictive models with traditional type checking.
search based software engineering.
our search based validation of types fits the search based software engineering theme which proposes to balance competing constraints in developer tools through metaheuristic search techniques.
in our case the search balances the need to validate an exponential number of combinations of type suggestions with the need to efficiently annotate types.
conclusions we present typewriter a learning based approach to the problem of inferring types for code written in python.
typewriter exploits the availability of partially annotated source code to learn a type prediction model and the availability of type checkers to refine and validate the predicted types.
typewriter s learned model can readily predict correct type annotations for half of the type slots on first try whereas its search component can help prevent annotating code with wrong types.
combined the neural prediction and the search based refinement helps annotate large code bases with minimal human intervention making typewriter the first practically applicable learning based tool for type annotation.
we are currently in the process of making typewriter available to developers at facebook.
we have tested the automation of the tool in the code review domain.
developers at facebook received type suggestions as comments on pull requests they had authored.
they would also receive pull requests containing type suggestions for their project.
the initial experience from applying the approach on a code base that powers tools used by billions of people has been positive several thousands of suggested types have already been accepted with minimal changes.