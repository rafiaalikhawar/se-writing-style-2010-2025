correlationsbetween deep neuralnetwork modelcoverage criteriaand model quality shenao yan shenao.yan rutgers.edu rutgers university usaguanhong tao taog purdue.edu purdueuniversity usaxuwei liu liu2598 purdue.edu purdueuniversity usa juanzhai juan.zhai rutgers.edu rutgers university usashiqing ma shiqing.ma rutgers.edu rutgers university usalei xu xlei nju.edu.cn nanjing university china xiangyuzhang xyzhang cs.purdue.edu purdueuniversity usa abstract inspiredbythegreatsuccessofusingcodecoverageasguidancein softwaretesting alotofneuralnetworkcoveragecriteriahavebeen proposed to guide testing of neural network models e.g.
model accuracyunderadversarialattacks .however whilethemonotonic relation between code coverage and software quality has been supportedbymanyseminalstudiesinsoftwareengineering itremains largelyunclearwhethersimilarmonotonicityexistsbetweenneural network model coverage and model quality.
this paper sets out to answer this question.
specifically this paper studies the correlationbetweendnnmodelqualityandcoveragecriteria effectsof coverage guided adversarial example generation compared with gradient decent based methods effectiveness of coverage based retraining compared with existing adversarial training and the internal relationships among coveragecriteria.
ccs concepts software and its engineering software testing and debugging computingmethodologies neuralnetworks .
keywords software testing deepneuralnetworks acmreference format shenao yan guanhong tao xuwei liu juan zhai shiqing ma lei xu and xiangyu zhang.
.
correlations between deep neural network model coverage criteria and model quality.
in proceedings of the 28th acmjointeuropeansoftwareengineeringconferenceandsymposiumonthe foundationsofsoftwareengineering esec fse november8 13 permissionto make digitalor hard copies of allorpart ofthis work for personalor classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation onthefirstpage.copyrights forcomponentsofthisworkownedbyothersthanthe author s mustbehonored.abstractingwithcreditispermitted.tocopyotherwise or republish topostonserversortoredistributetolists requirespriorspecificpermission and or a fee.
request permissions from permissions acm.org.
esec fse november 8 13 virtual event usa copyright heldby the owner author s .
publicationrightslicensed to acm.
acm isbn ... .
acm newyork ny usa 13pages.
.
introduction deepneuralnetwork dnn isbecomingan integralpartofthe new generation of software systems such as self driving vehicle systems computervisionsystems andvariouskindsofbotsystems.
just like software testing is a key step in traditional software developmentlife cycle manyresearchersand practitioners believe that dnnmodeltestingiscriticaltomodelqualityandhencethewhole system quality .
software testing can be classified as black box testing andwhite box testing with the former generating test cases from the specification without looking into the implementation whereas the later generating test cases based on the implementation.
specifically white box testing aims to generate testcasestoimprove codecoverage.
highcode coverageprovides moreconfidenceaboutthesubjectsoftware squality.itiswidely used in practice because progress can be easily quantified and test generationismoreamenabletoautomation comparedtoblack box testing .
the upper half of figure 1shows a typical life cycle of software testing.
given a subject software various test generation engines suchasrandominputgeneration fuzzing symbolicexecution searchbasedtestgeneration canbeusedtogeneratetest cases.thegeneratedtestsuiteisexecutedandthecodecoverageis measured and provided as feed back to the test generation engine whosegoalishencetogeneratemoretestcasesthatcanimprove coverage.
the failing test cases are reported to the developers who fix the corresponding faults bugs defects leading to a new version ofthe subjectsoftware.
inspiredbythegreatsuccessofwhite boxsoftwaretesting researchers have proposed white box dnn testing to improve model quality .
the life cycle of dnn model testing closelyresemblesthatofsoftwaretesting asshowninthelower half of figure .
specifically the subject becomes a dnn model instead of a program.
model input generation techniques are used to generate inputs.
the generation is guided by some coverage criterion just like in software test generation.
a failing input example esec fse november8 13 virtualevent usa s.yan g.tao x.liu j. zhai s.ma l.xu x.zhang softwaretest generation enginetest case code coverage bug fixing dnn modelmodel input generationtest case retraining neuron coverage figure software development vs. dnn development istheone thatcausesmodelmis classificationandcanbeusedto retrainthemodel.theretrainingprocedureisanalogoustothebug fixingprocedureinthesoftwaretestinglife cycle.ityieldsanew versionofthe model.
codecoveragecriteriaplayacriticalroleinsoftware white box testing.alargenumberofcoveragecriteriahavebeenproposedand used.
for example statement coverage measures the percentage of statementsthatareexecutedbyatestsuite edgecoveragemeasures the percentage of exercised control flow edges and path coverage measuresthepercentageofprogrampathsthatgetexecuted.differentcriteriahavedifferentlevelsofstrength indisclosingbugs and entail various amount of efforts.
specifically statement coverage is the simplest and also the weakest whereas path coverage is one of the most expensive and most powerful.
these criteria provide a spectrum of options for developers when they are balancing developmentcostandproductquality.inspiredbythesecodecoverage criteria researchershaveproposedalargesetofdnnmodelcoverage criteria .
specifically a dnn model consists of an input layer an output layer and a number of inner layers each containing a set of neurons.
during model inference prediction neuron activation values at a layer are computed based on those fromthepreviouslayer.duringtheactivationvaluecomputation foraneuron ifthevalueissmallerthan0 itissetto0andhence has contribution to the later layer s .
we say the neuron is not activated.
researchers observe the analogy between activating a neuron and covering a software artifact e.g.
statement .
therefore they propose a number of coverage criteria based on how neurons are activated.
for example neuron coverage measures the percentage of neurons that are activated analogous to statementcoverage and neuronpatterncoverage measuresthe activation path analogous to pathcoverage.
despite theinspiring correspondence betweensoftware testing anddnnmodeltesting thereareafewopenquestionsthatneed to be answered.
specifically the effectiveness of white box softwaretestingisbuiltonabasicassumptionfortherelationbetween codecoverageandsoftwarequality.specifically whilethedevelopers generate more test cases to achieve higher code coverage morebugsaredisclosedandfixedandhencethesoftwarequality ismonotonically improved without considering regression .
as such coverage driven test generation is one of the most popular test generation strategies.
while such monotonicity assumption is largely proved empirically in software testing its counter partin model testing is unclear as far as we know .
in fact the semantics of dnn models is substantially different from the semantics ofprograms.thesyntacticanalogybetweensoftwarestatements andneuronsmaynotdirectlytranslatetotheirsemanticanalogy.
intuitively program behaviors are largelydiscrete whereas model behaviors are continuous.
a statement being covered suggests that new functionality is exercised which is a discrete and modular event.
however a neuron being activated may not have similar implication.itmaywellbethatthesemanticsofamodelliesinthe distributionofthe entireactivationvector insteadofasetofdiscrete eventsofwhether individualneuronsare activated.
therefore this paper aims to study the following research questions that we believe are important for white box dnn model testing.
first we want to study if there is monotonic relation betweenmodelcoverageimprovementandmodelquality isittrue thattrainingsetswithincreasingmodelcoverageleadtoincreasing model quality.
second we want to study if coverage driven test generationiseffectiveinmodeltesting whencomparedtoexisting test generation techniques that are solely based on optimization and leverage continuity and differentiability of model behaviors.
third we want to study if test cases generated using model coveragecriteriahaveuniqueadvantageinimprovingmodelquality analogous to bug fixing .
last we aim to understand if there is correlation inthe variousmodelcoveragecriteria.
in our study we make use of models and datasets.
we leverage model coverage based test generation techniques in deepxplore deepgauge deephunter andsadl and thestate of the artoptimizationbasedadversarialexamplegeneration techniques c w and pgd to generate test cases thatleadtodifferentlevelsofcoverage.thesetestcasesareusedto retrain models.
then we measure the quality improvement using a setofwell establishedmetricsandstudytheaforementionedfour researchquestions.throughour study we find dnncoveragecriteriadonothavemonotonicrelationswith model quality measured by model accuracy in the presence ofadversarialexamples .
although dnn coverage criteria can be used as guidance to find adversarial examples effective adversarialexamples maynot leadto higher coverage.
existing methods used to generate adversarial examples based on coverage criteria usually add larger i.e.
measured bylpdistance and human visible i.e.
measured by visualsimilarity perturbations comparedtoexistinggradient based methods.
using such inputs in model testing is analogoustohavingprograminputsthatmayviolatethesoftware inputpreconditions.
adversarial examples generated by dnn coverage guided methods can be used to retrain a model to improve model robustness againsttheadversarial method used to generate the training inputs e.g.
performing semantic preserving operationslikechangingblurrinesstomaximizecoverage .
however such models are not robust against gradient based attacks e.g.
pgd .ontheotherhand pgdbasedadversarial training can improve the model robustness against pgd attacksbut not attacksbyusing coverageas guidance.
776correlations betweendeep neural network model coverage criteriaandmodel quality esec fse november8 13 virtualevent usa most existing dnn coverage criteria correlate with each other somehavingstrongcorrelations.thishelpsexplain their similar behaviors in all experiments.
however it is unclearwhetherthereexistspartialorderamongthemlike code coveragecriteria.
background .
deepneuralnetwork a deep neural network dnn is a parameterized function fthat mapsann dimensionalinput x rntooneofthe koutputclasses.
theoutputofthednn p rkisaprobabilitydistributionoverthe kclasses.
in particular p x jis the probability of the input belongingtoclassj.aninputxisdeemedasclass jwiththehighestprobabilitysuchthattheoutputclasslabel yhasy argmaxj p x j. during training with the assistance of a training dataset of inputswithknownground truthlabels theparametersincluding weights and bias of the dnn model are determined.
specifically givenalearningtask supposethetrainingdatasetisaset dtrain xi yi n i ofninput samples x x1 x2 ... xn rnand the corresponding ground truth labels y y1 y2 ... yn .fis the deep learning model that predicts the corresponding outcomes y based on the given input x i.e.
y f x .
within the course of model training there is a loss function l summationtext.
i n y i yi .
so the processofmodeltraining can be formalizedas min summationdisplay.
i n y i yi for a neuron o if it is activated i.e.
activation value is larger than some threshold value by some input examples in a set it becomes an activated neuron for the set.
when we provide the training dataset to the model the range of the observed neuron activation valuesisrepresentedas .
whenweprovidethetestdataset t theneuronactivationvalues maynotbelimitedin .instead thevaluescanalsofall in lowo or higho .wereferto lowo higho asthecornercaseregions oftheneuron .let x o betheoutput valueofneuron oforinputx thenuppercornerneuron ucn and lowercornerneuron lcn which represent the set of neurons that ever fall into the corner case regions respectively given some test inputs.formally ucn o o x t x o higho lcn o o x t x o lowo the symbolswe use inthis paper are shownas follows listofsymbols fa dl classifier onk classes where f xi yi anactivatedneuron lcnsetofactivatedneuronsthatfallinthecorner caseregions nthe number ofsamples ininputdataset ouniversal setofneuronsofadnn oneuron psoftmax layer outputof f wheref x argmax jp x j p x jthe j thprobability of p x wherej ... k ucnsetofactivatedneuronsthatfallinthecorner caseregionsxa ian adversarialexamples of xi xiinputsamples i n yithe corresponding ground truth label of xi whereyi ... k .
adversarialexamples anddnn robustness dnn models are vulnerable to adversarial examples.
that is given an original input xiand a small adversarial perturbation a dnn modelfhas f xa i f xi yt yi f xi here weuse xa itorepresent xi whichisusuallyreferredtoas anadversarialexample.theperturbation addedtotheinputismaliciouslymanipulatedbyanadversaryanditiscommonlybounded bylp norm i.e.
p .symbolytdenotesthepredictedlabel of adversarial example xa i different from the original prediction yi.
there exists a large body of different adversarial attacks.
we discuss two widelyused attacks in this paper and employ them in our experiment evaluation.
cwattackisasetofpowerfulattacksbasedondifferentnorm measurements on the magnitude of perturbations introduced by carlini and wagner .
in particular cw is formalized as an optimization problem to search for high confidence adversarial examples with small magnitude of perturbations.
they leverage the logits z i.e.
outputs right before the softmax layer instead of the final prediction f for generating perturbation.
the objective function for optimization is the combination of target label f xa i yt and small perturbation p which is achieved using optimizersuch as adam.
pgdattack isa first order universal adversary attack based on fastgradientsignmethod fgsm .fgsmperformsasingle step update on the original sample xalong the direction of the gradient ofalossfunction.the lossfunctionisusually definedas the cross entropy between the output of a network and the true labely.
pgd is an iterative variant of fgsm which applies the projected gradient descent algorithm with random starts to fgsm.thatis foreachattackiteration givenaninput xi itfirst adds a small random perturbation within given bound r p to the input i.e.
x i xi r. it then performs one step of fgsm and applies the gradient dxto the input i.e.
x i x i dx.
the updated sample is subsequently projected to the original bound i.e.
xa i clip x i xi xi .
theclip function sets small out of bound values to xi and large ones to xi .
the process continues until an adversarialexample isgeneratedortime out.
.
adversarialtraining adversarial training introduced by goodfellow et al.
is one of the most effective ways to improve dnn model robustness.
the overarchingideaofadversarialtrainingistoincorporateadversarial examplesformodeltraining.thatis duringeachtrainingiteration adversarialexamplesarefirstgeneratedagainstthecurrentstate ofthemodel andthenusedas trainingdata for optimizingmodel parameters.madryetal.
leveragethepgdattackwithmultiple steps for adversarial training.
adversarial training has been shown effective for large scaledataset such as imagenet .
777esec fse november8 13 virtualevent usa s.yan g.tao x.liu j. zhai s.ma l.xu x.zhang coveragebaseddnn testing in this section we first introduce a number of popular neuron coveragecriteriaanddiscuss theirintendedusage.
.
deepxplore pei etal.
introducedthe neuroncoverage nc nc an o where an denotesthenumberofactivatedneuronsand o means thetotalnumberofneurons.basically ncmeasuresthepercentage ofactivatedneurons i.e.
whoseactivationvalueislargerthan0 for agiventestsuiteanddnnmodel.deepxploreviewsncas thefirst white boxtestingmetricfordlsystemsthatcanestimatetheamount of dl logic explored by a set of test inputs.
and then deepxplore triestogeneratenewtestinputsthatcanmaximizencaswellas triggering differential behaviors in multiple dl systems that are designedtohavesimilarfunctionality.thesedifferentdnnmodels areusedascross referencetoavoidmanuallylabelingdatasets.the generationprocessappliesthreepre definedimagetransformations addingasingleblackrectangle changingallpixelvaluesbyacertain degree and adding multiple small black rectangles with the goal of covering more neurons.
after that deepxplore mixes these generated examples with benign inputs to re train the model to improve modelaccuracy.
.
deepgaugeanddeephunter ma et al.
extended the coverage concept to different levels i.e.
neuronlevel layerlevel andproposedmanynewdnncoverage based testing criteria in deepgauge and demonstrated that a testsuitewith ahighercoverageofthesecriteriapotentiallyindicates a higher chance to detect the dnn s defects.
here a defect is defined asmodelmispredictions.deephunter leveragessuchcoverage metricsasthefeedbacktofuzzdnnmodelstoproduceadversarial samples.thesenewmetrics include k multisection neuron coverage kmnc .
given a neuron o o thek multisection neuron coverage measures how thoroughlythegivensetoftestinputs tcoverstherange .
to quantify kmnc the range is divided into kequal sections i.e.
k multisections with k .
alsosomdenotes the m th section with m k. then x o sommeans them th section iscoveredbyat leastone input x t. kmnc summationtext.
o o som x t x o som k o neuron boundary coverage nbc .
nbc ucn lcn o fromthedefinition it seasytoseethat nbcshowshowthoroughlythegivensetoftestinputs tcoversthecorner caseregions.
strong neuron activation coverage snac .
snac ucn o similartonbc snacmeasuresthepercentageofuppercornercaseregionsthat are coveredbythe setoftest inputs t. top kneuron coverage tknc .
tknc uniontext.
x t uniontext.
l ltopk x l o here ldenoteslayersofadnn and l l isthel thlayer of a dnn.
function topk x l denotes the neurons that have the largestkoutputsonlayer lgivenx.tkncmeasuresthepercentage ofneuronsthathaveeverbeenthetop kneuronswithinitslayer for agiven inputset t. top kneuron patterns tknp .
given a test input x the sequence of the top kneurons on each layer also forms a pattern.
apatternisanelementof2u 2u 2u l where2u listhesetof subsetsoftheneuronsonthe l thlayer for1 l l.givenatest inputsett thenumberoftop kneuronpatternsfor tisdefined as follows.
tknp topk x ... topk x l x t intuitively tknpmeasuresthenumberofdifferentactivation patternsforthemostactive kneuronsoneachlayer.itisnotaratio but ratheranumber.
deephunter is a fuzz testing framework for finding potential dnndefects.itmostlyleveragestheabovecoveragecriteriaasfeedbacktoguidethetestgeneration andtogenerate newsamples.it performspixelvaluetransformations i.e.
changingimagecontrast brightness blurandnoise andaffinetransformations i.e.
image translation scaling shearingandrotation .togenerateimagesthat preserve its original semantics a l based threshold is set.
if the differencebetweentheoriginalandthegeneratedimageislarger thanthe threshold the generatedimageisdiscarded.
.
sadl kim et al.
introduced surprise adequacy to measure the coverage of discreditedinputsurpriserange fordlsystems .thesetermsare explainedinthefollowing.atestexampleis good ifitissufficiently butnotoverlysurprising comparingwiththetrainingdata thatis sufficiently but not overly deviant from the training distribution.
two measurements of surprise were introduced one is based on keneral density estimation kde to approximate the likelihood of the systemhavingseen asimilar inputduring training and the otherisbasedonthedistancebetweenthevectorsrepresentingthe neuron activation traces of the given input and the training data e.g.
euclideandistance .theproposedmetricswerealsocompared withothermetricsindeepgaugeanddeepxplore.theresultsshow that they are correlated.
moreover they were used to guide the retrainingofdnn models to improve robustness.
likelihood basedsurpriseadequacy lsa .
let o x denote theactivationvalueofasingleneuron owithrespecttoaninput x.forasetofneuronsinalayerofthednn denotedas o o o x denotes a vector of activation values that represents the activationtrace at of xoverneuronsin o .forasetofinputs x ao x o x x x denotes the set of activation traces observed for neurons in o .
given a training set t a bandwidth matrixhand a gaussian kernal function k the activation trace of anewinput x kdeproduces adensity function fas follows.
f x ao t summationdisplay.
xi tkh o x o xi 778correlations betweendeep neural network model coverage criteriaandmodel quality esec fse november8 13 virtualevent usa intuitively thefunctionmeasuresanormalizeddistancebetween the activation values of xand those of individual inputs in t regardingo .
then lsa isdefinedas follows.
lsa x log f x distance based surprise adequacy dsa .
assumeadlsystemf which consists of a set of neurons o is trained for a classificationtaskwithasetofclasses y usingatrainingdataset t.given thesetofactivationtraces ao t anewinput x andapredicted class of the new input c y. the closest neighbor of xthat shares thesameoutputclass denotedas xa andtheirdistance aredefined as follows.
xa argmin f xi y o x o xi dista o x o xa the closest neighbor in a class other than y denoted by xb and theirdistance distb are definedas follows.
xb argmin f xi y y o x o xi distb o x o xb thenthedsaisdefinedinthefollowing.intuitively itmeasures ifxiscloser to the target class oradifferentclass.
dsa x dista distb .
research questions testing is a critical step in software development life cycle to evaluatesoftwarequality findbugsandhelpdeveloperstoimprovethe software.intraditionalsoftwareengineering testcoveragecriteria including path coverage basic block coverage etc.
are a set of metricsusedtodescribethedegreetowhichthesubjectsoftware is tested given a test suite.
these criteria are usually computed as the number of some software artifacts e.g.
statements that are executed by at least one test case divided by the total numberofartifacts.animportanthypothesis whichhasbeenproven by numerous seminal studies is thathigher test coverage suggests better software quality given the same subject software.
this is because a test suite achieving higher coverage is believed to have a better chance of disclosing defectsinthesubjectsoftware.differentsoftwarecoveragecriteria denote the various trade offs between the difficulty to achieve highcoverage andthecapabilityofdisclosingdefects.forexample statementcoverageistheleastdifficulttoachievewiththeweakest effectiveness in finding bugs whereas path coverage is much more difficulttoachieve buthasstrongerbug finding capabilities.dnn coverage metrics were introduced with the goal of serving deep learningmodelengineeringinawaysimilartohowsoftwarecoverage criteria have been serving software engineering.
for example in deepxplore the authors believe that neuron coverage is a good metric for dnn testing comprehensiveness .in deepgauge the paperstatesthattheproposedcriteriacan effectivelycapturethe difference between the original test data and adversarial examples where dnns could and could not correctly recognize respectively demonstratingthat ahighercoverage ofourcriteriapotentiallyindicate a higher chance to detect the dnn s defects .
and in sadl thepaperconcludesthat sacanprovideguidanceformoreeffective retrainingagainstadversarialexamplesbasedonourinterpretation of the observed trend .
in software testing the correlations between coverage and software quality are well established which have beendrivingdecadesofpracticeandresearch.inthispaper weaim to study if the correlations between dnn coverage metrics and the intendedobjectives can be established.
basedontheabovequoted usageofdnn coveragecriteria we propose the following research questions.
.
.
rq are dnn coverage metrics correlated with dnn model robustness?
in software testing the correlation between test coverageandsoftwarequalityisintuitivelyestablishedasfollows.given asubjectprogramandaninitialtestsuite developersgeneratenew test cases to cover software artifacts that have not been covered before e.g.
new statements .
these new test cases may disclose defectsinthenewlycoveredcomponents.fixingthesedefectsleads totheimprovementofsoftwarequality.tovalidatethecorrelations between dnn model robustness and coverage criteria we draw the following analogies subject model in dnn testing vs. subject program insoftwaretesting trainingsetvs.initialsoftwaretest suite adversarialexample generationorganbasedexamplegenerationvs.softwaretestgeneration misclassifications causedby adversarialexamples vs.softwarebugs andadversarialtraining vs.softwarebugfixing.withsuchanalogies wehavethefollowing experiment design.
experiment design assume the subject model is f0.
we use adversarialexamplegenerationtechniquestogeneratealargesetof adversarialexamples.givenadnncoveragecriterion weperform input selection to select the examples that can lead to the most substantial coverage improvement and add them to the training suitet0andacquiret1.theprocessrepeatstoacquire t2 t3 ... and truntilthecoverageisfullorcannotimproveanymore.
here t2 is a superset of t1 t3is a superset of t2 and so on.
this process is analogous to how the developers enhance their test suite overtime toimprovecoverage.wethenperformadversarialtrainingusing t1 t2 ... andtr yieldingf1 f2 ... andfr respectively just like fixing software bugs disclosed by new test inputs.
then we use existingmethods tomeasuremodelrobustnessandstudythe correlationsbetweenrobustnessandcoverage.ideally wewould expectto see thesemodels have increasing levels of robustness.
.
.
rq is coverage driven test generation effective in disclosing dnn defects?
how does it compare to the other commonly used adversarialexamplegeneration techniques?
in software testing an importantfunctionalityofcodecoverageistoguidetestgeneration.
a large body of existing software test generation techniques such as symbolic concolic execution fuzzing andsearch based testing make use of code coverage as the guidance.
analogously researchers of deepxplore tries to generate inputs thatmaximizingneuroncoverage andbelievethat neuroncoverage helpsin increasing thediversityofgenerated inputs .
deephunter leverages multiple plugable coverage criteria asfeedbacktoguidethetestgenerationfromdifferentperspectives .
however whilesoftwarebehaviorsarelargelydiscrete dnnmodel behaviorsarecontinuous.assuch thereexisthighlyeffectiveinput generation techniques built on optimizations e.g.
based on gradients .whilethesetechniquesdonotexplicitlyutilizecoverage 779esec fse november8 13 virtualevent usa s.yan g.tao x.liu j. zhai s.ma l.xu x.zhang optimization algorithms have the implicit capabilities of exploring differentactivationpatternsbyfollowingthedirectionofgradients.
assuch weareinterestedinstudyingifcoverageguidedtesting hasadvantagesoverthepopulargradientdescentbasedmethods in disclosingdefects i.e.
generatingadversarialexamples .moreover gradientbasedtestgenerationoftenmakesuseof lp normtobound thescaleofperturbationsuchthatadversarialexamplesdonotlook substantially different from the original input in humans eyes .
in contrast existingcoverage guided testgeneration uses predefined imagetransformationssuchasaddingblackrectangles changing imagepixels byacertaindegree changing imageblurriness androtating images .see detailsin subsection .
andsubsection3.
.noticedeephunteralsousesa l basedthresholdvalue to limit the change of the image.
however because the pre defined operationsusuallychangetheimagesignificantly thisthreshold valueislargerthanwhatisusedingradientbasedmethods.also gradient based methods will try to minimize the change e.g.
lp distance while deephunter only checks if the value is smaller than the threshold.
therefore we also want to study the quality of generated examples in comparison with existing gradient descent basedtechniques.
experiment design.
toanswer rq weutilizethe test generationtechniquesindeephunter coveragecriteriabased and pdg gradientbased togeneratetestcases dhanddp respectively againstthe samednnmodel.then we compare dh anddpfrom a few aspects.
first we compare the effectiveness of these methods.namely we calculatethe percentage of samples indhanddpthat can lead to discovery of dnn defects i.e.
mis prediction .
second we compare the quality of generated samplesbycalculatingtheir similaritywiththe originalbenign image.
third we calculate the coverage metrics for dpand try to see if newadversarialexamplesleadto the growth ofcoverage.
.
.
rq are the adversarial examples generated by dnn coverage based testing effective in improving model robustness?
how are they compared to those generated by popular gradient descent based techniques?in software testing one aspect to measure effectiveness oftestgenerationtechniquesistheeffectivenessofthegenerated counter examples failing test cases in bug fixing.
similar objectives are explored in dnn testing.
particularly deepxplore and sadl use the generated adversarial examples as part of the new trainingdatasettoretrainthemodelsothatitcanachievebetter accuracy against adversarial examples i.e.
more robust .
for example thedeepxplorepaperstatesthat testinputsgeneratedby deepxplorecanalsobeusedtoretrainthecorrespondingdlmodel toimprovethemodel saccuracybyupto3 andthesadlpaper statesthat sadl canimproveclassificationaccuracyofdlsystems againstadversarialexamplesbyupto77.
viaretraining .thereforeinthisresearchquestion wewanttostudytheeffectiveness of dnn coverage based test generation in comparison with the existing popular alternatives.
experiment design.
to answer rq we conductthe following experiment.
first we use deepxplore and sadl to generate adversarialexamples andmixthemwithoriginaltrainingdatatoretrain themodel.wereusetheparameters e.g.
ratioofnewadversarial examples from the original papers.
second we train the models with adversarial training.
more specifically we use the pgd basedadversarial training.
the parameters used in our retraining are adopted from the original paper .
then we compare the model effectiveness ofthe twosetsof hardenedmodels.
.
.
rq howarethedifferentdnncoveragecriteriacorrelated?
in software testing there is a semi lattice for the strength of the differentcodecoveragecriteria.forexample statementcoverage edge coverage path coverage and condition coverage that aims tocoverthetrue falsevaluesofeachcomparativeexpressionina predicate is stronger than statement coverage weaker than path coverage andnotcomparablewithedgecoverage.wesaycriterion c1 is stronger than c2 if c1 coverage must imply c2 coverage.suchrelationsareimportantforchoosingtheappropriate techniques in software testing.
for example path coverage may be desiredforsafetycriticalcodedespiteitshighcost.inthisresearch question weaimtolookforcorrelationorevenpartialorderamong the variousdnn coveragecriteria.
experiment design.
to answer rq for a given model f we keepaddingnewinputsamplestotestthemodel andgatherthe coverage information to get a sequence of values for each test criteria.
for example for nc we can get a sequence of values ncf ncf ncf ncf ... ncfn where n is thetotalnumberof added input sample sets.
similarly we can get kmncf nbcfetc.
then weperformcorrelationanalysisonthesecollecteddata ncf kmncf nbcfandsoontoseeiftheyhavestrongcorrelations orpartialorder.
dnn model qualitymetrics in software testing quality of software is often measured by the number of bugs found within a certain period of time.
the quality metrics of dnn models are more diverse.
most dnn testing techniques draw analogy between bugs defects insoftware code andadversarialexamplesindnnmodel.anadversarialexample is considered manifestation of some undesirable behavior of the model as it causes misclassification.
just like software quality metrics are based on bugs model quality metrics are also centered around adversarial examples.
they fall into three categories model accuracy in the presence ofadversarial examples adversarialexample impreceptiblity that measures if an adversarial example looks natural and adversarialexamplerobustness .thesearethemetrics commonlyusedbyadversarialmachinelearning .details are explainedinthe subsections.
.
model accuracyforadversarialexamples state of the artadversarialexamplegenerationtechniques suchas c wandpgd optimizelogitsinordertogenerateinputs.since logits still have to go through a soft max layer to produce the final classification outputs the generated adversarial examples may not yieldtheintendedmis classificationeventhoughtheoptimizercan successfully reachits objective.modelaccuracy inthe presence of adversarialexamplesmeasureshowoftenthegeneratedadversarial examplesleadtocorrectclassificationresults.highaccuracymeans that the model is robust.
the analogy in software testing is to measurehowoftenthesubjectsoftwarefailswhenitisstresstested.
specifically weconsiderthefollowingmetricsthatarerelatedto modelaccuracy.
780correlations betweendeep neural network model coverage criteriaandmodel quality esec fse november8 13 virtualevent usa misclassification ratio mr .
mr nn summationdisplay.
i 1count f xa i yi here nis the number of generated samples and count is a functionusedtocountthenumberofmisclassifiedsamples.basically mrcalculatesthepercentageofmisclassifiedinputsamples inthe wholeinputset.a high qualitymodelhas alowmr.
average confidence ofadversarial class acac .
acac nn summationdisplay.
i 1p xa i f xa i wheren n n is the total number of adversarial examples thatcausemisclassification.and p xa i f xa i isthepredictionconfidencetowardstheincorrectclass f xa i .ingeneral acacmeasures theaveragepredictionconfidencetowardstheincorrectclassfor adversarialexamples.
average confidence oftrue class actc .
actc nn summationdisplay.
i 1p xa i yi heren n n is the total number of adversarial examples that causemisclassification.
p xa i yiisthepredictionconfidenceoftrue classesforadversarialexamples.justlikeacac actcmeasures the prediction confidence oftrue classesfor adversarialexamples.
.
adversarialexample imperceptibility this metric measures how realistic an adversarial example is in humaneyes.
itis usuallycomputed by using theoriginal example asareference.ahighqualityadversarialexampleisonethathas imperceptible perturbation.
we should not say a model is of low quality is it is susceptible to low quality adversarial examples.
the analogy in software testing is that the subject software may fail when it is provided with an input that substantially violates the implicit inputpre conditions.insuchcases theinducedfailures cannotbeusedasevidenceoflowsoftwarequality.specifically we use the following metrics.
average lpdistortion aldp .
aldp nn summationdisplay.
i xa i xi p xi p here pisthelpnormdistance whichisadoptedasdistortion metrics for evaluation.
specifically l0calculates the number of pixels changed by the perturbation l2computes the euclidean distance betweenoriginalexamples andadversarialexamples l measures the maximum change in all dimensions of adversarial examples.aldpmeasurestheaveragenormalized lpdistortionfor alladversarialexamplesthatcausemisclassification.thesmaller thealdp the more imperceptibilitythe adversarialexample has.
average structural similarity ass .
ass nn summationdisplay.
i 1ssim xa i xi here ssimisthemetricusedtoquantifythesimilaritybetween twoimages .asscanmeasuretheaverage ssimbetweenalladversarialexamplesthatcausemisclassificationandtheircorresponding original examples.
the larger the ssim the more imperceptibilitythe adversarialexamples has.
perturbationsensitivitydistance psd .
psd nn summationdisplay.
i 1m summationdisplay.
j 1 i jsen r xi j heremisthetotalnumberofpixelsforanexample i jdenotes thej thpixel ofthe i thexample r xi j representsthesurrounding square region of xi j andsen r xi j std r xi j with std r xi j denotingthestandarddeviationfunction.
psdevaluates human perception of perturbations.
the smaller the psd the more imperceptibilitythe adversarialexample has.
.
adversarialexample robustness in adversarial machine learning adversarial example robustness isoftenmeasured .itmeasuresthelevelofresiliencea successfuladversarialexample i.e.
aexample thatcausesmisclassification hasinthepresenceof input perturbation.intuitively an adversarial example that is not robust should not be used as evidenceoflowmodelquality.theanalogyinsoftwaretestingis thatatransientfailure e.g.
causedbynon deterministicfactors maynot indicatelowsoftwarequality.in particular we measure the following.
noisetolerance estimation nte .
nte nn summationdisplay.
i p xa i f xa i max p xa i j here p xa i f xa i is the probability of misclassified class and max p xa i j isthemaxprobabilityofallotherclasses j ... k j f xa i .ntemeasures the amount of noise adversarial examples can tolerate while keeping their misclassified label unchanged.
intuitively larger nteindicates more robust adversarial examples.
robustnessto gaussianblur rgb .
rgb count f gb xa i yi count f xa i yi wheregbdenotesthegaussianblurfunction analgorithmthat canreducenoisesinimagesand count isusedtocountthenumber of specific samples.
rgbcounts how many adversarial examples can maintain their misclassification function after gaussian blur.
the greater the rgbis the more robust adversarialexamples are.
robustnessto imagecompression ric .
ric count f ic xa i yi count f xa i yi here icis a specific image compression function.
like rgb riccounts how many adversarial examples can maintain their misclassification function after the image compression function.
the greater the ric the more robust the adversarialexamples.
781esec fse november8 13 virtualevent usa s.yan g.tao x.liu j. zhai s.ma l.xu x.zhang .
.
.
.
.
.
.
.
.
.
t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10relative coveragenc .
nc .
nc .
tknc tknp kmnc nbc snac figure coverage vs. trainingdatasets .
.
.
.
.
.
.
.
t t t t t t t t t t t attack metrics valuesmr acac actc nte alp l alp li ass rgb ric alp l psd figure robustnessvs.
trainingdatasets experiments and results .
setup datasets and models.
we use mnist cifar and svhn asourdatasets.thesearepopulardatasetsusedintheliterature of model coverage .
mnist is a handwritten digit recognition dataset a popular dataset for image classification.
the cifar dataset is widely used for easy image classification task benchmark in the research community.
it contains color images in different classes.
the street view housenumbers svhn datasetisobtainedfromhousenumbersin googlestreet viewimages.itconsists of73 257trainingsasmples and testing samples.
for mnist we use three pre trained lenet family models i.e.
lenet lenet and lenet as the baseline model.
for cifar we use vgg and resnet models.
for svhn we also use three cnn model and their modelarchitecturesareadoptedfrompreviouswork .wedirectly use pre trained models if possible.
our trained models are alsoavailable online .
configurations.
theconfigurationsforcoveragecriteriaisshown intable1.forallresearchquestions wesetthethresholdforncto be0.
.
.
.7and .
.for kmnc the kvalue i.e.
numberof multisections we useis10.for tknp and tknc the kvalue i.e.
the top kneuron coverage is .
for lsc and dsc the layers we analyze are shown in column in table the numbers of buckets forlscanddscareshownincolumns7and9 respectively andthe upperboundsofsaareshownincolumns8and10 respectively.
these are the same settings published in the original papers or published in their open source repository.
for c w attacks we use the implementation from cleverhans and use their default mr acac actc alp l0 alp l2 alp li ass psd nte rgb ric nc .
nc .
nc .
nc .
nc .
tknc tknp kmnc nbc snac dsa lsa0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0figure correlation ofcoverage criteria and robustness parameters.forpdgattackandpdgbasedadversarialtraining we also usethe defaultparameters usedin their paperandrepository.
.
results andanalysis .
.
rq are dnn coverage metrics correlated with dnn model robustness?
as mentioned in subsection .
to answer rq1 we obtain a set of new training suites t1 ... tn.
each one has more trainingdatathatcanenlargethecoveragemetrics.inourexperiment we obtain training datasets in total for each model i.e.
n includingtheoriginalone i.e.
t0 .ineachstep weadd250 new images to the dataset thus in total we add new training images to the training dataset.
compared with deepxplore and sadl itintroduces more newtraining samples.
figure2showsthecoveragemetricvaluechangesw.r.t.different trainingdatasets.inthisgraph they axisistherelativecoverage valueandthebasevalueweuseistheoneobtainedon t0.thisis becausetheresultsareinwidevalueranges.forexample tknp isanabsolutenumberwhosevalueislargerthanafewthousand and many others are ratios whose value range is .
for nc we only show lines with threshold values being .
.
and .
.
thisisbecausefor0 .1and0.
theoriginaldataset t0canachieve coverage.forsomemetrics theoriginalcoveragevaluesare alreadyveryhigh over98 andhenceinthegraph itdoesnot showsignificantgrowth.overall wecanseethatwithnewtraining samples allcoveragemetrics are growing.
figure 3shows the changes of attack metrics i.e.
model robustness w.r.t.
different training datasets.
similarly the values are also normalized based on the values obtained in t5 for better visualization .for mr acac aldpandpsd largeryvaluesindicateless robust.
while for actc ass nte rgb and ric a larger y value always indicates a more robust model.
from the graph we observe thatnone of them is monotonous .
it means from the attack s pointofview addingnewsamplestoimprovecoveragedoesnot alwaysleadto the improvement of modelrobustness.
duetothespacelimit figure2andfigure3showtheresultsfor themnistdatasetonthelenet 1model.othermodelsanddatasets 782correlations betweendeep neural network model coverage criteriaandmodel quality esec fse november8 13 virtualevent usa table configurations forrqs dataset modelnc threshold kmnc k tknc tknp k lsc dsc layer n ub n ub mnistlenet .
.
.
.
.
conv 2 lenet .
.
.
.
.
conv 2 lenet5 .
.
.
.
.
conv 2 cifarvgg .
.
.
.
.
conv 2 resnet .
.
.
.
.
block2 conv1 svhnsadl .
.
.
.
.
pool 1 sald .
.
.
.
.
pool 1 sald .
.
.
.
.
pool 1 5table comparisonofattack images dataset modelmr l dhdpdhdp mnistlenet .
.
.
lenet .
.
.
lenet .
.
.
cifarvgg .
.
.
.
resnet .
.
.
.
svhnsadl .
.
.
sadl .
.
.
.
sadl .
.
.
do have a similar pattern.
our generated datasets trained models and original results are available in github .
to have a better understanding of whether coverage criteria are correlated with robustness we also perform a correlation analysis on all trained models and datasets using the kendall s method which is a standardstatisticalmethodused tomeasure thelinearandnon linear relationships between two different variables.
the result is shown infigure .
in this figure each label in x axis represents one attack criterion and each label in y axis represents one coverage criterion.
eachcellrepresentsthecorrelationbetweencriteria.weuseblue color to represent negative correlation i.e.
variables change in opposite directions and red labels to represent positive correlation i.e.
both variables change in the same direction .
according to the definitionofcorrelationinguildfordscale ifthecorrelationis lessthan0.
thepositiveornegativecorrelationislow valuesin indicate that thecorrelation is moderate and high correlationvalues i.e.
.
.9orabove0.
representstrongcorrelation.
aswecansee mostcellsareinlightcolorsandhavelowcorrelation values indicatingneutralcorrelations.inotherwords thereisno clear relationship between thesevariables.
.
.
rq is coverage driven test generation effective in disclosing dnndefects?howdoesitcomparetotheothercommonlyusedadversarial example generation techniques?
we utilize deephunter and pdg to generate test cases dhanddp respectively againstthesamednnmodel.for dhanddp wefirstcompare theattacksuccessrateusingmr.foreachmethodandeachmodel we generate images and then testmr.
for both methods we limitthel byusingthedefaultvalueprovidedbydeephunter i.e.
.
.
the results are shown in columns and in table .
as we can see pgd achieves almost success rate on all models and datasets while deephunterachieves lower success rate i.e.
lowermrvalue about60 forsadl 1andsadl 3model andabout90 forothermodels .wealsocomparethequalityof thegeneratedadversarialexamplesbycalculatingthe average l distances of benign inputs and adversarial examples.
the results areshownincolumns5to6in table2.theaverage l distances for pgd dp is to times smaller than that of deephunter dh indicating that the adversarial image quality generated by pgdis betterthan deephunter.in figure we showtheoriginal images figure5a attackimagesbydeephunter figure5b and pgd figure 5c respectively.
the reason why deephunter has largerl perturbation is because of the affine transformations it uses whichresultsinlarger l valuesbutmaintainsthesemantics.
in this sense l distanceisnot an ideal metric touse andhowto a original images b deephunter c pgd figure generated adversarial examples .
.
.
.
9401tknp valuecoverage values new samplesnc kmnc nbc snac tknc tknp figure coverage vs. adversarial examples chooseabettermetricisoutofthescope.here weuseitbecauseit isusedbybothpgdanddeephuntertodetermineifthegenerated example isavalid input.
to understand howpgd generated adversarial examplescorrelatewithcoveragecriteria wealsocalculatethecoveragemetric value change by adding images in each single step.
the result is shown in figure .
the primary y axis shows the percentage for ratiovalues i.e.
nc kmnc nbc snac tknc andthesecond y axis shows the value for tknp.
from the graph we can see that the lines for nc nbc kmnc snac and tknc show a similar pattern they grow rapidly at the beginning and then plateau afterwards.thistellsusthatnewadversarialexamples thecoverage.however adversarialexamplesmaynotnecessarily increase coverage.
to future demonstrate this we use adaptive gradient based attacks to limit the coverage change during optimization while generating the adversarial examples.
the results show that it can still generate adversarial examples with almost successrateonallmodels anddatasets.
on the other hand tknp shows a almost linear relationship with thenumber ofnewsamples.to furtherstudythis we design another experiment which is to calculate the growth of these coveragemetricswhenweaddbenignsamplesinsteadofadversarial sample.
in our case we fix the setting i.e.
k and then we 783esec fse november8 13 virtualevent usa s.yan g.tao x.liu j. zhai s.ma l.xu x.zhang 9601tknpvalue newsamplesadv tknp benign tknp figure tknpvs.
new inputs table modelaccuracyunder differentscenarios dataset model benign dh pgd mnistlenet .
.
.
.
lenet .
.
.
.
lenet .
.
.
lenet 1adv.
.
.
.
.
.
.
lenet 4adv.
.
.
.
.
.
.
lenet 5adv.
.
.
.
.
.
cifarvgg .
.
.
.
.
resnet .
.
.
.
.
.
vgg 16adv.
.
.
.
.
.
resnet adv.
.
.
.
.
.
.
svhnsadl .
.
.
.
sadl .
.
.
.
.
.
sadl .
.
.
.
.
.
sadl adv.
.
.
.
.
.
.
sadl adv.
.
.
.
.
.
.
sadl adv.
.
.
.
.
.
.
addthecorrespondingseedinputasthenewsample.theresults is shown in figure .
as we can see adding benign samples and addingadversarialsampleshavealmostthesameeffects indicating that this criteria cannot really distinguish the differences of benignsamplesandadversarialsamples.inotherwords itisalmost equivalentto the count ofsamples.
.
.
rq are the adversarial examples generated by dnn coverage based testing effective in improving model robustness?
how are they compared to those generated by popular gradient descent based techniques?to answer rq3 we retrain the models using two different approaches for one set we use the adversarial examples generated by coverage guided testing and for the other set we use pgd basedadversarialtraining.theresultsareshownin table3.the firstcolumnshowsthedatasets.thesecondcolumnpresentsthe models including those retrained with coverage guided adversarial examples rows2 andthoseretrainedbypgd rows .foreachmodel weshowtheaccuracyonbenign samples adversarialexamplesgeneratedbycoverageguidanceand under pgd attack.
in each cell we show the model accuracy as wellasthedifferencecomparedwiththeoriginalmodel without retraining .numberswith meansthattheaccuracyishigherthan theoriginalmodelandnumberswith meansthatthemodelaccuracy is lower than the original one.
from the table we can see that most models with the same architecture have similar accuracy on benign testing datasets which is also similar to the original model accuracy .insomecases pgdbasedadversarialtraininggetslower nc .
nc .
nc .
nc .
nc .
tknc tknp kmnc nbc snac dsa lsa nc .
nc .
nc .
nc .
nc .
tknc tknp kmnc nbc snac dsa lsa0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0figure correlation between coverage criteria accuracy on benign testing datasets.
this is mainly because adversarialtrainingishardtoconverge .wetrainedseveralversions andgotsimilarresults.asimilarissuehappenedforvgg 16model on the cifar dataset.
we found that images generated by deephunterhavesignificantlylargeperturbations andthere training processisdifficult to converge.
most models also have a higher accuracy compared with the originalmodels ontheadversarialexamplesgeneratedbycoverage guided testing.
vgg on cifar does not converge and that is why the model accuracy is low.
we find that adversarial training does not necessarily improve the model accuracy against suchimages.forexample allthreemodelsonsvhnendupwith aloweraccuracyonthisdataset.onemorefindingisthatmodels retrained with coverage guided testinghas nearly accuracy underpgdattack whilepgdbasedadversarialtrainingincreasesthe modelaccuracyby30 formostcases.thisindicatesthatcoverage criteria guided retraining can improve model robustness under attacksthatusethesameimageperturbationstrategy butdoesnot increasethemodelrobustnessundergradientbasedattacks.similarly pgd based retraining can improve model accuracy against pgdattacksbutcannotimproveitsaccuracyagainstadversarial inputsgeneratedbycoverageguidedmethods.
.
.
rq howarethedifferentdnncoveragecriteriacorrelated?
if the coverage criteria have partial order relationship they ought to be correlated.
thus we first analyze the correlation among all the coverage criteria.
the results are shown in figure .
the meaning ofthegraphisthesameas figure4.fromthegraph wecanseethat nbc and snac are correlated with each other with high strength whereastheyhaveweakornocorrelationwiththeothermetrics.
nc knc tknc lsa and dsa are also correlated with each other to a certain degree but they do not show very strong correlations.
suchresults are alsoconsistent withexisting work .
threatto validityand discussion threattovalidity.
first mostexistingcoveragecriteriafocuses on dnn based image classifications and so is this work.
second the results are acquired with a limited set of models data sets and 784correlations betweendeep neural network model coverage criteriaandmodel quality esec fse november8 13 virtualevent usa specific training hyper parameter settings.
they may not be representative for other modelsorsettings.tomitigatethe threat we publishallthesetupdetails implementation data e.g.
thegenerated tests at for reproduction.
third our study is largely based on existing model coverage criteria and test generation techniques.
a possible threat is that we may not faithfully reproduce these existing works.
to mitigate the threat we use the models datasets and implementations when available from the original papers.
we also validate our results by cross checking with those in the originalpapers.
discussion and suggestions.
according to results from rq2 and rq3 weknowthatitisrelativelyeasytogenerateadversarialeven whenthemodelhasachieveover100 coverage.thisisdifferent from traditional software testing where software are generally less vulnerable after achieving high coverage.
this brings the doubt abouttheusefulnessofexistingcoveragecriteriaindnntesting.
however weareuncertainiftherewillbeothercoveragecriteria that showsavery positive correlation withdnn robustness.
oneimportantmessagefromthisstudyisthatourcommunity shall establish a set of standards for evaluating future proposals.
theyshallincludetestingvariousmodelproperties e.g.
robustness accuracyandfairness andcover various modelarchitecturesand taskssothattheresultscanbecomparable.asaninitialstep we have create a github reposotiry for this purpose andstrongly encourage our readers to contribute.
related work dnn testing and validation.
the relationship between coverageanddnntestinghasbeenstudiedbyothersaswell.donget al.
showthatthereislimitedcorrelationbetweencoverageand robustness for dnns.
li et al.
argues that previously reported fault detection capabilities conjecturedfromhighcoveragetesting are more likely due to the adversary oriented search but not the real high coverageitself.
besidescoveragecriteria alarge body oftestingmethodswas proposed for testing machine learning models such as fuzzing symbolicexecution runtime validation fairness testing etc.
deeptest utilizesninetypesofrealisticimagetransformationsforgenerating test images which discovered more than erroneous behaviors of dnns used in autonomous driving systems.
deeproad leverages generative adversarial networks gans to generate test cases simulating different weather conditions.deepbillboard manipulates contents on billboards to cause wrong steering anglesofautonomousdrivingsystems.deepimportance selects importantneuronsfrompre trainedmodelsandclustersthoseneurons with respect to their value regions.
the coverage of those important neurons is then used for testing model behaviors.
model testinghasalsobeenappliedonotherdomainssuchasautomatic speech recognition text classification image classification machine translation .
more related works can be foundinthis survey .
to validate safety of dnn models researchers leverage verificationtechniquestoprovideformalguarantees .
reluplex transforms dnn models to numerical constraintsandutilizessmt solvertoverifyrobustnessofdnns.modelsevaluatedinreluplexweresmallwith8layersand300relu nodes.
deepsafe was built on reluplex and cannot scale to largemodels.reluval leveragesintervalarithmetictoverifyrobustnessofdnns whichis200timesfasterthanreluplex.ai2 uses abstract interpretation to approximate the data region after relu activation function.
due to its over approximation nature ai2mayfailtoverifyaninputwhichhascertainproperty.deeppoly combinesfloatingpointpolyhedronwithintervals which canscaletolargemodels.dissector generatessub models forintermediatelayersoforiginalmodelsandvalidatesgiveninputs bymeasuring prediction probabilitiesfrom sub models.
adversarial machine learning.
the vulnerability of machine learning models has been an extensively discussed topic .
as we elaborate in subsection .
that an attacker can use gradient to perturb original inputs to induce misclassification of dnns.
there also exist other types of adversaries suchas black box attacks back doorattacks etc.a lot of defense mechanisms were proposed to mitigate the threat of adversarial examples such as input transformations differential certificate adversarial training model internal checking etc.more details regarding attacks and defensescan be foundinthesepapers .
dnntestingtechniqueshavealsobeenappliedongenerating counterexamples.
the proposed neuron coverage based metrics were utilized to guide the search .
in this paper we studytherelationshipbetweenthosecoveragebasedmetricsand adversarialexamplesin rq2.wealsoleverageadversarialtraining approach to study the robustness of dnn models trained with coveragebasedexamplesandgradientbasedones respectively see details in rq3 .
conclusion inthispaper westudyexistingneuralnetworkcoveragecriteria and find that they are not correlated with model robustness.
although they can be used for adversarial example generation and improvemodelrobustnessinlimitedscenarios i.e.
attackersuse the same perturbationstrategy they tendtogenerateadversarial examplesthathavesubstantialperturbationsandhenceperceptible by humans.
in contrast existing optimization based adversarial example generation techniques can generate less perceptible examples.thereare correlations betweenexistingcoveragecriteria.
however it remains unclear if they have partial order relations as code coverage criteria.
our experiments and data are public for reproduction andvalidation.