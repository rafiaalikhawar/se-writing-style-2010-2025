fuzz testing based data augmentation to improve robustness of deep neural networks xiang gao nationaluniversityof singapore singapore gaoxiang comp.nus.edu.sgripon k. saha fujitsu laboratories of america inc usa rsaha us.fujitsu.com mukul r. prasad fujitsu laboratories of america inc usa mukul us.fujitsu.comabhik roychoudhury nationaluniversityof singapore singapore abhik comp.nus.edu.sg abstract deepneuralnetworks dnn havebeenshowntobenotoriously brittle to small perturbations in their input data.
this problem is analogoustotheover fittingproblemintest basedprogramsynthesis and automatic program repair which is a consequence of the incompletespecification i.e.
the limited tests or training examples that the program synthesis or repair algorithm has to learn from.recently testgenerationtechniqueshavebeensuccessfully employedto augmentexisting specificationsof intendedprogram behavior to improve the generalizability of program synthesis and repair.
inspired by these approaches in this paper we propose a techniquethatre purposessoftwaretestingmethods specifically mutation basedfuzzing toaugmentthetrainingdataofdnns with the objective of enhancing their robustness.
our technique casts thednndataaugmentationproblemasanoptimizationproblem.it usesgeneticsearchtogeneratethemostsuitablevariantofaninput data to use for training the dnn while simultaneously identifying opportunitiestoacceleratetrainingbyskippingaugmentationin many instances.
we instantiate this technique in two tools sensei and sensei sa and evaluate them on dnn models spanning 5popularimagedata sets.ourevaluationshowsthatsenseican improve the robust accuracy of the dnn compared to the state of theart on eachofthe15models byupto11.
and5.
onaverage.
further sensei sacanreducetheaveragednntrainingtimeby while still improving robust accuracy.
ccsconcepts computingmethodologies neuralnetworks software and its engineering search based software engineering software testing and debugging.
keywords geneticalgorithm dnn robustness data augmentation thisworkwasmainlydonewhentheauthorwasaninternatfujitsulabsofamerica.
permissionto make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acm mustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may23 seoul republic of korea 2020association for computing machinery.
acm isbn ... .
reference format xianggao riponk.saha mukulr.prasad andabhikroychoudhury.
.
fuzz testing based data augmentation to improve robustness of deep neural networks.
in 42nd international conference on software engineering icse may23 seoul republicofkorea.
acm newyork ny usa pages.
introduction programmingbyexamples pbe suchastest basedprogramsynthesis and automated program repair automatically generates programsthat conformtothe specificationindicatedby thegiven examples.
since the given examples usually constitute an incomplete specification of intended behavior the generated program mayover fit the given examples and thereby notexhibit intended behaviorsontheun seenpartoftheinputspace.over fittingisa commonproblemintest basedprogramsynthesis andautomatedprogramrepair .similarly machine deeplearning also faces the over fitting problem.
in machine learning a model is usuallylearnedfromatrainingset ofinputs andthendeployed on a testing set.
over fitting in machine learning systems impacts two related but distinct properties of such systems inadequate standard generalization where the trained model shows high accuracyonthetrainingset butcannotbegeneralizedtodatapoints outside the training set e.g.
the testing set and inadequate robustgeneralization wherethemodelshowhighaccuracyonboth trainingandtestingsets butcannotbegeneralizedtoinputsthat are small perturbations of training testing inputs these small perturbations may stillconstitute legalinputs butthe learnedmodel often mis classifies such inputs.
robust generalization renders the learned models resilient against such small perturbations.
standardgeneralizationdoesnotimplyrobustnessgeneralization.forinstance amodel evenwith99 accuracyonitstesting dataset could misclassify the input variations generated by simple spatial transformation e.g.
rotation with high confidence .
figure presents an example from the gtsrb dataset.
robust generalizationitselfcanbeoftwotypes dependingontheoperation context.inasecuritysetting robustgeneralizationagainstadversarial inputs aims to protect a model against a powerful adversary thatcanmodifytheinputinsophisticatedandtargetedways i.e.
anattack suchasobscuringafewspecificpixelsinanimage to makethemodelmis predict.thisscenariohasbeenpopularizedby theworkonadversarialtesting andmorerecently adversarial robust generalization .
a second scenario however is p erturbations due to natural variations in .
oe oufsobujpobm pogfsfodf po 4pguxbsf ohjoffsjoh icse may seoul republic of korea xiang gao ripon k. saha mukul r. prasad and abhik roychoudhury a original image is correctly classified b incorrect classification afteronedegree rotation figure a motivating example from the gtsrb dataset environmental conditions when capturing input data.
examples of this are variations due to weather conditions e.g.
snow fog ortheillumination e.g.
dayornight forthe samelocation fora self driving car or variations in the position angle or exposuresettings for a camera capturing the same subject.
recent workhas highlighted the poor robustness of deep learning models to such variations .
in this work we focus on the latter scenarioandinvestigatehowsoftwaretestingtechniquescanbeap pliedfortherobustgeneralizationofdeepneuralnetworks dnns to natural environmental variations.although we present our approach in the context of dnns the core ideas are more broadly applicableto learning systems.
inthefieldoftestdrivenprogramsynthesisandprogramrepair a prominentclassofrecentte chniquesimprovesthe generalizationof generatedprogramsbyaugmentingexistingtestsuites .
test case generation techniques such as random testing searchbased evolutionarytesting symbolicexecution grey boxfuzzing have been used to augment existing specifications of intended programbehavior .inparticular coverage basedgrey box fuzzing approaches such as afl have shown utility in augmenting existing test suites for program repair .
in grey box fuzzing algorithms i nputs are randomly mutated to generate newinputsandhigherpriorityisassignedtoinputsthatexercise newandinterestingpaths.theintuitionbehindthesetechniquesisthatcoveringmoreprogrampathsenablesustofindrepresentative tests and covers more program functionality.
ataconceptuallevel trainingaimodelsisanalogoustoprogram synthesis .
a learning system generates a model that can clearly classifyagiveninputtoitscorrespondinglabel.specifically neuralnetworkmodelscanbeconsideredaslayersofprogramstatements connectedbyaparametermatrix.givenasetoftrainingdata inputs withlabels outputs theknowledgeacquiredduringtraining is encoded intoparameters that connect layers.
thus it is natural toask ifsoftware testgeneration basedaugmentationtechniques which have been successfully applied to improve generalization of programsynthesisandrepair canbere purposedforrobustgeneralizationofdnns.thispaperdevelopsthisidea byemploying mutation based fuzzing for data augmentation of dnns.
it is a common practice to boost the standard generalization ofdnns usingbasicdataaugmentation where duringtheinitial training each training input is substituted by a single randomlygenerated label preservingvariant .amoresophisticated versionof thisis donein mixup a recentlyproposedstate ofthe art data augmentation technique that trains a dnn on convex combinations of pairs of data and their labels.
however as shown in section5 while thisboosts thestandard generalizationand the robustgeneralizationtoadversarialexamples ithaslimitedimpactontherobustgeneralizationto naturalvariations.further thespace of potential label preserving transformations and their parameter a standard model b state of the art c sensei figure the loss for various transformations of the motivating example after augmented training.
values is very rich .
thus na ve augmentation with all or severalvariantsisnotviableeither itwouldsignificantlyincrease the size of the training set and training time.
in the past robust optimization based on gradient descent has been used to defend dnns against adversarial examples these techniques try to generatetheworstvariant basedonalossfunction andadditto the training set.
however as shown in figure a the input space ofspatialtransformations whichareprominentlyrepresentedin naturalenvironmentalvariations forvisionapplications ishighlynon convex.thegradientdescenttechniquesperformratherpoorly in such scenarios and therefore not applicable for our problem.
evenastate of the artaugmentationapproach performspoorly for such non convex space as shown in figure b .
it is important to note that while our data augmentation technique generates and usesthegenerateddata duringinitialtraining almostalltechniques toimproverobustness generateadversarial examplesbyanalyzingatrainedmodelandsubsequently re train themodelonthisnewdata.ourevaluation section5 demonstrates thatourtechniqueprovidesbetterrobustgeneralizationthanthe latter approach.
concurrent with our work yang et al.
have pro posed an orthogonal approach to improving dnn robustness to spatialvariations .theirapproachmodifiesthelossfunction ofthednnbyaddinganinvariant inducingregularizationtermtothestandardempiricalloss.thisiscomplementarytoourproposed data augmentation based mechanism of improving robustness.
exploring the combination of these two approaches could present an interesting opportunity forfuture work.
proposedtechnique.
inthispaper weproposeanewalgorithm thatusesguidedtestgenerationtechniquestoaddressthedataaug mentationproblemforrobustgeneralizationofdnnsundernatural environmentalvariations.specifically wecastdataaugmentation problemasanoptimizationproblem andusegeneticsearchona space of the natural environmental variants of each training input data toidentify the worstvariant foraugmentation.
theiterative nature of the genetic algorithm ga is naturally overlaid on the multi epoch training schedule of the dnn where in each iteration for eachtrainingdata the ga explores asmall population of variants and selects the worst for augmentation and further uses it as theseedforthesearchinthenextepoch graduallyapproachingthe worst variant without explicitly evaluating all possible variants.
further we propose a novel heuristic technique called selective augmentation which allows skipping augmentation completely for a training data point in certain epochs based on an analysis of the dnn scurrentrobustnessaroundthatpoint.thisallowsasubstan tial reduction in the dnn s training time under augmentation.
the contributionsof this paper include fuzz testing based data augmentation to improve robustness of deep neural networks icse may seoul republic of korea weformalizethedataaugmentationforrobustgeneralizationof dnns undernaturalenvironmentalvariations asasearchproblem and solve the search problem using fuzz testing approaches specifically using genetic search.
toreducetheoverheadcausedbydataaugmentation wepropose a selective data augmentation strategy where only part of datapoints are selected to be augmented.
asapracticalrealizationoftheproposedtechnique weimplementtwo prototype tools senseiand sensei sa.
weevaluatetheproposedapproachon15dnnmodels spanning popular image data sets.
the results show that the sensei can improve the robust accuracy of allthe models compared to the stateoftheart byupto11.
and5.
onaverage.sensei sacanreduce dnn average training time by while still improving robustaccuracy.currently ourapproachhasonlybeenevaluated onimageclassificationdatasets.however conceptually itmay have wider applicability.
background .
fuzz testing fuzztestingisacommonandpracticalapproachtofindsoftware bugs or vulnerabilities where new tests are generated by mutating existingseeds inputs .byselectingtheseedstomutateandcontrolling the number of generated mutations we can effectively and efficientlyachieveacertaintestinggoal e.g.highcodecoverage .
algorithm1brieflydescribeshowgreyboxfuzzing e.g.afl works.
given a set of initial seed inputs s the fuzzer chooses s fromsinacontinuousloop.foreach s thefuzzerdeterminesthe number of tests which is called the energyofs to be generated by mutating s. then we execute program pwith the newly generated tests prime line and monitor the run time behavior.
whether s primeis addedtotheseedqueueisdeterminedbyafitnessfunction line7 whichdefineshow good test s primeis toachieve a certaintesting goal.
algorithm1 test generation via greybox fuzzing input seed inputs s program p 1whiletimeout is not reached do 2s choosenext s 3energy assignenergy s 4forifrom to energy do s prime mutate s execute p s prime iffitness s prime threshold then s s s prime 9end 10end .
training deep neural networks given a dnn model mwith a set of parameters or weights rpbeing trained on a training dataset dthat consists of pairs of examples x rd drawn from a representative distribution and corresponding labels y the objective of training mis to infer optimal values of such that the aggregated loss over dcomputed bymis minimum.
following the treatment in this can beexpressed as the following minimization problem min e x y d wherel x y is a suitable cross entropy loss function for mand e x y d is a risk function that inversely measures the accuracy ofmoveritstrainingpopulation.inpractice thesolutiontothis problem is approximated in a series of iterative refinements to the values of calledepochs.
in each epoch is updated with the objective of minimizing the loss of training data.
.
robustness of dnns dnnshavedemonstratedimpressivesuccessinawiderangeofapplications .however dnnshavealsobeenshowntobequite brittle i.e.
not robust to small changes in their input.
specifically a dnnmmay correctly classify an input xwith its corresponding labell but incorrectly classify an input x that issimilartox with label l prime wherel nequall prime.
although our ideas are broadly applicable the sequel assumes a dnn performing an image classification task.inthiscontext xisanimage and x aperceptuallysimilar to the humanuser variant of x. as discussed earlier this work targets robustness issues arising from natural environmental perturbations in the input data and notperturbations constructed adversarially in a security context.
the allowed perturbations can be represented as a neighborhood saround input x such that s x constitutes legal input formandisperceptuallysimilarto xandhencecarriesthesame labell.scan be simulated through a set of parameterized transformations t vec x t1 1 x t2 2 x ... tk k x where vec angbracketleft 1 2 ... k angbracketright includingcommonimagetransformationssuchas rotation translation brightnessorcontrastchanges etc.
asdoneby recentworkonrobustnesstestingofdnns .alternatively scan be synthesized using generative models such generative adversarial neural networks gans .
we employ the former approach.
specifically a variant x primeof image xcan be computed by applying the composition of transformations t1 t2 ... tkin sequence denoted by t o nx as x prime t vec x tk k ... t2 2 t1 1 x ... .
data augmentation for dnns since dnns self learn the relevant features from the training data they may learn irrelevant features of the specific data i.e.
overfitting and generalize poorly to other data .
to improve standard generalizationofdnnsitiscommonpracticetoperforma basic form of data augmentation where during training in each epoch eachtrainingdataisreplacedbyavariantcreatedbyrandomly applying some sources of variation or noise for example the transformations tabove .
as shown in section this basic strategyalsoboostsrobustgeneralizationbutwithsignificantroom for improvement.
data augmentation can be performed in mainly twowaysfromthetrainingperspective i duringinitialtraining syntheticdataisgeneratedon the flybasedonsomeheuristicsand thenaugmentedwiththetrainingdataduringthetrainingofthe original model ii retraining in a two staged fashion where in the first step additional data are selected based on the feedbackon the original model and then in the second step the model is retrained with the augmented data.
icse may seoul republic of korea xiang gao ripon k. saha mukul r. prasad and abhik roychoudhury sensei an automatic data augmentation framework for dnns sensei targets improving the robust generalization of a dnn intraining under natural environmental variations by effectively augmenting the training data.
in general data augmentation could involve adding removing or replacing an arbitrary number of trainingdatainputs.however sensei likeseveralaugmentation approaches implements a strategy of either replacing each data with a suitable variant or leaving it unchanged.
thus the totalsizeofthetrainingdatasetisalsounchanged.thus thekey contributionof senseiis to identifythe optimal replacement for eachtrainingdata.inaddition weintroduceanoptimizedversionof senseicalledsensei satooptimizethetrainingtimebypotentially skippingaugmentationfor somedatainputs.
.
problem formulation thetaskoftrainingadnnunderrobustgeneralizationcanbecast as modified version of equation where in addition to optimizingforparameters wealsoneedtoselect foreachtrainingdatainput x a suitable variant x prime x where s. following this can be cast as the following saddle point optimization problem min e x y d max sl x y senseiapproximatesthesolutionofthisoptimizationproblem bydecouplingtheinnermaximizationproblem whichsolvesfor from the outer minimization problem which optimizes .
this is done by allowing the usual iterative epoch based training schedule to optimize for but in each epoch for each training data x solving the inner maximization problem to find the optimal variant x .
specifically giventhesetoftransformations t vec x definingneighborhoodsandusinganoverloadeddefinitionof sintermsofthe parameter vector vec sensei solves following optimization problem definition augmentation target .given a seed training datainput xandtransformation function t vec x definingneighborhoodsofx find vec yieldingtheoptimalvariant x prime perequation2 to optimize max vec sl t vec x y .
an overview in order to solve the optimization problem defined in equation 4effectively and efficiently our proposed approach includes twonovel insights.
our first insight is that although traditional dataaugmentation techniques improve the robust generalization by training the dnn with some random variations of the data points afuzztestingbasedapproachsuchasguidedsearchmaybemore effectivetofindoptimalvariantsofdatapointstotrainthednn andhence toimprovetherobustgeneralization.oursecondinsightisthatnotalldatapointsinthetrainingdatasetaredifficulttolearn.somedatapointsrepresentidealexamplesinthetrainingsetwhile some are confusing.
therefore treating all the points similarly regardlessoftheirdifficultieslevelsmayresultinwasteofvaluable trainingtime.wemaysaveasignificantamountoftrainingtime by spending the augmentation effort on only the challenging datapoints while skipping augmenting for ideal or near ideal examples.
!
!
figure an overview of sensei for one seed image in oneepoch.givenimagesareonlyforillustrationpurposeswith out proper scaling.
algorithm2 overall algorithm input training set x y number of training epochs ne population sizepopsize crossover probability p output modelm 1epoch 2m train x y train m with original data in first epoch 3foriinrange x do 4popi randominitpopulation x 5ispwrobust i false 6end 7whileepoch nedo 8epoch epoch 9foriinrange x do ifispwrobust ithen ispwrobust i isrobust x continue selective augmentation children genpop pop i p popsize alg.
f fitness m children equation replace original data with child with highest fitness x selectbest children f popi select children f new population ispwrobust i pointwiserobust x popi 18end 19m train x y 20end figure presents an overview of the proposed framework sensei for one seed image and for one epoch.
there are two maincomponents in sensei i optimal augmentation and ii selective augmentation whichbasicallyrealizethetwoaforementionedinsights.
algorithm provides even further detail on how the overall approach is overlaid on the multi epoch training schedule of m. sensei starts training mwith the original data point in the first epoch line2 .however fromthesecondepoch the optimalaugmentation module efficiently finds the most potential variation x prime fuzz testing based data augmentation to improve robustness of deep neural networks icse may seoul republic of korea that challenges mthe most replace xwithx primeand usex primefor training line13 .the selectiveaugmentation moduleisintendedto optimize the training time.
when it is enabled sensei does not augment every data point x right away.
rather the selective augmentation module first determines whether the current state of m is robust around x. if so sensei sakeeps skipping the augmentation ofx line until mbecomes unrobust around x. note that sensei is in training data augmentation approach i.e.
data generationand augmentationhappen on the fly during training.
.
optimalaugmentation theoretically a dnn could be trained with infinite number of realistic variants x prime to increase the robust generalization.
however it is impractical to explore many variations of original data points in abrute forcefashion.therefore themainchallengeinautomatic data augmentation is identifying the optimal variations of datapoints efficiently that would force the model to learn the correct feature of the representing class.
in sensei our key insight is that sincethegeneticalgorithmiswell knowntoexplorealargesearch spaceefficientlytofindoptimalsolutionsbymimickingevolution and natural selection we can effectively employ it to find an op timal variant for each data point in each epoch to improve the robust generalization.
furthermore the iterative nature of genetic algorithmsnaturallygetsoverlaidonthemulti epochtrainingof the dnn whichmakes thesearch very efficient.
adaptinggeneticalgorithms ga toanyprobleminvolvesdesign of three main steps i representation of chromosomes ii generation of population usinggenetic operators and iii mathematical formulationof a fitness function.
.
.
representationofchromosome.
ingeneticalgorithms ga achromosome consists of a set of genesthat defines a proposed solution that the ga is trying to solve.
in sensei we representa chromosome as a set of operations that would be applied on agiven input to get the realistic variations which is basically the transformation vector vec angbracketleft 1 2 ... k angbracketrightdescribed in section ?
?.
for instance we can derive a realistic variation x prime of image x by rotating xby one degree and then translating it by one pixel simulatingthe angleand movement of camera in real life.
.
.
generation of population.
in ga a population is a set of chromosomesthatrepresentsasubsetofsolutionsinthecurrent generation.insensei theinitialpopulation whichisthepopulation in the first epoch are created randomly.
in subsequent generations epochs thepopulationis constitutedthroughtwogeneticoperators mutationandcrossover andthenthroughaselectiontechnique.
given the current population a crossover probability and population size sensei applies mutation and crossover opera tions on the chromosomes in the current population to gen erate a new population as presented in algorithm .
muta tion is performed by randomly changing a single operation change parameter in the chromosome.
crossover is done to create a new chromosome by merging two randomly selectedexisting chromosomes.
specifically given two random chromo somes c1 rotation translation shear .
andc2 rotation translation shear .
thecrossover operatoralgorithm3 generationof population input currentpopulation pop crossover probability p population sizepopsize output offspring children 1children 2whilesize children popsize do 3r u 4ifr pthen use crossover x1 x2 selectparents pop x prime x prime crossover x1 x2 7else use mutate x selectparent pop op randomselectop operations x prime mutate x op 11end 12ifisvalid x prime then children children x prime 14end 15end 16returnchildren first generates a random number rbetween and the chromosome length l and merges c1andc2by taking to rtransformations from c1andr 1t oltransformations from c2to form a new chromosome cn.
for the given example r produces cn rotation translation shear .
.senseiapplies either mutationorcrossoveroperationbasedonthegivencrossoverprob ability.itshouldbenotedthatonceanewchromosomeisgeneratedthroughthe mutationor crossover itis validated tomake sure that itiswithintherangeofeachtransformationthatwesetglobally line12 .furthermore senseialwaysappliestheresultingtransformationvector chromosome on theoriginalimage asopposed to applyingonanalready transformeddata topreventtheresulting data from being unrealistic.
once the new population is generated they are evaluated and only best set is passed as a current populationforthenextgeneration line17inalgorithm2 .thebestsetis selected through a fitness function.
.
.
fitness function.
in ga a fitness function evaluates how close a proposed solution chromosome is compared to an optimal solution.thedesignofafitnessfunctionplaysanimportantrole in ga since if the fitness function becomes the bottleneck of thesystem the entire system would be inefficient.
furthermore the fitness function should be intuitive and clearly defined to measure the quality of a given solution.
in sensei we define the fitness function based on the empirical loss of the dnn.
more specifically since the training of dnn focuses on minimizing loss across the entire training data set the variant that suffers in more loss by the dnnshouldbe used in the augmented training to make the dnn more robust.
formally floss x prime l x prime y othermetricsasfitnessfunction.
anymetricthatquantifies the quality of a dnn with respect to a test input may be used as afitnessfunction.someoftheconcreteexamplesincludeneuron icse may seoul republic of korea xiang gao ripon k. saha mukul r. prasad and abhik roychoudhury table short data set descriptions and statistics data set train test cl md description gtsrb 4germantraffic signbenchmark fm zalando s article cfr object recognition svhn digit recognition imdb 2face data setwithgender age labels cfr cifar cl classes md dnn models coverage andsurpriseadequacy .nevertheless thecomputation of the fitness function should be reasonably fast so that it does not become the bottleneck of the system.
.
selective augmentation unliketraditionaltechniquesthataugmentallthedata pointinthe trainingsetirrespectivetheirnature sensei saskipsdata points thatarealreadyclassifiedby mrobustly.therefore theselectiveaugmentation technique is solely based on the robustness analysis ofmw.r.t.
a data point x. we formalize the robustness w.r.t.
a datapoint aspoint wise robustness which could be determined based on the followingtwo kinds metrics classification basedrobustness.
amodelis point wiserobust w.r.t.
a data point xif and only if it classifies xandallthe label preserving realistic variations x prime correctly.
loss based robustness.
a model is point wise robust w.r.t.
a data point xif and only if the prediction loss of xor any label preservingrealistic variations x prime is not greater than a loss threshold.
for selective augmentation sensei sa first determines whether mis point wise robust w.r.t.
the seed.
if the seed is robust senseisa does not augment it until the seed is incorrectly classified by m insubsequentepochsorthepredictionlossby mislessthan loss threshold.
at any point mis unrobust w.r.t.
the seed sensei sa uses the optimalaugmentationmodule to augment the seed.
experimental setup we evaluatesensei with respect to three research questions rq1howeffectivelydoessenseiimprovetherobustnessofdnn models compared to state of the art approaches?
rq2howeffectivethe selectiveaugmentation moduleinreducing thetraining time?
rq3howdoesthevalueofhyper parametersaffecttheeffectiveness andefficiency of sensei?
.
datasetandmodels since computer vision is one of the most popular applications of deep learning to evaluate our approach we selected a wide range ofimageclassificationdatasetsdescribedintable1.thesedatasets cover various applications such as traffic sign classification object recognition andage genderprediction.furthermore allofthese datasets have been widely used to evaluate training algorithms adversarial attack and adversarial defense techniques.
for eachdataset columns train test and clintable1showthenumber of training testing images and the number of classes respectively.
foreachdataset wecollectedmultiplemodels column md fromopen sourcerepositories.morespecifically weselectedfour models for gtsrbfrom three models from forfashion mnist fm .
the models for cifar include wideresnet andthreeresnet modelswith20 50layers respectively which are collected from .
forsvhn w eu s e da vggmodel andamodelfrom .asforimdb weconsider twomodels vgg16and vgg19 .exceptaugmentingthe trainingdata wedonotchangetheoriginalmodelarchitectures.
all the detailed parameters can be found in repository of sensei1.
.
generationof realistic variations senseifocusesonimprovingtherobustnessofdnnmodelsbyaugmentingtrainingdatawithnaturalenvironmentalvariations.since we focus on the applications with image we choose two major kindsofimageoperations i geometricoperationsii coloroperations to simulate the camera movements and lighting conditions in real life.
to make sure the translated images are visually similar to natural ones we restrict the space of allowed perturbations following where it is applicable.
the operations and corresponding restrictions with respect to an image xare as follows rotation x d rotatexbyddegree within a range .
translation x d horizontallyorverticallytranslate xbydpixels withina range of of image size.
shear x d horizontally shear xwith a shear factor dwithin a range of .
zoom x d zoom in out xwith a zoom factor dranging brightness x d uniformlyaddorsubtractavalue dforeachpixel ofxwithina range of contrast x d scalethergbvalueofeachpixelof xbyafactor dwithinrange of theseimageoperationspreservethecontentoforiginalimage.
sinceimagesin these datasets do not have any information about the pixels outsidetheirboundary the space beyond the boundary is assumed to be constant black at every point.
.
evaluation metric since sensei is focused on improving the robustness of dnn models followingengstrometal.
wecomputerobustaccuracyof sensei to answer each research question.
more specifically robust accuracyistheproportionofimagesinthetestingdatasetwhere the prediction of a dnn does not fluctuate with any small realistic perturbations.formally letusassumethatthereisanimage xin the testing datasetthat belongs to a class c.tsis a set of parametric transformations with a size of m. applying a transformation ts tsonxgives us a transformed image x prime.x primeis the set of all transformed images resulting from ts.s o ts x prime .
a dnn is robust around xif and only if m x prime cfor allx prime x prime.
finally let usassumethat ninstances isthenumberofimagesinthetesting dataset andamongthem nrobustinstances isthenumberofimages wheremisrobust.thentherobustaccuracyof mforthedataset is fuzz testing based data augmentation to improve robustness of deep neural networks icse may seoul republic of korea robustaccuracy nrobustinstances ninstances experimental results .
implementation weimplementsenseiontopofkerasversion2.
.
whichisawidelyusedplatformthatprovidesreliableapisfortraining and testing dnns.
more specifically we implement a new data generator that augments the training data during training.
our datageneratortakesasinputsthecurrentmodelandoriginaltrainingset augmentstheoriginaldataandthenfeedstheaugmented data to the training process at each step.
the augmented data is generated and selected using the approach described in section .
.
experimentalconfigurations weconductedalltheexperimentsonamachineequippedwithtwo titanvgpusandxeonsilver4108cpu128gmemoryand16.
ubuntu.alltheexperimentspecificconfigurationsaredescribedin the respective answers.
since genetic algorithm in sensei involves random initialization and decision we ran each experiment five timesindependently and reported the arithmetic average results.
.
rq1 effectiveness of sensei we perform a comprehensive set of experiments to evaluate the effectivenessof senseicomparedtothestate of the artdataaugmentationapproaches from various perspectives.
.
.
exp does sensei solve the saddle point problem effectively?
asweexplainedinsection3.
theeffectivenessofadataaugmentation technique lies in how effectively it solves the inner maximization of the saddle point problem in equation .
therefore in our first experiment we check whether sensei is indeed effective in finding the most lossy variants effectively than the state of the art techniques.tothisend wetrainedeachmodelfollowingthreedata augmentationstrategies randomaugmentation .thisisoneofthemostfrequently used data augmentation strategies in practice since it is abuilt in feature in the keras framework.
in this approach givenasetofperturbations arandomperturbationisperformed for each image at each step epoch .
however to makethecomparisonfairwecustomizetheapproachtogive it the same combination of transformations as in sensei.
w .themostrecentdataaugmentationapproachfornaturalvariants whichiscalledworst of .w 10generates tenperturbationsrandomlyforeachimageateachstep and replacestheoriginalimagewiththeoneonwhichthemodel performs worst e.g.
highest loss .
sensei.tomakethecomparisonfairwithw theresults of sensei are generated using a population size of .
results.
figure presents the logarithmic training loss for two models gtsrb and cifar .
the results show that although senseistartswithverysimilarperformanceintheinitialepochs due to the systematic nature of sensei soon it outperforms w10 for every model and dataset.
therefore the genetic algorithm a gtsrb b cifar figure effectiveness in identifying most lossy variants intwo models gtsrb and cifar under t3 baseddataselectioninsenseiismoreeffectivetosolvetheinner maximizationproblem than randomandw 10based techniques.
.
.
exp does sensei perform better than the state of the art data augmentation techniques in any number of transformations?
it isharderto achieverobustaccuracyas thenumberoftransformationoperatorsincreasessincetherearejustmoreoptionstofoolthe model.therefore wefurtherinvestigatehowtheeffectivenessof sensei vary as the number of transformations increases.
we calculaterobustaccuracyunderthree t3 andsiximagetransformationoperators t6 separately.t3experimentationincludestherotation translation and shear image operations as defined in section .
.
results.
table2presentscoreresultsofthepaper whichshows the robust accuracy of all the models trained using the standard random w 10and sensei strategy using three and six transformation operators.
from the results using t3 we see that even though thestandard training achieves over average standard accuracy shown in column testacc the robust accuracy sharply drops to on average column .
randomaugmentation and w basedtrainingsignificantlyimprovetherobustaccuracyforeach dataset.
however sensei achieves the highest robust accuracy for allmodelsofalldata sets highlighted .senseiimprovedtherobust accuracy from .
to .
w.r.t.
random augmentation and from .
to .
w.r.t.
state of the art w .
when we increased the number of transformation operators from three t3 to six t6 we see that the robust accuracy for all augmentation strategies decreased significantly.
this is expected due to two facts i under t6 the generated variants are less similar with original images and ii under t6 a larger number of perturbations are generated for eachimage andanimageismorelikelytobeconsideredasmisclassified since an image will be regarded as misclassified if one of its perturbation is misclassified.
however in this harder problem the improvementbysenseicomparedtobothrandomaugmentation andw 10isgreaterthanthatoft3.onaverage senseiachieves .
higher robust accuracy than random and .
than w .
thisalsodemonstratesthatsenseiperformsbetterinlargersearch space.
please note that we do not evaluate the models designed for imdb with six transformation operators because face image is very sensitive to the change of color palette.
.
.
exp does sensei perform better than the adversarialexample based retraining approaches?
in section .
we briefly described how data augmentation can be performed during initialtrainingvs.adversarialretraining.theeffectivenessof sensei icse may seoul republic of korea xiang gao ripon k. saha mukul r. prasad and abhik roychoudhury table the robust accuracy for random w andsensei.
sensei uses loss based fitness function.
model size mb testaccrobustaccuracyunder3 trans.
op.
t3 robustaccuracyunder6 trans.
op.
t6 standard random w sensei standard random w sensei gtsrb .
.
.
.
.
.
.
.
.
gtsrb .
.
.
.
.
.
.
.
.
gtsrb .
.
.
.
.
.
.
.
.
gtsrb .
.
.
.
.
.
.
.
.
fm .
.
.
.
.
.
.
.
.
fm .
.
.
.
.
.
.
.
.
fm .
.
.
.
.
.
.
.
.
cifar .
.
.
.
.
.
.
.
.
cifar .
.
.
.
.
.
.
.
.
cifar .
.
.
.
.
.
.
.
.
cifar .
.
.
.
.
.
.
.
.
imdb .
.
.
.
.
imdb .
.
.
.
.
svhn .
.
.
.
.
.
.
.
.
svhn .
.
.
.
.
.
.
.
.
avg .
.
.
.
.
.
.
.
.
lies in that it effectively selects the optimal variation of the seed data epoch by epoch.
in contrast in retraining based approach once the adversarial examples are selected they are fixed across epochs.
inthis experiment we comparethese two approaches.to thisend followingthepopularretrainingapproaches we replicated the adversarial training in a benign setting.
the step includes i trainingthemodelusingtheoriginaltrainingdata ii generating the adversarial examples by our transformations i.e.
thevariantsthatfoolthednns iii selectingthebestadversarial examples adding in the training data and retraining the model for additional epochs.
to make the comparison fair we generate theequalnumberofvariantsinstep 2asof sensei.forexample if sensei generates variants per data point in one epoch and runs100epochs wegenerate1 000adversarialexamplesforagiven data point and choose the best variant of each data by the loss function.westillusetheattackmodelandevaluationmetricshown in section .
.
table average robust accuracy bysensei vs.retraining approach gtsrb fm cifar imdb svhn retraining .
.
.
.
.30sensei .
.
.
.
.
results.
table presents the average robust accuracy of all modelsforsenseiandtheretrainingbasedapproach.theresults show that sensei improves the robust accuracy from to comparedtoadversarialretraining andtheresultsareconsistent across all datasets.
therefore although adversarial retraining is very effective in a security aware setting sensei is more effective in improving robust generalization in a benign setting.
.
.
exp cansenseipreservethestandardaccuracywhileimprovingrobustgeneralization?
improvingtherobustaccuracywould onlyaddvalueif senseicanretainthestandardaccuracy.therefore we investigate how sensei performs in terms of standard accuracy.results.
table presents the averagestandard accuracy of the experimented models without data augmentation 2nd column and trained by sensei 4th column .
results show that the orig inal standard models are highly accurate.
after we augmented eachmodelbysenseitoincreasetheirrobustgeneralization the standard accuracy increased even further for four out five datasets.
.
.
exp how does sensei perform compared to other state of the art generalization techniques?
mixup is a recent data augmentation approach that improves standard generalization and also guards against pixel level perturbations in security aware settings.mixuptrains a neural network on convex combinations of pairsofexamplesandtheirlabelstofavorsimplelinearbehavior in betweentrainingexamples.mixup ssource codetoreplicatethe results for cifar is available online .
we adapted mixup in our setting verified with cifar that our result is consistent with and ranit for all the models and dataset in our study.
table average accuracy of mixup vs.sensei modelsstandard accuracy robustaccuracy stand.
mixup sensei stand.
mixup sensei gtsrb .
.
.
.
.
.
fm .
.
.
.
.
.
cifar .
.
.
.
.
.
imdb .
.
.
.
.
.
svhn .
.
.
.
.
.
results.
table4comparestheaveragerobustaccuracyachieved by mixup and sensei both for standard generalization and robust generalization across all models.
the results show that mixup and sensei both overall imp rovedstandard generalization.
in fact mixup is a little better than sensei for standard generalization.however mixup performed poorly in improving robust generalization for real world naturally occurring variants.
it performed fuzz testing based data augmentation to improve robustness of deep neural networks icse may seoul republic of korea marginallybetterthanno augmentation.incontrast senseiclearly outperformed mixup for every dataset for robust generalization.
comparison with yang et al.
.
very recently yang et al.
have proposed an approach to increase dnn robustness to spatial transformations by modifying the loss function of the dnn.
specifically theyretaintherandomw 10augmentationstrategy butaddan invariant inducing regularization term to the standard empirical loss to boost robustness.
however their technique is implementedinadifferentframework tensorflow versuskeras forsensei andtheirexperimentsareperformedonsignificantly differentdnnmodelsandwithdifferenttransformations only2 versus up to used by sensei and evaluation metrics.
this makes afair head to headquantitativecomparisoninfeasibleatpresent.
however for an initial assessment of relative performance we ran senseioncifar withamodel substantiallysimilar tooneused in and using the same two transformations.
the results show thatsenseireducestheerror theevaluationmetricusedin by23.
overw 10whiletheirtechniquereduceditby21.
as reportedin .thus senseicanmatchorsurpasstheirtechnique in this case.
conceptually sensei and improve dnn robustness throughfundamentally differentmechanisms.senseiusesa dataaugmentation approach a black box technique while yang et al.useare designedlossfunction awhite boxapproach.these techniques are thus complementary and combining them for even greater robustness improvement could be interesting future work.
senseisolvesthe innermaximization problemmore effectively than the state of the art.
it is able to improve the robust accuracyofthednn comparedtothestateoftheart oneach of the models by upto .
while retaining standard test accuracy.furthermore forbenignvariations senseioutperformsdataaugmentationapproachesboththatarebasedon adversarial retraining or that target standard generalization.
.
rq2 effect of selective data augmentation we analyze the performance of sensei sa in terms of i training time and ii robust accuracy compared to sensei and w .
notethatthestandardtrainingrequiresonlyoneforwardpropagationforcalculatingthelossandonebackwardpropagationfor calculating gradients to update the weights per data point at each epoch.however thetrainingtimewithdataaugmentationincludes the additional time for optimal selection.
specifically sensei requiresnadditional forward propagation to calculate the fitness of nnewly generated population same for w 10to find the worst case .
following the standard protocol we do not count the imagetransformationtimebecausethesetasksarecompletedon cpu whichisexecutedinparallelwiththetrainingtaskprocessed on gpu.
for this experiment we set loss threshold in sensei sa to 1e described in section .
.
table summarizes the robust accuracy and training time for allthe dataset.column improvement representsthe improvement achieved by sensei sa compared to w 10and the last column presentscohen sdvalues toshowtheeffectsizeofimprovementsoverthefiveruns.
theevaluationresultsindicatethatsensei sa significantly improve robust accuracy over w .f r o mt h etable sensei sa vs.w and sensei modelsrobustaccuracy training time min w sensei sensei sa improve d value gtsrb .
.
.
.
.
.
.
.
fm .
.
.
.
.
.
.
.
cifar .
.
.
.
.
imdb .
.
.
.
.
svhn .
.
.
.
.
.
.
.
results wecanalsoseethatwiththehelpofselectiveaugmentation sensei sa reduced the training time by on average.
the trainingtimeofmodelsfor imdbisnotsignificantlyreduced because predicting the age of a person is very hard even for human and onlyveryfewdatapointsarerobustenoughtobeexcludedfrom augmentation.
it should be noted that although sensei sa selectively augmented the data points on average the robust accuracy achieved by sensei sa is higher than that of w .
however compared to sensei sensei sa reduced the robust accuracy by .
.
for mostof the models.
on average sensei sa reduces the training time by whileit improves robust accuracy by compared to w .
.
rq3 sensitivity of hyper parameters in this section we evaluate how the choice of different hyperparametersinfluencethe performance of sensei.
figure robust accuracy and normalized training time for one model in a gtsrb b cifar and c fm bysensei .
.
populationsize.
sincepopulationsizeisanimportantparameter in any genetic search we evaluate sensei using different population size.
figure presents the robust accuracy and training timeofthreemodelstrainedbysenseiwithpopulationsizeranging .
the training time in the graph is normalized compared tothetrainingtimeofpopulationsizeof3.theresultsshowthat therobustaccuracyineachmodelincreaseswiththeincreaseof populationsize.however ahighpopulationsizealsoimpactsthe trainingtimenegatively.resultsshowthatsenseiworksefficiently when the population size is between and .
further increase of the population size does not improve the robust accuracy a lot.
sensei works efficiently between the population size and for both robust accuracy and training time.
icse may seoul republic of korea xiang gao ripon k. saha mukul r. prasad and abhik roychoudhury table the robust accuracy of sensei with loss based and coverage based fitness function fitness gtsrb fm cifar imdb svhn loss based .
.
.
.
.
neuron coverage .
.
.
.
.
.
.
fitnessfunction.
theefficiencyofageneticsearchdependsa lotonthefitnessfunction.althoughweevaluatedsenseirigorously usingloss based fitnessfunction aswediscussedinsection3.
.
other metrics such as neuron coverage based o rsurprise adequacy canbealsousedasafitnessfunction.table6presents the robust accuracy of sensei for neuron coverage based fitness function.theresultsshowthatbothloss basedandcoverage based fitness functions achieve very similar robust accuracy.
since surprise adequacyis correlated toneuron coverage weexpecta similarperformancealsousingsurpriseadequacy.however loss basedfitnessfunctionmaybeabetterchoicesinceneuroncoverage based fitness function increases the training time by than that of loss based fitness function.
the reason is that computation of neuron coverage is more expensive than training loss.
thestate of the artdnnqualitymetricssuchasneuron coverage show similar performances in terms of robust ac curacy and thus can be used as a fitness function in sensei forrobustgeneralization.however loss based fitnessfunction maybe a better choice due to shorter training time.
table effect of selection metric in sensei sa in terms of average robust accuracy loss threshold 1e metrics gtsrb fm cifar imdb svhn loss based .
.
.
.
.
classification based .
.
.
.
.
.
.
selectionmetrics.
theperformanceof sensei saintermsof bothrobustaccuracyandtrainingtimedependsontheeffectiveness of thepoint wise robustness metric defined in section .
.
the evaluation results in table show that the loss based selection outperforms the classification based selection for all the models.
the reason is that the loss based selection is more conservative than the classification based selection.
still loss based selection is good enough to skip sufficient number of data points to achieve trainingtime reduction on average.
loss basedrobustness isbetterthan classification basedrobustnessin selective augmentation.
.
.
lossthreshold.
inloss based selection thelossthresholdis one of the important factors that may affect the effectiveness of sensei sa.figure6showsrobustaccuracyandnormalizedtrainingtimeof sensei sawithlossthresholdinrange 1e .thetraining time is normalized to standard training time.
from the results figure6 therobustaccuracyandnormalizedtrainingtime ofonemodelfrom a gtsrb b cifar c fmtrainedbysensei se with variouslossthresholds in range 1e .
as expected we observe that both the robust accuracy and training timedecreasewiththeincreaseoflossthreshold.h owever some datasets are more sensitive to the loss threshold than the others in terms of robust accuracy.
for instance the robust accuracy for the cifar 10model isverysensitive tothelossthreshold.however robust accuracy of gtsrb and fm models did not decrease a lot whenwe changed the loss threshold from 1e to 1e .
the effect of loss threshold in selective augmentation is datasetspecific.
however a value of 1e showed a balanced performance across datasets in terms of robust accuracy.
.
threatsto validity internal validity we have tried to be consistent with established practice in the choice and application of image transformations and training schedule for dnns.
for parameters specific to our technique includingpopulationsizeandfitnessfunctionforga selection function and loss threshold for selective augmentation we have performed a sensitivity analysis to justify the claims.
external validity although we used popular image datasets and several dnn models per dataset in our evaluation our results may not generalize to other datasets or models or for other applications.
further our experiments employ six commonly used imagetransformations tomodelnaturalvariations.however our results may not generalize to other sources of variations.
related work adnncanbeviewedasaspecialkindofprogram.softwareengineering techniques for automated test generation as well fortest driven program synthesis and repair have either directly inspired or have natural analogs in the area of dnn testing and training.
the contributions of these techniques relative to ours can be compared in terms of the following four facets.
test adequacy metrics.
inspired by structural code coverage criteria peietal.
proposedneuroncoveragetomeasurethe quality of a dnn s test suite.
deepgauge built upon this work and introduced a number of finer grained adequacy criteria includingk multisection neuron coverage and neuron boundary coverage.
kim et al.
also proposed a metric called surprise adequacy to select adversarial test inputs.
mode performs state differentialanalysistoidentifythebuggyfeaturesofthemodeland fuzz testing based data augmentation to improve robustness of deep neural networks icse may seoul republic of korea then performs training input selection on this basis.
our contributionisorthogonaltothesetestselectioncriteria.wedemonstrate howtoinstantiateourtechniquewitheitherstandardmodelloss or neuron coverage.
in principle sensei could be adapted to use othercriteriaas well.
testgenerationtechnique.
the earliest techniques proposed for dnn testing were in a security setting as adversarial testing initially for computer vision applications.
given an image the aim is to generate a variant with a few pixels selectively modified anadversarial instance on which the dnn mis predicts.
such techniques include fgsm jsma and so on.
at a high level these approaches model the generation of adversarialexamplesasanoptimizationproblemandsolvetheoptimizationproblem using first order methods gradient ascent .
generative machinelearning suchasgan canalsobeusedtogenerate adversarial inputs.
in contrast to the above techniques our focus is thespaceofbenignnaturalvariations suchasrotationandtranslation.
engstrom et al.
showed that such transformations yield anon convex landscape not conducive to first order methods .
thus our test generation technique uses fuzzing based on genetic search.
recently odena and goodfellow proposed tensorfuzz thatcombinescoverage guidedfuzzingwithproperty basedtestingtoexposeimplementationerrorsindnns.however theircoverage metric property oracles and fuzzing strategies are all designed around this specific objective and not suitable for our objective of data augmentation driven robustness training.
deepxplore generates test inputs that lead to exhibit different behaviors by different models for the same task.
our approach does not require multiplemodels.deeptest anddeeproad usemetamorphic testing to generate tests exposing dnn bugs in the context of anautonomousdrivingapplication.whilesensei ssearchspaceis also defined using metamorphic relation the mode of exploring the search space genetic search and incorporating them data augmentation is fundamentally different from these techniques.
test incorporation strategy.
the vast majority of dnn test generationtechniques firstuseatraineddnn togeneratethetests oradversarialinstances andthenusethemtore trainthednn toimproveitsaccuracyorrobustness.bycontrast senseifallsinthecategoryofdataaugmentationapproacheswherenewtestdataisgeneratedandusedduringtheinitialdnntraining.
as shown in our evaluation our data augmentation yields better robustness compared to the former generate and re train approach.
dataaugmentationcanbeperformedwithdifferentobjectives.
autoaugment uses reinforcement learning to find the best augmentationpoliciesinasearchspacesuchthattheneuralnetwork achievesthehighest standard accuracy.mixup isarecently proposedstate of the artdataaugmentationtechniquethattrainsaneuralnetworkonconvexcombinations ofpairsofexamples suchasimages andtheirlabels.ourevaluation section5 confirmsthat mixupimprovesbothstandardaccuracyandrobustaccuracyina security setting but performs poorly in terms of robust accuracy in abenignsetting.bycontrast sensei withacompletelydifferent search space and search strategy excels in this area.
ourworkisinspiredbythetheoreticalworkofmadryetal.
whoformulatedthegeneraldataaugmentationproblemasasaddlepointoptimizationproblem.ourworkinstantiatesapracticalsolution for that problem in the context robustness training for benignnaturalvariationsbyusingageneticsearchnaturallyoverlaid on the iterative training procedure for a dnn.
the work closestto ours is the one by engstrom et al.
who were the first to show that benign natural image perturbations notably rotationand translation can easily fool a dnn.
they proposed a simple dataaugmentationapproachrandomlysampling nperturbations and replacing the training example with the one with the worst loss.ourapproachimprovesonboththerobustaccuracyaswell as the training time of engstrom s approach by using a systematic genetic search to iteratively find the worst variant to augment and usingalocalrobustnesscalculationtosavetheaugmentationand training time.
robustmodels.
recently yangetal.
proposedanorthogonal approach to improve the robustness deep neural network modelsbymodifyingthednnlossfunctionandaddinganinvariantinducingregularizationtermtothestandardempiricalloss.conceptually this regularization based white box approach is complementary to our black box approach of data augmentation.
combining the two approaches for even greater robustness improvement could be interesting future work.
conclusion recent research has exposed the poor robustness of dnns to small perturbationstotheirinput.asimilarlackofgeneralizabilitymanifests as the over fitting problem in the case of test based program synthesis and repair techniques where test generation techniques have recently been successfully employed to augment existing specifications ofintended programbehavior.
inspiredby theseapproaches inthispaper weproposedsensei atechniqueandtool that adapts software testing methods for data augmentation ofdnns to enhance their robustness.
our technique uses genetic search to generate the most suitable variant of an input data to use for training the dnn while simultaneously identifying opportunities to accelerate training by skipping augmentation with minimal loss of robustness.
our evaluation of sensei on dnn modelsspanning popular image datasets shows that compared to thestate of the art sensei is able to improve the robust accuracy of the dnns by upto .
and on average .
while also reducing the dnn strainingtimeby .
since significant amount of decision making in public facing softwaresystemsarebeingaccomplishedviadeepneuralnetworks reasoningaboutneuralnetworkshasgainedprominence.insteadof developing verification or certification approaches this paper has espoused the approach of data augmentation via test generation to improveorrepairhyper propertiesofdeepneuralnetworks.ina broadersense thisworkalsoservesasanexampleofharnessing therichbodyofworkontesting maintenance andevolutionfor traditionalsoftware for developing ai based software systems.