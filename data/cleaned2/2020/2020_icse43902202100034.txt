deeplocalize fault localization for deep neural networks mohammad wardat department of computer science iowa state university osborn dr ames ia usa wardat iastate.eduwei le department of computer science iowa state university osborn dr ames ia usa weile iastate.eduhridesh rajan department of computer science iowa state university osborn dr ames ia usa hridesh iastate.edu abstract deep neural networks dnns are becoming an integral part of most software systems.
previous work has shown that dnns have bugs.
unfortunately existing debugging techniques don t support localizing dnn bugs because of the lack of understanding of model behaviors.
the entire dnn model appears as a black box.
to address these problems we propose an approach and a tool that automatically determines whether the model is buggy or not and identifies the root causes for dnn errors.
our key insight is that historic trends in values propagated between layers can be analyzed to identify faults and also localize faults.
to that end we first enable dynamic analysis of deep learning applications by converting it into an imperative representation and alternatively using a callback mechanism.
both mechanisms allows us to insert probes that enable dynamic analysis over the traces produced by the dnn while it is being trained on the training data.
we then conduct dynamic analysis over the traces to identify the faulty layer or hyperparameter that causes the error.
we propose an algorithm for identifying root causes by capturing any numerical error and monitoring the model during training and finding the relevance of every layer parameter on the dnn outcome.
we have collected a benchmark containing buggy models and patches that contain real errors in deep learning applications from stack overflow and github .
our benchmark can be used to evaluate automated debugging tools and repair techniques.
we have evaluated our approach using this dnn bug and patch benchmark and the results showed that our approach is much more effective than the existing debugging approach used in the state of the practice keras library.
for cases our approach was able to detect faults whereas the best debugging approach provided by keras detected faults.
our approach was able to localize bugs whereas keras did not localize any faults.
index terms deep neural networks fault location debugging program analysis deep learning bugs i. i ntroduction deep neural networks are a class of machine learning algorithms that have gained significant popularity in recent years due to their remarkable success in tasks that defy traditional algorithm techniques.
a deep neural network can be thought of as a graph where nodes called neurons are functions with adjustable weights.
the neurons of a dnn are organized in layers and edges feed output from a neuron to neurons in the next layer and eventually to the last layer called the output layer.
during the training step each training input is passed through the network to produce output.
this outputis compared to the expected output.
the difference between the actual output and the expected output measured using a function called the loss function is then used to adjust the weights of the neurons in the layers using a process called back propagation .
dnns are utilized in various software systems to make decisions.
thus software engineering for dnns has become essential.
to aid integration of the dnn in software systems a number of developers have produced industrial strength libraries and frameworks such as tensorflow cafe mxnet pytorch theano and keras to assist the programmers in designing reliable deep learning applications.
recent work has shown that applications that use dnn have bugs .
same group of researchers have also studied repair strategies for dnn .
zhang et al.
describe the challenges and limitations in detecting and localizing the bugs in the dnn model and indicate that current approaches are not effective to examine the state of the model at some point like the regular programs.
islam et al.
observe that dnn bug fix patterns are distinctive compared to traditional bug fix patterns and that dnn bug localization is among the major challenges faced by developers when fixing bugs .
despite the growing number of software debugging techniques such as automated bug repair fault localization delta debugging and slicing these techniques are still not applicable to identify the bugs in the dnn models and identify the faulty statements that cause the problem at a particular layer in the model.
regular software programs and the dnn models are fundamentally different with respect to fault and fault detection.
for example regular software programs are tested by comparing the actual output and the expected output.
if actual output doesn t match the expected output then we consider the program has a bug.
on the other hand the dnn based software has a complex structure and it is learning from a training dataset.
if the dnn produces incorrect classification during training we call it failure case it is not necessarily that dnn contains a bug because a dnn model cannot guarantee correct classifications.
furthermore the logic of a regular program is represented in terms of control flow while dnn programs use weights between neurons and different kinds of activation functions ieee acm 43rd international conference on software engineering icse .
ieee for similar purposes.
these differences make debugging and testing of software that deploys dnns challenging.
traditional practices for debugging uses aides such as print statements breakpoints and tracing the failing test case.
these manual debugging processes take a long time and effort from developers .
researchers have proposed several automated fault localization techniques .
these techniques are used to locate the root causes and understand the faulty states.
unfortunately current automated fault localization techniques cannot be applied directly to dnn since existing techniques are not able to identify plausible and distinct root causes for unexpected behavior known as failure in dnns.
to overcome these challenges this work introduces a white box based fault localization technique for dnns.
our approach requires the source code of the dnn model and the training data.
given the source code our approach enables dynamic trace collection for dnn.
we propose two techniques.
the first technique inspired by translates the code into an intermediate form which we call imperative representation of the dnn .
the purpose of the imperative representation is to make certain ensure that internal states of the dnn are observable thus our method uses a white box approach .
this conversion to an imperative representation allows us to insert probes that enable dynamic analysis over the traces produced by the dnn while it is being trained on the training data.
the second technique uses a novel callback mechanism to insert probes that also achieve the same purpose.
we then conduct dynamic analysis over the traces to identify the faulty layer or hyperparameter that causes the error.
we also propose an algorithm to identify root causes by capturing any numerical error and monitoring the model during training and finding the relevance of every layer parameter on the dnn outcome.
we have implemented our approach as an extension of the widely used keras library for deep learning.
to evaluate we have collected a benchmark containing buggy models and patches that contain real errors in deep learning application from stack overflow and github .
our benchmark can be used to evaluate automated debugging tools and repair techniques.
we compare our approach with three built in mechanisms for debugging in the keras library the state of the art in dnn libraries.
these mechanisms were terminateonnan earlystopping loss and earlystopping accuracy .
we have evaluated our approach using this dnn bug and patch benchmark and the result shows that our approach is much more effective than the existing debugging approach used in the state of the practice keras library.
for cases our approach was able to detect faults whereas the best debugging approach provided by keras detected faults.
our approach was able to localize bugs whereas keras did not localize any faults.
in summary this paper makes the following contributions we propose the first fault localization approach for dnns including callback and translation mechanisms for collecting dynamic traces and a localization algorithm .
we have built a dnn bug and patch benchmark with different types of buggy models from stack overflow andgithub .
this benchmark serves as the ground truth to evaluate our approach.
we also hope it can serve other researchers to validate their debugging and repair tools.
this benchmark is available from github .
our results show that our approach can effectively and efficiently identify out of buggy model and determine the root causes for out of buggy model.
outline ii motivates our approach.
iii presents our dynamic trace collection faulty detection and localization algorithms.
iv presents evaluation.
v discusses the threats to validity vi discusses related works and vii concludes.
ii.
a m otivating example listing shows a simple example from stack overflow to motivate our work.
in this example the developer is constructing a sequential model at line adding a dense input layer at lines adding a dense hidden layer at lines compiling the model to convert it to a graphical form on line and training this model on line .
a dense layer is a layer in a dnn where each neuron is connected to neurons in the next layer.
this dnn did not learn during training and in the post the developer asked why the model achieved low accuracy.
this dnn has a two problems.
first it handles a classification problem and thus categorical crossentropy should be used as a loss function at line instead ofmean squared error .
second the user has not added activation functions for the first two layers at lines and .
listing dnn is not learning 1model sequential 2layer1 dense input shape input shape kernel initializer randomnormal stddev bias initializer randomnormal stddev 3model .add layer1 4layer2 dense kernel initializer randomnormal stddev bias initializer randomnormal stddev 5model .add layer2 6model .compile optimizer sgd lr .
loss mean squared error metrics accuracy 7model .fit x train y train batch size epochs verbose keras provides a set of callback methods to give the developers information about internal status of the training process.
specifically we can use terminateonnan to monitor the loss and terminate the training when the loss becomes nan.
we can use earlystopping to monitor the loss or accuracy and stop if there is no improvement.
we can pass a callback method as a parameter to thefit method.
for listing when using terminateonnan earlystopping loss earlystopping accuracy and the union of the three keras methods the training was terminated after .
.
.
and .
seconds respectively.
once the training is stopped keras prints the epoch and the iteration number.
unfortunately such information cannot answer the developer s question and 252model preparation dynamic analysis numerical error yes check no update no model stop learning deep learning application feed forward backpropagation weight weight loss accuracy before activation after activation statistical analysis mean std dev yesinstrumentation imperative program callback apitranslate addfig.
an overview of deeplocalize.
left component shows two alternatives callback and translation for preparing models to collect dynamic traces.
middle component collects and analyzes these traces.
right component detects localizes bugs.
indicate which layer or hyperparameter prevented the model from learning.
our approach reports that the program has a bug after .
seconds using our tool and .
seconds using our callback function.
in addition we report that the bug is located in the back propagation stage of layer at line .
therefore the message gives the developer hint that the issue in the parameter of layer since the description of the message indicates the stage of the problem the developer can quickly determine previous calculation that causes the problem which is the loss function in our example.
compared to the existing methods inkeras our fault localization uses less time and provides a report stating the layer that the bug is located at.
iii.
a pproach a. an overview figure provides an overview of our work.
in the first step we prepare the dnn model to collect dynamic traces.
we propose two mechanism for this.
our first approach translates the deep learning program into an imperative program .
probes are inserted in this imperative program to capture and save model variables such as weights learning rate gradients during training.
our second approach uses a callback mechanism and passes a specialized callback method as a parameter during model training to the fit method .
this custom callback function allows the developers to capture and save model variables.
we record the key values during both the feed forward and backward propagation phases.
during the training an online statistical analysis is performed to compare the status of the program with the error conditions we defined.
finally we report if the program contains a bug and in which layer and phases the bug exists that prevented learning.
b. model preparation directly analyzing a deep learning program e.g.
one shown in listing is hard as the dnn libraries provide blackbox apis and it is hard to trace important values during training.to use our approach the developer either write extra code to instrument dnn training inside fit function or rewrite their code into an imperative form.
for our second approach we identified a list of the keras library apis that are important for training and implemented models simplified versions of these api calls following the machine learning literature and the keras documentation .
we inserted the probes to these library models so that the analysis can observe the internal behaviors of dnns during training.
the callback based approach of dynamic trace collection is implemented by overriding the keras.callbacks.callback class.
since our work is focusing on monitoring during the training phase we override the method called on train batch end self batch logs none .
this overridden method invokes algorithm after each batch of training.
to use this method the developer needs to pass this custom callback function as a parameter to the fit function.
second imperative approach as shown in table i on the left to build a training model a dnn program typically starts with creating a sequential model line then add all layers lines and optimizations line and finally call compile andfit at lines .
on the right we show the imperative programs using our library models.
first lines and are not changed.
second lines and show the conversions for the dense layer.
in our code we added a name for the layers e.g.
see name fc1 so that we can report in which layer the fault is located.
we inserted instrumentation in the fit function line to observe the model variables.
currently our translation tool supports the dense dropout maxpooling convolution batchnormalization and padding layers.
also it supports popular optimization methods losses and activation functions.
the translation from a dnn program to the program that uses our library models is currently done manually.
the keras library is being rapidly evolved resulting in a large number of releases .
due to the library versioning and the frequently changing api signatures it is hard for our tool to remain compatible 253table i translation from keras code to an imperative program no keras code imperati ve program 1batch size batch size 2nbepoch nbepoch lr .
4model sequential mymodel mysequential model.add dense units activation relu input dim mymodel.insert mydense num inputs num outputs lrrate lr name fc1 mymodel.insert relu 7model.add dropout .
mymodel.insert mydropout .
model.add dense units activation relu mymodel.insert mydense num inputs num outputs lrrate lr name fc2 mymodel.insert relu model.add dropout .
mymodel.insert mydropout .
model.add dense activation softmax mymodel.insert mydense num inputs num outputs lr rate lr name fc3 mymodel.insert softmax adam optimizers.adam model.compile loss binary crossentrop y optimizer adam metrics mymodel.mycompile loss binary crossentrop y optimizer adam metrics model.fit x train ytrain batch size batch size nbepoch verbose validation data x test ytest mymodel.fit instrument x train ytrain batch size batch size epoch model.evaluate x test ytest verbose mymodel.myevaluate x test ytest this table is showing all of the changes to translate from keras code to our tool code.
the color in each row indicates the change type green added a new line red removed an existing line.
with the keras library.
c. instrumentation the training of dnn has two phases feedforward and backpropagation.
in the feedforward stage we observe and monitor the training data including both input and label the results after applying forward function the results after applying the activation function the loss value and the accuracy.
all values are collected in each iteration during training.
the second stage is the backpropagation and it is used to adjust the weight based on the errors obtained from the feedforward stage.
backpropagation uses the gradient descent method to update the weights and minimize the errors.
it is started from the output layer and the result of the output layer is reused to compute the gradient for the previous layer until it reaches the input layer.
different optimizations can be applied during backpropagation.
in the backpropagation stage we can observe and monitor the update of weights the update of bias values and weights from applying the gradient descent for each layer.
the instrumentation is inserted in the fit function we implemented for modeling the keras fit function.
it is executed automatically when the dnn program runs during training.
d. statistical analysis to detect suspect behavior next we discuss our approach for statistical analysis to detect suspect behavior of the dnn during training.
we analyze three variables the learning rate the input data and activation loss functions.
incorrect learning rate backpropagation is important to fine tune the weights based on the loss value obtained from the loss function.
the learning rate has an effect on the weight updates during the backpropagation process.
during the backpropagation process the learning library computes gradient descent iteratively.
our key insight is that the mean and standard deviation of the weights in the correct model are continuously changing during the training process.
in contrast the mean and standard deviation of the weights in the buggy model are constant.
if there is a problem in the learning rate we can detect it from the output of the gradient for the output layer.
figure and figure show an example of this behavior.
in figure the weight varies as it should in a correct model whereas in figure the weight is constant indicating a potential bug.
to utilize this insight we compute the mean and the standard deviation for the output of the gradient as well as the weight parameter at each layer.
incorrect input data in some cases the training data are not properly normalized.
for example in the mnist model the pixel should be in the range and not .
also training data may have nan value and developers forget to invoke the assert function to check if there is an nan or not.
in the forward stage we retrieve the output before after applying the activation function for each year.
we then compute the mean and standard deviation for the output at each layer.
we will check if the output of the first layer after before applying activation has numerical error such as nan orinf.
our second approach for detecting this kind of error is to calculate how frequent the mean of the output for the first layer is equal to zero.
once we observe abnormal values we will report in which layer and whether it is before or after activation function that the error occurs.
incorrect activation loss functions after finishing the forward stage we compute loss and accuracy.
there are two indicators that the model has a problem.
first if one of the two metrics has a numerical error like inf ornan.
second the loss starts increasing instead or the accuracy starts decreasing after certain iterations.
e. dnn fault localization algorithm algorithm presents our dnn fault localization algorithm.
at its core it augments the dnn learning algorithm with analysis and error checking inserted during learning.
it serves three purposes determining whether the deep learning program contains a bug reporting the fault location in which layer and which phases feed forward and backward propagation the deep learning program has a bug reporting failure .
.
.
.
.
.
.
.
500value iterationweights of layer mean stdfig.
weight of correct model .
.
.
.
.
.
.
.
.
200250300350400450500value iterationweights of layer mean std fig.
weight of buggy model information in which iteration the learning is stopped.
it takes as input the training data set including input and labeling the translated imperative program as well as the dnn parameters the batch size and the epoch .
if the bug is found the output is a message including the fault location and failure information for the bug.
at line we define two lists to store the values of loss and accuracy in each iteration.
line represents how many iterations we make through the whole training dataset.
during the training process the training dataset is divided into several smaller batches.
for example if the model has training examples divided into batches then the model needs iterations to complete one epoch.
line shows the division of the batch size.
lines run for each batch in the training dataset.
at lines the algorithm performs dynamic analysis on the forward stage and at lines it performs analysis on the backward stage.
in our callback function the override method on train batch end self batch logs none will execute at the end of each batch and performs dynamic analysis on the forward backward stage after retrieving the value of each layer before after activation function loss accuracy updating weight and gradients.
feedforward phase at line the algorithm computes the output of a feed forward layer before applying the activation function.
at line we compute the output after applying the activation function.
then we invoke ana procedure to determine if the output contains a numerical error.
as shown in table ii the message eba indicates error before activation eaa indicates error after activation and l represents the faulty layer number.
at line we compute the loss andtable ii abbreviation of crash statements no statement abbreviation error before activation eba error after activation eaa error in loss function elf error in accuracy function eaf error backward in weight ebw error backward in weight ebdw model does not learn mdl correct model cm determine if there is any numerical error at line .
as shown int table ii the elf indicates error in loss function.
if the error is not detected here we save the loss value for each iteration at line .
at line we compute the accuracy and check if there is a numerical error from accuracy at lines .
if the error is not detected here we save the accuracy value during training at line .
at lines the algorithm checks if loss is not decreasing and accuracy is not increasing value for a long time.
in both cases the algorithm computes the slope to compare the loss accuracy for current step with the loss accuracy at a step which is at least num steps behind the current step.
in this case the algorithm reports a message mdl to indicate that the model does not learn.
otherwise the training continues.
backpropagation phase during this stage the algorithm collects the weight and weight for each layer in each iteration.
at line the weight and weight are the output of a back propagation.
at line the algorithm invokes the ana procedure and pass weight to check if there is any numerical error.
the algorithm will print error message if there is any error in the weight and determines which layer causes this issue line .
in the same way at line the algorithm can determine if there is an issue in the weight in each layer by invoke the ana procedure.
if the procedure decide that there is an issue in the weight then the algorithm will return message to indicate there is bug.
finally if there are no issues in the model the algorithm will terminate after finishing training at line and print this message correct model cm .
ana is invoked at lines and to determine if the error occurs based on the current values obtained from the instrumentation.
when the dnn does not learn there can be the following symptoms the update for weight weight are incorrect the loss or accuracy is not measured on the correct scale and the loss does not decease and the accuracy does not increase after the number of iterations.
the check is conducted in ana .
this procedure takes three parameters input value layer number and location.
since ana is called at different locations in the code the location tracks whether the value comes from feed forward fw backward bw propagation or weight wt .
line defines a set of lists to store the mean value from each location for each layer.
line computes the mean value for the input.
at line the procedure will check if the mean reports nan orinf.
also the procedure will check if the mean equal to zero at line if yes then we compute 255how frequent zero occurs for all values for each layer.
if the number of zeroes is greater than a threshold line the error is detected and the procedure will return true.
at line we store the mean value in the list.
finally the procedure will return the last n element from the list as slice to check if the mean value for last n iterations is changed or not.
from figure and figure we observe the model continues to learn if the mean value is changing in each iteration.
this procedure returns true if there is a numerical error otherwise it return false.
algorithm dnn fault localization input training data input label batchsize epochs imperative program output a message regarding fault location and failure information 1losslist acculist 2fore 0toepoches do fori 0tolength input stepbatchsize do x input y label forl 0tolength layers do v1 layer forward x ifana v1 l fw then return eba l v2 layer activation v1 ifana v2 l af then return eaa l x v2 end loss computeloss v2 y ifloss is equal to nan or inf then return elf losslist append loss accuracy computeaccuracy v2 y ifaccuracy is equal to nan or inf or 0then return eaf end acculist append accuracy ifnotdecreasing acculist notincreasing losslist then return mdl end dy y forl length layers to0do v3 w layer backward dy ifana v3 l bw then return ebdw l ifana w l wt then return ebw l dy v3 end end 31end 32return cm iv.
e valuation in this section we aim to answer the following research questions rq1 validation can our technique find bugs in deep learning programs effectively?
rq2 comparison how effectively and how fast can our technique localize the faults compared to existing methodology in the keras library?1function ana input layerno location meanlist meanv alue math mean input ifmeanv alue is equal nan or inf then return true ifmeanv alue 0then freqzero iffreqzero threshold then return true end end meanlist append meanv alue slicing meanlist ifall elem slicing for elem in slicing then return true end return false table iii keras result vs imperative result post epoch iterationkeras our tool runtime result runtime result loss accuracy loss accuracy .
nan .
.
nan .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
nan .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rq3 limitation in which cases does our technique fail to report the bug and localize the faults?
a. experimental setup implementation to perform our experiments and evaluation we implemented our techniques using python and keras .
our translation based tool supports the dense dropout maxpooling convolution batchnormalization andpadding layers.
also it supports popular optimization methods losses and activation functions.
we followed the machine learning