repairing deep neural networks fix patterns and challenges md johirul islam mislam iastate.edu dept.
of computer science iowa state university atanasoff hall ames ia usarangeet pan rangeet iastate.edu dept.
of computer science iowa state university atanasoff hall ames ia usa giang nguyen gnguyen iastate.edu dept.
of computer science iowa state university atanasoff hall ames ia usahridesh rajan hridesh iastate.edu dept.
of computer science iowa state university atanasoff hall ames ia usa abstract significant interest in applying deep neural network dnn has fueled the need to support engineering of software that uses dnns.
repairing software that uses dnns is one such unmistakable se need where automated tools could be beneficial however we do not fully understand challenges to repairing and patterns that are utilized when manually repairing dnns.
what challenges should automated repair tools address?
what are the repair patterns whose automation could help developers?
which repair patterns should be assigned a higher priority for building automated bug repair tools?
this work presents a comprehensive study of bug fix patterns to address these questions.
we have studied repairs from stack overflow and repairs from github for five popular deep learning libraries caffe keras tensorflow theano and torch to understand challenges in repairs and bug repair patterns.
our key findings reveal that dnn bug fix patterns are distinctive compared to traditional bug fix patterns the most common bug fix patterns are fixing data dimension and neural network connectivity dnn bug fixes have the potential to introduce adversarial vulnerabilities dnn bug fixes frequently introduce new bugs and dnn bug localization reuse of trained model and coping with frequent releases are major challenges faced by developers when fixing bugs.
we also contribute a benchmark of dnn bug repair instances.
ccs concepts software and its engineering software defect analysis computing methodologies machine learning .
keywords deep neural networks bugs bug fix bug fix patterns acm reference format md johirul islam rangeet pan giang nguyen and hridesh rajan.
.
repairing deep neural networks fix patterns and challenges.
in 42nd international conference on software engineering icse may permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may seoul republic of korea association for computing machinery.
acm isbn .. .
.
seoul republic of korea.
acm new york ny usa pages.
https introduction the availability of big data has fueled the emergence of deep neural networks dnn .
a dnn consists of a set of layers.
each layer contains a set of nodes collecting inputs from the previous layer and feeding the output to nodes in the next layer via a set of weighted edges.
these weights are adjusted using examples called training data and set to values that minimize the difference between actual outputs of the dnn and expected outputs measured using an objective function called loss function.
the availability of big data has made it possible to accurately adjust weights for dnns containing many layers.
thus many software systems are routinely utilizing dnns.
se for dnns has thus become important.
a significant se problem in the software that uses dnns is the presence of bugs.
what are the common bugs in such software?
how do they differ?
answering these questions has the potential to fuel se research on bug detection and repair for dnns.
fortunately recent work has shed some light on this issue.
zhang et al.
have identified bug types root causes and their effects in tensorflow library for dnn.
islam et al.
have studied an even larger set of libraries including caffe keras tensorflow theano and torch to identify bug characteristics.
while prior work presents an initial study on repair patterns for tensorflow these works have not focused on the characteristics of repairs.
since repairing software that uses dnns is an unmistakable se need where automated tools could be very helpful fully understanding the challenges to repairing and patterns that are utilized when manually repairing bugs in dnns is critical.
what challenges should automated repair tools address?
what are the repair patterns whose automation could help developers?
which repair patterns should be prioritized?
motivated by these questions we conduct a comprehensive study of bug repair patterns for five dnn libraries caffe keras tensorflow theano and torch .
we leverage the dataset of dnn bugs published by islam et al.
that consists of bugs from stack overflow and bugs from github .
we then collect the code snippets used to fix these bugs from both stack overflow andgithub .
we then manually study these repairs and label them according to a classification scheme developed using the open coding approach.
to study the fix in stack overflow we study the accepted answers and answers with score from stack overflow post that fixes the bug in the original post.
to study the bug fix patterns in github we take the bug fix commits in the dataset and study the code that is changed ieee acm 42nd international conference on software engineering icse to fix the bug.
if we do not find any fixes that match our selection criteria in stack overflow and relevant fix in github we discard those bugs.
in total we have studied 320bug fix codes in stack overflow and347bug fix codes in github .
we have also analyzed these bug fixes to answer the following research questions rq1 common bug fix patterns what are the most common bug fix patterns?
rq2 fix pattern across bug types are the bug fix patterns different for different bug types?
rq3 fix pattern across libraries are the bug fix pattern different for different libraries?
rq4 risk in fix does fixing a dnn bug introduces a new bug?
rq5 challenges what are the challenges in fixing dnn bugs?
our key findings are as follows dnn bug fix patterns are distinctive compared to traditional bug fix patterns the most common bug fix patterns are fixing data dimension and network connectivity dnn bug fixes have the potential to introduce adversarial vulnerabilities dnn bug fixes frequently introduce new bugs and dnn bug localization reuse of trained model and coping with frequent releases are major challenges faced by developers when fixing bugs.
we also contribute a benchmark of dnn bug repair instances.
this benchmark is also publicly accessible .
methodology .
dataset in our study we build on the bug dataset prepared by islam et al.
to collect and to prepare the dataset of bug fixes.
the bug dataset contains 415bugs from stack overflow and 555bugs from github for different deep learning libraries as shown in table .
table summary of the bug repair dataset.
library stack overflow github bugs fixes current bugs fixes current caffe keras tensorflow theano torch total collecting stack overflow bug fixes to collect the bug fixes instack overflow bug dataset we study all the answers corresponding to the post ids in stack overflow bug dataset.
if a post has accepted an answer with code then we consider that code snippet as a fix.
if the accepted answer doesn t have code but describes what needs to be fixed in the original bug we consider those as fix as well.
if a bug post does not have an accepted answer but has an answer with scores we consider them as fixes also as score is considered as an acceptable quality metric in prior works .
following this methodology we were able to find fixes for bug related posts in the stack overflow dataset.
collecting github bug fixes to collect github bug fixes we went to the link of the buggy code snippets in the dataset.
if the code snippet was fixed in a later revision then we take those fixes.
a single line may contain multiple bugs .
a single bug fixtable summary of the bug fix patterns.
bug fix patterndefinition loss function add remove or replace the loss function.
network connectionchange node connectivity in the dnn e.g.
change weights remove edges add backward propagation.
add layer add another layer to the dnn model layer dimensionchange a layer s input and output size e.g.
to make it compatible with adjacent layers dimension data dimensionalign the input data s dimension with the layer dimension accuracy metricreplace the accuracy metric being used to measure the correctness of a model often to match better data type change the type of data given as input to the dnn activation change the activation function used in the dnn iterations change the number of times the training would be done e.g.
modify batch size epoch or add a loop versioning adapt the code to the new version of the library api contract fix api compositions so that the output of an api meets the preconditions of another api data wranglingfix the form of the data for downstream operations without modifying its intent monitor add diagnostics code to monitor training optimizer change the optimization function used by the dnn change neural architectureoverhaul the design of the dnn s architecture including a new set of layers and hyperparameters generally because changes above can t fix the bug commit might fix multiple bugs.
we consider them different fixes.
for example in the same fix api name is updated from deprecated to a new version and the dimension is also fixed.
we consider them as two different fixes.
some of the bugs are not yet fixed in the repositories and some repositories have been made private or deleted since the previous study.
we omitted those bugs.
following this methodology we collected bug fixes from github .
.
bug fix pattern classification next we created a classification scheme to manually label the bug fix dataset.
we started with the classification scheme used by pan kim and whitehead and found that their classification scheme has non ml bug fix categories and among them only fix categories are applicable for the dnn related fixes.
then we used the open coding approach to refine it to come with a pattern of different kinds of dnn specific bug fix patterns.
we conducted a pilot study where two ph.d. students individually studied the fixes to come up with a possible classification.
each student proposed a set of classes that were then reconciled during an in person meeting where all the authors were present.
in the in person meeting the authors validated the classification schemes from two individual raters and updated the classification scheme based on the outcome of the reconciliation effort under the supervision of the moderator.
our pilot study revealed that there are a number of unique bug fix patterns in our dnn setting.
therefore the classification from prior work had to be significantly modified.
the final classification is shown in table and discussed below.
1136finding we found that dnn bug fix patterns are very different from traditional bug fix patterns such as .
.
.
loss function.
this group of fixes is based on the addition removal or update of the loss function during training.
the loss function is a key parameter that helps the training process to identify the deviation from the learned and actual examples.
different kind of problems demand a different loss function e.g.
cross entropy loss is widely used in the classification problems whereas mean square error loss mse is mostly used for regression based problems.
some problems ask for a custom loss function for better training result and we group this kind of fixes into this class.
.
.
network connection.
this group of fixes changes the connection between nodes in the dnn.
a dnn is a graph where edges are the weights and bias and nodes are the elements of each layer.
for example in a dense layer the weight edges are fully connected with the next layer and the dimension of the layer determines the number of nodes to be available in that layer.
those bug fixes that reconfigure these connections for better results are classified in this category.
the changes include change of weight removing edges by pruning the network adding backward propagation etc.
.
.
add layer.
in any classification based problem there will be at least two layers in the model the input layer and the output layer.
to learn the features of the input a dnn frequently needs more intermediate layers called hidden .
this group of fixes adds more layers to the dnn to improve performance.
added layers can be dense where two consecutive layers are fully connected convolution layer where convolution function has been applied to the input dropout layer for reducing the overfitting etc.
.
.
layer dimension.
these fixes change the dimensions of the layers to make them compatible with adjacent layers and input.
.
.
data dimension.
data dimension related fix is similar to layer dimension but it is related to the input data rather than to the dnn layers.
the dimension of the data needs to be aligned with the dnn.
this type of fix is mostly needed when the input dimensions of the dnn and the data dimension do not match.
.
.
accuracy metric.
to measure the correctness of a dnn the accuracy metric is one of the key parameters to be configured.
the problem type has a huge influence on the type of accuracy metric to be used e.g.
classification problems are judged using classification accuracy f1 score or confusion matrix but these metrics are unsuitable for assessing a regression based model where logarithmic loss is more suitable.
.
.
data type.
this group of fixes changes the data type of inputs to match the dnn s expectation.
.
.
activation.
the activation function for a node in a layer of dnn maps inputs to the output.
this group of fixes changes the activation function used in a layer to better match the problem.
.
.
iterations.
this group of fixes adjusts the number of times the training process will be run.this is generally done to improve accuracy or to reduce overfitting.
these fixes include changing batch size or epoch.
in some cases developers add a loop around the entire training process.
.
.
versioning.
dnn libraries are being rapidly developed and a number of releases are not backward compatible that breaks code.
this group of fixes adapts a code to work with the new version of the dnn library.
.
.
api contract.
when the output of a dnn api is fed to the input of another dnn api operation these two operations have to be compatible.
this group of fixes adds adapters to fix incompatibilities between composed operations.
.
.
data wrangling.
data wrangling refers to changing the form of data without changing its intent.
it is generally done to fix the data for the downstream operations.
this group of fixes adds data wrangling to fix a dnn e.g.
by data shifting shuffle etc.
.
.
monitor.
the fixes in this category add code for diagnostics during the training process typically to print training statistics.
this group of fixes do not repair the flaw in the code but they help to localize the bug.
.
.
optimizer.
this group of fixes modifies the optimization algorithms used by the dnn model.
the optimization algorithm which is dependent on the problem determines the iterative process followed to improve the accuracy of the dnn model.
.
.
change neural architecture.
this group of fixes essentially re do the dnn model because the initial model was unsuitable.
.
labeling for labeling we used the classification scheme shown in table .
two ph.d. students with expertise in these dnn libraries were requested to label the fixes according to the classification scheme.
we held multiple training sessions to train the raters with the classification scheme.
we used the kappa coefficient to measure the agreement between the raters after the labeling of every bug fix patterns.
we found that the kappa coefficient was for the first labelings for the second labeling.
this high value of the cohen s kappa coefficient indicates perfect agreement between the raters.
in the presence of a moderator the repair patterns for which there was a label conflict between the raters were reconciled.
we adapted this methodology from .
following this strategy we labeled all the fixes and reconciled the labeling conflicts through moderated discussions.
the kappa score throughout the process was indicating a clear understanding and perfect agreement.
bug fix patterns in this section we explore the answer to rq1 to understand what are the most common bug fix patterns in dnn.
to answer rq1 we take the labeled dataset and statistical distribution of the bug fix patterns across different categories.
we also analyze the source code and diffs for those fixes to understand the challenges underlying those patterns.
fig.
shows the distribution of different bug fix patterns in stack overflow andgithub .
.
data dimension 1137data dimension network connection data typelayer dimensionloss functionversioningiterationsadd layerchange neural archapi contract .
.
.
.
.
.
.
.
.
.
a distribution of bug fix patterns in stack overflow labels less than .
are hidden versioning network connection data typedata dimensiondata wranglingadd layerchange neural architerationsloss functionactivation .
layer dimension .
.
.
.
.
.
.
.
.
.
b distribution of bug fix patterns in github labels less than .
are hidden figure bug fix pattern distribution table bug fixes in stack overflow so and github gh caffe keras tensorflow theano torch so gh so gh so gh so gh so gh loss function .
.
.
.
.
.
.
.
.
.
network connection14.
.
.
.
.
.
.
.
.
add layer .
.
.
.
.
.
.
.
.
.
layer dimension3.
.
.
.
.
.
.
.
.
.
data dimension .
.
.
.
.
.
.
.
.
.
accuracy metric0.
.
.
.
.
.
.
.
.
.
data type .
.
.
.
.
.
.
.
.
.
activation .
.
.
.
.
.
.
.
.
.
iterations .
.
.
.
.
.
.
.
.
.
versioning .
.
.
.
.
.
.
.
.
.
api contract .
.
.
.
.
.
.
.
.
.
data wrangling .
.
.
.
.
.
.
.
.
.
monitor .
.
.
.
.
.
.
.
.
.
optimizer .
.
.
.
.
.
.
.
.
.
change neural arch.
.
.
.
.
.
.
.
.
.
.
finding fixing data dimension is the most common bug fix pattern .
in stack overflow that can affect the robustness of dnn model.
a large number of bugs out of in stack overflow are fixed by changing the data dimension.
this suggests that most dnn models can easily be broken if the data processing pipeline changes or a different format of data is fed to the dnn.
for example in the following code snippet we see how the bug discussed in a stack overflow post1is fixed by adding a dimension to the input images.
model sequential ... model .compile model .load weights .
ddx weights .h5 img cv2.imread car .
jpeg this is is a x32 rgb image img np.array img img img.
reshape y pred model .predict classes img print y pred in the listing the developer wants to read a cifar image whose dimension is but the expected image size was .
data dimension change can be categorized into the following kinds.
resizing the input data is common e.g.
resizing an input image of shape to .
a risk in this kind of fix is the loss of information from the input due to resizing.
surprisingly this risk is never stated in the fixes presented on the bug fixes we have studied.
out of the data dimension fixes involve resizing the data.
resizing can be done in two ways downscale or upscale.
the downscale is the method where the risk due to data loss is critical from our observation.
upsampling does not have this risk of data loss and recent results suggest that adding noise to the data can potentially increase the robustness of a model .
finding of the resize related posts in stack overflow utilize the downscaling that can decrease the robustness of a dnn.
out of the data resizing post in stack overflow involves downscaling.
downscaling decreases the robustness and has shown that a simple resize downscaling operation can have a negative impact on the robustness.
during downscaling significant information loss occurs and that eventually decreases the features learned by the dnn.
a dnn trained with downscaled images will be easier to attack compared to the one trained with original images.
our findings suggest that it would be useful to verify the effect of the resizing fix on the vulnerability of the dnn.
reshape.
reshaping the input occurs when the input vector shape is changed.
for example a vector of size is changed to .
in this case no data loss happens and the tensor order is changed from 2d to 3d.
an example of this fix is presented in thestack overflow post .
the reshaping does not lead to data loss.
out of data dimension fixes involve reshaping the dimension of the input.
reshape may also involve changing the dimension through one hot encoding like the following code snippet to fix stack overflow post train labels to categorical train labels reorder.
to make this kind of dimension change the input data is ordered mostly to change the channel position.
in image classification problems channel refers to the color channels of three primary 1138colors.
height width channel represents the typical structure of a 3d image.
for example the input of shape is changed to to fix some bugs.
here the channel number is moved to the first argument from the third argument.
it can also involve changing the image dimension order format like from rgbtobgras in the following snippet for fixing stack overflow post img caffe .io.load image ak.
png img img .
convert rgb bgr finding reorder and reshaping .
of the data dimension fixes in stack overflow need an understanding of the specifications of the dnn layers as well as the libraries.
out of data dimension fixes involve reordering the dimension of inputs.
this is done because some of the libraries require dimension in a specific order.
these fixes are seen in the bugs where the developer works with multiple libraries having different channel position requirements in the image data such as stack overflow post .
dnn training can be assumed as a gradient descent based optimization problem which can be computed when all the functions utilized in the model creation are differentiable.
data should be changed in such a fashion that does not affect the gradient descent computation to avoid side effects.
in reshape andreorder the only changes occur is the addition of dimension and reordering of the values that do not impact the gradient descent computation.
so these changes theoretically have no side effects in the dnn models behavior.
.
layer dimension finding ingithub layer dimensions fixes are used more frequently .
to fix the crash related bugs .
.
ingithub data dimension related fixes involve .
of all the fixes.
on the other hand fixing the layer dimensions to make the dnn compatible with input data is a more common practice in github .
dimension related fixes can be done by analyzing the input and output of the layers by converting a neural network into a data flow graph.
this kind of fixes includes dimension reduction or addition based on the adjacent layers structure.
however these fixes can be either done by changing the data dimension to match the data with the layer dimension or vice versa.
the choice of the fix has an impact on the performance of the model.
this phenomenon is known as the curse of dimensionality .
the curse of dimensionality states that increasing or decreasing the dimension can lead to overfitting underfitting problems.
pca t sne are some examples of the dimension reduction techniques that reduce the dimension of the features but these techniques suffer from the curse of dimensionality.
to build an automated approach to avoid this side effect a tool needs to optimize the performance of the model by either changing the data dimension or the layer dimension.
automl has done some preliminary work in this field that restructures the model by changing the layer dimension and layers to increase the performance.
to the best of our knowledge no tool currently exists that analyzes both data dimension and layer dimension changes to pick the optimum operations for a dnn model.
.
version related fixes finding versioning related bug fixes are the highest .
in github indicating the high maintenance cost in dnn software due to library versioning.
we have found that in github long running projects have to fix a lot of bugs due to frequently changing versions of the dnn libraries.
a number of these fixes require changing the api signatures to match with changes in the libraries.
we have also observed a more complicated fix pattern for projects that use tensorflow library as discussed in .
.
tensorflow often makes invasive backwardincompatible changes adding difficulties to fix the introduced bugs.
this indicates that the maintenance cost in dnn software is high.
.
network connection finding network connection is a prevalent fix in both stack overflow .
and github .
to fix crash .
incorrect functionality .
and bad performance .
effects.
the tensor and data flow through the network in a dnn during forward and backward propagation or prediction.
for a smooth flow of data the end to end connectivity in the network is essential.
out of fixes require fixing or adjusting the connectivity in the network.
we have found three kinds of network connection repairs.
merge layers.
a number of repair cases fixed bugs by merging two parallel layers into a single layer.
for example the following code snippet shows a fix main branch .
add merge mode dot where two different branches are connected through dot product.
the network was disconnected in the bug leading to a crash.
add feedback loops and input layers.
in some bug fixes a feedback loop is added in the dnn model.
in some of the fixes the model is connected to the input via an input layer like the following l s t m o u t lstm i n p u t s h a p e maxlen l e n c h a r s n e t i n p u t transfer learning.
transfer learning is a popular technique that takes an already trained network with a different dataset.
then the new model modifies the last layers to support the goal of the new problem and then performs some retraining without modifying the weights and biases of the layers from the previous network.
we have observed several network connection fixes needed when the developer is attempting transfer learning.
generally these fixes change the last few layers of the dnn.
one such kind of fix is shown below from stack overflow post model final .
fit generator train generator .
flow np.
array x train np.
array y train batch size validation data test generator .
flow np.
array x test np.
array y test batch size steps per epoch len x train validation steps len x test epochs in this example the developer wants to train the imagenet with a pretrained network vgg19 that has been used for face recognization.
in this bug the developer does not provide the correct data input size that leads to an error and fix was to include a data generator that loads the training data as expected by the vgg19 model.
.
add layer finding of the add layers related fixes in stack overflow includes adding dropout layer that can increase the training time 3x.
in a dnn model adding layers helps to increase the performance and learn the features more accurately.
we have found that a vast majority of these bug fix patterns includes the addition of the dropout layer.
dropout layer helps in removing the effect of the overfitting that can also be achieved by using backpropagation.
according to backpropagation works better for the training dataset but does not work for new data.
dropout layer randomizes the structure of the architecture that helps the neural network to learn more features with every iteration.
dropout layer removes the connection between layers randomly stopping the data flow through those nodes and edges.
randomly reducing connections can have a negative impact on training time.
with dropout layers the convergence of the training takes 3x more time .
.
loss function finding among dnn hyperparameters change of loss function happens to fix .
highest of the bugs in stack overflow and .
in github that helps to enhance prediction accuracy and increase the robustness against adversarial attacks.
loss function plays a crucial role in the convergence of the training process and in getting better accuracy during prediction.
a model with wrong loss function does not learn the decision boundary of the features well and there can be overlap between the decision boundaries in the high dimensional feature space making the model vulnerable to adversarial attacks .
by a careful and deeper analysis of these loss function related fixes we have found that they can be categorized into the following kinds add new loss function.
the fixes in this category involve adding a custom or built in loss function.
out of fixes fall into this category.
in some of the fixes it is needed to modify the network connectivity for the new loss function to work.
for example in the following fix of the bug in stack overflow post the last layer is kept outside the gradient descent computation during training by adding trainable false.
dense trainable false hidden a the custom loss function was designed by the developer in such a way that all but the output layer participate to lead to the convergence of the model.
however the convergence was not successful as the output layer was actively participating in the forward and backward propagation that caused an abrupt change in the value of the loss function.
fixing these bugs require live trainable parameter analysis.
this approach will help to monitor the active trainable parameters during the training to localize and fix these bugs.
currently the developer needs to rely on theoretical knowledge to fix these bugs due to the lack of such kind of analysis frameworks.
change loss function.
instances of bug fixes fall into the category of changing the loss function.
our analysis of these fixes reveals that the choice of these loss functions is sometimes confusing.
developers need to understand the data properties and the goal of the dnn task to come up with a proper loss function.
for example in constructing dnn models for classification problems the developers are confused between the choice of binary crossentropy and categorical crossentropy as discussed in the fix of stack overflow post 457994748andstack overflow post .
the first loss function works better for the binary classification problems however when the classification problem has more than two categories one should use categorical crossentropy as a loss function to avoid poor performance.
sometimes the fix involves adding some filter to the mathematical operation used in the loss function.
for example we see the following bug fix of stack overflow post cross entropy tf.reduce sum y tf.log tf.
clip by value y conv 1e .
caused by the following line cross entropy tf.reduce sum y tf.log y conv in the above code snippet the problem is that the user will get nan values if y conv becomes negative as the logof negative numbers is undefined.
the fix adds a clipper function to filter out negative values to the logoperation.
in another fix of the same kind of bug instack overflow post softmax is used as the filtering operation that stops propagating values 0to the logoperation.
softmax tf.nn.softmax logits xent tf.reduce sum labels tf.log softmax .
commonality of fix patterns in stack overflow andgithub finding the p value is .
between the bug fix pattern distributions of stack overflow andgithub indicating commonality of bug fix patterns in stack overflow and github .
1140api bug data bug nmsb.initialization bugnmsb.logic bugs nmsb.processing bug sb.control and sequence bugsb.data flow bugsb.initialization bugsb.logic bugs sb.processing bug bug type020406080100120percentage of fix patternsaccuracy metric add layer api contract versioningactivation data type iterations change neural archoptimizer data dimension data wrangling neural connectionlayer dimension loss function monitorfigure distribution of bug fix patterns for different bug types stack overflow api bug coding bugdata bug nmsb.control and sequence bugnmsb.logic bugs nmsb.processing bug sb.control and sequence bugsb.data flow bugsb.initialization bugsb.logic bugs sb.processing bug bug type020406080100120percentage of fix patternsaccuracy metric add layer api contract versioningactivation data type iterations change neural archoptimizer data dimension data wrangling neural connectionlayer dimension loss function monitor figure distribution of bug fix patterns for different bug types github we have conducted a t test at significance level to understand the distribution of bug fix patterns in stack overflow andgithub .
the null hypothesis is h0 the distributions are the same.
the null hypothesis is to be rejected if the p value is less than or .
.
our computation shows that the p value is very high .
.
so h0can not be rejected concluding that the distributions are similar.
we also notice that though in some bug fix categories e.g.
data dimension layer dimension and versioning there is a significant difference among the stack overflow andgithub distributions the other categories have a similar share of occurrences in stack overflow and github .
this indicates that the bug fix patterns have commonality across stack overflow andgithub .
figure fix of stack overflow fix patterns across bug types to answer rq2 we analyze the correlation between the bug types in the bug dataset presented by and the bug fix patterns studied by this paper using the distribution of the bugs and their corresponding fixes.
the distribution of bug fix patterns across different bug types in stack overflow and github are shown in the fig.
and respectively.
the horizontal and the vertical axes describe the different bug types from and the percentage of different fix patterns needed to fix those bugs respectively.
finding for api bugs fixing of the specifications between apis is dominant in stack overflow and ingithub .
fixing api specifications involves changing api contracts due to api versioning and supporting inter library operations within a model.
fixing api specifications is needed due to the following reasons change of specifications due to version upgrade.
fixes in stack overflow involve changing specifications which are required due to the change of the library version.
the changes during the upgrade of the library version involves the following changes change fully qualified method names change api signature and change probabilistic behavior of the apis.
though fixes due to the change of fully qualified method names and change of api signature are well studied problems the fixes due to the change of probabilistic behavior of the apis are hard and different from traditional api changes.
localizing of these bugs are difficult due to the lack of sophisticated probabilistic analysis tools for dnn.
for example the bug discussed in stack overflow 4974206112says that the results are different in two versions of tensorflow .
the fix of this bug involves adding a dead code line that tweaks around the underlying probabilistic behavior of the apis by overriding the modified random seed.
the fix of stack overflow is shown in fig.
.
the fix adds the line xf tf.contrib.layers.flatten x before the line r tf.random uniform shape .
this addition overrides the random seed in the new version with the one in the previous version.
our observation gives the intuition that the fix of versioning bugs due to the change of the probabilistic distribution in different version needs new dnn specific probabilistic analysis techniques.
1141figure fix of stack overflow change specification to support interlibrary.
in these fixes the dnn program uses more than one library.
these bugs arise due to the similar assumption of the behavior and specifications for different apis in different libraries.
fixing of these bugs requires the expertise in both the libraries e.g.
the bug discussed in stack overflow 5449713013that is shown in fig.
.
the discussion points to an issue in the official tensorflow repository.
the solution suggested to avoid using apis from other libraries to pre process images.
however in similar scenarios the use of specialized image processing libraries is recommended to get better performance.
from fig.
and we have found that fixing the data dimension is the most prominent pattern .
for fixing data bugs in stack overflow .
for fixing data bugs in github the most prominent fix patterns are the change of data type .
and data dimension .
.
this suggests that for fixing data bugs the major changes are related to data dimensions.
this happens because the dimension of the data is very important for the correct functionality of the dnn model.
for fixing logic bugs the most common practice is to modify the network connectivity .
in stack overflow and .
in github .
a detailed discussion on network connectivity is presented in .
.
whereas a significant amount of data flow bugs can be fixed by changing the layer dimension .
in stack overflow and .
in github .
a detailed discussion on fixing layer dimension is presented in .
.
these observations give us the intuition that for fixing different types of bugs unique technical approaches might be needed.
fix patterns across libraries to answer rq3 we have studied the distribution of fix patterns across the libraries.
then we have conducted statistical pairwise t test at significance level between the libraries.
table shows the p values found from this test across the libraries.
table p value of the distribution of bugs btween the libraries library caffe keras tensorflow theano torch caffe .
.
.
.
.
keras .
.
.
.
.
tensorflow .
.
.
.
.
theano .
.
.
.
.
torch .
.
.
.
.
we assume the null hypothesis is h0 the distribution of the fix patterns across two libraries are same.
if the p value is less than or .
then we reject h0.
the p value for the library pairs caffe theano .
caffe torch .
keras tensorflow .
theano torch .
are much greater than .
so in these cases we can not reject null hypothesis.
so the libraries caffe theano and torch show similar kind of bug fix patterns.
the pair keras tensorflow form a very strong related group with a p value close to .
this suggests that similar kinds of automatic bug fix tools may be reused for caffe theano and torch after converting into a common intermediate representation.
similarly keras andtensorflow bugs can be fixed using similar technical approaches.
introduction of bugs through fixes finding of the bug fixes introduce new bugs in the code adding technical debt and maintenance costs.
to answer rq4 we have analyzed randomly chosen fixes from stack overflow to understand whether fixing a bug can introduce a new bug.
we have read the replies to the answers selected by filtering criteria discussed in .
then we have identified whether the fix introduced new bugs by analyzing all replies to the answer fixing the original bug and classify them into bug type root cause and impact using the classification scheme proposed by the prior work .
we have found that fixes in the randomly sampled dataset introduce at least one new bug in the code.
here a new bug indicates that the original bug was fixed by the solution posted however the solution introduces a new bug that is different from the original bug type.
furthermore we have compared the bug type root cause and the effect of the bugs of stack overflow posts with the newly introduced bugs and have found that only .
.
and .
of the bugs match the classification of the parent bug type root cause and impact respectively.
this result shows that a majority of the bugs introduced are of new types and their behavior is entirely different than that of the parent bugs .
in the table we have shown the distribution of the new bugs across the different libraries and how these new bugs are classified into different categories of bug type root cause and impact.
we have also found that the crash .
is the most common impact of these new bugs and a majority of these bugs are of api bug .
and the most common root cause of these bugs are api change .
that includes the change of either the signature of the api or the fully qualified name of the api.
.
and .
of the newly introduced bugs are from keras andtensorflow .caffe theano and torch related bug fixes introduce .
.
and .
new bugs respectively.
finding .
of the new bugs are from api bug .
of them are due to api change and .
of them end in a crash.
in fig.
the relation between the parent bugs root cause type and effect with the newly introduced bugs distribution has been visualized.
in this visualization the oldrepresents the parent bug and the relation has been drawn by a connection between two bug distributions.
the width of the connection determines the strength of the relation.
the perimeter covered by each bug type root cause impact depicts its overall strength.
we have found that a large section of bug fixes introduces api bug and the major reason for that is the 1142old data old nmsb.l old sb.csbold sb.lold sb.papidatasb.dfsb.i sb.logic sb.p a root cause old atc old ccm old ipsold siold utatcapicapim ips si ut b bug type old bp old crashold ifold unknownbpcrash if c impact figure bug fix pattern distribution sb.p sb.processing sb.l sb.logic df data flow sb.i sb.initialization atc absence of type checking bp bad performance if incorrect functionality table statistics of the introduction of new bugs during bug fix bug type root cause impactlibraryapi bug data bug sb.df sb.i sb.l sb.p atc apic apim ips si ut bad performance crash if caffe .
.
.
.
.
keras .
.
.
.
.
.
.
.
.
.
.
.
.
tensorflow .
.
.
theano torch .
api change that mostly due to the versioning of the apis and these fixes primarily lead to a crash and bad performance.
challenges in fixing bugs in this section we explore the answer to rq5 to identify the common challenges faced by the developers in fixing the dnn bugs.
to understand the challenges we have used a classification scheme separate from bug fix patterns.
similar to the labeling performed for bug fix patterns two raters have independently classified each post used in this study.
these classes of new challenges are described below .
dnn reuse training dnn models can be expensive because it requires sophisticated computational resources and a large amount of labeled data that might not be readily available.
this has led to the reuse of dnn models that creates unique issues such as backdoor attack injection of bias and mismatch between the intent of the pretrained dnn model and the intent of the developer.
base model resnet50 i n p u t s h a p e i n c l u d e t o p f a l s e weights imagenet p o o l i n g avg x base model .
o u t p u t x dense a c t i v a t i o n r e l u x add new l a y e r x dropout .
x add new l a y e r x dense a c t i v a t i o n r e l u x add new l a y e r x dropout .
x in the example above from stack overflow post the developer wants to train a predefined dnn model structure resnet50 using the cancer dataset.
the trained network results in overfitting as the developer was not aware of the structure of the reused model and needed to modify the network by adding dropout and dense layers to reduce the effect of overfitting.
untraceable or semi traceable error in case of a crash bug the common strategy to localize the bug is to analyze the error message.
however we have found that bug localization is very challenging in dnn software because errors and faults are non trivially related.
to illustrate consider the code snippet below from stack overflow post model s e q u e n t i a l model .add dense h i d d e n s i z e input dim i n p u t s i z e i n i t uniform model .add a c t i v a t i o n tanh .
.
.
y pred model .p r e d i c t x nn this code produces the following error trace t t r i b u t e e r r o r traceback most r e c e n t c a l l l a s t ipython input e6d32bc0d547 in module y pred model .p r e d i c t x nn def p r e d i c t s e l f x b a t c h s i z e v e r b o s e x s t a n d a r d i z e x x return s e l f .
p r e d i c t l o o p s e l f .
p r e d i c t x b a t c h s i z e v e r b o s e def p r e d i c t p r o b a s e l f x b a t c h s i z e v e r b o s e a t t r i b u t e e r r o r s e q u e n t i a l object has no a t t r i b u t e p r e d i c t from this error message a developer might start digging into the code of predict function and the sequential object however the issue is the missing compilation step.
due to this the model connection is not initialized and error propagates to the predict operation and halts the training process.
we have studied randomly bugs yielding crash from stack overflow .we have found that out of posts does not indicate any error message and in rest of the posts have a fix that does not match with the error message.
.
fast and furious releases we have previously discussed that a large number of fixes are due to the rapid versioning and invasive changes in dnn libraries.
table tensorflow api changes.
change of operations changed in comparison to the previous version.
version of symbols change release date v2.
beta jun v1.
.
jun v1.
.
feb v1.
.
nov v1.
.
sep v1.
.
n a aug to study this challenge we have labeled all removed reconfigured or renamed operations of tensorflow from version .
to .
latest in june .
in table we have shown the number of symbols of operations available for each tensorflow releases and the number of operations that have been deleted renamed or reconfigured in comparison to the previous version.
we have found that from the v1.
to v2.
of the operations have been changed .
we have also studied keras v2.
v2.
v2.
and v2.
to understand whether this problem is only prevalent in tensorflow or not.
our study has found that during the transition from v2.
v2.
v2.
v. .
and v2.2v2.
the percentage of changes in operation are and respectively.
a non trivial challenge for repairing dnn software is the probabilistic behavior of the apis.
some of these version upgrades also change the probabilistic behavior of the apis causing some difficult bugs.
an example is presented below where the change of the probabilistic distribution changes the output of the same operation with different versions16.
with tensorflow .
z3 with tensorflow .
z3 threats to validity external threat.
a source of external threat can be the dataset used to study the bug repair pattern.
to alleviate this threat we use a benchmark dataset of dnn bugs prepared by .
internal threat.
an internal threat can be the coding approach used to classify the bug fix patterns.
we use the widely adopted open coding approach to come with a classification scheme to minimize this threat.
two ph.d. students independently came up with the classification schemes.
then these schemes were merged through moderated discussions.
the expertise of the raters can be another source of an internal threat.
we alleviate this threat by involving raters who have expertise in both the dnn libraries and the bug fix patterns.
the raters were also trained on the coding scheme before the labeling.
we also use kappa coefficient to measure the inter rater agreement throughout the labeling process.
and the value of kappa coefficient indicates that the labeling was successful with a perfect agreement.
threat is the number of posts in stack overflow for each library are not the same.
to mitigate this threat we have performed an anova test on the stack overflow bug fix patterns.
we have found that f .
f critical .
that implies that the distribution of the bug fix in stack overflow is not unbalanced.
discussion we have analyzed the bug fix patterns in dnn and have found that there are significantly different new patterns compared to the non ml bug fix patterns.
there are also new challenges in fixing these bugs.
in the analyses of rq1 we have found that major fixes are related to data dimension layer dimension network connection addition of layer and loss function.
to fix such issues we need to know the structure of the network how data flowing through the network is modified through various mathematical operations how the performance evolves during forward and backward propagation due to the use of loss function accuracy metrics etc.
this presents a number of immediate research challenges related to dnn api design and verification.
to apply the data related fixes we need to understand the implicit dependencies between the data and model.
this problem is akin to the notion of implicit coupling between modules.
studying various existing techniques to address strongly coupled data could be beneficial to fix the data related problems.
to fix the connections among consecutive layers in a neural network the dnn model needs to be converted to a suitable common intermediate representation ir .
then we need to perform a reachability analysis to find the portion of the graph disconnected from the rest to fix such connection related problems.
also the fixes related to the addition of layer and change of loss function can be addressed automatically by mining specifications related to such layers and loss function from large codebases .
in rq1 we have also shown that some of the bug fixes have a negative impact on the robustness of dnn models .
studying such cases further and developing tips for new developers is necessary so that they avoid falling into these traps without this knowledge.
in rq2 we have seen that bug fixes are different for different bug types.
we have noticed that fixing api bugs require fixing the specification between apis.
these fixes can be achieved by validating the compatibility among apis by adding robust test suites before releasing new versions.
in rq3 we have identified the commonalities among the fix patterns of different libraries.
efforts on repairing bugs in dnns are certainly needed and they can focus on these commonalities to cover more ground quickly.
in rq4 we have observed that fixing bugs can lead to new bugs.
our findings identifies some common situations where this happens e.g.
fixing layer dimension has the possibility of adding data bugs.
we concluded our analyses by showing that new challenges are present in fixing dnn bugs.
though some of these fixing strategies have been adopted by existing tools more work on validation and repair is warranted in several se sub communities such as program analysis runtime verification formal methods etc.
analysis representation specific to dnns can be developed to enable repair work.
runtime monitoring framework for dnns would be useful to prevent errors from occurring and to collect traces for dynamic analyses based repair techniques.
safety critical data science applications of dnns need these efforts to become more dependable .
related works the closest related works are that by zhang et al.
islam et al.
and pan kim and whitehead .
study on traditional non dnn bugs.
pan kim and whitehead have studied seven java projects to discuss the bug fix patterns in these projects.
they have also proposed a classification scheme in categorizing the bug fix patterns in java.
the classification includes broad categories and a total of lower level categories.
this prior research suggests that the if related and method call mc related bugs are most frequent.
in dnn bug fix strategies the mc and sequence addition or deletion related bug fix pattern is present.
we do not find any evidence of other bug fix strategies in dnn and that has inspired us to derive a classification scheme using the open coding approach to classify the dnn bug fix patterns.
programming bugs are well studied in software engineering.
there is a rich body of empirical studies on bugs e.g.
and bug repair e.g.
however these works have not studied dnn bugs and repairs that have their own set of unique challenges .
study on dnn bugs.
zhang et al.
have studied bug patterns intensorflow using both github and stack overflow .
they have discussed the new patterns and characteristics of the bugs by tensorflow users to write dnn applications.
they have also discussed the three new challenges in detecting and localizing these bugs.
the study was limited to tensorflow and also does not discuss the bug fix patterns.
we generalize to a number of deep learning libraries and identify the new patterns of fixing the bugs in dnn software.
we also discuss the new challenges in fixing these bugs.
we have discussed three new challenges in fixing dnn bugs.
islam et al.
have studied five different dnn libraries.
they have done a more general study on dnn bug characteristics beyond tensorflow .
however their work has not discussed the bug fix patterns and challenges in fixing those bugs.
our work focuses on the identification of the bug fix patterns and challenges in those fixes.
we have utilized the dataset prepared by this work.
sun et al.
studied the issues in ml libraries scikit learn caffe and paddle to understand the bug fix patterns in these libraries.
however we study the dnn models created using dnn libraries.
our findings do not have any commonalities.
zhang et al.
studied stack overflow bug related posts for tensorflow pytorch and deeplearning4j to classify the questions into different categories and built an automated tool that categorizes questions based on the frequently found words from each category and computing the tf idf value with respect to the keywords.
also the authors have studied the challenges of answering the question in stack overflow by calculating the response time for each category and have found categories of root causes for the bug related posts.
whereas our study has been on the bug fixing strategies for dnn libraries.
pham et al.
studied three deep learning dl libraries tensorflow cntk and theano to localize and detect deep learning bugs using cross validating different backend i.e.
tensorflow cntk.
in contrast our work studied five dl libraries using bug fix commits from github and stack overflow posts that allowed us to draw interlibrary observations of the fixing strategies.
also our work expanded the study to include deeper discussion about each fixpattern the common challenges developers face while fixing these bugs and how fixing bugs introduces a new bug.
conclusion and future work the widespread adoption of deep neural networks in software systems has fueled the need for software engineering practices specific to dnns.
previous work has shown that like traditional software dnn software is prone to bugs albeit with very different characteristics.
it is important to further understand the characteristics of bug fixes to inform strategies for repairing dnn software that has these bugs.
how do developers go about fixing these bugs?
what challenges should automated repair tools address?
to that end we conducted a comprehensive study to understand how bugs are fixed in the dnn software.
our study has led to several findings.
first of all we find that bug fix patterns in dnn are significantly different from traditional bug fix patterns.
second our results show that fixing the incompatibility between the data and the dnn alone can be of significant help to developers of dnn software especially if the developers can be warned about the impact of their bug fix on the robustness of the dnn model.
third our study shows that a prevalent bug fix pattern is version upgrade.
while version upgrade is well studied in se research our bug fix patterns suggest that automated repair tools will need to address at least two unique challenges invasive backward incompatible changes and probabilistic behavior change.
fourth our study shows that the structure of the dnn itself needs to be represented in repair tools because several fix patterns rely on identifying incompatibilities in that structure.
for instance network connection fixes where disconnected layers are identified and connected or adding missing layers etc.
fifth we have found that a significant number of bug fixes introduce new bugs in the code.
finally we have identified three challenges for fixing bugs bug localization is very difficult reuse of the dnn model is hard because of limited insights into its behavior and keeping up with rapid releases is hard.
this study opens up several avenues for future work.
first and perhaps most immediately a number of bug fix patterns identified by this work can be automated in repair tools.
such tools for bug repairs can help the developers integrating dnn into their software.
second an abstract representation of the dnn along with the code that uses it can be developed.
we saw several bug fix patterns that rely on analyzing such a representation.
third there is a critical need to improve bug localization for dnn by addressing unique challenges that arise and by creating dnn aware bug localization tools.
fourth there is an urgent need to detect bugs introduced by dimension mismatch and specially changes that have the potential to introduce vulnerabilities in the dnns.
fifth urgent work is needed on upgrade tools that encode the semantics of version changes and keep up with the change in the signature and semantics of dnn libraries.
this is important to keep pace with rapid development in this area.