probingmodel signal awarenessvia prediction preservinginput minimization sahilsuneja ibmresearch yorktownheights ny usa suneja us.ibm.comyunhui zheng ibmresearch yorktownheights ny usa zhengyu us.ibm.comyufanzhuang ibmresearch yorktownheights ny usa yufan.zhuang ibm.com jima.laredo ibmresearch yorktownheights ny usa laredoj us.ibm.comalessandromorari ibmresearch yorktownheights ny usa amorari us.ibm.com abstract thisworkexploresthesignalawarenessofaimodelsforsource codeunderstanding.usingasoftwarevulnerabilitydetectionuse case we evaluate the models ability to capture the correct vulnerability signals to produce their predictions.
our predictionpreserving input minimization p2im approach systematically reduces the original source code to a minimal snippet which a model needs to maintain its prediction.
the model s reliance on incorrectsignalsisthenuncoveredwhenthevulnerabilityinthe original code ismissing in the minimal snippet both of which the modelhoweverpredictsasbeingvulnerable.wemeasurethesignal awarenessofmodelsusinganewmetricwepropose signal aware recall sar .
we apply p2imon three different neural network architectures across multiple datasets.
the results show a sharp drop in the model s recall from the high 90s to sub 60s with the newmetric highlightingthatthemodelsarepresumablypicking up a lot of noise or dataset nuances while learning their vulnerability detection logic.
although the drop in model performance may be perceived as an adversarial attack but this isn t p2im s objective.theideaisrathertouncoverthesignal awarenessofa black box model in a data driven manner via controlled queries.
sar s purpose is to measure the impact of task agnostic model training andnottosuggestashortcomingintherecallmetric.the expectation infact isforsartomatchrecallintheidealscenario where the modeltruly captures task specific signals.
introduction recently ai research has made inroads in source code understanding and being able to perform tasks such as variable naming function naming summarygeneration variablemisusedetection and vulnerabilitydetection amongstothers .evermore sophisticated models are emerging and pushing the state of the art rapidly.althougheachnewmodelimprovesuponitspredecessor s predictionperformanceintermsoff1andaccuracymeasures what remainsrelativelyunexplorediswhetherthemodelsarepicking uptherightsignals to arrive at theirpredictions.
we call this aspect of the model s quality signal awareness and explore it using a software vulnerability detection use case.
what used to be a domain traditionally dominated by static and dynamic analysis is seeing assistance and competition from ai authorscontributed equally to thisresearch.models.
the high false positives of static analyzers and the lack of completeness of dynamic analysis are a few reasons promoting theentryofaiintothisfield .however unliketherules andpath flowanalysisofstaticanalyzersandtheexecutiontracing ofdynamicanalysis itremainsunclearastowhatsignalstheai modelsactuallypickupfordetectingvulnerabilitiesinsourcecode.
fromamodel sperspective itmightbe learning a separator between healthy and buggy code samples.
but itmayverywellbethe casethatitdoessobypicking upnoiseor certainnuancesfromthedatasetfortheirpredictions whichare not representative or even related to vulnerabilities.
although this canleadtohighaccuracyscoresforai baseddetectors whichmay beperfectlyacceptableinatheoreticalorstatisticalsetting thiscan leadtoafalsesenseofsecuritywhenappliedtoreal worldusage as substitutesfor traditional sourcecode analyzers.
to this end in this paper we present a systematic approach to uncover a model s vulnerability detection logicand evaluate its ability to capture real signals.
we use a data driven explainabilityapproachakin to howthe model wouldhavedeveloped its data drivenlogic.weborrowafaultisolationtechniquefromthe software engineering domain called delta debugging .
the coreideaofour prediction preservinginputminimization p2im approach is to first reduce the original source code input to a trained model into a minimalsnippet without changing the model prediction.
and then to verify whether the minimal snippet has the same vulnerabilityprofileastheoriginalcode.themodel srelianceon incorrect signals can then be uncovered when a vulnerability in the original code is missing in the minimalsnippet both of which themodelhoweverpredictsasbeingvulnerable.additionally we presentanewmetriccalled signal awarerecall sar tomeasure howwell amodelcaptures task specific signals.
we apply p2imon three different neural network architectures operating at different popular representations of source code in theaidomain i aconvolutionalneuralnetwork cnn treating codeasaphoto ii arecurrentneuralnetwork rnn treatingcode as a linear sequence of tokens and iii a graph neural network gnn operating on code as a graph.
we apply these models for vulnerability detection over three different datasets including a real worlddatasetweextractfromgithub.resultsshowmorethan of samples can be reduced while maintaining their vulnerable prediction across datasets and models.
however we observe a sharp performance drop across all models when probing them forsahilsuneja yunhuizheng yufanzhuang jima.laredo andalessandro morari signalawareness withrecalldroppingfromthehigh90stolow60s withournewmetric suggestingthatthemodelsarepresumably pickingupfeatures irrelevantto the taskat hand.
our work calls for a signal aware supplement to the traditional statisticalmeasurementsforaimodelsandeffortstofocusmore on therealsignals during model training.
note that we are not suggestingashortcomingintherecallmetric.infact theexpectation isforthemodel ssartoreachitsrecallintheidealscenariowhere themodeltrulycapturedtask specificsignals.
westronglybelieve that the models are powerful enough to pick such relevant signals withappropriate guidanceduringtraining.buttomotivateresearch inthatdirection theshortcomingsofcurrenttask agnostictraining needtoberevealed whichispreciselywhatwetackleinthiswork.
in ourp2imapproach to probe a model s signal awareness we maintain utmost fairness to the model.
we keep the model untouched and never alter its training process or change its training distributionatall whichisperformedontheoriginalunmodified dataset.
the model s recall and sar are evaluated on the dataset s original test set itself.
note that this work does not identify whata model is learning but rather ifthe model is learning vulnerabilityrelatedsignals.wetreatthetrainedmodelasablackboxandquery itforitspredictiononiterativelysmallerversionsforeachvulnerable sample from the test set.
with such input perturbations our goalisnottocraftprogramsforadversarialattacks but to uncover the signal awareness of a black box model in a data driven manner via controlled queries.
that is at a very superficial level we ask the trained model if it feels a code sample aaaa is vulnerable where a represents any atomic code chunk granularity then if it feels aaa is vulnerable then if it feels aa isvulnerable andsoonuntilit sverdictchanges.wethentestif theminimalreduced version which preserves the model s verdict actually contains the original ground truth vulnerability.
in our experiments weusetheinfertool toverifybugexistencein the reduced samples.
however p2imis independent of the specific bug checkerbeingemployed.
toensuremodelfairness weonly feed valid compilable reduced samples to the model for its verdict withoutintroducing any newbugs.
our method augments the common metrics toolbox with an alternativewaytoexaminethemodelquality givingreliabilityand trustworthinesstoblack boxaimodels.
p2imbringsinmultiple enhancementsascomparedtopopularperturbation basedmodel interpretabilitymethodsthatworkonindividualinputinstances suchaslime andothersassummarizedinarecentsurvey .
while other approaches are able to derive parts of an input that contribute most to the model s final prediction unlike our method theycannottellifthehighlightedpartsarethecorrecttask relevant signals.anothercontrastingcapability p2imoffersistoquantify how well a model learns the correct signals thereby providing a signal aware mechanism to compare different models.
this is especially useful when competing models have comparable performance on traditional metrics e.g.
f1 .
furthermore the search space and thus the time complexity for such approaches can be huge given thepossiblecombinationsof thedifferentpartsofthe inputstobeexplored.thankstothedeltadebuggingalgorithm p2imdirectly minimizes the input significantly accelerating the searchfor the relevantparts ofthe input.thesalientfeaturesofourprediction preservinginputminimizationapproach p2im are summarizedas follows black box p2imrequires no knowledge about the target model s internals and is applicable to all model types including classic machine learning as well as neural networks.
externally verifiable emittedminimalcodeisvalidand compilable enablingcross verificationwithtraditionalstatic anddynamic analysistools.
new metric we present a new signal aware recall metric to measure howwell a modelcaptures task specific signals.
thisenablesamorefairmodelevaluationandmeaningful comparison.
this paper is organized as follows.
we first discuss our motivation behind this work in section and then present a brief background on ai models for source code as well as the delta debugging technique in section .
then we present the details of ourp2imapproachinsection4 alongwithafewexamplesdemonstratingp2imin action.
we evaluate the models signal awareness usingourapproachinsection5.section6coversrelatedwork and finally section 7concludes this paper.
motivation the rapid proliferation of ai for source code understanding is leadingtoevermoresophisticatedmodels whicharegettingbiggerand better with each successive iteration.
we have been experimenting withthestateofthearttoassessitsqualityfromapracticalperspective.wenoticedthatthemodelsseemtosufferfromweakgenerality and robustness known ai frailties.
these concerns become ever moreimportantifthemodelsaretobeappliedtosensitivetasks such as ensuring code security.
we observed the same weakness acrossseveraldifferentmodelsanddatasetsinavulnerabilitydetectionsetting whichledustodoubtthequalityofthemodelsin terms ofwhatis it that they are actually learning.
we thus started exploringwaysto probe the models signal awareness .
signal awareness is different than correctness a model can learn a perfect separator between buggy and healthy code but it can very well arrive at the separator by picking up dataset nuances asopposedtoreal vulnerabilitysignals.
thiscan becaused when for example the model picks up unexpected correlations betweencode samplesand samplelengths orvariable names or certainprogrammingconstructs whichmayhappentodifferfor buggyand healthy samples ina particular dataset.
learning a separator based on these non representative signals which may lead to great looking performance numbers is perfectly acceptable from a model sperspective.the model isindeed itsjobof learning to classify but this providesafalse sense of security.
signal awareness or verifying if the models are learning the correctlogicrelevanttocodeanalysisiscrucialtogeneratetrust inmodelsiftheyaretobeputintothefieldincompetitionto or alongside traditionalstaticanddynamicanalyzers.furthermore it addsanimportantmeasureofmodelquality beyondthetraditional statisticalanalysismeasures whichcanmorefairlycompareand guide improvements across model evolutions.
in this work we uncover this signal awareness aspect of ai for code models and quantify how much impact it has on their robustness as well as reportedperformance numbers.probing model signal awarenessviaprediction preservinginput minimization atypicalexampleofweakrobustnessiswhenanimageclassifier s verdict on an input image changes on adding minor noise to theimageimperceptibletothenakedeye .asshownin table1 weobservethesameissuewithai for codevulnerability detection models where evena 99f1 model flips its prediction on only slightly different code variants.
however it should be reasonabletoexpectahigh qualitymodel whichcorrectlypicksupthe real vulnerability signals to demonstrate prediction robustness if it is ever to be trusted in a practical security setting.
although such perturbations canbe taken to be an adversarial attack on the model and similarly there can be defenses against such attacks such as training the model with several different code variants.but thislineofthoughtiscomplementarytoourwork.these observationsmerelytriggeredoursuspicionaroundadisconnect betweenthereportedmodelperformancenumbersversustheactualtask awarelearning similarinspirittootherprevailingdoubts regarding modelquality .
our goal is not to discredit ai for code models by reiterating their brittleness but to uncover and quantify how much impact task agnostic training has on their reported performance numbers.
inparticular weevaluatethetrainedmodelsontheoriginaltest set itself while queryingthemodelwith iteratively reduced versions ofeachsample touncoverpreciselywhatportionoftheoriginal sample does the model consider to be relevant for its prediction.
then by counting the occurrences across the test set where the minimalportionsstillcontainthesamevulnerabilityastheoriginal samples we can measure how signal aware the model is using our proposedsarmetric.
ourhopeisthatrevealingtheseshortcomingswouldmotivatefutureresearchtowardsmoretask awarelearning potentiallyguided by the sar recall divide to better utilize the potential of ai for sourcecode understanding.
background inthissection wefirstbrieflydescribethreedifferentneuralnetwork models which have been popularly employed for learning over source code each operating at a different code representation.
theseinclude i aconvolutionalneuralnetworktreatingcodeas a photo ii a recurrent neural network treating code as a linear sequenceoftokens and iii agraphneural networkoperatingon code as a graph.
in section we shall evaluate these models on ourproposedsarmetric.afterthemodeldescriptions wecover the basics of the delta debugging technique which we use to reduce the input samples to measure the models signal awareness.
.
ai for codemodels convolutionalneuralnetworks cnns learn on image inputs.
these are made up of convolutional and pooling layers.
the former act as filters to extract features from input images learning increasinglycomplexpatternswhentheneuralnetworkbecomes progressively deeper.
pooling layers on the other hand downsample the features so as to intensify the signal and control the size of the neural network.
this is done by selecting the most strongly activated neurons i.e.
max pooling or takingtheir average i.e.
mean pooling .
cnns have been successfully employedtable bad robustness a f1 gnn model flips on only slightly syntactically different but semantically identical function variants.
dataset from model from .
in thispaper we focuson type a reductions only.
a removing tokens changesmodelpredictions !
!
!
!
!
!
!
!
.
!
!
!
!
!
!
!
!
!
.
!
!
!
!
!
!
!
!
.
!
!
!
!
!
!
!
!
!
.
ground truth buggy buggy prediction buggy non buggy b adding tokens changesmodelpredictions !
!
!
!
!
.
.
!
!
!
!
!
!
.
.
!
.
!
ground truth buggy buggy prediction buggy non buggy incomputervisiontasks suchasimagerecognitionandobjectdetection .
these have also been used in the context of vulnerability detection with slight modifications .
in such a setting the sourcecode tokens are firstprojected intoan embedding space and then fed into a cnn as a real value matrix similar to an image.
recurrentneuralnetworks rnns aredesignedtolearnover sequentialinputs suchastextandaudio .a workingmemory is maintained and updated by a series of input output and forget gates using the current input and the previous memory at eachstep .dependingontheusecase rnnscanemitvector representations at each step or emit one final representation for a complete sequence.
rnns have been applied to source code by treating it as sequences of tokens.
usually one final representationisextractedforapieceofcodeinthecontextofvulnerability detection .
graphicalneuralnetworks gnns havegainedpopularitydue totheiruniqueabilitytolearnovergraph structureddatalikesocial network graphsand molecular structures .
mostgnns are made up of three modules i message passing which decides how information is exchanged among nodesvia edges ii message aggregation whichdetermineshoweachnodecombinesthereceived messages and iii message updating which controls how each node updates their representation after one cycle of information propagation .usinggnnsonsourcecodeisanaturalfit since multiple forms of graphs can be constructed on top of source code suchasabstractsyntaxtree dataflowgraph andcontrolflow graph.theyhaveachievedstate of the artperformanceonmultiplesahilsuneja yunhuizheng yufanzhuang jima.laredo andalessandro morari softwareengineeringtasks includingvulnerabilitydetection andcode summarization .
.
deltadebugging deltadebugging dd wasfirstintroducedtominimizefailureinducingbugreportsforthemozillabrowser.manyofthesebug reports contain long html files or user actions with lots of irrelevant information which makes it challenging to understand the root causes.
therefore developers were looking for techniques thatcansimplifytestcasesandgenerateminimizedonestriggeringthesamefailures.asaresult ddsignificantlysimplifiedthe crash inducing inputs in an automated manner and hence enabled aproductive bugdiagnosisandrepairexperience.
theinputto ddisasequencesatisfyingsomepredefinedoracle.
for example it s failure inducing html files or user actions in the mozilla web browser case.
the goal is to find a subset of the input satisfyingthefollowingtworequirements thesubsetleadsto the same outcome and not a single element can be removed to preserve the outcome.suchasubsetiscalled minimal .
algorithm1 simplifieddelta debuggingalgorithm input u1d447 oraclefunction.
u1d447 u1d465 ist r.sc u.sc e.scif u1d465has certain predefined property.
u1d446 inputsequence where u1d447 u1d446 t r.sc u.sc e.sc output u1d446 reduced outcome preserving minimal sequence function dd u1d447 u1d446 u1d45b u1d446 u1d446 divide u1d446equallyinto 1 ... u1d45bandthe complements u1d458 u1d446 u1d458 where u1d458 ... u1d45b testeach t 1 ... t u1d45b andt ... t u1d45b ifallf a.sc l.sc s.sc e.scthen u1d45b u1d45b if u1d45b u1d446 then return u1d446 else gotoline else ift u1d456 t r.sc u.sc e.scthen u1d446 u1d456 u1d45b u1d446 u1d456 if u1d446 1then return u1d446 else gotoline else ift u1d457 t r.sc u.sc e.scthen u1d446 u1d457 u1d45b u1d45b u1d446 u1d457 gotoline as presented in algorithm given an input sequence u1d446and anoraclefunction u1d447 dditerativelysplitstheinputsequenceand produces minimal u1d446 infourmain steps split and test .
in each iteration ddsplits the sequence in consideration into u1d45bsegments and u1d45bcorresponding complements line3 whereacomplementisdefinedas u1d456 u1d446 u1d456.
ddtests all partitions using the provided oracle function u1d447 line and checks if some partitions lead to the same outcome.
reduceasubset .ifthetestresultofasubset u1d456isthesame as u1d446 ddtreats u1d456as the sequence for the next iteration and resets the granularity u1d45b lines11 .
reduce a complement .
otherwise if complement u1d457is an outcome preserving input line ddadjusts u1d45band explores itwiththe same granularity line20 .
operateonafinergranularity .ifnoneofthepartitioncanpreservetheoutcome line5 dddoublesthepartitionnumber u1d45bto split the sequenceintosmallersegments line6 .
in each round ddtries to reduce the scope to a subset.
in the best casescenario ddworkslikeabinarysearch whichcansystematicallyandefficiently identifythe minimal .
our intuition behind data driven model probing comes from thisfailure inducinginputsimplificationidea.specifically webuild our method atop ddwhile replacing the mozilla target with the predictionmodel andthefailure inducinghtmlfileswithvulnerableprogramsamples.
dd sprocessofidentifyingaminimalsubsequenceoftheinputwhichleadstothesameoutput thentranslates to identifying the minimal sub program minimal which preserves the model s prediction.
themodel s signal awareness is then determined by testing the minimal for the original vulnerabilityexistence.
design programming defects are an inevitable reality in software creation.
vulnerabilities arise when such defects fall in a security related subsetsuch as null pointer dereference buffer overflow use afterfree amongstothers.staticanalyzersdetectthesevulnerabilities either by reasoning about the possible execution behaviours over a programmodel orbymatchingdefect specificrules.dynamicanalysis ontheotherhand directlyexecutestheprogram exploring differentexecutionpathsto concretelyexposethedefects.
unlike thetraditionalanalyzers thelogicofai for codemodelsisimplicit and not directly perceptible.
in this section we present our approachtowardsunderstandingthislogic whiletreatingthemodels asblackboxentities.givenexplaining whatanaimodelislearning is still an open problem especially in the context of source code understanding weframeourexplorationintermsofdetecting if the models are learningthe vulnerability relevantsignals.
.1p2imworkflow figure1depictstheoverallflow behindour prediction preserving inputminimization p2im approach.thesequenceofoperations isas follows step1.p2imtakes as input a trained model and a program sample whichthemodelpredictstobevulnerable.thissamplecomesfrom the test setofthe dataset usedfor modeltraining itself.
steps .
p2imthen iteratively keeps reducing the sample and queryingthemodelforitspredictiononthereducedsubprogram so long as the model maintains its vulnerability prediction.
this process continues till a minimal snippet called minimal is extractedfromtheprogramsample suchthatremovingevenasingle tokenfromitwouldchangethemodelprediction.toefficientlyand systematically reduce a program sample we employ the popularprobing model signal awarenessviaprediction preservinginput minimization awarenessprecisely.thus wetaketheconservativeapproachof giving the benefit of doubt to the model giving it credit for capturingvulnerabilitysignalsbasedonthe1 minimalreached even when it may not actually be so based on the global minimum .
this results in an upper bound measurement of the model s signal awareness sar .
nevertheless as shall be revealed in the next section even measuring the upper bound itself is sufficient to highlight the problems of the models not picking up the correct signals duringlearning.
checkerquality .
the signal awareness measurement is bounded by the quality of the checker used to verify bug existence in the reducedsubprograms.thiscanbedatasetdependentandcaninclude i theoriginaldatasetlabeler ii aline based bugmatcherwhich gives the benefit of doubt to the model on partial matches or iii a goodstaticanalyzertunedtowardshighrecall.
fortheexperiment settings and the datasets considered in this paper section .
the inferanalyzer workedquitewell withfallbacktoline based bug matching for samples with differing infer verdict and the original bug.
although infer as a checker is a fortunate fit given our targetdatasets p2imisnotreliantonit.whileinferprovidesfor amoreaccuratesarbound asimilarlackofsignalawarenessin ai for code models isstilldetected albeit withalooser bound by replacing infer with purely a line based bug matcher less accurate moremodelfavoring .
althoughusingtheoriginaldataset labeler might be even more accurate and expands the dataset and task applicability of p2im but it can be a harder task especially with human in the loop kind of labelers.
finally the existence of a perfect checker precludes the need for ai for code analysis.
yet to introducesomeaccountabilityintoday sai for codemodels we arguefor at leastasar like sanity check.
evaluation weusethefollowingmethodologyfortestingthesignalawareness ofvulnerabilitydetectionmodels.
thetrainedmodelswillassign labels to the code samples in the test set as either being vulnerable ornot baseduponthepredictedclassprobabilities.afterprediction allvulnerablesamplesinthetestsetfallundereithertruepositives tp orfalsenegatives fn .
wesubjecteach tppredictedbythe modeltop2imreduction.wefirstquerythemodelforitsprediction on eachtpsample s minimal version.
then we check the 1minimal for the presence tp or absence fn of the original programsample sbug.thiswaywesubdivide tpintosignal aware tp andsignal agnostic fn .
operating atop vulnerable samples tp fn recallis the best metric to target to fairly compare different models on the same dataset.
this is because number of vulnerable samples tp fn tp fn fnwillbethesameforallmodelsforthesamedataset.
wepresentanewmetric signal awarerecall sar tomeasure thesignal awarenessofvulnerabilitydetectionmodels.so while recall tp tp fn sar is defined as tp tp fn fn wheretp tp fn .
then we compare the two metrics for each model to highlight how much of its reported recall is attributable to task relevantsignal learning reflectedbysar .
forscalability werun p2imreductionacrossmultiplesamplesin parallel.asanexampleoftheruntimecost ittakes min avg max secondsfor the githubdataset samples describednext.
.
datasets p2im ssignalawarenessmeasurementrequiresdatasetswithground truthbuglocations.datasetsfromdraper anddevign are excluded because they donot specify buglocations.samples from vuldeepecker andsysevr areslicesconvertedintolinear sequences not valid compilable code which models are trained uponandthusexcludedfromexperiments.therefore weusethe following three datasets which contain this granularity of linelevelbuginformation.the train validate test splitiskeptat for allexperiments.
.
.
juliet.
thejuliettestsuite containssyntheticexamples withdifferentvulnerabilitytypes designedfortestingstaticanalyzers.
from its 64k test cases we extract 118k functions amongst whichalmost35 arevulnerable.samplestaggedas bad andwith clear bug information as per manifest.xml file are labeled as whilethe ones witha good tag are labeledas .
.
.
s babi .
the authors of s babi claim that the juliet datasetisfartoosmallandcomplextouseinlearningtopredictthe labeledsecuritydefects.theirproposeds babi syntheticdataset contains syntactically valid c programs with non trivial control flow focusing solely on the buffer overflow vulnerability.
we used thes babi generatortocreateabalanceddatasetofalmost475k functions.sampleswitha unsafe tagarelabeledas1 andthose with safe as .
.
.
github dataset.
different from the synthetic s babi and juliet datasets we also include a real word dataset with bug locationandbugtypeinformation whichwederivefromthed2a dataset .d2aistrace leveldatasetbuiltovermultiplegithub projects openssl ffmpeg httpd nginxandlibtiff.
it is generatedbyusingdifferentialanalysisatoptheinferstaticanalyzer outputs of consecutive versions before and after bug fixing commits.
from d2a s traces we derive function level samples.
from each before fix trace associated with a bug we extract the functionsspecifiedbythereportedbuglocationsandlabelthem as1.
wealsoextractthecorrespondingfunctionsintheafter fixtrace andlabelthosepatchedbythecorrespondingcommitas0.after deduplication we have 6728functionsintotal.
.
models we apply p2imon three different neural network architectures whichhavebeenpopularlyemployedforvulnerabilitydetection.
theseoperate upondifferentrepresentations of sourcecode.
cnn thismodeltreatssourcecodeasaphotoandtriestolearnthe pictorialrelationshipbetweensource codetokensand underlying bugs.
similar to we apply token normalization before feeding data into the model.
this involves normalizing the function names andvariablenamestofixedtokenssuchas funcandvar.wesetthe embeddinglayerdimensionas13 followedbya2d convolutional layerwithinputchannelas1 outputchannelas512 andkernelsize as .
the finalprediction isgenerated bya3 layermultilayer perceptron mlp withoutputdimensionsbeing and2.
rnn this model treats code as a linear sequence of tokens and tries to detect bugs in source code using the temporal relationship between itstokens.
we base ourrnn implementation on .
thesahilsuneja yunhuizheng yufanzhuang jima.laredo andalessandro morari table comparing ai for code models using standard as wellasproposedsignal awarerecall sar metric.notethe drop in model quality recall sar when probing it for signalawareness.
dataset model accuracy f1 recall sar s babi cnn .
.
.
.
s babi rnn .
.
.
.
s babi gnn .
.
.
.
juliet cnn .
.
.
juliet rnn .
juliet gnn .
.
.
.
github cnn .
.
.
github rnn .
.
.
github gnn .
.
.
.
inputfunctionisalsonormalizedduringpreprocessing thesame asthecnnmodel.wesettheembeddinglayerdimensionas500 followed by a two layer bi directional gru module with hidden sizeequalsto256 thefinalpredictionisgeneratedbyasingle layer mlp.
gnn insteadofborrowingtechniquesfromimageandtime series domain thismodeloperatesatamorenaturalgraph levelrepresentationofsourcecode asper .ittriestolearnvulnerability signatures in termsof relationships between nodes and edges of a codeproperty graph .
following we do not apply token normalization during preprocessing.
we set the embedding size as64 followedby aggnn layer with hidden size256and unrolling time steps.
the node representations are obtained via summation of all node tokens embedding and the graph representation read out is constructedas aglobal attention layer.
thefinal predictionisgeneratedbya2 layermlpwithoutputdimensions 256and2.
themodelsaretrainedoverthedatasetspresentedinsection5.
withthevulnerabilitydetectionproblemframedasabinaryclassification task predicting program samples as healthy label or buggy label .
for all of the models we set dropout rate as .2duringtraining andusedtheadam optimizer.wetuned learningratein andbatchsizein .
models are trained with maximum epochs and early stopping patience .
cross entropy loss with class weight calculated fromthetrainingset isemployed.foreachexperiment wesave the checkpointwiththe bestvalidation loss.
.
results table2comparestheperformanceofthethreemodelsundertest uponthethreedatasetsasdescribedearlier.includedarethecommonmeasuresofthemodels classificationperformance.themodel reproductionsachieveperformancesimilartopreviouswork including on the real world dataset with performance comparable to that of which also creates its dataset from github but lacksbuglocation information.
the focus is on how recall compares with the proposed sar metric.ascanbeseen evenforthesimplesynthetics babi dataset table p2imreductionstats.
dataset model samples average reduced reduction s babi cnn s babi rnn .
s babi gnn .
juliet cnn .
.
juliet rnn .
.
juliet gnn .
.
github cnn .
github rnn .
.
github gnn .
.
whichtargetsonlyonevulnerabilitytype bufferoverflow a95 recalldropsintothesub 60rangeacrosstheboard whenweprobe the models for signal awareness with our p2imapproach.
this indicates that the models are picking up features not relevant to vulnerabilitydetection presumablylearningdatasetnuanceswhich inflatestheirreportedperformancemeasures.theresultsaresimilar for the otherdatasets as well.
table3showsthereductionstatisticsobtainedwith p2im.column4showsthatasignificantreductionof canbeachieved in the samples without the models changing there prediction and theratesaresimilaracrossthedifferentmodelsforthesamedataset.
furthermore morethan85 ofthesamplescanbereducedwhile maintainingtheirvulnerableprediction acrossdatasetsandmodels.
the restare amixoftwocategories the samples which cannot be reduced due to the valid code requirementwhichweenforceon p2im soastoonlyfeed real compilable subprograms to the models.
for these we cannotascertainforsurewhetherthemodeltrulycaptures the vulnerabilitysignals.
thesampleswhichthemodeltrulyneedsasisformaking its prediction signifyingapotentialtrue signal capture.
weobservedthecompilablecoderequirementisthemainreason why tokens cannot be reduced.
taking the example of cnn juliet amongstthe .
oftps whichcannotbereduced .
ofthem havewindowsheaderswhicharenotcompilable inour linux environment.similarly forcnn github allofthe tpswhichcannotbe reducedare dueto compilation failures.
table shows the overlap between the subset of the program sampleswiththesamepredictionacrossdifferentmodels.onthe two synthetic datasets the overlap percentage is for the true positive samples tp as well as for the signal aware true positives tp across all three models.
this is unexpected since the models use vastly different architectures.
combined with the low sar values from table this suggests that the perfect performance on synthetic datasets is significantly influenced by dataset specific nuances whichallthreemodelsarepickingupinaverysimilarway and missing real vulnerability signals.
the corresponding overlap on the real world dataset is much lower partially due to the fact that there are more variety and less artificial nuances in real world data for models to pick up which however contributes to their performance drop when compared to synthetic datasets as shown intable2.probing model signal awarenessviaprediction preservinginput minimization table overlap between the subset of samples with the same prediction across different models.
shown is the overlap percentage for the tp as well as signal aware tp subsetsacross the cnn rnn gnn models foreachdataset.
dataset tp overlap tp overlap s babi .
juliet .
.
github .
as a takeaway with the addition of sar to the existing arsenal of model performance metrics it becomes possible to measure how much of the model s learning is actually task aware.
this can additionally provide a more fair comparison and more accurate improvement guidanceacrossmodelevolutions.
related work in the software engineering community efforts have been made to detect vulnerabilities by isolating relevant statements using program slicing based techniques .
essentially the main ideaofsuchapproachesistoidentifythesubsetsoftheprogram that introduce the defects.
recently program slicing has also been used in ai assisted vulnerability detection tasks to extract bug relevant programmingconstructs.
if a vulnerability detection model can capture the real signals it should be able to identify such subsets too.
in this work we treat models as black boxes and feedthemwithdifferentsubsetsofaprogramtoevaluatehowwell theypick upthe realsignals.
andtoefficiently andsystematically generatethesubsets weborrowapopularfaultisolationtechnique ofdelta debugging.
the most relevant work using delta debugging dd and its variant methods is software failure diagnosis and isolation .themainadvantageofddisthatitcansignificantlyreduce the number of tests needed to locate the problem.
to the best of our knowledge our approach is the first attempt of using delta debuggingtointerpretandcomparemodels signalawareness.a recent parallel effort also uses dd to minimize inputs to ai modelsonsoftwareengineeringtasks.itsfocusisonthequalitative properties of the reduced code samples as opposed to our sarbased quantification of the impact of signal agnostic training on the models reportedperformance numbers.
p2imcan be viewed as belonging to the metamorphic testing paradigm applied to ai models .
in particular based on an input code snippet and its prediction by a model under test we systematically construct new tests by minimizing the original snippet and make sure the model produces the same prediction as the original input.
then we check the metamorphic relations among the inputs and output predictions of multiple executions.
a testviolationcanbedetectediftheoriginalinputanditsminimal versiondonot have the same vulnerability.
incontrasttoclassicalstatisticallearningmodelsthathaveexplicitinput outputrelationshipanderrorbounds itishardtounderstandwhatdeeplearningmodelslearnandtoproviderobustnessguarantees forit.
toalleviate theproblem researchershave developed methods to either probe the model s gradient as used in a recent ai for code paper or to fit explainable linear models around a small local region of the model s prediction boundary .
explanation methods have also been created for graphical neural networks.
gat and cgcn use attention mechanism to attribute edge importance while gnnexplainer tries to mask out irrelevant edges and node features while maintaining max mutual information between inputs and outputs.
to explain models via concepts and prototypes proposes a method to learn the similarity between inputs and a small set of prototypesduringtrainingviaanauto encoderstructure.ourapproachiscomplementarytotheseapproachesasittreatsmodels as blackboxes andgenerates the precise minimalrepresentation of an input that a model absolutely requires to arrive at its prediction.
theaddedbenefitisthat p2imcanfinalizetheexplanationwithout approximating the proper threshold for ranked features which isconsiderablymoreconvenientandactionableforend users.in addition the emitted minimal sequence is valid compilable code andthuscan be independentlycross verifiedwithexistingsource code analyzers.
finally with regard to deep learning models robustness and reliability research metrics have beenproposed for distance ratio among samples in the same class and in all other classes .
this is based on the intuition that for a reliable model inputs in the same class should be closer to each other in the model s latent space as compared to inputs in the other class.
extensive research hasalsocontributedtoconstructingsophisticatedattack defense methods and evaluating accuracy under l u1d45d norms bounded adversarialperturbations .differentfromcurrent metrics we directly probe the existence of true signal inside the models minimalrepresentations.
to thebestof our knowledge our metric saris the first of its kind to evaluate deep learning models signal awareness inthis domain.
conclusion inthispaper wepresentaprediction preservinginputminimization approach called p2imto evaluate and compare the signal awareness of ai for code models.
in particular p2imsystematicallyreducesaprogramsampletoaminimalsnippetwhichamodel needs to arrive at and stick to its original vulnerable prediction.
by checkingiftheminimalsnippethasthesamevulnerabilityasthe originalsample p2immeasuresthemodel srelianceonincorrect signals.
we apply p2imon three state of the art neural network modelsacrossmultipledatasets andmeasuretheirsignalawareness usinganewmetricwepropose signal awarerecall sar .the resultsshowasharpperformancedrop whichsuggeststhemodelsarepresumablylearningalotofnoisesordatasetnuances as opposed to capturingvulnerability related signals.
sar augments thetraditionalmeasuresofmodelperformancewith anewmetric tomeasure task relevantlearning which can more fairly compare andguide improvements acrossmodelevolutions.sahilsuneja yunhuizheng yufanzhuang jima.laredo andalessandro morari