fairea a model behaviour mutation approach to benchmarking bias mitigation methods max hort max.hort.
ucl.ac.uk university college london ukjie m. zhang jie.zhang ucl.ac.uk university college london uk federica sarro f.sarro ucl.ac.uk university college london ukmark harman mark.harman ucl.ac.uk university college london uk abstract the increasingly wide uptake of machine learning ml has raised the significance of the problem of tackling bias i.e.
unfairness making it a primary software engineering concern.
in this paper we introduce fairea a model behaviour mutation approach to benchmarking ml bias mitigation methods.
we also report on a largescale empirical study to test the effectiveness of widely studied bias mitigation methods.
our results reveal that surprisingly bias mitigation methods have a poor effectiveness in of the cases.
in particular of the mitigation cases have worse fairness accuracy trade offs than the baseline established by fairea of the cases have a decrease in accuracy andan increase in bias.
fairea has been made publicly available for software engineers and researchers to evaluate their bias mitigation methods.
ccs concepts software and its engineering software creation and management extra functional properties .
keywords software fairness bias mitigation model mutation acm reference format max hort jie m. zhang federica sarro and mark harman.
.
fairea a model behaviour mutation approach to benchmarking bias mitigation methods.
in proceedings of acm conference conference .
acm new york ny usa pages.
introduction machine learning ml software is widely used for critical decision making applications such as loan applicant filtering justice risk assessment and job recommendations .
nevertheless ml software can exhibit unwanted discriminatory and permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
conference july washington dc usa association for computing machinery.
acm isbn x xxxx xxxx x yy mm.
.
.
.
behaviours .
the consequences of such bias1can be highly detrimental for example affecting human rights university admissions profit and revenue .
furthermore many ml software system fall within legal or regulatory control bringing to the software engineers who deploy them additional legal risk .
ml fairness an important non functional testing property of ml software has been widely studied in the past few years in both software engineering and machine learning literature .
some approaches adapt the training data to reduce data bias i.e.
pre processing some create classification models that consider fairness during the training process i.e.
in processing others apply changes to the model prediction outcomes to reduce bias i.e.
postprocessing .
while these bias mitigation methods are able to reduce bias in light of a given fairness metric the improvement in fairness often comes at the cost of a lower prediction accuracy .2in other words there is a software engineering trade off between accuracy and fairness for ml software as revealed by many previous theoretical and empirical studies .
the existence of such trade offs brings challenges for judging the effectiveness of bias mitigation methods.
previous work presented the trade offs in a qualitative manner.
they either report and analyse the bias mitigation effectiveness by plotting the accuracy and fairness for a visual comparison or display accuracy and fairness separately in tables or bar charts .
as far as we know there is no trade off baseline nor is there any quantitative approach that can automatically evaluate and compare the fairness accuracy trade offs of software bias mitigation methods.
this paper introduces fairea .fairea is a novel model behaviour mutation approach to automatically benchmarking and quantifying the fairness accuracy trade off achieved by bias mitigation methods for ml software.
with fairea we conduct a large scale empirical study to benchmark and compare the effectiveness of widely studied bias mitigation methods that are publicly available in the popular ibm ai fairness library aif360 .fairea is the 1we use bias and unfairness interchangeably to refer to the difference in ml behaviours towards protected attributes.
section .
introduces the specific metrics that can be used to define and measure the concept.
2in this paper the term accuracy refers to the standard accuracy in machine learning which is the number of correct predictions against the total number of predictions.conference july washington dc usa max hort jie m. zhang federica sarro and mark harman first quantitative approach to benchmarking the fairness accuracy trade off for bias mitigation methods.
our empirical study is also the first large scale systematic study to evaluate the effectiveness of existing bias mitigation methods.
our results reveal that surprisingly in of the cases bias mitigation methods have a poor bias mitigation effectiveness.
in particular which reduce bias exhibit worse trade offs than the baseline provided by fairea while lead to a decrease in accuracy and an increase in bias.
furthermore our observations reveal the following limitations among the existing bias mitigation methods it is challenging to achieve a good trade off between fairness and accuracy methods designed to optimise one fairness metric often decrease the values of other fairness metrics the effectiveness of a method is often dataset and model dependent.
only rarely does an approach work well on all datasets and ml models.
to conclude this paper makes the following primary contributions abaseline approach that enables evaluating the fairnessaccuracy trade off of ml bias mitigation methods through model behaviour mutation.
aquantitative measurement for comparing different ml bias mitigation methods and trade off parameters.
alarge scale study on widely studied bias mitigation methods in regards to their bias mitigation effectiveness as well as their achieved fairness accuracy trade offs.
an open source implementation of fairea that has been made publicly available for ml software developers and researchers to evaluating their bias mitigation methods.
the rest of the paper is organised as follows.
section provides the current state of fairness research.
section introduces the preliminaries.
section introduces our approach.
the experimental design is described in section .
experiments and results are presented in section .
section concludes.
current state of fairness research this section introduces the progress of fairness research in software engineering in section .
the existing studies on the fairnessaccuracy trade offs in bias mitigation methods and how the effectiveness and trade offs are evaluated in the literature in section .
.
for a more intuitive overview table summarises the related works we introduce.
the top rows show the research in the software engineering domain.
the remaining rows are about the research on fairness accuracy trade off in other domains.
.
software engineering for ml fairness fairness is an important non functional testing property of ml software .
the testing and improvement of fairness has been regarded as a critical part in the life cycle of software development .
brun and meliou published a vision paper on the fairness of ml software where they call this software fairness .
they stated that ensuring software fairness is a software engineering problem which can be tackled from multiple directions including requirements architecture and design testing verification and maintenance.
harrison et al.
studied the perceived fairness of humans in regards to ml models.
biswas and hridesh studied the fairnessof ml models on crowd sourced platforms.
finkelstein et al.
explored fairness in requirement analysis and showed different needs among customers.
several techniques have been proposed to conduct fairness testing.
themis used random test generation techniques to evaluate the degree of fairness.
aequitas combined random generation and local search to explore the presence of discriminatory inputs.
aggarwal et al.
used symbolic execution and local explainability to generate test inputs for fairness testing.
zhang et al.
uses gradient computation and clustering to detect individual discriminatory instances of dnn.
sun et al.
proposed transrepair combining mutation testing and metamorphic relation which can be adopted to automatically test and repair fairness bugs in machine translators.
the design of software can also support the reduction of bias.
for example tramer provided a framework for detecting fairness bugs .
burnett et al.
proposed gendermag to identify gender bias in interfaces and respective workflows.
chakraborty et al.
explained the effect of bias on ml and proposed two approaches to combat this.
zhang and harman studied the influence of enlarging feature set and training data set when building fair ml models and found that a richer feature set could effectively improve ml fairness which is also observed in the work of biswas and hridesh later on .
different from these existing research fairea applies mutation on ml model behaviours to compose a baseline for evaluating bias mitigation methods.
mutation analysis has been well studied in traditional software engineering .
recently mutation analysis has drawn attention and been proved to be effective in automatically testing and improving ml software .
as far as we know fairea is the first mutation analysis approach for fairness evaluation targeting ml software.
.
fairness accuracy trade off there have been numerous works studying the fairness accuracy trade off of bias mitigation methods .
kamishima et al.
proposed a regularisation approach that adjusts the fairness accuracy trade off based on parameter .
larger values of improve fairness but also cause a higher loss in accuracy.
berk et al.
normalised the loss of accuracy to study the severity of the fairness accuracy trade off.
they call the decrease of accuracy brought by bias mitigation price of fairness .
corbett davies et al.
analysed the trade off of public safety and racial disparities.
similar to berk et al.
they showed that trade offs can be very common in practice.
kamiran and calders gave a theoretical analysis of the tradeoff.
a classifier achieves an optimal trade off if it is not dominated by another classifier i.e.
with larger accuracy and less bias .
to compare the fairness accuracy trade off achieved by bias mitigation methods practitioners either observe the fairness and accuracy changes in separate graphs or visualise them in a 2dimensional graph one dimension is accuracy the other dimension is fairness .
the proposed mitigation methods are often compared with previous methods different configurations the original non optimised classifier or a classifier trained without using protected attributes .fairea conference july washington dc usa table state of the art of fairness research.
the last column shows the section where each study is introduced in this paper.
fairness has been widely studied in both software engineering the top rows and ml literature the bottom rows .
authors year venue description section finkelstein et al.
re multi objective optimisation to improve requirements for fairer software.
.
burnett et al.
interact comput evaluation of problem solving software for gender inclusiveness.
.
galhotra et al.
esec fse automatic test suite generation for fairness testing themis .
.
tramer et al.
euros p framework to discover unfair treatment in data driven applications.
.
brun and meliou esec fse vision paper on the power of software engineering to combat fairness issues.
.
udeshi et al.
ase automated approach to discover inputs that highlight fairness violations.
.
angell et al.
esec fse automated test suite generation for two types of discrimination.
.
aggarwal et al.
esec fse detection of individual discrimination with black box testing.
.
friedler et al.
fat benchmarking of various bias mitigation methods datasets and metrics.
.
harrison et al.
fat empircal study about the perceived fairness of machine learning models.
.
biswas and hridesh esec fse investigation of bias in crowd sourced machine learning models.
.
zhang et al.
tse survey on testing for machine learning systems.
.
chakraborty et al.
esec fse effect of biased training data on ml fairness and proposal of two approaches to combat this.
.
.
zhang et al.
icse a lightweight search approach to detect individual discriminatory instances.
.
zhang and harman icse effect of the richness of feature data set on ml fairness.
.
biswas and hridesh esec fse effect of data pre processing on ml fairness.
.
calders et al.
icdm modelling of classification models with independence constraints on attributes.
.
kamiran and calders ccct massaging of dataset to apply changes with little intrusion.
.
calders and verwer dmkdfd three approaches to make naive bayes discrimination free.
.
kamiran et al.
icdm adaptation of splitting criterion and pruning rules for decision tree fairness.
.
liobaite et al.
icdm developed techniques to allow for conditional discrimination if explanatory attributes are responsible.
.
kamiran and calders kais three pre processing data to remove discrimination before a classifier is learned.
.
kamishima et al.
ecml pkdd discussion of causes for unfairness in ml.
proposal of regularisation to achieve fairness during training.
.
kamiran et al.
icdm relabeling of predictions with high uncertainty.
.
zemel et al.
icml encoding of data to obfuscate protected attributes.
achieves group and individual fairness .
feldman et al.
sigkdd investigation of disparate impact difference in classification among groups .
.
corbett davies et al.
kdd fairness as a constrained optimisation problem.
.
berk et al.
fat fairness regularisation for linear and logistic regression with variable weights.
.
zafar et al.
aistats fair classifiers based on a novel notion of decision boundary un fairness.
.
calmon et al.
nips convex optimisation to learn fair data transformations.
.
pleiss et al.
nips investigation of calibration while minimising for error constraints.
.
kamiran et al.
inf.
sci.
framework to handle predictions with high uncertainty.
.
zhang et al.
aies bias mitigation with adversarial learning.
.
kearns et al.
pmlr fairness across exponentially many subgroups to avoid gerrymandering.
.
kearns et al.
fat empirical evaluation of rich subgroup fairness fairness constraints over a large collection of groups .
.
celis et al.
fat a meta algorithm to achieve fairness based on a given fairness metric.
.
in all of these works the loss of accuracy and improvement of fairness are measured and visualised separately.
it is unclear whether the improved fairness is simply the consequence of the loss in accuracy.
there is no unified baseline or quantitative measurement to evaluate and compare the fairness accuracy trade off throughout different studies.
fairea aims to provide a unified standard to evaluate bias mitigation methods.
the baseline fairea provides enables developers to classify the fairness accuracy trade offs of a bias mitigation method into good or poor.
the quantitative measurement fairea provides enables developers to compare different mitigation methods in a more fine grained way and help tune fairness penalty parameters.
preliminaries in this section we introduce two widely used metrics to define and measure fairness for binary classification problems in section .
and the widely studied bias mitigation methods in section .
.
.
fairness metrics fairness metrics are designed to define and quantitatively measure ml fairness.
there are two primary types of fairness as indicated by speicher et al.
individual fairness and group fairness.
individualfairness is satisfied when similar individuals according to a distance function receive the same prediction .group fairness requires that the predictive performance of a classification model is equal across different groups which are divided by the values of protected attributes i.e.
race age sex .
groups are either privileged more likely to get an advantageous outcome or unprivileged more likely to get a disadvantageous outcome .
in this paper we adopt two group fairness metrics widely studied in the literature statistical parity difference and average odds difference.
we choose group fairness metrics for two reasons.
first these metrics are widely adopted in the literature second most mitigation methods are designed to optimise group fairness.
in the following we use yto denote the predictions of a classification model.
we use dto denote a group privileged or unprivileged .
we useprto denote probability.
the statistical parity difference spd is a fairness metric requiring that decisions are made independently of protected attributes .
positive and negative classifications for each demographic group should be identical over the whole population spd pr y d unprivileged pr y d privileged conference july washington dc usa max hort jie m. zhang federica sarro and mark harman the average odds difference aod is a group fairness metric that averages the differences in true positive rate tpr and false positive rate fpr among privileged and unprivileged groups aod fprd unprivileged fprd privileged tprd unprivileged tprd privileged following previous work we are interested in the absolute values of these metrics thus a minimal value of zero indicates no bias detected by the corresponding metric.
larger metric values correspond to a higher bias.
.
bias mitigation methods in order to improve machine learning fairness researchers have proposed three primary types of bias mitigation methods preprocessing in processing and post processing methods.
.
.
pre processing.
pre processing methods aim at processing the training data to reduce bias in the data.
reweighing rw is a pre processing method that applies weights to different groups in the training data to achieve fairness .
optimised pre processing op is a method to learn probabilistic transformations to edit labels and features of the dataset .
learning fair representations lfr encodes data into an intermediate representation with the aim of obfuscating protected attribute information while minimising the overall information disruption .
other pre processing approaches that have been proposed include the removal of data points and editing values of nonprotected features .
.
.
in processing.
in processing methods aim to mitigate bias during training by directly optimising algorithms.
adversarial debiasing ad is a technique that trains a classifier while simultaneously minimising the ability to predict protected attributes .
prejudice remover pr learns a classifier with a regularisation term to optimise fairness .
a fair classifier in regards to gerrymandering is proposed by kearns et al.
.
this approach applies a two player game between a learner and auditor to adjust subgroups.
celis et al.
introduced a meta algorithm that creates an optimised classifier for a given input fairness metric.
other in processing approaches include training process fairness constraints adaptation of split rule for decision trees decision boundary un fairness and latent unbiased variables .
.
.
post processing.
post processing methods change the prediction outcomes of a model to mitigate bias after the model has been trained.
reject option based classification roc exploits predictions with high uncertainty .
in particular favourable outcomes are assigned to unprivileged groups and unfavourable outcomes to privileged groups.
post processing can also be applied in regards to the equal opportunity difference eod .
two post processing methods that optimise for eod are equalised odds eo and calibrated equalised odds .
other post processing approaches include the modification of the probability of positive decisions for naive bayes nb leaf relabelling for decision trees dt and further investigation of uncertain labels .
lowhigh biasaccuracybm high lowf f2 f f1 0fo mfigure the fairea fairness accuracy trade off baseline is represented by the fomtrade off point and the f10...f100 points obtained by model behaviour mutation.
a bias mitigation method bmis effective if it exhibits a better trade off than the fairea baseline i.e.
if it is above the red line .
the fairea approach there are three primary steps in fairea to benchmarking and quantitatively evaluating bias mitigation methods.
step1 baseline creation with model behaviour mutation .
first fairea builds the baseline by simulating the behaviours of a series of naive bias mitigation models.
fairea does this via model behaviour mutation.
the accuracy and fairness of these simulated models together with the original classification model are adopted to construct the fairness accuracy trade off baseline.
step2 bias mitigation effectiveness region division .
second fairea maps the effectiveness of a bias mitigation method into five mitigation regions with the fairea baseline constructed in the first step.
the division of such regions helps to classify bias mitigation effectiveness into different levels providing an intuitive overview of the changes in accuracy and fairness of a mitigation method.
step3 quantitative evaluation of trade off effectiveness .
third fairea quantifies the effectiveness of fairness accuracy trade off by measuring the gap between its effectiveness and the fairea baseline.
this step focuses on the bias mitigation methods that improve fairness but decrease accuracy and enables the quantitative comparison among their trade offs.
the details for each step are explained below.
.
baseline creation when presenting the fairness and accuracy of a bias mitigation method in a two dimensional coordinate system the baseline that fairea provides can be viewed as a line as shown by figure .
the line is constructed by connecting the fairness accuracy points of the original model i.e.
the model obtained by using the original classifier without applying any mitigation method and a series of naive mitigation models constructed by model behaviour mutation.
in the following we explain how we obtain these points.
trade off points collection the starting trade off point is based on the accuracy and fairness of the original model i.e.
the model without applying any bias mitigation method as shown by point fomin figure .
the remaining points are based on the accuracy and fairness of a series of pseudo models whose behaviours are mutated from the original model.
the hypothesis is that these modelsfairea conference july washington dc usa could improve the fairness of the original model in a naive way by blindly sacrificing its accuracy with model behaviour mutation.
for example when fairea mutates the original model into a random guessing model the fairness will be greatly improved because the predictive performance are equally worse among different protected groups yet the accuracy is largely sacrificed.
the fairness accuracy trade offs of such mutated models are expected to be surpassed by any reasonable bias mitigation methods.
this hypothesis holds unless the original model performs even worse than a random guess model.
moreover the bias measured by fairness metrics should monotonically decrease with an increased mutation degree.
as far as we know widely adopted fairness metrics such as spd aod and eod all satisfy this condition.
mutation degree to obtain mutated model behaviours we copy the original model predictions then mutate the predictions made by this model i.e.
instead of returning the original predicted label a random subset of the predictions is replaced by other labels .
we consider different mutation degrees i.e.
the fraction of predictions to mutate from to with a step size of .
for example when the mutation degree is we randomly choose of the predictions made by the original model to mutate.
mutation strategy there are different mutation strategies we can choose to mutate the prediction behaviours such as random mutation or mutating all the chosen predictions into the same label.
in this paper we choose the second strategy following the zeronormalisation principle introduced by speicher et al.
which states that fairness metrics are minimised when each individual receives the same label.
for an n class classification problem there arenlabels that one can choose to conduct mutation therefore nmutation strategies are possible one for each label.
we choose the label that will yield the highest accuracy when of the predictions are mutated in order to provide a tighter trade off baseline.
we explore the influence of different mutation strategies in rq4 see more details in section .
.
example table illustrates an example of the mutation process and its corresponding fairness accuracy trade off for binary classification.
there are instances in this example id from to belonging to two groups g1andg2 .
the column bias shows the absolute false positive rate fpr difference between group g1andg2.
a larger absolute fpr difference indicates more bias in the model towards the two groups.
the original model achieves an accuracy of .
with a bias of .
.
when the mutation degree is the accuracy is reduced to .
the fairness is improved with a bias of .
.
finally mutating of the labels achieves the best fairness with a bias of .
but also leads to a low accuracy of .
.
baseline construction as shown by table each mutation degree corresponds to one mutated model whose accuracy and fairness will form a point for constructing the baseline of fairea .
for example in figure f10 f20 f30 ... f100illustrate the fairness and accuracy of mutated models with mutation degree of ... respectively.
these points together with the initial fairness and accuracy of the original model are connected to form the baseline of fairea .
the shape of the baseline is not necessarily linear.
different fairness metrics may have different baseline shapes.
3in this example mutating the predictions to label and have equal effects on the baseline strictness.
we thus demonstrate only the results of mutating the predictions into .table an example of the mutation procedure in fairea .
bias is represented by the absolute false positive rate difference bias .
from the table bias can be reduced by simply sacrificising accuracy through mutating model predictions.
id accuracy bias group g1 g1 g1 g1 g1 g1 g2 g2 g2 g2 true label original model .
.
mutation degree .
.
mutation degree .
.
eldvdffxudf jrrg wudgh rii uhjlrq ehwwhu wkdq edvholqh srru wudgh rii uhjlrq zruvh wkdq edvholqh zlq zlq uhjlrq lqyhuwhg wudgh rii uhjlrq ehwwhu dffxudf exw zruvh idluqhvv orvh orvh uhjlrq edvholqh figure mitigation regions of bias mitigation methods based on changes in accuracy and fairness.
the baseline is created following the procedure we introduced in section .
both accuracy and bias values are re scaled to a range between and 14for ease of presentation which does not affect the relative comparison results among different bias mitigation methods.
.
bias mitigation outcome categorisation after obtaining a baseline fairea categorises the bias mitigation method s effectiveness into several regions with different regions representing different categories of bias mitigation effectiveness.
as shown by figure there are five mitigation regions.
if a bias mitigation method improves the accuracy and reduces the bias of the original model it belongs to the win win region.
this win win region is challenging to achieve but is still possible .
a bias mitigation method falls in the lose lose region if it reduces the accuracy but at the same time increases the bias of the original model i.e.
it produces worse results for both measures .
if a bias mitigation improves accuracy but introduces more bias it falls in theinverted trade off region.
the trade off region means that a bias mitigation method reduces bias but decreases accuracy.
there are two types of trade off regions the good trade off region indicates that the bias mitigation method achieves better trade off than the baseline of fairea otherwise it belongs to the poor trade off region.
this five region categorisation of fairea helps provide an overview of the overall effectiveness of a bias mitigation method.
in the following we introduce how fairea quantitatively measures the goodness of fairness accuracy trade off.
4given a list of values x each element xi xis re scaled given the minimum xmin and maximum xmax inx xi xi xmin xmax xmin.conference july washington dc usa max hort jie m. zhang federica sarro and mark harman biasaccuracybm 1f f 0bm bm figure quantifying the fairness accuracy trade off of a given bias mitigation method by measuring the area between fairea baseline and the mitigation method.
bmrepresents the accuracy and bias of the mitigation method the red line represents fairea s baseline the area is constructed by connecting bmhorizontally bm and vertically bm to thefairea baseline.
.
trade off quantitative evaluation the win win lose lose and poor trade off regions provide sufficiently clear signals on the effectiveness of the bias mitigation method.
thus in this section we focus on providing a quantitative measurement on the trade off goodness of bias mitigation methods that fall into the good trade off region to facilitate a more fine grained comparison for different bias mitigation methods.
fairea measures the goodness of such a trade off by calculating the area encompassed by a mitigation method and the fairea baseline.
figure illustrates the area obtained by connecting the bias mitigation trade off point to the fairea baseline vertically and horizontally.
the vertical line and horizontal line together with thefairea baseline form a closed area.
for example for the case in figure the closed area is shown by the filled blue area which is formed by five points bm bm bm f10 andf20.
when comparing the area of two bias mitigation methods the method with a larger area is regarded to have a better fairnessaccuracy trade off.
using the area as a trade off measurement instead of other criterion such as the distance to the baseline ensures a reasonable comparison when the baseline is curved.
experimental setup in this section we describe the design of the experiments we carry out to evaluate fairea .
we first introduce the research questions then introduce the subjects and the experimental procedure.
the implementation code and the results are available at our homepage to support reproducibility and future studies.
.
research questions our evaluation answers the following research questions rq1 which mitigation regions do the existing bias mitigation methods fall into according to fairea ?
this research question evaluates the overall performance of state ofthe art bias mitigation methods by checking how they are matched into the five mitigation regions shown by figure according to fairea .
to answer this question we analyse the effectiveness of popular state of the art bias mitigation methods when used with three classification models by mapping their accuracy bias tradeoff into mitigation regions as illustrated in figure .
we show theproportion of bias mitigation cases that fall into each mitigation region.
rq2.
what fairness accuracy trade off do state of the art bias mitigation methods achieve based on fairea ?
this research question compares the methods that fall into the good trade off region with the quantitative measurement fairea provides.
to answer this question we calculate the area for the target method under each mitigation task with different ml models datasets and fairness metrics .
this allows us to quantitatively compare the methods and determine which bias mitigation method achieves the best fairness accuracy trade off under each task.
rq3.
can fairea be used to tune trade off parameters for inprocessing bias mitigation methods?
for in processing methods there are usually trade off parameters for controlling the degree of bias mitigation.
a larger trade off parameter mitigates more bias thus may sacrifice more accuracy.
the quantitatively measurement of fairea naturally enables automatic tuning of such parameters for the purpose of achieving the best trade off.
to answer the question we investigate the in processing methods prejudice remover with fairness trade off parameter and adversarial debiasing with theadversary loss weight then check whether our measurement helps to easily spot parameters that yield good fairness accuracy trade off.
rq4.
how does the mutation strategy influence fairea ?
as explained in section different mutation strategies can be used to build fairea .
this question evaluates the difference among mutation strategies in providing the baseline.
to answer this question we compare the baselines created by the different strategies to motivate the choice of the most suitable mutation strategy.
.
datasets we perform our experiments on the three5mostly widely studied real world datasets in the fairness literature the adult german and compas datasets.
theadult census income adult contains financial and demographic information about individuals from the u.s. census.
a classification is made to determine whether individuals have an income above thousand dollars a year.
thecompas correctional offender management profiling for alternative sanctions dataset contains criminal history and demographic information of criminal offenders in broward county florida.
each individual is assigned with a recidivism label indicating whether they were caught re offending within two years.
thegerman credit data german dataset contains credit information of people with a classification of good or bad credit risk.
based on the given features the protected attribute sex can be derived.
these datasets are the most widely explored in the fairness literature.
for example galhotra et al.
used two datasets adult and german chakraborty et al.
used the same three datasets.
table provides more information about these three datasets.
this includes the size of the dataset column size the number of attributes column attri.
the favourable label and the majority label.
for each dataset we present the protected attributes that are 5the number of datasets we used align with the fairness literature.
according to our collection of fairness papers use no more than three datasets in their evaluation.fairea conference july washington dc usa table dataset information.
dataset size attri.
favour label majority label prot.attrib privileged adult income 50k sex male race white compas no recidivism sex female race caucasian german good credit sex male present in the dataset column prot.attrib .
privileged groups are outlined for protected attributes column priviledged .
.
bias mitigation methods we explore all the three types of bias mitigation methods during our evaluation see more details in section .
.
under each type we choose widely studied methods which have been implemented in the ibm aif360 library pre processing optimised pre processing op learning fair representations lfr reweighing rw in processing prejudice remover pr adversarial debiasing ad post processing reject option classification roc calibrated equalised odds co equalised odds eo .
in aif360 roc and co are implemented with three different fairness metrics to guide the bias mitigation process.
roc offers a choice between spd aod and eod co offers a choice between false negative rate fnr false positive rate fpr and a weighted metric to combine both.
we implemented and evaluated every of the three methods for roc and co. all together we study bias mitigation methods.
.
experimental configuration pre processing and post processing methods are model independent.
we implement them using three traditional classification models which have been widely adopted in previous works that study fairness logistic regression lr decision tree dt and support vector machine svm .
as in previous work we use the default configuration for each classifier as provided by scipy.
the two in processing methods studied in this paper have their own model with different trade off parameters.
in this case to build fairea when getting the original model we turn off the trade off parameters so that such a model does not use any bias mitigation function when evaluating the effectiveness of a in process method in rq1 and rq2 we use its default trade off parameter.
in rq3 we explore the trade off performance of different parameters and investigate whether fairea s quantitative measurement helps to tune the parameters to get the best trade off.
we apply each of the bias mitigation methods to the three datasets and their protected attributes with three ml models and two fairness metrics.
thus for each mitigation method it will be evaluated per dataset protected attribute ml model fairness metric combination.
we call such as a combination a mitigation task .
each optimisation process is repeated times each time with a random re spilt of the data based on a fixed train test split ratio we use the mean value of these multiple runs to represent the method s average performance as a common practice in the fairness literature .
we treat each single run as an individual mitigation case and present the proportion of cases that fall into each bias mitigation region for a bias mitigation method to answer rq1 .
the baseline is also obtained by repeating the label model behaviour mutation procedure times for each mutation degree ... .
the source code containing the implementation of fairea and the implementation configuration of each bias mitigation method as well as the results are available in our project repository .
.
threats to validity the primary threat to internal validity lies in the implementation of fairea .
to reduce this threat the authors independently reviewed the implementation code.
the adoption of ibm aif360 framework a widely adopted fairness tool in software fairness also reduces such threat.
the threats to external validity lie primarily with the subjects investigated.
to reduce this threat we use the three most widely adopted datasets in fairness research.
we study bias mitigation methods with different classification models to obtain more generalised conclusions.
moreover we make our scripts and data publicly available to allow for reproductions replications and its adoption in future bias mitigation studies .
empirical study results this section presents the results of our experiments to answer the research questions explained in section .
.
.
rq1 mitigation region distribution the first research question checks the mitigation region distribution of the existing bias mitigation methods.
we apply bias mitigation methods to the three datasets to evaluate their region distribution section according to the baseline provided by fairea .
we apply each pre and post processing bias mitigation method on three classification models lr dt svm used for five bias mitigation tasks i.e.
adult sex adult race compas sex compasrace german sex .
each task is repeated for times with different training test splits.
dt achieves a prediction accuracy below the majority class for the german dataset.
therefore it does not meet our baseline requirement as introduced in section .
and is disregarded in the subsequent experiments.
thus for each bias mitigation method there are evaluations.
for each in processing method as we introduced in section .
we build the baseline upon an original model without applying bias mitigation with the trade off parameter set to .
for prejudice remover its accuracy on the compas german dataset is too low to be reduced by mutation we thus only present its results on the adult dataset.
therefore our experiment conducts evaluates on prejudice remover and evaluations on adversarial debiasing.
we then calculate the percentage of evaluations that fall into each region.
we use the proportion as a high level indication of the bias mitigation performance of each method.
.
.
overall results.
table shows the results of the region classification of bias mitigation methods.
each row represents a bias mitigation method.
each cell contains a percentage of scenariosconference july washington dc usa max hort jie m. zhang federica sarro and mark harman table rq1 proportion of mitigation cases that fall into each mitigation region.
we observe that half of the existing bias mitigation methods either decrease accuracy and increase bias lose lose of the original model or have a worse trade off than thefairea baseline poor trade off .
bias mitigation method statistical parity difference spd average odds difference aod lose lose poor trade off inverted good trade off win win lose lose poor trade off inverted good trade off win win prelfr op rw inpr ad postcofnr cofpr coweighed rocspd rocaod roceod eo mean that fall into corresponding regions for a mitigation method.
the last row shows the overall ratios for each mitigation region.
we make the following primary observations from table .
first to our surprise a large proportion of bias mitigation performance falls into the lose lose trade off region.
for example for the cofnr post processing method the proportion is as high as for aod.
the mean value of the lose lose proportion is for spd and for aod which means that those bias mitigation methods perform worse than the original model.
for spd of the bias mitigation methods perform worse than fairea while perform better.
similarly of the bias mitigation methods achieve worse trade offs than fairea for aod while being better among of the evaluations.
one possible reason for this is that mitigation methods are often designed to optimise one fairness metric but such kind of one target optimisation usually affects other fairness metrics .
for example cofnrandcofprare designed to optimise the difference of false negative positive rate between privileged and unprivileged groups.
their lose lose percentages measured by spd and aod are over .
nevertheless we observe that when using the same metric to optimise and measure mitigation performance the loselosepercentages are still high i.e.
for rocspd measured by spd and for rocaod measured by aod .
second a notable proportion of evaluations fall into the poor trade off region for spd and for aod .
while this means that they achieve more fairness than the original model their fairness accuracy trade off is worse than the baseline of fairea .
we also observe a small ratio of evaluations falling into the win win region or inverted trade off region .
a larger proportion of pre processing methods belong to the win win region in comparison to in and post processing methods.
this may indicate that optimising training data has more promises in providing solutions to optimise both accuracy and fairness.
third pre processing methods are more likely to fall into the win win region with both accuracy and fairness being improved.
for example for spd the average proportion of pre processing methods that fall into the win win region is which is only table rq1 averaged proportion of mitigation cases that fall into each mitigation region organised by different ml models top three rows and datasets bottom five rows .
the differences across models and datasets indicate that the effectiveness of the methods we studied are model and dataset dependent.
lose lose poor inverted good win win lr dt svm adult sex adult race compas sex compas race german sex for post processing methods.
this suggests that if one pursues improving both accuracy and fairness it might be favourable to pre process the training data and prevent the bias from reaching the model than to mitigate the bias after the model has learned the bias from the data.
.
.
comparison among different models and datasets.
we further analyse the region distribution based on ml models for pre and post processing methods and datasets.
the purpose is to investigate whether the performance of different bias mitigation methods are influenced by ml models or datasets.
table shows the results.
among the three classification models we observe that different models have different results which indicates that the effectiveness of pre and post processing methods are model dependent.
overall lr and svm have a better effectiveness higher percentage of good trade offs than dt.
among different datasets and protected attributes the differences are also notable.
we observe that for the compas dataset there are more scenarios in the win win region and fewer scenarios in the lose lose region.
we suspect that this is because compas datasetfairea conference july washington dc usa table rq2 trade off assessment results for pre processings and post processing methods.
for each method in the good trade off region a trade off measurement value provided by fairea is given for other regions the region type is displayed.
the values in bold indicate the best mitigation method for each mitigation task.
from this table we observe that fairea provides distinguishable measurements for trade off comparison and helps to detect the best mitigation method under each bias mitigation task.
logistic regression lr decision tree svm adult compas german adult compas adult compas german sex race sex race sex sex race sex race sex race sex race sexstatistical parity differenceprelfr poor poor poor poor poor poor poor poor poor poor poor poor poor poor op poor .
.
.
lose lose .
.
win win inverted .
.
win win inverted lose lose rw .
.
.
.
poor .
.
win win win win .
.
win win win win lose lose postcofnr .
.
lose lose lose lose lose lose lose lose lose lose lose lose lose lose .
.
lose lose lose lose lose lose cofpr lose lose lose lose poor lose lose .
lose lose lose lose .
lose lose lose lose lose lose poor lose lose .
coweighed lose lose lose lose lose lose lose lose lose lose lose lose lose lose lose lose lose lose lose lose lose lose lose lose lose lose lose lose rocspd .
poor .
.
poor lose lose lose lose .
.
poor poor .
.
poor rocaod lose lose lose lose .
.
poor lose lose lose lose lose lose lose lose lose lose poor .
.
poor roceod lose lose lose lose .
.
poor lose lose lose lose lose lose lose lose lose lose poor .
.
poor eo .
.
.
.
.
poor lose lose .
.
.
.
.
.
.018average odds differenceprelfr lose lose lose lose poor poor poor lose lose lose lose poor poor poor poor poor poor poor op poor .
.
.
lose lose poor lose lose win win inverted .
.
win win inverted lose lose rw .
.
.
.
poor .
lose lose win win win win .
.
win win win win lose lose postcofnr .
.
lose lose lose lose lose lose lose lose lose lose lose lose lose lose .
.
lose lose lose lose lose lose cofpr lose lose lose lose poor lose lose .
lose lose lose lose .
lose lose lose lose lose lose lose lose lose lose .
coweighed lose lose lose lose lose lose lose lose lose lose lose lose lose lose lose lose lose lose lose lose lose lose lose lose lose lose lose lose rocspd lose lose poor .
.
poor lose lose lose lose .
.
poor poor .
.
lose lose rocaod poor poor .
.
poor lose lose lose lose lose lose lose lose poor .
.
.
lose lose roceod lose lose lose lose .
.
poor lose lose lose lose lose lose lose lose lose lose .
.
.
lose lose eo .
.
.
.
.
lose lose lose lose .
.
.
.
.
.
.
is more balanced than adult and german of majority labels v.s.
and majority labels according to table .
to conclude for rq1 we have the following answer answer to rq1 surprisingly approximately of the bias mitigation scenarios have a poor mitigation effectiveness with of them decreasing accuracy and increasing bias lose lose and of them exhibiting a poor trade off according to fairea .
.
rq2 quantitative measurement for fairness accuracy trade off to answer rq2 we present the quantitative measurement results of the fairness accuracy trade off achieved by different bias mitigation methods with fairea .
we quantify results that fall into the good trade off region as the other regions are either strictly dominating the original model win win dominated by the fairea baseline lose lose andpoor trade off or do not improve fairness inverted .
we use the arithmetic mean results of the runs to indicate the average level of mitigation effectiveness.
table shows the results for pre and post processing bias mitigation methods.
the values in bold indicate the best mitigation method for each mitigation task i.e.
the combination of dataset protected attribute ml model and fairness metric .
from the table the quantitative trade off measurement fairea provides helps to compare different the trade offs among different mitigation method and to choose the best one under each mitigation task.the same as rq1 we observe that the trade offs of bias mitigation methods are highly dataset dependent.
for example the best tradeoff on the adult dataset is achieved by eo highest scores for both aod and spd .
the best trade off on german is achieved by cofpr.
we also explore whether the protected attribute considered under each dataset impacts the performance of bias mitigation methods.
from table for the same dataset different protected attributes have very similar patterns.
specifically in of the cases bias mitigation methods are classified into the same mitigation region with different protected attributes.
this suggests that the protected attribute has a limited impact on the trade off performance of bias mitigation methods.
table rq2 trade off assessment results for in processing methods.
adult compas german sex race sex race sex statistical parity differencepr .
.
na na na ad .
.
lose lose lose lose lose lose average odds differencepr .
.
na na na ad lose lose lose lose lose lose lose lose lose lose due to the different characteristics of in processing methods we provide their quantitative results separately in table .
prejudice remover is not applicable to the compas and german dataset see section .
.
for more details so we mark the results as na .
adversarial debiasing is applicable for all three datasets however only achieves good trade offs on the adult dataset for spd.
all the other trade offs are in the lose lose region.
however when comparing the two in processing methods on adult dataset measured byconference july washington dc usa max hort jie m. zhang federica sarro and mark harman spd adversarial debiasing has a better trade off than prejudice remover.
these observations lead to the following answer to rq2 answer to rq2 the quantitative measurement of fairea allows us to determine and compare fairness accuracy tradeoffs achieved by different bias mitigation methods.
for example fairea measures that the eo method achieves a .
better trade off than cofnr i.e.
.
vs. .
for the case lr adult sex under statistical parity difference.
different datasets have different bias mitigation methods that achieve the best trade off i.e.
eo for adult cofpr for german .
.
rq3 parameter tuning in rq3 we investigate the effectiveness of fairea in evaluating the parameter tuning for in processing methods.
for this purpose we apply fairea on the original model of prejudice remover with and adversarial debiasing with an adversary loss weight .
as in previous experiments we perform train test splits for all numerical values of between for pr with a step size of .
we evaluate adversary loss weights in a range of .
with a step size of .
.
due to limited space we choose the adult dataset as an example to illustrate experiments on parameter tuning.
full results are available in our project repository .
we first plot the accuracy and fairness achieved by each parameter setting shown in figure .
for both methods all parameter settings for spd achieves better trade off than the original model.
however the bias mitigation effectiveness for aod is much worse.
although the different parameters all belong to the good tradeoff region it is difficult to determine which parameter setting achieves the best fairness accuracy trade off.
we therefore investigate whether our quantitative measurement in fairea helps spot the parameter that achieves the best trade off.
figure shows these results.
sub figures a and b show the trade off measurement results provided by fairea with different trade off parameters.
the remaining sub figures show the accuracy and fairness changes separately without fairea .
from sub figure .
a and sub figure .
b we observe that when the trade off parameter changes our trade off measurement first increases then decreases with a turning point indicating the parameter with the best trade off.
however from the remaining subfigures without the support from fairea it is difficult to choose a parameter with accuracy and fairness changing at the same time.
of course in practice the desired trade offs may depend on the application scenario and the specific requirement.
some applications may demand a higher degree of fairness with the capability of enduring more accuracy loss.
however the quantitative measurement in fairea provides an engineering solution for finding the best trade off as a reference for developers.
a prejudice remover b prejudice remover c adversarial debiasing d adversarial debiasing figure rq3 accuracy and fairness achieved by prejudice remover sub figure a and b and adversarial debiasing sub figure c and d with different parameters on adult sex.
each green point represents a trade off parameter the blue line represents the fairea baseline.
answer to rq3 our trade off measurement helps to quantify the fairness accuracy trade offs achieved by inprocessing methods with different trade off parameter settings and to identify parameters that achieve the best fairness accuracy trade offs.
.
rq4 influence of mutation strategies this research question is designed to investigate how the mutation strategy for simulating naive mitigation methods affects the construction of the fairea baseline.
we show and compare three different mutation strategies replace labels with replace labels with and replace labels at random.
figure shows the accuracy and fairness spd aod of the three mutation strategies.
we analysed all three datasets the conclusions are identical so we only present results for the adult sex task full results are available in our project repository .
as can be seen when we mutate the labels with the majority class label 0in the case for adult sex its baseline is on top of the other two strategies.
this means that overwriting with the majority label provides a more strict baseline than the other two strategies.
mutation with the minority class label 1in the case for adult sex instead leads to a baseline with lower accuracy on the same level of fairness.
using such a baseline would provide weaker conditions when checking the trade off of bias mitigation methods.
replacement with random labels leads to a baseline in between the other two strategies but with labels replaced the fairness values are not minimised at zero because of the imbalanced data distribution.
in this paper we adopted the strategy of mutating predictions with the majority class label in the training data.
although this isfairea conference july washington dc usa a prejudice remover b adversarial debiasing c prejudice remover d adversarial debiasing e prejudice remover f adversarial debiasing figure rq3 in processing trade off parameter tuning with fairea .
the horizontal axis in each sub figure shows different parameter values.
figure a and b show the tradeoff measurement changes provided by fairea .
figure c d e f show the changes of accuracy and fairness separately.
figure rq4 comparison of three mutation strategies mutate the original prediction into or randomly r on the adult dataset with the protected attribute sex.
the most strict among the three strategies it is still a naive bias mitigation method achieved simply by label overwriting which we expect that a reasonably effective bias mitigation method should outperform.
answer to rq4 among the different mutation strategies we explored replacing labels with the majority class label for a dataset leads to the strictest baseline.
conclusions and future work in this paper we proposed fairea a novel approach to evaluating and quantitatively measuring the fairness accuracy trade off.
there are three primary questions that previous work could not answer without fairea the fairea baseline tells whether a bias mitigation method trades accuracy for fairness or even worse than that .
the qualitative approach used by previous work is not able to differentiate good trade off and poor trade off like fairea does fairea provides extra information for developers by telling whether bias mitigation method a outperforms method b when they both achieve a good trade off fairea helps to tune the fairness mitigation parameter for in processing methods.
we performed a large scale empirical study to evaluate our baseline fairea on three widely used datasets and bias mitigation methods.
we found that half of the bias mitigation methods are not able to achieve a reasonable bias mitigation effectiveness either achieving a worse trade off than our baseline or decreasing accuracy and increasing bias .
in addition few methods perform well on all datasets and all models.
these results show the limitations and challenges of the existing bias mitigation methods suggesting the need for further research effort on improving ml software fairness.
in future we plan to involve fairea into the bias mitigation process to guide mitigation optimisation and develop new bias mitigation methods.