enabling software resilience in gpgpu applications via partial thread protection lishan yang william mary williamsburg v a lyang11 email.wm.edubin nie william mary williamsburg v a bnie email.wm.eduadwait jog william mary williamsburg v a ajog wm.eduevgenia smirni william mary williamsburg v a esmirni cs.wm.edu abstract graphics processing units gpus are widely used by various applications in a broad variety of fields to accelerate their computation but remain susceptible to transient hardware faults soft errors that can easily compromise application output.
by taking advantage of a general purpose gpu application hierarchical organization in threads warps and cooperative thread arrays we propose a methodology that identifies the resilience of threads and aims to map threads with the same resilience characteristics to the same warp.
this allows engaging partial replication mechanisms for error detection correction at the warp level.
by exploring benchmarks kernels from benchmark suites we illustrate that threads can be remapped into reliable or unreliable warps with only .
introduced overhead on average and then enable selective protection via replication to those groups of threads that truly need it.
furthermore we show that thread remapping to different warps does not sacrifice application performance.
we show how this remapping facilitates warp replication for error detection and or correction and achieves an average reduction of .
and .
execution cycles respectively comparing to standard duplication triplication.
index terms reliability gpgpu application resilience transient faults thread remapping i. i ntroduction as general purpose gpus gpgpus are becoming increasingly susceptible to transient hardware faults soft errors often from cosmic radiation or from operating under low voltage their reliable operation is of critical importance.
with gpgpus becoming omnipresent in fields such as highperformance computing hpc artificial intelligence deep learning virtual augmented reality and safety critical systems such as autonomous vehicles transient hardware faults can lead to bit flips in storage devices including the register file and dram.
such bit flips are increasing in frequency as system scales increase especially in the hpc domain .
if bit flips occur during application execution they may result in application crashes hangs or even worse in silent data corruption sdc where the application successfully completes execution but its output is incorrect.
executions that result in sdc outcomes are the most undesirable as they erroneously provide the user with the illusion of correct output although cases of sdc output that is within certain useracceptable ranges may exist .
to ensure reliable application execution several mechanisms are widely employed including error correction codes ecc but ecccannot still provide protection to datapath errors that originate from unprotected latches in functional units e.g.
arithmetic logic and load store units .
reliable execution of gpgpu applications requires highoverhead protection mechanisms such as check pointing or software solutions that are based on replication.
in the gpu domain such replication can be done at different levels at the kernel thread or instruction level.
at the thread level replication is based on using redundant copies of a thread or block of threads and then on comparing their results .
different compiler based implementations of this idea aim to reduce the unavoidable synchronization overhead between the original and redundant threads.
if replication is done at the instruction level then the overhead of redundant multi threading can still be significant.
in addition not all dynamic instructions are typically covered.
in this paper we offer an orthogonal approach that is based on the fact that thread resilience profiles within a gpgpu application may differ significantly some threads are inherently resilient while some are not thread resilience may also depend on application input .
application resilience eventually depends on the thread organization of gpgpu application software.
in gpgpu applications threads are arranged at three levels kernels thread blocks or cooperative thread arrays ctas in cuda terminology and warps.
each gpu core schedules work at a granularity of warp which is usually a group of threads.
each group executes the same instruction in a lock step manner.
this is essentially the basis of single instruction multiple thread simt execution.
our thesis is that gpgpu software resilience can be achieved via selective warp replication provide that threads remapped into warps such that warps consist of threads that are either reliable or unreliable.
therefore if warps consist of threads that are inherently reliable these warps their threads or their instructions do not have to be replicated to increase their resilience .
instead only warps that contain unreliable threads need to be replicated.
the advantage here comes from scheduling of warps in the single instruction multiple data simd paradigm as threads within the warps are scheduled in a lock step way it is a lot easier to replicate an entire warp of unreliable threads rather than replicate individual threads within warps or instructions within threads and reconcile their outcome as the classic redundant multi threading ieee acm 43rd international conference on software engineering icse .
ieee advocates.
the process of thread remapping at the warp level is transparent to the software developer and offers a simple way to reorganize code with minimal effort.
we stress that the application resilience profile i.e.
the percentage of application executions that result in crashes hangs sdc and correct executions in presence of bit flips strongly depends on branch divergence and input data taken by different threads .
since application resilience is tied to input it cannot possibly guide software development.
the remapping that we propose in this paper allows the developer to improve application resilience in a transparent way either by changing the thread warp mapping to activate replication for a fully transparent approach to the code developers or by providing guidance to the developer to simply reorganize threads in such a manner that facilitates replication but does not interfere with the parallelization and synchronization logic of the software.
in summary we make the following contributions based on individual thread resilience we categorize the warps into three classes a reliable warps where all threads are resilient to single bit errors b unreliable warps where all threads are unreliable and c mixed warps that contain both reliable and unreliable threads.
we show that mixed warps are abundant in kernels.
we propose a low overhead partial thread protection mechanism by remapping threads such that the number of mixed warps is minimized.
in other words we change the thread to warp mapping such that distinct reliable and unreliable warp groups are formed.
this facilitates the need for protecting only unreliable warps as this remapping maintains the per thread resilience profile.
we present experiments using benchmarks kernels from the axbench cuda polybench and rodinia suites and show that of these kernels can benefit from remapping.
we show that remapping increases on the average the percentage of reliable warps from .
to .
while incurring only .
execution overhead due to increased number of stalls in shared memory.
by duplicating or triplicating the warps we can easily detect when an error occurs if duplication is used or correct the error via triplication .
we show that by selectively replicating warps that contain unreliable threads after remapping i.e.
unreliable or mixed warps we achieve average performance savings .
and .
for detection and protection respectively.
the remaining of the paper is organized as follows.
section ii describes the background of gpu architecture and the fault model used in this paper.
section iii presents characterization regarding various thread resilience patterns observed in the studied benchmarks.
inspired by the characterization results we propose a partial thread protection mechanism via remapping the details can be found in section iv.
section v evaluates the performance gains as well as the overhead of remapping.
then section vi discusses related work andeventually we conclude in section vii.
ii.
b ackground in this section we provide a brief introduction on the baseline gpu architecture and the gpgpu execution model.
we also discuss the fault model fault injection method and application resilience profile.
a. gpus and gpgpu application structure baseline gpu architecture.
a gpu typically is equipped with a large number of cores also known as streamingmultiprocessors sms in nvidia terminology .
each core has its private l1 cache software managed scratchpad memory and a large register file.
an interconnection network connects all these cores to global memory which consists of various memory channels partitions .
every memory channel has a shared l2 cache and its associated memory requests are handled by a gddr5 memory controller.
there are various protection techniques for single bit faults in recent commercial gpus including single error correction doubleerror detection sec ded error correction codes eccs that protect register files l1 l2 caches shared memory and dram against soft errors.
other structures such as arithmetic logic units alus thread schedulers instruction dispatch units load store units lsus and interconnection network are not protected .
fig.
.
gpu software execution model.
gpgpu software execution model.
following the singleinstruction multiple thread simt philosophy gpgpu applications execute thousands of threads concurrently over large amounts of data.
this helps in masking the latency and achieving high throughput.
a typical gpgpu application launches various kernels on the gpus see figure .
each kernel is divided into groups of threads known as thread blocks which are called cooperative thread arrays ctas in cuda terminology.
a cta encapsulates all synchronization and barrier primitives among a group of threads .
this cta formation enables the gpu hardware to relax the execution order of the ctas for the purpose of maximizing parallelism.
threads inside one cta can be further divided into groups of individual threads known as warps.
as the most fine grained level in terms of scheduling warps execute a single instruction on the functional units in lock step.
this 1249table i selected benchmarks .
suite benchmark kernel name kernel id pct.
of reliable warps pct.
of reliable threads axbenchjmeint jmeint kernel k1 .
.
laplacian laplacianfilter k1 .
.
meanfilter averagefilter k1 .
.
cudaexecutefirstlayer k1 .
.
nn executesecondlayer k2 .
.
neuralnetwork executethirdlayer k3 .
.
executefourthlayer k4 .
.
scp scalarprodgpu k1 .
.
polybench2dconv convolution2d kernel k1 .
.
mvt mvt kernel1 k1 .
.
rodiniagaussianfan1 k1 .
.
fan2 k2 .
.
hotspot calculate temp k1 .
.
nearestneighbor euclid k1 .
.
pathfinder dynproc kernel k1 .
.
sradreduce k3 .
.
srad k4 .
.
sub division of warps is an architectural abstraction which is transparent to the application programmer.
b. fault model we assume that register files and other components such as caches and memory are protected by ecc which is the case in almost all gpus .
we simulate commonly occurring computation related errors due to transient faults known as soft errors in alus lsus.
these faults can lead to wrong alu output which would then be stored in destination registers or corrupted variables loaded by an lsu.
this erroneous computing operation is what we emulate by injecting faults directly to destination register values.
this is a standard experimental methodology for gpgpu reliability studies .
the fault injection methodology used here closely follows the one used in we flip a bit at a destination register identified by the thread id the instruction id and a bit position.
we perform our reliability evaluations on gpgpu sim with ptxplus mode.
gpgpu sim is a widely used cyclelevel gpu architectural simulator and its ptxplus mode provides a one to one mapping of instructions to actual isa for gpus .
any fault injection tool or technique.
e.g.
sassifi or nvbitfi can be used for evaluating the application reliability i.e.
the technique presented in this paper does not depend on gpgpu sim.
gpgpu application resilience profile.
for each fault injection experiment there are three possible outcomes masked output the application output is identical to that of fault free execution.
silent data corruption sdc output the fault injection run exits successfully without any error but the output is incorrect.
other the fault injection run results in a crash or hang.
to obtain the resilience profile of an application run we conduct an experimental campaign using the state of the art fault injection methodology proposed by nie et al.
that aggressively prunes the fault space while achieving accuracy that is remarkably close to the ground truth.
within the pruned fault space we conduct one run per fault location one single bit flip and evaluate the application outcome as masked sdc orother .
we aggregate the outcome of all experiments to obtain the application resilience profile i.e.
what percentage of the runs are expected to result in masked sdc or other outputs.
the lower the sdc percentage the higher the application resilience.
in this paper we focus on reducing the percentage of sdc outputs.
faults that lead to masked outputs can be ignored while faults that lead to a crash or hang are easily detected.
in this work we focus on how to improve application resilience protection when a single bit fault occurs.
the proposed methodology can be readily extended to multi bit fault models .
iii.
c haracterization we conducted experiments across benchmarks kernels selected from benchmark suites listed in table i. the selected benchmarks cover different application domains including 3d gaming image processing and scientific computations.
past work has established that different 1250fig.
.
all the threads are reliable in srad k3.
gray solid lines separate different warps and yellow dashed lines separate different ctas.
due to space constraint here we only show the first ctas in the kernel.
there are in total ctas in srad k3.
gpu threads have different resilience profiles and that the thread dynamic instruction count icnt can be used as a proxy of individual thread resilience .
indeed fault site pruning is based on this exact concept it demonstrates that threads with the same dynamic instruction count have the same resilience profile therefore it is sufficient to select one thread from each group with the same icnt for fault injection and extrapolate the thread resilience of the entire group from a single thread.
our experiments further corroborate here what past work has also shown different threads have typically different resilience.
understanding the patterns of thread resilience is helpful for scheduling purposes to improve application resilience.
we categorize benchmarks into three cases reliable unreliable and mixed based on the thread resilience within each warp.
we focus on warps because a warp is the smallest scheduling unit.
in addition it is not desirable to change the thread cta mapping.
if done so it breaks the synchronization and thread communication within a cta requiring significant effort in redesigning the parallel software logic.
the percentage of sdc outcomes across the various numbers of experiments can characterize one thread as reliable or unreliable.
for example if the percentage of fault injected runs that result in sdc outcomes is smaller than a small number typically in the range from zero to essentially if its resilience coverage is then we characterize the thread as reliable otherwise it is deemed unreliable.
in the following we show some example cases.
.
all threads are reliable.
some applications are very resilient to faults.
figure shows the resilience scatter plot of different threads in srad k3.
threads are organized in thread launching order.
we use the gray solid lines to separate different warps and use yellow dashed lines to separate different ctas.
due to space constraint here we only show the first ctas in srad k3 but the same pattern repeats across all ctas all threads in srad k3 are reliable.
similar to srad k3 srad k4 and all nn kernels are highly resilient to soft errors.
.
all threads are unreliable.
some applications have a high probability of sdc outputs when faults are injected.
figure shows the percentage of sdc outputs per thread fig.
.
all threads are unreliable in scp.
gray solid lines separate different warps and yellow dashed lines separate different ctas.
there are ctas in total but due to space constraint we only show the first ctas.
for scp.
here all threads have more than sdc outputs.
an application with similar resilience behavior is mvt with .
sdc outputs for all of its threads.
.
mixed reliable and unreliable threads within warps.
reliable and unreliable threads can co exist in the same warp and consequently cta see figure .
reliable threads are marked with a green while red x represents unreliable threads and marks their sdc probability.
we start from two simple benchmarks gaussian k1 and nearestneighbor figure a and b respectively .
for gaussian k1 there are in total threads organized in one cta only.
the first threads are unreliable and the remaining threads are very resilient their percentage of sdc outputs is .
nearestneighbor shows a similar resilience pattern threads at the beginning are unreliable those that are launched later are reliable.
there are in total ctas in nearestneighbor.
due to the space constraint here we only show the last ctas which can best express the idea of well organized warps.
for both gaussian k1 and nearestneighbor reliable threads and unreliable threads are already organized separately within different warps with the exception of a single warp either at the start for gaussian or at the tail for nearestneighbor that contains both reliable and unreliable threads .
however there are benchmarks where their threads are not that well organized.
for hotspot shown in figure c reliable and unreliable threads are mixed within different warps.
due to space constraint here we only show the first ctas at the beginning for hotspot.
as shown in table i the percentage of reliable warps warps containing only reliable threads is for hotspot but there are in total .
reliable threads.
similarly in jmeint there is no reliable warp because all of the warps have both reliable and unreliable threads as shown in figure c .
for jmeint more than half .
of the threads are reliable.
however since they are mixed in ctas with the remaining .
unreliable threads protecting via replication would require replication of the entire kernel i.e.
every warp.
similar observations apply to laplacian meanfilter 2dconv gaussian k2 and pathfinder see figure e h .
figure clearly illustrates that there is ample scope for partial protection if we group threads judiciously then we can a gaussian k1.
b nearestneighbor last ctas .
c hotspot first ctas .
d jmeint first ctas .
e laplacian first ctas .
f meanfilter first ctas .
g 2dconv .
h gaussian k2 first ctas .
i pathfinder.
fig.
.
reliable and unreliable threads exist together in the same warp.
due to space constraints we only show the a part of the ctas for nearestneighbor hotspot jmeint laplacian meanfilter and gaussian k2.
increase the percentage of reliable warps and avoid redundant protection of warps threads that are anyway resilient.
table ii summarizes the benchmark categorization.
summary.
from the reliability perspective there is no need to protect reliable threads.
benchmarks where all threads are reliable result in reliable kernel executions.
similarly for benchmarks that have warps that are all unreliable protectionneeds to be applied to the entire kernel.
approaching kernel reliability from the scheduling perspective threads are grouped and scheduled in units of warps which is transparent to software developers.
for benchmarks that consist of both reliable and unreliable threads we explore ways to remap threads into warps such that warps consist of only reliable or unreliable threads.
if this is done then it is not necessary 1252table ii benchmark categories category benchmark all threads are reliablenn k1 nn k2 nn k3 nn k4 srad k3 srad k4 all threads are unreliable scp mvt mixed warpswell organized gaussian k1 nearestneighbor need remappingjmeint meanfilter 2dconv hotspot gaussian k2 pathfinder laplacian fig.
.
workflow.
to protect the application fully but instead focus on protecting unreliable warps only.
iv.
r esilient software protection via remapping in threads are identified as the most important gpgpu component and the resilience pattern of an application can be derived from thread resilience.
here we propose a lowoverhead partial protection mechanism that leverages thread resilience patterns via remapping.
the main idea is to remap threads to warps for the purpose of separating reliable and unreliable threads as scheduling of threads can be done at the warp granularity.
by addressing the problem at the warp level we propose to recompute warps that contain unreliable threads essentially offering partial protection to a subset of warps and not the entire kernel without compromising application reliability.
figure shows the workflow of the proposed protection mechanism.
there are two components offline analysis to obtain the resilience aware thread order and online protection which uses this resilience aware thread order to achieve low overhead protection.
offline analysis.
for any target kernel and for a specific input the resilience of each thread needs to be first obtained.
there is no restriction on which method of reliability analysis is used.
fault injection campaigns ace architecturally correct execution analysis or a combined method leveraging both fault injection and ace analysis can be used.
the only requirement is that the resilience of every thread needs to be evaluated.
this is not difficult to do despite the fact that most gpgpu applications have tens of thousands of threads because for most benchmarks threads with the same di count have the same resilience behavior .
this reduces the number of experiments that need to be done to obtain the thread resilience profile.
in this work we evaluate thread and kernel resilience using the fault site pruning technique .
we first identify the resilience profile of threads in their launching order see figure .
then threads are re ordered into warps following the remapping logic threads with similar resilience are remapped into the same warp.
detailed explanation is in subsection iv a. online protection.
based on the resilience aware thread order obtained from offline analysis we can remap threads before execution.
the actual remapping idea can be implemented in various ways.
in this work we directly change the thread warp mapping see subsection iv a for implementation details .
after remapping threads are executed and error detection correction is applied to unreliable warps only.
error detection correction can be implemented and applied in various ways.
here we use warp duplication for error detection and warp triplication for error correction.
details are given in subsection iv b. a. remapping ctas are collections of threads defined by the cuda programmer.
the thread to warp mapping is done linearly by default i.e.
threads are allocated to warps in groups of as shown in figure a .
to change the mapping between thread to warp we first identify reliable unreliable threads in offline analysis then remap threads see figure b .
by changing the linear thread order we can group threads with the same resilience i.e.
percentage of sdc outputs into the same warp and use different resilience protection at the warp level according to their reliability profile.
note that remapping is done within each cta but not across ctas.
this is because synchronization is ensured inside each cta.
remapping across different ctas can affect their synchronization hence introduce errors in the software logic.
we use gpgpu sim to implement the above.
at the initialization phase ctas are constructed.
without remapping threads are launched linearly.
with remapping we fill each cta according to its resilience based launching order.
this resilience based launching order is decided based on offline profiling.
we start fetching threads from the beginning of the linear launching order and put reliable threads into a warp.
meanwhile we organize unreliable threads into another warp if any.
when a warp is filled threads the warp is ready for execution and a new warp is formed for the upcoming threads.
when all the threads are remapped into warps if there are any partially filled reliable or unreliable warps they are combined a thread warp mapping.
b remapping.
c remapping and protection.
unreliable warps are replicated once for detection.
if applying error correction there are two replicas.
fig.
.
logic of mapping remapping and protection.
into one mixed warp and ready for execution.
it is important to note that thread remapping to different warps does not affect their reliability profile because thread resilience is typically determined by branch divergence and input data and not the order of thread execution .
b. partial protection in addition to remapping error detection correction can be applied to unreliable warps.
here we use warp replication triplication to demonstrate how partial protection works.
during remapping when an unreliable warp is filled threads we replicate it into another warp and send both warps to execute.
after these two warps finish execution thread outputs usually the outputs are the computation results to be written into memory by store instructions are compared to detect whether there is any difference see figure c .
since warps are the smallest unit for scheduling at the gpu level duplication at the warp level is transparent for the programmer to handle than at the thread level.
duplication at the cta level would require re the logic of communication synchronization among threads a far more challengingsoftware effort.
this is fully avoided by handling replication at the warp level.
if error correction is applied then each unreliable warp is triplicated according to triple modular redundancy tmr .
in figure c warp is triplicated into warp as well as warp for error correction.
since reliable warps do not need error detection correction they are not replicated triplicated.
note that mixed warps that have both reliable and unreliable threads also need to be duplicated or triplicated for error detection correction.
v. e valuation in this section we present a detailed evaluation of thread remapping section v a .
then we discuss the overhead magnitude when applying protection via remapping section v b .
a. effectiveness of thread remapping we first show the resilience pattern of different benchmarks after remapping see figure .
in this figure if a thread has an sdc probability less or equal to it is considered reliable and remapping is performed based on this sdc threshold i.e.
our goal is a reliability coverage.
gray solid lines in figure separate different warps and yellow dashed lines separate different ctas.
for hotspot in the first cta originally all the reliable threads in figure c are distributed across all warps.
after remapping these reliable threads are gathered and scheduled in the first and last warp of the cta.
we end up with and reliable warps for the second third fourth and sixth cta respectively.
there is a mixed warp at the end of the third cta.
since there are still unreliable threads in this mixed warp it still needs protection.
for the fifth cta all threads are unreliable therefore the thread resilience pattern is the same before and after remapping.
the resilience patterns after remapping for jmeint laplacian meanfilter 2dconv gaussian k2 and pathfinder are shown in figure b f .
the improvement in terms of the percentage of reliable warps for applications is shown in figure .
on average originally the percentage of reliable warps is .
.
by remapping the percentage increases to .
.
the biggest improvement happens on jmeint where there are reliable warps after remapping from the original .
changing the sdc tolerance threshold can result in different remappings.
figure shows how remapping changes the resilience pattern when different sdc thresholds are applied in jmeint.
if the sdc threshold is set to there are a few reliable threads to be remapped see figure a .
for increased sdc thresholds remapping results in more reliable warps see the changes of resilience patterns in figure a to d .
for hotspot even for sdc threshold equal to there are still several reliable warps two warps in figure a within the first ctas and in total for the whole kernel .
with the sdc threshold increasing see figure b d remapping changes the resilience pattern and more reliable threads are gathered together.
a hotspot first ctas .
b jmeint first ctas .
c laplacian first ctas .
d meanfilter first ctas .
e 2dconv .
f gaussian k2 first ctas .
g pathfinder.
fig.
.
resilience patterns after remapping.
if a thread has sdc probability less or equal to it is considered reliable.
due to space constraint we only show the first several ctas for hotspot jmeint laplacian meanfilter and gaussian k2.
fig.
.
percentage of reliable warps before and after remapping.
figure shows how the percentage of sdc outputs changes when the sdc threshold increases for all benchmarks eligible for remapping.
jmeint and pathfinder are the first two benchmarks reaching reliable warps with sdc threshold less than .
gaussian k2 has reliable warps when the sdc threshold is .
only.
this is because thesdc percentage of its major thread group is .
.
hotspot reaches reliable when sdc threshold is about and 2dconv requires sdc threshold to be to get reliable warps.
meanfilter is the most complicated benchmark and it reaches reliable only when sdc threshold is set to because of the threads have sdc outputs.
in general we see that if we set the sdc threshold to only there is still ample room for remapping for most benchmarks as shown in figure .
b. overhead introduced by remapping and protection thread remapping and protection may affect the performance of program execution.
because of the shared cluster environment we are using pure timing measurement is not accurate.
instead we use the number of instruction cycles measured using gpgpu sim performance mode to reflect the execution performance.
a threshold b threshold c threshold d threshold fig.
.
remapped resilience patterns of jmeint under different sdc threshold.
due to space constraint we only show the first ctas.
for overhead analysis we first present the performance overhead due to remapping.
the remapping overhead of each benchmark kernel is shown in figure .
on average the remapping overhead is only .
.
note that there are some benchmarks with negative overhead in figure such as laplacian 2dconv hotspot and pathfinder in these cases execution cycles reduce with remapping.
to better understand why remapping may result in better performance we look into various performance measures that are normalized over the original thread mapping.
figure shows the normalized l1 data cache miss rate and the number of stalls caused by accessing shared memory.
the numbers are normalized by the execution without remapping.
on the one hand from figure a we observe that the l1 data cache miss rate is decreasing.
on the other hand figure b shows that the number of stalls increases for 2dconv jmeint hotspot and pathfinder for laplacian meanfilter a threshold b threshold c threshold d threshold fig.
.
remapped resilience patterns of hotspot under different sdc threshold.
due to space constraint we only show the first ctas.
fig.
.
percentage of reliable warps grows as the sdc tolerance threshold increases.
and gaussian k2 the number of stalls decreases.
trends are not consistent across benchmarks therefore some gain and some lose performance with remapping.
in sum we claim that remapping does not significantly affect performance which 1256fig.
.
overhead of remapping.
a l1 data cache miss rate b number of stalls fig.
.
detailed metrics of remapping overhead.
all numbers are normalized by the execution without remapping.
remains in the same ballpark as the original cases.
last but not least we show the performance savings of applying error detection correction after remapping.
in the case of error detection we compare our technique with rmt redundant multi threading where all the threads all warps are duplicated for error detection.
figure shows the execution performance of our remapping technique in execution cycles comparing to rmt.
we also present the percentage of saved execution cycles at the top of each application bar.
on average the percentage of saved execution cycles for error detection is .
while gaussian k2 achieves a significant .
savings.
in addition we compare partial protection via remapping with tmr triple modular redundancy results are shown in figure .
the average saving in terms of execution cycles is .
and again gaussian k2 has the highest savings of .
.
generally performance results are similar for both error detection and correction and the saving of error correction is always higher for every benchmark.
this is expected since partial protection using triplication avoids the execution of two copies for all reliable warps while for error detection with duplication we only save one copy execution of reliable warps.
the per benchmark savings are related to the benchmark resilience profile i.e.
the percentage of reliable threads in each cta.
since .
of threads in gaussian k2 is reliable this benchmark achieves the highest savings.
summary.
we show the effectiveness of remapping by analyzing the percentage of reliable warps which increases on fig.
.
comparison of execution performance using duplication for error detection between remapping and rmt.
fig.
.
comparison of execution performance using triplication for error correction between remapping and tmr.
average from .
to .
with remapping.
remapping introduces moderate to insignificant overhead.
after applying remapping with protection an average saving of .
and .
execution cycles for error detection duplication of ctas without remapping and correction triplication of ctas without remapping respectively.
vi.
r elated work several works address reliability within the software engineering domain.
chiminey provides a reliable platform for cloud computing.
bleser et al.
presents an automated approach to analyze the resilience of actor programs in distributed systems.
chan et al.
uses invariants to study error propagation in multi threading applications using software fault injection.
yang et al.
uses a software fault injection tool to evaluate different anomaly detectors.
however none of these works are applied in the context of gpus.
redundancy based solutions are used to protect gpgpu applications from errors.
such solutions rely on double execution for error detection called dual modular redundancy dmr and triple execution for error correction called triple modular redundancy tmr .
dimitrov et al.
first evaluate the overhead of introduced redundancy at kernel level thread level and instruction level and show that at all levels the overhead can be over .
wadden et al.
take a deeper look at two different ways of applying redundant multithreading rmt at the granularity of ctas i.e.
intergroup rmt and intragroup rmt and present the trade off between overhead and resilience coverage.
mahmoud et al.
choose instruction level redundancy as it is transparent to programmers and propose sinrg a collection of 1257several software and hardware optimizations to further reduce overhead.
in addition to those works targeting error detection researchers also propose various solutions to correct errors with reduced overhead as compared to a naive implementation of tmr with triple overhead.
while the aforementioned solutions all focus on comparing and analyzing various redundancy based protection solutions and seeking opportunities to reduce redundancy overhead the partial protection methodology approaches this problem from a totally different perspective by focusing on reducing the portion of threads that require protection and on organizing the threads in such a manner that result in more reliable software.
vii.
c onclusions we presented a methodology to remap threads into warps according to their resilience profile.
looking into benchmarks kernels from four benchmark suites we identified that of them are amenable to remapping for resilience.
the proposed solution reduces overhead by identifying the portion of threads that are unreliable and by applying any detection protection mechanism only on them instead of the entire kernel.
in other words our solution reduces overhead by identifying the portion of threads that are unreliable and by organizing them into warps that consist of threads that are either reliable or unreliable.
then any detection protection technique including rmt and tmr can be applied upon the identified unreliable warps only instead of the entire kernel.
even with the simplest error detection and correction technique warp duplication and triplication we achieve an average saving of .
and .
execution cycles for error detection and error correction respectively.
data availability this paper is based on already available open source benchmark data and existing fault injection tools.
the proposed technique offers a methodology for re organizing threads after evaluating thread resilience using the fault site pruning methodology .
any resilience evaluation technique in the literature can be also used to derive thread resilience to guide remapping.