neurodiff scalable differential verification of neural networks using fine grained approximation brandon paulsen university of southern california los angeles california usajingbo wang university of southern california los angeles california usa jiawei wang university of southern california los angeles california usachao wang university of southern california los angeles california usa abstract as neural networks make their way into safety critical systems where misbehavior can lead to catastrophes there is a growing interest in certifying the equivalence of two structurally similar neural networks a problem known as differential verification.
for example compression techniques are often used in practice fordeployingtrainedneuralnetworksoncomputationally andenergy constraineddevices whichraisesthequestionofhowfaithfullythe compressednetworkmimicstheoriginalnetwork.unfortunately existing methods either focus on verifying a single network or relyon loose approximations to prove the equivalence of two networks.
due to overly conservative approximation differential verification lacks scalability in terms of both accuracy and computational cost.
toovercometheseproblems weproposeneurodiff a symbolic andfine grained approximationtechniquethatdrasticallyincreases the accuracy of differential verification on feed forward relu networkswhileachievingmanyorders of magnitudespeedup.neurodiff has two key contributions.
the first one is new convex approximationsthatmoreaccuratelyboundthedifferenceoftwo networks under all possible inputs.
the second one is judicioususe of symbolic variables to represent neurons whose difference bounds have accumulated significant error.
we find that these two techniques are complementary i.e.
when combined the benefit is greater than the sum of their individual benefits.
we have evaluated neurodiff on a variety of differential verification tasks.
our results show that neurodiff is up to 1000x faster and 5x more accurate than the state of the art tool.
introduction there is a growing need for rigorous analysis techniques that can compare the behaviors of two or more neural networks trained for thesametask.forexample suchtechniqueshaveapplicationsin better understandingthe representations learnedby different networks and finding inputs where networks disagree .
the need is further motivated by the increasing use of neural network permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
ase september virtual event australia association for computing machinery.
acm isbn ... .
atechniquethataltersthenetwork sparameterstoreduceitsenergyandcomputationalcost wherewe expect the compressed network to be functionally equivalent to the originalnetwork.insafety criticalsystemswhereasingleinstanceof misbehaviorcanleadtocatastrophe having formalguarantees on the equivalence of the original and compressed networks is highly desirable.
unfortunately mostworkaimedatverifyingortestingneural networks does not provide formal guarantees on their equivalence.
forexample testingtechniquesgearedtoward refutation canprovideinputswhereasinglenetworkmisbehaves or multiple networks disagree but they do not guarantee theabsenceofmisbehaviorsordisagreements.whiletechniques geared toward verification can prove safety or robustness properties of a single network they lack crucial information needed to prove the equivalence of multiple networks.oneexceptionisthereludifftoolofpaulsenetal.
which computes a sound approximation of the difference of two neuralnetworks aproblemknownas differentialverification.while reludiffperformsbetterthanothertechniques theoverlyconservative approximation it computes often causes both accuracy and efficiency to suffer.
toovercometheseproblems weproposeneurodiff anew symbolicandfine grained approximationtechniquethatsignificantly increasestheaccuracyofdifferentialverificationwhileachieving many orders of magnitude speedup.
neurodiff has two key contributions.thefirst contributionisthedevelopmentof convexapproximations afine grainedapproximationtechniqueforbounding the output difference of neurons for all possible inputs which drasticallyimprovesoverthecoarse grained concretizations used byreludiff.thesecondcontributionisjudiciouslyintroducing symbolicvariablestorepresentneuronsinhiddenlayerswhosedifference bounds have accumulated significant approximation error.
thesetwotechniquesarealsocomplementary i.e.
whencombined the benefit is significantly greater than the sum of their individual benefits.
the overall flow of neurodiff is shown in figure where it takes as input two neural networks fandf prime a set of inputs to the neural networks xdefined by box intervals and a small constant thatquantifiesthetolerancefordisagreement.weassumethat f andf primehave the same network topology and only differ in the numerical values of their weights.
in practice f primecould be the compressed version of f or they could be networks constructed using the same network topology but slightly different training 35th ieee acm international conference on automated software engineering ase inputs neurodiff forward analysis convex approximation intermediate variables check yesverifiedpartition x nof f prime x rn x1 x2 proven?
figure the overall flow of neurodiff.
data.wealsonotethatthisassumptioncansupportcompression techniques such as weight pruning by setting edges weights to and even neuron removal by setting all of a neuron s incoming edge weights to .
neurodiff then aims to prove x x. f prime x f x .
it can return verifiedif a proof can be found or undetermined if a specified timeout is reached.
internally neurodifffirstperformsaforwardanalysisusing symbolic interval arithmetic to bound both the absolute value ranges of all neurons as in single network verification and the differencebetweentheneuronsofthetwonetworks.neurodiffthen checksifthedifferencebetweentheoutputneuronssatisfies and if so returns verified.
otherwise neurodiff uses a gradient based refinement topartition xinto twodisjoint subregions x1andx2 and attempts the analysis again on the individual regions.
since x1 andx2form independentsub problems wecan do theseanalyses inparallel hencegainingsignificantspeedup.
the new convex approximations used in neurodiff are significantlymoreaccuratethannotonlythecoarse grained concretizationsinreludiff butalsothestandardconvexapproximations in single network verification tools .
while these standard convex approximations aim to bound the absolute value range ofy relu x wherexis the input of the rectified linear unit relu activationfunction ournewconvexapproximations aim to bound the difference z relu x relu x where xandx arereluinputsoftwocorrespondingneurons.this issignificantlymorechallengingbecauseitinvolvesthesearchof bounding planes in a three dimensional space defined by x and z as opposed to a two dimensional space as in the prior work.
thesymbolicvariableswejudiciouslyaddtorepresentvaluesof neuronsinhiddenlayersshouldnotbeconfusedwiththesymbolicinputsusedbyexistingtoolseither.whiletheuseofsymbolicinputs is well understood e.g.
both in single network verification and differential verification this is the first time that symbolic variables are used to substitute values of hidden neurons duringdifferentialverification.whiletheimpactofsymbolicinputs often diminishes after the first few layers of neurons the impactof these new symbolic variables when judiciously added can be maintained in any hidden layer.
we have implemented the proposed neurodiff in a tool and evaluated it on a large set of differential verification tasks.
ourbenchmarks consists of networks from applications such as aircraftcollisionavoidance imageclassification andhumanactivityrecognition.wehaveexperimentallycomparedwithreludiff the state of the art tool which has also been shown to be superior1.
.
.
.
.
.
.
.
.
.0n0 n0 n1 2n1 n2 n2 2n3 1x1 x2 .
.
.
.
.
.
.
.
.
.
figure motivating example.
to reluval and deeppoly for differential verification.
our results show that neurodiff is up to 000x faster and 5x more accurate.
in addition neurodiff is able to prove many ofthe same properties as reludiff while considering much larger input regions.
to summarize this paper makes the following contributions we propose new convex approximations to more accurately boundthedifferencebetweencorrespondingneuronsoftwo structurally similar neural networks.
we proposea methodfor judiciouslyintroducing symbolic variables to neurons in hidden layers to mitigate the propagation of approximation error.
we implement and evaluate the proposed technique on alarge number of differential verification tasks and demonstrate its significant speed and accuracy gains.
the remainder of this paper is organized as follows.
first we provide a brief overview of our method in section .
then weprovide the technical background in section .
next we present thedetailedalgorithmsinsection4andtheexperimentalresultsinsection .
we review the relatedwork in section6.
finally we give our conclusions in section .
overview inthissection wehighlightourmaincontributionsandillustrate the shortcomings of previous work on a motivating example.
.
differential verification weusetheneuralnetworkinfigure2asarunningexample.the network has two input nodes n0 n0 two hidden layers with two neurons each n1 n1 2andn2 n2 and one output node n3 .
each neuron in the hidden layer performs a summation of their inputs followed by a rectified linear unit relu activation function definedas y max x wherexistheinputtotherelu activation function and yis the output.
letthisentirenetworkbe f andthevalueoftheoutputnodebe n3 f x1 x2 wherex1andx2arethevaluesofinputnodes n0 andn0 respectively.
thenetwork canbeevaluated onaspecific input by performing a series matrix multiplications i.e.
affine transformations followed by element wise relu transformations.
for example the output of the neurons of the first hidden layer is bracketleftbiggn1 n1 bracketrightbigg relu parenleftbigg bracketleftbigg1.
.
.
.
bracketrightbigg bracketleftbiggx1 x2 bracketrightbigg parenrightbigg bracketleftbiggrelu .9x1 .9x2 relu .1x1 .0x2 bracketrightbigg 785differential verification aims to compare fto another network f primethat is structurally similar.
for our example f primeis obtained by rounding the edge weights of fto the nearest whole numbers anetworkcompressiontechniqueknownas weightquantization.
thus f prime n prime k jandn prime f prime x1 x2 are counterparts of f nk jand n3 f x1 x2 for k and j .
our goal is to prove that f prime x1 x2 f x1 x2 islessthansomereasonablysmall for all inputs defined by the intervals x1 andx2 .
for easeofunderstanding weshowtheedgeweightsof finblack and f primein light blue in figure .
.
limitations of existing methods naively onecouldadaptanystate of the art single networkverificationtoolforourtask includingdeeppoly andneurify .
neurify in particular takes a neural network and an input region of the network and uses interval arithmetic to produce soundsymboliclowerandupperboundsforeachoutputnode.typically neurify would then use the computed bounds to certify the absence of adversarial examples for the network.
however forourtask theboundsmustbecomputedforbothnetworksfandf prime.then wesubtractthem andconcretizetocompute lower andupper boundson f prime x1 x2 f x1 x2 .
inour example theindividualboundswouldbe approximately duetorounding and for nodes n3 1andn prime respectively.
after the subtraction we would obtain the bounds .
after concretization we would obtain the bounds .
unfortunately the bounds are far from being accurate.
the reludiff method of paulsen et al.
showed that by directlycomputinga differenceinterval layer by layer theaccuracy canbegreatlyimproved.fortherunningexample reludiffwould firstcomputeboundsonthedifferencebetweentheneurons n1 andn prime whichis andthensimilarlycomputeboundsonthe differencebetweenoutputsof n1 2andn prime .then theresultswould be used to compute difference bounds of the subsequent layer.
the reason it is more accurate is because it begins computing part of the differencebound beforeerrors haveaccumulated whereasthe naive approach first accumulates significant errors at each neuron andthencomputesthedifferencebound.inourrunningexample reludiff wouldcomputethetighterbounds .
while reludiff improves over the naive approach in many cases ituses concretevaluesfortheupperandlowerbounds.inpractice thisapproachcansufferfromsevereerror explosion.specifically whenever a neuron of either network is in an unstablestate i.e.
whenarelu sinputintervalcontainsthevalue0 ithasto concretize the symbolic expressions.
.
our method thekeycontributioninneurodiff ournewmethod isa symbolic andfine grained approximation technique that both reduces the approximationerrorintroducedwhenaneuronisinanunstable state andmitigatestheexplosionofsuchapproximationerrorafter it is introduced.
.
.
convexapproximationforthedifferenceinterval.
ourfirst contributionisdevelopingconvexapproximationstodirectlybound thedifferencebetweentwoneuronsafterthesereluactivations.
specifically foraneuron ninfandcorrespondingneuron n primeinf prime wewanttoboundthevalueof relu n prime relu n .weillustrate the various choices using figures and .
the naive way to bound this difference is to first compute approximations of y relu n andy prime relu n prime separately and then subtract them.
since each of these functions has a single variable convex approximation is simple and is already used by singlenetwork verification tools .
figure shows the function y relu n and its bounding planes shown as dashed lines in a two dimensional space details in section .
however as we havealreadymentioned approximationerrorswouldbeaccumulatedintheboundsof relu n andrelu n prime andthenamplifiedby theinterval subtraction.thisisprecisely whythenaiveapproach performs poorly.
thereludiffmethodofpaulsenetal.
improvesuponthe new approximation by computing an interval bound on n prime n denoted thenrewriting z relu n prime relu n asz relu n relu n and finally bounding this new function instead.
figure shows the shape of z relu n relu n in a threedimensional space.
note that it has four piece wise linear subregions definedbyvaluesoftheinputvariables nand .whilethe bounds computed by reludiff shown as the horizontal yellow planes in figure are sound in practice they tend to be loosebecausetheupperandlowerboundsarebothconcretevalues.
such eager concretization eliminates symbolic information that contained before applying the relu activation.
in contrast our method computes a convex approximation of z shownbythe tilted yellowplanesinfigure5.sincethesetilted bounding planes are in a three dimensional space they are sig nificantly more challenging to compute than the standard two dimensionalconvexapproximations showninfigure6 usedbysingle network verification tools.
our approximations have the advantage of introducing significantly less error than the horizontal planes used in reludiff while maintaining some of the symbolic information for before applying the relu activation.
we will show through experimental evaluation section that our convex approximation can drastically improve the accuracy of thedifferencebounds andareparticularlyeffectivewhentheinput region being considered is large.
furthermore the tilted planes showninfigure5areforthegeneralcase.forcertainspecialcases we obtain even tighter bounding planes details in section .
in therunningexample usingournewconvexapproximationswould improve the final bounds to .
.
.
symbolicvariablesforhiddenneurons.
oursecondcontribution is introducing symbolic variables to represent the output valuesofsomeunstableneurons withthegoaloflimitingthepropagationofapproximationerrorsaftertheyareintroduced.inthe running example since both n1 1andn prime 1are in unstable states i.e.
theinputintervalsofthereluscontainthevalue0 wemay introduceanewsymbol x3 relu n prime relu n1 .inallsubsequentlayers wheneverthevalueof relu n prime relu n1 is needed we use the bounds instead of the actual bounds.
786figure the shape of z relu n relu n .
figure bounding planes computed by reludiff .
figure5 boundingplanescomputedbyournewmethod.lb n ub n ub relu n lb relu n figure bounding planes computed by neurify .
the reason why using x3can lead to more accurate results is because even though our convex approximations reduce the error introduced thereisinevitablysomeerrorthataccumulates.introducingx3allows this error to partially cancel in the subsequent layers.
in our running example introducing the new symbolic variablex3would be able to improve the final bounds to .
while creating x3improved the result in this case carelessly introducing new variables for all the unstable neurons can actually reduce the overall benefit see section .
in addition the computationalcostofintroducingnewvariablesisnotnegligible.therefore inpractice wemustintroducethesesymbolicvariablesjudiciously to maximize the benefit.
part of our contribution in neurodiff is in developing heuristics to automatically determine when to create new symbolic variables details in section .
background in this section we review the technical background and then introduce notations that we use throughout the paper.
.
neural networks wefocusonfeed forwardneuralnetworks whichwedefineasa functionfthat takes an n dimensional vector of real values x x wherex rn and maps it to an m dimensional vector y y wherey rm.wedenotethisfunctionas f x y.typically eachdimension of yrepresentsa score such asaprobability that the input xbelongs to class i where i m. a network with llayers has lweight matrices each of which is denoted wk for k l. for each weight matrix we have wk rlk lkwherelk 1isthenumberofneuronsinlayer k and likewisefor lk andl0 n. eachelement in wkrepresents the weight of an edge from a neuron in layer k to one in layer k. letnk jdenote the jthneuron of layer k andnk idenote theithneuronof layer k .w eusewk todenote theedge weight from nk itonk j. in our motivating example we have w1 .
andw1 .
.
mathematically the entire neural network can be represented byf x fl wl fl wl ...f1 w1 x ... wherefkis the activation function of the kthlayer and k l. we focus on neural networks with relu activations because they are the most widelyimplementedinpractice butourmethodcanbeextended to other activation functions such as si moidandtanh and other layer types such as convolutional and max pooling.
we leave this as future work.
.
symbolic intervals to compute approximations of the output nodes that are sound for all input values we leverage interval arithmetic which can be viewed as an instance of the abstract interpretation framework .
it is well suited to the verification task because interval arithmetic 787is soundly defined for basic operations of the network such as addition subtraction and scaling.
leti beanintervalwithlowerbound lb i and upper bound ub i .
then for intervals i1 i2 we have addition and subtractiondefinedas i1 i2 andi1 i2 respectively.
for a constant c scaling is defined as c i1 c lb i1 c ub i1 whenc andc i1 otherwise.
while interval arithmetic is a sound over approximation it is not always accurate.
to illustrate let f x 3x x and say we are interested in bounding f x whenx .
one way to boundfis by evaluating f i wherei .
so yields .
unfortunately the most accurate bounds are .
thereare atleast twowayswecanimprovetheaccuracy.first we can soundly refine the result by dividing the input intervals into disjointpartitions performing theanalysis independentlyon each partition and then unioning the resulting output intervals together.
previous work has shown the result will be at leastas precise andoftenbetter.forexample ifwepartition x intox andx and perform the analysis for each partition the resulting bounds improve to .
second thedependencebetweenthetwointervalsarenotleveraged when we subtract them i.e.
that they were both xterms and hence could partially cancel out.
to capture the dependence we canusesymbolic lowerandupperbounds whichareexpressions interms ofthe input variable i.e.
i .
evaluating f i thenyieldstheinterval if forx .whenusing symbolic bounds eventually we must concretize the lower and upperboundequations.wedenoteconcretizationof lb if 2x andub if 2xaslb if and ub if respectively.
compared to the naive solution this is a significant improvement.
when approximating the output of a given function f x y overaninputinterval x x onemayprovesoundnessbyshowing that the evaluation of the lower and upper bounds on any input x xare always greater than and less than respectively to the true value of f x .
formally for an interval i letlb i x be the evaluationofthelowerboundequationoninput x andsimilarlyfor ub i x .then theapproximationisconsideredsoundif x x we have lb i x f x ub i x .
.
convex approximations whilesymbolicintervalsareexactforlinearoperations i.e.theydo notintroduceerror thisis notthecasefornon linearoperations such as the relu activation.
this is because for efficiency reasons thesymboliclowerandupperboundsmustbekeptlinear.thus developing linear approximations for non linear activation functions hasbecomeasignifciantareaofresearchforsingleneuralnetwork verification .wereviewthebasicsbelow butcaution that they are different from our new convex approximations in neurodiff.
wedenotetheinputtothereluofaneuron nk jassin nk j and theoutputas s nk j .theapproachusedbyexistingsingle network verification tools is to apply an affine transformation to the upper bound of sin nk j such that ub sin nk j x where x x andxis the input region for the entire network.
for the lower bound thereexistseveralpossibletransformations includingthe one used by neurify shown in figure where n sin nk j and the dashed lines are the upper and lower bounds.
weillustratetheupperboundtransformationfor n1 1ofourmotivatingexample.aftercomputingtheupperboundofthereluinput ub sin n1 .9x1 .9x2 wherex1 andx2 itcomputestheconcretelowerandupperbounds.wedenotethese asub sin n1 .6and ub sin n1 .
.werefertothem aslandu respectively forshorthand.
then itcomputesthe line that passes through u u and l .
letting y ub sin n1 be the upper bound equation of the relu input it computes the upper bound of the relu output as ub s n1 u u l y l .95x1 .95x2 .
.
when considering a single relu of a single network convex approximation is simple because there are only three states thatthe neuron can be in namely active inactive and unstable.
fur thermore in only one of these states convex approximation is needed.incontrast differentialverificationhasto considerapair of neurons which has up to nine states to consider between the tworelus.furthermore differentstatesmayresultindifferentlinear approximations and some states can even have multiple linear approximationsdependingonthedifferenceboundof n prime n. aswewillshowinsection4 therearesignificantlymoreconsiderations in our problem domain.
our approach we first present our baseline procedure for differential verification offeed forwardneuralnetworks section4.
andthenpresentour algorithms for computing convex approximations section .
and introducing symbolic variables section .
.
.
differential verification baseline we build off the work of paulsen et al.
so in this section we reviewtherelevantpieces.weassumethattheinputtoneurodiff consists of two networks fandf prime each with llayers of the same size.let n prime k jinf primebe theneuron paired with nk jinf.
this implicitly creates a pairing of the edge weights between the two networks.
we first introduce additional notation.
wedenotethedifferencebetweenapairofneuronsas k j n prime k j nk j. for example 1 .
under the input x1 x2 in our motivating example shown in figure .
wedenotethedifferenceinapairofedgeweightsas w k w prime k wk .
for example w .
.
.
.
we extend the symbolic interval notation to these terms.
thatis sin k j denotestheintervalthatbounds n prime k j nk j beforeapplyingrelu and s k j denotestheintervalafter applying relu.
giventhatwehavecomputed s nk i s n prime k i s k i for every neuron in the layer k now we compute a single s k j in the subsequent layer kin two steps and then repeat for each j lk .
first wecompute sin k j bypropagatingtheoutputintervals from the previous layer through the edges connecting to the target x l u l prime u l l prime x u u prime l u l u primel u figure illustration of lemmas .
and .
.
neuron.
this is defined as sin k j summationdisplay.
i parenleftbigg s k i w prime k s nk i w k parenrightbigg weillustratethiscomputationonnode 1 1inourexample.first we initialize s 0 s 0 .
then we compute sin 1 .
.
.
.
.
forthesecondstep weapplyreluto sin k j toobtains k j .
thisiswhereweapplythenewconvexapproximations section4.
to obtain tighter bounds.
toward this end we will focus on the following two equations z1 relu nk j k j relu nk j z2 relu n prime k j relu n prime k j k j whilepaulsenetal.
alsocomputeboundsofthesetwoequations theyuse concretizations insteadof linearapproximations thus throwingawayallthesymbolicinformation.fortherunningexample their method wouldresult in the bounds of s 1 .
in contrast our method will be able to maintain some or all of the symbolic information thus improving the accuracy.
.
two useful lemmas before presenting our new linear approximations we introduce two useful lemmas which will simplify our presentation as well as our soundness proofs.
lemma4.
.
letxbea variablesuchthat l x uforconstants l 0and0 u. for a constant l primesuch that l l prime we have x x l u l prime u l l prime l prime.
lemma4.
.
letxbea variablesuchthat l x uforconstants l 0and0 u. for a constant u primesuch that u prime u we have u prime x u u prime l u l u prime x. weillustratetheselemmasinfigure7.thesolidbluelineshows the equation y xfor the input interval l x u. the upper dashed line illustrates the transformation of lemma .
and the lower dashed line illustrates lemma .
.
specifically lemma .
shows a transformation applied to xwhose result is always greater thanboth l primeandx.similarly lemma4.2showsatransformation applied to xwhose resultis always lessthan both u primeandx.
these lemmas will be useful in bounding equations and .
.
new convex approximations for s k j now wearereadytopresentournewapproximations whichare linear symbolic expressions derived from equations and .
we first assume that nk jandn prime k jcould both be unstable i.e.
theycouldtakevaluesbothgreaterthanandlessthan0.thisyieldsboundsforthegeneralcaseinthattheyaresoundinallstatesof nk j andn prime k j sections .
.
and .
.
.
then we consider special cases ofnk jandn prime k j inwhicheventighterupperandlowerboundsare derived section .
.
.
tosimplifynotation welet n n prime and standinfor nk j n prime k j and k jin the remainder of this section.
.
.
upperboundforthegeneralcase.
letl ub sin and u ub sin .
the upper bound approximation is ub s ub sin ub sin ub sin ub sin l u u lotherwise thatis whentheinput s delta upperboundisgreaterthan0forall x x wecanusetheinput supperboundunchanged.whenthe upperboundisalwayslessthan0 thenewoutput supperbound is then .
otherwise we apply a linear transformation to the upper bound which results in the upper plane illustrated in figure .
we prove all three cases sound.
proof.weconsidereachcaseaboveseparately.inthefollowing weuseequation1toderivethebounds butwenoteasymmetric proof using equation exists and produces the same bounds.
case1 ub sin .wefirstshowthat accordingtoequation when we have z1 .
this then implies that if ub sin thenz1 ub sin x for allx x and hence it is a valid upper bound for the output interval.
assume .
we consider two cases of n. first consider n. observe n n .
thus the relu s of equation simplify to z1 n n z1 .
when n equation simplifies to z1 relu n .
sincen we have n relu n .
thus z1 relu n so the approximation is sound.
case ub sin .this case was previously proven but we restate it here.
ub sin n prime n relu n prime relu n relu n prime relu n .
case .by case any ub s that satisfies ub s x andub s x ub sin x for allx xis sound.
both inequalitiesholdbylemma4.
with x ub sin l ub sin u ub sin andl prime .
square we illustrate the upper bound computation on node n1 1of our motivating example.
recall that ub sin n1 .1x1 .1x2.
since ub sin n1 .
and ub sin n1 .
we are in the third case of our linear approximation above.
thus we have ub sin n1 .1x1 .1x2 .
.
.
.
.5x1 .5x2 .
.this istheupperboundingplaneillustratedin figure5.the volume under this plane is less than the upper bounding plane of reludiff shown in figure .
.
.
lower bound for the general case.
letl lb sin and u lb sin the lower bound approximation is lb s lb sin lb sin lb sin lb sin u l u lotherwise that is whenthe inputlower boundis alwaysless than0 wecan leaveitunchanged.whenitisalwaysgreaterthan0 thenewlower boundisthen0.otherwise weapplyatransformationtothelower bound which results in the lower plane illustrated in figure .
we prove all three cases sound.
proof.weconsidereachcaseaboveseparately.inthefollowing weuseequation1toderivethebounds butwenoteasymmetric proof using equation exists and produces the same bounds.
case lb sin .we first show that according to equation when we have z1.
this then implies that if lb sin we have lb sin x z1for allx x and hence it is a valid lower bound for the output interval.
assume .
we consider two cases of n .
first let n .
observe n n s ow ec a n simplify equation to z1 n n z1.
second letn n. then equation simplifies to z1 relu n max n min n .
now observe n min n z1.
case lb sin .this case was previously proven sound but we restate it here.
lb sin n prime n relu n prime relu n relu n prime relu n .
case .by case any lb s that satisfies lb s x andlb s x lb sin x for allx xwill be valid.
both inequalities hold by lemma .
with x lb sin u prime l lb sin andu lb sin .
square we illustratethe lowerboundcomputation onnode n1 1ofour motivatingexample.recallthat lb sin n1 .1x1 .1x2.since lb sin n1 .
and lb sin n1 .
we are in the third case of our linear approximation.
thus we have lb s n1 .1x1 .1x2 .
.
.
.
.05x1 .05x2 .
.thisis thelowerboundingplaneillustratedinfigure5.thevolumeabove this plane is lessthan the lower bounding plane of reludiff shown in figure .
.
.
tighter bounds for special cases.
while the bounds presented so far apply in all states of nandn prime under certain conditions weareabletotightentheseboundsevenfurther.toward thisend werestatethefollowingtwolemmasprovedbypaulsen etal.
whichwillcomeinhandy.theyarerelatedtoproperties of equations and respectively.
lemma .
.
relu n n max n lemma .
.
n prime relu n prime min n prime theselemmasprovideboundswhen nandn primear epr o v edtobe linear based on the absolute bounds that we compute.
figure tighter upper bounding plane.
figure tighter lower bounding plane.
tighter upper bound when n primeis linear.
in this case we have ub s ub sin whichisanimprovementforthesecond or third case of our general upper bound.
proof.byourcaseassumption equation2simplifiestotheone in lemma .
.
thus z2 min n prime z2 .
square tighterupperboundwhen nislinear ub sin lb sin n ub sin .weillustratethe z1planeundertheseconstraints in figure .
let l ub sin and letu ub sin andl prime lb sin n weuselemma4.1toderive ub s ub sin l u l prime u l l prime.thisresultsintheupperplaneoffigure8.thisimproves over the third case in our general upper bound because it allows the lower bound of ub s to be less than .
proof.byourcaseassumption equation1simplifiestotheone inlemma4.
.bylemma4.
wehaveforall x x ub s x lb sin n andub s x ub sin x .thesetwoinequalities imply ub s max n .
square tighter lower bound when nis linear.
here we can use the approximation lb s lb sin .
this improves over the second and third cases of our general lower bound.
proof.byourcaseassumption equation1simplifiestotheone in lemma .
.
thus z1 max n z1 .
square tighterlowerboundwhen n primeislinear lb sin lb sin n prime lb sin .we illustrate the z2plane under these constraints in figure .
here letting l lb sin u lb sin andu prime lb sin n prime we can use lemma .
to derive the approximation lb s lb sin u u l prime u l u prime.thisresultsinthelower plane of figure .
this improves over the third case since it allows the upper bound to be greater than .
790proof.by our case assumption equation simplifies to the one shown in lemma .
.
by lemma .
we have for all x x lb s x lb sin x andlb s x lb sin n prime .
these two inequalities imply lb s x min n prime .
square .
intermediate symbolic variables for s whileconvexapproximationsreducetheerrorintroducedbyrelu even small errors tend to be amplified significantly after a few layers.
to combat the error explosion we introduce new symbolic terms to represent the output values of unstable neurons which allow their accumulated errors to cancel out.
we illustrate the impact of symbolic variables on n1 1of our motivatingexample.recallwehave s 1 .05x1 .05x2 .
.05x1 .05x2 .
.afterapplyingtheconvexapproximation we introduce anew variable x3such that x3 .05x1 .05x2 .
.05x1 .05x2 .
.thenweset s 1 andpropagate thisintervalasbefore.afterpropagatingthrough n2 1andn2 2and combining them at n3 thex3terms partially cancel out resulting in the tighter final output interval .
in principle symbolic variables may be introduced at any unstable neurons that introduce approximation errors however there areefficiencyvs.accuracytradeoffswhenintroducingthesesymbolic variables.
one consideration is how to deal with intermediate variables referencing other intermediate variables.
for example if we decide to introduce a variable x4forn2 thenx4will have anx3term in its equation.
then when we are evaluating a symbolic bound that contains an x4term which will be the case for n3 wewillhavetorecursivelysubstitutetheboundsofthepreviousintermediatevariables suchas x3.thisbecomesexpensive especially when it is used together with our bisection based refinement .thus inpractice wefirstremoveanyback