self checking deep neural networks in deployment yan xiao ivan beschastnikhy david s. rosenblumz changsheng sun sebastian elbaum yun lin and jin song dong school of computing national university of singapore singapore dcsxan nus.edu.sg changsheng sun outlook.com dcsliny nus.edu.sg dcsdjs nus.edu.sg ydepartment of computer science university of british columbia vancouver bc canada bestchai cs.ubc.ca zdepartment of computer science george mason university fairfax v a usa dsr gmu.edu department of computer science university of virginia charlottesville v a usa selbaum virginia.edu abstract the widespread adoption of deep neural networks dnns in important domains raises questions about the trustworthiness of dnn outputs.
even a highly accurate dnn will make mistakes some of the time and in settings like self driving vehicles these mistakes must be quickly detected and properly dealt with in deployment .
just as our community has developed effective techniques and mechanisms to monitor and check programmed components we believe it is now necessary to do the same for dnns.
in this paper we present dnn self checking as a process by which internal dnn layer features are used to check dnn predictions.
we detail selfchecker a self checking system that monitors dnn outputs and triggers an alarm if the internal layer features of the model are inconsistent with the final prediction.
selfchecker also provides advice in the form of an alternative prediction.
we evaluated selfchecker on four popular image datasets and three dnn models and found that selfchecker triggers correct alarms on .
of wrong dnn predictions and false alarms on .
of correct dnn predictions.
this is a substantial improvement over prior work s elforacle dissector and confidnet .
in experiments with self driving car scenarios selfchecker triggers more correct alarms than s elforacle for two dnn models da ve and chauffeur with comparable false alarms.
our implementation is available as open source.
index terms deep learning trustworthiness deployment i. i ntroduction deep neural networks dnns are now used in a variety of domains including speech processing nlp medical diagnostics image processing robotics and even reconstruction of brain circuits .
the power and accuracy of dnns have led to deployments of deep learning dl systems in safety and security critical domains including selfdriving cars malware detection and aircraft collision avoidance systems .
such domains have a low tolerance for mistakes.
the software systems in a self driving car for example must have high assurance in deployment.
unfortunately the stochastic nature of dl virtually ensures that dl models will not achieve accuracy even on the training dataset.
since in mission critical applications a wrong dnn decision could be costly we believe that such applications must include logic to check the trustworthiness of a dnn s output and raise an alarm when there is low confidence in the output.
our community has developed suchmethods for programmed components and now is the time to do so for learned ones like dnns.
trustworthiness of a simple dnn can be measured with softmax probabilities or information theoretic metrics such as entropy and mutual information .
however in complex dnns with many layers and neurons softmax probabilities and entropy are unreliable confidence estimators of the prediction .
even for abnormal samples dnns may still produce overconfident posterior probabilities.
for example when we built classifiers for vgg on cifar we found that of predictions that were incorrect had maximum softmax probabilities over and incorrect predictions had maximum softmax probabilities over .
we had similar results on other datasets and models.
this illustrates the unreliability of the softmax probabilities as confidence estimators of the final prediction.
our goal is to build a general purpose system that monitors a deployed dnn s predictions during inference raises an alarm if there is low confidence in the predictions and provides an alternative prediction that we call an advice .
a key challenge in building such a system is finding a source of additional information to check dnn outputs.
the inspiration for our work comes from kaya et al.
who study internal dnn behavior .
they found that a dnn can reach a correct prediction before the final layer.
in fact the final layer of a dnn may change a correct internal prediction into an incorrect prediction.
this work illustrates that features extracted from internal layers of a dnn contain information that can be used to cross check a model s output.
inspired by kaya et al.
s work we define self checking as a process by which internal dnn layer features are used to check dnn predictions.
in this paper we describe a novel selfchecking system called selfchecker that triggers an alarm if the internal layer features of the model are inconsistent with the final prediction.
selfchecker also provides advice in the form of an alternative prediction.
selfchecker assumes that the training and validation datasets come from a distribution similar to that of the inputs that the dnn model will face in deployment.
selfchecker uses kernel density estimation kde to extrapolate the probability density distributions of each layer s ieee acm 43rd international conference on software engineering icse .
ieee output by evaluating the dnn on the training data.
based on these distributions the density probability of each layer s outputs can be inferred when the dnn is given a test instance.
selfchecker measures how the layer features of the test instance are similar to the samples in the training set.
if a majority of the layers indicate inferred classes that are different from the model prediction then selfchecker triggers an alarm.
in addition not all layers can contribute positively to the final prediction .
selfchecker therefore uses a search based optimization to select a set of optimal layers to generate a high quality alarm and advice.
we evaluated selfchecker s alarm and advice mechanisms with experiments on four popular and publicly available datasets mnist fmnist cifar and cifar and three dnns convnet vgg and resnet against three competing approaches s elforacle d issector and confidnet .
our results show that selfchecker achieves the highest f1 score .
which is .
higher than the next best approach confidnet .
our evaluation of selfchecker s dnn prediction checking runtime shows an acceptable time overhead of .98ms.
we also compared selfchecker to the state of the art approach for self driving car scenarios s elforacle and found that selfchecker triggers more correct alarms and a comparable number of false alarms.
our paper makes the following three contributions ?we present the design of selfchecker which uses density distributions of layer features and a search based layer selection strategy to trigger an alarm if a dnn model output has low confidence.
we show that selfchecker achieves better alarm accuracy than previous work.
?unlike existing work selfchecker provides advice in the form of an alternative prediction.
we find that models on a class dataset can use this advice to achieve higher prediction accuracy.
?we demonstrate the effectiveness of selfchecker s alarms and advice on publicly available dnns ranging from small models convnet to large and complex models vgg and resnet and self driving car scenarios.
our implementation is open source1.
ii.
b ackground and motivation in a deep neural network dnn an input is fed into the input layer then passed through a series of hidden layers that extract features from the input using activation functions attached to neurons and the process concludes with the output layer which uses the extracted features to output a prediction using either classification from a categorical set of classes or regression in the form of real valued ordinals .
the behavior of a layer during inference thus can be characterized by its vector of neuron activation outputs.
in what follows we refer to these layer wise vectors of activation outputs as the layer features analyzed by our approach.
the promise of using layer features dnns make decisions based on features extracted from training data.
but how can we judge if a model is making a wrong decision for a given test instance?
one way is to check whether the model has previously observed a similar instance during training.
this raises the question of how to define the similarity between a test instance xand a training instance x0.
most existing studies use a distance based measure such aslpor cosine similarity.
we think this is problematic since the inputs are complex enough and need dnns to extract features so we doubt that a distance measure defined directly on the inputs can properly capture similarity.
instead we use the features of the inputs extracted by internal layers in dnns to capture similarity.
specifically we define the similarity as the likelihood of the dnn having seen a similar layer features during training.
we use probability density distributions extrapolated from the training process to measure the similarity between layer features of a given input and those observed for training data.
input 12convolutional layerconvolutional layerconvolutional layerfully connected layersoftmax v isualization...... similar to similar to similar to predicted as similar to similar to similar to predicted as input input attention score low high32 fig.
.
the top of the figure depicts the architecture of a convolutional neural network with three convolutional layers to classify digit images.
the two rows of images at the bottom depict attention heatmaps for the associated layers when given test inputs for digits and respectively.
fig.
presents a motivating example where a convolutional neural network cnn with three convolutional layers trained on mnist is used to classify images of digits and while outputting labels and as the respective predictions.
to visualize where the features of each layer focus we apply grad cam to highlight the attention heatmap on the original images as shown in the bottom two rows of images in fig.
.
the heatmap images show that different layers have different points of focus.
for example the first and second images of digit are similar to itself but the third image is closer to digit .
similarly the first image of digit is similar to digit but the second and third images are similar to .
although the cnn misclassifies the second image in both cases the images appear to be recognized correctly by one or more hidden layers.
this example thus illustrates the promise 373selfcheckerkde on training setdensity functionsper class of each layer extrapolatewith kde alg.
trained m layer selectionvalidationset estimate each layerdensity using densityfunctionstrained mtrainingset search for properlayer combinations alg.
and alg.
estimate each layerdensity using densityfunctionstrained m select layers forclass c test instancepredictionc alarm alg.
alarm trueadvice zalarm falsenoyeschecking modelin deploymentselected layers withmax alarm or adviceaccuracy per classfig.
.
the design of selfchecker and its integration with a trained model and model predictions.
of using layer features to check the model s classification of a test instance.
dnns exist in many variants and can be combined to form more complex models.
for example models used in urban flow prediction combine convolutional graph and recurrent neural nets.
however all these dnns extract features using internal layers and that is the focus of our research.
the design we present targets dnn classifiers with convolutional layers and fully connected layers.
our system also works for regression networks by transforming the network into a binary classification problem.
since our design uses layer features it should work on other types of dnns such as recurrent neural networks.
we leave the evaluation of our system on other dnn types to future work.
b. the challenges of using layer features the preceding example also raises two challenges that a technique using layer features must resolve which layers should be selected for checking the classification of a test instance?
for example does selecting more layers lead to a better checker?
how should the features from the different layers be aggregated either to determine if an alarm should be raised or to produce alternative advice?
resolving these questions is the goal of this paper.
problem statement.
given a trained dnn classifier and a test instance we aim to develop a systematic method called selfchecker for determining whether the dnn will misclassify the test instance based on extensive checking the dnn s internal features.
first selfchecker should trigger an alarm if it detects a potential misclassification of the test instance.
second and going beyond the previous studies selfchecker should provide advice once an alarm is triggered in the form of an alternative classification.
our goalis for selfchecker to achieve high accuracy in both triggering alarms and offering advice.
iii.
d esign of selfchecker the goals of selfchecker are to check a dnn s prediction to raise an alarm if the dnn s prediction is determined to be incorrect and to provide an advice or an alternative prediction.
selfchecker s training module is used after the model has been trained to configure selfchecker s behavior in deployment.
the training module uses the training and validation datasets as well as the trained model to generate a deployment configuration.
selfchecker s deployment module runs along with the inference process it analyses the internal features of a dnn when the model is given a test instance and provides an alarm as well as an advice if it detects an inconsistency in the model s output.
to detect these inconsistencies the deployment module uses the configuration supplied to it by the training module.
note that although selfchecker analyses the features extracted from the internal layers of a dnn the training module is independent from the architecture of the model and requires no model modifications or retraining.
the deployment module however is specific to a dnn.
fig.
overviews our approach.
given a dnn model m trained on training dataset dtrain and validated on validation setdvalid for each layer in m selfchecker s training module first computes layer wise density distributions of each class using kernel density estimation kde on dtrain section iii a .
based on the distributions selfchecker can estimate the density values of each validation or test instance on each class.
the higher the values of the class the more similar the features of the instance in this layer are to the specific class.
after selfchecker obtains all of estimated density values on dvalid across all layers selfchecker finds the uni00000017 uni00000015 uni00000013 uni00000015 uni00000017 uni0000003b uni00000013 uni00000014 uni00000013 uni00000015 uni00000013 uni00000016 uni00000013 uni00000027 uni00000048 uni00000051 uni00000056 uni0000004c uni00000057 uni0000005c a histogram uni00000017 uni00000015 uni00000013 uni00000015 uni00000017 uni0000003b uni00000013 uni00000011 uni00000013 uni00000013 uni00000011 uni00000015 uni00000013 uni00000011 uni00000017 uni00000013 uni00000011 uni00000019 uni00000027 uni00000048 uni00000051 uni00000056 uni0000004c uni00000057 uni0000005c uni00000003 uni00000029 uni00000058 uni00000051 uni00000046 uni00000057 uni0000004c uni00000052 uni00000051 b kde x0.
.
.
.6density functionh .
h .
c bandwidth fig.
.
an example to illustrate kde computation with a showing the set of input 1d points b showing how to obtain the distribution using a kde and c showing the distributions obtained by using different bandwidths.
optimal layer combinations to reach the best alarm and advice accuracy.
since different classes produce distinctive feature behaviors in different layers selfchecker uses global search to find the optimal layer combinations per class section iii b .
finally when the model is presented with a test instance in deployment selfchecker s deployment module decides whether to provide an alarm as well as an advice by using the density values and specific layer combinations section iii c .
we now detail each step in our approach.
a. kde of the training set given a trained classifier mwithllayers except for the input layer and cclasses letxt fx1 xngand yt fy1 y ngindtrain be the set of training inputs and corresponding ground truth labels.
similarly let xv yv and yvindvalid be the validation inputs corresponding ground truth labels and model predictions.
we denote the outputs of all layers in the training set as feature vectors vt fvt vt lg where the feature vectors of the layer lwithnlneurons are vt l2rnl.
we note that the feature vectors are trivially available after each execution of the trained model mover a given input.
in general mfocuses on different features in different layers for different classes.
selfchecker s aim is to compute the density probability of feature vectors in each layer for each class based on the training set dtrain.
using these density probabilities selfchecker will then estimate how close the features in a specific layer for a certain input are to those of the training set.
kde is a non parametric method for estimating a probability density function by using a finite number of samples from a population .
the resulting density function allows the estimation of relative likelihood of a given random variable.
in this paper we use the gaussian kernel which works well for the multivariate data common to most datasets and produces smooth functions.
given a data sample fx1 x2 x mg selfchecker estimates the kernel density function fas follows f x mhmx i 1k x xi h wherekis the gaussian kernel function and hisbandwidth .
to see how a kde with gaussian kernels works consider fig.
.
first each observation in the sample is replaced with a gaussian curve centered at that value green curves thesework as a kernel.
the green curves are then summed to compute the value of the density at each point.
fig.
b also shows the normalized curve in blue whose area under the curve is .
the bandwidth parameter hof the kde controls how tightly the estimate is fit to the sample data.
it corresponds to the width of the kernels green lines in fig.
b .
fig.
c shows that if his large the curve is smooth but flat.
and if h is small the curve is peaked and oscillating.
the choice of h is based on the number of sample points and their dimensions.
for each combination of class and layer selfchecker uses gaussian kde to estimate the density function that the training data for the class induces on the layer s feature vector.
then given a test instance selfchecker estimates the probability density for each class within each layer from the computed density functions.
finally selfchecker uses these probability densities to infer classes for each layer defined as follows definition inferred class for a layer given a test instance the inferred class for layer lis the class for which the test instance induces the maximum estimated probability density among l s per class density functions.
algorithm details selfchecker s procedure for kde estimation and inference.
lines show the gaussian kde used to extrapolate the density distribution functions of feature algorithm kde estimation and inference input input instances in dtrain dvalid xt xv true labels in dtrain yt trained model mwithllayers and cclasses variance threshold tvar output kde functions for each combination of class and layer kdes inferred classes for all layers on dvalid kdeinferlv estimation 2forc in c do obtain instancesxt cwhose true label is c forl in l do vt lc m output l xt c remove elements in vt lcwhose variance is less than tvar f x jvt lcjhpjvt lcj i 1k x vt lc h kdes f x end 10end inference 12forxinxvdo forl in l do vl m output l x remove values of the neurons filtered in the training set from vl forc in c do kde values kdes vl end kdeinferlv max kde values index end 21end 375vectors per class in each layer.
as illustrated with fig.
we want to extrapolate the patterns of the attention overlaid on the raw input.
since the input instances with different classes perform differently in different layers the attentions in the first layer of digit are different from the first one of that is also different from the second one of itself.
selfchecker therefore splits the original training input instances according to their true classes line .
based on these it obtains the outputs of each layer given the trained model m line .
selfchecker also uses mean pooling to reduce dimensions for convolutional layers and then filters out neurons whose values show variance lower than a pre defined threshold tvar to reduce the dimension of feature vectors as these neurons do not contribute much information to the kde lines .
selfchecker then uses the filtered feature vectors to extrapolate the density functions for each layer and class and stores them lines so that they can be used for inference on new examples such as dvalid lines .
during inference on a given input instance selfchecker first obtains the outputs in each layer line from which it removes the values of the neurons filtered in line line .
it then generates the estimated density values of each class given the corresponding kde functions lines .
finally the layer inference for the input instance is the class that has the maximum density value line which indicates that the feature vectors of the input instance in this layer are close to those in training set that belong to this specific class.
for instance in fig.
the class inferences given by algorithm in the three layers are for digit and for digit respectively.
b. layer selection in section ii we noted that different layers have different attentions but some of these focus on a particular part of the image and may be misleading.
for example in fig.
the second and third layers for are different from the final prediction.
if selfchecker would consider the outputs of these layers it can detect that the model is not confident about the final output.
and if selfchecker considers just these layers and uses maximum voting then it can also provide an alternative prediction that correctly classifies this image.
therefore the design of robust layer selection in selfchecker is important to accurately raise an alarm and to provide a high quality advice.
we first explain what we mean by a model output s confidence .
our definition is based on an observation given a test instance if the features of dnn layers are different from the final prediction then the decision made by the model on the test instance will tend to be incorrect.
for example in fig.
the attentions in the second and third images of a are more similar to those of a instead of the final prediction of .
in this case the model misclassifies the as .
we evaluated this observation by using spearman rank order correlation coefficient and p values .
spearman rank order measures the relationship between the prediction correctness and the consistency of inferred layer classes and final predictions.our results show that they are correlated with p value much less than .
at most .09e on all evaluated four image datasets and three dnn models listed in table i. we formally define the confidence of a model output y given a test instance xas follows nkdeinferl x y nselectedlayerc alarm wherenkdeinferl x yis the number of selected layers whose inferred class is the same as the final prediction yand nselectedlayerc alarm is the number of selected layers for the class y. based on the maximum voting if is lower than .
we say that a dnn has low confidence in prediction yfor a test instance x. we now discuss how selfchecker selects the proper layer combinations for each class to reach a high alarm accuracy algorithm .
we use the training set to estimate the density function from which the inferred class for each layer can be obtained for a given input instance.
as mentioned in section ii different layers have different attentions but some of these may be misleading we thus use the validation dataset to select layers.
given the validation dataset dvalid algorithm layer selection for alarm input input instances in dvalid xv true labels and predictions yv yv total classes c inferred classes for all layers on dvalid kdeinferlv output selected layers for all classes selectedlayerc alarm 1forc in c do obtain the indexes idxcof instancesxv cwhose prediction yvis c generate all kinds of layer combinations combl forlayers lsincombl do forl inlsdo ys.add kdeinferlv end kdepredpos .add index of sum ys!
yv sum ys yv truemisbehavior .add index of yv !
c tp truemisbehavior kdepredpos fp truemisbehavior kdepredpos fn truemisbehavior kdepredpos f1 tp tp fn fp iff1 is max then selectedlayerc alarm ls end end 18end selfchecker splits the input instances into csubsets based on their predictions line .
selfchecker then generates all possible layer combinations with lengths in range through l from which it searches for the best combination for each class to reach the highest accuracy lines .
to calculate the alarm accuracy selfchecker first obtains the inferred class of each layer in the given layer combination lines based on the generated kde inferences across all layers on dvalid kdeinferlv by algorithm .
to conclude whether or not the model has made a wrong prediction for an input selfchecker considers the layers in the layer combination.
if a majority of the layers indicate inferred classes that are different from the model prediction the confidence is less than .
then selfchecker concludes that the model is wrong line .
in 376this case if the model prediction is indeed different from the true label of this input the alarm is correct true positive otherwise it is incorrect false positive .
selfchecker uses the f1 score to measure the alarm accuracy lines and it selects the layer combination with the highest accuracy for the corresponding class lines .
algorithm layer selection for advice input input instances in dvalid xv true labels and predictions yv yv total classes c inferred classes for all layers on dvalid kdeinferlv selected layers for all classes selectedlayerc alarm output selected layers and weights per class selectedlayerposc advice wpos selectedlayernegc advice wneg 1forcpin c do obtain the indexes idxcpof instancesxv cpwhose prediction yviscp generate ysgiven selectedlayerc alarm generate all kinds of layer combinations combl kdepredpos .add index of sum ys!
yv sum ys yv truemisbehavior .add index ofyv !
cp fp truemisbehavior kdepredpos forctin c do idxct.add index of kdepredpos where yv ct select layers selectedlayerposc advice with highest accuracy accmax fromcombl ifct cpthen wpos len idxct accmax len kdepredpos else wpos len idxct accmax len kdepredpos fp end end kdepredneg .add index of sum ys!
yv sum ys yv tn truemisbehavior kdepredneg iterate lines to obtain selectedlayernegc advice andwneg 20end after selecting the layer combinations for the alarm selfchecker must determine the layer combinations that give a good advice whenever selfchecker raises an alarm about a prediction.
algorithm details selfchecker s procedures for layer selection to achieve the best advice accuracy.
first selfchecker splits the validation set dvalid intocsubsets line and for each subset it searches for the best layer combination.
given the layers selected for alarms by algorithm selfchecker generates the kde inferred classes in these layers as in lines in algorithm .
given a test instance if the confidence of the model prediction is less than .
selfchecker concludes that the model misbehaved line .
selfchecker then searches for the best layer combination where the model predicts the input with label ctascp lines .
since not all classes have correlation selfchecker obtains weights for different combinations lines .
for example is prone to be misclassified as but has little chance to be misclassified as .
subsequently in lines selfchecker finds the layer combination that achieves the highest accuracy for the case where the selected layers by algorithm indicate a negative decision the model behaves normally .
boosting strategy selfchecker searches for both positive and negative decisions made by the selected layers in algorithm in order to boost the quality of the alarm.
in particular if the layers selected by algorithm indicate an alarm butthe advice given by selectedlayerposc advice line is the same as the model prediction then selfchecker does not raise an alarm.
similarly if the layers selected by algorithm indicate that the model prediction is correct but the advice given by selectedlayernegc advice line is different from the model prediction selfchecker will raise an alarm.
c. checking the model in deployment selfchecker checks a trained dnn in deployment.
it raises an alarm if it disagrees with the model s prediction of a given test instance and also generates an advice alternative prediction .
algorithm presents this process.
algorithm checking model in deployment input input instance and its prediction by mwithllayers x y kde functions for all layers and classes kdes selected layers for all classes selectedlayerc alarm selectedlayerposc advice selectedlayernegc advice weights for advice wpos wneg output alarm andadvice z 1generate inferred class for each layer kdeinferl using kde functions kdes 2lalarm selectedlayerc alarm 3generate ysgiven lalarm andkdeinferl 4ifsum ys!
y sum ys y then initialize prob withcdimensions forc in c do ladvice selectedlayerposc advice forl inladvice do prob sum kdeinferl c end prob prob wpos len ladvice end advice max prob index ifadvice !
ythen alarm true z advice else alarm false end 19else iterate if the alarm is not triggered initially 21end first selfchecker generates inferred classes of all layers kdeinferl using layer outputs and kde functions kdes obtained from algorithm .
then as in lines in algorithm selfchecker generates ysconsisting of inferred classes given the selected layers for y. if the output class yisnotinferred in the majority of cases in ys then selfchecker has an initial alarm that still needs to go through the boosting strategy mentioned in the last section .
lines show that selfchecker first generates the probabilities of each class given selectedlayerposc advice which are weighted by wpos.
if the class with the largest probability is still different from the model prediction y selfchecker triggers the alarm and it selects the class with the largest probability as the advice.
otherwise selfchecker does not trigger the alarm.
a similar strategy is used if the alarm is not triggered initially where the output class yis inferred in the majority of cases in ys.
iv.
e valuation in this section we present experimental evidence for the effectiveness of selfchecker.
the goal of our evaluation is to answer the following research questions.
377table i dl models and datasets used in the experiments .
dataset class train valid testdl models convnet vgg resnet layers accuracy layers accuracy layers accuracy mnist .
.
fmnist .
.
.
cifar .
.
.
cifar .
.
resnet and convnet are seldom used for mnist and cifar .
we omit their results due to space limitation but we will release them with our code.
da ve and chauffeur for self driving cars are regression models so we exclude them in this table.
a. research questions rq1.
alarm accuracy how effective is selfchecker in predicting dnn misclassifications in deployment?
to evaluate the effectiveness of selfchecker for raising alarms in deployment we compare its alarm accuracy on the test dataset with related techniques namely s elforacle d issector and confidnet .
for the comparison we chose the variant from s elforacle the v ae variational autoencoder that achieved the best performance against other s elforacle variants with confidence threshold of .
.
since d issector did not provide the threshold for distinguishing beyond inputs from within inputs we used the validation dataset to choose a threshold in the range with the highest f1 score and the best weight growth type from linear logarithmic and exponential defined in with the highest area under curve auc for each dataset and dnn classifier.
we also used the validation dataset to find the best threshold of failure prediction for confidnet to reach the highest f1 score.
rq2.
advice accuracy does the advice given by selfchecker improve the accuracy of a dnn?
in cases where selfchecker raises an alarm about a model prediction we also determine whether it can provide an advice and the accuracy of this advice.
to answer this question we compare the advice accuracy of selfchecker against the accuracy of the original dl model m. for self driving cars we use the dataset released by s elforacle .
this dataset only includes anomalous normal labels which is not enough to provide realistic advice such as turning right left.
rq3.
deployment time what is the time overhead of selfchecker in deployment for a given test instance?
we consider what different algorithms do in deployment and evaluate the computation time of their deployment time components.
selfchecker performs dnn computation kde inferences and alarm and advice analysis.
s elforacle uses the reconstructor to compute a loss and anomaly detector.
d issector generates probability vectors and performs validity analysis2.
confidnet computes an output using two dnns.
2by contrast wang et al.
only include validity analysis.
we believe that the probability vector generation must also be performed during deployment since this is the input to validity analysis.rq4.
layer selection does the choice of layers for selection by selfchecker have an impact on its alarm accuracy?
kaya et al.
characterized over thinking as a prevalent weakness of dl models which occurs when a dl model can reach correct predictions before its final layer.
over thinking can be destructive when a correct prediction within hidden layers changes to a misclassification at the output layer see section ii .
therefore it is important to select proper layers for different classes.
to evaluate the impact of layer selections on the alarm accuracy we experimented with three layer selection strategies as discussed in section iv c rq4.
rq5.
boosting strategy does the boosting strategy improve selfchecker s alarm accuracy particularly in terms of decreasing the number of false alarms?
as discussed in sections iii b and iii c we use a boosting strategy to check whether or not to raise an alarm.
b. experimental setup we evaluate selfchecker on four popular datasets mnist fmnist cifar and cifar100 using three dl models convnet vgg and resnet .
we also compare the alarm accuracy of selfchecker against s elforacle for self driving car scenarios evaluated on two publicly available dl models nvidia s da ve and chauffeur .
to reduce the possibility of fluctuation due to randomness we ran all experiments involving mnist fmnist cifar and cifar100 three times and computed the average of all metrics.
for the experiments involving the driving datasets we ran each experiment just once since we used pre trained models released by the authors of s elforacle .
we conducted all experiments on an ubuntu .
server with intel i910900x core cpu .70ghz one rtx super gpu and 64gb ram.
datasets and dl models.
table i lists the number of classes and the number of training validation and test instances in each dataset as well as the number of layers and the testing accuracy of all trained dl models.
these datasets are widely used and each is a collection of images.
convnet vgg and resnet are commonly used dl models whose sizes range from small to large with the number of layers ranging from to .
table i presents the accuracy of each model we obtained for each dataset these accuracies are similar to the 378table ii alarm accuracy .
dataset dl tpr fpr f1 so dt cn sc so dt cn sc so dt cn sc mnistconvnet .
.
.
.
.
.
.
.
.
.
.
.
vgg .
.
.
.
.
.
.
.
.
.
.
.
fmnistconvnet .
.
.
.
.
.
.
.
.
.
.
.
vgg .
.
.
.
.
.
.
.
.
.
.
.
resnet .
.
.
.
.
.
.
.
.
.
.
.
cifar 10convnet .
.
.
.
.
.
.
.
.
.
.
.
vgg .
.
.
.
.
.
.
.
.
.
.
.
resnet .
.
.
.
.
.
.
.
.
.
.
.
cifar 100vgg .
.
.
.
.
.
.
.
.
.
.
.
resnet .
.
.
.
.
.
.
.
.
.
.
.
drivingdave .
.
.
.
.
.
chauffeur .
.
.
.
.
.
so dt cn and sc stand for s elforacle dissector confidnet and selfchecker respectively.
state of the art.
as mentioned in section iii selfchecker has a training module and a deployment module.
the training and validation dataset were used in the training module and the test dataset were used on the deployment module to evaluate the performance of selfchecker.
for our experiments with nvidia s da ve and chauffeur for self driving cars we used the dataset and pretrained models released by the authors of s elforacle .
there are training images validation images and testing images for da ve and for chauffeur.
the testing images are collected by the self driving car respectively equipped with the two trained dl models.
the collection process stops when the car has collisions or out ofbound episodes.
therefore the testing images are different for the two dl models.
da ve contains five convolutional layers followed by three fully connected layers while chauffeur consists of six convolutional layers followed by one fullyconnected layer.
configurations.
as discussed in section iii we filter out neurons whose activation values show variance lower than a pre defined threshold tvarin algorithm as these neurons do not contribute much information to the kde.
for all research questions the default variance threshold is set to and the bandwidth for kde is set using scott s rule based on the number of data points and dimensions.
metrics.
given the kde inferences of the selected layers if more layers disagree than agree with the model output selfchecker triggers an alarm.
we compute the confusion metrics tp fp tn and fn as our measurement.
consequently a true positive tp is defined when selfchecker triggers an alarm to predict a misclassification where the model output is indeed wrong.
conversely a false negative fn occurs when selfchecker does not trigger an alarm on a real misclassification by the model.
a false positive fp represents a false alarm by selfchecker whereas true negative tn cases occur when selfchecker is silent on correct classifications.
our goal is to achieve a high true positive rate tpr tp tp fn a low false positiverate fpr fp tn fp and a high f1 score f1 tp tp fn fp .
c. results and analyses we now present results that answer our research questions.
rq1.
alarm accuracy table ii presents the alarm accuracies of three dl models convnet vgg and resnet in deployment on four datasets mnist fmnist cifar and cifar100 checked by s elforacle dissector confidnet and selfchecker and the alarm accuracies of two self driving car dl models checked by selfchecker and s elforacle in terms of tpr fpr and f1 score.
fig.
shows the average confusion metrics of all datasets and dl models.
selfchecker can always trigger more correct alarms tp and miss fewer true alarms fn than s elforacle and confidnet.
on traditional dnn classifiers selfchecker correctly triggers an alarm on over half of the misclassifications average tpr .
which is much higher than that of selforacle average tpr .
and confidnet average tpr .
and comparable to d issector average tpr .
.
in particular the highest tpr of selfchecker is .
this means that over of misclassifications can be detected by selfchecker.
however there are four cases on which d issector achieves higher tpr.
similar to selfchecker d issector also benefits from the internal layer features.
it builds several sub models that are retrained on top of internal layers.
therefore additional information may be learned by the training process that selfchecker lacks.
but selfchecker outperforms d issector on tpr in the majority of cases which indicates that the additional information is limited.
significantly selfchecker outperforms s elforacle which has no internal information and confidnet which only considers high level representations on all datasets and dnn classifiers on tpr.
we thus conclude that the internal layer features obtained by selfchecker are important to detecting misclassifications.
on the other hand selfchecker achieves lower fpr than all the competitors.
the low fpr indicates that selfchecker triggers few false alarms.
this is expected since 379convnet vgg convnet vgg 16resnet convnet vgg 16resnet 20vgg resnet chauffeurdave 2selforacle dissector confidnet selfchecker a true positives tp convnet vgg convnet vgg 16resnet convnet vgg 16resnet 20vgg resnet chauffeurdave 2selforacle dissector confidnet selfchecker b false positives fp convnet vgg convnet vgg 16resnet convnet vgg 16resnet 20vgg resnet chauffeurdave 2selforacle dissector confidnet selfchecker c true negatives tn selforacle dissector confidnet selfchecker convnet vgg convnet vgg 16resnet convnet vgg 16resnet 20vgg resnet chauffeurdave d false negatives fn fig.
.
confusion metrics comparing the performance of all approaches.
the boosting strategy section iii b makes selfchecker very prudent in triggering alarms.
finally selfchecker has a higher f1 score than all the competing approaches with an average values of .
against .
.
and .
for selforacle dissector and confidnet respectively.
the reason s elforacle has worse accuracy on traditional dnn classifiers is that it is tailored for time series analysis on video frame sequences that change little over short periods of time.
confidnet is trained on top of the original dl model whose weights of feature extraction are frozen using the training dataset and it uses the loss function based on true class probability.
since there are few wrong predictions in the training dataset after the original model is trained overfitting leads to limited performance of confidnet.
note that the results of confidnet shown in table ii are different from those in since our study regards wrong predictions as positive cases discussed in metrics in section iv b while regards correct predictions as positive cases.
in the self driving car scenarios we transformed the regression network that predicts steering angles into a binary classification network that classifies steering angles as either normal or anomalous.
since the true class probability is the base of confidnet and the first and second highest class probabilities are necessary for d issector both of these cannot be used in the self driving car scenarios.
given the validation dataset a gamma distribution is fitted to the errors between the predictions and the real valued angles mse and density values of each layer generated by algorithm respectively.
given an value of .
the same as used in selforacle from the gamma fitting distribution if the error of an instance in the validation dataset is larger than the value corresponding to it is labeled as an anomaly.
similarly if the density value is less than the values corresponding to it is predicted as an anomaly.
we then use selfchecker to solve the regression problem as a binary classification problem.
table ii shows that selfchecker achieves a higher tpr than selforacle on both da ve and chauffeur indicating that selfchecker can trigger more correct alarms.
even though selfchecker triggers more false alarms for da ve it also triggers more true alarms against by s elforacle and misses only true alarms.
in addition the f1 score for selfchecker is higher than for s elforacle on both models.
for rq1 we conclude that selfchecker effectively triggers alarms that predict misbehaviors of dl models in deployment with high tpr and low fpr.
rq2.
advice accuracy table iii compares the accuracies of the original model m to those ofmhaving advice provided by selfchecker.
even though selfchecker achieves high alarm accuracies it is challenging for it to provide correct advice as we regard the advice as correct only if the inferred classes of most selected internal layers are the same as the true label.
this condition is more strict than triggering an alarm that requires the inferred classes of most selected internal layers to be different from the model s prediction.
our results show that table iii advice accuracy .
accuracy strategies convnet vgg resnet mnistm .
.
m sc .
.
fmnistm .
.
.
m sc .
.
.
cifar 10m .
.
.
m sc .
.
.
cifar 100m .
.
m sc .
.
sc stands for selfchecker.
even though the trained dl models have achieved state of theart accuracies the advice can still improve model s prediction accuracy by about .
for datasets with classes but decrease the prediction accuracy by about .
for datasets with classes.
there are two reasons for this.
first finding a correct prediction from classes is a harder problem.
second the validation set per class is more limited cifar10 has samples per class but cifar only has samples per class.
we empirically find that selfchecker s advice can improve model s prediction accuracy when the number of samples per class is over .
the results also show that the advice provided by selfchecker can improve 380the prediction accuracy at most .
without retraining with additional inputs or changing the architecture.
even though this difference is small for a safety critical domain such as self driving cars which make tens of decisions per second a difference of .
in decisions translates to fewer misclassifications.
for rq2 we showed that selfchecker s advice can improve the accuracy of the original models beyond their state of theart performance with a sufficiently large validation dataset.
rq3.
deployment time we measured the average time that it takes a method to check a model s inference on a single input.
table iv lists the average times for all the datasets in table ii for each dnn classifier.
the results for da ve and chauffeur are for their corresponding self driving datasets.
s elforacle and confidnet take the least time since they use an additional dl model and their deployment checking time is the time it takes for two dl models to compute their outputs.
however these methods have alarm accuracies that are lower than dissector and selfchecker.
d issector takes longer than selfchecker average of .47ms vs .98ms on traditional dnn classifiers.
table iv deployment time .
time ms so dt cn sc convnet .
.
.
.
vgg .
.
.
.
resnet .
.
.
.
da ve .
.
chauffeur .
.
so dt cn and sc stand for s elforacle dissector confidnet and selfchecker respectively we believe that these checking times are acceptable across a variety of application domains.
as is selfchecker can be used for applications ranging from medical image based diagnosis to airport security screening.
for real time applications e.g.
autonomous driving the latency of selfchecker and selforacle needs to improve.
the checking time in the self driving car scenarios is high because frames must be analyzed before raising an alarm.
efficiency is not this paper s focus but we acknowledge its importance for cyber physical systems.
we plan to parallelize selfchecker by using a process per class density function to decrease latency by number of classes .
rq4.
layer selection as discussed in section iii b we use search based optimization to select suitable layers for improving alarm accuracy.
we present the results of checking vgg on fmnist and chauffeur on the self driving car dataset in table v we omit results for the other models and dataset since they have similar properties.
we evaluate three layer selection strategies for triggering alarms and compare them in terms of alarm accuracy.
the first strategy involves random selection of layers for each class with the number of layers selected for each classbeing the same as the number selected using our approach in order to make a fair comparison.
the second strategy uses the full set of layers.
the third strategy is our own approach described in section iii b which selects suitable layers based on the validation dataset.
to ensure a fair comparison none of the strategies use the boosting strategy.
table v impact of layer selection on alarm accuracy .
fmnist tp fp tn fn tpr fpr f1 random .
.
.
full .
.
.
sc layera317 .
.
.
chauffeur tp fp tn fn tpr fpr f1 random .
.
.
full .
.
.
sc layera116 .
.
.
asc layer stands for selfchecker s layer selection.
the results in table v indicate that selfchecker s layer selection strategy always achieves the highest tpr and f1score compared to random selection and full selection.
even though using all layers to decide whether triggering an alarm achieves lower fpr than our approach it sacrifices the number of correct alarms by and for fmnist and driving dataset respectively.
therefore selecting more layers does not lead to a better checker.
for rq4 we conclude that a careful selection of layers allows selfchecker to identify more misclassifications and raise more correct alarms.
rq5.
boosting strategy table vi presents the alarm accuracies of selfchecker both with sc and without sc b the boosting strategy described in section iii b for resnet on fmnist and cifar we omit results for the other models and dataset since they have similar properties.
as indicated in table vi adopting the boosting strategy achieves much lower fpr the lower the better than sc b with larger f1 score the higher the better .
table vi impact of boosting on alarm accuracy checking resnet .
fmnist tp fp tn fn tpr fpr f1 sc b .
.
.
sc .
.
.
cifaratp fp tn fn tpr fpr f1 sc b .
.
.
sc .
.
.
acifar stands for cifar for rq5 we showed that the boosting strategy significantly improves alarm accuracy by reducing false alarms.
v. r elated work most studies that check dl model trustworthiness focus on the process of model engineering generate adversarial test instances increase test coverage and improve robust accuracy .
unlike our work which 381checks the model in production these approaches rely heavily on manually supplied ground truth labels.
our focus is on nonadversarial inputs which require different considerations .
we plan to consider adversarial inputs in our future work.
selfchecker s performance will depend on the difference in distribution.
we conducted preliminary experiments by slightly changing the testing dataset with random noise to push the dataset embeddings of the first fully connected layer after all convolutional layers away from the training dataset.
in this setup selfchecker performs similarly to the normal indistribution dataset.
besides there are existing studies detecting out of distribution data .
for example recent work uses temperature scaling and an input preprocessing strategy to make the max class probability a more effective score for detecting out of distribution data.
such studies are complementary to selfchecker they could first check for the input being out of distribution and then selfchecker can check the prediction.
in addition our problem cannot be subsumed by confidence calibration.
as stated in confidnet confidence calibration helps to create confidence criteria but confidnet s focus is failure prediction.
comparing selfchecker against a technique with temperature scaling is inappropriate because using temperature scaling to mitigate confidence values doesn t affect the ranking of the confidence score on different classes and therefore cannot separate errors from correct predictions.
in the se community several studies consider checking a dl model s trustworthiness in deployment .
selforacle proposed by stocco et al.
estimates the confidence of self driving car models.
in their work an alarm is triggered if the confidence of the model output is lower than a predefined threshold in which case a human is then involved.
it is designed for the scenario in which inputs are temporally ordered such as video frames.
its performance is limited on other dnn types see section iv .
wang et al.
propose dissector to detect inputs that deviate from normal inputs.
it trains several sub models on top of the pre trained dl model for validating samples fed into this dl model.
but the generation of sub models is manual and time consuming and dissector does not provide an explicit design of the threshold for distinguishing inputs which depends on the model and dataset.
in the dl community researchers have developed new learning based models to measure confidence .
these models may also be untrustworthy and may suffer from e.g.
overfitting.
in nearest neighbor classifiers are built to measure the model confidence.
a clear drawback of both approaches is the lack of scalability since computing nearest neighbors in large datasets and complex models is expensive.
corbi re et al.
propose a new confidence model namely confidnet on top of the pre trained model to learn the confidence criterion based on true class probability for failure prediction which outperforms in both effectiveness and efficiency.
but its performance is limited due to overfitting since it is trained on the training dataset where there are few wrong predictions.
except for which cannot scale to large datasets and models none of the above papers providealternative advice.
in contrast selfchecker achieves both high alarm and advice accuracy with sufficient validation data per class using internal features extracted from the dnn.
vi.
l imitations and conclusion limitations.
selfchecker builds on an assumption that the density functions and selected layers determined by the training module can be used to check model consistency in deployment.
this assumption depends on whether the training and validation datasets are representative of test instances.
selfchecker is a layer based approach that requires white box access and will have more limited power on shallow dnns with few layers.
conclusion.
to be used in mission critical contexts dnn outputs must be closely monitored since they will inevitably make mistakes on certain inputs.
in this paper we hypothesized that features in internal layers of a dnn can be used to construct a self checking system to check dnn outputs.
we presented the design of such a general purpose system called selfchecker and evaluated it on four popular publicly available datasets mnist fmnist cifar cifar and three dnns convnet vgg16 resnet .
selfchecker produces accurate alarms accuracy of .
and selfchecker generated advice improves model accuracy on the class dataset by .
on average within an acceptable deployment time about .98ms .
as compared to alternative approaches selfchecker achieves the highest f1 score with .
which is .
higher than the next best approach confidnet .
in the self driving car scenarios selfchecker triggers more correct alarms than selforacle for both da ve and chauffeur models with a comparable number of false alarms.
selfchecker is open source .
acknowledgment this work was supported in part by the national research foundation singapore and national university of singapore through its national satellite of excellence in trustworthy software systems nsoe tss office under the trustworthy software systems core technologies grant tssctg award no.
nsoe tss2019 .