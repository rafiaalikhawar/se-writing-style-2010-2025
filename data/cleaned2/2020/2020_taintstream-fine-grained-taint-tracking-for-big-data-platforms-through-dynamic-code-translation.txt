taintstream fine grained taint tracking for big data platforms through dynamic code translation chengxu yang key lab of high confidence software technologies peking university moe beijing china yangchengxu pku.edu.cnyuanchun li microsoft research beijing china yuanchun.li microsoft.commengwei xu state key laboratory of networking and switching technology beijing university of posts and telecommunications beijing china mwx bupt.edu.cn zhenpeng chen key lab of high confidence software technologies peking university moe beijing china czp pku.edu.cnyunxin liu institute for ai industry research air tsinghua university beijing china liuyunxin air.tsinghua.edu.cngang huang xuanzhe liu key lab of high confidence software technologies peking university moe beijing china hg pku.edu.cn liuxuanzhe pku.edu.cn abstract big data has become valuable property for enterprises and enabled various intelligent applications.
today it is common to host data in big data platforms e.g.
spark where developers can submit scripts to process the original and intermediate data tables.
meanwhile it is highly desirable to manage the data to comply with various privacy requirements.
to enable flexible and automated privacy policy enforcement we propose taintstream a fine grained taint tracking framework for spark like big data platforms.
taintstream works by automatically injecting taint tracking logic into the data processing scripts and the injected scripts are dynamically translated to maintain a taint tag for each cell during execution.
the dynamic translation rules are carefully designed to guarantee noninterference in the original data operation.
by defining different semantics of taint tags taintstream can enable various data management applications such as access control data retention and user data erasure.
our experiments on a self crafted benchmark suite show that taintstream is able to achieve accurate cell level taint tracking with a precision of .
and less than overhead.
we also demonstrate the usefulness of taintstream through several real world use cases of privacy policy enforcement.
this work was done while chengxu yang mengwei xu and yunxin liu were working at microsoft as an intern visiting scholar and researcher respectively .
chengxu yang and yuanchun li contributed equally.
correspondence goes to yuanchun li and xuanzhe liu.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august athens greece copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
concepts security and privacy information flow control information accountability and usage control .
keywords taint tracking big data platform privacy compliance gdpr acm reference format chengxu yang yuanchun li mengwei xu zhenpeng chen yunxin liu gang huang and xuanzhe liu.
.
taintstream fine grained taint tracking for big data platforms through dynamic code translation.
in proceedings of the 29th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse august athens greece.
acm new york ny usa pages.
introduction in the past decade we have witnessed an explosion of data and rapid advances in data analysis techniques.
to handle the massive amount of data and complicated data processing requests several big data analytics engines are proposed.
today managing big data with a unified platform that stores data in distributed file systems e.g.
hdfs and offers data processing ability with streamlike apis e.g.
spark has become a standard in industry .
there are typically three roles involved in a big data platform including data providers data analysts and data managers.
data providers are typically the end users or consumers that contribute the raw data to the platform.
for example each user of an email service would produce her email messages to the service maintainer.
data providers are usually subject to privacy protection and thus the data provided by them must be managed properly to ensure privacy compliance.
data analysts are individuals or organizations that submit code to process the data.
they may have different identities from different teams companies or countries with different permissions and or have different purposes for advertising productesec fse august athens greece chengxu yang yuanchun li mengwei xu zhenpeng chen yunxin liu gang huang and xuanzhe liu development scientific research etc.
.
data managers are usually the platform maintainers whose job is to ensure that the data is used legitimately.
they should arrange the data sweep the data periodically and manage data access to avoid privacy violations.
in this work we stand in the role of data managers and try to provide a generic solution for privacy protection.
the current industrial practice of privacy protection for big data platforms is mainly based on manual code review and coarsegrained control.
to meet different privacy requirements various non trivial mechanisms and rules for data storage access and processing are designed .
both the analysts and the managers are required to learn the rules and any script submitted to the platform must go through a time consuming review process to ensure compliance.
privacy policies such as data retention and access control are usually enforced in a per dataset manner which forces the data manager and analysts to make a difficult choice between convenience and data utilization ratio if they want to fully utilize the data they must manually organize the dataset in fragments so that the expiration and sensitivity of one fragment do not affect the availability of others.
maintaining and processing the data fragments are also cumbersome for developers.
as a result a more automated flexible and fine grained solution for privacy compliance is highly desirable.
taint analysis is a promising technique for finegrained data tracking and privacy protection.
the goal of taint analysis is to detect whether the sensitive information from a source point e.g.
original personal data may be propagated to a target point e.g.
unauthorized third parties .
taint analysis can be conducted statically by analyzing the reachability in data flow graphs or dynamically by tracking the variable definitions and assignments at runtime.
porting existing taint analysis techniques to big data platforms involves several challenges data processing scripts e.g.
spark scripts are designed to describe high level operations over the whole dataset select filter groupby etc.
rather than direct information propagation mov add etc.
which makes it difficult for static taint analysis methods to extract fine grained data flow.
the data in the platforms is usually scattered and transferred across different computing and storage nodes which makes it difficult to apply traditional register or memory based dynamic taint tracking methods .
big data platforms have a higher requirement for stability and maintainability of the runtime environment so directly modifying the system to achieve tracking is usually unacceptable for its high risk and maintenance cost.
in this paper we propose taintstream a fine grained taint tracking framework for big data platforms.
taintstream solves the above challenges through dynamic code translation which converts the original data processing script at runtime to additionally maintain a taint tag for each data cell the minimum unit in a dataset .
such a method combines the advantages of static and dynamic analysis techniques the code rewriting is performed statically before running the script so that the rewritten script can seamlessly be executed in the unmodified analytic engine.
the data processing pipeline translation is completed dynamically at runtime when the necessary information for taint propagation logic injection is available.
since the injected taint propagation logic is flexible and customizable similar to classic data flow analysis frameworks taintstream is able to fulfill various privacy protection requirements in fine granularity.
specifically given a data processing script pending execution taintstream first scans the code and wraps the data processing operations with a translation function .
when the rewritten script is executed is invoked to interpret the data processing operations.
the interpretation follows a set of translation rules which are carefully designed to ensure the conservativeness of taint propagation and the non interference in the original data processing operations.
the translated data processing pipeline will operate on an extended dataset where each cell has an accompanying taint tag field.
the taint tags are maintained during data processing and the output data will have taint tags as well.
based on the taint propagation mechanism various privacy policies can be automatically enforced by formulating the policy in taintstream s format i.e.
defining the tag type and initializing merging and sanitizing functions.
for example in data retention each taint tag is a timestamp indicating the expiration date of the data.
such a method is generic across most modern big data platforms.
we implement the prototype based on spark that is widely used in practice.
to ensure the robustness of the translated script we add an exception handling mechanism which will roll back the dataset to a safe and conservative state once the translation fails.
we also include some simple rules to fuse data propagation operations to improve the efficiency of translated scripts.
we evaluate taintstream with two sets of scripts.
the first is a self built benchmark that contains scripts that cover most common data processing operations.
the second one is a set of real world scripts obtained from a production data platform in industry.
the results show that taintstream is able to accurately track taint tags in the cell level granularity with a precision of .
and a recall of .
which outperforms baselines that can track only column level taint propagation with an accuracy lower than taintstream .
we also conduct a qualitative comparison between taintstream and the current privacy enforcement methods in industry which shows taintstream s benefits in reducing manual efforts and improving data utilization.
the average time overhead and storage overhead of taintstream are .
and .
respectively which are acceptable as compared with the extra manual efforts required by existing solutions.
we summarize our contributions as follows.
we propose a fine grained taint tracking framework1to flexibly support various privacy requirements on big data platforms while overcoming the limitations of existing static and dynamic taint analysis techniques.
we introduce a benchmark for evaluating the performance of information flow tracking methods for big data platforms.
our method achieves a high precision of .
and a recall with respect to the benchmark.
we demonstrate various privacy enforcement cases where taintstream can be used to largely automate the process and improve protection granularity as compared with existing solutions in practice.
1our code is open sourced at fine grained taint tracking for big data platforms through dynamic code translation esec fse august athens greece table syntax of stream like data processing apis.
df source df.transformation transformation select col drop col orderby col filter col join df on col union df withcolumn str col groupby col .agg fagg col fagg col ... map func .reduce func ... col column name const func col col ... fagg col background and motivation in this section we first introduce some background including the general syntax of stream like data processing apis .
and privacy compliance requirements in big data platforms .
.
we then introduce a simple but full fledged example to show the necessity of fine grained taint tracking.
.
stream like data processing api most big data processing engines including spark hadoop kafka etc.
adopt a stream like api for data processing.
the core concept of stream like apis is to view the data as a sequence of elements and support sequential and parallel functional style operations over the elements.
table shows the general syntax of the stream like data processing apis.
the data source e.g.
a table loaded from storage a live data stream generated from users etc.
is loaded as a dataframe df which can be viewed as a sequence of elements with named columns.
the dataframe can be transformed into another dataframe with various operations including select filter map reduce etc.
for example some operations are used to include exclude or create columns select drop withcolumn some are to filter convert or group elements filter map groupby and some are to merge the data from different streams join union .
the parameters of these operations are typically column names constant values and functions func fagg etc.
.
such a pipeline style data processing api is similar to sql.
in fact stream like apis and sql apis are interchangeable in some cases .
however the stream like apis have become the most common interface in most data platforms due to its simplicity flexibility seamless support of customized functions parallelizability support of efficient operations like map reduce and real time processing ability the data source can be real time data streams .
our taint tracking method can be applied to traditional sql databases as well.
.
data management and taint tracking the data processed on big data platforms must be properly managed to ensure privacy compliance because it is usually collected or generated from users.there are several laws and regulations related to user privacy protection all around the world such as gpdr hipaa coppa etc.
these regulations apply to a broad range of enterprises that are processing the personal information of individuals under protection.
in addition many companies and data platform owners introduce more strict data access processing and sharing policies to further ensure data security and earn users trust.
below we highlight three typical examples of data management tasks that will be commonly used throughout the paper data retention.
a data retention policy regulates how long the personal data can be retained by the data collector.
the raw data and the data inferred from it must be deleted after a certain retention period e.g.
three months .
the retention period may vary for different types of data.
data retention requirements are common in many regulations including gdpr and hipaa.
access control.
access control is needed when multiple parties are sharing the data platform.
the access to certain data should be restricted if the requester is unauthorized.
for example in an international company with multiple teams the teams in one country may not be allowed to access the data from another country and the advertisement team may not be allowed to read children s data.
user data erasure.
gdpr also emphasizes the users right to erasure i.e.
the right to be forgotten .
when a user requests erasure the data collected from him her and generated based on it must be deleted.
taint tracking aka.
information flow tracking is a common technique to achieve the goal of privacy preserving data management.
in a taint tracking system the data records are tagged with marks indicating whether and how the record is tainted and the tags may be propagated during computation to keep track of the information flow.
a straightforward example is the access control mechanism in modern operating systems.
each file is typically tagged with its owner and or access mode readable writable executable etc.
and the files computed from it will inherit the tags.
then the system can simply check the permission tags to decide whether an operation of a user is allowed on the files.
.
motivating example and challenges we are motivated to investigate the problem of taint tracking in big data platforms based on our experience working with an industrial data management team represented as team a in the rest of this paper who operates a big data platform.
we believe the motivation is shared by many other companies and data platforms.
in such data platforms confidential user data is continuously generated from end users and or stored in a large database and the raw user datasets are processed by various developers like data scientists and business analysts for different purposes including supporting personalized services mining user behavior and gaining new business insights.
ensuring data security and privacy compliance is usually necessary for these platforms.
a motivating scenario is shown in figure .
team a has a data platform hosting the data generated from client side applications.
the data is open to different teams through spark apis to facilitate data analytic and development of new features.
we assumeesec fse august athens greece chengxu yang yuanchun li mengwei xu zhenpeng chen yunxin liu gang huang and xuanzhe liu raw user datacontinuous user data streams data processing script generated data ...a sample data processing script user defined functionthatconstructsemailreply pairsfuncbuild pairs arg ... ... dataset emails contains emails in different conversations selectconversations containingmorethanoneemailconvs emails.groupby conversationid .agg count .alias count collect list msg .alias msglist .filter count constructemailreplypairsemails emails.join convs on conversationid inner emails emails.orderby date emails emails.withcolumn replypairs build pairs msglist write to hdfs emails information flow in big data platform a record created on jan that must be deleted before april according to gdpr figure a motivating scenario for fine grained taint tracking.
the expired records in the raw datasets and generated datasets must be deleted to fulfill gdpr data retention requirements.
the privacy policy to be enforced is a simplified version of data retention which requires any personal data records created three months ago must be deleted so do other data records computed from it.
deleting the expired data is challenging because the expiration date information may be missing in the datasets generated by the analytic scripts.
an example script is shown on the right side of figure which is simplified from a real world script that takes user messaging data as input and generates training data for a smart reply machine learning model.
note that the scripts are chainable i.e.
the output of a script can be the input of another script making the data management even more complicated.
the current practice in team a to ensure privacy compliance is mainly based on manual code review and coarse grained dataset lineage analysis.
for example the datasets are partitioned and labeled with different data types and sensitivity levels that are propagated to the generated datasets.
the privacy related requirements are enforced based on these dataset labels in an all or nothing manner e.g.
the whole dataset needs to be deleted if any record in it is expired.
in cases where coarse grained automated checking is insufficient the developers must follow some coding rules to keep track of sensitive data e.g.
manually maintain a user id column for each record to handle user erasure requests when writing the scripts and submit the scripts for expert review before actually executing them which typically takes hours or days.
the current solutions are time consuming inconvenient and hard to maintain since both developers and code reviewers are required to learn and obey complicated privacy requirements.
an automated solution to fine grained information flow tracking is highly desirable.
taint analysis is a popular technique to achieve fine grained data tracking in traditional programs.
however directly applying them to our problem involves the following technical challenges missing traceable information in scripts.
static dataflow analysis can extract data dependency between variables in a program.
however a script in big data platforms describes only the high level operations e.g.
map groupby etc.
to be performed on the dataset rather than the detailed assignments and computations mov add etc.
between variables.
meanwhile the schema and actual values of the datasets are missing in the scripts making it difficultto extract fine grained data flow between data records from high level operations.
complicated low level computation and storage.
dynamic taint tracking techniques can track information propagation in traditional systems at the register memory and file level.
however applying these techniques to big data platforms is difficult because the storage and computation in big data platforms may be distributed replicated and out of order.
diverse and flexible privacy policies.
moreover the privacy requirements may vary across different countries and organizations and evolve quickly.
supporting them in system level may require frequent system updating and rebooting.
this would significantly harm the system stability and service quality since many data processing jobs are longrunning non interruptable tasks.
to sum up an ideal taint tracking mechanism for big data platforms should achieve three objectives fine granularity tracking values rather than datasets lightweight tracking at the semantic level rather than variable level and ease of use supporting flexible privacy policies .
our approach taintstream we introduce taintstream a fine grained taint tracking technique for big data platforms.
the core idea of taintstream is to translate the data processing scripts during script execution to add taint propagation logic while retaining the fidelity of original operations.
.
overview the workflow of taintstream is shown in figure .
for each data processing script submitted to the platform we first instrument the script by wrapping the original stream pipeline with a dynamic translation function .
the instrumented script is then executed in the original runtime e.g.
spark where is invoked to interpret the stream pipeline.
the instrumented script operates on an extended version of the original dataset in which each data cell is attached a or multiple taint tag s .
the taint tags for the raw data are generated upon dataset creation by the data managers according to the privacytaintstream fine grained taint tracking for big data platforms through dynamic code translation esec fse august athens greece original scriptconversationidmsgdatexxx hi i am ... 3yyy thanks ... .........conversationidmsgdatevaltagvaltagvaltagxxx false hi i am ... true10 3falseyyyfalse thanks ... true12 4false..................conversationidreplypairsdatevaltagvaltagvaltagyyyfalse true10 3false..................attach taint tag...context getcontext extract contextcode emails emails.orderby date emails code context dynamic code translationcontext getcontext code emails emails.withcolumn replypairs build pairs msglist emails code context ...script after static rewritingrewrite code original datadata with taint tagunmodified runtime e.g.
spark output data with taint tag...emails emails.orderby date emails emails.withcolumn replypairs build pairs msglist ...policy checkerprivacypoliciesallowordeny...emails emails.orderby date.val emails emails.withcolumn replypairs tagmerge build pairs msglist.val msglist.tag ...script after dynamic translation figure taintstream workflow.
policies and the extended versions of intermediate datasets are generated by our translated scripts.
if the data cell has a hierarchical structure e.g.
a int string pair the taint tags are attached to the lowest level values rather than the whole cell.
the type and semantic meaning of the taint tags are customizable based on the privacy policy to enforce.
for example in access control the taint tag of each record can be set as the owner of the data and anyone other than the owner can be denied to access the information.
in date retention each record can be tagged with its expiration date so that data cleaning can be performed periodically to delete the expired records.
for simplicity the taint tags in figure are boolean values.
based on how the data is processed in the original pipeline the function determines how the taint tags should be propagated in each operation.
the taint propagation logic is written as a normal stream operation and weaved into the original pipeline so that the translated pipeline is also executable in the original runtime.
for example in withcolumn operation a new field in this paper we use field and column mutually will be created for each element in the stream with a function that takes other fields as inputs.
when being interpreted by thewithcolumn operation will additionally aggregate the taint tags of input fields and pass the aggregated tag to the new field s tag.
the aggregation function is also customizable based on the tracking objective.
for example when aggregating multiple values with different expiration dates the earliest expiration date should be kept to ensure strict compliance.
when aggregating two records with different authorized groups the output record should be tagged with the intersection of the groups.
meanwhile the aggregation function can also be customized based on the underlying data processing operation.
for example if the operation is to compute the average message length of users messages the taint tags can be cleared since the output value is no longer sensitive.
finally the translated script will produce output data with cellwise taint tags.
privacy policy enforcement can be performed by checking the taint tags.
for example to enforce data retention we can periodically scan the datasets and sweep the records that have reached the expiration date.
to support fine grained access control we can make sure the records are only visible to the developers in authorized groups.
here we explain how taintstream addresses the challenges described in .
.
first our method combines the benefits of static analysis and dynamic analysis.
the static code rewriting phase overwrites the original data processing pipeline with a modified extended pipeline and the statically unavailable information such as data schema is available during runtime dynamic translation.
second our solution is light weight because the taint propagation logic is injected into the script through code rewriting rather than system modification which hides the details about how the data is actually stored and processed at the low level.
last but not least the injected taint propagation logic can be customized with code translation rules and thus able to support flexible privacy requirements by defining different types of taint tags aggregation functions etc.
the following sections will introduce the core components of taintstream in more detail.
.
static code rewriting intaintstream a script submitted to the data platform is first passed through a static code rewriting phase in which the script is instrumented to redirect the control flow to the translation functions.
specifically we first scan the script to locate the data processing pipelines.
this can be achieved by parsing the script and searching the parsed syntax tree for the stream apis described in .
.
for instance in pyspark python interface for spark scripts the data processing operations can be located by finding the transformation functions invoked on dataframe instances.
in the rewritten script each data processing pipeline is wrapped with a translation function .
the function takes the original lines of code i.e.
the code to be translated at runtime as input together with a context variable.
the context variable includes the current data schema types variables etc.
which are used to provide necessary information for the dynamic code translation.
we introduce a getcontext function to collect the context information at runtime.
both the getcontext function and the code translation function are packed as a library and linked to the script during running.esec fse august athens greece chengxu yang yuanchun li mengwei xu zhenpeng chen yunxin liu gang huang and xuanzhe liu table examples of dynamic code translation rules.
code represents applying translation rules to the code.
value tag is to pack a value field and a taint tag field into a structured value field the new field name is set to the same as the value field name .
vcolandtcolare used to get the value and tag of the column respectively.
tagmerge tag tag ... and tagagg tag are to merge multiple taint tags or aggregate a group of tags using a customized merging function.
represents the default tag for const insensitive values.
index original translated 1 source sourcetagged 2 df.transformation df .
transformation 3 select col select col 4 drop col drop col 5 orderby col orderby v col 6 filter col filter v col 7 join df2 on col join df2 on v col 8 union df2 union df2 9 withcolumn str col withcolumn str col 10 groupby col .agg fagg col ... groupby v col .agg tagagg t col fagg col ... .map x x1 x2 x3 x4 ... 11 map x fi xi1 xi2 ... fj xj1 xj2 ... ... map x fi vxi1 vxi2 ... tagmerge txi1 txi2 ... fj vxj1 vxj2 ... tagmerge txj1 txj2 ... ... 12 reduce a b gi ai bi ... reduce a b gi vai vbi tagmerge tai tbi ... ... 13 column name column name 14 const const 15 func col col ... func v col v col ... tagmerge t col t col ... 16 fagg col fagg v col tagagg t col .
dynamic translation rules the static code rewriting phase sets up a proxy between the script and the original code interpreter.
the actual code logic translation is completed dynamically when the modified script is being executed.
the core of the dynamic translation phase is the function that converts the original data processing pipeline to add the effect of taint propagation.
the translation is based on a set of carefully designed rules as shown in table .
the rules are applied recursively on the stream apis in a top down manner which starts from the overall pipeline e.g.
df.transformation to the individual transformation operations select groupby etc.
and the arguments to the operations col func etc.
.
below shows how the first statement in figure is translated emails.orderby date emails .
orderby date .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rule emailstagged.orderby v date .
.
.
.
.
.
.
.rule rule emailstagged.orderby v date .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rule the rules are designed with two principles non interference and conservativeness.
non interference means that the translated data processing pipeline must have the identical effect as the original script on the original data so that the developers tasks are correctly executed by taintstream .
we validate the non interference of our translation rules through formal proofs2 in which we try 2details in our repository.
usrid hashid hashid filtered usrid usridothers columns ...df1 read from inputpath df2 df1.withcolumn hashid hash usrid df3 df2.filter hashid hash alice policy column usrid can never be filtered upon.others columns ...others columns ...figure an example of the data flow graph built by taintstream .
to demonstrate that the dataframe produced by the translated operation after removing taint tags is equivalent to the dataframe produced by the original operation.
second we make sure that the taint tag of a cell is propagated to any cell that is computed from it.
similar to common data flow analysis techniques taintstream is designed to ensure conservativeness i.e.
any cell that carries sensitive information is marked as tainted while there might be cells marked as tainted that are actually not sensitive false positives .
the conservativeness is guaranteed in the translation rules.
first similar to classic lattice theoretic data flow analysis approaches the taint tag merging function is designed to be a conservative approximation to avoid false negatives produced during tag merging.
second in each translation rule all of the arguments that can possibly contribute to the output computation are passed into the tag merging function to produce the output tag.
specifically for user defined functions we conservatively assume that the function return value is dependent on all arguments no matter what the function actually does.
.
handling implicit flow our dynamic translation rules are also to pass taint tags between the inputs and outputs of operations while in some cases sensitive information may be leaked to values that are not the direct output of the operation.
a typical example is the filter operation which scans the data and filters out the records that do not satisfy certain conditions.
as a result all the remaining records in the produced data would satisfy the specified conditions.
another example is the sortby operation which sorts the records in the dataframe according to certain key s and thus the first or last few records in the sorted dataframe would convey ranking information.
such implicit information propagation operations are mostly performed on a whole column rather than on a value or a group of values.
thus tracking such information flow in cell granularity may be inappropriate.
instead we design a column level data flow graph dfg to track these operations.
specifically we maintain a graph for each dataframe to represent how each column in the dataframe is generated from raw data.
as shown in figure the graph contains multiple layers each of which represents a dataframe state.
the last layer is the current dataframe and the first layer represents the original dataframe and the middle layers are the intermediate dataframes.
each node in a layer represents a column and the edge between nodes representstaintstream fine grained taint tracking for big data platforms through dynamic code translation esec fse august athens greece the dependency between cross dataframe columns.
for example we can know that the hashid column in figure is generated from the usrid column in the original dataframe.
if an implicit flow operation is performed the corresponding column will be marked with the operation name e.g.
filter sort etc.
.
various checking rules can be defined on such column level dfgs to restrict implicit information flows.
for example in figure one can require that the column user id can never be filtered upon and the requirement can be checked by seeing whether the relevant columns are tagged with implicit operations.
these checking rules are designed to prohibit hazardous data processing requests e.g.
extracting information of a specific user as shown in figure which are independent from the taint tag based privacy enforcement mechanisms that are designed to enforce general privacy policies.
implementation we implement the prototype of taintstream on spark through the pyspark apis.
both the spark platform and its python apis are widely used in industry.
we use astroid .
.
a library for python code static analysis to parse and rewrite the scripts.
the implementation contains .9k lines of code.
other platforms can be easily supported by customizing the parser and operation translation rules accordingly.
here we further introduce our efforts on making taintstream more robust and efficient.
exception handling.
taintstream may fail in some cases for example when it encounters an unsupported operator or the translated code causes crashes details in .
.
to ensure the completion of data processing operations we handle the failures by sacrificing some precision.
the high level idea is to run the original operations on the untainted dataset and then re taint the dataset conservatively.
specifically when taintstream catches a failure during executing a translated operation it first calculates an upper bound taint tag by merging all tags in the dataset before the operation and untaints the dataset by removing all taint tags.
then we execute the original operation on the untainted dataset to get the untainted post operation dataset.
finally we recover the taint tags in the dataset using the upper bound taint tag.
with this mechanism taintstream can robustly deal with complicated and unseen scripts although the precision may be sacrificed in rare cases.
the overhead of exception handling is minimal o n wherenis the number of tags as compared with a typical data processing task.
performance optimization.
inspired by compiler optimization techniques we implement two key operation fusion rules to reduce redundant computations.
when the translated code matches the pattern of v value tag ort value tag taintstream simply translates the corresponding code snippets to value andtag.
this optimization can reduce redundant operations to pack and get value tag.
another optimization is applied when taintstream finds nested tagmerge operations in the translated code.
we implementtagmerge as a function accepting arbitrary numbers of parameters so nested calls can be simplified to a single call of tagmerge .
for example suppose a code snippet in the translated code istagmerge tag tagmerge tag tag taintstream will optimize the code snippet to tagmerge tag tag tag .
these two optimizations effectively reduce time overhead on average.more advanced optimization rules will be added in the future to further improve the performance.
evaluation our evaluation addresses the following research questions what is the accuracy of taintstream on taint tracking?
how does it compare with baselines?
.
what is the system overhead of taintstream on running time and storage?
.
how can taintstream support real world data management tasks in big data platforms?
.
.
experiment setup we first introduce how we set up our experiments including benchmarks baselines and the experimental environment.
benchmarks.
there are benchmark suites designed for measuring the performance of data processing or evaluating the accuracy of taint analysis in web android applications .
however to the best of our knowledge there is no existing benchmark suite tailored for big data taint analysis tasks.
thus we decide to create our own benchmark suite to evaluate the accuracy of taintstream .
we construct two test suites based on a set of real world data processing scripts obtained from an industry data platform the platform managed by team a as we mentioned before .
the first one is called cellbench which contains hand crafted data processing scripts that are abstracted from the real world scripts by extracting the common operations and custom functions from the original scripts and combining them into executable data processing pipelines.
the pipelines in cellbench are similar to real world scripts while the operations are simplified in order to run on fake data fake data generated with fake .
the scripts are grouped into six categories each of which is designed to evaluate a representative class of operations such as operations that group the data groupby operate on structured data structured data or use user defined functions udf .
the second one is probench which contains real world scripts that can be executed on a subset of data that we have access to the authors personal data .
the other scripts are not used because they rely on end user datasets that we do not have access to.
cellbench is used to evaluate the accuracy of taint analysis because we can automatically generate the ground truth by customizing the operations and datasets.
probench is primarily used to measure the overhead of taintstream since its scripts and datasets are more realistic.
we use a value based approach to generate the ground truth incellbench .
specifically we randomly pick some cells from the input dataset and add a unique property to the cell values.
for example the string typed cells are attached with a special string and the numerical cells are added a huge number indicating the cells are tainted.
the data processing operations in cellbench are designed to keep the taint properties when propagating information.
for example only string concatenation and number addition are allowed when generating new cells with existing cells.
thus the output dataset will automatically carry the ground truth taint information i.e.
a cell should be tainted only if its value contains the predefined taint property.
figure shows an example in theesec fse august athens greece chengxu yang yuanchun li mengwei xu zhenpeng chen yunxin liu gang huang and xuanzhe liu userid msg date xxx hi i am ... yyy thanks ... ... ... ... taint property scriptlower msg date hi iam ... thanks ... ... ... input datasetoutput datasettaint property will be propagated figure a simplified test case in cellbench .
each test case includes a script an input dataset and a ground truth output dataset.
input dataset the first msg field is tainted and its string value is inserted a substring in the beginning.
after processing suppose the msg field goes through a lower operation that converts all letters to the lower case the cells that contain the substring should be identified as tainted cells.
baselines.
to evaluate the taint tracking accuracy we choose two baselines to compare against.
the first one is pysa a generic static taint analysis framework for python code developed by facebook.
pysa is designed to detect information flow between variables in code thus it can only support dataset level tracking i.e.
analyzing a dataset is tainted or not.
the second baseline is plananalyzer which builds a column level data flow graph dfg based on the spark logical execution plan .
the latter is generated by spark at runtime to decide how to execute the data processing pipeline.
the dfg created from plananalyzer is similar to the one shown in figure .
an output column is considered tainted if there is a flow from a tainted input column to the output column.
we do not compare with other existing approaches mainly due to the unavailability of their source code or the difficulty in porting them to our benchmark .
in the case study .
the baselines are the current approaches ofteam a to three common data management tasks.
we qualitatively compare taintstream with current approaches to show its usefulness.
experimental environment.
the experiments are conducted on a cluster with four worker nodes and each node is equipped with an intel xeon e5 processor 128gb memory and 3tb storage.
the server runs a bit ubuntu .
.
the spark version is .
.
built on java .
and scala .
.
the python version is .
.
.
.
accuracy of taint tracking we use the scripts and datasets in cellbench to measure the taint tracking accuracy of taintstream and other baselines.
to simplify the measurement we consider the taint tags to be tracked are booleans indicating whether each cell is sensitive.
random cells in random columns i.e.
of all cells in the input datasets are marked as sensitive carrying true tags and the taint tracking accuracy is measured by examining whether the sensitive columns cells in the output datasets are correctly labeled with the true tag.
specifically the precision is measured by tp tp fpand the recall is tp tp fn.
since the baselines do not support cell level tracking we additionally design a column tracking experiment to compare taintstreamtable the taint analysis accuracy of taintstream and baselines on cellbench .
the sign represents that exception handling is triggered on the corresponding script.
script namecolumn level cell level pysa plananalyzer taintstream taintstream correct total precision recall basic orderby 1 .
.
orderby 2 .
.
select 1 .
.
select 2 .
.
withcolumn 1 .
.
withcolumn 2 .
.
withcolumn 3 .
.
groupby count 1 .
.
count 2 .
.
count 3 .
.
count 4 .
.
statistics 1 .
.
statistics 2 .
.
join inner join 1 .
.
inner join 2 .
.
inner join 3 .
.
left join .
.
right join .
.
outer join .
.
structured data array type 1 .
.
array type 2 .
.
struct type 1 .
.
struct type 2 .
.
struct type 3 .
.
udf udf 1 .
.
udf 2 .
.
udf 3 .
.
class udf 1 .
.
class udf 2 .
.
map reduce map reduce 1 .
.
map reduce 2 .
.
map reduce 3 .
.
map reduce 4 .
.
summary .
.
with the baselines.
a column is considered tainted if any of its cells is tainted.
we measure how many columns in the output datasets are correctly labeled for each tracking method.
the detailed results are shown in table .
column tracking results.
we observe that taintstream correctly labels all the columns in basic groupby and join categories and gets more correct results on other categories compared to the baselines.
it labels wrong tags on some columns mainly for two reasons i.e.
exception handling mechanism and missing of the dataflow in udfs.
we will explain them in detail in the more strict cell tracking experiments.
plananalyzer gains comparable results in basic groupby join andudf categories.
however its accuracy is worse in structured data category and poor in map reduce category.
for the structured data category plananalyzer treats structured columns as a whole due to the non trivial cost on achieving field sensitive in the dfg.
for the map reduce category plananalyzer cannot determine the column dependencies for the map reduce operations because it cannot infer the output schema from the spark execution plan.taintstream fine grained taint tracking for big data platforms through dynamic code translation esec fse august athens greece pysa is worse than the other two tools in all categories.
this is reasonable because as a generic taint analysis framework pysa is column agnostic and supports only dataset level tracking.
consequently it gets many false positives results.
cell tracking results.
given that baselines are worse than taintstream in the column tracking experiment and they do not support a cell level tracking their results cannot be improved in the more strict cell tracking experiment.
as a result we report only taintstream s results.
as shown in table taintstream achieves recall on all scripts this is because we carefully design the translation rules to ensure soundness refer to .
.taintstream achieves a high precision of .
on average.
the false positive results come from two reasons i.e.
the exception handling mechanism and the missing of data flow in udfs.
taintstream successfully handles exceptions such as unsupported operators missing taint tags etc.
for example in script struct type 3 a from json function that accepts a json string as the parameter is invoked in the pipeline taintstream cannot determine each field s taint tag and thus throws an exception.
for the second reason taintstream conservatively merges all the parameters tags without analyzing if there is a flow from udf s parameter to the return value.
for example in script udf 3 a udf takes a tainted column as a parameter but never uses it.
conclusion.
from the two experiments we can conclude that taintstream can not only achieve more accurate results compared to baselines at column level but also track data at cell level with a precision of .
and perfect recall.
handling data flow in udfs and reducing exception triggered are two of the optimization directions.
.
system overhead the overhead of taintstream is measured with real world scripts inprobench .
specifically we run each script in probench with or without taintstream enabled and compare their running time and storage space.
similar to the previous experiments we consider the boolean taint tags to simplify the measurement cases that use more complicated tags are also measured but reported in the case study for clearer organization .
for time assessment we measure the endto end running time including reading processing and writing and the results are shown in figure .
for storage assessment we compare the tag size with the original output size and the results are shown in table .
running time.
as shown in figure with the increase of the time spent by the original scripts the running time of taintstream increases proportionally.
taintstream is slower than the original scripts by .
on average with a maximum value of .
on the script action provider .
the input data of this script contains many complicated structured columns.
propagating taint tags in these columns requires more operations refer to because taintstream needs to first package a sub column s value and tag and then package this sub column to its parent column.
we further break down to investigate what are the main reasons for the overhead.
the overhead can be divided into three parts i.e.
static code rewriting dynamic translation and injected taint propagation logic.
among them the former two can be precisely measured.
the results show that taintstream spends .28s on static figure taintstream s overhead on running time.
table the storage overhead of taintstream .
we usexto represent the average size of a taint tag.
the overhead column reports the overhead when the average tag size is byte.
script name original output size byte tag size overhead email tokenizer x .
never replied emails x .
top name x .
action provider x .
email provider x .
data statistics x .
extract documents x .
code rewriting and .87s on dynamic translation on average.
although they are slightly influenced by the length of the script the maximum value does not exceed three seconds given a script of around lines of code.
given that the time spent on taint propagation could increase proportionally with the growth of the input data we can conclude that taintstream s overhead mainly comes from the taint propagation and the time spent on static code rewriting and dynamic translation is negligible.
according to the results taintstream introduces a .
overhead on the running time on average.
considering the time and manpower saved for various stakeholders as we will show in .
this cost is rather acceptable.
storage.
by comparing the tag size and the original output size in table we derive the following observations.
the size of the taint tags and the original output are positively correlated.
this is reasonable because taintstream tracks data at the cell level.
when we set the taint tag to a boolean value the storage overhead is .
on average.
the overhead would grow larger if more advanced taint tags are adopted.
we believe that the overhead can be further decreased after compression because taint tags may contain many duplicated values.
conclusion.
we test taintstream onprobench and compare the results with the original scripts in terms of running time and storage.
the results show that the taintstream introduces a .
overhead on the running time and .
overhead on the storage.
considering the manpower saved this overhead is rather acceptable.
.
case study finally we show how taintstream can be used to enforce three common and important privacy policies including data retention esec fse august athens greece chengxu yang yuanchun li mengwei xu zhenpeng chen yunxin liu gang huang and xuanzhe liu table comparison between taintstream and the current practice on three data management tasks.
task approachstorage overheadrunning time overheadcode reviewextra efforts for data managers and data analysts granularity current practice nodata managers organize the raw data by date and maintain a dataset linage graph that records the relationship between datasets.
once the raw data expire all related datasets will be deleted.
no requirements for data analysts.dataset leveldata retention taintstream .
.
no data managers formulate the privacy policy in taintstream format.
cell level current practice yesdata managers assign storage space with different access permissions to data analysts.
reading from or writing to an unauthorized space is prohibited through a process of code review.dataset levelaccess control taintstream .
.
no data managers formulate the privacy policy in taintstream format.
cell level current practice yesdata managers maintain a data linage graph to track the data movement.
once a user opts to delete her data all the records rows in related datasets will be deleted according to the user identifier.
data analysts are required to properly maintain and propagate the user identifier column.
this will be enforced by a process of code review.row leveluser data erasure taintstream .
.
no data managers formulate the privacy policy in taintstream format.
cell level access control and user data erasure.
we qualitatively compare the solutions enabled by taintstream with the current solutions in team a to demonstrate its advantages.
data retention.
in current practice data managers organize the raw data by date and the platform maintains a data linage graph that records the input output relationship between datasets.
once some of the raw data expires all related datasets will be deleted by a sweeper.
for taintstream a feasible policy can be defined as follows.
tag type integert.
timestamp of the expiration date tag init settto the time when the data expires.
tag merge minimum t1 t2 enforcement periodically scan the datasets and delete any recordxif the current timestamp is behind its timestamp tx.
access control.
in current practice data analysts are provided storage spaces in different confidentiality levels.
they also have different access rights to these storage spaces.
any reading writing access to an unauthorized space will be prohibited.
they are also required to store confidential data in certain spaces only unless the data is sanitized which is enforced by code review.
the control is based on dataset level.
taintstream could achieve a more finegrained control by defining the following policy.
tag type set of identifiers s. set of authorized analysts tag init sx the analysts with access to the raw data x. tag merge intersect s1 s2 enforcement a data record xis only visible to the analysts in its authorized analysts set sx.
user data erasure.
in current practice data managers build a data linage graph same as the one in the data retention graph.
data analysts are required to keep the user id of each record in the dataset which will be enforced by the code review.
once a user opts to delete her data all the records rows in related datasets will be deleted according to the user id.
with taintstream we can define the following policy so that data analysts no longer need to maintain the user id in the datasets.
tag type set of identifiers s. set of involved data providers tag init sx the provider the raw data x tag merge union s1 s2 enforcement if a useruasks to be forgotten delete any record xwhose tagsxcontainsu.since taint tags in the user data erasure task are sets of identifiers and the merge function is set union the taint tag size may grow infinitely.
to avoid this issue we set the tag to all once the set size is larger than a threshold e.g.
.
also to save computational cost the user erasure requests are processed in batches periodically.
comparison.
we qualitatively compare taintstream with current practice and summarize the results in table .
additional code review .
in current practice two out of three tasks need a code review which takes hours or days before the script is approved to be executed in the platform.
extra manual efforts and restrictions .
to support these tasks data managers take non trivial efforts including developing specialized features and organizing raw data properly.
data analysts are also restricted by many rules when writing scripts.
coarse granularity .
current practice usually works in a coarse granularity.
for example in the data retention task a dataset will be deleted once it contains an expiration record even if most of the records do not expire.
these non expiration data are falsely deleted causing a waste of valuable computational resources.
by contrast taintstream realizes these tasks more automatically.
no code review is required and no restrictions for data analysts.
data managers only need to formulate the policy in taintstream format and a fine grained result is reported.
to realize the tasks taintstream brings a less than overhead on the storage and an averaged .
overhead on the running time.
this is a rather acceptable proportion considering the convenience and saved efforts for various stakeholders as aforementioned.
related work our work is close to two classes of research.
taint analysis is a widely studied technique to track information flow in a program.
it can be done statically or dynamically.
the static approach is performed on the source code or byte code without executing the program while the dynamic taint tracking operates during programming execution.
taint analysis has been implemented for various platforms including android javascript webassembly etc.
among them taintart adopted similar design ideas as taintstream which implemented an instrumented compiler that inserts code blocks to handle taint propagation between variables.
as explained in .
these approaches can hardlytaintstream fine grained taint tracking for big data platforms through dynamic code translation esec fse august athens greece be ported to big data platforms due to the lack of traceable information and the complicated storage and computation mechanisms.
there are also several taint tracking solutions proposed for data management systems.
sch tte et al.
introduced a data usage control system which models a query script as a column level data flow graph similar to the one in figure .
dbtaint was designed for dynamic taint tracking in sql engines which extended the data types in databases and modifies some sql operations to maintain the extended data types.
flowdebug proposed to analyze fine grained data provenance by extending the scale type system and inserting custom data abstractions through sourceto source transformation.
taintstream is conceptually similar to these approaches especially flowdebug while having several differences taintstream is a generic framework that can serve various privacy compliance tasks while the prior approaches were designed for other purposes e.g.
debugging or specific scenarios e.g.
access control .
taintstream is able to perform non intrusive taint tracking with no modification to the legacy systems like spark runtime or scala type system while prior work needs to integrate extra modules to the systems.
taintstream is tailored for streamlike data processing engines that have more complicated and flexible operations than traditional sql such as map reduce customized functions etc.
privacy policy enforcement.
there are many other approaches related to automated privacy policy enforcement.
many researchers have attempted to formulate privacy policies in machine readable formats.
for example tschantz et al.
provided semantics of purpose restrictions to describe whether an action is for a purpose or not.
chowdhury et al.
presented a policy specification language which can capture the privacy requirements of hipaa.
wang et al.
introduced a formal policy language that can govern how the data is processed.
we also have a specific format for policy definition.
these formats are designed for different purposes and checking mechanisms and there isn t any policy format that is universally agreed upon today.
based on the formalized privacy policies privacy compliance can be examined by analyzing the program behavior and checking its consistency with the policies.
there have been a great deal of methods in tracking or restricting information flows in programs including language based role based and purpose based approaches.
to facilitate privacy risk assessment and privacy compliance checking researchers have also proposed new programming languages and frameworks that can weave privacy policies into the code or simplify the analysis of program behaviors .
for big data platforms existing privacy compliance checking approaches are either tailored for a specific type of privacy requirements such as access control or based on a new framework or paradigm that requires non trivial adaption efforts from developers .taintstream is able to support flexible and fine grained policy enforcement without any manual modification to the analytic script and runtime engine.
conclusion and discussion we have presented taintstream a light weight fine grained taint tracking framework for spark like big data platforms.
by customizing the injected taint propagation logic taintstream is able to fulfillvarious privacy protection requirements such as data retention access control user data erasure etc.
experiments on a self built benchmark and several real world cases show that taintstream is able to achieve accurate cell level tracking with .
precision and recall and is able to flexibly support various privacy requirements with acceptable overhead.
a potential obstacle to adopting taintstream in practice is the over conservative taint propagation i.e.
taint tags may be propagated to many irrelevant cells even the whole dataset by insensitive operations e.g.
count mean etc.
.
this may also lead to huge storage overhead if the taint tag size is large.
a possible countermeasure is to define exception operations in which the taint tags should not be propagated.
another limitation of taintstream is that the tracking mechanisms may be evaded by intentional data analysts e.g.
by obfuscating the scripts or using complicated custom functions.
although in most cases the analysts are honest we believe some mechanisms to regulate code style and or detect abnormal code would be helpful in the future.
acknowledgment we thank all anonymous reviewers for the valuable feedback.
this work was partly supported by the national key research and development program of china under the grant number 2020yfb2104100 the national natural science foundation of china under the grant number the beijing outstanding young scientist program under the grant number bjjwzyjh01201910001004 and pku baidu fund project under the grant number 2020bd007.
mengwei xu was partly supported by national industrial internet innovation and development project no.tc190a3x1 .
for any questions about the code and data please contact yuanchun li.