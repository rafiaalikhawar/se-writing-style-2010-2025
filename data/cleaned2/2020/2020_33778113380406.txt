learning from understanding and supporting devops artifacts for docker jordan henkel university of wisconsin madison usa jjhenkel cs.wisc.educhristian bird microsoft research usa christian.bird microsoft.com shuvendu k. lahiri microsoft research usa shuvendu.lahiri microsoft.comthomas reps university of wisconsin madison usa reps cs.wisc.edu abstract with the growing use of devops tools and frameworks there is an increased need for tools and techniques that support more than code.
the current state of the art in static developer assistance for tools like docker is limited to shallow syntactic validation.
we identify three core challenges in the realm of learning from understanding and supporting developers writing devops artifacts i nested languages in devops artifacts ii rule mining and iii the lack of semantic rule based analysis.
to address these challenges we introduce a toolset binnacle that enabled us to ingest github repositories.
focusing on docker we extracted approximately unique dockerfiles and also identified a gold set of dockerfiles written by docker experts.
we addressed challenge i by reducing the number of effectively uninterpretable nodes in our asts by over via a technique we call phased parsing.
to address challenge ii we introduced a novel rule mining technique capable of recovering two thirds of the rules in a benchmark we curated.
through this automated mining we were able to recover new rules that were not found during manual rule collection.
to address challenge iii we manually collected a set of rules for dockerfiles from commits to the files in the gold set.
these rules encapsulate best practices avoid docker build failures and improve image size and build latency.
we created an analyzer that used these rules and found that on average dockerfiles on github violated the rules five times more frequently than the dockerfiles in our gold set.
we also found that industrial dockerfiles fared no better than those sourced from github.
the learned rules and analyzer in binnacle can be used to aid developers in the ide when creating dockerfiles and in a post hoc fashion to identify issues in and to improve existing dockerfiles.
ccs concepts software and its engineering empirical software validation general programming languages theory of computation permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may seoul republic of korea association for computing machinery.
acm isbn .. .
.
semantics abstraction information systems data mining .
keywords docker devops mining static checking acm reference format jordan henkel christian bird shuvendu k. lahiri and thomas reps. .
learning from understanding and supporting devops artifacts for docker.
in42nd international conference on software engineering icse may seoul republic of korea.
acm new york ny usa pages.
introduction with the continued growth and rapid iteration of software an increasing amount of attention is being placed on services and infrastructure to enable developers to test deploy and scale their applications quickly.
this situation has given rise to the practice of devops a blend of the words development andoperations which seeks to build a bridge between both practices including deploying managing and supporting a software system .
bass et al.
define devops as the set of practices intended to reduce the time between committing a change to a system and the change being placed into normal production while ensuring high quality .
devops activities include building testing packaging releasing configuring and monitoring software.
to aid developers in these processes tools such as travisci circleci docker and kubernetes have become an integral part of the daily workflow of thousands of developers.
much has been written about devops see for example and and various practices of devops have been studied extensively .
devops tools exist in a heterogenous and rapidly evolving landscape.
as software systems continue to grow in scale and complexity so do devops tools.
part of this increase in complexity can be seen in the input formats of devops tools many tools like docker jenkins and terraform have custom dsls to describe their input formats.
we refer to such input files as devops artifacts.
historically devops artifacts have been somewhat neglected in terms of industrial and academic research though they have received interest in recent years .
they are not traditional code and therefore out of the reach of various efforts in automatic mining and analysis but at the same time these artifacts are complex.
our discussions with developers tasked with working on these artifacts indicate that they learn just enough to get the job done.
ieee acm 42nd international conference on software engineering icse phillips et al.
found that there is little perceived benefit in becoming an expert because developers working on builds told them if you are good no one ever knows about it .
however there is a strong interest in tools to assist the development of devops artifacts even with its relatively shallow syntactic support the vs code docker extension has over .
million unique installations .
unfortunately the availability of such a tool has not translated into the adoption of best practices.
we find that on average dockerfiles on github have nearly five times as many rule violations as those written by docker experts.
these rule violations which we describe in more detail in range from true bugs such as simply forgetting the yflag when using apt get install which causes the build to hang to violations of community established best practices such as forgetting to use apk add s no cache flag .
the goal of our work is as follows we seek to address the need for more effective semantics aware tooling in the realm of devops artifacts with the ultimate goal of reducing the gap in quality between artifacts written by experts and artifacts found in open source repositories.
we have observed that best practices for tools like docker have arisen but engineers are often unaware of these practices and therefore unable to follow them.
failing to follow these best practices can cause longer build times and larger docker images at best and eventual broken builds at worst.
to ameliorate this problem we introducebinnacle the first toolset for semantics aware rule mining from and rule enforcement in dockerfiles.
we selected dockerfiles as the initial type of artifact because it is the most prevalent devops artifact in industry some of it companies use it has become the de facto container technology in oss and it has a characteristic that we observe in many other types of devops artifacts namely fragments of shell code are embedded within its declarative structure.
because many developers are comfortable with the bash shell in an interactive context they may be unaware of the differences and assumptions of shell code in the context of devops tools.
for example many bash tools use a caching mechanism for efficiency.
relying on and not removing the cache can lead to wasted space outdated packages or data and in some cases broken builds.
consequently one must always invoke apt get update before installing packages and one should also delete the cache after installation.
default options for commands may need to be overridden often in a docker setting.
for instance users almost always want to install recommended dependencies.
however using recommended dependencies which may change over time in the external environment of apt package lists can silently break future dockerfile builds and in the near term create a likely wastage of space as well as the possibility of implicit dependencies hence the need to use the no recommends option .
thus a developer who may be considered a bash or linux expert can still run afoul of docker bash pitfalls.
to create the binnacle toolset we had to address three challenges associated with devops artifacts c1 the challenge of nested languages e.g.
arbitrary shell code is embedded in various parts of the artifact c2 the challenge of rule encoding and automated rule mining and c3 the challenge of static rule enforcement.
as repository ingestorgithub metadata store file downloader rule minermined rulessetup and collect learn rulesphased parser enforce rulesgold set rule enforcement enginedockerfile corpusphased parser rulesrule violationsdockerfile corpusgold setfile filters d d ockerfile.
query filters stars created after created before 2019fig.
an overview of the binnacle toolset.
a prerequisite to our analysis and experimentation we also collected approximately github repositories and from these repositories captured approximately dockerfiles of which are unique .
within this large corpus of dockerfiles we identified a subset written by docker experts this gold set is a collection of high quality dockerfiles that our techniques use as an oracle for docker best practices.
to address c1 we introduced a novel technique for generating structured representations of devops artifacts in the presence of nested languages which we call phased parsing.
by observing that there are a relatively small number of commonly used commandline tools and that each of these tools has easily accessible documentation via manual help pages we were able to enrich our devops asts and reduce the percentage of effectively uninterpretable leaves defined in .
in the asts by over .
for the challenge of rule encoding and rule mining c2 we took a three pronged approach we introduced tree association rules tars and created a corpus of gold rules manually extracted from changes made to dockerfiles by docker experts .
.
we built an automated rule miner based on frequent sub tree mining .
.
we performed a study of the quality of the automatically mined rules using the the gold rules as our ground truth benchmark .
.
in seminal work by sidhu et al .
they attempted to learn rules to aid developers in creating devops artifacts specifically travis cifiles.
they concluded that their vision of a tool that provides suggestions to build ci specifications based on popular sequences of phases and commands cannot be realized.
in our work we adopt their vision and show that it is indeed achievable.
there is a simple explanation for why our results differ from theirs.
in our work 1data avaliable at 39we use our phased parser to go two levels deep in a hierarchy of nested languages whereas sidhu et al.
only considered one level of nested languages.
moreover when we mine rules we mine them starting with the deepest level of language nesting.
thus our rules are mined from the results of a layer of parsing that sidhu et al.
did not perform and they are mined only from that layer.
finally to address c3 the challenge of static rule enforcement we implemented a static enforcement engine that takes as input tree association rules tars .
we find that dockerfiles on github are nearly five times worse with respect to rule violations when compared to dockerfiles written by experts and that dockerfiles collected from industry sources are no better.
this gap in quality is precisely what the binnacle toolset seeks to address.
in summary we make four core contributions a dataset of unique dockerfiles processed by our phased parser harvested from every public github repository with or more stars 2and a toolset called binnacle capable of ingesting and storing devops artifacts.
a technique for addressing the nested languages in devops artifacts that we call phased parsing.
an automatic rule miner based on frequent sub tree mining that produces rules encoded as tree association rules tars .
a static rule enforcement engine that takes as input a dockerfile and a set of tars and produces a listing of rule violations.
for the purpose of evaluation we provide experimental results against dockerfiles but in general the techniques we describe in this work are applicable to any devops artifact with nested shell e.g.
travis ci andcircle ci .
the only additional component thatbinnacle requires to operate on a new artifact type is a toplevel parser capable of identifying any instances of embedded shell.
given such a top level parser the rest of the binnacle toolset can be applied to learn rules and detect violations.
our aim is to provide help to developers in various activities.
as such binnacle s rule engine can be used to aid developers when writing modifying devops artifacts in an ide to inspect pull requests or to improve existing artifacts already checked in and in use.
data acquisition a prerequisite to analyzing and learning from devops artifacts is gathering a large sample of representative data.
there are two challenges we must address with respect to data acquisition d1 the challenge of gathering enough data to do interesting analysis and d2 the challenge of gathering high quality data from which we can mine rules.
to address the first challenge we created the binnacle toolset a dockerized distributed system capable of ingesting a large number of devops artifacts from a configurable selection of github repositories.
binnacle uses a combination of docker and apache kafka to enable dynamic scaling of resources when ingesting a large number of artifacts.
fig.
gives an overview of the three primary tools provided by the binnacle toolset a tool for data acquisition which we discuss in this section a tool for 2we selected repositories created after january 1st and before june 1st .rule learning discussed further in .
and a tool for static rule enforcement discussed further in .
.
although the architecture of binnacle is interesting in its own right we refer the reader to the binnacle github repository for more details.3for the remainder of this section we instead describe the data we collected using binnacle and our approach to challenge d2 the need for high quality data.
usingbinnacle we ingested every public repository on github with ten or more stars.
this process yielded approximately github repositories.
for each of these repositories we gathered a listing of all the files present in each repository.
this listing of files was generated by looking at the head of the default branch for each repository.
together the metadata and file listings for each repository were stored in a database.
we ran a script against this database to identify the files that were likely dockerfiles using a permissive filename based filter.
this process identified approximately likely dockerfiles.
of those likely dockerfiles only were successfully downloaded and parsed as dockerfiles.
of the remaining files approximately were unique based on their sha1 hash.
it is this set of approximately dockerfiles that we will refer to as our corpus of dockerfiles.
although both the number of repositories we ingested and the number of dockerfiles we collected were large we still had not addressed challenge d2 high quality data.
to find high quality data we looked within our dockerfile corpus and extracted every dockerfile that originally came from the docker library github organization.
this organization is run by docker and houses a set of official dockerfiles written by and maintained by docker experts.
there are approximately such files in our dockerfile corpus.
we will refer to this smaller subset of dockerfiles as the gold set.
because these files are dockerfiles created and maintained by docker s own experts they presumably represent a higher standard of quality than those produced by non experts.
this set provides us with a solution to challenge d2 the gold set can be used as an oracle for good dockerfile hygiene.
in addition to the gold set we also collected approximately dockerfiles from several industrial repositories with the hope that these files would also be a source of high quality data.
approach thebinnacle toolset shown in fig.
can be used to ingest large amounts of data from github.
this capability is of general use to anyone looking to analyze github data.
in this section we describe the three core contributions of our work phased parsing rule mining and rule enforcement.
each of these contributions is backed by a corresponding tool in the binnacle toolset i phased parsing is enabled by binnacle s phased parser shown in the learn rules and enforce rules sections of fig.
ii rule mining is enabled by binnacle s novel frequent sub tree based rule miner shown in the learn rules section of fig.
and rule enforcement is provided by binnacle s static rule enforcement engine shown in the enforce rules section of fig.
.
each of these three tools and contributions was inspired by one of the three challenges we identified in the realm of learning from and understating devops artifacts nested languages prior work that identifies rule mining as unachievable 40fromubuntu latest runapt get update apt get install qqy ... run.
scripts custom.sh a an example dockerfile docker file docker from ubuntu latestdocker run apt get update apt get install qqy ...docker run .
scripts custom.shdocker file docker from ubuntu latestdocker run bash and bash command apt get updatebash command apt get install qqy ...docker run bash command .
scripts custom.shdocker file docker from ubuntu latestdocker run bash and bash command apt get updatebash command apt get install flag yes flag quiet packages package ...2docker run bash command unknown b phase i top level parsing is performed c phase ii embedded bash is parsed d phase iii the ast is enriched with the results of parsing the top commands fig.
an example dockerfile at each of the three phases of our phased parsing technique gray nodes are effectively uninterpretable eu and static rule enforcement .
together these contributions combine to create the binnacle toolset the first structure aware automatic rule miner and enforcement engine for dockerfiles and devops artifacts in general .
.
phased parsing one challenging aspect of devops artifacts in general and dockerfiles in particular is the prevalence of nested languages.
many devops artifacts have a top level syntax that is simple and declarative json yaml and xml are popular choices .
this top level syntax albeit simple usually allows for some form of embedded scripting.
most commonly these embedded scripts are bash .
further complicating matters is the fact that bash scripts usually reference common command line tools such as apt get andgit.
some popular command line tools like python andphp may even allow for further nesting of languages.
other tools like gnu s find allow for more bash to be embedded as an argument to the command.
this complex nesting of different languages creates a challenge how do we represent devops artifacts in a structured way?
previous approaches to understanding and analyzing devops artifacts have either ignored the problem of nested languages or only addressed one level of nesting the embedded shell within the top level format .
we address the challenge of structured representations in a new way we employ phased parsing to progressively enrich the ast created by an initial top level parse.
fig.
gives an example of phased parsing note how in fig.
b we have a shallow representation given to us by a simple top level parse of the example dockerfile.
after this first phase almost all of the interesting information is wrapped up in leaf nodes that are string literals.
we call such nodes effectively uninterpretable eu because we have no way of reasoning about their contents.
these literal nodes which have further interesting structure are shown in gray.
after the second phase shown in fig.
c we have enriched the structured representation from phase i by parsing the embedded bash .
this second phase of parsing further refines the ast constructed for the example but somewhat counterintuitively this refinement also introduces even more literal nodes with undiscovered structure.
finally the third phase of parsing enriches the ast by parsing the options languages of popular command line tools see fig.
d .
by parsing within these command line languages we create a representation of devops artifacts that contains more structured information than competing approaches.
to create our phased parser we leverage the following observations there are a small number of commonly used command line tools.
supporting the top most frequently used tools allows us to cover over of command line tool invocations in our corpus.
popular command line tools have documented options.
this documentation is easily accessible via manual pages or some form of embedded help.
because of observation we can focus our attention on the most popular command line tools which makes the problem of phased parsing tractable.
instead of somehow supporting all possible embedded command line tool invocations we can instead provide support for the top n commands where n is determined by the amount of effort we are willing to expend .
to make this process uniform and simple we created a parser generator that takes as input a declarative schema for the options language of the command line tool of interest.
from this schema the parser generator outputs a parser that can be used to enrich the asts during phase iii of parsing.
the use of a parser generator was inspired by observation the information available in manual pages and embedded help although free form english text closely corresponds to the schema we provide our parser generator.
this correspondence is intentional.
to support more command line tools one merely needs to identify appropriate documentation and transliterate it into the schema format we support.
in practice creating the schema for a typical command line tool took us between and minutes.
although the parser generator is an integral and interesting piece of infrastructure we forego a detailed description of the input schema the generator requires and the process of transliterating manual pages instead we now present the rule encoding scheme thatbinnacle uses both for rule enforcement and rule mining.
.
tree association rules tars the second challenge the binnacle toolset seeks to address rule encoding is motivated by the need for both automated rule mining 41table detailed breakdown of the gold rules.
all rules are listed the rules that passed confidence support filtering described in .
are shaded.
rule namebash best practice?immediate violation consequencesfuture violation consequencesgold set supportgold set confidence pipusecachedir no space wastage increased attack surface .
npmcachecleanuseforce no space wastage increased attack surface .
mkdirusrsrcthenremove yes space wastage increased attack surface .
rmrecurisveaftermktempd yes space wastage increased attack surface .
curluseflagf no none easier to add future bugs .
tarsomethingrmthesomething yes space wastage increased attack surface .
apkaddusenocache no space wastage increased attack surface .
aptgetinstallusenorec no space wastage build failure .
curlusehttpsurl yes insecure insecure .
gpgusebatchflag no build reliability build reliability .
sha256sumechoonespace yes build failure n a .
gpgusehapools no build reliability build reliability .
configureusebuildflag no none easier to add future bugs .
wgetusehttpsurl yes insecure insecure .
aptgetinstallrmaptlists no space wastage increased attack surface .
aptgetinstallusey no build failure n a .
aptgetupdateprecedesinstall no build failure n a .
gpgverifyascrmasc yes space wastage increased attack surface .
npmcachecleanafterinstall no space wastage increased attack surface .
gemupdatesystemrmrootgem no space wastage increased attack surface .
gemupdatenodocument no space wastage increased attack surface .
yuminstallforceyes no build failure n a .
yuminstallrmvarcacheyum no space wastage increased attack surface .
precedes apt get install apt get update a intuitively this rule states that an apt get install must be preceded in the same layer of the dockerfile by an apt get update.
follows apt get install rm rm f recursive rm path abs apt lists b intuitively this rule states that a certain directory must be removed in the same layer of the dockerfile following an apt get install.
child of apt get install flag no recommends c here the user must select where in the antecedent subtree to bind a region to search for the consequent.
this binding is represented by the marker.
fig.
three example tree association rules tars .
each tar has above the bar an antecedent subtree encoded as an s expression and below the bar a consequent subtree encoded in the same way.
and static rule enforcement.
in both applications there needs to be a consistent and powerful encoding of expressive rules with straightforward syntax and clear semantics.
as part of developing this encoding we curated a set of gold rules and wrote a rule enforcement engine capable of detecting violations of these rules.
we describe this enforcement engine in greater detail in .
.
to create the set ofgold rules we returned to the data in our gold set of dockerfiles.these dockerfiles were obtained from the docker library organization on github.
we manually reviewed merged pull requests to the repositories in this organization.
from the merged pull requests if we thought that a change was applying a best practice or a fix we manually formulated as english prose a description of the change.
this process gave us approximately examples of concrete changes made by docker experts paired with descriptions of the general pattern being applied.
from these concrete examples we devised rules.
a summary of these rules is given in table .
most examples that we saw could be framed as association rules of some form.
as an example a rule may dictate that using apt get install .
.
.
requires a preceding apt get update .
rules of this form can be phrased in terms of an antecedent and consequent.
the only wrinkle in this simple approach is that both the antecedent and the consequent are sub trees of the tree representation of dockerfiles.
to deal with tree structured data we specify two pieces of information that helps restrict where the consequent can occur in the tree relative to the antecedent its location the consequent can either i precede the antecedent ii follow the antecedent or iii be a child of the antecedent in the tree.
its scope the consequent can either be i in the same piece of embedded shell as the antecedent intra directive or ii it can be allowed to exist in a separate piece of embedded shell inter directive .
although we can encode and enforce inter directive rules our miner is only capable of returning intra directive rules as explained in .
.
therefore all of the rules we show have an intra directive scope.
42apt get install flag yes flag quiet packages package python32apt get install flag yes flag no recommends packages package package mysql nginxapt get install flag yes flag no recommends packages package python3apt get install flag yes flag no recommends packages package package gcc make a four sub tree instances with root apt get install .binnacle uses a frequent sub tree miner with a support threshold of to identify frequently occurring sub trees.
we have highlighted two such possible frequent sub trees in gray and dashed outlines respectively.
apt get install flag no recommendsapt get install flag yes packages packagechild of apt get install flag no recommends child of apt get install flag yes packages package b the two frequently occuring sub trees extracted from the example input corpus in fig.
a these trees become likely consequents.
c tree association rules created automatically from the likely consequents in fig.
b .
the antecedent denotes the set of all sub trees with the indicated root node type.
fig.
a depiction of rule mining in binnacle via frequent sub tree mining.
from an antecedent a consequent and these two pieces of localizing information we can form a complete rule against which the enriched asts created by the phased parser can be checked.
we call these tree association rules tars .
three example tars are given in fig.
.
we are not the first to propose tree association rules mazuran et al .
proposed tars in the context of extracting knowledge from xml documents.
the key difference is that their tars require that the consequent be a child of the antecedent in the tree while we allow for the consequent to occur outside of the antecedent either preceding it or succeeding it.
although we allow for this more general definition of tars our miner is only capable of mining local tars that is tars in the style of mazuran et al.
however our static rule enforcement engine has no such limitation.
rule impacts.
for each of the gold rules table provides the consequences of a rule violation and a judgement as to whether a given rule is unique to dockerfiles or more aligned with general bash best practices.
in general we note that rule violations have varying consequences including space wastage container bloat and consequent increased attack surface and instances of outright build failure.
additionally two thirds of the gold rules are unique to using bash in the context of a dockerfile.
abs url https h t t p s abs url http h t t p abs path rel .
abs url .
.
.
a example named regular expressions curl url b before abstractioncurl url abs url https abs url c after abstraction fig.
an example of the abstraction process.
.
abstraction binnacle s rule miner and static rule enforcement engine both employ an abstraction process.
the abstraction process is complementary to phased parsing there may still be information within literal values even when those literals are not from some welldefined sub language.
during the abstraction process for each tree in the input corpus every literal value residing in the tree is removed fed to an abstraction subroutine and replaced by either zero one or several abstract nodes these abstract nodes are produced by the abstraction subroutine .
the abstraction subroutine simply applies a user defined list of named regular expressions to the input literal value.
for every matched regular expression the abstraction subroutine returns an abstract node whose type is set to the name of the matched expression.
for example one abstraction we use attempts to detect urls another detects if the literal value is a unix path and if so whether it is relative or absolute.
the abstraction process is depicted in fig.
.
the reason for these abstractions is to help both binnacle s rule learning and static rule enforcement phases by giving these tools the vocabulary necessary to reason about properties of interest.
.
rule mining thebinnacle toolset approaches rule mining by first focusing on a specific class of rules that are more amenable to automatic recovery rules that are local.
we define a local tree association rule tar as one in which the consequent sub tree exists within the antecedent sub tree.
this matches the same definition of tars introduced by mazuran et al.
.
based on this definition we note that local tars must be intra directive scope and must be child of location .
three examples of local tars each of which our rule miner is able to discover automatically are given in figs.
c and c .
in general the task of finding arbitrary tars from a corpus of hundreds of thousands of trees is computationally infeasible.
by focusing on local tars the task of automatic mining becomes tractable.
to identify local tars binnacle collects for each node type of interest the set of all sub trees with roots of the given type e.g.
43docker file docker from ubuntu latestdocker run bash and bash command apt get updatebash command apt get install flag yes flag quiet packages package ...2bash command makedocker run bash command unknowndocker file docker from ubuntu latestdocker run bash and bash command apt get updatebash command apt get install flag yes flag quiet packages package ...2bash command makedocker run bash command unknown1 5docker file docker from ubuntu latestdocker run bash and bash command apt get updatebash command apt get install flag no recommends flag yes flag quiet packages package ...2bash command makedocker run bash command unknown a stage i the enforcement engine attempts to match the tar s antecedent shown in the outlined box above .
a match is found when the subtree in a tar s antecedent can be aligned with any subtree in the input tree.
all three rules given in fig.
have antecedents that match the above tree.
b stage ii if the enforcement engine matches the tar s antecedent then depending on the location andscope of the tar the enforcement engine will bind one of the five shaded regions above.
for the rule given in fig.
a intradirective preceding region is matched.
for the rule in fig.
b intra directive following region is matched.
the darker shaded regions are the inter directive variants of regions .
c stage iii the enforcement engine searches for the consequent in the bound region.
for the rule in fig.
a the blue shaded region is bound and the consequent shown with a dashed black outline is matched therefore the rule in fig.
a has been validated.
conversely for the rule in fig.
c the green region is bound and there are no matches for the consequent of this rule represented by the dashed red box therefore the rule in fig.
c has been violated.
fig.
binnacle s rule engine applied to an example dockerfile all sub trees with apt get as the root .
on this set of sub trees binnacle employs frequent sub tree mining to recover a set of likely consequents.
specifically binnacle uses the cmtreeminer algorithm to identify frequent maximal induced ordered subtrees.
induced indicates that all child of relationships in the subtree exist in the original tree as opposed to the more permissive descendent of relationship which defines an embedded sub tree .
ordered signifies that order of the child nodes in the sub tree matters as opposed to unordered sub trees .
a frequent sub tree is maximal for a given support threshold if there is no super tree of the sub tree with occurrence frequency above the support threshold though there may be sub trees of the given sub tree that have a higher occurrence frequency .
for more details on frequent sub trees see chiet al.
.
binnacle returns rules in which the antecedent is the root node of a sub tree where the type of the root node matches the input node type and the consequent is a sub tree identified by the frequent sub tree miner.
an example of the rule mining process is given in fig.
.
in the first stage of rule mining all sub trees with the same root node type apt get install are grouped together and collected.
for each group of sub trees with the same root node type binnacle employs frequent sub tree mining to find likely consequents.
in our example two frequently occurring sub trees in gray and dashed outlines respectively are given in fig.
b .
finally binnacle creates local tars by using the root node as the antecedent and each of the frequent sub trees as a consequent as shown in fig.
c .
one tar is created for each identified frequent sub tree.
.
static rule enforcement currently the state of the art in static dockerfile support for developers is the vscode docker extension and the hadolintdockerfile linting tool .
the vscode extension provides highlighting and basic linting whereas hadolint employs a shell parser shellcheck the same shell parser we use to parse embedded bash similar to our tool s second phase of parsing.
the capabilities of these tools represent steps in the right direction but ultimately they do not offer enough in the way of deep semantic support.
hadolint does not support parsing of the arguments of individual commands as binnacle does in its third phase of parsing.
instead hadolint resorts to fuzzy string matching and regular expressions to detect simple rule violations.
binnacle s static rule enforcement engine takes as input a dockerfile and a set of tars.
binnacle s rule engine runs for each rule three stages of processing on the input corpus stage i the dockerfile is parsed into a tree representation and the enforcement engine attempts to match the tar s antecedent by aligning it with a sub tree in the input tree .
if no matches are found the engine continues processing with the next tar.
if a match is found then the enforcement engine continues to stage ii.
this process is depicted in fig.
a .
stage ii depending on the scope andlocation of the given tar the enforcement engine binds a region of the input tree.
this region is where in stage iii the enforcement engine will look for a sub tree with which the consequent can be aligned.
fig.
b depicts this process and highlights the various possible binding regions in the example input tree.
stage iii given a tar with a matched antecedent and a bound region of the input tree the enforcement engine attempts to align the consequent of the tar with a sub tree within the bound region.
if the engine is able to find such an alignment then the rule has been satisfied.
if not the rule has been violated.
fig.
c depicts this process and both possible .
.
.
.
.
ofeuleavesdensity0.
.
.
.
.
ofeuleavesdensity0.
.
.
.
.
ofeuleaves that remain unresolveddensity a density histogram of m1 the fraction of leaves that are euafter the first phase of parsing .
on average .
of leaves median .
are euat this phase.
b density histogram of m2 the fraction of leaves that are euafter the second phase of parsing .
on average .
of leaves median .
are euat this phase.
c density histogram of m3 the fraction of leaves that were euafter the second phase of parsing and unresolved in the third phase .
on average just .
of leaves median .
remained eu.
fig.
density histograms showing the distributions of our three metrics m1 m2 and m3 .
the green shaded box in each plot highlights the interquartile range for each distribution the middle .
outcomes for the rule in fig.
a the matched antecedent is shown with a thick black outline the bound region is shown in blue and the matched consequent is shown with a dashed black outline.
in contrast for the rule in fig.
c the matched antecedent is the same as above the bound region is shown in green however the tree is missing the consequent represented by the dashed red sub tree.
the implementation of binnacle s enforcement engine utilizes a simple declarative encoding for the tars.
to reduce the bias in the manually extracted gold rules introduced in .
we used binnacle s static rule enforcement engine and the gold set of dockerfiles introduced in to gather statistics that we used to filter the gold rules.
for each of the rules encoded as tree association rules we made the following measurements i the support of the rule which is the number of times the rule s antecedent is matched ii the confidence of the rule which is the percentage of occurrences of the rule s consequent that match successfully given that the rule s antecedent matched successfully and iii the violation rate of the rule which is the percentage of occurrences of the antecedent where the consequent is not matched.
note that our definitions of support andconfidence are the same as that used in traditional association rule mining .
we validated our gold rules by keeping only those rules with support greater than or equal to andconfidence greater than or equal to on the gold set.
these support and confidence measurements are given in table .
by this filtering we increase the selectivity of our gold rules set and reduce the bias of our manual selection process.
of the original rules in our gold rules pass the minimum support threshold and of those rules pass the minimum confidence threshold.
henceforth we use the term gold rules to refer to the rules that passed quantitative filtering.
these rules are highlighted in gray in table .
together binnacle s phased parser rule miner and static ruleenforcement engine enable both rule learning and the enforcement of learned rules.
fig.
depicts how these tools interact to provide the aforementioned features.
taken together the binnacle toolset fills the need for structure aware analysis of devops artifacts andprovides a foundation for continued research into improving the state of the art in learning from understanding and analyzing devops artifacts.
evaluation in this section for each of the three core components of the binnacle toolset s learning and enforcement tools we measure and analyze quantitative results relating to the efficacy of the techniques behind these tools.
all experiments were performed on a core workstation with 32gb of ram running windows and a recent version of docker.
.
results phased parsing to understand the impacts of phased parsing we need a metric for quantifying the amount of useful information present in our devops artifacts represented as trees after each stage of parsing.
the metric we use is the fraction of leaves in our trees that are effectively uninterpretable eu .
we define a leaf as effectively uninterpretable eu if it is after the current stage of parsing a string literal that could be further refined by parsing the string with respect to the grammar of an additional embedded language.
we will also count nodes explicitly marked as unknown by our parser as being eu.
for example after the first phase of parsing the top level parse a dockerfile will have nodes in its parse tree that represent embedded bash these nodes are euat this stage because they have further structure that can be discovered given a bash parser however after the first stage of parsing these leaves are simply treated as literal values and therefore marked eu.
we took three measurements over the corpus of unique dockerfiles introduced in m1 the distribution of the fraction of leaves that are euafter the first phase of parsing m2 the distribution of the fraction of leaves that are euafter the second phase of parsing and m3 the distribution of the fraction of leaves that are euafter the second phase of parsing and unresolved during the third phase of parsing.
4for m3 we make a relative measurement the reason for using a different metric is to accommodate the large number of new leaf nodes that the third phase of parsing 45child of apk add flag no cache child of sc curl url abs url protocol https child of cp cp path cp path child of sed flag in place a a gold rule b a semantic rule c a syntactic rule d an ungeneralizable rule fig.
four examples of actual rules recovered by binnacle s automated miner.
through abstraction interesting semantic rules such as using https urls with curl are captured.
density histograms that depict the three distributions are given in fig.
.
as shown in fig.
after the first phase of parsing the trees in our corpus have on average .
euleaves.
this number quantifies the difficulty of reasoning over devops artifacts without more sophisticated parsing.
furthermore the nodes in the tree most likely to play a role in rules happen to be the eunodes at this stage.
this aspect is something that our quantitative metric does not take into account and hence over estimates the utility of the representation available after phase i and phase ii.
counterintuitively the second phase of parsing makes the situation worse on average .
of leaves in second phase trees are eu.
competing tools like hadolint work over devops artifacts with a similar representation.
in practice competing tools must either stay at what we consider a phase i representation just a top level parse or utilize something similar to our phase ii representations.
such tools are faced with the high fraction of euleaves present in a phase ii ast.
tools using phase ii representations like hadolint are forced to employ regular expressions or other fuzzy matching techniques as part of their analysis.
finally we use our parser generator and the generated parsers for the top commands to perform a third phase of parsing.
the plot in fig.
c shows the m3 distribution obtained after performing the third parsing phase on our corpus of dockerfiles.
at this stage almost all of the eunodes are gone on average only .
of leaves that were euat phase ii remain euin phase iii.
in fact over of trees in phase ii had all euleaves resolved after the third phase of parsing.
these results provide concrete evidence of the efficacy of our phased parsing technique and in contrast to what is possible with existing tools the phase iii structured representations are easily amenable to static analysis and rule mining.
.
results rule mining we applied binnacle s rule miner to the gold set of dockerfiles defined in .
we chose the gold set as our corpus for rule learning because it presumably contains dockerfiles of high quality.
as described in .
binnacle s rule miner takes as input a corpus of trees and a set of node types.
we chose to mine for patterns using any new node type introduced by the third phase of parsing.
we selected these node types because i they represent new information gained in the third phase of our phased parsing process and ii all rules in our manually collected gold rules set used nodes created in this phase.
rules involving these new nodes which come from the most deeply nested languages in our artifacts were invisible to prior work.
introduces.
without this adjustment one could argue that our measurements are biased because the absolute fraction of euleaves would be low due to the sheer number of new leaves introduced by the third parsing phase.
to avoid this bias we measure the fraction of previously eu leaves that remain unresolved as opposed to the absolute fraction of euleaves that remain after the third phase of parsing which is quite small due to the large number of new leaves introduced in the third phase .to evaluate binnacle s rule miner we used the gold rules introduced in .
.
from the original gold rules we removed rules that did not pass a set of quantitative filters this filtering is described more in .
.
of the remaining gold rules there are rules that are local as defined in .
.
in principal these rules are all extractable by our rule miner.
furthermore it is conceivable that there exist interesting and useful rules outside of the gold rules that did not appear in the dockerfile changes that we examined in our manual extraction process.
to assess binnacle s rule miner we asked the following three questions q1 how many rules are we able to extract from the data automatically?
q2 how many of these rules match one of the local gold rules ?
equivalently what is our recall on the set of local gold rules ?
q3 how many new rules do we find and if we find new rules outside of our local gold rules what can we say about them e.g.
are the new rules useful correct general etc.
?
for q1 we found that binnacle s automated rule miner returns a total of rules.
binnacle s automated rule miner is selective enough to produce a small number of output rules this selectivity has the benefit of allowing for easy manual review.
to provide a point of comparison we also ran a traditional association rule miner over sequences of tokens in our phase iii asts we generated these sequences via a pre order traversal .
the association rule miner returned thousands of possible association rules.
the number of rules could be reduced by setting very high confidence thresholds but in so interesting rules could be missed.
for q2 we found that two thirds of local gold rules were recovered by binnacle s rule miner.
because binnancle s rule miner is based on frequent sub tree mining it is only capable of returning rules that when checked against the corpus they were mined from have a minimum confidence equal to the minimum support supplied to the frequent sub tree miner.
in addition to measuring recall on the local gold rules we also examined the rules encoded in hadolint to identify all of its rules that were local.
because hadolint has a weaker representation of dockerfiles we are not able to translate many of its rules into local tars.
however there were three rules that fit the definition of local tars.
furthermore binnacle s automated miner was able to recover each of those three rules one rule requires the use of apt get install s yflag another requires the use of apt get install s no install recommends flag and the third requires the use of apk add s no cache flag .
to classify the rules returned by our automated miner we assigned one of the following four classifications to each of the rules returned syntactic these are rules that enforce simple properties for example a rule encoding the fact that the cpcommand takes two paths as arguments see fig.
c .
semantic these are rules that encode more than just syntax.
for example a rule that says the url passed to the curl utility must include the https prefix see fig.
b .
gold these are rules that match or supersede one of the rules in our collection of gold rules see fig.
a .
ungeneralizable these are rules that are correct on the corpus from which they were mined but upon further inspection seem unlikely to generalize.
for example a rule that asserts that the sedutility is always used with the in place flag is ungeneralizable see fig.
d .
to answer q3 we assigned one of the above classifications to each of the automatically mined rules.
we found that out of rules were syntactic were semantic were gold and were ungeneralizable.
fig.
depicts a rule that was mined automatically in each of the four classes.
surprisingly binnacle s automated miner discovered new rules syntactic semantic that we missed in our manual extraction.
of the newly discovered rules one could argue that only the semantic rules are interesting and therefore one might expect a human to implicitly filter out syntactic rules during manual mining .
we would argue that even these syntactic rules are of value.
the lack of basic validation in tools like vs code s docker extension creates a use case for these kind of basic structural constraints.
furthermore the novel semantic rules include things such as i the use of the lflag with curl following redirects which introduces resilience to resources that may have moved ii the use of the pflag with mkdir which creates nested directories when required and iii the common practice of preferring soft links over hard links by using ln s s flag.
with q3 we have demonstrated the feasibility of automated mining for dockerfiles we hope that these efforts inspire further work into mining from dockerfiles and devops artifacts in general.
.
results rule enforcement using the gold rules we measured the average violation rate of the gold rules with respect to the gold dockerfiles .
the average violation rate is the arithmetic mean of the violation rates of each of the gold rules with respect to the gold dockerfiles.
this measurement serves as a kind of baseline it gives us a sense of how good dockerfiles written by experts are with respect to thegold rules.
the average violation rate we measured was .
which unsurprisingly is quite low.
we also measured the average violation rate of the gold rules with respect to our overall corpus.
we hypothesized that dockerfiles in the wild would fare worse with respect to violations than those written by experts.
this hypothesis was supported by our findings the average violation rate was .
.
we had expected an increase in the violation rate but were surprised by the magnitude of the increase.
these results highlight the dire state of static devops support dockerfiles authored by non experts are nearly five times worse when compared to those authored by experts.
bridging this gap is one of the overarching goals of the binnacle ecosystem.
we also obtained a set of approximately dockerfiles from the source code repositories of an industrial source and assessedtheir quality by checking them against our gold rules.
to our surprise the violation rate was no lower for these industrial dockerfiles.
this result provides evidence that the quality of dockerfiles suffers in industry as well and that there is a need for tools such asbinnacle to aid industrial developers.
related work our paper is most closely related to the work of sidhu et al.
who explored reuse in ci specifications in the specific context of travis ci and concluded that there was not enough reuse to develop a tool that provides suggestions to build ci specifications based on popular sequences of phases and commands.
we differ in the devops artifact targeted dockerfiles versus travis ci files representation of the configuration file and the rule mining approach.
in a related piece of work gallaba and mcintosh analyzed the use of travis ci across nearly repositories in github and identified best practices based on documentation linting tools blog posts and stack overflow questions.
they used their list of best practices to deduce four anti patterns and developed hansel a tool to identify anti patterns in travis ci config files and gretel a tool to automatically correct them.
similar to our second phase of parsing they used a bash parser bashlex to gain a partial understanding of the shell code in config files.
zhang et al.
examined the impact of changes to dockerfiles on build time and quality issues via the docker linting tool hadolint .
they found that fewer and larger docker layers results in lower latency and fewer quality issues in general and that the architecture and trajectory of docker files how the size of the file changes over time impact both latency and quality.
many of the rules in our gold set and those learned by binnacle would result in lower latency and smaller images if the rules were followed.
xuet al.
described a specific kind of problem in docker image creation that they call the temporary file smell.
temporary files are often created but not deleted in docker images.
they present two approaches for identifying such temporary files.
in this paper we also observed that removing temporary files is a best practice employed by dockerfile experts and both our manual gold set and our learned rules contained rules that address this.
zhang et al.
explored the different methods of continuous deployment cd that use containerized deployment.
while they found that developers see many benefits when using cd adopting cd also poses many challenges.
one common way of addressing them is through containerization typically using docker.
their findings also reinforce the need for developer assistance for devops they concluded that bad experiences or frustration with a specific ci tool can turn developers away from ci as a practice.
our work falls under broader umbrella of infrastructure ascode .
this area has received increasing attention recently .
as examples sharma et al.
examined quality issues so called smells in software configuration files and jiang et al.
examined the coupling between infrastructure as code files and traditional sourcecode files.
there have been a number of studies that mine docker artifacts as we do.
xu and marinov mined container image repositories such as dockerhub and discussed the challenges and opportunities 47that arise from such mining.
zerouali et al.
studied vulnerabilities in docker images based on the versions of packages installed in them.
guidotti et al.
attempted to use docker image metadata to determine if certain combinations of image attributes led to increased popularity in terms of stars and pulls.
cito et al.
conducted an empirical study of the docker ecosystem on github by mining over docker files and examining how they evolve the types of quality issues that arise in them and problems when building them.
a number of tools related to dockerfiles have been developed in recent years as well.
brogi et al.
found that searching for docker images is currently a difficult problem and limited to simple keyword matching.
they developed dockerfinder a tool that allows multi attribute search including attributes such as image size software distribution or popularity.
yinet al.
posited that tag support in docker repositories would improve reusability of docker images by mitigating the discovery problem.
they addressed this issue by building star a tool that uses latent dirichlet allocation to automatically recommend tags.
docker files may need to be updated when the requirements of the build environment or execution environment changes.
hassan et al.
developed rudsea a tool that can recommend updates to dockerfiles based on analyzing changes in assumptions about the software environment and identifying their impacts.
to tackle the challenge of creating the right execution environment for python code snippets e.g.
from gists or stackoverflow horton and parnin developed dockerizeme a tool which infers python package dependencies and automatically generates a dockerfile that will build an execution environment for pieces of python code.
threats to validity we created tools and techniques that are general in their ability to operate over devops artifacts with embedded shell but we focused our evaluation on dockerfiles.
it is possible that our findings do not translate directly to other classes of devops artifacts.
we ingested a large amount of data for analysis and as part of that process we used very permissive filtering.
it is possible that our corpus of dockerfiles contains files that are not dockerfiles duplicates or other forms of noise.
it is also possible that there are bugs in the infrastructure used to collect repositories and dockerfiles.
to mitigate these risks we kept a log of the data we collected and verified some coarse statistics through other sources e.g.
we used github s api to download data and then cross checked our on disk data against github s public infrastructure for web based search .
through these cross checks we were able to verify that for the over repositories we ingested greater than completed the ingestion process successfully.
furthermore of the approximately likely dockerfiles we identified made it through downloading parsing and validation.
of this set of files approximately were unique based on their sha1 hash.
of the files rejected during processing downloading parsing and validation most were either malformed dockerfiles or files withnames matching our .
dockerfile.
filter that were not actual dockerfiles e.g.
docker compose.yml files .
we identified a gold set of dockerfiles and used these files as the ideal standard for the dockerfiles in our larger corpus.
it is possible that developers do not want to achieve the same level of quality as the files in our gold set.
it is also possible that the gold set is too small and too specific to be of real value.
it is conceivable but unlikely that the gold set is not representative of good practice.
even if that were the case our finding still holds that there is a significant difference between certain characteristics of dockerfiles written by presumed docker experts and those written by gardenvariety github users.
we acknowledge that the average violation rate of our gold rules is only a proxy for quality but given the data and tools currently available it is a reasonable and crucially measurable choice of metric.
for rule mining we created manually a set of gold rules against which we benchmarked our automated mining.
because the results of automated mining did not agree with three of the manually extracted rules there is evidence that the manual process did have some bias.
we sought to mitigate this issue through the use of quantitative filtering after filtering we retained only of our original gold rules.
conclusion thus far we have identified the ecosystem of devops tools and artifacts as an ecosystem in need of greater support both academically and industrially.
we found that on average dockerfiles on github are nearly five times worse with respect to violations of our gold rules compared to dockerfiles written by experts.
furthermore we found that industrial dockerfiles are no better.
through automated rule mining and static rule enforcement we created tools to help bridge this gap in quality.
without increased developer assistance the vast disparity between the quality of devops artifacts authored by experts and non experts is likely to continue to grow.
there are a number of pieces of follow on work that we hope to pursue.
we envision the binnacle tool the data we have collected and the analysis we have done on dockerfiles as a foundation on which new tools and new analysis can be carried out.
to that end we plan to continue to evolve the binnacle ecosystem by expanding to more devops artifacts travis circleci etc.
.
additionally the encoding of rules we utilize has the advantage of implicitly encoding a repair or at least part of a repair localizing the insertion point for the implicit repair may be a challenge .
furthermore the kinds of rules that we mine are limited to local rules.
we believe that more rules may be within the reach of automated mining.
finally we hope to integrate binnacle s mined rules and analysis engine into language servers and ide plugins to provide an avenue for collecting real feedback that can be used to improve the assistance we provide to devops developers.