object detection for graphical user interface old fashioned or deep learning or a combination?
jieshan chen jieshan.chen anu.edu.au australian national university australiamulong xie u6462764 anu.edu.au australian national university australiazhenchang xing zhenchang.xing anu.edu.au australian national university australia chunyang chen chunyang.chen monash.edu monash university australiaxiwei xu xiwei.xu data61.csiro.au data61 csiroliming zhu liming.zhu data61.csiro.au data61 csiro australia guoqiang li li.g sjtu.edu.cn shanghai jiao tong university china abstract detecting graphical user interface gui elements in gui images is a domain specific object detection task.
it supports many software engineering tasks such as gui animation and testing gui search and code generation.
existing studies for gui element detection directly borrow the mature methods from computer vision cv domain including old fashioned ones that rely on traditional image processing features e.g.
canny edge contours and deep learning models that learn to detect from large scale gui data.
unfortunately these cv methods are not originally designed with the awareness of the unique characteristics of guis and gui elements and the high localization accuracy of the gui element detection task.
we conduct the first large scale empirical study of seven representative gui element detection methods on over 50k gui images to understand the capabilities limitations and effective designs of these methods.
this study not only sheds the light on the technical challenges to be addressed but also informs the design of new gui element detection methods.
we accordingly design a new gui specific oldfashioned method for non text gui element detection which adopts a novel top down coarse to fine strategy and incorporate it with the mature deep learning model for gui text detection.
our evaluation on gui images shows that our method significantly advances the start of the art performance in gui element detection.
also with data61 csiro.
corresponding author.
also with university of new south wales.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november virtual event usa association for computing machinery.
acm isbn .
.
.
.
concepts software and its engineering software development techniques human centered computing graphical user interfaces .
keywords android object detection user interface deep learning computer vision acm reference format jieshan chen mulong xie zhenchang xing chunyang chen xiwei xu liming zhu and guoqiang li.
.
object detection for graphical user interface old fashioned or deep learning or a combination?.
in proceedings of the 28th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november virtual event usa.
acm new york ny usa pages.
introduction gui allows users to interact with software applications through graphical elements such as widgets images and text.
recognizing gui elements in a gui is the foundation of many software engineering tasks such as gui automation and testing supporting advanced gui interactions gui search and code generation .
recognizing gui elements can be achieved by instrumentation based or pixel based methods.
instrumentationbased methods are intrusive and requires the support of accessibility apis or runtime infrastructures that expose information about gui elements within a gui.
in contrast pixel based methods directly analyze the image of a gui and thus are non intrusive and generic.
due to the cross platform characteristics of pixel based methods they can be widely used for novel applications such as robotic testing of touch screen applications linting of gui visual effects in both android and ios.
pixel based recognition of gui elements in a gui image can be regarded as a domain specific object detection task.
object detection is a computer vision technology that detects instances of semantic objects of a certain class such as human building or car in digitalarxiv .05132v2 sep 2020esec fse november virtual event usa chen j. xie m. xing z. chen c. xu x. zhu l. and li g. images and videos.
it involves two sub tasks region detection or proposal locate the bounding box bbox for short i.e.
the smallest rectangle region that contains an object and region classification determine the class of the object in the bounding box.
existing object detection techniques adopt a bottom up strategy starts with primitive shapes and regions e.g.
edges or contours and aggregate them progressively into objects.
old fashioned techniques relies on image features and aggregation heuristics generated by expert knowledge while deep learning techniques use neural networks to learn to extract features and their aggregation rules from large image data.
gui elements can be broadly divided into text elements and non text elements see figure for the examples of android gui elements .
both old fashioned techniques and deep learning models have been applied for gui element detection .
as detailed in section .
considering the image characteristics of guis and gui elements the high accuracy requirement of gui element region detection and the design rationale of existing object detection methods we raise a set of research questions regarding the effectiveness features and models originally designed for generic object detection on gui elements the region detection accuracy of statistical machine learning models the impact of model architectures hyperparameter settings and training data and the appropriate ways of detecting text and non text elements.
these research questions have not been systematically studied.
first existing studies evaluate the accuracy of gui element detection by only a very small number dozens to hundreds of guis.
the only large scale evaluation is gui component design gallery but it tests only the default anchor box setting i.e.
a predefined set of bboxes of faster rcnn a two stage model .
second none of existing studies including have investigated the impact of training data size and anchor box setting on the performance of deep learning object detection models.
furthermore the latest development of anchor free object detection has never been attempted.
third no studies have compared the performance of different methods for example old fashioned versus deep learning or different styles of deep learning e.g.
two stage versus one stage anchor box or free .
fourth gui text is simply treated by optical character recognition ocr techniques despite the significant difference between gui text and document text that ocr is designed for.
to answer the raised research questions we conduct the first large scale comprehensive empirical study of gui element detection methods involving a dataset of gui screenshots extracted from android mobile applications see section .
.
and two representative old fashioned methods remaui and xianyu and three deep learning models faster rcnn yolov3 and centernet that cover all major method styles see section .
.
.
old fashioned detection methods perform poorly remaui f1 .
and xianyu f1 .
at iou .
for non text gui element detection.
iou is the intersection area over union area of the detected bounding box and the ground truth box.
deep learning methods perform much better than old fashioned methods and the two stage anchor box based faster rcnn performs the best f1 .
at iou .
and demands less training data.
however even faster rcnn cannot achieve a good balance of the coverage button spinner chronometer checkbox imagebuttonratingbar seekbar edittextradiobuttonimageview progressbar switch togglebutton textviewvideoview figure characteristics of gui elements large in class variance and high cross class similarity of the gui elements and the accuracy of the detected bounding boxes.
it is surprising that anchor box based models are robust to the anchor box settings and merging the detection results by different anchor box settings can improve the final performance.
our study shows that detecting text and non text gui elements by a single model performs much worse than by a dedicated text and nontext model respectively.
gui text should be treated as scene text rather than document text and the state of the art deep learning scene text model east pretrained without fine tuning can accurately detect gui text.
inspired by these findings we design a novel approach for gui element detection.
for non text gui element detection we adopt the simple two stage architecture perform region detection and region classification in a pipeline.
for non text region detection we prefer the simplicity and the bounding box accuracy of oldfashioned methods.
by taking into account the unique boundary shape texture and layout characteristics of gui elements we design a novel old fashioned method with a top down coarse to fine detection strategy rather than the current bottom up edge contour aggregation strategy in existing methods .
for non text region classification and gui text detection we adopt the mature easy to deploy resnet50 image classifier and the east scene text detector respectively.
by a synergy of our novel old fashioned methods and existing mature deep learning models our new method achieves .
in f1 for all gui elements .
in f1 for non text gui elements and .
in f1 for text elements in a large scale evaluation with gui images which significantly outperform existing old fashioned methods and outperform the best deep learning model by .
increase in f1 for non text elements and .
increase in f1 for all gui elements.
this paper makes the following contributions we perform the first systematic analysis of the problem scope and solution space of gui element detection and identify the key challenges to be addressed the limitations of existing solutions and a set of unanswered research questions.
we conduct the first large scale empirical study of seven representative gui element detection methods which systematically answers the unanswered questions.
we identify the pros and cons of existing methods which informs the design of new methods for gui element detection.
we develop a novel approach that effectively incorporates the advantages of different methods and achieves the state of the art performance in gui element detection.object detection for graphical user interface old fashioned or deep learning or a combination?
esec fse november virtual event usa table existing solutions for non text gui element detection and their limitations style method region detection region classification old fashionededge contour aggregation detect primitive edges and or regions and merge them into larger regions windows or objects merge with text regions recognized by ocr ineffective for artificial gui elements e.g.
images heuristically distinguish image text list container .
can be enhanced by a cnn classification like in template matching depend on manual feature engineering either sample images or abstract prototypes match samples prototypes to detect object bounding box and class at the same time only applicable to simple and standard gui elements e.g.
button checkbox hard to apply to gui elements with large variance of visual features deep learninganchor box two stage must define anchor boxes pipeline region detection and region classification gallery d.c. is the only work that tests the faster rcnn on large scale real guis but it uses default settingsa cnn classifier for region classification trained jointly with region proposal network anchor box one stage yolov2 and yolov3 uses k means to determine anchor boxes k is user defined simultaneously region detection and region classification uses yolov2 trains and tests on artificial desktop guis only tests on real guis anchor free never applied problem scope and solution space in this section we identify the unique characteristics of guis and gui elements which have been largely overlooked when designing or choosing gui element detection methods section .
.
we also summarize representative methods for gui element detection and point out the challenges that the unique characteristics of guis and gui elements pose to these methods section .
.
.
problem scope figure and figure shows examples of gui elements and guis in our dataset.
we observe two element level characteristics large in class variance and high cross class similarity and two gui level characteristics packed scene and close by elements and mix of heterogeneous objects.
in face of these characteristics gui element detection must achieve high accuracy on region detection.
large in class variance gui elements are artificially designed and their properties e.g.
height width aspect ratio and textures depend on the content to display the interaction to support and the overall gui designs.
for example the width of button or edittext depends on the length of displayed texts.
progressbar may have different styles vertical horizontal or circle .
imageview can display images with any objects or contents.
furthermore different designers may use different texts colors backgrounds and look and feel even for the same gui functionality.
in contrast physical world objects such as human car or building share many shape appearance and physical constraints in common within one class.
large in class variance of gui elements pose main challenge of accurate region detection of gui elements.
high cross class similarity gui elements of different classes often have similar size shape and visual features.
for example button spinner and chronometer all have rectangle shape with some text in the middle.
both seekbar and horizontal progressbar show a bar with two different portions.
the visual differences to distinguish different classes of gui elements can be subtle.
for example the difference between button and spinner lies in a small triangle at the right side of spinner while a thin underline distinguishes edittext from textview.
small widgets are differentiated by smallvisual cues.
existing object detection tasks usually deal with physical objects with distinct features across classes for example horses trucks persons and birds in the popular coco2015 dataset .
high cross class similarity affects not only region classification but also region detection by deep learning models as these two subtasks are jointly trained.
mix of heterogeneous objects guis display widgets images and texts.
widgets are artificially rendered objects.
as discussed above they have large in class variance and high cross class similarity.
imageview has simple rectangle shape but can display any contents and objects.
for the gui element detection task we want to detect the imageviews themselves but not any objects in the images.
however the use of visual features designed for physical objects e.g.
canny edge contour map contradicts this goal.
in figure and figure we can observe a key difference between gui texts and general document texts.
that is gui texts are often highly cluttered with the background and close to other gui elements which pose main challenge of accurate text detection.
these heterogeneous properties of gui elements must be taken into account when designing gui element detection methods.
packed scene and close by elements as seen in figure guis especially those of mobile applications are often packed with many gui elements covering almost all the screen space.
in our dataset see section .
.
of guis contain more than seven gui elements.
furthermore gui elements are often placed close side by side and separated by only small padding in between.
in contrast there are only an average of seven objects placed sparsely in an image in the popular coco object detection challenge .
gui images can be regarded as packed scenes.
detecting objects in packed scenes is still a challenging task because close by objects interfere the accurate detection of each object s bounding box.
high accuracy of region detection for generic object detection a typical correct detection is defined loosely e.g.
by an iou .
between the detected bounding box and its ground truth e.g.
the pascal voc challenge standard since people can recognize an object easily from major part of it.
in contrast gui element detection has a much stricter requirement on the accuracy of regionesec fse november virtual event usa chen j. xie m. xing z. chen c. xu x. zhu l. and li g. detection.
inaccurate region detection may not only result in inaccurate region classification but more importantly it also significantly affects the downstream applications for example resulting in incorrect layout of generated gui code or clicking on the background in vain during gui testing.
however the above gui characteristics make the accurate region detection a challenging task.
note that accurate region classification is also important but the difficulty level of region classification relies largely on the downstream applications.
it can be as simple as predicting if a region is tapable or editable for gui testing or if a region is a widget image or text in order to wireframe a gui or which of dozens of gui framework component s can be used to implement the region.
.
solution space we summarize representative methods for gui element detection and raise questions that have not been systematically answered.
.
.
non text element detection.
table summarizes existing methods for non text gui element detection.
by contrasting these methods and the gui characteristics in section .
we raise a series questions for designing effective gui element detection methods.
we focus our discussion on region detection which aims to distinguish gui element regions from the background.
region classification can be well supported by a cnn based image classifier .
the effectiveness of physical world visual features.
oldfashioned methods for non text gui element detection rely on either edge contour aggregation or template matching .
canny edge and contour map are primitive visual features of physical world objects which are designed to capture fine grained texture details of objects.
however they do not intuitively correspond to the shape and composition of gui elements.
it is error prone to aggregate these fine grained regions into gui elements especially when guis contain images with physical world objects.
template matching methods improve over edge contour aggregation by guiding the region detection and aggregation with high quality sample images or abstract prototypes of gui elements.
but this improvement comes with the high cost of manual feature engineering.
as such it is only applicable to simple and standard gui widgets e.g.
button and checkbox of desktop applications .
it is hard to apply template matching method to gui elements of mobile applications which have large variance of visual features.
deep learning models remove the need of manual feature engineering by learning gui element features and their composition from large numbers of guis.
how effective can deep learning models learn gui element features and their composition in face of the unique characteristics of guis and gui elements?
the accuracy of bounding box regression.
deep learning based object detection learns a statistical regression model to predict the bounding box of an object.
this regression model makes the prediction in the feature map of a high layer of the cnn where one pixel stands for a pixel block in the original image.
can such statistical regression satisfy the high accuracy requirement of region detection in face of large in class variance of gui element and packed or close by gui elements?
the impact of model architectures hyperparameters and training data.
faster rcnn and yolov2 have beenapplied to gui element detection.
these two models rely on a set of pre defined anchor boxes.
the number of anchor boxes and their height width and aspect ratio are all the model hyperparameters which are either determined heuristically or by clustering the training images using k means and then using the metrics of the centroid images considering large in class variance of gui elements how sensitive are these anchor box based models to the definition of anchor boxes when they are applied to gui element detection?
furthermore the recently proposed anchor free model e.g.
centernet removes the need of pre defined anchor boxes but has never been applied to gui element detection.
can anchorfree model better deal with large in class variance of gui elements?
last but not least the performance of deep learning models heavily depends on sufficient training data.
how well these models perform with different amount of training data?
.
.
text element detection.
existing methods either do not detect gui texts or detect gui texts separately from non text gui element detection.
they simply use off the shelf ocr tools e.g.
tesseract for gui text detection.
ocr tools are designed for recognizing texts in document images but gui texts are very different from document texts.
is ocr really appropriate for detecting gui texts?
considering the cluttered background of gui texts would it better to consider gui text as scene text?
can the deep learning scene text model effectively detect gui texts?
finally considering the heterogeneity of gui widgets images and texts can a single model effectively detect text and non text elements?
empirical study to answer the above unanswered questions we conduct the first large scale empirical study of using both old fashioned and deep learning methods for gui element detection.
our study is done on a dataset of gui screenshots from the rico dataset which were extracted from android mobile applications from application categories.
our study involves a systematic comparison of two old fashioned methods including the representative method remaui in the literature and the method xianyu recently developed by the industry and three popular deep learning methods that cover all major model design styles including two anchor box based methods faster rcnn two stage style and yolo v3 one stage style and one one stage anchor free model centernet .
for gui text detection we compare ocr tool tesseract and scene text detector east and compare separate and unified detection of text and non text gui elements.
.
research questions as region classification can be well supported by a cnn based image classifier the study focuses on three research questions rqs on region detection in gui element detection task rq1 performance how effective can different methods detect the region of non text gui elements in terms of the accuracy of predicted bounding boxes and the coverage of gui elements?
rq2 sensitivity how sensitive are deep learning techniques to anchor box settings and amount of training data?
rq3 text detection does scene text recognition fit better for gui text detection than ocr technique?
which option separated versus unified text and non text detection is more appropriate?object detection for graphical user interface old fashioned or deep learning or a combination?
esec fse november virtual event usa figure gui elements distribution in our dataset .
experiment setup .
.
dataset.
we leverage rico dataset to construct our experimental dataset.
in this study we consider types of commonly used gui elements in the android platform see examples in figure .
the rico dataset contains guis and we filter out guis.
among them guis do not belong to the app itself they are captured outside the app such as android home screen a redirection to the social media login page e.g.
facebook or to a browser.
we identify them by checking whether the package name in the metadata for each gui is different from the app s true package name.
guis do not have useful metadata which only contain elements that describe the layout or elements with invalid bounds or do not have visible leaf elements.
of them do not contain any of the elements.
the rest guis are removed because they only contain text elements or non text elements.
as a result we obtain gui screenshots.
these guis are from android mobile applications of categories.
these guis contain gui elements among which are non text elements and are text elements.
we remove the standard os status and navigation bars from all gui screenshots as they are not part of application guis.
we obtain the bounding box and class of the gui elements from the corresponding gui metadata.
figure shows the distribution of guis per application and the number of gui elements per gui.
compared with the number of objects per image in coco2015 our gui images are much more packed.
we split these guis into train validation test dataset with a ratio of 40k 5k 5k .
all guis of an application will be in only one split to avoid the bias of seen samples across training validation and testing.
we perform fold cross validation in all the experiments.
.
.
baselines.
the baseline methods used in this study include remaui detects text and non text elements separately.
for text elements it uses the ocr tool tesseract .
for nontext elements it detects the structural edge of gui elements using canny edge s detection by using gaussian filter to smooth the image and reduce noises and then performing multi stage filtering to identify the true edges in images.
after that remaui performs edge merging obtains the contours and obtains the bounding box of the gui elements by merging partial overlapping regions.
we use the remaui tool provided by its authors in our experiments.
xianyu is a tool developed by the alibaba to generate code from gui images.
we only use the element detection part of this tool.
xianyu binarizes the image and performs horizontal vertical slicing i.e.
cutting the whole images horizontally vertically in half recursively to obtain the gui elements.
it uses the edge detection method with laplace filter which highlights regions of rapid intensity change to detect the edges and get contours in the binarized image.
then it leverages the flood fill algorithm which mergethe qualified neighbor points gradually to identify the connected regions and filters out the noise from the complex background.
faster rcnn is a two stage anchor box based deep learning technique for object detection.
it first generates a set of region proposals by a region proposal network rpn also called as region of interests rois which likely contain objects.
rpn uses a fixed set of user defined boxes with different scales and aspect ratios called anchor boxes and computes these anchor boxes in each point in the feature map.
for each box rpn then computes an objectness score to determine whether it contains an object or not and regresses it to fit the actual bounding box of the contained object.
the second stage is a cnn based image classifier that determines the object class in the rois.
yolov3 is an one stage anchor box based object detection technique.
different from the manually defined anchor box of faster rcnn yolov3 uses k means method to cluster the ground truth bounding boxes in the training dataset and takes the box scale and aspect ratio of the kcentroids as the anchor boxes.
it also extracts image features using cnn and for each grid of the feature map it generates a set of bounding boxes.
for each box it computes the objectness scores regresses the box coordinates and classifies the object in the bounding box at the same time.
centernet is an one stage anchor free object detection technique.
instead of generating bounding box based on the predefined anchor boxes it predicts the position of the top left and bottom right corners and the center of an object and then assembles them to get the bounding box of an object.
it computes the distance between each top left corner and each bottom right corner and outputs the bounding box of the matched pair if the distance of them is smaller than a threshold and the center point of the bounding box has a certerness score higher than a threshold.
tesseract is an ocr tool for document texts.
it consists of two steps text line detection and text recognition.
only the text line detection is relevant to our study.
the tesseract s text line detection is old fashioned.
it first converts the image into binary map and then performs a connected component analysis to find the outlines of the elements.
these outlines are then grouped into blobs which are further merged together.
finally it merges text lines that overlap at least half horizontally.
east is a deep learning technique to detect text in natural scenes.
an input image is first fed into a feature pyramid network.
east then computes six values for each point based on the final feature map namely an objectness score top left bottom right offsets and a rotation angle.
for this baseline we directly use the pre trained model to detect texts in guis without any fine tuning.
.
.
model training.
for faster rcnn yolov3 and centernet we initialize their parameters using the corresponding pre trained models of coco object detection dataset and fine tune all parameters using our gui training dataset.
we train each model for iterations with a batch size of and use adam as the optimizer.
faster rcnn uses resnet as the backbone.
yolov3 uses darknet as the backbone.
centernet uses hourglass as the backbone.
for xianyu and remaui we perform the parameter tuning and use the best setting in all our evaluation.
we perform non maximum suppression nms to remove highly duplicated predictions in all experiments.
it keeps the prediction with the highestesec fse november virtual event usa chen j. xie m. xing z. chen c. xu x. zhu l. and li g. figure performance at different iou thresholds objectness in the results and removes others if they have a iou with the selected object over a certain value.
we find the best object confidence threshold for each model using the validation dataset.
all codes and models are released at our github repository1.
.
.
metrics.
for region detection evaluation we ignore the class prediction results and only evaluate the ability of different methods to detect the bounding box of gui elements.
we use precision recall and f1 score to measure the performance of region detection.
precision is tp tp fp and recall is tp tp fn .
true positive tp refers to a detected bounding box which matches a ground truth box.
false positive fp refers to a detected box which does not match any ground truth boxes.
false negative fn refers to a ground truth bounding box which is not matched by any detected boxes.
we compute f1 score as f1 precision recall precision recall .
tp is determined based on the intersection over union iou of the two boxes.
iou is calculated by dividing the intersection areaiof the two boxes aandbby the union area of the two boxes i.e.
i a b i .
a detected box is considered as a tp if the highest iou of this box with any ground truth boxes in the input gui image is higher than a predefined iou threshold.
each ground truth box can only be matched at most once and nms technique is used to determine the optimal matching results.
considering the high accuracy requirement of gui element detection we take the iou threshold .
in most of our experiments.
.
results rq1 performance this section reports the performance of five methods for detecting the regions of non text gui elements in a gui.
in this rq faster rcnn uses the customized anchor box setting and yolov3 uses k see section .
.
.
the models are trained using all 40k training data and tested on 5k gui images in fold cross validation setting.
.
.
trade off between bounding box accuracy and gui element coverage.
figure shows the performance of five methods at different iou thresholds.
the f1 score of all deep learning models drop significantly when the iou threshold increases from .
to .
with the and decrease for faster rcnn yolov3 and centernet respectively.
the bounding box of a roi is predicted by statistical regression in the high layer feature map of the cnn where one pixel in this abstract feature map corresponds to a pixel block in the original image.
that is a minor change of the predicted coordinates in the abstract feature map will lead to a large change in the exact position in the original image.
therefore deep learning models either detect more elements with loose bounding boxes or detect less elements with accurate bounding boxes.
in contrast the f1 score of remaui and xianyu does not drop as significantly performance non text element detection iou .
method bbox precision recall f1 remaui .
.
.
xianyu .
.
.
faster rcnn .
.
.
yolov3 .
.
.
centernet .
.
.
as that of deep learning models as the iou threshold increases but their f1 scores are much lower than those of deep learning models.
this suggests that the detected element regions by these old fashioned methods are mostly noise but when they do locate real elements the detected bounding boxes are fairly accurate.
.
.
performance comparison.
we observe that if the detected bounding box has .
iou over the corresponding gui element not only does the box miss some portion of this element but it also includes some portions of adjacent elements due to the packed characteristic of gui design.
therefore we use iou .9as an acceptable accuracy of bounding box prediction.
table shows the overall performance of the five methods at iou .
threshold for detecting non text gui elements.
xianyu performs the worst with all metrics below .
.
we observe that xianyu works fine for simple guis containing some gui elements on a clear or gradient background e.g.
xianyu c d in figure .
when the gui elements are close by or placed on a complex background image xianyu s slicing method and its background de noising algorithms do not work well.
for example in xianyu a b in figure it misses most of gui elements.
xianyu performs slicing by the horizontal or vertical lines across the whole gui.
such lines often do not exist in guis especially when they have complex background images xianyu a or the gui elements are very closeby xianyu b .
this results in many under segmentation of gui images and the misses of many gui elements.
furthermore xianyu sometimes may over segment the background image xianyu a resulting in many noise non gui element regions.
remaui performs better than xianyu but it is still much worse than deep learning models.
remaui suffers from similar problems as xianyu including ineffective background de noising and oversegmentation.
it outperforms xianyu because it merges close by edges to construct bounding boxes instead of the simple slicing method by horizontal vertical lines.
however for guis with image background its edge merging heuristics often fail due to the noisy edges of physical world objects in the images.
as such it often reports some non gui element regions of the image as element regions or erroneously merges close by elements as shown in figure .
furthermore remaui merges text and non text region heuristically which are not very reliable either see the text elements detected as non text elements in remaui b c d .
deep learning models perform much better than old fashioned methods.
in figure we see that they all locate some gui elements accurately even those overlaying on the background picture.
these models are trained with large scale data see many sophisticated guis and thus can locate gui elements even in a noisy background.
however we also observe that the detected bounding boxes by deep learning models may not be very accurate as they are estimated by a statistical regression model.
the two stage model faster rcnn outperforms the other two one stage models yolov3 andobject detection for graphical user interface old fashioned or deep learning or a combination?
esec fse november virtual event usa table impact of anchor box settings iou .
setting precision recall f1 faster rcnn default .
.
.
faster rcnn customized .
.
.
faster rcnn union .
.
.
faster rcnn intersection .
.
.
yolov3 k .
.
.
yolov3 k .
.
.
yolo union .
.
.
yolo intersection .
.
.
table impact of amount of training data iou .
method size precision recall f1 faster rcnn2k .
.
.
10k .
.
.
40k .
.
.
yolov32k .
.
.
10k .
.
.
40k .
.
.
centernet2k .
.
.
10k .
.
.
40k .
.
.
centernet.
as discussed in section .
gui elements have large in class variance and high cross class similarity.
two stage models perform region detection and region classification in a pipeline so that the region detection and region classification are less mutually interfered compared with one stage models that perform region detection and region classification simultaneously.
between the two one stage models anchor free centernet outperforms anchor box based yolov3 at iou .
.
however yolov3 performs better than centernet at lower iou thresholds see figure .
anchor free model is flexible to handle the large in class variance of gui elements and gui texts see more experiments on gui text detection in section .
.
however as shown in figure this flexibility is a double blade which may lead to less accurate bounding boxes or bound several elements in one box e.g.
centernet a d .
because gui elements are often close by or packed in a gui centernet very likely assembles the top left and bottom right corners of different gui elements together which leads to the wrong bounding boxes.
deep learning models significantly outperform old fashioned detection methods.
two stage anchor box based models perform the best in non text gui element detection task.
but it is challenging for the deep learning models to achieve a good balance between the accuracy of the detected bounding boxes and the detected gui elements especially for anchor free models.
.
results rq2 sensitivity this section reports the sensitivity analysis of the deep learning models for region detection from two aspects anchor box settings and amount of training data.
.
.
anchor box settings.
for faster rcnn we use two settings the default setting three anchor box scales and and three aspect ratios and and the customized setting five anchor box scales and and four aspect width height ratios and .
this customized settingis drawn from the frequent scales and aspect ratios of the gui elements in our dataset.
considering the size of gui elements we add two small scales and .
furthermore we add two more aspect ratios to accommodate the large variance of gui elements.
for yolov3 we use two ksettings and which are commonly used in the literature.
yolov3 automatically derives anchor box metrics from kclusters of gui images in the dataset.
all models are trained using 40k training data and tested on 5k gui images.
table shows the model performance at iou .
of these different anchor box settings.
it is somehow surprising that there is only a small increase in f1 when we use more anchor box scales and aspect ratios.
we further compare the tps of different anchor box settings.
we find that of tps overlap between the two settings for faster rcnn and of tps overlap between the two settings for yolov3.
as the scales and aspect ratios of gui elements follow standard distributions using a smaller number of anchor boxes can still covers a large portion of the element distribution.
as different settings detect some different bounding boxes we want to see if the differences may complement each other.
to that end we adopt two strategies to merge the detected boxes by the two settings union strategy and intersection strategy.
for two overlapped boxes we take the maximum objectness of them and then merge the two boxes by taking the union intersection area for union interaction strategy.
for the rest of the boxes we directly keep them.
we find the best object confidence threshold for the combined results using the validation dataset.
the union strategy does not significantly affect the f1 which means that making the bounding boxes larger is not very useful.
in fact for the boxes which are originally tps by one setting the enlarged box could even become fps.
however the intersection strategy can boost the performance of both faster rcnn and yolov3 achieving .
and .
in f1 respectively.
it is reasonable because the intersection area is confirmed by the two settings and thus more accurate.
.
.
amount of training data.
in this experiment faster rcnn uses the customized anchor box setting and yolov3 uses k .
we train the models with 2k 10k 40k training data separately and test the models on the same 5k gui images.
each 2k or 10k experiment uses randomly selected 2k or 10k guis in the 40k training data.
as shown in table the performance of all models drops as the training data decreases.
this is reasonable because deep learning models cannot effectively learn the essential features of the gui elements without sufficient training data.
the relative performance of the three models is consistent at the three training data sizes with yolov3 always being the worst.
this indicates the difficulty in training one stage anchor box model.
faster rcnn with 2k or 10k training data achieves the comparable or higher f1 than that of yolov3 and centernet with 10k or 40k training data.
this result further confirms that two stage model fits better for gui element detection tasks than one stage model and one stage anchor free model performs better than one stage anchor box model.
anchor box settings do not significantly affect the performance of anchor box based models because a small number of anchor boxes can cover the majority of gui elements.
two stage anchor boxbased model is the easiest to train which requires one magnitude less training data to achieve comparable performance as one stage model.
one stage anchor box model is the most difficult to train.esec fse november virtual event usa chen j. xie m. xing z. chen c. xu x. zhu l. and li g. table text detection separated versus unified processing method element precision recall f1 faster rcnnnontext only .
.
.
mixnontext .
.
.
text .
.
.
both .
.
.
yolov3nontext only .
.
.
mixnon text .
.
.
text .
.
.
both .
.
.
centernetnontext only .
.
.
mixnon text .
.
.
text .
.
.
both .
.
.
.
results rq3 text detection .
.
separated versus unified text non text element detection.
all existing works detect gui text separately from non text elements.
this is intuitive in that gui text and non text elements have very different visual features.
however we were wondering if this is a must or text and non text elements can be reliably detected by a single model.
to answer this we train faster rcnn yolov3 and centernet to detect both text and non text gui elements.
faster rcnn uses the customized anchor box setting and yolov3 uses k .
the model is trained with 40k data and tested on 5k gui images.
in this rq both non text and text elements in guis are used for model training and testing.
table shows the results.
when trained to detect text and nontext elements together faster rcnn still performs the best in terms of detecting non text elements.
but the performance of all three models for detecting non text elements degrades compared with the models trained to detect non text elements only.
this indicates that mixing the learning of text and non text element detection together interfere with the learning of detecting non text elements.
centernet performs much better for detecting text elements than faster rcnn and yolov3 which results in the best overall performance for the mixed text and non text detection.
centernet is anchor free which makes it flexible to handle large variance of text patterns.
so it has comparable performance for text and non text elements.
in contrast anchor box based faster rcnn and yolov3 are too rigid to reliably detect text elements.
however the performance of centernet in detecting text elements is still poor.
text elements always have space between words and lines.
due to the presence of these spaces centernet often detects a partial text element or erroneously groups separate text elements as one element when assembling object corners.
.
.
ocr versus scene test recognition.
since it is not feasible to detect text and non text gui elements within a single model we want to investigate what is the most appropriate method for gui text detection.
all existing works e.g.
remaui xianyu simply use ocr tool like tesseract.
we observe that gui text is more similar to scene text than to document text.
therefore we adopt a deep learning scene text recognition model east for gui text detection and compare it with tesseract.
we directly use the pre trained east model without any fine tuning on gui text.
as shown in table east achieves .
in precision .
in recall and .
in f1 which is significantly higher than tesseract .
in precision .
in recall and .
in f1 .
both xianyu and remaui perform some post processing of the tesseract s ocrtable text detection ocr versus scene text method precision recall f1 tesseract .
.
.
east .
.
.
remaui .
.
.
xianyu .
.
.
results in order to filter out false positives.
but it does not significantly change the performance of gui text detection.
as east is specifically designed for scene text recognition its performance is significantly better than using generic object detection models for gui text detection see table .
east detects almost all texts in a gui including those on the gui widgets e.g.
the button labels in figure c .
however those texts on gui widgets are considered as part of the widgets in our ground truth data rather than stand alone texts.
this affects the precision of east against our ground truth data even though the detected texts are accurate.
figure presents some detection results.
tesseract achieves the comparable results as east only for the left side of figure d where text is shown on a white background just like in a document.
from all other detection results we can observe the clear advantages of treating gui text as scene text than as document text.
first east can accurately detect text in background image figure a while tesseract outputs many inaccurate boxes in such images.
second east can detect text in a low contrast background figure b while tesseract often misses such texts.
third east can ignore nontext elements e.g.
the bottom right switch buttons in figure b and the icons on the left side of figure d while tesseract often erroneously detects such non text elements as text elements.
gui text and non text elements should be detected separately.
neither ocr techniques nor generic object detection models can reliably detect gui texts.
as gui texts have the characteristics of scene text the deep learning scene text recognition model can be used even without fine tuning to accurately detect gui texts.
a novel approach based on the findings in our empirical study we design a novel approach for gui element detection.
our approach combines the simplicity of old fashioned computer vision methods for non textelement region detection and the mature easy to deploy deep learning models for region classification and gui text detection section .
.
this synergy achieves the state of the art performance for the gui element detection task section .
.
.
approach design our approach detects non text gui elements and gui texts separately.
for gui text detection we simply use the pre trained stateof the art scene text detector east .
for non text gui element detection we adopt the two stage design i.e perform region detection and region classification in a pipeline.
for region detection we develop a novel old fashioned method with a top down coarse tofine strategy and a set of gui specific image processing algorithms.
for region classification we fine tune the pre trained resnet50 image classifier with gui element images.
.
.
region detection for non text gui elements.
according to the performance and sensitivity experiments results we do not want to use generic deep learning object detection models object detection for graphical user interface old fashioned or deep learning or a combination?
esec fse november virtual event usa figure examples ocr versus scene text .
first they demand sufficient training data and different model designs require different scale of training data to achieve stable performance.
furthermore the model performance is still less optimal even with a large set of training data and varies across different model designs.
second the nature of statistical regression based region detection cannot satisfy the high accuracy requirement of gui element detection.
unlike generic object detection where a typical correct detection is defined loosely e.g iou .
detecting gui elements is a fine grained recognition task which requires a correct detection that covers the full region of the gui elements as accurate as possible but the region of non gui elements and other close by gui elements as little as possible.
unfortunately neither anchor box based nor anchor free models can achieve this objective because they are either too strict or too flexible in face of large in class variance of element sizes and texture high cross class shape similarity and the presence of close by gui elements.
unlike deep learning models old fashioned methods do not require any training which makes them easy to deploy.
furthermore when old fashioned methods locate some gui elements the detected bounding boxes are usually accurate which is desirable.
therefore we adopt old fashioned methods for non text guielement region detection.
however existing old fashioned methods use a bottom up strategy which aggregates the fine details of the objects e.g.
edge or contour into objects.
this bottom up strategy performs poorly especially affected by the complex background or objects in the guis and gui elements.
as shown in figure our method adopts a completely different strategy top down coarse tofine.
this design carefully considers the regularity of gui layouts and gui element shapes and boundaries as well as the significant differences between the shapes and boundaries of artificial gui elements and those of physical world objects.
our region detection method first detects the layout blocks of a gui.
the intuition is that guis organize gui elements into distinct blocks and these blocks generally have rectangle shape.
xianyu also detects blocks but it assumes the presence of clear horizontal and vertical lines.
our method does not make this naive assumption.
instead it first uses the flood filling algorithm over the greyscale map of the input gui to obtain the maximum regions with similar colors and then uses the shape recognition to determine if a region is a rectangle.
each rectangle region is considered as a block.
finally it uses the suzuki s contour tracing algorithm figure our method for non text gui element detection to compute the boundary of the block and produce a block map.
in figure we show the detected block in different colors for the presentation clarity.
note that blocks usually contain some gui elements but some blocks may correspond to a particular gui element.
next our method generates a binary map of the input gui and for each detected block it segments the corresponding region of the binary map.
binarization simplifies the input image into a black white image on which the foreground gui elements can be separated from the background.
existing methods perform binarization through canny edge detection and sobel edge detection which are designed to keep fine texture details in nature scene images.
unfortunately this detail keeping capability contradicts the goal of gui element detection which is to detect the shape of gui elements rather than their content and texture details.
for example we want to detect an imageview element no matter what objects are shown in the image see figure .
we develop a simple but effective binarization method based on the gradient map of the gui images.
a gradient map captures the change of gradient magnitude between neighboring pixels.
if a pixel has small gradient with neighboring pixels it becomes black on the binary map otherwise white.
as shown in figure the gui elements stand out from the background in the binary map either as white region on the black background or black region with white edge.
our method uses the connected component labeling to identify gui element regions in each binary block segment.
it takes as input the binarized image and performs two pass scanning to label the connected pixels.
as gui elements can be any shape it identifies a smallest rectangle box that covers the detected regions as the bounding boxes.
although our binarization method does not keep many texture details of non gui objects the shape of non gui objects e.g.
those buildings in the pictures may still be present in the binary map.
these noisy shapes interfere existing bottom up aggregation methods for gui element detection esec fse november virtual event usa chen j. xie m. xing z. chen c. xu x. zhu l. and li g. figure region detection results for non text gui element our method versus five baselines table detection performance of our approach iou .
elements precision recall f1 non text .
.
.
text .
.
.
both .
.
.
table region classification results for tp regions non text elements all elements method bbox accuracy bbox accuracy fasterrcnn .
.
yolov3 .
.
centernet .
.
our method .
.
table overall results of object detection iou .
non text elements all elements method precision recall f1 precision recall f1 faster rcnn .
.
.
.
.
.
yolov3 .
.
.
.
.
.
centernet .
.
.
.
.
.
xianyu .
.
.
.
.
.
remaui .
.
.
.
.
.
our method .
.
.
.
.
.
which results in over segmentation of gui elements.
in contrast our top down detection strategy minimizes the influence of these non gui objects because it uses relaxed grey scale map to detect large blocks and then uses strict binary map to detect gui elements.
if a block is classified as an image our method will not further detect gui elements in this block.
.
.
region classification for non text gui elements.
for each detected gui element region in the input gui we use a resnet50 image classifier to predict its element type.
in this work we consider element types as shown in figure .
the resnet50 image classifieris pre trained with the imagenet data.
we fine tune the pre trained model with gui elements per element type randomly selected from the 40k guis in our training dataset.
.
.
gui text detection.
section .
shows that gui text should be treated as scene text and be processed separately from nontext elements.
furthermore scene text recognition model performs much better than generic object detection models.
therefore we use the state of the art deep learning scene text detector east to detect gui text.
as shown in figure c east may detect texts that are part of non image gui widgets e.g.
the text on the buttons .
therefore if the detected gui text is inside the region of a nonimage gui widgets we discard this text.
.
evaluation we evaluate our approach on the same testing data used in our empirical study by fold cross validation.
table shows the regiondetection performance for non text text and both types of elements.
for non text gui elements our approach performs better than the best baseline faster rcnn .
versus .
in f1 .
for text elements our approach is overall the same as east.
it is better than east in precision because our approach discards some detected texts that are a part of gui widgets.
but this degrades the recall.
for text and non text elements as a whole our approach performs better than the best baseline centernet .
versus .
in f1 .
figure shows the examples of the detection results by our approach and the five baselines.
compared with remaui and xianyu our method detects much more gui elements and much less noisy non gui element regions because of our robust top down coarse to fine strategy and gui specific image processing e.g.
connected component labeling rather than canny edge and contour .object detection for graphical user interface old fashioned or deep learning or a combination?
esec fse november virtual event usa our method also detects more gui elements than the three deep learning models.
furthermore it outputs more accurate bounding boxes and less overlapping bounding boxes because our method performs accurate pixel analysis rather than statistical regression in the high layer of cnn.
note that deep learning models may detect objects in images as gui elements because there are gui elements of that size and with similar visual features.
in contrast our method detects large blocks that are images and treats such images as whole.
as such our method suffers less over segmentation problem.
for failure analysis of our model we conclude three main reasons when our model fails.
first same look and feel ui regions may correspond to different types of widgets such as text label versus text button without border.
this is similar to the widget tappability issue studied in .
second the repetitive regions in a dense ui e.g.
figure b often have inconsistent detection results.
third it is sometimes hard to determine whether a text region is a text label or part of a widget containing text for example the spinner showing usa at the top of figure b .
note that these challenges affect all methods.
we leave them as our future work.
table shows the region classification results of our cnn classifier and the three deep learning baselines.
the results consider only true positive bounding boxes i.e.
the classification performance given the accurate element regions.
as text elements are outputted by east directly we show the results for non text elements and all elements.
we can see that our method outputs more true positive gui element regions and achieves higher classification accuracy .
for non text elements and .
for all elements and the other three deep models achieves about .
accuracy .
our classification accuracy is consistent with which confirms that the effectiveness of a pipeline design for gui element detection.
table shows the overall object detection results i.e.
the truepositive bounding box with the correct region classification over all detected element regions.
among the three baseline models faster rcnn performs the best for non text elements .
in f1 but centernet due to this model flexibility to handle gui texts achieves the best performance for all elements .
in f1 .
compared with these three baselines our method achieves much better f1 for both non text elements .
and all elements .
due to its strong capability in both region detection and region classification.
related work gui design implementation and testing are important software engineering tasks to name a few gui code generation gui search gui design examination reverse engineering gui dataset gui accessibility gui testing and gui security .
many of these tasks require the detection of gui elements.
as an example the rq4 in shows exploiting exact widget locations by instrumentation achieves significantly higher branch coverage than predicted locations in gui testing but widget detection by yolov2 can interact with widgets not detected by instrumentation.
our work focuses on the foundational technique to improve widget detection accuracy which opens the door to keep the advantage of widget detection while achieving the benefits of instrumentation in downstream applications like gui testing.table and section .
.
summarizes the old fashioned methods e.g.
remaui and xianyu designed for gui element detection the generic object detection models faster rcnn yolov3 and centernet applied for non text gui element detection and the old fashioned ocr tool tesseract and the state of the art scene text detector east for gui text detection.
our empirical study shows that old fashioned methods perform poorly for both text and non text gui element detection.
generic object detection models perform better than old fashioned ones but they cannot satisfy the high accuracy requirement of gui element detection.
our method advances the state of the art in gui element detection by effectively assembling the effective designs of existing methods and a novel gui specific old fashioned region detection method.
besides some researchers apply image captioning to generate gui code from gui design .
however image captioning only predicts what elements are in a ui but not the bounding box of these elements.
therefore pix2code assumes uis use a small set of predefined fonts shapes sizes and layouts to compensate for this shortage.
however due to this limitation our experiments showed that pix2code does not work on real app uis which use much more diverse fonts shapes sizes layouts.
in comparison in this work we study the object detection methods which directly obtain the bounding box of the elements.
we further propose a novel top down coarse to fine method to detect elements.
there are also some works which perform object detection on other platforms e.g.
ios website and other kinds of design e.g.
sketch .
in this work we perform our experiments on android app uis because of the availability of large scale rico dataset while other datasets like remaui cannot effectively train and experiment deep learning models see our results of training data size in section .
.
.
however since our model does not make any specific assumptions about android uis we believe that our model could be easily generalized to other platforms and other kinds of gui design.
we leave them as future work because they demand significant manual labelling effort.
we release our tool to public.
conclusion this paper investigates the problem of gui element detection.
we identify four unique characteristics of guis and gui elements including large in class variance high cross class similarity packed or close by elements and mix of heterogeneous objects.
these characteristics make it a challenging task for existing methods no matter old fashioned or deep learning to accurately detect gui elements in gui images.
our empirical study reveals the underperformance of existing methods borrowed from computer vision domain and the underlying reasons and identifies the effective designs of gui element detection methods.
informed by our study findings we design a new gui element detection approach with both the effective designs of existing methods and the gui characteristics in mind.
our new method achieves the state of the art performance on the largest ever evaluation of gui element detection methods.