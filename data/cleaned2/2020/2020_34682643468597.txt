vulnerability detection with fine grained interpretations yi li new jersey inst.
of technology new jersey usa yl622 njit.edushaohua wang new jersey inst.
of technology new jersey usa davidsw njit.edutien n. nguyen university of texas at dallas texas usa tien.n.nguyen utdallas.edu abstract despite the successes of machine learning ml and deep learning dl based vulnerability detectors vd they are limited to providing only the decision on whether a given code is vulnerable or not without details on what part of the code is relevant to the detected vulnerability .
we present ivdetect aninterpretable vulnerability detector with the philosophy of using artificial intelligence ai to detect vulnerabilities while using intelligence assistant ia to provide vd interpretations in terms of vulnerable statements.
for vulnerability detection we separately consider the vulnerable statements and their surrounding contexts via data and control dependencies.
this allows our model better discriminate vulnerable statements than using the mixture of vulnerable code and contextual code as in existing approaches.
in addition to the coarsegrained vulnerability detection result we leverage interpretable ai to provide users with fine grained interpretations that include the sub graph in the program dependency graph pdg with the crucial statements that are relevant to the detected vulnerability.
our empirical evaluation on vulnerability databases shows that ivdetect outperforms the existing dl based approaches by and in top ndcg and map ranking scores.
ivdetect correctly points out the vulnerable statements relevant to the vulnerability via its interpretation in of the cases with a top ranked list.
ivdetect improves over the baseline interpretation models by .
and in accuracy.
ccs concepts security and privacy software security engineering .
keywords vulnerability detection deep learning intelligence assistant explainable ai xai interpretable ai acm reference format yi li shaohua wang and tien n. nguyen.
.
vulnerability detection with fine grained interpretations.
in proceedings of the 29th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse august athens greece.
acm new york ny usa pages.
corresponding author permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august athens greece association for computing machinery.
acm isbn .
.
.
.
introduction software vulnerabilities have caused substantial damage to our society s software infrastructures.
to address the problem several automated vulnerability detection vd approaches have been proposed.
they can be broadly classified into two categories program analysis pa based and machine learning ml based .
the pa based vd techniques have often focused on solving the specific types of vulnerabilities such as bufferoverflow sql injection cross site scripting authentication bypass etc.
in addition to those types the more general software vulnerabilities e.g.
the software vulnerabilities occurring in api usages of libraries frameworks have manifested in various forms.
to detect them machine learning ml and deep learning dl have been leveraged to implicitly learn the patterns of vulnerabilities from prior vulnerable code .
despite several advantages the ml dl based vd approaches are still limited to providing only coarse grained detection results on whether an entire given method is vulnerable or not.
in comparison with the pa based approaches they fall short in the ability to elaborate on the fine grained details of the lines of code with specific statements that might be involved in the detected vulnerability.
one could use fault localization fl techniques to locate the vulnerable statements however they require large effective test suites.
due to such feedback at the coarse granularity from the existing ml dl based vd tools developers would not know where and what to look for and to fix the vulnerability in their code.
this hinders them in investigating the potential vulnerabilities.
to raise the level of ml dl based vd we present ivdetect an interpretable vd with the philosophy of using artificial intelligence to detect coarse grained vulnerability while leveraging intelligence assistant via interpretable ml to provide fine grained interpretations in term of vulnerable statements relevant to the vulnerability.
forcoarse grained vulnerability detection our novelty is the context aware representation learning of the vulnerable code .
during training the existing ml dl based vd approaches take the entire vulnerable code in a method as the input without distinguishing the vulnerable statements from the surrounding contextual code.
such distinction from vulnerable code and the contexts during training enables ivdetect to better learn to discriminate the vulnerable code and benign ones.
we represent source code via program dependence graph pdg and we treat the vulnerability detection problem as graph based classification via graph convolution network gcn with feature attention fa namely fa gcn.
the vulnerable statements along with surrounding code are encoded during the code representation learning.
forfine grained interpretation as the given method is deemed as vulnerable by ivdetect our novelty is to leverage interpretable ml to provide the interpretation in term of the vulnerable statements as part of the pdg that are involved to the detected vulnerability .
esec fse august athens greece yi li shaohua wang and tien n. nguyen the rationale for choosing pdg sub graph as an interpretation is that a vulnerability often involves the data and control dependencies among the statements .
to derive the vulnerable statements as the interpretation we leverage the interpretable ml model gnnexplainer that explains on why a model has arrived at its decision .
specifically after vulnerability detection to produce interpretation ivdetect takes as input the fa gcn model along with its decision vulnerable or not and the input pdg gmof the given method m. the goal is to find the interpretation subgraph which is defined as a minimal sub graphgin the pdg of mthat minimizes the prediction scores between using the entire gmand usingg.
to that end we leverage gnnexplainer in which the searching for gis formulated as the learning of the edge mask set em.
the idea is that if an edge belongs toem i.e.
if it is removed fromgm and the decision of the model is affected then the edge is crucial and must be included in the interpretation for the detection result .
thus the minimal sub graph g in pdg contains the nodes and edges i.e.
the crucial statements and program dependencies that are most decisive relevant to the detected vulnerability when the decision is vulnerable.
using ivdetect s results a practitioner could examine the ranked list of potentially vulnerable methods and use the interpretation to further investigate what statements in the code that cause the model to predict that vulnerability.
we conducted several experiments to evaluate ivdetect in both vulnerability detection at the method level and interpretation in term of vulnerable statements.
we use three large c c vulnerability datasets fan reveal and ffmpeg qemu .
for the method level vd our results show that ivdetect outperforms the existing ml dl based approaches by and at the top list for two ranking scores ndcg and map respectively.
for the statement level interpretation ivdetect correctly points out the vulnerable statements relevant to the vulnerability in of the cases with a top ranked list.
it improves over the baseline att and grad interpretation models by .
and in accuracy respectively.
the contributions of this paper include a. interpretable vd with fine grained interpretations a. vulnerability detection with fine grained interpretations ivdetect is the first approach to leverage interpretable ml to enhance vd with fine grained details on pdg sub graphs statements and dependencies relevant to the detected vulnerability.
b. context aware representation learning of vulnerable code the novelty of our representation learning of vulnerable code isthe consideration of the contextual code surrounding the vulnerable statements and fixes to better train the vd model.
b. empirical evaluation.
our results show ivdetect s high accuracy in both detection and interpretation see data results at .
motivation .
motivating example figure shows the method ec device ioctl xcmd in linux .
which constructs the i o control command for the cromeos devices.
this is listed as a vulnerable code within common vulnerabilities and exposures cve in the national vulnerability database.
the commit log of the corresponding fix stated that1 static long ec device ioctl xcmd struct cros ec dev ec void user arg long ret struct cros ec command u cmd struct cros ec command s cmd if copy from user u cmd arg sizeof u cmd return efault if u cmd.outsize ec max msg bytes u cmd.insize ec max ... return einval s cmd kmalloc sizeof s cmd max u cmd.outsize u cmd.insize .
if !s cmd return enomem if copy from user s cmd arg sizeof s cmd u cmd.outsize ret efault goto exit if u cmd.outsize !
s cmd outsize u cmd.insize !
s cmd insize ret einval goto exit s cmd command ec cmd offset ret cros ec cmd xfer ec ec dev s cmd only copy data to userland if data was received.
if ret goto exit if copy to user arg s cmd sizeof s cmd u cmd.insize if copy to user arg s cmd sizeof s cmd s cmd insize ret efault exit kfree s cmd return ret figure cve vulnerability in linux .
at line and line the driver fetches user space data by pointer argviacopy from user .
the first fetched value stored in u cmd line is used to get the in sizeand out sizeelements and allocation a buffer s cmd at line so as to copy the whole message to driver later at line which means the copy size of the whole message s cmd is based on the old value u cmd.outsize from the first fetch.
besides the whole message copied at the second fetch also contains the elements of in sizeand out size which are the new values.
the new values from the second fetch might be changed by another user thread under race condition which will result in a double fetch bug when the inconsistent values are used.
thus to fix this bug a developer added the code at lines to make sure that u cmd.outsize and u cmd.insize have not changed due to race condition between the two fetching calls.
moreover memory access might be also beyond the array boundary causing a buffer overflow within the method call cros ec cmd xfer ... when the command is transferred to the chromeos device at line .
another issue is at line with copy to user.
the method call cros ec cmd xfer ... can set s cmd insize to a lower value.
thus the new smaller value must be used to avoid copying too much data to the user u cmd.insize at line is changed into s cmd insize .
this vulnerable code could potentially cause the damages such as denial of service buffer overflow program crash etc.
deep learning dl advances enable several approaches toimplicitly learn from the history the patterns of vulnerable code and to detect more general vulnerabilities .
however they are still limited in comparison with program analysis based approaches in the ability to provide any detail on the fine grained level of the vulnerable statements and on why the model has decided on the vulnerability.
for example the pa based approaches e.g.
a race detection technique could potentially detect the involvement of the two fetching statements at line and line .
the method in figure might be deemed 293vulnerability detection with fine grained interpretations esec fse august athens greece figure interpretation sub graph for figure as vulnerable by a dl based model.
but without any fine grained details a developer would not know where and what to investigate next.
this would make the output of a dl model less constructive in vd.
moreover a fault localization technique which locates buggy statements does not solve the problem because it would need a large and effective test suite.
regarding detection the existing dl based approaches do not fully exploit all the available information on the vulnerable code during training.
for example during training we know that lines and are vulnerable buggy and other relevant statements via data control dependencies provide contextual information for the vulnerable ones .
however the existing approaches do not consider the vulnerable statements and do not use the contextual code to help a model discriminate the vulnerable and nonvulnerable ones.
the entire method would be fed to a dl model.
.
approach overview and key ideas we introduce ivdetect an dl based interpretable vulnerability detection approach that goes beyond the decision of vulnerability by providing the fine grained interpretation in term of the vulnerable statements.
specifically as the method is deemed as vulnerable by ivdetect it will provide a list of important statements as part of the program dependence graph pdg that are relevant to the detected vulnerability .
for example it provides the partial sub graph of the pdg including the statements at the lines and in figure for the vulnerable code at line and line .
we use the pdg sub graph including important statements for finegrained vd since they will give a developer the hints on the program dependencies relevant to the vulnerability for further investigation.
moreover if our model determines the code as non vulnerable it can also produce the key sub graph of the pdg with the key statements that are deemed to be safe.
ivdetect has two main modules figure graph based vulnerability detection model and graph based interpretation model.
the input is the source code of all methods in a project.
the output figure overview of ivdetect is the ranked list of methods with the detection result score and the interpretation pdg sub graph .
let us explain our key ideas.
.
.
graph based vulnerability detection model section .
as seen in section .
a vulnerability is usually exhibited as multiple statements being exploited thus it is natural to capture the vulnerable code as a sub graph in the pdg with the data and control flows.
this also helps developers further investigate the detected vulnerability with those flows.
toward that goal we model the vulnerability detection via the graph convolutional network gcn as follows.
the pdg of a method mis represented as a graph gn v e in whichvis a set of nodes representing the statements and eis a set of edges representing the data control dependencies.
a feature descriptionxvis for every node v which represents a property of a node e.g.
variable name etc.
features are summarized in a n d feature matrix xm n number of nodes and dis the number of input features .
let fbe a label function on the statements and methodsf v ... c that maps a node in vand an entire method to one of the cclasses.
in ivdetect c for vulnerable v and non vulnerable nv .
for training on non vulnerable code in the training set gcn performs similar operations as cnn where it learns the features with a small filter window sliding over pdg sub structure.
differing from image data with cnn the neighbors of a node in gcn are unordered and variable in size.
to predict if a method mis vulnerable its pdggmwith the associated feature set xm xj vj gm are built.
gcn learns a conditional distribution p y gm xm where yis a random variable representing the labels ... c .
that distribution indicates the probability of the graph gmbelonging to each of the classes ... c i.e.
mis vulnerable or not section .
.
.
distinction between vulnerable statements and surrounding contexts.
during training for each vulnerable statement sin a method in the training dataset we distinguish sand the surrounding contextual statements for s. a context consists of the statements with data and or control dependencies with s. this is expected to help our model recognize better the vulnerable code appearing in specific surrounding contexts and discriminate better the vulnerable code from the benign one.
for example the existing approaches feed the entire pdg of the method in figure into a model.
ivdetect distinguishes and learns the vector representation for the vulnerable statement at line while considering as contexts the statements with data control dependencies with line the data dependency context lines and and the control dependency context lines and .
.
.
graph based interpretation model for vulnerability detection section .
after prediction ivdetect performs fine grained interpretation.
it uses both the pdg gmof the method mand the gcn model as the input to obtain the interpretation.
to that end we leverage the interpretable ml technique gnnexplainer .
its 294esec fse august athens greece yi li shaohua wang and tien n. nguyen figure context aware vulnerable code representation learning for statement s27 in graph based vulnerability detection goal is to take the gcn and a specific input graph gm and produce the crucial sub graph structures and features ingmthat affect the decision of the model.
gnnexplainer s idea is that if removing or altering a node feature does affect the prediction outcome the node feature is considered as essential and thus must be included in the crucial set let us call it the interpretation set .
gnnexplainer searches for a sub graph gmingmthat minimizes the difference in the prediction scores between using the whole graph gmand using the minimal graph gm section .
because without that subgraph gmin the input pdg gm gcn model would not decide gmas vulnerable gmis considered as crucial pdg sub graph consisting ofcrucial statements and data control dependencies relevant to the detected vulnerability if the outcome is v .
if the outcome is non vulnerability gmcan be considered as the safe statements in pdg for the model to decide the input method mas benign code.
graph based vulnerability detection model this section describes our graph based vulnerability detection model.
we first explain how we build the context aware representation learning for vulnerable code and then how we use such learned vectors for vulnerability detection using fa gcn .
.
context aware representation learning let us present how we build the vector representations for code features.
for a statement we extract the following types of features .
.
sequence of sub tokens of a statement.
at the lexical level we capture the content of a statement in term of the sequence of sub tokens.
we choose the sub token granularity because the sub tokens are more likely to be repeated than the entire lexical tokens in source code .
we tokenize each statement and keep only the variables method and class names.
the names are broken into sub tokens using camelcase or hungarian convention.
we remove the sub tokens with one character to avoid the influence of noises.
for example in figure the tokens of s27are collected and broken down into the sequence copy to user arg etc.
then we use glove to build the vectors for tokens together withgate recurrent unit gru to build the feature vector for the sequence of sub tokens for s27.
glove is known to capture well semantic similarity among tokens.
gru is chosen to summarize the sequence of vectors into one feature vector for the next step.
.
.
code structure of a statement.
we capture code structure via the ast sub tree.
in figure the ast sub tree for s27is extracted and fed to tree lstm to capture the structure into a vector f2.
.
.
variables and types.
for each node i.e.
a statement we collect the names of the variables and their static types at their locations break them into the sub tokens.
for example we collect the variable s cmdand its static type cross ec command .
we use the same vector building techniques as for the sub token sequences as in the feature including glove and gru to apply on the sequences of the sub tokens built from the variables names e.g.
s cmd and those from the variables types e.g.
cross ec command .
.
.
surrounding contexts.
during training for a statement s we also encode the statements surrounding s which we refer to ascontext .
we have two contexts.
data and control dependency contexts contain the statements having such dependencies with the current statement.
for example the data dependency context for s27includes the statements at the lines and .
if the control dependencies are considered the statements with control dependencies with s27at the lines and are included.
the vectors for the statements in the context are calculated via glove and gru as described earlier.
because the number of dependencies could be different the lengths of the gru model inputs could be different.
thus we apply zero padding with a masking layer allowing the model to skip the zeros at the end of the sequence of sub tokens.
those zeros will not be included in training.
.
.
attention based bidirectional gru.
after having all vectors for the features f1 f2 ... we use a bi directional gru and an attention layer to learn the weight vector wifor each feature fi based on the hidden states from that model.
then we compute the weighted vector for each feature by multiplying the original vector for the feature by the weight that is we have f i wi.fi.
295vulnerability detection with fine grained interpretations esec fse august athens greece figure vulnerability detection with fa gcn finally we need to consider the impacts from the dependent statements to the current statement in the pdg .
the rationale is that those neighboring statements in the pdg must have the influence on the current statement if one of them is vulnerable.
for example the neighboring statements for s27in the pdg include the statements at lines and .
thus we combine and summarize them into the final feature vector fs27for the statement s27as follows fs27 iwiconcat h f i j wiis the trainable weight for combination concat is the concatenate layer to link all values into one vector his the hidden layer to summarize vector into a value i s6 s22 s25 s27 s29 jis feature index.f27is used in the next step with gcn model for detection.
.
vulnerability detection with fa gcn figure presents how we use feature attention gcn model fagcn for detection.
the rationale is that fa gcn can deal well with the graphs with sparse features not all the statements share the same properties and potentially noisy features in a pdg.
first we parse the method minto pdg.
similar to cnn using the filter on an image fa gcn performs sliding a small window along all the nodes statements of the pdg.
for example in figure the window marked with afor the node s27consists of itself and the neighboring statements nodes s6 s22 s25 ands29.
another window marked with b is for the node s23 including itself and the neighboring nodes s22ands25.
for each window fa gcn generates the feature representation matrix for the statement at the center.
for example for the window centered at s27 it generates the feature vector fs27fors27 using the process explained in figure .
from the representation vectors for all statements fa gcn uses a join layer to link all these vectors into the feature matrix fmfor methodm.
a row infmcorresponds to a window in pdg.
next fa gcn performs the convolution operation by first calculating the symmetric normalized laplacian matrix a and then calculating the convolution to generate the representation matrix mmfor the method m. after that we use the traditional steps as in a cnn model using a spatial pyramid pooling layer to normalize the method representation matrix into a uniform size and reduce its total size and connecting its output to a fully connected layer to transform the matrix into a vector vmto represent m. withvm figure masking to derive interpretation sub graphs we perform classification by using two hidden layers controlling the length of vectors and output and a softmax function to produce a prediction score for m. we use those scores as vulnerability scores to rank the methods in a project.
the decision for masvornv is done via a trainable threshold on the prediction score .
graph based interpretation model let us explain how we use gnnexplainer to build our graphbased interpretation.
the input includes the trained fa gcn model the pdg gm of the method m and the detection result vornv and prediction score.
figure illustrates our process for the case of v vulnerable the case of nv is done similarly .
to derive the interpretations the key goal is to find a sub graph gmin the pdggmof the method mthat minimizes the difference in the prediction scores between using the entire graph gmand using the minimal graph gm.
to do so we use gnnexplainer with the masking technique which treats the searching for the minimal graphgmas a learning problem of the edge mask setem of the edges.
the idea is that learning emhelps ivdetect derive the interpretation sub graph gmby masking out the edges in em fromgm masked out is denoted by gm gm em figure illustrates gnnexplainer s principle.
as an edge mask set is applied gnnexplainer checks if the fa gcn model produces the same result in this case the result is v .
if yes the edge in the edge mask is not important and is not included in gm.
otherwise the edge is important and included in gm.
because the numbers of possible sub graphs and the edge mask sets are untractable gnnexplainer uses a learning approach for the edge mask em.
let us formally explain how gnnexplainer works.
it formulates the problem by maximizing the mutual information mi 296esec fse august athens greece yi li shaohua wang and tien n. nguyen between the minimal graph gmand the input pdg gm max gmmi y gm h y h y g gm yis the outcome decision by the fa gcn model.
thus the entropy termh y is constant for the trained fa gcn model.
maximizing themivalue for allgmis equivalent to minimizing conditional entropyh y g gm which by definition of conditional entropy can be expressed as ey gm the meaning of this conditional entropy formula is a measure of how much uncertainty remains about the outcome ywhen we knowg gm.
gnnexplainer also limits the size of gmbykm i.e.
takingkmedges that give the highest mutual information with the prediction outcome y. direct optimization of the formula is not tractable thus gnnexplainer treats gmas a random graph variableg.
the objective in equation becomes min gegm gh y g gm min gh y g eg from equation we obtain equation with jensen s inequality.
the conditional entropy in equation can be optimized by replacing eg to be optimized by masking with emon the input graph gm.
now we can reduce the problem to learning the mask em.
details on training can be found in .
the resulting sub graph gmis directly used as an interpretation.
we can similarly produce the interpretations for the cases of non vulnerability result.
empirical evaluation .
research questions to evaluate ivdetect we seek to answer the following questions rq1.
comparison on method level vulnerability detection vd .
how well does ivdetect perform in comparison with the state of the art method level deep learning vd approaches?
rq2.
comparison with other interpretation models for finegrained vd interpretation.
how well does ivdetect perform in comparison with the state of the art interpretation models for fine grained vd interpretation to point out vulnerable statements?
rq3.
vulnerable code patterns and fixing patterns.
isivdetect useful in detecting vulnerable code patterns and fixes?
rq4.
sensitivity analysis for internal features.
how do internal features affect the overall performance of ivdetect ?
rq5.
sensitivity analysis on training data.
how do different data splitting schemes affect ivdetect s performance?
rq6.
time complexity.
what is time complexity of ivdetect ?
.
datasets to empirically evaluate ivdetect we have conducted several experiments on three public vulnerability datasets including fan et al.
s reveal and ffmpeg qemu .
fan et al.
dataset covers the cwes from to with features for each vulnerability.
at the method level the dataset contains 10k vulnerable methods and fixed code.
the reveal dataset table experimental datasets dataset fan reveal devign vulnerabilities non vulnerabilities ratio vul non vul .
.
contains 18k methods with .
of them being vulnerable ones.
the ffmpeg qemu dataset has been used in devign study with 22k methods and .
of the entries are vulnerable.
.
experimental methodology rq1.
comparison on method level dl based vd approaches.
baselines.
we compare ivdetect with the state of the art dlbased vulnerability detection approaches vuldeepecker a dl based approach using bidirectional lstm on the statements and their data control dependencies.
devign an dl based approach that uses ggcn model with gated graph recurrent layers on the ast cfg dfg and code sequences for graph classification.
sysevr in addition to statements and program dependencies this approach also uses program slicing and leverages several dl models lr mlp dbn cnn lstm etc.
.
russell et al.
this dl approach encodes source code as matrices of code tokens and leverages convolution model with random forest rf via ensemble classifier.
reveal this approach uses ggnn mlp and with triplet loss on graph representations of source code.
procedure.
a dataset contains a number of vulnerable and nonvulnerable methods.
we randomly split all of its vulnerable methods into and to be used for training tuning and testing respectively.
for training we added to that part the same number of non vulnerable methods as the vulnerable ones to obtain a balanced training data.
for tuning and testing we also added the non vulnerable methods but we used the real ratio between vulnerable and non vulnerable methods in the original dataset to build the tuning testing data.
we used automl on all models to automatically tune hyper parameters on the tuning dataset.
we also performed the evaluation across the datasets.
we first trained our model on the combination of two datasets reveal and ffmpeg qemu which has a balanced number of vulnerable methods and non vulnerable ones.
we then tested the model on fandataset which has a more realistic ratio of vulnerable and non vulnerable methods.
to ensure the model suitable for cross data evaluation we also used of fandataset for tuning the parameters and performed prediction on the remaining .
evaluation metrics.
we use the following evaluation metrics.
mean average precision map q q 1avgp q q with average precisionavgp n k 1p k rel k wherenis the total number of results kis the current rank in the list rel k is an indicator function equaling to if the item at rank kis actually vulnerable and to zero otherwise.
qis the total number of classification types.
it is1because we only have two types including vulnerable and non vulnerable classes however we rank all the methods based on their scores indicates vulnerable and otherwise .
normalized dcg atk ndcgk dcg k idcg k with discounted cumulative gain at rankk dcgk k i 1ri log2 i and ideal dcg 297vulnerability detection with fine grained interpretations esec fse august athens greece atk idcgk rk i 12ri log2 i whereriis the score of the result at position i andrkthe rank of the actual vulnerable methods ordered by their scores in the resulting list up to the position k. first ranking fr is the rank of the first correctly predicted vulnerable method.
average ranking ar is the average rank of the correctly predicted vulnerable methods in the top ranked list.
accuracy under curve auc is defined as auc p d m1 d m2 in whichpis the probability dis the detection model can be regarded as a binary classifier m1is a randomly chosen positive instance and m2is a randomly chosen negative instance.
precision p is the ratio of relevant instances among the retrieved ones.
it is calculated as precision tp tp fpwheretpis the number of true positives and the fpis the number of false positives.
recall r is the ratio of relevant instances that were retrieved.
it is calculated as recall tp tp fnwheretpis the number of true positives and the fnis the number of false negatives.
f score f is the harmonic mean of precision and recall.
it is calculated as fscore 2precision recall precision recall.
rq2.
comparison with other interpretation models for finegrained interpretation.
baselines.
we compare ivdetect with the following interpretation models att this model is a graph attention network that uses the attention mechanism to evaluate the weights importance degrees of the edges in the input graph grad this approach uses a gradient based method that computes the gradient of the gnn s loss function w.r.t.
the adjacency matrix.
procedure.
our goal here is to evaluate how well ivdetect produces the fine grained interpretations pointing to vulnerable statements.
thus to train test the interpretation model we need to use thefandataset because it contains the vulnerable statements and respective fixes.
the other two datasets contain only the vulnerabilities at the method level and no fixes.
therefore in this rq2 for the vulnerability prediction part we used the fa gcn model that was trained on reveal and ffmpeg qemu and predicted on the fandataset.
for the methods that are vulnerable but predicted as non vulnerable we considered those cases as incorrect because the resulting interpretations do not make sense for incorrect detection.
for the methods that are actually non vulnerable regardless of the prediction results of vulnerable or non vulnerable we could not use them because the non vulnerable methods do not have the fixed statements as the ground truth for measuring the correctness of interpretations.
thus we use the set of methods that are vulnerable and correctly detected as vulnerable for the evaluation of the interpretation model.
let us use dto denote this set.
for the interpretation we randomly split dinto and for training tuning and testing.
for training we used the fixed statements as the labels for interpretation because those fixed statements were vulnerable.
for testing we compared the relevant statements from the interpretation model against the actual fixed statements.
each method in the testing set and the trained fa gcn model are the input of the interpretation model in this rq2.
evaluation metrics .
given an interpretation sub graph gmgenerated from the graph based interpretation model we evaluate the accuracy of the interpretation for a model as follows.
for a method ifgmhas an overlap with any statement in the code changes that fix the vulnerability gmis considered as a correct interpretation i.e.
relevant to that detected vulnerability.
we then calculate accuracyas the ratio between the number of correct interpretations over the total number of interpretations.
because code changes could include addition deletion and modification we further define such overlap as follows.
if one of the statements sin the vulnerable version was deleted ormodified for fixing and ifgm s then we consider the interpretation sub graph gmas correct otherwise as incorrect.
if one of the statements s was added to the vulnerable version for fixing we check on the fixed version whether gmcontains any statement with data or control dependencies with s .
if yes we consider it as correct otherwise as incorrect.
in figure gmcontains the statement s23 having the data control dependencies with one of the added lines from .
thus gmis correct.
the rationale is that if the interpretation sub graph gmcontains some statement relevant to the added statement to fix the vulnerability that interpretation is useful in pointing out the code relevant to the vulnerability.
we also use mean first ranking mfr i.e.
the mean of the rankings for the first statement that needs to be fixed in the interpretation statements and mean average ranking mar i.e.
the mean of the rankings for all statements to be fixed in the interpretation statements.
if a statement to be fixed has not been selected as interpretation we do not consider it when calculating mfr mar.
rq3.
vulnerable code patterns and fixing patterns.
procedure.
we use a mining algorithm on the set of interpretation sub graphs to mine patterns of vulnerable code.
we also mine fixing patterns for those vulnerabilities.
see details in section .
.
evaluation metrics.
we counted the identified patterns.
rq4.
sensitivity analysis for features.
procedure.
we first built a base model with only the feature that represents the code as the sequence of tokens.
we then built other variants of our model by gradually adding one more feature in section .
to the base model including the sequence of subtokens ast subtree variable names data dependencies and control dependencies.
we measured the accuracy for each variant.
we used thefandataset and the same experiment setting as in rq1.
evaluation metrics.
we use the same metrics as in rq1.
rq5.
sensitivity analysis on training data.
we used different ratios in data splitting for training tuning and testing and .
we used thefandataset and the same setting as in rq1.
evaluation metrics .
we use the same metrics as in rq1.
rq6.
time complexity analysis.
we measure the actual training and predicting time.
experimental results .
rq1.
comparison on method level vd in table among the top prediction results ivdetect has the most correct predictions vulnerable methods .
the vulnerable methods correctly detected by ivdetect are also pushed higher in the top ranked list with correct results out of top results.
all other baselines have only correct detection in the top list.
importantly the first rank for ivdetect i.e.
the rank of the first correctly detected vulnerable methods is 1st while those of the 298esec fse august athens greece yi li shaohua wang and tien n. nguyen table rq1.
top vulnerability detection results on ffmpeg qemu dataset.
incorrect correct top rank total vuldeepecker sysevr russell et al.
devign reveal ivdetect table rq1.
method level vd on ffmpeg qemu dataset vuldee peckersysevr russell et al.devign reveal ivdetect ndcg ndcg .
ndcg .
.
.
.
ndcg .
.
.
.
.
.
ndcg .
.
.
.
.
.
ndcg .
.
.
.
.
.
map map .
map .
.
.
.
map .
.
.
.
.
.
map .
.
.
.
.
.
map .
.
.
.
.
.
fr n a n a n a n a n a fr n a n a n a n a n a fr fr fr fr ar n a n a n a n a n a ar n a n a n a n a n a ar n a n a .
ar .
.
.
.
.
.
ar .
.
.
.
ar .
.
.
.
.
.
auc .
.
.
.
.
.
baselines are 4th 5th 5th 6th and 7th the bold values in table .
moreover ivdetect can detect and vulnerabilities among top top and top prediction results.
tables and show the comparison among the approaches on three datasets.
ivdetect consistently performs better in all the metrics .
for ndcg all the baselines get zeros because they did not have correct detections in top results.
ivdetect can improve ndcg from and ndcg from as compared to the baselines.
higher ndcg indicates that ivdetect achieves the ranking closer to the perfect ranking and the correct vulnerable methods appear higher in the top list.
for map scores ivdetect relatively improves over the baselines from for top and from for top accuracy.
with higher map ivdetect has higher precision on average for all the top ranked positions in the top list.
that is the top ranked result is highly precise in detecting the vulnerable methods.
ivdetect also achieves better first ranking fr and average ranking ar .
while its best fr is that of next best performer is .
for ar a correct vulnerable method is on average ranked by ivdetect .
.
positions higher in the ranked list than by the baselines.
our tool also has relatively higher auc from .
the comparative results on fanand reveal datasets are similar tables and .
in fandataset ivdetect can improve the ndcg and map scores over the baselines by for top10 and for top .
ivdetect s frs and arstable rq1.
method level vd on fan dataset vuldee peckersysevr russell et al.devign reveal ivdetect ndcg ndcg .
ndcg .
.
.
.
ndcg .
.
.
.
ndcg .
.
.
.
.
.
map map .
map .
.
.
.
map .
.
.
.
map .
.
.
.
.
.
fr n a n a n a n a n a n a fr n a n a n a n a n a fr n a n a fr n a n a fr ar n a n a n a n a n a n a ar n a n a n a n a n a ar n a n a .
ar n a n a .
.
.
ar .
.
.
.
auc .
.
.
.
.
.
table rq1.
method level vd on reveal dataset vuldee peckersysevr russell et al.devign reveal ivdetect ndcg ndcg .
ndcg .
.
ndcg .
.
.
.
.
ndcg .
.
.
.
.
.
ndcg .
.
.
.
.
.
map map .
map .
.
map .
.
.
.
.
map .
.
.
.
.
.
map .
.
.
.
.
.
fr n a n a n a n a n a n a fr n a n a n a n a n a fr n a n a n a n a fr n a fr fr ar n a n a n a n a n a n a ar n a n a n a n a n a ar n a n a n a n a ar n a ar .
.
.
.
ar .
.
.
.
auc .
.
.
.
.
.
are better from positions and .
.
positions for top and positions and .
.
positions for top .
in reveal dataset the improvements in ndcg map fr and ar are positions and positions for top and positions and .
.
positions for top .
the results on three datasets are different due to the ratio between the vulnerable and non vulnerable methods.
that ratio is and .
in fanand reveal datasets.
that number is .
in ffmpeg qemu dataset thus there are more vulnerable methods and the results are consistently higher across all the models.
table shows the precision and recall results of ivdetect and the baselines.
specifically ivdetect has higher precision than all the baselines across all three datasets.
it can improve precision by .
.
for recall it is marginally worse than reveal on fan and 299vulnerability detection with fine grained interpretations esec fse august athens greece table rq1.
precision and recall results of method level vd on three datasets p precision r recall f f score ffmpeg qemu fan reveal p r f p r f p r f vuldeepecker .
.
.
.
.
.
.
.
.
sysevr .
.
.
.
.
.
.
.
.
russell et al.
.
.
.
.
.
.
.
.
.
devign .
.
.
.
.
.
.
.
.
reveal .
.
.
.
.
.
.
.
.
ivdetect .
.
.
.
.
.
.
.
.
a map scores b ndcg scores figure scores from top to top on fan dataset a ndcg map scores b fr and ar figure rq1.
cross dataset validation training on reveal and ffmpeg qemu datasets testing on fan dataset.
ffmpeg qemu datasets .
and .
and worse than sysevr on fan dataset .
.
on the reveal dataset ivdetect outperforms reveal by .
in terms of precision but has lower recall by .
.
however in terms of f score ivdetect can outperform the best performing baseline reveal by .
on ffmpeg qemu dataset .
on fan dataset and .
on reveal dataset.
figure shows that ivdetect consistently has better map and ndcg scores when considering top to top ranked lists.
for cross dataset validation as seen in figure the results for map and ndcg in the within dataset setting are better than those in the cross dataset setting.
this is expected because the model might see similar vulnerable code before in the same projects in the same dataset.
the fr and ar values for the cross dataset setting are one rank higher than those of the within dataset setting.
figure shows our analysis on the overlapping results between ivdetect and the baselines on fandataset for top .
as seen ivdetect can detect and vulnerable methods that vuldeepecker sysevr russell devign and reveal missed respectively while they can detect only and vulnerable methods that ivdetect missed.
in summary ivdetect can detect and more vulnerable methods than the baselines.
figure overlapping analysis table rq2.
fine grained vd interpretation comparison interp.
accuracymfr marmodel n1 n2 n3 n4 n5 n6 n7 n8 n9 n10 att .
.
.
.
.
.
.
.
.
.
.
.
grad .
.
.
.
.
.
.
.
.
.
.
.
ge .
.
.
.
.
.
.
.
.
.
.
.
ge gnnexplainer nx x is the number of nodes in the interpretation .
rq2.
comparison with interpretation models for fine grained vd interpretation table shows the accuracy of different interpretation models.
as seen using gnnexplainer improves over att and grad from .
and .
in accuracy respectively as we vary the size of interpretation sub graphs i.e.
the number of statements from .
higher accuracy indicates that ivdetect can provide better fine grained vulnerability detection interpretation at the statement level.
that is in more cases if ivdetect detects correctly vulnerable methods it can point out more precisely the vulnerable statements relevant to the vulnerabilities.
for ranking vulnerable statements using gnnexplainer improves mfr by .
and .
ranks and improves mar by .
and .
ranks over att and grad.
att uses the edge attention in the graph attention network to assign the weights for the edges while gnnexplainer directly gives a score for the subgraph after masking.
thus for the case in which there are more than one path from a node to another the weight for an edge is the average weight of the weights through multiple paths i.e.
att might be less precise than gnnexplainer.
grad computes the gradient of the loss function with respect to the input for computing the weight of an edge.
however such gradient based approach may not perform well with respect to the discrete inputs an input graph is represented as an adjacency matrix .
as the number of nodes in gmincreases the number of statements covered also increases accuracy is higher.
however the computation time is higher and developers need to investigate more statements.
as seen when the number of statements is higher than accuracy increases more slowly.
thus we chose as a default.
.
rq3.
vulnerable code pattern analysis this section describes another experiment that we exploit ivdetect s capability of providing the interpretation sub graphs to mine the patterns of vulnerable code and fixes.
a vulnerable code pattern is a fragment of vulnerable code that repeats frequently i.e.
more than a certain threshold.
the detected vulnerability patterns and corresponding fixes are the good resources for developers to learn about the vulnerable code that others have frequently made and learn to fix the vulnerable code in the same patterns.
from the results in rq2 we first collected into a set gthe interpretation sub graphs gms with the correctly detected statements 300esec fse august athens greece yi li shaohua wang and tien n. nguyen table rq3.
numbers of vulnerable code patterns thresh thresh thresh thresh size size size size total pattern if is link stringliteral fprintf stderr error invalid etc skel .zshrc file n patt exit intliteral if copy file stringliteral var intliteral ... pattern var udf get filename var var var var if var ... goto label figure vulnerable code patterns as relevant to the vulnerability in the methods.
in total we obtain 700gms.
note thatgmis a sub graph of pdg.
for each gm we abstract out the variables names with a keyword var and the literals with their data types.
we then ran the sub graph pattern mining algorithm ongwith different thresholds of frequencies and collected different sizes of the sub graph patterns.
the outputs are the frequent isomorphic sub graphs within gms which are considered as vulnerable code patterns because we chose gm that contains correct interpretation statements relevant to the correctly detected vulnerabilities.
after manual verification we obtain a number of correct patterns .
as seen as the frequency threshold or the size of pattern is larger the number of patterns decreases as expected.
when they are both larger than we found no pattern.
let us explain a few examples.
figure shows two examples of vulnerable code patterns.
the first pattern lines and shows an api misuse in the project firejail involving is link ... exit and copy file ... .
the usage is to check the validity of a link and if yes to copy the file or otherwise to stop the execution.
this pattern appeared three times with different string literals and was fixed by developers to replace the statements.
an interesting observation is that ivdetect is able to eliminate thefprintf statement at line from the interpretation sub graph thus eliminating it from the pattern even though the fprintf statement appears with the other statements three times in the project.
this shows a benefit of ivdetect because if a tool does not have statement level vd interpretation and it mines pattern from the entire methods it will incorrectly include fprintf in the pattern.
the second pattern lines shows a pattern involving a vulnerable method call udf get filename and the checking on its return value.
the method was later fixed to add the 5thparameter.
another interesting finding is that ivdetect enables the discovery of not only vulnerable code patterns but also the fixing patterns for them.
figure shows two fixing patterns for vulnerable code.
the first vulnerability from linux kernel lines is about the method f16 update dst ... .
according to the commit log to avoid another thread changing a data record concurrently developers need to provide mutual exclusion access and deferencing.
this fixing pattern was repeated times in the methods dccp v6 send response fixing pattern var fl6 update dst var var var rcu read lock final p fl6 update dst var rcu dereference var var rcu read unlock fixing pattern char var malloc var char var if var var litconst error line stringliteral var return litconst var malloc var figure fixing patterns removal addition table rq4.
evaluation for the impact of internal features.
st a a sst b b ast c c var d d cd e e dd f ndcg .
.
.
.
.
.
ndcg .
.
.
.
.
.
map .
.
.
.
.
.
map .
.
.
.
.
.
fr fr ar .
.
.
ar .
.
.
.
.
auc .
.
.
.
.
.
st sequence of tokens sst sequence of sub tokens ast sub ast var variables cd control dependencies dd data dependencies f ivdetect inet6 csk route req and net6 csk route socket .
this fixing pattern would be useful for a developer to learn the fix from one method and apply to the other two methods.
the second pattern lines shows a fixing pattern to a vulnerability on buffer overflow with themalloc call in parsedsdiffheaderconfig method of wavpack .
.
according to cve this problem allows a remote attacker to cause a denial of service heap based buffer over read or possibly overwrite the heap via a maliciously crafted dsdiff file .
this fixing pattern occurred three times in the same project.
.
rq4.
sensitivity analysis for features table shows the changes to the metrics as we incrementally added each internal feature into our model in figure .
generally each internal feature contributes positively to the better performance of ivdetect as both the score metrics ndcg map and auc and the ranking metrics fr and ar are improved.
when ivdetect considers only the sequence of tokens st in the code the first correct detection fr is at the position thus ndcg and map not shown .
when considering the code as the sequence of sub tokens sst ivdetect deals with the unique tokens better because the sub tokens appear more frequently than the tokens .
at top fr improves positions ar improves .
positions and ndcg and map relatively improve .
and .
.
when ast is additionally considered the model can distinguish vulnerable code structures and statements.
at top fr and ar improve and .
positions and ndcg and map improve .
and .
.
however fr is still and ndcg and map not shown because tokens and ast do not help much discriminate the vulnerable statements.
the feature on variables also helps improve fr and ar from to and .
to .
and ndcg and map relatively improve .
and .
at top .
ndcg and map improve from to .
and 301vulnerability detection with fine grained interpretations esec fse august athens greece static int validate group struct perf event event ... if !validate event fake pmu leader if !validate event event pmu fake pmu leader return einval list for each entry sibling leader sibling list group entry if !validate event fake pmu sibling if !validate event event pmu fake pmu sibling return einval if !validate event fake pmu event if !validate event event pmu fake pmu event return einval ... figure a detected vulnerable method in android kernel to .
respectively not shown .
this feature allows the model to detect similar incorrect variable usages.
by additionally integrating control dependencies cd fr and ar improve from down to and .
down to .
and ndcg and map relatively improve .
and .
.
by adding data dependencies dd fr and ar improve from to and .
to .
.
ndcg and map improve .
and .
for top .
this result confirms that vulnerable code often involves the statements with control and or data dependencies .
figure shows a detected vulnerable method validate event ... was vulnerable and replaced with a new version with an additional parameter.
we used the models a f for detection and observed that the rank for validate event ... in the candidate list improves from a to b c d e and f .
while the features on tokens sub tokens and ast are contributing they do not help much because the model did not see them in vulnerable methods before.
however the variable method names especially control data dependencies between the surrounding statements and validate event ... help discriminate this vulnerability and push it to the top list.
control dependencies e.g.
between validate event ... and return einval help improve ranks.
generally the improvement in ranking shows the positive contributions of all the features.
this example also shows a fixing pattern appearing three times with different variables leader sibling and event .
.
rq5.
sensitivity analysis on training data as seen in table with more training data the performance is better as expected.
even with ivdetect still achieves ncdg of .
and map of .
which are still higher than those of the other baselines for top highest ndcg and map of the baselines are .
and .
.
with less training data vs ivdetect drops auc only by .
.
.
rq6.
time complexity to generate the interpretation sub graphs for all methods it takes about days days and days to finish on fan reveal and ffmpeg qemu datasets respectively.
it took hours to train ivdetect on fan reveal and ffmpeg qemu datasets.
for vd prediction it takes only 2s per method.
threats to validity.
we only tested on the vulnerabilities in c and c code.
in principle ivdetect can apply to other programming languages.
we tried our best to tune the baselines on same dataset for fair comparisons.
we focus only on dl based vd models.table rq5.
sensitivity analysis on training data train tune test ndcg map fr ar auc .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
related work various techniques have been developed to detect vulnerabilities.
the rule based approaches were developed to leverage known vulnerability patterns to discover possible vulnerable code such as flawfinder rats its4 checkmarx fortify and coverity .
typically the patterns are manually defined by human experts.
the state of the art vulnerability detection tools using static analysis provide the rules for each vulnerability type.
another type of vd approaches is machine learning ml based or metrics based.
typically these approaches require human crafted or summarized metrics as features to characterize vulnerabilities and train machine learning models on the defined features to predict whether a given code is vulnerable or not.
various ml based approaches have been built on top of distinct metrics such as terms and their occurrence frequencies imports and function calls complexity code churn and developer activity dependency relation api symbols and subtrees .
recently deep learning dl has been applied to detect vulnerabilities.
for example some approaches train a dl model on different code representations to detect vulnerabilities such as the lexical representations of functions in a synthetic codebase code snippets related to api calls to detect two types of vulnerabilities syntax based semantics based and vector representations graph based representations .
none of them is designed to provide interpretations for a model in term of vulnerable statements.
conclusion we present ivdetect a novel dl based approach to provide subgraphs in pdg that explains the prediction results of graph based vulnerability detection.
our empirical evaluation on vulnerability databases shows that ivdetect outperforms the existing dl based approaches by and in top ndcg and map ranking scores.
our key limitations include un seen vulnerabilities the vulnerable statements incorrectly identified due to data control dependencies with vulnerable ones missed vulnerable statements due to multiple edges of data control dependencies.
with ivdetect being a ml dl based vulnerability detection model we aim to raise the level of ml dl based approaches which are not able to point out the statements that caused the model to predict the vulnerability.
thus we compared ivdetect with the detection approaches of the same category.
in the future we plan to compare ivdetect with static analysis vd approaches.