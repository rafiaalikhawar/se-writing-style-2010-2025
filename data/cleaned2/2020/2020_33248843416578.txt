retrieve and refine exemplar based neural comment generation bolin wei key lab of high confidence software technology moe peking university beijing china bolin.wbl gmail.comyongmin li key lab of high confidence software technology moe peking university beijing china liyongmin pku.edu.cnge li key lab of high confidence software technology moe peking university beijing china lige pku.edu.cn xin xia faculty of information technology monash university australia xin.xia monash.eduzhi jin key lab of high confidence software technology moe peking university beijing china zhijin pku.edu.cn abstract codecommentgenerationwhichaimstoautomaticallygenerate naturallanguagedescriptionsforsourcecode isacrucialtaskin the field of automatic software development.
traditional comment generationmethodsusemanually craftedtemplatesorinformation retrieval ir techniques to generate summaries for source code.
in recent years neural network based methods which leveraged acclaimed encoder decoder deep learning framework to learn commentgenerationpatternsfromalarge scaleparallelcodecorpus haveachievedimpressiveresults.however theseemergingmethods only take code related information as input.
software reuse is commonintheprocessofsoftwaredevelopment meaningthatcomments of similar code snippets are helpful for comment generation.
inspired by the ir based and template based approaches in this paper weproposeaneuralcommentgenerationapproachwhere weusetheexistingcommentsofsimilarcodesnippetsasexemplars to guide comment generation.
specifically given a piece of code we first use an ir technique to retrieve a similar code snippet and treat its comment as an exemplar.
then we design a novel seq2seq neural network that takes the given code its ast its similar code and its exemplar as input and leverages the information from the exemplartoassistinthetargetcommentgenerationbasedonthese manticsimilaritybetweenthesourcecodeandthesimilarcode.weevaluateourapproachonalarge scalejavacorpus whichcontains about 2m samples and experimental results demonstrate that our model outperforms the state of the art methods by a substantial margin.
corresponding authors.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
ase september virtual event australia association for computing machinery.
acm isbn ... .
concepts computing methodologies artificial intelligence softwareanditsengineering softwaremaintenancetools .
keywords comment generation deep learning acm reference format bolinwei yongminli geli xinxia andzhijin.
.retrieveandrefine exemplar basedneuralcommentgeneration.in 35thieee acminternationalconferenceonautomatedsoftwareengineering ase september virtualevent australia.
acm newyork ny usa 12pages.
introduction code comments provide a clear natural language description for a piece of the source code which can help software developers understandprogramsquicklyandcorrectly .previousstudies showed that during software maintenance program comprehensiontakesmorethanhalfofthetime .althoughproper commentsareveryhelpful forsoftwaremaintenance theyareabsentorout datedinmanysoftwareprojects .ontheotherhand manually writing comments is very time consuming and labor intensive and the comments should be updated as the softwareis upgraded.
therefore automatic comment generation becomes greatly crucial for software development and maintenance.
creating manually crafted templates is a common way to generate comments automatically .
these methods defined different templates for differenttypes of programs to generate readable text descriptions.
sridhara et al.
used software word usage modelandheuristicstoselectimportantcodestatements defined templatesforeachcodestatement andgeneratedcorresponding comments.morenoetal.
predefinedheuristicrulestoextract information from source code and defined templates for different typesofinformationtohelpgeneratecodesummaries.manuallycraftedtemplatesareintroducedintheseapproachestoextractkeyinformationinthesourcecodeintocomments helpingimprovethe readability and comprehensibility of comments.
however defining atemplateisatime consumingtaskandrequiresextensivedomain knowledge.
also different projects might use different kinds of templates.
35th ieee acm international conference on automated software engineering ase asanalternative informationretrieval ir techniquesarewidely used in automatic comment generation .
some researchersusedirtechniquestoselecttermsfromsourcecodeto generateterm basedcomments .haiducetal.
applied thevectorspacemodelandlatentsemanticindexingtoretrieve the appropriate terms while eddy et al.
introduced a hierarchical topic model for comment generation.
based on the idea that softwarereuseiscommon otherresearchersleveragedcode clonedetectiontechniquestodetectsimilarcodesnippetsandused their corresponding comments for comment generation.
note that similarcodesnippetscanberetrievedfromexistingopen source software repositories in github or software q a sites such as stackoverflow .however codesnippetsmaycontainsome informationthatisinconsistentwiththecontentincommentsof their similar code snippets.
in recent years more and more researchers have focused on applyingneuralmachinetranslationmodelsforcommentgenera tion and viewedthe processof generatingcomments fromsource code as a language translation task e.g.
translating english to german .theseresearchworkshaveadoptedthemainstream encoder decoder framework of neural machine translation withsourcecodeasinputandcommentsasoutput andachieved state of the artperformanceonthecommentgeneration.themain difference between these works is the source code encoding methods.iyeretal.
directlymodeledthesourcecodeasasequenceof tokens while huet al.
used the traversal sequence of abstract syntax tree ast tokens as the model input.
leclair et al.
integrated previous work and used two different ways to representsource code.
by virtue of the naturalness of the source code theseneuralmodelscanminepatternsforgeneratingcomments fromlargecorpora buttheyonlyreliedonsourcecodeinformation such as tokens or structures of source code to create comments.
notethattheseaforementionedcommentgenerationmethods have their own advantages.
the comments generated based on themanually craftedtemplatemethodsareusuallyfluentandinformative their basedmethodscantakeadvantageoftokensin comments of similar code snippets the neural based methods can learnthesemanticconnectionbetweennaturalandprogramming languages.
although the neural based methods have achieved the bestperformance ittendstogeneratehigh frequency wordsincommentsor losecontrol sometimes.forexample according to leclair et al.
s study comments in the test set contain tokens with the frequency of less than .
conversely only7 commentspredictedbytheirproposedapproachcontain tokens with the frequency of less than .
besides more than two thousandgeneratedcommentsevendonothaveanormalend ofsequence s token.
specifically the comments generated by the neural model suffer a loss in readability and informativeness.
this phenomenon also appears in the use of neural models in machine translation .therefore wearguethatitisnotenoughforthe neural model to generate comments only based on the source code.
inspired by the template based methods and ir based methods weassumethatcommentsofsimilarcodesnippetscanberetrieved as templates to guide the process of neural comment generation.
these templates onthe one hand provide reference examplesfor generating comments and on the other hand may contain lowfrequencywordsrelatedtothesourcecode enhancingtheneuralmodel sabilitytooutputlow frequencywords.consideringthedif ferencesbetweencommentsofsimilarcodesnippetsandmanually craftedtemplates wecallexistingsimilarcommentsas exemplars .
due to the strong pattern recognition capabilities of neural networks we argue that encoder decoder neural networks can be combined with traditional template based and ir based methods.
therefore in this paper we propose a novel comment generationframework namelyre2com whichconsistsoftwomodules aretrieve module and a refine module.
in the retrieve module givenaninputcodesnippet weexploitirtechniquestoretrieve themostsimilarcodesnippetfromalargeparallelcorpusofcode snippetsandtheircorrespondingcomments andtreatthecomment of the similar code snippet as an exemplar.
in the refine module we apply a novel seq2seq neural network to learn patterns for generatingcomments.morespecifically theencodertakesthegiven codesnippet thesimilarcodesnippet andtheexemplarasinput andthedecodergeneratesthe tokensequenceofacomment.itis worth noting that the similar code snippet retrieved may not be semantically similar to the given code snippet.
with the similar code snippetasinput wecanperformasemanticcomparisonthrough a neural network and decide whether to use the exemplar basedon the degree of similarity.
furthermore we adopt the attention mechanism to focus on the important parts of the input.
in the testing phase given a new piece of code snippet without a comment our approach retrieves a similar code snippet and comment pair from the corpus utilizes the trained neural model to generate anannotation andselectstokenswiththehighestprobabilityin thevocabularyastheoutput.inthisway weleveragetheadvantages of template based and ir based methods and model them intotheneuralnetworktoimprovetheperformanceofcomment generation.
to train and evaluate our approach we conduct experiments on a real world java dataset.
the dataset comes from the sourcerer repository1andhasbeenprocessedbyleclairetal.
including removing duplicates dividing into training validation and test sets by projects.
we employthe evaluation metrics bleu score in machinetranslationtoevaluatethegeneratedcommentsandalso performahumanevaluation.experimentalresultsshowthatour methodperformssubstantiallybetterthantheir basedmethodand outperforms the state of the art approaches.
besides experimental results also show that our proposed modules are orthogonal to other techniques i.e.
applying the retrieve and refine modules to other neural models can improve the performance of the models.
the contributions of our work are shown as follows we propose an exemplar based neural comment generation method which combines traditional template based and irbasedmethodswithneuralmethods.weusecommentsof similar code snippets as exemplars to assist in generating comments.
weconductextensiveexperimentstoevaluateourapproach onalarge scaledatasetofjavamethods.theexperimental results show that our retrieve and refine modules substantially improve the performance of the neural model and achieve the state of the art results.
lopes datasets !
!
!
!
!
!
!
!
!
!
!
!
!
!
!
.
!
!
.
!
!
!
!
!
!
!
!
!
!
.
!
!
.
a an example of the retrieved similar code snippet and the input code that are semantically similar.
!
!
!
.
.
.
!
.
!
!
!
!
!
.
.
b an example of retrieved similar code snippet and input code that are not semantically similar.
figure1 examplesofexemplar basedcommentgeneration.
sametokensingroundtruths exemplarsandpredictionsofast attendgruaremarkedinred.sametokens splitoncamelcase intheinputcodeandthesimilarcodearealsomarkedin red.
paperorganization .therestofourpaperisorganizedasfollows.
section describes motivating examples.
section presents our proposed method.
section and section describe the experiment setup and results.
section and section discuss some results and describetherelatedwork respectively.finally section8concludes the paper and points out future directions.
motivating examples to explainwhywe usethecomment ofthe retrievedsimilarcode snippet as an exemplar to guide the neural model to generate acomment we selecttwo representativeexamples fromthe dataset usedintheevaluation asshowninfigure1.theinputcodeandthe similar code are java methods and we also display the comments predicted by the ast attendgru model for the input code.
for theinputcode weleveragetheopen sourcesearchenginelucene2 to retrieve the most similar code snippet from the training corpus.
the retrieval technique is based on the lexical level similarity of the source code which will be explained in detail in section .
.
comment generation methods based on neural networks are difficulttogeneratelow frequencytokens whereascommentsof similar code snippets selected based on ir based methods may containlow frequencytokens.forexample infigure1a wecan observethatthespecificphrase sleepingonthisconditionvariable appearsinboththegroundtruthandtheexemplar meaningthat the input code and the similar code are semantically similar.
in addition sleeping is a low frequency token in the corpus which appearsonly71times.thisisoneofthereasonsthattheprediction of the ast attendgru ignores the token.
although the prediction of the neural network is very close to the ground truth it still lacks some key information in the source code.
therefore with the exemplarasinputtotheneuralmodel thelow frequencytokens willaffectthecommentgenerationprocess andtheneuralnetwork will generate more informative comments.
however itisnotenoughtoonlytakeanexemplarasadditional input andthesimilarcoderetrievedbysearchenginesisnotnecessarily semantically similar which is partly due to the fact that there isno realsourcecode reusein thecorpus and partlydue to thelimitationsoftheretrievaltechnique.forinstance infigure1b eventhoughtheinputcodeandthesimilarcodehavesomesame tokens tokensinthesourcecodearesplitoncamelcase.
theyare not similar in semantics and behavior.
in this case the exemplar is unsuitableforguidingthecommentgenerationprocessofneural networks.incontrast ast attendgrucangenerateacommentthatisclosetothegroundtruthwithouttheexemplar.forthisreason wearguethatitisnecessarytousethesimilarcodeandtheinputcode asthe inputoftheneural networkatthe sametime andcalculate thesemanticsimilaritybetweenthesimilarcodeandtheinputcode throughtheneuralmodel anddeterminethedegreeofusingthe exemplaraccordingtothesemanticsimilarity.wedesignanovel network structure to implement this idea the details of which will be described in section .
.
in view of the many discussions on the effectiveness of deep learning methods in the field of software engineering in recentyears we think that our study may be a good starting point combiningtraditionalmethodsonspecifictaskswithdeep learning methods.
previous methods applied neural networks tosolve tasks in software engineering.
although specific input for specific tasks was proposed such as ast and control flow graphs previous researchers did not analyze the existing problems of deep learningmethods e.g.
overfitting whichtendstogeneratehighfrequencyterms .therefore webelievethattraditionalmethods can be modeled into neural networks to improve performance.
figure an overview of our approach for exemplar based comment generation.
proposed approach in this work we propose the exemplar based comment generation method re2com modeling traditional ir based and templatebasedmethodsintotheneuralmethod.differentfromtheir based methods weemployaneuralnetworktomodifythecommentofthesimilarcodetoconformtothesemanticsoftheinputcode.differentfromthetraditionaltemplate basedmethod whichrequiresmanual definitionofthetemplate wetreatthecommentoftheretrievedcode as an exemplar.
re 2com consists of two parts a retrieve module and a refine module.
the retrieve module uses the irtechnique to explore the similar code and extract its comment fromaparallelcorpus whiletherefinemoduleisanovelneural network based on a seq2seq network with an attention mechanism to generate a comment.
the overall framework is illustrated in figure .
the data preprocessingstepreferstotheextraction cleaning andpartitionof thedataset andthetrainingandteststepreferstothere2com.we useamassivetrainingsetasaretrievalcorpuswhiletrainingthe refine module.
the details of the retrieve module are describedin section .
.
to take advantage of the structure information of inputcode wenotonlyusethetokensequencerepresentationof the code but also use the ast of the code as an input of the refine module.
the details of this part will be described in section .
.
.
retrieve module consideringthatsoftwarereuseiswidespreadinsoftwaredevel opment a similar code snippet usually has a similar comment.
furthermore as we analyzed in section there are some potential problemswiththepreviousneural basedmethods.therefore we arguethatitisbeneficialfortheneuralnetworktouseanexemplar asareferencewhengeneratingnewcomments.inpractice developershavesimilarexperiencesduringsoftwaredevelopment.for example whenaddingacommenttoapieceofsourcecode they willrefertothecommentofasimilarcodesnippet.inourframework the goal of the retrieve module is to retrieve similar code from a retrieval corpus given the input code and treat its comment as an exemplar.
toidentifywhichpieceofcodeintheretrievalcorpusismost similar to the input code we need to define and calculate the similarity between two pieces of code snippets.
in this work we chose the similarity of the lexical level of the source code to measure thecode similarity which was inspired by .
specifically we adoptbm25asthesimilarityevaluationmetric whichisabag ofwords retrieval function to estimate the relevance of documents to agivensearchqueryinir.givenaqueryandadocument basedon tf idf the bm25scoringfunctioncalculatesthetermfrequency in the document of each keyword in the query and multiplies it by the inverse document frequency of this term.
the more relevant two documents are the higher the value of bm25score is.
we leverage the open source search engine luceneto build our retrieve module.
since the size of the training set is quite large over1.9m weuseitastheretrievalcorpus i.e.
giventheinput codesnippet wesearchforthemostsimilarcodefromthetraining set.
the retrieve module contains two parts creating the indexand searching.
we first tokenize the source code and comments usingwhitespaceanalyzer inlucene.thenweprocesseachcode and comment pair into a document add it to the index library and store it on disk.
in the search phase for each query code we get similar code sequences arranged in descending order of similarity choose the first ranked similar code when training we choose the second ranked and use its comment as an exemplar.
we keep the default settings of bm25in lucene.
.
refine module oncewehaveanexemplar astraightforwardwayistotreatitas acomment forthe inputcode.
however dueto thenon existence of software reuse or the limitation of retrieval technology thereisa certaindifferencebetweenthe semanticof theexemplar and the semantic of the input code.
especially the similar code usually contains information that is inconsistent with the input code such asdifferentapicallsandoperations.therefore weusetheexemplarasasoft templateandrefineitaccordingtothesemanticdifference betweenthesourcecodeandthesimilarcode.basedonawidely usedseq2seqneuralnetwork wedesignanovelnetwork structure that can learn the semantic similarity between the input code and the similar code refine the exemplar and generate a comment.
therefinemodulecontainsthreecomponents fourencoders a decoder and an attention mechanism module between encoders and the decoder.
figure illustrates the detailed refine module.
.
.
encoders.
the four encoders take a token sequence of the input code x an ast traversal sequence of the input code t a 352tokensequenceofthesimilarcode sandtheexemplar rasinput respectively.
amongthem the inputcode xandits asttraversal sequence tconstitute the input code representation in figure .
we use the structure based traversal sbt method to obtain the traversal sequence of ast to utilize the structural information oftheinputcode.thereisnodifferenceinthestructureofthefour encoders.
take the input code xas an example.
theencoderoftheinputcodefirstmapstheone hotembedding of a token wiinto a word embedding xi xi w latticetop ewi whereweisatrainableembeddingmatrix.thentoleveragethecontextualinformation weuseabidirectionallongshort termmemory lstm toprocessthesequenceofthewordembeddings which isexplicitlydesignedtoavoidthelong termdependencyproblem.
ateachtimestep t thehiddenstateoftheforwardlstm htcan be represented by ht lstm xt ht the hidden states of the backward lstm can be obtained with anotherlstm.
weconcatenatehidden statesfromtwo directions as the representation of the t th token htin the input code i.e.
ht .
for the traversal sequence of ast the similar code and the exemplar we get their respective hidden states as ht hs andhrinthesameway.wedenotethehiddenstatesoftokensof theinputcodeas hx.notethatinourexperiments weusedfour separate lstms to encode different input sequences.
then we explore the difference between the input code and the similar code using a nonlinear sigmoidfunction to obtain a semantic similarity score sim sim wsim hx hs wherewsimare trainable weights stands for the sigmoidfunction andtheindex standsforthelasthiddenstate.accordingto previousworkinthenaturallanguageprocessingcommunity thisstructureperformswellintherelevancemeasurement.alarger valueofthescore sim rangesfrom0to1 indicatesthatthesemantics of the input code and the retrieved code is more similar.
.
.
attention.
attention is a component that allows the decoder to focus and place more attention on the relevant parts of the input sequence as needed.
it is useful to introduce this mechanism intothecommentgenerationmodel.forexample whendevelopers writecomments theyusethetoken get becausetheynoticethe token return inthesourcecode.therefore wearguethatdifferent parts of the comment are related to different parts of the source code.
similarly after the introduction of the exemplar the decoder can also focus on some parts of the exemplar when generating comments.
the attention mechanism in the refine module is built by the classic method of bahdanau et al.
.
takethe attentionbetweenthetarget commentandtheinput code as an example.
specifically for each target token yi w eu s e thehiddenstateofitsprevioustoken h prime i 1tocalculatetheattention figure the structure of the refine module.
the calculation details of the sim block and the combination blockare equation and equation respectively.
the dashedlines represent information used to initialize the decoderand to calculate the context vector.
weights as tildewide ij a h prime i hx j ij exp tildewide ij summationtext.
kexp tildewide ik whereais an alignment model which scores how well the input around position jand the output at position imatch.
we use a multi layer perception mlp unit as the alignment model.
then the context vector cx iis computed as a weighted sum of all hidden states of the input code cx i summationdisplay.
j ijhx j theattentionweightsandcontextvectorfortheexemplar cr iand the ast traversal sequence ct ican be computed in the same way.
.
.
decoder.
thepurposeofthedecoderistogeneratethetarget comment y.whengeneratingthe t thtokeninthecomment the decoder first uses an lstm to get the t th hidden state h prime t h prime t lstm h prime t yt theinitialstateofthedecoderisacombinationoftheinputcode representation and the last hidden state of the exemplar h prime hc sim hr sim wherehcis the feature vector of the input code which is obtained byconcatenatingthelasthiddenstateof xandthelasthiddenstate oftand performing an affine transformation hc wc hx ht bc wherewcandbcaretrainableparameters.thepurposeofthecombination in eqn.
is that if the input code is different semantically from the similar code that is the value of the similarity score is low then the decoder should pay more attention to the content of 353the input code.
we can obtain the context vector ctin the same way c prime t wc cx t ct t bc sim cr t sim then the probability of a token ytis conditioned on the context vectorctand its previous generated tokens y1 ... yt i.e.
p yt yt ... y1 x t s r g yt h prime t ct wheregis a mlp layer with the softmaxactivation function.
thetrainingobjectiveoftherefinemoduleistominimizethe cross entropy h y nn summationdisplay.
i summationdisplay.
jlogp yi j yi j xi ti si ri wherenisthetotalnumberoftrainingsamples and yi jmeansthe jthtokeninthe i thsample.throughgradientdescentoptimization methods the parameters of the refine module can be estimated.
duringinference weuseabeamsearch togeneratecomments.
specifically the decoder generates the comment token by token fromleft to rightwhilekeeping b bestcandidatesateachtimestep wherebis the beam size.
experiment setup dataset.
we evaluate our approach on the dataset provided by leclairetal.
.theoriginaldatasetcomesfromlopesetal.
containing .
million java methods from the sourcerer repository.
because the original dataset contains a large number of samples thatarenotsuitableforevaluatingneuralmodels suchasrepeated and auto generated code leclair et al.
preprocessed the data.
more specifically they first extracted java methods and comments from the code repository.
assuming the first sentence of the javadoc summarizes the method s behavior the authors extracted the first sentence or line from the javadoc as a comment of the method and filtered out non english samples.
considering that the auto generated and duplicate code due to name changes code cloning etc.
willhaveanegativeimpactonneuralmodelevaluation the authors removed these samples using heuristic rules andaddedunique auto generatedcodetothetrainingsettoensure that no testing was performed on these samples.
after splitting camel case and underscore tokens removing non alpha characters andsettingtolowercase theauthorsdividedthedatasetby project into training validation and test set meaning that all methods in one project are grouped into one category.
they argue that the preprocessing of the dataset is necessary for evaluating theperformanceofneuralmodels.withoutthesepreprocessing theevaluation results of neural models will be inflated.
for example in the icpc paper the reported bleu score of deepcom is about while the result on this dataset is only about .
aftergettingtheprocesseddataset theauthorsusedthe srcml tool to parse the source code into ast and traversed the astthrough the sbt method to convert the ast into a token sequence.tosimulatemorecomplicatedscenarios suchasmissing keywordsinthesourcecode duetopoorly writtencodeorsome scenarioswithonlybytecode theyreplacedalltokensinthesource code with a other token and got a token sequence called sbtaoforsbtastonly.insuchcases onlythestructureofastistable statistics of datasets dataset train valid test count avg.
tokens in comment .
.
.
avg.
tokens in code .
.
.17avg.
tokens in sbt ao .
.
.
a code length distribution b comment length distribution figure length distribution of test data preserved.thentheauthorscreatedtwodatasetstoevaluatethe performance of neural models.
thestandarddataset containsthreeelementsforeachsample acodesequenceofthejavamethod ansbt aosequence of java method and a comment.
thechallengedataset containstwoelementsforeachsample an sbt ao sequence of java method and a comment.
thechallengedatasetisusedtoevaluatetheperformanceofneural modelswhenonlytheaststructureisavailable.inourexperimentalsetup theretrievalcorpusisconstructedusingsourcecodetoken sequence and comment pairs from the training set.
for training and evaluation we select the second ranked since the first ranked similar code is itself and top ranked retrieved code as the similar code respectively.thestatisticalresultsofthedatasetareshown in table .
figure shows the length distribution of source code and comment on the test data.
trainingdetails.
ourmodelisimplementedbasedonthetensorflow framework.
we set token embeddings and lstm states to dimensionsand256dimensionsrespectively.theout of vocabulary tokensarereplacedby unk.tomaximizetheutilizationofgpu memory wesetthebatchsizeto256.wechoosethewidely used stochastic gradient descent to optimize all parameters with the initiallearningrateof0.
.thelearningrateisdecayedwithafactorof .
every epoch.
to mitigate overfitting we use dropout with .
.
andtopreventexplodinggradient weclipthegradientsnormby .
according to the statistics of the dataset in figure we limit the maximum length of the encoder lstm to and the maximum length of the decoder to .
training runs for about epochs and thebestparametersareselectedaccordingtotheperformanceof the validation set.
during the test the beam size bis set to .
each experiment is run three times and the average results are reported.
weconductourexperimentsonalinuxserverwiththenvidia gtx titan xp gpu with gb memory.
evaluationmetrics.
followingthepreviouscommentgeneration work weevaluatedifferentapproachesusingthemetric bleu .
bleu measures the quality of generated comments and 354canrepresentthehuman sjudgment whichcalculatesthesimilarity betweenthegeneratedcommentsandreferences.itisdefinedas thegeometricmeanof n grammatchingprecisionscoresmultiplied by a brevity penalty to prevent very short generated sentences bleu bp exp n summationdisplay.
n 1wnlogpn wherepnis then gram matching precision scores nis set to in our paper and bpis a brevity penalty to prevent very short generated sentences.bleu score rangesfrom to the higher thescore themore thecandidate correlatestothe reference.this evaluation metric is also widely used in various tasks of automatic softwareengineering.liuetal.
introducedbleutoevaluatethe quality of the generated commit message.
gu et al.
employed it to evaluate the accuracy of the generated api sequence.
jiang et al.
exploitedittoevaluatethegeneratedsummariesforcommit messages.theirexperimentsshowthatitisreasonabletousebleu to evaluate the quality of comments.
in our experiments we report a composite bleu score in addition to bleu 1through bleu .
results to evaluate our approach in this section we will answer the following research questions rq1 how does the re2com perform compared to the stateof the art neural models?
rq2 how effective is the exemplar to all neural models?
rq3 how does the re2com perform compared to the ir methods?
in the challenge dataset all models have no source code tokens as input which is an experimental scenario to evaluate the ability ofthemodeltouselimitedinformation.incontrast thestandard datasetisclosetothereal worldscenariowherealltheinformation of the source code is available.
hence we evaluate our approach andallbaselinesonthestandarddatasetinthissectionanddiscuss the experimental results on the challenge dataset in the section .
.
rq1 re2com vs. neural baselines .
.
baseline.
to answer this research question we compare our approach to four state of the art neural methods.
code nn isthefirstdeeplearningmodelandthefirst end to end encoder decoder framework for comment generationtask.itencodesthesourcecodesequenceintotoken embeddings then uses an lstm as a decoder to generate comments andemploystheattentionmechanismtointroduce information on the encoder side.
note that code nn only uses token embedding as the encoder not the lstm.
attendgru is a standard attentional seq2seq model where the encoder and the decoder are both gated recurrent unit gru .
gru is similar to lstm and is a variant of rnn.
for a fair comparison we replace gru with lstm in the model.
hence the difference between this model and code nn is whether the encoder uses an rnn.
ast attendgru isalsoanattentionalseq2seqmodel.differentfromattendgru itintroducesthestructureinformation of the source code and uses a new encoder to process thetable2 theperformanceofourmodelcomparedwithneural baselines.
methods params b b1 b2 b3 b4 code nn .3m .
.
.
.
.
deepcom .9m .
.
.
.
.
attendgru .7m .
.
.
.
.
ast attendgru .7m .
.
.
.
.
re2com .4m24.
.
.
.
.
traversalsequenceofast.itconcatenatestheinformation fromthetwoencodersasinputtothedecoderandgenerates comments.
in our experiments we used lstms as encoders for a fair comparison.
deepcom isaseq2seqmodelthatuseslstmsasthe encoder and the decoder and also utilizes the attention mechanism.itisthefirstcommentgenerationmodelusing ast straversalsequenceasinputandproposedthetraversal method sbt.
we set the embedding size and lstm states of all baselines to dimensions which can ensure that the number of re2com s parameters is less than the number of baselines parameters.
as in theicse 19paper wedidnotcompareourmodelwithother baselinesinthefieldofnaturallanguageprocessing becausethe tricksintroducedbythosemodelswouldmakeitdifficulttocompare exactly which part of the model played a key role.
compared with theabovefourbaselines itcannotonlyexplaintheeffectiveness of our model but also show that the components of our model arehelpful.
.
.
results.
we calculate the gap between the comments generated by different methods and the ground truth.
the experimental results are shown in table .
the bleu scores of the best baseline ast attendgruarecomparabletothosereportedinthestudy althoughwemadesomemodificationstotheirencoders.thisresult shows that the performance difference of an lstm and a gru on this task is very limited.
although ast attendgru introduces structuralinformationofthesourcecodecomparedtoattendgru it does not substantially improve the results.
the phenomenon also appears in the paper explaining that after excluding custom identifiers in the ast the structural information of the source codehaslimitedhelpingeneratingcomments andtheinformation containedinthetokensequenceofsourcecodeissufficient.theperformance of deepcom is much lower than the results in indicatingthattheirdatapreprocessinghaspotentialproblems.the auto generated and duplicate code has agreat negative impact on the experimental results.
the similar conclusion was reached in theallamanis spaper .thedifferencebetweendeepcomand attendgruisonlyintheinputinformation whiletheformerisabout5pointsworsethanthelatter.onereasonisthattheast straversal sequenceprocessedbydeepcomisabout7timeslongerthanthe tokensequenceprocessedbyattendgru whichmightcontainmoreuselessandredundantinformation.koehnandknowles found that encoder decoder frameworks have low generation quality on verylongsentences.fromtable2 wealsonoticethatcode nn performsworstcomparedwithothermethods sinceitdoesnotuse anrnntoprocessthetokensequence whichmakesitunableto graspthesemanticinformationofthesourcecodecontext.fromthe 355table effectiveness of exemplar on all neural methods.
methods params bb 1b 2b 3b code nn e .
.0m .
.
.
.
.
deepcom e. .4m .
.
.
.
.
attendgru e. .2m .
.
.
.
.
ast attendgru e. .2m .
.
.
.
.
re2com .4m24.
.
.
.
.
results wecanalsoobservethatbleu 1tobleu 4areindescending order.bleu 1isveryhighcomparedtobleu 4onallmodels revealing that the matching accuracy of the grams between comments generated by neural networks and gold